{"title": [{"text": "Domain Adaptation via Pseudo In-Domain Data Selection", "labels": [], "entities": [{"text": "Domain Adaptation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7354386448860168}]}], "abstractContent": [{"text": "We explore efficient domain adaptation for the task of statistical machine translation based on extracting sentences from a large general-domain parallel corpus that are most relevant to the target domain.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 21, "end_pos": 38, "type": "TASK", "confidence": 0.7678935825824738}, {"text": "statistical machine translation", "start_pos": 55, "end_pos": 86, "type": "TASK", "confidence": 0.746331512928009}]}, {"text": "These sentences maybe selected with simple cross-entropy based methods, of which we present three.", "labels": [], "entities": []}, {"text": "As these sentences are not themselves identical to the in-domain data, we call them pseudo in-domain subcorpora.", "labels": [], "entities": []}, {"text": "These subcorpora-1% the size of the original-can then used to train small domain-adapted Statistical Machine Translation (SMT) systems which out-perform systems trained on the entire corpus.", "labels": [], "entities": [{"text": "Statistical Machine Translation (SMT)", "start_pos": 89, "end_pos": 126, "type": "TASK", "confidence": 0.7788586467504501}]}, {"text": "Performance is further improved when we use these domain-adapted models in combination with a true in-domain model.", "labels": [], "entities": []}, {"text": "The results show that more training data is not always better, and that best results are attained via proper domain-relevant data selection, as well as combining in-and general-domain systems during decoding.", "labels": [], "entities": []}], "introductionContent": [{"text": "Statistical Machine Translation (SMT) system performance is dependent on the quantity and quality of available training data.", "labels": [], "entities": [{"text": "Statistical Machine Translation (SMT)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.8351669609546661}]}, {"text": "The conventional wisdom is that more data is better; the larger the training corpus, the more accurate the model can be.", "labels": [], "entities": []}, {"text": "The trouble is that -except for the few all-purpose SMT systems -there is never enough training data that is directly relevant to the translation task at hand.", "labels": [], "entities": [{"text": "SMT", "start_pos": 52, "end_pos": 55, "type": "TASK", "confidence": 0.9879109859466553}, {"text": "translation task", "start_pos": 134, "end_pos": 150, "type": "TASK", "confidence": 0.9196774661540985}]}, {"text": "Even if there is no formal genre for the text to be translated, any coherent translation task will have its own argot, vocabulary or stylistic preferences, such that the corpus characteristics will necessarily deviate from any all-encompassing model of language.", "labels": [], "entities": []}, {"text": "For this reason, one would prefer to use more in-domain data for training.", "labels": [], "entities": []}, {"text": "This would empirically provide more accurate lexical probabilities, and thus better target the task at hand.", "labels": [], "entities": []}, {"text": "However, parallel in-domain data is usually hard to find 1 , and so performance is assumed to be limited by the quantity of domain-specific training data used to build the model.", "labels": [], "entities": []}, {"text": "Additional parallel data can be readily acquired, but at the cost of specificity: either the data is entirely unrelated to the task at hand, or the data is from abroad enough pool of topics and styles, such as the web, that any use this corpus may provide is due to its size, and not its relevance.", "labels": [], "entities": []}, {"text": "The task of domain adaptation is to translate a text in a particular (target) domain for which only a small amount of training data is available, using an MT system trained on a larger set of data that is not restricted to the target domain.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 12, "end_pos": 29, "type": "TASK", "confidence": 0.7406143397092819}]}, {"text": "We call this larger set of data a general-domain corpus, in lieu of the standard yet slightly misleading out-of-domain corpus, to allow a large uncurated corpus to include some text that maybe relevant to the target domain.", "labels": [], "entities": []}, {"text": "Many existing domain adaptation methods fall into two broad categories.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 14, "end_pos": 31, "type": "TASK", "confidence": 0.7526476383209229}]}, {"text": "Adaptation can be done at the corpus level, by selecting, joining, or weighting the datasets upon which the models (and by extension, systems) are trained.", "labels": [], "entities": [{"text": "Adaptation", "start_pos": 0, "end_pos": 10, "type": "TASK", "confidence": 0.9742834568023682}]}, {"text": "It can be also achieved at the model level by combining multiple translation or language models together, often in a weighted manner.", "labels": [], "entities": []}, {"text": "We explore both categories in this work.", "labels": [], "entities": []}, {"text": "First, we present three methods for ranking the sentences in a general-domain corpus with respect to an in-domain corpus.", "labels": [], "entities": []}, {"text": "A cutoff can then be applied to produce a very small-yet useful-subcorpus, which in turn can be used to train a domain-adapted MT system.", "labels": [], "entities": [{"text": "MT", "start_pos": 127, "end_pos": 129, "type": "TASK", "confidence": 0.9436191320419312}]}, {"text": "The first two data selection methods are applications of language-modeling techniques to MT (one for the first time).", "labels": [], "entities": [{"text": "MT", "start_pos": 89, "end_pos": 91, "type": "TASK", "confidence": 0.988372266292572}]}, {"text": "The third method is novel and explicitly takes into account the bilingual nature of the MT training corpus.", "labels": [], "entities": [{"text": "MT training", "start_pos": 88, "end_pos": 99, "type": "TASK", "confidence": 0.9217353165149689}]}, {"text": "We show that it is possible to use our data selection methods to subselect less than 1% (or discard 99%) of a large general training corpus and still increase translation performance by nearly 2 BLEU points.", "labels": [], "entities": [{"text": "translation", "start_pos": 159, "end_pos": 170, "type": "TASK", "confidence": 0.9678791761398315}, {"text": "BLEU", "start_pos": 195, "end_pos": 199, "type": "METRIC", "confidence": 0.9988068342208862}]}, {"text": "We then explore how best to use these selected subcorpora.", "labels": [], "entities": []}, {"text": "We test their combination with the indomain set, followed by examining the subcorpora to see whether they are actually in-domain, out-ofdomain, or something in between.", "labels": [], "entities": []}, {"text": "Based on this, we compare translation model combination methods.", "labels": [], "entities": [{"text": "translation model combination", "start_pos": 26, "end_pos": 55, "type": "TASK", "confidence": 0.8851016561190287}]}, {"text": "Finally, we show that these tiny translation models for model combination can improve system performance even further over the current standard way of producing a domain-adapted MT system.", "labels": [], "entities": [{"text": "MT", "start_pos": 178, "end_pos": 180, "type": "TASK", "confidence": 0.9521416425704956}]}, {"text": "The resulting process is lightweight, simple, and effective.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Baseline translation results for in-domain and  general-domain systems.", "labels": [], "entities": [{"text": "Baseline translation", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.6664669811725616}]}, {"text": " Table 2: Translation results using only a subset of the  general-domain corpus.", "labels": [], "entities": [{"text": "Translation", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.9850495457649231}]}, {"text": " Table 3: Translation results concatenating the in-domain  and pseudo in-domain data to train a single model.", "labels": [], "entities": [{"text": "Translation", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.9788081645965576}]}, {"text": " Table 4: Translation model combination results", "labels": [], "entities": [{"text": "Translation model combination", "start_pos": 10, "end_pos": 39, "type": "TASK", "confidence": 0.8773063818613688}]}, {"text": " Table 5: Translation results from using in-domain and  pseudo in-domain translation models together.", "labels": [], "entities": [{"text": "Translation", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.9849212765693665}]}]}