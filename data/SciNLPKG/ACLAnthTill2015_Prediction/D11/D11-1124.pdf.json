{"title": [{"text": "Summarize What You Are Interested In: An Optimization Framework for Interactive Personalized Summarization", "labels": [], "entities": [{"text": "Summarize What", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.9245083332061768}, {"text": "Interactive Personalized Summarization", "start_pos": 68, "end_pos": 106, "type": "TASK", "confidence": 0.5930772324403127}]}], "abstractContent": [{"text": "Most traditional summarization methods treat their outputs as static and plain texts, which fail to capture user interests during summa-rization because the generated summaries are the same for different users.", "labels": [], "entities": [{"text": "summarization", "start_pos": 17, "end_pos": 30, "type": "TASK", "confidence": 0.9835256338119507}]}, {"text": "However, users have individual preferences on a particular source document collection and obviously a universal summary for all users might not always be satisfactory.", "labels": [], "entities": []}, {"text": "Hence we investigate an important and challenging problem in summary generation, i.e., Interactive Personalized Summarization (IPS), which generates summaries in an interactive and personalized manner.", "labels": [], "entities": [{"text": "summary generation", "start_pos": 61, "end_pos": 79, "type": "TASK", "confidence": 0.8402442038059235}, {"text": "Interactive Personalized Summarization (IPS)", "start_pos": 87, "end_pos": 131, "type": "TASK", "confidence": 0.7611059844493866}]}, {"text": "Given the source documents, IPS captures user interests by enabling interactive clicks and incorporates personalization by model-ing captured reader preference.", "labels": [], "entities": []}, {"text": "We develop experimental systems to compare 5 rival algorithms on 4 instinctively different datasets which amount to 5197 documents.", "labels": [], "entities": []}, {"text": "Evaluation results in ROUGE metrics indicate the comparable performance between IPS and the best competing system but IPS produces summaries with much more user satisfaction according to evaluator ratings.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 22, "end_pos": 27, "type": "METRIC", "confidence": 0.7547153234481812}]}, {"text": "Besides, low ROUGE consistency among these user preferred summaries indicates the existence of personalization.", "labels": [], "entities": [{"text": "ROUGE consistency", "start_pos": 13, "end_pos": 30, "type": "METRIC", "confidence": 0.8886114954948425}]}], "introductionContent": [{"text": "In the era of information explosion, people need new information to update their knowledge whilst information on Web is updating extremely fast.", "labels": [], "entities": []}, {"text": "Multidocument summarization has been proposed to address such dilemma by producing a summary delivering the majority of information content from a document set, and hence is a necessity.", "labels": [], "entities": [{"text": "Multidocument summarization", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.7759588658809662}]}, {"text": "Traditional summarization methods play an important role with the exponential document growth on the Web.", "labels": [], "entities": [{"text": "summarization", "start_pos": 12, "end_pos": 25, "type": "TASK", "confidence": 0.9594692587852478}]}, {"text": "However, for the readers, the impact of human interests has seldom been considered.", "labels": [], "entities": []}, {"text": "Traditional summarization utilizes the same methodology to generate the same summary no matter who is reading.", "labels": [], "entities": [{"text": "summarization", "start_pos": 12, "end_pos": 25, "type": "TASK", "confidence": 0.9786418676376343}]}, {"text": "However, users may have bias on what they prefer to read due to their potential interests: they need personalization.", "labels": [], "entities": []}, {"text": "Therefore, traditional summarization methods are to some extent insufficient.", "labels": [], "entities": [{"text": "summarization", "start_pos": 23, "end_pos": 36, "type": "TASK", "confidence": 0.9901870489120483}]}, {"text": "Topic biased summarization tries for personalization by pre-defining human interests as several general categories, such as health or science.", "labels": [], "entities": [{"text": "Topic biased summarization", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.5804840723673502}]}, {"text": "Readers are required to select their possible interests before summary generation so that the chosen topic has priority during summarization.", "labels": [], "entities": [{"text": "summary generation", "start_pos": 63, "end_pos": 81, "type": "TASK", "confidence": 0.7166301012039185}, {"text": "summarization", "start_pos": 127, "end_pos": 140, "type": "TASK", "confidence": 0.9720550775527954}]}, {"text": "Unfortunately, such topic biased summarization is not sufficient for two reasons: (1) interests cannot usually be accurately pre-defined by ambiguous topic categories and (2) user interests cannot always be foreknown.", "labels": [], "entities": [{"text": "topic biased summarization", "start_pos": 20, "end_pos": 46, "type": "TASK", "confidence": 0.4994698663552602}]}, {"text": "Often users do not really know what general ideas or detail information they are interested in until they read the summaries.", "labels": [], "entities": []}, {"text": "Therefore, more flexible interactions are required to establish personalization.", "labels": [], "entities": []}, {"text": "Due to all the insufficiencies of existed summarization approaches, we introduce anew multidocument summarization task of Interactive Personalized Summarization (IPS) and a novel solution for the task.", "labels": [], "entities": [{"text": "summarization", "start_pos": 42, "end_pos": 55, "type": "TASK", "confidence": 0.9803026914596558}, {"text": "Interactive Personalized Summarization (IPS)", "start_pos": 122, "end_pos": 166, "type": "TASK", "confidence": 0.7570361743370692}]}, {"text": "Taking a document collection as input, the system outputs a summary aligned both with source corpus and with user personalization, which is captured by flexible human\u2212system interactions.", "labels": [], "entities": []}, {"text": "We 1342 build an experimental system on 4 real datasets to verify the effectiveness of our methods compared with 4 rivals.", "labels": [], "entities": []}, {"text": "The contribution of IPS is manifold by addressing following challenges: \u2022 The 1st challenge for IPS is to integrate user interests into traditional summary components.", "labels": [], "entities": []}, {"text": "We measure the utilities of these components and combine them.", "labels": [], "entities": []}, {"text": "We formulate the task into a balanced optimization framework via iterative substitution to generate summaries with maximum overall utilities.", "labels": [], "entities": []}, {"text": "\u2022 The 2nd challenge is to capture user interests through interaction.", "labels": [], "entities": []}, {"text": "We develop an interactive mechanism of \"click\" and \"examine\" between readers and summaries and address sparse data by \"click smoothing\" under the scenario of few user clicks.", "labels": [], "entities": []}, {"text": "We start by reviewing previous works.", "labels": [], "entities": []}, {"text": "In Section 3 we provide IPS overview, describe user interaction and optimize component combination with personalization.", "labels": [], "entities": []}, {"text": "We conduct empirical evaluation and demonstrate the experimental system in Section 4.", "labels": [], "entities": []}, {"text": "Finally we draw conclusions in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "IPS can be tested on any document set but a tiny corpus to summarize may not cover abundant effective interests to attract user clicks indicating their end if 21: end while preference.", "labels": [], "entities": []}, {"text": "Besides, the scenario of small corpus is not quite practical for the exponential growing web.", "labels": [], "entities": []}, {"text": "Therefore, we test IPS on large real world datasets.", "labels": [], "entities": [{"text": "IPS", "start_pos": 19, "end_pos": 22, "type": "TASK", "confidence": 0.9190206527709961}]}, {"text": "We build 4 news story sets which consist of documents and reference summaries to evaluate our proposed framework empirically.", "labels": [], "entities": []}, {"text": "We downloaded 5197 news articles from 10 selected sources.", "labels": [], "entities": []}, {"text": "As shown in, three of the sources are in UK, one of them is in China and the rest are in US.", "labels": [], "entities": []}, {"text": "We choose them because many of these websites provide handcrafted summaries for their special reports, which serve as reference summaries.", "labels": [], "entities": []}, {"text": "These events belong to different categories of Rule of Interpretation (ROI) ().", "labels": [], "entities": [{"text": "Rule of Interpretation (ROI)", "start_pos": 47, "end_pos": 75, "type": "TASK", "confidence": 0.741645947098732}]}, {"text": "Given a collection of documents, we first decompose them into sentences.", "labels": [], "entities": []}, {"text": "Stop-words are removed and words stemming is performed.", "labels": [], "entities": [{"text": "words stemming", "start_pos": 27, "end_pos": 41, "type": "TASK", "confidence": 0.7596136033535004}]}, {"text": "Then the word distributions can be calculated.", "labels": [], "entities": []}, {"text": "Users are required to specify the overall compression rate \u03c6 and the system extracts \u03c6|D| sentences according to user utilityFigure 1: A demonstration system for Interactive Personalized Summarization when compression rate \u03c6 is specified (e.g. 5%).", "labels": [], "entities": [{"text": "Interactive Personalized Summarization", "start_pos": 162, "end_pos": 200, "type": "TASK", "confidence": 0.5965478420257568}]}, {"text": "For convenience of browsing, we number the selected sentences (see in part 3).", "labels": [], "entities": []}, {"text": "Extracted semantic units, such as \"drilling mud\", are in bold and underlined format (see in part 1).", "labels": [], "entities": []}, {"text": "When the user clicks a sentence (part 4), the clicked sentence ID is kept in the click record (part 2).", "labels": [], "entities": []}, {"text": "Mis-clicked records revocation can be operated by clicking the deletion icon \"X\" (see in part 3).", "labels": [], "entities": []}, {"text": "Once a sentence is clicked, user can track the sentence into the popup source document to examine the contexts.", "labels": [], "entities": []}, {"text": "The selected sentences are highlighted in the source documents (see in part 5).  and traditional utility.", "labels": [], "entities": []}, {"text": "User utility is obtained from interaction.", "labels": [], "entities": []}, {"text": "The system keeps the clicked sentence records and calculates the user feedback by Equation (3) during every session.", "labels": [], "entities": [{"text": "Equation (3)", "start_pos": 82, "end_pos": 94, "type": "METRIC", "confidence": 0.9518460631370544}]}, {"text": "Consider sometimes users click into the summary due to confusion or mis-operations, but not their real interests.", "labels": [], "entities": []}, {"text": "The system supports click records revocation.", "labels": [], "entities": [{"text": "click records revocation", "start_pos": 20, "end_pos": 44, "type": "TASK", "confidence": 0.5638962288697561}]}, {"text": "More details of the user interface is demonstrated in.", "labels": [], "entities": []}, {"text": "We include both subjective evaluation from 3 evaluators based on their personalized interests and preference, and the objective evaluation based on the widely used ROUGE metrics ( .  The DUC usually officially employs ROUGE measures for summarization evaluation, which measures summarization quality by counting overlapping units such as the N-gram, word sequences, and word pairs between the candidate summary and the reference summary.", "labels": [], "entities": [{"text": "DUC", "start_pos": 187, "end_pos": 190, "type": "DATASET", "confidence": 0.8165193796157837}, {"text": "summarization evaluation", "start_pos": 237, "end_pos": 261, "type": "TASK", "confidence": 0.9463049471378326}]}, {"text": "We use ROUGE-N as follows: where N stands for the length of the N-gram and Ngram\u2208RefSum denotes the N-grams in the reference summaries while N-gram\u2208CandSum denotes the Ngrams in the candidate summaries.", "labels": [], "entities": [{"text": "ROUGE-N", "start_pos": 7, "end_pos": 14, "type": "METRIC", "confidence": 0.93543541431427}, {"text": "CandSum", "start_pos": 148, "end_pos": 155, "type": "DATASET", "confidence": 0.9304018020629883}]}, {"text": "Count match (Ngram) is the maximum number of N-gram in the candidate summary and in the set of reference summaries.", "labels": [], "entities": [{"text": "Count match (Ngram)", "start_pos": 0, "end_pos": 19, "type": "METRIC", "confidence": 0.9556090712547303}]}, {"text": "Count is the number of N-grams in the reference summaries or candidate summary.", "labels": [], "entities": [{"text": "Count", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9893372058868408}]}, {"text": "According to , among all sub-metrics in ROUGE, ROUGE-N (N=1, 2) is relatively simple and works well.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 40, "end_pos": 45, "type": "METRIC", "confidence": 0.4401324689388275}, {"text": "ROUGE-N", "start_pos": 47, "end_pos": 54, "type": "METRIC", "confidence": 0.9358130693435669}]}, {"text": "In this paper, we evaluate our experiments using all methods provided by the ROUGE package (version 1.55) and only report ROUGE-1, since the conclusions drawn from different methods are quite similar.", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 122, "end_pos": 129, "type": "METRIC", "confidence": 0.9518522024154663}]}, {"text": "Intuitively, the higher the ROUGE scores, the similar two summaries are.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 28, "end_pos": 33, "type": "METRIC", "confidence": 0.9961100220680237}]}], "tableCaptions": [{"text": " Table 2: News sources of 4 datasets  News Sources Nation  News Sources  Nation  BBC  UK  Fox News  US  Xinhua  China  MSNBC  US  CNN  US  Guardian  UK  ABC  US  New York Times  US  Reuters  UK  Washington Post  US", "labels": [], "entities": [{"text": "Nation  News Sources  Nation  BBC  UK  Fox News  US  Xinhua  China  MSNBC  US  CNN  US  Guardian  UK  ABC  US  New York Times  US  Reuters  UK  Washington Post", "start_pos": 51, "end_pos": 210, "type": "DATASET", "confidence": 0.9356294605467055}]}, {"text": " Table 3: Detailed basic information of 4 datasets.  News Subjects  #size  #docs #RS Avg.L  1.Influenza A  115026 2557  5  83  2.BP Oil Spill  63021  1468  6  76  3.Haiti Earthquake 12073  247  2  32  4.Jackson Death  37819  925  3  64", "labels": [], "entities": [{"text": "RS Avg.L  1.Influenza A  115026 2557  5  83  2.BP Oil Spill  63021  1468  6  76  3.Haiti Earthquake 12073  247", "start_pos": 82, "end_pos": 192, "type": "DATASET", "confidence": 0.7379998501978422}]}, {"text": " Table 4: Overall performance comparison on Influenza A.  ROI  *  category: Science.  Systems  R-1  95%-conf. H-1 H-2 H-3  RefSum  0.491  0.44958  3.5  3.0  3.9  Random  0.257  0.75694  1.2  1.0  1.0  Centroid 0.331  0.45073  2.5  3.0  3.5  GMDS  0.364  0.33269  3.0  2.7  3.5  IPS ini  0.302  0.21213  2.0  2.5  2.5  IPS  0.337  0.46757  4.8  4.5  4.5", "labels": [], "entities": []}, {"text": " Table 5: Overall performance comparison on BP Oil  Leak. ROI category: Accidents.  Systems  R-1  95%-conf. H-1 H-2 H-3  RefSum  0.517  0.48618  4.0  3.3  3.9  Random  0.262  0.64406  1.5  1.0  1.5  Centroid 0.369  0.34743  3.2  3.0  3.5  GMDS  0.389  0.43877  3.5  3.0  3.9  IPS ini  0.327  0.53722  3.0  2.5  3.0  IPS  0.372  0.35681  4.8  4.5  4.5", "labels": [], "entities": [{"text": "BP Oil  Leak", "start_pos": 44, "end_pos": 56, "type": "DATASET", "confidence": 0.9588548938433329}, {"text": "ROI", "start_pos": 58, "end_pos": 61, "type": "METRIC", "confidence": 0.8321363925933838}]}, {"text": " Table 6: Overall performance comparison on Haiti Earth- quake. ROI category: Disasters.  Systems  R-1  95%-conf. H-1 H-2 H-3  RefSum  0.528  0.30450  3.8  4.0  4.0  Random  0.266  0.75694  1.5  1.5  1.8  Centroid 0.362  0.43045  3.6  3.0  4.0  GMDS  0.380  0.33694  3.9  3.5  4.0  IPS ini  0.331  0.34120  2.8  2.5  3.0  IPS  0.391  0.40069  5.0  4.7  5.0", "labels": [], "entities": []}, {"text": " Table 7: Overall performance comparison on Michael  Jackson Death. ROI category: Legal Cases.  Systems  R-1  95%-conf. H-1 H-2 H-3  RefSum  0.482  0.47052  3.5  3.5  4.0  Random  0.232  0.52426  1.2  1.0  1.5  Centroid 0.320  0.21045  3.0  2.5  2.7  GMDS  0.341  0.30070  3.5  3.3  3.9  IPS ini  0.287  0.48526  2.5  2.0  2.2  IPS  0.324  0.36897  5.0  4.5  4.8", "labels": [], "entities": [{"text": "Michael  Jackson Death", "start_pos": 44, "end_pos": 66, "type": "DATASET", "confidence": 0.6082177062829336}]}, {"text": " Table 8: Ratings consistency between evaluators: mean  \u00b1 standard deviation over the 4 datasets.  RefSum  Evaluator 1 Evaluator 2 Evaluator 3  Evaluator 1  0.35\u00b10.09  0.30\u00b10.33  Evaluator 2  0.50\u00b10.14", "labels": [], "entities": [{"text": "RefSum", "start_pos": 99, "end_pos": 105, "type": "DATASET", "confidence": 0.8938130736351013}]}, {"text": " Table 9: Content consistency among evaluators super- vised summaries.  Evaluator 1 Evaluator 2 Evaluator 3  Evaluator 1  0.273  0.398  Evaluator 2  0.289  0.257  Evaluator 3  0.407  0.235  RefSum  0.365  0.302  0.394", "labels": [], "entities": []}]}