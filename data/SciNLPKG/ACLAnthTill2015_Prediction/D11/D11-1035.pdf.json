{"title": [{"text": "Better Evaluation Metrics Lead to Better Machine Translation", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 41, "end_pos": 60, "type": "TASK", "confidence": 0.7815141081809998}]}], "abstractContent": [{"text": "Many machine translation evaluation met-rics have been proposed after the seminal BLEU metric, and many among them have been found to consistently outperform BLEU, demonstrated by their better correlations with human judgment.", "labels": [], "entities": [{"text": "machine translation evaluation met-rics", "start_pos": 5, "end_pos": 44, "type": "TASK", "confidence": 0.8205654472112656}, {"text": "BLEU", "start_pos": 82, "end_pos": 86, "type": "METRIC", "confidence": 0.9488235712051392}, {"text": "BLEU", "start_pos": 158, "end_pos": 162, "type": "METRIC", "confidence": 0.9924386143684387}]}, {"text": "It has long been the hope that by tuning machine translation systems against these new generation metrics, advances in automatic machine translation evaluation can lead directly to advances in automatic machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 41, "end_pos": 60, "type": "TASK", "confidence": 0.7037954032421112}, {"text": "machine translation evaluation", "start_pos": 129, "end_pos": 159, "type": "TASK", "confidence": 0.7719304859638214}, {"text": "machine translation", "start_pos": 203, "end_pos": 222, "type": "TASK", "confidence": 0.6939654797315598}]}, {"text": "However, to date there has been no unambiguous report that these new metrics can improve a state-of-the-art machine translation system over its BLEU-tuned baseline.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 108, "end_pos": 127, "type": "TASK", "confidence": 0.7107950299978256}, {"text": "BLEU-tuned", "start_pos": 144, "end_pos": 154, "type": "METRIC", "confidence": 0.9182211756706238}]}, {"text": "In this paper, we demonstrate that tuning Joshua, a hierarchical phrase-based statistical machine translation system, with the TESLA metrics results in significantly better human-judged translation quality than the BLEU-tuned baseline.", "labels": [], "entities": [{"text": "phrase-based statistical machine translation", "start_pos": 65, "end_pos": 109, "type": "TASK", "confidence": 0.628737062215805}, {"text": "TESLA", "start_pos": 127, "end_pos": 132, "type": "METRIC", "confidence": 0.9393694996833801}, {"text": "BLEU-tuned", "start_pos": 215, "end_pos": 225, "type": "METRIC", "confidence": 0.9880120754241943}]}, {"text": "TESLA-M in particular is simple and performs well in practice on large datasets.", "labels": [], "entities": [{"text": "TESLA-M", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.4610300660133362}]}, {"text": "We release all our implementation under an open source license.", "labels": [], "entities": []}, {"text": "It is our hope that this work will encourage the machine translation community to finally move away from BLEU as the unquestioned default and to consider the new generation metrics when tuning their systems.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 49, "end_pos": 68, "type": "TASK", "confidence": 0.8010417222976685}, {"text": "BLEU", "start_pos": 105, "end_pos": 109, "type": "METRIC", "confidence": 0.9705962538719177}]}], "introductionContent": [{"text": "The dominant framework of machine translation (MT) today is statistical machine translation (SMT).", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 26, "end_pos": 50, "type": "TASK", "confidence": 0.8790979981422424}, {"text": "statistical machine translation (SMT)", "start_pos": 60, "end_pos": 97, "type": "TASK", "confidence": 0.7963453829288483}]}, {"text": "At the core of the system is the decoder, which performs the actual translation.", "labels": [], "entities": []}, {"text": "The decoder is parameterized, and estimating the optimal set of parameter values is of paramount importance in getting good translations.", "labels": [], "entities": []}, {"text": "In SMT, the parameter space is explored by a tuning algorithm, typically MERT (Minimum Error Rate Training), though the exact method is not important for our purpose.", "labels": [], "entities": [{"text": "SMT", "start_pos": 3, "end_pos": 6, "type": "TASK", "confidence": 0.992316484451294}, {"text": "MERT (Minimum Error Rate Training)", "start_pos": 73, "end_pos": 107, "type": "METRIC", "confidence": 0.7725332038743156}]}, {"text": "The tuning algorithm carries out repeated experiments with different decoder parameter values over a development data set, for which reference translations are given.", "labels": [], "entities": []}, {"text": "An automatic MT evaluation metric compares the output of the decoder against the reference(s), and guides the tuning algorithm towards iteratively better decoder parameters and output translations.", "labels": [], "entities": [{"text": "MT", "start_pos": 13, "end_pos": 15, "type": "TASK", "confidence": 0.982071578502655}]}, {"text": "The quality of the automatic MT evaluation metric therefore has an immediate effect on the whole system.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 29, "end_pos": 42, "type": "TASK", "confidence": 0.9258934557437897}]}, {"text": "The first automatic MT evaluation metric to show a high correlation with human judgment is BLEU ().", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 20, "end_pos": 33, "type": "TASK", "confidence": 0.886594146490097}, {"text": "BLEU", "start_pos": 91, "end_pos": 95, "type": "METRIC", "confidence": 0.998490571975708}]}, {"text": "Together with its close variant the NIST metric, they have quickly become the standard way of tuning statistical machine translation systems.", "labels": [], "entities": [{"text": "NIST metric", "start_pos": 36, "end_pos": 47, "type": "DATASET", "confidence": 0.775226891040802}, {"text": "statistical machine translation", "start_pos": 101, "end_pos": 132, "type": "TASK", "confidence": 0.5940261880556742}]}, {"text": "While BLEU is an impressively simple and effective metric, recent evaluations have shown that many new generation metrics can outperform BLEU in terms of correlation with human judgment).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 6, "end_pos": 10, "type": "METRIC", "confidence": 0.996852457523346}, {"text": "BLEU", "start_pos": 137, "end_pos": 141, "type": "METRIC", "confidence": 0.9952676296234131}]}, {"text": "Some of these new metrics include METEOR (), TER (), MAXSIM, and TESLA ().", "labels": [], "entities": [{"text": "METEOR", "start_pos": 34, "end_pos": 40, "type": "METRIC", "confidence": 0.9604711532592773}, {"text": "TER", "start_pos": 45, "end_pos": 48, "type": "METRIC", "confidence": 0.9967973828315735}, {"text": "MAXSIM", "start_pos": 53, "end_pos": 59, "type": "METRIC", "confidence": 0.881187379360199}, {"text": "TESLA", "start_pos": 65, "end_pos": 70, "type": "METRIC", "confidence": 0.988955557346344}]}, {"text": "Given the close relationship between automatic MT and automatic MT evaluation, the logical expectation is that a better MT evaluation metric would lead to better MT systems.", "labels": [], "entities": [{"text": "MT", "start_pos": 47, "end_pos": 49, "type": "TASK", "confidence": 0.9687348008155823}, {"text": "MT evaluation", "start_pos": 64, "end_pos": 77, "type": "TASK", "confidence": 0.9606870412826538}, {"text": "MT evaluation", "start_pos": 120, "end_pos": 133, "type": "TASK", "confidence": 0.8974944353103638}, {"text": "MT", "start_pos": 162, "end_pos": 164, "type": "TASK", "confidence": 0.9885593056678772}]}, {"text": "However, this linkage has not yet been realized.", "labels": [], "entities": []}, {"text": "In the SMT community, MT tuning still uses BLEU almost exclusively.", "labels": [], "entities": [{"text": "SMT", "start_pos": 7, "end_pos": 10, "type": "TASK", "confidence": 0.9903621673583984}, {"text": "MT tuning", "start_pos": 22, "end_pos": 31, "type": "TASK", "confidence": 0.985142707824707}, {"text": "BLEU", "start_pos": 43, "end_pos": 47, "type": "METRIC", "confidence": 0.9972757697105408}]}, {"text": "Some researchers have investigated the use of better metrics for MT tuning, with mixed results.", "labels": [], "entities": [{"text": "MT tuning", "start_pos": 65, "end_pos": 74, "type": "TASK", "confidence": 0.9921812415122986}]}, {"text": "Most notably, reported improved human judgment using their entailment-based metric.", "labels": [], "entities": []}, {"text": "However, the metric is heavyweight and slow in practice, with an estimated runtime of 40 days on the NIST MT 2002/2006/2008 dataset, and the authors had to resort to a two-phase MERT process with a reduced n-best list.", "labels": [], "entities": [{"text": "NIST MT 2002/2006/2008 dataset", "start_pos": 101, "end_pos": 131, "type": "DATASET", "confidence": 0.9519705697894096}]}, {"text": "As we shall see, our experiments use the similarly sized WMT 2010 dataset, and most of our runs take less than one day.", "labels": [], "entities": [{"text": "WMT 2010 dataset", "start_pos": 57, "end_pos": 73, "type": "DATASET", "confidence": 0.9658348162968954}]}, {"text": "compared tuning a phrase-based SMT system with BLEU, NIST, METEOR, and TER, and concluded that BLEU and NIST are still the best choices for MT tuning, despite the proven higher correlation of METEOR and TER with human judgment.", "labels": [], "entities": [{"text": "SMT", "start_pos": 31, "end_pos": 34, "type": "TASK", "confidence": 0.8937897086143494}, {"text": "BLEU", "start_pos": 47, "end_pos": 51, "type": "METRIC", "confidence": 0.9949797987937927}, {"text": "TER", "start_pos": 71, "end_pos": 74, "type": "METRIC", "confidence": 0.9872840642929077}, {"text": "BLEU", "start_pos": 95, "end_pos": 99, "type": "METRIC", "confidence": 0.9942154288291931}, {"text": "MT tuning", "start_pos": 140, "end_pos": 149, "type": "TASK", "confidence": 0.9934053719043732}]}, {"text": "In this work, we investigate the effect of MERT using BLEU, TER, and two variants of TESLA, TESLA-M and TESLA-F, on Joshua (), a state-of-the-art hierarchical phrase-based SMT system.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 54, "end_pos": 58, "type": "METRIC", "confidence": 0.9983299374580383}, {"text": "TER", "start_pos": 60, "end_pos": 63, "type": "METRIC", "confidence": 0.984529972076416}, {"text": "SMT system", "start_pos": 172, "end_pos": 182, "type": "TASK", "confidence": 0.8423887193202972}]}, {"text": "Our empirical study is carried out in the context of WMT 2010, for the French-English, Spanish-English, and German-English machine translation tasks.", "labels": [], "entities": [{"text": "WMT 2010", "start_pos": 53, "end_pos": 61, "type": "DATASET", "confidence": 0.7615056335926056}, {"text": "machine translation tasks", "start_pos": 123, "end_pos": 148, "type": "TASK", "confidence": 0.765111098686854}]}, {"text": "We show that Joshua responds well to the change of evaluation metric, in that a system trained on metric M typically does well when judged by the same metric M.", "labels": [], "entities": []}, {"text": "We further evaluate the different systems with manual judgments and show that the TESLA family of metrics (both TESLA-M and TESLA-F) significantly outperforms BLEU when used to guide the MERT search.", "labels": [], "entities": [{"text": "TESLA", "start_pos": 82, "end_pos": 87, "type": "METRIC", "confidence": 0.7896153330802917}, {"text": "TESLA-M", "start_pos": 112, "end_pos": 119, "type": "METRIC", "confidence": 0.7914720177650452}, {"text": "TESLA-F", "start_pos": 124, "end_pos": 131, "type": "METRIC", "confidence": 0.7779284715652466}, {"text": "BLEU", "start_pos": 159, "end_pos": 163, "type": "METRIC", "confidence": 0.9983221888542175}, {"text": "MERT search", "start_pos": 187, "end_pos": 198, "type": "TASK", "confidence": 0.7817595899105072}]}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we describe the four evaluation metrics used.", "labels": [], "entities": []}, {"text": "Section 3 outlines our experimental setup using the WMT 2010 machine translation tasks.", "labels": [], "entities": [{"text": "WMT 2010 machine translation", "start_pos": 52, "end_pos": 80, "type": "TASK", "confidence": 0.791642427444458}]}, {"text": "Section 4 presents the evaluation results, both automatic and manual.", "labels": [], "entities": []}, {"text": "Finally, we discuss our findings in Section 5, future work in Section 6, and conclude in Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "This section describes the metrics used in our experiments.", "labels": [], "entities": []}, {"text": "We do not seek to explain all their variants and intricate details, but rather to outline their core characteristics and to highlight their similarities and differences.", "labels": [], "entities": []}, {"text": "In particular, since all our experiments are based on single references, we omit the complications due to multiple references and refer our readers instead to the respective original papers for the details.", "labels": [], "entities": []}, {"text": "The average English sentence length is 21 words for all three language pairs.", "labels": [], "entities": []}, {"text": "The text domain is newswire report, and the English sides of the training texts for the three language pairs overlap substantially.", "labels": [], "entities": []}, {"text": "The development data are 2525 four-way translated sentences, in English, French, Spanish, and German respectively.", "labels": [], "entities": []}, {"text": "Similarly, the test data are 2489 four-way translated sentences.", "labels": [], "entities": []}, {"text": "As a consequence, all MT evaluations involve only single references.", "labels": [], "entities": [{"text": "MT evaluations", "start_pos": 22, "end_pos": 36, "type": "TASK", "confidence": 0.9168802201747894}]}, {"text": "We follow the standard approach for training hierarchical phrase-based SMT systems.", "labels": [], "entities": [{"text": "SMT", "start_pos": 71, "end_pos": 74, "type": "TASK", "confidence": 0.8355453610420227}]}, {"text": "First, we tokenize and lowercase the training texts and create fr-en es-en de-en to build a trigram model with modified Kneser-Ney smoothing from the monolingual training data supplied in WMT 2010.", "labels": [], "entities": [{"text": "WMT 2010", "start_pos": 188, "end_pos": 196, "type": "DATASET", "confidence": 0.9065497815608978}]}, {"text": "Parameter tuning is carried out using Z-MERT.", "labels": [], "entities": [{"text": "Parameter tuning", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8612425327301025}]}, {"text": "TER and BLEU are already implemented in the publicly released version of Z-MERT, and Z-MERT's modular design makes it easy to integrate TESLA-M and TESLA-F into the package.", "labels": [], "entities": [{"text": "TER", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.9730075001716614}, {"text": "BLEU", "start_pos": 8, "end_pos": 12, "type": "METRIC", "confidence": 0.997488260269165}]}, {"text": "The maximum number of MERT iterations is set to 100, although we observe that in practice, the algorithm converges after 3 to 6 iterations.", "labels": [], "entities": [{"text": "MERT", "start_pos": 22, "end_pos": 26, "type": "METRIC", "confidence": 0.6867233514785767}]}, {"text": "The number of intermediate initial points per iteration is set to 20 and the n-best list is capped to 300 translations.", "labels": [], "entities": []}, {"text": "shows the training times and the number of MERT iterations for each of the language pairs and evaluation metrics.", "labels": [], "entities": [{"text": "MERT iterations", "start_pos": 43, "end_pos": 58, "type": "TASK", "confidence": 0.6253513097763062}]}, {"text": "We use the publicly available version of TESLA-F, which comes with phrase tables and a ranking SVM model trained on the WMT 2010 development data.", "labels": [], "entities": [{"text": "WMT 2010 development data", "start_pos": 120, "end_pos": 145, "type": "DATASET", "confidence": 0.9557795524597168}]}, {"text": "The results of the automatic evaluations are presented in.", "labels": [], "entities": []}, {"text": "The best score according to each metric is shown in bold.", "labels": [], "entities": []}, {"text": "Note that smaller TER scores are better, as are larger BLEU, TESLA-M, and TESLA-F scores.", "labels": [], "entities": [{"text": "TER", "start_pos": 18, "end_pos": 21, "type": "METRIC", "confidence": 0.9987531900405884}, {"text": "BLEU", "start_pos": 55, "end_pos": 59, "type": "METRIC", "confidence": 0.9988126754760742}, {"text": "TESLA-M", "start_pos": 61, "end_pos": 68, "type": "METRIC", "confidence": 0.9358388781547546}, {"text": "TESLA-F", "start_pos": 74, "end_pos": 81, "type": "METRIC", "confidence": 0.9493805766105652}]}, {"text": "We note that Joshua generally responds well to the change of tuning metric.", "labels": [], "entities": []}, {"text": "A system tuned on met-: Inter-annotator agreement ric M usually does the best or very close to the best when evaluated by M. On the other hand, the differences between different systems can be substantial, especially between BLEU/TER and TESLA-M/TESLA-F.", "labels": [], "entities": [{"text": "BLEU/TER", "start_pos": 225, "end_pos": 233, "type": "METRIC", "confidence": 0.788144846757253}]}, {"text": "In addition to the automatic evaluation, we enlisted twelve judges to manually evaluate the first 200 test sentences.", "labels": [], "entities": []}, {"text": "Four judges are assigned to each of the three language pairs.", "labels": [], "entities": []}, {"text": "For each test sentence, the judges are presented with the source sentence, the reference English translation, and the output from the four competing Joshua systems.", "labels": [], "entities": []}, {"text": "The order of the translation candidates is randomized so that the judges will not see any patterns.", "labels": [], "entities": []}, {"text": "The judges are instructed to rank the four candidates, and ties are allowed.", "labels": [], "entities": []}, {"text": "The inter-annotator agreement is reported in Table 5.", "labels": [], "entities": []}, {"text": "We consider the judgment fora pair of system outputs as one data point.", "labels": [], "entities": []}, {"text": "Let P (A) be the proportion of times that the annotators agree, and P (E) fr-en es-en de-en BLEU 44.1% 33.8% 49.6% TER 41.4% 34.4% 47.8% TESLA-M 65.8% 49.5% 57.8% TESLA-F 66.4% 53.8% 55.1%: Percentage of times each system produces the best translation be the proportion of times that they would agree by chance.", "labels": [], "entities": [{"text": "P (E) fr-en es-en de-en", "start_pos": 68, "end_pos": 91, "type": "METRIC", "confidence": 0.8139650651386806}, {"text": "BLEU", "start_pos": 92, "end_pos": 96, "type": "METRIC", "confidence": 0.5047572255134583}, {"text": "TER", "start_pos": 115, "end_pos": 118, "type": "METRIC", "confidence": 0.9009532332420349}]}, {"text": "The Kappa coefficient is defined as Kappa = P(A) \u2212 P(E) 1 \u2212 P(E) In our experiments, each data point has three possible values: A is preferred, B is preferred, and no preference, hence P (E) = 1/3.", "labels": [], "entities": []}, {"text": "Our Kappa is calculated in the same way as the WMT workshops.", "labels": [], "entities": [{"text": "Kappa", "start_pos": 4, "end_pos": 9, "type": "METRIC", "confidence": 0.9887558817863464}, {"text": "WMT workshops", "start_pos": 47, "end_pos": 60, "type": "DATASET", "confidence": 0.7622842192649841}]}, {"text": "Kappa coefficients between 0.4 and 0.6 are considered moderate, and our values are inline with those reported in the WMT 2010 translation campaign.", "labels": [], "entities": [{"text": "Kappa", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9375444650650024}, {"text": "WMT 2010 translation", "start_pos": 117, "end_pos": 137, "type": "TASK", "confidence": 0.6197789708773295}]}, {"text": "The exception is the German-English pair, where the annotators only reach slight agreement.", "labels": [], "entities": []}, {"text": "This might be caused by the lower quality of German to English translations compared to the other two language pairs.", "labels": [], "entities": []}, {"text": "shows the proportion of times each system produces the best translation among the four.", "labels": [], "entities": []}, {"text": "We observe that the rankings are largely consistent across different language pairs: Both TESLA-F and TESLA-M strongly outperform BLEU and TER.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 130, "end_pos": 134, "type": "METRIC", "confidence": 0.9977090358734131}, {"text": "TER", "start_pos": 139, "end_pos": 142, "type": "METRIC", "confidence": 0.716544508934021}]}, {"text": "Note that the values in each column do not add up to 100%, since the candidate translations are often identical, and even a different translation can receive the same human judgment.", "labels": [], "entities": []}, {"text": "shows our main result, the pairwise comparison between the four systems for each of the language pairs.", "labels": [], "entities": []}, {"text": "Again the rankings consistently show that both TESLA-F and TESLA-M strongly outperform BLEU and TER.", "labels": [], "entities": [{"text": "TESLA-F", "start_pos": 47, "end_pos": 54, "type": "METRIC", "confidence": 0.8967876434326172}, {"text": "TESLA-M", "start_pos": 59, "end_pos": 66, "type": "METRIC", "confidence": 0.7403586506843567}, {"text": "BLEU", "start_pos": 87, "end_pos": 91, "type": "METRIC", "confidence": 0.9980937838554382}, {"text": "TER", "start_pos": 96, "end_pos": 99, "type": "METRIC", "confidence": 0.9754713773727417}]}, {"text": "All differences are statistically significant under the Sign Test at p = 0.01, with the exception of TESLA-M vs TESLA-F in the French-English task, BLEU vs TER in the Spanish-English task, and TESLA-M vs TESLA-F and BLEU vs TER in the German-English task.", "labels": [], "entities": [{"text": "TESLA-M", "start_pos": 101, "end_pos": 108, "type": "METRIC", "confidence": 0.958836019039154}, {"text": "BLEU", "start_pos": 148, "end_pos": 152, "type": "METRIC", "confidence": 0.9983310103416443}, {"text": "BLEU", "start_pos": 216, "end_pos": 220, "type": "METRIC", "confidence": 0.9959337115287781}]}, {"text": "translation systems using the TESLA metrics leads to significantly better translation output.", "labels": [], "entities": [{"text": "TESLA metrics", "start_pos": 30, "end_pos": 43, "type": "DATASET", "confidence": 0.7249566912651062}]}], "tableCaptions": [{"text": " Table 1: Selected system-level Spearman's rho cor- relation with the human judgment for the into- English task, as reported in WMT 2010.", "labels": [], "entities": [{"text": "WMT 2010", "start_pos": 128, "end_pos": 136, "type": "DATASET", "confidence": 0.9522019624710083}]}, {"text": " Table 2: Selected system-level Spearman's rho cor- relation with the human judgment for the out-of- English task, as reported in WMT 2010.", "labels": [], "entities": [{"text": "WMT 2010", "start_pos": 130, "end_pos": 138, "type": "DATASET", "confidence": 0.9482347071170807}]}, {"text": " Table 4: Automatic evaluation scores", "labels": [], "entities": []}, {"text": " Table 7: Pairwise system comparisons. Each cell shows the proportion of time the system tuned on A is  preferred over the system tuned on B, and the proportion of time the opposite happens. Notice that the upper  right half of each table is the mirror image of the lower left half.", "labels": [], "entities": []}]}