{"title": [{"text": "Syntactic Decision Tree LMs: Random Selection or Intelligent Design?", "labels": [], "entities": [{"text": "Syntactic Decision Tree LMs", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.7128023952245712}]}], "abstractContent": [{"text": "Decision trees have been applied to a variety of NLP tasks, including language mod-eling, for their ability to handle a variety of attributes and sparse context space.", "labels": [], "entities": []}, {"text": "Moreover , forests (collections of decision trees) have been shown to substantially outperform individual decision trees.", "labels": [], "entities": []}, {"text": "In this work, we investigate methods for combining trees in a forest , as well as methods for diversifying trees for the task of syntactic language modeling.", "labels": [], "entities": [{"text": "syntactic language modeling", "start_pos": 129, "end_pos": 156, "type": "TASK", "confidence": 0.7326052387555441}]}, {"text": "We show that our tree interpolation technique outperforms the standard method used in the literature, and that, on this particular task, restricting tree contexts in a principled way produces smaller and better forests, with the best achieving an 8% relative reduction in Word Error Rate over an n-gram baseline.", "labels": [], "entities": [{"text": "Word Error Rate", "start_pos": 272, "end_pos": 287, "type": "METRIC", "confidence": 0.6020615796248118}]}], "introductionContent": [{"text": "Language Models (LMs) are an essential part of NLP applications that require selection of the most fluent word sequence among multiple hypotheses.", "labels": [], "entities": []}, {"text": "The most prominent applications include Automatic Speech Recognition (ASR) and Machine Translation (MT).", "labels": [], "entities": [{"text": "Automatic Speech Recognition (ASR)", "start_pos": 40, "end_pos": 74, "type": "TASK", "confidence": 0.7700223624706268}, {"text": "Machine Translation (MT)", "start_pos": 79, "end_pos": 103, "type": "TASK", "confidence": 0.8511829257011414}]}, {"text": "Statistical LMs formulate the problem as the computation of the model's probability to generate the word sequence w 1 , w 2 , . .", "labels": [], "entities": []}, {"text": ", w m (denoted as w m 1 ), assuming that higher probability corresponds to more fluent hypotheses.", "labels": [], "entities": []}, {"text": "LMs are often represented in the following generative form: is arbitrarily long, necessitating some independence assumption, which usually consists of reducing the relevant context to n\u22121 immediately preceding tokens: These distributions are typically estimated from observed counts of n-grams w i i\u2212n+1 in the training data.", "labels": [], "entities": []}, {"text": "The context space is still far too large 1 ; therefore, the models are recursively smoothed using lower order distributions.", "labels": [], "entities": []}, {"text": "For instance, in a widely used n-gram LM, the probabilities are estimated as follows: where \u03c1 is a discounted probability 2 . Note that this type of model is a simple Markov chain lacking any notion of syntax.", "labels": [], "entities": []}, {"text": "It is widely accepted that languages do have some structure.", "labels": [], "entities": []}, {"text": "Moreover, it has been shown that incorporating syntax into a language model can improve its performance.", "labels": [], "entities": []}, {"text": "A straightforward way of incorporating syntax into a language model is by assigning a tag to each word and modeling them jointly; then to obtain the proba-bility of a word sequence, the tags must be marginalized out: An independence assumption similar to Eq.", "labels": [], "entities": []}, {"text": "1 can be made: p(w it i |w i\u22121 1 t i\u22121 1 ) \u2248 p(w it i |w i\u22121 i\u2212n+1 t i\u22121 i\u2212n+1 ) (3) A primary goal of our research is to build strong syntactic language models and provide effective methods for constructing them to the research community.", "labels": [], "entities": []}, {"text": "Note that the tags in the context of the joint model in Eq.", "labels": [], "entities": [{"text": "Eq", "start_pos": 56, "end_pos": 58, "type": "DATASET", "confidence": 0.9281912446022034}]}, {"text": "3 exacerbate the already sparse problem in Eq.", "labels": [], "entities": [{"text": "Eq", "start_pos": 43, "end_pos": 45, "type": "DATASET", "confidence": 0.861247181892395}]}, {"text": "1, which makes the probability estimation particularly challenging.", "labels": [], "entities": [{"text": "probability estimation", "start_pos": 19, "end_pos": 41, "type": "TASK", "confidence": 0.7294567823410034}]}, {"text": "We utilize decision trees for joint syntactic language models to cluster context because of their strengths (reliance on information theoretic metrics to cluster context in the face of extreme sparsity and the ability to incorporate attributes of different types ), and at the same time, unlike log-linear models, computationally expensive probability normalization does not have to be postponed until runtime.", "labels": [], "entities": []}, {"text": "In Section 2, we describe the details of the syntactic decision tree LM.", "labels": [], "entities": []}, {"text": "Construction of a single-tree model is difficult due to the inevitable greediness of the tree construction process and its tendency to overfit the data.", "labels": [], "entities": []}, {"text": "This problem is often addressed by interpolating with lower order decision trees.", "labels": [], "entities": []}, {"text": "In Section 3, we point out the inappropriateness of backoff methods borrowed from n-gram models for decision tree LMs and briefly describe a generalized interpolation for such models.", "labels": [], "entities": []}, {"text": "The generalized interpolation method allows the addition of any number of trees to the model, and thus raises the question: what is the best way to create diverse decision trees so that their combination results in a stronger model, while at the same timekeeping the total number of trees in the model relatively low for computational practicality.", "labels": [], "entities": []}, {"text": "In Section 4, we explore and evaluate a variety of methods for creating different trees.", "labels": [], "entities": []}, {"text": "To support our findings, we evaluate several of the models on an ASR rescoring task in Section 5.", "labels": [], "entities": [{"text": "ASR rescoring task", "start_pos": 65, "end_pos": 83, "type": "TASK", "confidence": 0.9315451979637146}]}, {"text": "Finally, we discuss our findings in Section 6.", "labels": [], "entities": [{"text": "Section 6", "start_pos": 36, "end_pos": 45, "type": "DATASET", "confidence": 0.8296379446983337}]}], "datasetContent": [{"text": "To train our models we use 35M words of WSJ 94-96 from LDC2008T13.", "labels": [], "entities": [{"text": "WSJ 94-96", "start_pos": 40, "end_pos": 49, "type": "DATASET", "confidence": 0.8934409320354462}, {"text": "LDC2008T13", "start_pos": 55, "end_pos": 65, "type": "DATASET", "confidence": 0.5003290176391602}]}, {"text": "The text was converted into speech-like form, namely numbers and abbreviations were verbalized, text was downcased, punctuation was removed, and contractions and possessives were joined with the previous word (i.e., they 'll becomes they'll).", "labels": [], "entities": []}, {"text": "For the syntactic modeling, we used tags comprised of the POS tags of the word and it's head.", "labels": [], "entities": []}, {"text": "Parsing of the text for tag extraction occurred after verbalization of numbers and abbreviations but before any further processing; we used a latent variable PCFG parser as in).", "labels": [], "entities": [{"text": "tag extraction", "start_pos": 24, "end_pos": 38, "type": "TASK", "confidence": 0.8823937773704529}]}, {"text": "For reference, we include an n-gram model with modified interpolated KN discounting.", "labels": [], "entities": []}, {"text": "All models use the same vocabulary of approximately 50k words.", "labels": [], "entities": []}, {"text": "Perplexity numbers reported in, 3, and 4 are computed on WSJ section 23 (tokenized in the same way) . In, we show results reported in (), which we use as the baseline for further experiments.", "labels": [], "entities": [{"text": "WSJ section 23", "start_pos": 57, "end_pos": 71, "type": "DATASET", "confidence": 0.9642017881075541}]}, {"text": "We constructed two sets of decision trees (a joint syntactic model and a word-tree model) as described in Section 2.", "labels": [], "entities": []}, {"text": "Each set was comprised of a fourgram tree with backoff trigram, bigram, and unigram trees.", "labels": [], "entities": []}, {"text": "We combined these trees using either Eq.", "labels": [], "entities": []}, {"text": "9. The \u03bb parameters in Eq.", "labels": [], "entities": [{"text": "Eq", "start_pos": 23, "end_pos": 25, "type": "DATASET", "confidence": 0.9283314347267151}]}, {"text": "8 were estimated using EM by maximizing likelihood of a heldout set (we utilized 4-way crossvalidation); whereas, the parameters in Eq.", "labels": [], "entities": [{"text": "Eq", "start_pos": 132, "end_pos": 134, "type": "DATASET", "confidence": 0.9229158163070679}]}, {"text": "9 were estimated using L-BFGS because the denominator in Eq.", "labels": [], "entities": [{"text": "Eq", "start_pos": 57, "end_pos": 59, "type": "DATASET", "confidence": 0.937383234500885}]}, {"text": "9 makes the maximization step problematic.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Perplexity results on PTB WSJ section 23. Percentage numbers in parentheses denote the reduction of  perplexity relative to the lower order model of the same type.", "labels": [], "entities": [{"text": "PTB WSJ section 23", "start_pos": 32, "end_pos": 50, "type": "DATASET", "confidence": 0.931195855140686}]}, {"text": " Table 2: Perplexity numbers obtained using fourgram  trees only. Note that \"undgr\" and \"rnd\" denote unde- graded and randomly grown trees with Bernoulli trials,  respectively, and the number indicates the number of  trees in the forest. Also \"baseline\" refers to the fourgram  models with lower order trees (from", "labels": [], "entities": []}, {"text": " Table 3: Perplexity numbers obtained using fourgram  trees produced using random initialization of the Ex- change algorithm (Exchng. columns) and, additionally,  variations in training data folds (+data columns). Note  that \"baseline\" refers to the fourgram models with lower  order trees (from", "labels": [], "entities": []}, {"text": " Table 4: Perplexity results using the standard syntactic  model with additional trees. \"bernoulli-rnd\" and \"data- rnd\" indicate fourgram trees randomized using Bernoulli  trials and varying training data, respectively. The second  column shows the combined size of decision trees in the  forest.", "labels": [], "entities": []}, {"text": " Table 4. The randomly grown trees (de- noted \"bernoulli-rnd\") are grown utilizing the full  context 4w4t using the methods described in Sec- tion 4.2. All models utilize the generalized interpo- lation method described in Section 3.2.", "labels": [], "entities": []}]}