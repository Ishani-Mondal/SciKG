{"title": [{"text": "Multiword Expression Identification with Tree Substitution Grammars: A Parsing tour de force with French", "labels": [], "entities": [{"text": "Multiword Expression Identification", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.8083815375963846}]}], "abstractContent": [{"text": "Multiword expressions (MWE), a known nuisance for both linguistics and NLP, blur the lines between syntax and semantics.", "labels": [], "entities": [{"text": "Multiword expressions (MWE)", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.8134022116661072}]}, {"text": "Previous work on MWE identification has relied primarily on surface statistics, which perform poorly for longer MWEs and cannot model discontin-uous expressions.", "labels": [], "entities": [{"text": "MWE identification", "start_pos": 17, "end_pos": 35, "type": "TASK", "confidence": 0.9906513094902039}]}, {"text": "To address these problems, we show that even the simplest parsing models can effectively identify MWEs of arbitrary length, and that Tree Substitution Grammars achieve the best results.", "labels": [], "entities": []}, {"text": "Our experiments show a 36.4% F1 absolute improvement for French over an n-gram surface statistics baseline, currently the predominant method for MWE identification.", "labels": [], "entities": [{"text": "F1 absolute", "start_pos": 29, "end_pos": 40, "type": "METRIC", "confidence": 0.8701415657997131}, {"text": "French", "start_pos": 57, "end_pos": 63, "type": "METRIC", "confidence": 0.6032362580299377}, {"text": "MWE identification", "start_pos": 145, "end_pos": 163, "type": "TASK", "confidence": 0.9802609086036682}]}, {"text": "Our models are useful for several NLP tasks in which MWE pre-grouping has improved accuracy.", "labels": [], "entities": [{"text": "MWE pre-grouping", "start_pos": 53, "end_pos": 69, "type": "TASK", "confidence": 0.8262080252170563}, {"text": "accuracy", "start_pos": 83, "end_pos": 91, "type": "METRIC", "confidence": 0.996967613697052}]}], "introductionContent": [{"text": "Multiword expressions (MWE) have long been a challenge for linguistic theory and NLP.", "labels": [], "entities": [{"text": "Multiword expressions (MWE)", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.8577083945274353}]}, {"text": "There is no universally accepted definition of the term, but MWEs can be characterized as \"idiosyncratic interpretations that crossword boundaries (or spaces)\" () such as traffic light, or as \"frequently occurring phrasal units which are subject to a certain level of semantic opaqueness, or noncompositionality\".", "labels": [], "entities": []}, {"text": "MWEs are often opaque fixed expressions, although the degree to which they are fixed can vary.", "labels": [], "entities": []}, {"text": "Some MWEs do not allow morphosyntactic variation or internal modification (e.g., in short, but *in shorter or *in very short).", "labels": [], "entities": []}, {"text": "Other MWEs are \"semifixed,\" meaning that they can be inflected or undergo internal modification.", "labels": [], "entities": []}, {"text": "The type of modification is often limited, but not predictable, so it is not possible to enumerate all variants.: Semi-fixed MWEs in French and English.", "labels": [], "entities": []}, {"text": "The French adverb \u00e0 terme 'in the end' can be modified by a small set of adjectives, and in turn some of these adjectives can be modified by an adverb such as tr\u00e8s 'very'.", "labels": [], "entities": []}, {"text": "Similar restrictions appear in English.", "labels": [], "entities": []}, {"text": "Merging known MWEs into single tokens has been shown to improve accuracy fora variety of NLP tasks: dependency parsing (), constituency parsing (Arun and), sentence generation (, and machine translation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.9987701773643494}, {"text": "dependency parsing", "start_pos": 100, "end_pos": 118, "type": "TASK", "confidence": 0.8646413683891296}, {"text": "constituency parsing", "start_pos": 123, "end_pos": 143, "type": "TASK", "confidence": 0.8248936533927917}, {"text": "sentence generation", "start_pos": 156, "end_pos": 175, "type": "TASK", "confidence": 0.8248921036720276}, {"text": "machine translation", "start_pos": 183, "end_pos": 202, "type": "TASK", "confidence": 0.799249529838562}]}, {"text": "Most experiments use gold MWE pre-grouping or languagespecific resources like WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 78, "end_pos": 85, "type": "DATASET", "confidence": 0.9786449670791626}]}, {"text": "For unlabeled text, the best MWE identification methods, which are based on surface statistics, suffer from sparsity induced by longer n-grams (.", "labels": [], "entities": [{"text": "MWE identification", "start_pos": 29, "end_pos": 47, "type": "TASK", "confidence": 0.9741092026233673}]}, {"text": "A dilemma thus exists: MWE knowledge is useful, but MWEs are hard to identify.", "labels": [], "entities": [{"text": "MWE knowledge", "start_pos": 23, "end_pos": 36, "type": "TASK", "confidence": 0.8466664254665375}]}, {"text": "In this paper, we show the effectiveness of statistical parsers for MWE identification.", "labels": [], "entities": [{"text": "MWE identification", "start_pos": 68, "end_pos": 86, "type": "TASK", "confidence": 0.9850893020629883}]}, {"text": "Specifically, Tree Substitution Grammars (TSG) can achieve a 36.4% F1 absolute improvement over a state-of-theart surface statistics method.", "labels": [], "entities": [{"text": "Tree Substitution Grammars (TSG", "start_pos": 14, "end_pos": 45, "type": "TASK", "confidence": 0.7443300962448121}, {"text": "F1 absolute", "start_pos": 67, "end_pos": 78, "type": "METRIC", "confidence": 0.8932006657123566}]}, {"text": "We choose French, which has pervasive MWEs, for our experiments.", "labels": [], "entities": [{"text": "French", "start_pos": 10, "end_pos": 16, "type": "DATASET", "confidence": 0.9651198983192444}]}, {"text": "Parsing models naturally accommodate discontinuous MWEs like phrasal verbs, and provide syntactic subcategorization.", "labels": [], "entities": []}, {"text": "By contrast, surface statistics methods are usually limited to binary judgements for contiguous n-grams or dependency bigrams.", "labels": [], "entities": []}, {"text": "725: Frequency distribution of the 11 MWE subcategories in the FTB (training set).", "labels": [], "entities": [{"text": "Frequency", "start_pos": 5, "end_pos": 14, "type": "METRIC", "confidence": 0.9808062314987183}, {"text": "FTB (training set", "start_pos": 63, "end_pos": 80, "type": "DATASET", "confidence": 0.8145894408226013}]}, {"text": "MWEs account for 7.08% of the bracketings and 13.0% of the tokens in the treebank.", "labels": [], "entities": []}, {"text": "Only 21% of the MWEs occur once (\"single\").", "labels": [], "entities": []}, {"text": "We first introduce anew instantiation of the French Treebank that, unlike previous work, does not use gold MWE pre-grouping.", "labels": [], "entities": [{"text": "French Treebank", "start_pos": 45, "end_pos": 60, "type": "DATASET", "confidence": 0.9922932088375092}]}, {"text": "Consequently, our experimental results also provide a better baseline for parsing raw French text.", "labels": [], "entities": [{"text": "parsing raw French text", "start_pos": 74, "end_pos": 97, "type": "TASK", "confidence": 0.9091082364320755}]}], "datasetContent": [{"text": "We evaluate parsing accuracy of the Stanford and DP-TSG models).", "labels": [], "entities": [{"text": "parsing", "start_pos": 12, "end_pos": 19, "type": "TASK", "confidence": 0.9748953580856323}, {"text": "accuracy", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9549575448036194}, {"text": "DP-TSG", "start_pos": 49, "end_pos": 55, "type": "DATASET", "confidence": 0.5266618728637695}]}, {"text": "For comparison, we also include the Berkeley parser ().", "labels": [], "entities": []}, {"text": "For the DP-TSG, we initialized all b s with fair coin tosses and ran for 400 iterations, after which likelihood stopped improving.", "labels": [], "entities": [{"text": "DP-TSG", "start_pos": 8, "end_pos": 14, "type": "DATASET", "confidence": 0.819436252117157}, {"text": "likelihood", "start_pos": 101, "end_pos": 111, "type": "METRIC", "confidence": 0.9822147488594055}]}, {"text": "We report two different parsing metrics.", "labels": [], "entities": []}, {"text": "Evalb is the standard labeled precision/recall metric.", "labels": [], "entities": [{"text": "Evalb", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.5342181324958801}, {"text": "precision", "start_pos": 30, "end_pos": 39, "type": "METRIC", "confidence": 0.9925439357757568}, {"text": "recall", "start_pos": 40, "end_pos": 46, "type": "METRIC", "confidence": 0.9289810657501221}]}, {"text": "Leaf Ancestor measures the cost of transforming guess trees to the reference (.", "labels": [], "entities": []}, {"text": "It was developed in response to the nonterminal/terminal ratio bias of Evalb, which penalizes flat treebanks like the FTB.", "labels": [], "entities": [{"text": "Evalb", "start_pos": 71, "end_pos": 76, "type": "DATASET", "confidence": 0.9228705167770386}, {"text": "FTB", "start_pos": 118, "end_pos": 121, "type": "DATASET", "confidence": 0.9767850041389465}]}, {"text": "The range of the score is between 0 and 1 (higher is better).", "labels": [], "entities": []}, {"text": "We report micro-averaged (whole corpus) and macro-averaged (per sentence) scores.", "labels": [], "entities": []}, {"text": "In terms of parsing accuracy, the Berkeley parser exceeds both Stanford and DP-TSG.", "labels": [], "entities": [{"text": "parsing", "start_pos": 12, "end_pos": 19, "type": "TASK", "confidence": 0.97557133436203}, {"text": "accuracy", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9848757386207581}, {"text": "DP-TSG", "start_pos": 76, "end_pos": 82, "type": "DATASET", "confidence": 0.9517874121665955}]}, {"text": "This is consistent with previous experiments for French by, who show that the Berkeley parser outperforms other models.", "labels": [], "entities": []}, {"text": "It also matches the ordering for English (.", "labels": [], "entities": []}, {"text": "However, the standard baseline for TSG models is a simple parent-annotated PCFG (PA-PCFG).", "labels": [], "entities": []}, {"text": "For English, showed that a similar DP-TSG improved over PA-PCFG by 4.2% F1.", "labels": [], "entities": [{"text": "DP-TSG", "start_pos": 35, "end_pos": 41, "type": "METRIC", "confidence": 0.9356399178504944}, {"text": "F1", "start_pos": 72, "end_pos": 74, "type": "METRIC", "confidence": 0.9975330829620361}]}, {"text": "For French, our gain is a more substantial 8.2% F1.", "labels": [], "entities": [{"text": "F1", "start_pos": 48, "end_pos": 50, "type": "METRIC", "confidence": 0.998454213142395}]}, {"text": "lists overall and per-category MWE identification results for the parsing models.", "labels": [], "entities": [{"text": "MWE identification", "start_pos": 31, "end_pos": 49, "type": "TASK", "confidence": 0.9086956083774567}, {"text": "parsing", "start_pos": 66, "end_pos": 73, "type": "TASK", "confidence": 0.9741995334625244}]}, {"text": "Although DP-TSG is less accurate as a general parsing model, it is more effective at identifying MWEs.", "labels": [], "entities": [{"text": "identifying MWEs", "start_pos": 85, "end_pos": 101, "type": "TASK", "confidence": 0.7961495816707611}]}, {"text": "The predominant approach to MWE identification is the combination of lexical association measures (surface statistics) with a binary classifier.", "labels": [], "entities": [{"text": "MWE identification", "start_pos": 28, "end_pos": 46, "type": "TASK", "confidence": 0.9895715713500977}]}, {"text": "A state-of-the-art, language independent package that implements this approach for higher order n-grams is mwetoolkit (.", "labels": [], "entities": []}, {"text": "In, which is the CFG from which the TSG is extracted.", "labels": [], "entities": []}, {"text": "For mwetoolkit, All indicates the inclusion of all n-grams in the training corpus.", "labels": [], "entities": []}, {"text": "Filter indicates prefiltering of the training corpus by removing rare n-grams (see \u00a7A.2 for details).", "labels": [], "entities": []}, {"text": "mwetoolkit and the CFG from the which the TSG is extracted.", "labels": [], "entities": [{"text": "CFG", "start_pos": 19, "end_pos": 22, "type": "DATASET", "confidence": 0.8632838129997253}]}, {"text": "The TSG-based parsing model outperforms mwetoolkit by 36.4% F1 while providing syntactic subcategory information.", "labels": [], "entities": [{"text": "TSG-based parsing", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.5893790274858475}, {"text": "F1", "start_pos": 60, "end_pos": 62, "type": "METRIC", "confidence": 0.9989423155784607}]}], "tableCaptions": [{"text": " Table 2: Gross corpus statistics for the pre-processed FTB  (training set) and WSJ (sec. 2-21). The FTB sentences are  longer with broader syntactic trees. The FTB POS tag set  has 33% fewer types than the WSJ. The FTB dev set OOV  rate is 17.77% vs. 12.78% for the WSJ.", "labels": [], "entities": [{"text": "FTB  (training set", "start_pos": 56, "end_pos": 74, "type": "DATASET", "confidence": 0.7444073259830475}, {"text": "WSJ", "start_pos": 80, "end_pos": 83, "type": "DATASET", "confidence": 0.9049780368804932}, {"text": "FTB POS tag set", "start_pos": 161, "end_pos": 176, "type": "DATASET", "confidence": 0.8818563520908356}, {"text": "FTB dev set OOV  rate", "start_pos": 216, "end_pos": 237, "type": "METRIC", "confidence": 0.6868973731994629}, {"text": "WSJ", "start_pos": 267, "end_pos": 270, "type": "DATASET", "confidence": 0.9507877230644226}]}, {"text": " Table 3: Frequency distribution of the 11 MWE subcate- gories in the FTB (training set). MWEs account for 7.08%  of the bracketings and 13.0% of the tokens in the treebank.  Only 21% of the MWEs occur once (\"single\").", "labels": [], "entities": [{"text": "Frequency", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9369856715202332}, {"text": "FTB (training set)", "start_pos": 70, "end_pos": 88, "type": "DATASET", "confidence": 0.8295024156570434}]}, {"text": " Table 4: Effects on grammar size and labeled F1 for each  of the manual state splits (development set, sentences \u2264  40 words). markMWE decreases overall accuracy, but  increases both the number of correctly parsed trees (by  0.30%) and per category MWE accuracy.", "labels": [], "entities": [{"text": "F1", "start_pos": 46, "end_pos": 48, "type": "METRIC", "confidence": 0.90101557970047}, {"text": "markMWE", "start_pos": 128, "end_pos": 135, "type": "METRIC", "confidence": 0.7244977355003357}, {"text": "accuracy", "start_pos": 154, "end_pos": 162, "type": "METRIC", "confidence": 0.999334990978241}, {"text": "accuracy", "start_pos": 254, "end_pos": 262, "type": "METRIC", "confidence": 0.9888490438461304}]}, {"text": " Table 6: Standard parsing experiments (test set, sentences  \u2264 40 words). All parsers exceed 96% tagging accuracy.  Berkeley and DP-TSG results are the average of three in- dependent runs.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 105, "end_pos": 113, "type": "METRIC", "confidence": 0.9725788235664368}]}, {"text": " Table 7: MWE identification per category and overall re- sults (test set, sentences \u2264 40 words). MWI and MWCL  do not occur in the test set.", "labels": [], "entities": [{"text": "MWE identification", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.9433588087558746}, {"text": "re- sults", "start_pos": 54, "end_pos": 63, "type": "METRIC", "confidence": 0.8842162489891052}]}, {"text": " Table 8: MWE identification F1 of the best parsing model  vs. the mwetoolkit baseline (test set, sentences \u2264 40  words). PA-PCFG+Features includes the grammar fea- tures in", "labels": [], "entities": [{"text": "MWE identification", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.9260202050209045}, {"text": "F1", "start_pos": 29, "end_pos": 31, "type": "METRIC", "confidence": 0.8590184450149536}]}]}