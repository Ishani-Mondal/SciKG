{"title": [{"text": "Approximate Scalable Bounded Space Sketch for Large Data NLP", "labels": [], "entities": [{"text": "Approximate", "start_pos": 0, "end_pos": 11, "type": "METRIC", "confidence": 0.9674413204193115}]}], "abstractContent": [{"text": "We exploit sketch techniques, especially the Count-Min sketch, a memory, and time efficient framework which approximates the frequency of a word pair in the corpus without explicitly storing the word pair itself.", "labels": [], "entities": []}, {"text": "These methods use hashing to deal with massive amounts of streaming text.", "labels": [], "entities": []}, {"text": "We apply Count-Min sketch to approximate word pair counts and exhibit their effectiveness on three important NLP tasks.", "labels": [], "entities": []}, {"text": "Our experiments demonstrate that on all of the three tasks, we get performance comparable to Exact word pair counts setting and state-of-the-art system.", "labels": [], "entities": [{"text": "Exact word pair counts setting", "start_pos": 93, "end_pos": 123, "type": "TASK", "confidence": 0.5511472404003144}]}, {"text": "Our method scales to 49 GB of unzipped web data using bounded space of 2 billion counters (8 GB memory).", "labels": [], "entities": []}], "introductionContent": [{"text": "There is more data available today on the web than there has ever been and it keeps increasing.", "labels": [], "entities": []}, {"text": "Use of large data in the Natural Language Processing (NLP) community is not new.", "labels": [], "entities": []}, {"text": "Many NLP problems () have benefited from having large amounts of data.", "labels": [], "entities": []}, {"text": "However, processing large amounts of data is still challenging.", "labels": [], "entities": []}, {"text": "This has motivated NLP community to use commodity clusters.", "labels": [], "entities": []}, {"text": "For example, used 1500 machines fora day to compute the relative frequencies of n-grams from 1.8TB of web data.", "labels": [], "entities": []}, {"text": "In another work, a corpus of roughly 1.6 Terawords was used by to compute pairwise similarities of the words in the test sets using the MapReduce infrastructure on 2, 000 cores.", "labels": [], "entities": []}, {"text": "However, the inaccessibility of clusters to an average user has attracted the NLP community to use streaming, randomized, and approximate algorithms to handle large amounts of data (.", "labels": [], "entities": []}, {"text": "Streaming approaches) provide memory and time-efficient framework to deal with terabytes of data.", "labels": [], "entities": []}, {"text": "However, these approaches are proposed to solve a singe problem.", "labels": [], "entities": []}, {"text": "For example, our earlier work ( and build approximate language models and show their effectiveness in Statistical Machine Translation (SMT).", "labels": [], "entities": [{"text": "Statistical Machine Translation (SMT)", "start_pos": 102, "end_pos": 139, "type": "TASK", "confidence": 0.840688685576121}]}, {"text": "Streambased translation models ( has been shown effective to handle large parallel streaming data for SMT.", "labels": [], "entities": [{"text": "Streambased translation", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.5853940099477768}, {"text": "SMT", "start_pos": 102, "end_pos": 105, "type": "TASK", "confidence": 0.9927714467048645}]}, {"text": "In Van Durme and Lall (2009b), a Talbot Osborne Morris Bloom (TOMB) Counter (Van Durme and Lall, 2009a) was used to find the top-K verbs \"y\" given verb \"x\" using the highest approximate online Pointwise Mutual Information (PMI) values.", "labels": [], "entities": [{"text": "Talbot Osborne Morris Bloom (TOMB) Counter", "start_pos": 33, "end_pos": 75, "type": "METRIC", "confidence": 0.6789077632129192}]}, {"text": "In this paper, we explore sketch techniques, especially the Count-Min sketch to build a single model to show its effectiveness on three important NLP tasks: \u2022 Predicting the Semantic Orientation of words \u2022 Distributional Approaches for word similarity \u2022 Unsupervised Dependency Parsing) with a little linguistics knowledge.", "labels": [], "entities": [{"text": "Predicting the Semantic Orientation of words", "start_pos": 159, "end_pos": 203, "type": "TASK", "confidence": 0.7506501972675323}]}, {"text": "In all these tasks, we need to compute association measures like Pointwise Mutual Information (PMI), and Log Likelihood ratio (LLR) between words.", "labels": [], "entities": [{"text": "Pointwise Mutual Information (PMI)", "start_pos": 65, "end_pos": 99, "type": "METRIC", "confidence": 0.7401358087857565}, {"text": "Log Likelihood ratio (LLR)", "start_pos": 105, "end_pos": 131, "type": "METRIC", "confidence": 0.7552145222822825}]}, {"text": "To compute association scores (AS), we need to count the number of times pair of words appear together within a certain window size.", "labels": [], "entities": [{"text": "association scores (AS)", "start_pos": 11, "end_pos": 34, "type": "METRIC", "confidence": 0.8481254816055298}]}, {"text": "However, explicitly storing the counts of all word pairs is both computationally expensive and memory intensive.", "labels": [], "entities": []}, {"text": "Moreover, the memory usage keeps increasing with increase in corpus size.", "labels": [], "entities": []}, {"text": "We explore Count-Min (CM) sketch to address the issue of efficient storage of such data.", "labels": [], "entities": []}, {"text": "The CM sketch stores counts of all word pairs within a bounded space.", "labels": [], "entities": []}, {"text": "Storage space saving is achieved by approximating the frequency of word pairs in the corpus without explicitly storing the word pairs themselves.", "labels": [], "entities": [{"text": "Storage space saving", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.7173433502515157}]}, {"text": "Both updating (adding anew word pair or increasing the frequency of existing word pair) and querying (finding the frequency of a given word pair) are constant time operations making it efficient online storage data structure for large data.", "labels": [], "entities": []}, {"text": "Sketches are scalable and can easily be implemented in distributed setting.", "labels": [], "entities": []}, {"text": "We use CM sketch to store counts of word pairs (except word pairs involving stop words) within a window of size 1 7 over different size corpora.", "labels": [], "entities": []}, {"text": "We store exact counts of words (except stop words) in hash table (since the number of unique words is not large that is quadratically less than the number of unique word pairs).", "labels": [], "entities": []}, {"text": "The approximate PMI and LLR scores are computed using these approximate counts and are applied to solve our three NLP tasks.", "labels": [], "entities": []}, {"text": "Our experiments demonstrate that on all of the three tasks, we get performance comparable to Exact word pair counts setting and state-of-the-art system.", "labels": [], "entities": [{"text": "Exact word pair counts setting", "start_pos": 93, "end_pos": 123, "type": "TASK", "confidence": 0.5511473417282104}]}, {"text": "Our method scales to 49 GB of unzipped web data using bounded space of 2 billion counters (8 GB memory).", "labels": [], "entities": []}, {"text": "This work expands upon our earlier workshop papers ().", "labels": [], "entities": []}], "datasetContent": [{"text": "To show the effectiveness of the CM sketch and CM sketch with conservative update (CU) in the context of NLP, we perform intrinsic evaluations.", "labels": [], "entities": [{"text": "conservative update (CU)", "start_pos": 62, "end_pos": 86, "type": "METRIC", "confidence": 0.8004617333412171}]}, {"text": "First, the intrinsic evaluations are designed to measure the error in the approximate counts returned by CM sketch compared to their true counts.", "labels": [], "entities": []}, {"text": "Second, we compare the word pairs association rankings obtained using PMI and LLR with sketch and exact counts.", "labels": [], "entities": [{"text": "exact", "start_pos": 98, "end_pos": 103, "type": "METRIC", "confidence": 0.9734647870063782}]}, {"text": "It is memory and time intensive to perform many intrinsic evaluations on large data).", "labels": [], "entities": []}, {"text": "Hence, we use a subset of corpus of 2 million sentences (Subset) from Gigaword (Graff, 2003) for it.", "labels": [], "entities": [{"text": "Gigaword (Graff, 2003)", "start_pos": 70, "end_pos": 92, "type": "DATASET", "confidence": 0.8426244656244913}]}, {"text": "We generate words and word pairs over a window of size 7.", "labels": [], "entities": []}, {"text": "We store exact counts of words (except stop words) in a hash table and store approximate counts of word pairs (except word pairs involving stop words) in the sketch.", "labels": [], "entities": []}, {"text": "Our experiments are on a dependency-converted version of section 23 of the Penn Treebank using modified Collins' head finding rules.", "labels": [], "entities": [{"text": "section 23 of the Penn Treebank", "start_pos": 57, "end_pos": 88, "type": "DATASET", "confidence": 0.7326309382915497}, {"text": "head finding", "start_pos": 113, "end_pos": 125, "type": "TASK", "confidence": 0.7117948681116104}]}, {"text": "We measure accuracies as directed, unlabeled dependency accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.969021201133728}]}, {"text": "We separately report results of sentences of length at most 10, at most 20 and finally of all length.", "labels": [], "entities": []}, {"text": "Note that there is no training or cross-validation: we simply run our MST parser on test data directly.", "labels": [], "entities": []}, {"text": "The results of the parsing experiments are shown in.", "labels": [], "entities": [{"text": "parsing", "start_pos": 19, "end_pos": 26, "type": "TASK", "confidence": 0.9816810488700867}]}, {"text": "We compare against the following alternative systems.", "labels": [], "entities": []}, {"text": "The first, Cohen-Dirichlet and Cohen-Best, are previously reported state-of-the-art results for unsupervised Bayesian dependency parsing.", "labels": [], "entities": [{"text": "Bayesian dependency parsing", "start_pos": 109, "end_pos": 136, "type": "TASK", "confidence": 0.5784314572811127}]}, {"text": "The first is results using a simple Dirichlet prior; the second is the best reported results for any system from that paper.", "labels": [], "entities": []}, {"text": "Next, we compare against an \"oracle\" system that uses LLR extracted from the training data for the Penn Treebank, where the LLR is based on the probability of observing an edge given two words.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 99, "end_pos": 112, "type": "DATASET", "confidence": 0.9940813779830933}]}, {"text": "This is not a true oracle in the sense that we might be able to do better, but it is unlikely.", "labels": [], "entities": []}, {"text": "The next two baseline system are simple right branching baseline trees.", "labels": [], "entities": []}, {"text": "The Baseline system is a purely rightbranching tree.", "labels": [], "entities": []}, {"text": "The Baseline+Ling system is one that is right branching except that it can only create edges that are compatible with the linguistic rules, provided a relevant rule exists.", "labels": [], "entities": []}, {"text": "For short sentences, this is competitive with the Dirichlet prior results.", "labels": [], "entities": []}, {"text": "Finally we report variants of our approach using association scores computed on the GWB50 using CU sketch with 2 billion counters.", "labels": [], "entities": [{"text": "GWB50", "start_pos": 84, "end_pos": 89, "type": "DATASET", "confidence": 0.9872063398361206}]}, {"text": "We experiment with two association scores: LLR and PMI.", "labels": [], "entities": [{"text": "PMI", "start_pos": 51, "end_pos": 54, "type": "METRIC", "confidence": 0.8440483808517456}]}, {"text": "For each measure, we report results based on the three approaches described earlier for setting the \u03b1 hyperparameters.", "labels": [], "entities": []}, {"text": "Error bars for our approaches are 95% confidence intervals based on bootstrap resampling.", "labels": [], "entities": []}, {"text": "The results show that, for this task, PMI seems slightly better than LLR, across the board.", "labels": [], "entities": [{"text": "PMI", "start_pos": 38, "end_pos": 41, "type": "METRIC", "confidence": 0.704259991645813}]}, {"text": "The OP-TIMAL performance (based on tuning two hyperparameters) is amazingly strong: clearly beating out all the baselines, and only about 15 points behind the ORACLE system.", "labels": [], "entities": [{"text": "OP-TIMAL", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.8795287013053894}, {"text": "ORACLE", "start_pos": 159, "end_pos": 165, "type": "METRIC", "confidence": 0.9946687817573547}]}, {"text": "Using the BALANCED approach causes a degradation of only 3 points from the OPTIMAL on sentences of all lengths.", "labels": [], "entities": [{"text": "BALANCED", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9846845865249634}, {"text": "OPTIMAL", "start_pos": 75, "end_pos": 82, "type": "METRIC", "confidence": 0.9653883576393127}]}, {"text": "In general, the balancing approach seems to be slightly worse than the semi-supervised approach, except on very short sentences: for those, it is substantially better.", "labels": [], "entities": []}, {"text": "Overall, though, the results for both Balanced and Semisup are competitive with state-of-the-art unsupervised learning algorithms.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Evaluating the PMI and LLR rankings obtained using", "labels": [], "entities": [{"text": "PMI", "start_pos": 25, "end_pos": 28, "type": "TASK", "confidence": 0.6705992221832275}]}]}