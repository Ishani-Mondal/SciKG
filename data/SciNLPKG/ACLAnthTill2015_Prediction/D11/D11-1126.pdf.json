{"title": [{"text": "Watermarking the Outputs of Structured Prediction with an application in Statistical Machine Translation", "labels": [], "entities": [{"text": "Structured Prediction", "start_pos": 28, "end_pos": 49, "type": "TASK", "confidence": 0.7536080777645111}, {"text": "Statistical Machine Translation", "start_pos": 73, "end_pos": 104, "type": "TASK", "confidence": 0.8245070775349935}]}], "abstractContent": [{"text": "We propose a general method to watermark and probabilistically identify the structured outputs of machine learning algorithms.", "labels": [], "entities": []}, {"text": "Our method is robust to local editing operations and provides well defined trade-offs between the ability to identify algorithm outputs and the quality of the watermarked output.", "labels": [], "entities": []}, {"text": "Unlike previous work in the field, our approach does not rely on controlling the inputs to the algorithm and provides probabilistic guarantees on the ability to identify collections of results from one's own algorithm.", "labels": [], "entities": []}, {"text": "We present an application in statistical machine translation, where machine translated output is watermarked at minimal loss in translation quality and detected with high recall.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 29, "end_pos": 60, "type": "TASK", "confidence": 0.676791230837504}, {"text": "recall", "start_pos": 171, "end_pos": 177, "type": "METRIC", "confidence": 0.9977273344993591}]}, {"text": "1 Motivation Machine learning algorithms provide structured results to input queries by simulating human behavior.", "labels": [], "entities": []}, {"text": "Examples include automatic machine translation (Brown et al., 1993) or automatic text and rich media summarization (Goldstein et al., 1999).", "labels": [], "entities": [{"text": "machine translation", "start_pos": 27, "end_pos": 46, "type": "TASK", "confidence": 0.6653906255960464}, {"text": "automatic text and rich media summarization", "start_pos": 71, "end_pos": 114, "type": "TASK", "confidence": 0.5863945335149765}]}, {"text": "These algorithms often estimate some portion of their models from publicly available human generated data.", "labels": [], "entities": []}, {"text": "As new services that output structured results are made available to the public and the results disseminated on the web, we face a daunting new challenge: Machine generated structured results contaminate the pool of naturally generated human data.", "labels": [], "entities": []}, {"text": "For example, machine translated output and human generated translations are currently both found extensively on the web, with no automatic way of distinguishing between them.", "labels": [], "entities": []}, {"text": "Algorithms that mine data from the web (Uszko-reit et al., 2010), with the goal of learning to simulate human behavior, will now learn models from this contaminated and potentially self-generated data, reinforcing the errors committed by earlier versions of the algorithm.", "labels": [], "entities": []}, {"text": "It is beneficial to be able to identify a set of encountered structured results as having been generated by one's own algorithm, with the purpose of filtering such results when building new models.", "labels": [], "entities": []}, {"text": "Problem Statement: We define a struc-tured result of a query q as r = {z 1 \u00b7 \u00b7 \u00b7 z L } where the order and identity of elements z i are important to the quality of the result r.", "labels": [], "entities": []}, {"text": "The structural aspect of the result implies the existence of alternative results (across both the order of elements and the elements themselves) that might vary in their quality.", "labels": [], "entities": []}, {"text": "Given a collection of N results, C N = r 1 \u00b7 \u00b7 \u00b7 r N , where each result r i has k ranked alternatives D k (q i) of relatively similar quality and queries q 1 \u00b7 \u00b7 \u00b7 q N are arbitrary and not controlled by the watermarking algorithm, we define the watermarking task as: Task.", "labels": [], "entities": []}, {"text": "Replace r i with r i \u2208 D k (q i) for some subset of results in C N to produce a watermarked collection C N such that: \u2022 C N is probabilistically identifiable as having been generated by one's own algorithm.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "We evaluate our watermarking approach applied to the outputs of statistical machine translation under the following experimental setup.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 64, "end_pos": 95, "type": "TASK", "confidence": 0.7012426952521006}]}, {"text": "A repository of parallel (aligned source and target language) web documents is sampled to produce a large corpus on which to evaluate the watermarking classification performance.", "labels": [], "entities": []}, {"text": "The 1368 corpora represent translations into 4 diverse target languages, using English as the source language.", "labels": [], "entities": []}, {"text": "Each document in this corpus can be considered a collection of un-watermarked structured results, where source sentences are queries and each target sentence represents a structured result.", "labels": [], "entities": []}, {"text": "Using a state-of-the-art phrase-based statistical machine translation system () trained on parallel documents identified by, we generate a set of 100 alternative translations for each source sentence.", "labels": [], "entities": [{"text": "phrase-based statistical machine translation", "start_pos": 25, "end_pos": 69, "type": "TASK", "confidence": 0.6072444170713425}]}, {"text": "We apply the proposed watermarking approach, along with the proposed refinements that address task specific loss (Section 3.4) and robustness to edit operations (Section 3.3) to generate watermarked corpora.", "labels": [], "entities": []}, {"text": "Each method is controlled via a single parameter (like k or \u03bb) which is varied to generate alternative watermarked collections.", "labels": [], "entities": []}, {"text": "For each parameter value, we evaluate the Recall Rate and Quality Degradation with the goal of finding a setting that yields a high recall rate, minimal quality degradation.", "labels": [], "entities": [{"text": "Recall Rate", "start_pos": 42, "end_pos": 53, "type": "METRIC", "confidence": 0.9777099788188934}, {"text": "recall rate", "start_pos": 132, "end_pos": 143, "type": "METRIC", "confidence": 0.9879870116710663}]}, {"text": "False positive rates are evaluated based on a fixed classification significance level of \u03b1 = 0.05.", "labels": [], "entities": []}, {"text": "The false positive and recall rates are evaluated on the word level; a document that is misclassified or correctly identified contributes its length in words towards the error calculation.", "labels": [], "entities": [{"text": "recall", "start_pos": 23, "end_pos": 29, "type": "METRIC", "confidence": 0.9988802075386047}, {"text": "length", "start_pos": 142, "end_pos": 148, "type": "METRIC", "confidence": 0.9533652067184448}]}, {"text": "In this work, we use \u03b1 = 0.05 during classification corresponding to an expected 5% false positive rate.", "labels": [], "entities": [{"text": "classification", "start_pos": 37, "end_pos": 51, "type": "TASK", "confidence": 0.9438039660453796}, {"text": "false positive rate", "start_pos": 84, "end_pos": 103, "type": "METRIC", "confidence": 0.8665988047917684}]}, {"text": "The false positive rate is a function of hand the significance level \u03b1 and therefore constant across the parameter values k and \u03bb.", "labels": [], "entities": [{"text": "false positive rate", "start_pos": 4, "end_pos": 23, "type": "METRIC", "confidence": 0.8747534155845642}, {"text": "significance level \u03b1", "start_pos": 50, "end_pos": 70, "type": "METRIC", "confidence": 0.9052663842837015}]}, {"text": "We evaluate quality degradation on human translated test corpora that are more typical for machine translation evaluation.", "labels": [], "entities": [{"text": "machine translation evaluation", "start_pos": 91, "end_pos": 121, "type": "TASK", "confidence": 0.8477592269579569}]}, {"text": "Each test corpus consists of 5000 source sentences randomly selected from the web and translated into each respective language.", "labels": [], "entities": []}, {"text": "We chose to evaluate quality on test corpora to ensure that degradations are not hidden by imperfectly matched web corpora and are consistent with the kind of results often reported for machine translation systems.", "labels": [], "entities": []}, {"text": "As with the classification corpora, we create watermarked versions at each parameter value.", "labels": [], "entities": []}, {"text": "For a given pa- rameter value, we measure false positive and recall rates on the classification corpora and quality degradation on the evaluation corpora.", "labels": [], "entities": [{"text": "recall", "start_pos": 61, "end_pos": 67, "type": "METRIC", "confidence": 0.9930807948112488}]}, {"text": "shows corpus statistics for the classification and test corpora and non-watermarked BLEU scores for each target language.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 84, "end_pos": 88, "type": "METRIC", "confidence": 0.9947023987770081}]}, {"text": "All source texts are in English.", "labels": [], "entities": []}, {"text": "Our first set of experiments demonstrates baseline performance using the watermarking criteria in Equation 5 versus the refinements suggested in Section 3.4 to mitigate quality degradation.", "labels": [], "entities": [{"text": "Equation", "start_pos": 98, "end_pos": 106, "type": "METRIC", "confidence": 0.7493521571159363}]}, {"text": "The h function is computed on the full sentence result r with no sub-event mapping.", "labels": [], "entities": []}, {"text": "The following methods are evaluated in.", "labels": [], "entities": []}, {"text": "\u2022 Baseline method (labeled \"max K-best\"): selects r purely based on gain in watermarking signal (Equation 5) and is parameterized by k: the number of alternatives considered for each result.", "labels": [], "entities": []}, {"text": "\u2022 Rank interpolation: incorporates rank into w, varying the interpolation parameter \u03bb.", "labels": [], "entities": []}, {"text": "\u2022 Cost interpolation: incorporates cost into w, varying the interpolation parameter \u03bb.", "labels": [], "entities": []}, {"text": "The observed false positive rate on the French classification corpora is 1.9%.", "labels": [], "entities": [{"text": "false positive rate", "start_pos": 13, "end_pos": 32, "type": "METRIC", "confidence": 0.9692638119061788}, {"text": "French classification corpora", "start_pos": 40, "end_pos": 69, "type": "DATASET", "confidence": 0.956917405128479}]}, {"text": "We consider 0.2% BLEU loss as a threshold for acceptable quality degradation.", "labels": [], "entities": [{"text": "BLEU loss", "start_pos": 17, "end_pos": 26, "type": "METRIC", "confidence": 0.9905807375907898}]}, {"text": "Each method is judged by its ability to achieve high recall below this quality degradation threshold.", "labels": [], "entities": [{"text": "recall", "start_pos": 53, "end_pos": 59, "type": "METRIC", "confidence": 0.998880922794342}]}, {"text": "Applying cost interpolation yields the best results in, achieving a recall of 85% at 0.2% BLEU loss, while rank interpolation achieves a recall of 76%.", "labels": [], "entities": [{"text": "recall", "start_pos": 68, "end_pos": 74, "type": "METRIC", "confidence": 0.9995232820510864}, {"text": "BLEU loss", "start_pos": 90, "end_pos": 99, "type": "METRIC", "confidence": 0.989608108997345}, {"text": "recall", "start_pos": 137, "end_pos": 143, "type": "METRIC", "confidence": 0.9994021654129028}]}, {"text": "The baseline approach of selecting the highest gain candidate within a depth of k candidates does not provide sufficient parameterization to yield low quality degradation.", "labels": [], "entities": []}, {"text": "At k = 2, this method yields almost 90% recall, but with approximately 0.4% BLEU loss.", "labels": [], "entities": [{"text": "recall", "start_pos": 40, "end_pos": 46, "type": "METRIC", "confidence": 0.9997088313102722}, {"text": "BLEU", "start_pos": 76, "end_pos": 80, "type": "METRIC", "confidence": 0.9996985197067261}]}, {"text": "In Section 5.2, we proposed mapping results into sub-events or features.", "labels": [], "entities": []}, {"text": "We considered alternative feature mappings in, finding that mapping sentence results into a collection of 3-5 grams yields acceptable false positive rates at varied levels of \u03b1. presents results that compare moving from the result level hashing to the 3-5 gram sub-result mapping.", "labels": [], "entities": []}, {"text": "We show the impact of the mapping on the baseline max K-best method as well as for cost interpolation.", "labels": [], "entities": []}, {"text": "There are substantial reductions in recall rate at the 0.2% BLEU loss level when applying sub-result mappings in cases.", "labels": [], "entities": [{"text": "recall rate", "start_pos": 36, "end_pos": 47, "type": "METRIC", "confidence": 0.9712204039096832}, {"text": "BLEU loss level", "start_pos": 60, "end_pos": 75, "type": "METRIC", "confidence": 0.9809102813402811}]}, {"text": "The cost interpolation method recall drops from 85% to 77% when using the 3-5 grams event mapping.", "labels": [], "entities": [{"text": "recall", "start_pos": 30, "end_pos": 36, "type": "METRIC", "confidence": 0.9994818568229675}]}, {"text": "The observed false positive rate of the 3-5 gram mapping is 4.7%.", "labels": [], "entities": [{"text": "false positive rate", "start_pos": 13, "end_pos": 32, "type": "METRIC", "confidence": 0.9075582027435303}]}, {"text": "By using the 3-5 gram mapping, we expect to increase robustness against local word edit operations, but we have sacrificed recall rate due to the inherent distributional bias discussed in Section 3.3.", "labels": [], "entities": [{"text": "word edit", "start_pos": 78, "end_pos": 87, "type": "TASK", "confidence": 0.7241464853286743}, {"text": "recall rate", "start_pos": 123, "end_pos": 134, "type": "METRIC", "confidence": 0.9827910363674164}]}, {"text": "The watermarking approach proposed here introduces no language specific watermarking operations and it is thus broadly applicable to translating into all languages.", "labels": [], "entities": []}, {"text": "In, we report results for the baseline and cost interpolation methods, considering both the result level and 3-5 gram mapping.", "labels": [], "entities": []}, {"text": "We set \u03b1 = 0.05 and measure recall at 0.2% BLEU degradation for translation from English into Arabic, French, Hindi and Turkish.", "labels": [], "entities": [{"text": "recall", "start_pos": 28, "end_pos": 34, "type": "METRIC", "confidence": 0.9993446469306946}, {"text": "BLEU degradation", "start_pos": 43, "end_pos": 59, "type": "METRIC", "confidence": 0.9763432145118713}, {"text": "translation from English", "start_pos": 64, "end_pos": 88, "type": "TASK", "confidence": 0.8702025413513184}]}, {"text": "The observed false positive rates for full sentence hashing are: Arabic: 2.4%, French: 1.8%, Hindi: 5.6% and Turkish: 5.5%, while for the 3-5 gram mapping, they are: Arabic: 5.8%, French: 7.5%, Hindi:3.5% and Turkish: 6.2%.", "labels": [], "entities": [{"text": "full sentence hashing", "start_pos": 38, "end_pos": 59, "type": "TASK", "confidence": 0.7301947871843973}]}, {"text": "Underlying translation quality plays an important role in translation quality degradation when watermarking.", "labels": [], "entities": [{"text": "translation quality degradation", "start_pos": 58, "end_pos": 89, "type": "TASK", "confidence": 0.8654319842656454}]}, {"text": "Without a sub-result mapping, French (BLEU: 26.45%) 1370 achieves recall of 85% at 0.2% BLEU loss, while the other languages achieve over 90% recall at the same BLEU loss threshold.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 38, "end_pos": 42, "type": "METRIC", "confidence": 0.9940842986106873}, {"text": "recall", "start_pos": 66, "end_pos": 72, "type": "METRIC", "confidence": 0.9993466734886169}, {"text": "BLEU loss", "start_pos": 88, "end_pos": 97, "type": "METRIC", "confidence": 0.9883522987365723}, {"text": "recall", "start_pos": 142, "end_pos": 148, "type": "METRIC", "confidence": 0.9983970522880554}, {"text": "BLEU loss threshold", "start_pos": 161, "end_pos": 180, "type": "METRIC", "confidence": 0.9720455805460612}]}, {"text": "Using a subresult mapping degrades quality for each language pair, but changes the relative performance.", "labels": [], "entities": []}, {"text": "Turkish experiences the highest relative drop in recall, unlike French and Arabic, where results are relatively more robust to using sub-sentence mappings.", "labels": [], "entities": [{"text": "recall", "start_pos": 49, "end_pos": 55, "type": "METRIC", "confidence": 0.9966508746147156}]}, {"text": "This is likely a result of differences in n-gram distributions across these languages.", "labels": [], "entities": []}, {"text": "The languages considered here all use space separated words.", "labels": [], "entities": []}, {"text": "For languages that do not, like Chinese or Thai, our approach can be applied at the character level.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Content statistics for classification and quality degradation corpora. Non-watermarked BLEU  scores are reported for the quality corpora.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 97, "end_pos": 101, "type": "METRIC", "confidence": 0.9948847889900208}]}]}