{"title": [{"text": "A Fast, Accurate, Non-Projective, Semantically-Enriched Parser", "labels": [], "entities": []}], "abstractContent": [{"text": "Dependency parsers are critical components within many NLP systems.", "labels": [], "entities": [{"text": "Dependency parsers", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7974264025688171}]}, {"text": "However, currently available dependency parsers each exhibit at least one of several weaknesses, including high running time, limited accuracy, vague dependency labels, and lack of non-projectivity support.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 134, "end_pos": 142, "type": "METRIC", "confidence": 0.9977130889892578}]}, {"text": "Furthermore, no commonly used parser provides additional shallow semantic interpretation, such as preposition sense disambiguation and noun compound interpretation.", "labels": [], "entities": [{"text": "preposition sense disambiguation", "start_pos": 98, "end_pos": 130, "type": "TASK", "confidence": 0.6310388545195261}, {"text": "noun compound interpretation", "start_pos": 135, "end_pos": 163, "type": "TASK", "confidence": 0.7491870125134786}]}, {"text": "In this paper, we present anew dependency-tree conversion of the Penn Treebank along with its associated fine-grain dependency labels and a fast, accurate parser trained on it.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 65, "end_pos": 78, "type": "DATASET", "confidence": 0.9955139458179474}]}, {"text": "We explain how a non-projective extension to shift-reduce parsing can be incorporated into non-directional easy-first parsing.", "labels": [], "entities": []}, {"text": "The parser performs well when evaluated on the standard test section of the Penn Treebank, outperforming several popular open source dependency parsers; it is, to the best of our knowledge, the first dependency parser capable of parsing more than 75 sentences per second at over 93% accuracy.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 76, "end_pos": 89, "type": "DATASET", "confidence": 0.9723341464996338}, {"text": "accuracy", "start_pos": 283, "end_pos": 291, "type": "METRIC", "confidence": 0.9939509630203247}]}], "introductionContent": [{"text": "Parsers are critical components within many natural language processing (NLP) systems, including systems for information extraction, question answering, machine translation, recognition of textual entailment, summarization, and many others.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 109, "end_pos": 131, "type": "TASK", "confidence": 0.7957773506641388}, {"text": "question answering", "start_pos": 133, "end_pos": 151, "type": "TASK", "confidence": 0.8896384239196777}, {"text": "machine translation", "start_pos": 153, "end_pos": 172, "type": "TASK", "confidence": 0.7854932248592377}, {"text": "recognition of textual entailment", "start_pos": 174, "end_pos": 207, "type": "TASK", "confidence": 0.8753438591957092}, {"text": "summarization", "start_pos": 209, "end_pos": 222, "type": "TASK", "confidence": 0.98411625623703}]}, {"text": "Unfortunately, currently available dependency parsers suffer from at least one of several weaknesses including high running time, limited accuracy, vague dependency labels, and lack of non-projectivity support.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 138, "end_pos": 146, "type": "METRIC", "confidence": 0.9974520802497864}]}, {"text": "Furthermore, few parsers include any sort of additional semantic interpretation, such as interpretations for prepositions, possessives, or noun compounds.", "labels": [], "entities": []}, {"text": "In this paper, we describe 1) anew dependency conversion (Section 3) of the Penn Treebank) along with the associated dependency label scheme, which is based upon the Stanford parser's popular scheme (, and a fast, accurate dependency parser with non-projectivity support (Section 4) and additional integrated semantic annotation modules for automatic preposition sense disambiguation and noun compound interpretation (Section 5).", "labels": [], "entities": [{"text": "dependency conversion", "start_pos": 35, "end_pos": 56, "type": "TASK", "confidence": 0.7240623235702515}, {"text": "Penn Treebank", "start_pos": 76, "end_pos": 89, "type": "DATASET", "confidence": 0.9951573312282562}, {"text": "automatic preposition sense disambiguation", "start_pos": 341, "end_pos": 383, "type": "TASK", "confidence": 0.6007818877696991}, {"text": "noun compound interpretation", "start_pos": 388, "end_pos": 416, "type": "TASK", "confidence": 0.7563573718070984}]}, {"text": "We show how swap-based reordering technique for non-projective shift-reduce-style parsing can be integrated into the non-directional easy-first framework of to support non-projectivity, and we report the results of our parsing experiments on the standard test section of the PTB, providing comparisons with several freely available parsers, including implementation, MALTPARSER), MSTPARSER (), the Charniak (2000) parser, and the Berkeley parser (.", "labels": [], "entities": [{"text": "PTB", "start_pos": 275, "end_pos": 278, "type": "DATASET", "confidence": 0.89229416847229}]}, {"text": "The experimental results show that the parser is substantially more accurate than Goldberg and Elhadad's original implementation, with fairly similar overall speed.", "labels": [], "entities": []}, {"text": "Furthermore, the results prove that Stanford-granularity dependency labels can be learned by modern dependency parsing systems when using our Treebank conversion, unlike the Stanford conversion, for which show that this isn't the case.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 100, "end_pos": 118, "type": "TASK", "confidence": 0.7069898545742035}]}, {"text": "The optional semantic annotation modules also 1257 perform well, with the preposition sense disambiguation module exceeding the accuracy of the previous best reported result for fine-grained preposition sense disambiguation (85.7% vs 84.8%), the possessives interpretation system achieving over 85% accuracy, and the noun compound interpretation system performing similarly to an earlier version described by  at just over 79% accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 128, "end_pos": 136, "type": "METRIC", "confidence": 0.9987039566040039}, {"text": "possessives interpretation", "start_pos": 246, "end_pos": 272, "type": "TASK", "confidence": 0.7368130087852478}, {"text": "accuracy", "start_pos": 299, "end_pos": 307, "type": "METRIC", "confidence": 0.9956135749816895}, {"text": "noun compound interpretation", "start_pos": 317, "end_pos": 345, "type": "TASK", "confidence": 0.7370899120966593}, {"text": "accuracy", "start_pos": 427, "end_pos": 435, "type": "METRIC", "confidence": 0.9923886656761169}]}, {"text": "A tree is non-projective if the sequence of words visited in a left-to-right, depth-first traversal of the sentence's parse tree is different than the actual word order of the sentence.", "labels": [], "entities": []}, {"text": "These latter two issues are not problems for constituent parses with binarized output and functional tags.", "labels": [], "entities": [{"text": "constituent parses", "start_pos": 45, "end_pos": 63, "type": "TASK", "confidence": 0.6472757756710052}]}, {"text": "Though there are many syntactic parsers than can reconstruct the grammatical structure of a text, there are few, if any, accurate and widely accepted systems that also produce shallow semantic analysis of the text.", "labels": [], "entities": []}, {"text": "For example, a parser may indicate that, in the case of 'ice statue', 'ice' modifies 'statue' but will not indicate that 'ice' is the substance of the statue.", "labels": [], "entities": []}, {"text": "Similarly, a parser will indicate which words a preposition connects but will not give any semantic interpretation (e.g., 'the boy with the pirate hat' \u2192 wearing or carrying, 'wash with cold water' \u2192 means, 'shave with the grain' \u2192 in the same direction as).", "labels": [], "entities": []}, {"text": "While, in some cases, it maybe possible to use the output from a separate system for this purpose, doing so is often difficult in practice due to a wide variety of complications, including programming language differences, alternative data formats, and, sometimes, other parsers.", "labels": [], "entities": []}], "datasetContent": [{"text": "The following split of the Penn Treebank) was used for the experiments: sections 2-21 for training, 22 for development, and 23 for testing.", "labels": [], "entities": [{"text": "Penn Treebank)", "start_pos": 27, "end_pos": 41, "type": "DATASET", "confidence": 0.9934561053911845}]}, {"text": "For part-of-speech (POS) tagging, we used an inhouse SVM-based POS tagger modeled after the work of . The training data was tagged in a 10-fold fashion; each fold was tagged using a tagger trained from the nine remaining folds.", "labels": [], "entities": [{"text": "part-of-speech (POS) tagging", "start_pos": 4, "end_pos": 32, "type": "TASK", "confidence": 0.617949378490448}]}, {"text": "The development and test sections were tagged by an instance of the tagger trained using the entire training set.", "labels": [], "entities": []}, {"text": "The full details of the POS tagger are outside the scope of this paper; it is included with the parser download.", "labels": [], "entities": [{"text": "POS tagger", "start_pos": 24, "end_pos": 34, "type": "TASK", "confidence": 0.5796083807945251}]}, {"text": "The final parser was trained for 31 iterations, which is the point at which its performance on the development set peaked.", "labels": [], "entities": []}, {"text": "One test run was performed with non-projectivity support disabled in order to get some idea of the impact of the move operations on the parser's overall performance; also, since the parsers used for comparison had no access to the unsupervised word clusters, an additional instance of the parser was trained with every word treated as belonging to the same cluster so as to facilitate a more fair comparison.", "labels": [], "entities": []}, {"text": "Seven different dependency parsing models were) 8 . The model trained using easy-first parser serves as something of a baseline.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 16, "end_pos": 34, "type": "TASK", "confidence": 0.6805316209793091}]}, {"text": "The four MALTPARSER parsing models used the arceager, arc-standard, stack-eager, and stack-lazy algorithms.", "labels": [], "entities": [{"text": "MALTPARSER parsing", "start_pos": 9, "end_pos": 27, "type": "TASK", "confidence": 0.7403008341789246}]}, {"text": "One of the MSTPARSER models used the Chu-Liu-Edmonds maximum spanning tree approach, and the other used the algorithm with second order features and a nonprojective rewriting post-processing step.", "labels": [], "entities": []}, {"text": "Unfortunately, it is not possible to directly compare the parser's accuracy with most popular constituent parsers such as the Charniak (2000) and Berkeley () parsers 9 both because they do not produce functional tags for subjects, direct objects, etc., which are required for the final script of the constituent-to-dependency conversion routine, and because they determine part-of-speech tags in conjunction with the parsing.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 67, "end_pos": 75, "type": "METRIC", "confidence": 0.9991527795791626}]}, {"text": "However, it is possible to compute approximate unlabeled accuracy scores by training the constituent parsers on the NP-patched () version of the data and then running the test output through just the first conversion script-that is, the modified version of converter.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.9806112051010132}]}, {"text": "1263 The results of the experiment are given in Table 3, including accuracy for individual arcs, nonprojective arcs only, and full sentence match.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 67, "end_pos": 75, "type": "METRIC", "confidence": 0.9996615648269653}]}, {"text": "Punctuation is excluded in all the result computations.", "labels": [], "entities": [{"text": "Punctuation", "start_pos": 0, "end_pos": 11, "type": "METRIC", "confidence": 0.9763240218162537}]}, {"text": "To determine whether an arc is non-projective, the following heuristic was used.", "labels": [], "entities": []}, {"text": "Traverse the sentence in a depth-first search, starting from the imaginary root node and pursuing child arcs in order of increasing absolute distance from their parent.", "labels": [], "entities": []}, {"text": "Whenever an arc being traversed is found to cross a previously traversed arc, mark it as non-projective and continue.", "labels": [], "entities": []}, {"text": "To evaluate the impact of part-of-speech tagging error, results for parsing using the gold standard partof-speech tags are also included.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 26, "end_pos": 48, "type": "TASK", "confidence": 0.6917887628078461}]}, {"text": "We also measured the speed of the parser on the various sentences in the test collection.", "labels": [], "entities": []}, {"text": "For reasonable sentence lengths, the parser scales quite well.", "labels": [], "entities": []}, {"text": "The scatterplot depicting the relation between sentence length and parsing time is presented in. for the parsers on a PC with a 2.4Ghz Q6600 processor and 8GB RAM.", "labels": [], "entities": []}, {"text": "MALTPARSER ran substantially slower than the others, perhaps due to its use of polynomial kernels, and isn't shown.", "labels": [], "entities": [{"text": "MALTPARSER", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.4711490869522095}]}, {"text": "(C-L-E -Chu-Liu-Edmonds, G&E -Goldberg and Elhadad).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Top 15 part-of-speech tag changes performed by  the conversion script.", "labels": [], "entities": []}, {"text": " Table 3: Parsing results for section 23 of the Penn Treebank (punctuation excluded). Results in parentheses were  produced using gold POS tags.  \u2020 Eisner (1996) algorithm with non-projective rewriting and second order features.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 48, "end_pos": 61, "type": "DATASET", "confidence": 0.956432580947876}]}]}