{"title": [{"text": "Experimental Support fora Categorical Compositional Distributional Model of Meaning", "labels": [], "entities": [{"text": "Categorical Compositional Distributional Model of Meaning", "start_pos": 26, "end_pos": 83, "type": "TASK", "confidence": 0.7095443407694498}]}], "abstractContent": [{"text": "Modelling compositional meaning for sentences using empirical distributional methods has been a challenge for computational linguists.", "labels": [], "entities": [{"text": "Modelling compositional meaning for sentences", "start_pos": 0, "end_pos": 45, "type": "TASK", "confidence": 0.861937654018402}]}, {"text": "We implement the abstract categorical model of Coecke et al.", "labels": [], "entities": []}, {"text": "(2010) using data from the BNC and evaluate it.", "labels": [], "entities": [{"text": "BNC", "start_pos": 27, "end_pos": 30, "type": "DATASET", "confidence": 0.9611309766769409}]}, {"text": "The implementation is based on unsupervised learning of matrices for relational words and applying them to the vectors of their arguments.", "labels": [], "entities": []}, {"text": "The evaluation is based on the word disambiguation task developed by Mitchell and Lapata (2008) for intransitive sentences, and on a similar new experiment designed for transitive sentences.", "labels": [], "entities": [{"text": "word disambiguation", "start_pos": 31, "end_pos": 50, "type": "TASK", "confidence": 0.7043934017419815}]}, {"text": "Our model matches the results of its competitors in the first experiment, and betters them in the second.", "labels": [], "entities": []}, {"text": "The general improvement in results with increase in syntactic complexity show-cases the compositional power of our model.", "labels": [], "entities": []}], "introductionContent": [{"text": "As competent language speakers, we humans can almost trivially make sense of sentences we've never seen or heard before.", "labels": [], "entities": []}, {"text": "We are naturally good at understanding ambiguous words given a context, and forming the meaning of a sentence from the meaning of its parts.", "labels": [], "entities": []}, {"text": "But while human beings seem comfortable doing this, machines fail to deliver.", "labels": [], "entities": []}, {"text": "Search engines such as Google either fallback on bag of words models-ignoring syntax and lexical relations-or exploit superficial models of lexical semantics to retrieve pages with terms related to those in the query (.", "labels": [], "entities": []}, {"text": "However, such models fail to shine when it comes to processing the semantics of phrases and sentences.", "labels": [], "entities": []}, {"text": "Discovering the process of meaning assignment in natural language is among the most challenging and foundational questions of linguistics and computer science.", "labels": [], "entities": [{"text": "Discovering the process of meaning assignment in natural language", "start_pos": 0, "end_pos": 65, "type": "TASK", "confidence": 0.8521420690748427}]}, {"text": "The findings thereof will increase our understanding of cognition and intelligence and shall assist in applications to automating language-related tasks such as document search.", "labels": [], "entities": [{"text": "document search", "start_pos": 161, "end_pos": 176, "type": "TASK", "confidence": 0.7780908346176147}]}, {"text": "Compositional type-logical approaches and distributional models of lexical semantics have provided two partial orthogonal solutions to the question.", "labels": [], "entities": []}, {"text": "Compositional formal semantic models stem from classical ideas from mathematical logic, mainly Frege's principle that the meaning of a sentence is a function of the meaning of its parts.", "labels": [], "entities": [{"text": "Compositional formal semantic", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.6798941493034363}]}, {"text": "Distributional models are more recent and can be related to Wittgenstein's later philosophy of 'meaning is use', whereby meanings of words can be determined from their context.", "labels": [], "entities": []}, {"text": "The logical models relate to well known and robust logical formalisms, hence offering a scalable theory of meaning which can be used to reason inferentially.", "labels": [], "entities": []}, {"text": "The distributional models have found their way into real world applications such as thesaurus extraction) or automated essay marking, and have connections to semantically motivated information retrieval (.", "labels": [], "entities": [{"text": "thesaurus extraction", "start_pos": 84, "end_pos": 104, "type": "TASK", "confidence": 0.7703648507595062}, {"text": "essay marking", "start_pos": 119, "end_pos": 132, "type": "TASK", "confidence": 0.6983532607555389}]}, {"text": "This two-sortedness of defining properties of meaning: 'logical form' versus 'contextual use', has left the quest for 'what is the foundational structure of meaning?'", "labels": [], "entities": []}, {"text": "even more of a challenge.", "labels": [], "entities": []}, {"text": "Recently, used high level cross-disciplinary techniques from logic, category theory, and physics to bring the above two approaches together.", "labels": [], "entities": []}, {"text": "They developed a unified mathematical framework whereby a sentence vector is by definition a function of the Kronecker product of its word vectors.", "labels": [], "entities": []}, {"text": "A concrete instantiation of this theory was exemplified on a toy hand crafted corpus by.", "labels": [], "entities": []}, {"text": "In this paper we implement it by training the model over the entire BNC.", "labels": [], "entities": [{"text": "BNC", "start_pos": 68, "end_pos": 71, "type": "DATASET", "confidence": 0.9558586478233337}]}, {"text": "The highlight of our implementation is that words with relational types, such as verbs, adjectives, and adverbs are matrices that act on their arguments.", "labels": [], "entities": []}, {"text": "We provide a general algorithm for building (or indeed learning) these matrices from the corpus.", "labels": [], "entities": []}, {"text": "The implementation is evaluated against the task provided by for disambiguating intransitive verbs, as well as a similar new experiment for transitive verbs.", "labels": [], "entities": []}, {"text": "Our model improves on the best method evaluated in and offers promising results for the transitive case, demonstrating its scalability in comparison to that of other models.", "labels": [], "entities": []}, {"text": "But we still feel there is need fora different class of experiments to showcase merits of compositionality in a statistically significant manner.", "labels": [], "entities": []}, {"text": "Our work shows that the categorical compositional distributional model of meaning permits a practical implementation and that this opens the way to the production of large scale compositional models.", "labels": [], "entities": []}], "datasetContent": [{"text": "Methodology The evaluation methodology for the second experiment was identical to that of the first, as are the scales for means and scores.", "labels": [], "entities": []}, {"text": "Here also, Spearman's \u03c1 is deemed a more rigorous way of determining how well a model tracks difference in meaning.", "labels": [], "entities": []}, {"text": "This is both because of the imprecise nature of the classification of verb pairs as HIGH or LOW; and since the objective similarity scores produced by a model that distinguishes sentences of different meaning from those of similar meaning can be renormalised in practice.", "labels": [], "entities": [{"text": "LOW", "start_pos": 92, "end_pos": 95, "type": "METRIC", "confidence": 0.6758243441581726}]}, {"text": "Therefore the delta between HIGH means and LOW mean cannot serve as a definite indication of the practical applicability (or lack thereof) of semantic models; the means are provided just to aid comparison with the results of the first experiment.", "labels": [], "entities": [{"text": "HIGH", "start_pos": 28, "end_pos": 32, "type": "METRIC", "confidence": 0.9468026161193848}, {"text": "LOW mean", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.9728977978229523}]}, {"text": "Model Parameters As in the first experiment, the lexical vectors from were used for the other models evaluated (additive, multiplicative and baseline) and for the noun vec-8 Kintsch was not evaluated as it required optimising model parameters against a held-out segment of the test set, and we could not replicate the methodology of Mitchell and Lapata tors of our categorical model.", "labels": [], "entities": []}, {"text": "Transitive verb vectors were trained as described in \u00a74 with S = N \u2297N .  In we present the comparison of the selected models.", "labels": [], "entities": []}, {"text": "Our categorical model performs significantly better than the existing second-place (Kintsch) and obtains a \u03c1 quasiidentical to the multiplicative model, indicating significant correlation with the annotator scores.", "labels": [], "entities": []}, {"text": "There is not a large difference between the mean High score and mean Low score, but the distribution in shows that our model makes a non-negligible distinction between high similarity phrases and low similarity phrases, despite the absolute scores not being different by more than a few percentiles.", "labels": [], "entities": []}, {"text": "The multiplicative model presented here is what is qualified as best in.", "labels": [], "entities": []}, {"text": "However, they also present a slightly better performing (\u03c1 = 0.19) model which is a combination of their multiplicative model and a weighted additive model.", "labels": [], "entities": []}, {"text": "The difference in \u03c1 is qualified as \"not statistically significant\" in the original paper, and furthermore the mixed model requires parametric optimisation hence was not evaluated against the entire test set.", "labels": [], "entities": []}, {"text": "For these reasons, we chose not to include it in the comparison.", "labels": [], "entities": []}, {"text": "The results for the models evaluated against the second dataset are presented in.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Sample weights for selected noun vectors.", "labels": [], "entities": []}, {"text": " Table 2: Sample semantic matrix for 'show'.", "labels": [], "entities": []}, {"text": " Table 3: Selected model means for High and Low similar- ity items and correlation coefficients with human judge- ments, first experiment (Mitchell and Lapata, 2008). p <  0.05 for each \u03c1.", "labels": [], "entities": []}, {"text": " Table 5: Selected model means for High and Low similar- ity items and correlation coefficients with human judge- ments, second experiment. p < 0.05 for each \u03c1.", "labels": [], "entities": []}]}