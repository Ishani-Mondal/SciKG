{"title": [{"text": "Active Learning with Amazon Mechanical Turk", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 21, "end_pos": 43, "type": "DATASET", "confidence": 0.9210444291432699}]}], "abstractContent": [{"text": "Supervised classification needs large amounts of annotated training data that is expensive to create.", "labels": [], "entities": [{"text": "Supervised classification", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.6585564613342285}]}, {"text": "Two approaches that reduce the cost of annotation are active learning and crowd-sourcing.", "labels": [], "entities": []}, {"text": "However, these two approaches have not been combined successfully to date.", "labels": [], "entities": []}, {"text": "We evaluate the utility of active learning in crowdsourcing on two tasks, named entity recognition and sentiment detection, and show that active learning outperforms random selection of annotation examples in a noisy crowd-sourcing scenario.", "labels": [], "entities": [{"text": "entity recognition", "start_pos": 80, "end_pos": 98, "type": "TASK", "confidence": 0.7136777192354202}, {"text": "sentiment detection", "start_pos": 103, "end_pos": 122, "type": "TASK", "confidence": 0.889320433139801}]}], "introductionContent": [{"text": "Supervised classification is the predominant technique fora large number of natural language processing (NLP) tasks.", "labels": [], "entities": [{"text": "Supervised classification", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.9209999442100525}, {"text": "natural language processing (NLP) tasks", "start_pos": 76, "end_pos": 115, "type": "TASK", "confidence": 0.757721619946616}]}, {"text": "The large amount of labeled training data that supervised classification relies on is time-consuming and expensive to create, especially when experts perform the data annotation.", "labels": [], "entities": []}, {"text": "Recently, crowdsourcing services like Amazon Mechanical Turk (MTurk) have become available as an alternative that offers acquisition of non-expert annotations at low cost.", "labels": [], "entities": []}, {"text": "MTurk is a software service that outsources small annotation tasks -called HITs -to a large group of freelance workers.", "labels": [], "entities": [{"text": "MTurk", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9204085469245911}]}, {"text": "The cost of MTurk annotation is low, but a consequence of using non-expert annotators is much lower annotation quality.", "labels": [], "entities": [{"text": "MTurk annotation", "start_pos": 12, "end_pos": 28, "type": "TASK", "confidence": 0.8662095665931702}]}, {"text": "This requires strategies for quality control of the annotations.", "labels": [], "entities": []}, {"text": "Another promising approach to the data acquisition bottleneck for supervised learning is active learning (AL).", "labels": [], "entities": [{"text": "active learning (AL)", "start_pos": 89, "end_pos": 109, "type": "TASK", "confidence": 0.6450265765190124}]}, {"text": "AL reduces annotation effort by setting up an annotation loop where, starting from a small seed set, only the maximally informative examples are chosen for annotation.", "labels": [], "entities": []}, {"text": "With these annotated examples, the classifier is then retrained to again select more informative examples for further annotation.", "labels": [], "entities": []}, {"text": "In general, AL needs a lot fewer annotations to achieve a desired performance level than random sampling.", "labels": [], "entities": [{"text": "AL", "start_pos": 12, "end_pos": 14, "type": "TASK", "confidence": 0.9496174454689026}]}, {"text": "AL has been successfully applied to a number of NLP tasks such as part-of-speech tagging (, parsing (), text classification (), sentiment detection (, and named entity recognition (NER).", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 66, "end_pos": 88, "type": "TASK", "confidence": 0.7266639471054077}, {"text": "parsing", "start_pos": 92, "end_pos": 99, "type": "TASK", "confidence": 0.962833046913147}, {"text": "text classification", "start_pos": 104, "end_pos": 123, "type": "TASK", "confidence": 0.7773471176624298}, {"text": "sentiment detection", "start_pos": 128, "end_pos": 147, "type": "TASK", "confidence": 0.9483494460582733}, {"text": "named entity recognition (NER)", "start_pos": 155, "end_pos": 185, "type": "TASK", "confidence": 0.7220534483591715}]}, {"text": "Until recently, most AL studies focused on simulating the annotation process by using already available gold standard data.", "labels": [], "entities": []}, {"text": "In reality, however, human annotators make mistakes, leading to noise in the annotations.", "labels": [], "entities": []}, {"text": "For this reason, some authors have questioned the applicability of AL to noisy annotation scenarios such as MTurk (.", "labels": [], "entities": [{"text": "MTurk", "start_pos": 108, "end_pos": 113, "type": "DATASET", "confidence": 0.8743764162063599}]}, {"text": "AL and crowdsourcing are complementary approaches: AL reduces the number of annotations used while crowdsourcing reduces the cost per annotation.", "labels": [], "entities": [{"text": "AL", "start_pos": 51, "end_pos": 53, "type": "METRIC", "confidence": 0.9415627717971802}]}, {"text": "Combined, the two approaches could substantially lower the cost of creating training sets.", "labels": [], "entities": []}, {"text": "Our main contribution in this paper is that we show for the first time that AL is significantly better than randomly selected annotation examples in areal crowdsourcing annotation scenario.", "labels": [], "entities": [{"text": "AL", "start_pos": 76, "end_pos": 78, "type": "METRIC", "confidence": 0.9886406660079956}]}, {"text": "Our experiments directly address two tasks, named entity recognition and sentiment detection, but our 1546 evidence suggests that AL is of general benefit in crowdsourcing.", "labels": [], "entities": [{"text": "entity recognition", "start_pos": 50, "end_pos": 68, "type": "TASK", "confidence": 0.7231995314359665}, {"text": "sentiment detection", "start_pos": 73, "end_pos": 92, "type": "TASK", "confidence": 0.934342235326767}]}, {"text": "We also show that the effectiveness of MTurk annotation with AL can be further enhanced by using two techniques that increase label quality: adaptive voting and fragment recovery.", "labels": [], "entities": [{"text": "MTurk annotation", "start_pos": 39, "end_pos": 55, "type": "TASK", "confidence": 0.8957799077033997}, {"text": "fragment recovery", "start_pos": 161, "end_pos": 178, "type": "TASK", "confidence": 0.7959094941616058}]}], "datasetContent": [{"text": "In our NER experiments, we have workers reannotate the English corpus of the CoNLL-2003 NER shared task.", "labels": [], "entities": [{"text": "CoNLL-2003 NER shared task", "start_pos": 77, "end_pos": 103, "type": "DATASET", "confidence": 0.8184423744678497}]}, {"text": "We chose this corpus to be able to compare crowdsourced annotations with gold standard It can take awhile in this scheme for annotators to agree on a final annotation fora sentence.", "labels": [], "entities": []}, {"text": "We make tentative labels of a sentence available to the classifier immediately and replace them with the final labels once voting is completed. annotations.", "labels": [], "entities": []}, {"text": "A HIT is one sentence and is offered fora base payment of $0.01.", "labels": [], "entities": [{"text": "HIT", "start_pos": 2, "end_pos": 5, "type": "METRIC", "confidence": 0.6468296647071838}]}, {"text": "We filtered out answers that contained unannotated tokens or were obvious spam (e.g., all tokens labeled as MISC).", "labels": [], "entities": []}, {"text": "For testing NER performance, we used a system based on conditional random fields with standard named entity features including the token itself, orthographic features like the occurrence of capitalization or special characters and context information about the tokens to the left/right of the current token.", "labels": [], "entities": [{"text": "NER", "start_pos": 12, "end_pos": 15, "type": "TASK", "confidence": 0.963494062423706}]}, {"text": "The sentiment detection task was modeled after a well-known document analysis setup for sentiment classification, introduced by.", "labels": [], "entities": [{"text": "sentiment detection task", "start_pos": 4, "end_pos": 28, "type": "TASK", "confidence": 0.9386488000551859}, {"text": "sentiment classification", "start_pos": 88, "end_pos": 112, "type": "TASK", "confidence": 0.892890989780426}]}, {"text": "We use their corpus of 1000 positive and 1000 negative movie reviews and the Stanford maximum entropy classifier to predict the sentiment label of each document d from a unigram representation of d.", "labels": [], "entities": []}, {"text": "We randomly split this corpus into a test set of 500 reviews and an active learning pool of 1500 reviews.", "labels": [], "entities": []}, {"text": "Each HIT consists of one document, valued at $0.01.", "labels": [], "entities": []}, {"text": "We compare random sampling (RS) and AL in combination with the proposed voting and fragment strategies with different parameters.", "labels": [], "entities": [{"text": "AL", "start_pos": 36, "end_pos": 38, "type": "METRIC", "confidence": 0.998082160949707}]}, {"text": "We want to avoid rerunning experiments on MTurk over and over again, but on the other hand, we believe that using synthetic data for simulations is problematic because it is difficult to generate synthetic data with a realistic model of annotator errors.", "labels": [], "entities": [{"text": "MTurk", "start_pos": 42, "end_pos": 47, "type": "DATASET", "confidence": 0.8629492521286011}]}, {"text": "Thus, we logged a play-by-play record of the annotator interactions and labels.", "labels": [], "entities": []}, {"text": "With this recording, we can then rerun strategies with different parameters.", "labels": [], "entities": []}, {"text": "We chose voting with at most d = 5 repetitions asour main reannotation strategy for both random and active sampling for NER annotation.", "labels": [], "entities": [{"text": "NER annotation", "start_pos": 120, "end_pos": 134, "type": "TASK", "confidence": 0.859098881483078}]}, {"text": "We use simple majority voting (\u03b1 = .5) for NER.", "labels": [], "entities": [{"text": "NER", "start_pos": 43, "end_pos": 46, "type": "TASK", "confidence": 0.748042643070221}]}, {"text": "For sentiment, we set d = 4 and minimum agreement \u03b1 = .75 because the number of labels is smaller (2 vs. 5) and so random agreement is more likely for sentiment.", "labels": [], "entities": [{"text": "minimum agreement \u03b1", "start_pos": 32, "end_pos": 51, "type": "METRIC", "confidence": 0.8312540253003439}]}, {"text": "To get results for 3-voting NER, we take the recording and discard 5-voting votes not needed in 3-voting.", "labels": [], "entities": [{"text": "NER", "start_pos": 28, "end_pos": 31, "type": "TASK", "confidence": 0.5202615857124329}]}, {"text": "This will result in roughly the same number of annotated sentences, but at a lower cost.", "labels": [], "entities": []}, {"text": "This simulation of 3-voting is not exactly what would have happened on MTurk (e.g., the final vote on a sentence might be different, which then influences AL example selection), but we will assume that differences are rare and simulated and actual results are similar.", "labels": [], "entities": [{"text": "MTurk", "start_pos": 71, "end_pos": 76, "type": "DATASET", "confidence": 0.9192835688591003}]}, {"text": "The same considerations apply to single votes and to the sentiment experiments.", "labels": [], "entities": []}, {"text": "We always compare two strategies for the same annotation budget.", "labels": [], "entities": []}, {"text": "For example, the number of training sentences in differ in the two relevant columns, but all strategies compared use exactly the same annotation budget.", "labels": [], "entities": []}, {"text": "For the single annotation strategy, each interaction record contained only about 40% usable annotations, the rest were repeats.", "labels": [], "entities": []}, {"text": "A comparison with the single annotation strategy over approx. 2000 sentences or 450 documents would not have been meaningful; therefore we chose to run an extra experiment with the single annotation strategy to match this up with the budgets of the voting strategies.", "labels": [], "entities": []}, {"text": "The results are presented in two separate columns of Table 1 (budgets 6931 and 1756).", "labels": [], "entities": [{"text": "budgets 6931", "start_pos": 62, "end_pos": 74, "type": "DATASET", "confidence": 0.7500480711460114}]}], "tableCaptions": [{"text": " Table 1: For NER, active learning consistently beats random sampling on MTurk. NER F 1 evaluated on  CoNLL test set A. #train = number of sentences in training set, S = single, 3-v = 3-voting, 5/4-voting = 5- and 4-voting for NER and sentiment resp., +f = using fragments; sentiment budget 1130 for run 1, sentiment  budget 1756 averaged over 2 runs.", "labels": [], "entities": [{"text": "NER", "start_pos": 14, "end_pos": 17, "type": "TASK", "confidence": 0.9221832752227783}, {"text": "MTurk", "start_pos": 73, "end_pos": 78, "type": "DATASET", "confidence": 0.8834757804870605}, {"text": "CoNLL test set A", "start_pos": 102, "end_pos": 118, "type": "DATASET", "confidence": 0.9484028220176697}, {"text": "sentiment resp.", "start_pos": 235, "end_pos": 250, "type": "TASK", "confidence": 0.7260935008525848}]}]}