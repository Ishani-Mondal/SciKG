{"title": [{"text": "Semantic Topic Models: Combining Word Distributional Statistics and Dictionary Definitions", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper, we propose a novel topic model based on incorporating dictionary definitions.", "labels": [], "entities": []}, {"text": "Traditional topic models treat words as surface strings without assuming predefined knowledge about word meaning.", "labels": [], "entities": []}, {"text": "They infer topics only by observing surface word co-occurrence.", "labels": [], "entities": []}, {"text": "However, the co-occurred words may not be semantically related in a manner that is relevant for topic coherence.", "labels": [], "entities": []}, {"text": "Exploiting dictionary definitions explicitly in our model yields a better understanding of word semantics leading to better text modeling.", "labels": [], "entities": [{"text": "text modeling", "start_pos": 124, "end_pos": 137, "type": "TASK", "confidence": 0.7261469960212708}]}, {"text": "We exploit WordNet as a lexical resource for sense definitions.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 11, "end_pos": 18, "type": "DATASET", "confidence": 0.9647898077964783}, {"text": "sense definitions", "start_pos": 45, "end_pos": 62, "type": "TASK", "confidence": 0.8259463906288147}]}, {"text": "We show that explicitly mod-eling word definitions helps improve performance significantly over the baseline fora text categorization task.", "labels": [], "entities": []}], "introductionContent": [{"text": "Latent Dirichlet Allocation (LDA) () serves as a data-driven framework in modeling text corpora.", "labels": [], "entities": [{"text": "Latent Dirichlet Allocation (LDA)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.6574734548727671}]}, {"text": "The statistical model allows variable extensions to integrate linguistic features such as syntax (, and has been applied in many areas.", "labels": [], "entities": []}, {"text": "In LDA, there are two factors which determine the topic of a word: the topic distribution of the document, and the probability of a topic to emit this word.", "labels": [], "entities": []}, {"text": "This information is learned in an unsupervised manner to maximize the likelihood of the corpus.", "labels": [], "entities": []}, {"text": "However, this data-driven approach has some limitations.", "labels": [], "entities": []}, {"text": "If a word is not observed frequently enough in the corpus, then it is likely to be assigned the dominant topic in this document.", "labels": [], "entities": []}, {"text": "For example, the word grease (a thick fatty oil) in apolitical domain document should be assigned the topic chemicals.", "labels": [], "entities": []}, {"text": "However, since it is an infrequent word, LDA cannot learn its correct semantics from the observed distribution, the LDA model will assign it the dominant document topic politics.", "labels": [], "entities": []}, {"text": "If we lookup the semantics of the word grease in a dictionary, we will not find any of its meanings indicating the politics topic, yet there is ample evidence for the chemical topic.", "labels": [], "entities": []}, {"text": "Accordingly, we hypothesize that if we know the semantics of words in advance, we can get a better indication of their topics.", "labels": [], "entities": []}, {"text": "Therefore, in this paper, we test our hypothesis by exploring the integration of word semantics explicitly in the topic modeling framework.", "labels": [], "entities": []}, {"text": "In order to incorporate word semantics from dictionaries, we recognize the need to model sense-topic distribution rather than word-topic distribution, since dictionaries are constructed at the sense level.", "labels": [], "entities": []}, {"text": "We use WordNet as our lexical resource of choice.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 7, "end_pos": 14, "type": "DATASET", "confidence": 0.968349814414978}]}, {"text": "The notion of a sense in WordNet goes beyond atypical word sense in a traditional dictionary since a WordNet sense links senses of different words that have similar meanings.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 25, "end_pos": 32, "type": "DATASET", "confidence": 0.9575951099395752}]}, {"text": "Accordingly, the sense for the first verbal entry for buy and for purchase will have the same sense id (and same definition) in WordNet, while they could have different meaning definitions in a traditional dictionary such as the Merriam Webster Dictionary or LDOCE.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 128, "end_pos": 135, "type": "DATASET", "confidence": 0.9574474692344666}, {"text": "Merriam Webster Dictionary or LDOCE", "start_pos": 229, "end_pos": 264, "type": "DATASET", "confidence": 0.8829892992973327}]}, {"text": "In our model, a topic will first emit a WordNet sense, then the sense will generate a word.", "labels": [], "entities": []}, {"text": "This is inspired by the intuition that words are instantiations of concepts.", "labels": [], "entities": []}, {"text": "The paper is organized as follows: In Sections 2 and 3, we describe our models based on WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 88, "end_pos": 95, "type": "DATASET", "confidence": 0.9660427570343018}]}, {"text": "In Section 4, experiment results on text categorization are presented.", "labels": [], "entities": [{"text": "text categorization", "start_pos": 36, "end_pos": 55, "type": "TASK", "confidence": 0.8121697008609772}]}, {"text": "Moreover, we analyze both qualitatively and quantitatively the contribution of modeling definitions (by teasing out the contribution of explicit sense modeling in a word sense disambiguation task).", "labels": [], "entities": [{"text": "word sense disambiguation task", "start_pos": 165, "end_pos": 195, "type": "TASK", "confidence": 0.696184866130352}]}, {"text": "Related work is introduced in Section 5.", "labels": [], "entities": []}, {"text": "We conclude in Section 6 by discussing some possible future directions.", "labels": [], "entities": []}], "datasetContent": [{"text": "Data: We experiment with several datasets, namely, the Brown Corpus (Brown), New York Times (NYT) from the American National Corpus, Reuters (R20) and WordNet definitions.", "labels": [], "entities": [{"text": "Brown Corpus (Brown)", "start_pos": 55, "end_pos": 75, "type": "DATASET", "confidence": 0.9558879852294921}, {"text": "New York Times (NYT)", "start_pos": 77, "end_pos": 97, "type": "DATASET", "confidence": 0.7229454517364502}, {"text": "American National Corpus, Reuters (R20)", "start_pos": 107, "end_pos": 146, "type": "DATASET", "confidence": 0.7556059211492538}, {"text": "WordNet definitions", "start_pos": 151, "end_pos": 170, "type": "DATASET", "confidence": 0.8941724896430969}]}, {"text": "Ina preprocessing step, we remove all the non-content words whose part of speech tags are not one of the following set {noun, adjective, adverb, verb}.", "labels": [], "entities": []}, {"text": "Moreover, words that do not have a valid lemma in WordNet are removed.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 50, "end_pos": 57, "type": "DATASET", "confidence": 0.9685034155845642}]}, {"text": "For WordNet definitions, we remove stop words hence focusing on relevant content words.", "labels": [], "entities": []}, {"text": "Corpora statistics after each step of preprocessing is presented in.", "labels": [], "entities": []}, {"text": "The column WN token lists the number of word#pos tokens after preprocessing.", "labels": [], "entities": []}, {"text": "Note that now we treat word#pos as a word token.", "labels": [], "entities": []}, {"text": "The column word types shows corresponding word#pos types, and the total number of possible sense types is listed in column sense types.", "labels": [], "entities": []}, {"text": "The DOCs size for WordNet is the total number of senses defined in WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 18, "end_pos": 25, "type": "DATASET", "confidence": 0.948283851146698}, {"text": "WordNet", "start_pos": 67, "end_pos": 74, "type": "DATASET", "confidence": 0.9543918371200562}]}, {"text": "Experiments: We design two tasks to test our models: (1) text categorization task for evaluating the quality of values of topic nodes, and (2) a WSD task for evaluating the quality of the values of the sense nodes, mainly as a diagnostic tool targeting the specific aspect of sense definitions incorporation and distinguish that component's contribution to text categorization performance.", "labels": [], "entities": [{"text": "sense definitions incorporation", "start_pos": 276, "end_pos": 307, "type": "TASK", "confidence": 0.7140445113182068}]}, {"text": "We compare the performance of four topic models.", "labels": [], "entities": []}, {"text": "(a) LDA: the traditional topic model proposed in ( except that it uses Gibbs Sampling for inference.", "labels": [], "entities": []}, {"text": "(b) LDA+def: is LDA with sense definitions.", "labels": [], "entities": []}, {"text": "However they are not explicitly modeled; rather they are treated as documents and used as augmented data.", "labels": [], "entities": []}, {"text": "(c) STM0: the topic model with an additional explicit sense node in the model, but we do not model the sense definitions.", "labels": [], "entities": []}, {"text": "And finally (d) STMn is the full model with definitions explicitly modeled.", "labels": [], "entities": []}, {"text": "In this setting n is the \u03b3 value.", "labels": [], "entities": []}, {"text": "We experiment with different \u03b3 values in the STM models, and investigate the semantic scope of words/senses by choosing different window size . We report mean and standard deviation based on 10 runs.", "labels": [], "entities": []}, {"text": "It is worth noting that a larger window size suggests documents have larger impact on the model (\u03c6, \u03b7) than definitions, since each document word has copies.", "labels": [], "entities": []}, {"text": "This is not a desirable property when we want to investigate the weight of definitions by choosing different \u03b3 values.", "labels": [], "entities": []}, {"text": "Accordingly, we only use z jj , s jj , w jj to estimate \u03c6, \u03b7, so that the impact of documents is fixed.", "labels": [], "entities": [{"text": "\u03c6", "start_pos": 56, "end_pos": 57, "type": "METRIC", "confidence": 0.9809083938598633}]}, {"text": "This makes more sense, in that after the approximation in section 3.1, there is no need to use {z ij , s ij , | i = j} (they have the same values as z jj , s jj ).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Classification results on 3 datasets using hyperparameters tuned on Brown.", "labels": [], "entities": [{"text": "Classification", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.9764996767044067}]}, {"text": " Table 3: Statistics of SemCor per POS", "labels": [], "entities": [{"text": "POS", "start_pos": 35, "end_pos": 38, "type": "DATASET", "confidence": 0.34911176562309265}]}, {"text": " Table  4. This is understandable since topic information  content is mostly borne by nouns and adjectives,  while adverbs and verbs tend to be less informa- tive about topics (e.g., even, indicate, take), and  used more across different domain documents.  Hence topic models are weaker in their ability  to identify clear cues for senses for verbs and  adverbs. In support of our hypothesis about the  POS distribution, we compute the average TF-IDF", "labels": [], "entities": [{"text": "TF-IDF", "start_pos": 444, "end_pos": 450, "type": "METRIC", "confidence": 0.9101294875144958}]}, {"text": " Table 4: Disambiguation results per POS on polysemous words.", "labels": [], "entities": []}]}