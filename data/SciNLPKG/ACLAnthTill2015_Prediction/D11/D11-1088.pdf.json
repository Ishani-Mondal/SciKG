{"title": [{"text": "A Non-negative Matrix Factorization Based Approach for Active Dual Supervision from Document and Word Labels", "labels": [], "entities": [{"text": "Active Dual Supervision from Document and Word Labels", "start_pos": 55, "end_pos": 108, "type": "TASK", "confidence": 0.6964269392192364}]}], "abstractContent": [{"text": "In active dual supervision, not only informative examples but also features are selected for labeling to build a high quality classifier with low cost.", "labels": [], "entities": []}, {"text": "However, how to measure the infor-mativeness for both examples and feature on the same scale has not been well solved.", "labels": [], "entities": [{"text": "infor-mativeness", "start_pos": 28, "end_pos": 44, "type": "METRIC", "confidence": 0.8272333741188049}]}, {"text": "In this paper, we propose a non-negative matrix factorization based approach to address this issue.", "labels": [], "entities": []}, {"text": "We first extend the matrix factorization framework to explicitly model the corresponding relationships between feature classes and examples classes.", "labels": [], "entities": []}, {"text": "Then by making use of the reconstruction error, we propose a unified scheme to determine which feature or example a classifier is most likely to benefit from having labeled.", "labels": [], "entities": []}, {"text": "Empirical results demonstrate the effectiveness of our proposed methods.", "labels": [], "entities": []}], "introductionContent": [{"text": "An ideal active dual supervision scheme should be able to evaluate the value of acquiring labels for documents and words on the same scale.", "labels": [], "entities": []}, {"text": "In the initial study of dual active supervision, different scores are used for documents and words (e.g. uncertainty for documents and certainty for words), and thus they are not on the same scale ( . Recently, the framework of Expected Utility (Estimated Risk Minimization) is proposed in (.", "labels": [], "entities": [{"text": "certainty", "start_pos": 135, "end_pos": 144, "type": "METRIC", "confidence": 0.9644625782966614}]}, {"text": "At each step of the framework, the next word or document selected for labeling is the one that will result in the highest estimated improvement in classifier performance as defined as: where K is the class number, P (q j = ck ) indicates the probability that q j , j-th query (a word or document), belongs to the k-th class, and the U (q j = ck ) indicates the utility that q j belongs to the k-th class.", "labels": [], "entities": []}, {"text": "However, the choice of the utility measure is still a challenge.", "labels": [], "entities": []}], "datasetContent": [{"text": "Three popular binary text classification datasets are used in the experiments: ibm-mac (1937 examples), baseball-hockey (1988 examples) and med-space (1972 examples) datasets.", "labels": [], "entities": []}, {"text": "All of them are drawn from the 20-newsgroups text collection 1 where the task is to assign messages into the newsgroup in which they appeared.", "labels": [], "entities": []}, {"text": "Top 1500 frequent words in each dataset are used as features in the binary vector representation.", "labels": [], "entities": []}, {"text": "These datasets have labels for all the documents.", "labels": [], "entities": []}, {"text": "For a document query, the oracle returns its label.", "labels": [], "entities": []}, {"text": "We construct the word oracle in the same manner as in ( ): first compute the information gain of words with respect to the known true class labels in the training splits of a dataset, and then the top 100 words as ranked by information gain are assigned the label which is the class in which the word appears more frequently.", "labels": [], "entities": []}, {"text": "To those words with labels, the word oracle returns its label; otherwise, the oracle returns a \"don't know\" response (no word label is obtained for learning, but the word is excluded from the following query selection).", "labels": [], "entities": []}, {"text": "Results are averaged over 10 random trainingtest splits.", "labels": [], "entities": []}, {"text": "For each split, 30% examples are used for testing.", "labels": [], "entities": []}, {"text": "All methods are initialized by a random choice of 10 document labels and 10 word labels.", "labels": [], "entities": []}, {"text": "For simplicity, we follow the widely used cost model) where features are roughly 5 times cheaper to label than examples, so we assume the cost is 1 fora word query and is 5 fora document query.", "labels": [], "entities": []}, {"text": "We set \u03b1 = \u03b2 = 5, \u03b3 = 1 for all the following experiments 2 . 2 We do not perform fine tuning on the parameters since the main objective of the paper is to demonstrate the effectiveness of matrix factorization based methods for dual active supervision.", "labels": [], "entities": []}, {"text": "A vigorous investigation on the parameter choices is our further work.", "labels": [], "entities": []}, {"text": "Effect of Constraints on S in Constrained Tri-NMF demonstrates the effectiveness of dual supervision with explicit class alignment via Tri-NMF as described in Section 4.", "labels": [], "entities": []}, {"text": "When there are enough labeled documents and words, the constraints on S have a relative small impact on the performance of dual supervision.", "labels": [], "entities": []}, {"text": "However, in the beginning phase of active learning, the labeled dataset can be small (such as 10 labeled documents and 10 labeled words).", "labels": [], "entities": []}, {"text": "In this case, without the constraint of S, the matrix factorization may generate incorrect class alignment, thus lead to almost random classification results (around 50% accuracy), as shown in, and further make unreasonable the following evaluation of queries.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 170, "end_pos": 178, "type": "METRIC", "confidence": 0.9988592863082886}]}, {"text": "compares our proposed unified scheme (denoted as Expected-reconstruction-error) with the following baselines using Tri-NMF as the classifier for dual supervision: (1).", "labels": [], "entities": []}, {"text": "Interleaved-uncertainty which first selects feature query by certainty and sample query by uncertainty and then combines the two types of queries using an interleaving scheme.", "labels": [], "entities": []}, {"text": "The interleaving probability (probability to select the query as a document) is set as 0.2, 0.4, 0.6 and 0.8.", "labels": [], "entities": []}, {"text": "Expected-log-gain which selects feature and sample query by maximizing the expected log gain.", "labels": [], "entities": []}, {"text": "Expected-reconstruction-error outperforms interleaving schemes with all the different interleaving probability values with which we experimented.", "labels": [], "entities": []}, {"text": "It also has a better performance than Expected-loggain.", "labels": [], "entities": []}, {"text": "Although log gain is a finer-grained utility measure of classifier performance than accuracy and has a good performance in the setting with a large set of starting labeled documents (e.g., 100 documents), it is not reliable especially in the setting with a small set of labeled data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 84, "end_pos": 92, "type": "METRIC", "confidence": 0.9991229176521301}]}, {"text": "Different from the Expected-loggain, Expected-reconstruction-error estimates the utility using the matrix reconstruction error, making use of information of all documents and words, including those unlabeled.", "labels": [], "entities": []}], "tableCaptions": []}