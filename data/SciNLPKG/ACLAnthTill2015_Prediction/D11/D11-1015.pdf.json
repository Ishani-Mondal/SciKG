{"title": [{"text": "Unsupervised Discovery of Discourse Relations for Eliminating Intra-sentence Polarity Ambiguities", "labels": [], "entities": [{"text": "Eliminating Intra-sentence Polarity Ambiguities", "start_pos": 50, "end_pos": 97, "type": "TASK", "confidence": 0.8234720379114151}]}], "abstractContent": [{"text": "Polarity classification of opinionated sentences with both positive and negative sentiments 1 is a key challenge in sentiment analysis.", "labels": [], "entities": [{"text": "Polarity classification", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.6495638489723206}, {"text": "sentiment analysis", "start_pos": 116, "end_pos": 134, "type": "TASK", "confidence": 0.9561045467853546}]}, {"text": "This paper presents a novel unsuper-vised method for discovering intra-sentence level discourse relations for eliminating polarity ambiguities.", "labels": [], "entities": []}, {"text": "Firstly, a discourse scheme with discourse constraints on polarity was defined empirically based on Rhetorical Structure Theory (RST).", "labels": [], "entities": [{"text": "Rhetorical Structure Theory (RST)", "start_pos": 100, "end_pos": 133, "type": "TASK", "confidence": 0.7121711273988088}]}, {"text": "Then, a small set of cue-phrase-based patterns were utilized to collect a large number of discourse instances which were later converted to semantic sequential representations (SSRs).", "labels": [], "entities": []}, {"text": "Finally, an unsuper-vised method was adopted to generate, weigh and filter new SSRs without cue phrases for recognizing discourse relations.", "labels": [], "entities": []}, {"text": "Experimental results showed that the proposed methods not only effectively recognized the defined discourse relations but also achieved significant improvement by integrating discourse information in sentence-level polarity classification .", "labels": [], "entities": [{"text": "sentence-level polarity classification", "start_pos": 200, "end_pos": 238, "type": "TASK", "confidence": 0.704293837149938}]}], "introductionContent": [{"text": "As an important task of sentiment analysis, polarity classification is critically affected by discourse structure).", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 24, "end_pos": 42, "type": "TASK", "confidence": 0.9392014145851135}, {"text": "polarity classification", "start_pos": 44, "end_pos": 67, "type": "TASK", "confidence": 0.7351580560207367}]}, {"text": "Previous research developed discourse schema) ( and proved that the utilization of discourse relations could improve the performance of polarity classification on dialogues ().", "labels": [], "entities": []}, {"text": "However, cur-1 Defined as ambiguous sentences in this paper rent state-of-the-art methods for sentence-level polarity classification are facing difficulties in ascertaining the polarity of some sentences.", "labels": [], "entities": [{"text": "sentence-level polarity classification", "start_pos": 94, "end_pos": 132, "type": "TASK", "confidence": 0.7032345235347748}]}, {"text": "For example: Example (a) is a positive sentence holding a Contrast relation between first two segments and a Cause relation between last two segments.", "labels": [], "entities": [{"text": "Contrast", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.9723188281059265}]}, {"text": "The polarity of \"criticized\", \"hated\" and \"corrupted\" are recognized as negative expressions while \"loved\" is recognized as a positive expression.", "labels": [], "entities": []}, {"text": "Example (a) is difficult for existing polarity classification methods for two reasons: (1) the number of positive expressions is less than negative expressions; (2) the importance of each sentiment expression is unknown.", "labels": [], "entities": [{"text": "polarity classification", "start_pos": 38, "end_pos": 61, "type": "TASK", "confidence": 0.7065329551696777}]}, {"text": "However, consider, if we know that the polarity of the first two segments holding a Contrast relation is determined by the nucleus () segment and the polarity of the last two segments holding a Cause relation is also determined by the nucleus segment, the polarity of the sentence will be determined by the polarity of \"[he...population]\".", "labels": [], "entities": []}, {"text": "Thus, the polarity of Example (a) is positive.", "labels": [], "entities": [{"text": "Example", "start_pos": 22, "end_pos": 29, "type": "DATASET", "confidence": 0.8725247383117676}]}, {"text": "Statistics showed that 43% of the opinionated sentences in NTCIR 2 MOAT (Multilingual Opinion Analysis Task) Chinese corpus are ambiguous.", "labels": [], "entities": [{"text": "NTCIR 2 MOAT (Multilingual Opinion Analysis Task) Chinese corpus", "start_pos": 59, "end_pos": 123, "type": "DATASET", "confidence": 0.7470817457545887}]}, {"text": "Existing sentence-level polarity classification methods ignoring discourse structure often give wrong results for these sentences.", "labels": [], "entities": [{"text": "sentence-level polarity classification", "start_pos": 9, "end_pos": 47, "type": "TASK", "confidence": 0.7104663650194804}]}, {"text": "We implemented state-of-the- art method () in NTCIR-8 Chinese MOAT as the baseline polarity classifier (BPC) in this paper.", "labels": [], "entities": [{"text": "NTCIR-8 Chinese MOAT", "start_pos": 46, "end_pos": 66, "type": "DATASET", "confidence": 0.8900315960248312}, {"text": "baseline polarity classifier (BPC)", "start_pos": 74, "end_pos": 108, "type": "METRIC", "confidence": 0.6615588068962097}]}, {"text": "Error analysis of BPC showed that 49% errors came from ambiguous sentences.", "labels": [], "entities": [{"text": "Error", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.959642767906189}, {"text": "BPC", "start_pos": 18, "end_pos": 21, "type": "DATASET", "confidence": 0.5627139806747437}, {"text": "errors", "start_pos": 38, "end_pos": 44, "type": "METRIC", "confidence": 0.9584523439407349}]}, {"text": "In this paper, we focused on the automation of recognizing intra-sentence level discourse relations for polarity classification.", "labels": [], "entities": [{"text": "polarity classification", "start_pos": 104, "end_pos": 127, "type": "TASK", "confidence": 0.7413639426231384}]}, {"text": "Based on the previous work of Rhetorical Structure Theory (RST) (, a discourse scheme with discourse constraints on polarity was defined empirically (see Section 3).", "labels": [], "entities": [{"text": "Rhetorical Structure Theory (RST)", "start_pos": 30, "end_pos": 63, "type": "TASK", "confidence": 0.8240222831567129}]}, {"text": "The scheme contains 5 relations: Contrast, Condition, Continuation, Cause and Purpose.", "labels": [], "entities": [{"text": "Continuation", "start_pos": 54, "end_pos": 66, "type": "METRIC", "confidence": 0.9477800726890564}]}, {"text": "From a raw corpus, a small set of cuephrase-based patterns were used to collect discourse instances.", "labels": [], "entities": []}, {"text": "These instances were then converted to semantic sequential representations (SSRs).", "labels": [], "entities": []}, {"text": "Finally, an unsupervised SSR learner was adopted to generate, weigh and filter high quality new SSRs without cue phrases.", "labels": [], "entities": []}, {"text": "Experimental results showed that the proposed methods could effectively recognize the defined discourse relations and achieve significant improvement in sentence-level polarity classification comparing to BPC.", "labels": [], "entities": [{"text": "sentence-level polarity classification", "start_pos": 153, "end_pos": 191, "type": "TASK", "confidence": 0.7219654421011606}]}, {"text": "The remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 introduces the related work.", "labels": [], "entities": []}, {"text": "Section 3 presents the discourse scheme with discourse constraints on polarity.", "labels": [], "entities": []}, {"text": "Section 4 gives the detail of proposed method.", "labels": [], "entities": []}, {"text": "Experimental results are reported and discussed in Section 5 and Section 6 concludes this paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "Refer to, the performance of cSSR was significantly affected by minconf . Note that we performed the tuning process of minconf on different development data (1/4 instances randomly selected from NTC-7) and showed the average performance.", "labels": [], "entities": [{"text": "NTC-7", "start_pos": 195, "end_pos": 200, "type": "DATASET", "confidence": 0.9367930293083191}]}, {"text": "cSSR became Baseline when minconf = 0.", "labels": [], "entities": []}, {"text": "A significant drop of precision was observed when minconf was less than \u22122.5.", "labels": [], "entities": [{"text": "precision", "start_pos": 22, "end_pos": 31, "type": "METRIC", "confidence": 0.9996976852416992}, {"text": "minconf", "start_pos": 50, "end_pos": 57, "type": "METRIC", "confidence": 0.9339247941970825}]}, {"text": "The recall remained around 0.495 when minconf \u2264 \u22124.0.", "labels": [], "entities": [{"text": "recall", "start_pos": 4, "end_pos": 10, "type": "METRIC", "confidence": 0.9959724545478821}, {"text": "minconf", "start_pos": 38, "end_pos": 45, "type": "METRIC", "confidence": 0.9282621741294861}]}, {"text": "The best performance was observed when minconf =\u22123.5.", "labels": [], "entities": [{"text": "minconf", "start_pos": 39, "end_pos": 46, "type": "METRIC", "confidence": 0.9901941418647766}]}, {"text": "As a result, \u22123.5 was utilized as the threshold value for cSSR in the following experiments.", "labels": [], "entities": []}, {"text": "presented the experimental results for discourse relation classification.", "labels": [], "entities": [{"text": "discourse relation classification", "start_pos": 39, "end_pos": 72, "type": "TASK", "confidence": 0.8133574724197388}]}, {"text": "it showed that: (1) Cue-phrase-based patterns could find only limited number of discourse relations  recall) with a very high precision (96.17% of average precision).", "labels": [], "entities": [{"text": "precision", "start_pos": 126, "end_pos": 135, "type": "METRIC", "confidence": 0.998291552066803}, {"text": "precision", "start_pos": 155, "end_pos": 164, "type": "METRIC", "confidence": 0.9874229431152344}]}, {"text": "This is a proof of assumption given in Section 4.", "labels": [], "entities": []}, {"text": "On the other side, M&E which only considered word pairs between two segments of discourse instances got a higher recall with a large drop of precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 113, "end_pos": 119, "type": "METRIC", "confidence": 0.9996082186698914}, {"text": "precision", "start_pos": 141, "end_pos": 150, "type": "METRIC", "confidence": 0.9983363747596741}]}, {"text": "The drop of precision maybe caused by the neglect of structural and semantic information of discourse instances.", "labels": [], "entities": [{"text": "precision", "start_pos": 12, "end_pos": 21, "type": "METRIC", "confidence": 0.9993031024932861}]}, {"text": "However, M&E still outperformed Baseline in average F -score.", "labels": [], "entities": [{"text": "M&E", "start_pos": 9, "end_pos": 12, "type": "DATASET", "confidence": 0.7432526151339213}, {"text": "F -score", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.9884109695752462}]}, {"text": "(2) cSSR enhanced Baseline by increasing the average recall by about 15% with only a small drop of precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 53, "end_pos": 59, "type": "METRIC", "confidence": 0.9644834399223328}, {"text": "precision", "start_pos": 99, "end_pos": 108, "type": "METRIC", "confidence": 0.9987577199935913}]}, {"text": "The performance of cSSR demonstrated that our method could effectively discover high quality common SSRs.", "labels": [], "entities": []}, {"text": "The most remarkable improvement was observed on Continuation in which the recall increased by almost 20% with only a minor drop of precision.", "labels": [], "entities": [{"text": "Continuation", "start_pos": 48, "end_pos": 60, "type": "DATASET", "confidence": 0.5535522103309631}, {"text": "recall", "start_pos": 74, "end_pos": 80, "type": "METRIC", "confidence": 0.9959838390350342}, {"text": "precision", "start_pos": 131, "end_pos": 140, "type": "METRIC", "confidence": 0.9989847540855408}]}, {"text": "Actually, cSSR outperformed Baseline in all discourse relations except for Contrast.", "labels": [], "entities": []}, {"text": "In Discourse Tree Bank () only 26% of Contrast relations were indicated by cue phrases while in NTC-7 about 70% of Contrast were indicated by cue phrases.", "labels": [], "entities": [{"text": "Discourse Tree Bank", "start_pos": 3, "end_pos": 22, "type": "DATASET", "confidence": 0.8243967493375143}, {"text": "NTC-7", "start_pos": 96, "end_pos": 101, "type": "DATASET", "confidence": 0.9629022479057312}]}, {"text": "A possible reason was that we were dealing with Chinese news text which were usually well written.", "labels": [], "entities": []}, {"text": "Another important observation was that the performance of cSSR was very close to the result of SVM.", "labels": [], "entities": []}, {"text": "(3) SVM+SSRs achieved the best F -score on Continuation and average performance.", "labels": [], "entities": [{"text": "SVM+SSRs", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.5129237870375315}, {"text": "F -score", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.9956766963005066}, {"text": "Continuation", "start_pos": 43, "end_pos": 55, "type": "METRIC", "confidence": 0.993863582611084}]}, {"text": "The integration of SSRs to the feature set of SVM contributed to a remarkable increase in average F -score.", "labels": [], "entities": [{"text": "F -score", "start_pos": 98, "end_pos": 106, "type": "METRIC", "confidence": 0.9690863092740377}]}, {"text": "The results of cSSR and SVM+SSRs demonstrated the effectiveness of common SSRs mined by the proposed unsupervised method.", "labels": [], "entities": []}, {"text": "presented the performance of integrating discourse classifiers to polarity classification.", "labels": [], "entities": [{"text": "polarity classification", "start_pos": 66, "end_pos": 89, "type": "TASK", "confidence": 0.7396244406700134}]}, {"text": "For Baseline and cSSR, the information of nucleus and satellite could be obtained directly from cue-  phrase-based patterns and SSRs, respectively.", "labels": [], "entities": []}, {"text": "For SVM+cSSR, the nucleus and satellite information was acquired by cSSR if a segment pair could match a cSSR.", "labels": [], "entities": []}, {"text": "Otherwise, we used manually annotated nucleus and satellite information.", "labels": [], "entities": []}, {"text": "It's clear that the performance of polarity classification was enhanced with the improvement of discourse relation recognition.", "labels": [], "entities": [{"text": "polarity classification", "start_pos": 35, "end_pos": 58, "type": "TASK", "confidence": 0.7238357663154602}, {"text": "discourse relation recognition", "start_pos": 96, "end_pos": 126, "type": "TASK", "confidence": 0.6356912652651469}]}, {"text": "M&E was not included in this experiment because the performance of polarity classification was decreased by the mis-classified discourse relations.", "labels": [], "entities": [{"text": "polarity classification", "start_pos": 67, "end_pos": 90, "type": "TASK", "confidence": 0.739341139793396}]}, {"text": "SVM+SSRs achieved significant (p<0.01) improvement in polarity classification compared to BPC.", "labels": [], "entities": [{"text": "SVM+SSRs", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.7061354915301005}, {"text": "polarity classification", "start_pos": 54, "end_pos": 77, "type": "TASK", "confidence": 0.7126764208078384}, {"text": "BPC", "start_pos": 90, "end_pos": 93, "type": "DATASET", "confidence": 0.9129489064216614}]}], "tableCaptions": [{"text": " Table 1: Examples of cue phrases", "labels": [], "entities": []}, {"text": " Table 5: Performance of recognizing discourse relations. (The evaluation criteria are Precision, Recall and F-score)", "labels": [], "entities": [{"text": "Precision", "start_pos": 87, "end_pos": 96, "type": "METRIC", "confidence": 0.9989680051803589}, {"text": "Recall", "start_pos": 98, "end_pos": 104, "type": "METRIC", "confidence": 0.9933595061302185}, {"text": "F-score", "start_pos": 109, "end_pos": 116, "type": "METRIC", "confidence": 0.9977315068244934}]}]}