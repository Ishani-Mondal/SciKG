{"title": [{"text": "Using Syntactic and Semantic Structural Kernels for Classifying Definition Questions in Jeopardy!", "labels": [], "entities": [{"text": "Classifying Definition Questions in Jeopardy", "start_pos": 52, "end_pos": 96, "type": "TASK", "confidence": 0.7567629933357238}]}], "abstractContent": [{"text": "The last decade has seen many interesting applications of Question Answering (QA) technology.", "labels": [], "entities": [{"text": "Question Answering (QA)", "start_pos": 58, "end_pos": 81, "type": "TASK", "confidence": 0.889458167552948}]}, {"text": "quiz show is certainly one of the most fascinating, from the viewpoints of both its broad domain and the complexity of its language.", "labels": [], "entities": [{"text": "quiz show", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.8763855695724487}]}, {"text": "In this paper, we study kernel methods applied to syntactic/semantic structures for accurate classification of Jeopardy ! definition questions.", "labels": [], "entities": [{"text": "classification of Jeopardy ! definition questions", "start_pos": 93, "end_pos": 142, "type": "TASK", "confidence": 0.8193871180216471}]}, {"text": "Our extensive empirical analysis shows that our classification models largely improve on classifiers based on word-language models.", "labels": [], "entities": []}, {"text": "Such classifiers are also used in the state-of-the-art QA pipeline constituting Watson, the IBM Jeopardy!", "labels": [], "entities": [{"text": "Watson", "start_pos": 80, "end_pos": 86, "type": "DATASET", "confidence": 0.9045812487602234}]}, {"text": "Our experiments measuring their impact on Watson show enhancements in QA accuracy and a consequent increase in the amount of money earned in game-based evaluation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 73, "end_pos": 81, "type": "METRIC", "confidence": 0.8892168998718262}]}], "introductionContent": [{"text": "Question Answering (QA) is an important research area of Information Retrieval applications, which requires the use of core NLP capabilities, such as syntactic and semantic processing fora more effective user experience.", "labels": [], "entities": [{"text": "Question Answering (QA)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.9034570097923279}, {"text": "Information Retrieval", "start_pos": 57, "end_pos": 78, "type": "TASK", "confidence": 0.792626678943634}]}, {"text": "While the development of most existing QA systems are driven by organized evaluation efforts such as TREC (), CLEF (, and NT-CIR (, there exist efforts that leverage data from popular quiz shows, such as Who Wants to be a Millionaire ( and Jeopardy!", "labels": [], "entities": [{"text": "TREC", "start_pos": 101, "end_pos": 105, "type": "METRIC", "confidence": 0.9623861312866211}, {"text": "CLEF", "start_pos": 110, "end_pos": 114, "type": "METRIC", "confidence": 0.7498317360877991}, {"text": "Who Wants to be a Millionaire", "start_pos": 204, "end_pos": 233, "type": "TASK", "confidence": 0.6804486314455668}]}, {"text": "(, to demonstrate the generality of the technology.", "labels": [], "entities": []}, {"text": "Jeopardy! is a popular quiz show in the US which has been on the air for 27 years.", "labels": [], "entities": [{"text": "Jeopardy!", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.8360902070999146}]}, {"text": "In each game, three contestants compete for the opportunity to answer 60 questions in 12 categories of 5 questions each.", "labels": [], "entities": []}, {"text": "questions cover an incredibly broad domain, from science, literature, history, to popular culture.", "labels": [], "entities": []}, {"text": "We are drawn to Jeopardy!", "labels": [], "entities": []}, {"text": "as a test bed for open-domain QA technology due to its broad domain, complex language, as well as the emphasis on accuracy, confidence, and speed during game play.", "labels": [], "entities": [{"text": "QA", "start_pos": 30, "end_pos": 32, "type": "TASK", "confidence": 0.8230657577514648}, {"text": "accuracy", "start_pos": 114, "end_pos": 122, "type": "METRIC", "confidence": 0.9991400241851807}, {"text": "confidence", "start_pos": 124, "end_pos": 134, "type": "METRIC", "confidence": 0.9699086546897888}]}, {"text": "While the vast majority of Jeopardy!", "labels": [], "entities": []}, {"text": "questions are factoid questions, we find several other types of questions in the Jeopardy!", "labels": [], "entities": []}, {"text": "data, which can benefit from specialized processing in the QA system.", "labels": [], "entities": []}, {"text": "The additional processing in these questions complements that of the factoid questions to achieve improved overall QA performance.", "labels": [], "entities": []}, {"text": "Among the various types of questions handled by the system are definition questions shown in the examples below: (1) GON TOMORROW: It can be the basket below a hot-air balloon or a flat-bottomed boat used on a canal (answer: gondola); (2) I LOVE YOU, \"MIN\": Overbearing (answer: domineering); (3) INVEST: From the Latin for \"year\", it's an investment or retirement fund that pays out yearly (answer: an annuity) where the uppercase text indicates the Jeopardy!", "labels": [], "entities": [{"text": "GON", "start_pos": 117, "end_pos": 120, "type": "METRIC", "confidence": 0.9496358036994934}, {"text": "TOMORROW", "start_pos": 121, "end_pos": 129, "type": "METRIC", "confidence": 0.688351571559906}, {"text": "INVEST", "start_pos": 297, "end_pos": 303, "type": "METRIC", "confidence": 0.9929875731468201}]}, {"text": "category for each question . Several characteristics of this class of questions warrant special processing: first, the clue (question) often aligns well with dictionary entries, making dictionary resources potentially effective.", "labels": [], "entities": []}, {"text": "Second, these clues often do not indicate an answer type, which is an important feature for identifying correct answers in factoid questions (in the examples above, only (3) provided an answer type, \"fund\").", "labels": [], "entities": []}, {"text": "Third, definition questions are typically shorter in length than the average factoid question.", "labels": [], "entities": [{"text": "definition questions", "start_pos": 7, "end_pos": 27, "type": "TASK", "confidence": 0.9211649596691132}]}, {"text": "These differences, namely the shorter clue length and the lack of answer types, make the use of a specialized machine learning model potentially promising for improving the overall system accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 188, "end_pos": 196, "type": "METRIC", "confidence": 0.9933440685272217}]}, {"text": "The first step for handling definitions is, of course, the automatic separation of definitions from other question types, which is not a simple task in the Jeopardy!", "labels": [], "entities": [{"text": "automatic separation of definitions from other question types", "start_pos": 59, "end_pos": 120, "type": "TASK", "confidence": 0.7597515769302845}]}, {"text": "For instance, consider the following example which is a variation of (3) above: (4) INVEST: From the Latin for \"year\", an annuity is an investment or retirement fund that pays out this often (answer: yearly) Even though the clue is nearly identical to (3), the clue does not provide a definition for the answer yearly, although at first glance we may have been misled.", "labels": [], "entities": [{"text": "INVEST", "start_pos": 84, "end_pos": 90, "type": "METRIC", "confidence": 0.9986128807067871}]}, {"text": "The source of complexity is given by the fact that Jeopardy!", "labels": [], "entities": [{"text": "Jeopardy!", "start_pos": 51, "end_pos": 60, "type": "DATASET", "confidence": 0.8463966846466064}]}, {"text": "clues are not phrased in interrogative form as questions typically are.", "labels": [], "entities": []}, {"text": "This complicates the design of definition classifiers since we cannot directly use either typical structural patterns that characterize definition/description questions, or previous approaches, e.g. ().", "labels": [], "entities": []}, {"text": "Given the complexity and the novelty of the task, we found it useful to exploit the kernel methods technology.", "labels": [], "entities": []}, {"text": "This has shown state-of-the-art performance in Question Classification (QC), e.g. ( and it is very well suited for engineering feature representations for novel tasks.", "labels": [], "entities": [{"text": "Question Classification (QC)", "start_pos": 47, "end_pos": 75, "type": "TASK", "confidence": 0.7706723868846893}]}, {"text": "In this paper, we apply SVMs and kernel methods to syntactic/semantic structures for modeling accurate classification of Jeopardy!", "labels": [], "entities": [{"text": "classification of Jeopardy!", "start_pos": 103, "end_pos": 130, "type": "TASK", "confidence": 0.6195672526955605}]}, {"text": "For this purpose, we use several levels of linguistic information: word and POS tag sequences, dependency, constituency and predicate argument structures and we combined them using state-ofthe-art structural kernels, e.g. ().", "labels": [], "entities": []}, {"text": "The extensive empirical analysis of several advanced models shows that our best model, which combines different kernels, improves the F1 of our baseline model by 67% relative, from 40.37 to 67.48.", "labels": [], "entities": [{"text": "F1", "start_pos": 134, "end_pos": 136, "type": "METRIC", "confidence": 0.9996731281280518}]}, {"text": "Surprisingly, with respect to previous findings on standard QC, e.g. (), the Syntactic Tree Kernel () is not effective whereas the exploitation of partial tree patterns proves to be essential.", "labels": [], "entities": []}, {"text": "This is due to the different nature of Jeopardy!", "labels": [], "entities": []}, {"text": "questions, which are not expressed in the usual interrogative form.", "labels": [], "entities": []}, {"text": "To demonstrate the benefit of our question classifier, we integrated it into our Watson by coupling it with search and candidate generation against specialized dictionary resources.", "labels": [], "entities": []}, {"text": "We show that in endto-end evaluations, Watson with kernel-based definition classification and specialized definition question processing achieves statistically significant improvement compared to our baseline systems.", "labels": [], "entities": [{"text": "kernel-based definition classification", "start_pos": 51, "end_pos": 89, "type": "TASK", "confidence": 0.6183341244856516}, {"text": "definition question processing", "start_pos": 106, "end_pos": 136, "type": "TASK", "confidence": 0.6693075100580851}]}, {"text": "In the reminder of this paper, Section 2 describes Watson by focusing on the problem of definition question classification, Section 3 describes our models for such classifiers, Section 4 presents our experiments on QC, whereas Section 5 shows the final impact on Watson.", "labels": [], "entities": [{"text": "definition question classification", "start_pos": 88, "end_pos": 122, "type": "TASK", "confidence": 0.8334435621897379}]}, {"text": "Finally, Section 6 discusses related work and Section 7 derives the conclusions.", "labels": [], "entities": []}], "datasetContent": [{"text": "In these experiments, we study the role of kernel technology for the design of accurate classification of definition questions.", "labels": [], "entities": [{"text": "classification of definition questions", "start_pos": 88, "end_pos": 126, "type": "TASK", "confidence": 0.7926670908927917}]}, {"text": "We build several classifiers based on SVMs and kernel methods.", "labels": [], "entities": []}, {"text": "Each classifier uses advanced syntactic/semantic structural features and their combination.", "labels": [], "entities": []}, {"text": "We carryout an extensive comparison in terms of F1 between the different models on the Jeopardy!", "labels": [], "entities": [{"text": "F1", "start_pos": 48, "end_pos": 50, "type": "METRIC", "confidence": 0.9975276589393616}, {"text": "Jeopardy!", "start_pos": 87, "end_pos": 96, "type": "DATASET", "confidence": 0.9350377321243286}]}, {"text": "We integrated the classifier into the question analysis module, and incorporated additional components to search against dictionary resources and extract candidate answers from these search results when a question is classified as definitional.", "labels": [], "entities": [{"text": "question analysis", "start_pos": 38, "end_pos": 55, "type": "TASK", "confidence": 0.7371784150600433}]}, {"text": "In the final machine learning models, a separate model is trained for definition questions to enable scoring tailored to the specific characteristics of those questions.", "labels": [], "entities": []}, {"text": "Based on our manually annotated gold standard, less than 10% of Jeopardy!", "labels": [], "entities": [{"text": "Jeopardy!", "start_pos": 64, "end_pos": 73, "type": "DATASET", "confidence": 0.8683047294616699}]}, {"text": "questions are classified as definition questions.", "labels": [], "entities": []}, {"text": "Due to their relatively low frequency we conduct two types of evaluations.", "labels": [], "entities": []}, {"text": "The first is definition-only evaluation, in which we apply our definition question classifier to identify a large set of definition questions and evaluate the end-toend system's performance on this large set of questions.", "labels": [], "entities": []}, {"text": "These results enable us to draw statistically significant conclusions about our approach to addressing definition questions.", "labels": [], "entities": [{"text": "addressing definition questions", "start_pos": 92, "end_pos": 123, "type": "TASK", "confidence": 0.8493526577949524}]}, {"text": "The second type of evaluation is game-based evaluation, which assesses the impact of our definition question processing on Watson performance while preserving the natural distribution of these question types in Jeopardy!", "labels": [], "entities": []}, {"text": "Game-based evaluations situate the system's performance on definition questions relative to other types of questions, and enable us to gauge the component's contributions in a game-based setting.", "labels": [], "entities": []}, {"text": "For both evaluation settings, three configurations of Watson are used as follows: \u2022 the NoDef system, in which Watson is configured without definition classification and processing, thereby treating all definition questions as regular factoid questions; \u2022 the StatDef system, which leverages the statistical classifier and subsequent definition specific search and candidate generation components as described above; and \u2022 the RuleDef system, in which Watson adopts RBC and employs the same additional definition search and candidate generation components as the StatDef system.", "labels": [], "entities": [{"text": "definition classification", "start_pos": 140, "end_pos": 165, "type": "TASK", "confidence": 0.71015565097332}, {"text": "RBC", "start_pos": 466, "end_pos": 469, "type": "METRIC", "confidence": 0.8960851430892944}]}, {"text": "For the definition-only evaluation, we selected all questions recognized as definitional by the statistical classifier from roughly 1000 unseen games (60000 questions), resulting in a test set of 1606 questions.", "labels": [], "entities": []}, {"text": "Due to the size of the initial set, it is impractical to manually create a gold standard for measuring Precision and Recall of the classifier.", "labels": [], "entities": [{"text": "Precision", "start_pos": 103, "end_pos": 112, "type": "METRIC", "confidence": 0.9942044615745544}, {"text": "Recall", "start_pos": 117, "end_pos": 123, "type": "METRIC", "confidence": 0.9919439554214478}]}, {"text": "Instead, we compare the StatDef system against the NoDef on these 1606 questions using two metrics: accuracy, defined as the percentage of questions correctly answered, and p@70, the system's Precision when answering only the top 70% most confident questions.", "labels": [], "entities": [{"text": "NoDef", "start_pos": 51, "end_pos": 56, "type": "DATASET", "confidence": 0.918526828289032}, {"text": "accuracy", "start_pos": 100, "end_pos": 108, "type": "METRIC", "confidence": 0.9993414282798767}, {"text": "Precision", "start_pos": 192, "end_pos": 201, "type": "METRIC", "confidence": 0.9991856217384338}]}, {"text": "P@70 is an important metric in Jeopardy!", "labels": [], "entities": [{"text": "P@70", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.7768641511599222}, {"text": "Jeopardy!", "start_pos": 31, "end_pos": 40, "type": "DATASET", "confidence": 0.7445400059223175}]}, {"text": "game play as well as in real world applications where the system may refrain from answering a question when it is not confident about any of its answers.", "labels": [], "entities": []}, {"text": "Since RBC identifies significantly more definition questions, we started  with an initial set of roughly 300 games, from which the RBC identified 1875 questions as definitional.", "labels": [], "entities": [{"text": "RBC", "start_pos": 6, "end_pos": 9, "type": "DATASET", "confidence": 0.6689636707305908}]}, {"text": "We compared the RuleDef system's performance on these questions against the NoDef baseline using the accuracy and p@70 metrics.", "labels": [], "entities": [{"text": "NoDef baseline", "start_pos": 76, "end_pos": 90, "type": "DATASET", "confidence": 0.9048430621623993}, {"text": "accuracy", "start_pos": 101, "end_pos": 109, "type": "METRIC", "confidence": 0.9992953538894653}]}, {"text": "For the game-based evaluation, we randomly selected 66 unseen Jeopardy!", "labels": [], "entities": []}, {"text": "games, consisting of 3546 questions after excluding audio/visual questions.", "labels": [], "entities": []}, {"text": "We contrast the StatDef system performance against that of NoDef and RuleDef along several dimensions: accuracy and p@70, described above, as well as earnings, the average amount of money earned for each game.", "labels": [], "entities": [{"text": "NoDef", "start_pos": 59, "end_pos": 64, "type": "DATASET", "confidence": 0.8949453234672546}, {"text": "RuleDef", "start_pos": 69, "end_pos": 76, "type": "DATASET", "confidence": 0.8703043460845947}, {"text": "accuracy", "start_pos": 103, "end_pos": 111, "type": "METRIC", "confidence": 0.999653697013855}]}, {"text": "Since the kernel-based classifiers perform substantially better than RBC, we incorporate the PTK-CT+WSK+CSK model into Watson for definition classification and evaluated the QA performance against two baseline systems.", "labels": [], "entities": [{"text": "PTK-CT+WSK+CSK", "start_pos": 93, "end_pos": 107, "type": "DATASET", "confidence": 0.762498962879181}, {"text": "definition classification", "start_pos": 130, "end_pos": 155, "type": "TASK", "confidence": 0.879467099905014}]}, {"text": "For the end-to-end experiments, we used Watson's English Slot Grammar parser to generate the constituency trees.", "labels": [], "entities": []}, {"text": "The component level evaluation shows that we achieved comparable performance as previously discussed with ESG.", "labels": [], "entities": [{"text": "ESG", "start_pos": 106, "end_pos": 109, "type": "DATASET", "confidence": 0.908746600151062}]}, {"text": "For the definition-only evaluation, we compared the StatDef system against the NoDef system on a set of 1606 questions that the StatDef system classified as definitional.", "labels": [], "entities": []}, {"text": "The results are shown in the first two columns in.", "labels": [], "entities": []}, {"text": "To contrast the gain obtained by the StatDef system against that achieved by the RuleDef system, we ran the RuleDef system over the 1875 questions identified as definitional by the rule-based classifier.", "labels": [], "entities": []}, {"text": "We contrast the RuleDef system performance with that of the NoDef system, as shown in the last two columns in.", "labels": [], "entities": []}, {"text": "Our results show that based on both evaluation metrics, StatDef improved upon the NoDef baseline more than RuleDef improved on the same baseline system.", "labels": [], "entities": [{"text": "NoDef baseline", "start_pos": 82, "end_pos": 96, "type": "DATASET", "confidence": 0.8289686143398285}]}, {"text": "Furthermore, for the accuracy metric where all samples are paired and independent, the difference in performance between the StatDef and NoDef systems is statistically significant at p<0.05, while that between the RuleDef and NoDef systems is not.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9986041188240051}]}, {"text": "The game-based evaluation was carried out on 66 unseen games (roughly 3500 questions).", "labels": [], "entities": []}, {"text": "Of these  questions, the StatDef system classified 131 of them as definitional while the RuleDef system identified 480 definition questions.", "labels": [], "entities": [{"text": "RuleDef", "start_pos": 89, "end_pos": 96, "type": "DATASET", "confidence": 0.9124985933303833}]}, {"text": "Both systems were compared against the NoDef system using the accuracy, p@70, and earnings metric computed overall questions, as shown in.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.9996869564056396}]}, {"text": "Our results show that even though in the definition-only evaluation both the RuleDef and StatDef systems outperformed the NoDef baseline, in our game-based evaluation, the RuleDef system performed worse than the NoDef baseline.", "labels": [], "entities": [{"text": "NoDef baseline", "start_pos": 212, "end_pos": 226, "type": "DATASET", "confidence": 0.8735657334327698}]}, {"text": "The lowered performance is due to the fact that the Precision of the RBC is much lower than that of the statistical classifier, and the special definition processing applied to questions that are erroneously classified as definitional was harmful.", "labels": [], "entities": [{"text": "Precision", "start_pos": 52, "end_pos": 61, "type": "METRIC", "confidence": 0.9989995360374451}]}, {"text": "Our evaluation of this false positive set showed that its accuracy dropped by 6% compared to the NoDef system.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.9995893836021423}]}, {"text": "On the other hand, the StatDef system outperformed the two other systems, and its accuracy improvement upon the RuleDef system is statistically significant at p<0.05.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 82, "end_pos": 90, "type": "METRIC", "confidence": 0.9991214871406555}]}], "tableCaptions": [{"text": " Table 1: Kernel performance using leave-one-out cross- validation.", "labels": [], "entities": [{"text": "Kernel", "start_pos": 10, "end_pos": 16, "type": "TASK", "confidence": 0.9822589159011841}]}, {"text": " Table 3: Definition-Only Evaluation Results", "labels": [], "entities": []}, {"text": " Table 4: Game-Based Evaluation Results", "labels": [], "entities": []}]}