{"title": [{"text": "Data-Driven Response Generation in Social Media", "labels": [], "entities": [{"text": "Data-Driven Response Generation", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.5638135373592377}]}], "abstractContent": [{"text": "We present a data-driven approach to generating responses to Twitter status posts, based on phrase-based Statistical Machine Translation.", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 105, "end_pos": 136, "type": "TASK", "confidence": 0.5599141617616018}]}, {"text": "We find that mapping conversational stimuli onto responses is more difficult than translating between languages, due to the wider range of possible responses, the larger fraction of unaligned words/phrases, and the presence of large phrase pairs whose alignment cannot be further decomposed.", "labels": [], "entities": []}, {"text": "After addressing these challenges, we compare approaches based on SMT and Information Retrieval in a human evaluation.", "labels": [], "entities": [{"text": "SMT", "start_pos": 66, "end_pos": 69, "type": "TASK", "confidence": 0.9910295009613037}, {"text": "Information Retrieval", "start_pos": 74, "end_pos": 95, "type": "TASK", "confidence": 0.7019102573394775}]}, {"text": "We show that SMT outperforms IR on this task, and its output is preferred over actual human responses in 15% of cases.", "labels": [], "entities": [{"text": "SMT", "start_pos": 13, "end_pos": 16, "type": "TASK", "confidence": 0.9920052886009216}, {"text": "IR", "start_pos": 29, "end_pos": 31, "type": "TASK", "confidence": 0.9782248735427856}]}, {"text": "As far as we are aware, this is the first work to investigate the use of phrase-based SMT to directly translate a linguistic stimulus into an appropriate response.", "labels": [], "entities": [{"text": "SMT", "start_pos": 86, "end_pos": 89, "type": "TASK", "confidence": 0.7449551224708557}]}], "introductionContent": [{"text": "Recently there has been an explosion in the number of people having informal, public conversations on social media websites such as Facebook and Twitter.", "labels": [], "entities": []}, {"text": "This presents a unique opportunity to build collections of naturally occurring conversations that are orders of magnitude larger than those previously available.", "labels": [], "entities": []}, {"text": "These corpora, in turn, present new opportunities to apply data-driven techniques to conversational tasks.", "labels": [], "entities": []}, {"text": "We investigate the problem of response generation: given a conversational stimulus, generate an appropriate response.", "labels": [], "entities": [{"text": "response generation", "start_pos": 30, "end_pos": 49, "type": "TASK", "confidence": 0.8298227787017822}]}, {"text": "Specifically, we employ a large corpus of status-response pairs found on Twitter to create a system that responds to Twitter status posts.", "labels": [], "entities": []}, {"text": "Note that we make no mention of context, intent or dialogue state; our goal is to generate any response that fits the provided stimulus; however, we do so without employing rules or templates, with the hope of creating a system that is both flexible and extensible when operating in an open domain.", "labels": [], "entities": []}, {"text": "Success in open domain response generation could be immediately useful to social media platforms, providing a list of suggested responses to a target status, or providing conversation-aware autocomplete for responses in progress.", "labels": [], "entities": [{"text": "open domain response generation", "start_pos": 11, "end_pos": 42, "type": "TASK", "confidence": 0.6340138167142868}]}, {"text": "These features are especially important on hand-held devices.", "labels": [], "entities": []}, {"text": "Response generation should also be beneficial in building \"chatterbots\") for entertainment purposes or companionship).", "labels": [], "entities": [{"text": "Response generation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8547858595848083}]}, {"text": "However, we are most excited by the future potential of data-driven response generation when used inside larger dialogue systems, where direct consideration of the user's utterance could be combined with dialogue state) to generate locally coherent, purposeful dialogue.", "labels": [], "entities": [{"text": "response generation", "start_pos": 68, "end_pos": 87, "type": "TASK", "confidence": 0.7194771319627762}]}, {"text": "In this work, we investigate statistical machine translation as an approach for response generation.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 29, "end_pos": 60, "type": "TASK", "confidence": 0.6508259276549021}, {"text": "response generation", "start_pos": 80, "end_pos": 99, "type": "TASK", "confidence": 0.9106343686580658}]}, {"text": "We are motivated by the following observation: In naturally occurring discourse, there is often a strong structural relationship between adjacent utterances.", "labels": [], "entities": []}, {"text": "For example, consider the stimulusresponse pair from the data: Stimulus: I'm slowly making this soup ...... and it smells gorgeous!", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to compare various approaches to automated response generation, we used human evalu-ators from Amazon's Mechanical Turk (.", "labels": [], "entities": [{"text": "automated response generation", "start_pos": 42, "end_pos": 71, "type": "TASK", "confidence": 0.6764186024665833}, {"text": "Amazon's Mechanical Turk", "start_pos": 104, "end_pos": 128, "type": "DATASET", "confidence": 0.8893516957759857}]}, {"text": "Human evaluation also provides us with data fora preliminary investigation into the feasibility of automatic evaluation metrics.", "labels": [], "entities": []}, {"text": "While automated evaluation has been investigated in the area of spoken dialogue systems (, it is unclear how well it will correlate with human judgment in open-domain conversations where the range of possible responses is very large.", "labels": [], "entities": []}, {"text": "We performed pairwise comparisons of several response-generation systems.", "labels": [], "entities": []}, {"text": "Similar work on evaluating MT output) has asked Turkers to rank more than two choices, but in order to keep our evaluation as straightforward as possible, we limited our experiments to pairwise comparisons.", "labels": [], "entities": [{"text": "MT", "start_pos": 27, "end_pos": 29, "type": "TASK", "confidence": 0.9647670388221741}]}, {"text": "For each experiment comparing 2 systems (a and b), we built a test set by selecting a random sample of 200 tweets which had received responses, and which had a length between 4 and 20 words.", "labels": [], "entities": []}, {"text": "These tweets were selected from conversations collected from a later, non-overlapping time-period from those used in training.", "labels": [], "entities": []}, {"text": "Each experiment used a different random sample of 200 tweets.", "labels": [], "entities": []}, {"text": "For each of the 200 statuses, we generated a response using method a and b, then showed the status and both responses to the Turkers, asking them to choose the best response.", "labels": [], "entities": [{"text": "Turkers", "start_pos": 125, "end_pos": 132, "type": "DATASET", "confidence": 0.8742178678512573}]}, {"text": "The order of the systems used to generate a response was randomized, and each of the 200 HITs was submitted to 3 different Turkers.", "labels": [], "entities": [{"text": "Turkers", "start_pos": 123, "end_pos": 130, "type": "DATASET", "confidence": 0.9087159037590027}]}, {"text": "Turkers were paid 1\u00a2 per judgment.", "labels": [], "entities": []}, {"text": "The Turkers were instructed that an appropriate response should be on the same topic as the status, and should also \"make sense\" in response to it.", "labels": [], "entities": []}, {"text": "While this is an inherently subjective task, from inspecting the results, we found Turkers to be quite competent in judging between two responses.", "labels": [], "entities": []}, {"text": "The systems used in these pairwise comparisons are summarized in table 2, and example output generated by each system is presented in.", "labels": [], "entities": []}, {"text": "The field of SMT has benefited greatly from the existence of an automatic evaluation metric, BLEU, which grades an output candidate according to n-gram matches to one or more reference outputs.", "labels": [], "entities": [{"text": "SMT", "start_pos": 13, "end_pos": 16, "type": "TASK", "confidence": 0.9909691214561462}, {"text": "BLEU", "start_pos": 93, "end_pos": 97, "type": "METRIC", "confidence": 0.9986907839775085}]}, {"text": "To evaluate whether BLEU is an appropriate automatic evaluation measure for response generation, we attempted to measure its agreement with the human judgments.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 20, "end_pos": 24, "type": "METRIC", "confidence": 0.9980611205101013}, {"text": "response generation", "start_pos": 76, "end_pos": 95, "type": "TASK", "confidence": 0.8944983184337616}]}, {"text": "We calculate BLEU using a single reference derived from our parallel corpus.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 13, "end_pos": 17, "type": "METRIC", "confidence": 0.9984439015388489}]}, {"text": "We show the smoothed BLEU 1-4 scores for each system on each dataset evaluated in.", "labels": [], "entities": [{"text": "BLEU 1-4", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9732594192028046}]}, {"text": "Although these scores are extremely low, the overall BLEU scores agree with overall annotator judgments in all cases except when comparing MT-CHAT and IR-RESPONSE.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 53, "end_pos": 57, "type": "METRIC", "confidence": 0.9989199638366699}, {"text": "MT-CHAT", "start_pos": 139, "end_pos": 146, "type": "DATASET", "confidence": 0.7667552828788757}]}, {"text": "It would seem that BLEU has some agreement with human judgments on this task, but perhaps not enough to be immediately useful.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 19, "end_pos": 23, "type": "METRIC", "confidence": 0.9966512322425842}]}], "tableCaptions": [{"text": " Table 3: Example responses from each system. We tried to pick examples where most (or all) systems generate  reasonable responses for illustration purposes.", "labels": [], "entities": []}, {"text": " Table 4: Results of pairwise comparisons between various response-generation methods. Each row presents a com- parison between systems a and b on 200 randomly selected tweets. The column Fraction A lists the fraction of HITs  where the majority of annotators agreed System A's response was better. The winning system is indicated with an  asterisk  *  . All differences are significant.", "labels": [], "entities": []}]}