{"title": [{"text": "Collaborative Ranking: A Case Study on Entity Linking", "labels": [], "entities": [{"text": "Entity Linking", "start_pos": 39, "end_pos": 53, "type": "TASK", "confidence": 0.749968409538269}]}], "abstractContent": [{"text": "In this paper, we present anew ranking scheme, collaborative ranking (CR).", "labels": [], "entities": []}, {"text": "In contrast to traditional non-collaborative ranking scheme which solely relies on the strengths of isolated queries and one stand-alone ranking algorithm, the new scheme integrates the strengths from multiple collaborators of a query and the strengths from multiple ranking algorithms.", "labels": [], "entities": []}, {"text": "We elaborate three specific forms of collaborative ranking, namely, micro col-laborative ranking (MiCR), macro collabora-tive ranking (MaCR) and micro-macro collab-orative ranking (MiMaCR).", "labels": [], "entities": []}, {"text": "Experiments on entity linking task show that our proposed scheme is indeed effective and promising.", "labels": [], "entities": [{"text": "entity linking task", "start_pos": 15, "end_pos": 34, "type": "TASK", "confidence": 0.8481059273084005}]}], "introductionContent": [{"text": "Many natural language processing tasks can be formalized as a ranking problem, namely to rank a collection of candidate \"objects\" with respect to a \"query\".", "labels": [], "entities": []}, {"text": "For example, intensive studies were devoted to parsing in which multiple possible parsing trees or forests are ranked with respect to a sentence, machine translation in which multiple translation hypotheses are ranked with respect to a source sentence), anaphora resolution in which multiple antecedents are ranked with respect to an anaphora (, and question answering in which multiple possible answers are ranked with respect to a question (.", "labels": [], "entities": [{"text": "parsing", "start_pos": 47, "end_pos": 54, "type": "TASK", "confidence": 0.9757618308067322}, {"text": "machine translation", "start_pos": 146, "end_pos": 165, "type": "TASK", "confidence": 0.7693420052528381}, {"text": "anaphora resolution", "start_pos": 254, "end_pos": 273, "type": "TASK", "confidence": 0.7210703045129776}, {"text": "question answering", "start_pos": 350, "end_pos": 368, "type": "TASK", "confidence": 0.8223949372768402}]}, {"text": "Previous studies mainly focused on improving the ranking performance using one stand-alone learning algorithm on isolated queries.", "labels": [], "entities": []}, {"text": "Although a wide range of learning algorithms (unsupervised, supervised or semi-supervised) is available, each with its strengths and weaknesses, there is not a learning algorithm that can work best on all types of data.", "labels": [], "entities": []}, {"text": "In such a situation, it would be desirable to build a \"collaborative\" model by integrating multiple models.", "labels": [], "entities": []}, {"text": "Such an idea forms the basis of ensemble methodology and it is wellknown that ensemble methods (e.g., bagging, boosting) can improve the performance of many problems, in which classification is the most intensively studied.", "labels": [], "entities": []}, {"text": "The other situation is related with isolated queries handled by learning algorithms.", "labels": [], "entities": []}, {"text": "The single query may not be formulated with the best terms or the query itself may not contain comprehensive information required fora highperformance ranking algorithm.", "labels": [], "entities": []}, {"text": "Therefore, techniques of query expansion or query reformulation can be introduced and previous research has shown the effectiveness of those techniques in such applications as information retrieval and question answering (.", "labels": [], "entities": [{"text": "query expansion", "start_pos": 25, "end_pos": 40, "type": "TASK", "confidence": 0.8109336793422699}, {"text": "query reformulation", "start_pos": 44, "end_pos": 63, "type": "TASK", "confidence": 0.7042637169361115}, {"text": "information retrieval", "start_pos": 176, "end_pos": 197, "type": "TASK", "confidence": 0.8093021214008331}, {"text": "question answering", "start_pos": 202, "end_pos": 220, "type": "TASK", "confidence": 0.9065252244472504}]}, {"text": "Nevertheless, previous research normally considers query reformulation as anew query for the ranking system, it would be more desirable to form a largerscale \"collaborative\" group for the query and make a unified decision based on the group.", "labels": [], "entities": [{"text": "query reformulation", "start_pos": 51, "end_pos": 70, "type": "TASK", "confidence": 0.761521577835083}]}, {"text": "Inspired from human collaborative learning in which two or more people form a group and accomplish work together, we propose anew ranking scheme, collaborative ranking, which aims to imitate human collaborative learning and enhance system ranking performance.", "labels": [], "entities": []}, {"text": "The main idea is to seek collaborations for each query from two levels: (1) query-level: search a group of query collaborators, and make the joint decision from the group together with the query using a stand-alone ranking algorithm.", "labels": [], "entities": []}, {"text": "(2) ranker-level: design a group of multiple rankers, and make the joint decision from the entire group on a single query.", "labels": [], "entities": []}, {"text": "771 Figure 1: Non-collaborative ranking and three collaborative ranking approaches.", "labels": [], "entities": []}, {"text": "presents an intuitive illustration of four ranking approaches, including the traditional noncollaborative ranking and three collaborative ranking forms: micro collaborative ranking (MiCR), macro collaborative ranking (MaCR), and micromacro collaborative ranking (MiMaCR).", "labels": [], "entities": []}, {"text": "Compared with the traditional non-collaborative ranking that only leverages the information contained in a single query and only applies one ranking function), the three collaborative ranking approaches have the following advantages: (1)MiCR (corresponding to query-level collaboration 1 ) leverages the information contained in the collaborators of a query.", "labels": [], "entities": []}, {"text": "(b) demonstrates that 6 query collaborators together with the query form a query collaboration group.", "labels": [], "entities": []}, {"text": "(2)MaCR (corresponding to ranker-level collaboration 2 ) integrates the strengths from two or more rankers.", "labels": [], "entities": [{"text": "MaCR", "start_pos": 3, "end_pos": 7, "type": "METRIC", "confidence": 0.5164936184883118}]}, {"text": "(c) demonstrates an example of 3 rankers.", "labels": [], "entities": []}, {"text": "(3)MiMaCR combines the advantages from MiCR and MaCR as shown in.", "labels": [], "entities": []}, {"text": "In this paper, we will show the efficacy of collaborative ranking on the entity linking task defined in the Knowledge Base Population (KBP) track  at Text Analysis Conference (TAC).", "labels": [], "entities": [{"text": "Knowledge Base Population (KBP) track  at Text Analysis Conference (TAC)", "start_pos": 108, "end_pos": 180, "type": "DATASET", "confidence": 0.7527477443218231}]}, {"text": "Each query in the task is associated with a name string and its context document.", "labels": [], "entities": []}, {"text": "Traditional approaches for entity linking only made use of the lexical or document level information contained in the query, however, it may not be sufficient for the task.", "labels": [], "entities": [{"text": "entity linking", "start_pos": 27, "end_pos": 41, "type": "TASK", "confidence": 0.7658461034297943}]}, {"text": "The intuition why query-level collaboration may work is that it leverages more comprehensive information about the entity mention from multiple \"collaborators\" (re-lated documents containing the name string).", "labels": [], "entities": []}, {"text": "Furthermore, previous work on this task mainly focused on comparing one ranking algorithm with the others, however, each ranking algorithm has its own strengths, and therefore, ranker-level collaboration can potentially improve the performance.", "labels": [], "entities": []}, {"text": "Last, the combination of query-level and ranker-level collaboration can lead to further performance gains.", "labels": [], "entities": []}], "datasetContent": [{"text": "We used TAC-KBP2009 evaluation data as our training (75%) and development set (25%), and used TAC-KBP2010 evaluation data as our blind testing set (shown in  An answer is considered as correct if the system output (either a KB entry id or NIL) exactly matches the key.", "labels": [], "entities": [{"text": "TAC-KBP2009 evaluation data", "start_pos": 8, "end_pos": 35, "type": "DATASET", "confidence": 0.77392578125}, {"text": "TAC-KBP2010 evaluation data", "start_pos": 94, "end_pos": 121, "type": "DATASET", "confidence": 0.8235577344894409}]}, {"text": "shows the performance of the 8 baseline rankers in 4 columns: Overall for all queries, PER for person queries, ORG for organization queries, and GPE for geo-political queries.", "labels": [], "entities": [{"text": "PER", "start_pos": 87, "end_pos": 90, "type": "METRIC", "confidence": 0.99828040599823}, {"text": "ORG", "start_pos": 111, "end_pos": 114, "type": "METRIC", "confidence": 0.9984594583511353}, {"text": "GPE", "start_pos": 145, "end_pos": 148, "type": "METRIC", "confidence": 0.9983705878257751}]}, {"text": "Each column is further split into All, KB (for Non-NIL queries) and NIL (for NIL queries).", "labels": [], "entities": []}, {"text": "It shows that all the four supervised rankers perform better than the four unsupervised rankers.", "labels": [], "entities": []}, {"text": "Naive ranker obtains the lowest overall micro-average accuracy (54.5%) but the highest NIL accuracy (100%).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.9115776419639587}, {"text": "NIL", "start_pos": 87, "end_pos": 90, "type": "METRIC", "confidence": 0.9083095192909241}, {"text": "accuracy", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.6816162467002869}]}, {"text": "Among the four unsupervised rankers, profile ranker performs the best, which clearly shows that the extracted attributes of entities are effective for disambiguating confusable names.", "labels": [], "entities": []}, {"text": "For example, our data analysis shows that the attribute value of \"per:alternative-name\" from the context document is particularly useful if a person query is only mentioned by its last name.", "labels": [], "entities": []}, {"text": "The attribute \"per:title\" is another important indicator to discriminate one person from the other.", "labels": [], "entities": []}, {"text": "For geopolitical queries, if the query is a city name, attribute \"gpe:state\" is useful to distinguish cities with the same name but in different states or provinces.", "labels": [], "entities": []}, {"text": "Among the four supervised rankers, ListNet outperforms SVM ranking and then SVM ranking outperforms the two pointwise rankers.", "labels": [], "entities": []}, {"text": "It may confirm previous research findings that listwise ranking is superior to pairwise ranking and pairwise ranking is superior to pointwise ranking ().", "labels": [], "entities": []}, {"text": "The best baseline ranker (ListNet) obtains an absolute overall accuracy gain of 26.6% over the naive ranker.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 63, "end_pos": 71, "type": "METRIC", "confidence": 0.9878496527671814}]}], "tableCaptions": [{"text": " Table 2: Training, development and testing corpus.", "labels": [], "entities": []}, {"text": " Table 3: Comparison of 8 baseline rankers.", "labels": [], "entities": []}]}