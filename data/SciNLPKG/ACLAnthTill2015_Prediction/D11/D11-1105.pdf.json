{"title": [{"text": "Generating Aspect-oriented Multi-Document Summarization with Event-aspect model", "labels": [], "entities": [{"text": "Generating Aspect-oriented Multi-Document Summarization", "start_pos": 0, "end_pos": 55, "type": "TASK", "confidence": 0.533501498401165}]}], "abstractContent": [{"text": "In this paper, we propose a novel approach to automatic generation of aspect-oriented summaries from multiple documents.", "labels": [], "entities": [{"text": "automatic generation of aspect-oriented summaries from multiple documents", "start_pos": 46, "end_pos": 119, "type": "TASK", "confidence": 0.7892596125602722}]}, {"text": "We first develop an event-aspect LDA model to cluster sentences into aspects.", "labels": [], "entities": []}, {"text": "We then use extended LexRank algorithm to rank the sentences in each cluster.", "labels": [], "entities": []}, {"text": "We use Integer Linear Programming for sentence selection.", "labels": [], "entities": [{"text": "sentence selection", "start_pos": 38, "end_pos": 56, "type": "TASK", "confidence": 0.7877964079380035}]}, {"text": "Key features of our method include automatic grouping of semantically related sentences and sentence ranking based on extension of random walk model.", "labels": [], "entities": [{"text": "grouping of semantically related sentences", "start_pos": 45, "end_pos": 87, "type": "TASK", "confidence": 0.7982413291931152}, {"text": "sentence ranking", "start_pos": 92, "end_pos": 108, "type": "TASK", "confidence": 0.7339414954185486}]}, {"text": "Also, we implement anew sentence compression algorithm which use dependency tree instead of parser tree.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 24, "end_pos": 44, "type": "TASK", "confidence": 0.7270786017179489}]}, {"text": "We compare our method with four baseline methods.", "labels": [], "entities": []}, {"text": "Quantitative evaluation based on Rouge metric demonstrates the effectiveness and advantages of our method.", "labels": [], "entities": [{"text": "Rouge metric", "start_pos": 33, "end_pos": 45, "type": "DATASET", "confidence": 0.8707070648670197}]}], "introductionContent": [{"text": "In recent years, there has been much interest in the task of multi-document summarization.", "labels": [], "entities": [{"text": "multi-document summarization", "start_pos": 61, "end_pos": 89, "type": "TASK", "confidence": 0.6257857084274292}]}, {"text": "In this paper, we study the task of automatically generating aspect-oriented summaries from multiple documents.", "labels": [], "entities": []}, {"text": "The goal of aspect-oriented summarization is to present the most important content to the user in a condensed form and a well-organized structure to satisfy the user's needs.", "labels": [], "entities": []}, {"text": "A summary should follow a readable structure and coverall the aspects users are interested in.", "labels": [], "entities": []}, {"text": "For example, a summary about natural disasters should include aspects about what happened, when/where it happened, reasons, damages, rescue efforts, etc. and these aspects maybe scattered in multiple articles written by different news agencies.", "labels": [], "entities": []}, {"text": "Our goal is to automatically collect aspects and construct summaries from multiple documents.", "labels": [], "entities": []}, {"text": "Aspect-oriented summarization can be used in many scenarios.", "labels": [], "entities": [{"text": "Aspect-oriented summarization", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.7753236293792725}]}, {"text": "First of all, it can be used to generate Wikipedia-like summary articles, especially used to generate introduction sections that summarizes the subject of articles before the table of contents and other elaborate sections.", "labels": [], "entities": [{"text": "summarizes the subject of articles before the table of contents", "start_pos": 129, "end_pos": 192, "type": "TASK", "confidence": 0.6708152502775192}]}, {"text": "Second, opinionated text often contains multiple viewpoints about an issue generated by different people.", "labels": [], "entities": []}, {"text": "Summarizing these multiple opinions can help people easily digest them.", "labels": [], "entities": []}, {"text": "Furthermore, combined with search engines and question&answering systems, we can better organize the summary content based on aspects to improve user experience.", "labels": [], "entities": [{"text": "question&answering", "start_pos": 46, "end_pos": 64, "type": "TASK", "confidence": 0.6231846213340759}]}, {"text": "Despite its usefulness, the problem of modeling domain specific aspects for multi-document summarization has not been well studied.", "labels": [], "entities": [{"text": "multi-document summarization", "start_pos": 76, "end_pos": 104, "type": "TASK", "confidence": 0.6200675070285797}]}, {"text": "The most relevant work is by) on exploring content models for multi-document summarization.", "labels": [], "entities": [{"text": "multi-document summarization", "start_pos": 62, "end_pos": 90, "type": "TASK", "confidence": 0.5920403003692627}]}, {"text": "They proposed a HIERSUM model for finding the subtopics or aspects which are combined by using KL-divergence criterion for selecting relevant sentences.", "labels": [], "entities": []}, {"text": "They introduced a general content distribution and several specific content distributions to discover the topic and aspects fora single document collection.", "labels": [], "entities": []}, {"text": "However, the aspects maybe shared not only across documents in a single collection, but also across documents in different topic-related collections.", "labels": [], "entities": []}, {"text": "Their model is conceptually inadequate for simultaneously summarizing multiple topic-related document collections.", "labels": [], "entities": [{"text": "summarizing multiple topic-related document collections", "start_pos": 58, "end_pos": 113, "type": "TASK", "confidence": 0.8687512516975403}]}, {"text": "Furthermore, their sentence selection method based on KLdivergence cannot prevent redundancy across different aspects.", "labels": [], "entities": [{"text": "sentence selection", "start_pos": 19, "end_pos": 37, "type": "TASK", "confidence": 0.7428304553031921}]}, {"text": "In this paper, we study how to overcome these limitations.", "labels": [], "entities": []}, {"text": "We hypothesize that comparatively summarizing topics across similar collections can improve the effectiveness of aspect-oriented multidocument summarization.", "labels": [], "entities": [{"text": "summarizing topics", "start_pos": 34, "end_pos": 52, "type": "TASK", "confidence": 0.9043683111667633}, {"text": "aspect-oriented multidocument summarization", "start_pos": 113, "end_pos": 156, "type": "TASK", "confidence": 0.5103353559970856}]}, {"text": "We propose a novel extraction-based approach which consists of four main steps listed below: Sentence Clustering: Our goal in this step is to automatically identify the different aspects and cluster sentences into aspects (See Section 2).", "labels": [], "entities": [{"text": "Sentence Clustering", "start_pos": 93, "end_pos": 112, "type": "TASK", "confidence": 0.8697813153266907}]}, {"text": "We substantially extend the entity-aspect model in () for generating general sentence clusters.", "labels": [], "entities": []}, {"text": "Sentence Ranking: In this step, we use an extension of LexRank algorithm proposed by  to score representative sentences in each cluster (See Section 3).", "labels": [], "entities": []}, {"text": "Sentence Compression: In this step, we aim to improve the linguistic quality of the summaries by simplifying the sentence expressions.", "labels": [], "entities": [{"text": "Sentence Compression", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8437985181808472}]}, {"text": "We prune sentences using grammatical relations defined on dependency trees for recognizing important clauses and removing redundant subtrees (See Section 4).", "labels": [], "entities": []}, {"text": "Sentence Selection: Finally, we select one compressed version of the sentences from each aspect cluster.", "labels": [], "entities": [{"text": "Sentence Selection", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8754322230815887}]}, {"text": "We use Integer Linear Programming (ILP) algorithm, which optimizes a global objective function, for sentence selection (See Section 5).", "labels": [], "entities": [{"text": "sentence selection", "start_pos": 100, "end_pos": 118, "type": "TASK", "confidence": 0.798058807849884}]}, {"text": "We evaluate our method using TAC2010 Guided Summarization task data sets 1 (Section 6).", "labels": [], "entities": [{"text": "TAC2010 Guided Summarization task data sets 1", "start_pos": 29, "end_pos": 74, "type": "DATASET", "confidence": 0.8186563849449158}]}, {"text": "Our evaluation shows that our method obtains better ROUGE recall score compared with four baseline methods, and it also achieve reasonably high-quality aspect clusters in terms of purity.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 52, "end_pos": 57, "type": "METRIC", "confidence": 0.9926983714103699}, {"text": "recall score", "start_pos": 58, "end_pos": 70, "type": "METRIC", "confidence": 0.9317252039909363}]}], "datasetContent": [{"text": "In order to systematically evaluate our method, we want to check (1) whether the whole system is effective, which means to quantitatively evaluate summary quality, and (2) whether individual components like clustering and compression algorithms are useful.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: ROUGE evaluation results on TAC2010 Summarization data sets", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9349784255027771}, {"text": "TAC2010 Summarization data", "start_pos": 38, "end_pos": 64, "type": "DATASET", "confidence": 0.656224936246872}]}, {"text": " Table 4: The true numbers of aspects as judged by the  human annotator (A), and the purity of the clusters.", "labels": [], "entities": [{"text": "purity", "start_pos": 85, "end_pos": 91, "type": "METRIC", "confidence": 0.9888131022453308}]}, {"text": " Table 5: The average score of each event topic.", "labels": [], "entities": []}]}