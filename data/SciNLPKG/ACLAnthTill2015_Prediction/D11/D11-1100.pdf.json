{"title": [{"text": "Harnessing WordNet Senses for Supervised Sentiment Classification", "labels": [], "entities": [{"text": "Harnessing WordNet Senses", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.6870466868082682}, {"text": "Supervised Sentiment Classification", "start_pos": 30, "end_pos": 65, "type": "TASK", "confidence": 0.6747252146402994}]}], "abstractContent": [{"text": "Traditional approaches to sentiment classification rely on lexical features, syntax-based features or a combination of the two.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 26, "end_pos": 50, "type": "TASK", "confidence": 0.9620092809200287}]}, {"text": "We propose semantic features using word senses fora supervised document-level sentiment classi-fier.", "labels": [], "entities": []}, {"text": "To highlight the benefit of sense-based features, we compare word-based representation of documents with a sense-based representation where WordNet senses of the words are used as features.", "labels": [], "entities": []}, {"text": "In addition, we highlight the benefit of senses by presenting a part-of-speech-wise effect on sentiment classification.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 94, "end_pos": 118, "type": "TASK", "confidence": 0.9456603229045868}]}, {"text": "Finally, we show that even if a WSD engine disambiguates between a limited set of words in a document, a sentiment classifier still performs better than what it does in absence of sense annotation.", "labels": [], "entities": []}, {"text": "Since word senses used as features show promise, we also examine the possibility of using similarity metrics defined on WordNet to address the problem of not finding a sense in the training corpus.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 120, "end_pos": 127, "type": "DATASET", "confidence": 0.9528896808624268}]}, {"text": "We perform experiments using three popular similarity metrics to mitigate the effect of unknown synsets in a test corpus by replacing them with similar synsets from the training corpus.", "labels": [], "entities": []}, {"text": "The results show promising improvement with respect to the baseline.", "labels": [], "entities": []}], "introductionContent": [{"text": "Sentiment Analysis (SA) is the task of prediction of opinion in text.", "labels": [], "entities": [{"text": "Sentiment Analysis (SA)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.9054410457611084}, {"text": "prediction of opinion in text", "start_pos": 39, "end_pos": 68, "type": "TASK", "confidence": 0.8761769890785217}]}, {"text": "Sentiment classification deals with tagging text as positive, negative or neutral from the perspective of the speaker/writer with respect to a topic.", "labels": [], "entities": [{"text": "Sentiment classification", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.9539127349853516}]}, {"text": "In this work, we follow the definition of and consider a binary classification task for output labels as positive and negative.", "labels": [], "entities": []}, {"text": "Traditional supervised approaches for SA have explored lexeme and syntax-level units as features.", "labels": [], "entities": [{"text": "SA", "start_pos": 38, "end_pos": 40, "type": "TASK", "confidence": 0.9758408665657043}]}, {"text": "Approaches using lexeme-based features use bagof-words) or identify the roles of different parts-of-speech (POS) like adjectives ().", "labels": [], "entities": []}, {"text": "Approaches using syntax-based features construct parse trees () or use text parsers to model valence shifters ().", "labels": [], "entities": []}, {"text": "Our work explores incorporation of semantics in a supervised sentiment classifier.", "labels": [], "entities": []}, {"text": "We use the synsets in Wordnet as the feature space to represent word senses.", "labels": [], "entities": [{"text": "Wordnet", "start_pos": 22, "end_pos": 29, "type": "DATASET", "confidence": 0.9637835025787354}]}, {"text": "Thus, a document consisting of words gets mapped to a document consisting of corresponding word senses.", "labels": [], "entities": []}, {"text": "Harnessing WordNet senses as features helps us address two issues: 1.", "labels": [], "entities": []}, {"text": "Impact of WordNet sense-based features on the performance of supervised SA 2.", "labels": [], "entities": []}, {"text": "Use of WordNet similarity metrics to solve the problem of features unseen in the training corpus The first points deals with evaluating sense-based features against word-based features.", "labels": [], "entities": []}, {"text": "The second issue that we address is in fact an opportunity to improve the performance of SA that opens up because of the choice of sense space.", "labels": [], "entities": [{"text": "SA", "start_pos": 89, "end_pos": 91, "type": "TASK", "confidence": 0.9447793364524841}]}, {"text": "Since sense-based features prove to generate superior sentiment classifiers, we get an opportunity to mitigate unknown 1081 synsets in the test corpus by replacing them with known synsets in the training corpus.", "labels": [], "entities": []}, {"text": "Note that such replacement is not possible if word-based representation were used as it is not feasible to make such a large number of similarity comparisons.", "labels": [], "entities": []}, {"text": "We use the corpus by that consists of travel domain reviews marked as positive or negative at the document level.", "labels": [], "entities": []}, {"text": "Our experiments on studying the impact of Wordnet sense-based features deal with variants of this corpus manually or automatically annotated with senses.", "labels": [], "entities": []}, {"text": "Besides showing the overall impact, we perform a POS-wise analysis of the benefit to SA.", "labels": [], "entities": [{"text": "POS-wise", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.9712905287742615}]}, {"text": "In addition, we compare the effect of varying training samples on a sentiment classifier developed using word based features and sense based features.", "labels": [], "entities": []}, {"text": "Through empirical evidence, we also show that disambiguating some words in a document also provides a better accuracy as compared to not disambiguating any words.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 109, "end_pos": 117, "type": "METRIC", "confidence": 0.9991464614868164}]}, {"text": "These four sets of experiments highlight our hypothesis that WordNet senses are better features as compared to words.", "labels": [], "entities": []}, {"text": "Wordnet sense-based space allows us to mitigate unknown features in the test corpus.", "labels": [], "entities": [{"text": "Wordnet sense-based space", "start_pos": 0, "end_pos": 25, "type": "DATASET", "confidence": 0.845347265402476}]}, {"text": "Our synset replacement algorithm uses Wordnet similarity-based metrics which replace an unknown synset in the test corpus with the closest approximation in the training corpus.", "labels": [], "entities": [{"text": "synset replacement", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.8036739230155945}]}, {"text": "Our results show that such a replacement benefits the performance of SA.", "labels": [], "entities": [{"text": "SA", "start_pos": 69, "end_pos": 71, "type": "TASK", "confidence": 0.6617367267608643}]}, {"text": "The roadmap for the rest of the paper is as follows: Existing related work in SA and the differentiating aspects of our work are explained in section 2 Section 3 describes the sense-based features that we use for this work.", "labels": [], "entities": []}, {"text": "We explain the similaritybased replacement technique using WordNet synsets in section 4.", "labels": [], "entities": [{"text": "similaritybased replacement", "start_pos": 15, "end_pos": 42, "type": "TASK", "confidence": 0.6513254642486572}]}, {"text": "Our experiments have been described in section 5.", "labels": [], "entities": []}, {"text": "In section 6, we present our results and related discussions.", "labels": [], "entities": []}, {"text": "Section 7 analyzes some of the causes for erroneous classification.", "labels": [], "entities": []}, {"text": "Finally, section 8 concludes the paper and points to future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "We describe the variants of the corpus generated and the experiments in this section.", "labels": [], "entities": []}, {"text": "The experiments are performed using C-SVM (linear kernel with default parameters 1 ) available as apart of LibSVM 2 package.", "labels": [], "entities": []}, {"text": "We choose to use SVM since it performs the best for sentiment classification ().", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 52, "end_pos": 76, "type": "TASK", "confidence": 0.9607277512550354}]}, {"text": "All results reported are average of five-fold cross-validation accuracies.", "labels": [], "entities": []}, {"text": "To conduct experiments on words as features, we first perform stop-word removal.", "labels": [], "entities": [{"text": "stop-word removal", "start_pos": 62, "end_pos": 79, "type": "TASK", "confidence": 0.7870955765247345}]}, {"text": "The words are not stemmed since stemming is known to be detrimental to sentiment classification (  To evaluate the result, we use accuracy, F-score, recall and precision as the metrics.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 71, "end_pos": 95, "type": "TASK", "confidence": 0.9375225901603699}, {"text": "accuracy", "start_pos": 130, "end_pos": 138, "type": "METRIC", "confidence": 0.9990915060043335}, {"text": "F-score", "start_pos": 140, "end_pos": 147, "type": "METRIC", "confidence": 0.9942130446434021}, {"text": "recall", "start_pos": 149, "end_pos": 155, "type": "METRIC", "confidence": 0.9990180730819702}, {"text": "precision", "start_pos": 160, "end_pos": 169, "type": "METRIC", "confidence": 0.9986690282821655}]}, {"text": "Classification accuracy defines the ratio of the number of true instances to the total number of instances.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.8863444924354553}]}, {"text": "Recall is calculated as a ratio of the true instances found to the total number of false positives and true positives.", "labels": [], "entities": [{"text": "Recall", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9876149892807007}]}, {"text": "Precision is defined as the number of true instances divided by number of true positives and false negatives.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9727221727371216}]}, {"text": "Positive Precision (PP) and Positive Recall (PR) are precision and recall for positive documents while Negative Precision (NP) and Negative Recall (NR) are precision and recall for negative documents.", "labels": [], "entities": [{"text": "Positive Recall (PR)", "start_pos": 28, "end_pos": 48, "type": "METRIC", "confidence": 0.6795892357826233}, {"text": "precision", "start_pos": 53, "end_pos": 62, "type": "METRIC", "confidence": 0.9989632368087769}, {"text": "recall", "start_pos": 67, "end_pos": 73, "type": "METRIC", "confidence": 0.9969984292984009}, {"text": "precision", "start_pos": 156, "end_pos": 165, "type": "METRIC", "confidence": 0.9990378618240356}, {"text": "recall", "start_pos": 170, "end_pos": 176, "type": "METRIC", "confidence": 0.9970952272415161}]}, {"text": "F-score is the weighted precision-recall score.", "labels": [], "entities": [{"text": "F-score", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.9828517436981201}, {"text": "precision-recall score", "start_pos": 24, "end_pos": 46, "type": "METRIC", "confidence": 0.9712342023849487}]}, {"text": "shows results of classification for different feature representations.", "labels": [], "entities": []}, {"text": "The baseline for our results is the unigram bag-of-words model (Baseline).", "labels": [], "entities": []}, {"text": "An improvement of 4.2% is observed in the accuracy of sentiment prediction when manually annotated sense-based features (M) are used in place of word-based features (Words).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.9992524981498718}, {"text": "sentiment prediction", "start_pos": 54, "end_pos": 74, "type": "TASK", "confidence": 0.9433430433273315}]}, {"text": "The precision of both the classes using features based on semantic space is also better than one based on lexeme space.", "labels": [], "entities": [{"text": "precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9992714524269104}]}], "tableCaptions": [{"text": " Table 1: Annotation Statistics for IWSD; P-Precision,R- Recall", "labels": [], "entities": [{"text": "IWSD", "start_pos": 36, "end_pos": 40, "type": "TASK", "confidence": 0.368960440158844}, {"text": "P-Precision,R- Recall", "start_pos": 42, "end_pos": 63, "type": "METRIC", "confidence": 0.8701169888178507}]}, {"text": " Table 2: Classification Results; PF-Positive F-score(%), NF-Negative F-score (%), PP-Positive Precision (%), NP- Negative Precision (%), PR-Positive Recall (%), NR-Negative Recall (%)", "labels": [], "entities": [{"text": "PF-Positive F-score", "start_pos": 34, "end_pos": 53, "type": "METRIC", "confidence": 0.678445428609848}, {"text": "NF-Negative F-score", "start_pos": 58, "end_pos": 77, "type": "METRIC", "confidence": 0.7111502885818481}, {"text": "PR-Positive Recall", "start_pos": 138, "end_pos": 156, "type": "METRIC", "confidence": 0.8913783133029938}, {"text": "NR-Negative Recall", "start_pos": 162, "end_pos": 180, "type": "METRIC", "confidence": 0.6107967495918274}]}, {"text": " Table 3: POS-wise F-score for sense (M) and Words;PF- Positive F-score(%), NF-Negative F-score (%)", "labels": [], "entities": [{"text": "POS-wise F-score", "start_pos": 10, "end_pos": 26, "type": "METRIC", "confidence": 0.7618512213230133}, {"text": "PF- Positive F-score", "start_pos": 51, "end_pos": 71, "type": "METRIC", "confidence": 0.925631046295166}, {"text": "NF-Negative F-score", "start_pos": 76, "end_pos": 95, "type": "METRIC", "confidence": 0.8082248568534851}]}, {"text": " Table 5: Similarity Metric Analysis using different similarity metrics with synsets and a combinations of synset and  words;PF-Positive F-score(%), NF-Negative F-score (%), PP-Positive Precision (%), NP-Negative Precision (%), PR- Positive Recall (%), NR-Negative Recall (%)", "labels": [], "entities": [{"text": "Similarity Metric Analysis", "start_pos": 10, "end_pos": 36, "type": "TASK", "confidence": 0.8920681277910868}, {"text": "PR- Positive Recall", "start_pos": 228, "end_pos": 247, "type": "METRIC", "confidence": 0.771313339471817}]}, {"text": " Table 6: Comparison of top information gain-based features of manually annotated corpora and automatically anno- tated corpora", "labels": [], "entities": []}]}