{"title": [{"text": "A Fast Re-scoring Strategy to Capture Long-Distance Dependencies", "labels": [], "entities": [{"text": "Capture Long-Distance Dependencies", "start_pos": 30, "end_pos": 64, "type": "TASK", "confidence": 0.8856656750043234}]}], "abstractContent": [{"text": "A re-scoring strategy is proposed that makes it feasible to capture more long-distance dependencies in the natural language.", "labels": [], "entities": []}, {"text": "Two pass strategies have become popular in a number of recognition tasks such as ASR (au-tomatic speech recognition), MT (machine translation) and OCR (optical character recognition).", "labels": [], "entities": [{"text": "ASR (au-tomatic speech recognition)", "start_pos": 81, "end_pos": 116, "type": "TASK", "confidence": 0.6296203434467316}, {"text": "MT (machine translation", "start_pos": 118, "end_pos": 141, "type": "TASK", "confidence": 0.7130772769451141}, {"text": "OCR (optical character recognition)", "start_pos": 147, "end_pos": 182, "type": "TASK", "confidence": 0.6079386919736862}]}, {"text": "The first pass typically applies a weak language model (n-grams) to a lattice and the second pass applies a stronger language model to N best lists.", "labels": [], "entities": []}, {"text": "The stronger language model is intended to capture more long-distance dependencies.", "labels": [], "entities": []}, {"text": "The proposed method uses RNN-LM (recurrent neural network language model), which is along span LM, to re-score word lattices in the second pass.", "labels": [], "entities": []}, {"text": "A hill climbing method (iterative decoding) is proposed to search over islands of confusability in the word lattice.", "labels": [], "entities": []}, {"text": "An evaluation based on Broadcast News shows speedups of 20 over basic N best re-scoring, and word error rate reduction of 8% (relative) on a highly competitive setup.", "labels": [], "entities": [{"text": "Broadcast News", "start_pos": 23, "end_pos": 37, "type": "DATASET", "confidence": 0.9579869508743286}, {"text": "speedups", "start_pos": 44, "end_pos": 52, "type": "METRIC", "confidence": 0.9900908470153809}, {"text": "word error rate reduction", "start_pos": 93, "end_pos": 118, "type": "METRIC", "confidence": 0.8040114641189575}]}], "introductionContent": [{"text": "Statistical Language Models (LMs) have received considerable attention in the past few decades.", "labels": [], "entities": [{"text": "Statistical Language Models (LMs)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.835557371377945}]}, {"text": "They have proved to bean essential component in many statistical recognition systems such as ASR (automatic speech recognition), MT (machine translation) and OCR (optical character recognition).", "labels": [], "entities": [{"text": "statistical recognition", "start_pos": 53, "end_pos": 76, "type": "TASK", "confidence": 0.7989945709705353}, {"text": "ASR (automatic speech recognition", "start_pos": 93, "end_pos": 126, "type": "TASK", "confidence": 0.5980898320674897}, {"text": "MT (machine translation", "start_pos": 129, "end_pos": 152, "type": "TASK", "confidence": 0.7666377276182175}, {"text": "OCR (optical character recognition", "start_pos": 158, "end_pos": 192, "type": "TASK", "confidence": 0.5646278262138367}]}, {"text": "The task of a language model is to assign probability to any word sequence possible in the language.", "labels": [], "entities": []}, {"text": "The probability of the word sequence W \u2261 w 1 , . .", "labels": [], "entities": []}, {"text": ", w m \u2261 w m 1 is typically factored using the chain rule: In modern statistical recognition systems, an LM tends to be restricted to simple n-gram models, where the distribution of the predicted word depends on the previous (n \u2212 1) words i.e. P (w i |w i\u22121 1 ) \u2248 P (w i |w i\u22121 i\u2212n+1 ).", "labels": [], "entities": [{"text": "statistical recognition", "start_pos": 68, "end_pos": 91, "type": "TASK", "confidence": 0.7467363476753235}]}, {"text": "Noam Chomsky argued that n-grams cannot learn long-distance dependencies that span over more than n words.", "labels": [], "entities": []}, {"text": "While that might seem obvious in retrospect, there was a lot of excitement at the time over the Shannon-McMillanBreiman Theorem which was interpreted to say that, in the limit, under just a couple of minor caveats and a little bit of not-very-important fine print, n-gram statistics are sufficient to capture all the information in a string (such as an English sentence).", "labels": [], "entities": []}, {"text": "Chomsky realized that while that maybe true in the limit, n-grams are far from the most parsimonious representation of many linguistic facts.", "labels": [], "entities": []}, {"text": "Ina practical system, we will have to truncate n-grams at some (small) fixed n (such as trigrams or perhaps 5-grams).", "labels": [], "entities": []}, {"text": "Truncated n-gram systems can capture many agreement facts, but not all.", "labels": [], "entities": []}, {"text": "By long-distance dependencies, we mean facts like agreement and collocations that can span over many words.", "labels": [], "entities": []}, {"text": "With increasing order of n-gram models we can, in theory, capture more regularities in the language.", "labels": [], "entities": []}, {"text": "In addition, if we can move to more general models then we could hope to capture more, as well.", "labels": [], "entities": []}, {"text": "However, due to data sparsity, it is hard to estimate a robust n-gram distribution for large values of n ( say, n > 10) using the conventional Maximum Likelihood techniques, unless a more robust technique is employed for modeling which generalizes well on unseen events.", "labels": [], "entities": []}, {"text": "Some of these well known long span / complex language models which have shown to perform very well on many speech tasks include: structured language model;, latent semantic analysis language model), topic mixture language models), whole sentence exponential language models), feedforward neural networks (), recurrent neural network language models (), among many others.", "labels": [], "entities": []}, {"text": "Although better modeling techniques can now capture longer dependencies in a language, their incorporation in decoders of speech recognition or machine translation systems becomes computationally challenging.", "labels": [], "entities": [{"text": "speech recognition or machine translation", "start_pos": 122, "end_pos": 163, "type": "TASK", "confidence": 0.6757065892219544}]}, {"text": "Due to the prohibitive increase in the search space of sentence hypotheses (or longer length word sub sequences), it becomes challenging to use along span language model in the first pass decoding.", "labels": [], "entities": []}, {"text": "A word graph (word lattices for speech recognition systems and hypergraphs for machine translation systems), encoding exponential number of hypotheses is hence outputted at the first pass output on which a sophisticated and complex language model is deployed for re-scoring.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 32, "end_pos": 50, "type": "TASK", "confidence": 0.7194591462612152}, {"text": "machine translation", "start_pos": 79, "end_pos": 98, "type": "TASK", "confidence": 0.7670677602291107}]}, {"text": "However, sometimes even re-scoring of this refined search space can be computationally expensive due to explosion of state space.", "labels": [], "entities": []}, {"text": "Previously, we showed in (Deoras et al., 2011) how to tackle the problem of incorporating long span information during decoding in speech recognition systems by variationaly approximating, pp.", "labels": [], "entities": [{"text": "variationaly approximating", "start_pos": 161, "end_pos": 187, "type": "TASK", "confidence": 0.7515958845615387}]}, {"text": "462) the long span language model by a tractable substitute such that this substitute model comes closest to the long span model (closest in terms of).", "labels": [], "entities": []}, {"text": "The tractable substitute was then used directly in the first pass speech recognition systems.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 66, "end_pos": 84, "type": "TASK", "confidence": 0.6780071556568146}]}, {"text": "In this paper we propose an approach that keeps the model intact but approximates the search space instead (which can become intractable to handle especially under along span model), thus enabling the use of full blown model for re-scoring.With this approach, we can achieve full lattice re-scoring with a complex model, at a cost more than 20 times less than of a naive brute force approach that is commonly used today.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows: We discuss a particular form of long span language model in Sec.", "labels": [], "entities": []}, {"text": "3 we discuss two standard re-scoring techniques and then describe and demonstrate our proposed technique in Sec.", "labels": [], "entities": []}, {"text": "4. We present experimental results in Sec.", "labels": [], "entities": [{"text": "Sec.", "start_pos": 38, "end_pos": 42, "type": "TASK", "confidence": 0.7589907944202423}]}, {"text": "5 followed by conclusions and some remarks in Sec.", "labels": [], "entities": []}], "datasetContent": [{"text": "We performed recognition on the Broadcast News (BN) dev04f, rt03 and rt04 task using the stateof-the-art acoustic models trained on the English Broadcast News (BN) corpus (430 hours of audio) provided to us by IBM (.", "labels": [], "entities": [{"text": "recognition", "start_pos": 13, "end_pos": 24, "type": "TASK", "confidence": 0.9527234435081482}, {"text": "Broadcast News (BN) dev04f", "start_pos": 32, "end_pos": 58, "type": "DATASET", "confidence": 0.6804110209147135}, {"text": "English Broadcast News (BN) corpus", "start_pos": 136, "end_pos": 170, "type": "DATASET", "confidence": 0.781188598700932}]}, {"text": "IBM also provided us its state-of-the-art speech recognizer, Attila () and two Kneser-Ney smoothed backoff n-gram LMs containing 4.7M ngrams (n \u2264 4) and 54M n-grams (n \u2264 4), both trained on 400M word tokens.", "labels": [], "entities": [{"text": "speech recognizer", "start_pos": 42, "end_pos": 59, "type": "TASK", "confidence": 0.704749196767807}]}, {"text": "We will refer to them as KN:BN-Small and KN:BN-Big respectively.", "labels": [], "entities": []}, {"text": "We refer readers to) for more details about the recognizer and corpora used for training the models.", "labels": [], "entities": []}, {"text": "We trained two RNN based language modelsthe first one, denoted further as RNN-limited, was trained on a subset of the training data (58M tokens).", "labels": [], "entities": []}, {"text": "It used 400 neurons in the hidden layer.", "labels": [], "entities": []}, {"text": "The second model, denoted as RNN-all, was trained on all of the training data (400M tokens), but due to the computational complexity issues, we had to restrict its hidden layer size to 320 neurons.", "labels": [], "entities": []}, {"text": "We followed IBM's multi-pass decoding recipe using KN:BN-Small in the first pass followed by either N best list re-scoring or word lattice re-scoring using bigger and better models.", "labels": [], "entities": [{"text": "BN-Small", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.8790659308433533}]}, {"text": "For the purpose of re-scoring, we combined all the relevant statistical models in one unified log linear framework reminiscent of work by.", "labels": [], "entities": []}, {"text": "We, however, trained the model weights by optimizing expected WER rather than 1-best loss as described in.", "labels": [], "entities": [{"text": "WER", "start_pos": 62, "end_pos": 65, "type": "METRIC", "confidence": 0.9906355738639832}]}, {"text": "Training was done on N best lists of size 2K.", "labels": [], "entities": []}, {"text": "We will refer to the log linear com- We used two sets for decoding: rt03+dev04f set was used as a development set while rt04 was used as a blind set for the purpose of evaluating the performance of long span RNN models using the proposed approach.", "labels": [], "entities": []}, {"text": "We made use of OpenFst C++ libraries () for manipulating lattice graphs and generating N best lists.", "labels": [], "entities": []}, {"text": "Due to the presence of hesitation tokens in reference transcripts and the need to access the silence/pause tokens for penalizing short sentences, we treated these tokens as regular words before extracting sentence hypotheses.", "labels": [], "entities": []}, {"text": "This, and poorly segmented nature of the test corpora, led to huge enumeration of sentence hypotheses.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: The starting LM is a weak n-gram LM (KN:BN- Small) and the re-scoring LM is a much stronger but n- gram LM (KN:BN-Big). The baseline WER in this case  is 12% and the optimal performance by the re-scoring LM  is 11.0%. The proposed method outperforms N best list  approach, in terms of search efforts, obtaining optimal  WER.", "labels": [], "entities": [{"text": "WER", "start_pos": 143, "end_pos": 146, "type": "METRIC", "confidence": 0.9866278767585754}, {"text": "WER", "start_pos": 330, "end_pos": 333, "type": "METRIC", "confidence": 0.8188313841819763}]}]}