{"title": [{"text": "A Simple Word Trigger Method for Social Tag Suggestion", "labels": [], "entities": [{"text": "Word Trigger", "start_pos": 9, "end_pos": 21, "type": "TASK", "confidence": 0.7218138873577118}, {"text": "Social Tag Suggestion", "start_pos": 33, "end_pos": 54, "type": "TASK", "confidence": 0.8468562761942545}]}], "abstractContent": [{"text": "It is popular for users in Web 2.0 era to freely annotate online resources with tags.", "labels": [], "entities": []}, {"text": "To ease the annotation process, it has been great interest in automatic tag suggestion.", "labels": [], "entities": [{"text": "automatic tag suggestion", "start_pos": 62, "end_pos": 86, "type": "TASK", "confidence": 0.6662894785404205}]}, {"text": "We propose a method to suggest tags according to the text description of a resource.", "labels": [], "entities": []}, {"text": "By considering both the description and tags of a given resource as summaries to the resource written in two languages, we adopt word alignment models in statistical machine translation to bridge their vocabulary gap.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 129, "end_pos": 143, "type": "TASK", "confidence": 0.7539324164390564}, {"text": "statistical machine translation", "start_pos": 154, "end_pos": 185, "type": "TASK", "confidence": 0.6261695822079977}]}, {"text": "Based on the translation probabilities between the words in descriptions and the tags estimated on a large set of description-tags pairs, we build a word trigger method (WTM) to suggest tags according to the words in a resource description.", "labels": [], "entities": []}, {"text": "Experiments on real world datasets show that WTM is effective and robust compared with other methods.", "labels": [], "entities": [{"text": "WTM", "start_pos": 45, "end_pos": 48, "type": "TASK", "confidence": 0.9116719365119934}]}, {"text": "Moreover, WTM is relatively simple and efficient, which is practical for Web applications.", "labels": [], "entities": [{"text": "WTM", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.6943432092666626}]}], "introductionContent": [{"text": "In Web 2.0, Web users often use tags to collect and share online resources such as Web pages, photos, videos, movies and books.", "labels": [], "entities": []}, {"text": "shows a book entry annotated with multiple tags by users . On the top of we list the title and a short introduction of the novel \"The Count of Monte Cristo\".", "labels": [], "entities": []}, {"text": "The bottom half of shows the annotated tags, each of which is followed by a number in bracket, the total number of users who The original record is obtained from the book review website Douban (www.douban.com) in Chinese.", "labels": [], "entities": []}, {"text": "Here we translate it to English for comprehension.", "labels": [], "entities": []}, {"text": "use the tag to annotate this book.", "labels": [], "entities": []}, {"text": "Since the tags of a resource are annotated collaboratively by multiple users, we also name these tags as social tags.", "labels": [], "entities": []}, {"text": "For a resource, we refer to the additional information, such as the title and introduction of a book, as description, and the user-annotated social tags as annotation.: An example of social tagging.", "labels": [], "entities": []}, {"text": "The number in the bracket after each tag is the total count of users that annotate the tag on this book.", "labels": [], "entities": []}, {"text": "Social tags concisely indicate the main content of the given resource, and potentially reflect user interests.", "labels": [], "entities": []}, {"text": "Social tagging has thus been widely studied and successfully applied in recommender systems (, trend detection and tracking), personalization), advertising (), etc.", "labels": [], "entities": [{"text": "Social tagging", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7437158823013306}, {"text": "trend detection and tracking", "start_pos": 95, "end_pos": 123, "type": "TASK", "confidence": 0.8387197703123093}]}], "datasetContent": [{"text": "Datasets In our experiments, we select two real world datasets which are of diverse properties to evaluate our methods.", "labels": [], "entities": []}, {"text": "In we show the detailed statistical information of the two datasets.: Statistical information of two datasets.", "labels": [], "entities": []}, {"text": "R, W , T , \u00af N wand \u00af N tare the number of resources, the vocabulary of descriptions, the vocabulary of tags, the average number of words in each description and the average number of tags in each resource, respectively.", "labels": [], "entities": []}, {"text": "The first dataset, denoted as BOOK, is obtained from a popular Chinese book review website www.", "labels": [], "entities": [{"text": "BOOK", "start_pos": 30, "end_pos": 34, "type": "METRIC", "confidence": 0.9951139092445374}]}, {"text": "douban.com, which contains the descriptions of books and the tags collaboratively annotated by users.", "labels": [], "entities": [{"text": "douban.com", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.9158946871757507}]}, {"text": "The second dataset, denoted as BIBTEX, is obtained from an English online bibliography website www.bibsonomy.org 2 . The dataset contains the descriptions for academic papers (including the title and note for each paper) and the tags annotated by users.", "labels": [], "entities": [{"text": "BIBTEX", "start_pos": 31, "end_pos": 37, "type": "METRIC", "confidence": 0.9897937774658203}]}, {"text": "As shown in, the average length of descriptions in the BIBTEX dataset is much shorter than the BOOK dataset.", "labels": [], "entities": [{"text": "BIBTEX dataset", "start_pos": 55, "end_pos": 69, "type": "DATASET", "confidence": 0.9563590586185455}, {"text": "BOOK dataset", "start_pos": 95, "end_pos": 107, "type": "DATASET", "confidence": 0.8125759363174438}]}, {"text": "Moreover, the BIBTEX dataset does not provide how many times each tag is annotated to a resource.", "labels": [], "entities": [{"text": "BIBTEX dataset", "start_pos": 14, "end_pos": 28, "type": "DATASET", "confidence": 0.8819020688533783}]}, {"text": "We use precision, recall and F-measure to evaluate the performance of tag suggestion methods.", "labels": [], "entities": [{"text": "precision", "start_pos": 7, "end_pos": 16, "type": "METRIC", "confidence": 0.9996050000190735}, {"text": "recall", "start_pos": 18, "end_pos": 24, "type": "METRIC", "confidence": 0.9995161294937134}, {"text": "F-measure", "start_pos": 29, "end_pos": 38, "type": "METRIC", "confidence": 0.9994088411331177}]}, {"text": "For a resource, we denote the original tags (gold standard) as Ta , the suggested tags as T s , and the correctly suggested tags as T s \u2229 Ta . Precision, recall and F-measure are defined as The final evaluation scores are computed by microaveraging (i.e., averaging on resources of test set).", "labels": [], "entities": [{"text": "Precision", "start_pos": 143, "end_pos": 152, "type": "METRIC", "confidence": 0.9791316986083984}, {"text": "recall", "start_pos": 154, "end_pos": 160, "type": "METRIC", "confidence": 0.997684121131897}, {"text": "F-measure", "start_pos": 165, "end_pos": 174, "type": "METRIC", "confidence": 0.9961854815483093}]}, {"text": "We perform 5-fold cross validation for each method on all two datasets.", "labels": [], "entities": []}, {"text": "In experiments, the number of suggested tags M ranges from 1 to 10.", "labels": [], "entities": []}, {"text": "\u2022 WTM consistently performs the best on both datasets.", "labels": [], "entities": [{"text": "WTM", "start_pos": 2, "end_pos": 5, "type": "DATASET", "confidence": 0.6781851053237915}]}, {"text": "This indicates that WTM is robust and effective for tag suggestion.", "labels": [], "entities": [{"text": "WTM", "start_pos": 20, "end_pos": 23, "type": "DATASET", "confidence": 0.5622679591178894}, {"text": "tag suggestion", "start_pos": 52, "end_pos": 66, "type": "TASK", "confidence": 0.9040333926677704}]}, {"text": "\u2022 The advantage of WTM is more significant on the BOOK dataset.", "labels": [], "entities": [{"text": "WTM", "start_pos": 19, "end_pos": 22, "type": "DATASET", "confidence": 0.38259103894233704}, {"text": "BOOK dataset", "start_pos": 50, "end_pos": 62, "type": "DATASET", "confidence": 0.9106870591640472}]}, {"text": "The reason is that WTM can take a good advantage of annotation count information of tags compared to other methods.", "labels": [], "entities": [{"text": "WTM", "start_pos": 19, "end_pos": 22, "type": "DATASET", "confidence": 0.5411514639854431}]}, {"text": "\u2022 The average length of resource descriptions is short in the BIBTEX dataset, which makes it difficult to determine the trigger powers of words.", "labels": [], "entities": [{"text": "BIBTEX dataset", "start_pos": 62, "end_pos": 76, "type": "DATASET", "confidence": 0.9351372122764587}]}, {"text": "But even on the BIBTEX dataset with no count information of tags, WTM still outperforms other methods especially when recommending first several tags.", "labels": [], "entities": [{"text": "BIBTEX dataset", "start_pos": 16, "end_pos": 30, "type": "DATASET", "confidence": 0.9273810386657715}]}, {"text": "To further demonstrate the performance of WTM and other baseline methods, in we show the 5 GIZA++ is freely available on code.google.com/p/ giza-pp.", "labels": [], "entities": [{"text": "WTM", "start_pos": 42, "end_pos": 45, "type": "DATASET", "confidence": 0.5971018671989441}]}, {"text": "The toolkit is widely used for word alignment in SMT.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 31, "end_pos": 45, "type": "TASK", "confidence": 0.8208698034286499}, {"text": "SMT", "start_pos": 49, "end_pos": 52, "type": "TASK", "confidence": 0.8766009211540222}]}, {"text": "In this paper, we use the default setting of parameters for training.", "labels": [], "entities": []}, {"text": "An Example In we show top 10 tags suggested by NB, CRM, TAM and WTM for the book in  the results of kNN because the tags suggested by kNN are totally unrelated to the book due to the insufficient finding of nearest neighbors.", "labels": [], "entities": [{"text": "TAM", "start_pos": 56, "end_pos": 59, "type": "METRIC", "confidence": 0.7241906523704529}]}, {"text": "From, we observe that NB, CRM and TAM, as generative models, tend to suggest general tags such as \"novel\", \"literature\", \"classic\" and \"France\", and fail in suggesting specific tags such as \"Alexandre Dumas\" and \"Count of Monte Cristo\".", "labels": [], "entities": [{"text": "TAM", "start_pos": 34, "end_pos": 37, "type": "METRIC", "confidence": 0.8778654336929321}]}, {"text": "On the contrary, WTM succeeds in suggesting both general and specific tags related to the book.", "labels": [], "entities": [{"text": "WTM", "start_pos": 17, "end_pos": 20, "type": "DATASET", "confidence": 0.8062177896499634}]}, {"text": "In, we list four important words (using TF-IRF was weighting metric) of the description and their corresponding tags with the highest translation probabilities.", "labels": [], "entities": []}, {"text": "The values in brackets are the probability of tag t given word w, Pr(t|w).", "labels": [], "entities": [{"text": "Pr", "start_pos": 66, "end_pos": 68, "type": "METRIC", "confidence": 0.9717540740966797}]}, {"text": "For each word, we eliminated the tags with the probability less than 0.1.", "labels": [], "entities": []}, {"text": "We can see that the translation probabilities can map the words in descriptions to their semantically corresponding tags in annotations.: Four important words (in bold face) in the book description in and their corresponding tags with the highest translation probabilities.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Statistical information of two datasets. R,  W , T , \u00af  N w and \u00af  N t are the number of resources, the  vocabulary of descriptions, the vocabulary of tags,  the average number of words in each description  and the average number of tags in each resource,  respectively.", "labels": [], "entities": []}, {"text": " Table 1. The number in bracket after  the name of each method is the count of correctly  suggested tags. The correctly suggested tags are  marked in bold face. We select not to show", "labels": [], "entities": []}, {"text": " Table 3: Comparing results of NB, kNN, CRM,  TAM and WTM on BOOK dataset when suggesting  M = 3 tags.", "labels": [], "entities": [{"text": "TAM", "start_pos": 46, "end_pos": 49, "type": "METRIC", "confidence": 0.9619024395942688}, {"text": "WTM", "start_pos": 54, "end_pos": 57, "type": "METRIC", "confidence": 0.7199430465698242}, {"text": "BOOK dataset", "start_pos": 61, "end_pos": 73, "type": "DATASET", "confidence": 0.7250999957323074}]}, {"text": " Table 6: Evaluation results for different tag weight- ing types when M = 3 on the BOOK dataset.", "labels": [], "entities": [{"text": "M", "start_pos": 70, "end_pos": 71, "type": "METRIC", "confidence": 0.9318095445632935}, {"text": "BOOK dataset", "start_pos": 83, "end_pos": 95, "type": "DATASET", "confidence": 0.8906455338001251}]}, {"text": " Table 7: Evaluation results for different methods for  computing word trigger powers when M = 3 on the  BOOK dataset.", "labels": [], "entities": [{"text": "BOOK dataset", "start_pos": 105, "end_pos": 117, "type": "DATASET", "confidence": 0.9177874326705933}]}, {"text": " Table 8: The evaluation results of EWTM with dif- ferent methods for computing word trigger powers  when M = 3 on the BOOK dataset.", "labels": [], "entities": [{"text": "EWTM", "start_pos": 36, "end_pos": 40, "type": "DATASET", "confidence": 0.8623643517494202}, {"text": "BOOK dataset", "start_pos": 119, "end_pos": 131, "type": "DATASET", "confidence": 0.927930474281311}]}]}