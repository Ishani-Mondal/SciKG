{"title": [{"text": "Identifying Relations for Open Information Extraction", "labels": [], "entities": [{"text": "Identifying Relations", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.9059795439243317}, {"text": "Open Information Extraction", "start_pos": 26, "end_pos": 53, "type": "TASK", "confidence": 0.638648529847463}]}], "abstractContent": [{"text": "Open Information Extraction (IE) is the task of extracting assertions from massive corpora without requiring a pre-specified vocabulary.", "labels": [], "entities": [{"text": "Open Information Extraction (IE)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7745557824770609}]}, {"text": "This paper shows that the output of state-of-the-art Open IE systems is rife with uninfor-mative and incoherent extractions.", "labels": [], "entities": []}, {"text": "To overcome these problems, we introduce two simple syntactic and lexical constraints on binary relations expressed by verbs.", "labels": [], "entities": []}, {"text": "We implemented the constraints in the REVERB Open IE system, which more than doubles the area under the precision-recall curve relative to previous extractors such as TEXTRUNNER and WOE pos.", "labels": [], "entities": [{"text": "REVERB", "start_pos": 38, "end_pos": 44, "type": "METRIC", "confidence": 0.9275025725364685}, {"text": "precision-recall", "start_pos": 104, "end_pos": 120, "type": "METRIC", "confidence": 0.9964556694030762}, {"text": "TEXTRUNNER", "start_pos": 167, "end_pos": 177, "type": "METRIC", "confidence": 0.8464553952217102}]}, {"text": "More than 30% of REVERB's extractions are at precision 0.8 or higher-compared to virtually none for earlier systems.", "labels": [], "entities": [{"text": "REVERB", "start_pos": 17, "end_pos": 23, "type": "METRIC", "confidence": 0.7795639634132385}, {"text": "precision", "start_pos": 45, "end_pos": 54, "type": "METRIC", "confidence": 0.9954511523246765}]}, {"text": "The paper concludes with a detailed analysis of REVERB's errors, suggesting directions for future work.", "labels": [], "entities": [{"text": "REVERB's errors", "start_pos": 48, "end_pos": 63, "type": "METRIC", "confidence": 0.6843887368837992}]}], "introductionContent": [], "datasetContent": [{"text": "We compare REVERB to the following systems: \u2022 REVERB \u00aclex -The REVERB system described in the previous section, but without the lexical constraint.", "labels": [], "entities": [{"text": "REVERB", "start_pos": 11, "end_pos": 17, "type": "METRIC", "confidence": 0.8998523354530334}, {"text": "REVERB", "start_pos": 46, "end_pos": 52, "type": "METRIC", "confidence": 0.99620121717453}, {"text": "REVERB", "start_pos": 63, "end_pos": 69, "type": "METRIC", "confidence": 0.9458426833152771}]}, {"text": "REVERB \u00aclex uses the same confidence function as REVERB.", "labels": [], "entities": [{"text": "REVERB", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9754987955093384}, {"text": "REVERB", "start_pos": 49, "end_pos": 55, "type": "METRIC", "confidence": 0.9448288083076477}]}, {"text": "\u2022 TEXTRUNNER -Banko and Etzioni's 2008 extractor, which uses a second order linear-chain CRF trained on extractions heuristically generated from the Penn Treebank.", "labels": [], "entities": [{"text": "TEXTRUNNER", "start_pos": 2, "end_pos": 12, "type": "METRIC", "confidence": 0.9827002286911011}, {"text": "Penn Treebank", "start_pos": 149, "end_pos": 162, "type": "DATASET", "confidence": 0.9951386451721191}]}, {"text": "TEXTRUNNER uses shallow linguistic features in its CRF, which come from the same POS tagger and NPchunker that REVERB uses.", "labels": [], "entities": [{"text": "TEXTRUNNER", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.8796151280403137}]}, {"text": "\u2022 TEXTRUNNER-R -Our modification to TEXTRUNNER, which uses the same extraction code, but with a model of relations trained on REVERB extractions.", "labels": [], "entities": [{"text": "TEXTRUNNER-R", "start_pos": 2, "end_pos": 14, "type": "METRIC", "confidence": 0.8910276889801025}, {"text": "REVERB extractions", "start_pos": 126, "end_pos": 144, "type": "TASK", "confidence": 0.8264875411987305}]}, {"text": "\u2022 WOE pos -Wu and Weld's modification to TEXTRUNNER, which uses a model of relations learned from extractions heuristically generated from Wikipedia.", "labels": [], "entities": [{"text": "TEXTRUNNER", "start_pos": 41, "end_pos": 51, "type": "METRIC", "confidence": 0.8113570809364319}]}, {"text": "\u2022 WOE parse -Wu and Weld's parser-based extractor, which uses a large dictionary of dependency path patterns learned from heuristic extractions generated from Wikipedia.", "labels": [], "entities": [{"text": "WOE parse", "start_pos": 2, "end_pos": 11, "type": "TASK", "confidence": 0.6098949015140533}]}, {"text": "Each system is given a set of sentences as input, and returns a set of binary extractions as output.", "labels": [], "entities": []}, {"text": "We created a test set of 500 sentences sampled from the Web, using Yahoo's random link service.", "labels": [], "entities": []}, {"text": "3 After run- ning each extractor over the input sentences, two human judges independently evaluated each extraction as corrector incorrect.", "labels": [], "entities": []}, {"text": "The judges reached agreement on 86% of the extractions, with an agreement score of \u03ba = 0.68.", "labels": [], "entities": [{"text": "agreement", "start_pos": 64, "end_pos": 73, "type": "METRIC", "confidence": 0.9881717562675476}]}, {"text": "We report results on the subset of the data where the two judges concur.", "labels": [], "entities": []}, {"text": "The judges labeled uninformative extractions conservatively.", "labels": [], "entities": []}, {"text": "That is, if critical information was dropped from the relation phrase but included in the second argument, it is labeled correct.", "labels": [], "entities": []}, {"text": "For example, both the extractions (Ackerman, is a professor of, biology) and (Ackerman, is, a professor of biology) are considered correct.", "labels": [], "entities": []}, {"text": "Each system returns confidence scores for its extractions.", "labels": [], "entities": []}, {"text": "For a given threshold, we can measure the precision and recall of the output.", "labels": [], "entities": [{"text": "precision", "start_pos": 42, "end_pos": 51, "type": "METRIC", "confidence": 0.9996110796928406}, {"text": "recall", "start_pos": 56, "end_pos": 62, "type": "METRIC", "confidence": 0.999014139175415}]}, {"text": "Precision is the fraction of returned extractions that are correct.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9898381233215332}]}, {"text": "Recall is the fraction of correct extractions in 1541 the corpus that are returned.", "labels": [], "entities": [{"text": "Recall", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9712533950805664}]}, {"text": "We use the total number of extractions labeled as correct by the judges as our measure of recall for the corpus.", "labels": [], "entities": [{"text": "recall", "start_pos": 90, "end_pos": 96, "type": "METRIC", "confidence": 0.9996210336685181}]}, {"text": "In order to avoid double-counting, we treat extractions that differ superficially (e.g., different punctuation or dropping inessential modifiers) as a single extraction.", "labels": [], "entities": []}, {"text": "We compute a precision-recall curve by varying the confidence threshold, and then compute the area under the curve (AUC).", "labels": [], "entities": [{"text": "precision-recall", "start_pos": 13, "end_pos": 29, "type": "METRIC", "confidence": 0.9966959953308105}, {"text": "AUC", "start_pos": 116, "end_pos": 119, "type": "METRIC", "confidence": 0.978424072265625}]}, {"text": "shows the AUC of each system.", "labels": [], "entities": [{"text": "AUC", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.883208155632019}]}, {"text": "REVERB achieves an AUC that is 30% higher than WOE parse and is more than double the AUC of WOE pos or TEXTRUNNER.", "labels": [], "entities": [{"text": "REVERB", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9687946438789368}, {"text": "AUC", "start_pos": 19, "end_pos": 22, "type": "METRIC", "confidence": 0.9982187151908875}, {"text": "WOE parse", "start_pos": 47, "end_pos": 56, "type": "DATASET", "confidence": 0.5244519710540771}, {"text": "AUC", "start_pos": 85, "end_pos": 88, "type": "METRIC", "confidence": 0.9976539015769958}, {"text": "TEXTRUNNER", "start_pos": 103, "end_pos": 113, "type": "METRIC", "confidence": 0.8832232356071472}]}, {"text": "The lexical constraint provides a significant boost in performance, with REVERB achieving an AUC 23% higher than REVERB \u00aclex . REVERB proves to be a useful source of training data, with TEXTRUNNER-R having an AUC 71% higher than TEXTRUNNER and performing on par with WOE pos . From the training data, TEXTRUNNER-R was able to learn a model that predicts contiguous relation phrases, but still returned incoherent relation phrases (e.g., starting with a preposition) and overspecified relation phrases.", "labels": [], "entities": [{"text": "REVERB", "start_pos": 73, "end_pos": 79, "type": "METRIC", "confidence": 0.987421989440918}, {"text": "AUC", "start_pos": 93, "end_pos": 96, "type": "METRIC", "confidence": 0.9895943403244019}, {"text": "REVERB", "start_pos": 127, "end_pos": 133, "type": "METRIC", "confidence": 0.859733521938324}, {"text": "AUC", "start_pos": 209, "end_pos": 212, "type": "METRIC", "confidence": 0.9817246198654175}]}, {"text": "These errors are due to TEXTRUNNER-R overfitting the training data and not having access to the lexical constraint.", "labels": [], "entities": [{"text": "TEXTRUNNER-R", "start_pos": 24, "end_pos": 36, "type": "METRIC", "confidence": 0.8420608639717102}]}, {"text": "shows the precision-recall curves of the systems introduced in this paper.", "labels": [], "entities": [{"text": "precision-recall", "start_pos": 10, "end_pos": 26, "type": "METRIC", "confidence": 0.9987133741378784}]}, {"text": "TEXTRUNNER-R has much lower precision than REVERB and REVERB \u00aclex at all levels of recall.", "labels": [], "entities": [{"text": "TEXTRUNNER-R", "start_pos": 0, "end_pos": 12, "type": "METRIC", "confidence": 0.9417237639427185}, {"text": "precision", "start_pos": 28, "end_pos": 37, "type": "METRIC", "confidence": 0.9992511868476868}, {"text": "REVERB", "start_pos": 43, "end_pos": 49, "type": "METRIC", "confidence": 0.9975542426109314}, {"text": "REVERB \u00aclex", "start_pos": 54, "end_pos": 65, "type": "METRIC", "confidence": 0.9192837278048197}, {"text": "recall", "start_pos": 83, "end_pos": 89, "type": "METRIC", "confidence": 0.9965560436248779}]}, {"text": "The lexical constraint gives REVERB a boost in precision over REVERB \u00aclex , reducing overspecified extractions from 20% of REVERB \u00aclex 's output to 1% of REVERB's.", "labels": [], "entities": [{"text": "REVERB", "start_pos": 29, "end_pos": 35, "type": "METRIC", "confidence": 0.9178532958030701}, {"text": "precision", "start_pos": 47, "end_pos": 56, "type": "METRIC", "confidence": 0.9994157552719116}]}, {"text": "The lexical constraint also boosts recall over REVERB \u00aclex , since REVERB is able to find a correct relation phrase where REVERB \u00aclex finds an overspecified one.", "labels": [], "entities": [{"text": "recall", "start_pos": 35, "end_pos": 41, "type": "METRIC", "confidence": 0.9990702271461487}, {"text": "REVERB", "start_pos": 47, "end_pos": 53, "type": "METRIC", "confidence": 0.9621301889419556}]}, {"text": "shows the precision-recall curves of REVERB and the external systems.", "labels": [], "entities": [{"text": "precision-recall", "start_pos": 10, "end_pos": 26, "type": "METRIC", "confidence": 0.9989297986030579}, {"text": "REVERB", "start_pos": 37, "end_pos": 43, "type": "METRIC", "confidence": 0.963586151599884}]}, {"text": "REVERB has much higher precision than the other systems at nearly all levels of recall.", "labels": [], "entities": [{"text": "REVERB", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9725586175918579}, {"text": "precision", "start_pos": 23, "end_pos": 32, "type": "METRIC", "confidence": 0.9994843006134033}, {"text": "recall", "start_pos": 80, "end_pos": 86, "type": "METRIC", "confidence": 0.9927567839622498}]}, {"text": "In particular, more than 30% of REVERB's extractions are at precision 0.8 or higher, compared to virtually none for the other systems.", "labels": [], "entities": [{"text": "REVERB", "start_pos": 32, "end_pos": 38, "type": "METRIC", "confidence": 0.836669385433197}, {"text": "precision", "start_pos": 60, "end_pos": 69, "type": "METRIC", "confidence": 0.9964731335639954}]}, {"text": "WOE parse achieves a slightly higher recall than REVERB (0.62 versus 0.64), but at the cost of lower precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 37, "end_pos": 43, "type": "METRIC", "confidence": 0.9996215105056763}, {"text": "REVERB", "start_pos": 49, "end_pos": 55, "type": "METRIC", "confidence": 0.9986628293991089}, {"text": "precision", "start_pos": 101, "end_pos": 110, "type": "METRIC", "confidence": 0.998233437538147}]}, {"text": "Section 5.1 shows that REVERB outperforms existing Open IE systems when evaluated on a sample of sentences.", "labels": [], "entities": [{"text": "REVERB", "start_pos": 23, "end_pos": 29, "type": "METRIC", "confidence": 0.9598449468612671}]}, {"text": "Previous work has shown that the frequency of an extraction in a large corpus is useful for assessing the correctness of extractions ().", "labels": [], "entities": []}, {"text": "Thus, it is possible a priori that REVERB's gains over previous systems will diminish when extraction frequency is taken into account.", "labels": [], "entities": [{"text": "REVERB", "start_pos": 35, "end_pos": 41, "type": "METRIC", "confidence": 0.9918743968009949}]}, {"text": "In fact, we found that REVERB's advantage over TEXTRUNNER when run at scale is qualitatively similar to its advantage on single sentences.", "labels": [], "entities": [{"text": "REVERB", "start_pos": 23, "end_pos": 29, "type": "METRIC", "confidence": 0.9879399538040161}, {"text": "TEXTRUNNER", "start_pos": 47, "end_pos": 57, "type": "METRIC", "confidence": 0.8690669536590576}]}, {"text": "We ran both REVERB and TEXTRUNNER on Banko and Etzioni's corpus of 500 million Web sentences and examined the effect of redundancy on precision.", "labels": [], "entities": [{"text": "REVERB", "start_pos": 12, "end_pos": 18, "type": "METRIC", "confidence": 0.970893919467926}, {"text": "TEXTRUNNER", "start_pos": 23, "end_pos": 33, "type": "METRIC", "confidence": 0.953548014163971}, {"text": "Banko and Etzioni's corpus of 500 million Web sentences", "start_pos": 37, "end_pos": 92, "type": "DATASET", "confidence": 0.9154573500156402}, {"text": "precision", "start_pos": 134, "end_pos": 143, "type": "METRIC", "confidence": 0.997500479221344}]}, {"text": "As Downey's work predicts, precision increased in both systems for extractions found multiple times, compared with extractions found only once.", "labels": [], "entities": [{"text": "precision", "start_pos": 27, "end_pos": 36, "type": "METRIC", "confidence": 0.9995762705802917}]}, {"text": "However, REVERB had higher precision than 1543 TEXTRUNNER at all frequency thresholds.", "labels": [], "entities": [{"text": "REVERB", "start_pos": 9, "end_pos": 15, "type": "METRIC", "confidence": 0.9960930943489075}, {"text": "precision", "start_pos": 27, "end_pos": 36, "type": "METRIC", "confidence": 0.9988861680030823}, {"text": "TEXTRUNNER", "start_pos": 47, "end_pos": 57, "type": "METRIC", "confidence": 0.9689548015594482}]}, {"text": "In fact, REVERB's frequency 1 extractions had a precision of 0.75, which TEXTRUNNER could not approach even with frequency 10 extractions, which had a precision of 0.34.", "labels": [], "entities": [{"text": "REVERB", "start_pos": 9, "end_pos": 15, "type": "METRIC", "confidence": 0.9171362519264221}, {"text": "precision", "start_pos": 48, "end_pos": 57, "type": "METRIC", "confidence": 0.9977645874023438}, {"text": "TEXTRUNNER", "start_pos": 73, "end_pos": 83, "type": "METRIC", "confidence": 0.8761075735092163}, {"text": "precision", "start_pos": 151, "end_pos": 160, "type": "METRIC", "confidence": 0.9696226716041565}]}, {"text": "Thus, REVERB is able to return more correct extractions at a higher precision than TEXTRUNNER, even when redundancy is taken into account.", "labels": [], "entities": [{"text": "REVERB", "start_pos": 6, "end_pos": 12, "type": "METRIC", "confidence": 0.9943987727165222}, {"text": "precision", "start_pos": 68, "end_pos": 77, "type": "METRIC", "confidence": 0.9894338250160217}, {"text": "TEXTRUNNER", "start_pos": 83, "end_pos": 93, "type": "METRIC", "confidence": 0.9927531480789185}]}], "tableCaptions": [{"text": " Table 4: REVERB uses these features to assign a confi- dence score to an extraction (x, r, y) from a sentence s  using a logistic regression classifier.", "labels": [], "entities": [{"text": "REVERB", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9020293951034546}]}]}