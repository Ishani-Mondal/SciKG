{"title": [{"text": "Inducing Sentence Structure from Parallel Corpora for Reordering", "labels": [], "entities": [{"text": "Inducing Sentence Structure", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.7559722264607748}, {"text": "Reordering", "start_pos": 54, "end_pos": 64, "type": "TASK", "confidence": 0.8392863869667053}]}], "abstractContent": [{"text": "When translating among languages that differ substantially in word order, machine translation (MT) systems benefit from syntactic pre-ordering-an approach that uses features from a syntactic parse to permute source words into a target-language-like order.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 74, "end_pos": 98, "type": "TASK", "confidence": 0.8319615066051483}]}, {"text": "This paper presents a method for inducing parse trees automatically from a parallel corpus, instead of using a supervised parser trained on a tree-bank.", "labels": [], "entities": []}, {"text": "These induced parses are used to pre-order source sentences.", "labels": [], "entities": []}, {"text": "We demonstrate that our induced parser is effective: it not only improves a state-of-the-art phrase-based system with integrated reordering, but also approaches the performance of a recent pre-ordering method based on a supervised parser.", "labels": [], "entities": []}, {"text": "These results show that the syntactic structure which is relevant to MT pre-ordering can be learned automatically from parallel text, thus establishing anew application for unsuper-vised grammar induction.", "labels": [], "entities": [{"text": "MT pre-ordering", "start_pos": 69, "end_pos": 84, "type": "TASK", "confidence": 0.8857598900794983}, {"text": "unsuper-vised grammar induction", "start_pos": 173, "end_pos": 204, "type": "TASK", "confidence": 0.752167264620463}]}], "introductionContent": [{"text": "Recent work in statistical machine translation (MT) has demonstrated the effectiveness of syntactic preordering: an approach that permutes source sentences into a target-like order as a pre-processing step, using features of a source-side syntactic parse (.", "labels": [], "entities": [{"text": "statistical machine translation (MT)", "start_pos": 15, "end_pos": 51, "type": "TASK", "confidence": 0.8051019161939621}]}, {"text": "Syntactic pre-ordering is particularly effective at applying structural transformations, such as the ordering change from a subject-verb-object (SVO) language like English to a subject-object-verb (SOV) language like Japanese.", "labels": [], "entities": []}, {"text": "However, state-of-the-art pre-ordering methods require a supervised syntactic parser to provide structural information about each sentence.", "labels": [], "entities": []}, {"text": "We propose a method that learns both a parsing model and a reordering model directly from a word-aligned parallel corpus.", "labels": [], "entities": []}, {"text": "Our approach, which we call Structure Induction for Reordering (STIR), requires no syntactic annotations to train, but approaches the performance of a recent syntactic pre-ordering method in a large-scale English-Japanese MT system.", "labels": [], "entities": [{"text": "Structure Induction for Reordering (STIR)", "start_pos": 28, "end_pos": 69, "type": "TASK", "confidence": 0.7643350533076695}, {"text": "MT", "start_pos": 222, "end_pos": 224, "type": "TASK", "confidence": 0.9059857130050659}]}, {"text": "STIR predicts a pre-ordering via two pipelined models: (1) parsing and (2) tree reordering.", "labels": [], "entities": []}, {"text": "The first model induces a binary parse, which defines the space of possible reorderings.", "labels": [], "entities": []}, {"text": "In particular, only trees that properly separate verbs from their object noun phrases will license an SVO to SOV transformation.", "labels": [], "entities": [{"text": "SOV transformation", "start_pos": 109, "end_pos": 127, "type": "TASK", "confidence": 0.7369881868362427}]}, {"text": "The second model locally permutes this tree.", "labels": [], "entities": []}, {"text": "Our approach resembles work with binary synchronous grammars, but is distinct in its emphasis on monolingual parsing as a first phase, and in selecting reorderings without the aid of a target-side language model.", "labels": [], "entities": []}, {"text": "The parsing model is trained to maximize the conditional likelihood of trees that license the reorderings implied by observed word alignments in a parallel corpus.", "labels": [], "entities": []}, {"text": "This objective differs from those of previous grammar induction models, which typically focus on succinctly explaining the observed source language corpus via latent hierarchical structure ().", "labels": [], "entities": []}, {"text": "Our convex objective allows us to train a feature-rich log-linear parsing model, even without supervised treebank data.", "labels": [], "entities": []}, {"text": "Focusing on pre-ordering for MT leads to anew 193 perspective on the canonical NLP task of grammar induction-one which marries the wide-spread scientific interest in unsupervised parsing models with a clear application and extrinsic evaluation methodology.", "labels": [], "entities": [{"text": "MT", "start_pos": 29, "end_pos": 31, "type": "TASK", "confidence": 0.9846361875534058}, {"text": "grammar induction-one", "start_pos": 91, "end_pos": 112, "type": "TASK", "confidence": 0.7608829438686371}]}, {"text": "To support this perspective, we highlight several avenues of future research throughout the paper.", "labels": [], "entities": []}, {"text": "We evaluate STIR in a large-scale EnglishJapanese machine translation system.", "labels": [], "entities": [{"text": "STIR", "start_pos": 12, "end_pos": 16, "type": "TASK", "confidence": 0.9101109504699707}, {"text": "EnglishJapanese machine translation", "start_pos": 34, "end_pos": 69, "type": "TASK", "confidence": 0.6493772168954214}]}, {"text": "We measure how closely our predicted reorderings match those implied by hand-annotated word alignments.", "labels": [], "entities": []}, {"text": "STIR approaches the performance of the state-of-the-art pre-ordering method described in, which learns reordering rules for supervised treebank parses.", "labels": [], "entities": [{"text": "STIR", "start_pos": 0, "end_pos": 4, "type": "TASK", "confidence": 0.4930860698223114}]}, {"text": "STIR gives a translation improvement of 3.84 BLEU over a standard phrase-based system with an integrated reordering model.", "labels": [], "entities": [{"text": "STIR", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.6235275864601135}, {"text": "translation", "start_pos": 13, "end_pos": 24, "type": "TASK", "confidence": 0.9059150815010071}, {"text": "BLEU", "start_pos": 45, "end_pos": 49, "type": "METRIC", "confidence": 0.9989055395126343}]}], "datasetContent": [{"text": "As training data for our models we used 14,000 English sentences that were sampled from the web, translated into Japanese, and manually annotated with word alignments.", "labels": [], "entities": []}, {"text": "The annotation was carried out by the original translators to promote consistency of analysis.", "labels": [], "entities": [{"text": "consistency", "start_pos": 70, "end_pos": 81, "type": "METRIC", "confidence": 0.978486955165863}]}, {"text": "describes this corpus in further detail.", "labels": [], "entities": []}, {"text": "A held-out test set of 396 manually aligned sentence pairs was used to evaluate reordering accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.9568989872932434}]}, {"text": "Statistics used for features were computed from the full, unreordered, automatically word aligned, parallel training corpus used for the translation experiments described below.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Accuracy of individual monolingual parsing and reordering models, as well as complete pipelines trained on  annotated and learned word alignments.", "labels": [], "entities": []}, {"text": " Table 2: Translation quality, measured by BLEU, for En- glish to Japanese. STIR results use both manually anno- tated and learned alignments.", "labels": [], "entities": [{"text": "Translation", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.9488073587417603}, {"text": "BLEU", "start_pos": 43, "end_pos": 47, "type": "METRIC", "confidence": 0.9990311861038208}, {"text": "STIR", "start_pos": 76, "end_pos": 80, "type": "TASK", "confidence": 0.5392335653305054}]}]}