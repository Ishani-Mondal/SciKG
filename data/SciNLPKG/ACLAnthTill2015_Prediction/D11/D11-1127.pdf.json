{"title": [], "abstractContent": [{"text": "This paper compares several translation representations fora synchronous context-free grammar parse including CFGs/hypergraphs, finite-state automata (FSA), and pushdown automata (PDA).", "labels": [], "entities": []}, {"text": "The representation choice is shown to determine the form and complexity of target LM intersection and shortest-path algorithms that follow.", "labels": [], "entities": []}, {"text": "Intersection, shortest path, FSA expansion and RTN replacement algorithms are presented for PDAs.", "labels": [], "entities": [{"text": "FSA expansion", "start_pos": 29, "end_pos": 42, "type": "TASK", "confidence": 0.7226706147193909}, {"text": "RTN replacement", "start_pos": 47, "end_pos": 62, "type": "TASK", "confidence": 0.8277515470981598}]}, {"text": "Chinese-to-English translation experiments using HiFST and HiPDT, FSA and PDA-based decoders, are presented using admissible (or exact) search, possible for HiFST with compact SCFG rulesets and HiPDT with compact LMs.", "labels": [], "entities": [{"text": "HiFST", "start_pos": 49, "end_pos": 54, "type": "DATASET", "confidence": 0.9440138339996338}, {"text": "HiPDT", "start_pos": 59, "end_pos": 64, "type": "DATASET", "confidence": 0.9553589820861816}, {"text": "FSA", "start_pos": 66, "end_pos": 69, "type": "DATASET", "confidence": 0.7599391341209412}]}, {"text": "For large rulesets with large LMs, we introduce a two-pass search strategy which we then analyze in terms of search errors and translation performance.", "labels": [], "entities": []}], "introductionContent": [{"text": "Hierarchical phrase-based translation, using asynchronous context-free translation grammar (SCFG) together with an n-gram target language model (LM), is a popular approach in machine translation . Given a SCFG G and an ngram language model M , this paper focuses on how to decode with them, i.e. how to apply them to the source text to generate a target translation.", "labels": [], "entities": [{"text": "Hierarchical phrase-based translation", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.6198944747447968}, {"text": "machine translation", "start_pos": 175, "end_pos": 194, "type": "TASK", "confidence": 0.7972427010536194}]}, {"text": "Decoding has three basic steps, which we first describe in terms of the formal languages and relations involved, with data representations and algorithms to follow.", "labels": [], "entities": []}, {"text": "Of course, decoding requires explicit data representations and algorithms for combining and searching them.", "labels": [], "entities": []}, {"text": "In common to the approaches we will consider here, sis applied to G by using the CYK algorithm in Step 1 and M is represented by a finite automaton in Step 2.", "labels": [], "entities": []}, {"text": "The choice of the representation of T in many ways determines the remaining decoder representations and algorithms needed.", "labels": [], "entities": []}, {"text": "Since {s} is a finite language and we assume throughout that G does not allow unbounded insertions, T and L are, in fact, regular languages.", "labels": [], "entities": []}, {"text": "As such, T and L have finite automaton representations T f and L f . In this case, weighted finite-state intersection and single-source shortest path algorithms (using negative log probabilities) can be used to solve Steps 2 and 3.", "labels": [], "entities": []}, {"text": "This is the approach taken in (.", "labels": [], "entities": []}, {"text": "Instead T and L can be represented by hypergraphs Th and L h (or very similarly context-free rules, and-or trees, or deductive systems).", "labels": [], "entities": []}, {"text": "In this case, hypergraph intersection with a finite automaton and hypergraph shortest path algorithms can be used to solve Steps 2 and 3.", "labels": [], "entities": [{"text": "hypergraph intersection", "start_pos": 14, "end_pos": 37, "type": "TASK", "confidence": 0.7999789714813232}]}, {"text": "This is the approach taken by . In this paper, we will consider another representation for context-free languages T and L as well, pushdown automata (PDA) T p and L p , familiar from formal language theory).", "labels": [], "entities": []}, {"text": "We will describe PDA intersection with a finite automaton and PDA shortest-path algorithms in Section 2 that can be used to solve Steps 2 and 3.", "labels": [], "entities": [{"text": "PDA intersection", "start_pos": 17, "end_pos": 33, "type": "TASK", "confidence": 0.92307910323143}]}, {"text": "It cannot be over-emphasized that the CFG, hypergraph and PDA representations of T are used for their compactness rather than for expressing non-regular languages.", "labels": [], "entities": [{"text": "CFG", "start_pos": 38, "end_pos": 41, "type": "DATASET", "confidence": 0.944415271282196}]}, {"text": "As presented so far, the search performed in Step 3 is admissible (or exact) -the true shortest path is found.", "labels": [], "entities": []}, {"text": "However, the search space in MT can be quite large.", "labels": [], "entities": [{"text": "MT", "start_pos": 29, "end_pos": 31, "type": "TASK", "confidence": 0.9472013115882874}]}, {"text": "Many systems employ aggressive pruning during the shortest-path computation with little theoretical or empirical guarantees of correctness.", "labels": [], "entities": []}, {"text": "Further, such pruning can greatly complicate any complexity analysis of the underlying representations and algorithms.", "labels": [], "entities": []}, {"text": "In this paper, we will exclude any inadmissible pruning in the shortest-path algorithm itself.", "labels": [], "entities": []}, {"text": "This allows us in Section 3 to compare the computational complexity of using these different representations.", "labels": [], "entities": []}, {"text": "We show that the PDA representation is particularly suited for decoding with large SCFGs and compact LMs.", "labels": [], "entities": []}, {"text": "We present Chinese-English translation results under the FSA and PDA translation representations.", "labels": [], "entities": [{"text": "FSA", "start_pos": 57, "end_pos": 60, "type": "DATASET", "confidence": 0.9414950609207153}]}, {"text": "We describe a two-pass translation strategy which we have developed to allow use of the PDA representation in large-scale translation.", "labels": [], "entities": []}, {"text": "In the first pass, translation is done using a lattice-generating version of the shortest path algorithm.", "labels": [], "entities": [{"text": "translation", "start_pos": 19, "end_pos": 30, "type": "TASK", "confidence": 0.9779608845710754}]}, {"text": "The full translation grammar is used but with a compact, entropy-pruned version of the full language model.", "labels": [], "entities": []}, {"text": "This first-step uses admissible pruning and lattice generation under the compact language model.", "labels": [], "entities": [{"text": "lattice generation", "start_pos": 44, "end_pos": 62, "type": "TASK", "confidence": 0.7261647433042526}]}, {"text": "In the second pass, the original, unpruned LM is simply applied to the lattices produced in the first pass.", "labels": [], "entities": []}, {"text": "We find that entropy-pruning and first-pass translation can be done so as to introduce very few search errors in the overall process; we can identify search errors in this experiment by comparison to exact translation under the full translation grammar and language model using the FSA representation.", "labels": [], "entities": [{"text": "first-pass translation", "start_pos": 33, "end_pos": 55, "type": "TASK", "confidence": 0.707334578037262}, {"text": "FSA representation", "start_pos": 282, "end_pos": 300, "type": "DATASET", "confidence": 0.8945463299751282}]}, {"text": "We then investigate a translation grammar which is large enough that exact translation under the FSA representation is not possible.", "labels": [], "entities": [{"text": "translation", "start_pos": 22, "end_pos": 33, "type": "TASK", "confidence": 0.9632272124290466}, {"text": "FSA representation", "start_pos": 97, "end_pos": 115, "type": "DATASET", "confidence": 0.8713831305503845}]}, {"text": "We find that translation is possible using the two-pass strategy with the PDA translation representation and that gains in BLEU score result from using the larger translation grammar.", "labels": [], "entities": [{"text": "translation", "start_pos": 13, "end_pos": 24, "type": "TASK", "confidence": 0.9723395705223083}, {"text": "BLEU score", "start_pos": 123, "end_pos": 133, "type": "METRIC", "confidence": 0.9801416099071503}]}], "datasetContent": [{"text": "We use two hierarchical phrase-based SMT decoders.", "labels": [], "entities": [{"text": "SMT decoders", "start_pos": 37, "end_pos": 49, "type": "TASK", "confidence": 0.8800380825996399}]}, {"text": "The first one is a lattice-based decoder implemented with weighted finite-state transducers (de Gispert et al., 2010) and described in Section 3.", "labels": [], "entities": []}, {"text": "The second decoder is a modified version using PDAs as described in Section 2.", "labels": [], "entities": []}, {"text": "In order to distinguish both decoders we call them HiFST and HiPDT, respectively.", "labels": [], "entities": [{"text": "HiFST", "start_pos": 51, "end_pos": 56, "type": "DATASET", "confidence": 0.9245923161506653}, {"text": "HiPDT", "start_pos": 61, "end_pos": 66, "type": "DATASET", "confidence": 0.8860509991645813}]}, {"text": "The principal difference between the two decoders is where the finite-state expansion step is done.", "labels": [], "entities": []}, {"text": "In HiFST, the RTN representation is immediately expanded to an FSA.", "labels": [], "entities": [{"text": "HiFST", "start_pos": 3, "end_pos": 8, "type": "DATASET", "confidence": 0.9153942465782166}, {"text": "FSA", "start_pos": 63, "end_pos": 66, "type": "DATASET", "confidence": 0.6562861800193787}]}, {"text": "In HiPDT, this expansion is delayed as late as possible -in the output of the shortest path algorithm.", "labels": [], "entities": [{"text": "HiPDT", "start_pos": 3, "end_pos": 8, "type": "DATASET", "confidence": 0.8521153926849365}]}, {"text": "Another possible configuration is to expand after the LM intersection step but before the shortest path algorithm; in practice this is quite similar to HiFST.", "labels": [], "entities": [{"text": "HiFST", "start_pos": 152, "end_pos": 157, "type": "DATASET", "confidence": 0.9144832491874695}]}, {"text": "In the following sections we report experiments in Chinese-to-English translation.", "labels": [], "entities": [{"text": "Chinese-to-English translation", "start_pos": 51, "end_pos": 81, "type": "TASK", "confidence": 0.6512852758169174}]}, {"text": "For translation model training, we use a subset of the GALE 2008 evaluation parallel text; 3 this is 2.1M sentences and approximately 45M words per language.", "labels": [], "entities": [{"text": "translation model", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.9248638451099396}, {"text": "GALE 2008 evaluation parallel text", "start_pos": 55, "end_pos": 89, "type": "DATASET", "confidence": 0.9142509579658509}]}, {"text": "We report translation results on a development set tune-nw (1,755 sentences) and a test set test-nw (1,671 sentences).", "labels": [], "entities": []}, {"text": "These contain translations produced by the GALE program and portions of the newswire sections of MT02 through MT06.", "labels": [], "entities": [{"text": "GALE program", "start_pos": 43, "end_pos": 55, "type": "DATASET", "confidence": 0.8197606205940247}, {"text": "MT02", "start_pos": 97, "end_pos": 101, "type": "DATASET", "confidence": 0.9180764555931091}, {"text": "MT06", "start_pos": 110, "end_pos": 114, "type": "DATASET", "confidence": 0.766120970249176}]}, {"text": "In tuning the sys-) in both source-to-target and target-to-source directions.", "labels": [], "entities": []}, {"text": "We then follow standard heuristics  and filtering strategies () to extract hierarchical phrases from the union of the directional word alignments.", "labels": [], "entities": []}, {"text": "We calla translation grammar the set of rules extracted from this process.", "labels": [], "entities": [{"text": "translation grammar", "start_pos": 9, "end_pos": 28, "type": "TASK", "confidence": 0.9392845034599304}]}, {"text": "We extract two translation grammars: \u2022 A restricted grammar where we apply the following additional constraint: rules are only considered if they have a forward translation probability p > 0.01.", "labels": [], "entities": []}, {"text": "We call this G 1 . As will be discussed later, the interest of this grammar is that decoding under it can be exact, that is, without any pruning in search.", "labels": [], "entities": []}, {"text": "\u2022 An unrestricted one without the previous constraint.", "labels": [], "entities": []}, {"text": "We call this G 2 . This is a superset of the previous grammar, and exact search under it is not feasible for HiFST: pruning is required in search.", "labels": [], "entities": [{"text": "HiFST", "start_pos": 109, "end_pos": 114, "type": "DATASET", "confidence": 0.7994498610496521}]}, {"text": "2. We then use entropy-based pruning of the language model) under a relative perplexity threshold of \u03b8 to reduce the size of M 1 . We will call the resulting language model as M \u03b8 1 . shows the number of n-grams (in millions) obtained for different \u03b8 values.", "labels": [], "entities": []}, {"text": "3. We translate with M \u03b8 1 using the same parameters obtained in MERT in step 1, except for the word penalty, tuned over the lattices under BLEU performance.", "labels": [], "entities": [{"text": "M \u03b8 1", "start_pos": 21, "end_pos": 26, "type": "METRIC", "confidence": 0.9385052919387817}, {"text": "MERT", "start_pos": 65, "end_pos": 69, "type": "DATASET", "confidence": 0.5787370800971985}, {"text": "BLEU", "start_pos": 140, "end_pos": 144, "type": "METRIC", "confidence": 0.9909582138061523}]}, {"text": "This produces a translation lattice in the topmost cell that contains hypotheses with exact scores under the translation grammar and M \u03b8 1 . 4. Translation lattices in the topmost cell are pruned with a likelihood-based beam width \u03b2.", "labels": [], "entities": [{"text": "M \u03b8 1", "start_pos": 133, "end_pos": 138, "type": "METRIC", "confidence": 0.9601790507634481}]}, {"text": "5. We remove the M \u03b8 1 scores from the pruned translation lattices and reapply M 1 , moving the word penalty back to the original value obtained in MERT.", "labels": [], "entities": [{"text": "MERT", "start_pos": 148, "end_pos": 152, "type": "DATASET", "confidence": 0.7547955513000488}]}, {"text": "These operations can be carried out efficiently via standard FSA operations.", "labels": [], "entities": [{"text": "FSA", "start_pos": 61, "end_pos": 64, "type": "TASK", "confidence": 0.6335960030555725}]}, {"text": "6. Additionally, we can rescore the translation lattices obtained in steps 1 or 5 with the larger language model M 2 . Again, this can be done via standard FSA operations.", "labels": [], "entities": [{"text": "FSA", "start_pos": 156, "end_pos": 159, "type": "DATASET", "confidence": 0.6020674705505371}]}, {"text": "Note that if \u03b2 = \u221e or if \u03b8 = 0, the translation lattices obtained in step 1 should be identical to the ones of step 5.", "labels": [], "entities": []}, {"text": "While the goal is to increase \u03b8 to reduce the size of the language model used at Step 3, \u03b2 will have to increase accordingly so as to avoid pruning away desirable hypotheses in Step 4.", "labels": [], "entities": []}, {"text": "If \u03b2 defines a sufficiently wide beam to contain the hypotheses which would be favoured by M 1 , faster decoding with M \u03b8 1 would be possible without incurring search errors M 1 . This is investigated next.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Number of ngrams (in millions) in the 1st pass 4-gram", "labels": [], "entities": [{"text": "Number", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9629009366035461}]}, {"text": " Table 3: Results (lowercase IBM BLEU scores) under G 1 with various M \u03b8  1 as obtained with several values of \u03b8.  Performance in subsequent rescoring with M 1 and M 2 after likelihood-based pruning of the translation lattices for  various \u03b2 is also reported. Decoding time, in seconds/word over test-nw, refers strictly to first-pass decoding.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 33, "end_pos": 37, "type": "METRIC", "confidence": 0.9183393120765686}]}, {"text": " Table 4: Percentage of success in producing the 1-best translation under G 2 with various M \u03b8  1 when applying a hard  memory limitation of 10 GB, as measured over tune-nw (1755 sentences). If decoder fails, we report what step was  being done when the limit was reached. HiFST could be expanding into an FSA or composing the FSA with M \u03b8  1 ;  HiPDT could be PDA composing with M \u03b8  1 or PDA expanding into an FSA.", "labels": [], "entities": [{"text": "HiFST", "start_pos": 273, "end_pos": 278, "type": "DATASET", "confidence": 0.8978833556175232}, {"text": "FSA", "start_pos": 306, "end_pos": 309, "type": "DATASET", "confidence": 0.8472711443901062}, {"text": "FSA", "start_pos": 327, "end_pos": 330, "type": "DATASET", "confidence": 0.898958683013916}, {"text": "HiPDT", "start_pos": 346, "end_pos": 351, "type": "DATASET", "confidence": 0.9314458966255188}, {"text": "FSA", "start_pos": 412, "end_pos": 415, "type": "DATASET", "confidence": 0.930149257183075}]}, {"text": " Table 5: HiPDT performance on grammar G 2 with \u03b8 = 7.5 \u00d7 10 \u22127 . Exact search with HiFST is not possible under  these conditions: pruning during search would be required.", "labels": [], "entities": []}]}