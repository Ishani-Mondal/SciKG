{"title": [{"text": "A Word Reordering Model for Improved Machine Translation", "labels": [], "entities": [{"text": "Word Reordering", "start_pos": 2, "end_pos": 17, "type": "TASK", "confidence": 0.751591295003891}, {"text": "Improved Machine Translation", "start_pos": 28, "end_pos": 56, "type": "TASK", "confidence": 0.6228773494561514}]}], "abstractContent": [{"text": "Preordering of source side sentences has proved to be useful in improving statistical machine translation.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 74, "end_pos": 105, "type": "TASK", "confidence": 0.7283945480982462}]}, {"text": "Most work has used a parser in the source language along with rules to map the source language word order into the target language word order.", "labels": [], "entities": []}, {"text": "The requirement to have a source language parser is a major drawback, which we seek to overcome in this paper.", "labels": [], "entities": []}, {"text": "Instead of using a parser and then using rules to order the source side sentence we learn a model that can directly reorder source side sentences to match target word order using a small parallel corpus with high-quality word alignments.", "labels": [], "entities": []}, {"text": "Our model learns pairwise costs of a word immediately preceding another word.", "labels": [], "entities": []}, {"text": "We use the Lin-Kernighan heuristic to find the best source reordering efficiently during training and testing and show that it suffices to provide good quality reordering.", "labels": [], "entities": []}, {"text": "We show gains in translation performance based on our reordering model for translating from Hindi to English, Urdu to English (with a public dataset), and English to Hindi.", "labels": [], "entities": []}, {"text": "For English to Hindi we show that our technique achieves better performance than a method that uses rules applied to the source side En-glish parse.", "labels": [], "entities": []}], "introductionContent": [{"text": "Languages differ in the way they order words to produce sentences representing the same meaning.", "labels": [], "entities": []}, {"text": "Machine translation systems need to reorder words in the source sentence to produce fluent output in the target language that preserves the meaning of the source sentence.", "labels": [], "entities": [{"text": "Machine translation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7219612300395966}]}, {"text": "Current phrase based machine translation systems can capture short range reorderings via the phrase table.", "labels": [], "entities": [{"text": "phrase based machine translation", "start_pos": 8, "end_pos": 40, "type": "TASK", "confidence": 0.6429833695292473}]}, {"text": "Even the capturing of these local reordering phenomena is constrained by the amount of training data available.", "labels": [], "entities": []}, {"text": "For example, if adjectives precede nouns in the source language and follow nouns in the target language we still need to see a particular adjective noun pair in the parallel corpus to handle the reordering via the phrase table.", "labels": [], "entities": []}, {"text": "Phrase based systems also rely on the target side language model to produce the right target side order.", "labels": [], "entities": [{"text": "Phrase based", "start_pos": 0, "end_pos": 12, "type": "TASK", "confidence": 0.8789971768856049}]}, {"text": "This is known to be inadequate), and this inadequacy has spurred various attempts to overcome the problem of handling differing word order in languages.", "labels": [], "entities": []}, {"text": "One approach is through distortion models, that try to model which reorderings are more likely than others.", "labels": [], "entities": []}, {"text": "The simplest models just penalize long jumps in the source sentence when producing the target sentence.", "labels": [], "entities": []}, {"text": "These models have also been generalized) to allow for lexical dependencies on the source.", "labels": [], "entities": []}, {"text": "While these models are simple, and can be integrated with the decoder they are insufficient to capture long-range reordering phenomena especially for language pairs that differ significantly.", "labels": [], "entities": []}, {"text": "The weakness of these simple distortion models has been overcome using syntax of either the source or target sentence (;;).", "labels": [], "entities": []}, {"text": "While these methods have shown to be useful in improving machine translation perfor-486 mance they generally involve joint parsing of the source and target language which is significantly more computationally expensive when compared to phrase based translation systems.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 57, "end_pos": 76, "type": "TASK", "confidence": 0.7578185200691223}, {"text": "phrase based translation", "start_pos": 236, "end_pos": 260, "type": "TASK", "confidence": 0.675722340742747}]}, {"text": "Another approach that overcomes this weakness, is to to reorder the source sentence based on rules applied on the source parse (either handwritten or learned from data) both when training and testing.", "labels": [], "entities": []}, {"text": "In this paper we propose a novel method for dealing with the word order problem that is efficient and does not rely on a source or target side parse being available.", "labels": [], "entities": []}, {"text": "We cast the word ordering problem as a Traveling Salesman Problem (TSP) based on previous work on word-based and phrased-based statistical machine translation (.", "labels": [], "entities": [{"text": "word ordering problem", "start_pos": 12, "end_pos": 33, "type": "TASK", "confidence": 0.831648071606954}, {"text": "phrased-based statistical machine translation", "start_pos": 113, "end_pos": 158, "type": "TASK", "confidence": 0.5892552733421326}]}, {"text": "Words are the cities in the TSP and the objective is to learn the distance between words so that the shortest tour corresponds to the ordering of the words in the source sentence in the target language.", "labels": [], "entities": []}, {"text": "We show that the TSP distances for reordering can be learned from a small amount of high-quality word alignment data by means of pairwise word comparisons and an informative feature set involving words and part-of-speech (POS) tags adapted and extended from prior work on dependency parsing).", "labels": [], "entities": [{"text": "TSP", "start_pos": 17, "end_pos": 20, "type": "TASK", "confidence": 0.7286859154701233}, {"text": "word alignment", "start_pos": 97, "end_pos": 111, "type": "TASK", "confidence": 0.717649519443512}, {"text": "dependency parsing", "start_pos": 272, "end_pos": 290, "type": "TASK", "confidence": 0.7397329211235046}]}, {"text": "Obtaining high-quality word alignments that we require for training is fairly easy compared with obtaining a treebank required to obtain parses for use in syntax based methods.", "labels": [], "entities": []}, {"text": "We show experimentally that our reordering model, even when used to reorder sentences for training and testing (rather than being used as an additional score in the decoder) improves machine translation performance for: Hindi \u2192 English, English \u2192 Hindi, and Urdu \u2192 English.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 183, "end_pos": 202, "type": "TASK", "confidence": 0.7346978783607483}]}, {"text": "Although Urdu is similar to Hindi from the point of reordering phenomena we include it in our experiments since there are publicly available datasets for Urdu-English.", "labels": [], "entities": []}, {"text": "For English \u2192 Hindi we obtained better machine translation performance with our reordering model as compared to a method that uses reordering rules applied to the source side parse.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 39, "end_pos": 58, "type": "TASK", "confidence": 0.6757830083370209}]}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 reviews related work and places our work in context.", "labels": [], "entities": []}, {"text": "Section 3 outlines reordering issues due to syntactic differences between Hindi and English.", "labels": [], "entities": []}, {"text": "Section 4 presents our reordering model, Section 5 presents experimental results and Section 6 presents our conclusions and possible future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we report on experiments to evaluate our reordering model.", "labels": [], "entities": []}, {"text": "The first method we use for evaluation (monolingual BLEU) is by generating the desired reordering of the source sentence (as described in Section 4.2) and compare the reordered output to this desired reordered sentence using the BLEU metric.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 52, "end_pos": 56, "type": "METRIC", "confidence": 0.9754143357276917}, {"text": "BLEU", "start_pos": 229, "end_pos": 233, "type": "METRIC", "confidence": 0.9969504475593567}]}, {"text": "In addition, to these monolingual BLEU results, we also evaluate (in Section 5.5) the reordering by its effect on eventual machine translation performance.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 34, "end_pos": 38, "type": "METRIC", "confidence": 0.99072265625}, {"text": "machine translation", "start_pos": 123, "end_pos": 142, "type": "TASK", "confidence": 0.727919727563858}]}, {"text": "We note that our reordering techniques uses POS information for the input sentence.", "labels": [], "entities": []}, {"text": "The POS taggers used in this paper are Maximum Entropy Markov models trained using manually annotated POS corpora.", "labels": [], "entities": [{"text": "POS taggers", "start_pos": 4, "end_pos": 15, "type": "TASK", "confidence": 0.7764722406864166}]}, {"text": "For Hindi, we used roughly fifty thousand words with twenty six tags from the corpus described in ().", "labels": [], "entities": []}, {"text": "For Urdu we used roughly fifty thousand words and forty six tags from the CRULP corpus and for English we used the Wall Street Journal section of the Penn Treebank.", "labels": [], "entities": [{"text": "CRULP corpus", "start_pos": 74, "end_pos": 86, "type": "DATASET", "confidence": 0.9575485587120056}, {"text": "Wall Street Journal section of the Penn Treebank", "start_pos": 115, "end_pos": 163, "type": "DATASET", "confidence": 0.9517372101545334}]}], "tableCaptions": [{"text": " Table 3: Monolingual BLEU scores for Hindi to English  reordering using models trained on different alignment  types and tested on a development set of 280 Hindi sen- tences (5590 tokens).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 22, "end_pos": 26, "type": "METRIC", "confidence": 0.9818843007087708}, {"text": "Hindi to English  reordering", "start_pos": 38, "end_pos": 66, "type": "TASK", "confidence": 0.6478032171726227}]}, {"text": " Table 5: Monolingual BLEU scores comparing the orig- inal source order with desired target reorder without re- ordering, and reordering using our model (TSP) and the  model proposed in (Tromble and Eisner, 2009) (LOP).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 22, "end_pos": 26, "type": "METRIC", "confidence": 0.9783037304878235}]}, {"text": " Table 6: An analyis of reordering for a few POS bigrams", "labels": [], "entities": []}, {"text": " Table 7: Translation performance without reordering  (baseline) compared with performance after preordering  with our reordering model.", "labels": [], "entities": [{"text": "Translation", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.9733648300170898}]}, {"text": " Table 8: Translation performance with/without reorder- ing with varying decoder search space.", "labels": [], "entities": [{"text": "Translation", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.9825050234794617}]}]}