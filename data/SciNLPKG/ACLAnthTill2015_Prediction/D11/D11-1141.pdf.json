{"title": [{"text": "Named Entity Recognition in Tweets: An Experimental Study", "labels": [], "entities": [{"text": "Named Entity Recognition in Tweets", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.7334309756755829}]}], "abstractContent": [{"text": "People tweet more than 100 Million times daily, yielding a noisy, informal, but sometimes informative corpus of 140-character messages that mirrors the zeitgeist in an unprecedented manner.", "labels": [], "entities": []}, {"text": "The performance of standard NLP tools is severely degraded on tweets.", "labels": [], "entities": []}, {"text": "This paper addresses this issue by rebuilding the NLP pipeline beginning with part-of-speech tagging, through chunking, to named-entity recognition.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 78, "end_pos": 100, "type": "TASK", "confidence": 0.698623538017273}, {"text": "named-entity recognition", "start_pos": 123, "end_pos": 147, "type": "TASK", "confidence": 0.704207569360733}]}, {"text": "Our novel T-NER system doubles F 1 score compared with the Stanford NER system.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 31, "end_pos": 40, "type": "METRIC", "confidence": 0.9877663254737854}, {"text": "Stanford NER system", "start_pos": 59, "end_pos": 78, "type": "DATASET", "confidence": 0.8608913421630859}]}, {"text": "T-NER leverages the redundancy inherent in tweets to achieve this performance, using LabeledLDA to exploit Freebase dictionaries as a source of distant supervision.", "labels": [], "entities": [{"text": "T-NER", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8282284140586853}]}, {"text": "LabeledLDA outperforms co-training, increasing F 1 by 25% over ten common entity types.", "labels": [], "entities": [{"text": "F 1", "start_pos": 47, "end_pos": 50, "type": "METRIC", "confidence": 0.9942809045314789}]}, {"text": "Our NLP tools are available at: http:// github.com/aritter/twitter_nlp", "labels": [], "entities": []}], "introductionContent": [{"text": "Status Messages posted on Social Media websites such as Facebook and Twitter present anew and challenging style of text for language technology due to their noisy and informal nature.", "labels": [], "entities": []}, {"text": "Like SMS (, tweets are particularly terse and difficult (See).", "labels": [], "entities": []}, {"text": "Yet tweets provide a unique compilation of information that is more upto-date and inclusive than news articles, due to the low-barrier to tweeting, and the proliferation of mobile devices.", "labels": [], "entities": []}, {"text": "The corpus of tweets already exceeds the size of the Library of Congress and is growing far more rapidly.", "labels": [], "entities": []}, {"text": "Due to the volume of tweets, it is natural to consider named-entity recognition, information extraction, and text mining over tweets.", "labels": [], "entities": [{"text": "named-entity recognition", "start_pos": 55, "end_pos": 79, "type": "TASK", "confidence": 0.7081239819526672}, {"text": "information extraction", "start_pos": 81, "end_pos": 103, "type": "TASK", "confidence": 0.7838976383209229}, {"text": "text mining", "start_pos": 109, "end_pos": 120, "type": "TASK", "confidence": 0.7204798758029938}]}, {"text": "Not surprisingly, the performance of \"off the shelf\" NLP tools, which were trained on news corpora, is weak on tweet corpora.", "labels": [], "entities": []}, {"text": "In response, we report on a re-trained \"NLP pipeline\" that leverages previously-tagged out-ofdomain text, 2 tagged tweets, and unlabeled tweets to achieve more effective part-of-speech tagging, chunking, and named-entity recognition.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 170, "end_pos": 192, "type": "TASK", "confidence": 0.6849322468042374}, {"text": "named-entity recognition", "start_pos": 208, "end_pos": 232, "type": "TASK", "confidence": 0.725417286157608}]}, {"text": "The Hobbit has FINALLY started filming!", "labels": [], "entities": [{"text": "FINALLY", "start_pos": 15, "end_pos": 22, "type": "METRIC", "confidence": 0.9391693472862244}]}, {"text": "Its official Nintendo announced today that they Will release the Nintendo 3DS in north America march 27 for $250 3 Government confirms blast n nuclear plants n japan...don't knw wht s gona happen nw...", "labels": [], "entities": []}, {"text": "We find that classifying named entities in tweets is a difficult task for two reasons.", "labels": [], "entities": [{"text": "classifying named entities in tweets", "start_pos": 13, "end_pos": 49, "type": "TASK", "confidence": 0.8850390434265136}]}, {"text": "First, tweets contain a plethora of distinctive named entity types (Companies, Products, Bands, Movies, and more).", "labels": [], "entities": []}, {"text": "Almost all these types (except for People and Locations) are relatively infrequent, so even a large sample of manually annotated tweets will contain few training examples.", "labels": [], "entities": []}, {"text": "Secondly, due to Twitter's 140 character limit, tweets often lack sufficient context to determine an entity's type without the aid of background knowledge.", "labels": [], "entities": []}, {"text": "To address these issues we propose a distantly supervised approach which applies LabeledLDA to leverage large amounts of unlabeled data in addition to large dictionaries of entities gathered from Freebase, and combines information about an entity's context across its mentions.", "labels": [], "entities": []}, {"text": "We make the following contributions: 1.", "labels": [], "entities": []}, {"text": "We experimentally evaluate the performance of off-the-shelf news trained NLP tools when applied to Twitter.", "labels": [], "entities": []}, {"text": "For example POS tagging accuracy drops from about 0.97 on news to 0.80 on tweets.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 12, "end_pos": 23, "type": "TASK", "confidence": 0.7692376971244812}, {"text": "accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9765571355819702}]}, {"text": "By utilizing in-domain, outof-domain, and unlabeled data we are able to substantially boost performance, for example obtaining a 52% increase in F 1 score on segmenting named entities.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 145, "end_pos": 154, "type": "METRIC", "confidence": 0.9904220302899679}, {"text": "segmenting named entities", "start_pos": 158, "end_pos": 183, "type": "TASK", "confidence": 0.8772374788920084}]}, {"text": "2. We introduce a novel approach to distant supervision () using Topic Models.", "labels": [], "entities": []}, {"text": "LabeledLDA is applied, utilizing constraints based on an open-domain database (Freebase) as a source of supervision.", "labels": [], "entities": []}, {"text": "This approach increases F 1 score by 25% relative to co-training on the task of classifying named entities in Tweets.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 24, "end_pos": 33, "type": "METRIC", "confidence": 0.9836039940516154}, {"text": "classifying named entities in Tweets", "start_pos": 80, "end_pos": 116, "type": "TASK", "confidence": 0.7090550661087036}]}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "We successively build the NLP pipeline for Twitter feeds in Sections 2 and 3.", "labels": [], "entities": []}, {"text": "We first present our approaches to shallow syntax -part of speech tagging ( \u00a72.1), and shallow parsing ( \u00a72.2).", "labels": [], "entities": [{"text": "speech tagging", "start_pos": 59, "end_pos": 73, "type": "TASK", "confidence": 0.7187407612800598}]}, {"text": "\u00a72.3 describes a novel classifier that predicts the informativeness of capitalization in a tweet.", "labels": [], "entities": []}, {"text": "All tools in \u00a72 are used as features for named entity segmentation in \u00a73.1.", "labels": [], "entities": [{"text": "named entity segmentation", "start_pos": 41, "end_pos": 66, "type": "TASK", "confidence": 0.6399862468242645}]}, {"text": "Next, we present our algorithms and evaluation for entity classification ( \u00a73.2).", "labels": [], "entities": [{"text": "entity classification", "start_pos": 51, "end_pos": 72, "type": "TASK", "confidence": 0.7758868336677551}]}, {"text": "We describe related work in \u00a74 and conclude in \u00a75.", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate T-CLASS's ability to classify entity mentions in context, we annotated the 2,400 tweets with 10 types which are both popular on Twitter, and have good coverage in Freebase: PERSON, GEO-LOCATION, COMPANY, PRODUCT, FACIL-ITY, TV-SHOW, MOVIE, SPORTSTEAM, BAND, and OTHER.", "labels": [], "entities": [{"text": "PERSON", "start_pos": 185, "end_pos": 191, "type": "METRIC", "confidence": 0.8367066383361816}, {"text": "GEO-LOCATION", "start_pos": 193, "end_pos": 205, "type": "DATASET", "confidence": 0.6209229230880737}, {"text": "FACIL-ITY", "start_pos": 225, "end_pos": 234, "type": "METRIC", "confidence": 0.747599720954895}, {"text": "BAND", "start_pos": 264, "end_pos": 268, "type": "METRIC", "confidence": 0.9850496649742126}, {"text": "OTHER", "start_pos": 274, "end_pos": 279, "type": "METRIC", "confidence": 0.724166214466095}]}, {"text": "Note that these type annotations are only used for evaluation purposes, and not used during training T-CLASS, which relies only on distant supervision.", "labels": [], "entities": []}, {"text": "In some cases, we combine multiple Freebase types to create a dictionary of entities representing a single type (for example the COM-PANY dictionary contains Freebase types /business/consumer company and /business/brand).", "labels": [], "entities": []}, {"text": "Because our approach does not rely on any manually labeled examples, it is straightforward to extend it fora different sets of types based on the needs of downstream applications.", "labels": [], "entities": []}, {"text": "Training: To gather unlabeled data for inference, we run T-SEG, our entity segmenter (from \u00a73.1), on 60M tweets, and keep the entities which appear 100 or more times.", "labels": [], "entities": []}, {"text": "This results in a set of 23,651 distinct entity strings.", "labels": [], "entities": []}, {"text": "For each entity string, we collect words occurring in a context window of 3 words from all mentions in our data, and use a vocabulary of the 100K most frequent words.", "labels": [], "entities": []}, {"text": "We run Gibbs sampling for 1,000 iterations, using the last sample to estimate entity-type distributions \u03b8 e , in addition to type-word distributions \u03b2 t . displays the 20 entities (not found in Freebase) whose posterior distribution \u03b8 e assigns highest probability to selected types.", "labels": [], "entities": []}, {"text": "Results: presents the classification results of T-CLASS compared against a majority baseline which simply picks the most frequent class (PERSON), in addition to the Freebase baseline, which only makes predictions if an entity appears in exactly one dictionary (i.e., appears unambiguous).", "labels": [], "entities": [{"text": "PERSON", "start_pos": 137, "end_pos": 143, "type": "METRIC", "confidence": 0.9857287406921387}, {"text": "Freebase baseline", "start_pos": 165, "end_pos": 182, "type": "DATASET", "confidence": 0.9439075589179993}]}, {"text": "T-CLASS also outperforms a simple supervised baseline which applies a MaxEnt classifier using 4-fold cross validation over the 1,450 entities which were annotated for testing.", "labels": [], "entities": []}, {"text": "Additionally we compare against the co-training algorithm of which also leverages unlabeled data and uses our Freebase type lists; for seed rules we use the \"unambiguous\" Freebase entities.", "labels": [], "entities": [{"text": "Freebase type lists", "start_pos": 110, "end_pos": 129, "type": "DATASET", "confidence": 0.8902223308881124}, {"text": "Freebase entities", "start_pos": 171, "end_pos": 188, "type": "DATASET", "confidence": 0.9445260763168335}]}, {"text": "Our results demonstrate that T-CLASS outperforms the baselines and achieves a 25% increase in F 1 score over co-training.", "labels": [], "entities": [{"text": "T-CLASS", "start_pos": 29, "end_pos": 36, "type": "METRIC", "confidence": 0.8308742642402649}, {"text": "F 1 score", "start_pos": 94, "end_pos": 103, "type": "METRIC", "confidence": 0.991982102394104}]}, {"text": "tity string, and infers a distribution over its possible types, whereas DL-Cotrain considers the entity mentions separately as unlabeled examples and predicts a type independently for each.", "labels": [], "entities": []}, {"text": "In order to ensure that the difference in performance between LabeledLDA and DL-Cotrain is not simply due to this difference in representation, we compare both DL-Cotrain and LabeledLDA using both unlabeled datasets (grouping words by all mentions vs. keeping mentions separate) in.", "labels": [], "entities": []}, {"text": "As expected, DL-Cotrain performs poorly when the unlabeled examples group mentions; this makes sense, since CoTraining uses a discriminative learning algorithm, so when trained on entities and tested on individual mentions, the performance decreases.", "labels": [], "entities": []}, {"text": "Additionally, LabeledLDA's performance is poorer when considering mentions as \"documents\".", "labels": [], "entities": []}, {"text": "This is likely due to the fact that there isn't enough context to effectively learn topics when the \"documents\" are very short (typically fewer than 10 words).", "labels": [], "entities": []}, {"text": "End to End System: Finally we present the end to end performance on segmentation and classification (T-NER) in.", "labels": [], "entities": [{"text": "segmentation and classification", "start_pos": 68, "end_pos": 99, "type": "TASK", "confidence": 0.8969291647275289}]}, {"text": "We observe that T-NER again outperforms co-training.", "labels": [], "entities": []}, {"text": "Moreover, comparing against the Stanford Named Entity Recognizer on the 3 MUC types, T-NER doubles F 1 score.", "labels": [], "entities": [{"text": "Stanford Named Entity Recognizer", "start_pos": 32, "end_pos": 64, "type": "DATASET", "confidence": 0.8212272673845291}, {"text": "F 1 score", "start_pos": 99, "end_pos": 108, "type": "METRIC", "confidence": 0.985241969426473}]}, {"text": "In contrast to previous work, we have demonstrated the utility of features based on Twitterspecific POS taggers and Shallow Parsers in segmenting Named Entities.", "labels": [], "entities": [{"text": "segmenting Named Entities", "start_pos": 135, "end_pos": 160, "type": "TASK", "confidence": 0.885037899017334}]}, {"text": "In addition we take a distantly supervised approach to Named Entity Classification which exploits large dictionaries of entities gathered from Freebase, requires no manually annotated data, and as a result is able to handle a larger number of types than previous work.", "labels": [], "entities": [{"text": "Named Entity Classification", "start_pos": 55, "end_pos": 82, "type": "TASK", "confidence": 0.6217300991217295}]}, {"text": "Although we found manually annotated data to be very beneficial for named entity segmentation, we were motivated to explore approaches that don't rely on manual labels for classification due to Twitter's wide range of named entity types.", "labels": [], "entities": [{"text": "named entity segmentation", "start_pos": 68, "end_pos": 93, "type": "TASK", "confidence": 0.6601488292217255}]}, {"text": "Additionally, unlike previous work on NER in informal text, our approach allows the sharing of information across an entity's mentions which is quite beneficial due to Twitter's terse nature.", "labels": [], "entities": [{"text": "sharing of information across an entity's mentions", "start_pos": 84, "end_pos": 134, "type": "TASK", "confidence": 0.749845951795578}]}], "tableCaptions": [{"text": " Table 2: POS tagging performance on tweets. By training  on in-domain labeled data, in addition to annotated IRC  chat data, we obtain a 41% reduction in error over the  Stanford POS tagger.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.8467185199260712}, {"text": "error", "start_pos": 155, "end_pos": 160, "type": "METRIC", "confidence": 0.9865344762802124}]}, {"text": " Table 3: Most common errors made by the Stanford POS  Tagger on tweets. For each case we list the fraction of  times the gold tag is misclassified as the predicted for  both our system and the Stanford POS tagger. All verbs  are collapsed into VB for compactness.", "labels": [], "entities": []}, {"text": " Table 4: Token-Level accuracy at shallow parsing tweets.  We compare against the OpenNLP chunker as a baseline.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9847316741943359}, {"text": "OpenNLP chunker", "start_pos": 82, "end_pos": 97, "type": "DATASET", "confidence": 0.9273082613945007}]}, {"text": " Table 5: Performance at predicting reliable capitalization.", "labels": [], "entities": [{"text": "predicting reliable capitalization", "start_pos": 25, "end_pos": 59, "type": "TASK", "confidence": 0.8339211344718933}]}, {"text": " Table 6: Performance at segmenting entities varying the  features used. \"None\" removes POS, Chunk, and capital- ization features. Overall we obtain a 52% improvement  in F 1 score over the Stanford Named Entity Recognizer.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 171, "end_pos": 180, "type": "METRIC", "confidence": 0.9913671811421713}, {"text": "Stanford Named Entity Recognizer", "start_pos": 190, "end_pos": 222, "type": "DATASET", "confidence": 0.7900869399309158}]}, {"text": " Table 8: Named Entity Classification performance on the  10 types. Assumes segmentation is given as in (Collins  and Singer, 1999), and (Elsner et al., 2009).", "labels": [], "entities": [{"text": "Named Entity Classification", "start_pos": 10, "end_pos": 37, "type": "TASK", "confidence": 0.6377640763918558}]}, {"text": " Table 9: F 1 classification scores for the 3 MUC types  PERSON, LOCATION, ORGANIZATION. Results are  shown using LabeledLDA (LL), Freebase Baseline (FB),  DL-Cotrain (CT) and Supervised Baseline (SP). N is the  number of entities in the test set.", "labels": [], "entities": [{"text": "F", "start_pos": 10, "end_pos": 11, "type": "METRIC", "confidence": 0.9891257286071777}, {"text": "PERSON", "start_pos": 57, "end_pos": 63, "type": "METRIC", "confidence": 0.9209156036376953}, {"text": "ORGANIZATION", "start_pos": 75, "end_pos": 87, "type": "METRIC", "confidence": 0.9823253750801086}]}, {"text": " Table 10: F 1 scores for classification broken down by  type for LabeledLDA (LL), Freebase Baseline (FB), DL- Cotrain (CT) and Supervised Baseline (SP). N is the num- ber of entities in the test set.", "labels": [], "entities": [{"text": "F 1", "start_pos": 11, "end_pos": 14, "type": "METRIC", "confidence": 0.9737618267536163}]}, {"text": " Table 11: Comparing LabeledLDA and DL-Cotrain  grouping unlabeled data by entities vs. mentions.", "labels": [], "entities": []}, {"text": " Table 12: Performance at predicting both segmentation  and classification. Systems labeled with PLO are evalu- ated on the 3 MUC types PERSON, LOCATION, ORGA- NIZATION.", "labels": [], "entities": [{"text": "predicting", "start_pos": 26, "end_pos": 36, "type": "TASK", "confidence": 0.9753190279006958}, {"text": "segmentation  and classification", "start_pos": 42, "end_pos": 74, "type": "TASK", "confidence": 0.6048644185066223}, {"text": "PERSON", "start_pos": 136, "end_pos": 142, "type": "METRIC", "confidence": 0.9401825070381165}, {"text": "LOCATION", "start_pos": 144, "end_pos": 152, "type": "METRIC", "confidence": 0.9095568060874939}, {"text": "ORGA- NIZATION", "start_pos": 154, "end_pos": 168, "type": "METRIC", "confidence": 0.891299287478129}]}]}