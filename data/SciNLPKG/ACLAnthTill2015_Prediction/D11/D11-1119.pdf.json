{"title": [{"text": "Exploiting Syntactic and Distributional Information for Spelling Correction with Web-Scale N-gram Models", "labels": [], "entities": []}], "abstractContent": [{"text": "We propose a novel way of incorporating dependency parse and word co-occurrence information into a state-of-the-art web-scale n-gram model for spelling correction.", "labels": [], "entities": [{"text": "dependency parse", "start_pos": 40, "end_pos": 56, "type": "TASK", "confidence": 0.7826004028320312}, {"text": "spelling correction", "start_pos": 143, "end_pos": 162, "type": "TASK", "confidence": 0.9113644957542419}]}, {"text": "The syntactic and distributional information provides extra evidence in addition to that provided by a web-scale n-gram corpus and especially helps with data sparsity problems.", "labels": [], "entities": []}, {"text": "Experimental results show that introducing syntactic features into n-gram based models significantly reduces errors by up to 12.4% over the current state-of-the-art.", "labels": [], "entities": [{"text": "errors", "start_pos": 109, "end_pos": 115, "type": "METRIC", "confidence": 0.993287980556488}]}, {"text": "The word co-occurrence information shows potential but only improves overall accuracy slightly.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.9878634214401245}]}], "introductionContent": [{"text": "The function of context-sensitive text correction is to identify word-choice errors in text (.", "labels": [], "entities": [{"text": "context-sensitive text correction", "start_pos": 16, "end_pos": 49, "type": "TASK", "confidence": 0.600957860549291}]}, {"text": "It can be viewed as a lexical disambiguation task), where a system selects from a predefined confusion word set, such as {affect, effect} or {complement, compliment}, and provides the most appropriate word choice given the context.", "labels": [], "entities": []}, {"text": "Typically, one determines if a word has been used correctly based on lexical, syntactic and semantic information from the context of the word.", "labels": [], "entities": []}, {"text": "One of the top performing models of spelling correction () is based on web-scale n-gram counts, which reflect both syntax and meaning.", "labels": [], "entities": [{"text": "spelling correction", "start_pos": 36, "end_pos": 55, "type": "TASK", "confidence": 0.9541180729866028}]}, {"text": "However, even with a large-scale n-gram corpus, data sparsity can hurt performance in two ways.", "labels": [], "entities": []}, {"text": "* This work was done when the first author was an intern for Educational Testing Service.", "labels": [], "entities": [{"text": "Educational Testing Service", "start_pos": 61, "end_pos": 88, "type": "DATASET", "confidence": 0.9573933283487955}]}, {"text": "First, n-gram based methods require exact word and order matches.", "labels": [], "entities": []}, {"text": "If there is a low frequency word in the context, such as a person's name, there will belittle, if any, evidence in the n-gram data to support the usage.", "labels": [], "entities": []}, {"text": "Second, if the target confusable word is rare, there will not be enough n-gram support or training data to render a confident decision.", "labels": [], "entities": []}, {"text": "Because of the data sparsity problem, language modeling is not always sufficient to capture the meaning of the sentence and the correct usage of the word.", "labels": [], "entities": []}, {"text": "Take a sentence from The New York Times (NYT) for example: \"'This fellow's won a war,' the dean of the capital's press corps, David Broder, announced on 'Meet the Press' after complimenting the president on the 'great sense of authority and command' he exhibited in a flight suit.\"", "labels": [], "entities": []}, {"text": "Unfortunately, neither the phrase \"complementing the president\" nor \"complimenting the president\" exists in the web-scale Google N-gram corpus).", "labels": [], "entities": [{"text": "Google N-gram corpus", "start_pos": 122, "end_pos": 142, "type": "DATASET", "confidence": 0.6548873086770376}]}, {"text": "The n-gram models decide solely based on the frequency of the bi-grams \"after comple(i)menting\" and \"comple(i)menting the\", which are common usages for both words.", "labels": [], "entities": []}, {"text": "The real question is whether we are more likely to \"compliment\" or \"complement\" a person, the \"president\".", "labels": [], "entities": []}, {"text": "Several clues could help us answer that question.", "labels": [], "entities": []}, {"text": "A dependency parser can identify the word \"president\" as the subject of \"compliment\" or \"complement\" which also maybe the casein some of the training data.", "labels": [], "entities": [{"text": "complement", "start_pos": 89, "end_pos": 99, "type": "METRIC", "confidence": 0.9590939879417419}]}, {"text": "Lexical co-occurrence ( and semantic word relatedness measurements, such as Random Indexing), could provide evidence that \"compliment\" is more likely to co-occur with \"president\" than \"complement\".", "labels": [], "entities": []}, {"text": "Fur-thermore, some important clues can be quite distant from the target word, e.g. outside the 9-word context window and used.", "labels": [], "entities": [{"text": "Fur-thermore", "start_pos": 0, "end_pos": 12, "type": "METRIC", "confidence": 0.8478928208351135}]}, {"text": "Consider another sentence in the NYT corpus, \"GM says the addition of OnStar, which includes a system that automatically notifies an OnStar operator if the vehicle is involved in a collision, complements the Vue's top five-star safety rating for the driver and front passenger in both front-and sideimpact crash tests.\"", "labels": [], "entities": [{"text": "NYT corpus", "start_pos": 33, "end_pos": 43, "type": "DATASET", "confidence": 0.9728599786758423}, {"text": "GM", "start_pos": 46, "end_pos": 48, "type": "DATASET", "confidence": 0.9092265963554382}]}, {"text": "The dependency parser finds the object of \"complement\" is \"rating\", which is outside the 9-word window.", "labels": [], "entities": []}, {"text": "We propose enhancing state-of-the-art web-scale n-gram models for spelling correction with syntactic structures and distributional information.", "labels": [], "entities": [{"text": "spelling correction", "start_pos": 66, "end_pos": 85, "type": "TASK", "confidence": 0.9500093162059784}]}, {"text": "For our work, we build on a baseline system that combines n-gram and lexical features ().", "labels": [], "entities": []}, {"text": "Specifically, this paper makes the following contributions: 1.", "labels": [], "entities": []}, {"text": "We show that the baseline system can be improved by augmenting it with dependency parse features.", "labels": [], "entities": [{"text": "dependency parse", "start_pos": 71, "end_pos": 87, "type": "TASK", "confidence": 0.7033862322568893}]}, {"text": "2. We show that the impact of parse features can be further augmented when combined with distributional information, specifically word cooccurrence information.", "labels": [], "entities": []}, {"text": "In the following section, we describe related work and how our approach differs from these approaches.", "labels": [], "entities": []}, {"text": "In Sections 3 and 4, we discuss our methods for using parse features and word co-occurrence information.", "labels": [], "entities": []}, {"text": "In Section 5, we present experimental results and analysis.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate the effectiveness of syntactic and distributional information on spelling correction.", "labels": [], "entities": [{"text": "spelling correction", "start_pos": 77, "end_pos": 96, "type": "TASK", "confidence": 0.9148365259170532}]}, {"text": "The performance of the system is measured by accuracy: the percentage of sentences in the test data for which the system chooses the correct word.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.9993250370025635}]}, {"text": "We compare our results against two baselines: 1) MA-JOR chooses the most frequent candidate from the confusion set in the training corpus, and 2) Bergsma et al.'s (2010) best systems, NG+LEX.", "labels": [], "entities": []}, {"text": "We include inflectional variants (\"-ing\", \"-ed\", \"-s\", \"-ly\") of confusion words in the evaluation, such as complementing, complimenting in addition to complement, compliment, because this better corresponds to the range of errors that maybe encountered in actual use and thus increases the scope of the system as areal world application.", "labels": [], "entities": []}, {"text": "Also following, we use a linear SVM, more exactly, the L2-regularized L2-loss dual SVM in LIBLINEAR.", "labels": [], "entities": []}, {"text": "Unlike Bergsma et al., who used development data to optimize parameters, we always use default parameters, since training data is limited for many of the words we are dealing with.", "labels": [], "entities": []}, {"text": "We present the results for each set separately because each set may behave very differently, depending upon its frequency, part-of-speech, number of senses and other differences between the words in each confusion set.", "labels": [], "entities": []}, {"text": "The overall accuracy across confusion sets is also presented to show the effectiveness of different approaches.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9992089867591858}]}, {"text": "The results are tested for statistical significance using McNemar's test of correlated proportions.", "labels": [], "entities": [{"text": "McNemar's test", "start_pos": 58, "end_pos": 72, "type": "DATASET", "confidence": 0.7759558955828348}]}, {"text": "The performance differences are marked as significant when p < 0.05.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Spelling correction precision (%), impact of adding parse features  SVM trained on 1G words of news text, tested on 9-months of NYT data.  *: Improvement of (NG+)LEX+PAR vs. (NG+)LEX is statistically significant.  \u03b1: Improvement of NG+LEX+PAR vs. NG is statistically significant.  &: Relative increase or decrease of error rate compared to \"NG+LEX\"  #: As in Bergsma et al. (2009; 2010) no morphological variants of the words are used in evaluation", "labels": [], "entities": [{"text": "Spelling correction", "start_pos": 10, "end_pos": 29, "type": "TASK", "confidence": 0.7352987825870514}, {"text": "precision", "start_pos": 30, "end_pos": 39, "type": "METRIC", "confidence": 0.6294640898704529}, {"text": "NYT data", "start_pos": 138, "end_pos": 146, "type": "DATASET", "confidence": 0.9553810954093933}, {"text": "Relative increase or decrease of error rate", "start_pos": 294, "end_pos": 337, "type": "METRIC", "confidence": 0.8901201401438031}]}, {"text": " Table 4: Spelling correction accuracy (%), impact of combining word co-occurrence  CLASSIFIER: Logistic Regression trained on 1G words of news text, tested on 9-months NYT data.  COMBINED SYSTEM: CLASSIFER plus system based on first-order word co-occurrence.  &: Relative increase or decrease in error rate compared to CLASSIFIER  #: As in", "labels": [], "entities": [{"text": "Spelling correction", "start_pos": 10, "end_pos": 29, "type": "TASK", "confidence": 0.6142571568489075}, {"text": "accuracy", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.5128244161605835}, {"text": "NYT data", "start_pos": 169, "end_pos": 177, "type": "DATASET", "confidence": 0.9558503925800323}, {"text": "COMBINED", "start_pos": 180, "end_pos": 188, "type": "METRIC", "confidence": 0.9447222352027893}, {"text": "Relative increase or decrease in error rate", "start_pos": 264, "end_pos": 307, "type": "METRIC", "confidence": 0.8574985606329781}]}]}