{"title": [{"text": "Lateen EM: Unsupervised Training with Multiple Objectives, Applied to Dependency Grammar Induction", "labels": [], "entities": [{"text": "Dependency Grammar Induction", "start_pos": 70, "end_pos": 98, "type": "TASK", "confidence": 0.6552505294481913}]}], "abstractContent": [{"text": "We present new training methods that aim to mitigate local optima and slow convergence in unsupervised training by using additional imperfect objectives.", "labels": [], "entities": []}, {"text": "In its simplest form, lateen EM alternates between the two objectives of ordinary \"soft\" and \"hard\" expectation max-imization (EM) algorithms.", "labels": [], "entities": []}, {"text": "Switching objectives when stuck can help escape local optima.", "labels": [], "entities": []}, {"text": "We find that applying a single such alternation already yields state-of-the-art results for En-glish dependency grammar induction.", "labels": [], "entities": [{"text": "En-glish dependency grammar induction", "start_pos": 92, "end_pos": 129, "type": "TASK", "confidence": 0.563680000603199}]}, {"text": "More elaborate lateen strategies track both objectives , with each validating the moves proposed by the other.", "labels": [], "entities": []}, {"text": "Disagreements can signal earlier opportunities to switch or terminate, saving iterations.", "labels": [], "entities": []}, {"text": "De-emphasizing fixed points in these ways eliminates some guesswork from tuning EM.", "labels": [], "entities": [{"text": "tuning EM", "start_pos": 73, "end_pos": 82, "type": "TASK", "confidence": 0.5190807282924652}]}, {"text": "An evaluation against a suite of unsu-pervised dependency parsing tasks, fora variety of languages, showed that lateen strategies significantly speedup training of both EM algorithms , and improve accuracy for hard EM.", "labels": [], "entities": [{"text": "dependency parsing tasks", "start_pos": 47, "end_pos": 71, "type": "TASK", "confidence": 0.7990133166313171}, {"text": "accuracy", "start_pos": 197, "end_pos": 205, "type": "METRIC", "confidence": 0.9980311989784241}]}], "introductionContent": [{"text": "Expectation maximization (EM) algorithms) play important roles in learning latent linguistic structure.", "labels": [], "entities": [{"text": "Expectation maximization (EM)", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.6292797148227691}, {"text": "learning latent linguistic structure", "start_pos": 66, "end_pos": 102, "type": "TASK", "confidence": 0.7539052069187164}]}, {"text": "Unsupervised techniques from this family excel at core natural language processing (NLP) tasks, including segmentation, alignment, tagging and parsing.", "labels": [], "entities": []}, {"text": "Typical implementations specify a probabilistic framework, pick an initial model instance, and iteratively improve parameters using EM.", "labels": [], "entities": []}, {"text": "A key guarantee is that subsequent model instances are no worse than the previous, according to training data likelihood in the given framework.", "labels": [], "entities": []}, {"text": "Another attractive feature that helped make EM instrumental) is its initial efficiency: Training tends to begin with large steps in a parameter space, sometimes bypassing many local optima at once.", "labels": [], "entities": [{"text": "EM", "start_pos": 44, "end_pos": 46, "type": "TASK", "confidence": 0.9786462187767029}]}, {"text": "After a modest number of such iterations, however, EM lands close to an attractor.", "labels": [], "entities": []}, {"text": "Next, its convergence rate necessarily suffers: Disproportionately many (and ever-smaller) steps are needed to finally approach this fixed point, which is almost invariably a local optimum.", "labels": [], "entities": []}, {"text": "Deciding when to terminate EM often involves guesswork; and finding ways out of local optima requires trial and error.", "labels": [], "entities": [{"text": "EM", "start_pos": 27, "end_pos": 29, "type": "TASK", "confidence": 0.8456240892410278}]}, {"text": "We propose several strategies that address both limitations.", "labels": [], "entities": []}, {"text": "Unsupervised objectives are, at best, loosely correlated with extrinsic performance.", "labels": [], "entities": []}, {"text": "This fact justifies (occasionally) deviating from a prescribed training course.", "labels": [], "entities": []}, {"text": "For example, since multiple equi-plausible objectives are usually available, a learner could cycle through them, optimizing alternatives when the primary objective function gets stuck; or, instead of trying to escape, it could aim to avoid local optima in the first place, by halting search early if an improvement to one objective would come at the expense of harming another.", "labels": [], "entities": []}, {"text": "We test these general ideas by focusing on nonconvex likelihood optimization using EM.", "labels": [], "entities": [{"text": "nonconvex likelihood optimization", "start_pos": 43, "end_pos": 76, "type": "TASK", "confidence": 0.7003067235151926}]}, {"text": "This setting is standard and has natural and well-understood objectives: the classic, \"soft\" EM; and Viterbi, or \"hard\" EM ().", "labels": [], "entities": [{"text": "Viterbi", "start_pos": 101, "end_pos": 108, "type": "METRIC", "confidence": 0.9607187509536743}]}, {"text": "The name \"lateen\" comes from the sea -triangular lateen sails can take wind on either side, enabling sailing vessels to tack (see).", "labels": [], "entities": []}, {"text": "As a captain can't count on favorable winds, so an unsupervised learner can't rely on co-operative gradients: soft EM maximizes 1269: A triangular sail atop a traditional Arab sailing vessel, the dhow (right).", "labels": [], "entities": [{"text": "soft EM maximizes 1269", "start_pos": 110, "end_pos": 132, "type": "METRIC", "confidence": 0.8283779174089432}]}, {"text": "Older square sails permitted sailing only before the wind.", "labels": [], "entities": []}, {"text": "But the efficient lateen sail worked like a wing (with high pressure on one side and low pressure on the other), allowing a ship to go almost directly into a headwind.", "labels": [], "entities": []}, {"text": "By tacking, in a zig-zag pattern, it became possible to sail in any direction, provided there was some wind at all (left).", "labels": [], "entities": [{"text": "tacking", "start_pos": 3, "end_pos": 10, "type": "TASK", "confidence": 0.9568349123001099}]}, {"text": "For centuries seafarers expertly combined both sails to traverse extensive distances, greatly increasing the reach of medieval navigation.", "labels": [], "entities": []}, {"text": "likelihoods of observed data across assignments to hidden variables, whereas hard EM focuses on most likely completions.", "labels": [], "entities": []}, {"text": "These objectives are plausible, yet both can be provably \"wrong\" (Spitkovsky et al., 2010a, \u00a77.3).", "labels": [], "entities": []}, {"text": "Thus, it is permissible for lateen EM to maneuver between their gradients, for example by tacking around local attractors, in a zig-zag fashion.", "labels": [], "entities": []}], "datasetContent": [{"text": "We now summarize our baseline models and briefly review the proposed lateen algorithms.", "labels": [], "entities": []}, {"text": "For details of the default systems (standard soft and hard EM), all control variables and both regressions (against final accuracies and iteration counts) see Appendix A.  Statistical techniques are vital to many aspects of computational linguistics.", "labels": [], "entities": [{"text": "computational linguistics", "start_pos": 224, "end_pos": 249, "type": "TASK", "confidence": 0.7524774968624115}]}, {"text": "We used factorial designs, which are standard throughout the natural and social sciences, to assist with experimental design and statistical analyses.", "labels": [], "entities": []}, {"text": "Combined with ordinary regressions, these methods provide succinct and interpretable summaries that explain which settings meaningfully contribute to changes in dependent variables, such as running time and accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 207, "end_pos": 215, "type": "METRIC", "confidence": 0.98618084192276}]}], "tableCaptions": [{"text": " Table 4: Regressions for accuracies and natural-log-iterations, using 86 binary predictors (all p-values jointly adjusted  for simultaneous hypothesis testing; {langyear} indicators not shown). Accuracies' estimated coefficients\u02c6\u03b2coefficients\u02c6 coefficients\u02c6\u03b2 that are  statistically different from 0 -and iteration counts' multipliers e", "labels": [], "entities": [{"text": "Regressions", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9671880006790161}]}, {"text": " Table 5: Performance (directed dependency accuracies measured against all sentences in the evaluation sets) and  efficiency (numbers of iterations) for standard training (soft and hard EM), early-stopping lateen EM (A3) and simple  lateen EM with hard EM's primary objective (A1 h ), for all 23 train/test splits, with adhoc and sweet settings on.", "labels": [], "entities": [{"text": "A1 h )", "start_pos": 277, "end_pos": 283, "type": "METRIC", "confidence": 0.9202192823092142}]}, {"text": " Table 6: Performance (directed dependency accuracies measured against all sentences in the evaluation sets) and  efficiency (numbers of iterations) for standard training (soft and hard EM), early-stopping lateen EM (A3) and simple  lateen EM with hard EM's primary objective (A1 h ), for all 23 train/test splits, with setting adhoc off and sweet on.", "labels": [], "entities": [{"text": "A1 h )", "start_pos": 277, "end_pos": 283, "type": "METRIC", "confidence": 0.9349661469459534}]}]}