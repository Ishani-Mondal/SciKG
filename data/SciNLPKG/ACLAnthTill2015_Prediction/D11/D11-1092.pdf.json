{"title": [{"text": "Harnessing different knowledge sources to measure semantic relatedness under a uniform model", "labels": [], "entities": []}], "abstractContent": [{"text": "Measuring semantic relatedness between words or concepts is a crucial process to many Natural Language Processing tasks.", "labels": [], "entities": []}, {"text": "Exiting methods exploit semantic evidence from a single knowledge source, and are predominantly evaluated only in the general domain.", "labels": [], "entities": []}, {"text": "This paper introduces a method of harnessing different knowledge sources under a uniform model for measuring semantic relatedness between words or concepts.", "labels": [], "entities": []}, {"text": "Using Wikipedia and WordNet as examples, and evaluated in both the general and biomedical domains, it successfully combines strengths from both knowledge sources and outperforms state-of-the-art on many datasets.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 20, "end_pos": 27, "type": "DATASET", "confidence": 0.9345932602882385}]}], "introductionContent": [{"text": "Semantic relatedness (SR) measures how much two (strings of) words or concepts are related by encompassing all kinds of relations between them ().", "labels": [], "entities": [{"text": "Semantic relatedness (SR)", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.801631510257721}]}, {"text": "It is more general than semantic similarity.", "labels": [], "entities": [{"text": "semantic similarity", "start_pos": 24, "end_pos": 43, "type": "TASK", "confidence": 0.7316225171089172}]}, {"text": "SR is often an important pre-processing step to many complex Natural Language Processing (NLP) tasks, such as Word Sense Disambiguation (, and information retrieval ().", "labels": [], "entities": [{"text": "SR", "start_pos": 0, "end_pos": 2, "type": "TASK", "confidence": 0.9024097323417664}, {"text": "Word Sense Disambiguation", "start_pos": 110, "end_pos": 135, "type": "TASK", "confidence": 0.6479161381721497}, {"text": "information retrieval", "start_pos": 143, "end_pos": 164, "type": "TASK", "confidence": 0.8388157784938812}]}, {"text": "In the biomedical domain, SR is an important technique for discovering gene functions and interactions ().", "labels": [], "entities": [{"text": "SR", "start_pos": 26, "end_pos": 28, "type": "TASK", "confidence": 0.9850100874900818}]}, {"text": "There is an abundant literature on measuring SR between words or concepts.", "labels": [], "entities": [{"text": "SR between words or concepts", "start_pos": 45, "end_pos": 73, "type": "TASK", "confidence": 0.6988615393638611}]}, {"text": "Typically, these methods extract semantic evidence of words and concepts from a background knowledge source, with which their relatedness is assessed.", "labels": [], "entities": []}, {"text": "The knowledge sources can be unstructured documents or (semi-)structured resources such as Wikipedia, WordNet, and domain specific ontologies (e.g., the Gene Ontology 1 ).", "labels": [], "entities": [{"text": "WordNet", "start_pos": 102, "end_pos": 109, "type": "DATASET", "confidence": 0.8932561874389648}]}, {"text": "In this paper, we identify two issues that have not been addressed in the previous works.", "labels": [], "entities": []}, {"text": "First, existing works typically employ a single knowledge source of semantic evidence.", "labels": [], "entities": []}, {"text": "Research ( has shown that the accuracy of an SR method differs depending on the choice of the knowledge sources, and there is no conclusion which knowledge source is superior to others.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9993008375167847}, {"text": "SR", "start_pos": 45, "end_pos": 47, "type": "TASK", "confidence": 0.9729439616203308}]}, {"text": "argue that this indicates different knowledge sources may complement each other.", "labels": [], "entities": []}, {"text": "Second, the majority of SR methods have been evaluated in general domains only, except a few earlier WordNet-based methods that have been adapted to biomedical ontologies and evaluated in that domain (.", "labels": [], "entities": [{"text": "SR", "start_pos": 24, "end_pos": 26, "type": "TASK", "confidence": 0.9720973372459412}]}, {"text": "Given the significant attention that SR has received in specific domains (, evaluation of SR methods in specific domains is increasingly important.", "labels": [], "entities": [{"text": "SR", "start_pos": 37, "end_pos": 39, "type": "TASK", "confidence": 0.9787836074829102}]}, {"text": "This paper addresses these issues by proposing a generic and uniform model for computing SR between words or concepts using multiple knowledge sources, and evaluating the proposed method in both general and specific domains.", "labels": [], "entities": [{"text": "computing SR between words or concepts", "start_pos": 79, "end_pos": 117, "type": "TASK", "confidence": 0.8032630383968353}]}, {"text": "The method combines and integrates semantic evidence of words or concepts extracted from any knowledge source in a generic graph representation, with which the SR between concepts or words is computed.", "labels": [], "entities": []}, {"text": "Using two of the most popular general-domain knowledge sources, Wikipedia and WordNet as examples, the method is evaluated on 7 benchmarking datasets, including three datasets from the biomedical domain and four from the general domain.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 78, "end_pos": 85, "type": "DATASET", "confidence": 0.9258745908737183}]}, {"text": "It has achieved excellent results: compared to the baselines that use each single knowledge sources, combining both knowledge sources has improved the accuracy on all datasets by 2~11%; compared to state-ofthe-art on the general domain datasets, the method achieves the best results on three datasets; and on the other three biomedical datasets, it obtains the best result in one case; and second and third best results on the other two among eight participating methods, where all other competitors exploit some domain-specific knowledge sources.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 151, "end_pos": 159, "type": "METRIC", "confidence": 0.9994433522224426}]}, {"text": "The remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 discusses related work; Section 3 presents the proposed method; Section 4 describes the experiments and evaluation; Section 5 discusses results and findings; Section 6 concludes this paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate the method based on correlation against human judgment (gold standard) on seven benchmarking datasets covering both general and technical domains.", "labels": [], "entities": []}, {"text": "These include four general domain datasets: the show largely varying Inter-Annotator-Agreement (IAA) between the two sets, and argue that they should be treated as separate datasets.", "labels": [], "entities": [{"text": "Inter-Annotator-Agreement (IAA)", "start_pos": 69, "end_pos": 100, "type": "METRIC", "confidence": 0.9034447222948074}]}, {"text": "shows statistics of the seven datasets.", "labels": [], "entities": []}, {"text": "The correlation is computed using the Spearman rank order coefficient for two reasons.", "labels": [], "entities": [{"text": "correlation", "start_pos": 4, "end_pos": 15, "type": "METRIC", "confidence": 0.9788970351219177}, {"text": "Spearman rank order coefficient", "start_pos": 38, "end_pos": 69, "type": "METRIC", "confidence": 0.6322744265198708}]}, {"text": "First, it is a better metric than other alternatives.", "labels": [], "entities": []}, {"text": "Second, it is consistent with the majority of studies such that results can be compared.", "labels": [], "entities": []}, {"text": "Size We distribute feature weights w(l) across different feature types L evenly in each feature representation.", "labels": [], "entities": []}, {"text": "Although show that discriminated feature weights leads to improved accuracy; this is not the focus of this study.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 67, "end_pos": 75, "type": "METRIC", "confidence": 0.9993395209312439}]}, {"text": "Since we aim to investigate the effects of harnessing different knowledge sources, we obtained baseline performances by applying the method to those feature representations based on single knowledge sources (i.e., wk-4F, wk-1F, wn-10F, wn-4F, wn-1F).", "labels": [], "entities": []}, {"text": "show the best results obtained with baselines and corresponding knowledge sources and feature representation.", "labels": [], "entities": []}, {"text": "Given the fact that some datasets (i.e., MC30, Ped30-p, Ped30-c, MeSH36) have a relatively low sample size, we cannot always be sure that correlation values are accurate or occurred by chance.", "labels": [], "entities": [{"text": "MeSH36", "start_pos": 65, "end_pos": 71, "type": "DATASET", "confidence": 0.8858414888381958}]}, {"text": "Therefore, we measure the statistical significance of correlation by computing the pvalue for the correlation values reported for our system in.", "labels": [], "entities": []}, {"text": "For all cases, a p-value of less than 0.001 is obtained, which indicates that correlation values are statistically significant.", "labels": [], "entities": [{"text": "correlation", "start_pos": 78, "end_pos": 89, "type": "METRIC", "confidence": 0.959348201751709}]}], "tableCaptions": [{"text": " Table 1: Information of benchmarking datasets", "labels": [], "entities": []}, {"text": " Table 2: Correlation obtained using WordNet.  Many word pairs are not covered due to sparse  feature space and lack of coverage. Only covered  pairs are accounted.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 37, "end_pos": 44, "type": "DATASET", "confidence": 0.9588909149169922}]}, {"text": " Table 3: Correlation obtained using only  Wikipedia. All word pairs are 100% covered.", "labels": [], "entities": []}, {"text": " Tables 4 -6 show results obtained with  enriched feature representation.", "labels": [], "entities": []}, {"text": " Table 5: Number of feature types with which best  results are obtained on each dataset. KS:  Knowledge Source", "labels": [], "entities": []}]}