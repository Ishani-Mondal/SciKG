{"title": [{"text": "Third-order Variational Reranking on Packed-Shared Dependency Forests", "labels": [], "entities": [{"text": "Variational Reranking", "start_pos": 12, "end_pos": 33, "type": "TASK", "confidence": 0.7790817320346832}]}], "abstractContent": [{"text": "We propose a novel forest reranking algorithm for discriminative dependency parsing based on a variant of Eisner's generative model.", "labels": [], "entities": [{"text": "discriminative dependency parsing", "start_pos": 50, "end_pos": 83, "type": "TASK", "confidence": 0.6372174123922983}]}, {"text": "In our framework, we define two kinds of gener-ative model for reranking.", "labels": [], "entities": []}, {"text": "One is learned from training data offline and the other from a forest generated by a baseline parser on the fly.", "labels": [], "entities": []}, {"text": "The final prediction in the reranking stage is performed using linear interpolation of these models and discriminative model.", "labels": [], "entities": []}, {"text": "In order to efficiently train the model from and decode on a hypergraph data structure representing a forest, we apply extended inside/outside and Viterbi algorithms.", "labels": [], "entities": []}, {"text": "Experimental results show that our proposed forest reranking algorithm achieves significant improvement when compared with conventional approaches.", "labels": [], "entities": [{"text": "forest reranking", "start_pos": 44, "end_pos": 60, "type": "TASK", "confidence": 0.680732786655426}]}], "introductionContent": [{"text": "Recently, much of research on statistical parsing has been focused on k-best (or forest) reranking.", "labels": [], "entities": [{"text": "statistical parsing", "start_pos": 30, "end_pos": 49, "type": "TASK", "confidence": 0.7810793519020081}]}, {"text": "Typically, reranking methods first generate a list of top-k candidates (or a forest) from a baseline system, then rerank the candidates with arbitrary features that are intractable within the baseline system.", "labels": [], "entities": []}, {"text": "In the reranking framework, the baseline system is usually modeled with a generative model, and a discriminative model is used for reranking.", "labels": [], "entities": []}, {"text": "reversed the usual order of the two models for dependency parsing by employing a generative model to rescore the k-best candidates provided by a discriminative model.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 47, "end_pos": 65, "type": "TASK", "confidence": 0.847721517086029}]}, {"text": "They use a variant of Eisner's generative model C) for reranking and extend it to capture higher-order information than Eisner's second-order generative model.", "labels": [], "entities": []}, {"text": "Their reranking model showed large improvements in dependency parsing accuracy.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 51, "end_pos": 69, "type": "TASK", "confidence": 0.8220913410186768}, {"text": "accuracy", "start_pos": 70, "end_pos": 78, "type": "METRIC", "confidence": 0.9528309106826782}]}, {"text": "They reported that the discriminative model is very effective at filtering out bad candidates, while the generative model is able to further refine the selection among the few best candidates.", "labels": [], "entities": [{"text": "generative", "start_pos": 105, "end_pos": 115, "type": "TASK", "confidence": 0.9594132900238037}]}, {"text": "In this paper, we propose a forest generative reranking algorithm, opposed to's approach which reranks only k-best candidates.", "labels": [], "entities": []}, {"text": "Forests usually encode better candidates more compactly than k-best lists.", "labels": [], "entities": []}, {"text": "Moreover, our reranking uses not only a generative model obtained from training data, but also a sentence specific generative model learned from a forest.", "labels": [], "entities": []}, {"text": "In the reranking stage, we use linearly combined model of these models.", "labels": [], "entities": []}, {"text": "We call this variational reranking model.", "labels": [], "entities": []}, {"text": "The model proposed in this paper is factored in the third-order structure, therefore, its non-locality makes it difficult to perform the reranking with an usual 1-best Viterbi search.", "labels": [], "entities": []}, {"text": "To solve this problem, we also propose anew search algorithm, which is inspired by the third-order dynamic programming parsing algorithm (.", "labels": [], "entities": [{"text": "dynamic programming parsing", "start_pos": 99, "end_pos": 126, "type": "TASK", "confidence": 0.6596909761428833}]}, {"text": "This algorithm enables us an exact 1-best reranking without any approximation.", "labels": [], "entities": []}, {"text": "We summarize our contributions in this paper as follows.", "labels": [], "entities": []}, {"text": "\u2022 To extend k-best to forest generative reranking.", "labels": [], "entities": [{"text": "forest generative reranking", "start_pos": 22, "end_pos": 49, "type": "TASK", "confidence": 0.6723732153574625}]}, {"text": "\u2022 We introduce variational reranking which is a combination approach of generative reranking and variational decoding ().", "labels": [], "entities": []}, {"text": "\u2022 To obtain 1-best tree in the reranking stage, wepropose an exact 1-best search algorithm with the third-order model.", "labels": [], "entities": []}, {"text": "In experiments on English Penn Treebank data, we show that our proposed methods bring significant improvement to dependency parsing.", "labels": [], "entities": [{"text": "English Penn Treebank data", "start_pos": 18, "end_pos": 44, "type": "DATASET", "confidence": 0.9553235918283463}, {"text": "dependency parsing", "start_pos": 113, "end_pos": 131, "type": "TASK", "confidence": 0.8684280514717102}]}, {"text": "Moreover, our variational reranking framework achieves consistent improvement, compared to conventional approaches, such as simple k-best and forest-based generative reranking algorithms.", "labels": [], "entities": []}], "datasetContent": [{"text": "Experiments are performed on English Penn Treebank data.", "labels": [], "entities": [{"text": "English Penn Treebank data", "start_pos": 29, "end_pos": 55, "type": "DATASET", "confidence": 0.9456460922956467}]}, {"text": "We split WSJ part of the Treebank into sections 02-21 for training, sections 22 for development, sections 23 for testing.", "labels": [], "entities": [{"text": "WSJ part of the Treebank", "start_pos": 9, "end_pos": 33, "type": "DATASET", "confidence": 0.6960046768188477}]}, {"text": "We use's head rules to convert phrase structure to dependency structure.", "labels": [], "entities": []}, {"text": "We obtain k-best lists and forests generated from the baseline discriminative model which has the same feature set as provided in), using the secondorder Eisner algorithms.", "labels": [], "entities": []}, {"text": "We use MIRA for training as it is one of the learning algorithms that achieves the best performance in dependency parsing.", "labels": [], "entities": [{"text": "MIRA", "start_pos": 7, "end_pos": 11, "type": "METRIC", "confidence": 0.5520801544189453}, {"text": "dependency parsing", "start_pos": 103, "end_pos": 121, "type": "TASK", "confidence": 0.8319567143917084}]}, {"text": "We set the scaling factor \u03b3 = 1.0.", "labels": [], "entities": []}, {"text": "We also train a generative reranking model from the training data.", "labels": [], "entities": [{"text": "generative reranking", "start_pos": 16, "end_pos": 36, "type": "TASK", "confidence": 0.9359825551509857}]}, {"text": "To reduce the data sparseness problem, we use the back-off strategy proposed in.", "labels": [], "entities": []}, {"text": "Parameters \u03b8 are trained using MERT and for each sentence in the development data, 300-best dependency trees are extracted from its forest.", "labels": [], "entities": [{"text": "MERT", "start_pos": 31, "end_pos": 35, "type": "METRIC", "confidence": 0.7313637137413025}]}, {"text": "Our variational reranking does not need much time to train the model because the training is performed over not the training data (39832 sentences) but the development data (1700 sentences) . After MERT was performed until the convergence, the variational reranking finally achieved a 94.5 accuracy score on development data.", "labels": [], "entities": [{"text": "MERT", "start_pos": 198, "end_pos": 202, "type": "METRIC", "confidence": 0.894565761089325}, {"text": "accuracy", "start_pos": 290, "end_pos": 298, "type": "METRIC", "confidence": 0.9956396818161011}]}, {"text": "shows the relationship between the size of data structure (the number of hyperedges) and accuracy scores on development data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 89, "end_pos": 97, "type": "METRIC", "confidence": 0.9992832541465759}]}, {"text": "Obviously, forests can encode a large number of potential candidates more compactly than k-best lists.", "labels": [], "entities": []}, {"text": "This means that for reranking, there is more possibility of selecting good candidates in forests than k-best lists.", "labels": [], "entities": [{"text": "reranking", "start_pos": 20, "end_pos": 29, "type": "TASK", "confidence": 0.970805287361145}]}, {"text": "shows the statistics of forests and 20-best lists on development data.", "labels": [], "entities": []}, {"text": "This setting, threshold \u03c1 = 10 \u22123 for pruning, is also used for testing.", "labels": [], "entities": []}, {"text": "Forests, which have an average of 180.67 hyperedges per sentence, achieve oracle score of 98.76, which is about 1.0% higher than the 96.78 oracle score of 20-best lists with 255.04 hyperedges per sentence.", "labels": [], "entities": []}, {"text": "Though the size of forests is smaller than that of k-best lists, the oracle scores of forests are much higher than those of k-best lists.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: The statistics of forests and 20-best lists on de- velopment data: this shows the average number of hyper- edges and nodes per sentence and oracle scores.  forest  20-best  pruning threshold  \u03c1 = 10 \u22123  - ave. num of hyperedges  180.67  255.04  ave. num of nodes  135.74  491.42  oracle scores  98.76  96.78", "labels": [], "entities": []}, {"text": " Table 7: The parsing time (CPU second per sentence) and  accuracy score of the baseline k-best, generative rerank- ing and variational reranking parsers  k  baseline  generative  variational  2 0.09 (91.9) +0.03 (92.67) +0.05 (92.76)  4  0.1 (91.9) +0.05 (92.68) +0.09 (92.81)  8 0.13 (91.9) +0.06 (92.72) +0.11 (92.87)  16 0.18 (91.9) +0.07 (92.75) +0.12 (92.89)  32 0.29 (91.9) +0.07 (92.73) +0.13 (92.89)  64 0.54 (91.9) +0.08 (92.72) +0.15 (92.87)", "labels": [], "entities": [{"text": "accuracy", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.9994202852249146}, {"text": "generative rerank- ing", "start_pos": 97, "end_pos": 119, "type": "TASK", "confidence": 0.7669287323951721}]}, {"text": " Table 8: The comparison of tri-sibling and grandsibling  models: the performance of the grandsibling model out- performs that of the tri-sibling model.  P P P P P P P P", "labels": [], "entities": []}, {"text": " Table 10: Comparison of our best result (using 16-best  forests) with other best-performing Systems on the whole  section 23  Parser  English  McDonald et al. (2005)  90.9  McDonald and Pereira (2006)  91.5  Koo et al. (2008) standard  92.02  Huang and Sagae (2010)  92.1  Koo and Collins (2010) model1  93.04  Koo and Collins (2010) model2  92.93  this work  92.89  Koo et al. (2008) semi-sup  93.16  Suzuki et al. (2009)  93.79", "labels": [], "entities": [{"text": "Parser  English  McDonald et al. (2005)  90.9  McDonald and Pereira (2006)  91.5  Koo et al. (2008) standard  92.02  Huang and Sagae (2010)  92.1  Koo and Collins (2010) model1  93.04  Koo", "start_pos": 127, "end_pos": 315, "type": "DATASET", "confidence": 0.9260223990394956}]}, {"text": " Table 9: Examples of outputs for input sentence No.148 and No.283 in section 23 from baseline and variational  reranking parsers. The underlined portions show the effect of the grandsibling model.  sent (No.148) A quick turnaround is crucial to Quantum because its cash requirements remain heavy .  correct  3 3 4 0 4 5 6  4  11 11  12 8 12 4  baseline  3 3 4 0 4 5 6  4  11 11  8 8 12 4  proposed  3 3 4 0 4 5 6  4  11 11  12 8 12 4  sent (No.283) Many called it simply a contrast in styles .  correct  2 0 2 6 6 2  6 7  2  baseline  2 0 2 2 6 2  6 7  2  proposed  2 0 2 6 6 2  6 7  2", "labels": [], "entities": []}]}