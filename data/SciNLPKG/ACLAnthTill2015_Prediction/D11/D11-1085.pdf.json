{"title": [{"text": "Minimum Imputed Risk: Unsupervised Discriminative Training for Machine Translation", "labels": [], "entities": [{"text": "Minimum Imputed Risk", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.7406065265337626}, {"text": "Machine Translation", "start_pos": 63, "end_pos": 82, "type": "TASK", "confidence": 0.8362172842025757}]}], "abstractContent": [{"text": "Discriminative training for machine translation has been well studied in the recent past.", "labels": [], "entities": [{"text": "Discriminative training", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.8849520981311798}, {"text": "machine translation", "start_pos": 28, "end_pos": 47, "type": "TASK", "confidence": 0.8016358911991119}]}, {"text": "A limitation of the work to date is that it relies on the availability of high-quality in-domain bilingual text for supervised training.", "labels": [], "entities": []}, {"text": "We present an unsupervised discriminative training framework to incorporate the usually plentiful target-language monolingual data by using a rough \"reverse\" translation system.", "labels": [], "entities": []}, {"text": "Intuitively , our method strives to ensure that prob-abilistic \"round-trip\" translation from a target-language sentence to the source-language and back will have low expected loss.", "labels": [], "entities": []}, {"text": "Theoretically , this maybe justified as (discrimina-tively) minimizing an imputed empirical risk.", "labels": [], "entities": []}, {"text": "Empirically, we demonstrate that augmenting supervised training with unsupervised data improves translation performance over the supervised case for both IWSLT and NIST tasks.", "labels": [], "entities": [{"text": "translation", "start_pos": 96, "end_pos": 107, "type": "TASK", "confidence": 0.9599167704582214}, {"text": "IWSLT", "start_pos": 154, "end_pos": 159, "type": "DATASET", "confidence": 0.7124667763710022}]}], "introductionContent": [{"text": "Missing data is a common problem in statistics when fitting the parameters \u03b8 of a model.", "labels": [], "entities": []}, {"text": "A common strategy is to attempt to impute, or \"fill in,\" the missing data (, as typified by the EM algorithm.", "labels": [], "entities": []}, {"text": "In this paper we develop imputation techniques when \u03b8 is to be trained discriminatively.", "labels": [], "entities": []}, {"text": "We focus on machine translation (MT) as our example application.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 12, "end_pos": 36, "type": "TASK", "confidence": 0.8741721749305725}]}, {"text": "A Chinese-to-English machine translation system is given a Chinese sentence x and * Zhifei Li is currently working at Google Research, and this work was done while he was a PHD student at Johns Hopkins University.", "labels": [], "entities": [{"text": "Chinese-to-English machine translation", "start_pos": 2, "end_pos": 40, "type": "TASK", "confidence": 0.6298740208148956}]}, {"text": "asked to predict its English translation y.", "labels": [], "entities": [{"text": "English translation", "start_pos": 21, "end_pos": 40, "type": "TASK", "confidence": 0.4653982073068619}]}, {"text": "This system employs statistical models p \u03b8 (y | x) whose parameters \u03b8 are discriminatively trained using bilingual sentence pairs (x, y).", "labels": [], "entities": []}, {"text": "But bilingual data for such supervised training maybe relatively scarce fora particular language pair (e.g., Urdu-English), especially for some topics (e.g., technical manuals) or genres (e.g., blogs).", "labels": [], "entities": []}, {"text": "So systems seek to exploit additional monolingual data, i.e., a corpus of English sentences y with no corresponding source-language sentences x, to improve estimation of \u03b8.", "labels": [], "entities": []}, {"text": "This is our missing data scenario.", "labels": [], "entities": []}, {"text": "Discriminative training of the parameters \u03b8 of p \u03b8 (y | x) using monolingual English data is a curious idea, since there is no Chinese input x to translate.", "labels": [], "entities": [{"text": "Discriminative training", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.8654866516590118}]}, {"text": "We propose an unsupervised training approach, called minimum imputed risk training, which is conceptually straightforward: First guess x (probabilistically) from the observed y using a reverse Englishto-Chinese translation model p \u03c6 (x | y).", "labels": [], "entities": []}, {"text": "Then train the discriminative Chinese-to-English model p \u03b8 (y | x) to do a good job at translating this imputed x back toy, as measured by a given performance metric.", "labels": [], "entities": []}, {"text": "Intuitively, our method strives to ensure that probabilistic \"round-trip\" translation from a targetlanguage sentence to the source-language and back again will have low expected loss.", "labels": [], "entities": []}, {"text": "Our approach can be applied in an application scenario where we have (1) enough out-of-domain bilingual data to build two baseline translation systems, with parameters \u03b8 for the forward direction, and \u03c6 for the reverse direction; (2) a small amount of in-domain bilingual development data to discriminatively tune a small number of parameters in \u03c6; and (3) a large amount of in-domain English monolingual data.", "labels": [], "entities": []}, {"text": "The novelty here is to exploit (3) to discriminatively tune the parameters \u03b8 of all translation model components, 2 p \u03b8 (y|x) and p \u03b8 (y), not merely train a generative language model p \u03b8 (y), as is the norm.", "labels": [], "entities": []}, {"text": "Following the theoretical development below, the empirical effectiveness of our approach is demonstrated by replacing a key supervised discriminative training step in the development of large MT systems -learning the log-linear combination of several component model scores (viewed as features) to optimize a performance metric (e.g. BLEU) on a set of (x, y) pairs -with our unsupervised discriminative training using only y.", "labels": [], "entities": [{"text": "MT", "start_pos": 192, "end_pos": 194, "type": "TASK", "confidence": 0.9798989295959473}, {"text": "BLEU", "start_pos": 334, "end_pos": 338, "type": "METRIC", "confidence": 0.9918962121009827}]}, {"text": "One may hence contrast our approach with the traditional supervised methods applied to the MT task such as minimum error rate training, the averaged Perceptron (), maximum conditional likelihood (, minimum risk, and MIRA (.", "labels": [], "entities": [{"text": "MT task", "start_pos": 91, "end_pos": 98, "type": "TASK", "confidence": 0.9364136755466461}, {"text": "MIRA", "start_pos": 216, "end_pos": 220, "type": "METRIC", "confidence": 0.9922521710395813}]}, {"text": "We perform experiments using the open-source MT toolkit Joshua (, and show that adding unsupervised data to the traditional supervised training setup improves performance.", "labels": [], "entities": [{"text": "MT toolkit Joshua", "start_pos": 45, "end_pos": 62, "type": "DATASET", "confidence": 0.6736669937769572}]}], "datasetContent": [{"text": "We report results on Chinese-to-English translation tasks using Joshua, an open-source implementation of Hiero (Chiang, 2007).", "labels": [], "entities": [{"text": "Chinese-to-English translation tasks", "start_pos": 21, "end_pos": 57, "type": "TASK", "confidence": 0.715824544429779}, {"text": "Hiero (Chiang, 2007)", "start_pos": 105, "end_pos": 125, "type": "DATASET", "confidence": 0.8804517388343811}]}], "tableCaptions": [{"text": " Table 2: BLEU scores for semi-supervised training for  IWSLT task. The supervised system (\"Sup\") is trained  on a subset of Dev \u03b8 containing 200 Chinese sentences  and 200\u00d716 English translations. \"+Unsup\" means that  we include additional (monolingual) English sentences  from Dev \u03b8 for semi-supervised training; for each En- glish sentence, we impute the 1-best Chinese translation.  A star  *  indicates a result that is signicantly better than  the \"Sup\" baseline (paired permutation test, p < 0.05).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9993219375610352}, {"text": "IWSLT task", "start_pos": 56, "end_pos": 66, "type": "TASK", "confidence": 0.5201026499271393}]}, {"text": " Table 3: BLEU scores for semi-supervised training for  NIST task. The \"Sup\" system is trained on MT03, while  the \"+Unsup\" system is trained with additional 1788 En- glish sentences from MT04. (Note that while MT04 has  1788\u00d74 English sentences as it has four sets of refer- ences, we only use one such set, for computational ef- ficiency of discriminative training.) A star  *  indicates a  result that is signicantly better than the \"Sup\" baseline  (paired permutation test, p < 0.05).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9992639422416687}, {"text": "MT03", "start_pos": 98, "end_pos": 102, "type": "DATASET", "confidence": 0.9701391458511353}, {"text": "MT04", "start_pos": 188, "end_pos": 192, "type": "DATASET", "confidence": 0.9467644691467285}]}]}