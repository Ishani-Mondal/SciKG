{"title": [{"text": "Language Models for Machine Translation: Original vs. Translated Texts", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 20, "end_pos": 39, "type": "TASK", "confidence": 0.7164265811443329}]}], "abstractContent": [{"text": "We investigate the differences between language models compiled from original target-language texts and those compiled from texts manually translated to the target language.", "labels": [], "entities": []}, {"text": "Corroborating established observations of Translation Studies, we demonstrate that the latter are significantly better predictors of translated sentences than the former, and hence fit the reference set better.", "labels": [], "entities": [{"text": "Translation Studies", "start_pos": 42, "end_pos": 61, "type": "TASK", "confidence": 0.9472154378890991}]}, {"text": "Furthermore, translated texts yield better language models for statistical machine translation than original texts.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 63, "end_pos": 94, "type": "TASK", "confidence": 0.7113769054412842}]}], "introductionContent": [{"text": "Statistical machine translation (MT) uses large target language models (LMs) to improve the fluency of generated texts, and it is commonly assumed that for constructing language models, \"more data is better data\".", "labels": [], "entities": [{"text": "Statistical machine translation (MT)", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.8262436091899872}]}, {"text": "Not all data, however, are created the same.", "labels": [], "entities": []}, {"text": "In this work we explore the differences between LMs compiled from texts originally written in the target language and LMs compiled from translated texts.", "labels": [], "entities": []}, {"text": "The motivation for our work stems from much research in Translation Studies that suggests that original texts are significantly different from translated ones in various aspects.", "labels": [], "entities": [{"text": "Translation Studies", "start_pos": 56, "end_pos": 75, "type": "TASK", "confidence": 0.9745065271854401}]}, {"text": "Recently, corpus-based computational analysis corroborated this observation, and apply it to statistical machine translation, showing that for an English-to-French MT system, a translation model trained on an English-translated-toFrench parallel corpus is better than one trained on French-translated-to-English texts.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 93, "end_pos": 124, "type": "TASK", "confidence": 0.6301915347576141}, {"text": "MT", "start_pos": 164, "end_pos": 166, "type": "TASK", "confidence": 0.8840839862823486}]}, {"text": "Our research question is whether a language model compiled from translated texts may similarly improve the results of machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 118, "end_pos": 137, "type": "TASK", "confidence": 0.7161561399698257}]}, {"text": "We test this hypothesis on several translation tasks, where the target language is always English.", "labels": [], "entities": [{"text": "translation tasks", "start_pos": 35, "end_pos": 52, "type": "TASK", "confidence": 0.9082154929637909}]}, {"text": "For each language pair we build two English language models from two types of corpora: texts originally written in English, and human translations from the source language into English.", "labels": [], "entities": []}, {"text": "We show that for each language pair, the latter language model better fits a set of reference translations in terms of perplexity.", "labels": [], "entities": []}, {"text": "We also demonstrate that the differences between the two LMs are not biased by content but rather reflect differences on abstract linguistic features.", "labels": [], "entities": []}, {"text": "Research in Translation Studies suggests that all translated texts, irrespective of source language, share some so-called translation universals.", "labels": [], "entities": [{"text": "Translation Studies", "start_pos": 12, "end_pos": 31, "type": "TASK", "confidence": 0.951825737953186}]}, {"text": "Consequently, translated texts from several languages to a single target language resemble each other along various axes.", "labels": [], "entities": []}, {"text": "To test this hypothesis, we compile additional English LMs, this time using texts translated to English from languages other than the source.", "labels": [], "entities": []}, {"text": "Again, we use perplexity to assess the fit of these LMs to reference sets of translated-to-English sentences.", "labels": [], "entities": []}, {"text": "We show that these LMs depend on the source language and differ from each other.", "labels": [], "entities": []}, {"text": "Whereas they outperform original-based LMs, LMs compiled from texts that were translated from the source language still fit the reference set best.", "labels": [], "entities": []}, {"text": "Finally, we train phrase-based MT systems () for each language pair.", "labels": [], "entities": [{"text": "MT", "start_pos": 31, "end_pos": 33, "type": "TASK", "confidence": 0.8834816813468933}]}, {"text": "We use four types of LMs: original; translated from 363 the source language; translated from other languages; and a mixture of translations from several languages.", "labels": [], "entities": []}, {"text": "We show that the translatedfrom-source-language LMs provide a significant improvement in the quality of the translation output overall other LMs, and that the mixture LMs always outperform the original LMs.", "labels": [], "entities": []}, {"text": "This improvement persists even when the original LMs are up to ten times larger than the translated ones.", "labels": [], "entities": []}, {"text": "The main contributions of this work are therefore a computational corroboration of the hypotheses that 1.", "labels": [], "entities": []}, {"text": "original and translated texts exhibit significant, measurable differences; 2.", "labels": [], "entities": []}, {"text": "LMs compiled from translated texts better fit translated references than LMs compiled from original texts of the same (and much larger) size (and, to a lesser extent, LMs compiled from texts translated from languages other than the source language); and 3.", "labels": [], "entities": []}, {"text": "MT systems that use LMs based on manually translated texts significantly outperform LMs based on originally written texts.", "labels": [], "entities": [{"text": "MT", "start_pos": 0, "end_pos": 2, "type": "TASK", "confidence": 0.9638113379478455}]}, {"text": "It is important to emphasize that translated texts abound: Many languages, especially lowresource ones, are more likely to have translated texts (religious scripts, educational materials, etc.) than original ones.", "labels": [], "entities": []}, {"text": "Some numeric data are listed in.", "labels": [], "entities": []}, {"text": "Furthermore, such data can be automatically identified (see Section 2).", "labels": [], "entities": []}, {"text": "The practical impact of our work on MT is therefore potentially dramatic.", "labels": [], "entities": [{"text": "MT", "start_pos": 36, "end_pos": 38, "type": "TASK", "confidence": 0.991363525390625}]}, {"text": "This paper is organized as follows: Section 2 provides background and describes related work.", "labels": [], "entities": []}, {"text": "We explain our research methodology and resources in Section 3 and detail our experiments and results in Section 4.", "labels": [], "entities": []}, {"text": "Section 5 discusses the results and their implications.", "labels": [], "entities": []}], "datasetContent": [{"text": "We detail in this section the experiments performed to test the three hypotheses: that translated texts can be distinguished from original ones, and provide better language models of other translated texts; that texts translated from other languages than the source are still better predictors of translations than original texts (Section 4.1); and that these differences are important for SMT", "labels": [], "entities": [{"text": "SMT", "start_pos": 390, "end_pos": 393, "type": "TASK", "confidence": 0.9937264919281006}]}], "tableCaptions": [{"text": " Table 1: Europarl corpus statistics", "labels": [], "entities": [{"text": "Europarl corpus statistics", "start_pos": 10, "end_pos": 36, "type": "DATASET", "confidence": 0.9709089199701945}]}, {"text": " Table 2: Hansard corpus statistics", "labels": [], "entities": [{"text": "Hansard corpus statistics", "start_pos": 10, "end_pos": 35, "type": "DATASET", "confidence": 0.9569821953773499}]}, {"text": " Table 3: Hebrew-to-English corpus statistics", "labels": [], "entities": []}, {"text": " Table 4: SMT training data details", "labels": [], "entities": [{"text": "SMT training", "start_pos": 10, "end_pos": 22, "type": "TASK", "confidence": 0.9100176692008972}]}, {"text": " Table 6: Fitness of various LMs to the reference set", "labels": [], "entities": []}, {"text": " Table 7: Fitness of O-vs. T-based LMs to the refer- ence set (DE-EN), different abstraction levels", "labels": [], "entities": []}, {"text": " Table 8: Fitness of O-vs. T-based LMs to the refer- ence set (HE-EN)", "labels": [], "entities": []}, {"text": " Table 9: Fitness of O-vs. T-based LMs to the refer- ence set (HE-EN), different abstraction levels", "labels": [], "entities": []}, {"text": " Table 10: Machine translation with various LMs", "labels": [], "entities": [{"text": "Machine translation", "start_pos": 11, "end_pos": 30, "type": "TASK", "confidence": 0.8340139985084534}]}, {"text": " Table 12: The effect of LM size on MT performance", "labels": [], "entities": [{"text": "MT", "start_pos": 36, "end_pos": 38, "type": "TASK", "confidence": 0.98639976978302}]}]}