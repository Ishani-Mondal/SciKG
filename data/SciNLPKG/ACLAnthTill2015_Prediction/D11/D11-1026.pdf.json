{"title": [{"text": "Linear Text Segmentation Using Affinity Propagation", "labels": [], "entities": [{"text": "Linear Text Segmentation", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.5766231516997019}]}], "abstractContent": [{"text": "This paper presents anew algorithm for linear text segmentation.", "labels": [], "entities": [{"text": "linear text segmentation", "start_pos": 39, "end_pos": 63, "type": "TASK", "confidence": 0.7172085046768188}]}, {"text": "It is an adaptation of Affinity Propagation, a state-of-the-art clustering algorithm in the framework of factor graphs.", "labels": [], "entities": []}, {"text": "Affinity Propagation for Segmenta-tion, or APS, receives a set of pairwise similarities between data points and produces segment boundaries and segment centres-data points which best describe all other data points within the segment.", "labels": [], "entities": []}, {"text": "APS iteratively passes messages in a cyclic factor graph, until convergence.", "labels": [], "entities": []}, {"text": "Each iteration works with information on all available similarities, resulting in high-quality results.", "labels": [], "entities": []}, {"text": "APS scales linearly for realistic segmentation tasks.", "labels": [], "entities": [{"text": "APS", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.669377863407135}]}, {"text": "We derive the algorithm from the original Affinity Propagation formulation , and evaluate its performance on topical text segmentation in comparison with two state-of-the art segmenters.", "labels": [], "entities": [{"text": "topical text segmentation", "start_pos": 109, "end_pos": 134, "type": "TASK", "confidence": 0.6810776988665262}]}, {"text": "The results suggest that APS performs on par with or outper-forms these two very competitive baselines.", "labels": [], "entities": [{"text": "APS", "start_pos": 25, "end_pos": 28, "type": "TASK", "confidence": 0.7997157573699951}]}], "introductionContent": [{"text": "In complex narratives, it is typical for the topic to shift continually.", "labels": [], "entities": []}, {"text": "Some shifts are gradual, othersmore abrupt.", "labels": [], "entities": []}, {"text": "Topical text segmentation identifies the more noticeable topic shifts.", "labels": [], "entities": [{"text": "Topical text segmentation", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.7608767946561178}]}, {"text": "A topical segmenter's output is a very simple picture of the document's structure.", "labels": [], "entities": []}, {"text": "Segmentation is a useful intermediate step in such applications as subjectivity analysis, automatic summarization (, question answering and others.", "labels": [], "entities": [{"text": "Segmentation", "start_pos": 0, "end_pos": 12, "type": "TASK", "confidence": 0.9256001710891724}, {"text": "subjectivity analysis", "start_pos": 67, "end_pos": 88, "type": "TASK", "confidence": 0.8020735681056976}, {"text": "automatic summarization", "start_pos": 90, "end_pos": 113, "type": "TASK", "confidence": 0.60389444231987}, {"text": "question answering", "start_pos": 117, "end_pos": 135, "type": "TASK", "confidence": 0.8841440379619598}]}, {"text": "That is why improved quality of text segmentation can benefit other language-processing tasks.", "labels": [], "entities": [{"text": "text segmentation", "start_pos": 32, "end_pos": 49, "type": "TASK", "confidence": 0.706781342625618}]}, {"text": "We present Affinity Propagation for Segmentation (APS), an adaptation of a state-of-the-art clustering algorithm, Affinity Propagation (.", "labels": [], "entities": [{"text": "Affinity", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.9579417705535889}]}, {"text": "The original AP algorithm considerably improved exemplarbased clustering both in terms of speed and the quality of solutions.", "labels": [], "entities": [{"text": "exemplarbased clustering", "start_pos": 48, "end_pos": 72, "type": "TASK", "confidence": 0.5781170427799225}]}, {"text": "That is why we chose to adapt it to segmentation.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 36, "end_pos": 48, "type": "TASK", "confidence": 0.9700128436088562}]}, {"text": "At its core, APS is suitable for segmenting any sequences of data, but we present it in the context of segmenting documents.", "labels": [], "entities": [{"text": "APS", "start_pos": 13, "end_pos": 16, "type": "TASK", "confidence": 0.9082319140434265}]}, {"text": "APS takes as input a matrix of pairwise similarities between sentences and, for each sentence, a preference value which indicates an a priori belief in how likely a sentence is to be chosen as a segment centre.", "labels": [], "entities": [{"text": "APS", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.47065597772598267}]}, {"text": "APS outputs segment assignments and segment centres -data points which best explain all other points in a segment.", "labels": [], "entities": [{"text": "APS outputs segment assignments", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.6158119887113571}]}, {"text": "The algorithm attempts to maximize net similarity -the sum of similarities between all data points and their respective segment centres.", "labels": [], "entities": [{"text": "net similarity -", "start_pos": 35, "end_pos": 51, "type": "METRIC", "confidence": 0.650950292746226}]}, {"text": "APS operates by iteratively passing messages in a factor graph) until a good set of segments emerges.", "labels": [], "entities": [{"text": "APS", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.7739715576171875}]}, {"text": "Each iteration considers all similarities -takes into account all available information.", "labels": [], "entities": []}, {"text": "An iteration includes sending at most O(N 2 ) messages.", "labels": [], "entities": []}, {"text": "For the majority of realistic segmentation tasks, however, the upper bound is O(M N ) messages, where M is a constant.", "labels": [], "entities": [{"text": "segmentation tasks", "start_pos": 30, "end_pos": 48, "type": "TASK", "confidence": 0.9000048935413361}, {"text": "O", "start_pos": 78, "end_pos": 79, "type": "METRIC", "confidence": 0.9746730923652649}]}, {"text": "This is more computationally expensive than the requirements of locally informed segmentation algorithms such as those based on HMM or CRF (see Section 2), but fora globallyinformed algorithm the requirements are very reasonable.", "labels": [], "entities": []}, {"text": "APS is an instance of loopy-belief propagation (belief propagation on cyclic graphs) which has been used to achieved state-of-the-art performance in error-correcting decoding, image processing and data compression.", "labels": [], "entities": [{"text": "APS", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.6279253363609314}, {"text": "loopy-belief propagation (belief propagation on cyclic graphs", "start_pos": 22, "end_pos": 83, "type": "TASK", "confidence": 0.7895576767623425}, {"text": "image processing", "start_pos": 176, "end_pos": 192, "type": "TASK", "confidence": 0.7942157983779907}, {"text": "data compression", "start_pos": 197, "end_pos": 213, "type": "TASK", "confidence": 0.7660307884216309}]}, {"text": "Theoretically, such algorithms are not guaranteed to converge or to maximize the objective function.", "labels": [], "entities": []}, {"text": "Yet in practice they often achieve competitive results.", "labels": [], "entities": []}, {"text": "APS works on an already pre-compiled similaritiy matrix, so it offers flexibility in the choice of similarity metrics.", "labels": [], "entities": [{"text": "APS", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8617268800735474}]}, {"text": "The desired number of segments can beset by adjusting preferences.", "labels": [], "entities": []}, {"text": "We evaluate the performance of APS on three tasks: finding topical boundaries in transcripts of course lectures), identifying sections in medical textbooks and identifying chapter breaks in novels.", "labels": [], "entities": [{"text": "APS", "start_pos": 31, "end_pos": 34, "type": "TASK", "confidence": 0.9192345142364502}]}, {"text": "We compare APS with two recent systems: the Minimum Cut segmenter) and the Bayesian segmenter.", "labels": [], "entities": [{"text": "APS", "start_pos": 11, "end_pos": 14, "type": "TASK", "confidence": 0.9150069355964661}, {"text": "Minimum Cut segmenter", "start_pos": 44, "end_pos": 65, "type": "TASK", "confidence": 0.5664055148760477}]}, {"text": "The comparison is based on the WindowDiff metric).", "labels": [], "entities": []}, {"text": "APS matches or outperforms these very competitive baselines.", "labels": [], "entities": [{"text": "APS", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.4960229992866516}]}, {"text": "Section 2 of the paper outlines relevant research on topical text segmentation.", "labels": [], "entities": [{"text": "topical text segmentation", "start_pos": 53, "end_pos": 78, "type": "TASK", "confidence": 0.7751277089118958}]}, {"text": "Section 3 briefly covers the framework of factor graphs and outlines the original Affinity Propagation algorithm for clustering.", "labels": [], "entities": []}, {"text": "Section 4 contains the derivation of the new update messages for APSeg.", "labels": [], "entities": []}, {"text": "Section 5 describes the experimental setting, Section 6 reports the results, Section 7 discusses conclusions and future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate the performance of the APS algorithm on three datasets.", "labels": [], "entities": [{"text": "APS", "start_pos": 35, "end_pos": 38, "type": "TASK", "confidence": 0.9083261489868164}]}, {"text": "The first, compiled by, consists of manually transcribed and segmented lectures on Artificial Intelligence, 3 development files and 19 test files.", "labels": [], "entities": []}, {"text": "The second dataset consists of 227 chapters from medical textbooks, 5 of which we use for development.", "labels": [], "entities": []}, {"text": "In this dataset the gold standard segment boundaries correspond to section breaks specified by the authors.", "labels": [], "entities": []}, {"text": "The third dataset consists of 85 works of fiction downloaded from Project Gutenberg, 3 of which are used for development.", "labels": [], "entities": []}, {"text": "The segment boundaries correspond to chapter breaks or to breaks between individual stories.", "labels": [], "entities": []}, {"text": "They were inserted automatically using HTML markup in the downloaded files.", "labels": [], "entities": []}, {"text": "The datasets exhibit different characteristics.", "labels": [], "entities": []}, {"text": "The lecture dataset and the fiction dataset are challenging because they are less cohesive than medical textbooks.", "labels": [], "entities": [{"text": "lecture dataset", "start_pos": 4, "end_pos": 19, "type": "DATASET", "confidence": 0.7581335008144379}]}, {"text": "The textbooks are cognitively more difficult to process and the authors rely on repetition of terminology to facilitate comprehension.", "labels": [], "entities": []}, {"text": "Since lexical repetition is the main source of information for text segmentation, we expect a higher performance on this dataset.", "labels": [], "entities": [{"text": "text segmentation", "start_pos": 63, "end_pos": 80, "type": "TASK", "confidence": 0.7740415632724762}]}, {"text": "Transcribed speech, on the other hand, is considerably less cohesive.", "labels": [], "entities": []}, {"text": "The lecturer makes an effort to speak in \"plain language\" and to be comprehensible, relying lesson terminology.", "labels": [], "entities": []}, {"text": "The use of pronouns is very common, as is the use of examples.", "labels": [], "entities": []}, {"text": "Repeated use of the same words is also uncommon in fiction.", "labels": [], "entities": []}, {"text": "In addition, the dataset was compiled automatically using HTML markup.", "labels": [], "entities": []}, {"text": "The markup is not always reliable and occasionally the e-book proofreaders skip it altogether, which potentially adds noise to the dataset.", "labels": [], "entities": []}, {"text": "We compare the performance of APS with that of two state-of-the-art segmenters: the Minimum Cut segmenter) and the Bayesian segmenter.", "labels": [], "entities": [{"text": "Minimum Cut segmenter", "start_pos": 84, "end_pos": 105, "type": "TASK", "confidence": 0.522294282913208}]}, {"text": "The authors have made Java implementations publicly available.", "labels": [], "entities": []}, {"text": "For the Minimum Cut segmenter, we select the best parameters using the script included with that distribution.", "labels": [], "entities": [{"text": "Minimum Cut segmenter", "start_pos": 8, "end_pos": 29, "type": "TASK", "confidence": 0.6056336661179861}]}, {"text": "The Bayesian segmenter automatically estimates all necessary parameters from the data.", "labels": [], "entities": [{"text": "Bayesian segmenter", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.6307227611541748}]}, {"text": "Preprocessing and the choice of similarity metric.", "labels": [], "entities": [{"text": "Preprocessing", "start_pos": 0, "end_pos": 13, "type": "METRIC", "confidence": 0.9671717882156372}]}, {"text": "As described in Section 4, the APS algorithm takes as inputs a matrix of pairwise similarities between sentences in the document and also, for each sentence, a preference value.", "labels": [], "entities": [{"text": "APS", "start_pos": 31, "end_pos": 34, "type": "TASK", "confidence": 0.9427215456962585}]}, {"text": "This paper focuses on comparing globally informed segmentation algorithms, and leaves for future work the exploration of best similarity metrics.", "labels": [], "entities": []}, {"text": "To allow fair comparison, then, we use the same metric as the Minimum Cut segmenter, cosine similarity.", "labels": [], "entities": [{"text": "Minimum Cut segmenter", "start_pos": 62, "end_pos": 83, "type": "TASK", "confidence": 0.5344166656335195}]}, {"text": "Each sentence is represented as a vector of token-type frequencies.", "labels": [], "entities": []}, {"text": "Following), the frequency vectors are smoothed by adding counts of words from the adjacent sentences and then weighted using a tf.idf metric (for details, see ibid.)", "labels": [], "entities": []}, {"text": "The similarity between sentence vectors s 1 and s 2 is computed as follows: The representation used by the Bayesian segmenter is too different to be incorporated into our model directly, but ultimately it is based on the distribution of unigrams in documents.", "labels": [], "entities": []}, {"text": "This is close enough to our representation to allow fair comparison.", "labels": [], "entities": []}, {"text": "The fiction dataset consists of books: novels or collections of short stories.", "labels": [], "entities": []}, {"text": "Fiction is known to exhibit less lexical cohesion.", "labels": [], "entities": []}, {"text": "That is why -when working on this dataset -we work at the paragraph level: the similarity is measured not between sentences but between paragraphs.", "labels": [], "entities": [{"text": "similarity", "start_pos": 79, "end_pos": 89, "type": "METRIC", "confidence": 0.9557555317878723}]}, {"text": "We use this representation with all three segmenters.", "labels": [], "entities": []}, {"text": "All parameters have been fine-tuned on the development portions of the datasets.", "labels": [], "entities": []}, {"text": "For APS algorithm per se we needed to set three parameters: the size of the sliding window for similarity computations, the dampening factor \u03bb and the preference values.", "labels": [], "entities": [{"text": "APS", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.8995380401611328}]}, {"text": "parameters for the similarity metric (best variation of tf.idf, the window size and the decay factor for smoothing) were set using the script provided in the Minimum Cut segmenter's distribution.", "labels": [], "entities": [{"text": "Minimum Cut segmenter's distribution", "start_pos": 158, "end_pos": 194, "type": "DATASET", "confidence": 0.6119575142860413}]}, {"text": "We have measured the performance of the segmenters with the WindowDiff metric ().", "labels": [], "entities": []}, {"text": "It is computed by sliding a window through reference and through segmentation output and, at each window position, comparing the number of reference breaks to the number of breaks inserted by the segmenter (hypothetical breaks).", "labels": [], "entities": []}, {"text": "It is a penalty measure which reports the number of windows where the reference and hypothetical breaks do not match, normalized by the total number of windows.", "labels": [], "entities": []}, {"text": "In Equation 21, ref and hyp denote the number of reference and hypothetical segment breaks within a window.", "labels": [], "entities": []}, {"text": "6 Experimental Results and Discussion compares the performance of the three segmenters using WindowDiff values.", "labels": [], "entities": []}, {"text": "On the lecture and fiction datasets, the APS segmenter outperforms the others by a small margin, around 8% over the better of the two.", "labels": [], "entities": [{"text": "lecture and fiction datasets", "start_pos": 7, "end_pos": 35, "type": "DATASET", "confidence": 0.6685860455036163}, {"text": "APS segmenter", "start_pos": 41, "end_pos": 54, "type": "TASK", "confidence": 0.6515957117080688}]}, {"text": "It is second-best on the clinical textbook dataset.", "labels": [], "entities": [{"text": "clinical textbook dataset", "start_pos": 25, "end_pos": 50, "type": "DATASET", "confidence": 0.664914478858312}]}, {"text": "According to a one-tailed paired t-test with 95% confidence cut-off, the improvement is statistically significant only on the fiction dataset.", "labels": [], "entities": [{"text": "fiction dataset", "start_pos": 126, "end_pos": 141, "type": "DATASET", "confidence": 0.7862752377986908}]}, {"text": "All datasets are challenging and the baselines are very competitive, so drawing definitive conclusions is difficult.", "labels": [], "entities": []}, {"text": "Still, we can be fairly confident that APS performs at least as well as the other two segmenters.", "labels": [], "entities": [{"text": "APS", "start_pos": 39, "end_pos": 42, "type": "TASK", "confidence": 0.6987060308456421}]}, {"text": "It also has certain advantages.", "labels": [], "entities": []}, {"text": "One important difference between APS and the other segmenters is that APS does not require the number of segments as an input parameter.", "labels": [], "entities": []}, {"text": "This is very helpful, because such information is generally unavailable in any realistic deployment setting.", "labels": [], "entities": []}, {"text": "The parameters are fine-tuned to maximize WindowDiff values, so this results in high-precision, low-recall segment assignments; that is because WindowDiff favours missing boundaries over near-hits.", "labels": [], "entities": []}, {"text": "APS also outputs segment centres, thus providing some information about a segment's topic.", "labels": [], "entities": [{"text": "APS", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8004116415977478}]}, {"text": "We have not evaluated how descriptive the segment centres are; this is left for future work.", "labels": [], "entities": []}, {"text": "APS performs slightly better than the other segmenters but not by much.", "labels": [], "entities": [{"text": "APS", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.4964296817779541}]}, {"text": "We hypothesize that one of the reasons is that APS relies on the presence of descriptive segment centres which are not necessarily present for large, coarse-grained segments such as chapters in novels.", "labels": [], "entities": [{"text": "APS", "start_pos": 47, "end_pos": 50, "type": "TASK", "confidence": 0.9464564323425293}]}, {"text": "It is possible for APS to have an advantage performing fine-grained segmentation.", "labels": [], "entities": [{"text": "APS", "start_pos": 19, "end_pos": 22, "type": "TASK", "confidence": 0.9419835209846497}]}], "tableCaptions": [{"text": " Table 1: Results of segmenting the three datasets us- ing the Bayesian segmenter, the Minimum Cut seg- menter and APS.", "labels": [], "entities": [{"text": "APS", "start_pos": 115, "end_pos": 118, "type": "METRIC", "confidence": 0.7482439875602722}]}]}