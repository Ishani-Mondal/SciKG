{"title": [{"text": "Unsupervised Learning of Selectional Restrictions and Detection of Argument Coercions", "labels": [], "entities": [{"text": "Selectional Restrictions", "start_pos": 25, "end_pos": 49, "type": "TASK", "confidence": 0.8472999036312103}]}], "abstractContent": [{"text": "Metonymic language is a pervasive phenomenon.", "labels": [], "entities": [{"text": "Metonymic language", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8691206872463226}]}, {"text": "Metonymic type shifting, or argument type coercion, results in a selectional restriction violation where the argument's semantic class differs from the class the predicate expects.", "labels": [], "entities": [{"text": "Metonymic type shifting", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.79176398118337}]}, {"text": "In this paper we present an un-supervised method that learns the selectional restriction of arguments and enables the detection of argument coercion.", "labels": [], "entities": []}, {"text": "This method also generates an enhanced probabilistic resolution of logical metonymies.", "labels": [], "entities": []}, {"text": "The experimental results indicate substantial improvements the detection of coercions and the ranking of metonymic interpretations.", "labels": [], "entities": []}], "introductionContent": [{"text": "Metonymic language is pervasive in today's social interactions.", "labels": [], "entities": []}, {"text": "For example, it is typical to find questions that require metonymic resolution: (Q1) Did you enjoy War and Peace?", "labels": [], "entities": [{"text": "metonymic resolution", "start_pos": 58, "end_pos": 78, "type": "TASK", "confidence": 0.7490347325801849}]}, {"text": "(Q2) Does anyone have any advice on how to start a bowling team?", "labels": [], "entities": []}, {"text": "1 In order to process such questions and capture the intention of the person that posed them, coercions are needed.", "labels": [], "entities": []}, {"text": "Question (Q1) is interpreted as whether you enjoyed reading \"War and Peace\", while (Q2) is interpreted as asking for advice on organizing, forming, or registering a bowling team.", "labels": [], "entities": [{"text": "reading \"War and Peace\"", "start_pos": 52, "end_pos": 75, "type": "TASK", "confidence": 0.5585141728321711}, {"text": "organizing, forming, or registering a bowling team", "start_pos": 127, "end_pos": 177, "type": "TASK", "confidence": 0.5070887936486138}]}, {"text": "The quality of the answers therefore depends on the ability to (1) recognize when metonymic language is used, and (2) to produce coercions that capture the user's intention.", "labels": [], "entities": []}, {"text": "One important step in this direction was Both questions taken from Yahoo Answers.", "labels": [], "entities": [{"text": "Yahoo Answers", "start_pos": 67, "end_pos": 80, "type": "DATASET", "confidence": 0.8882299065589905}]}, {"text": "taken by, which focused on the ability to recognize (a) an argument's selectional restriction for predicates such as arrive at, cancel, or hear, and (b) the type of coercion that licensed a correct interpretation of the metonymy.", "labels": [], "entities": []}, {"text": "Details of the task are reported in (.", "labels": [], "entities": []}, {"text": "Approaches to metonymy based on this task are limited, however, because (a) the task is focused only on semantically non-ambiguous predicates and (b) the selectional restrictions of the arguments were chosen from a pre-defined set of six semantic classes (artifact, document, event, location, proposition, and sound).", "labels": [], "entities": []}, {"text": "However, metonymy coercion systems capable of providing the interpretations of questions (Q1) and (Q2) clearly cannot operate with the simplifications designed for this task.", "labels": [], "entities": []}, {"text": "Inspired by recent advances in modeling selectional preferences with latent-variable models, we propose an unsupervised model for learning selectional restrictions.", "labels": [], "entities": []}, {"text": "The model assumes that (1) arguments have a single selected class exemplified by the selectional restriction, and (2) the selected class can be inferred from the data, in part by modeling how coercive each predicate is.", "labels": [], "entities": []}, {"text": "The model is capable of operating with both ambiguous and disambiguated predicates, producing superior results for predicates that have been disambiguated.", "labels": [], "entities": []}, {"text": "The selectional restrictions and coercions detected by the model reported in this paper can be used to enhance the logical metonymy approach reported in.", "labels": [], "entities": []}, {"text": "The experimental results show a significant improvement in the ranking of interpretations.", "labels": [], "entities": []}, {"text": "980 The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 discusses related work.", "labels": [], "entities": []}, {"text": "Section 3 details unsupervised models that inform detection of metonymies.", "labels": [], "entities": [{"text": "detection of metonymies", "start_pos": 50, "end_pos": 73, "type": "TASK", "confidence": 0.8448696931203207}]}, {"text": "Section 4 outlines a method for disambiguating ambiguous predicates.", "labels": [], "entities": [{"text": "disambiguating ambiguous predicates", "start_pos": 32, "end_pos": 67, "type": "TASK", "confidence": 0.8654659191767374}]}, {"text": "Section 5 describes the enhanced interpretation of logical metonymies when conventional constraints are known.", "labels": [], "entities": [{"text": "interpretation of logical metonymies", "start_pos": 33, "end_pos": 69, "type": "TASK", "confidence": 0.8174321055412292}]}, {"text": "Section 6 outlines our implementation and experimental design.", "labels": [], "entities": []}, {"text": "Section 7 presents our experimental results in three broad tasks: (i) semantic class induction, (ii) coercion detection, and (iii) logical metonymy interpretation.", "labels": [], "entities": [{"text": "semantic class induction", "start_pos": 70, "end_pos": 94, "type": "TASK", "confidence": 0.7630410393079122}, {"text": "coercion detection", "start_pos": 101, "end_pos": 119, "type": "TASK", "confidence": 0.6992836147546768}, {"text": "logical metonymy interpretation", "start_pos": 131, "end_pos": 162, "type": "TASK", "confidence": 0.6378917197386423}]}, {"text": "Section 8 summarizes the conclusions.", "labels": [], "entities": []}, {"text": "propose a probabilistic ranking model for logical metonymies.", "labels": [], "entities": []}, {"text": "They estimate these probabilities using co-occurrence frequencies of predicate-argument pairs in a corpus.", "labels": [], "entities": []}, {"text": "extends this approach to provide sense-disambiguated interpretations from WordNet by using the alternative interpretations to disambiguate polysemous words.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 74, "end_pos": 81, "type": "DATASET", "confidence": 0.9424982666969299}]}, {"text": "extend this approach further by clustering these sense-disambiguated interpretations into distinct groups of meaning (e.g., {read, browse, look through} and {write, produce, work on} for \"enjoy book\").", "labels": [], "entities": []}, {"text": "Not only do these approaches assume logical metonymies have already been identified, but they are susceptible to providing interpretations that are themselves logical metonymies (e.g., finish book).", "labels": [], "entities": []}, {"text": "In this paper, we propose an enhancement to resolving logical metonymies by ruling out event-invoking predicates in order to provide more semantically valid interpretations.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use the NYT subsection of the English Gigaword Fourth Edition () fora total of 1.8M newswire articles.", "labels": [], "entities": [{"text": "NYT subsection of the English Gigaword Fourth Edition", "start_pos": 11, "end_pos": 64, "type": "DATASET", "confidence": 0.8903777673840523}]}, {"text": "The Stanford Dependency Parser (de) is used to extract verb-object relations (dobj) that form the input to our model.", "labels": [], "entities": []}, {"text": "To reduce noise, we keep only verbs listed in VerbNet () with at least 100 argument instances, discarding have and say, which are too semantically flexible to select from clear semantic classes and so common they distort the class distributions.", "labels": [], "entities": [{"text": "VerbNet", "start_pos": 46, "end_pos": 53, "type": "DATASET", "confidence": 0.9379245638847351}]}, {"text": "This results in 4,145 unique verbs with 51M argument instances (388K unique arguments).", "labels": [], "entities": []}, {"text": "Additionally we use the dependency parser to extract open clausal complements of verbs (e.g., \"like to swim\") for use in logical metonymy interpretation.", "labels": [], "entities": [{"text": "logical metonymy interpretation", "start_pos": 121, "end_pos": 152, "type": "TASK", "confidence": 0.6205432911713918}]}, {"text": "We believe this to be a more reliable alternative to the phrase chunk extraction patterns used in.", "labels": [], "entities": [{"text": "phrase chunk extraction", "start_pos": 57, "end_pos": 80, "type": "TASK", "confidence": 0.6006587743759155}]}, {"text": "We keep clausal complements (xcomp) where the dependent is either a gerund or infinitive in order to estimate P c (v|e) in Equation.", "labels": [], "entities": [{"text": "Equation", "start_pos": 123, "end_pos": 131, "type": "DATASET", "confidence": 0.8673163056373596}]}, {"text": "For tiered clustering we use the same implementation as to partition the surface form of the verb into one or more induced forms.", "labels": [], "entities": []}, {"text": "Instead of using a fixed number of iterations, the clustering was run for 100 iterations past the best recorded log-likelihood in order to find the best possible fit to the data.", "labels": [], "entities": []}, {"text": "We tuned the hyperparameters by maximizing the log-likelihood on a small held-out set of 20 predicate-argument pairs (10 selections, 10 coercions).", "labels": [], "entities": []}, {"text": "The resulting partitions were fairly conservative, yielding 12,332 induced verbs or about 3 induced verb forms for every surface form, with 305 verbs not being partitioned at all.", "labels": [], "entities": []}, {"text": "We implemented both LDA and cLDA as described in Sections 3.1 and 3.2.", "labels": [], "entities": []}, {"text": "For the \u03b1 and \u03b2 hyper-parameters, we used the MALLET) defaults of 1.0 and 0.1, respectively, for both LDA and cLDA.", "labels": [], "entities": [{"text": "MALLET) defaults", "start_pos": 46, "end_pos": 62, "type": "METRIC", "confidence": 0.944959839185079}]}, {"text": "We used the 20 predicateargument pairs mentioned above to tune the \u03b3 hyperparameters as well as the number of iterations.", "labels": [], "entities": []}, {"text": "Both \u03b3 0 and \u03b3 1 were set to 100.", "labels": [], "entities": []}, {"text": "We observed that for both LDA and cLDA, longer runs (in iterations) resulted in improved model log-likelihood but inferior results in terms of detecting coercions.", "labels": [], "entities": []}, {"text": "It is not uncommon in topic modeling for model likelihood to not be completely correlated with the score on the task for which the topic model was intended (see).", "labels": [], "entities": []}, {"text": "Both LDA and cLDA were found to perform best at 50 iterations on this data, after which their class distributions were less \"smooth\" and became rigidly associated with just a few classes, thus having a negative impact on coercion detection.", "labels": [], "entities": [{"text": "coercion detection", "start_pos": 221, "end_pos": 239, "type": "TASK", "confidence": 0.7588821351528168}]}, {"text": "While further iterations hurt coercion detection, only minor gains in model likelihood are seen.", "labels": [], "entities": [{"text": "coercion detection", "start_pos": 30, "end_pos": 48, "type": "TASK", "confidence": 0.7196965217590332}]}, {"text": "We believe the small number of iterations necessary for the model to converge is therefore a function of the data.", "labels": [], "entities": []}, {"text": "In traditional topic modeling, documents are generally of similar size (i.e., within an order of magnitude).", "labels": [], "entities": [{"text": "topic modeling", "start_pos": 15, "end_pos": 29, "type": "TASK", "confidence": 0.803801417350769}]}, {"text": "But in our data, many predicates have 10,000 times more instances than others.", "labels": [], "entities": []}, {"text": "We have not yet empirically explored the impact of using a more uniform number of arguments for each predicate.", "labels": [], "entities": []}, {"text": "This issue also makes it difficult to take multiple samples, which we experimented with unsuccessfully.", "labels": [], "entities": []}, {"text": "Our a priori intuition was that as the number of classes was increased, LDA would improve and cLDA would degrade due to its assumption of a single selected class.", "labels": [], "entities": [{"text": "LDA", "start_pos": 72, "end_pos": 75, "type": "METRIC", "confidence": 0.8885259628295898}]}, {"text": "However, this did not always bear out in the results for every task described below.", "labels": [], "entities": []}, {"text": "As such, instead of choosing a specific number of classes for each model, we describe results for each model with K = 10, 25, and 50.", "labels": [], "entities": []}, {"text": "For logical metonymy, both LM T H and LM W T require learned parameters.", "labels": [], "entities": []}, {"text": "LM T H needs a learned threshold while LM W T needs two learned weights.", "labels": [], "entities": []}, {"text": "For both, we split the data set into two partitions, learn the optimal threshold/weights on one partition, and use it as the parameters for the other partition.", "labels": [], "entities": []}, {"text": "Both methods are trained on the final scoring metric, described in Section 7.3.", "labels": [], "entities": []}, {"text": "For threshold learning, this involves finding the optimal cut-off to maximize the score.", "labels": [], "entities": [{"text": "threshold learning", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.8726643323898315}]}, {"text": "For weight learning, we use an exhaustive   search over the range {1.0, 0.9, . .", "labels": [], "entities": [{"text": "weight learning", "start_pos": 4, "end_pos": 19, "type": "TASK", "confidence": 0.9483103454113007}]}, {"text": ", 0.2, 0.1, 10 \u22122 , 10 \u22123 , . .", "labels": [], "entities": []}, {"text": ", 10 \u221214 } for both w 1 and w 2 .", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Clustering scores for induced classes.", "labels": [], "entities": []}, {"text": " Table 3: Accuracy on SemEval-2010 Task 7 data.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9943917393684387}, {"text": "SemEval-2010 Task 7 data", "start_pos": 22, "end_pos": 46, "type": "DATASET", "confidence": 0.8544071614742279}]}, {"text": " Table 4: Mean average precision (MAP) scores on the Shutova and Teufel (2009) data set. The bold items indicate the  best scores with/without induced predicates as well as using/not using a threshold-based interpretation method.", "labels": [], "entities": [{"text": "Mean average precision (MAP)", "start_pos": 10, "end_pos": 38, "type": "METRIC", "confidence": 0.9655163586139679}, {"text": "Shutova and Teufel (2009) data set", "start_pos": 53, "end_pos": 87, "type": "DATASET", "confidence": 0.6697838455438614}]}, {"text": " Table 5: Mean average precision (MAP) scores on 100 logical metonymies manually annotated with interpretations.  The bold items indicate the best scores with/without induced predicates as well as using/not using a threshold-based  interpretation method.", "labels": [], "entities": [{"text": "Mean average precision (MAP) scores", "start_pos": 10, "end_pos": 45, "type": "METRIC", "confidence": 0.9591738496507917}]}]}