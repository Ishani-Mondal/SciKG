{"title": [{"text": "Extreme Extraction --Machine Reading in a Week", "labels": [], "entities": [{"text": "Extreme Extraction --Machine Reading", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.7110380411148072}]}], "abstractContent": [{"text": "We report on empirical results in extreme extraction.", "labels": [], "entities": [{"text": "extreme extraction", "start_pos": 34, "end_pos": 52, "type": "TASK", "confidence": 0.7365996241569519}]}, {"text": "It is extreme in that (1) from receipt of the ontology specifying the target concepts and relations, development is limited to one week and that (2) relatively little training data is assumed.", "labels": [], "entities": []}, {"text": "We are able to surpass human recall and achieve an F1 of 0.51 on a question-answering task with less than 50 hours of effort using a hybrid approach that mixes active learning, boot-strapping, and limited (5 hours) manual rule writing.", "labels": [], "entities": [{"text": "recall", "start_pos": 29, "end_pos": 35, "type": "METRIC", "confidence": 0.9938187003135681}, {"text": "F1", "start_pos": 51, "end_pos": 53, "type": "METRIC", "confidence": 0.9898931384086609}, {"text": "rule writing", "start_pos": 222, "end_pos": 234, "type": "TASK", "confidence": 0.7172189503908157}]}, {"text": "We compare the performance of three systems: extraction with handwritten rules, bootstrapped extraction, and a combination.", "labels": [], "entities": [{"text": "bootstrapped extraction", "start_pos": 80, "end_pos": 103, "type": "TASK", "confidence": 0.6294542104005814}]}, {"text": "We show that while the recall of the handwritten rules surpasses that of the learned system, the learned system is able to improve the overall recall and F1.", "labels": [], "entities": [{"text": "recall", "start_pos": 23, "end_pos": 29, "type": "METRIC", "confidence": 0.9990412592887878}, {"text": "recall", "start_pos": 143, "end_pos": 149, "type": "METRIC", "confidence": 0.9994103908538818}, {"text": "F1", "start_pos": 154, "end_pos": 156, "type": "METRIC", "confidence": 0.9991629123687744}]}], "introductionContent": [{"text": "Throughout the Automatic Content Extraction 1 (ACE) evaluations and the Message Understanding Conferences 2 (MUC), teams typically had a year or more from release of the target to submitting system results.", "labels": [], "entities": [{"text": "Automatic Content Extraction 1 (ACE)", "start_pos": 15, "end_pos": 51, "type": "TASK", "confidence": 0.7746799417904445}, {"text": "Message Understanding Conferences 2 (MUC)", "start_pos": 72, "end_pos": 113, "type": "TASK", "confidence": 0.7335555468286786}]}, {"text": "One exception was MUC-6, in which scenario templates for changing positions were extracted given only one month.", "labels": [], "entities": [{"text": "MUC-6", "start_pos": 18, "end_pos": 23, "type": "DATASET", "confidence": 0.780465304851532}]}, {"text": "Our goal was to confine development to a calendar week, in fact, <50 person hours.", "labels": [], "entities": []}, {"text": "This 1 http://www.nist.gov/speech/tests/ace/ 2 http://www-nlpir.nist.gov/related_projects/muc/ is significant in two ways: the less effort it takes to bring up anew domain, (1) the more broadly applicable the technology is and (2) the less effort required to run a diagnostic research experiment.", "labels": [], "entities": []}, {"text": "Our second goal concerned minimizing training data.", "labels": [], "entities": [{"text": "minimizing training", "start_pos": 26, "end_pos": 45, "type": "TASK", "confidence": 0.853649228811264}]}, {"text": "Rather than approximately 250k words of entity and relation annotation as in ACE, only ~20 example pairs per relation-type were provided as training.", "labels": [], "entities": []}, {"text": "Reducing the training requirements has the same two desirable outcomes: demonstrating that the technology can be broadly applicable and reducing the overhead for running experiments.", "labels": [], "entities": []}, {"text": "The system achieved recall of 0.49 and precision of 0.53 (for an F1 of 0.51) on a blind test set of 60 queries of the form R i (arg 1 , arg 2 ), where R i is one of the 5 new relations and exactly one of arg 1 or arg 2 is a free variable for each query.", "labels": [], "entities": [{"text": "recall", "start_pos": 20, "end_pos": 26, "type": "METRIC", "confidence": 0.9997301697731018}, {"text": "precision", "start_pos": 39, "end_pos": 48, "type": "METRIC", "confidence": 0.9997523427009583}, {"text": "F1", "start_pos": 65, "end_pos": 67, "type": "METRIC", "confidence": 0.9932897686958313}]}, {"text": "Key to this achievement was a hybrid of: \uf0b7 a variant of) to learn two new classes of entities via automatically induced word classes and active learning (6 hours) \uf0b7 bootstrap relation learning) to learn 5 new relation classes (2.5 hours), \uf0b7 handwritten patterns over predicate-argument structure (5 hours), and \uf0b7 coreference Our bootstrap learner is initialized with relation tuples (not annotated text) and uses LDC's Gigaword and Wikipedia as a background corpus to learn patterns for relation detection that are based on normalized predicate argument structure as well as surface strings.", "labels": [], "entities": [{"text": "relation detection", "start_pos": 487, "end_pos": 505, "type": "TASK", "confidence": 0.7742762863636017}]}, {"text": "These early empirical results suggest the following: (1) It is possible to specify a domain, adapt our system, and complete manual scoring, includ-ing human performance, within a month.", "labels": [], "entities": []}, {"text": "Experiments in machine reading (and in extraction) can be performed much more quickly and cheaply than ever before.", "labels": [], "entities": [{"text": "machine reading", "start_pos": 15, "end_pos": 30, "type": "TASK", "confidence": 0.79527747631073}]}, {"text": "(2) Through machine learning and limited human pattern writing (6 hours), we adapted a machine reading system within a week (using less than 50 person hours), achieving question answering performance with an F1 of 0.5 and with recall 11% higher (relative) to a human reader.", "labels": [], "entities": [{"text": "question answering", "start_pos": 169, "end_pos": 187, "type": "TASK", "confidence": 0.7276558578014374}, {"text": "F1", "start_pos": 208, "end_pos": 210, "type": "METRIC", "confidence": 0.9973042011260986}, {"text": "recall", "start_pos": 227, "end_pos": 233, "type": "METRIC", "confidence": 0.9988815188407898}]}, {"text": "(3) Unfortunately, machine learning, though achieving 80% precision, 3 significantly lags behind a gifted human pattern writer in recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 58, "end_pos": 67, "type": "METRIC", "confidence": 0.9993334412574768}, {"text": "recall", "start_pos": 130, "end_pos": 136, "type": "METRIC", "confidence": 0.9925485253334045}]}, {"text": "Thus, bootstrap learning with much higher recall at minimal sacrifice in precision is highly desirable.", "labels": [], "entities": [{"text": "recall", "start_pos": 42, "end_pos": 48, "type": "METRIC", "confidence": 0.9988458156585693}, {"text": "precision", "start_pos": 73, "end_pos": 82, "type": "METRIC", "confidence": 0.9991357922554016}]}], "datasetContent": [{"text": "Our effort was divided into four phases.", "labels": [], "entities": []}, {"text": "During the first phase, a third party produced an ontology and the resources, which included: brief (~1 paragraph) guidelines for each relation and class in the ontolo-gy; ~20 examples for each relation in the ontology; 2K documents that are rich in domain relations.", "labels": [], "entities": []}, {"text": "In phase two, we spent one week extending our extraction system for the new ontology.", "labels": [], "entities": []}, {"text": "During the third phase, we ran our system over 10K documents to extract all instances of domain relations from those documents.", "labels": [], "entities": []}, {"text": "In the fourth phase, our question answering system used the extracted information to answer queries.", "labels": [], "entities": [{"text": "question answering", "start_pos": 25, "end_pos": 43, "type": "TASK", "confidence": 0.8210391104221344}]}, {"text": "Our question answering evaluation was inspired by the evaluation in DARPA's machine reading program, which requires systems to map the information in text into a formal ontology and answer questions based on that ontology.", "labels": [], "entities": [{"text": "question answering evaluation", "start_pos": 4, "end_pos": 33, "type": "TASK", "confidence": 0.8755974372227987}]}, {"text": "Unlike ACE, this allows evaluators to measure performance without exhaustively annotating documents, allows for balance between rare and common relations, and implicitly measures coreference without requiring explicit annotation of answer keys for coreference.", "labels": [], "entities": []}, {"text": "However because the evaluation only measures performance on the set of queries, many relation instances will be unscored.", "labels": [], "entities": []}, {"text": "Furthermore, the system is not rewarded for finding the same relation multiple times; finding 100 instances of isPossibleTreatment(Penicillin, Strep Throat) is the same as finding 1 (or 10) instances.", "labels": [], "entities": []}, {"text": "The evaluation included only queries of the type Find all instances for which the relation P(X, Z) is true where one of X or Z is constant.", "labels": [], "entities": []}, {"text": "For example, Find possible treatments for diabetes; or What is expected date to market for Abilify?", "labels": [], "entities": []}, {"text": "There were 60 queries in the evaluation set to be answered from a 10K document corpus.", "labels": [], "entities": []}, {"text": "To produce a preliminary answer key, annotators were given the queries and corpus indexed by Google Desktop.", "labels": [], "entities": []}, {"text": "Annotators were given 1 hour to find potential answers to each query.", "labels": [], "entities": []}, {"text": "If no answers were found after 1 hour, the annotators were given a second hour to look for answers.", "labels": [], "entities": []}, {"text": "For two queries, both of the form Find treatments with an expected date to market of MM-YYYY, even after two hours of searching the annotators were unable to find any answers.", "labels": [], "entities": [{"text": "MM-YYYY", "start_pos": 85, "end_pos": 92, "type": "METRIC", "confidence": 0.7096189856529236}]}, {"text": "Annotator answers served as the initial goldstandard.", "labels": [], "entities": [{"text": "goldstandard", "start_pos": 40, "end_pos": 52, "type": "METRIC", "confidence": 0.9114537239074707}]}, {"text": "Given this initial answer key, annotators reviewed system answers and aligned them with gold-standard answers.", "labels": [], "entities": []}, {"text": "System output not aligned with the initial gold standard was assessed as corrector incorrect.", "labels": [], "entities": [{"text": "corrector incorrect", "start_pos": 73, "end_pos": 92, "type": "METRIC", "confidence": 0.9524952173233032}]}, {"text": "We assume that the final goldstandard constitutes a complete answer key, and are thus able to calculate recall for our system and for humans . Because we had only one annotator for each query and because we assumed that any answer found by an annotator was correct, we could not estimate human precision on this task.", "labels": [], "entities": [{"text": "recall", "start_pos": 104, "end_pos": 110, "type": "METRIC", "confidence": 0.9916912317276001}, {"text": "precision", "start_pos": 294, "end_pos": 303, "type": "METRIC", "confidence": 0.9894648790359497}]}, {"text": "Answers can be specific named concepts (e.g. Penicillin) or generic descriptions (e.g. drug, illness).", "labels": [], "entities": []}, {"text": "Given the sentence, ACME produces a wide range of drugs including treatments for malaria and athletes foot,' our reading system would extract the relations responsibleForTreatment(drugs, ACME), possibleTreatment(drugs, malaria), possibleTreatment(drugs, athletes foot).", "labels": [], "entities": []}, {"text": "When a name was available in the document, annotators marked the answer as correct, but underspecified.", "labels": [], "entities": []}, {"text": "We calculated precision and recall treating underspecified answers as incorrect and separately calculated precision and recall counting underspecified answers as correct.", "labels": [], "entities": [{"text": "precision", "start_pos": 14, "end_pos": 23, "type": "METRIC", "confidence": 0.9995798468589783}, {"text": "recall", "start_pos": 28, "end_pos": 34, "type": "METRIC", "confidence": 0.9990676045417786}, {"text": "precision", "start_pos": 106, "end_pos": 115, "type": "METRIC", "confidence": 0.9994171857833862}, {"text": "recall", "start_pos": 120, "end_pos": 126, "type": "METRIC", "confidence": 0.9979488253593445}]}, {"text": "When treated as correct, there was less than a 0.05 absolute increase in both precision and recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 78, "end_pos": 87, "type": "METRIC", "confidence": 0.9997279047966003}, {"text": "recall", "start_pos": 92, "end_pos": 98, "type": "METRIC", "confidence": 0.9985522627830505}]}, {"text": "Unless otherwise specified, all scores reported here use the stricter condition which treats underspecified answers as incorrect.", "labels": [], "entities": []}, {"text": "We also evaluated extracting all information in a small document collection (here human search of the 10k documents does not play a role in finding answers).", "labels": [], "entities": []}, {"text": "Individuals were asked to annotate every instance of the 5 relations in a set of 102 documents.", "labels": [], "entities": []}, {"text": "Recall, Precision, and F were calculated by aligning system responses to the answer key.", "labels": [], "entities": [{"text": "Recall", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9678698182106018}, {"text": "Precision", "start_pos": 8, "end_pos": 17, "type": "METRIC", "confidence": 0.9977268576622009}, {"text": "F", "start_pos": 23, "end_pos": 24, "type": "METRIC", "confidence": 0.9993575215339661}]}, {"text": "System answers that aligned are correct; those that did not are incorrect; and answers in the key that were not found by the system are misses.", "labels": [], "entities": []}, {"text": "Unlike the question answering evaluation, this evaluation measures the ability to find every instance of a fact.", "labels": [], "entities": [{"text": "question answering", "start_pos": 11, "end_pos": 29, "type": "TASK", "confidence": 0.7546803653240204}]}, {"text": "If the gold standard includes 100 instances of isPossibleTreatment(Penicillin, Strep Throat), recall will decrease for each instance missed.", "labels": [], "entities": [{"text": "recall", "start_pos": 94, "end_pos": 100, "type": "METRIC", "confidence": 0.9997914433479309}]}, {"text": "The -extraction\u2016 evaluation does not penalize systems for missing coreference.", "labels": [], "entities": []}, {"text": "The answer key may contain some answers that were found neither by the annotator nor by the systems described here, since the answer key includes answers pooled from other systems not reported in this paper.", "labels": [], "entities": []}, {"text": "The system reported here was the highest performing of all those participating in the experiment.", "labels": [], "entities": []}, {"text": "Furthermore, if a system answer is marked as correct, but underspecified, the specific answer is put in the key.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1 lists the 5 new relations and number of ex- amples provided for each. Arguments in italics  were known by the system prior to the evaluation.  Relation  Ex.", "labels": [], "entities": []}, {"text": " Table 2: Effort and Approach for New Domain", "labels": [], "entities": [{"text": "Effort", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9691592454910278}, {"text": "Approach", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9906355738639832}]}, {"text": " Table 4: Cross Validation: Condition & Substance", "labels": [], "entities": [{"text": "Cross Validation", "start_pos": 10, "end_pos": 26, "type": "TASK", "confidence": 0.829001396894455}]}, {"text": " Table 5: Relative Change in Recall and Precision When  Non-Useful Answers are Removed", "labels": [], "entities": [{"text": "Relative Change in Recall", "start_pos": 10, "end_pos": 35, "type": "METRIC", "confidence": 0.8330521136522293}]}, {"text": " Table 6: Question Answering Results by Relation Type  Relation Type  Total Number of Answers  % Queries with At Least 1 Corr. Ans  A  C  HW  L  A  C  HW  L  possTreatment  66  303  261  100  100.0%  90.0%  90.0%  90.0%  respForTreat  98  67  41  40  100.0%  66.7%  60.0%  60.0%  expectDateMarkt  54  13  12  0  72.7%  45.5%  45.5%  0.0%  studiesDisease  68  379  347  33  100.0%  61.5%  46.2%  46.2%  hasSideEffect  83  12  20  2  72.7%  36.4%  45.5%  18.2%  Total  369  774  681  175  90.0%  60.0%  56.7%  43.3%  Table 7: Number of Answers and Number of Queries Answered", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.8642558157444}]}, {"text": " Table 8: Extraction Results on the 102 Document Test Set Annotated for All Instances of the Relations", "labels": [], "entities": [{"text": "102 Document Test Set Annotated", "start_pos": 36, "end_pos": 67, "type": "DATASET", "confidence": 0.8126705765724183}]}]}