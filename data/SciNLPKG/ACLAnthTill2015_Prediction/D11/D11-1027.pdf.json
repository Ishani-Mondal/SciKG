{"title": [], "abstractContent": [{"text": "This paper develops a minimally supervised approach, based on focused distributional similarity methods and discourse connectives, for identifying of causality relations between events in context.", "labels": [], "entities": []}, {"text": "While it has been shown that distributional similarity can help identifying causality, we observe that discourse con-nectives and the particular discourse relation they evoke in context provide additional information towards determining causality between events.", "labels": [], "entities": []}, {"text": "We show that combining discourse relation predictions and distributional similarity methods in a global inference procedure provides additional improvements towards determining event causality.", "labels": [], "entities": []}], "introductionContent": [{"text": "An important part of text understanding arises from understanding the semantics of events described in the narrative, such as identifying the events that are mentioned and how they are related semantically.", "labels": [], "entities": [{"text": "text understanding", "start_pos": 21, "end_pos": 39, "type": "TASK", "confidence": 0.7702099978923798}]}, {"text": "For instance, when given a sentence \"The police arrested him because he killed someone.\", humans understand that there are two events, triggered by the words \"arrested\" and \"killed\", and that there is a causality relationship between these two events.", "labels": [], "entities": []}, {"text": "Besides being an important component of discourse understanding, automatically identifying causal relations between events is important for various natural language processing (NLP) applications such as question answering, etc.", "labels": [], "entities": [{"text": "discourse understanding", "start_pos": 40, "end_pos": 63, "type": "TASK", "confidence": 0.7054183334112167}, {"text": "question answering", "start_pos": 203, "end_pos": 221, "type": "TASK", "confidence": 0.9147675931453705}]}, {"text": "In this work, we automatically detect and extract causal relations between events in text.", "labels": [], "entities": []}, {"text": "Despite its importance, prior work on event causality extraction in context in the NLP literature is relatively sparse.", "labels": [], "entities": [{"text": "event causality extraction", "start_pos": 38, "end_pos": 64, "type": "TASK", "confidence": 0.7929059465726217}]}, {"text": "In (, the author used noun-verb-noun lexico-syntactic patterns to learn that \"mosquitoes cause malaria\", where the cause and effect mentions are nominals and not necessarily event evoking words.", "labels": [], "entities": []}, {"text": "In (, the authors focused on detecting causality between search query pairs in temporal query logs.) tried to detect causal relations between verbs in a corpus of screen plays, but limited themselves to consecutive, or adjacent verb pairs.", "labels": [], "entities": []}, {"text": "In (, the authors first cluster sentences into topic-specific scenarios, and then focus on building a dataset of causal text spans, where each span is headed by a verb.", "labels": [], "entities": []}, {"text": "Thus, their focus was not on identifying causal relations between events in a given text document.", "labels": [], "entities": []}, {"text": "In this paper, given a text document, we first identify events and their associated arguments.", "labels": [], "entities": []}, {"text": "We then identify causality or relatedness relations between event pairs.", "labels": [], "entities": []}, {"text": "To do this, we develop a minimally supervised approach using focused distributional similarity methods, such as co-occurrence counts of events collected automatically from an unannotated corpus, to measure and predict existence of causality relations between event pairs.", "labels": [], "entities": []}, {"text": "Then, we build on the observation that discourse connectives and the particular discourse relation they evoke in context provide additional information towards determining causality between events.", "labels": [], "entities": []}, {"text": "For instance, in the example sentence provided at the beginning of this section, the words \"arrested\" and \"killed\" probably have a relatively high apriori likelihood of being ca-sually related.", "labels": [], "entities": [{"text": "apriori likelihood", "start_pos": 147, "end_pos": 165, "type": "METRIC", "confidence": 0.934504508972168}]}, {"text": "However, knowing that the connective \"because\" evokes a contingency discourse relation between the text spans \"The police arrested him\" and \"he killed someone\" provides further evidence towards predicting causality.", "labels": [], "entities": []}, {"text": "The contributions of this paper are summarized below: \u2022 Our focus is on identifying causality between event pairs in context.", "labels": [], "entities": []}, {"text": "Since events are often triggered by either verbs (e.g. \"attack\") or nouns (e.g. \"explosion\"), we allow for detection of causality between verb-verb, verb-noun, and noun-noun triggered event pairs.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, this formulation of the task is novel.", "labels": [], "entities": []}, {"text": "\u2022 We developed a minimally supervised approach for the task using focused distributional similarity methods that are automatically collected from an unannotated corpus.", "labels": [], "entities": []}, {"text": "We show that our approach achieves better performance than two approaches: one based on a frequently used metric that measures association, and another based on the effect-control-dependency (ECD) metric described in a prior work ().", "labels": [], "entities": []}, {"text": "\u2022 We leverage on the interactions between event causality prediction and discourse relations prediction.", "labels": [], "entities": [{"text": "event causality prediction", "start_pos": 42, "end_pos": 68, "type": "TASK", "confidence": 0.6979707578818003}, {"text": "discourse relations prediction", "start_pos": 73, "end_pos": 103, "type": "TASK", "confidence": 0.6488375465075175}]}, {"text": "We combine these knowledge sources through a global inference procedure, which we formalize via an Integer Linear Programming (ILP) framework as a constraint optimization problem ().", "labels": [], "entities": []}, {"text": "This allows us to easily define appropriate constraints to ensure that the causality and discourse predictions are coherent with each other, thereby improving the performance of causality identification.", "labels": [], "entities": [{"text": "causality identification", "start_pos": 178, "end_pos": 202, "type": "TASK", "confidence": 0.7545075118541718}]}], "datasetContent": [{"text": "To collect the distributional statistics for measuring CEA as defined in Equation, we applied partof-speech tagging, lemmatization, and dependency parsing () on about 760K documents in the English Gigaword corpus (LDC catalog number LDC2003T05).", "labels": [], "entities": [{"text": "partof-speech tagging", "start_pos": 94, "end_pos": 115, "type": "TASK", "confidence": 0.7573786079883575}, {"text": "dependency parsing", "start_pos": 136, "end_pos": 154, "type": "TASK", "confidence": 0.6910769492387772}, {"text": "English Gigaword corpus (LDC catalog number LDC2003T05)", "start_pos": 189, "end_pos": 244, "type": "DATASET", "confidence": 0.87196946144104}]}, {"text": "We are not aware of any benchmark corpus for evaluating event causality extraction in contexts.", "labels": [], "entities": [{"text": "event causality extraction", "start_pos": 56, "end_pos": 82, "type": "TASK", "confidence": 0.703222652276357}]}, {"text": "Hence, we created an evaluation corpus using the following process: Using news articles collected from CNN 7 during the first three months of 2010, we randomly selected 20 articles (documents) as evaluation data, and 5 documents as development data.", "labels": [], "entities": [{"text": "CNN 7", "start_pos": 103, "end_pos": 108, "type": "DATASET", "confidence": 0.7871673405170441}]}, {"text": "Two annotators annotated the documents for causal event pairs, using two simple notions for causality: the Cause event should temporally precede the Effect event, and the Effect event occurs because the Cause event occurs.", "labels": [], "entities": []}, {"text": "However, sometimes it is debatable whether two events are involved in a causal relation, or whether they are simply involved in an uninteresting temporal relation.", "labels": [], "entities": []}, {"text": "Hence, we allowed annotations of C to indicate causality, and R to indicate relatedness (for situations when the existence of causality is debatable).", "labels": [], "entities": []}, {"text": "The annotators will simply identify and annotate the C or R relations between predicates of event pairs.", "labels": [], "entities": []}, {"text": "Event arguments are not explicitly annotated, although the annotators are free to look at the entire document text while making their annotation decisions.", "labels": [], "entities": []}, {"text": "Finally, they are free   to annotate relations between predicates that have any number of sentences in between and are not restricted to a fixed sentence window-size.", "labels": [], "entities": []}, {"text": "After adjudication, we obtained a total of 492 C +R relation annotations, and 414 C relation annotations on the evaluation documents.", "labels": [], "entities": []}, {"text": "On the development documents, we obtained 92 C + Rand 71 C relation annotations.", "labels": [], "entities": []}, {"text": "The annotators overlapped on 10 evaluation documents.", "labels": [], "entities": [{"text": "overlapped", "start_pos": 15, "end_pos": 25, "type": "METRIC", "confidence": 0.890428900718689}]}, {"text": "On these documents, the first (second) annotator annotated 215 (199) C + R relations, agreeing on 166 of these relations.", "labels": [], "entities": []}, {"text": "Together, they annotated 248 distinct relations.", "labels": [], "entities": []}, {"text": "Using this number, their agreement ratio would be 0.67 (166/248).", "labels": [], "entities": [{"text": "agreement ratio", "start_pos": 25, "end_pos": 40, "type": "METRIC", "confidence": 0.9739595055580139}]}, {"text": "The corresponding agreement ratio for C relations is 0.58.", "labels": [], "entities": []}, {"text": "These numbers highlight that causality identification is a difficult task, as there could be as many as N 2 event pairs in a document (N is the number of events in the document).", "labels": [], "entities": [{"text": "causality identification", "start_pos": 29, "end_pos": 53, "type": "TASK", "confidence": 0.8343980610370636}]}, {"text": "We plan to make this annotated dataset available soon.", "labels": [], "entities": []}, {"text": "8  As mentioned in Section 5.1, to enable translating (the unbounded) CEA scores into binary causal, \u00accausal predictions, we need to rescale or calibrate these scores to range in.", "labels": [], "entities": []}, {"text": "To do this, we first rank all the CEA scores of all event pairs in the development documents.", "labels": [], "entities": [{"text": "CEA scores", "start_pos": 34, "end_pos": 44, "type": "METRIC", "confidence": 0.9147638082504272}]}, {"text": "Most of these event pairs will be \u00accausal.", "labels": [], "entities": []}, {"text": "Based on the relation annotations in these development documents, we scanned through this ranked list of scores to locate the CEA score t that gives the highest F1-score (on the development documents) when used as a threshold between causal vs \u00accausal decisions.", "labels": [], "entities": [{"text": "CEA score t", "start_pos": 126, "end_pos": 137, "type": "METRIC", "confidence": 0.9572921991348267}, {"text": "F1-score", "start_pos": 161, "end_pos": 169, "type": "METRIC", "confidence": 0.9972426891326904}]}, {"text": "We then ranked all the CEA scores of all event pairs gathered from the 760K Gigaword documents, discretized all scores higher than t into B bins, and all scores lower than t into B bins.", "labels": [], "entities": [{"text": "CEA", "start_pos": 23, "end_pos": 26, "type": "METRIC", "confidence": 0.7363356351852417}, {"text": "760K Gigaword documents", "start_pos": 71, "end_pos": 94, "type": "DATASET", "confidence": 0.8996316393216451}]}, {"text": "Together, these 2B bins represent the range.", "labels": [], "entities": []}, {"text": "We used B = 500.", "labels": [], "entities": [{"text": "B", "start_pos": 8, "end_pos": 9, "type": "METRIC", "confidence": 0.9964591860771179}]}, {"text": "Thus, consecutive bins represent a difference of 0.001 in calibrated scores.", "labels": [], "entities": []}, {"text": "To measure the causality between a pair of events e i and e j , a simple baseline is to calculate PM I(p i , p j ).", "labels": [], "entities": [{"text": "PM I", "start_pos": 98, "end_pos": 102, "type": "METRIC", "confidence": 0.8984200060367584}]}, {"text": "Using a similar thresholding and calibration process to translate PM I(p i , p j ) scores into binary causality decisions, we obtained a F1 score of 23.1 when measured over the causality C relations, as shown in the row PM I pp of.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 137, "end_pos": 145, "type": "METRIC", "confidence": 0.9880377948284149}]}, {"text": "As mentioned in Section 2.1.2, proposed the ECD metric to measure causality between two events.", "labels": [], "entities": []}, {"text": "Thus, as a point of comparison, we replaced s pp of Equation with ECD(a, b) of Equation, substituting a = pi and b = p j . After thresholding and calibrating the scores of this approach, we obtained a F1-score of 29.7, as shown in the row ECD pp &P MI pa,aa of.", "labels": [], "entities": [{"text": "ECD", "start_pos": 66, "end_pos": 69, "type": "METRIC", "confidence": 0.9685111045837402}, {"text": "Equation", "start_pos": 79, "end_pos": 87, "type": "METRIC", "confidence": 0.6167712211608887}, {"text": "F1-score", "start_pos": 201, "end_pos": 209, "type": "METRIC", "confidence": 0.9991740584373474}]}, {"text": "Next, we evaluated our proposed CEA approach and obtained a F1-score of 38.6, as shown in the row CEA of.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.9996775388717651}, {"text": "CEA", "start_pos": 98, "end_pos": 101, "type": "METRIC", "confidence": 0.5685638189315796}]}, {"text": "Thus, our proposed approach obtained significantly better performance than the PMI baseline and the ECD approach.", "labels": [], "entities": [{"text": "PMI baseline", "start_pos": 79, "end_pos": 91, "type": "DATASET", "confidence": 0.7419192492961884}]}, {"text": "Next, we performed joint inference with the discourse relation predictions as described in Section 5 and obtained an improved F1-score of 41.7.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 126, "end_pos": 134, "type": "METRIC", "confidence": 0.9997342228889465}]}, {"text": "We note that we obtained improvements in both recall and precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 46, "end_pos": 52, "type": "METRIC", "confidence": 0.9996073842048645}, {"text": "precision", "start_pos": 57, "end_pos": 66, "type": "METRIC", "confidence": 0.999408483505249}]}, {"text": "This means that with the aid of discourse relations, we are able to recover more causal relations, as well as reduce false-positive predictions.", "labels": [], "entities": []}, {"text": "Constraint Equations and help to recover causal relations.", "labels": [], "entities": []}, {"text": "For improvements in precision, as stated in the last paragraph of Section 5.2, identifying other discourse relations such as \"Comparison\", \"Contrast\", etc., provides counterevidence to causality.", "labels": [], "entities": [{"text": "precision", "start_pos": 20, "end_pos": 29, "type": "METRIC", "confidence": 0.9977032542228699}]}, {"text": "Together with constraint Equation, this helps to eliminate false-positive event pairs as classified by CEA and contributes towards CEA+Discourse having a higher precision than CEA.", "labels": [], "entities": [{"text": "precision", "start_pos": 161, "end_pos": 170, "type": "METRIC", "confidence": 0.9969362020492554}]}, {"text": "The corresponding results for extracting both causality and relatedness C + R relations are given in.", "labels": [], "entities": []}, {"text": "For these experiments, the aim was fora more relaxed evaluation and we simply collapsed C and R into a single label.", "labels": [], "entities": []}, {"text": "Finally, we also measured the precision of the top K causality C predictions, showing the precision trends in.", "labels": [], "entities": [{"text": "precision", "start_pos": 30, "end_pos": 39, "type": "METRIC", "confidence": 0.9993071556091309}, {"text": "precision", "start_pos": 90, "end_pos": 99, "type": "METRIC", "confidence": 0.9967365860939026}]}, {"text": "As shown, CEA in general achieves higher precision when compared to PM I pp and ECD pp &P MI pa,aa . The trends for C + R predictions are similar.", "labels": [], "entities": [{"text": "CEA", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.8111364245414734}, {"text": "precision", "start_pos": 41, "end_pos": 50, "type": "METRIC", "confidence": 0.9992901086807251}]}, {"text": "Thus far, we had included both verbal and nominal predicates in our evaluation.", "labels": [], "entities": []}, {"text": "When we repeat the experiments for ECD pp &P MI pa,aa and CEA on just verbal predicates, we obtained the respective F1-scores of 31.8 and 38.3 on causality relations.", "labels": [], "entities": [{"text": "CEA", "start_pos": 58, "end_pos": 61, "type": "METRIC", "confidence": 0.9871314167976379}, {"text": "F1-scores", "start_pos": 116, "end_pos": 125, "type": "METRIC", "confidence": 0.9981892704963684}]}, {"text": "The corresponding F1-scores for casuality and relatedness relations are 35.7 and 43.3.", "labels": [], "entities": [{"text": "F1-scores", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.9990635514259338}]}, {"text": "These absolute F1-scores are similar to those in, differing by 1-2%.", "labels": [], "entities": [{"text": "F1-scores", "start_pos": 15, "end_pos": 24, "type": "METRIC", "confidence": 0.9735748171806335}]}], "tableCaptions": [{"text": " Table 2: Performance of baseline systems and our ap- proaches on extracting Causal event relations.", "labels": [], "entities": []}, {"text": " Table 3: Performance of the systems on extracting Causal  and Related event relations.", "labels": [], "entities": [{"text": "extracting Causal  and Related event relations", "start_pos": 40, "end_pos": 86, "type": "TASK", "confidence": 0.8328738709290823}]}]}