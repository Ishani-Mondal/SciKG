{"title": [{"text": "A Bayesian Mixture Model for Part-of-Speech Induction Using Multiple Features", "labels": [], "entities": [{"text": "Part-of-Speech Induction", "start_pos": 29, "end_pos": 53, "type": "TASK", "confidence": 0.759783923625946}]}], "abstractContent": [{"text": "In this paper we present a fully unsupervised syntactic class induction system formulated as a Bayesian multinomial mixture model, where each word type is constrained to belong to a single class.", "labels": [], "entities": []}, {"text": "By using a mixture model rather than a sequence model (e.g., HMM), we are able to easily add multiple kinds of features, including those at both the type level (mor-phology features) and token level (context and alignment features, the latter from parallel corpora).", "labels": [], "entities": []}, {"text": "Using only context features, our system yields results comparable to state-of-the art, far better than a similar model without the one-class-per-type constraint.", "labels": [], "entities": []}, {"text": "Using the additional features provides added benefit, and our final system outperforms the best published results on most of the 25 corpora tested.", "labels": [], "entities": []}], "introductionContent": [{"text": "Research on unsupervised learning for NLP has become widespread recently, with part-of-speech induction, or syntactic class induction, being a particularly popular task.", "labels": [], "entities": [{"text": "part-of-speech induction", "start_pos": 79, "end_pos": 103, "type": "TASK", "confidence": 0.7522634565830231}, {"text": "syntactic class induction", "start_pos": 108, "end_pos": 133, "type": "TASK", "confidence": 0.6510486503442129}]}, {"text": "However, despite a recent proliferation of syntactic class induction systems, careful comparison indicates that very few systems perform better than some much simpler and quicker methods dating back tenor even twenty years.", "labels": [], "entities": []}, {"text": "This fact suggests that we should consider which features of the older systems led to their success, and attempt to combine these features with some of the machine learning methods introduced by the more recent systems.", "labels": [], "entities": []}, {"text": "We pursue this strategy here, developing a system based on Bayesian methods where the probabilistic model incorporates several insights from previous work.", "labels": [], "entities": []}, {"text": "Perhaps the most important property of our model is that it is type-based, meaning that all tokens of a given word type are assigned to the same cluster.", "labels": [], "entities": []}, {"text": "This property is not strictly true of linguistic data, but is a good approximation: as note, assigning each word type to its most frequent part of speech yields an upper bound accuracy of 93% or more for most languages.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 176, "end_pos": 184, "type": "METRIC", "confidence": 0.9698540568351746}]}, {"text": "Since this is much better than the performance of current unsupervised syntactic class induction systems, constraining the model in this way seems likely to improve performance by reducing the number of parameters in the model and incorporating useful linguistic knowledge.", "labels": [], "entities": []}, {"text": "Both of the older systems discussed by, i.e., and, included this constraint and achieved very good performance relative to token-based systems.", "labels": [], "entities": []}, {"text": "More recently, presented anew type-based model, and also reported very good results.", "labels": [], "entities": []}, {"text": "A second property of our model, which distinguishes it from the type-based Bayesian model of, is that the underlying probabilistic model is a clustering model, (specifically, a multinomial mixture model) rather than a sequence model.", "labels": [], "entities": []}, {"text": "In this sense, our model is more closely re-lated to several non-probabilistic systems that cluster context vectors or lower-dimensional representations of them ().", "labels": [], "entities": []}, {"text": "Sequence models are by far the most common method of supervised partof-speech tagging, and have also been widely used for unsupervised part-of-speech tagging both with and without a dictionary.", "labels": [], "entities": [{"text": "partof-speech tagging", "start_pos": 64, "end_pos": 85, "type": "TASK", "confidence": 0.7327927052974701}, {"text": "part-of-speech tagging", "start_pos": 135, "end_pos": 157, "type": "TASK", "confidence": 0.690130427479744}]}, {"text": "However, systems based on context vectors have also performed well in these latter scenarios and present a viable alternative to sequence models.", "labels": [], "entities": []}, {"text": "One advantage of using a clustering model rather than a sequence model is that the features used for clustering need not be restricted to context words.", "labels": [], "entities": []}, {"text": "Additional types of features can easily be incorporated into the model and inference procedure using the same general framework as in the basic model that uses only context word features.", "labels": [], "entities": []}, {"text": "In particular, we present two extensions to the basic model.", "labels": [], "entities": []}, {"text": "The first uses morphological features, which serve as cues to syntactic class and seemed to partly explain the success of two best-performing systems analysed by.", "labels": [], "entities": []}, {"text": "The second extension to our model uses alignment features gathered from parallel corpora.", "labels": [], "entities": []}, {"text": "Previous work suggests that using parallel text can improve performance on various unsupervised NLP tasks.", "labels": [], "entities": []}, {"text": "We evaluate our model on 25 corpora in 20 languages that vary substantially in both syntax and morphology.", "labels": [], "entities": []}, {"text": "As in previous work (, we find that the one-class-per-type restriction boosts performance considerably over a comparable tokenbased model and yields results that are comparable to state-of-the-art even without the use of morphology or alignment features.", "labels": [], "entities": []}, {"text": "Including morphology features yields the best published results on 14 or 15 of our 25 corpora (depending on the measure) and alignment features can improve results further.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our models using an increasing level of complexity, starting with a model that uses only monolingual context features.", "labels": [], "entities": []}, {"text": "We use the F = 100 most frequent words as features, and consider two versions of this model: one with two kinds of features (one left and one right context word) and one with four (two context words on each side).", "labels": [], "entities": []}, {"text": "For the model with morphology features we ran the unsupervised morphological segmentation system Morfessor () to get a \u03b1 \u03b8 z f \u03c6 \u03b2 . .", "labels": [], "entities": [{"text": "Morfessor", "start_pos": 97, "end_pos": 106, "type": "METRIC", "confidence": 0.8866981863975525}]}, {"text": "\u03b2 segmentation for each word type in the corpus.", "labels": [], "entities": []}, {"text": "We then extracted the suffix of each word type and used it as a feature type.", "labels": [], "entities": []}, {"text": "This process yielded on average Fm = 110 morphological feature types . Each word type generates at most one of these possible features.", "labels": [], "entities": [{"text": "Fm", "start_pos": 32, "end_pos": 34, "type": "METRIC", "confidence": 0.9974671602249146}]}, {"text": "If there are overlapping possibilities (e.g. -ingly and -y) we take the longest possible match.", "labels": [], "entities": []}, {"text": "We also explore the idea of extending the morphology feature space beyond suffixes, by including features like capitalisation and punctuation.", "labels": [], "entities": []}, {"text": "Specifically we use the features described in, namely initial-capital, containshyphen, contains-digit and we add an extra feature contains-punctuation.", "labels": [], "entities": []}, {"text": "For the model with alignment features, we follow () in using only bidirectional alignments: using Giza++ (, we get the word alignments in both directions between all possible language pairs in our parallel corpora (i.e., alternating the source and target languages within each pair).", "labels": [], "entities": []}, {"text": "We then use only those alignments that are found in both directions.", "labels": [], "entities": []}, {"text": "As discussed Since Morfessor yields multiple affixes for each word we concatenated all the suffixes into a single suffix.", "labels": [], "entities": []}, {"text": "There was large variance in the number of feature types for each language ranging from 11 in Chinese to more than 350 in German and Czech.", "labels": [], "entities": []}, {"text": "above, we use two kinds of alignment features: the left and right context words of the aligned token in the other language.", "labels": [], "entities": []}, {"text": "The feature space is set to the F = 100 most frequent words in that language.", "labels": [], "entities": [{"text": "F", "start_pos": 32, "end_pos": 33, "type": "METRIC", "confidence": 0.9842738509178162}]}, {"text": "Instead of fixing the hyperparameters \u03b1 and \u03b2, we used the Metropolis-Hastings sampler presented by to get updated values based on the likelihood of the data with respect to those hyperparameters 6 . In order to improve convergence of the sampler, we used simulated annealing with a sigmoid-shaped cooling schedule from an initial temperature of 2 down to 1.", "labels": [], "entities": []}, {"text": "Preliminary experiments indicated that we could achieve better results by cooling even further (approximating the MAP solution rather than a sample from the posterior), so for all experiments reported here, we ran the sampler fora total of 2000 iterations, with the last 400 of these decreasing the temperature from 1 to 0.66.", "labels": [], "entities": []}, {"text": "Finally, we investigated two different initialisation techniques: First, we use random class assignments to word types (referred to as method 1) and second, we assign each of the Z most frequent word types to a separate class and then randomly distribute the rest of the word types to the classes (method 2).", "labels": [], "entities": []}, {"text": "Although unsupervised systems should in principle be language-and corpus-independent, most part-ofspeech induction systems (especially in the early literature) have been developed on English.", "labels": [], "entities": []}, {"text": "Whether because English is simply an easier language, or because of bias introduced during development, these systems' performance is considerably worse in other languages ( Since we aim to use our system mostly on nonEnglish corpora, and ones that are significantly smaller than the large English treebank corpora, we developed our models using one of the languages of the MULTEXT-East corpus, namely Bulgarian.", "labels": [], "entities": [{"text": "MULTEXT-East corpus", "start_pos": 374, "end_pos": 393, "type": "DATASET", "confidence": 0.9198342263698578}]}, {"text": "The other languages in the corpus were used during development as a source of word alignments, but otherwise were only used for testing final versions of our models.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 78, "end_pos": 93, "type": "TASK", "confidence": 0.7267820537090302}]}, {"text": "Since none of the authors speak any of the languages in the MULTEXT col-lection, we also used the Penn Treebank WSJ corpus () for development.", "labels": [], "entities": [{"text": "Penn Treebank WSJ corpus", "start_pos": 98, "end_pos": 122, "type": "DATASET", "confidence": 0.9737477153539658}]}, {"text": "Following we created a smaller version of the WSJ corpus (referred to as wsj-s) to approximate the size of the corpora in MULTEXT-East.", "labels": [], "entities": [{"text": "WSJ corpus", "start_pos": 46, "end_pos": 56, "type": "DATASET", "confidence": 0.9394159913063049}, {"text": "MULTEXT-East", "start_pos": 122, "end_pos": 134, "type": "DATASET", "confidence": 0.892206072807312}]}, {"text": "For comparison to other systems, we also used the full WSJ attest time.", "labels": [], "entities": [{"text": "WSJ attest time", "start_pos": 55, "end_pos": 70, "type": "METRIC", "confidence": 0.7297452886899313}]}, {"text": "For further testing, we used the remaining MUL-TEXT languages, as well as the languages of the CONNL-X () shared task.", "labels": [], "entities": []}, {"text": "This dataset contains 13 languages, 4 of which are freely available (Danish, Dutch, Portuguese and Swedish) and 9 that are used with permission from the creators of the corpora ( Arabic 7 , Bulgarian 8 , Czech 9 , German 10 , Chinese 11 , Japanese 12 , Slovene , Spanish 14 , Turkish 15 ).", "labels": [], "entities": []}, {"text": "Following we used only the training sections for each language.", "labels": [], "entities": []}, {"text": "Finally, to widen the scope of our system, we generated two more corpora in French and Ancient Greek 17 , extracting the gold standard parts of speech from the respective dependency treebanks.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: V-measure (VM) and many-to-one (M-1) results  on the MULTEXT-Bulgarian corpus for various mod- els using either \u00b11 or \u00b12 context words as features.  base: context features only; (tokens): token-based model;  (init): Initialisation method 2-other results use method  1; (ext): Extended morphological features.", "labels": [], "entities": [{"text": "MULTEXT-Bulgarian corpus", "start_pos": 63, "end_pos": 87, "type": "DATASET", "confidence": 0.8534545004367828}]}, {"text": " Table 2: V-measure and many-to-one results on the wsj-s  corpus for various models, as described in", "labels": [], "entities": [{"text": "wsj-s  corpus", "start_pos": 51, "end_pos": 64, "type": "DATASET", "confidence": 0.7797687947750092}]}, {"text": " Table 3: V-measure (VM) and many-to-one (M-1) results on the languages in the MULTEXT-East corpus using  the gold standard number of classes shown in Table 4. BASE results use \u00b11-word context features alone or with  morphology. ALIGNMENTS adds alignment features, reporting the average score across all possible choices of paired  language and the scores under the best performing paired language (in parens), alone or with morphology features.", "labels": [], "entities": [{"text": "MULTEXT-East corpus", "start_pos": 79, "end_pos": 98, "type": "DATASET", "confidence": 0.9252146184444427}, {"text": "BASE", "start_pos": 160, "end_pos": 164, "type": "METRIC", "confidence": 0.9733899235725403}]}, {"text": " Table 4: Final results on 25 corpora in 20 languages, with the number of induced classes equal to the number of gold  standard tags in all cases. k-means and SVD2 models could not produce a clustering in the Czech CoNLL corpus due  its size. Best published results are from  *  Christodoulopoulos et al. (2010),  \u2020 Berg-Kirkpatrick et al. (2010) and  \u2021 Lee  et al. (2010). The latter two papers do not report VM scores. No best published results are shown for the MULTEXT  languages; Christodoulopoulos et al. (2010) report results based on 45 tags suggesting that clark performs best on  these corpora.", "labels": [], "entities": [{"text": "Czech CoNLL corpus", "start_pos": 209, "end_pos": 227, "type": "DATASET", "confidence": 0.8926850358645121}, {"text": "MULTEXT  languages", "start_pos": 465, "end_pos": 483, "type": "DATASET", "confidence": 0.8681129813194275}]}]}