{"title": [{"text": "Joint Models for Chinese POS Tagging and Dependency Parsing", "labels": [], "entities": [{"text": "POS Tagging", "start_pos": 25, "end_pos": 36, "type": "TASK", "confidence": 0.7111889719963074}, {"text": "Dependency Parsing", "start_pos": 41, "end_pos": 59, "type": "TASK", "confidence": 0.6365657299757004}]}], "abstractContent": [{"text": "Part-of-speech (POS) is an indispensable feature in dependency parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 52, "end_pos": 70, "type": "TASK", "confidence": 0.8309201598167419}]}, {"text": "Current research usually models POS tagging and dependency parsing independently.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 32, "end_pos": 43, "type": "TASK", "confidence": 0.8966180682182312}, {"text": "dependency parsing", "start_pos": 48, "end_pos": 66, "type": "TASK", "confidence": 0.798319548368454}]}, {"text": "This may suffer from error propagation problem.", "labels": [], "entities": [{"text": "error propagation", "start_pos": 21, "end_pos": 38, "type": "TASK", "confidence": 0.6320155560970306}]}, {"text": "Our experiments show that parsing accuracy drops by about 6% when using automatic POS tags instead of gold ones.", "labels": [], "entities": [{"text": "parsing", "start_pos": 26, "end_pos": 33, "type": "TASK", "confidence": 0.9784907698631287}, {"text": "accuracy", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9807397127151489}]}, {"text": "To solve this issue, this paper proposes a solution by jointly optimizing POS tagging and dependency parsing in a unique model.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 74, "end_pos": 85, "type": "TASK", "confidence": 0.7843554317951202}, {"text": "dependency parsing", "start_pos": 90, "end_pos": 108, "type": "TASK", "confidence": 0.7383948564529419}]}, {"text": "We design several joint models and their corresponding decoding algorithms to incorporate different feature sets.", "labels": [], "entities": []}, {"text": "We further present an effective pruning strategy to reduce the search space of candidate POS tags, leading to significant improvement of parsing speed.", "labels": [], "entities": [{"text": "parsing", "start_pos": 137, "end_pos": 144, "type": "TASK", "confidence": 0.9790417551994324}]}, {"text": "Experimental results on Chinese Penn Treebank 5 show that our joint models significantly improve the state-of-the-art parsing accuracy by about 1.5%.", "labels": [], "entities": [{"text": "Chinese Penn Treebank 5", "start_pos": 24, "end_pos": 47, "type": "DATASET", "confidence": 0.9201890528202057}, {"text": "parsing", "start_pos": 118, "end_pos": 125, "type": "TASK", "confidence": 0.893179714679718}, {"text": "accuracy", "start_pos": 126, "end_pos": 134, "type": "METRIC", "confidence": 0.9628556370735168}]}, {"text": "Detailed analysis shows that the joint method is able to choose such POS tags that are more helpful and dis-criminative from parsing viewpoint.", "labels": [], "entities": []}, {"text": "This is the fundamental reason of parsing accuracy improvement.", "labels": [], "entities": [{"text": "parsing", "start_pos": 34, "end_pos": 41, "type": "TASK", "confidence": 0.9881411790847778}, {"text": "accuracy", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.9577769637107849}]}], "introductionContent": [{"text": "In dependency parsing, features consisting of partof-speech (POS) tags are very effective, since pure lexical features lead to severe data sparseness problem.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 3, "end_pos": 21, "type": "TASK", "confidence": 0.8127239644527435}]}, {"text": "Typically, POS tagging and dependency parsing are modeled in a pipelined way.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 11, "end_pos": 22, "type": "TASK", "confidence": 0.875848799943924}, {"text": "dependency parsing", "start_pos": 27, "end_pos": 45, "type": "TASK", "confidence": 0.8298074901103973}]}, {"text": "However, the pipelined method is prone to error propagation, especially for Chinese.", "labels": [], "entities": [{"text": "error propagation", "start_pos": 42, "end_pos": 59, "type": "TASK", "confidence": 0.6928291767835617}]}, {"text": "Due to the lack of morphological features, Chinese POS tagging is even harder than other languages such as English.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 51, "end_pos": 62, "type": "TASK", "confidence": 0.6718466281890869}]}, {"text": "The state-ofthe-art accuracy of Chinese POS tagging is about 93.5%, which is much lower than that of English (about 97%).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9995778203010559}, {"text": "POS tagging", "start_pos": 40, "end_pos": 51, "type": "TASK", "confidence": 0.6807507574558258}]}, {"text": "Our experimental results show that parsing accuracy decreases by about 6% on Chinese when using automatic POS tagging results instead of gold ones (see in Section 5).", "labels": [], "entities": [{"text": "parsing", "start_pos": 35, "end_pos": 42, "type": "TASK", "confidence": 0.9773058295249939}, {"text": "accuracy", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.9753191471099854}, {"text": "POS tagging", "start_pos": 106, "end_pos": 117, "type": "TASK", "confidence": 0.7038250416517258}]}, {"text": "Recent research on dependency parsing usually overlooks this issue by simply adopting gold POS tags for Chinese data.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 19, "end_pos": 37, "type": "TASK", "confidence": 0.8500500321388245}]}, {"text": "In this paper, we address this issue by jointly optimizing POS tagging and dependency parsing.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 59, "end_pos": 70, "type": "TASK", "confidence": 0.7883211076259613}, {"text": "dependency parsing", "start_pos": 75, "end_pos": 93, "type": "TASK", "confidence": 0.781261146068573}]}, {"text": "Joint modeling has been a popular and effective approach to simultaneously solve related tasks.", "labels": [], "entities": [{"text": "Joint modeling", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.8828608691692352}]}, {"text": "Recently, many successful joint models have been proposed, such as joint tokenization and POS tagging (, joint lemmatization and POS tagging, joint tokenization and parsing, joint named entity recognition and parsing, joint parsing and semantic role labeling (SRL) (, joint word sense disambiguation and SRL, joint tokenization and machine translation (MT) and joint parsing and MT ( . Note that the aforementioned \"parsing\" all refer to constituent parsing.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 90, "end_pos": 101, "type": "TASK", "confidence": 0.7554766833782196}, {"text": "POS tagging", "start_pos": 129, "end_pos": 140, "type": "TASK", "confidence": 0.7137331664562225}, {"text": "joint named entity recognition and parsing", "start_pos": 174, "end_pos": 216, "type": "TASK", "confidence": 0.7299770265817642}, {"text": "joint parsing and semantic role labeling (SRL", "start_pos": 218, "end_pos": 263, "type": "TASK", "confidence": 0.7640029825270176}, {"text": "word sense disambiguation", "start_pos": 274, "end_pos": 299, "type": "TASK", "confidence": 0.6089422305425009}, {"text": "SRL", "start_pos": 304, "end_pos": 307, "type": "TASK", "confidence": 0.924711287021637}, {"text": "joint tokenization and machine translation (MT)", "start_pos": 309, "end_pos": 356, "type": "TASK", "confidence": 0.6962411254644394}, {"text": "joint parsing", "start_pos": 361, "end_pos": 374, "type": "TASK", "confidence": 0.5658167898654938}, {"text": "MT", "start_pos": 379, "end_pos": 381, "type": "TASK", "confidence": 0.7836820483207703}]}, {"text": "As far as we know, there are few successful models for jointly solving dependency parsing and other tasks.", "labels": [], "entities": [{"text": "solving dependency parsing", "start_pos": 63, "end_pos": 89, "type": "TASK", "confidence": 0.6782981057961782}]}, {"text": "Being facilitated by Conference on Computational Natural Language Learning (CoNLL) 2008 and 2009 shared tasks, several joint models of dependency parsing and SRL have been proposed.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 135, "end_pos": 153, "type": "TASK", "confidence": 0.832976371049881}, {"text": "SRL", "start_pos": 158, "end_pos": 161, "type": "TASK", "confidence": 0.9786840081214905}]}, {"text": "Nevertheless, the top-ranked systems all adopt pipelined approaches (.", "labels": [], "entities": []}, {"text": "Theoretically, joint modeling of POS tagging and dependency parsing should be helpful to the two individual tasks.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 33, "end_pos": 44, "type": "TASK", "confidence": 0.8529403805732727}, {"text": "dependency parsing", "start_pos": 49, "end_pos": 67, "type": "TASK", "confidence": 0.7607542276382446}]}, {"text": "On the one hand, syntactic information can help resolve some POS ambiguities which are difficult to handle for the sequential POS tagging models.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 126, "end_pos": 137, "type": "TASK", "confidence": 0.5981780588626862}]}, {"text": "On the other hand, more accurate POS tags should further improve dependency parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 65, "end_pos": 83, "type": "TASK", "confidence": 0.8177732825279236}]}, {"text": "For joint POS tagging and dependency parsing, the major issue is to design effective decoding algorithms to capture rich features and efficiently search out the optimal results from a huge hypothesis space.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.9030221402645111}, {"text": "dependency parsing", "start_pos": 26, "end_pos": 44, "type": "TASK", "confidence": 0.8102157115936279}]}, {"text": "In this paper, we propose several dynamic programming (DP) based decoding algorithms for our joint models by extending existing parsing algorithms.", "labels": [], "entities": []}, {"text": "We also present effective pruning techniques to speedup our decoding algorithms.", "labels": [], "entities": []}, {"text": "Experimental results on Chinese Penn Treebank show that our joint models can significantly improve the state-ofthe-art parsing accuracy by about 1.5%.", "labels": [], "entities": [{"text": "Chinese Penn Treebank", "start_pos": 24, "end_pos": 45, "type": "DATASET", "confidence": 0.9063624938329061}, {"text": "parsing", "start_pos": 119, "end_pos": 126, "type": "TASK", "confidence": 0.9034768342971802}, {"text": "accuracy", "start_pos": 127, "end_pos": 135, "type": "METRIC", "confidence": 0.9596673846244812}]}, {"text": "The remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes the pipelined method, including the POS tagging and parsing models.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 56, "end_pos": 67, "type": "TASK", "confidence": 0.6422702074050903}]}, {"text": "Section 3 discusses the joint models and the decoding algorithms, while Section 4 presents the pruning techniques.", "labels": [], "entities": []}, {"text": "Section 5 reports the experimental results and error analysis.", "labels": [], "entities": [{"text": "error analysis", "start_pos": 47, "end_pos": 61, "type": "METRIC", "confidence": 0.9301954209804535}]}, {"text": "We review previous work closely related to our method in Section 6, and conclude this paper in Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use the Penn Chinese Treebank 5.1 (CTB5) ().", "labels": [], "entities": [{"text": "Penn Chinese Treebank 5.1 (CTB5)", "start_pos": 11, "end_pos": 43, "type": "DATASET", "confidence": 0.9585863692419869}]}, {"text": "Following the setup of, and, we split CTB5 into training (secs 001-815 and 1001-1136), development (secs 886-931 and 1148-1151), and test (secs 816-885 and 1137-1147) sets.", "labels": [], "entities": []}, {"text": "We use the head-finding rules of to turn the bracketed sentences into dependency structures.", "labels": [], "entities": []}, {"text": "We use the standard tagging accuracy to evaluate POS tagging.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.8328617811203003}, {"text": "POS tagging", "start_pos": 49, "end_pos": 60, "type": "TASK", "confidence": 0.757992684841156}]}, {"text": "For dependency parsing, we use word accuracy (also known as dependency accuracy), root accuracy and complete match rate (all excluding punctuation) . For the averaged training, we train each model for 15 iterations and select the parameters that perform best on the development set.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.8552443087100983}, {"text": "accuracy", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.6423621773719788}, {"text": "accuracy", "start_pos": 71, "end_pos": 79, "type": "METRIC", "confidence": 0.5039788484573364}, {"text": "root accuracy", "start_pos": 82, "end_pos": 95, "type": "METRIC", "confidence": 0.6345416605472565}, {"text": "complete match rate", "start_pos": 100, "end_pos": 119, "type": "METRIC", "confidence": 0.775451143582662}]}, {"text": "shows the distribution of words with different number of candidate POS tags and the k-best oracle tagging accuracy under different \u03bb t . To avoid dealing with words that have many candidate POS tags, we further apply a hard criterion that the decoding algorithms only consider top k candidate POS tags.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 106, "end_pos": 114, "type": "METRIC", "confidence": 0.9383838176727295}]}], "tableCaptions": [{"text": " Table 1: Performance of the second-order joint model of  version 1 with different pruning threshold \u03bb t (top k = 5)  on the development set. \"Acc.\" means the tagging accu- racy. \"Speed\" refers to the parsing speed (the number of  sentences processed per second).", "labels": [], "entities": [{"text": "Acc.", "start_pos": 143, "end_pos": 147, "type": "METRIC", "confidence": 0.9877234697341919}, {"text": "Speed", "start_pos": 180, "end_pos": 185, "type": "METRIC", "confidence": 0.9846732020378113}]}, {"text": " Table 3: Final results on the test set. \"Gold POS\" means that gold POS tags are used as input by the pipelined parsing  models; while \"Auto POS\" means that the POS tags are generated by the baseline POS tagging model.", "labels": [], "entities": []}, {"text": " Table 2: Performance of the second-order joint model of  version 1 with different top k (\u03bb t = 0.01) on the devel- opment set.", "labels": [], "entities": []}, {"text": " Table 4: Error analysis of POS tagging. # means the  error number of the corresponding pattern made by the  baseline tagging model. \u2193 and \u2191 mean the error number  reduced or increased by the joint model.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 28, "end_pos": 39, "type": "TASK", "confidence": 0.801812082529068}, {"text": "error number", "start_pos": 54, "end_pos": 66, "type": "METRIC", "confidence": 0.9636309146881104}]}, {"text": " Table 5: Comparison of parsing error rates on different  POS tag patterns between the pipelined and joint models.  Given a pattern \"X \u2192 Y\", \"prop\" means its proportion in  all occurrence of 'X' ( Count(X\u2192Y )  Count(X) ), and \"error\" refers  to its parsing error rate ( Count(wrongly headed X\u2192Y )", "labels": [], "entities": [{"text": "error", "start_pos": 227, "end_pos": 232, "type": "METRIC", "confidence": 0.9872254729270935}, {"text": "parsing error rate", "start_pos": 249, "end_pos": 267, "type": "METRIC", "confidence": 0.7593139012654623}, {"text": "Count", "start_pos": 270, "end_pos": 275, "type": "METRIC", "confidence": 0.9562283754348755}]}]}