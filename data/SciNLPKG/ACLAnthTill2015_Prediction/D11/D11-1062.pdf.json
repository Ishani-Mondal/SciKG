{"title": [{"text": "Divide and Conquer: Crowdsourcing the Creation of Cross-Lingual Textual Entailment Corpora", "labels": [], "entities": [{"text": "Divide and Conquer", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8174526492754618}, {"text": "Cross-Lingual Textual Entailment Corpora", "start_pos": 50, "end_pos": 90, "type": "TASK", "confidence": 0.6910631656646729}]}], "abstractContent": [{"text": "We address the creation of cross-lingual tex-tual entailment corpora by means of crowd-sourcing.", "labels": [], "entities": [{"text": "cross-lingual tex-tual entailment corpora", "start_pos": 27, "end_pos": 68, "type": "TASK", "confidence": 0.6687934249639511}]}, {"text": "Our goal is to define a cheap and replicable data collection methodology that minimizes the manual work done by expert annotators, without resorting to preprocess-ing tools or already annotated monolingual datasets.", "labels": [], "entities": []}, {"text": "In line with recent works emphasizing the need of large-scale annotation efforts for textual entailment, our work aims to: i) tackle the scarcity of data available to train and evaluate systems, and ii) promote the recourse to crowdsourcing as an effective way to reduce the costs of data collection without sacrificing quality.", "labels": [], "entities": [{"text": "textual entailment", "start_pos": 85, "end_pos": 103, "type": "TASK", "confidence": 0.6990151405334473}]}, {"text": "We show that a complex data creation task, for which even experts usually feature low agreement scores, can be effectively decomposed into simple subtasks assigned to non-expert annotators.", "labels": [], "entities": []}, {"text": "The resulting dataset, obtained from a pipeline of different jobs routed to Amazon Mechanical Turk, contains more than 1,600 aligned pairs for each combination of texts-hypotheses in English, Italian and German.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 76, "end_pos": 98, "type": "DATASET", "confidence": 0.9417081276575724}]}], "introductionContent": [{"text": "Cross-lingual Textual Entailment (CLTE) has been recently proposed by) as an extension of Textual Entailment ().", "labels": [], "entities": [{"text": "Cross-lingual Textual Entailment (CLTE)", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.7405143280824026}]}, {"text": "The task consists of deciding, given a text (T) and an hypothesis (H) in different languages, if the meaning of H can be inferred from the meaning of T.", "labels": [], "entities": []}, {"text": "As in other NLP applications, both for monolingual and cross-lingual TE, the availability of large quantities of annotated data is an enabling factor for systems development and evaluation.", "labels": [], "entities": []}, {"text": "Until now, however, the scarcity of such data on the one hand, and the costs of creating new datasets of reasonable size on the other, have represented a bottleneck fora steady advancement of the state of the art.", "labels": [], "entities": []}, {"text": "In the last few years, monolingual TE corpora for English and other European languages have been created and distributed in the framework of several evaluation campaigns, including the RTE Challenge 1 , the Answer Validation Exercise at CLEF 2 , and the Textual Entailment task at EVALITA 3 . Despite the differences in the design of the tasks, all the released datasets were collected through similar procedures, always involving expensive manual work done by expert annotators.", "labels": [], "entities": [{"text": "RTE Challenge 1", "start_pos": 185, "end_pos": 200, "type": "TASK", "confidence": 0.5516534248987833}, {"text": "Answer Validation Exercise at CLEF 2", "start_pos": 207, "end_pos": 243, "type": "TASK", "confidence": 0.7110579361518224}]}, {"text": "Moreover, in the data creation process, large amounts of hand-crafted T-H pairs often have to be discarded in order to retain only those featuring full agreement, in terms of the assigned entailment judgements, among multiple annotators.", "labels": [], "entities": [{"text": "data creation", "start_pos": 17, "end_pos": 30, "type": "TASK", "confidence": 0.7483039498329163}]}, {"text": "The amount of discarded pairs is usually high, contributing to increase the costs of creating textual entailment datasets . The issues related to the shortage of datasets and the high costs for their creation are more evident in the CLTE scenario, where: i) the only dataset currently available is an English-Spanish corpus obtained by translating the RTE-3 corpus ( , and ii) the application of the standard methods adopted to build RTE pairs requires proficiency in multiple languages, thus significantly increasing the costs of the data creation process.", "labels": [], "entities": [{"text": "RTE-3 corpus", "start_pos": 352, "end_pos": 364, "type": "DATASET", "confidence": 0.8246656954288483}]}, {"text": "To address these issues, in this paper we devise a cost-effective methodology to create cross-lingual textual entailment corpora.", "labels": [], "entities": [{"text": "cross-lingual textual entailment corpora", "start_pos": 88, "end_pos": 128, "type": "TASK", "confidence": 0.6624755561351776}]}, {"text": "In particular, we focus on the following problems: (1) Is it possible to collect T-H pairs minimizing the intervention of expert annotators?", "labels": [], "entities": []}, {"text": "To address this question, we explore the feasibility of crowdsourcing the corpus creation process.", "labels": [], "entities": [{"text": "corpus creation process", "start_pos": 74, "end_pos": 97, "type": "TASK", "confidence": 0.7374453743298849}]}, {"text": "As a contribution beyond the few works on TE/CLTE data acquisition, we define an effective methodology that: i) does not involve experts in the most complex (and costly) stages of the process, ii) does not require preprocessing tools, and iii) does not rely on the availability of already annotated RTE corpora.", "labels": [], "entities": [{"text": "TE/CLTE data acquisition", "start_pos": 42, "end_pos": 66, "type": "TASK", "confidence": 0.733084762096405}]}, {"text": "(2) How can we guarantee good quality of the collected data at a low cost?", "labels": [], "entities": []}, {"text": "We address the quality control issue through the decomposition of a complex task (i.e. creating and annotating entailment pairs) into smaller sub-tasks.", "labels": [], "entities": []}, {"text": "Complex tasks are usually hard to explain in a simple way understandable to non-experts, difficult to accomplish, and not suitable for the application of the quality-check mechanisms provided by current crowdsourcing services.", "labels": [], "entities": []}, {"text": "Our \"divide and conquer\" solution represents the first attempt to address a complex task involving content generation and labelling through the definition of a cheap and reliable pipeline of simple tasks which are easy to define, accomplish, and control.", "labels": [], "entities": [{"text": "content generation and labelling", "start_pos": 99, "end_pos": 131, "type": "TASK", "confidence": 0.6718844547867775}]}, {"text": "(3) Can we adapt such methodology to collect cross-lingual T-H pairs?", "labels": [], "entities": []}, {"text": "We tackle this question by separating the problem of creating and annotating TE pairs from the issues related to the multilingual dimension.", "labels": [], "entities": []}, {"text": "Our solution builds on the assumption that entailment annotations can be projected across aligned T-H pairs in different languages.", "labels": [], "entities": []}, {"text": "In this case, a complex multilingual task is reduced to a sequence of simpler subtasks where the most difficult one, the generation of entailment pairs, is entirely monolingual.", "labels": [], "entities": []}, {"text": "Besides ensuring cost-effectiveness, our solution allows us to overcome the problem of finding workers that are proficient in multiple languages.", "labels": [], "entities": []}, {"text": "Moreover, since the core monolingual tasks of the process are carried out by manipulating English texts, we are able to address the very large community of English speaking workers, with a considerable reduction of costs and execution time.", "labels": [], "entities": []}, {"text": "Finally, as a by-product of our method, the acquired pairs are fully aligned for all language combinations, thus enabling meaningful comparisons between scenarios of different complexity (monolingual TE, and CLTE between close or distant languages).", "labels": [], "entities": []}, {"text": "We believe that, in the same spirit of recent works promoting large-scale annotation efforts around entailment corpora, the proposed approach and the resulting dataset 5 will contribute to meeting the strong need for resources to develop and evaluate novel solutions for textual entailment.", "labels": [], "entities": [{"text": "textual entailment", "start_pos": 271, "end_pos": 289, "type": "TASK", "confidence": 0.7361913919448853}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: The monolingual dataset creation pipeline.", "labels": [], "entities": []}]}