{"title": [{"text": "Unsupervised Semantic Role Induction with Graph Partitioning", "labels": [], "entities": [{"text": "Semantic Role Induction", "start_pos": 13, "end_pos": 36, "type": "TASK", "confidence": 0.6172948578993479}]}], "abstractContent": [{"text": "In this paper we present a method for unsuper-vised semantic role induction which we formalize as a graph partitioning problem.", "labels": [], "entities": [{"text": "semantic role induction", "start_pos": 52, "end_pos": 75, "type": "TASK", "confidence": 0.6778229971726736}]}, {"text": "Argument instances of a verb are represented as vertices in a graph whose edge weights quantify their role-semantic similarity.", "labels": [], "entities": []}, {"text": "Graph partitioning is realized with an algorithm that iteratively assigns vertices to clusters based on the cluster assignments of neighboring ver-tices.", "labels": [], "entities": [{"text": "Graph partitioning", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9255657494068146}]}, {"text": "Our method is algorithmically and conceptually simple, especially with respect to how problem-specific knowledge is incorporated into the model.", "labels": [], "entities": []}, {"text": "Experimental results on the CoNLL 2008 benchmark dataset demonstrate that our model is competitive with other unsupervised approaches in terms of F1 whilst attaining significantly higher cluster purity.", "labels": [], "entities": [{"text": "CoNLL 2008 benchmark dataset", "start_pos": 28, "end_pos": 56, "type": "DATASET", "confidence": 0.9668354839086533}, {"text": "F1", "start_pos": 146, "end_pos": 148, "type": "METRIC", "confidence": 0.9993773102760315}]}], "introductionContent": [{"text": "Recent years have seen increased interest in the shallow semantic analysis of natural language text.", "labels": [], "entities": [{"text": "shallow semantic analysis of natural language text", "start_pos": 49, "end_pos": 99, "type": "TASK", "confidence": 0.8247857902731214}]}, {"text": "The term is most commonly used to describe the automatic identification and labeling of the semantic roles conveyed by sentential constituents ().", "labels": [], "entities": [{"text": "identification and labeling of the semantic roles conveyed by sentential constituents", "start_pos": 57, "end_pos": 142, "type": "TASK", "confidence": 0.6888149163939736}]}, {"text": "Semantic roles describe the semantic relations that hold between a predicate and its arguments (e.g., \"who\" did \"what\" to \"whom\", \"when\", \"where\", and \"how\") abstracting over surface syntactic configurations.", "labels": [], "entities": []}, {"text": "In the example sentences below, window occupies different syntactic positions -it is the object of broke in sentences (1a,b), and the subject in (1c) -while bearing the same semantic role, i.e., the physical object affected by the breaking event.", "labels": [], "entities": []}, {"text": "Analogously, ball is the instrument of break both when realized as a prepositional phrase in (1a) and as a subject in (1b).", "labels": [], "entities": []}, {"text": "[ The semantic roles in the examples are labeled in the style of), a broadcoverage human-annotated corpus of semantic roles and their syntactic realizations.", "labels": [], "entities": []}, {"text": "Under the PropBank annotation framework (which we will assume throughout this paper) each predicate is associated with a set of core roles (named A0, A1, A2, and so on) whose interpretations are specific to that predicate 1 and a set of adjunct roles such as location or time whose interpretation is common across predicates (e.g., last night in sentence (1c)).", "labels": [], "entities": []}, {"text": "The availability of PropBank and related resources (e.g., FrameNet;) has sparked the development of great many semantic role labeling systems most of which conceptualize the task as a supervised learning problem and rely on role-annotated data for model training.", "labels": [], "entities": []}, {"text": "Most of these systems implement a two-stage architecture consisting of argument identification (determining the arguments of the verbal predicate) and argument classification (labeling these arguments with semantic roles).", "labels": [], "entities": [{"text": "argument identification", "start_pos": 71, "end_pos": 94, "type": "TASK", "confidence": 0.7287407070398331}, {"text": "argument classification", "start_pos": 151, "end_pos": 174, "type": "TASK", "confidence": 0.7078572660684586}]}, {"text": "Despite being relatively shallow, se-mantic role analysis has the potential of benefiting a wide spectrum of applications ranging from information extraction ( and question answering, to machine translation () and summarization).", "labels": [], "entities": [{"text": "role analysis", "start_pos": 44, "end_pos": 57, "type": "TASK", "confidence": 0.693458691239357}, {"text": "information extraction", "start_pos": 135, "end_pos": 157, "type": "TASK", "confidence": 0.8554278314113617}, {"text": "question answering", "start_pos": 164, "end_pos": 182, "type": "TASK", "confidence": 0.8916217684745789}, {"text": "machine translation", "start_pos": 187, "end_pos": 206, "type": "TASK", "confidence": 0.8076741397380829}, {"text": "summarization", "start_pos": 214, "end_pos": 227, "type": "TASK", "confidence": 0.9910462498664856}]}, {"text": "Current approaches have high performance -a system will recall around 81% of the arguments correctly and 95% of those will be assigned a correct semantic role (see for details), however only on languages and domains for which large amounts of role-annotated training data are available.", "labels": [], "entities": []}, {"text": "For instance, systems trained on PropBank demonstrate a marked decrease in performance (approximately by 10%) when tested on out-of-domain data (.", "labels": [], "entities": [{"text": "PropBank", "start_pos": 33, "end_pos": 41, "type": "DATASET", "confidence": 0.9261775612831116}]}, {"text": "Unfortunately, the reliance on role-annotated data which is expensive and time-consuming to produce for every language and domain, presents a major bottleneck to the widespread application of semantic role labeling.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 192, "end_pos": 214, "type": "TASK", "confidence": 0.6002886394659678}]}, {"text": "In this paper we argue that unsupervised methods offer a promising yet challenging alternative.", "labels": [], "entities": []}, {"text": "If successful, such methods could lead to significant savings in terms of annotation effort and ultimately yield more portable semantic role labelers that require overall less engineering effort.", "labels": [], "entities": []}, {"text": "Our approach formalizes semantic role induction as a graph partitioning problem.", "labels": [], "entities": [{"text": "semantic role induction", "start_pos": 24, "end_pos": 47, "type": "TASK", "confidence": 0.742707093556722}]}, {"text": "Given a verbal predicate, it constructs a weighted graph whose vertices correspond to argument instances of the verb and whose edge weights quantify the similarity between these instances.", "labels": [], "entities": []}, {"text": "The graph is partitioned into vertex clusters representing semantic roles using a variant of Chinese Whispers, a graph-clustering algorithm proposed by.", "labels": [], "entities": [{"text": "Chinese Whispers", "start_pos": 93, "end_pos": 109, "type": "DATASET", "confidence": 0.9078285992145538}]}, {"text": "The algorithm iteratively assigns cluster labels to graph vertices by greedily choosing the most common label amongst the neighbors of the vertex being updated.", "labels": [], "entities": []}, {"text": "Beyond extending Chinese Whispers to the semantic role induction task, we also show how it can be understood as a type of Gibbs sampling when our graph is interpreted as a Markov random field.", "labels": [], "entities": [{"text": "Chinese Whispers", "start_pos": 17, "end_pos": 33, "type": "TASK", "confidence": 0.5279660671949387}, {"text": "semantic role induction task", "start_pos": 41, "end_pos": 69, "type": "TASK", "confidence": 0.7018698304891586}]}, {"text": "Experimental results on the CoNLL 2008 benchmark dataset demonstrate that our method, despite its simplicity, improves upon competitive approaches in terms of F1 and achieves significantly higher cluster purity.", "labels": [], "entities": [{"text": "CoNLL 2008 benchmark dataset", "start_pos": 28, "end_pos": 56, "type": "DATASET", "confidence": 0.9612051397562027}, {"text": "F1", "start_pos": 159, "end_pos": 161, "type": "METRIC", "confidence": 0.9992682337760925}]}], "datasetContent": [{"text": "In this section we describe how we assessed the performance of our model.", "labels": [], "entities": []}, {"text": "We discuss the dataset on which our experiments were carried out, explain how our system's output was evaluated and present the methods used for comparison with our approach.", "labels": [], "entities": []}, {"text": "Data We compared the output of our model against the PropBank gold standard annotations contained in the CoNLL 2008 shared task dataset (Surdeanu et al., 2008).", "labels": [], "entities": [{"text": "PropBank gold standard annotations", "start_pos": 53, "end_pos": 87, "type": "DATASET", "confidence": 0.934423178434372}, {"text": "CoNLL 2008 shared task dataset", "start_pos": 105, "end_pos": 135, "type": "DATASET", "confidence": 0.8658510684967041}]}, {"text": "The latter was taken from the Wall Street Journal portion of the Penn Treebank and converted into a dependency format (.", "labels": [], "entities": [{"text": "Wall Street Journal portion of the Penn Treebank", "start_pos": 30, "end_pos": 78, "type": "DATASET", "confidence": 0.953251801431179}]}, {"text": "In addition to gold standard dependency parses, the dataset also contains automatic parses obtained from the MaltParser ().", "labels": [], "entities": []}, {"text": "The dataset provides annotations for verbal and nominal predicate-argument constructions, but we only considered the former, following previous work on semantic role labeling).", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 152, "end_pos": 174, "type": "TASK", "confidence": 0.6624568104743958}]}, {"text": "All the experiments described in this paper use the CoNLL 2008 training dataset.", "labels": [], "entities": [{"text": "CoNLL 2008 training dataset", "start_pos": 52, "end_pos": 79, "type": "DATASET", "confidence": 0.9708661288022995}]}, {"text": "Evaluation Metrics For each verb, we determine the extent to which argument instances in the clusters share the same gold standard role (purity) and the extent to which a particular gold standard role is assigned to a single cluster (collocation).", "labels": [], "entities": []}, {"text": "More formally, for each group of verb-specific clusters we measure the purity of the clusters as the percentage of instances belonging to the majority gold class in their respective cluster.", "labels": [], "entities": []}, {"text": "Let N denote the total number of instances, G j the set of instances belonging to the j-th gold class and Ci the set of instances belonging to the i-th cluster.", "labels": [], "entities": []}, {"text": "Purity can be then written as: Collocation is defined as follows.", "labels": [], "entities": []}, {"text": "For each gold role, we determine the cluster with the largest number of instances for that role (the role's primary cluster) and then compute the percentage of instances that belong to the primary cluster for each gold role: Per-verb scores are aggregated into an overall score by averaging overall verbs.", "labels": [], "entities": []}, {"text": "We use the micro-average obtained by weighting the scores for individual verbs proportionately to the number of instances for that verb.", "labels": [], "entities": []}, {"text": "Finally, we use the harmonic mean of purity and collocation as a single measure of clustering quality: Model Parameters Recall that our algorithm prioritizes updates with confidence higher than a threshold \u03b8.", "labels": [], "entities": []}, {"text": "Initially, \u03b8 is set to 1 and its value decreases at each iteration by a small constant \u2206 which we set to 0.0025.", "labels": [], "entities": []}, {"text": "The algorithm terminates when a minimum confidence \u03b8 min is reached.", "labels": [], "entities": []}, {"text": "While choosing a value for \u2206 is straightforward -it simply has to be a small fraction of the maximal possible confidence -specifying \u03b8 min on the basis of objective prior knowledge is less so.", "labels": [], "entities": []}, {"text": "And although a human judge could determine the optimal termination point based on several criteria such as clustering quality or the number of clusters, we used a development set instead for the sake of reproducibility and comparability.", "labels": [], "entities": []}, {"text": "Specifically, we optimized \u03b8 min on the CoNLL test set and obtained best results with \u03b8 min = 1 3 . This value was used for all our experiments and was also kept fixed for all verbs.", "labels": [], "entities": [{"text": "CoNLL test set", "start_pos": 40, "end_pos": 54, "type": "DATASET", "confidence": 0.9663273294766744}]}, {"text": "Importantly, the development set was not used for any kind of supervised training.", "labels": [], "entities": []}, {"text": "1326: Evaluation of the output of our graph partitioning algorithm compared to our previous models and a baseline that assigns arguments to clusters based on their syntactic function.", "labels": [], "entities": []}, {"text": "Recall that one of the components in our similarity function is lexical similarity which we measure using a vector-based model (see Section 5.4).", "labels": [], "entities": []}, {"text": "We created such a model from the Google N-Grams corpus () using a context window of two words on both sides of the target word and co-occurrence frequencies as vector components (no weighting was applied).", "labels": [], "entities": [{"text": "Google N-Grams corpus", "start_pos": 33, "end_pos": 54, "type": "DATASET", "confidence": 0.7119345267613729}]}, {"text": "The large size of this corpus allows us to use bigram frequencies, rather than frequencies of individual words and to distinguish between left and right bigrams.", "labels": [], "entities": []}, {"text": "We used randomized algorithms () to build the semantic space efficiently.", "labels": [], "entities": []}, {"text": "Comparison Models We compared our graph partitioning algorithm against three competitive approaches.", "labels": [], "entities": [{"text": "graph partitioning", "start_pos": 34, "end_pos": 52, "type": "TASK", "confidence": 0.7126722633838654}]}, {"text": "The first one assigns argument instances to clusters according to their syntactic function (e.g., subject, object) as determined by a parser.", "labels": [], "entities": []}, {"text": "This baseline has been previously used as a point of comparison by other unsupervised semantic role induction systems ( and shown difficult to outperform.", "labels": [], "entities": []}, {"text": "Our implementation allocates up to N = 21 clusters 5 for each verb, one for each of the 20 most frequent syntactic functions and a default cluster for all other functions.", "labels": [], "entities": []}, {"text": "We also compared our approach to using the same model settings (with 10 latent variables) and feature set proposed in that paper.", "labels": [], "entities": []}, {"text": "Finally, our third comparison model is Lang and Lapata's (2011) split-merge clustering algorithm.", "labels": [], "entities": [{"text": "split-merge clustering", "start_pos": 64, "end_pos": 86, "type": "TASK", "confidence": 0.6393010318279266}]}, {"text": "Again we used the same parameters and number of clusters (on average 10 per verb).", "labels": [], "entities": []}, {"text": "Our graph partitioning method uses identical cues for assessing role-semantic similarity as the method described in Lang and Lapata (2011).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Evaluation of the output of our graph partitioning algorithm compared to our previous models and a baseline  that assigns arguments to clusters based on their syntactic function.", "labels": [], "entities": []}, {"text": " Table 2: Clustering results for individual verbs on the auto/auto dataset with our graph partitioning algorithm and the  syntactic function baseline; the scores were taken from a single run.", "labels": [], "entities": []}]}