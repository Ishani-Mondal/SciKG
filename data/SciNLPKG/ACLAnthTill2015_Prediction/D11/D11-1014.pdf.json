{"title": [{"text": "Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions", "labels": [], "entities": [{"text": "Predicting Sentiment Distributions", "start_pos": 43, "end_pos": 77, "type": "TASK", "confidence": 0.9078362981478373}]}], "abstractContent": [{"text": "We introduce a novel machine learning framework based on recursive autoencoders for sentence-level prediction of sentiment label distributions.", "labels": [], "entities": [{"text": "sentence-level prediction of sentiment label distributions", "start_pos": 84, "end_pos": 142, "type": "TASK", "confidence": 0.7878815978765488}]}, {"text": "Our method learns vector space representations for multi-word phrases.", "labels": [], "entities": []}, {"text": "In sentiment prediction tasks these representations outperform other state-of-the-art approaches on commonly used datasets, such as movie reviews, without using any pre-defined sentiment lexica or polarity shifting rules.", "labels": [], "entities": [{"text": "sentiment prediction", "start_pos": 3, "end_pos": 23, "type": "TASK", "confidence": 0.9676637053489685}]}, {"text": "We also evaluate the model's ability to predict sentiment distributions on anew dataset based on confessions from the experience project.", "labels": [], "entities": []}, {"text": "The dataset consists of personal user stories annotated with multiple labels which, when aggregated, form a multinomial distribution that captures emotional reactions.", "labels": [], "entities": []}, {"text": "Our algorithm can more accurately predict distributions over such labels compared to several competitive baselines.", "labels": [], "entities": []}], "introductionContent": [{"text": "The ability to identify sentiments about personal experiences, products, movies etc. is crucial to understand user generated content in social networks, blogs or product reviews.", "labels": [], "entities": []}, {"text": "Detecting sentiment in these data is a challenging task which has recently spawned a lot of interest (.", "labels": [], "entities": [{"text": "Detecting sentiment", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.94825479388237}]}, {"text": "Current baseline methods often use bag-of-words representations which cannot properly capture more complex linguistic phenomena in sentiment analysis ().", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 131, "end_pos": 149, "type": "TASK", "confidence": 0.9481798410415649}]}, {"text": "For instance, while the two phrases \"white blood cells destroying an infection\" and \"an infection destroying white blood cells\" have the same bag-of-words representation, the former is a positive reaction while the later is very negative.", "labels": [], "entities": []}, {"text": "More advanced methods such as (Nakagawa et al., 2010) that can capture such phenomena use many manually constructed resources (sentiment lexica, parsers, polarity-shifting rules).", "labels": [], "entities": []}, {"text": "This limits the applicability of these methods to a broader range of tasks and languages.", "labels": [], "entities": []}, {"text": "Lastly, almost all previous work is based on single, positive/negative categories or scales such as star ratings.", "labels": [], "entities": []}, {"text": "Examples are movie reviews), opinions ), customer reviews ( or multiple aspects of restaurants.", "labels": [], "entities": []}, {"text": "Such a one-dimensional scale does not accurately reflect the complexity of human emotions and sentiments.", "labels": [], "entities": []}, {"text": "In this work, we seek to address three issues.", "labels": [], "entities": []}, {"text": "(i) Instead of using a bag-of-words representation, our model exploits hierarchical structure and uses compositional semantics to understand sentiment.", "labels": [], "entities": []}, {"text": "(ii) Our system can be trained both on unlabeled domain data and on supervised sentiment data and does not require any language-specific sentiment lexica, 151 parsers, etc.", "labels": [], "entities": []}, {"text": "(iii) Rather than limiting sentiment to a positive/negative scale, we predict a multidimensional distribution over several complex, interconnected sentiments.", "labels": [], "entities": []}, {"text": "We introduce an approach based on semisupervised, recursive autoencoders (RAE) which use as input continuous word vectors.", "labels": [], "entities": []}, {"text": "shows an illustration of the model which learns vector representations of phrases and full sentences as well as their hierarchical structure from unsupervised text.", "labels": [], "entities": []}, {"text": "We extend our model to also learn a distribution over sentiment labels at each node of the hierarchy.", "labels": [], "entities": []}, {"text": "We evaluate our approach on several standard datasets where we achieve state-of-the art performance.", "labels": [], "entities": []}, {"text": "Furthermore, we show results on the recently introduced experience project (EP) dataset) that captures a broader spectrum of human sentiments and emotions.", "labels": [], "entities": [{"text": "experience project (EP) dataset", "start_pos": 56, "end_pos": 87, "type": "DATASET", "confidence": 0.700491855541865}]}, {"text": "The dataset consists of very personal confessions anonymously made by people on the experience project website www.experienceproject.com.", "labels": [], "entities": []}, {"text": "Confessions are labeled with a set of five reactions by other users.", "labels": [], "entities": []}, {"text": "Reaction labels are you rock (expressing approvement), tehee (amusement), I understand, Sorry, hugs and Wow, just wow (displaying shock).", "labels": [], "entities": []}, {"text": "For evaluation on this dataset we predict both the label with the most votes as well as the full distribution over the sentiment categories.", "labels": [], "entities": []}, {"text": "On both tasks our model outperforms competitive baselines.", "labels": [], "entities": []}, {"text": "A set of over 31,000 confessions as well as the code of our model are available at www.socher.org.", "labels": [], "entities": []}, {"text": "After describing the model in detail, we evaluate it qualitatively by analyzing the learned n-gram vector representations and compare quantitatively against other methods on standard datasets and the EP dataset.", "labels": [], "entities": [{"text": "EP dataset", "start_pos": 200, "end_pos": 210, "type": "DATASET", "confidence": 0.9416632950305939}]}], "datasetContent": [{"text": "We first describe the new experience project (EP) dataset, results of standard classification tasks on this dataset and how to predict its sentiment label distributions.", "labels": [], "entities": []}, {"text": "We then show results on other commonly used datasets and conclude with an analysis of the important parameters of the model.", "labels": [], "entities": []}, {"text": "In all experiments involving our model, we represent words using 100-dimensional word vectors.", "labels": [], "entities": []}, {"text": "We explore the two settings mentioned in Sec.", "labels": [], "entities": []}, {"text": "We compare performance on standard datasets when using randomly initialized word vectors (random word init.) or word vectors trained by the model of Collobert and Weston (2008) and provided by.", "labels": [], "entities": []}, {"text": "These vectors were trained on an unlabeled corpus of the English Wikipedia.", "labels": [], "entities": [{"text": "English Wikipedia", "start_pos": 57, "end_pos": 74, "type": "DATASET", "confidence": 0.7865874767303467}]}, {"text": "Note that alternatives such as Brown clusters are not suitable since they do not capture sentiment information (good and bad are usually in the same cluster) and cannot be modified via backpropagation.", "labels": [], "entities": []}, {"text": "The confessions section of the experience project website 3 lets people anonymously write short personal stories or \"confessions\".", "labels": [], "entities": []}, {"text": "Once a story is on the site, each user can give a single vote to one of five label categories (with our interpretation):.", "labels": [], "entities": []}, {"text": "Since an entry with less than 4 votes is not very well identified, we train and test only on entries with at least 4 total votes.", "labels": [], "entities": []}, {"text": "There are 6,129 total such entries.", "labels": [], "entities": []}, {"text": "The distribution over total votes in the 5 classes is similar: [0.22; 0.2; 0.11; 0.37; 0.1].", "labels": [], "entities": []}, {"text": "The average length of entries is 129 words.", "labels": [], "entities": []}, {"text": "Some entries contain multiple sentences.", "labels": [], "entities": []}, {"text": "In these cases, we average the predicted label distributions from the sentences.", "labels": [], "entities": []}, {"text": "shows statistics of this and other commonly used sentiment datasets (which we compare on in later experiments).", "labels": [], "entities": []}, {"text": "shows example entries as well as gold and predicted label distributions as described in the next sections.", "labels": [], "entities": []}, {"text": "Compared to other datasets, the EP dataset contains a wider range of human emotions that goes far beyond positive/negative product or movie reviews.", "labels": [], "entities": [{"text": "EP dataset", "start_pos": 32, "end_pos": 42, "type": "DATASET", "confidence": 0.8575271368026733}]}, {"text": "Each item is labeled with a multinomial distribu-tion over interconnected response categories.", "labels": [], "entities": []}, {"text": "This is in contrast to most other datasets (including multiaspect rating) where several distinct aspects are rated independently but on the same scale.", "labels": [], "entities": []}, {"text": "The topics range from generic happy statements, daily clumsiness reports, love, loneliness, to relationship abuse and suicidal notes.", "labels": [], "entities": []}, {"text": "As is evident from the total number of label votes, the most common user reaction is one of empathy and an ability to relate to the authors experience.", "labels": [], "entities": []}, {"text": "However, some stories describe horrible scenarios that are not common and hence receive more offers of condolence.", "labels": [], "entities": []}, {"text": "In the following sections we show some examples of stories with predicted and true distributions but refrain from listing the most horrible experiences.", "labels": [], "entities": []}, {"text": "For all experiments on the EP dataset, we split the data into train (49%), development (21%) and test data (30%).", "labels": [], "entities": [{"text": "EP dataset", "start_pos": 27, "end_pos": 37, "type": "DATASET", "confidence": 0.9408648312091827}]}], "tableCaptions": [{"text": " Table 1: Statistics on the different datasets. K is the num- ber of classes. Distr. is the distribution of the different  classes (in the case of 2, the positive/negative classes, for  EP the rounded distribution of total votes in each class).  |W | is the average number of words per instance. We use  EP\u2265 4, a subset of entries with at least 4 votes.", "labels": [], "entities": [{"text": "Distr.", "start_pos": 78, "end_pos": 84, "type": "METRIC", "confidence": 0.9926309585571289}]}, {"text": " Table 2: Example EP confessions from the test data with KL divergence between our predicted distribution (light blue,  left bar on each of the 5 classes) and ground truth distribution (red bar and numbers underneath), number of votes. The  5 classes are [Sorry, Hugs ;You Rock; Teehee;I Understand;Wow, Just Wow]. Even when the KL divergence is higher,  our model makes reasonable alternative label choices. Some entries are shortened.", "labels": [], "entities": []}, {"text": " Table 3: Accuracy of predicting the class with most votes.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9972469806671143}]}, {"text": " Table 4: Accuracy of sentiment classification on movie  review polarity (MR) and the MPQA dataset.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9869597554206848}, {"text": "sentiment classification", "start_pos": 22, "end_pos": 46, "type": "TASK", "confidence": 0.9432487189769745}, {"text": "movie  review polarity (MR)", "start_pos": 50, "end_pos": 77, "type": "TASK", "confidence": 0.4416505694389343}, {"text": "MPQA dataset", "start_pos": 86, "end_pos": 98, "type": "DATASET", "confidence": 0.9713127315044403}]}]}