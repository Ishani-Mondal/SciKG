{"title": [], "abstractContent": [{"text": "Statistical machine translation systems are usually trained on a large amount of bilingual sentence pairs and translate one sentence at a time, ignoring document-level information.", "labels": [], "entities": [{"text": "Statistical machine translation", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.6150551239649454}]}], "introductionContent": [{"text": "During last decade, tremendous work has been done to improve the quality of statistical machine __________________ * Corresponding author.", "labels": [], "entities": []}, {"text": "translation (SMT) systems.", "labels": [], "entities": [{"text": "translation (SMT)", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7760399132966995}]}, {"text": "However, there is still a huge performance gap between the state-of-theart SMT systems and human translators.", "labels": [], "entities": [{"text": "SMT", "start_pos": 75, "end_pos": 78, "type": "TASK", "confidence": 0.9708244800567627}]}, {"text": "suggested nine ways to improve machine translation by imitating the best practices of human translators, with parsing the entire document before translation as the first priority.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 31, "end_pos": 50, "type": "TASK", "confidence": 0.7767004072666168}, {"text": "parsing the entire document before translation", "start_pos": 110, "end_pos": 156, "type": "TASK", "confidence": 0.8109335700670878}]}, {"text": "However, most SMT systems still treat parallel corpora as a list of independent sentence-pairs and ignore document-level information.", "labels": [], "entities": [{"text": "SMT", "start_pos": 14, "end_pos": 17, "type": "TASK", "confidence": 0.9928408861160278}]}, {"text": "Document-level information can and should be used to help document-level machine translation.", "labels": [], "entities": [{"text": "document-level machine translation", "start_pos": 58, "end_pos": 92, "type": "TASK", "confidence": 0.5966407954692841}]}, {"text": "At least, the topic of a document can help choose specific translation candidates, since when taken out of the context from their document, some words, phrases and even sentences maybe rather ambiguous and thus difficult to understand.", "labels": [], "entities": []}, {"text": "Another advantage of document-level machine translation is its ability in keeping a consistent translation.", "labels": [], "entities": [{"text": "document-level machine translation", "start_pos": 21, "end_pos": 55, "type": "TASK", "confidence": 0.625732551018397}]}, {"text": "However, document-level translation has drawn little attention from the SMT research community.", "labels": [], "entities": [{"text": "document-level translation", "start_pos": 9, "end_pos": 35, "type": "TASK", "confidence": 0.7192698419094086}, {"text": "SMT", "start_pos": 72, "end_pos": 75, "type": "TASK", "confidence": 0.9940802454948425}]}, {"text": "First of all, most of parallel corpora lack the annotation of document boundaries.", "labels": [], "entities": []}, {"text": "Secondly, although it is easy to incorporate anew feature into the classical log-linear model, it is difficult to capture document-level information and model it via some simple features.", "labels": [], "entities": []}, {"text": "Thirdly, reference translations of a test document written by human translators tend to have flexible expressions in order to avoid producing monotonous texts.", "labels": [], "entities": [{"text": "reference translations of a test document written by human translators", "start_pos": 9, "end_pos": 79, "type": "TASK", "confidence": 0.7776482105255127}]}, {"text": "This makes the evaluation of document-level SMT systems extremely difficult.", "labels": [], "entities": [{"text": "SMT", "start_pos": 44, "end_pos": 47, "type": "TASK", "confidence": 0.8649905323982239}]}, {"text": "showed that the repetition and consistency are very important when modeling natural language and translation.", "labels": [], "entities": [{"text": "repetition", "start_pos": 16, "end_pos": 26, "type": "METRIC", "confidence": 0.9904504418373108}, {"text": "consistency", "start_pos": 31, "end_pos": 42, "type": "METRIC", "confidence": 0.9898019433021545}]}, {"text": "He proposed to employ cache-based language and translation models in a phrase-based SMT system for domain adaptation.", "labels": [], "entities": [{"text": "SMT", "start_pos": 84, "end_pos": 87, "type": "TASK", "confidence": 0.8837655186653137}, {"text": "domain adaptation", "start_pos": 99, "end_pos": 116, "type": "TASK", "confidence": 0.7641752660274506}]}, {"text": "Especially, the cache in the translation model dynamically grows up by adding bilingual phrase pairs from the best translation hypotheses of previous sentences.", "labels": [], "entities": []}, {"text": "One problem with the dynamic cache is that those initial sentences in a test document may not benefit from the dynamic cache.", "labels": [], "entities": []}, {"text": "Another problem is that the dynamic cache maybe prone to noise and cause error propagation.", "labels": [], "entities": []}, {"text": "This explains why the dynamic cache fails to much improve the performance.", "labels": [], "entities": []}, {"text": "This paper proposes a cache-based approach for document-level SMT using a static cache and a dynamic cache.", "labels": [], "entities": [{"text": "SMT", "start_pos": 62, "end_pos": 65, "type": "TASK", "confidence": 0.7056491374969482}]}, {"text": "While such a approach applies to both phrase-based and syntax-based SMT, this paper focuses on phrase-based SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 68, "end_pos": 71, "type": "TASK", "confidence": 0.8726267218589783}, {"text": "phrase-based SMT", "start_pos": 95, "end_pos": 111, "type": "TASK", "confidence": 0.5652940273284912}]}, {"text": "In particular, the static cache is employed to store relevant bilingual phrase pairs extracted from similar bilingual document pairs (i.e. source documents similar to the test document and their target counterparts) in the training parallel corpus while the dynamic cache is employed to store bilingual phrase pairs from the best translation hypotheses of previous sentences in the test document.", "labels": [], "entities": []}, {"text": "In this way, our cache-based approach can provide useful data at the beginning of the translation process via the static cache.", "labels": [], "entities": []}, {"text": "As the translation process continues, the dynamic cache grows and contributes more and more to the translation of subsequent sentences.", "labels": [], "entities": []}, {"text": "Our motivation to employ similar bilingual document pairs in the training parallel corpus is simple: a human translator often collects similar bilingual document pairs to help translation.", "labels": [], "entities": [{"text": "translation", "start_pos": 176, "end_pos": 187, "type": "TASK", "confidence": 0.9698110818862915}]}, {"text": "If there are translation pairs of sentences/phrases/words in similar bilingual document pairs, this makes the translation much easier.", "labels": [], "entities": [{"text": "translation", "start_pos": 110, "end_pos": 121, "type": "TASK", "confidence": 0.9672294855117798}]}, {"text": "Given a test document, our approach imitates this procedure by first retrieving similar bilingual document pairs from the training parallel corpus, which has often been applied in IR-based adaptation of SMT systems (;) and then extracting bilingual phrase pairs from similar bilingual document pairs to store them in a static cache.", "labels": [], "entities": [{"text": "IR-based adaptation of SMT", "start_pos": 180, "end_pos": 206, "type": "TASK", "confidence": 0.7429632395505905}]}, {"text": "However, such a cache-based approach may introduce many noisy/unnecessary bilingual phrase pairs in both the static and dynamic caches.", "labels": [], "entities": []}, {"text": "In order to resolve this problem, this paper employs a topic model to weaken those noisy/unnecessary bilingual phrase pairs by recommending the decoder to choose most likely phrase pairs according to the topic words extracted from the target-side text of similar bilingual document pairs.", "labels": [], "entities": []}, {"text": "Just like a human translator, even with a big bilingual dictionary, is often confused when he meets a source phrase which corresponds to several possible translations.", "labels": [], "entities": []}, {"text": "In this case, some topic words can help reduce the perplexity.", "labels": [], "entities": []}, {"text": "In this paper, the topic words are stored in a topic cache.", "labels": [], "entities": []}, {"text": "In some sense, it has the similar effect of employing an adaptive language model with the advantage of avoiding the interpolation of a global language model with a specific domain language model.", "labels": [], "entities": []}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 reviews the related work.", "labels": [], "entities": []}, {"text": "Section 3 presents our cache-based approach to documentlevel SMT.", "labels": [], "entities": [{"text": "documentlevel SMT", "start_pos": 47, "end_pos": 64, "type": "TASK", "confidence": 0.4759805202484131}]}, {"text": "Section 4 presents the experimental results.", "labels": [], "entities": []}, {"text": "Session 5 gives new insights on cachebased document-level translation.", "labels": [], "entities": [{"text": "document-level translation", "start_pos": 43, "end_pos": 69, "type": "TASK", "confidence": 0.6768360435962677}]}, {"text": "Finally, we conclude this paper in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "We have systematically evaluated our cache-based approach to document-level SMT on the ChineseEnglish translation task.", "labels": [], "entities": [{"text": "SMT", "start_pos": 76, "end_pos": 79, "type": "TASK", "confidence": 0.8040913939476013}, {"text": "ChineseEnglish translation task", "start_pos": 87, "end_pos": 118, "type": "TASK", "confidence": 0.7322244842847189}]}, {"text": "Here, we use SRI language modeling toolkit to train a trigram general language model on English newswire text, mostly from the Xinhua portion of the Gigaword corpus (2007) and performed word alignment on the training parallel corpus using GIZA++) in two directions.", "labels": [], "entities": [{"text": "SRI language modeling", "start_pos": 13, "end_pos": 34, "type": "TASK", "confidence": 0.6550416946411133}, {"text": "Gigaword corpus (2007)", "start_pos": 149, "end_pos": 171, "type": "DATASET", "confidence": 0.9236335039138794}, {"text": "word alignment", "start_pos": 186, "end_pos": 200, "type": "TASK", "confidence": 0.7582384049892426}]}, {"text": "For evaluation, the NIST BLEU script (version 13) with the default setting is used to calculate the Bleu score (, which measures case-insensitive matching of n-grams with n up to 4.", "labels": [], "entities": [{"text": "NIST", "start_pos": 20, "end_pos": 24, "type": "DATASET", "confidence": 0.8060439825057983}, {"text": "BLEU", "start_pos": 25, "end_pos": 29, "type": "METRIC", "confidence": 0.943213701248169}, {"text": "Bleu score", "start_pos": 100, "end_pos": 110, "type": "METRIC", "confidence": 0.972513735294342}]}, {"text": "To see whether an improvement is statistically significant, we also conduct significance tests using the paired bootstrap approach) 2 . In this paper, '***', '**', and '*' denote p-values less than or equal to 0.01, in-between (0.01, 0.05), and bigger than 0.05, which mean significantly better, moderately better and slightly better, respectively.", "labels": [], "entities": []}, {"text": "In particular, the sizes of the static, topic and dynamic caches are fine-tuned to 2000, 1000 and 5000 items, respectively.", "labels": [], "entities": []}, {"text": "For the dynamic cache, we only keep those most recently-visited items, while for the static cache; we always keep the most frequently-occurring items.", "labels": [], "entities": []}, {"text": "shows the contribution of various caches in our cache-based document-level SMT system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 75, "end_pos": 78, "type": "TASK", "confidence": 0.8133735060691833}]}, {"text": "The column of \"BLEU_W\" means the BLEU score computed over the whole test set and \"BLEU_D\" corresponds to the average BLEU score over separated documents.", "labels": [], "entities": [{"text": "BLEU_W", "start_pos": 15, "end_pos": 21, "type": "METRIC", "confidence": 0.9529561996459961}, {"text": "BLEU score", "start_pos": 33, "end_pos": 43, "type": "METRIC", "confidence": 0.9789907932281494}, {"text": "BLEU_D\"", "start_pos": 82, "end_pos": 89, "type": "METRIC", "confidence": 0.9647875279188156}, {"text": "BLEU score", "start_pos": 117, "end_pos": 127, "type": "METRIC", "confidence": 0.9769163131713867}]}, {"text": "Contribution of dynamical cache (Fd) shows that the dynamic cache slightly improves the performance by 0.27 (*) in BLEU_W.", "labels": [], "entities": [{"text": "dynamical cache (Fd)", "start_pos": 16, "end_pos": 36, "type": "METRIC", "confidence": 0.5891417145729065}, {"text": "BLEU", "start_pos": 115, "end_pos": 119, "type": "METRIC", "confidence": 0.9766967296600342}]}, {"text": "However, detailed analysis indicates that the dynamic cache does have negative effect on about one third of documents, largely due to the instability of the dynamic cache at the beginning of translating a document.", "labels": [], "entities": []}, {"text": "shows the distribution of the BLEU_D difference of 100 test documents (sorted by BLEU_D).", "labels": [], "entities": [{"text": "BLEU_D difference", "start_pos": 30, "end_pos": 47, "type": "METRIC", "confidence": 0.9592934250831604}, {"text": "BLEU_D", "start_pos": 81, "end_pos": 87, "type": "METRIC", "confidence": 0.9579532146453857}]}, {"text": "It shows that about 55% of test documents benefit from the dynamic cache.", "labels": [], "entities": []}, {"text": "shows that the combination of the static cache with the dynamic cache further improves the performance by 0.27(*) in BLEU_W.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 117, "end_pos": 121, "type": "METRIC", "confidence": 0.9913944602012634}]}, {"text": "This suggests the effectiveness of the static cache in eliminating the instability of the dynamic cache when translating first few sentences of a test document.", "labels": [], "entities": []}, {"text": "Together, the dynamic and static caches much improve the performance by 0.54 (**) in BLEU_W over Moses.", "labels": [], "entities": [{"text": "BLEU_W", "start_pos": 85, "end_pos": 91, "type": "METRIC", "confidence": 0.8745122154553732}]}, {"text": "shows the distribution of the BLEU_D difference of 100 test documents (sorted by BLEU_D), with more positive effect on those borderline documents, compared to. shows that the topic cache has comparable effect on improving the performance as the static cache when combined with the dynamic cache (0.48 vs. 0.54 in BLEU_W).", "labels": [], "entities": [{"text": "BLEU_D difference", "start_pos": 30, "end_pos": 47, "type": "METRIC", "confidence": 0.9498994201421738}, {"text": "BLEU_D", "start_pos": 81, "end_pos": 87, "type": "METRIC", "confidence": 0.9542881647745768}, {"text": "BLEU", "start_pos": 313, "end_pos": 317, "type": "METRIC", "confidence": 0.8969795107841492}]}, {"text": "shows the effectiveness of combining the dynamic and topic caches (sorted by BLEU_D).", "labels": [], "entities": [{"text": "BLEU_D", "start_pos": 77, "end_pos": 83, "type": "METRIC", "confidence": 0.9439281622568766}]}], "tableCaptions": [{"text": " Table 4: Contribution of various caches in our cache- based document-level SMT system. Note that signific- ance tests are done against Moses.", "labels": [], "entities": [{"text": "SMT", "start_pos": 76, "end_pos": 79, "type": "TASK", "confidence": 0.830155611038208}]}, {"text": " Table 5: Impact of the topic cache size", "labels": [], "entities": []}]}