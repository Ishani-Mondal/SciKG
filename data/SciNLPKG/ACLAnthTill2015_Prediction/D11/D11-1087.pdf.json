{"title": [{"text": "Entire Relaxation Path for Maximum Entropy Problems", "labels": [], "entities": []}], "abstractContent": [{"text": "We discuss and analyze the problem of finding a distribution that minimizes the relative entropy to a prior distribution while satisfying max-norm constraints with respect to an observed distribution.", "labels": [], "entities": []}, {"text": "This setting generalizes the classical maximum entropy problems as it relaxes the standard constraints on the observed values.", "labels": [], "entities": []}, {"text": "We tackle the problem by introducing a re-parametrization in which the unknown distribution is distilled to a single scalar.", "labels": [], "entities": []}, {"text": "We then describe a homotopy between the relaxation parameter and the distribution characterizing parameter.", "labels": [], "entities": []}, {"text": "The homotopy also reveals an aesthetic symmetry between the prior distribution and the observed distribution.", "labels": [], "entities": []}, {"text": "We then use the reformulated problem to describe a space and time efficient algorithm for tracking the entire relaxation path.", "labels": [], "entities": []}, {"text": "Our derivations are based on a compact geometric view of the relaxation path as a piecewise linear function in a two dimensional space of the relaxation-characterization parameters.", "labels": [], "entities": []}, {"text": "We demonstrate the usability of our approach by applying the problem to Zipfian distributions over a large alphabet.", "labels": [], "entities": []}], "introductionContent": [{"text": "Maximum entropy (max-ent) models and its dual counterpart, logistic regression, is a popular and effective tool in numerous natural language processing tasks.", "labels": [], "entities": []}, {"text": "The principle of maximum entropy was spelled out explicitly by E.T. Jaynes.", "labels": [], "entities": []}, {"text": "Applications of maximum entropy approach to natural language processing are numerous.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 44, "end_pos": 71, "type": "TASK", "confidence": 0.6659966707229614}]}, {"text": "A notable example and probably one of the earliest usages and generalizations of the maximum entropy principle to language processing is the work of Berger, Della Pietra\u00d72, and).", "labels": [], "entities": []}, {"text": "The original formulation of max-ent cast the problem as the task of finding the distribution attaining the highest entropy subject to equality constraints.", "labels": [], "entities": []}, {"text": "While this formalism is aesthetic and paves the way to a simple dual in the form of a unique Gibbs distribution (Della), it does not provide sufficient tools to deal with input noise and sparse representation of the target Gibbs distribution.", "labels": [], "entities": []}, {"text": "To mitigate these issues, numerous relaxation schemes of the equality constraints have been proposed.", "labels": [], "entities": []}, {"text": "A notable recent work by provided a general constraint-relaxation framework.", "labels": [], "entities": []}, {"text": "See also the references therein for an in depth overview of other approaches and generalizations of max-ent.", "labels": [], "entities": []}, {"text": "The constraint relaxation surfaces a natural parameter, namely, a relaxation value.", "labels": [], "entities": []}, {"text": "The dual form of this free parameter is the regularization value of penalized logistic regression problems.", "labels": [], "entities": []}, {"text": "Typically this parameter is set by experimentation using cross validation technique.", "labels": [], "entities": []}, {"text": "The relaxed maximum-entropy problem setting is the starting point of this paper.", "labels": [], "entities": []}, {"text": "In this paper we describe and analyze a framework for efficiently tracking the entire relaxation path of constrained max-ent problems.", "labels": [], "entities": []}, {"text": "2 with a generalization in which we discuss the problem of finding a distribution that minimizes the relative entropy to a given prior distribution while satisfying max-norm constraints with respect to an observed distribution.", "labels": [], "entities": []}, {"text": "3 we tackle the problem by introducing a re-parametrization in which the 941 unknown distribution is distilled to a single scalar.", "labels": [], "entities": []}, {"text": "We next describe in Sec.", "labels": [], "entities": []}, {"text": "4 a homotopy between the relaxation parameter and the distribution characterizing parameter.", "labels": [], "entities": []}, {"text": "This formulation also reveals an aesthetic symmetry between the prior distribution and the observed distribution.", "labels": [], "entities": []}, {"text": "We use the reformulated problem to describe in Secs.", "labels": [], "entities": []}, {"text": "5-6 space and time efficient algorithms for tracking the entire relaxation path.", "labels": [], "entities": []}, {"text": "Our derivations are based on a compact geometric view of the relaxation path as a piecewise linear function in a two dimensional space of the relaxation-characterization parameters.", "labels": [], "entities": []}, {"text": "In contrast to common homotopy methods for the Lasso Osborne et al.", "labels": [], "entities": []}, {"text": "(2000), our procedure for tracking the max-ent homotopy results in an uncharacteristically low complexity bounds thus renders the approach applicable for large alphabets.", "labels": [], "entities": []}, {"text": "We provide preliminary experimental results with Zipf distributions in Sec.", "labels": [], "entities": []}, {"text": "8 that demonstrate the merits of our approach.", "labels": [], "entities": []}, {"text": "Finally, we conclude in Sec.", "labels": [], "entities": []}, {"text": "9 with a brief discussion of future directions.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}