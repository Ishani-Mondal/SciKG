{"title": [{"text": "Correcting Semantic Collocation Errors with L1-induced Paraphrases", "labels": [], "entities": []}], "abstractContent": [{"text": "We present a novel approach for automatic collocation error correction in learner English which is based on paraphrases extracted from parallel corpora.", "labels": [], "entities": [{"text": "automatic collocation error correction", "start_pos": 32, "end_pos": 70, "type": "TASK", "confidence": 0.5527003258466721}]}, {"text": "Our key assumption is that collocation errors are often caused by semantic similarity in the first language (L1-language) of the writer.", "labels": [], "entities": []}, {"text": "An analysis of a large corpus of annotated learner English confirms this assumption.", "labels": [], "entities": []}, {"text": "We evaluate our approach on real-world learner data and show that L1-induced paraphrases outperform traditional approaches based on edit distance, ho-mophones, and WordNet synonyms.", "labels": [], "entities": []}], "introductionContent": [{"text": "Grammatical error correction (GEC) is emerging as a commercially attractive application of natural language processing (NLP) for the booming market of English as foreign or second language (EFL/ESL 1 ).", "labels": [], "entities": [{"text": "Grammatical error correction (GEC)", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.8428066472212473}, {"text": "natural language processing (NLP)", "start_pos": 91, "end_pos": 124, "type": "TASK", "confidence": 0.7715994020303091}]}, {"text": "The de facto standard approach to GEC is to build a statistical model that can choose the most likely correction from a confusion set of possible correction choices.", "labels": [], "entities": [{"text": "GEC", "start_pos": 34, "end_pos": 37, "type": "TASK", "confidence": 0.934556782245636}]}, {"text": "The way the confusion set is defined depends on the type of error.", "labels": [], "entities": []}, {"text": "Work in contextsensitive spelling error correction) has traditionally focused on confusion sets with similar spelling (e.g., {dessert, desert}) or similar pronunciation (e.g., {there, their}).", "labels": [], "entities": [{"text": "contextsensitive spelling error correction", "start_pos": 8, "end_pos": 50, "type": "TASK", "confidence": 0.5808815807104111}]}, {"text": "In other words, the words in a confusion set are deemed confusable because of orthographic or phonetic similarity.", "labels": [], "entities": []}, {"text": "Other work in GEC has defined the confu-sion sets based on syntactic similarity, for example all English articles or the most frequent English prepositions form a confusion set (see for example) among others).", "labels": [], "entities": [{"text": "GEC", "start_pos": 14, "end_pos": 17, "type": "DATASET", "confidence": 0.9018840789794922}]}, {"text": "In contrast, we investigate in this paper a class of grammatical errors where the source of confusion is the similar semantics of the words, rather than orthography, phonetics, or syntax.", "labels": [], "entities": []}, {"text": "In particular, we focus on collocation errors in EFL writing.", "labels": [], "entities": [{"text": "EFL writing", "start_pos": 49, "end_pos": 60, "type": "TASK", "confidence": 0.8624245822429657}]}, {"text": "The term collocation) describes a sequence of words that is conventionally used together in a particular way by native speakers and appears more often together than one would expect by chance.", "labels": [], "entities": []}, {"text": "The correct use of collocations is a major difficulty for EFL students.", "labels": [], "entities": []}, {"text": "In this work, we present a novel approach for automatic correction of collocation errors in EFL writing.", "labels": [], "entities": [{"text": "automatic correction of collocation errors", "start_pos": 46, "end_pos": 88, "type": "TASK", "confidence": 0.767153549194336}, {"text": "EFL writing", "start_pos": 92, "end_pos": 103, "type": "TASK", "confidence": 0.7717940509319305}]}, {"text": "Our key observation is that words are potentially confusable for an EFL student if they have similar translations in the writer's first language (L1-language), or in other words if they have the same semantics in the L1-language of the writer.", "labels": [], "entities": []}, {"text": "The Chinese word \u770b (k\u00e0n), for example, has over a dozen translations in English, including the words see, look, read, and watch.", "labels": [], "entities": []}, {"text": "A Chinese speaker who still \"thinks\" in Chinese has to choose from all these possible translations when he wants to express a sentence like I like to watch movies and might instead produce a sentence like *I like to look movies.", "labels": [], "entities": []}, {"text": "Although the meanings of watch and look are similar, the former is clearly the more fluent choice in this context.", "labels": [], "entities": []}, {"text": "While these types of L1-transfer er-rors have been known in the EFL teaching literature, research in GEC has mostly ignored this fact.", "labels": [], "entities": [{"text": "EFL teaching literature", "start_pos": 64, "end_pos": 87, "type": "DATASET", "confidence": 0.8718357682228088}, {"text": "GEC", "start_pos": 101, "end_pos": 104, "type": "DATASET", "confidence": 0.7318698167800903}]}, {"text": "We first analyze collocation errors in the NUS Corpus of Learner English (NUCLE), a fully annotated one-million-word corpus of learner English which we will make available to the community for research purposes (see Section 3 for details about the corpus).", "labels": [], "entities": [{"text": "NUS Corpus of Learner English (NUCLE)", "start_pos": 43, "end_pos": 80, "type": "DATASET", "confidence": 0.9568408876657486}]}, {"text": "Our analysis confirms that many collocation errors can be traced to similar translations in the writer's L1-language.", "labels": [], "entities": []}, {"text": "Based on this result, we propose a novel approach for automatic collocation error correction.", "labels": [], "entities": [{"text": "automatic collocation error correction", "start_pos": 54, "end_pos": 92, "type": "TASK", "confidence": 0.5614169016480446}]}, {"text": "The key component in our approach generates L1-induced paraphrases which we automatically extract from an L1-English parallel corpus.", "labels": [], "entities": []}, {"text": "Our proposed approach outperforms traditional approaches based on edit distance, homophones, and WordNet synonyms on a test set of real-world learner data in an automatic and a human evaluation.", "labels": [], "entities": []}, {"text": "Finally, we present a detailed analysis of unsolved instances in our data set to highlight directions for future work.", "labels": [], "entities": []}, {"text": "Our work adds to a growing body of research that leverages parallel corpora for semantic NLP tasks, for example in word sense disambiguation, paraphrasing, and machine translation evaluation).", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 115, "end_pos": 140, "type": "TASK", "confidence": 0.6837193369865417}, {"text": "machine translation evaluation", "start_pos": 160, "end_pos": 190, "type": "TASK", "confidence": 0.8410189946492513}]}, {"text": "The remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "The next section reviews related work.", "labels": [], "entities": []}, {"text": "Section 3 presents our analysis of collocation errors.", "labels": [], "entities": []}, {"text": "Section 4 describes our approach for automatic collocation error correction.", "labels": [], "entities": [{"text": "automatic collocation error correction", "start_pos": 37, "end_pos": 75, "type": "TASK", "confidence": 0.5752645209431648}]}, {"text": "The experimental setup and the results are described in Sections 5 and 6, respectively.", "labels": [], "entities": []}, {"text": "Section 7 provides further analysis.", "labels": [], "entities": []}, {"text": "Section 8 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we empirically evaluate our approach on real collocation errors in learner English.", "labels": [], "entities": []}, {"text": "We conduct an automatic and a human evaluation.", "labels": [], "entities": []}, {"text": "Our main evaluation metric is mean reciprocal rank (MRR) which is the arithmetic mean of the inverse ranks of the first correct answer returned by the system where N is the size of the test set.", "labels": [], "entities": [{"text": "mean reciprocal rank (MRR)", "start_pos": 30, "end_pos": 56, "type": "METRIC", "confidence": 0.9578163127104441}]}, {"text": "If the system did not return a correct answer fora test instance, we set 1 rank(i) to zero.", "labels": [], "entities": []}, {"text": "In the human evaluation, we additionally report precision at rank k, k = 1, 2, 3, which we calculate as follows: where A is the set of returned answers of rank k or less and score(\u00b7) is a real-valued scoring function between zero and one.", "labels": [], "entities": [{"text": "precision", "start_pos": 48, "end_pos": 57, "type": "METRIC", "confidence": 0.9991436004638672}, {"text": "score", "start_pos": 174, "end_pos": 179, "type": "METRIC", "confidence": 0.9608432054519653}]}, {"text": "Automatic correction of collocation errors can conceptually be divided into two steps: i) identification of wrong collocations in the input, and ii) correction of the identified collocations.", "labels": [], "entities": [{"text": "Automatic correction of collocation errors", "start_pos": 0, "end_pos": 42, "type": "TASK", "confidence": 0.7915527760982514}]}, {"text": "In this work, we focus on the second step and assume that the erroneous collocation has already been identified.", "labels": [], "entities": []}, {"text": "While this might seem like a simplification, it has been the common evaluation setup in collocation error correction (see for example ().", "labels": [], "entities": [{"text": "collocation error correction", "start_pos": 88, "end_pos": 116, "type": "TASK", "confidence": 0.6184801260630289}]}, {"text": "It also has a practical application where the user first selects a word or phrase and the system displays possible corrections.", "labels": [], "entities": []}, {"text": "In our experiments, we use the start and end offset of the collocation error provided by the human annotator to identify the location of the collocation error.", "labels": [], "entities": [{"text": "start and end offset", "start_pos": 31, "end_pos": 51, "type": "METRIC", "confidence": 0.7417512685060501}]}, {"text": "We fix the translation of the rest of the sentence to its identity.", "labels": [], "entities": []}, {"text": "We remove phrase table entries where the phrase and the candidate correction are identical, thus practically forcing the system to change the identified phrase.", "labels": [], "entities": []}, {"text": "We set the distortion limit of the decoder to zero to achieve monotone decoding.", "labels": [], "entities": []}, {"text": "We previously observed that word order errors are virtually absent in our collocation errors.", "labels": [], "entities": []}, {"text": "For the language model, we use a 5-gram language model trained on the English Gigaword corpus with modified Kneser-Ney smoothing.", "labels": [], "entities": [{"text": "English Gigaword corpus", "start_pos": 70, "end_pos": 93, "type": "DATASET", "confidence": 0.830562154452006}]}, {"text": "All experiments use the same language model to allow a fair comparison.", "labels": [], "entities": []}, {"text": "We perform MERT training with the popular BLEU metric () on the development set of erroneous sentences and their corrections.", "labels": [], "entities": [{"text": "MERT", "start_pos": 11, "end_pos": 15, "type": "TASK", "confidence": 0.8384881615638733}, {"text": "BLEU metric", "start_pos": 42, "end_pos": 53, "type": "METRIC", "confidence": 0.9758688807487488}]}, {"text": "As the search space is restricted to changing a single phrase per sentence, training converges relatively quickly after two or three iterations.", "labels": [], "entities": []}, {"text": "After convergence, the model can be used to automatically correct new collocation errors.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of the NUS Corpus of Learner En- glish (NUCLE)", "labels": [], "entities": [{"text": "NUS Corpus of Learner En- glish (NUCLE)", "start_pos": 28, "end_pos": 67, "type": "DATASET", "confidence": 0.942771178483963}]}, {"text": " Table 3: Examples of collocation errors with different sources of confusion. The correction is shown in parenthesis.  For L1-transfer, we also show the shared Chinese translation. The L1-transfer examples shown here do not belong to  any of the other categories.", "labels": [], "entities": []}, {"text": " Table 2: Analysis of collocation errors. The threshold for  spelling errors is one for phrases of up to six characters  and two for the remaining phrases.", "labels": [], "entities": []}, {"text": " Table 4: Results of automatic evaluation. Columns two to six show the number of gold answers that are ranked within  the top k answers. The last column shows the mean reciprocal rank in percentage. Bigger values are better.", "labels": [], "entities": []}, {"text": " Table 6: Results of human evaluation. Rank and MRR results are shown for the intersection (first value) and union  (second value) of human judgments.", "labels": [], "entities": [{"text": "Rank", "start_pos": 39, "end_pos": 43, "type": "METRIC", "confidence": 0.9659459590911865}, {"text": "MRR", "start_pos": 48, "end_pos": 51, "type": "METRIC", "confidence": 0.9910725951194763}]}]}