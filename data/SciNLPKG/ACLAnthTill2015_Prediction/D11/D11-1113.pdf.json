{"title": [{"text": "Parse Correction with Specialized Models for Difficult Attachment Types", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper develops a framework for syntactic dependency parse correction.", "labels": [], "entities": [{"text": "syntactic dependency parse correction", "start_pos": 36, "end_pos": 73, "type": "TASK", "confidence": 0.8761708587408066}]}, {"text": "Dependencies in an input parse tree are revised by selecting , fora given dependent, the best governor from within a small set of candidates.", "labels": [], "entities": []}, {"text": "We use a discriminative linear ranking model to select the best governor from a group of candidates fora dependent, and our model includes a rich feature set that encodes syntactic structure in the input parse tree.", "labels": [], "entities": []}, {"text": "The parse correction framework is parser-agnostic, and can correct attachments using either a generic model or specialized models tailored to difficult attachment types like coordination and pp-attachment.", "labels": [], "entities": [{"text": "parse correction", "start_pos": 4, "end_pos": 20, "type": "TASK", "confidence": 0.9289039373397827}]}, {"text": "Our experiments show that parse correction, combining a generic model with specialized models for difficult attachment types, can successfully improve the quality of predicted parse trees output by several representative state-of-the-art dependency parsers for French.", "labels": [], "entities": [{"text": "parse correction", "start_pos": 26, "end_pos": 42, "type": "TASK", "confidence": 0.9835814535617828}]}], "introductionContent": [{"text": "In syntactic dependency parse correction, attachments in an input parse tree are revised by selecting, fora given dependent, the best governor from within a small set of candidates.", "labels": [], "entities": [{"text": "syntactic dependency parse correction", "start_pos": 3, "end_pos": 40, "type": "TASK", "confidence": 0.7497040033340454}]}, {"text": "The motivation behind parse correction is that attachment decisions, especially traditionally difficult ones like pp-attachment and coordination, may require substantial contextual information in order to be made accurately.", "labels": [], "entities": [{"text": "parse correction", "start_pos": 22, "end_pos": 38, "type": "TASK", "confidence": 0.9590860605239868}]}, {"text": "Because syntactic dependency parsers predict the parse tree for an entire sentence, they may not be able to take into account sufficient context when making attachment decisions, due to computational complexity.", "labels": [], "entities": [{"text": "syntactic dependency parsers predict the parse", "start_pos": 8, "end_pos": 54, "type": "TASK", "confidence": 0.7209513833125433}]}, {"text": "Assuming nonetheless that a predicted parse tree is mostly accurate, parse correction can revise difficult attachments by using the predicted tree's syntactic structure to restrict the set of candidate governors and extract a rich set of features to help select among them.", "labels": [], "entities": [{"text": "parse correction", "start_pos": 69, "end_pos": 85, "type": "TASK", "confidence": 0.9388244152069092}]}, {"text": "Parse correction is also appealing because it is parser-agnostic: it can be trained to correct the output of any dependency parser.", "labels": [], "entities": [{"text": "Parse correction", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.728312075138092}]}, {"text": "In Section 2 we discuss work related to parse correction, pp-attachment and coordination resolution.", "labels": [], "entities": [{"text": "parse correction", "start_pos": 40, "end_pos": 56, "type": "TASK", "confidence": 0.9788848161697388}, {"text": "coordination resolution", "start_pos": 76, "end_pos": 99, "type": "TASK", "confidence": 0.8138244450092316}]}, {"text": "In Section 3 we discuss dependency structure and various statistical dependency parsing approaches.", "labels": [], "entities": [{"text": "dependency structure", "start_pos": 24, "end_pos": 44, "type": "TASK", "confidence": 0.8973671793937683}, {"text": "statistical dependency parsing", "start_pos": 57, "end_pos": 87, "type": "TASK", "confidence": 0.617457777261734}]}, {"text": "In Section 4 we introduce the parse correction framework, and Section 5 describes the features and learning model used in our implementation.", "labels": [], "entities": [{"text": "parse correction", "start_pos": 30, "end_pos": 46, "type": "TASK", "confidence": 0.964895486831665}]}, {"text": "In Section 6 we present experiments in which parse correction revises the predicted parse trees of four state-of-the-art dependency parsers for French.", "labels": [], "entities": [{"text": "parse correction", "start_pos": 45, "end_pos": 61, "type": "TASK", "confidence": 0.9290481507778168}]}, {"text": "We provide concluding remarks in Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "We present experiments where we applied parse correction to the output of four state-of-the-art dependency parsers for French.", "labels": [], "entities": [{"text": "parse correction", "start_pos": 40, "end_pos": 56, "type": "TASK", "confidence": 0.8932192027568817}]}, {"text": "We conducted our evaluation on the FTB using the standard training, development (dev), and test splits (containing 9,881, 1,235 and 1,235 sentences, respectively).", "labels": [], "entities": [{"text": "FTB", "start_pos": 35, "end_pos": 38, "type": "DATASET", "confidence": 0.9379051923751831}]}, {"text": "To train our parse correction models, we generated specialized training sets corresponding to each parser by doing 10-fold jackknifing on the FTB training set (cf. Section 5.1).", "labels": [], "entities": [{"text": "parse correction", "start_pos": 13, "end_pos": 29, "type": "TASK", "confidence": 0.9735988676548004}, {"text": "FTB training set", "start_pos": 142, "end_pos": 158, "type": "DATASET", "confidence": 0.9548027912775675}]}, {"text": "Each parser was run on the FTB dev and test sets, providing baseline unlabeled attachment score (UAS) results and output parse trees to be corrected.", "labels": [], "entities": [{"text": "FTB dev and test sets", "start_pos": 27, "end_pos": 48, "type": "DATASET", "confidence": 0.9337984204292298}, {"text": "baseline unlabeled attachment score (UAS) results", "start_pos": 60, "end_pos": 109, "type": "METRIC", "confidence": 0.7819351069629192}]}], "tableCaptions": [{"text": " Table 1: Parse correction oracle UAS (%) for differ- ent neighborhood sizes, by dependent type (coordinating  conjunctions, prepositions, or all dependents). Also, a  reranking oracle for MSTParser using the top-100 parses.", "labels": [], "entities": [{"text": "Parse correction oracle UAS", "start_pos": 10, "end_pos": 37, "type": "METRIC", "confidence": 0.7393333315849304}]}, {"text": " Table 2: Coordinating conjunction, preposition, and over- all UAS (%) by corrective configuration on the test set.  Significant improvements over the baseline starred.", "labels": [], "entities": [{"text": "UAS", "start_pos": 63, "end_pos": 66, "type": "METRIC", "confidence": 0.9113920331001282}]}, {"text": " Table 3: Breakdown of modifications made under the  specialized configuration for each parser, by dependent  type. w\u2192c is wrong-to-correct, c\u2192w is correct-to- wrong, w\u2192w is wrong-to-wrong, and Mods is the per- centage of tokens modified.", "labels": [], "entities": []}]}