{"title": [{"text": "Improved Transliteration Mining Using Graph Reinforcement", "labels": [], "entities": [{"text": "Improved Transliteration Mining", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.8815620342890421}, {"text": "Graph Reinforcement", "start_pos": 38, "end_pos": 57, "type": "TASK", "confidence": 0.622532069683075}]}], "abstractContent": [{"text": "Mining of transliterations from comparable or parallel text can enhance natural language processing applications such as machine translation and cross language information retrieval.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 121, "end_pos": 140, "type": "TASK", "confidence": 0.8198525011539459}, {"text": "cross language information retrieval", "start_pos": 145, "end_pos": 181, "type": "TASK", "confidence": 0.7707478255033493}]}, {"text": "This paper presents an enhanced transliteration mining technique that uses a generative graph reinforcement model to infer mappings between source and target character sequences.", "labels": [], "entities": [{"text": "transliteration mining", "start_pos": 32, "end_pos": 54, "type": "TASK", "confidence": 0.9033032655715942}, {"text": "generative graph reinforcement", "start_pos": 77, "end_pos": 107, "type": "TASK", "confidence": 0.8362398346265157}]}, {"text": "An initial set of mappings are learned through automatic alignment of transliteration pairs at character sequence level.", "labels": [], "entities": []}, {"text": "Then, these mappings are modeled using a bipartite graph.", "labels": [], "entities": []}, {"text": "A graph reinforcement algorithm is then used to enrich the graph by inferring additional mappings.", "labels": [], "entities": []}, {"text": "During graph reinforcement, appropriate link reweighting is used to promote good mappings and to demote bad ones.", "labels": [], "entities": [{"text": "graph reinforcement", "start_pos": 7, "end_pos": 26, "type": "TASK", "confidence": 0.6751601845026016}]}, {"text": "The enhanced transliteration mining technique is tested in the context of mining transliterations from parallel Wikipedia titles in 4 alphabet-based languages pairs, namely English-Arabic, English-Russian, English-Hindi, and English-Tamil.", "labels": [], "entities": [{"text": "transliteration mining", "start_pos": 13, "end_pos": 35, "type": "TASK", "confidence": 0.8654336333274841}]}, {"text": "The improvements in F1-measure over the baseline system were 18.7, 1.0, 4.5, and 32.5 basis points for the four language pairs respectively.", "labels": [], "entities": [{"text": "F1-measure", "start_pos": 20, "end_pos": 30, "type": "METRIC", "confidence": 0.9982740879058838}]}, {"text": "The results herein outperform the best reported results in the literature by 2.6, 4.8, 0.8, and 4.1 basis points for the four language pairs respectively.", "labels": [], "entities": []}], "introductionContent": [{"text": "Transliteration Mining (TM) is the process of finding transliterated word pairs in parallel or comparable corpora.", "labels": [], "entities": [{"text": "Transliteration Mining (TM)", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.8967686176300049}]}, {"text": "TM has many potential applications such as mining training data for transliteration, improving lexical coverage for machine translation, and cross language retrieval via translation resource expansion.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 116, "end_pos": 135, "type": "TASK", "confidence": 0.7900774776935577}, {"text": "cross language retrieval", "start_pos": 141, "end_pos": 165, "type": "TASK", "confidence": 0.7258705298105875}, {"text": "translation resource expansion", "start_pos": 170, "end_pos": 200, "type": "TASK", "confidence": 0.7602362235387167}]}, {"text": "TM has been gaining some attention lately with a shared task in the ACL 2010 NEWS workshop.", "labels": [], "entities": [{"text": "TM", "start_pos": 0, "end_pos": 2, "type": "TASK", "confidence": 0.7286767363548279}, {"text": "ACL 2010 NEWS workshop", "start_pos": 68, "end_pos": 90, "type": "DATASET", "confidence": 0.8646605461835861}]}, {"text": "One popular statistical TM approach is performed in two stages.", "labels": [], "entities": [{"text": "TM", "start_pos": 24, "end_pos": 26, "type": "TASK", "confidence": 0.8944343328475952}]}, {"text": "First, a generative model is trained by performing automatic character level alignment of parallel transliterated word pairs to find character segment mappings between source and target languages.", "labels": [], "entities": [{"text": "generative", "start_pos": 9, "end_pos": 19, "type": "TASK", "confidence": 0.9702243208885193}, {"text": "character level alignment of parallel transliterated word pairs", "start_pos": 61, "end_pos": 124, "type": "TASK", "confidence": 0.7672657296061516}]}, {"text": "Second, given comparable or parallel text, the trained generative model is used to generate possible transliterations of a word in the source language while constraining the transliterations to words that exist in the target language.", "labels": [], "entities": []}, {"text": "However, two problems arise in this approach: 1.", "labels": [], "entities": []}, {"text": "Many possible character sequence mappings between source and target languages may not be observed in training data, particularly when limited training data is available -hurting recall.", "labels": [], "entities": [{"text": "character sequence mappings between source and target languages", "start_pos": 14, "end_pos": 77, "type": "TASK", "confidence": 0.7804813459515572}, {"text": "recall", "start_pos": 178, "end_pos": 184, "type": "METRIC", "confidence": 0.9967427849769592}]}, {"text": "2. Conditional probability estimates of obtained mappings maybe inaccurate, because some mappings and some character sequences may not appear a sufficient number of times in training to properly estimate their probabilities -hurting precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 233, "end_pos": 242, "type": "METRIC", "confidence": 0.9985260367393494}]}, {"text": "In this paper we focus on overcoming these two problems to improve overall TM.", "labels": [], "entities": [{"text": "TM", "start_pos": 75, "end_pos": 77, "type": "TASK", "confidence": 0.9446203112602234}]}, {"text": "To address the first problem, we modeled the automatically obtained character sequence mappings (from alignment) as a bipartite graph and then we performed graph reinforcement to enrich the graph and predict possible mappings that were not directly obtained from training data.", "labels": [], "entities": []}, {"text": "The example in motivates graph reinforcement.", "labels": [], "entities": []}, {"text": "In the example, the Arabic letter -\u202b\u2016\u0642\u202c (pronounced as -qa\u2016) was not aligned to the English letter -c\u2016 in training data.", "labels": [], "entities": []}, {"text": "Such a mapping seems probable given that another Arabic letter, -\u202b\u2016\u0643\u202c (pronounced as -ka\u2016), maps to two English letters, -q\u2016 and -k\u2016, to which -\u202b\u2016\u0642\u202c also maps.", "labels": [], "entities": []}, {"text": "In this case, there are multiple paths that would lead to a mapping between the Arabic letter -\u202b\u2016\u0642\u202c and the English letter -c\u2016, namely \u202b\u0642\u202c \uf0e8 q \uf0e8 \u202b\u0643\u202c \uf0e8 c and \u202b\u0642\u202c \uf0e8 k \uf0e8 \u202b\u0643\u202c \uf0e8 c.", "labels": [], "entities": []}, {"text": "By using multiple paths as sources of evidence, we can infer the new mapping and estimate its probability.", "labels": [], "entities": []}, {"text": "Another method for overcoming the missing mappings problem entails assigning small smoothing probabilities to unseen mappings.", "labels": [], "entities": []}, {"text": "However, from looking at the graph, it is evident that some mappings could be inferred and should be assigned probabilities that are higher than a small smoothing probability.", "labels": [], "entities": []}, {"text": "The second problem has to do primarily with some characters in one language, typically vowels, mapping to many character sequences in the other language, with some of these mappings assuming very high probabilities (due to limited training data).", "labels": [], "entities": []}, {"text": "To overcome this problem, we used link reweighting in graph reinforcement to scale down the likelihood of mappings to target character sequences in proportion to how many source sequences map to them.", "labels": [], "entities": []}, {"text": "We tested the proposed method using the ACL 2010 NEWS workshop data for English-Arabic, English-Russian, English-Hindi, and EnglishTamil ().", "labels": [], "entities": [{"text": "ACL 2010 NEWS workshop data", "start_pos": 40, "end_pos": 67, "type": "DATASET", "confidence": 0.9255893230438232}, {"text": "EnglishTamil", "start_pos": 124, "end_pos": 136, "type": "DATASET", "confidence": 0.9354546070098877}]}, {"text": "For each language pair, the standard ACL 2010 NEWS workshop data contained abase set of 1,000 transliteration pairs for training, and set of 1,000 parallel Wikipedia titles for testing.", "labels": [], "entities": [{"text": "ACL 2010 NEWS workshop data", "start_pos": 37, "end_pos": 64, "type": "DATASET", "confidence": 0.8709545373916626}]}, {"text": "The contributions of the paper are: 1.", "labels": [], "entities": []}, {"text": "Employing graph reinforcement to improve the coverage of automatically aligned data -as they apply to transliteration mining.", "labels": [], "entities": [{"text": "transliteration mining", "start_pos": 102, "end_pos": 124, "type": "TASK", "confidence": 0.7749767899513245}]}, {"text": "2. Applying link reweighting to overcome situations where certain tokens -character sequences in the case of transliteration -tend to have many mappings, which are often erroneous.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows: Section 2 surveys prior work on transliteration mining; Section 3 describes the baseline TM approach and reports on its effectiveness; Section 4 describes the proposed graph reinforcement along with link reweighting and reports on the observed improvements; and Section 5 concludes the paper.", "labels": [], "entities": [{"text": "transliteration mining", "start_pos": 79, "end_pos": 101, "type": "TASK", "confidence": 0.8280443251132965}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Baseline results for all language pairs.  Results with smoothing are shaded.", "labels": [], "entities": []}, {"text": " Table 2: Best results obtained in ACL-2010 NEWS TM  shared task compared to graph reinforcement with link  reweighting after 10 iterations", "labels": [], "entities": [{"text": "ACL-2010 NEWS TM  shared task", "start_pos": 35, "end_pos": 64, "type": "TASK", "confidence": 0.6131495177745819}]}]}