{"title": [{"text": "Parser Evaluation over Local and Non-Local Deep Dependencies in a Large Corpus", "labels": [], "entities": [{"text": "Parser Evaluation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8904948830604553}]}], "abstractContent": [{"text": "In order to obtain a fine-grained evaluation of parser accuracy over naturally occurring text, we study 100 examples each often reasonably frequent linguistic phenomena, randomly selected from a parsed version of the En-glish Wikipedia.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 55, "end_pos": 63, "type": "METRIC", "confidence": 0.8721860647201538}, {"text": "En-glish Wikipedia", "start_pos": 217, "end_pos": 235, "type": "DATASET", "confidence": 0.7885113060474396}]}, {"text": "We construct a corresponding set of gold-standard target dependencies for these 1000 sentences, operationalize map-pings to these targets from seven state-of-the-art parsers, and evaluate the parsers against this data to measure their level of success in identifying these dependencies.", "labels": [], "entities": []}], "introductionContent": [{"text": "The terms \"deep\" and \"shallow\" are frequently used to characterize or contrast different approaches to parsing.", "labels": [], "entities": []}, {"text": "Inevitably, such informal notions lack a clear definition, and there is little evidence of community consensus on the relevant dimension(s) of depth, let alone agreement on applicable metrics.", "labels": [], "entities": []}, {"text": "At its core, the implied dichotomy of approaches alludes to differences in the interpretation of the parsing task.", "labels": [], "entities": [{"text": "parsing task", "start_pos": 101, "end_pos": 113, "type": "TASK", "confidence": 0.7998137772083282}]}, {"text": "Its abstract goal, on the one hand, could be pre-processing of the linguistic signal, to enable subsequent stages of analysis.", "labels": [], "entities": []}, {"text": "On the other hand, it could be making explicit the (complete) contribution that the grammatical form of the linguistic signal makes to interpretation, working out who did what to whom.", "labels": [], "entities": []}, {"text": "Stereotypically, one expects corresponding differences in the choice of interface representations, ranging from various levels of syntactic analysis to logical-form representations of semantics.", "labels": [], "entities": []}, {"text": "In this paper, we seek to probe aspects of variation in automated linguistic analysis.", "labels": [], "entities": [{"text": "automated linguistic analysis", "start_pos": 56, "end_pos": 85, "type": "TASK", "confidence": 0.6450332005818685}]}, {"text": "We make the assumption that an integral part of many (albeit not all) applications of parsing technology is the recovery of structural relations, i.e. dependencies at the level of interpretation.", "labels": [], "entities": [{"text": "parsing", "start_pos": 86, "end_pos": 93, "type": "TASK", "confidence": 0.9742610454559326}]}, {"text": "We suggest a selection often linguistic phenomena that we believe (a) occur with reasonably high frequency in running text and (b) have the potential to shed some light on the depths of linguistic analysis.", "labels": [], "entities": []}, {"text": "We quantify the frequency of these constructions in the English Wikipedia, then annotate 100 example sentences for each phenomenon with gold-standard dependencies reflecting core properties of the phenomena of interest.", "labels": [], "entities": []}, {"text": "This gold standard is then used to estimate the recall of these dependencies by seven commonly used parsers, providing the basis fora qualitative discussion of the state of the art in parsing for English.", "labels": [], "entities": [{"text": "recall", "start_pos": 48, "end_pos": 54, "type": "METRIC", "confidence": 0.997884213924408}]}, {"text": "In this work, we answer the call by for \"construction-focused parser evaluation\", extending and complementing their work in several respects: (i) we investigate both local and non-local dependencies which prove to be challenging for many existing state-of-the-art parsers; (ii) we investigate a wider range of linguistic phenomena, each accompanied with an in-depth discussion of relevant properties; and (iii) we draw our data from the 50-million sentence English Wikipedia, which is more varied and a thousand times larger than the venerable WSJ corpus, to explore a more level and ambitious playing field for parser comparison.", "labels": [], "entities": [{"text": "parser evaluation", "start_pos": 62, "end_pos": 79, "type": "TASK", "confidence": 0.7828891575336456}, {"text": "WSJ corpus", "start_pos": 544, "end_pos": 554, "type": "DATASET", "confidence": 0.9590805172920227}, {"text": "parser comparison", "start_pos": 612, "end_pos": 629, "type": "TASK", "confidence": 0.8823705911636353}]}], "datasetContent": [{"text": "With the test data consisting of 100 items for each of our ten selected phenomena, we ran all seven parsing systems and recorded their dependency-style outputs for each sentence.", "labels": [], "entities": []}, {"text": "While these outputs are not directly comparable with each other, we were able to associate our manually-annotated target dependencies with parser-specific dependencies, by defining sets of phenomenon-specific regular expressions for each parser.", "labels": [], "entities": []}, {"text": "In principle, we allow this mapping to be somewhat complex (and forgiving to non-contentful variation), though we require that it work deterministically and not involve specific lexical information.", "labels": [], "entities": []}, {"text": "An example set is given in.: Regexp set to evaluate C&C for absol.", "labels": [], "entities": [{"text": "Regexp", "start_pos": 29, "end_pos": 35, "type": "METRIC", "confidence": 0.9196874499320984}]}, {"text": "These expressions fit the output that we got from the C&C parser, illustrated in with a relevant portion of the dependencies produced for the example in.", "labels": [], "entities": []}, {"text": "Here the C&C dependency (ncsubj passed 4 Act 1 ) matches the first target in the gold-standard), but no matching C&C dependency is found for the other two targets.", "labels": [], "entities": [{"text": "ncsubj passed 4 Act 1", "start_pos": 25, "end_pos": 46, "type": "DATASET", "confidence": 0.9613671183586121}]}, {"text": "(xmod _ Act_1 passed_4) (ncsubj passed_4 Act_1 _) (ncmod _ withdrew,_9 Jessop_8) (dobj year,_7 withdrew,_9) Figure 3: Excerpts of C&C output for item in.", "labels": [], "entities": []}, {"text": "The regular expressions operate solely on the dependency labels and are not lexically-specific.", "labels": [], "entities": []}, {"text": "They are specific to each phenomenon, as we did not attempt to write a general dependency converter, but rather to discover what patterns of dependency relations describe the phenomenon when it is correctly identified by each parser.", "labels": [], "entities": []}, {"text": "Thus, though we did not holdout a test set, we believe that they would generalize to additional gold standard material annotated in the same way for the same phenomena.", "labels": [], "entities": []}, {"text": "In total, we wrote 364 regular expressions to handle the output of the seven parsers, allowing some leeway in the role labels used by a parser for any given target dependency.", "labels": [], "entities": []}, {"text": "The supplementary materials for this paper include the test data, parser outputs, target annotations, and evaluation script.", "labels": [], "entities": []}, {"text": "provides a visualization of the results of our evaluation.", "labels": [], "entities": []}, {"text": "Each column of points represents one dependency type.", "labels": [], "entities": []}, {"text": "Dependency types for the same phenomenon are represented by adjacent columns.", "labels": [], "entities": []}, {"text": "The order of the columns within a phenomenon follows the order of the dependency descriptions in: For each pair, the dependency type with the higher score for the majority of the parsers is shown first (to the left).", "labels": [], "entities": []}, {"text": "The phenomena themselves are also arranged according to increasing (average) difficulty.", "labels": [], "entities": []}, {"text": "itexpl only has one column, as we annotated just one dependency per instance here.", "labels": [], "entities": []}, {"text": "(The two descriptions in reflect different, mutually-incompatible instance types.)", "labels": [], "entities": []}, {"text": "Since expletive it should not be the semantic dependent of any head, the targets are generalized for this phenomenon and the evaluation script counts as incor- rect any dependency involving referential it.", "labels": [], "entities": []}, {"text": "We observe fairly high recall of the dependencies for vpart and vger (with the exception of RASP), and high recall for both dependencies representing control for five systems.", "labels": [], "entities": [{"text": "recall", "start_pos": 23, "end_pos": 29, "type": "METRIC", "confidence": 0.9987874627113342}, {"text": "recall", "start_pos": 108, "end_pos": 114, "type": "METRIC", "confidence": 0.9972484707832336}]}, {"text": "While Enju, Stanford, MST, and RASP all found between 70 and 85% of the dependency between the adjective and its complement in the tough construction, only Enju and XLE represented the dependency between the subject of the adjective and the gap inside the adjective's complement.", "labels": [], "entities": []}, {"text": "For the remaining phenomena, each parser performed markedly worse on one dependency type, compared to the other.", "labels": [], "entities": []}, {"text": "The only exceptions here are XLE and C&C's (and to a lesser extent, C&J's) scores for barerel.", "labels": [], "entities": [{"text": "XLE", "start_pos": 29, "end_pos": 32, "type": "METRIC", "confidence": 0.9543476700782776}]}, {"text": "No system scored higher than 33% on the harder of the two dependencies in rnror absol, and Stanford, MST, and RASP all scored below 25% on the harder dependency in barerel.", "labels": [], "entities": [{"text": "RASP", "start_pos": 110, "end_pos": 114, "type": "DATASET", "confidence": 0.7911760807037354}]}, {"text": "Only XLE scored higher than 10% on the second dependency for ned and higher than 50% for itexpl.", "labels": [], "entities": [{"text": "XLE", "start_pos": 5, "end_pos": 8, "type": "METRIC", "confidence": 0.9029436111450195}]}], "tableCaptions": [{"text": " Table 1: Relative frequencies of phenomena matches in  Wikipedia, and number of candidate strings vetted.", "labels": [], "entities": [{"text": "Relative", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.976285457611084}, {"text": "Wikipedia", "start_pos": 56, "end_pos": 65, "type": "DATASET", "confidence": 0.9479236006736755}]}, {"text": " Table 2: Sample annotations for sentence # 1011079100200: The-0 act-1 having-2 been-3 passed-4 in-5 that-6 year-7  Jessop-8 withdrew-9 and-10 Whitworth-11 carried-12 on-13 with-14 the-15 assistance-16 of-17 his-18 son-19.", "labels": [], "entities": []}, {"text": " Table 3: Dependencies labeled for each phenomenon type, including average and maximum surface distances.", "labels": [], "entities": []}]}