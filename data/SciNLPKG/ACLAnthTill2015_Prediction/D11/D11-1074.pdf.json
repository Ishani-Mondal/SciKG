{"title": [{"text": "Linking Entities to a Knowledge Base with Query Expansion", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper we present a novel approach to entity linking based on a statistical language model-based information retrieval with query expansion.", "labels": [], "entities": [{"text": "entity linking", "start_pos": 45, "end_pos": 59, "type": "TASK", "confidence": 0.7975945472717285}, {"text": "statistical language model-based information retrieval", "start_pos": 71, "end_pos": 125, "type": "TASK", "confidence": 0.5559604644775391}, {"text": "query expansion", "start_pos": 131, "end_pos": 146, "type": "TASK", "confidence": 0.6796339452266693}]}, {"text": "We use both local contexts and global world knowledge to expand query language models.", "labels": [], "entities": []}, {"text": "We place a strong emphasis on named entities in the local contexts and explore a positional language model to weigh them differently based on their distances to the query.", "labels": [], "entities": []}, {"text": "Our experiments on the TAC-KBP 2010 data show that incorporating such contextual information indeed aids in disambiguating the named entities and consistently improves the entity linking performance.", "labels": [], "entities": [{"text": "TAC-KBP 2010 data", "start_pos": 23, "end_pos": 40, "type": "DATASET", "confidence": 0.9355469346046448}]}, {"text": "Compared with the official results from KBP 2010 participants, our system shows competitive performance.", "labels": [], "entities": [{"text": "KBP 2010 participants", "start_pos": 40, "end_pos": 61, "type": "DATASET", "confidence": 0.8558289806048075}]}], "introductionContent": [{"text": "When people read news articles, Web pages and other documents online, they may encounter named entities which they are not familiar with and therefore would like to look them up in an encyclopedia.", "labels": [], "entities": []}, {"text": "It would be very useful if these entities could be automatically linked to their corresponding encyclopedic entries.", "labels": [], "entities": []}, {"text": "This task of linking mentions of entities within specific contexts to their corresponding entries in an existing knowledge base is called entity linking and has been proposed and studied in the Knowledge Base Population (KBP) track of the Text Analysis Conference (TAC).", "labels": [], "entities": [{"text": "entity linking", "start_pos": 138, "end_pos": 152, "type": "TASK", "confidence": 0.7285928875207901}, {"text": "Knowledge Base Population (KBP) track", "start_pos": 194, "end_pos": 231, "type": "DATASET", "confidence": 0.741544076374599}, {"text": "Text Analysis Conference (TAC)", "start_pos": 239, "end_pos": 269, "type": "TASK", "confidence": 0.6751118103663126}]}, {"text": "Besides improving an online surfer's browsing experience, entity linking also has potential usage in many other applications such as normalizing entity mentions for information extraction.", "labels": [], "entities": [{"text": "entity linking", "start_pos": 58, "end_pos": 72, "type": "TASK", "confidence": 0.7905887961387634}, {"text": "information extraction", "start_pos": 165, "end_pos": 187, "type": "TASK", "confidence": 0.745664581656456}]}, {"text": "The major challenge of entity linking is to resolve name ambiguities.", "labels": [], "entities": [{"text": "entity linking", "start_pos": 23, "end_pos": 37, "type": "TASK", "confidence": 0.7143000215291977}]}, {"text": "There are generally two types of ambiguities: (1) Polysemy: This type of ambiguities refers to the case when more than one entity shares the same name.", "labels": [], "entities": []}, {"text": "E.g. George Bush may refer to the 41st President of the U.S., the 43rd President of the U.S., or any other individual who has the same name.", "labels": [], "entities": []}, {"text": "Clearly polysemous names cause difficulties for entity linking.", "labels": [], "entities": [{"text": "entity linking", "start_pos": 48, "end_pos": 62, "type": "TASK", "confidence": 0.706193819642067}]}, {"text": "(2) Synonymy: This type of ambiguities refers to the case when more than one name variation refers to the same entity.", "labels": [], "entities": []}, {"text": "E.g. Metro-Goldwyn-Mayer Inc. is often abbreviated as MGM.", "labels": [], "entities": [{"text": "MGM", "start_pos": 54, "end_pos": 57, "type": "DATASET", "confidence": 0.9065954685211182}]}, {"text": "Synonymy affects entity linking when the entity mention in the document uses a name variation not covered in the entity's knowledge base entry.", "labels": [], "entities": [{"text": "entity linking", "start_pos": 17, "end_pos": 31, "type": "TASK", "confidence": 0.7495633363723755}]}, {"text": "Intuitively, to disambiguate a polysemous entity name, we should make use of the context in which the name occurs, and to address synonymy, external world knowledge is usually needed to expand acronyms or find other name variations.", "labels": [], "entities": []}, {"text": "Indeed both strategies have been explored in existing literature (.", "labels": [], "entities": []}, {"text": "However, most existing work uses supervised learning approaches that require careful feature engineering and a large amount of training data.", "labels": [], "entities": []}, {"text": "In this paper, we take a simpler unsupervised approach using statistical language model-based information retrieval.", "labels": [], "entities": [{"text": "statistical language model-based information retrieval", "start_pos": 61, "end_pos": 115, "type": "TASK", "confidence": 0.7103432655334473}]}, {"text": "We use the KL-divergence retrieval model ) and expand the query language models by considering both the local contexts within the query documents and global world knowledge obtained from the Web.", "labels": [], "entities": []}, {"text": "We evaluate our retrieval method with query expansion on the 2010 TAC-KBP data set.", "labels": [], "entities": [{"text": "2010 TAC-KBP data set", "start_pos": 61, "end_pos": 82, "type": "DATASET", "confidence": 0.8420514464378357}]}, {"text": "We find that our expanded query language models can indeed improve the performance significantly, demonstrating the effectiveness of our principled and yet simple techniques.", "labels": [], "entities": []}, {"text": "Comparison with the official results from KBP participants also shows that our system is competitive.", "labels": [], "entities": []}, {"text": "In particular, when no disambiguation text from the knowledge base is used, our system can achieve an overall 85.2% accuracy and 9.3% relative improvement over the best performance reported in KBP 2010.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 116, "end_pos": 124, "type": "METRIC", "confidence": 0.998847484588623}, {"text": "KBP 2010", "start_pos": 193, "end_pos": 201, "type": "DATASET", "confidence": 0.8910195827484131}]}], "datasetContent": [{"text": "Data Set: We evaluate our system on the TAC-KBP 2010 data set ().", "labels": [], "entities": [{"text": "TAC-KBP 2010 data set", "start_pos": 40, "end_pos": 61, "type": "DATASET", "confidence": 0.9651784300804138}]}, {"text": "The knowledge base was constructed from Wikipedia with 818,741 entries.", "labels": [], "entities": []}, {"text": "The data set contains 2250 queries and query documents come from news wire and Web pages.", "labels": [], "entities": []}, {"text": "Around 45% of the queries have non-Nil entries in the KB.", "labels": [], "entities": []}, {"text": "Some statistics of the queries are shown in.", "labels": [], "entities": []}, {"text": "Tools: In our experiments, to extract named entities within D Q and to determine T Q , we use the Stanford NER tagger . An example output of the NER tagger is shown below: <PERSON>Hugh Jackman<PERSON> is Jacked!!", "labels": [], "entities": [{"text": "Stanford NER tagger", "start_pos": 98, "end_pos": 117, "type": "DATASET", "confidence": 0.8505189021428426}]}, {"text": "This piece of text comes from a query document where the query name string is \"Jackman.\"", "labels": [], "entities": []}, {"text": "We can see that the NER tagger can help locate the full name of the person.", "labels": [], "entities": [{"text": "NER tagger", "start_pos": 20, "end_pos": 30, "type": "TASK", "confidence": 0.4761931151151657}]}, {"text": "We use the Lemur/Indri 2 search engine for retrieval.", "labels": [], "entities": [{"text": "Lemur/Indri 2 search engine", "start_pos": 11, "end_pos": 38, "type": "DATASET", "confidence": 0.8933673103650411}]}, {"text": "It implements the KL-divergence retrieval model as well as many other useful functionalities.", "labels": [], "entities": [{"text": "KL-divergence retrieval", "start_pos": 18, "end_pos": 41, "type": "TASK", "confidence": 0.6354834139347076}]}, {"text": "Evaluation Metric: We adopt the Micro-averaged accuracy metric, which is the mean accuracy overall queries.", "labels": [], "entities": [{"text": "Micro-averaged accuracy metric", "start_pos": 32, "end_pos": 62, "type": "METRIC", "confidence": 0.8289844592412313}, {"text": "accuracy", "start_pos": 82, "end_pos": 90, "type": "METRIC", "confidence": 0.7755332589149475}]}, {"text": "It was used in as the official metric to evaluate the performance of entity linking.", "labels": [], "entities": [{"text": "entity linking", "start_pos": 69, "end_pos": 83, "type": "TASK", "confidence": 0.7344072759151459}]}, {"text": "This metric is simply defined as the percentage of queries that have been correctly linked.", "labels": [], "entities": []}, {"text": "Methods to Compare: Recall that our system consists of a KB entry selection stage and a KB entry ranking stage.", "labels": [], "entities": []}, {"text": "At the selection stage, a set S Q of alternative name strings are used to select candidate KB entries.", "labels": [], "entities": []}, {"text": "We first define a few settings where different alternative name string sets are used to select candidate KB entries: \u2022 Q represents the baseline setting which uses only the original query name string N Q to select candidate KB entries.", "labels": [], "entities": []}, {"text": "\u2022 Q+L represents the setting where alternative name strings obtained from the query document D Q are combined with N Q to select candidate KB entries.", "labels": [], "entities": []}, {"text": "\u2022 Q+G represents the setting where the alternative name string obtained from Wikipedia is combined with N Q to select candidate KB entries.", "labels": [], "entities": []}, {"text": "\u2022 Q+L+G represents the setting as we described in Section 2.1, that is, alternative name strings from both D Q and Wikipedia are used together with N Q to select candidate KB entries.", "labels": [], "entities": []}, {"text": "After selecting candidate KB entries, in the KB entry ranking stage, we have four options for the query language model and two options for the KB entry language model.", "labels": [], "entities": []}, {"text": "For the query language model, we have (1) \u03b8 Q , the original query language model, (2) \u03b8 L Q , an expanded query language model using local context from D Q , (3) \u03b8 G Q , an expanded query language model using global world knowledge, and (4) \u03b8 L+G Q , an expanded query language model using both local context and global world knowledge.", "labels": [], "entities": []}, {"text": "For the KB entry language model, we can choose whether or not to use the KB disambiguation text DE and obtain \u03b8 NE and \u03b8 NE +D E , respectively.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Percentages of Nil and non-Nil queries.", "labels": [], "entities": []}, {"text": " Table 3: Comparing the effect of candidate entry selec- tion using different methods -KB entry selection stage  recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 113, "end_pos": 119, "type": "METRIC", "confidence": 0.9480639100074768}]}, {"text": " Table 4: Comparing the performance of using different sets of query name strings for candidate KB entry selection.  \u03b8 Q and \u03b8 NE are used in KB entry ranking.", "labels": [], "entities": [{"text": "KB entry selection", "start_pos": 96, "end_pos": 114, "type": "TASK", "confidence": 0.5811931788921356}]}, {"text": " Table 5: Comparison between the performance of \u03b8 Q and expanded query language models in terms of micro average  accuracy. \u03b8 NE was used in ranking.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 114, "end_pos": 122, "type": "METRIC", "confidence": 0.5034072995185852}]}, {"text": " Table 6: Comparing the performance using KB text and without using KB text for all methods using expanded query  models in terms of micro average accuracy on 2250 queries. \u03b8 NE +DE represents method using KB text and \u03b8 NE  represents methods without using KB text.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 147, "end_pos": 155, "type": "METRIC", "confidence": 0.753444254398346}]}, {"text": " Table 8: Comparison of the best configuration of our sys- tem (Q+L+G with \u03b8 L+G", "labels": [], "entities": []}]}