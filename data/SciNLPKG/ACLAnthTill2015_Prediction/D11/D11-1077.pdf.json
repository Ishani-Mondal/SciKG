{"title": [{"text": "Identification of Multi-word Expressions by Combining Multiple Linguistic Information Sources", "labels": [], "entities": [{"text": "Identification of Multi-word Expressions", "start_pos": 0, "end_pos": 40, "type": "TASK", "confidence": 0.8258565217256546}]}], "abstractContent": [{"text": "We propose an architecture for expressing various linguistically-motivated features that help identify multi-word expressions in natural language texts.", "labels": [], "entities": []}, {"text": "The architecture combines various linguistically-motivated classification features in a Bayesian Network.", "labels": [], "entities": []}, {"text": "We introduce novel ways for computing many of these features, and manually define linguistically-motivated interrelationships among them, which the Bayesian network models.", "labels": [], "entities": []}, {"text": "Our methodology is almost entirely unsupervised and completely language-independent; it relies on few language resources and is thus suitable fora large number of languages.", "labels": [], "entities": []}, {"text": "Furthermore, unlike much recent work, our approach can identify expressions of various types and syntactic constructions.", "labels": [], "entities": []}, {"text": "We demonstrate a significant improvement in identification accuracy, compared with less sophisticated baselines.", "labels": [], "entities": [{"text": "identification", "start_pos": 44, "end_pos": 58, "type": "TASK", "confidence": 0.9191277623176575}, {"text": "accuracy", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.9788475632667542}]}], "introductionContent": [{"text": "Multi-word Expressions (MWEs) are lexical items that consist of multiple orthographic words (e.g., ad hoc, by and large, New York, kick the bucket).", "labels": [], "entities": [{"text": "Multi-word Expressions (MWEs)", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.7746953904628754}]}, {"text": "MWEs are numerous and constitute a significant portion of the lexicon of any natural language).", "labels": [], "entities": []}, {"text": "They area heterogeneous class of constructions with diverse sets of characteristics, distinguished by their idiosyncratic behavior.", "labels": [], "entities": []}, {"text": "Morphologically, some MWEs allow some of their constituents to freely inflect while restricting (or preventing) the inflection of other constituents.", "labels": [], "entities": []}, {"text": "In some cases MWEs may allow constituents to undergo non-standard morphological inflections that they would not undergo in isolation.", "labels": [], "entities": [{"text": "MWEs", "start_pos": 14, "end_pos": 18, "type": "TASK", "confidence": 0.96401047706604}]}, {"text": "Syntactically, some MWEs behave like words while other are phrases; some occur in one rigid pattern (and a fixed order), while others permit various syntactic transformations.", "labels": [], "entities": []}, {"text": "Semantically, the compositionality of MWEs is gradual, ranging from fully compositional to idiomatic ().", "labels": [], "entities": []}, {"text": "Because of their prevalence and irregularity, MWEs must be stored in lexicons of natural language processing applications.", "labels": [], "entities": []}, {"text": "Correct handling of MWEs has been proven beneficial for various applications, including information retrieval, building ontologies, text alignment, and machine translation.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 88, "end_pos": 109, "type": "TASK", "confidence": 0.8368920683860779}, {"text": "text alignment", "start_pos": 132, "end_pos": 146, "type": "TASK", "confidence": 0.8104754984378815}, {"text": "machine translation", "start_pos": 152, "end_pos": 171, "type": "TASK", "confidence": 0.8143268525600433}]}, {"text": "We propose a novel architecture for identifying MWEs of various types and syntactic categories in monolingual corpora.", "labels": [], "entities": []}, {"text": "Unlike much existing work, which focuses on a particular syntactic construction, our approach addresses MWEs of all types by focusing on the general idiosyncratic properties of MWEs rather than on specific properties of each sub-class thereof.", "labels": [], "entities": []}, {"text": "While we only evaluate our methodology on bi-grams, it can in principle be extended to longer MWEs.", "labels": [], "entities": []}, {"text": "The architecture uses Bayesian Networks (BN) to express multiple interdependent linguistically-motivated features.", "labels": [], "entities": []}, {"text": "First, we automatically generate a small (training) set of MWE and non-MWE bi-grams (positive and negative instances, respectively).", "labels": [], "entities": []}, {"text": "We then define a set of linguistically-motivated features that embody observed characteristics of MWEs.", "labels": [], "entities": []}, {"text": "We augment these by features that reflect collocation measures.", "labels": [], "entities": []}, {"text": "Finally, we define dependencies among these features, expressed in the structure of a Bayesian Network model, which we then use for classification.", "labels": [], "entities": []}, {"text": "This is a directed graph, whose nodes express the features used for classification, and whose edges de-836 fine causal relationships among these features.", "labels": [], "entities": []}, {"text": "In this architecture, learning does not result in a black box, expressed solely as feature weights.", "labels": [], "entities": []}, {"text": "Rather, the structure of the BN allows us to learn the impact of different MWE features on the classification.", "labels": [], "entities": [{"text": "BN", "start_pos": 29, "end_pos": 31, "type": "DATASET", "confidence": 0.7638570666313171}]}, {"text": "The result is anew unsupervised method for identifying MWEs of various types in text corpora.", "labels": [], "entities": [{"text": "identifying MWEs", "start_pos": 43, "end_pos": 59, "type": "TASK", "confidence": 0.6670792400836945}]}, {"text": "It combines statistics with a large array of linguisticallymotivated features, organized in an architecture that reflects interdependencies among the features.", "labels": [], "entities": []}, {"text": "The contribution of this work is manifold.", "labels": [], "entities": []}, {"text": "First, we show how to generate training material (almost) automatically, so the method is almost completely unsupervised.", "labels": [], "entities": []}, {"text": "The methodology we advocate is thus language-independent, requiring relatively few language resources, and is therefore optimal for medium-density languages ().", "labels": [], "entities": []}, {"text": "Second, we propose several linguisticallymotivated features that can be computed from data and that are demonstrably productive for improving the accuracy of MWE identification.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 146, "end_pos": 154, "type": "METRIC", "confidence": 0.9981502294540405}, {"text": "MWE identification", "start_pos": 158, "end_pos": 176, "type": "TASK", "confidence": 0.9832570850849152}]}, {"text": "These feature focus on the expression of linguistic idiosyncrasies of various types, a phenomenon typical of MWEs.", "labels": [], "entities": []}, {"text": "We propose novel computational modeling of many of these features; in particular, we account for the morphological idiosyncrasy of MWEs using a histogram of the number of inflected forms, in a technique that draws from image processing.", "labels": [], "entities": []}, {"text": "Third, we advocate the use of Bayesian Networks as a mechanism for expressing manually-crafted dependencies among features; the use of BN significantly improves the classification accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 180, "end_pos": 188, "type": "METRIC", "confidence": 0.9087200164794922}]}, {"text": "Finally, we demonstrate the utility of our methodology by applying it to Hebrew.", "labels": [], "entities": []}, {"text": "1 Our evaluation shows that the use of linguistically-motivated features results in reduction of 23% of the errors compared with a collocation baseline; organizing the knowledge in a BN reduces the error rate by additional 8.7%.", "labels": [], "entities": [{"text": "error rate", "start_pos": 198, "end_pos": 208, "type": "METRIC", "confidence": 0.967939704656601}]}, {"text": "After discussing related work in the next section, we describe in Section 3 the methodology we propose, including a detailed discussion of the features and their implementation.", "labels": [], "entities": []}, {"text": "Section 4 provides a thorough evaluation of the results.", "labels": [], "entities": []}, {"text": "We conclude with suggestions for future research.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Statistics of the sample space from which the  training set is generated", "labels": [], "entities": []}, {"text": " Table 3. We  randomly select from the sample space this many in- stances for each class. Since much of the procedure  of preparing training data is automatic, the results  may be somewhat noisy. As Bayesian Network are  known to be robust to noisy data, we expect the BN  to compensate for this problem.", "labels": [], "entities": [{"text": "BN", "start_pos": 269, "end_pos": 271, "type": "METRIC", "confidence": 0.8484402894973755}]}, {"text": " Table 3: Sizes of each training set", "labels": [], "entities": [{"text": "Sizes", "start_pos": 10, "end_pos": 15, "type": "TASK", "confidence": 0.9817750453948975}]}, {"text": " Table 4: 10-fold cross validation evaluation results", "labels": [], "entities": []}, {"text": " Table 5: Evaluation results: noun-noun constructions", "labels": [], "entities": []}]}