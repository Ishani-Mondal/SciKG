{"title": [{"text": "The Imagination of Crowds: Conversational AAC Language Modeling using Crowdsourcing and Large Data Sources", "labels": [], "entities": [{"text": "AAC Language Modeling", "start_pos": 42, "end_pos": 63, "type": "TASK", "confidence": 0.7644701798756918}]}], "abstractContent": [{"text": "Augmented and alternative communication (AAC) devices enable users with certain communication disabilities to participate in everyday conversations.", "labels": [], "entities": [{"text": "Augmented and alternative communication (AAC)", "start_pos": 0, "end_pos": 45, "type": "TASK", "confidence": 0.608104122536523}]}, {"text": "Such devices often rely on statistical language models to improve text entry by offering word predictions.", "labels": [], "entities": [{"text": "text entry", "start_pos": 66, "end_pos": 76, "type": "TASK", "confidence": 0.6903757005929947}]}, {"text": "These predictions can be improved if the language model is trained on data that closely reflects the style of the users' intended communications.", "labels": [], "entities": []}, {"text": "Unfortunately, there is no large dataset consisting of genuine AAC messages.", "labels": [], "entities": [{"text": "AAC messages", "start_pos": 63, "end_pos": 75, "type": "TASK", "confidence": 0.8289397954940796}]}, {"text": "In this paper we demonstrate how we can crowd-source the creation of a large set of fictional AAC messages.", "labels": [], "entities": [{"text": "AAC messages", "start_pos": 94, "end_pos": 106, "type": "TASK", "confidence": 0.8673683106899261}]}, {"text": "We show that these messages model conversational AAC better than the currently used datasets based on telephone conversations or newswire text.", "labels": [], "entities": [{"text": "AAC", "start_pos": 49, "end_pos": 52, "type": "TASK", "confidence": 0.6399316787719727}]}, {"text": "We leverage our crowdsourced messages to intelligently select sentences from much larger sets of Twitter, blog and Usenet data.", "labels": [], "entities": [{"text": "Usenet data", "start_pos": 115, "end_pos": 126, "type": "DATASET", "confidence": 0.8645610809326172}]}, {"text": "Compared to a model trained only on telephone transcripts, our best performing model reduced perplexity on three test sets of AAC-like communications by 60-82% relative.", "labels": [], "entities": []}, {"text": "This translated to a potential keystroke savings in a predictive keyboard interface of 5-11%.", "labels": [], "entities": []}], "introductionContent": [{"text": "Users with certain communication disabilities rely on augmented and alternative communication (AAC) devices to take part in everyday conversations.", "labels": [], "entities": []}, {"text": "Often these devices consist of a predictive text input method coupled with text-to-speech output.", "labels": [], "entities": []}, {"text": "Unfortunately, the text entry rates provided by AAC devices are typically low, between 0.5 and 16 words-per-minute (.", "labels": [], "entities": [{"text": "text entry", "start_pos": 19, "end_pos": 29, "type": "TASK", "confidence": 0.6599751561880112}]}, {"text": "As a consequence, researchers have made numerous efforts to increase AAC text entry rates by employing a variety of improved language modeling techniques.", "labels": [], "entities": [{"text": "AAC text entry", "start_pos": 69, "end_pos": 83, "type": "TASK", "confidence": 0.9014519254366556}]}, {"text": "Examples of approaches include adapting the language model to recently used words (, using syntactic information), using semantic information (, and modeling topics ().", "labels": [], "entities": []}, {"text": "For a recent survey, see Garay-Vitoria and.", "labels": [], "entities": [{"text": "Garay-Vitoria", "start_pos": 25, "end_pos": 38, "type": "METRIC", "confidence": 0.5386621952056885}]}, {"text": "While such language model improvement techniques are undoubtedly helpful, certainly they can all benefit from starting with a long-span language model trained on large amounts of closely matched data.", "labels": [], "entities": []}, {"text": "For AAC devices this means closely modeling everyday face-to-face communications.", "labels": [], "entities": []}, {"text": "However, a long-standing problem in the field is the lack of good data sources that adequately model such AAC communications.", "labels": [], "entities": [{"text": "AAC communications", "start_pos": 106, "end_pos": 124, "type": "TASK", "confidence": 0.9490550756454468}]}, {"text": "Due to privacy-reasons and other ethical concerns, there is no large dataset consisting of genuine AAC messages.", "labels": [], "entities": [{"text": "AAC messages", "start_pos": 99, "end_pos": 111, "type": "TASK", "confidence": 0.8280970752239227}]}, {"text": "Therefore, previous research has used transcripts of telephone conversations or newswire text.", "labels": [], "entities": []}, {"text": "However, these data sources are unlikely to bean ideal basis for AAC language models.", "labels": [], "entities": [{"text": "AAC language", "start_pos": 65, "end_pos": 77, "type": "TASK", "confidence": 0.8846802413463593}]}, {"text": "In this paper we show that it is possible to significantly improve conversational AAC language modeling by first crowdsourcing the creation of a fictional collection of AAC messages on the Amazon Mechanical Turk microtask market.", "labels": [], "entities": [{"text": "AAC language modeling", "start_pos": 82, "end_pos": 103, "type": "TASK", "confidence": 0.8944881558418274}, {"text": "Amazon Mechanical Turk microtask market", "start_pos": 189, "end_pos": 228, "type": "DATASET", "confidence": 0.7637085676193237}]}, {"text": "Using a care-700 fully designed microtask we collected 5890 messages from 298 unique workers.", "labels": [], "entities": []}, {"text": "As we will see, word-for-word these fictional AAC messages are better at predicting AAC test sets than a wide-range of other text sources.", "labels": [], "entities": [{"text": "AAC messages", "start_pos": 46, "end_pos": 58, "type": "TASK", "confidence": 0.813117116689682}, {"text": "predicting AAC", "start_pos": 73, "end_pos": 87, "type": "TASK", "confidence": 0.8001534044742584}]}, {"text": "Further, we demonstrate that Twitter, blog and Usenet data outperform telephone transcripts or newswire text.", "labels": [], "entities": []}, {"text": "While our crowdsourced AAC data is better than other text sources, it is too small to train high-quality long-span language models.", "labels": [], "entities": [{"text": "AAC", "start_pos": 23, "end_pos": 26, "type": "TASK", "confidence": 0.7364891171455383}]}, {"text": "We therefore investigate how to use our crowdsourced collection to intelligently select AAC-like sentences from Twitter, blog and Usenet data.", "labels": [], "entities": [{"text": "Usenet data", "start_pos": 130, "end_pos": 141, "type": "DATASET", "confidence": 0.8021421134471893}]}, {"text": "We compare a variety of different techniques for doing this intelligent selection.", "labels": [], "entities": []}, {"text": "We find that the best selection technique is the recently proposed cross-entropy difference method.", "labels": [], "entities": []}, {"text": "Using this method, we build a compact and well-performing mixture model from the Twitter, blog and Usenet sentences most similar to our crowdsourced data.", "labels": [], "entities": []}, {"text": "We evaluate our mixture model on four different test sets.", "labels": [], "entities": []}, {"text": "On the three most AAC-like test sets, we found substantial reductions in not only perplexity but also in potential keystroke savings when used in a predictive keyboard interface.", "labels": [], "entities": [{"text": "AAC-like test sets", "start_pos": 18, "end_pos": 36, "type": "DATASET", "confidence": 0.8314948678016663}]}, {"text": "Finally, to aid other AAC researchers, we have publicly released our crowdsourced AAC collection, word lists and best-performing language models 1 .", "labels": [], "entities": []}], "datasetContent": [{"text": "We trained language models on each dataset, varying the number of training words from 4K to 24K (the limit of the TURKTRAIN set).", "labels": [], "entities": [{"text": "TURKTRAIN", "start_pos": 114, "end_pos": 123, "type": "METRIC", "confidence": 0.9235345125198364}]}, {"text": "For each dataset and training amount, we built 20 different models by choosing sentences from the full training set at random.", "labels": [], "entities": []}, {"text": "We computed the mean and standard deviation of the per-word perplexity of the set of 20 models.", "labels": [], "entities": [{"text": "standard deviation", "start_pos": 25, "end_pos": 43, "type": "METRIC", "confidence": 0.9283988773822784}]}, {"text": "As shown in, word-for-word the TURK-TRAIN data was superior for our three most AAClike test sets.", "labels": [], "entities": [{"text": "TURK-TRAIN", "start_pos": 31, "end_pos": 41, "type": "METRIC", "confidence": 0.9854546785354614}, {"text": "AAClike test sets", "start_pos": 79, "end_pos": 96, "type": "DATASET", "confidence": 0.7457811335722605}]}, {"text": "Thus it appears our crowdsourcing procedure was successful at generating AAC-like data.", "labels": [], "entities": []}, {"text": "TWITTER was consistently the second best.", "labels": [], "entities": [{"text": "TWITTER", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.6891907453536987}]}, {"text": "BLOG, USENET and SWITCHBOARD also performed well.", "labels": [], "entities": [{"text": "BLOG", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.7356926798820496}, {"text": "USENET", "start_pos": 6, "end_pos": 12, "type": "DATASET", "confidence": 0.5532422065734863}, {"text": "SWITCHBOARD", "start_pos": 17, "end_pos": 28, "type": "DATASET", "confidence": 0.628254771232605}]}, {"text": "The previous experiment used a small amount of training data.", "labels": [], "entities": []}, {"text": "We selected the best three datasets having tens of millions of words of training data: USENET, BLOG, and TWITTER.", "labels": [], "entities": [{"text": "USENET", "start_pos": 87, "end_pos": 93, "type": "METRIC", "confidence": 0.5182432532310486}, {"text": "BLOG", "start_pos": 95, "end_pos": 99, "type": "METRIC", "confidence": 0.9798864722251892}]}, {"text": "As in the previous experiment, we computed the mean and standard deviation of the per-word perplexity of a set of 20 models.", "labels": [], "entities": [{"text": "standard deviation", "start_pos": 56, "end_pos": 74, "type": "METRIC", "confidence": 0.9440100789070129}]}, {"text": "Increasing the amount of training data substantially reduced perplexity compared to our small TURKTRAIN collection (figure 4).", "labels": [], "entities": [{"text": "perplexity", "start_pos": 61, "end_pos": 71, "type": "METRIC", "confidence": 0.9764043688774109}, {"text": "TURKTRAIN collection", "start_pos": 94, "end_pos": 114, "type": "DATASET", "confidence": 0.752714991569519}]}, {"text": "Tweets were clearly well suited for modeling AAC-like text as 3M words of TWITTER data was better than 40M words of BLOG data.", "labels": [], "entities": [{"text": "BLOG data", "start_pos": 116, "end_pos": 125, "type": "DATASET", "confidence": 0.8191406726837158}]}, {"text": "In the previous section, we found our crowdsourced data was good at predicting AAC-like test sets.", "labels": [], "entities": [{"text": "predicting AAC-like test", "start_pos": 68, "end_pos": 92, "type": "TASK", "confidence": 0.630849023660024}]}, {"text": "However, in order to build a good long-span language model, we would require millions of such communications.", "labels": [], "entities": []}, {"text": "Crowdsourcing such a large collection would be prohibitively expensive.", "labels": [], "entities": []}, {"text": "Therefore, we instead investigated how to use our crowdsourced data to intelligently select AAC-like data from other large datasets.", "labels": [], "entities": []}, {"text": "For large datasets, we used TWITTER, BLOG and USENET as they were both large and well-matched to AAC data.", "labels": [], "entities": [{"text": "TWITTER", "start_pos": 28, "end_pos": 35, "type": "METRIC", "confidence": 0.5802823305130005}, {"text": "BLOG", "start_pos": 37, "end_pos": 41, "type": "METRIC", "confidence": 0.9722303748130798}, {"text": "USENET", "start_pos": 46, "end_pos": 52, "type": "METRIC", "confidence": 0.6037050485610962}, {"text": "AAC data", "start_pos": 97, "end_pos": 105, "type": "DATASET", "confidence": 0.7970177531242371}]}, {"text": "In this section, we compare our mixture model against baseline models.", "labels": [], "entities": []}, {"text": "We show performance with respect to usage in atypical AAC text entry interface based on word prediction.", "labels": [], "entities": [{"text": "AAC text entry interface", "start_pos": 54, "end_pos": 78, "type": "TASK", "confidence": 0.78799869120121}, {"text": "word prediction", "start_pos": 88, "end_pos": 103, "type": "TASK", "confidence": 0.7298548221588135}]}, {"text": "We compared our mixture model using crossentropy difference selection with three baseline models trained on all of TWITTER, SWITCHBOARD and TURKTRAIN.", "labels": [], "entities": [{"text": "TWITTER", "start_pos": 115, "end_pos": 122, "type": "METRIC", "confidence": 0.5375354290008545}]}, {"text": "The baseline models were unpruned 4-gram models trained using interpolated modified Kneser-Ney smoothing.", "labels": [], "entities": []}, {"text": "They had 72M, 5M, and 129K parameters respectively.", "labels": [], "entities": [{"text": "72M", "start_pos": 9, "end_pos": 12, "type": "METRIC", "confidence": 0.976558268070221}]}, {"text": "As shown in  mixture model provided substantial increases in keystroke savings compared to a model trained solely on Switchboard.", "labels": [], "entities": []}, {"text": "The mixture model also performed better than simply training a model on a large amount of Twitter data.", "labels": [], "entities": []}, {"text": "The model trained on only 24K words of Turk data did surprisingly well given its extremely limited training data.", "labels": [], "entities": [{"text": "Turk data", "start_pos": 39, "end_pos": 48, "type": "DATASET", "confidence": 0.8360649347305298}]}, {"text": "Our Switchboard model performed the best on SWITCHTEST with a keystroke savings of 58.8%.", "labels": [], "entities": [{"text": "keystroke", "start_pos": 62, "end_pos": 71, "type": "METRIC", "confidence": 0.9709545969963074}]}, {"text": "For comparison, past work reported a keystroke savings of 55.7% on SWITCHTEST using a 3-gram model trained on Switchboard (.", "labels": [], "entities": []}, {"text": "While our mixture model performed less well on SWITCHTEST (52.8%), it is likely the other three test sets better represent AAC communications.", "labels": [], "entities": [{"text": "SWITCHTEST", "start_pos": 47, "end_pos": 57, "type": "METRIC", "confidence": 0.4422092139720917}, {"text": "AAC communications", "start_pos": 123, "end_pos": 141, "type": "TASK", "confidence": 0.8602556586265564}]}, {"text": "Our mixture language model used the best thresholds with respect to TURKDEV.", "labels": [], "entities": [{"text": "TURKDEV", "start_pos": 68, "end_pos": 75, "type": "METRIC", "confidence": 0.8934177756309509}]}, {"text": "This resulted in throwing away most of the training data.", "labels": [], "entities": []}, {"text": "This might be suboptimal in practice if an AAC user's communications are somewhat different or more diverse than the language generated by the Turk workers.", "labels": [], "entities": []}, {"text": "We trained a series of mixture models in which we varied the cross-entropy difference thresholds  by adding a constant to all three thresholds.", "labels": [], "entities": []}, {"text": "The mixture weights for each new model were optimized with respect to TURKDEV.", "labels": [], "entities": [{"text": "TURKDEV", "start_pos": 70, "end_pos": 77, "type": "METRIC", "confidence": 0.9291408658027649}]}, {"text": "Using somewhat larger models did improve keystroke savings for all test sets except for TURKTEST ().", "labels": [], "entities": [{"text": "TURKTEST", "start_pos": 88, "end_pos": 96, "type": "METRIC", "confidence": 0.9165641665458679}]}, {"text": "However, using too large thresholds eventually hurt performance except on SWITCHTEST.", "labels": [], "entities": []}, {"text": "Performance on SWITCHTEST steadily increased from 52.8% to 56.6%.", "labels": [], "entities": [{"text": "SWITCHTEST", "start_pos": 15, "end_pos": 25, "type": "TASK", "confidence": 0.6521999835968018}]}, {"text": "These gains however came at the cost of bigger models.", "labels": [], "entities": []}, {"text": "The model using +1.0 of the optimal thresholds had 384M parameters and a compressed size of 3.0 GB.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: Perplexity (PPL) and keystroke savings  (KS) of different language models on four test sets.  The bold line shows the best performing language  model on each test set.", "labels": [], "entities": [{"text": "keystroke savings  (KS)", "start_pos": 31, "end_pos": 54, "type": "METRIC", "confidence": 0.9352978229522705}]}]}