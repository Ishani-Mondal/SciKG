{"title": [], "abstractContent": [{"text": "Real document collections do not fit the independence assumptions asserted by most statistical topic models, but how badly do they violate them?", "labels": [], "entities": []}, {"text": "We present a Bayesian method for measuring how well a topic model fits a corpus.", "labels": [], "entities": []}, {"text": "Our approach is based on posterior predictive checking, a method for diagnosing Bayesian models in user-defined ways.", "labels": [], "entities": [{"text": "posterior predictive checking", "start_pos": 25, "end_pos": 54, "type": "TASK", "confidence": 0.6801608204841614}]}, {"text": "Our method can identify where a topic model fits the data, where it falls short, and in which directions it might be improved.", "labels": [], "entities": []}], "introductionContent": [{"text": "Probabilistic topic models area suite of machine learning algorithms that decompose a corpus into a set of topics and represent each document with a subset of those topics.", "labels": [], "entities": []}, {"text": "The inferred topics often correspond with the underlying themes of the analyzed collection, and the topic modeling algorithm organizes the documents according to those themes.", "labels": [], "entities": []}, {"text": "Most topic models are evaluated by their predictive performance on held out data.", "labels": [], "entities": []}, {"text": "The idea is that topic models are fit to maximize the likelihood (or posterior probability) of a collection of documents, and so a good model is one that assigns high likelihood to a held outset (.", "labels": [], "entities": [{"text": "posterior probability)", "start_pos": 69, "end_pos": 91, "type": "METRIC", "confidence": 0.8684591054916382}]}, {"text": "But this evaluation is not inline with how topic models are frequently used.", "labels": [], "entities": []}, {"text": "Topic models seem to capture the underlying themes of a collection-indeed the monicker \"topic model\" is retrospective-and so we expect that these themes are useful for exploring, summarizing, and learning about its documents ().", "labels": [], "entities": [{"text": "summarizing, and learning about its documents", "start_pos": 179, "end_pos": 224, "type": "TASK", "confidence": 0.634186578648431}]}, {"text": "In such exploratory data analysis, however, we are not concerned with the fit to held out data.", "labels": [], "entities": []}, {"text": "In this paper, we develop and study new methods for evaluating topic models.", "labels": [], "entities": []}, {"text": "Our methods are based on posterior predictive checking, which is a model diagnosis technique from Bayesian statistics.", "labels": [], "entities": [{"text": "posterior predictive checking", "start_pos": 25, "end_pos": 54, "type": "TASK", "confidence": 0.6716518004735311}]}, {"text": "The goal of a posterior predictive check (PPC) is to assess the validity of a Bayesian model without requiring a specific alternative model.", "labels": [], "entities": [{"text": "posterior predictive check (PPC)", "start_pos": 14, "end_pos": 46, "type": "TASK", "confidence": 0.5848236829042435}, {"text": "validity", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.9550854563713074}]}, {"text": "Given data, we first compute a posterior distribution over the latent variables.", "labels": [], "entities": []}, {"text": "Then, we estimate the probability of the observed data under the data-generating distribution that is induced by the posterior (the \"posterior predictive distribution\").", "labels": [], "entities": []}, {"text": "A data set that is unlikely calls the model into question, and consequently the posterior.", "labels": [], "entities": []}, {"text": "PPCs can show where the model fits and doesn't fit the observations.", "labels": [], "entities": []}, {"text": "They can help identify the parts of the posterior that are worth exploring.", "labels": [], "entities": []}, {"text": "The key to a posterior predictive check is the discrepancy function.", "labels": [], "entities": []}, {"text": "This is a function of the data that measures a property of the model which is important to capture.", "labels": [], "entities": []}, {"text": "While the model is often chosen for computational reasons, the discrepancy function might capture aspects of the data that are desirable but difficult to model.", "labels": [], "entities": []}, {"text": "In this work, we will design a discrepancy function to measure an independence assumption that is implicit in the modeling assumptions but is not enforced in the posterior.", "labels": [], "entities": []}, {"text": "We will embed this function in a posterior predictive check and use it to evaluate and visualize topic models in new ways.", "labels": [], "entities": []}, {"text": "227 Specifically, we develop discrepancy functions for latent Dirichlet allocation (the simplest topic model) that measure how well its statistical assumptions about the topics are matched in the observed corpus and inferred topics.", "labels": [], "entities": [{"text": "latent Dirichlet allocation", "start_pos": 55, "end_pos": 82, "type": "TASK", "confidence": 0.5168096125125885}]}, {"text": "LDA assumes that each observed word in a corpus is assigned to a topic, and that the words assigned to the same topic are drawn independently from the same multinomial distribution (.", "labels": [], "entities": []}, {"text": "For each topic, we measure the whether this assumption holds by computing the mutual information between the words assigned to that topic and which document each word appeared in.", "labels": [], "entities": []}, {"text": "If the assumptions hold, these two variables should be independent: low mutual information indicates that the assumptions hold; high mutual information indicates a mismatch to the modeling assumptions.", "labels": [], "entities": []}, {"text": "We embed this discrepancy in a PPC and study it in several ways.", "labels": [], "entities": []}, {"text": "First, we focus on topics that model their observations well; this helps separate interpretable topics from noisy topics (and \"boilerplate\" topics, which exhibit too little noise).", "labels": [], "entities": []}, {"text": "Second, we focus on individual terms within topics; this helps display a model applied to a corpus, and understand which terms are modeled well.", "labels": [], "entities": []}, {"text": "Third, we replace the document identity with an external variable that might plausibly be incorporated into the model (such as timestamp or author).", "labels": [], "entities": []}, {"text": "This helps point the modeler towards the most promising among more complicated models, or save the effort in fitting one.", "labels": [], "entities": []}, {"text": "Finally, we validate this strategy by simulating data from a topic model, and assessing whether the PPC \"accepts\" the resulting data.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}