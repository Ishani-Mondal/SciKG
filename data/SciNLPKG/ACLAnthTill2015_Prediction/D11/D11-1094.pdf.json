{"title": [{"text": "Latent Vector Weighting for Word Meaning in Context", "labels": [], "entities": [{"text": "Word Meaning in Context", "start_pos": 28, "end_pos": 51, "type": "TASK", "confidence": 0.7973290085792542}]}], "abstractContent": [{"text": "This paper presents a novel method for the computation of word meaning in context.", "labels": [], "entities": [{"text": "computation of word meaning in context", "start_pos": 43, "end_pos": 81, "type": "TASK", "confidence": 0.830926368633906}]}, {"text": "We make use of a factorization model in which words, together with their window-based context words and their dependency relations, are linked to latent dimensions.", "labels": [], "entities": []}, {"text": "The factorization model allows us to determine which dimensions are important fora particular context, and adapt the dependency-based feature vector of the word accordingly.", "labels": [], "entities": []}, {"text": "The evaluation on a lexical substitution task-carried out for both English and French-indicates that our approach is able to reach better results than state-of-the-art methods in lexical substitution, while at the same time providing more accurate meaning representations .", "labels": [], "entities": [{"text": "lexical substitution", "start_pos": 179, "end_pos": 199, "type": "TASK", "confidence": 0.7412093877792358}]}], "introductionContent": [{"text": "According to the distributional hypothesis of meaning, words that occur in similar contexts tend to be semantically similar.", "labels": [], "entities": []}, {"text": "In the spirit of this by now well-known adage, numerous algorithms have sprouted up that try to capture the semantics of words by looking at their distribution in texts, and comparing those distributions in a vector space model.", "labels": [], "entities": []}, {"text": "Up till now, the majority of computational approaches to semantic similarity represent the meaning of a word as the aggregate of the word's contexts, and hence do not differentiate between the different senses of a word.", "labels": [], "entities": []}, {"text": "The meaning of a word, however, is largely dependent on the particular context in which it appears.", "labels": [], "entities": []}, {"text": "Take for example the word work in sentences (1) and (2).", "labels": [], "entities": []}, {"text": "(1) The painter's recent work is a classic example of art brut.", "labels": [], "entities": []}, {"text": "(2) Equal pay for equal work!", "labels": [], "entities": [{"text": "Equal pay", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.8861405551433563}]}, {"text": "The meaning of work is quite different in both sentences.", "labels": [], "entities": []}, {"text": "In sentence (1), work refers to the product of a creative act, viz.", "labels": [], "entities": []}, {"text": "In sentence (2), it refers to labour carried out as a source of income.", "labels": [], "entities": []}, {"text": "The NLP community's standard answer to the ambiguity problem has always been some flavour of word sense disambiguation (WSD), which in its standard form boils down to choosing the best-possible fit from a pre-defined sense inventory.", "labels": [], "entities": [{"text": "word sense disambiguation (WSD)", "start_pos": 93, "end_pos": 124, "type": "TASK", "confidence": 0.7187022467454275}]}, {"text": "In recent years, it has become clear that this is in fact a very hard task to solve for computers and humans alike.", "labels": [], "entities": []}, {"text": "With these findings in mind, researchers have started looking at different methods to tackle language's ambiguity, ranging from coarser-grained sense inventories () and graded sense assignment , over word sense induction), to the computation of individual word meaning in context.", "labels": [], "entities": [{"text": "graded sense assignment", "start_pos": 169, "end_pos": 192, "type": "TASK", "confidence": 0.6139915287494659}]}, {"text": "This research inscribes itself in the same line of thought, in which the meaning disambiguation of a word is not just the assignment of a pre-defined sense; instead, the original meaning representation of a word is adapted 'on the fly', according to -and specifically tailored for -the particular context in which it appears.", "labels": [], "entities": []}, {"text": "To be able to do so, we build a factorization model in which words, together with their window-based context words and their dependency relations, are linked to latent dimensions.", "labels": [], "entities": []}, {"text": "The factorization model allows us to determine which dimensions are important fora particular context, and adapt the dependency-based feature vector of the word accordingly.", "labels": [], "entities": []}, {"text": "The evaluation on a lexical substitution task -carried out for both English and French -indicates that our method is able to reach better results than state-of-the-art methods in lexical substitution, while at the same time providing more accurate meaning representations.", "labels": [], "entities": [{"text": "lexical substitution", "start_pos": 179, "end_pos": 199, "type": "TASK", "confidence": 0.7449119091033936}]}, {"text": "The remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "In section 2, we present some earlier work that is related to the research presented here.", "labels": [], "entities": []}, {"text": "Section 3 describes the methodology of our method, focusing on the factorization model, and the computation of meaning in context.", "labels": [], "entities": []}, {"text": "Section 4 presents a thorough evaluation on a lexical substitution task, both for English and French.", "labels": [], "entities": [{"text": "lexical substitution task", "start_pos": 46, "end_pos": 71, "type": "TASK", "confidence": 0.7606970469156901}]}, {"text": "The last section then draws conclusions, and presents a number of topics that deserve further exploration.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we present a thorough evaluation of the method described above, and compare it with related methods for meaning computation in context.", "labels": [], "entities": [{"text": "meaning computation", "start_pos": 121, "end_pos": 140, "type": "TASK", "confidence": 0.815870612859726}]}, {"text": "In order to test the applicability of the method to multiple languages, we present evaluation results for both English and French.", "labels": [], "entities": []}, {"text": "For English, we make use of the SEMEVAL 2007 English Lexical Substitution task).", "labels": [], "entities": [{"text": "SEMEVAL 2007 English Lexical Substitution task", "start_pos": 32, "end_pos": 78, "type": "TASK", "confidence": 0.6172989706198374}]}, {"text": "The task's goal is to find suitable substitutes fora target word in a particular context.", "labels": [], "entities": []}, {"text": "The complete data set contains 200 target words (about 50 for each part of speech, viz. nouns, verbs, adjectives, and adverbs).", "labels": [], "entities": []}, {"text": "Each target word occurs in 10 different sentences, which yields a total of 2000 sentences.", "labels": [], "entities": []}, {"text": "Five annotators provided suitable substitutes for each target word in the different contexts.", "labels": [], "entities": []}, {"text": "For French, we developed a small-scale lexical substitution task ourselves, closely following the guidelines of the original English task.", "labels": [], "entities": []}, {"text": "We manually selected 10 ambiguous French nouns, and for each noun we selected 10 different sentences from the FRWaC corpus (.", "labels": [], "entities": [{"text": "FRWaC corpus", "start_pos": 110, "end_pos": 122, "type": "DATASET", "confidence": 0.9858978390693665}]}, {"text": "Four different native French speakers were then asked to provide suitable substitutes for the nouns in context.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Kendall's \u03c4 b and GAP paraphrase ranking scores  for the English lexical substitution task. Scores marked  with ' ' are copied from the authors' respective papers.  Scores marked with ' ' are statistically significant with  p < 0.01 compared to the second baseline.", "labels": [], "entities": [{"text": "English lexical substitution task", "start_pos": 67, "end_pos": 100, "type": "TASK", "confidence": 0.6696905940771103}]}, {"text": " Table 2: Kendall's \u03c4 b paraphrase ranking scores for the  English lexical substitution task across different parts of  speech", "labels": [], "entities": [{"text": "English lexical substitution task", "start_pos": 59, "end_pos": 92, "type": "TASK", "confidence": 0.7236333116889}]}, {"text": " Table 3: R best and P 10 paraphrase induction scores for  the English lexical substitution task", "labels": [], "entities": [{"text": "R", "start_pos": 10, "end_pos": 11, "type": "METRIC", "confidence": 0.9873616695404053}, {"text": "English lexical substitution", "start_pos": 63, "end_pos": 91, "type": "TASK", "confidence": 0.5352880358695984}]}, {"text": " Table 4: P 10 paraphrase induction scores for the English  lexical substitution task across different parts of speech.  Scores marked with ' ' and ' ' are statistically significant  with respectively p < 0.01 and p < 0.05 compared to the  baseline.", "labels": [], "entities": [{"text": "paraphrase induction", "start_pos": 15, "end_pos": 35, "type": "TASK", "confidence": 0.7478384673595428}, {"text": "English  lexical substitution task", "start_pos": 51, "end_pos": 85, "type": "TASK", "confidence": 0.7213161215186119}]}, {"text": " Table 5: Kendall's \u03c4 b and GAP paraphrase ranking scores  for the French lexical substitution task", "labels": [], "entities": [{"text": "French lexical substitution", "start_pos": 67, "end_pos": 94, "type": "TASK", "confidence": 0.6062812308470408}]}, {"text": " Table 6: R best and P 10 paraphrase induction scores for  the French lexical substitution task", "labels": [], "entities": [{"text": "R", "start_pos": 10, "end_pos": 11, "type": "METRIC", "confidence": 0.9918569326400757}, {"text": "French lexical substitution", "start_pos": 63, "end_pos": 90, "type": "TASK", "confidence": 0.7185680667559305}]}]}