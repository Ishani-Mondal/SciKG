{"title": [{"text": "Exploiting Parse Structures for Native Language Identification", "labels": [], "entities": [{"text": "Native Language Identification", "start_pos": 32, "end_pos": 62, "type": "TASK", "confidence": 0.6832919120788574}]}], "abstractContent": [{"text": "Attempts to profile authors according to their characteristics extracted from textual data, including native language, have drawn attention in recent years, via various machine learning approaches utilising mostly lexical features.", "labels": [], "entities": []}, {"text": "Drawing on the idea of contrastive analysis, which postulates that syntactic errors in a text are to some extent influenced by the native language of an author, this paper explores the usefulness of syntactic features for native language identification.", "labels": [], "entities": [{"text": "contrastive analysis", "start_pos": 23, "end_pos": 43, "type": "TASK", "confidence": 0.7076599895954132}, {"text": "native language identification", "start_pos": 222, "end_pos": 252, "type": "TASK", "confidence": 0.6746805310249329}]}, {"text": "We take two types of parse substructure as features-horizontal slices of trees, and the more general feature schemas from discriminative parse reranking-and show that using this kind of syntactic feature results in an accuracy score in classification of seven native languages of around 80%, an error reduction of more than 30%.", "labels": [], "entities": [{"text": "accuracy score", "start_pos": 218, "end_pos": 232, "type": "METRIC", "confidence": 0.9806319773197174}, {"text": "error reduction", "start_pos": 295, "end_pos": 310, "type": "METRIC", "confidence": 0.9817730486392975}]}], "introductionContent": [{"text": "Inferring characteristics of authors from their textual data, often termed authorship profiling, has seen a number of computational approaches proposed in recent years.", "labels": [], "entities": []}, {"text": "The problem is typically treated as a classification task, where an author is classified with respect to characteristics such as gender, age, native language, and soon.", "labels": [], "entities": []}, {"text": "This profile information is often of interest to marketing organisations for product promotional reasons as well as governments or law enforcements for crime investigation purposes.", "labels": [], "entities": []}, {"text": "The particular application that motivates the present study is detection of phishing, the attempt to defraud through texts that are designed to deceive Internet users into giving away confidential details.", "labels": [], "entities": [{"text": "detection of phishing", "start_pos": 63, "end_pos": 84, "type": "TASK", "confidence": 0.898032526175181}]}, {"text": "One class of countermeasures to phishing consists of technical methods such as email authentication; another looks at profiling of the text's author(s), to find any indications of the source of the text.", "labels": [], "entities": []}, {"text": "In this paper we investigate classification of a text with respect to an author's native language, where this is not the language that that text is written in (which is often the casein phishing); we refer to this as native language identification.", "labels": [], "entities": [{"text": "classification of a text", "start_pos": 29, "end_pos": 53, "type": "TASK", "confidence": 0.8547932803630829}, {"text": "native language identification", "start_pos": 217, "end_pos": 247, "type": "TASK", "confidence": 0.7377811670303345}]}, {"text": "Initial work by was followed by,, and.", "labels": [], "entities": []}, {"text": "By and large, the problem was tackled using various supervised machine learning approaches, with mostly lexical features over characters, words, and parts of speech, as well as some document structure.", "labels": [], "entities": []}, {"text": "Syntactic features, in contrast, in particular those that capture grammatical errors, which might potentially be useful for this task, have received little attention.", "labels": [], "entities": []}, {"text": "did suggest using syntactic errors in their work but did not investigate them in any detail.", "labels": [], "entities": []}, {"text": "noted the relevance of the concept of contrastive analysis, which postulates that native language constructions lead to characteristic errors in a second language.", "labels": [], "entities": [{"text": "contrastive analysis", "start_pos": 38, "end_pos": 58, "type": "TASK", "confidence": 0.7602120041847229}]}, {"text": "In their experimental work, however, they used only three manual syntactic constructions drawn from the literature; an ANOVA analysis showed a detectable effect, but they did not improve classification accuracy over purely lexical features.", "labels": [], "entities": [{"text": "ANOVA", "start_pos": 119, "end_pos": 124, "type": "METRIC", "confidence": 0.8475624918937683}, {"text": "accuracy", "start_pos": 202, "end_pos": 210, "type": "METRIC", "confidence": 0.8989171981811523}]}, {"text": "In this paper, we investigate syntactic features for native language identification that are more general than, and that do not require the manual construction of, the above approach.", "labels": [], "entities": [{"text": "native language identification", "start_pos": 53, "end_pos": 83, "type": "TASK", "confidence": 0.6909640828768412}]}, {"text": "Taking the trees produced by statistical parsers, we use tree cross-sections as features in a machine learning approach to determine which ones characterise non-native speaker errors.", "labels": [], "entities": []}, {"text": "Specifically, we look at two types of parse tree substructure to use as features: horizontal slices of the trees-that is, characterising parse trees as sets of context-free grammar production rules-and the features schemas used in discriminative parse reranking.", "labels": [], "entities": [{"text": "discriminative parse reranking", "start_pos": 231, "end_pos": 261, "type": "TASK", "confidence": 0.6661723057428995}]}, {"text": "The goal of the present study is therefore to investigate the influence to which syntactic features represented by parse structures would have on the classification task of identifying an author's native language relative to, and in combination with, lexical features.", "labels": [], "entities": []}, {"text": "The remainder of this paper is structured as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we discuss some related work on the two key topics of this paper: primarily on comparable work in native language identification, and then on how the notion of contrastive analysis can be applicable here.", "labels": [], "entities": [{"text": "native language identification", "start_pos": 112, "end_pos": 142, "type": "TASK", "confidence": 0.6493677000204722}, {"text": "contrastive analysis", "start_pos": 174, "end_pos": 194, "type": "TASK", "confidence": 0.845129668712616}]}, {"text": "We then describe the models examined in Section 3, followed by experimental setup in Section 4.", "labels": [], "entities": []}, {"text": "Section 5 presents results, and Section 6 discussion of those results.", "labels": [], "entities": []}], "datasetContent": [{"text": "Given our relatively small amount of data, we use kfold cross-validation, choosing k = 5.", "labels": [], "entities": []}, {"text": "While testing for statistical significance of classification results is often not carried out in NLP, we do so here because the quantity of data could raise questions about the certainty of any effect.", "labels": [], "entities": [{"text": "certainty", "start_pos": 177, "end_pos": 186, "type": "METRIC", "confidence": 0.9906625151634216}]}, {"text": "In an encyclopedic survey of cross-validation in machine learning contexts, Refaeilzadeh et al. note that there is as yet no universal standard for testing of statistical significance; and that while more sophisticated techniques have been proposed, none is more widely accepted than a paired t-test over folds.", "labels": [], "entities": []}, {"text": "We therefore use this paired t-test over folds, as formulated of Alpaydin.", "labels": [], "entities": []}, {"text": "Under this cross-validation, 5 separate training feature sets are constructed, excluding the test fold; 3 folds are used for training, 1 fold for tuning and 1 fold for testing.", "labels": [], "entities": []}, {"text": "We also use a held-out test set for comparison, as it is well-known that cross-validation can overestimate prediction error ().", "labels": [], "entities": []}, {"text": "We do not carryout significance testing here-with this held-out test set size (n = 125), two models would have to differ by a great deal to be significant.", "labels": [], "entities": []}, {"text": "We only use it as a check on the effect of applying to completely new data.", "labels": [], "entities": []}, {"text": "presents the results for the three models individually under cross-validation.", "labels": [], "entities": []}, {"text": "The first point to note is that PROD-RULE, under both parsers, is a substantial improvement over LEXICAL when (non-lexicalised) parse rules together with rules lexicalised with function words are used (rows marked with * in, with the largest difference as much as 77.75% for PROD-RULE[both]* (n = all) versus 64.29% for LEXICAL; these differences with respect to LEXICAL are statistically significant.", "labels": [], "entities": []}, {"text": "(To give an idea, the paired t-test standard error for this largest difference is 2.52%.)", "labels": [], "entities": [{"text": "paired t-test standard error", "start_pos": 22, "end_pos": 50, "type": "METRIC", "confidence": 0.7347247079014778}]}, {"text": "In terms of error reduction, this is over 30%.", "labels": [], "entities": [{"text": "error reduction", "start_pos": 12, "end_pos": 27, "type": "METRIC", "confidence": 0.884484201669693}]}], "tableCaptions": [{"text": " Table 1: Classification results based on 5-fold cross vali- dation with parse rules as syntactic features (accuracy %)", "labels": [], "entities": [{"text": "Classification", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.9414615631103516}, {"text": "accuracy", "start_pos": 108, "end_pos": 116, "type": "METRIC", "confidence": 0.9992114305496216}]}, {"text": " Table 2: Classification results based on hold-out valida- tion with parse rules as syntactic features (accuracy %)", "labels": [], "entities": [{"text": "accuracy", "start_pos": 104, "end_pos": 112, "type": "METRIC", "confidence": 0.9995484948158264}]}, {"text": " Table 3: Classification results based on 5-fold cross vali- dation for combined models (accuracy %)", "labels": [], "entities": [{"text": "accuracy", "start_pos": 89, "end_pos": 97, "type": "METRIC", "confidence": 0.9992979764938354}]}, {"text": " Table 4: Classification results based on hold-out valida- tion for combined models (accuracy %)", "labels": [], "entities": [{"text": "accuracy", "start_pos": 85, "end_pos": 93, "type": "METRIC", "confidence": 0.9995548129081726}]}, {"text": " Table 7: Top 10 rules for the Stanford parser according to Information Gain on the held-out set", "labels": [], "entities": []}]}