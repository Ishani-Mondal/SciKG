{"title": [], "abstractContent": [{"text": "In the last few years, the interest of the research community in micro-blogs and social media services, such as Twitter, is growing exponentially.", "labels": [], "entities": []}, {"text": "Yet, so far not much attention has been paid on a key characteristic of micro-blogs: the high level of information redundancy.", "labels": [], "entities": [{"text": "information redundancy", "start_pos": 103, "end_pos": 125, "type": "TASK", "confidence": 0.7042068392038345}]}, {"text": "The aim of this paper is to systematically approach this problem by providing an operational definition of redundancy.", "labels": [], "entities": []}, {"text": "We cast redundancy in the framework of Textual En-tailment Recognition.", "labels": [], "entities": [{"text": "Textual En-tailment Recognition", "start_pos": 39, "end_pos": 70, "type": "TASK", "confidence": 0.5925349493821462}]}, {"text": "We also provide quantitative evidence on the pervasiveness of redundancy in Twitter, and describe a dataset of redundancy-annotated tweets.", "labels": [], "entities": []}, {"text": "Finally, we present a general purpose system for identifying redundant tweets.", "labels": [], "entities": []}, {"text": "An extensive quantitative evaluation shows that our system successfully solves the redundancy detection task, improving over baseline systems with statistical significance.", "labels": [], "entities": [{"text": "redundancy detection task", "start_pos": 83, "end_pos": 108, "type": "TASK", "confidence": 0.8609326680501302}]}], "introductionContent": [{"text": "Micro-blogs and social media services, such as Twitter, have experienced an exponential growth in the last few years.", "labels": [], "entities": []}, {"text": "The interest of the research community and the industry in these services has followed a similar trend.", "labels": [], "entities": []}, {"text": "Web companies such as Google, Yahoo, and Bing are integrating more and more social content to their sites.", "labels": [], "entities": []}, {"text": "At the same time, the computational linguistic community is getting increasingly interested in studying social and linguistic properties of Twitter and other micro-blogs ().", "labels": [], "entities": []}, {"text": "Yet, so far, not much attention has been paid on a key characteristic of micro-blogs: the high level of information redundancy.", "labels": [], "entities": [{"text": "information redundancy", "start_pos": 104, "end_pos": 126, "type": "TASK", "confidence": 0.7038742005825043}]}, {"text": "Users often post messages with the same, or very similar, content, especially when reporting or commenting on news and events.", "labels": [], "entities": []}, {"text": "For example, the following two tweets are part of a large set of redundant tweets issued during the 2010 winter Olympics: (example 1) t 1 : \"Swiss ski jumper Simon Ammann takes first gold of Vancouver\" t 2 : \"Swiss (Suisse) get the Gold on Normal Hill ski jump.", "labels": [], "entities": [{"text": "Normal Hill ski jump", "start_pos": 240, "end_pos": 260, "type": "DATASET", "confidence": 0.975381463766098}]}, {"text": "#Vancouver2010\" By performing an editorial study (described later in the paper) we discovered that a large part of eventrelated tweets are indeed redundant.", "labels": [], "entities": [{"text": "Vancouver2010", "start_pos": 1, "end_pos": 14, "type": "DATASET", "confidence": 0.9803856611251831}]}, {"text": "Detecting information redundancy is important for various reasons.", "labels": [], "entities": [{"text": "Detecting information redundancy", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.9080818295478821}]}, {"text": "First, most applications based on Twitter share the goal of providing tweets that are both informative and diverse, with respect to an initial user information need.", "labels": [], "entities": []}, {"text": "For example, Twitter search engines should ideally select the most informative and diverse set of tweets in return to a user query.", "labels": [], "entities": []}, {"text": "Similarly, a news web portal that attaches tweets to a given news article should attach those tweets that provide the broadest and most diverse set of information, opinions, and updates about the news item.", "labels": [], "entities": []}, {"text": "To keep a high level of diversity, redundant tweets should be removed from the set of tweets displayed to the user.", "labels": [], "entities": []}, {"text": "shows an example of a Twitter search engine where redundant tweets are 659 present (left) and where they are discarded (right).", "labels": [], "entities": []}, {"text": "Also, from a computational linguistic point of view, the high redundancy in micro-blogs gives the unprecedented opportunity to study classical tasks such as text summarization), textual entailment recognition) and paraphrase detection) on very large corpora characterized by an original and emerging linguistic style, pervaded with ungrammatical and colloquial expressions, abbreviations, and new linguistic forms.", "labels": [], "entities": [{"text": "text summarization", "start_pos": 157, "end_pos": 175, "type": "TASK", "confidence": 0.711108848452568}, {"text": "textual entailment recognition", "start_pos": 178, "end_pos": 208, "type": "TASK", "confidence": 0.7640920480092367}, {"text": "paraphrase detection", "start_pos": 214, "end_pos": 234, "type": "TASK", "confidence": 0.7965534925460815}]}, {"text": "The aim of this paper is to formally define, for the first time, the problem of redundancy in micro-blogs and to systematically approach the task of automatic redundancy detection.", "labels": [], "entities": [{"text": "automatic redundancy detection", "start_pos": 149, "end_pos": 179, "type": "TASK", "confidence": 0.6714845101038615}]}, {"text": "Note that we focus on linguistic redundancy, i.e. tweets that convey the same information with different wordings, and ignore the more trivial issue of detecting retweets, which can be considered the most basic expression of redundancy.", "labels": [], "entities": []}, {"text": "The main contributions of this paper are the following: \u2022 We formally define the problem of redundancy detection in micro-blogs within the framework of Textual Entailment theory; \u2022 We report results from an editorial study and provide quantitative evidence of the pervasiveness of redundancy in Twitter; \u2022 We present a set of simple and effective machine learning models for solving the task of redundancy detection; \u2022 We provide promising experimental results that show that these models outperform baseline approaches with statistical significance, and we report a qualitative evaluation revealing the advantages of the proposed model.", "labels": [], "entities": [{"text": "redundancy detection in micro-blogs", "start_pos": 92, "end_pos": 127, "type": "TASK", "confidence": 0.8402684926986694}, {"text": "Textual Entailment theory", "start_pos": 152, "end_pos": 177, "type": "TASK", "confidence": 0.7116036613782247}, {"text": "redundancy detection", "start_pos": 395, "end_pos": 415, "type": "TASK", "confidence": 0.7559618055820465}]}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "First, we shortly describe related work in Section 2.", "labels": [], "entities": []}, {"text": "Next, we provide our operational definition of redundancy and introduce our editorial study and dataset in Section 3.", "labels": [], "entities": []}, {"text": "In Section 4 we describe our models for redundancy detection.", "labels": [], "entities": [{"text": "redundancy detection", "start_pos": 40, "end_pos": 60, "type": "TASK", "confidence": 0.910722941160202}]}, {"text": "In Section 5 we provide a quantitative and qualitative evaluation of our models.", "labels": [], "entities": []}, {"text": "In Section 6 we conclude the paper with final observations and future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we present an evaluation of the different redundancy detection models.", "labels": [], "entities": [{"text": "redundancy detection", "start_pos": 58, "end_pos": 78, "type": "TASK", "confidence": 0.7310580015182495}]}, {"text": "First, we define the experimental setup in Section 5.1.", "labels": [], "entities": []}, {"text": "Then, we analyze the results of the experiments in Section 5.2.", "labels": [], "entities": []}, {"text": "We experiment with the redundancy detection dataset described in Section 3.", "labels": [], "entities": [{"text": "redundancy detection", "start_pos": 23, "end_pos": 43, "type": "TASK", "confidence": 0.795214056968689}]}, {"text": "We randomly divide the corpus into two sets: 50% for training and 50% for testing.", "labels": [], "entities": []}, {"text": "The training set contains 185 positive tweet-pairs and 416 negative pairs.", "labels": [], "entities": []}, {"text": "The test set contains 182 positive pairs and 466 negatives.", "labels": [], "entities": []}, {"text": "We evaluate the performance of the SVM models using the following feature combinations: LEX+BOW, LEX+WBOW, SYNT+BOW, SYNT+WBOW, FOR+BOW, FOR+WBOW.", "labels": [], "entities": [{"text": "BOW", "start_pos": 92, "end_pos": 95, "type": "METRIC", "confidence": 0.5070170164108276}, {"text": "FOR+BOW", "start_pos": 128, "end_pos": 135, "type": "METRIC", "confidence": 0.8477305769920349}, {"text": "FOR", "start_pos": 137, "end_pos": 140, "type": "METRIC", "confidence": 0.9628334045410156}]}, {"text": "We compare to the system baselines BOW and WBOW.", "labels": [], "entities": [{"text": "BOW", "start_pos": 35, "end_pos": 38, "type": "METRIC", "confidence": 0.9874354600906372}, {"text": "WBOW", "start_pos": 43, "end_pos": 47, "type": "DATASET", "confidence": 0.8680704236030579}]}, {"text": "The performance of the different models is computed using the Area Under the ROC curve (AROC) applied to the classification score returned by the SVM.", "labels": [], "entities": [{"text": "Area Under the ROC curve (AROC)", "start_pos": 62, "end_pos": 93, "type": "METRIC", "confidence": 0.9398863539099693}]}, {"text": "The ROC curve allows us to study the behavior of the classifier in detail, and also provides a powerful way to compare among systems when the dataset is unbalanced (as in our case).", "labels": [], "entities": [{"text": "ROC", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.93319171667099}]}, {"text": "To determine the statistical significance of the difference in the performance of the systems we analyzed, we use the model described in) as implemented in).", "labels": [], "entities": []}, {"text": "We pre-process the dataset with the following tools: the Charniak Parser) for parsing sentences, the WordNet similarity package () for computing WBOW and for linking the two tweets in a pair, and SVMlight, extended with the syntactic first-order rule kernels described in for creating the SYNT and the FOR feature spaces.", "labels": [], "entities": []}, {"text": "We used the Charniak syntactic parser without any specific adaptation to the Twitter language.", "labels": [], "entities": []}, {"text": "reports the results of the experiment.", "labels": [], "entities": []}, {"text": "The first and most important result is that models using content features (LEX, SYNT, and FOR) along with similarity features (BOW and WBOW) outperform the two baseline models using only similarity features with statistical significance, up to more than 15% AROC points.", "labels": [], "entities": [{"text": "FOR", "start_pos": 90, "end_pos": 93, "type": "METRIC", "confidence": 0.9981310963630676}, {"text": "BOW", "start_pos": 127, "end_pos": 130, "type": "METRIC", "confidence": 0.9570461511611938}, {"text": "AROC", "start_pos": 258, "end_pos": 262, "type": "METRIC", "confidence": 0.9932428002357483}]}, {"text": "At first glance, WordNet similarities are not useful: the performance of the WBOW model is indeed comparable and statistically insignificant with respect to the pure token based model BOW.", "labels": [], "entities": []}, {"text": "This seems to be intuitive as the language of the tweets can be far from proper English, i.e. it may contain many out-of-dictionary words that are not present in WordNet, thus impairing the similarity measure used by This trend is also confirmed in the case of contentbased systems like LEX and SYNT.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 162, "end_pos": 169, "type": "DATASET", "confidence": 0.9354342818260193}]}, {"text": "Using BOW or WBOW in combination with these features has the same effect on the final performance.", "labels": [], "entities": [{"text": "BOW", "start_pos": 6, "end_pos": 9, "type": "METRIC", "confidence": 0.9425959587097168}, {"text": "WBOW", "start_pos": 13, "end_pos": 17, "type": "DATASET", "confidence": 0.8035931587219238}]}, {"text": "Only the FOR features are positively affected by the WordNetbased distance.", "labels": [], "entities": [{"text": "FOR", "start_pos": 9, "end_pos": 12, "type": "METRIC", "confidence": 0.9972152709960938}, {"text": "WordNetbased distance", "start_pos": 53, "end_pos": 74, "type": "DATASET", "confidence": 0.8769599199295044}]}, {"text": "This maybe explained by the fact that in the FOR+WBOW system, the WordNet similarity is also used to link words in the two tweets of a pair.", "labels": [], "entities": [{"text": "FOR+WBOW", "start_pos": 45, "end_pos": 53, "type": "DATASET", "confidence": 0.695122500260671}]}, {"text": "This increases the possibility of finding reasonable and useful first-order rules.", "labels": [], "entities": []}, {"text": "In the quali-tative analysis that follows, we show some examples that support this intuition.", "labels": [], "entities": []}, {"text": "On the other hand, syntax plays a key role for detecting redundancy.", "labels": [], "entities": []}, {"text": "The two syntax based models SYNT and FOR outperform the lexical based models LEX between 1 and 2 AROC points.", "labels": [], "entities": [{"text": "FOR", "start_pos": 37, "end_pos": 40, "type": "METRIC", "confidence": 0.9895330667495728}, {"text": "LEX", "start_pos": 77, "end_pos": 80, "type": "METRIC", "confidence": 0.929931104183197}]}, {"text": "This is surprising, since the Charniak parser used in the experiments has not been adapted to the Tweet language, and therefore could have produced many interpretation errors, thus impairing the use of syntax.", "labels": [], "entities": []}, {"text": "This seems to suggest that if the interpretations of the part-of speech tags of the unknown words is correct, the syntax of tweets is reasonably similar to the syntax of the generic English language.", "labels": [], "entities": []}, {"text": "The best performing model is FOR+WBOW: firstorder rules successfully emerge in tweets and are positively exploited by the learning system.", "labels": [], "entities": [{"text": "FOR+WBOW", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.795336902141571}]}, {"text": "In the next section we report examples that support this observation.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Ranks of some tweet pairs according to the scores of the different classifiers.", "labels": [], "entities": []}]}