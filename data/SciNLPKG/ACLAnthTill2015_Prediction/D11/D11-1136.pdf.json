{"title": [{"text": "Closing the Loop: Fast, Interactive Semi-Supervised Annotation With Queries on Features and Instances", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper describes DUALIST, an active learning annotation paradigm which solicits and learns from labels on both features (e.g., words) and instances (e.g., documents).", "labels": [], "entities": []}, {"text": "We present a novel semi-supervised training algorithm developed for this setting, which is (1) fast enough to support real-time interactive speeds, and (2) at least as accurate as pre-existing methods for learning with mixed feature and instance labels.", "labels": [], "entities": []}, {"text": "Human annotators in user studies were able to produce near-state-of-the-art classifiers-on several corpora in a variety of application domains-with only a few minutes of effort.", "labels": [], "entities": []}], "introductionContent": [{"text": "In active learning, a classifier participates in its own training process by posing queries, such as requesting labels for documents in a text classification task.", "labels": [], "entities": [{"text": "text classification task", "start_pos": 138, "end_pos": 162, "type": "TASK", "confidence": 0.7338070770104727}]}, {"text": "The goal is to maximize the accuracy of the trained system in the most economically efficient way.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.9992088675498962}]}, {"text": "This paradigm is well-motivated for natural language applications, where unlabeled data maybe readily available (e.g., text on the Internet), but the annotation process can be slow and expensive.", "labels": [], "entities": []}, {"text": "Nearly all previous work in active learning, however, has focused on selecting queries from the learner's perspective.", "labels": [], "entities": []}, {"text": "For example, experiments are often run in simulation rather than with user studies, and results are routinely evaluated in terms of training set size rather than human annotation time or labor costs (which are more reasonable measures of labeling effort).", "labels": [], "entities": []}, {"text": "Many state-of-the-art algorithms are also too slow to run or too tedious to implement to be useful for real-time interaction with human annotators, and few analyses have taken these factors into account.", "labels": [], "entities": []}, {"text": "Furthermore, there is very little work on actively soliciting domain knowledge from humans (e.g., information about features) and incorporating this into the learning process.", "labels": [], "entities": []}, {"text": "While selecting good queries is clearly important, if our goal is to reduce actual annotation effort these human factors must betaken into account.", "labels": [], "entities": []}, {"text": "In this work, we propose anew interactive annotation interface which addresses some of these issues; in particular it has the ability to pose queries on both features (e.g., words) and instances (e.g., documents).", "labels": [], "entities": []}, {"text": "We present a novel semi-supervised learning algorithm that is fast, flexible, and accurate enough to support these interface design constraints interactively.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conduct four sets of experiments to evaluate our approach.", "labels": [], "entities": []}, {"text": "The first two are \"offline\" experiments, designed to better understand (1) how our training algorithm compares to existing methods for featurelabel learning, and (2) the effects of tuning the \u03b1 parameter.", "labels": [], "entities": []}, {"text": "The other experiments are user studies designed to empirically gauge how well human annotators make use of DUALIST in practice.", "labels": [], "entities": []}, {"text": "We use a variety of benchmark corpora in the following evaluations.", "labels": [], "entities": []}, {"text": "Reuters () is a collection of news articles organized into topics, such as acquisitions, corn, earnings, etc.", "labels": [], "entities": [{"text": "Reuters", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.908632218837738}]}, {"text": "As in previous work) we use the 10 most frequent topics, but further process the corpus by removing ambiguous documents (i.e., that belong to multiple topics) so that all articles have a unique label, resulting in a corpus of 9,002 articles.) consists of 4,199 university web pages of four types: course, faculty, project, and student.", "labels": [], "entities": []}, {"text": "20 Newsgroups is a set of 18,828 Usenet messages from 20 different online discussion groups.", "labels": [], "entities": []}, {"text": "For certain experiments (such as the one shown in), we also use topical subsets.", "labels": [], "entities": []}, {"text": "Movie Reviews () is a set of 2,000 online movie reviews categorized as positive or negative in sentiment.", "labels": [], "entities": []}, {"text": "All data sets were processed using lowercased unigram features, with punctuation and common stop-words removed.", "labels": [], "entities": []}, {"text": "To evaluate our system in practice, we conducted a series of user experiments.", "labels": [], "entities": []}, {"text": "This is in contrast to most previous work, which simulates active learning by using known document labels and feature labels from a simulated oracle (which can be flawed, as we saw in the previous section).", "labels": [], "entities": []}, {"text": "We argue that this is an important contribution, as it gives us a better sense of how well the approach actually works in practice.", "labels": [], "entities": []}, {"text": "It also allows us to analyze behavioral results, which in turn may help inform future protocols for human interaction in active learning.", "labels": [], "entities": []}, {"text": "DUALIST is implemented as a web-based application in Java and was deployed online.", "labels": [], "entities": [{"text": "DUALIST", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.8500474095344543}]}, {"text": "We used three different configurations: active dual (as in, implementing everything from Section 2), active instance (instance queries only, no features), and a passive instance baseline (instances only, but selected at random).", "labels": [], "entities": []}, {"text": "We also began by randomly selecting instances in the active configurations, until every class has at least one labeled instance or one labeled feature.", "labels": [], "entities": []}, {"text": "D = 2 documents and V = 100 features were selected for each round of active learning.", "labels": [], "entities": [{"text": "D", "start_pos": 0, "end_pos": 1, "type": "METRIC", "confidence": 0.9452065825462341}]}, {"text": "We recruited five members of our research group to label three data sets using each configuration, in an order of their choosing.", "labels": [], "entities": []}, {"text": "Users were first allowed to spend a minute or two familiarizing themselves with DUALIST, but received no training regarding the interface or data sets.", "labels": [], "entities": []}, {"text": "All experiments used a fixed 90% train, 10% test split which was consistent across all users, and annotators were not allowed to seethe accuracy of the classifier they were training at anytime.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 136, "end_pos": 144, "type": "METRIC", "confidence": 0.9918338060379028}]}, {"text": "Each annotation action was timestamped and logged for analysis, and each experiment automatically terminated after six minutes.", "labels": [], "entities": []}, {"text": "shows learning curves, in terms of accuracy vs. annotation time, for each trial in the user study.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9989526271820068}]}, {"text": "The first thing to note is that the active 1472: User experiments involving human annotators for text classification.", "labels": [], "entities": [{"text": "active 1472", "start_pos": 36, "end_pos": 47, "type": "DATASET", "confidence": 0.8010816276073456}, {"text": "text classification", "start_pos": 97, "end_pos": 116, "type": "TASK", "confidence": 0.8119906783103943}]}, {"text": "Each row plots accuracy vs. time learning curves fora particular user (under all three experimental conditions) for each of the three corpora (one column per data set).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9990355968475342}]}, {"text": "For clarity, vertical axes are scaled differently for each corpus, but held constant across all users.", "labels": [], "entities": []}, {"text": "The thin dashed lines at the top of each plot represents the idealized fully-supervised accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 88, "end_pos": 96, "type": "METRIC", "confidence": 0.9980564117431641}]}, {"text": "Horizontal axes show labeling cost in terms of actual elapsed annotation time (in seconds).", "labels": [], "entities": []}, {"text": "dual configuration yields consistently better learning curves than either active or passive learning with instances alone, often getting within 90% of fullysupervised accuracy (in under six minutes).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 167, "end_pos": 175, "type": "METRIC", "confidence": 0.9769440293312073}]}, {"text": "The only two exceptions make interesting (and different) case studies.", "labels": [], "entities": []}, {"text": "User 4 only provided four labeled features in the Movie Review corpus, which partially explains the similarity in performance to the instance-only cases.", "labels": [], "entities": [{"text": "Movie Review corpus", "start_pos": 50, "end_pos": 69, "type": "DATASET", "confidence": 0.8906816840171814}]}, {"text": "Moreover, these were manually-added features, i.e., he never answered any of the classifier's feature queries, thus depriving the learner of the information it requested.", "labels": [], "entities": []}, {"text": "User 5, on the other hand, never manually added features and only answered queries.", "labels": [], "entities": []}, {"text": "With the WebKB corpus, however, he apparently found feature queries for the course label to be easier than the other classes, and 71% of all his feature labels came from that class (sometimes noisily, e.g., \"instructor\" might also indicate faculty pages).", "labels": [], "entities": [{"text": "WebKB corpus", "start_pos": 9, "end_pos": 21, "type": "DATASET", "confidence": 0.9674012660980225}]}, {"text": "This imbalance ultimately biased the learner toward the course label, which led to classification errors.", "labels": [], "entities": []}, {"text": "These pathological cases represent potential pitfalls that could be alleviated with additional user studies and training.", "labels": [], "entities": []}, {"text": "However, we note that the active dual interface is not particularly worse in these cases, it is simply not significantly better, as in the other 13 trials.", "labels": [], "entities": []}, {"text": "Feature queries were less costly than instances, which is consistent with findings in previous work).", "labels": [], "entities": []}, {"text": "The least expensive actions in these experiments were labeling (mean 3.2 seconds) and unlabeling (1.8s) features, while manually adding new features took only slightly longer (5.9s).", "labels": [], "entities": []}, {"text": "The most expensive actions were labeling (10.8s) and ignoring (9.9s) instance queries.", "labels": [], "entities": []}, {"text": "Interestingly, we observed that the human annotators spent most of the first three minutes performing feature-labeling actions ( ), and switched to more instance-labeling activity for the final three minutes ( ).", "labels": [], "entities": []}, {"text": "As hypothesized in Section 3.2, it seems that the active learner is exhausting the most salient feature queries early on, and users begin to focus on more interpretable instance queries overtime.", "labels": [], "entities": []}, {"text": "However, more study (and longer annotation periods) are warranted to better understand this phenomenon, which may suggest additional user interface design improvements.", "labels": [], "entities": []}, {"text": "We also saw surprising trends in annotation quality.", "labels": [], "entities": []}, {"text": "In active settings, users made an average of one instance-labeling error per trial (relative to the goldstandard labels), but in the passive case this rose to 1.6, suggesting they are more accurate on the active queries.", "labels": [], "entities": []}, {"text": "However, they also explicitly ignored more instances in the active dual condition (7.7) than either active instance (5.9) or passive (2.5), indicating that they find these queries more ambiguous.", "labels": [], "entities": []}, {"text": "This seems reasonable, since these are the instances the classifier is least certain about.", "labels": [], "entities": []}, {"text": "But if we look at the time users spent on these actions, they are much faster to label/ignore (9.7s/7.5s) in the active dual scenario than in the active instance (10.0s/10.7s) or passive (12.3s/15.4s) cases, which means they are being more efficient.", "labels": [], "entities": []}, {"text": "The differences in time between dual and passive are statistically significant 4 .", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Summary of results using DUALIST for web- scale information extraction.", "labels": [], "entities": [{"text": "web- scale information extraction", "start_pos": 47, "end_pos": 80, "type": "TASK", "confidence": 0.5471524596214294}]}]}