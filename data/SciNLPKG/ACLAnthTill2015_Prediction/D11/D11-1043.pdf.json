{"title": [{"text": "Ranking Human and Machine Summarization Systems", "labels": [], "entities": []}], "abstractContent": [{"text": "The Text Analysis Conference (TAC) ranks summarization systems by their average score over a collection of document sets.", "labels": [], "entities": [{"text": "Text Analysis Conference (TAC)", "start_pos": 4, "end_pos": 34, "type": "TASK", "confidence": 0.8325686454772949}, {"text": "summarization", "start_pos": 41, "end_pos": 54, "type": "TASK", "confidence": 0.9766104221343994}]}, {"text": "We investigate the statistical appropriateness of this score and propose an alternative that better distinguishes between human and machine evaluation systems.", "labels": [], "entities": []}], "introductionContent": [{"text": "For the past several years, the National Institute of Standards and Technology (NIST) has hosted the Text Analysis Conference (TAC) (previously called the Document Understanding Conference (DUC)).", "labels": [], "entities": [{"text": "Text Analysis Conference (TAC)", "start_pos": 101, "end_pos": 131, "type": "TASK", "confidence": 0.8334952195485433}, {"text": "Document Understanding Conference (DUC))", "start_pos": 155, "end_pos": 195, "type": "TASK", "confidence": 0.791721761226654}]}, {"text": "A major theme of this conference is multi-document summarization: machine summarization of sets of related documents, sometimes query-focused and sometimes generic.", "labels": [], "entities": [{"text": "multi-document summarization", "start_pos": 36, "end_pos": 64, "type": "TASK", "confidence": 0.6348827183246613}, {"text": "machine summarization of sets of related documents", "start_pos": 66, "end_pos": 116, "type": "TASK", "confidence": 0.8304364510944912}]}, {"text": "The summarizers are judged by how well the summaries match human-generated summaries in either automatic metrics such as ROUGE ( or manual metrics such as responsiveness or pyramid evaluation (.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 121, "end_pos": 126, "type": "METRIC", "confidence": 0.9613471031188965}]}, {"text": "Typically the systems are ranked by their average score overall document sets.", "labels": [], "entities": []}, {"text": "Ranking by average score is quite appropriate under certain statistical hypotheses, for example, when each sample is drawn from a distribution which differs from the distribution of other samples only through a location shift.", "labels": [], "entities": []}, {"text": "However, a non-parametric (rank-based) analysis of variance on the summarizers' scores on each document set revealed an impossibly small p-value (less     than 10 \u221212 using Matlab's kruskalwallis 1 ), providing evidence that a summary's score is not independent of the document set.", "labels": [], "entities": []}, {"text": "This effect can be seen in, showing the confidence bands, as computed by a Tukey honestly significant difference test for each document set's difficulty as measured by the mean rank responsiveness score for TAC 2010.", "labels": [], "entities": [{"text": "Tukey honestly significant difference test", "start_pos": 75, "end_pos": 117, "type": "METRIC", "confidence": 0.6356485903263092}, {"text": "TAC 2010", "start_pos": 207, "end_pos": 215, "type": "DATASET", "confidence": 0.847013920545578}]}, {"text": "The test clearly shows that the summarizer performances on different document sets have different averages.", "labels": [], "entities": []}, {"text": "We further illustrate this in, which show the scores of various summarizers on various document sets using standard human and automatic evaluation methods of overall responsiveness, linguistic quality, pyramid scores, and ROUGE-2 using color to indicate the value of the score.", "labels": [], "entities": [{"text": "ROUGE-2", "start_pos": 222, "end_pos": 229, "type": "METRIC", "confidence": 0.9825853109359741}]}, {"text": "Some rows are clearly darker, indicating overall lower scores for the sum-maries of these documents, and the variances of the scores differ row-by-row.", "labels": [], "entities": []}, {"text": "These plots show qualitatively what the non-parametric analysis of variance demonstrates statistically.", "labels": [], "entities": []}, {"text": "While the data presented was for the TAC 2010 update document sets, similar results hold for all the data.", "labels": [], "entities": [{"text": "TAC 2010 update document sets", "start_pos": 37, "end_pos": 66, "type": "DATASET", "confidence": 0.9518905520439148}]}, {"text": "Hence, it maybe advantageous to measure summarizer quality by accounting for heterogeneity of documents within each test set.", "labels": [], "entities": [{"text": "summarizer", "start_pos": 40, "end_pos": 50, "type": "TASK", "confidence": 0.9709711074829102}]}, {"text": "A non-parametric paired test like the Wilcoxon signed-rank is one way to do this.", "labels": [], "entities": []}, {"text": "Another way would be paired t-tests.", "labels": [], "entities": []}, {"text": "In the paper () the authors noted that while there is a significant gap in performance between machine systems and human summarizers when measured by average manual metrics, this gap is not present when measured by the averages of the best automatic metric (ROUGE).", "labels": [], "entities": [{"text": "automatic metric (ROUGE)", "start_pos": 240, "end_pos": 264, "type": "METRIC", "confidence": 0.6200379312038422}]}, {"text": "In particular, in the DUC 2005-2007 data some systems have ROUGE performance within the 95% confidence intervals of several human summarizers, but their pyramid, linguistic, and responsiveness scores do not achieve this level of performance.", "labels": [], "entities": [{"text": "DUC 2005-2007 data", "start_pos": 22, "end_pos": 40, "type": "DATASET", "confidence": 0.9696884155273438}, {"text": "ROUGE", "start_pos": 59, "end_pos": 64, "type": "METRIC", "confidence": 0.9854099750518799}]}, {"text": "Thus, the inexpensive automatic metrics, as currently employed, do not predict well how machine summaries compare to human summaries.", "labels": [], "entities": []}, {"text": "In this work we explore the use of documentpaired testing for summarizer comparison.", "labels": [], "entities": [{"text": "summarizer comparison", "start_pos": 62, "end_pos": 83, "type": "TASK", "confidence": 0.9611951410770416}]}, {"text": "Our main approach is to consider each pair of two summarizers' sets of scores (over all documents) as a balanced two-sample dataset, and to assess that pair's mean difference in scores through a two-sample T or Wilcoxon test, paired or unpaired.", "labels": [], "entities": []}, {"text": "Our goal has been to confirm that human summarizer scores are uniformly different and better on average than machine summarizer scores, and to rate the quality of the statistical method (T or W, paired or unpaired) by the consistency with which the human versus machine scores show superior human performance.", "labels": [], "entities": [{"text": "summarizer", "start_pos": 40, "end_pos": 50, "type": "TASK", "confidence": 0.9247686862945557}]}, {"text": "Our hope is that paired testing, using either the standard paired two-sample t-test or the distributionfree Wilcoxon signed-rank test, can provide greater power in the statistical analysis of automatic metrics such as ROUGE.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 218, "end_pos": 223, "type": "METRIC", "confidence": 0.674622654914856}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Number of significant differences found when testing for the difference of all pairs of summarization systems  (including humans).", "labels": [], "entities": []}, {"text": " Table 2: Number of significant differences resulting from 8 \u00d7 (N \u2212 8) tests for human-machine system means or  signed-rank comparisons.", "labels": [], "entities": []}]}