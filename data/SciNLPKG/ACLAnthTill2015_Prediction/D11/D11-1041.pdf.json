{"title": [{"text": "Corpus-Guided Sentence Generation of Natural Images", "labels": [], "entities": [{"text": "Corpus-Guided Sentence Generation of Natural Images", "start_pos": 0, "end_pos": 51, "type": "TASK", "confidence": 0.6469724873701731}]}], "abstractContent": [{"text": "We propose a sentence generation strategy that describes images by predicting the most likely nouns, verbs, scenes and prepositions that makeup the core sentence structure.", "labels": [], "entities": [{"text": "sentence generation", "start_pos": 13, "end_pos": 32, "type": "TASK", "confidence": 0.7251611649990082}]}, {"text": "The input are initial noisy estimates of the objects and scenes detected in the image using state of the art trained detectors.", "labels": [], "entities": []}, {"text": "As predicting actions from still images directly is unreliable, we use a language model trained from the English Gi-gaword corpus to obtain their estimates; together with probabilities of co-located nouns, scenes and prepositions.", "labels": [], "entities": [{"text": "predicting actions from still images", "start_pos": 3, "end_pos": 39, "type": "TASK", "confidence": 0.8462335944175721}, {"text": "English Gi-gaword corpus", "start_pos": 105, "end_pos": 129, "type": "DATASET", "confidence": 0.6357530256112417}]}, {"text": "We use these estimates as parameters on a HMM that models the sentence generation process, with hidden nodes as sentence components and image detections as the emissions.", "labels": [], "entities": [{"text": "sentence generation process", "start_pos": 62, "end_pos": 89, "type": "TASK", "confidence": 0.7876207629839579}]}, {"text": "Experimental results show that our strategy of combining vision and language produces readable and descriptive sentences compared to naive strategies that use vision alone.", "labels": [], "entities": []}], "introductionContent": [{"text": "What happens when you see a picture?", "labels": [], "entities": []}, {"text": "The most natural thing would be to describe it using words: using speech or text.", "labels": [], "entities": []}, {"text": "This description of an image is the output of an extremely complex process that involves: 1) perception in the Visual space, 2) grounding to World Knowledge in the Language Space and 3) speech/text production (see).", "labels": [], "entities": [{"text": "speech/text production", "start_pos": 186, "end_pos": 208, "type": "TASK", "confidence": 0.5824863761663437}]}, {"text": "Each of these components are challenging in their own right and are still considered open problems in the vision and linguistics fields.", "labels": [], "entities": []}, {"text": "In this paper, we introduce a computational framework that attempts to integrate these \u2020 indicates equal contribution.", "labels": [], "entities": []}, {"text": "Our hypothesis is based on the assumption that natural images accurately reflect common everyday scenarios which are captured in language.", "labels": [], "entities": []}, {"text": "For example, knowing that boats usually occur over water will enable us to constrain the possible scenes a boat can occur and exclude highly unlikely ones -street, highway.", "labels": [], "entities": []}, {"text": "It also enables us to predict likely actions (Verbs) given the current object detections in the image: detecting a dog with a person will likely induce walk rather than swim, jump, fly.", "labels": [], "entities": [{"text": "Verbs", "start_pos": 46, "end_pos": 51, "type": "METRIC", "confidence": 0.9908768534660339}]}, {"text": "Key to our approach is the use of a large generic corpus such as the English Gigaword as the semantic grounding to predict and correct the initial and often noisy visual detections of an image to produce a reasonable sentence that succinctly describes the image.", "labels": [], "entities": []}, {"text": "In order to get an idea of the difficulty of this task, it is important to first define what makes up a description of an image.", "labels": [], "entities": []}, {"text": "Based on our observations of annotated image data (see), a descriptive sentence for an image must contain at minimum: 1) the important objects (Nouns) that participate in the image, 2) Some description of the actions (Verbs) associated with these objects, 3) the scene where this image was taken and 4) the preposition that relates the objects to the scene.", "labels": [], "entities": []}, {"text": "That is, a quadruplet of T = {n, v, s, p} (Noun-Verb-ScenePreposition) that represents the core sentence structure.", "labels": [], "entities": []}, {"text": "Generating a sentence from this quadruplet is obviously a simplification from state of the art generation work, but as we will show in the experimental results (sec. 4), it is sufficient to describe images.", "labels": [], "entities": []}, {"text": "The key challenge is that detecting objects, actions and scenes directly from images is often noisy and unreliable.", "labels": [], "entities": [{"text": "detecting objects, actions and scenes directly from images", "start_pos": 26, "end_pos": 84, "type": "TASK", "confidence": 0.7892834213044908}]}, {"text": "We illustrate this using example images from the Pascal-Visual Object Classes (VOC) 2008 challenge.", "labels": [], "entities": [{"text": "Pascal-Visual Object Classes (VOC) 2008 challenge", "start_pos": 49, "end_pos": 98, "type": "TASK", "confidence": 0.5668372288346291}]}, {"text": "First, shows the variability of images in their raw image representations: pixels, edges and local features.", "labels": [], "entities": []}, {"text": "This makes it difficult for state of the art object detectors to reliably detect important objects in the scene: boat, humans and water -average precision scores reported in manages around 42% for humans and only 11% for boat over a dataset of almost 5000 images in 20 object categories.", "labels": [], "entities": [{"text": "water -average precision scores", "start_pos": 130, "end_pos": 161, "type": "METRIC", "confidence": 0.702573561668396}]}, {"text": "Yet, these images are semantically similar in terms of their high level description.", "labels": [], "entities": []}, {"text": "Second, cognitive studies have proposed that inferring the action from static images (known as an \"implied action\") is often achieved by detecting the pose of humans in the image: the position of the limbs with respect to one another, under the assumption that a unique pose occurs fora unique action.", "labels": [], "entities": []}, {"text": "Clearly, this assumption is weak as 1) similar actions maybe represented by different poses due to the inherent dynamic nature of the action itself: e.g. walking a dog and 2) different actions may have the same pose: e.g. walking a dog versus running).", "labels": [], "entities": []}, {"text": "The missing component here is whether the key object (dog) under interaction is considered.", "labels": [], "entities": []}, {"text": "Recent works that used poses for recognition of actions achieved 70% and 61% accuracy respectively under extremely limited testing conditions with only 5-6 action classes each.", "labels": [], "entities": [{"text": "recognition of actions", "start_pos": 33, "end_pos": 55, "type": "TASK", "confidence": 0.885555108388265}, {"text": "accuracy", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.9993867874145508}]}, {"text": "Finally, state of the art scene detectors] need to have enough representative training examples of scenes from pre-defined scene classes fora classification to be successfulwith a reported average precision of 83.7% tested over a dataset of 2600 images.", "labels": [], "entities": [{"text": "precision", "start_pos": 197, "end_pos": 206, "type": "METRIC", "confidence": 0.9740972518920898}]}, {"text": "Addressing all these visual challenges is clearly a formidable task which is beyond the scope of this paper.", "labels": [], "entities": []}, {"text": "Our focus instead is to show that with the addition of language to ground the noisy initial visual detections, we are able to improve the quality of the generated sentence as a faithful description of the image.", "labels": [], "entities": []}, {"text": "In particular, we show that it is possible to avoid predicting actions directly from images -which is still unreliable -and to use the corpus instead to guide our predictions.", "labels": [], "entities": [{"text": "predicting actions directly from images", "start_pos": 52, "end_pos": 91, "type": "TASK", "confidence": 0.8510148286819458}]}, {"text": "Our proposed strategy is also generic, that is, we make no prior assumptions on the image domain considered.", "labels": [], "entities": []}, {"text": "While other works (sec. 2) depend on strong annotations between images and text to ground their predictions (and to remove wrong sentences), we show that a large generic corpus is also able to provide the same grounding over larger domains of images.", "labels": [], "entities": []}, {"text": "It represents a relatively new style of learning: distant supervision.", "labels": [], "entities": []}, {"text": "Here, we do not require \"labeled\" data containing images and captions but only separate data from each side.", "labels": [], "entities": []}, {"text": "Another contribution is a computationally feasible way via dynamic programming to determine the most likely quadruplet T * = {n * , v * , s * , p * } that describes the image for generating possible sentences.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use the UIUC Pascal Sentence dataset, first introduced in and available online 1 . It contains 1000 images taken from a subset of the Pascal-VOC 2008 challenge image dataset and are hand annotated with sentences that describe the image by paid human annotators using Amazon Mechanical Turk.", "labels": [], "entities": [{"text": "UIUC Pascal Sentence dataset", "start_pos": 11, "end_pos": 39, "type": "DATASET", "confidence": 0.9450655877590179}, {"text": "Pascal-VOC 2008 challenge image dataset", "start_pos": 137, "end_pos": 176, "type": "DATASET", "confidence": 0.6866816759109498}, {"text": "Amazon Mechanical Turk", "start_pos": 270, "end_pos": 292, "type": "DATASET", "confidence": 0.9037539958953857}]}, {"text": "shows some sample images with their annotations.", "labels": [], "entities": []}, {"text": "There are 5 annotations per image, and each annotation is usually short -around 10 words long.", "labels": [], "entities": []}, {"text": "We randomly selected 900 images (4500 sentences) as the learning corpus to construct the verb and scene sets, {V, S} as described in sec.", "labels": [], "entities": []}, {"text": "3.3, and kept the remaining 100 images for testing and evaluation.", "labels": [], "entities": []}, {"text": "We performed several experiments to evaluate our proposed approach.", "labels": [], "entities": []}, {"text": "The different metrics used for evaluation and comparison are also presented, followed by a discussion of the experimental results.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Sentence generation evaluation results with hu- man gold standard. Human R 1 scores are averaged over  the 5 sentences using a leave one out procedure. Values  in bold are the top scores.", "labels": [], "entities": [{"text": "Sentence generation", "start_pos": 10, "end_pos": 29, "type": "TASK", "confidence": 0.937236487865448}, {"text": "hu- man gold standard", "start_pos": 54, "end_pos": 75, "type": "DATASET", "confidence": 0.6738944947719574}, {"text": "Human R 1 scores", "start_pos": 77, "end_pos": 93, "type": "METRIC", "confidence": 0.6867506727576256}]}]}