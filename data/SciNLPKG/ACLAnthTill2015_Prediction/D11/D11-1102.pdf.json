{"title": [{"text": "Hypotheses Selection Criteria in a Reranking Framework for Spoken Language Understanding", "labels": [], "entities": [{"text": "Spoken Language Understanding", "start_pos": 59, "end_pos": 88, "type": "TASK", "confidence": 0.8025379379590353}]}], "abstractContent": [{"text": "Reranking models have been successfully applied to many tasks of Natural Language Processing.", "labels": [], "entities": []}, {"text": "However, there are two aspects of this approach that need a deeper investigation: (i) Assessment of hypotheses generated for reranking at classification phase: baseline models generate a list of hypotheses and these are used for reranking without any assessment ; (ii) Detection of cases where rerank-ing models provide a worst result: the best hypothesis provided by the reranking model is assumed to be always the best result.", "labels": [], "entities": []}, {"text": "In some cases the reranking model provides an incorrect hypothesis while the baseline best hypothesis is correct, especially when base-line models are accurate.", "labels": [], "entities": []}, {"text": "In this paper we propose solutions for these two aspects: (i) a semantic inconsistency metric to select possibly more correct n-best hypotheses, from a large set generated by an SLU basiline model.", "labels": [], "entities": []}, {"text": "The selected hypotheses are reranked applying a state-of-the-art model based on Partial Tree Kernels, which encode SLU hypotheses in Support Vector Machines with complex structured features; (ii) finally, we apply a decision strategy, based on confidence values , to select the final hypothesis between the first ranked hypothesis provided by the base-line SLU model and the first ranked hypothesis provided by the re-ranker.", "labels": [], "entities": []}, {"text": "We show the effectiveness of these solutions presenting comparative results obtained reranking hypotheses generated by a very accurate Conditional Random Field model.", "labels": [], "entities": []}, {"text": "We evaluate our approach on the French MEDIA corpus.", "labels": [], "entities": [{"text": "French MEDIA corpus", "start_pos": 32, "end_pos": 51, "type": "DATASET", "confidence": 0.9148175120353699}]}, {"text": "The results show significant improvements with respect to current state-of-the-art and previous re-ranking models.", "labels": [], "entities": []}], "introductionContent": [{"text": "Discriminative reranking is a widely used approach for several Natural Language Processing (NLP) tasks: Syntactic Parsing (), Named Entity Recognition), Semantic Role Labelling (, Machine Translation (), Question Answering (.", "labels": [], "entities": [{"text": "Discriminative reranking", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.7850668132305145}, {"text": "Syntactic Parsing", "start_pos": 104, "end_pos": 121, "type": "TASK", "confidence": 0.8305675685405731}, {"text": "Semantic Role Labelling", "start_pos": 153, "end_pos": 176, "type": "TASK", "confidence": 0.7099290092786154}, {"text": "Machine Translation", "start_pos": 180, "end_pos": 199, "type": "TASK", "confidence": 0.8307786881923676}, {"text": "Question Answering", "start_pos": 204, "end_pos": 222, "type": "TASK", "confidence": 0.8737822771072388}]}, {"text": "Recently reranking approaches have been successfully applied also to Spoken Language Understanding (SLU) (.", "labels": [], "entities": [{"text": "Spoken Language Understanding (SLU)", "start_pos": 69, "end_pos": 104, "type": "TASK", "confidence": 0.8864247997601827}]}, {"text": "Discriminative Reranking combines two models: a first SLU model is used to generate a ranked list of n-best hypotheses; a reranking model sorts the list based on a different score and the final result is the new top ranked hypothesis.", "labels": [], "entities": [{"text": "Discriminative Reranking", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.6286922991275787}]}, {"text": "The advantage of reranking approaches is in the possibility to learn directly complex dependencies in the output domain, as this is provided in the hypotheses generated by the baseline model.", "labels": [], "entities": []}, {"text": "In previous approaches complex features are extracted from the hypotheses for both training and classification phase, but there are very few studies on approaches that can be applied to search in the hypotheses space generated by the baseline SLU model.", "labels": [], "entities": [{"text": "classification", "start_pos": 96, "end_pos": 110, "type": "TASK", "confidence": 0.958825409412384}]}, {"text": "Moreover, to keep overall computational cost reasonable, the size of the n-best list is typically small (few tens).", "labels": [], "entities": []}, {"text": "This is a limitation since the larger is the hypotheses space generated, the more likely is to find a better hypothesis.", "labels": [], "entities": []}, {"text": "On the other hand, reranking a large set of hypotheses is computationally expensive, thus a strategy to select the best hypotheses to be re-ranked would overcome this problem.", "labels": [], "entities": []}, {"text": "Another aspect of reranking that deserves to be deeper studied is its applicability.", "labels": [], "entities": [{"text": "reranking", "start_pos": 18, "end_pos": 27, "type": "TASK", "confidence": 0.9519327282905579}]}, {"text": "Although a reranking model improves the baseline model in the overall performance, in some cases the reranked best hypotheses can contain more mistakes than the baseline best hypothesis.", "labels": [], "entities": []}, {"text": "A strategy to decide when the reranking model should be applied and when the first hypothesis of the baseline model is more accurate would improve reranking performances.", "labels": [], "entities": []}, {"text": "In this paper, we propose two new models for improving discriminative reranking: (a) a semantic inconsistency metric that can be applied to SLU hypotheses to select those that are more likely to be correct; (b) a model selection strategy based on the confidence scores provided by the baseline SLU model and the reranker.", "labels": [], "entities": []}, {"text": "This provides a decision function that detects if the original top ranked hypothesis is more accurate than the reranked best hypothesis.", "labels": [], "entities": []}, {"text": "Our re-ranking strategies turnout to be effective on very accurate baseline models based on state-ofthe-art Conditinal Random Fields (CRF) implementation (.", "labels": [], "entities": []}, {"text": "We evaluate our approach on the well-known French MEDIA corpus for SLU ().", "labels": [], "entities": [{"text": "French MEDIA corpus for SLU", "start_pos": 43, "end_pos": 70, "type": "DATASET", "confidence": 0.8029242038726807}]}, {"text": "The results show that our approach significantly improves both \"traditional\" reranking approaches and stateof-the-art SLU models.", "labels": [], "entities": []}, {"text": "The remainder of the paper is organized as follows: in Section 2 we introduce the SLU task.", "labels": [], "entities": [{"text": "SLU task", "start_pos": 82, "end_pos": 90, "type": "TASK", "confidence": 0.8083435893058777}]}, {"text": "Section 3 describes our discriminative reranking framework for SLU, in particular the baseline model adopted, in sub-section 3.1, and the reranking model, in sub-section 3.2.", "labels": [], "entities": []}, {"text": "Section 4 describes the two strategies proposed in this paper for SLU reranking, whereas the experiments to evaluate our approaches are described in Section 5.", "labels": [], "entities": [{"text": "SLU reranking", "start_pos": 66, "end_pos": 79, "type": "TASK", "confidence": 0.8710340857505798}]}, {"text": "Finally, after a discussion in Section 6, in Section 7 we draw some conclusions.", "labels": [], "entities": []}], "datasetContent": [{"text": "The data used in our experiments are taken from the French MEDIA corpus ().", "labels": [], "entities": [{"text": "French MEDIA corpus", "start_pos": 52, "end_pos": 71, "type": "DATASET", "confidence": 0.8474293748537699}]}, {"text": "The corpus is made of 1.250 HumanMachine dialogs acquired with a Wizard-of-Oz approach in the domain of informtation and reservation of French hotels.", "labels": [], "entities": []}, {"text": "The data are split into training, development and test set.", "labels": [], "entities": []}, {"text": "Statistics of the corpus are presented in table 1.", "labels": [], "entities": []}, {"text": "For our CRF models, both Automatic Concept Annotation and Attribute Value Extraction SLU phases, we used wapiti 2 ().", "labels": [], "entities": []}, {"text": "The CRF model for the first SLU phase integrates a traditional set of features like word prefixes and suffixes (of length up to 5), plus some Yes/No features like \"Does the word start with capital letter ?\", \"Does the word contain non alphanumeric characters ?\", \"Is the word preceded by non alphanumeric characteris ?\" etc.", "labels": [], "entities": []}, {"text": "The CRF model for AVE integrates only words, prefixes and suffixes (length 3 and 4) concatenated with concepts.", "labels": [], "entities": []}, {"text": "Since in this case labels are attribute values, which area huge set with   respect to concepts ( \u02dc 700 VS 99), using a lot of features would make model training problematic.", "labels": [], "entities": [{"text": "VS 99", "start_pos": 103, "end_pos": 108, "type": "METRIC", "confidence": 0.9369531869888306}]}, {"text": "Despite the reduced set of features, training error rate at both token and sentence level is under 1%.", "labels": [], "entities": [{"text": "training error rate", "start_pos": 37, "end_pos": 56, "type": "METRIC", "confidence": 0.734038511912028}]}, {"text": "We didn't carryout optimization for parameters \u03c1 1 and \u03c1 2 of the elastic net (see section 3.1), default values lead inmost cases to very accurate models.", "labels": [], "entities": []}, {"text": "Reranking models based on SVM and PTK have been trained with \"SVM-Light-TK\" 3 . Kernel parameters M and SVM parameter C have been optimized on the development set, as well as thresholds for the WRR (see section 4.2).", "labels": [], "entities": [{"text": "WRR", "start_pos": 194, "end_pos": 197, "type": "METRIC", "confidence": 0.5089731812477112}]}, {"text": "Concerning hypotheses generation, for training we generate 100 hypotheses, we select the best with respect to the edit distance and the reference annotation and we keep a total of 10 hypotheses to build pairs.", "labels": [], "entities": [{"text": "hypotheses generation", "start_pos": 11, "end_pos": 32, "type": "TASK", "confidence": 0.808453768491745}]}, {"text": "For classification, with the \"standard\" reranking approach we generate and we keep the 10 best hypotheses.", "labels": [], "entities": [{"text": "classification", "start_pos": 4, "end_pos": 18, "type": "TASK", "confidence": 0.9799550175666809}]}, {"text": "While using SIM for hypotheses selection, we generate 1.000 hypotheses and we keep the 10 best with respect to SIM.", "labels": [], "entities": [{"text": "hypotheses selection", "start_pos": 20, "end_pos": 40, "type": "TASK", "confidence": 0.6915107071399689}]}, {"text": "1.000 is the best threshold between oracle accuracy and computational cost for evaluating the hypotheses.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.9820801019668579}]}, {"text": "Experiments have been performed on both manual and automatic transcriptions of dialog turns.", "labels": [], "entities": []}, {"text": "For automatic transcriptions the WER of the ASR is 30.3% on development set and 31.4% on test set.", "labels": [], "entities": [{"text": "WER", "start_pos": 33, "end_pos": 36, "type": "METRIC", "confidence": 0.999394416809082}, {"text": "ASR", "start_pos": 44, "end_pos": 47, "type": "TASK", "confidence": 0.7216684818267822}]}, {"text": "All results are reported in terms of Concept Error Rate (CER), which is the same as WER, but it is computed on concept sequences.", "labels": [], "entities": [{"text": "Concept Error Rate (CER)", "start_pos": 37, "end_pos": 61, "type": "METRIC", "confidence": 0.841106171409289}, {"text": "WER", "start_pos": 84, "end_pos": 87, "type": "METRIC", "confidence": 0.9235552549362183}]}, {"text": "In all cases we give results for both attributes only and attributes and values extraction", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of the MEDIA training and evaluation sets used for all experiments.", "labels": [], "entities": [{"text": "MEDIA training and evaluation sets", "start_pos": 28, "end_pos": 62, "type": "DATASET", "confidence": 0.6198630571365357}]}, {"text": " Table 2: Ranks of average score given by the CRF model to feature", "labels": [], "entities": [{"text": "average score", "start_pos": 19, "end_pos": 32, "type": "METRIC", "confidence": 0.9356760680675507}]}, {"text": " Table 4: Results of baseline CRF model and reranking models on", "labels": [], "entities": []}, {"text": " Table 5: Analysis over 10-best hypotheses for CRF baseline and the", "labels": [], "entities": [{"text": "CRF baseline", "start_pos": 47, "end_pos": 59, "type": "DATASET", "confidence": 0.6137226670980453}]}, {"text": " Table 6: Significance tests on results of models described in this", "labels": [], "entities": []}]}