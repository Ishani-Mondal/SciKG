{"title": [], "abstractContent": [{"text": "This paper describes a novel approach to the semantic relation detection problem.", "labels": [], "entities": [{"text": "semantic relation detection problem", "start_pos": 45, "end_pos": 80, "type": "TASK", "confidence": 0.885447159409523}]}, {"text": "Instead of relying only on the training instances fora new relation, we leverage the knowledge learned from previously trained relation detectors.", "labels": [], "entities": []}, {"text": "Specifically, we detect anew semantic relation by projecting the new relation's training instances onto a lower dimension topic space constructed from existing relation detectors through a three step process.", "labels": [], "entities": []}, {"text": "First, we construct a large relation repository of more than 7,000 relations from Wikipedia.", "labels": [], "entities": []}, {"text": "Second, we construct a set of non-redundant relation topics defined at multiple scales from the relation repository to characterize the existing relations.", "labels": [], "entities": []}, {"text": "Similar to the topics defined over words, each relation topic is an interpretable multinomial distribution over the existing relations.", "labels": [], "entities": []}, {"text": "Third, we integrate the relation topics in a kernel function, and use it together with SVM to construct detectors for new relations.", "labels": [], "entities": []}, {"text": "The experimental results on Wikipedia and ACE data have confirmed that background-knowledge-based topics generated from the Wikipedia relation repository can significantly improve the performance over the state-of-the-art relation detection approaches.", "labels": [], "entities": [{"text": "ACE data", "start_pos": 42, "end_pos": 50, "type": "DATASET", "confidence": 0.8453147709369659}, {"text": "relation detection", "start_pos": 222, "end_pos": 240, "type": "TASK", "confidence": 0.7348872274160385}]}], "introductionContent": [{"text": "Detecting semantic relations in text is very useful in both information retrieval and question answering because it enables knowledge bases to be leveraged to score passages and retrieve candidate answers.", "labels": [], "entities": [{"text": "Detecting semantic relations in text", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.8862493634223938}, {"text": "information retrieval", "start_pos": 60, "end_pos": 81, "type": "TASK", "confidence": 0.7696729004383087}, {"text": "question answering", "start_pos": 86, "end_pos": 104, "type": "TASK", "confidence": 0.838975340127945}]}, {"text": "To extract semantic relations from text, three types of approaches have been applied.", "labels": [], "entities": []}, {"text": "Rule-based methods () employ a number of linguistic rules to capture relation patterns.", "labels": [], "entities": []}, {"text": "Featurebased methods) transform relation instances into a large amount of linguistic features like lexical, syntactic and semantic features, and capture the similarity between these feature vectors.", "labels": [], "entities": []}, {"text": "Recent results mainly rely on kernel-based approaches.", "labels": [], "entities": []}, {"text": "Many of them focus on using tree kernels to learn parse tree structure related features).", "labels": [], "entities": []}, {"text": "Other researchers study how different approaches can be combined to improve the extraction performance.", "labels": [], "entities": []}, {"text": "For example, by combining tree kernels and convolution string kernels, () achieved the state of the art performance on ACE, which is a benchmark dataset for relation extraction.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 157, "end_pos": 176, "type": "TASK", "confidence": 0.834725022315979}]}, {"text": "Although a large set of relations have been identified, adapting the knowledge extracted from these relations for new semantic relations is still a challenging task.", "labels": [], "entities": []}, {"text": "Most of the work on domain adaptation of relation detection has focused on how to create detectors from ground up with as little training data as possible through techniques such as bootstrapping ().", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 20, "end_pos": 37, "type": "TASK", "confidence": 0.7831481695175171}, {"text": "relation detection", "start_pos": 41, "end_pos": 59, "type": "TASK", "confidence": 0.8311589062213898}]}, {"text": "We take a different approach, focusing on how the knowledge extracted from the existing relations can be reused to help build detectors for new relations.", "labels": [], "entities": []}, {"text": "We believe by reusing knowledge one can build a more cost effective relation detector, but there are several challenges associated with reusing knowledge.", "labels": [], "entities": []}, {"text": "The first challenge to address in this approach is how to construct a relation repository that has suffi-cient coverage.", "labels": [], "entities": []}, {"text": "In this paper, we introduce a method that automatically extracts the knowledge characterizing more than 7,000 relations from Wikipedia.", "labels": [], "entities": []}, {"text": "Wikipedia is comprehensive, containing a diverse body of content with significant depth and grows rapidly.", "labels": [], "entities": [{"text": "Wikipedia", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.9458413124084473}]}, {"text": "Wikipedia's infoboxes are particularly interesting for relation extraction.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 55, "end_pos": 74, "type": "TASK", "confidence": 0.9772879779338837}]}, {"text": "They are short, manually-created, and often have a relational summary of an article: a set of attribute/value pairs describing the article's subject.", "labels": [], "entities": []}, {"text": "Another challenge is how to deal with overlap of relations in the repository.", "labels": [], "entities": []}, {"text": "For example, Wikipedia authors may makeup a name when anew relation is needed without checking if a similar relation has already been created.", "labels": [], "entities": []}, {"text": "This leads to relation duplication.", "labels": [], "entities": [{"text": "relation duplication", "start_pos": 14, "end_pos": 34, "type": "TASK", "confidence": 0.8826539218425751}]}, {"text": "We refine the relation repository based on an unsupervised multiscale analysis of the correlations between existing relations.", "labels": [], "entities": []}, {"text": "This method is parameter free, and able to produce a set of non-redundant relation topics defined at multiple scales.", "labels": [], "entities": []}, {"text": "Similar to the topics defined over words (, we define relation topics as multinomial distributions over the existing relations.", "labels": [], "entities": []}, {"text": "The relation topics extracted in our approach are interpretable, orthonormal to each other, and can be used as basis relations to re-represent the new relation instances.", "labels": [], "entities": []}, {"text": "The third challenge is how to use the relation topics fora relation detector.", "labels": [], "entities": [{"text": "relation detector", "start_pos": 59, "end_pos": 76, "type": "TASK", "confidence": 0.8632035553455353}]}, {"text": "We map relation instances in the new domains to the relation topic space, resulting in a set of new features characterizing the relationship between the relation instances and existing relations.", "labels": [], "entities": []}, {"text": "By doing so, background knowledge from the existing relations can be introduced into the new relations, which overcomes the limitations of the existing approaches when the training data is not sufficient.", "labels": [], "entities": []}, {"text": "Our work fits in to a class of relation extraction research based on \"distant supervision\", which studies how knowledge and resources external to the target domain can be used to improve relation extraction.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 31, "end_pos": 50, "type": "TASK", "confidence": 0.819915771484375}, {"text": "relation extraction", "start_pos": 187, "end_pos": 206, "type": "TASK", "confidence": 0.8444793522357941}]}, {"text": "(. One distinction between our approach and other existing approaches is that we represent the knowledge from distant supervision using automatically constructed topics.", "labels": [], "entities": []}, {"text": "When we test on new instances, we do not need to search against the knowledge base.", "labels": [], "entities": []}, {"text": "In addition, our topics also model the indirect relationship between relations.", "labels": [], "entities": []}, {"text": "Such information cannot be directly found from the knowledge base.", "labels": [], "entities": []}, {"text": "The contributions of this paper are three-fold.", "labels": [], "entities": []}, {"text": "Firstly, we extract a large amount of training data for more than 7,000 semantic relations from Wikipedia (Wikipedia, 2011) and.", "labels": [], "entities": [{"text": "Wikipedia (Wikipedia, 2011)", "start_pos": 96, "end_pos": 123, "type": "DATASET", "confidence": 0.7806240717569987}]}, {"text": "A key part of this step is how we handle noisy data with little human effort.", "labels": [], "entities": []}, {"text": "Secondly, we present an unsupervised way to construct a set of relation topics at multiple scales.", "labels": [], "entities": []}, {"text": "This step is parameter free, and results in a nonredundant, multiscale relation topic space.", "labels": [], "entities": []}, {"text": "Thirdly, we design anew kernel for relation detection by integrating the relation topics into the relation detector construction.", "labels": [], "entities": [{"text": "relation detection", "start_pos": 35, "end_pos": 53, "type": "TASK", "confidence": 0.9194833934307098}]}, {"text": "The experimental results on Wikipedia and ACE data) have confirmed that background-knowledge-based features generated from the Wikipedia relation repository can significantly improve the performance over the state-of-the-art relation detection approaches.", "labels": [], "entities": [{"text": "Wikipedia", "start_pos": 28, "end_pos": 37, "type": "DATASET", "confidence": 0.9469251036643982}, {"text": "ACE data", "start_pos": 42, "end_pos": 50, "type": "DATASET", "confidence": 0.8349869847297668}, {"text": "relation detection", "start_pos": 225, "end_pos": 243, "type": "TASK", "confidence": 0.724290519952774}]}], "datasetContent": [{"text": "We used SVMLight) together with the user defined kernel setting in our approach.", "labels": [], "entities": [{"text": "SVMLight", "start_pos": 8, "end_pos": 16, "type": "DATASET", "confidence": 0.8899637460708618}]}, {"text": "The trade-off parameter between training error and margin c is 1 for all experiments.", "labels": [], "entities": [{"text": "training error", "start_pos": 32, "end_pos": 46, "type": "METRIC", "confidence": 0.9321196377277374}, {"text": "margin c", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.9633248448371887}]}, {"text": "Our approach to learn multiscale relation topics is largely parameter free.", "labels": [], "entities": []}, {"text": "The only parameter to beset is the precision \u03b5 = 10 \u22125 , which is also the default value in the diffusion wavelets implementation.", "labels": [], "entities": [{"text": "precision \u03b5", "start_pos": 35, "end_pos": 46, "type": "METRIC", "confidence": 0.9698855876922607}]}], "tableCaptions": [{"text": " Table 1: Number of topics at different levels (DBpe- dia Relations) under 5 different settings: use args, noun,  preposition and verb; arg 1 only; arg 2 only; noun only and  verb only.  Level args & words arg 1  arg 2 noun verb  1  7628  7628 7628 7628 7628  2  269  119  155  249  210  3  32  17  19  25  35  4  7  5  5  7  10  5  3  2  3  4  4  6  2  1  2  2  2  7  1  1  1  1", "labels": [], "entities": []}, {"text": " Table 4: F-measure comparison of different approaches  over 100 DBpedia relations with 5, 20 and 100 posi- tive examples per relation. AG: K Argument , DP: K P ath ,  BOW: K BOW , TF k : K T F k .  Approaches  100  20  5  Rule Based  37.70% 27.45% 13.20%  AG+ DP  73.64% 51.85% 22.95%  AG+ DP+ BOW  78.74% 62.76% 31.98%  AG+ DP+ BOW+ TF 2 81.18% 68.03% 41.60%", "labels": [], "entities": [{"text": "BOW: K BOW", "start_pos": 168, "end_pos": 178, "type": "METRIC", "confidence": 0.7550540566444397}, {"text": "Approaches", "start_pos": 199, "end_pos": 209, "type": "METRIC", "confidence": 0.9877578020095825}]}, {"text": " Table 5: Performance comparison of different approaches  with SVM over the ACE 2004 data. P: Precision, R: Re- call, F: F-measure, AG: K Argument , DP: K P ath , BOW:  K BOW , TF k : K T F k .  Approaches  P(%) R(%) F(%)  Convolution Tree Kernel  72.5  56.7  63.6  Composite Kernel (linear) 73.50 67.00 70.10  Syntactic Kernel  69.23 70.50 69.86  Nguyen, et al. (2009)  76.60 67.00 71.50  AG  59.56 46.22 52.02  AG + DP  64.44 54.93 59.28  AG + DP + BOW  62.00 61.19 61.15  AG + DP + BOW + TF 3  69.63 76.51 72.90  AG + DP + BOW + TF 2  69.15 77.88 73.24", "labels": [], "entities": [{"text": "ACE 2004 data", "start_pos": 76, "end_pos": 89, "type": "DATASET", "confidence": 0.9744885563850403}, {"text": "BOW", "start_pos": 163, "end_pos": 166, "type": "METRIC", "confidence": 0.9714491367340088}]}]}