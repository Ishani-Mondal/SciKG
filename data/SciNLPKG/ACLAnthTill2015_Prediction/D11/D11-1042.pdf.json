{"title": [{"text": "Corroborating Text Evaluation Results with Heterogeneous Measures", "labels": [], "entities": [{"text": "Text Evaluation", "start_pos": 14, "end_pos": 29, "type": "TASK", "confidence": 0.7245443165302277}]}], "abstractContent": [{"text": "Automatically produced texts (e.g. translations or summaries) are usually evaluated with n-gram based measures such as BLEU or ROUGE, while the wide set of more sophisticated measures that have been proposed in the last years remains largely ignored for practical purposes.", "labels": [], "entities": [{"text": "Automatically produced texts (e.g. translations or summaries)", "start_pos": 0, "end_pos": 61, "type": "TASK", "confidence": 0.5722565584712558}, {"text": "BLEU", "start_pos": 119, "end_pos": 123, "type": "METRIC", "confidence": 0.9989079236984253}, {"text": "ROUGE", "start_pos": 127, "end_pos": 132, "type": "METRIC", "confidence": 0.9880644679069519}]}, {"text": "In this paper we first present an in-depth analysis of the state of the art in order to clarify this issue.", "labels": [], "entities": []}, {"text": "After this, we formalize and verify empirically a set of properties that every text evaluation measure based on similarity to human-produced references satisfies.", "labels": [], "entities": []}, {"text": "These properties imply that corroborating system improvements with additional measures always increases the overall reliability of the evaluation process.", "labels": [], "entities": []}, {"text": "In addition, the greater the heterogeneity of the measures (which is measurable) the higher their combined reliability.", "labels": [], "entities": [{"text": "reliability", "start_pos": 107, "end_pos": 118, "type": "METRIC", "confidence": 0.991392970085144}]}, {"text": "These results support the use of heterogeneous measures in order to consolidate text evaluation results.", "labels": [], "entities": []}], "introductionContent": [{"text": "The automatic evaluation of textual outputs is a core issue in many Natural Language Processing (NLP) tasks such as Natural Language Generation, Machine Translation (MT) and Automatic Summarization (AS).", "labels": [], "entities": [{"text": "Natural Language Generation", "start_pos": 116, "end_pos": 143, "type": "TASK", "confidence": 0.6392861704031626}, {"text": "Machine Translation (MT)", "start_pos": 145, "end_pos": 169, "type": "TASK", "confidence": 0.8554888010025025}, {"text": "Automatic Summarization (AS)", "start_pos": 174, "end_pos": 202, "type": "TASK", "confidence": 0.8481465578079224}]}, {"text": "State-of-the-art automatic evaluation methods all operate by rewarding similarities between automatically-produced candidate outputs and manually-produced reference solutions, socalled human references or models.", "labels": [], "entities": []}, {"text": "Over the last decade, a wide variety of measures, based on different quality assumptions, have been proposed.", "labels": [], "entities": []}, {"text": "Recent work suggests exploiting external knowledge sources and/or deep linguistic annotation, and measure combination (see Section 2).", "labels": [], "entities": []}, {"text": "However, original measures based on lexical matching, such as BLEU () and ROUGE) are still preferred as de facto standards in MT and AS, respectively.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 62, "end_pos": 66, "type": "METRIC", "confidence": 0.9989232420921326}, {"text": "ROUGE", "start_pos": 74, "end_pos": 79, "type": "METRIC", "confidence": 0.9844611287117004}, {"text": "MT", "start_pos": 126, "end_pos": 128, "type": "TASK", "confidence": 0.8061501383781433}]}, {"text": "There are, in our opinion, two main reasons behind this fact.", "labels": [], "entities": []}, {"text": "First, the use of a common measure certainly allows researchers to carryout objective comparisons between their work and other published results.", "labels": [], "entities": []}, {"text": "Second, the advantages of novel measures are not easy to demonstrate in terms of correlation with human judgements.", "labels": [], "entities": []}, {"text": "Our goal is not to answer which is the most reliable metric or to propose yet another novel measure.", "labels": [], "entities": []}, {"text": "Rather than this, we first analyze in depth the state of the art, concluding that it is not easy to determine the reliability of a measure.", "labels": [], "entities": [{"text": "reliability", "start_pos": 114, "end_pos": 125, "type": "METRIC", "confidence": 0.98124760389328}]}, {"text": "In absence of a clear proof of the advantages of novel measures, system developers naturally tend to prefer well-known standard measures.", "labels": [], "entities": []}, {"text": "Second, we formalize and check empirically two intrinsic properties that any evaluation measure based on similarity to human-produced references satisfies.", "labels": [], "entities": []}, {"text": "Assuming that a measure satisfies a set of basic formal constraints, these properties imply that corroborating a system comparison with additional measures always increases the overall reliability of the evaluation process, even when the added measures have a low correlation with human judgements.", "labels": [], "entities": []}, {"text": "In most papers, evaluation results are corroborated with similar n-gram based measures (eg. BLEU and ROUGE).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 92, "end_pos": 96, "type": "METRIC", "confidence": 0.9982134103775024}, {"text": "ROUGE", "start_pos": 101, "end_pos": 106, "type": "METRIC", "confidence": 0.99457186460495}]}, {"text": "However, according to our second property, the greater the heterogeneity of 455 the measures (which is measurable) the higher their reliability.", "labels": [], "entities": [{"text": "reliability", "start_pos": 132, "end_pos": 143, "type": "METRIC", "confidence": 0.9931614398956299}]}, {"text": "The practical implication is that, corroborating evaluation results with measures based on higher linguistic levels increases the heterogeneity, and therefore, the reliability of evaluation results.", "labels": [], "entities": [{"text": "reliability", "start_pos": 164, "end_pos": 175, "type": "METRIC", "confidence": 0.9780983924865723}]}], "datasetContent": [{"text": "Meta-evaluation methods have been gradually introduced together with evaluation measures.", "labels": [], "entities": []}, {"text": "For instance, evaluated the reliability of the BLEU metric according to its ability to emulate human assessors, as measured in terms of Pearson correlation with human assessments of adequacy and fluency at the document level.", "labels": [], "entities": [{"text": "reliability", "start_pos": 28, "end_pos": 39, "type": "METRIC", "confidence": 0.9813389778137207}, {"text": "BLEU", "start_pos": 47, "end_pos": 51, "type": "METRIC", "confidence": 0.9568556547164917}, {"text": "Pearson correlation", "start_pos": 136, "end_pos": 155, "type": "METRIC", "confidence": 0.9672589302062988}]}, {"text": "The measure NIST) was meta-evaluated also in terms of correlation with human assessments, but over different document sources and fora varying number of references and segment sizes.", "labels": [], "entities": []}, {"text": "argued, at the time of introducing the GTM metric, that Pearson correlation coefficients can be affected by scale properties.", "labels": [], "entities": []}, {"text": "They suggested using the non-parametric Spearman correlation coefficients instead.", "labels": [], "entities": []}, {"text": "Lin and Och meta-evaluated ROUGE over both Pearson and Spearman correlation over a wide set of metrics, including NIST, WER, PER, and variants of ROUGE, BLEU and GTM.", "labels": [], "entities": [{"text": "NIST", "start_pos": 114, "end_pos": 118, "type": "DATASET", "confidence": 0.8364493250846863}, {"text": "WER", "start_pos": 120, "end_pos": 123, "type": "METRIC", "confidence": 0.9164696335792542}, {"text": "PER", "start_pos": 125, "end_pos": 128, "type": "METRIC", "confidence": 0.969394862651825}, {"text": "BLEU", "start_pos": 153, "end_pos": 157, "type": "METRIC", "confidence": 0.9956058859825134}]}, {"text": "They obtained similar results in both cases.", "labels": [], "entities": []}, {"text": "argued that the reliability of metrics at the document level can be due to averaging effects but might not be robust across sentence translations.", "labels": [], "entities": []}, {"text": "In order to address this issue, they computed the translation-bytranslation correlation with human assessments (i.e., correlation at the sentence level).", "labels": [], "entities": []}, {"text": "However, correlation with human judgements is not enough to determine the reliability of measures.", "labels": [], "entities": [{"text": "reliability", "start_pos": 74, "end_pos": 85, "type": "METRIC", "confidence": 0.984478235244751}]}, {"text": "First, correlation at sentence level (unlike correlation at system level) tends to below and difficult to interpret.", "labels": [], "entities": []}, {"text": "Second, correlation at system and segment levels can produce contradictory results.", "labels": [], "entities": []}, {"text": "In) it is observed that higher linguistic levels in measures increases the correlation with human judgements at the system level at the cost of correlation at the segment level.", "labels": [], "entities": []}, {"text": "As far as we know, a clear explanation for these phenomena has not been provided yet.", "labels": [], "entities": []}, {"text": "Third, a high correlation at system level does not ensure a high reliability.", "labels": [], "entities": [{"text": "correlation", "start_pos": 14, "end_pos": 25, "type": "METRIC", "confidence": 0.9677339196205139}, {"text": "reliability", "start_pos": 65, "end_pos": 76, "type": "METRIC", "confidence": 0.9604043960571289}]}, {"text": "Culy and Rieheman observed that, although BLEU can achieve a high correlation at system level in some test suites, it over-scores a poor automatic translation of \"Tom Sawyer\" against a human produced translation (Culy and.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 42, "end_pos": 46, "type": "METRIC", "confidence": 0.9969236254692078}, {"text": "translation of \"Tom Sawyer\"", "start_pos": 147, "end_pos": 174, "type": "TASK", "confidence": 0.5134845525026321}]}, {"text": "This meta-evaluation criterion based on the ability to discern between manual and automatic translations have been referred to as human likeness), in contrast to correlation with human judgements which is referred to as human acceptability.", "labels": [], "entities": []}, {"text": "Examples of metameasures based on this criterion are ORANGE ().", "labels": [], "entities": [{"text": "ORANGE", "start_pos": 53, "end_pos": 59, "type": "METRIC", "confidence": 0.9916183352470398}]}, {"text": "In addition, many of the approaches to metric combination described in Section 2.2 take human likeness as the optimization criterion).", "labels": [], "entities": [{"text": "metric combination", "start_pos": 39, "end_pos": 57, "type": "TASK", "confidence": 0.8786981999874115}]}, {"text": "The main advantage of meta-evaluation based on human likeness is that, since human assessments are not required, metrics can be evaluated over larger test beds.", "labels": [], "entities": []}, {"text": "However, the meta-evaluation in terms of human likeness is difficult to interpret.", "labels": [], "entities": []}, {"text": "In general, the state of the art includes a wide set of results that show the drawbacks of n-gram based measures as BLEU, and a wide set of proposals for new single and combined measures which are metaevaluated in terms of human acceptability (i.e., their ability to emulate human judges, typically measured in terms of correlation with human judgements) or human-likeness (i.e., their ability to discern between automatic and human translations)).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 116, "end_pos": 120, "type": "METRIC", "confidence": 0.994059681892395}]}, {"text": "However, the original measures BLEU and ROUGE are still preferred.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 31, "end_pos": 35, "type": "METRIC", "confidence": 0.9992893934249878}, {"text": "ROUGE", "start_pos": 40, "end_pos": 45, "type": "METRIC", "confidence": 0.9978231191635132}]}, {"text": "We believe that one of the reasons is the lack of an in-depth study onto what extent providing additional evaluation results with other metrics contributes to the reliability of such results.", "labels": [], "entities": []}, {"text": "The state of the art suggests that the use of heterogeneous measures can improve the evaluation reliability.", "labels": [], "entities": []}, {"text": "However, as far as we know, there is no comprehensive analysis on the contribution of novel measures when corroborating evaluation results with additional measures.", "labels": [], "entities": []}, {"text": "In general, automatic evaluation measures applied in tasks like MT or AS are similarity measures between system outputs and human references.", "labels": [], "entities": [{"text": "MT or AS", "start_pos": 64, "end_pos": 72, "type": "TASK", "confidence": 0.7273040612538656}]}, {"text": "These measures are related with precision, recall or overlap over specific types of linguistic units.", "labels": [], "entities": [{"text": "precision", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.999660849571228}, {"text": "recall", "start_pos": 43, "end_pos": 49, "type": "METRIC", "confidence": 0.9993625283241272}]}, {"text": "For instance, ROUGE measures n-gram recall.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 14, "end_pos": 19, "type": "METRIC", "confidence": 0.9848213791847229}, {"text": "recall", "start_pos": 36, "end_pos": 42, "type": "METRIC", "confidence": 0.9885970950126648}]}, {"text": "Other measures that work at higher linguistic levels apply precision, recall or overlap of linguistic components such as dependency relations, grammatical categories, semantic roles, etc.", "labels": [], "entities": [{"text": "precision", "start_pos": 59, "end_pos": 68, "type": "METRIC", "confidence": 0.999257504940033}, {"text": "recall", "start_pos": 70, "end_pos": 76, "type": "METRIC", "confidence": 0.9989523887634277}]}, {"text": "In order to delimit our hypothesis, let us first define what is a similarity measure in this context.", "labels": [], "entities": []}, {"text": "Unfortunately, as far as we know, there is no formal concept covering the properties of current evaluation similarity measures.", "labels": [], "entities": []}, {"text": "A close concept is that of \"metric\" or \"distance function\".", "labels": [], "entities": []}, {"text": "But, actually, measures such as ROUGE or BLEU are not proper \"metrics\", because they do not satisfy the symmetry and the triangle inequality properties.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 32, "end_pos": 37, "type": "METRIC", "confidence": 0.9899926781654358}, {"text": "BLEU", "start_pos": 41, "end_pos": 45, "type": "METRIC", "confidence": 0.9920157194137573}]}, {"text": "Therefore, we need anew definition.", "labels": [], "entities": []}, {"text": "Being \u2126 the universe of system outputs sand gold-standards g, we assume that a similarity measure, in our context, is a function x : \u2126 2 \u2212\u2192 such that there exists a decomposition function f : \u2126 \u2212\u2192 {e 1 ..e n } (e.g., words or other linguistic units or relationships) satisfying the following constraints: (i) maximum similarity is achieved only when then the decomposition of the system output resembles exactly the gold-standard decomposition; and (ii) growing overlap or removing non overlapped elements implies growing x.", "labels": [], "entities": []}, {"text": "Formally, if x ranges from 0 to 1: For instance, a random function and the reversal of a similarity funtion (f (s) = 1 f (s) ) do not satisfy these constraints.", "labels": [], "entities": []}, {"text": "While the F measure over Precision and Recall satisfies these constraints 1 , precision and recall in isolation do not satisfy all of them: maximum recall can be achieved without resembling the goldstandard text decomposition; and maximum precision can be achieved with only a few overlapped elements.", "labels": [], "entities": [{"text": "F measure", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9773451387882233}, {"text": "precision", "start_pos": 78, "end_pos": 87, "type": "METRIC", "confidence": 0.9993330836296082}, {"text": "recall", "start_pos": 92, "end_pos": 98, "type": "METRIC", "confidence": 0.9985976815223694}, {"text": "recall", "start_pos": 148, "end_pos": 154, "type": "METRIC", "confidence": 0.993861198425293}, {"text": "precision", "start_pos": 239, "end_pos": 248, "type": "METRIC", "confidence": 0.9949663281440735}]}, {"text": "BLEU () computes the ngram precision while the metric ROUGE () computes the n-gram recall.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9866546392440796}, {"text": "ngram", "start_pos": 21, "end_pos": 26, "type": "METRIC", "confidence": 0.8844360709190369}, {"text": "precision", "start_pos": 27, "end_pos": 36, "type": "METRIC", "confidence": 0.5217535495758057}, {"text": "ROUGE", "start_pos": 54, "end_pos": 59, "type": "METRIC", "confidence": 0.9759130477905273}, {"text": "recall", "start_pos": 83, "end_pos": 89, "type": "METRIC", "confidence": 0.9897215962409973}]}, {"text": "However, in general, both metrics satisfy all the constraints, given that BLEU includes a brevity penalty and ROUGE penalizes or limits the system output length.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 74, "end_pos": 78, "type": "METRIC", "confidence": 0.9987051486968994}, {"text": "ROUGE", "start_pos": 110, "end_pos": 115, "type": "METRIC", "confidence": 0.9947046637535095}]}, {"text": "The measure METEOR creates an alignment between the two strings ().", "labels": [], "entities": [{"text": "METEOR", "start_pos": 12, "end_pos": 18, "type": "METRIC", "confidence": 0.969968318939209}]}, {"text": "This overlap-based measure satisfies also the previous constraints.", "labels": [], "entities": []}, {"text": "Measures based on edit distance over n-grams () or other linguistic units () match also our definition of similarity measure.", "labels": [], "entities": []}, {"text": "The editing distance is minimum when the two compared text are equal.", "labels": [], "entities": [{"text": "editing distance", "start_pos": 4, "end_pos": 20, "type": "METRIC", "confidence": 0.8959398567676544}]}, {"text": "The more the evaluated text contains elements from the gold-standard the more the editing distance is reduced (higher similarity).", "labels": [], "entities": [{"text": "similarity", "start_pos": 118, "end_pos": 128, "type": "METRIC", "confidence": 0.9831478595733643}]}, {"text": "The word ordering can be also expressed in terms of a decomposition function.", "labels": [], "entities": []}, {"text": "A similar reasoning applies to every relevant measure in the state-of-the art.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Description of the test beds from 2004 and 2005 NIST MT evaluation campaigns used in the experiments  throughout the paper.", "labels": [], "entities": [{"text": "NIST MT evaluation", "start_pos": 58, "end_pos": 76, "type": "TASK", "confidence": 0.5742691953976949}]}, {"text": " Table 2: Description of the test beds from 2005 and 2006 DUC evaluation campaigns used in the experiments through- out the paper.", "labels": [], "entities": [{"text": "DUC", "start_pos": 58, "end_pos": 61, "type": "DATASET", "confidence": 0.8598194122314453}]}]}