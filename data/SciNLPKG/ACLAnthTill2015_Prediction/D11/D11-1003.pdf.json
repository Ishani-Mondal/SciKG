{"title": [{"text": "Exact Decoding of Phrase-Based Translation Models through Lagrangian Relaxation", "labels": [], "entities": [{"text": "Phrase-Based Translation", "start_pos": 18, "end_pos": 42, "type": "TASK", "confidence": 0.8455950021743774}]}], "abstractContent": [{"text": "This paper describes an algorithm for exact decoding of phrase-based translation models, based on Lagrangian relaxation.", "labels": [], "entities": [{"text": "exact decoding of phrase-based translation", "start_pos": 38, "end_pos": 80, "type": "TASK", "confidence": 0.5415038585662841}]}, {"text": "The method recovers exact solutions, with certificates of optimality, on over 99% of test examples.", "labels": [], "entities": []}, {"text": "The method is much more efficient than approaches based on linear programming (LP) or integer linear programming (ILP) solvers: these methods are not feasible for anything other than short sentences.", "labels": [], "entities": [{"text": "integer linear programming (ILP) solvers", "start_pos": 86, "end_pos": 126, "type": "TASK", "confidence": 0.6300602895872933}]}, {"text": "We compare our method to MOSES (Koehn et al., 2007), and give precise estimates of the number and magnitude of search errors that MOSES makes.", "labels": [], "entities": [{"text": "MOSES", "start_pos": 25, "end_pos": 30, "type": "DATASET", "confidence": 0.7523499727249146}]}], "introductionContent": [{"text": "Phrase-based models) area widely-used approach for statistical machine translation.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 51, "end_pos": 82, "type": "TASK", "confidence": 0.7473957439263662}]}, {"text": "The decoding problem for phrase-based models is NPhard ; because of this, previous work has generally focused on approximate search methods, for example variants of beam search, for decoding.", "labels": [], "entities": []}, {"text": "This paper describes an algorithm for exact decoding of phrase-based models, based on Lagrangian relaxation.", "labels": [], "entities": []}, {"text": "The core of the algorithm is a dynamic program for phrasebased translation which is efficient, but which allows some ill-formed translations.", "labels": [], "entities": [{"text": "phrasebased translation", "start_pos": 51, "end_pos": 74, "type": "TASK", "confidence": 0.8243441581726074}]}, {"text": "More specifically, the dynamic program searches over the space of translations where exactly N words are translated (N is the number of words in the source-language sentence), but where some source-language words maybe translated zero times, or some source-language words maybe translated more than once.", "labels": [], "entities": []}, {"text": "Lagrangian relaxation is used to enforce the constraint We refer hereto the phrase-based models of (, considered in this paper.", "labels": [], "entities": []}, {"text": "Other variants of phrase-based models, which allow polynomial time decoding, have been proposed, seethe related work section. that each source-language word should be translated exactly once.", "labels": [], "entities": []}, {"text": "A subgradient algorithm is used to optimize the dual problem arising from the relaxation.", "labels": [], "entities": []}, {"text": "The first technical contribution of this paper is the basic Lagrangian relaxation algorithm.", "labels": [], "entities": []}, {"text": "By the usual guarantees for Lagrangian relaxation, if this algorithm converges to a solution where all constraints are satisfied (i.e., where each word is translated exactly once), then the solution is guaranteed to be optimal.", "labels": [], "entities": [{"text": "Lagrangian relaxation", "start_pos": 28, "end_pos": 49, "type": "TASK", "confidence": 0.6477174460887909}]}, {"text": "For some source-language sentences however, the underlying relaxation is loose, and the algorithm will not converge.", "labels": [], "entities": []}, {"text": "The second technical contribution of this paper is a method that incrementally adds constraints to the underlying dynamic program, thereby tightening the relaxation until an exact solution is recovered.", "labels": [], "entities": []}, {"text": "We describe experiments on translation from German to English, using phrase-based models trained by MOSES ().", "labels": [], "entities": [{"text": "translation from German to English", "start_pos": 27, "end_pos": 61, "type": "TASK", "confidence": 0.8812865972518921}, {"text": "MOSES", "start_pos": 100, "end_pos": 105, "type": "DATASET", "confidence": 0.8019925355911255}]}, {"text": "The method recovers exact solutions, with certificates of optimality, on over 99% of test examples.", "labels": [], "entities": []}, {"text": "On over 78% of examples, the method converges with zero added constraints (i.e., using the basic algorithm); 99.67% of all examples converge with 9 or fewer constraints.", "labels": [], "entities": []}, {"text": "We compare to a linear programming (LP)/integer linear programming (ILP) based decoder.", "labels": [], "entities": []}, {"text": "Our method is much more efficient: LP or ILP decoding is not feasible for anything other than short sentences, 2 whereas the average decoding time for our method (for sentences of length 1-50 words) is 121 seconds per sentence.", "labels": [], "entities": []}, {"text": "We also compare our method to MOSES, and give precise estimates of the number and magnitude of search errors that MOSES makes.", "labels": [], "entities": [{"text": "MOSES", "start_pos": 30, "end_pos": 35, "type": "DATASET", "confidence": 0.8045526742935181}]}, {"text": "Even with large beam sizes, MOSES makes a significant number of search errors.", "labels": [], "entities": [{"text": "MOSES", "start_pos": 28, "end_pos": 33, "type": "DATASET", "confidence": 0.5516508221626282}]}, {"text": "As far as we are aware, previous work has not successfully re-covered exact solutions for the type of phrase-based models used in MOSES.", "labels": [], "entities": [{"text": "MOSES", "start_pos": 130, "end_pos": 135, "type": "TASK", "confidence": 0.7225230932235718}]}], "datasetContent": [{"text": "In this section, we present experimental results to demonstrate the efficiency of the decoding algorithm.", "labels": [], "entities": []}, {"text": "We compare to MOSES (), a phrase-based decoder using beam search, and to a general purpose integer linear programming (ILP) solver, which solves the problem exactly.", "labels": [], "entities": [{"text": "integer linear programming (ILP) solver", "start_pos": 91, "end_pos": 130, "type": "TASK", "confidence": 0.6225475072860718}]}, {"text": "The experiments focus on translation from German to English, using the Europarl data).", "labels": [], "entities": [{"text": "translation from German to English", "start_pos": 25, "end_pos": 59, "type": "TASK", "confidence": 0.860521936416626}, {"text": "Europarl data", "start_pos": 71, "end_pos": 84, "type": "DATASET", "confidence": 0.9935752749443054}]}, {"text": "We tested on 1,824 sentences of length at most 50 words.", "labels": [], "entities": []}, {"text": "The experiments use the algorithm shown in.", "labels": [], "entities": []}, {"text": "We limit the algorithm to a maximum of 250 iterations and a maximum of 9 hard constraints.", "labels": [], "entities": []}, {"text": "The distortion limit dis set to be four, and we prune the phrase translation table to have 10 English phrases per German phrase.", "labels": [], "entities": [{"text": "phrase translation", "start_pos": 58, "end_pos": 76, "type": "TASK", "confidence": 0.7378728687763214}]}, {"text": "Our method finds exact solutions on 1,818 out of 1,824 sentences (99.67%).", "labels": [], "entities": []}, {"text": "(6 examples do not converge within 250 iterations.) shows the number of iterations required for convergence, and shows the number of constraints required for convergence, broken down by sentence length.", "labels": [], "entities": []}, {"text": "In 1,436/1,818 (78.7%) sentences, the method converges without adding hard constraints to tighten the relaxation.", "labels": [], "entities": []}, {"text": "For sentences with 1-10 words, the vast majority (183 out of 185 examples) converge with 0 constraints added.", "labels": [], "entities": []}, {"text": "As sentences get longer, more constraints are often required.", "labels": [], "entities": []}, {"text": "However most examples converge with 9 or fewer constraints.", "labels": [], "entities": []}, {"text": "shows the average times for decoding, broken down by sentence length, and by the number of constraints that are added.", "labels": [], "entities": []}, {"text": "As expected, decoding times increase as the length of sentences, and the number of constraints required, increase.", "labels": [], "entities": []}, {"text": "The average run time across all sentences is 120.9 seconds.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Table showing the number of iterations taken for the algorithm to converge. x indicates sentences that fail to  converge after 250 iterations. 97% of the examples converge within 120 iterations.", "labels": [], "entities": []}, {"text": " Table 3: The average time (in seconds) for decoding using the algorithm in Figure 3, with and without A* algorithm, broken down", "labels": [], "entities": []}, {"text": " Table 4: Average and median time of the LP/ILP solver (in", "labels": [], "entities": [{"text": "Average", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9975134134292603}, {"text": "LP/ILP solver", "start_pos": 41, "end_pos": 54, "type": "TASK", "confidence": 0.5008286908268929}]}, {"text": " Table 5: Table showing the number of examples where", "labels": [], "entities": []}, {"text": " Table 6: Table showing statistics for the difference between the", "labels": [], "entities": []}, {"text": " Table 7: BLEU score comparisons. We consider only  those sentences where both decoders produce a transla- tion.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9974943399429321}]}]}