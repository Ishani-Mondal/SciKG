{"title": [], "abstractContent": [{"text": "Conversations provide rich opportunities for interactive, continuous learning.", "labels": [], "entities": []}, {"text": "When something goes wrong, a system can ask for clarification , rewording, or otherwise redirect the interaction to achieve its goals.", "labels": [], "entities": []}, {"text": "In this paper , we present an approach for using conversational interactions of this type to induce semantic parsers.", "labels": [], "entities": []}, {"text": "We demonstrate learning without any explicit annotation of the meanings of user utterances.", "labels": [], "entities": []}, {"text": "Instead, we model meaning with latent variables, and introduce a loss function to measure how well potential meanings match the conversation.", "labels": [], "entities": []}, {"text": "This loss drives the overall learning approach, which induces a weighted CCG grammar that could be used to automatically bootstrap the semantic analysis component in a complete dialog system.", "labels": [], "entities": []}, {"text": "Experiments on DARPA Communicator conversational logs demonstrate effective learning, despite requiring no explicit meaning annotations.", "labels": [], "entities": [{"text": "DARPA Communicator conversational logs", "start_pos": 15, "end_pos": 53, "type": "DATASET", "confidence": 0.7493712902069092}]}], "introductionContent": [{"text": "Conversational interactions provide significant opportunities for autonomous learning.", "labels": [], "entities": []}, {"text": "A well-defined goal allows a system to engage in remediations when confused, such as asking for clarification, rewording, or additional explanation.", "labels": [], "entities": []}, {"text": "The user's response to such requests provides a strong, if often indirect, signal that can be used to learn to avoid the original confusion in future conversations.", "labels": [], "entities": []}, {"text": "In this paper, we show how to use this type of conversational feedback to learn to better recover the meaning of user utterances, by inducing semantic parsers from unannotated conversational logs.", "labels": [], "entities": []}, {"text": "We believe that this style of learning will contribute to the long term goal of building self-improving dialog systems that continually learn from their mistakes, with little or no human intervention.", "labels": [], "entities": []}, {"text": "Many dialog systems use a semantic parsing component to analyze user utterances (e.g.,.", "labels": [], "entities": []}, {"text": "For example, in a flight booking system, the sentence Sent: I want to go to Seattle on Friday LF: \u03bbx.to(x, SEA) \u2227 date(x, F RI) might be mapped to the logical form (LF) meaning representation above, a lambda-calculus expression defining the set of flights that match the user's desired constraints.", "labels": [], "entities": [{"text": "Sent: I want to go to Seattle on Friday LF: \u03bbx.to(x, SEA) \u2227 date(x, F RI)", "start_pos": 54, "end_pos": 127, "type": "METRIC", "confidence": 0.6336820079730108}]}, {"text": "This LF is a representation of the semantic content that comes from the sentence, and would be input to a context-dependent understanding component in a full dialog system, for example to find the date that the symbol F RI refers to.", "labels": [], "entities": []}, {"text": "To induce semantic parsers from interactions, we consider user statements in conversational logs and model their meaning with latent variables.", "labels": [], "entities": []}, {"text": "We demonstrate that it is often possible to use the dialog that follows a statement (including remediations such as clarifications, simplifications, etc.) to learn the meaning of the original sentence.", "labels": [], "entities": []}, {"text": "For example, consider the first user utterance in, where the system failed to understand the user's request.", "labels": [], "entities": []}, {"text": "To complete the task, the system must use a remediation strategy.", "labels": [], "entities": []}, {"text": "Here, it takes the initiative by asking for and confirming each flight constraint in turn.", "labels": [], "entities": []}, {"text": "This strategy produces an unnatural conversation but provides supervision for learning the meaning of the original utterance.", "labels": [], "entities": []}, {"text": "We can easily record representations of the meanings the system intended to convey at each step, as seen in, and use this indirect supervision for learning.", "labels": [], "entities": []}, {"text": "Learning from this weak signal is challenging.", "labels": [], "entities": []}, {"text": "In any specific conversation, the system's remediations can fail to recover aspects of the original user meaning and can introduce spurious constraints, for example when users change their goals mid conversation.", "labels": [], "entities": []}, {"text": "To learn effectively, the model must accumulate evidence from many interactions to best recover the meaning of each specific sentence.", "labels": [], "entities": []}, {"text": "We will learn semantic parsers defined by probabilistic Combinatory Categorial Grammars (PCCGs), which include both a lexicon and a weighted linear model for parse selection.", "labels": [], "entities": [{"text": "parse selection", "start_pos": 158, "end_pos": 173, "type": "TASK", "confidence": 0.8572731018066406}]}, {"text": "The lexicon specifies the meanings of individual words and phrases, while the parameters of a parsing model define how to best combine word-and phrase-level meanings to analyze complete sentences.", "labels": [], "entities": []}, {"text": "To learn without labeled meaning representations, we make use of a variant of the loss-sensitive Perceptron algorithm.", "labels": [], "entities": []}, {"text": "We define loss functions to provide a rough measure of (1) how well a candidate meaning fora utterance matches the conversation that follows it and (2) how well the candidate matches our expectations about the types of things that are often said in the dialog's domain.", "labels": [], "entities": []}, {"text": "These notions of loss drive not only the parameter estimation but also the grammar induction process that constructs the CCG lexicon.", "labels": [], "entities": []}, {"text": "Experiments on conversation logs from the DARPA Communicator corpus () demonstrate the feasibility of our approach.", "labels": [], "entities": [{"text": "DARPA Communicator corpus", "start_pos": 42, "end_pos": 67, "type": "DATASET", "confidence": 0.8872108658154806}]}, {"text": "This paper makes the following contributions: \u2022 A formalization of the problem of learning the meaning of user statements from conversational feedback, without requiring annotation.", "labels": [], "entities": [{"text": "learning the meaning of user statements from conversational feedback", "start_pos": 82, "end_pos": 150, "type": "TASK", "confidence": 0.7379966610007815}]}, {"text": "\u2022 A new loss-sensitive learning algorithm for this problem that induces semantic parsers from conversation logs.", "labels": [], "entities": []}, {"text": "\u2022 Loss functions to measure the quality of hypothetical utterance meanings within the conversation in which they appear.", "labels": [], "entities": []}, {"text": "\u2022 An evaluation on logs from two dialog systems SYSTEM: how can i help you?", "labels": [], "entities": []}, {"text": "(OPEN TASK) USER: i would like to fly from atlanta georgia to london england on september twenty fourth in the early evening i would like to return on october first departing from london in the late morning SYSTEM: leaving what city?", "labels": [], "entities": [{"text": "OPEN TASK)", "start_pos": 1, "end_pos": 11, "type": "METRIC", "confidence": 0.8127152522404989}, {"text": "USER", "start_pos": 12, "end_pos": 16, "type": "METRIC", "confidence": 0.7078274488449097}]}, {"text": "(ASK:\u03bbx.f rom(f l, x)) USER: atlanta georgia SYSTEM: leaving atlanta.", "labels": [], "entities": [{"text": "USER", "start_pos": 23, "end_pos": 27, "type": "METRIC", "confidence": 0.6219714283943176}]}, {"text": "(CONFIRM:f rom(f l, AT L)) going to which city?", "labels": [], "entities": [{"text": "CONFIRM", "start_pos": 1, "end_pos": 8, "type": "METRIC", "confidence": 0.4956880509853363}, {"text": "AT L", "start_pos": 20, "end_pos": 24, "type": "METRIC", "confidence": 0.9526981115341187}]}, {"text": "(ASK:\u03bbx.to(f l, x)) USER: london SYSTEM: arriving in london england.", "labels": [], "entities": [{"text": "USER", "start_pos": 20, "end_pos": 24, "type": "METRIC", "confidence": 0.9770470261573792}]}, {"text": "(CONFIRM:to(f l, LON )) what date would you like to depart atlanta?", "labels": [], "entities": [{"text": "CONFIRM", "start_pos": 1, "end_pos": 8, "type": "METRIC", "confidence": 0.5728824138641357}, {"text": "LON", "start_pos": 17, "end_pos": 20, "type": "METRIC", "confidence": 0.7352332472801208}]}, {"text": "(ASK:\u03bbx.f rom(f l, AT L) \u2227 departdate(f l, x)) USER: september twenty fourth in the early evening [conversation continues]: Conversational excerpt from a DARPA Communicator travel-planning dialog.", "labels": [], "entities": [{"text": "AT L)", "start_pos": 19, "end_pos": 24, "type": "METRIC", "confidence": 0.9585853219032288}, {"text": "departdate", "start_pos": 27, "end_pos": 37, "type": "METRIC", "confidence": 0.48779967427253723}, {"text": "USER", "start_pos": 47, "end_pos": 51, "type": "METRIC", "confidence": 0.3966818153858185}]}, {"text": "Each system statement is labeled with representations of its speech act and logical meaning, in parentheses.", "labels": [], "entities": []}, {"text": "The user utterances have no labels.", "labels": [], "entities": []}, {"text": "Conversations of this type provide the training data to learn semantic parsers for user utterances. that demonstrate effective learning from conversations alone.", "labels": [], "entities": []}], "datasetContent": [{"text": "This section describes our experimental setup and comparisons.", "labels": [], "entities": []}, {"text": "We follow the setup of Zettlemoyer and Collins where possible, including feature design, initialization of the semantic parser, and evaluation metrics, as reviewed below.", "labels": [], "entities": [{"text": "feature design", "start_pos": 73, "end_pos": 87, "type": "TASK", "confidence": 0.723479226231575}]}, {"text": "Features and Parser The features include indicators for lexical item use, properties of the logical form that is being constructed, and indicators for parsing operators used to build the tree.", "labels": [], "entities": [{"text": "Parser", "start_pos": 13, "end_pos": 19, "type": "METRIC", "confidence": 0.9465681314468384}]}, {"text": "The parser attempts to boost recall with a two-pass strategy that allows for word skipping if the initial parse fails.", "labels": [], "entities": [{"text": "recall", "start_pos": 29, "end_pos": 35, "type": "METRIC", "confidence": 0.9978598952293396}, {"text": "word skipping", "start_pos": 77, "end_pos": 90, "type": "TASK", "confidence": 0.6905579417943954}]}], "tableCaptions": [{"text": " Table 1: Data set statistics for Lucent and BBN systems.", "labels": [], "entities": []}, {"text": " Table 2: Mean exact-match results for cross fold evaluation on the development sets.", "labels": [], "entities": [{"text": "exact-match", "start_pos": 15, "end_pos": 26, "type": "METRIC", "confidence": 0.9623289704322815}]}, {"text": " Table 3: Exact-and partial-match results on the test sets.", "labels": [], "entities": []}]}