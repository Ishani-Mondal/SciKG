{"title": [{"text": "Training a Parser for Machine Translation Reordering", "labels": [], "entities": [{"text": "Machine Translation Reordering", "start_pos": 22, "end_pos": 52, "type": "TASK", "confidence": 0.8813076615333557}]}], "abstractContent": [{"text": "We propose a simple training regime that can improve the extrinsic performance of a parser, given only a corpus of sentences and away to automatically evaluate the extrinsic quality of a candidate parse.", "labels": [], "entities": []}, {"text": "We apply our method to train parsers that excel when used as part of a reordering component in a statistical machine translation system.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 97, "end_pos": 128, "type": "TASK", "confidence": 0.6174685259660085}]}, {"text": "We use a corpus of weakly-labeled reference reorderings to guide parser training.", "labels": [], "entities": []}, {"text": "Our best parsers contribute significant improvements in subjective translation quality while their intrinsic attachment scores typically regress.", "labels": [], "entities": [{"text": "attachment", "start_pos": 109, "end_pos": 119, "type": "METRIC", "confidence": 0.900481641292572}]}], "introductionContent": [{"text": "The field of syntactic parsing has received a great deal of attention and progress since the creation of the Penn Treebank (.", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 13, "end_pos": 30, "type": "TASK", "confidence": 0.849513053894043}, {"text": "Penn Treebank", "start_pos": 109, "end_pos": 122, "type": "DATASET", "confidence": 0.9953181147575378}]}, {"text": "A commonand valid-criticism, however, is that parsers typically get evaluated only on Section 23 of the Wall Street Journal portion of the Penn Treebank.", "labels": [], "entities": [{"text": "Section 23 of the Wall Street Journal portion of the Penn Treebank", "start_pos": 86, "end_pos": 152, "type": "DATASET", "confidence": 0.9076937139034271}]}, {"text": "This is problematic for many reasons.", "labels": [], "entities": []}, {"text": "As previously observed, this test set comes from a very narrow domain that does not necessarily reflect parser performance on text coming from more varied domains), especially web text.", "labels": [], "entities": []}, {"text": "There is also evidence that after so much repeated testing, parsers are indirectly over-fitting to this set (.", "labels": [], "entities": []}, {"text": "Furthermore, parsing was never meant as a stand-alone task, but is rather a means to an end, towards the goal of building systems that can process natural language input.", "labels": [], "entities": [{"text": "parsing", "start_pos": 13, "end_pos": 20, "type": "TASK", "confidence": 0.9744116067886353}]}, {"text": "This is not to say that parsers are not used in larger systems.", "labels": [], "entities": []}, {"text": "All to the contrary, as parsing technology has become more mature, parsers have become efficient and accurate enough to be useful in many natural language processing systems, most notably in machine translation.", "labels": [], "entities": [{"text": "parsing", "start_pos": 24, "end_pos": 31, "type": "TASK", "confidence": 0.9714897274971008}, {"text": "machine translation", "start_pos": 191, "end_pos": 210, "type": "TASK", "confidence": 0.8074585497379303}]}, {"text": "While it has been repeatedly shown that using a parser can bring net gains on downstream application quality, it is often unclear how much intrinsic parsing accuracy actually matters.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 157, "end_pos": 165, "type": "METRIC", "confidence": 0.9018771052360535}]}, {"text": "In this paper we try to shed some light on this issue by comparing different parsers in the context of machine translation (MT).", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 103, "end_pos": 127, "type": "TASK", "confidence": 0.831397008895874}]}, {"text": "We present experiments on translation from English to three Subject-ObjectVerb (SOV) languages, 1 because those require extensive syntactic reordering to produce grammatical translations.", "labels": [], "entities": []}, {"text": "We evaluate parse quality on a number of extrinsic metrics, including word reordering accuracy, BLEU score and a human evaluation of final translation quality.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 86, "end_pos": 94, "type": "METRIC", "confidence": 0.591978132724762}, {"text": "BLEU score", "start_pos": 96, "end_pos": 106, "type": "METRIC", "confidence": 0.9873442947864532}]}, {"text": "We show that while there is a good correlation between those extrinsic metrics, parsing quality as measured on the Penn Treebank is not a good indicator of the final downstream application quality.", "labels": [], "entities": [{"text": "parsing", "start_pos": 80, "end_pos": 87, "type": "TASK", "confidence": 0.9701138138771057}, {"text": "Penn Treebank", "start_pos": 115, "end_pos": 128, "type": "DATASET", "confidence": 0.9960944354534149}]}, {"text": "Since the word reordering metric can be computed efficiently offline (i.e. without the use of the final MT system), we then propose to tune parsers specifically for that metric, with the goal of improving the performance of the overall system.", "labels": [], "entities": []}, {"text": "To this end we propose a simple training regime which we refer to as targeted self-training (Section 2).", "labels": [], "entities": []}, {"text": "Similar to self-training, a baseline model is used to produce predictions on an unlabeled data set.", "labels": [], "entities": []}, {"text": "However, rather than directly training on the output of the baseline model, we generate a list of hypotheses and use an external signal to select the best candidate.", "labels": [], "entities": []}, {"text": "The selected parse trees are added to the training data and the model is then retrained.", "labels": [], "entities": []}, {"text": "The experiments in Section 5 show that this simple procedure noticeably improves our parsers for the task at hand, resulting in significant improvements in downstream translation quality, as measured in a human evaluation on web text.", "labels": [], "entities": []}, {"text": "This idea is similar in vein to and , except that we use an extrinsic quality metric instead of a second parsing model for making the selection.", "labels": [], "entities": []}, {"text": "It is also similar to and, but again avoiding the added complexity introduced by the use of additional (bilingual) models for candidate selection.", "labels": [], "entities": [{"text": "candidate selection", "start_pos": 126, "end_pos": 145, "type": "TASK", "confidence": 0.8143932521343231}]}, {"text": "It should be noted that our extrinsic metric is computed from data that has been manually annotated with reference word reorderings.", "labels": [], "entities": []}, {"text": "Details of the reordering metric and the annotated data we used are given in Sections 3 and 4.", "labels": [], "entities": []}, {"text": "While this annotation requires some effort, such annotations are much easier to obtain than full parse trees.", "labels": [], "entities": []}, {"text": "In our experiments in Section 6 we show that we can obtain similar improvements on downstream translation quality by targeted self-training with weakly labeled data (in form of word reorderings), as with training on the fully labeled data (with full syntactic parse trees).", "labels": [], "entities": []}], "datasetContent": [{"text": "We experimented with parsers trained in three different ways: 1.", "labels": [], "entities": []}, {"text": "Baseline: trained only on WSJ-Train.", "labels": [], "entities": [{"text": "Baseline", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9291602373123169}, {"text": "WSJ-Train", "start_pos": 26, "end_pos": 35, "type": "DATASET", "confidence": 0.9471648335456848}]}], "tableCaptions": [{"text": " Table 1: English\u2192Japanese reordering scores on Web-Test for standard self-training and targeted self-training on  Web-Train. Label \"10x\" indicates that the self-training data was weighted 10x relative to the WSJ training data.  Bolded reordering scores are different from WSJ-only baseline with 95% confidence but are not significantly different  from each other within the same group.", "labels": [], "entities": [{"text": "Web-Test", "start_pos": 48, "end_pos": 56, "type": "DATASET", "confidence": 0.9654032588005066}, {"text": "WSJ training data", "start_pos": 209, "end_pos": 226, "type": "DATASET", "confidence": 0.9005622267723083}]}, {"text": " Table 2: BLEU scores and human evaluation results for translation between three language pairs, varying only the  parser between systems. \"WSJ-only\" corresponds to the baseline WSJ-only shift-reduce parser; \"Targeted\" corre- sponds to the Web-Train targeted self-training 10x shift-reduce parser.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9973064661026001}]}, {"text": " Table 3: Counts on Web-Test of \"click\" tagged as a noun and verb and percentage of sentences parsed imperatively.", "labels": [], "entities": []}, {"text": " Table 4: Reordering and labeled attachment scores on QTB-Test for treebank training and targeted self-training on  QTB-Train.", "labels": [], "entities": [{"text": "Reordering", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9545667171478271}, {"text": "QTB-Test", "start_pos": 54, "end_pos": 62, "type": "DATASET", "confidence": 0.9554492235183716}, {"text": "QTB-Train", "start_pos": 116, "end_pos": 125, "type": "DATASET", "confidence": 0.9482101202011108}]}, {"text": " Table 5: BLEU scores and human evaluation results for English\u2192Japanese translation of the QTB-Test corpus, varying  only the parser between systems between the WSJ-only shift-reduce parser and the QTB-Train targeted self-training  10x shift-reduce parser.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9986503720283508}, {"text": "QTB-Test corpus", "start_pos": 91, "end_pos": 106, "type": "DATASET", "confidence": 0.930468887090683}]}]}