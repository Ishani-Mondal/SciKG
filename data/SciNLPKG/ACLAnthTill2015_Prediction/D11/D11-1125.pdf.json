{"title": [], "abstractContent": [{"text": "We offer a simple, effective, and scalable method for statistical machine translation parameter tuning based on the pairwise approach to ranking (Herbrich et al., 1999).", "labels": [], "entities": [{"text": "statistical machine translation parameter tuning", "start_pos": 54, "end_pos": 102, "type": "TASK", "confidence": 0.822376036643982}]}, {"text": "Unlike the popular MERT algorithm (Och, 2003), our pairwise ranking optimization (PRO) method is not limited to a handful of parameters and can easily handle systems with thousands of features.", "labels": [], "entities": [{"text": "pairwise ranking optimization (PRO)", "start_pos": 51, "end_pos": 86, "type": "TASK", "confidence": 0.6710511495669683}]}, {"text": "Moreover, unlike recent approaches built upon the MIRA algorithm of Crammer and Singer (2003) (Watanabe et al., 2007; Chi-ang et al., 2008b), PRO is easy to implement.", "labels": [], "entities": [{"text": "PRO", "start_pos": 142, "end_pos": 145, "type": "TASK", "confidence": 0.9346321821212769}]}, {"text": "It uses off-the-shelf linear binary classi-fier software and can be built on top of an existing MERT framework in a matter of hours.", "labels": [], "entities": []}, {"text": "We establish PRO's scalability and effectiveness by comparing it to MERT and MIRA and demonstrate parity on both phrase-based and syntax-based systems in a variety of language pairs, using large scale data scenarios.", "labels": [], "entities": [{"text": "PRO", "start_pos": 13, "end_pos": 16, "type": "TASK", "confidence": 0.9413195848464966}, {"text": "MIRA", "start_pos": 77, "end_pos": 81, "type": "METRIC", "confidence": 0.5864566564559937}]}], "introductionContent": [{"text": "The MERT algorithm is currently the most popular way to tune the parameters of a statistical machine translation (MT) system.", "labels": [], "entities": [{"text": "MERT", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.8520671725273132}, {"text": "statistical machine translation (MT)", "start_pos": 81, "end_pos": 117, "type": "TASK", "confidence": 0.7639571378628413}]}, {"text": "MERT is well-understood, easy to implement, and runs quickly, but can behave erratically and does not scale beyond a handful of features.", "labels": [], "entities": []}, {"text": "This lack of scalability is a significant weakness, as it inhibits systems from using more than a couple dozen features to discriminate between candidate translations and stymies feature development innovation.", "labels": [], "entities": []}, {"text": "Several researchers have attempted to address this weakness.", "labels": [], "entities": []}, {"text": "Recently, and have developed tuning methods using the MIRA algorithm) as a nucleus.", "labels": [], "entities": [{"text": "MIRA", "start_pos": 54, "end_pos": 58, "type": "METRIC", "confidence": 0.586470901966095}]}, {"text": "The MIRA technique of Chiang et al. has been shown to perform well on large-scale tasks with hundreds or thousands of features.", "labels": [], "entities": [{"text": "MIRA", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.8393750786781311}]}, {"text": "However, the technique is complex and architecturally quite different from MERT.", "labels": [], "entities": [{"text": "MERT", "start_pos": 75, "end_pos": 79, "type": "DATASET", "confidence": 0.6771555542945862}]}, {"text": "Tellingly, in the entire proceedings of ACL 2010, only one paper describing a statistical MT system cited the use of MIRA for tuning, while 15 used MERT.", "labels": [], "entities": [{"text": "ACL 2010", "start_pos": 40, "end_pos": 48, "type": "DATASET", "confidence": 0.6857571303844452}, {"text": "MT", "start_pos": 90, "end_pos": 92, "type": "TASK", "confidence": 0.9792779088020325}, {"text": "MIRA", "start_pos": 117, "end_pos": 121, "type": "METRIC", "confidence": 0.9841104745864868}, {"text": "MERT", "start_pos": 148, "end_pos": 152, "type": "METRIC", "confidence": 0.9742276668548584}]}, {"text": "Here we propose a simpler approach to tuning that scales similarly to high-dimensional feature spaces.", "labels": [], "entities": []}, {"text": "We cast tuning as a ranking problem, where the explicit goal is to learn to correctly rank candidate translations.", "labels": [], "entities": []}, {"text": "Specifically, we follow the pairwise approach to ranking, in which the ranking problem is reduced to the binary classification task of deciding between candidate translation pairs.", "labels": [], "entities": []}, {"text": "Of primary concern to us is the ease of adoption of our proposed technique.", "labels": [], "entities": []}, {"text": "Because of this, we adhere as closely as possible to the established MERT architecture and use freely available machine learning software.", "labels": [], "entities": []}, {"text": "The end result is a technique that scales and performs just as well as MIRA-based tuning, but which can be implemented in a couple of hours by anyone with an existing MERT implementation.", "labels": [], "entities": [{"text": "MIRA-based tuning", "start_pos": 71, "end_pos": 88, "type": "TASK", "confidence": 0.539446234703064}]}, {"text": "Mindful that many would-be enhancements to the The remainder either did not specify their tuning method (though a number of these used the Moses toolkit (, which uses MERT for tuning) or, in one case, set weights by hand.", "labels": [], "entities": [{"text": "The remainder", "start_pos": 47, "end_pos": 60, "type": "DATASET", "confidence": 0.9219375848770142}, {"text": "MERT", "start_pos": 167, "end_pos": 171, "type": "METRIC", "confidence": 0.9511147737503052}]}, {"text": "state-of-the-art are false positives that only show improvement in a narrowly defined setting or with limited data, we validate our claims on both syntax and phrase-based systems, using multiple language pairs and large data sets.", "labels": [], "entities": []}, {"text": "We describe tuning in abstract and somewhat formal terms in Section 2, describe the MERT algorithm in the context of those terms and illustrate its scalability issues via a synthetic experiment in Section 3, introduce our pairwise ranking optimization method in Section 4, present numerous large-scale MT experiments to validate our claims in Section 5, discuss some related work in Section 6, and conclude in Section 7.", "labels": [], "entities": [{"text": "MT", "start_pos": 302, "end_pos": 304, "type": "TASK", "confidence": 0.9561243057250977}]}], "datasetContent": [{"text": "We now turn to real machine translation conditions to validate our thesis: We can cleanly replace MERT's line optimization with pairwise ranking optimization and immediately realize the benefits of high-dimension tuning.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 20, "end_pos": 39, "type": "TASK", "confidence": 0.7498500049114227}, {"text": "MERT's line optimization", "start_pos": 98, "end_pos": 122, "type": "TASK", "confidence": 0.6975788623094559}]}, {"text": "We now detail the three language pairs, two feature scenarios, and two MT models used for our experiments.", "labels": [], "entities": [{"text": "MT", "start_pos": 71, "end_pos": 73, "type": "TASK", "confidence": 0.9420024156570435}]}, {"text": "For each language pair and each MT model we used MERT, MIRA, and PRO to tune with a standard set of baseline features, and used the latter two methods to tune with an extended set of features.", "labels": [], "entities": [{"text": "MT", "start_pos": 32, "end_pos": 34, "type": "TASK", "confidence": 0.9480249285697937}, {"text": "MERT", "start_pos": 49, "end_pos": 53, "type": "METRIC", "confidence": 0.9773566126823425}, {"text": "MIRA", "start_pos": 55, "end_pos": 59, "type": "METRIC", "confidence": 0.9858800768852234}]}, {"text": "8 At the end of every experiment we used the final feature weights to decode a held-out test set and evaluated it with case-sensitive BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 134, "end_pos": 138, "type": "METRIC", "confidence": 0.989357054233551}]}], "tableCaptions": [{"text": " Table 1: Machine translation performance for the experiments listed in this paper. Scores are case-sensitive IBM  BLEU. For every choice of system, language pair, and feature set, PRO performs comparably with the other methods.", "labels": [], "entities": [{"text": "Machine translation", "start_pos": 10, "end_pos": 29, "type": "TASK", "confidence": 0.8366475701332092}, {"text": "IBM", "start_pos": 110, "end_pos": 113, "type": "METRIC", "confidence": 0.8313499689102173}, {"text": "BLEU", "start_pos": 115, "end_pos": 119, "type": "METRIC", "confidence": 0.7638953328132629}]}, {"text": " Table 2: Data sizes for the experiments reported in this  paper (English words shown).", "labels": [], "entities": []}, {"text": " Table 3: Summary of features used in experiments in this paper.", "labels": [], "entities": [{"text": "Summary", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.7537552714347839}]}]}