{"title": [{"text": "Personalized Recommendation of User Comments via Factor Models", "labels": [], "entities": []}], "abstractContent": [{"text": "In recent years, the amount of user-generated opinionated texts (e.g., reviews, user comments) continues to grow at a rapid speed: featured news stories on a major event easily attract thousands of user comments on a popular online News service.", "labels": [], "entities": []}, {"text": "How to consume subjective information of this volume becomes an interesting and important research question.", "labels": [], "entities": []}, {"text": "In contrast to previous work on review analysis that tried to filter or summarize information fora generic average user, we explore a different direction of enabling personalized recommendation of such information.", "labels": [], "entities": []}, {"text": "For each user, our task is to rank the comments associated with a given article according to personalized user preference (i.e., whether the user is likely to like or dislike the comment).", "labels": [], "entities": []}, {"text": "To this end, we propose a factor model that incorporates rater-comment and rater-author interactions simultaneously in a principled way.", "labels": [], "entities": []}, {"text": "Our full model significantly outperforms strong baselines as well as related models that have been considered in previous work.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recent years have seen rapid growth in usergenerated opinions online.", "labels": [], "entities": []}, {"text": "Many of them are user reviews: a best-seller or a popular restaurant can get over 1000 reviews on top review sites like Amazon or Yelp.", "labels": [], "entities": [{"text": "Yelp", "start_pos": 130, "end_pos": 134, "type": "DATASET", "confidence": 0.9410561323165894}]}, {"text": "A large quantity of them also come in the form of user comments on blogs or news articles.", "labels": [], "entities": []}, {"text": "Most notably, during the short period of time for which a major event is active, news stories on one single event can easily attract over ten thousand comments on a popular online news site like Yahoo!", "labels": [], "entities": []}, {"text": "One question becomes immediate: how can we help people consume such gigantic amount of opinionated information?", "labels": [], "entities": []}, {"text": "One possibility is to take the summarization route.", "labels": [], "entities": [{"text": "summarization", "start_pos": 31, "end_pos": 44, "type": "TASK", "confidence": 0.9726607799530029}]}, {"text": "Briefly speaking (see Section 2 fora more detailed discussion), previous work has largely formulated review summarization as automatically or manually identify ratable aspects, and present overall sentiment polarity for each aspect (.", "labels": [], "entities": []}, {"text": "A related line of research looked into predicting helpfulness of reviews in the hope of promoting those with better quality, where helpfulness is usually defined as some function over the percentage of users who found the review to be helpful (;.", "labels": [], "entities": [{"text": "predicting helpfulness of reviews", "start_pos": 39, "end_pos": 72, "type": "TASK", "confidence": 0.8486846685409546}]}, {"text": "In short, the focus of previous work has been on distilling subjective information for an average user.", "labels": [], "entities": []}, {"text": "Whether opinion consumers are looking for quality information or just wondering what other people think, each may have different purposes or preferences that is not well represented by a generic average user.", "labels": [], "entities": []}, {"text": "If we think about how we deal with information content overflow on the Web, there have been two main frameworks to identify relevant information for each person.", "labels": [], "entities": []}, {"text": "Indeed many top review sites allow users to search within reviews fora given entity.", "labels": [], "entities": []}, {"text": "But this is only useful when users have explicit information needs that can be formulated as queries.", "labels": [], "entities": []}, {"text": "The other paradigm is recommendation: based on what users have liked or disliked in the past, the system will automatically recommend 571 new items.", "labels": [], "entities": []}, {"text": "Can we provide similar recommendation mechanisms to help users consume large quantities of subjective information?", "labels": [], "entities": []}, {"text": "Many commenting environments allow users to mark \"like\" or \"dislike\" over existing comments (e.g., Yahoo!", "labels": [], "entities": []}, {"text": "News comments, Facebook posts, or review sites that allow helpfulness votes).", "labels": [], "entities": []}, {"text": "Can we learn from users' past preferences, so that when a user is reading anew article, we have a system that automatically ranks its comments according to their likelihood of being liked by the user?", "labels": [], "entities": []}, {"text": "This can be used directly to create personalized presentation of comments (e.g., into a \"like\" column and a \"dislike\" column), as well as enabling down-stream applications such as personalized summarization.", "labels": [], "entities": [{"text": "personalized summarization", "start_pos": 180, "end_pos": 206, "type": "TASK", "confidence": 0.654603123664856}]}, {"text": "Recommending textual information has recently attracted more attention.", "labels": [], "entities": [{"text": "Recommending textual information", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.9048172036806742}]}, {"text": "So far, the focus has been mainly on recommending news articles (.", "labels": [], "entities": []}, {"text": "Our task differs in several aspects.", "labels": [], "entities": []}, {"text": "Intuitively, recommending news articles is largely about identifying the topics of interest to a given user, and it is conceivable that unigram representation of full-length articles can reasonably capture that information.", "labels": [], "entities": []}, {"text": "In our case, most comments for an article a user is reading are already of interest to that user topically.", "labels": [], "entities": []}, {"text": "Which ones the user ends up liking may depend on several non-topical aspects of the text: whether the user agrees with the viewpoint expressed in the comment, whether the comment is convincing and well-written, etc.", "labels": [], "entities": []}, {"text": "Previous work has shown that such analysis can be more difficult than topic-based analysis, and we have the additional challenge that comments are typically much shorter than full-length articles.", "labels": [], "entities": []}, {"text": "However, the difficulty in analyzing the textual information in comments can be alleviated by additional contextual information such as author identities.", "labels": [], "entities": []}, {"text": "If between a pair of users one consistently likes or dislikes the other, then at least for the heavy users, this authorship information alone could be highly informative.", "labels": [], "entities": []}, {"text": "Indeed, previous work in collaborative filtering has usually found no additional gain from leveraging content information when entity-level preference information is abundant.", "labels": [], "entities": []}, {"text": "In this paper, we present a principled way of utilizing multiple sources of information for the task of recommending user comments, which significantly outperforms strong baseline methods, as well as previous methods proposed for text recommendation.", "labels": [], "entities": [{"text": "text recommendation", "start_pos": 230, "end_pos": 249, "type": "TASK", "confidence": 0.774011105298996}]}, {"text": "While using authorship information alone tends to provide stronger signal than using textual information alone, to our surprise, even for heavy users, adding textual information to the authorship information yields additional improvements.", "labels": [], "entities": []}], "datasetContent": [{"text": "Features: All comments were tokenized, lowercased, with stopwords and punctuations removed.", "labels": [], "entities": []}, {"text": "We limited the vocabulary to the 10K most frequent tokens in all comments associated with the training articles.", "labels": [], "entities": []}, {"text": "(See Section 4.3.3 fora discussion on the effect of the vocabulary size.)", "labels": [], "entities": []}, {"text": "For a given comment j, x j is its bag of words representation, L 2 normalized.", "labels": [], "entities": []}, {"text": "For term weighting, we experimented with both presence value and tf-idf weighting.", "labels": [], "entities": [{"text": "term weighting", "start_pos": 4, "end_pos": 18, "type": "TASK", "confidence": 0.655113697052002}]}, {"text": "The latter gives slight better performance.", "labels": [], "entities": []}, {"text": "Rater feature vector xi is created by summing over the feature vectors of all comments rated positively by rater i, which is then L 2 normalized.", "labels": [], "entities": []}, {"text": "Methods: We compare the following methods based on our model: The full model vv+uc, as well as the three main special cases, vv, uc, and bilinear, as defined in Section 3.", "labels": [], "entities": []}, {"text": "The dimensions of vi , u i and c j (i.e., r v and r u ), and the rank of bilinear are selected to obtain the best AUC on the tuning set.", "labels": [], "entities": [{"text": "AUC", "start_pos": 114, "end_pos": 117, "type": "METRIC", "confidence": 0.9659600853919983}]}, {"text": "In our experiments, r v = 2, r u = 3 and rank of bilinear is 3.", "labels": [], "entities": []}, {"text": "In addition, we also evaluate the following baseline methods that predict per-user preferences in isolation, primarily based on textual information.", "labels": [], "entities": []}, {"text": "\u2022 Cosine similarity (cos): xi x j . This is simply based on how similar anew comment j is to the comments rater i has liked in the past.", "labels": [], "entities": [{"text": "Cosine similarity (cos)", "start_pos": 2, "end_pos": 25, "type": "METRIC", "confidence": 0.9000034928321838}]}, {"text": "\u2022 Per-user SVM (svm): For each rater, train a support vector machine (SVM) classifier using only comments (x j ) rated by that user.", "labels": [], "entities": []}, {"text": "\u2022 Per-user Naive Bayes (nb): For each rater, train a Naive Bayes classifier using only comments (x j ) rated by that user.", "labels": [], "entities": []}, {"text": "1 Note that SVMs typically yield the best performance on text classification tasks; a Naive Bayes classifier can be more robust over shorter text spans common in user comments given the high variance.", "labels": [], "entities": [{"text": "text classification tasks", "start_pos": 57, "end_pos": 82, "type": "TASK", "confidence": 0.8328943252563477}]}, {"text": "For fair comparisons, for the three baseline methods, we use a simple way of utilizing author information: the feature space is augmented with author IDs and each x j is augmented with a(j) 2 . In Section 4.3, we only report results using the augmented feature vectors since they yield better performance (though the difference is fairly small).", "labels": [], "entities": []}, {"text": "Performance metrics: We use two types of metrics to measure the performance of a method: (1) A global metric based on Receiver Operating Characteristic (ROC) and (2) Precision at rank k (P@k).", "labels": [], "entities": [{"text": "Precision", "start_pos": 166, "end_pos": 175, "type": "METRIC", "confidence": 0.9932490587234497}]}, {"text": "The former measures the overall correlation of predicted scores fora method with the observed ratings in the test set, while the latter measures the performance of a hypothetical top-k recommendation scenario using the method.", "labels": [], "entities": []}, {"text": "To summarize an ROC curve into a single number, we use the Area Under the ROC Curve (AUC).", "labels": [], "entities": [{"text": "Area Under the ROC Curve (AUC)", "start_pos": 59, "end_pos": 89, "type": "METRIC", "confidence": 0.6344139948487282}]}, {"text": "Since random guess yields AUC score of 0.5, regardless of the class distribution, using this measure makes it convenient for us to compare the performance over different subsets of the data (where class distributions could be different).", "labels": [], "entities": [{"text": "AUC score", "start_pos": 26, "end_pos": 35, "type": "METRIC", "confidence": 0.9874582290649414}]}, {"text": "The P@k of a method is computed as follows: (1) For each rater, rank comments that the rater rated in the test set according to the scores predicted by the method, and compute the precision at rank k for that rater; and then (2) average the perrater precision numbers overall raters.", "labels": [], "entities": [{"text": "precision", "start_pos": 180, "end_pos": 189, "type": "METRIC", "confidence": 0.9968957901000977}, {"text": "perrater precision", "start_pos": 241, "end_pos": 259, "type": "METRIC", "confidence": 0.7900890111923218}]}, {"text": "To report P@k, fork = 5, 10, 20, we only use raters who have at least 50 ratings in the test set.", "labels": [], "entities": []}, {"text": "Statistical significance based on a two-sample t-test across raters is also reported.", "labels": [], "entities": [{"text": "significance", "start_pos": 12, "end_pos": 24, "type": "METRIC", "confidence": 0.8469839096069336}]}], "tableCaptions": [{"text": " Table 2: AUCs and precisions of different models.", "labels": [], "entities": [{"text": "precisions", "start_pos": 19, "end_pos": 29, "type": "METRIC", "confidence": 0.9855470657348633}]}]}