{"title": [{"text": "Unsupervised Dependency Parsing without Gold Part-of-Speech Tags", "labels": [], "entities": [{"text": "Dependency Parsing without Gold Part-of-Speech Tags", "start_pos": 13, "end_pos": 64, "type": "TASK", "confidence": 0.6733326415220896}]}], "abstractContent": [{"text": "We show that categories induced by unsuper-vised word clustering can surpass the performance of gold part-of-speech tags in dependency grammar induction.", "labels": [], "entities": [{"text": "word clustering", "start_pos": 49, "end_pos": 64, "type": "TASK", "confidence": 0.7632398307323456}, {"text": "dependency grammar induction", "start_pos": 124, "end_pos": 152, "type": "TASK", "confidence": 0.8130020300547282}]}, {"text": "Unlike classic clustering algorithms, our method allows a word to have different tags in different contexts.", "labels": [], "entities": []}, {"text": "In an ablative analysis, we first demonstrate that this context-dependence is crucial to the superior performance of gold tags-requiring a word to always have the same part-of-speech significantly degrades the performance of manual tags in grammar induction, eliminating the advantage that human annotation has over unsupervised tags.", "labels": [], "entities": [{"text": "grammar induction", "start_pos": 240, "end_pos": 257, "type": "TASK", "confidence": 0.6934096962213516}]}, {"text": "We then introduce a sequence modeling technique that combines the output of a word clustering algorithm with context-colored noise, to allow words to be tagged differently in different contexts.", "labels": [], "entities": [{"text": "sequence modeling", "start_pos": 20, "end_pos": 37, "type": "TASK", "confidence": 0.708085373044014}, {"text": "word clustering", "start_pos": 78, "end_pos": 93, "type": "TASK", "confidence": 0.7339312136173248}]}, {"text": "With these new induced tags as input, our state-of-the-art dependency grammar inducer achieves 59.1% directed accuracy on Section 23 (all sentences) of the Wall Street Journal (WSJ) corpus-0.7% higher than using gold tags.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 110, "end_pos": 118, "type": "METRIC", "confidence": 0.9587240219116211}, {"text": "Wall Street Journal (WSJ) corpus-0.7", "start_pos": 156, "end_pos": 192, "type": "DATASET", "confidence": 0.8628291828291756}]}], "introductionContent": [{"text": "Unsupervised learning -machine learning without manually-labeled training examples -is an active area of scientific research.", "labels": [], "entities": []}, {"text": "In natural language processing, unsupervised techniques have been successfully applied to tasks such as word alignment for machine translation.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 3, "end_pos": 30, "type": "TASK", "confidence": 0.6735097765922546}, {"text": "word alignment", "start_pos": 104, "end_pos": 118, "type": "TASK", "confidence": 0.7710287868976593}, {"text": "machine translation", "start_pos": 123, "end_pos": 142, "type": "TASK", "confidence": 0.7665303349494934}]}, {"text": "And since the advent of the web, algorithms that induce structure from unlabeled data have continued to steadily gain importance.", "labels": [], "entities": []}, {"text": "In this paper we focus on unsupervised part-of-speech tagging and dependency parsing -two related problems of syntax discovery.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 39, "end_pos": 61, "type": "TASK", "confidence": 0.6754011958837509}, {"text": "dependency parsing", "start_pos": 66, "end_pos": 84, "type": "TASK", "confidence": 0.7766639590263367}, {"text": "syntax discovery", "start_pos": 110, "end_pos": 126, "type": "TASK", "confidence": 0.7439499050378799}]}, {"text": "Our methods are applicable to vast quantities of unlabeled monolingual text.", "labels": [], "entities": []}, {"text": "Not all research on these problems has been fully unsupervised.", "labels": [], "entities": []}, {"text": "For example, to the best of our knowledge, every new state-of-the-art dependency grammar inducer since relied on gold part-of-speech tags.", "labels": [], "entities": []}, {"text": "For sometime, multipoint performance degradations caused by switching to automatically induced word categories have been interpreted as indications that \"good enough\" partsof-speech induction methods exist, justifying the focus on grammar induction with supervised part-ofspeech tags), pace.", "labels": [], "entities": [{"text": "grammar induction", "start_pos": 231, "end_pos": 248, "type": "TASK", "confidence": 0.6933056116104126}]}, {"text": "One of several drawbacks of this practice is that it weakens any conclusions that could be drawn about how computers (and possibly humans) learn in the absence of explicit feedback ).", "labels": [], "entities": []}, {"text": "In turn, not all unsupervised taggers actually induce word categories: Many systems -known as part-of-speech disambiguators rely on external dictionaries of possible tags.", "labels": [], "entities": []}, {"text": "Our work builds on two older part-of-speech inducers -word clustering algorithms of and -that were recently shown to be more robust than other well-known fully unsupervised techniques.", "labels": [], "entities": [{"text": "part-of-speech inducers -word clustering", "start_pos": 29, "end_pos": 69, "type": "TASK", "confidence": 0.6198569118976593}]}, {"text": "We investigate which properties of gold part-ofspeech tags are useful in grammar induction and parsing, and how these properties could be introduced into induced tags.", "labels": [], "entities": [{"text": "grammar induction", "start_pos": 73, "end_pos": 90, "type": "TASK", "confidence": 0.765826016664505}]}, {"text": "We also explore the number of word classes that is good for grammar induction: in particular, whether categorization is needed at all.", "labels": [], "entities": [{"text": "grammar induction", "start_pos": 60, "end_pos": 77, "type": "TASK", "confidence": 0.7133734077215195}]}, {"text": "By removing the \"unrealistic simplification\" of using gold tags (Petrov et al., 2011, \u00a73.2, Footnote 4), we will goon to demonstrate why grammar induction from plain text is no longer \"still too difficult.\"", "labels": [], "entities": [{"text": "grammar induction from plain text", "start_pos": 137, "end_pos": 170, "type": "TASK", "confidence": 0.8276656866073608}]}, {"text": "1281 Figure 1: A dependency structure fora short WSJ sentence and its probability, factored by the DMV, using gold tags, after summing out PORDER ().", "labels": [], "entities": [{"text": "PORDER", "start_pos": 139, "end_pos": 145, "type": "METRIC", "confidence": 0.9917410612106323}]}], "datasetContent": [{"text": "Evaluation is against the training set, as is standard practice in unsupervised learning, in part because did not smooth the DMV (.", "labels": [], "entities": [{"text": "DMV", "start_pos": 125, "end_pos": 128, "type": "DATASET", "confidence": 0.6081664562225342}]}, {"text": "For most of our experiments (#1-4, \u00a73-4), this entails starting with the reference trees from WSJ15 (as modified in \u00a72.3), automatically converting their labeled constituents into unlabeled dependencies using deterministic \"headpercolation\" rules, and then computing (directed) dependency accuracy scores of the corresponding induced trees.", "labels": [], "entities": [{"text": "WSJ15", "start_pos": 94, "end_pos": 99, "type": "DATASET", "confidence": 0.9671549797058105}, {"text": "accuracy", "start_pos": 289, "end_pos": 297, "type": "METRIC", "confidence": 0.7954846024513245}]}, {"text": "We report overall percentages of correctly guessed arcs, including the arcs from sentence root symbols, as is standard practice).", "labels": [], "entities": []}, {"text": "For a meaningful comparison with previous work, we also test some of the models from our earlier experiments (#1,3) -and both models from final experiments (#5,6) -against Section 23 of WSJ \u221e , after applying Laplace (a.k.a.", "labels": [], "entities": [{"text": "Section 23 of WSJ \u221e", "start_pos": 172, "end_pos": 191, "type": "DATASET", "confidence": 0.6407261669635773}]}, {"text": "\"add one\") smoothing.: Directed accuracies for the \"less is more\" DMV, trained on WSJ15 (after 40 steps of EM) and evaluated also against WSJ15, using various lexical categories in place of gold part-of-speech tags.", "labels": [], "entities": [{"text": "WSJ15", "start_pos": 82, "end_pos": 87, "type": "DATASET", "confidence": 0.9602265357971191}, {"text": "WSJ15", "start_pos": 138, "end_pos": 143, "type": "DATASET", "confidence": 0.9536011219024658}]}, {"text": "For each tag-set, we include its effective number of (non-empty) categories in WSJ15 and the oracle skylines (supervised performance).", "labels": [], "entities": [{"text": "WSJ15", "start_pos": 79, "end_pos": 84, "type": "DATASET", "confidence": 0.9339039325714111}]}, {"text": "Our first set of experiments attempts to isolate the effect that replacing gold part-of-speech tags with deterministic one class per word mappings has on performance, quantifying the cost of switching to a monosemous clustering (see: manual; and).", "labels": [], "entities": []}, {"text": "Grammar induction with gold tags scores 50.7%, while the oracle skyline (an ideal, supervised instance of the DMV) could attain 78.0% accuracy.", "labels": [], "entities": [{"text": "Grammar induction", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.819856196641922}, {"text": "accuracy", "start_pos": 134, "end_pos": 142, "type": "METRIC", "confidence": 0.9969220757484436}]}, {"text": "It maybe worth noting that only 6,620 (13.5%) of 49,180 unique tokens in WSJ appear with multiple part-of-speech tags.", "labels": [], "entities": []}, {"text": "Most words, like it, are always tagged the same way (5,768 times PRP).", "labels": [], "entities": []}, {"text": "Some words,  like gains, usually serve as one part of speech (227 times NNS, as in the gains) but are occasionally used differently (5 times VBZ, as in he gains).", "labels": [], "entities": [{"text": "VBZ", "start_pos": 141, "end_pos": 144, "type": "METRIC", "confidence": 0.7711119055747986}]}, {"text": "Only 1,322 tokens (2.7%) appear with three or more different gold tags.", "labels": [], "entities": []}, {"text": "However, this minority includes the most frequent word -the (50,959 times DT, 7 times JJ, 6 times NNP and once as each of CD, NN and VBP).", "labels": [], "entities": [{"text": "VBP", "start_pos": 133, "end_pos": 136, "type": "DATASET", "confidence": 0.8189986348152161}]}, {"text": "We experimented with three natural reassignments of part-of-speech categories (see).", "labels": [], "entities": []}, {"text": "The first, most frequent class (mfc), simply maps each token to its most common gold tag in the entire WSJ (with ties resolved lexicographically).", "labels": [], "entities": [{"text": "WSJ", "start_pos": 103, "end_pos": 106, "type": "DATASET", "confidence": 0.9199463129043579}]}, {"text": "This approach discards two gold tags (types PDT and RBR are not most common for any of the tokens in WSJ15) and costs about three-and-a-half points of accuracy, in both supervised and unsupervised regimes.", "labels": [], "entities": [{"text": "RBR", "start_pos": 52, "end_pos": 55, "type": "METRIC", "confidence": 0.9397923350334167}, {"text": "WSJ15", "start_pos": 101, "end_pos": 106, "type": "DATASET", "confidence": 0.8862398266792297}, {"text": "accuracy", "start_pos": 151, "end_pos": 159, "type": "METRIC", "confidence": 0.9986621141433716}]}, {"text": "Another reassignment, union all (ua), maps each token to the set of all of its observed gold tags, again in the entire WSJ.", "labels": [], "entities": [{"text": "union all (ua)", "start_pos": 22, "end_pos": 36, "type": "METRIC", "confidence": 0.9250892400741577}, {"text": "WSJ", "start_pos": 119, "end_pos": 122, "type": "DATASET", "confidence": 0.9658284187316895}]}, {"text": "This inflates the number of groupings by nearly a factor often (effectively lexicalizing the most ambiguous words), 3 yet improves the oracle skyline by half-a-point over actual gold tags; however, learning is harder with this tag-set, losing more than six points in unsupervised training.", "labels": [], "entities": []}, {"text": "Our last reassignment, most frequent pair (mfp), allows up to two of the most common tags into a token's label set (with ties, once again, resolved lexicographically).", "labels": [], "entities": []}, {"text": "This intermediate approach performs strictly worse than union all, in both regimes.", "labels": [], "entities": []}, {"text": "Our next set of experiments assesses the benefits of categorization, turning to lexicalized baselines that avoid grouping words altogether.", "labels": [], "entities": []}, {"text": "All three models discussed below estimated the DMV without using the gold tags in anyway (see: lexicalized).", "labels": [], "entities": []}, {"text": "First, not surprisingly, a fully-lexicalized model over nearly 50,000 unique words is able to essentially memorize the training set, supervised.", "labels": [], "entities": []}, {"text": "(Without smoothing, it is possible to deterministically attach most rare words in a dependency tree correctly, etc.)", "labels": [], "entities": []}, {"text": "Of course, local search is unlikely to find good instantiations for so many parameters, causing unsupervised accuracy for this model to drop in half.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 109, "end_pos": 117, "type": "METRIC", "confidence": 0.9992843270301819}]}, {"text": "For our next experiment, we tried an intermediate, partially-lexicalized approach.", "labels": [], "entities": []}, {"text": "We mapped frequent words -those seen at least 100 times in the training corpus) -to their own individual categories, lumping the rest into a single \"unknown\" cluster, fora total of under 200 groups.", "labels": [], "entities": []}, {"text": "This model is significantly worse for supervised learning, compared even with the monosemous clusters derived from gold tags; yet it is only slightly more learnable than the broken fully-lexicalized variant.", "labels": [], "entities": []}, {"text": "Finally, for completeness, we trained a model that maps every token to the same one \"unknown\" category.", "labels": [], "entities": []}, {"text": "As expected, such a trivial \"clustering\" is ineffective in supervised training; however, it outperforms both lexicalized variants unsupervised, 4 strongly suggesting that lexicalization alone maybe insufficient for the DMV and hinting that some degree of categorization is essential to its learnability.", "labels": [], "entities": []}, {"text": "Our main purely unsupervised results are with a flat clustering) that groups words having similar context distributions, according to KullbackLeibler divergence.", "labels": [], "entities": []}, {"text": "(A word's context is an ordered pair: its left-and right-adjacent neighboring words.)", "labels": [], "entities": []}, {"text": "To avoid overfitting, we employed an implementation from previous literature.", "labels": [], "entities": []}, {"text": "The number of clusters (200) and the sufficient amount of training data (several hundredmillion words) were tuned to a task (NER) that is not directly related to dependency parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 162, "end_pos": 180, "type": "TASK", "confidence": 0.7939557135105133}]}, {"text": "shows representative entries for two of the clusters.)", "labels": [], "entities": []}, {"text": "We added one more category (#0) for unknown words.", "labels": [], "entities": []}, {"text": "Now every token in WSJ could again be replaced by a coarse identifier (one of at most 201, instead of just 36), in both supervised and unsupervised training.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 19, "end_pos": 22, "type": "DATASET", "confidence": 0.7822136878967285}]}, {"text": "(Our training code did not change.)", "labels": [], "entities": []}, {"text": "The resulting supervised model, though not as good as the fully-lexicalized DMV, was more than five points more accurate than with gold part-ofspeech tags (see: flat).", "labels": [], "entities": []}, {"text": "Unsupervised accuracy was lower than with gold tags (see also) but higher than with all three derived hard assignments.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.9905099272727966}]}, {"text": "This suggests that polysemy (i.e., ability to tag a word differently in context) maybe the primary advantage of manually constructed categorizations.", "labels": [], "entities": []}, {"text": "The purpose of this batch of experiments is to show that Clark's (2000) algorithm isn't unique in its suitability for grammar induction.", "labels": [], "entities": [{"text": "grammar induction", "start_pos": 118, "end_pos": 135, "type": "TASK", "confidence": 0.81047523021698}]}, {"text": "We found that older information-theoretic approach, which does not explicitly address the problems of rare and ambiguous words and was designed to induce large numbers of plausible syntactic and semantic clusters, can perform just as well.", "labels": [], "entities": []}, {"text": "Once again, the sufficient amount of data (43 million words) was tuned in earlier work.", "labels": [], "entities": []}, {"text": "His task of interest was, in fact, dependency parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 35, "end_pos": 53, "type": "TASK", "confidence": 0.8863067626953125}]}, {"text": "But since this algorithm is hierarchical (i.e., there isn't a parameter for the number of categories), we doubt that there was a strong enough risk of overfitting to question the clustering's unsupervised nature.", "labels": [], "entities": []}, {"text": "As there isn't a set number of categories, we used binary prefixes of length k from each word's address in the computed hierarchy as cluster labels.", "labels": [], "entities": []}, {"text": "Results for 7 \u2264 k \u2264 9 bits (approximately 100-250 nonempty clusters, close to the 200 we used before) are similar to those of flat clusters (see: hierarchical).", "labels": [], "entities": []}, {"text": "Outside of this range, however, performance can be substantially worse (see, consistent with earlier findings: demonstrated that (constituent) grammar induction, using the singular-value decomposition (SVD-based) tagger of, also works best with 100-200 clusters.", "labels": [], "entities": [{"text": "constituent) grammar induction", "start_pos": 130, "end_pos": 160, "type": "TASK", "confidence": 0.6301653236150742}]}, {"text": "Important future research directions may include learning to automatically select a good number of word categories (in the case of flat clusterings) and ways of using multiple clustering assignments, perhaps of different granularities/resolutions, in tandem (e.g., in the case of a hierarchical clustering).", "labels": [], "entities": []}, {"text": "It is important to enable easy comparison with previous and future work.", "labels": [], "entities": []}, {"text": "Since WSJ15 is not a standard test set, we evaluated two key experiments -\"less is more\" with gold part-of-speech tags (#1, Table 1: gold) and with Clark's (2000) clusters (#3, Table 1: flat) -on all sentences (not just length fifteen and shorter), in Section 23 of WSJ (see).", "labels": [], "entities": [{"text": "WSJ15", "start_pos": 6, "end_pos": 11, "type": "DATASET", "confidence": 0.9443877339363098}, {"text": "WSJ", "start_pos": 266, "end_pos": 269, "type": "DATASET", "confidence": 0.5598058104515076}]}, {"text": "This required smoothing both final models ( \u00a72.4).", "labels": [], "entities": []}, {"text": "We showed that two classic unsupervised word 1285  clusterings -one flat and one hierarchical -can be better for dependency grammar induction than monosemous syntactic categories derived from gold part-of-speech tags.", "labels": [], "entities": [{"text": "dependency grammar induction", "start_pos": 113, "end_pos": 141, "type": "TASK", "confidence": 0.825911263624827}]}, {"text": "And we confirmed that the unsupervised tags are worse than the actual gold tags, in a simple dependency grammar induction system.", "labels": [], "entities": []}, {"text": "As in experiment #3 ( \u00a74.1), we modified the base system in exactly one way: we swapped out gold part-of-speech tags and replaced them with a flat distributional similarity clustering.", "labels": [], "entities": []}, {"text": "In contrast to simpler models, which suffer multi-point drops inaccuracy from switching to unsupervised tags (e.g., 2.6%), our new system's performance degrades only slightly, by 0.2% (see).", "labels": [], "entities": []}, {"text": "This result improves over substantial performance degradations previously observed for unsupervised dependency parsing with induced word categories (, inter alia).", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 100, "end_pos": 118, "type": "TASK", "confidence": 0.7046652436256409}]}, {"text": "One risk that arises from using gold tags is that newer systems could be finding cleverer ways to exploit manual labels (i.e., developing an over-reliance on gold tags) instead of actually learning to acquire language.", "labels": [], "entities": []}, {"text": "Part-of-speech tags are known to contain significant amounts of information for unlabeled dependency parsing.1), so we find it reassuring that our latest grammar inducer is less dependent on gold tags than its predecessors.", "labels": [], "entities": [{"text": "dependency parsing.1", "start_pos": 90, "end_pos": 110, "type": "TASK", "confidence": 0.6990198940038681}]}, {"text": "Results of experiments #1 and 3 ( \u00a73.1, 4.1) suggest that grammar induction stands to gain from relaxing the one class per word assumption.", "labels": [], "entities": [{"text": "grammar induction", "start_pos": 58, "end_pos": 75, "type": "TASK", "confidence": 0.8202173709869385}]}, {"text": "We next test this conjecture by inducing a polysemous unsupervised word clustering, then using it to induce a grammar.", "labels": [], "entities": [{"text": "word clustering", "start_pos": 67, "end_pos": 82, "type": "TASK", "confidence": 0.7491482496261597}]}, {"text": "Previous work found that simple bitag hidden Markov models, classically trained using the variant of EM (HMM-EM), perform quite well, 8 on average, across different grammar induction tasks.", "labels": [], "entities": []}, {"text": "Such sequence models incorporate a sensitivity to context via state transition probabilities PTRAN(ti | ti\u22121), capturing the likelihood that a tag ti immediately follows the tag t i\u22121 ; emission probabilities capture the likelihood that a word of type ti is w i .", "labels": [], "entities": [{"text": "PTRAN", "start_pos": 93, "end_pos": 98, "type": "METRIC", "confidence": 0.9551419019699097}]}], "tableCaptions": [{"text": " Table 1: Directed accuracies for the \"less is more\" DMV,  trained on WSJ15 (after 40 steps of EM) and evaluated  also against WSJ15, using various lexical categories in  place of gold part-of-speech tags. For each tag-set, we  include its effective number of (non-empty) categories in  WSJ15 and the oracle skylines (supervised performance).", "labels": [], "entities": [{"text": "WSJ15", "start_pos": 70, "end_pos": 75, "type": "DATASET", "confidence": 0.962696373462677}, {"text": "WSJ15", "start_pos": 127, "end_pos": 132, "type": "DATASET", "confidence": 0.9409997463226318}, {"text": "WSJ15", "start_pos": 287, "end_pos": 292, "type": "DATASET", "confidence": 0.9668461084365845}]}, {"text": " Table 3: Representative members for two of the flat word  groupings: cluster #173 (left) contains adjectives, espe- cially ones that take comparative (or other) complements;  cluster #188 comprises bare-stem verbs (infinitive stems).  (Of course, many of the words have other syntactic uses.)", "labels": [], "entities": []}, {"text": " Table 1: hierar- chical). Outside of this range, however, performance  can be substantially worse (see", "labels": [], "entities": []}]}