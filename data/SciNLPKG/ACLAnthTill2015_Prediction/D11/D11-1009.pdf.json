{"title": [{"text": "A Generate and Rank Approach to Sentence Paraphrasing", "labels": [], "entities": [{"text": "Rank Approach", "start_pos": 15, "end_pos": 28, "type": "METRIC", "confidence": 0.9256795048713684}, {"text": "Sentence Paraphrasing", "start_pos": 32, "end_pos": 53, "type": "TASK", "confidence": 0.9037508368492126}]}], "abstractContent": [{"text": "We present a method that paraphrases a given sentence by first generating candidate paraphrases and then ranking (or classifying) them.", "labels": [], "entities": []}, {"text": "The candidates are generated by applying existing paraphrasing rules extracted from parallel corpora.", "labels": [], "entities": []}, {"text": "The ranking component considers not only the overall quality of the rules that produced each candidate, but also the extent to which they preserve gram-maticality and meaning in the particular context of the input sentence, as well as the degree to which the candidate differs from the input.", "labels": [], "entities": []}, {"text": "We experimented with both a Maximum Entropy classifier and an SVR ranker.", "labels": [], "entities": []}, {"text": "Experimental results show that incorporating features from an existing paraphrase recog-nizer in the ranking component improves performance , and that our overall method compares well against a state of the art paraphrase generator, when paraphrasing rules apply to the input sentences.", "labels": [], "entities": []}, {"text": "We also propose anew methodology to evaluate the ranking components of generate-and-rank paraphrase generators , which evaluates them across different combinations of weights for grammaticality, meaning preservation, and diversity.", "labels": [], "entities": [{"text": "meaning preservation", "start_pos": 195, "end_pos": 215, "type": "TASK", "confidence": 0.7683639228343964}]}, {"text": "The paper is accompanied by a paraphrasing dataset we constructed for evaluations of this kind.", "labels": [], "entities": []}], "introductionContent": [{"text": "In recent years, significant effort has been devoted to research on paraphrasing.", "labels": [], "entities": [{"text": "paraphrasing", "start_pos": 68, "end_pos": 80, "type": "TASK", "confidence": 0.9606149196624756}]}, {"text": "The methods that have been proposed can be roughly classified into three categories: (i) recognition methods, i.e., methods that detect whether or not two input sentences or other texts are paraphrases; (ii) generation methods, where the aim is to produce paraphrases of a given input sentence; and (iii) extraction methods, which aim to extract paraphrasing rules (e.g., \"X wrote Y \" \"\u2194 Y was authored by X\") or similar patterns from corpora.", "labels": [], "entities": []}, {"text": "Most of the methods that have been proposed belong in the first category, possibly because of the thrust provided by related research on textual entailment recognition ( , where the goal is to decide whether or not the information of a given text is entailed by that of another.", "labels": [], "entities": [{"text": "textual entailment recognition", "start_pos": 137, "end_pos": 167, "type": "TASK", "confidence": 0.8157780369122823}]}, {"text": "Significant progress has also been made in paraphrase extraction, where most recent methods produce large numbers of paraphrasing rules from multilingual parallel corpora.", "labels": [], "entities": [{"text": "paraphrase extraction", "start_pos": 43, "end_pos": 64, "type": "TASK", "confidence": 0.9851066172122955}]}, {"text": "In this paper, we are concerned with paraphrase generation, which has received less attention than the other two categories.", "labels": [], "entities": [{"text": "paraphrase generation", "start_pos": 37, "end_pos": 58, "type": "TASK", "confidence": 0.9817836582660675}]}, {"text": "There are currently two main approaches to paraphrase generation.", "labels": [], "entities": [{"text": "paraphrase generation", "start_pos": 43, "end_pos": 64, "type": "TASK", "confidence": 0.9752004146575928}]}, {"text": "The first one treats paraphrase generation as a machine translation problem, with the peculiarity that the target language is the same as the source one.", "labels": [], "entities": [{"text": "paraphrase generation", "start_pos": 21, "end_pos": 42, "type": "TASK", "confidence": 0.9580509960651398}, {"text": "machine translation", "start_pos": 48, "end_pos": 67, "type": "TASK", "confidence": 0.7617095708847046}]}, {"text": "To bypass the lack of large monolingual parallel corpora, which are needed to train statistical machine translation (SMT) systems for paraphrasing, monolingual clusters of news articles referring to the same event () or other similar monolingual comparable corpora can be used, though sentence alignment methods for parallel corpora may perform poorly on comparable corpora); alternatively, large collections of paraphrasing rules obtained via paraphrase extraction from multilingual parallel corpora can be used as monolingual phrase tables in a 96 phrase-based SMT systems (); in both cases, paraphrases can then be generated by invoking an SMT system's decoder.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 84, "end_pos": 121, "type": "TASK", "confidence": 0.7925775448481241}, {"text": "sentence alignment", "start_pos": 285, "end_pos": 303, "type": "TASK", "confidence": 0.7337748110294342}]}, {"text": "A second paraphrase generation approach is to treat existing machine translation engines as black boxes, and translate each input sentence to a pivot language and then back to the original language).", "labels": [], "entities": [{"text": "paraphrase generation", "start_pos": 9, "end_pos": 30, "type": "TASK", "confidence": 0.9206253588199615}, {"text": "machine translation", "start_pos": 61, "end_pos": 80, "type": "TASK", "confidence": 0.7366839349269867}]}, {"text": "An extension of this approach uses multiple translation engines and pivot languages (.", "labels": [], "entities": []}, {"text": "In this paper, we investigate a different paraphrase generation approach, which does not produce paraphrases by invoking machine translation system(s).", "labels": [], "entities": [{"text": "paraphrase generation", "start_pos": 42, "end_pos": 63, "type": "TASK", "confidence": 0.9270214140415192}]}, {"text": "We use an existing collection of monolingual paraphrasing rules extracted from multilingual parallel corpora (; each rule is accompanied by one or more scores, intended to indicate the rule's overall quality without considering particular contexts where the rule maybe applied.", "labels": [], "entities": []}, {"text": "Instead of using the rules as a monolingual phrase table and invoking an SMT system's decoder, we follow a generate and rank approach, which is increasingly common in several language processing tasks.", "labels": [], "entities": [{"text": "SMT", "start_pos": 73, "end_pos": 76, "type": "TASK", "confidence": 0.9643718600273132}]}, {"text": "Given an input sentence, we use the paraphrasing rules to generate a large number of candidate paraphrases.", "labels": [], "entities": []}, {"text": "The candidates are then represented as feature vectors, and a ranker (or classifier) selects the best ones; we experimented with a Maximum Entropy classifier and a Support Vector Regression (SVR) ranker.", "labels": [], "entities": []}, {"text": "The vector of each candidate paraphrase includes features indicating the overall quality of the rules that produced the candidate, the extent to which the rules preserve grammaticality and meaning in the particular context of the input sentence, and the degree to which the candidate's surface form differs from that of the input; we call the latter factor diversity.", "labels": [], "entities": []}, {"text": "The intuition is that a good paraphrase is grammatical, preserves the meaning of the original sentence, while also being as different as possible.", "labels": [], "entities": []}, {"text": "Experimental results show that including in the ranking (or classification) component features from an existing paraphrase recognizer leads to improved results.", "labels": [], "entities": [{"text": "paraphrase recognizer", "start_pos": 112, "end_pos": 133, "type": "TASK", "confidence": 0.6924007534980774}]}, {"text": "We also propose anew methodology to evaluate the ranking components of generate-and-rank paraphrase generators, which evaluates them across different combinations of weights for grammatical-1 See, for example, ity, meaning preservation, and diversity.", "labels": [], "entities": [{"text": "meaning preservation", "start_pos": 215, "end_pos": 235, "type": "TASK", "confidence": 0.763803243637085}]}, {"text": "The paper is accompanied by anew publicly available paraphrasing dataset we constructed for evaluations of this kind.", "labels": [], "entities": []}, {"text": "Further experiments indicate that when paraphrasing rules apply to the input sentences, our paraphrasing method is competitive to a state of the art paraphrase generator that uses multiple translation engines and pivot languages (.", "labels": [], "entities": []}, {"text": "We note that paraphrase generation is useful in several language processing tasks.", "labels": [], "entities": [{"text": "paraphrase generation", "start_pos": 13, "end_pos": 34, "type": "TASK", "confidence": 0.9528015851974487}]}, {"text": "In question answering, for example, paraphrase generators can be used to paraphrase the user's queries; and in machine translation, paraphrase generation can help improve the translations, or it can be used when evaluating machine translation systems (.", "labels": [], "entities": [{"text": "question answering", "start_pos": 3, "end_pos": 21, "type": "TASK", "confidence": 0.8283132016658783}, {"text": "machine translation", "start_pos": 111, "end_pos": 130, "type": "TASK", "confidence": 0.7848750054836273}, {"text": "paraphrase generation", "start_pos": 132, "end_pos": 153, "type": "TASK", "confidence": 0.7129689157009125}]}, {"text": "The remainder of this paper is structured as follows: Section 2 explains how our method generates candidate paraphrases; Section 3 introduces the dataset we constructed, which is also used in subsequent sections; Section 4 discusses how candidate paraphrases are ranked; Section 5 compares our overall method to a state of the art paraphrase generator; and Section 6 concludes.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our generate and rank method relies on existing large collections of paraphrasing rules to generate candidate paraphrases.", "labels": [], "entities": []}, {"text": "Our main contribution is in the ranking of the candidates.", "labels": [], "entities": []}, {"text": "To be able to evaluate the performance of different rankers in the task we are concerned with, we first constructed an evaluation dataset that contains pairs S, C of source (input) sentences and candidate paraphrases, and we asked human judges to assess the degree to which the C of each pair was a good paraphrase of S.", "labels": [], "entities": []}, {"text": "We selected randomly 75 source (S) sentences from the AQUAINT corpus, such that at least one of the paraphrasing rules applied to each S.", "labels": [], "entities": [{"text": "AQUAINT corpus", "start_pos": 54, "end_pos": 68, "type": "DATASET", "confidence": 0.9551702439785004}]}, {"text": "For each S, we generated candidate Cs using Zhao et al.'s rules, as discussed in Section 2.", "labels": [], "entities": []}, {"text": "This led to 1,935 S, C pairs, approx. 26 pairs for each S.", "labels": [], "entities": []}, {"text": "The pairs were given to 13 judges other than the authors.", "labels": [], "entities": []}, {"text": "Each judge evaluated approx. 148 (different) S, C pairs; each of the 1,935 pairs was evaluated by one judge.", "labels": [], "entities": []}, {"text": "The judges were asked to provide grammaticality, meaning preservation, and overall paraphrase quality scores for each S, C pair, each score on a 1-4 scale (1 for totally unacceptable, 4 for perfect); guidelines and examples were also provided.", "labels": [], "entities": [{"text": "meaning preservation", "start_pos": 49, "end_pos": 69, "type": "TASK", "confidence": 0.735914796590805}]}, {"text": "shows the distribution of the overall quality scores in the 1,935 S, C pairs of the evaluation dataset; the distributions of the grammaticality and meaning preservation scores are similar.", "labels": [], "entities": [{"text": "meaning preservation", "start_pos": 148, "end_pos": 168, "type": "TASK", "confidence": 0.6356740593910217}]}, {"text": "Notice that although we used only the 20 applicable paraphrasing rules with the highest scores to generate the S, C pairs, less than half of the candidate paraphrases (C) were considered good, and approximately only 20% perfect.", "labels": [], "entities": []}, {"text": "In other words, applying paraphrasing rules (even only those with the 20 best scores) to each input sentence Sand randomly picking one of the resulting candidate paraphrases C, without any further filtering (or ranking) of the candidates, would on average produce unacceptable paraphrases more frequently than acceptable ones.", "labels": [], "entities": []}, {"text": "Hence, the role of the ranking component is crucial.", "labels": [], "entities": []}, {"text": "We also measured inter-annotator agreement by constructing, in the same way, 100 additional S, C pairs (other than the 1,935) and asking 3 of the 13 judges to evaluate all of them.", "labels": [], "entities": []}, {"text": "We measured the mean absolute error, i.e., the mean absolute difference in the judges' scores (averaged overall pairs of judges) and the mean (over all pairs of judges) K statistic.", "labels": [], "entities": [{"text": "mean absolute error", "start_pos": 16, "end_pos": 35, "type": "METRIC", "confidence": 0.7735980749130249}]}, {"text": "In the overall scores, K was 0.64, which is in the range often taken to indicate substantial agreement (0.61-0.80).", "labels": [], "entities": [{"text": "K", "start_pos": 23, "end_pos": 24, "type": "METRIC", "confidence": 0.9982954859733582}, {"text": "agreement", "start_pos": 93, "end_pos": 102, "type": "METRIC", "confidence": 0.9628212451934814}]}, {"text": "Agreement was higher for grammaticality (K = 0.81), It is also close to 0.67, which is sometimes taken to be a cutoff for substantial agreement in computational linguistics. and lower (K = 0.59) for meaning preservation.", "labels": [], "entities": [{"text": "Agreement", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9927124977111816}, {"text": "meaning preservation", "start_pos": 199, "end_pos": 219, "type": "TASK", "confidence": 0.896584689617157}]}, {"text": "shows that the mean absolute difference in the annotators' scores was 1 5 to 1 4 of a point.", "labels": [], "entities": [{"text": "mean absolute difference", "start_pos": 15, "end_pos": 39, "type": "METRIC", "confidence": 0.7756019035975138}]}, {"text": "Several judges commented that they had trouble deciding to what extent the overall quality score should reflect grammaticality or meaning preservation.", "labels": [], "entities": [{"text": "meaning preservation", "start_pos": 130, "end_pos": 150, "type": "TASK", "confidence": 0.7328197509050369}]}, {"text": "They also wondered if it was fair to consider as perfect candidate paraphrases that differed in only one or two words from the source sentences, i.e., candidates with low diversity.", "labels": [], "entities": []}, {"text": "These comments led us to ignore the judges' overall quality scores in some experiments, and to use a weighted average of grammaticality, meaning preservation, and (automatically measured) diversity instead, with different weight combinations corresponding to different application requirements, as discussed further below.", "labels": [], "entities": [{"text": "meaning preservation", "start_pos": 137, "end_pos": 157, "type": "TASK", "confidence": 0.6807779967784882}]}, {"text": "In the same way, 1,500 more S, C pairs (other than the 1,935 and the 100, not involving previously seen Ss) were constructed, and they were evaluated by the first author.", "labels": [], "entities": []}, {"text": "The 1,500 pairs were used as a training dataset in experiments discussed below.", "labels": [], "entities": []}, {"text": "Both the 1,500 training and the 1,935 evaluation (test) pairs are publicly available.", "labels": [], "entities": []}, {"text": "We occasionally refer to the training and evaluation datasets as a single dataset, but they are clearly separated.", "labels": [], "entities": []}, {"text": "As already noted, when our dataset were constructed the judges felt it was not always clear to what extent the overall quality scores should reflect meaning preservation or grammaticality; and they also wondered if the overall quality scores should have also taken into consideration diversity.", "labels": [], "entities": [{"text": "meaning preservation", "start_pos": 149, "end_pos": 169, "type": "TASK", "confidence": 0.7076990306377411}]}, {"text": "To address these concerns, in the experiments described in this section (and the remainder of the paper) we ignored the judges' overall scores, and we used a weighted average of the grammaticality, meaning preservation, and diversity scores instead; the grammaticality and meaning preservation scores were those provided by the judges, while diversity was automatically computed as the edit distance (Levenshtein, computed on tokens) between Sand C. Stated otherwise, the correct score y(x i ) of each training or test instance xi (i.e., of each feature vector of an S, C pair) was taken to be a linear combination of the grammaticality score g(x i ), the meaning preservation score m(x i ), and the diversity d(x i ), as in Equation, where y( We believe that the \u03bb i weights should in practice be application-dependent.", "labels": [], "entities": []}, {"text": "For example, when paraphrasing user queries to a search engine that turns them into bags of words, diversity and meaning preservation maybe more important than grammaticality; by contrast, when paraphrasing the sentences of a generated text to avoid repeating the same expressions, grammaticality is very important.", "labels": [], "entities": [{"text": "meaning preservation", "start_pos": 113, "end_pos": 133, "type": "TASK", "confidence": 0.7007204294204712}]}, {"text": "Hence, generic paraphrase generators, like ours, intended to be useful in many different applications, should be evaluated for many different combinations of the \u03bb i weights.", "labels": [], "entities": []}, {"text": "Consequently, in the experiments of this section we trained and evaluated the ranking component of our method (on the training and evaluation part, respectively, of the dataset of Section 3) several times, each time with a different combination of \u03bb 1 , \u03bb 2 , \u03bb 3 values, with the values of each \u03bb i ranging from 0 to 1 with a step of 0.2.", "labels": [], "entities": []}, {"text": "We employed a Support Vector Regression (SVR) model in the experiments of this section, instead of 101 a classifier, given that the y(x i ) scores that we want to predict are real values.", "labels": [], "entities": [{"text": "Support Vector Regression (SVR)", "start_pos": 14, "end_pos": 45, "type": "METRIC", "confidence": 0.8316439787546793}]}, {"text": "An SVR is very similar to a Support Vector Machine), but it is trained on examples of the form xi , y(x i ), where xi \u2208 Rn and y(x i ) \u2208 R, and it learns a ranking function f : Rn \u2192 R that is intended to return f (x i ) values as close as possible to the correct ones y(x i ), given feature vectors xi . In our case, the correct y(x i ) values were those of Equation (6).", "labels": [], "entities": []}, {"text": "We call SVR-REC the SVR ranker with all the 151 features of Section 4.2, and SVR-BASE the SVR ranker without the 136 features of the paraphrase recognizer.", "labels": [], "entities": []}, {"text": "We used the squared correlation coefficient \u03c1 2 to evaluate SVR-REC against SVR-BASE.", "labels": [], "entities": [{"text": "squared correlation coefficient \u03c1", "start_pos": 12, "end_pos": 45, "type": "METRIC", "confidence": 0.7734221071004868}, {"text": "SVR-REC", "start_pos": 60, "end_pos": 67, "type": "DATASET", "confidence": 0.7799912691116333}, {"text": "SVR-BASE", "start_pos": 76, "end_pos": 84, "type": "DATASET", "confidence": 0.9549450874328613}]}, {"text": "The \u03c1 2 coefficient shows how well the scores returned by the SVR are correlated with the desired scores y(x i ); the higher the \u03c1 2 the higher the agreement.", "labels": [], "entities": [{"text": "agreement", "start_pos": 148, "end_pos": 157, "type": "METRIC", "confidence": 0.9846306443214417}]}, {"text": "proceed to investigate how well our overall generateand-rank method (with SVR-REC) compares against a state of the art paraphrase generator.", "labels": [], "entities": []}, {"text": "As already mentioned, recently presented a method (we call it that outperforms their previous method (, which used paraphrasing rules and an SMT-like decoder (we call that previous method ZHAO-RUL).", "labels": [], "entities": []}, {"text": "Given an input sentence S, ZHAO-ENG produces candidate paraphrases by translating S to 6 pivot languages via 3 different commercial machine translation engines (treated as black boxes) and then back to the original language, again via 3 machine translation engines (54 combinations).", "labels": [], "entities": []}, {"text": "Roughly speaking, ZHAO-ENG then ranks the candidate paraphrases by their average distance from all the other candidates, selecting the candidate(s) with the smallest distance; distance is measured as BLEU score).", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 200, "end_pos": 210, "type": "METRIC", "confidence": 0.9810542166233063}]}, {"text": "Hence, ZHAO-ENG is also, in effect, a generate-and-rank paraphraser, but the candidates are generated by invoking multiple machine translation engines instead of applying paraphrasing rules, and they are ranked by the average distance measure rather than using an SVR.", "labels": [], "entities": []}, {"text": "An obvious practical advantage of ZHAO-ENG is that it exploits the vast resources of existing commercial machine translation engines when generating candidate paraphrases, which allows it to always obtain large numbers of candidate paraphrases.", "labels": [], "entities": []}, {"text": "By contrast, the collection of paraphrasing rules that we currently use does not manage to produce any candidate paraphrases in 40% of the sentences of the New York Times part of AQUAINT, because no rule applies.", "labels": [], "entities": [{"text": "New York Times part of AQUAINT", "start_pos": 156, "end_pos": 186, "type": "DATASET", "confidence": 0.7755514681339264}]}, {"text": "Hence, in terms of ability to always paraphrase the input, ZHAO-ENG is clearly better, though it should be possible to improve our methods's performance in that respect by using larger collections of paraphrasing rules.", "labels": [], "entities": []}, {"text": "A further interesting question, however, is how good the paraphrases of the two methods are, when both methods manage to paraphrase the input, i.e., when at least one para-phrasing rule applies to S.", "labels": [], "entities": []}, {"text": "This scenario can be seen as an emulation of the case where the collection of paraphrasing rules is sufficiently large to guarantee that at least one rule applies to any source sentence.", "labels": [], "entities": []}, {"text": "To answer the latter question, we re-implemented ZHAO-ENG, with the same machine translation engines and languages used by.", "labels": [], "entities": []}, {"text": "We also trained our paraphraser (with SVR-REC) on the training part of the dataset of Section 3.", "labels": [], "entities": [{"text": "SVR-REC", "start_pos": 38, "end_pos": 45, "type": "METRIC", "confidence": 0.7260305285453796}]}, {"text": "We then selected 300 random source sentences S from AQUAINT that matched at least one of the paraphrasing rules, excluding sentences that had been used before.", "labels": [], "entities": [{"text": "AQUAINT", "start_pos": 52, "end_pos": 59, "type": "DATASET", "confidence": 0.8830133080482483}]}, {"text": "Then, for each one of the 300 S sentences, we kept the single best candidate paraphrase C 1 and C 2 , respectively, returned by our paraphraser and ZHAO-ENG.", "labels": [], "entities": []}, {"text": "The resulting S, C 1 and S, C 2 pairs were given to 10 human judges.", "labels": [], "entities": []}, {"text": "This time the judges assigned only grammaticality and meaning preservation scores (on a 1-4 scale); diversity was again computed as edit distance.", "labels": [], "entities": [{"text": "meaning preservation", "start_pos": 54, "end_pos": 74, "type": "TASK", "confidence": 0.7196101993322372}]}, {"text": "Each pair was evaluated by one judge, who was given an equal number of pairs from the two methods, without knowing which method each pair came from.", "labels": [], "entities": []}, {"text": "The same judge never rated two pairs with the same S.", "labels": [], "entities": []}, {"text": "Since we had noway to make ZHAO-ENG sensitive to \u03bb 1 , \u03bb 2 , \u03bb 3 , we trained SVR-REC with \u03bb 1 = \u03bb 2 = 1/3, as the most neutral combination of weights.", "labels": [], "entities": [{"text": "SVR-REC", "start_pos": 78, "end_pos": 85, "type": "DATASET", "confidence": 0.7026743292808533}]}, {"text": "lists the average grammaticality, meaning preservation, and diversity scores of the two methods.", "labels": [], "entities": [{"text": "meaning preservation", "start_pos": 34, "end_pos": 54, "type": "TASK", "confidence": 0.7882584929466248}, {"text": "diversity", "start_pos": 60, "end_pos": 69, "type": "METRIC", "confidence": 0.956670880317688}]}, {"text": "All scores were normalized in, but the reader should keep in mind that diversity was computed as edit distance, whereas the other two scores were provided by human judges on a 1-4 scale.", "labels": [], "entities": [{"text": "edit distance", "start_pos": 97, "end_pos": 110, "type": "METRIC", "confidence": 0.8895772099494934}]}, {"text": "The grammaticality score of our method was better than ZHAO-ENG's, and the difference was statistically significant.", "labels": [], "entities": []}, {"text": "In meaning preservation, ZHAO-ENG was slightly better, but the difference was not statistically significant.", "labels": [], "entities": [{"text": "meaning preservation", "start_pos": 3, "end_pos": 23, "type": "TASK", "confidence": 0.8944141268730164}, {"text": "ZHAO-ENG", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9690343737602234}]}, {"text": "The difference in diversity was larger and statistically significant, with the diversity scores indicating that it takes approximately twice as many edit operations (insert, delete, replace) to turn each source sentence to ZHAO-ENG's paraphrase, compared to the paraphrase of our method.", "labels": [], "entities": []}, {"text": "We note that our method can be tuned, by adjusting the \u03bb i weights, to produce paraphrases with score (%) our method  higher grammaticality, meaning preservation, or diversity scores; for example, we could increase \u03bb 3 and decrease \u03bb 1 to obtain higher diversity at the cost of lower grammaticality in the results of.", "labels": [], "entities": []}, {"text": "It is unclear how ZHAO-ENG could be tuned that way.", "labels": [], "entities": []}, {"text": "Overall, our method seems to perform well against ZHAO-ENG, despite the vastly larger resources of ZHAO-ENG, provided of course that we limit ourselves to source sentences to which paraphrasing rules apply.", "labels": [], "entities": []}, {"text": "It would be interesting to investigate in future work if our method's coverage (sentences it can paraphrase) can increase to ZHAO-ENG's level by using larger collections of paraphrasing rules.", "labels": [], "entities": []}, {"text": "It would also be interesting to combine the two methods, perhaps by using SVR-REC (without features for the quality scores of the rules) to rank candidate paraphrases generated by ZHAO-ENG.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Inter-annotator agreement when manually eval- uating candidate paraphrases.", "labels": [], "entities": []}, {"text": " Table 2: Evaluation of our paraphrasing method (with  SVR-REC) against ZHAO-ENG, using human judges. Re- sults in bold indicate statistically significant differences.", "labels": [], "entities": [{"text": "Re- sults", "start_pos": 102, "end_pos": 111, "type": "METRIC", "confidence": 0.9462141791979471}]}]}