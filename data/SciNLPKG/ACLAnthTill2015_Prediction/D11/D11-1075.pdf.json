{"title": [{"text": "Unsupervised Information Extraction with Distributional Prior Knowledge", "labels": [], "entities": [{"text": "Unsupervised Information Extraction", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.6014498670895895}]}], "abstractContent": [{"text": "We address the task of automatic discovery of information extraction template from a given text collection.", "labels": [], "entities": []}, {"text": "Our approach clusters candidate slot fillers to identify meaningful template slots.", "labels": [], "entities": []}, {"text": "We propose a generative model that incorporates distributional prior knowledge to help distribute candidates in a document into appropriate slots.", "labels": [], "entities": []}, {"text": "Empirical results suggest that the proposed prior can bring substantial improvements to our task as compared to a K-means baseline and a Gaussian mixture model baseline.", "labels": [], "entities": []}, {"text": "Specifically, the proposed prior has shown to be effective when coupled with discriminative features of the candidates.", "labels": [], "entities": []}], "introductionContent": [{"text": "Information extraction (IE) is the task of extracting information from natural language texts to fill a database record following a structure called a template.", "labels": [], "entities": [{"text": "Information extraction (IE)", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.8832369685173035}]}, {"text": "Such templates are usually defined based on the domain of interest.", "labels": [], "entities": []}, {"text": "For example, the domain in the Sixth Message Understanding Conference) is management succession, and the pre-defined template consists of the slots position, the person leaving, the person joining, and the organization.", "labels": [], "entities": []}, {"text": "Previous research on IE often requires the predefinition of templates.", "labels": [], "entities": [{"text": "IE", "start_pos": 21, "end_pos": 23, "type": "TASK", "confidence": 0.990993082523346}]}, {"text": "Template construction is usually done manually by domain experts, and annotated documents are often created to facilitate supervised learning approaches to IE.", "labels": [], "entities": [{"text": "Template construction", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.9519914090633392}, {"text": "IE", "start_pos": 156, "end_pos": 158, "type": "TASK", "confidence": 0.9833342432975769}]}, {"text": "However, both manual template construction and data annotation are labor-intensive.", "labels": [], "entities": [{"text": "template construction", "start_pos": 21, "end_pos": 42, "type": "TASK", "confidence": 0.8065250217914581}]}, {"text": "More importantly, templates and annotated data usually cannot be re-used in new domains due to domain dependency.", "labels": [], "entities": []}, {"text": "It is therefore natural to consider the problem of unsupervised template induction and information extraction.", "labels": [], "entities": [{"text": "template induction", "start_pos": 64, "end_pos": 82, "type": "TASK", "confidence": 0.7312949746847153}, {"text": "information extraction", "start_pos": 87, "end_pos": 109, "type": "TASK", "confidence": 0.8785869479179382}]}, {"text": "This is the topic of this paper.", "labels": [], "entities": []}, {"text": "There have been a few previous attempts to address the unsupervised IE problem).", "labels": [], "entities": [{"text": "IE", "start_pos": 68, "end_pos": 70, "type": "TASK", "confidence": 0.9709721207618713}]}, {"text": "These approaches have a commonality: they try to cluster candidate slot fillers, which are often nouns and noun phrases, into slots of the template to be constructed.", "labels": [], "entities": []}, {"text": "However, most of them have neglected the following important observation: a single document or text segment tends to cover different slots rather than redundantly fill the same slot.", "labels": [], "entities": []}, {"text": "In other words, during clustering, candidates within the same text segment should be more likely to be distributed into different clusters.", "labels": [], "entities": []}, {"text": "In this paper, we propose a generative model that incorporates this distributional prior knowledge.", "labels": [], "entities": [{"text": "generative", "start_pos": 28, "end_pos": 38, "type": "TASK", "confidence": 0.9631880521774292}]}, {"text": "We define a prior distribution over the possible label assignments in a document or a text segment such that a more diversified label assignment is preferred.", "labels": [], "entities": []}, {"text": "This prior is based on the Poisson distribution.", "labels": [], "entities": []}, {"text": "We also compare a number of generative models for generating slot fillers and find that the Gaussian mixture model is the best.", "labels": [], "entities": []}, {"text": "We then combine the Poissonbased label assignment prior with the Gaussian mixture model to perform slot clustering.", "labels": [], "entities": [{"text": "slot clustering", "start_pos": 99, "end_pos": 114, "type": "TASK", "confidence": 0.857152909040451}]}, {"text": "We find that compared with a K-means baseline and a Gaussian mixture model baseline, our combined model with the proposed label assignment prior substantially performs better on two of the three data sets we use for evaluation.", "labels": [], "entities": []}, {"text": "We further analyze the results on the third data set and find that the proposed prior will have little effect if there are no good discriminative features to begin with.", "labels": [], "entities": []}, {"text": "In summary, we find that our Poisson-based label assignment prior is effective when coupled with good discriminative features.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we first describe the data sets we used in our experiments, detailing the target slots and candidates in each data set, as well as features we extract for the candidates.", "labels": [], "entities": []}, {"text": "We then describe our evaluation metrics, followed by experimental results.", "labels": [], "entities": []}, {"text": "We use the standard K-means algorithm as a non-generative baseline, since K-means is commonly used for clustering.", "labels": [], "entities": []}, {"text": "To evaluate clustering results, we match each slot in the labeled data to the cluster that gives the best F1-measure when evaluated for the slot.", "labels": [], "entities": [{"text": "F1-measure", "start_pos": 106, "end_pos": 116, "type": "METRIC", "confidence": 0.9983108043670654}]}, {"text": "We report the precision (P), recall (R) and F1-measure for individual slot labels, as well as the macro-and micro-average results across all labels for each experiment.", "labels": [], "entities": [{"text": "precision (P)", "start_pos": 14, "end_pos": 27, "type": "METRIC", "confidence": 0.9625006467103958}, {"text": "recall (R)", "start_pos": 29, "end_pos": 39, "type": "METRIC", "confidence": 0.965454563498497}, {"text": "F1-measure", "start_pos": 44, "end_pos": 54, "type": "METRIC", "confidence": 0.9888912439346313}]}, {"text": "We conduct 10 trials of experiment on each model and each data set with different random initializations.", "labels": [], "entities": []}, {"text": "We report the trials that give the smallest within-cluster sum-of-squares (WCSS) distance for K-means, and those that give the highest log-likelihood of data for all other models.", "labels": [], "entities": [{"text": "within-cluster sum-of-squares (WCSS) distance", "start_pos": 44, "end_pos": 89, "type": "METRIC", "confidence": 0.7155009011427561}]}, {"text": "Experimental trials are run until the change in WCSS/log-likelihood between two EM iterations is smaller than 1 \u00d7 10 \u22126 . All trials converged within 30 minutes.", "labels": [], "entities": [{"text": "WCSS", "start_pos": 48, "end_pos": 52, "type": "METRIC", "confidence": 0.7901504039764404}]}, {"text": "All models we evaluate involve a parameter K, which is the number of values that y can take on.", "labels": [], "entities": []}, {"text": "The value of K is manually fixed in this study.", "labels": [], "entities": [{"text": "K", "start_pos": 13, "end_pos": 14, "type": "METRIC", "confidence": 0.9707697629928589}]}, {"text": "As noted, we use a garbage slot to capture irrelevant candidates, thus the value of K is set to the number of target slots plus 1 for each data set.", "labels": [], "entities": []}, {"text": "We empirically set the adjustable parameters in the proposed prior, and the weight of the regularization term in the locally normalized logistic regression model, denoted by \u03b2.", "labels": [], "entities": []}, {"text": "Exact settings are given in the next subsection.", "labels": [], "entities": []}, {"text": "Note that the focus of our experiments is on evaluating the effectiveness of the proposed prior.", "labels": [], "entities": []}, {"text": "We leave the task of learning the various parameter values to future work.", "labels": [], "entities": []}, {"text": "We first evaluate the existing generative models described in Section 5 with the multinomial prior.", "labels": [], "entities": []}, {"text": "summarizes the performance of Naive Bayes (NB), the Bernoulli mixture model (BMM), the Gaussian mixture model (GMM), the locally normalized logistic regression (LNLR) model, and Kmeans.", "labels": [], "entities": []}, {"text": "We only show the F1 measures in the table due to space limit.", "labels": [], "entities": [{"text": "F1", "start_pos": 17, "end_pos": 19, "type": "METRIC", "confidence": 0.9980218410491943}]}, {"text": "We first observe that NB does not perform well for our task.", "labels": [], "entities": []}, {"text": "LNLR, which is an interesting contribution in its own right, does not seem to be suitable for our task as well.", "labels": [], "entities": [{"text": "LNLR", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.983190655708313}]}, {"text": "While NB and LNLR are inferior to K-means for all three data sets, BMM shows mixed results.", "labels": [], "entities": [{"text": "NB", "start_pos": 6, "end_pos": 8, "type": "DATASET", "confidence": 0.6087977886199951}, {"text": "LNLR", "start_pos": 13, "end_pos": 17, "type": "METRIC", "confidence": 0.4947899281978607}, {"text": "BMM", "start_pos": 67, "end_pos": 70, "type": "METRIC", "confidence": 0.616645336151123}]}, {"text": "Specifically, BMM outperforms Kmeans for aviation incidents, but performs poorly for seminar announcements.", "labels": [], "entities": [{"text": "BMM", "start_pos": 14, "end_pos": 17, "type": "METRIC", "confidence": 0.708925187587738}, {"text": "seminar announcements", "start_pos": 85, "end_pos": 106, "type": "TASK", "confidence": 0.663727879524231}]}, {"text": "GMM and K-means achieve similar results, which is not surprising because K-means can be viewed as a special case of the spherical GMM we used (.", "labels": [], "entities": [{"text": "GMM", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9364318251609802}]}, {"text": "Overall speaking, results show that GMM is the best among the four generative models for the distri-(a) Results on seminar announcements.", "labels": [], "entities": []}, {"text": "No macro-and micro-average result is reported for NB and BMM as they merged the etime cluster with the stime cluster.", "labels": [], "entities": [{"text": "NB", "start_pos": 50, "end_pos": 52, "type": "DATASET", "confidence": 0.8808593153953552}, {"text": "BMM", "start_pos": 57, "end_pos": 60, "type": "DATASET", "confidence": 0.4793463349342346}]}, {"text": "Numbers in brackets are the respective measures of the stime cluster when evaluated for etime.", "labels": [], "entities": []}, {"text": "bution p(x|y; \u0398).", "labels": [], "entities": []}, {"text": "We proceed with incorporating the proposed prior into GMM for further explorations.", "labels": [], "entities": [{"text": "GMM", "start_pos": 54, "end_pos": 57, "type": "DATASET", "confidence": 0.9612242579460144}]}], "tableCaptions": [{"text": " Table 1: Performance summary of the different generative models and K-means in terms of F1.", "labels": [], "entities": [{"text": "F1", "start_pos": 89, "end_pos": 91, "type": "METRIC", "confidence": 0.9980173110961914}]}, {"text": " Table 3: Comparison between the combined model (GMM with the proposed prior), GMM and K-means.", "labels": [], "entities": [{"text": "GMM", "start_pos": 79, "end_pos": 82, "type": "DATASET", "confidence": 0.7761096954345703}]}, {"text": " Table 4: Top-10 features in the PersonIn cluster, as  learned by GMM with the proposed prior.", "labels": [], "entities": [{"text": "GMM", "start_pos": 66, "end_pos": 69, "type": "DATASET", "confidence": 0.9206215739250183}]}, {"text": " Table 5: Top-10 features in the PersonOut cluster, as  learned by GMM with the proposed prior.", "labels": [], "entities": [{"text": "GMM", "start_pos": 67, "end_pos": 70, "type": "DATASET", "confidence": 0.910257875919342}]}]}