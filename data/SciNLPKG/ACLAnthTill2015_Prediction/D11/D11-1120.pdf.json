{"title": [], "abstractContent": [{"text": "Accurate prediction of demographic attributes from social media and other informal online content is valuable for marketing, personalization, and legal investigation.", "labels": [], "entities": []}, {"text": "This paper describes the construction of a large, multilingual dataset labeled with gender, and investigates statistical models for determining the gender of uncharacterized Twitter users.", "labels": [], "entities": []}, {"text": "We explore several different classifier types on this dataset.", "labels": [], "entities": []}, {"text": "We show the degree to which classifier accuracy varies based on tweet volumes as well as when various kinds of profile metadata are included in the models.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.9724332690238953}]}, {"text": "We also perform a large-scale human assessment using Amazon Mechanical Turk.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 53, "end_pos": 75, "type": "DATASET", "confidence": 0.9656424323717753}]}, {"text": "Our methods significantly out-perform both baseline models and almost all humans on the same task.", "labels": [], "entities": []}], "introductionContent": [{"text": "The rapid growth of social media in recent years, exemplified by Facebook and Twitter, has led to a massive volume of user-generated informal text.", "labels": [], "entities": []}, {"text": "This in turn has sparked a great deal of research interest in aspects of social media, including automatically identifying latent demographic features of online users.", "labels": [], "entities": []}, {"text": "Many latent features have been explored, but gender and age have generated great interest (.", "labels": [], "entities": []}, {"text": "Accurate prediction of these features would be useful for marketing and personalization concerns, as well as for legal investigation.", "labels": [], "entities": []}, {"text": "In this work, we investigate the development of highperformance classifiers for identifying the gender of Twitter users.", "labels": [], "entities": [{"text": "identifying the gender of Twitter users", "start_pos": 80, "end_pos": 119, "type": "TASK", "confidence": 0.7434792915980021}]}, {"text": "We cast gender identification as the obvious binary classification problem, and explore the use of a number of text-based features.", "labels": [], "entities": [{"text": "gender identification", "start_pos": 8, "end_pos": 29, "type": "TASK", "confidence": 0.7450278997421265}]}, {"text": "In Section 2, we describe our Twitter corpus, and our methods for labeling a large subset of this data for gender.", "labels": [], "entities": []}, {"text": "In Section 3 we discuss the features that are used in our classifiers.", "labels": [], "entities": []}, {"text": "We describe our Experiments in Section 4, including our exploration of several different classifier types.", "labels": [], "entities": []}, {"text": "In we present and analyze performance results, and discuss some directions for acquiring additional data by simple self-training techniques.", "labels": [], "entities": []}, {"text": "Finally in Section 6 we summarize our findings, and describe extensions to the work that we are currently exploring.", "labels": [], "entities": []}], "datasetContent": [{"text": "We formulate gender labeling as the obvious binary classification problem.", "labels": [], "entities": [{"text": "gender labeling", "start_pos": 13, "end_pos": 28, "type": "TASK", "confidence": 0.8319638073444366}]}, {"text": "The sheer volume of data presents a challenge for many of the available machine learning toolkits, e.g. WEKA () or MALLET).", "labels": [], "entities": [{"text": "WEKA", "start_pos": 104, "end_pos": 108, "type": "METRIC", "confidence": 0.4481815695762634}, {"text": "MALLET", "start_pos": 115, "end_pos": 121, "type": "METRIC", "confidence": 0.6131079196929932}]}, {"text": "Our 4.1 million tweet training corpus contains 15.6 million distinct features, with feature vectors for some experiments requiring over 20 gigabytes of storage.", "labels": [], "entities": []}, {"text": "To speed experimentation and reduce the memory footprint, we perform a one-time feature generation preprocessing step in which we convert each feature pattern (such as \"caseful screen name character trigram: Joh\") to an integer codeword.", "labels": [], "entities": []}, {"text": "The learning algorithms do not access the codebook at anytime and instead deal solely with vectors of integers.", "labels": [], "entities": []}, {"text": "We compress the data further by concatenating all of a user's features into a single vector that represents the union of every tweet produced by that user.", "labels": [], "entities": []}, {"text": "This condenses the dataset to about 180,000 vectors occupying 11 gigabytes of storage.", "labels": [], "entities": []}, {"text": "We performed initial feasibility experiments using a wide variety of different classifier types, including Support Vector Machines, Naive Bayes, and Balanced Win-now2.", "labels": [], "entities": []}, {"text": "These initial experiments were based only on caseful word unigram features from tweet texts, which represent less than 3% of the total feature space but still include large numbers of irrelevant features.", "labels": [], "entities": []}, {"text": "Performance as measured on the development set ranged from Naive Bayes at 67.0% accuracy to Balanced Winnow2 at 74.0% accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 80, "end_pos": 88, "type": "METRIC", "confidence": 0.9990830421447754}, {"text": "accuracy", "start_pos": 118, "end_pos": 126, "type": "METRIC", "confidence": 0.9933112859725952}]}, {"text": "A LIBSVM) implementation of SVM with a linear kernel achieved 71.8% accuracy, but required over fifteen hours of training time while Winnow needed less than seven minutes.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.9994518160820007}, {"text": "Winnow", "start_pos": 133, "end_pos": 139, "type": "DATASET", "confidence": 0.8738946914672852}]}, {"text": "No classifier that we evaluated was able to match Winnow's combination of accuracy, speed, and robustness to increasing amounts of irrelevant features.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.99944669008255}, {"text": "speed", "start_pos": 84, "end_pos": 89, "type": "METRIC", "confidence": 0.9871593713760376}]}, {"text": "We built our own implementation of the Balanced Winnow2 algorithm which allowed us to iterate repeatedly over the training data on disk rather than caching the entire dataset in memory.", "labels": [], "entities": []}, {"text": "This reduced our memory requirements to the point that we were able to train on the entire dataset using a single machine with 8 gigabytes of RAM.", "labels": [], "entities": []}, {"text": "We performed a grid search to select learning parameters by measuring their affect on Winnow's performance on the development set.", "labels": [], "entities": []}, {"text": "We found that two sets of parameters were required: a low learning rate (0.03) was effective when using only one type of input feature (such as only screen name features, or only tweet text features), and a higher learning rate (0.20) was required when mixing multiple types of features in one classifier.", "labels": [], "entities": []}, {"text": "In both cases we used a relatively large margin (35%) and cooled the learning rate by 50% after each iteration.", "labels": [], "entities": []}, {"text": "These learning parameters were used during all of the experiments that follow.", "labels": [], "entities": []}, {"text": "All gender prediction models were trained using data from the training set and evaluated on data from the development set.", "labels": [], "entities": [{"text": "gender prediction", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.7612272799015045}]}, {"text": "The test set was held out entirely until we finalized our best performing models.", "labels": [], "entities": []}], "tableCaptions": []}