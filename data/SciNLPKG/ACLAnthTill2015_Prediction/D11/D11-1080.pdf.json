{"title": [{"text": "Statistical Machine Translation with Local Language Models", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.7098668813705444}]}], "abstractContent": [{"text": "Part-of-speech language modeling is commonly used as a component in statistical machine translation systems, but there is mixed evidence that its usage leads to significant improvements.", "labels": [], "entities": [{"text": "Part-of-speech language modeling", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7026927272478739}, {"text": "statistical machine translation", "start_pos": 68, "end_pos": 99, "type": "TASK", "confidence": 0.6400836010773977}]}, {"text": "We argue that its limited effectiveness is due to the lack of lexicalization.", "labels": [], "entities": []}, {"text": "We introduce anew approach that builds a separate local language model for each word and part-of-speech pair.", "labels": [], "entities": []}, {"text": "The resulting models lead to more context-sensitive probability distributions and we also exploit the fact that different local models are used to estimate the language model probability of each word during decoding.", "labels": [], "entities": []}, {"text": "Our approach is evaluated for Arabic-and Chinese-to-English translation.", "labels": [], "entities": [{"text": "Chinese-to-English translation", "start_pos": 41, "end_pos": 71, "type": "TASK", "confidence": 0.5689937174320221}]}, {"text": "We show that it leads to statistically significant improvements for multiple test sets and also across different genres, when compared against a competitive baseline and a system using a part-of-speech model.", "labels": [], "entities": []}], "introductionContent": [{"text": "Language models are an important component of current statistical machine translation systems.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 54, "end_pos": 85, "type": "TASK", "confidence": 0.6431228717168173}]}, {"text": "They affect the selection of phrase translation candidates and reordering choices by estimating the probability that an application of a phrase translation is a fluent continuation of the current translation hypothesis.", "labels": [], "entities": [{"text": "phrase translation", "start_pos": 29, "end_pos": 47, "type": "TASK", "confidence": 0.7770066559314728}]}, {"text": "The size and domain of the language model can have a significant impact on translation quality.", "labels": [], "entities": [{"text": "translation", "start_pos": 75, "end_pos": 86, "type": "TASK", "confidence": 0.9691628217697144}]}, {"text": "have shown that each doubling of the training data from the news domain (used to build the language model), leads to improvements of approximately 0.5 BLEU points.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 151, "end_pos": 155, "type": "METRIC", "confidence": 0.9994003772735596}]}, {"text": "On the other hand, each doubling using general web data leads to improvements of approximately 0.15 BLEU points.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 100, "end_pos": 104, "type": "METRIC", "confidence": 0.9992851614952087}]}, {"text": "While large n-gram language models do lead to improved translation quality, they still lack any generalization beyond the surface forms.", "labels": [], "entities": []}, {"text": "Consider example (1), which is a short sentence fragment from the MT09 Arabic-English test set, with the corresponding machine translation output (1.b), from a phrase-based statistical machine translation system, and reference translation (1.c).", "labels": [], "entities": [{"text": "MT09 Arabic-English test set", "start_pos": 66, "end_pos": 94, "type": "DATASET", "confidence": 0.8722684383392334}, {"text": "phrase-based statistical machine translation", "start_pos": 160, "end_pos": 204, "type": "TASK", "confidence": 0.6086559146642685}, {"text": "reference translation", "start_pos": 217, "end_pos": 238, "type": "TASK", "confidence": 0.7507725954055786}]}, {"text": "the background of press statements of controversial and accused him ... c. ...", "labels": [], "entities": []}, {"text": "the background of controversial press statements and accused him ...", "labels": [], "entities": []}, {"text": "Clearly, the adjective \"controversial\" should precede the nouns \"press statement\", but since the AFP and Xinhua portions of the Gigaword corpus, used to build the language model for the translation system, do not contain this surface n-gram, translations with obviously ungrammatical constructions such as (1.b) can result.", "labels": [], "entities": [{"text": "AFP and Xinhua portions of the Gigaword corpus", "start_pos": 97, "end_pos": 143, "type": "DATASET", "confidence": 0.7587802931666374}]}, {"text": "For unseen n-grams, one would like to model adjectives as being likely to precede nouns in English, for example.", "labels": [], "entities": []}, {"text": "A straightforward approach to address this is to exploit the part-of-speech (POS) tags of the target words during translation).", "labels": [], "entities": []}, {"text": "Though models exploiting POS information are not expressive enough to model long-distance dependencies, they can account for locally ungrammatical constructions such as.", "labels": [], "entities": []}, {"text": "Several attempts have been made to interpolate POS language models 869 with surface models.", "labels": [], "entities": []}, {"text": "Under constrained data conditions, this can lead to improvements.", "labels": [], "entities": []}, {"text": "But once larger amounts of training data are used, the gains obtained from adding POS language models decline substantially.", "labels": [], "entities": []}, {"text": "This raises the question of why POS language models are not more effective.", "labels": [], "entities": []}, {"text": "We argue that one of the short-comings of previous approaches to using POS language models is that these models are estimated globally, not lexically anchored, and hence rather context insensitive.", "labels": [], "entities": []}, {"text": "In this paper, we introduce a novel approach that builds and uses individual, local POS language models for each word in the vocabulary.", "labels": [], "entities": []}, {"text": "Our experiments show that it leads to statistically significant improvements over a competitive baseline, using lexicalized reordering and a sizable 5-gram word language model, as well as a standard 7-gram POS language model approach.", "labels": [], "entities": []}], "datasetContent": [{"text": "Three approaches are compared in our experiments: the baseline system is a phrase-based statistical machine translation system (, very similar to Moses ( , using a wordbased 5-gram language model.", "labels": [], "entities": [{"text": "phrase-based statistical machine translation", "start_pos": 75, "end_pos": 119, "type": "TASK", "confidence": 0.5741662755608559}]}, {"text": "The second approach extends the baseline by including a 7-gram POSbased language model.", "labels": [], "entities": []}, {"text": "The third approach represents the work described in this paper, extending the baseline by including 4-gram local language models.", "labels": [], "entities": []}, {"text": "Translation quality is evaluated for two language pairs: Arabic-to-English and Chinese-to-English.", "labels": [], "entities": []}, {"text": "NIST's MT-Eval test sets are used for both pairs.", "labels": [], "entities": [{"text": "NIST", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9798449873924255}, {"text": "MT-Eval test sets", "start_pos": 7, "end_pos": 24, "type": "DATASET", "confidence": 0.8679451942443848}]}, {"text": "Only resources allowed under NIST's constrained data conditions are used to train the language, translation, and lexicalized distortion models.", "labels": [], "entities": [{"text": "translation", "start_pos": 96, "end_pos": 107, "type": "TASK", "confidence": 0.9592099785804749}]}, {"text": "To see whether our local language models result in improvements over a competitive baseline, we designed the baseline to use a large 5-gram word language model and lexicalized distortion modeling, both of which are known to cancel-out improvements gained from POS language models ().", "labels": [], "entities": []}, {"text": "The 5-gram word language model is trained on the Xinhua and AFP sections of the Gigaword corpus (3rd edition, LDC2007T40) and the target side of the bitext.", "labels": [], "entities": [{"text": "AFP", "start_pos": 60, "end_pos": 63, "type": "DATASET", "confidence": 0.8293810486793518}, {"text": "Gigaword corpus", "start_pos": 80, "end_pos": 95, "type": "DATASET", "confidence": 0.9191245138645172}]}, {"text": "We removed from the training data all documents released during the periods that overlap with the publication dates of the documents included in our development or test data sets.", "labels": [], "entities": []}, {"text": "In total, 630 million tokens were used to build the word language model.", "labels": [], "entities": []}, {"text": "The language model was trained using SRILM with modified Kneser-Ney smoothing and interpolation.", "labels": [], "entities": [{"text": "SRILM", "start_pos": 37, "end_pos": 42, "type": "DATASET", "confidence": 0.5352057218551636}]}, {"text": "It is common practice not to include higher-order n-grams that occur fewer than a predefined number of times.", "labels": [], "entities": []}, {"text": "Here, we applied rather conservative cut-offs, by ignoring 3-, 4-, and 5-grams that occurred only once.", "labels": [], "entities": []}, {"text": "The 7-gram POS and 4-gram local language models were both trained on the POS tagged English side of the bitext and 10M sentences from Gigaword's Xinhua and AFP sections.", "labels": [], "entities": [{"text": "Gigaword's Xinhua and AFP sections", "start_pos": 134, "end_pos": 168, "type": "DATASET", "confidence": 0.7919701039791107}]}, {"text": "The data for building the translation models were primarily drawn from the parallel news resources distributed by the Linguistic Data Consortium (LDC).", "labels": [], "entities": []}, {"text": "The Arabic-English bitext consists LDC catalog numbers for Arabic-English: LDC2004E72, of 11.4M source and 12.6M target tokens, and the Chinese-English bitext of 10.6M source and 12.3M target tokens.", "labels": [], "entities": []}, {"text": "Word alignment was performed running GIZA++ in both directions and generating the symmetric alignments using the 'grow-diag-finaland' heuristics.", "labels": [], "entities": [{"text": "Word alignment", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.6417616605758667}]}, {"text": "All three approaches, including the baseline, use lexicalized distortion, distinguishing between monotone, swap, and discontinuous reordering, all with respect to the previous and next phrase).", "labels": [], "entities": []}, {"text": "The distortion limit is set to 5 for Arabicto-English, and 6 for Chinese-to-English.", "labels": [], "entities": [{"text": "distortion", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.988015353679657}]}, {"text": "For each source phrase the top 30 translations are considered.", "labels": [], "entities": []}, {"text": "For tuning and testing we use NIST's official MTEval test sets.", "labels": [], "entities": [{"text": "NIST's official MTEval test sets", "start_pos": 30, "end_pos": 62, "type": "DATASET", "confidence": 0.8630580206712087}]}, {"text": "MT04 was used as the development set for both language pairs.", "labels": [], "entities": [{"text": "MT04", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9780324697494507}]}, {"text": "Testing was carried out on MT05 to MT09 for Arabic-English and MT05 to MT08 for Chinese-English.", "labels": [], "entities": [{"text": "MT05", "start_pos": 27, "end_pos": 31, "type": "DATASET", "confidence": 0.8601599931716919}, {"text": "MT09", "start_pos": 35, "end_pos": 39, "type": "DATASET", "confidence": 0.8474413752555847}, {"text": "MT05", "start_pos": 63, "end_pos": 67, "type": "DATASET", "confidence": 0.8457912802696228}, {"text": "MT08", "start_pos": 71, "end_pos": 75, "type": "DATASET", "confidence": 0.7339595556259155}]}, {"text": "NIST did not release anew Chinese-English test set for MT-Eval 2009.", "labels": [], "entities": [{"text": "NIST", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9886610507965088}, {"text": "Chinese-English test set", "start_pos": 26, "end_pos": 50, "type": "DATASET", "confidence": 0.7530852556228638}, {"text": "MT-Eval 2009", "start_pos": 55, "end_pos": 67, "type": "DATASET", "confidence": 0.6829822063446045}]}, {"text": "Parameter tuning of the decoder was done with minimum error rate training (MERT), adapted to BLEU maximization.", "labels": [], "entities": [{"text": "Parameter tuning", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8485912382602692}, {"text": "minimum error rate training (MERT)", "start_pos": 46, "end_pos": 80, "type": "METRIC", "confidence": 0.8407879344054631}, {"text": "BLEU", "start_pos": 93, "end_pos": 97, "type": "METRIC", "confidence": 0.9814487099647522}]}, {"text": "As evaluation metrics we used NIST's adaptation of BLEU-4 (), version 13a, where the brevity penalty is based on the reference translation with the closest length, and translation error rate (TER) version 0.7.25).", "labels": [], "entities": [{"text": "NIST", "start_pos": 30, "end_pos": 34, "type": "DATASET", "confidence": 0.8773454427719116}, {"text": "BLEU-4", "start_pos": 51, "end_pos": 57, "type": "METRIC", "confidence": 0.9927263259887695}, {"text": "translation error rate (TER) version", "start_pos": 168, "end_pos": 204, "type": "METRIC", "confidence": 0.859863919871194}]}, {"text": "All results reported here are case-insensitive.", "labels": [], "entities": []}, {"text": "TER scores are shown as 1-TER.", "labels": [], "entities": [{"text": "TER", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.9911859631538391}]}, {"text": "To see whether the differences between the approaches we compared in our experiments are statistically significant, we apply approximate randomization; have shown that approximate randomization is less sensitive to Type-I errors, i.e., less likely to falsely reject the null hypothesis, than bootstrap resampling) in the context of machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 332, "end_pos": 351, "type": "TASK", "confidence": 0.7983603775501251}]}], "tableCaptions": [{"text": " Table 1 LM  Kendall's \u03c4 Pearson r BLEU[%]  wordLM  0.53  0.71  80.20  POS 7gLM  0.01  0.01  48.44  locLM  0.45  0.62  76.03  \u03bbwordLM+(1\u2212\u03bb)locLM  0.54  0.73  80.98  (\u03bb = 0.92)", "labels": [], "entities": [{"text": "BLEU", "start_pos": 35, "end_pos": 39, "type": "METRIC", "confidence": 0.9550138711929321}, {"text": "wordLM", "start_pos": 44, "end_pos": 50, "type": "METRIC", "confidence": 0.8383140563964844}]}, {"text": " Table 1: Correlation between randomly permuted English  reference translations and BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 84, "end_pos": 88, "type": "METRIC", "confidence": 0.997276246547699}]}, {"text": " Table 2: Results for Arabic-to-English translation. Comparison of our approach (+locLM, rows 4a/b) to the baseline  using a word language model (wordLM, rows 1a/b) and a competing approach using a POS-based language model  (+posLM, rows 2a/b). Results are presented using BLEU", "labels": [], "entities": [{"text": "Arabic-to-English translation", "start_pos": 22, "end_pos": 51, "type": "TASK", "confidence": 0.6577485203742981}, {"text": "BLEU", "start_pos": 273, "end_pos": 277, "type": "METRIC", "confidence": 0.9915225505828857}]}, {"text": " Table 4: BLEU n-gram precision (1 \u2264 n \u2264 4) and Brevity  Penalty (BP) scores over all test sets.", "labels": [], "entities": [{"text": "BLEU n-gram precision", "start_pos": 10, "end_pos": 31, "type": "METRIC", "confidence": 0.837568461894989}, {"text": "Brevity  Penalty (BP) scores", "start_pos": 48, "end_pos": 76, "type": "METRIC", "confidence": 0.98111492395401}]}]}