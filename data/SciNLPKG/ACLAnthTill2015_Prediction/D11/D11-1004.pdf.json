{"title": [{"text": "Optimal Search for Minimum Error Rate Training", "labels": [], "entities": [{"text": "Minimum Error Rate", "start_pos": 19, "end_pos": 37, "type": "METRIC", "confidence": 0.6827345887819926}]}], "abstractContent": [{"text": "Minimum error rate training is a crucial component to many state-of-the-art NLP applications, such as machine translation and speech recognition.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 102, "end_pos": 121, "type": "TASK", "confidence": 0.8386543691158295}, {"text": "speech recognition", "start_pos": 126, "end_pos": 144, "type": "TASK", "confidence": 0.8068777918815613}]}, {"text": "However, common evaluation functions such as BLEU or word error rate are generally highly non-convex and thus prone to search errors.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 45, "end_pos": 49, "type": "METRIC", "confidence": 0.9976264834403992}, {"text": "word error rate", "start_pos": 53, "end_pos": 68, "type": "METRIC", "confidence": 0.5922704637050629}]}, {"text": "In this paper, we present LP-MERT, an exact search algorithm for minimum error rate training that reaches the global optimum using a series of reductions to linear programming.", "labels": [], "entities": []}, {"text": "Given a set of N-best lists produced from S input sentences, this algorithm finds a linear model that is globally optimal with respect to this set.", "labels": [], "entities": []}, {"text": "We find that this algorithm is polynomial in N and in the size of the model, but exponential in S.", "labels": [], "entities": []}, {"text": "We present extensions of this work that let us scale to reasonably large tuning sets (e.g., one thousand sentences), by either searching only promising regions of the parameter space, or by using a variant of LP-MERT that relies on a beam-search approximation.", "labels": [], "entities": []}, {"text": "Experimental results show improvements over the standard Och algorithm.", "labels": [], "entities": []}], "introductionContent": [{"text": "Minimum error rate training (MERT)-also known as direct loss minimization in machine learning-is a crucial component in many complex natural language applications such as speech recognition (;), statistical machine translation, dependency parsing (), summarization), and phonetic alignment.", "labels": [], "entities": [{"text": "Minimum error rate training (MERT", "start_pos": 0, "end_pos": 33, "type": "METRIC", "confidence": 0.7244284848372141}, {"text": "speech recognition", "start_pos": 171, "end_pos": 189, "type": "TASK", "confidence": 0.7341421246528625}, {"text": "statistical machine translation", "start_pos": 195, "end_pos": 226, "type": "TASK", "confidence": 0.742922862370809}, {"text": "dependency parsing", "start_pos": 228, "end_pos": 246, "type": "TASK", "confidence": 0.7737135887145996}, {"text": "summarization", "start_pos": 251, "end_pos": 264, "type": "TASK", "confidence": 0.9895166754722595}, {"text": "phonetic alignment", "start_pos": 271, "end_pos": 289, "type": "TASK", "confidence": 0.8008704483509064}]}, {"text": "MERT directly optimizes the evaluation metric under which systems are being evaluated, yielding superior performance) when compared to a likelihood-based discriminative method ().", "labels": [], "entities": []}, {"text": "In complex text generation tasks like SMT, the ability to optimize BLEU (), TER (), and other evaluation metrics is critical, since these metrics measure qualities (such as fluency and adequacy) that often do not correlate well with task-agnostic loss functions such as log-loss.", "labels": [], "entities": [{"text": "text generation tasks", "start_pos": 11, "end_pos": 32, "type": "TASK", "confidence": 0.8010375102361044}, {"text": "SMT", "start_pos": 38, "end_pos": 41, "type": "TASK", "confidence": 0.9376402497291565}, {"text": "BLEU", "start_pos": 67, "end_pos": 71, "type": "METRIC", "confidence": 0.9965428709983826}, {"text": "TER", "start_pos": 76, "end_pos": 79, "type": "METRIC", "confidence": 0.9967114925384521}]}, {"text": "While competitive in practice, MERT faces several challenges, the most significant of which is search.", "labels": [], "entities": [{"text": "MERT", "start_pos": 31, "end_pos": 35, "type": "TASK", "confidence": 0.9056943655014038}]}, {"text": "The unsmoothed error count is a highly non-convex objective function and therefore difficult to optimize directly; prior work offers no algorithm with a good approximation guarantee.", "labels": [], "entities": [{"text": "unsmoothed error count", "start_pos": 4, "end_pos": 26, "type": "METRIC", "confidence": 0.6729369362195333}]}, {"text": "While much of the earlier work in MERT () relies on standard convex optimization techniques applied to non-convex problems, the Och algorithm) represents a significant advance for MERT since it applies a series of special line minimizations that happen to be exhaustive and efficient.", "labels": [], "entities": []}, {"text": "Since this algorithm remains inexact in the multidimensional case, much of the recent work on MERT has focused on extending Och's algorithm to find better search directions and starting points, and on experimenting with other derivative-free methods such as the Nelder-Mead simplex algorithm.", "labels": [], "entities": []}, {"text": "In this paper, we present LP-MERT, an exact search algorithm for N -best optimization that exploits general assumptions commonly made with MERT, e.g., that the error metric is decomposable by sentence.", "labels": [], "entities": []}, {"text": "While there is no known optimal algo-1 Note that MERT makes two types of approximations.", "labels": [], "entities": [{"text": "MERT", "start_pos": 49, "end_pos": 53, "type": "DATASET", "confidence": 0.5900229215621948}]}, {"text": "First, the set of all possible outputs is represented only approximately, by N -best lists, lattices, or hypergraphs.", "labels": [], "entities": []}, {"text": "Second, error functions on such representations are non-convex and previous work only offers approximate techniques to optimize them.", "labels": [], "entities": []}, {"text": "Our work avoids the second approximation, while the first one is unavoidable when optimization and decoding occur in distinct steps.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our experiments in this paper focus on only the application of machine translation, though we believe that the current approach is agnostic to the particular system used to generate hypotheses.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 63, "end_pos": 82, "type": "TASK", "confidence": 0.7462947070598602}]}, {"text": "Both phrasebased systems (e.g.,) and syntaxbased systems (e.g.,,) commonly use MERT to train free parameters.", "labels": [], "entities": []}, {"text": "Our experiments use a syntax-directed translation approach: it first applies a dependency parser to the source language data at both training and test time.", "labels": [], "entities": [{"text": "syntax-directed translation", "start_pos": 22, "end_pos": 49, "type": "TASK", "confidence": 0.6751997470855713}]}, {"text": "Multi-word translation mappings constrained to be connected subgraphs of the source tree are extracted from the training data; these provide most lexical translations.", "labels": [], "entities": [{"text": "Multi-word translation mappings", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.7601979672908783}]}, {"text": "Partially lexicalized templates capturing reordering and function word insertion and deletion are also extracted.", "labels": [], "entities": [{"text": "function word insertion", "start_pos": 57, "end_pos": 80, "type": "TASK", "confidence": 0.6515024801095327}]}, {"text": "At runtime, these mappings and templates are used to construct transduction rules to convert the source tree into a target string.", "labels": [], "entities": []}, {"text": "The best transduction is sought using approximate search techniques.", "labels": [], "entities": []}, {"text": "Each hypothesis is scored by a relatively standard set of features.", "labels": [], "entities": []}, {"text": "The mappings contain five features: maximum-likelihood estimates of source given target and vice versa, lexical weighting estimates of source given target and vice versa, and a constant value that, when summed across a whole hypothesis, indicates the number of mappings used.", "labels": [], "entities": []}, {"text": "For each template, we include a maximum-likelihood estimate of the target reordering given the source structure.", "labels": [], "entities": []}, {"text": "The system may fallback to templates that mimic the source word order; the count of such templates is a feature.", "labels": [], "entities": []}, {"text": "Likewise we include a feature to count the number of source words deleted by templates, and a feature to count the number of target words inserted by templates.", "labels": [], "entities": []}, {"text": "The log probability of the target string according to a language models is also a feature; we add one such feature for each language model.", "labels": [], "entities": []}, {"text": "We include the number of target words as features to balance hypothesis length.", "labels": [], "entities": []}, {"text": "For the present system, we use the training data of WMT 2010 to construct and evaluate an English-to-44 German translation system.", "labels": [], "entities": [{"text": "WMT 2010", "start_pos": 52, "end_pos": 60, "type": "DATASET", "confidence": 0.7903800010681152}]}, {"text": "This consists of approximately 1.6 million parallel sentences, along with a much larger monolingual set of monolingual data.", "labels": [], "entities": []}, {"text": "We train two language models, one on the target side of the training data (primarily parliamentary data), and the other on the provided monolingual data (primarily news).", "labels": [], "entities": []}, {"text": "The 2009 test set is used as development data for MERT, and the 2010 one is used as test data.", "labels": [], "entities": [{"text": "MERT", "start_pos": 50, "end_pos": 54, "type": "TASK", "confidence": 0.49317580461502075}]}, {"text": "The resulting system has 13 distinct features.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Number of tested combinations for the experi- ments of", "labels": [], "entities": []}, {"text": " Table 2: BLEUn4r1[%] scores for English-German on  WMT09 for tuning sets ranging from 32 to 1024 sentences.", "labels": [], "entities": [{"text": "BLEUn4r1", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9988188147544861}, {"text": "WMT09", "start_pos": 52, "end_pos": 57, "type": "DATASET", "confidence": 0.9764919877052307}]}]}