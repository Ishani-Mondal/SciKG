{"title": [], "abstractContent": [{"text": "In this paper, we describe a novel approach to cascaded learning and inference on sequences.", "labels": [], "entities": []}, {"text": "We propose a weakly joint learning model on cascaded inference on sequences, called multilayer sequence labeling.", "labels": [], "entities": [{"text": "multilayer sequence labeling", "start_pos": 84, "end_pos": 112, "type": "TASK", "confidence": 0.5934462547302246}]}, {"text": "In this model, inference on sequences is modeled as cascaded decision.", "labels": [], "entities": []}, {"text": "However, the decision on a sequence labeling sequel to other decisions utilizes the features on the preceding results as marginalized by the probabilistic models on them.", "labels": [], "entities": []}, {"text": "It is not novel itself, but our idea central to this paper is that the probabilis-tic models on succeeding labeling are viewed as indirectly depending on the probabilistic models on preceding analyses.", "labels": [], "entities": []}, {"text": "We also propose two types of efficient dynamic programming which are required in the gradient-based optimization of an objective function.", "labels": [], "entities": []}, {"text": "One of the dynamic programming algorithms resembles back propagation algorithm for mul-tilayer feed-forward neural networks.", "labels": [], "entities": []}, {"text": "The other is a generalized version of the forward-backward algorithm.", "labels": [], "entities": []}, {"text": "We also report experiments of cascaded part-of-speech tagging and chunking of English sentences and show effectiveness of the proposed method.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 39, "end_pos": 61, "type": "TASK", "confidence": 0.6787461489439011}]}], "introductionContent": [{"text": "Machine learning approach is widely used to classify instances into discrete categories.", "labels": [], "entities": []}, {"text": "In many tasks, however, some set of inter-related labels should be decided simultaneously.", "labels": [], "entities": []}, {"text": "Such tasks are called structured prediction.", "labels": [], "entities": [{"text": "structured prediction", "start_pos": 22, "end_pos": 43, "type": "TASK", "confidence": 0.768471360206604}]}, {"text": "Sequence labeling is the simplest subclass of structured prediction problems.", "labels": [], "entities": [{"text": "Sequence labeling", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9371098279953003}]}, {"text": "In sequence labeling, the most likely one among all the possible label sequences is predicted fora given input.", "labels": [], "entities": [{"text": "sequence labeling", "start_pos": 3, "end_pos": 20, "type": "TASK", "confidence": 0.6804704368114471}]}, {"text": "Although sequence labeling is the simplest subclass, a lot of real-world tasks are modeled as problems of this simplest subclass.", "labels": [], "entities": [{"text": "sequence labeling", "start_pos": 9, "end_pos": 26, "type": "TASK", "confidence": 0.771185576915741}]}, {"text": "In addition, it might offer valuable insight and a toehold for more general and complex structured prediction problems.", "labels": [], "entities": []}, {"text": "Many models have been proposed for sequence labeling tasks, such as Hidden Markov Models (HMM), Conditional Random Fields (CRF) (), Max-Margin Markov Networks () and others.", "labels": [], "entities": [{"text": "sequence labeling tasks", "start_pos": 35, "end_pos": 58, "type": "TASK", "confidence": 0.7259992957115173}]}, {"text": "These models have been applied to lots of practical tasks in natural language processing (NLP), bioinformatics, speech recognition, and soon.", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 61, "end_pos": 94, "type": "TASK", "confidence": 0.7918444871902466}, {"text": "speech recognition", "start_pos": 112, "end_pos": 130, "type": "TASK", "confidence": 0.8335633277893066}]}, {"text": "And they have shown great success in recent years.", "labels": [], "entities": []}, {"text": "In real-world tasks, it is often needed to cascade multiple predictions.", "labels": [], "entities": []}, {"text": "A cascade of predictions here means the situation in which some of predictions are made based upon the results of other predictions.", "labels": [], "entities": []}, {"text": "Sequence labeling is not an exception.", "labels": [], "entities": [{"text": "Sequence labeling", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9399691820144653}]}, {"text": "For example, in NLP, we perform named entity recognition or base-phrase chunking forgiven sentences based on part-of-speech (POS) labels predicted by another sequence labeler.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 32, "end_pos": 56, "type": "TASK", "confidence": 0.6225226620833079}]}, {"text": "Natural languages are especially interpreted to have a hierarchy of sequential structures on different levels of abstraction.", "labels": [], "entities": []}, {"text": "Therefore, many tasks in NLP are modeled as a cascade of sequence predictions.", "labels": [], "entities": []}, {"text": "If a prediction is based upon the result of another prediction, we call the former upper stage and the latter lower stage.", "labels": [], "entities": []}, {"text": "Methods pursued fora cascade of predictionsincluding sequence predictions, of course-, are desired to perform certain types of capability.", "labels": [], "entities": []}, {"text": "One de-sired capability is rich forward information propagation, that is, the learning and estimation on each stage of predictions should utilize rich information of the results of lower stages whenever possible.", "labels": [], "entities": [{"text": "rich forward information propagation", "start_pos": 27, "end_pos": 63, "type": "TASK", "confidence": 0.701605424284935}]}, {"text": "\"Rich information\" here includes next bests and confidence information of the results of lower stages.", "labels": [], "entities": [{"text": "confidence", "start_pos": 48, "end_pos": 58, "type": "METRIC", "confidence": 0.9024397730827332}]}, {"text": "Another is backward information propagation, that is, the rich annotated data on an upper stage should affect the models on lower stages retroactively.", "labels": [], "entities": [{"text": "backward information propagation", "start_pos": 11, "end_pos": 43, "type": "TASK", "confidence": 0.6314313511053721}]}, {"text": "Many current systems fora cascade of sequence predictions adopt a simple 1-best feed-forward approach.", "labels": [], "entities": []}, {"text": "They simply take the most likely output at each prediction stage and transfer it to the next upper stage.", "labels": [], "entities": []}, {"text": "Such a framework can maximize reusability of existing sequence labeling systems.", "labels": [], "entities": []}, {"text": "On the other hand, it exhibits a strong tendency to propagate errors to upper labelers.", "labels": [], "entities": []}, {"text": "Typical improvement on the 1-best approach is to keep k-best results in the cascade of predictions.", "labels": [], "entities": []}, {"text": "However, the larger k becomes, the more difficult it is to enumerate and maintain the k-best results.", "labels": [], "entities": []}, {"text": "It is particularly prominent in sequence labeling.", "labels": [], "entities": [{"text": "sequence labeling", "start_pos": 32, "end_pos": 49, "type": "TASK", "confidence": 0.6877232789993286}]}, {"text": "The essence of this orientation is that the labeler on an upper stage utilizes the information of all the possible output candidates on lower stages.", "labels": [], "entities": []}, {"text": "However, the size of the output space can become quite large in sequence labeling.", "labels": [], "entities": [{"text": "sequence labeling", "start_pos": 64, "end_pos": 81, "type": "TASK", "confidence": 0.648970901966095}]}, {"text": "It effectively forbids explicit enumeration of all possible outputs, so it is required to represent all the labeling possibilities compactly or employ some approximation schemes.", "labels": [], "entities": []}, {"text": "Several studies are in this direction.", "labels": [], "entities": []}, {"text": "In the method proposed in, a cascades of sequence predictions is viewed as a Bayesian network, and sample sequences are drawn at each stage according to the output distribution.", "labels": [], "entities": []}, {"text": "The samples are then used to estimate the entire distribution of the cascade.", "labels": [], "entities": []}, {"text": "In the method proposed in, an upper labeler uses the probabilities marginalized on the parts of the output sequences on lower stages as weights for the features.", "labels": [], "entities": []}, {"text": "The weighted features are integrated in the model of the labeler on the upper stage.", "labels": [], "entities": []}, {"text": "A k-best approach (e.g.,) and the methods mentioned above are effective to improve the forward information propagation.", "labels": [], "entities": [{"text": "forward information propagation", "start_pos": 87, "end_pos": 118, "type": "TASK", "confidence": 0.6200221677621206}]}, {"text": "However, they can never contribute on backward information propagation.", "labels": [], "entities": [{"text": "backward information propagation", "start_pos": 38, "end_pos": 70, "type": "TASK", "confidence": 0.6081748604774475}]}, {"text": "To improve the both directions of information propagation, Some studies propose the joint learning of multiple sequence labelers.", "labels": [], "entities": [{"text": "information propagation", "start_pos": 34, "end_pos": 57, "type": "TASK", "confidence": 0.7757443189620972}]}, {"text": "proposes the joint learning method in case where multiple labels are assigned to each time slice of the input sequences.", "labels": [], "entities": []}, {"text": "It enables simultaneous learning and estimation of multiple sequence labelings on the same input sequences, where time slices of the outputs of all the out sequences are regularly aligned.", "labels": [], "entities": [{"text": "estimation of multiple sequence labelings", "start_pos": 37, "end_pos": 78, "type": "TASK", "confidence": 0.7436927318572998}]}, {"text": "However, it puts the distribution of states into Bayesian networks with cyclic dependencies, and exact inference is not tractable in such a model in general.", "labels": [], "entities": []}, {"text": "Therefore, it requires some approximate inference algorithms in learning or predictions.", "labels": [], "entities": []}, {"text": "Moreover, it only considers the cases where labels of an input sequence and all output sequences are regularly aligned.", "labels": [], "entities": []}, {"text": "It is not clear how to build a joint labeling model which handles irregular output label sequences like semi-Markov models.", "labels": [], "entities": []}, {"text": "In this paper, we propose a middle ground fora cascade of sequence predictions.", "labels": [], "entities": [{"text": "sequence predictions", "start_pos": 58, "end_pos": 78, "type": "TASK", "confidence": 0.6770044416189194}]}, {"text": "The proposed method adopts the basic idea of.", "labels": [], "entities": []}, {"text": "We first assume that the model on all the sequence labeling stages is probabilistic one.", "labels": [], "entities": []}, {"text": "In modeling of an upper stage, a feature is weighted by the marginal probability of the fragment of the outputs from a lower stage.", "labels": [], "entities": []}, {"text": "However, this is not novel itself because it is just a paraphrase of Bunescu's core idea.", "labels": [], "entities": []}, {"text": "Our intuition behind the proposed method is as follows.", "labels": [], "entities": []}, {"text": "Features integrated in the model on each stage are weighted by the marginal probabilities of the fragments of the outputs on lower stages.", "labels": [], "entities": []}, {"text": "So, if the output distributions on lower stages change, the marginal probabilities of any fragments also change, and this in turn can change the value of the features on the upper stage.", "labels": [], "entities": []}, {"text": "In other words, the features on an upper stage indirectly depend on the models on the lower stages.", "labels": [], "entities": []}, {"text": "Based on this intuition, the learning procedure of the model on an upper stage can affect not only direct model parameters, but also the weights of the features by changing the model on the lower stages.", "labels": [], "entities": []}, {"text": "Supervised learning based on annotated data on an upper stage may affect the model or model parameters on the lower stages.", "labels": [], "entities": []}, {"text": "It could be said that the information of annotation data on an upper stage is propagated back to the model on lower stages.", "labels": [], "entities": []}, {"text": "In the next section, we describe the formal nota-tion of our model.", "labels": [], "entities": []}, {"text": "In Section 3, we propose an optimization procedure according to the intuition noted above.", "labels": [], "entities": []}, {"text": "In Section 4, we report an experimental result of our method.", "labels": [], "entities": []}, {"text": "The proposed method shows some improvements on a real-world task in comparison with ordinary methods.", "labels": [], "entities": []}], "datasetContent": [{"text": "We examined effectiveness of the method proposed in this paper on areal task.", "labels": [], "entities": []}, {"text": "The task is to annotate the POS tags and to perform base-phrase chunking on English sentences.", "labels": [], "entities": []}, {"text": "Base-phrase chunking is a task to classify continuous subsequences of words into syntactic categories.", "labels": [], "entities": [{"text": "Base-phrase chunking", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.6958921551704407}]}, {"text": "This task is performed by annotating a chunking label on each word.", "labels": [], "entities": []}, {"text": "The types of chunking label consist of \"Begin-Category\", which represents the beginning of a chunk, \"Inside-Category\", which represents the inside of a chunk, and \"Other.\"", "labels": [], "entities": []}, {"text": "Usually, POS labeling runs first before base-phrase chunking is performed.", "labels": [], "entities": [{"text": "POS labeling", "start_pos": 9, "end_pos": 21, "type": "TASK", "confidence": 0.8206905722618103}]}, {"text": "Therefore, this task is atypical interesting case where a sequence labeling depends on the output from other sequence labelers.", "labels": [], "entities": []}, {"text": "The data used for our experiment consist of English sentences from the Penn Treebank project) consisting of 10948 sentences and 259104 words.", "labels": [], "entities": [{"text": "Penn Treebank project", "start_pos": 71, "end_pos": 92, "type": "DATASET", "confidence": 0.9856150150299072}]}, {"text": "We divided them into two groups, training data consisting of 8936 sentences and 211727 words and test data consisting of 2012 Algorithm 2 Forward-backward Algorithm for Calculating Feature Covariances end for sentences and 47377 words.", "labels": [], "entities": [{"text": "2012", "start_pos": 121, "end_pos": 125, "type": "DATASET", "confidence": 0.9182668924331665}]}, {"text": "The number of the POS label types is equal to 45.", "labels": [], "entities": []}, {"text": "The number of the label types used in base-phrase chunking is equal to 23.", "labels": [], "entities": [{"text": "base-phrase chunking", "start_pos": 38, "end_pos": 58, "type": "TASK", "confidence": 0.6405469179153442}]}, {"text": "We compare the proposed method to two existing sequence labeling methods as baselines.", "labels": [], "entities": []}, {"text": "The POS labeler is the same in all the three methods used in this experiment.", "labels": [], "entities": [{"text": "POS labeler", "start_pos": 4, "end_pos": 15, "type": "METRIC", "confidence": 0.8001282215118408}]}, {"text": "This labeler is a simple CRF and learned by ordinary optimization procedure.", "labels": [], "entities": []}, {"text": "One baseline method is the 1-best pipeline method.", "labels": [], "entities": []}, {"text": "A simple CRF model is learned for the chunking labeling, on the input sentences and the most likely POS label sequences predicted by the already learned POS labeler.", "labels": [], "entities": [{"text": "chunking labeling", "start_pos": 38, "end_pos": 55, "type": "TASK", "confidence": 0.85079225897789}]}, {"text": "We call this method \"CRF + CRF.\"", "labels": [], "entities": []}, {"text": "The other baseline method has a CRF model for the chunking labeling, which uses the marginalized features offered by the POS labeler.", "labels": [], "entities": [{"text": "chunking labeling", "start_pos": 50, "end_pos": 67, "type": "TASK", "confidence": 0.9123467206954956}]}, {"text": "However, the parameters of the POS labeler are fixed in the training of the chunking model.", "labels": [], "entities": []}, {"text": "This method corresponds to the method proposed in.", "labels": [], "entities": []}, {"text": "We call this baseline \"CRF + CRF-MF\" (\"MF\" for \"marginalized features\").", "labels": [], "entities": []}, {"text": "The proposed method is the same as \"CRF + CRF-MF\", except that the both labelers are jointly trained by the  procedure described in Section 3.", "labels": [], "entities": []}, {"text": "We call this proposed method \"CRF + CRF-BP\" (\"BP\" for \"back propagation\").", "labels": [], "entities": []}, {"text": "In \"CRF + CRF-BP,\" the objective function for joint learning (10) is not guaranteed to be convex, so optimization procedure is sensible to the initial configuration of the model parameters.", "labels": [], "entities": []}, {"text": "In this experiment, we set the parameter values learned by \"CRF + CRF-MF\" as the initial values for the training of the \"CRF + CRF-BP\" method.", "labels": [], "entities": []}, {"text": "Feature templates used in this experiment are listed in.", "labels": [], "entities": []}, {"text": "Although we only described the formalization and optimization procedure of the models with arc features, We use node features in the experiment.   tioned.", "labels": [], "entities": []}, {"text": "In, bold numbers indicate significant improvement over the baseline models with \u03b1 = 0.05.", "labels": [], "entities": [{"text": "\u03b1", "start_pos": 80, "end_pos": 81, "type": "METRIC", "confidence": 0.9602569937705994}]}, {"text": "From, the proposed method significantly outperforms two baseline methods on chunking performance.", "labels": [], "entities": []}, {"text": "Although the improvement on POS labeling performance by the proposed method \"CRF + CRF-BP\" is not significant, it might show that optimization procedure provides some form of backward information propagation in comparison to \"CRF + CRF-MF.\"", "labels": [], "entities": [{"text": "POS labeling", "start_pos": 28, "end_pos": 40, "type": "TASK", "confidence": 0.735161304473877}]}], "tableCaptions": [{"text": " Table 2: Experimental result (F-measure)", "labels": [], "entities": [{"text": "F-measure", "start_pos": 31, "end_pos": 40, "type": "METRIC", "confidence": 0.9452149868011475}]}]}