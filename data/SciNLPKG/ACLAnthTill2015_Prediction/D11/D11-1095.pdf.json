{"title": [{"text": "Hierarchical Verb Clustering Using Graph Factorization", "labels": [], "entities": []}], "abstractContent": [{"text": "Most previous research on verb clustering has focussed on acquiring flat classifications from corpus data, although many manually built classifications are taxonomic in nature.", "labels": [], "entities": [{"text": "verb clustering", "start_pos": 26, "end_pos": 41, "type": "TASK", "confidence": 0.7087199091911316}]}, {"text": "Also Natural Language Processing (NLP) applications benefit from taxonomic classifications because they vary in terms of the granularity they require from a classification.", "labels": [], "entities": [{"text": "taxonomic classifications", "start_pos": 65, "end_pos": 90, "type": "TASK", "confidence": 0.6922774463891983}]}, {"text": "We introduce anew clustering method called Hierarchical Graph Factorization Clustering (HGFC) and extend it so that it is optimal for the task.", "labels": [], "entities": []}, {"text": "Our results show that HGFC outperforms the frequently used agglomerative clustering on a hierarchical test set extracted from VerbNet, and that it yields state-of-the-art performance also on a flattest set.", "labels": [], "entities": [{"text": "VerbNet", "start_pos": 126, "end_pos": 133, "type": "DATASET", "confidence": 0.8416589498519897}]}, {"text": "We demonstrate how the method can be used to acquire novel classifications as well as to extend existing ones on the basis of some prior knowledge about the classification.", "labels": [], "entities": []}], "introductionContent": [{"text": "A variety of verb classifications have been built to support NLP tasks.", "labels": [], "entities": []}, {"text": "These include syntactic and semantic classifications, as well as ones which integrate aspects of both ().", "labels": [], "entities": []}, {"text": "Classifications which integrate a wide range of linguistic properties can be particularly useful for tasks suffering from data sparseness.", "labels": [], "entities": []}, {"text": "One such classification is the taxonomy of English verbs proposed by which is based on shared (morpho-)syntactic and semantic properties of verbs.", "labels": [], "entities": []}, {"text": "Levin's taxonomy or its extended version in VerbNet () has proved helpful for various NLP application tasks, including e.g. parsing, word sense disambiguation, semantic role labeling, information extraction, question-answering, and machine translation ().", "labels": [], "entities": [{"text": "VerbNet", "start_pos": 44, "end_pos": 51, "type": "DATASET", "confidence": 0.9582539796829224}, {"text": "word sense disambiguation", "start_pos": 133, "end_pos": 158, "type": "TASK", "confidence": 0.6511629025141398}, {"text": "semantic role labeling", "start_pos": 160, "end_pos": 182, "type": "TASK", "confidence": 0.6542808910210928}, {"text": "information extraction", "start_pos": 184, "end_pos": 206, "type": "TASK", "confidence": 0.8357217311859131}, {"text": "machine translation", "start_pos": 232, "end_pos": 251, "type": "TASK", "confidence": 0.8019920885562897}]}, {"text": "Because verbs change their meaning and behaviour across domains, it is important to be able to tune existing classifications as well to build novel ones in a cost-effective manner, when required.", "labels": [], "entities": []}, {"text": "In recent years, a variety of approaches have been proposed for automatic induction of Levin style classes from corpus data which could be used for this purpose ().", "labels": [], "entities": []}, {"text": "The best of such approaches have yielded promising results.", "labels": [], "entities": []}, {"text": "However, they have mostly focussed on acquiring and evaluating flat classifications.", "labels": [], "entities": []}, {"text": "Levin's classification is not flat, but taxonomic in nature, which is practical for NLP purposes since applications differ in terms of the granularity they require from a classification.", "labels": [], "entities": []}, {"text": "In this paper, we experiment with hierarchical Levin-style clustering.", "labels": [], "entities": []}, {"text": "We adopt as our baseline method a well-known hierarchical method -agglomerative clustering (AGG) -which has been previously used to acquire flat Levin-style classifications as well as hierarchical verb classifications not based on; Schulte im.", "labels": [], "entities": []}, {"text": "The method has also been popular in the related task of noun clus-1023 tering;).", "labels": [], "entities": []}, {"text": "We introduce then anew method called Hierarchical Graph Factorization Clustering (HGFC) ().", "labels": [], "entities": []}, {"text": "This graph-based, probabilistic clustering algorithm has some clear advantages over AGG (e.g. it delays the decision on a verb's cluster membership at any level until a full graph is available, minimising the problem of error propagation) and it has been shown to perform better than several other hierarchical clustering methods in recent comparisons ().", "labels": [], "entities": []}, {"text": "The method has been applied to the identification of social network communities (), but has not been used (to the best of our knowledge) in NLP before.", "labels": [], "entities": [{"text": "identification of social network communities", "start_pos": 35, "end_pos": 79, "type": "TASK", "confidence": 0.8735623121261596}]}, {"text": "We modify HGFC with anew tree extraction algorithm which ensures a more consistent result, and we propose two novel extensions to it.", "labels": [], "entities": []}, {"text": "The first is a method for automatically determining the tree structure (i.e. number of clusters to be produced for each level of the hierarchy).", "labels": [], "entities": []}, {"text": "This avoids the need to predetermine the number of clusters manually.", "labels": [], "entities": []}, {"text": "The second is addition of soft constraints to guide the clustering performance (.", "labels": [], "entities": [{"text": "clustering", "start_pos": 56, "end_pos": 66, "type": "TASK", "confidence": 0.9592497944831848}]}, {"text": "This is useful for situations where a partial (e.g. a flat) verb classification is available and the goal is to extend it.", "labels": [], "entities": []}, {"text": "Adopting a set of lexical and syntactic features which have performed well in previous works, we compare the performance of the two methods on test sets extracted from Levin and VerbNet.", "labels": [], "entities": [{"text": "VerbNet", "start_pos": 178, "end_pos": 185, "type": "DATASET", "confidence": 0.8991308212280273}]}, {"text": "When evaluated on a flat clustering task, HGFC outperforms AGG and performs very similarly with the best flat clustering method reported on the same test set.", "labels": [], "entities": [{"text": "AGG", "start_pos": 59, "end_pos": 62, "type": "DATASET", "confidence": 0.6401852369308472}]}, {"text": "When evaluated on a hierarchical task, HGFC performs considerably better than AGG at all levels of gold standard classification.", "labels": [], "entities": []}, {"text": "The constrained version of HGFC performs the best, as expected, demonstrating the usefulness of soft constraints for extending partial classifications.", "labels": [], "entities": [{"text": "HGFC", "start_pos": 27, "end_pos": 31, "type": "DATASET", "confidence": 0.8301883935928345}]}, {"text": "Our qualitative analysis shows that HGFC is capable of detecting novel information not included in our gold standards.", "labels": [], "entities": []}, {"text": "The unconstrained version can be used to acquire novel classifications from scratch while the constrained version can be used to extend existing ones with additional class members, classes and levels of hierarchy.", "labels": [], "entities": []}], "datasetContent": [{"text": "We applied the clustering methods introduced in section 3 to the test sets described in section 2 and evaluated them both quantitatively and qualitatively, as described in the subsequent sections.", "labels": [], "entities": []}, {"text": "We used class based accuracy (ACC) and adjusted rand index (R adj ) to evaluate the results on the flattest set T1 (see section 2 for details of T1-T3).", "labels": [], "entities": [{"text": "class based accuracy (ACC)", "start_pos": 8, "end_pos": 34, "type": "METRIC", "confidence": 0.7541588495175043}, {"text": "adjusted rand index (R adj )", "start_pos": 39, "end_pos": 67, "type": "METRIC", "confidence": 0.8961604833602905}]}, {"text": "ACC is the proportion of members of dominant clusters DOM-CLUST i within all classes c i . The formula of R adj is (: where n ij is the size of the intersection between class i and cluster j.", "labels": [], "entities": [{"text": "ACC", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.8901574611663818}]}, {"text": "We used normalized mutual information (NMI) and F-Score (F) to evaluate hierarchical clustering results on T2 and T3.", "labels": [], "entities": [{"text": "F-Score (F)", "start_pos": 48, "end_pos": 59, "type": "METRIC", "confidence": 0.9728763848543167}]}, {"text": "NMI measures the amount of statistical information shared by two random variables representing the clustering result and the goldstandard labels.", "labels": [], "entities": []}, {"text": "Given random variables A and B: where |v k \u2229 c j | is the number of shared membership between cluster v k and gold-standard class c j . The normalized variant of mutual information (MI) enables the comparison of clustering with different cluster numbers (.", "labels": [], "entities": [{"text": "mutual information (MI)", "start_pos": 162, "end_pos": 185, "type": "TASK", "confidence": 0.6977077603340149}]}, {"text": "F is the harmonic mean of precision (P) and recall (R).", "labels": [], "entities": [{"text": "F", "start_pos": 0, "end_pos": 1, "type": "METRIC", "confidence": 0.9807834029197693}, {"text": "precision (P)", "start_pos": 26, "end_pos": 39, "type": "METRIC", "confidence": 0.950770378112793}, {"text": "recall (R)", "start_pos": 44, "end_pos": 54, "type": "METRIC", "confidence": 0.9573792070150375}]}, {"text": "P is calculated using modified purity -a global measure which evaluates the mean precision of clusters.", "labels": [], "entities": [{"text": "P", "start_pos": 0, "end_pos": 1, "type": "METRIC", "confidence": 0.9697526693344116}, {"text": "purity", "start_pos": 31, "end_pos": 37, "type": "METRIC", "confidence": 0.7074865698814392}, {"text": "precision", "start_pos": 81, "end_pos": 90, "type": "METRIC", "confidence": 0.792163610458374}]}, {"text": "Each cluster is associated with its prevalent class.", "labels": [], "entities": []}, {"text": "The number of verbs in a cluster K that take this class is denoted by n prevalent (K).", "labels": [], "entities": []}, {"text": "n prevalent(ki) number of verbs R is calculated using ACC.", "labels": [], "entities": [{"text": "n prevalent(ki) number of verbs R", "start_pos": 0, "end_pos": 33, "type": "METRIC", "confidence": 0.6792000896400876}, {"text": "ACC", "start_pos": 54, "end_pos": 57, "type": "METRIC", "confidence": 0.8911861777305603}]}, {"text": "F is not suitable for comparing results with different cluster numbers.", "labels": [], "entities": [{"text": "F", "start_pos": 0, "end_pos": 1, "type": "METRIC", "confidence": 0.95692378282547}]}, {"text": "Therefore, we only report NMI when the number of classes in clustering and gold-standard is substantially different.", "labels": [], "entities": []}, {"text": "Finally, we supplemented quantitative evaluation with qualitative evaluation of clusters produced by different methods.", "labels": [], "entities": []}, {"text": "We first evaluated AGG and the basic (unconstrained) HGFC on the small flattest set T1.", "labels": [], "entities": [{"text": "AGG", "start_pos": 19, "end_pos": 22, "type": "DATASET", "confidence": 0.8249160647392273}, {"text": "HGFC", "start_pos": 53, "end_pos": 57, "type": "DATASET", "confidence": 0.8890748620033264}]}, {"text": "The main purpose of this evaluation was to compare the results of our methods against previously published results on the same test set.", "labels": [], "entities": []}, {"text": "The number of clusters (K) and levels (L) were inferred automatically for HGFC as described in section 3.2.3.", "labels": [], "entities": []}, {"text": "However, tomake the results comparable with previously published ones, we cut the resulting hierarchy at the level of closest match (12 clusters) to the K (13) in the gold-standard.", "labels": [], "entities": []}, {"text": "For AGG, we cut the hierarchy at 13 clusters.", "labels": [], "entities": []}, {"text": "shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.", "labels": [], "entities": [{"text": "AGG", "start_pos": 86, "end_pos": 89, "type": "DATASET", "confidence": 0.8042734265327454}]}, {"text": "In this experiment, we used the same feature set as (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.", "labels": [], "entities": []}, {"text": "When using this simple feature set, HGFC outperforms the best performing AGG clearly: 8.5% in ACC and 7.3% in R adj . We also compared HGFC against the best reported clustering method on T1 to date -that of spectral clustering by.", "labels": [], "entities": []}, {"text": "We used the feature sets C and D which are similar to the features (SCF parameterized by lexical prefences) in their experiments.", "labels": [], "entities": []}, {"text": "HGFC obtains F of 49.93% on T1 which is 5% lower than the result of.", "labels": [], "entities": [{"text": "F", "start_pos": 13, "end_pos": 14, "type": "METRIC", "confidence": 0.9998329877853394}]}, {"text": "The difference comes from the tree consistency requirement.", "labels": [], "entities": [{"text": "consistency", "start_pos": 35, "end_pos": 46, "type": "METRIC", "confidence": 0.9239180088043213}]}, {"text": "When the HGFC is forced to produce a flat clustering (a one level tree only), it achieves the F of 52.55% which is very close to the performance of spectral clustering.", "labels": [], "entities": [{"text": "F", "start_pos": 94, "end_pos": 95, "type": "METRIC", "confidence": 0.9989878535270691}]}, {"text": "We then evaluated our methods on the hierarchical test sets T2 and T3.", "labels": [], "entities": []}, {"text": "In the first set of experiments, we pre-defined the tree structure for HGFC by setting L to 3 and K at each level to be the K in the hierarchical gold standard.", "labels": [], "entities": []}, {"text": "The hierarchy produced by AGG was cut into 3 levels according to Ks in the gold standard.", "labels": [], "entities": [{"text": "AGG", "start_pos": 26, "end_pos": 29, "type": "DATASET", "confidence": 0.826318621635437}]}, {"text": "This enabled direct evaluation of the results against the 3 level gold standards using both NMI and F. The results are reported in tables 2 and 3.", "labels": [], "entities": [{"text": "NMI", "start_pos": 92, "end_pos": 95, "type": "DATASET", "confidence": 0.9269108176231384}, {"text": "F.", "start_pos": 100, "end_pos": 102, "type": "METRIC", "confidence": 0.9625120162963867}]}, {"text": "In these tables, N c is the number of clusters in HGFC clustering while N l is the number of classes in the gold standard (the two do not always correspond perfectly because a few clusters have zero members).: Performance on T3 using a pre-defined tree structure.", "labels": [], "entities": []}, {"text": "compares the results of the unconstrained version of HGFC against those of AGG on our largest test set T2.", "labels": [], "entities": [{"text": "HGFC", "start_pos": 53, "end_pos": 57, "type": "DATASET", "confidence": 0.909488320350647}, {"text": "AGG", "start_pos": 75, "end_pos": 78, "type": "DATASET", "confidence": 0.8877313733100891}]}, {"text": "As with T1, HGFC outperforms AGG clearly.", "labels": [], "entities": [{"text": "AGG", "start_pos": 29, "end_pos": 32, "type": "DATASET", "confidence": 0.6959542036056519}]}, {"text": "The benefit can now be seen at 3 different levels of hierarchy.", "labels": [], "entities": []}, {"text": "On average, the HGFC outperforms AGG 3.5% in NMI and 4.8% in F.", "labels": [], "entities": [{"text": "AGG", "start_pos": 33, "end_pos": 36, "type": "METRIC", "confidence": 0.9085853695869446}, {"text": "F", "start_pos": 61, "end_pos": 62, "type": "METRIC", "confidence": 0.9933608770370483}]}, {"text": "The difference between the methods becomes clearer when moving towards the upper levels of the hierarchy.", "labels": [], "entities": []}, {"text": "shows the results of both unconstrained and constrained versions of HGFC and those of AGG on the test set T3 (where singular classes are removed to enable proper evaluation of the constrained method).", "labels": [], "entities": [{"text": "HGFC", "start_pos": 68, "end_pos": 72, "type": "DATASET", "confidence": 0.9309099912643433}, {"text": "AGG", "start_pos": 86, "end_pos": 89, "type": "DATASET", "confidence": 0.8625993728637695}]}, {"text": "The results are generally generally better on this test set than on T2 -which is to be expected since T3 is a refined subset of T2 1 . Recall that the constrained version of HGFC learns the upper levels of classification on the basis of soft constraints set at the bottom level, as described earlier in section 3.2.4.", "labels": [], "entities": []}, {"text": "As a consequence, NMI and F are both greater than 90% at the bottom level and the results at the top level are notably lower because the impact of the constraints degrades the further away one moves from the bottom level.", "labels": [], "entities": [{"text": "NMI", "start_pos": 18, "end_pos": 21, "type": "METRIC", "confidence": 0.9976696372032166}, {"text": "F", "start_pos": 26, "end_pos": 27, "type": "METRIC", "confidence": 0.9952781200408936}]}, {"text": "Yet, the relatively high result across all levels shows that the constrained version of HGFC can be employed a useful method to extend the hierarchical structure of known classifications.", "labels": [], "entities": [{"text": "HGFC", "start_pos": 88, "end_pos": 92, "type": "DATASET", "confidence": 0.7861809134483337}]}, {"text": "Finally, shows the results for the unconstrained HGFC on T2 and and T3 when the tree structure is not pre-defined but inferred automatically as described in section 3.2.3. 6 levels are learned for T2 and 5 for T3.", "labels": [], "entities": []}, {"text": "The number of clusters produced ranges from 3 to 148 for T2 and from 6 to 64 for T3.", "labels": [], "entities": []}, {"text": "We can see that the automatically detected cluster numbers distribute evenly across different levels.", "labels": [], "entities": []}, {"text": "The scale of the clustering structure is more complete here than in the gold standards.", "labels": [], "entities": [{"text": "clustering", "start_pos": 17, "end_pos": 27, "type": "TASK", "confidence": 0.9585314393043518}]}, {"text": "In the table, N c indicates the number of clusters in the inferred tree, while N l indicates the closest match to the number of classes in the gold standard.", "labels": [], "entities": []}, {"text": "This evaluation is not fully reliable because the match between the gold standard and the clustering is poor at some levels of hierarchy.", "labels": [], "entities": []}, {"text": "However, it is encouraging to see that the results do not drop dramatically until the match between the two is really poor.", "labels": [], "entities": []}, {"text": "To gain a better insight into the performance of HGFC, we conducted further qualitative analysis of the clusters the two versions of this method produced for T3.", "labels": [], "entities": []}, {"text": "We focussed on the top level of 11 clusters (in the evaluation against the hierarchical gold standard, see table 3) as the impact of soft constraints is the weakest for the constrained method at this level.", "labels": [], "entities": []}, {"text": "As expected, the constrained HGFC kept many individual verbs belonging to same Verbnet subclass together (e.g. verbs enjoy, hate, disdain, regret, love, despise, detest, dislike, fear for the class 31.2.1) so that most clusters simply group lower level classes and their members together.", "labels": [], "entities": [{"text": "HGFC", "start_pos": 29, "end_pos": 33, "type": "DATASET", "confidence": 0.8871573805809021}]}, {"text": "Three nearly clean clusters were produced which only include sub-classes of the same class (e.g. 31.2.0 and 31.2.1 which both belong to 31.2 Admire verbs).", "labels": [], "entities": []}, {"text": "However, the remaining 8 clusters group together sub-classes (and their members) belonging to unrelated parent classes.", "labels": [], "entities": []}, {"text": "Interestingly, 6 of these make both syntactic and semantic sense.", "labels": [], "entities": []}, {"text": "For example, several such 37.7 Say verbs and 29.5 Conjencture verbs are found together which share the meaning of communication and which take similar sentential complements.", "labels": [], "entities": []}, {"text": "In contrast, none of the clusters produced by the unconstrained HGFC represent a single VerbNet class.", "labels": [], "entities": [{"text": "HGFC", "start_pos": 64, "end_pos": 68, "type": "DATASET", "confidence": 0.911584734916687}]}, {"text": "The majority represent a high number of classes and fewer members per class.", "labels": [], "entities": []}, {"text": "Yet many of the clusters make syntactic and semantic sense.", "labels": [], "entities": []}, {"text": "A good example is a cluster which includes member verbs from 9.7 Spray/Load verbs, 21.2 Carve verbs, 51.3.1 Roll verbs, and 10.4 Wipe verbs.", "labels": [], "entities": []}, {"text": "The verbs included in this cluster share the meaning of specific type of motion and show similar syntactic behaviour.", "labels": [], "entities": []}, {"text": "Thorough Levin style investigation of especially the unconstrained method would require looking at shared diathesis alternations between cluster members.", "labels": [], "entities": []}, {"text": "We left this for future work.", "labels": [], "entities": []}, {"text": "However, the analysis we conducted confirmed that the constrained method could indeed be used for extending known classifications, while the unconstrained method is more suitable for acquiring novel classifications from scratch.", "labels": [], "entities": []}, {"text": "The errors in clusters produced by both methods were mostly due to syntactic idiosyncracy and the lack of semantic information in clustering.", "labels": [], "entities": []}, {"text": "We plan to address the latter problem in our future work.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Comparison against Stevenson and Joanis  (2003)'s result on T1 (using similar features).", "labels": [], "entities": []}, {"text": " Table 2: Performance on T2 using a pre-defined tree  structure.", "labels": [], "entities": []}, {"text": " Table 3: Performance on T3 using a pre-defined tree  structure.", "labels": [], "entities": []}, {"text": " Table 4: NMI of unconstrained HGFC when trees for T2  and T3 are inferred automatically.", "labels": [], "entities": [{"text": "NMI", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9673681259155273}]}]}