{"title": [{"text": "Evaluating Dependency Parsing: Robust and Heuristics-Free Cross-Annotation Evaluation", "labels": [], "entities": [{"text": "Evaluating Dependency Parsing", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.7416397631168365}]}], "abstractContent": [{"text": "Methods for evaluating dependency parsing using attachment scores are highly sensitive to representational variation between dependency treebanks, making cross-experimental evaluation opaque.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 23, "end_pos": 41, "type": "TASK", "confidence": 0.8229337930679321}]}, {"text": "This paper develops a robust procedure for cross-experimental evaluation , based on deterministic unification-based operations for harmonizing different representations and a refined notion of tree edit distance for evaluating parse hypotheses relative to multiple gold standards.", "labels": [], "entities": [{"text": "cross-experimental evaluation", "start_pos": 43, "end_pos": 72, "type": "TASK", "confidence": 0.6881724894046783}]}, {"text": "We demonstrate that, for different conversions of the Penn Treebank into dependencies, performance trends that are observed for parsing results in isolation change or dissolve completely when parse hypotheses are normalized and brought into the same common ground.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 54, "end_pos": 67, "type": "DATASET", "confidence": 0.9925323128700256}, {"text": "parsing", "start_pos": 128, "end_pos": 135, "type": "TASK", "confidence": 0.9731312990188599}]}], "introductionContent": [{"text": "Data-driven dependency parsing has seen a considerable surge of interest in recent years.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 12, "end_pos": 30, "type": "TASK", "confidence": 0.7261258065700531}]}, {"text": "Dependency parsers have been tested on parsing sentences in English () as well as many other languages ().", "labels": [], "entities": [{"text": "Dependency parsers", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7684721052646637}]}, {"text": "The evaluation metric traditionally associated with dependency parsing is based on scoring labeled or unlabeled attachment decisions, whereby each correctly identified pair of head-dependent words is counted towards the success of the parser).", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 52, "end_pos": 70, "type": "TASK", "confidence": 0.8023935854434967}]}, {"text": "As it turns out, however, such evaluation procedures are sensitive to the annotation choices in the data on which the parser was trained.", "labels": [], "entities": []}, {"text": "Different annotation schemes often make different assumptions with respect to how linguistic content is represented in a treebank.", "labels": [], "entities": []}, {"text": "The consequence of such annotation discrepancies is that when we compare parsing results across different experiments, even ones that use the same parser and the same set of sentences, the gap between results in different experiments may not reflect a true gap in performance, but rather a difference in the annotation decisions made in the respective treebanks.", "labels": [], "entities": []}, {"text": "Different methods have been proposed for making dependency parsing results comparable across experiments.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 48, "end_pos": 66, "type": "TASK", "confidence": 0.8249911367893219}]}, {"text": "These methods include picking a single gold standard for all experiments to which the parser output should be converted, evaluating parsers by comparing their performance in an embedding task (, or neutralizing the arc direction in the native representation of dependency trees).", "labels": [], "entities": []}, {"text": "Each of these methods has its own drawbacks.", "labels": [], "entities": []}, {"text": "Picking a single gold standard skews the results in favor of parsers which were trained on it.", "labels": [], "entities": []}, {"text": "Transforming dependency trees to a set of pre-defined labeled dependencies, or into task-based features, requires the use of heuristic rules that run the risk of distorting correct information and introducing noise of their own.", "labels": [], "entities": []}, {"text": "Neutralizing the direction of arcs is limited to unlabeled evaluation and local context, and thus may not coverall possible discrepancies.", "labels": [], "entities": []}, {"text": "This paper proposes anew three-step protocol for cross-experiment parser evaluation, and in particular for comparing parsing results across data sets that adhere to different annotation schemes.", "labels": [], "entities": [{"text": "cross-experiment parser evaluation", "start_pos": 49, "end_pos": 83, "type": "TASK", "confidence": 0.809305727481842}]}, {"text": "In the 385 first step all structures are brought into a single formal space of events that neutralizes representation peculiarities (for instance, arc directionality).", "labels": [], "entities": []}, {"text": "The second step formally computes, for each sentence in the data, the common denominator of the different gold standards, containing all and only linguistic content that is shared between the different schemes.", "labels": [], "entities": []}, {"text": "The last step computes the normalized distance from this common denominator to parse hypotheses, minus the cost of distances that reflect mere annotation idiosyncrasies.", "labels": [], "entities": []}, {"text": "The procedure that implements this protocol is fully deterministic and heuristics-free.", "labels": [], "entities": []}, {"text": "We use the proposed procedure to compare dependency parsing results trained on Penn Treebank trees converted into dependency trees according to five different sets of linguistic assumptions.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 41, "end_pos": 59, "type": "TASK", "confidence": 0.8550358712673187}, {"text": "Penn Treebank trees", "start_pos": 79, "end_pos": 98, "type": "DATASET", "confidence": 0.9805036584536234}]}, {"text": "We show that when starting off with the same set of sentences and the same parser, training on different conversion schemes yields apparently significant performance gaps.", "labels": [], "entities": []}, {"text": "When results across schemes are normalized and compared against the shared linguistic content, these performance gaps decrease or dissolve completely.", "labels": [], "entities": []}, {"text": "This effect is robust across parsing algorithms.", "labels": [], "entities": []}, {"text": "We conclude that it is imperative that cross-experiment parse evaluation be a well thoughtthrough endeavor, and suggest ways to extend the protocol to additional evaluation scenarios.", "labels": [], "entities": [{"text": "cross-experiment parse evaluation", "start_pos": 39, "end_pos": 72, "type": "TASK", "confidence": 0.832534392674764}]}], "datasetContent": [{"text": "We propose anew protocol for cross-experiment parse evaluation, consisting of three fundamental components: (i) abstracting away from annotation peculiarities, (ii) generalizing theory-specific structures into a single linguistically coherent gold standard that contains all and only consistent information from all sources, and (iii) defining a sound metric that takes into account the different gold standards that are being considered in the experiments.", "labels": [], "entities": [{"text": "cross-experiment parse evaluation", "start_pos": 29, "end_pos": 62, "type": "TASK", "confidence": 0.8687087893486023}]}, {"text": "In this section we first define functional trees as the common space of formal objects and define a deterministic conversion procedure from dependency trees to functional trees.", "labels": [], "entities": []}, {"text": "Next we define a set of formal operations on functional trees that compute, for every pair of corresponding trees of the same yield, a single gold tree that resolves inconsistencies among gold standard alternatives and combines the information that they share.", "labels": [], "entities": []}, {"text": "Finally, we define scores based on tree edit distance, refined to consider the distance from parses to the overall gold tree as well as the different annotation alternatives.", "labels": [], "entities": []}, {"text": "Let T be a finite set of terminal symbols and let L be a set of grammatical relation labels.", "labels": [], "entities": []}, {"text": "A dependency graph dis a directed graph which consists of nodes V d and arcs We assume that all nodes in V dare labeled by terminal symbols via a function label V : .., tn is any dependency graph that is a directed tree originating out of anode v 0 labeled t 0 = ROOT , and spans all terminals in the sentence, that is, for every ti \u2208 S there exists v j \u2208 V d labeled label V (v j ) = ti . For simplicity we assume that every node v j is indexed according to the position of the terminal label, i.e., that for each ti labeling v j , i always equals j.", "labels": [], "entities": [{"text": "ROOT", "start_pos": 263, "end_pos": 267, "type": "METRIC", "confidence": 0.9929721355438232}]}, {"text": "Ina labeled dependency tree, arcs in A dare labeled by elements of L via a function label A : A d \u2192 L that encodes the grammatical relation between the terminals labeling the connected nodes.", "labels": [], "entities": []}, {"text": "We define two auxiliary functions on nodes in dependency trees.", "labels": [], "entities": []}, {"text": "The function subtree : V d \u2192 P(V d ) assigns to every node v \u2208 V d the set of nodes accessible by it through the reflexive transitive closure of the arc relation A d . The function span : V d \u2192 P(T ) assigns to every node v \u2208 V d a set of terminals such that span(v) = {t \u2208 T |t = label V (u) and u \u2208 subtree(v)}.", "labels": [], "entities": []}, {"text": "1 Step 1: Functional Representation Our first goal is to define a representation format that keeps all functional relationships that are represented in the dependency trees intact, but remains neutral with respect to the directionality of the head-dependent relations.", "labels": [], "entities": [{"text": "Functional Representation", "start_pos": 10, "end_pos": 35, "type": "TASK", "confidence": 0.8144014179706573}]}, {"text": "To do so we define functional trees -linearly-ordered labeled trees which, instead of head-to-head binary relations, represent the complete functional structure of a sentence.", "labels": [], "entities": []}, {"text": "Assuming the same sets of terminal symbols T and grammatical relation labels L, and assuming extended sets of nodes V and arcs A \u2286 V \u00d7 V , a functional tree \u03c0 = (V, A) is a directed tree originating from a single root v 0 \u2208 V where all non-terminal nodes in \u03c0 are labeled with grammatical relation labels that signify the grammatical function of the chunk they dominate inside the tree via label NT : V \u2192 L.", "labels": [], "entities": []}, {"text": "All terminal nodes in \u03c0 are labeled with terminal symbols via a label T : V \u2192 T function.", "labels": [], "entities": []}, {"text": "The function span : V \u2192 P(V ) now picks out the set of terminal labels of the terminal nodes accessible by anode v \u2208 V via A.", "labels": [], "entities": []}, {"text": "We obtain functional trees from dependency trees using the following procedure: \u2022 Initialize the set of nodes and arcs in the tree.", "labels": [], "entities": []}, {"text": "\u2022 Label each node v \u2208 V with the label of its incoming arc.", "labels": [], "entities": []}, {"text": "\u2022 In case |span(v)| > 1 add anew node u as a daughter designating the lexical head, labeled with the wildcard symbol *: \u2022 For each node v such that |span(v)| = 1, add anew node u as a daughter, labeled with its own terminal: That is to say, we label all nodes with spans greater than 1 with the grammatical function of their head, and for each node we add anew daughter u designating the headword, labeled with its grammatical function.", "labels": [], "entities": []}, {"text": "Wildcard labels are compatible with any, more specific, grammatical function of the word inside the phrase.", "labels": [], "entities": []}, {"text": "This gives us a constituencylike representation of dependency trees labeled with functional information, which retains the linguistic assumptions reflected in the dependency trees.", "labels": [], "entities": []}, {"text": "When applying this procedure, examples (1)-(3) get transformed into (4)-(6) respectively.", "labels": [], "entities": []}, {"text": "Considering the functional trees resulting from our procedure, it is easy to see that for tree pairs (4a)-(4b) and (5a)-(5b) the respective functional trees are identical modulo wildcards, while tree pairs (5b)-(5c) and (6a)-(6b) end up with different tree structures that realize different assumptions concerning the internal structure of the tree.", "labels": [], "entities": []}, {"text": "In order to compare, combine or detect inconsistencies in the information inherent in different functional trees, we define a set of formal operations that are inspired by familiar notions from unification-based formalisms (Shieber (1986) and references therein).", "labels": [], "entities": []}, {"text": "Step 2: Formal Operations on Trees The intuition behind the formal operations we define is simple.", "labels": [], "entities": []}, {"text": "A completely flat tree over a span is the most general structural description that can be given to it.", "labels": [], "entities": []}, {"text": "The more nodes dominate a span, the more linguistic assumptions are made with respect to its structure.", "labels": [], "entities": []}, {"text": "If an arc structure in one tree merely elaborates an existing flat span in another tree, the theories underlying the schemes are compatible, and their information can be combined.", "labels": [], "entities": []}, {"text": "Otherwise, there exists a conflict in the linguistic assumptions, and we need to relax some of the assumptions, i.e., remove functional nodes, in order to obtain a coherent structure that contains the information on which they agree.", "labels": [], "entities": []}, {"text": "Let \u03c0 1 , \u03c0 2 be functional trees over the same yield t 1 , .., tn . Let the function span(v) pick out the terminals labeling terminal nodes that are accessible via anode v \u2208 V in the functional tree through the relation A.", "labels": [], "entities": []}, {"text": "We define first the tree subsumption relation for comparing the amount of information inherent in the arc-structure of two trees.", "labels": [], "entities": []}, {"text": "T-Subsumption, denoted \ud97b\udf59 t , is a relation between trees which indicates that a tree \u03c0 1 is consistent with and more general than tree \u03c0 2 . Formally: \u03c0 1 \ud97b\udf59 t \u03c0 2 iff for every node n \u2208 \u03c0 1 there exists anode m \u2208 \u03c0 2 such that span(n) = span(m) and label(n) = label(m).", "labels": [], "entities": []}, {"text": "Looking at the functional trees of (4a)-(4b) we see that their unlabeled skeletons mutually subsume each other.", "labels": [], "entities": []}, {"text": "In their labeled versions, however, each tree contains labeling information that is lacking in the other.", "labels": [], "entities": []}, {"text": "In the functional trees (5b)-(5c) a flat structure over a span in (5b) is more elaborated in (5c).", "labels": [], "entities": []}, {"text": "In order to combine information in trees with compatible arc structures, we define tree unification.", "labels": [], "entities": [{"text": "tree unification", "start_pos": 83, "end_pos": 99, "type": "TASK", "confidence": 0.7935643494129181}]}, {"text": "T-Unification, denoted \ud97b\udf59 t , is the operation that returns the most general tree structure \u03c0 3 that is subsumed by both \u03c0 1 , \u03c0 2 if such exists, and fails otherwise.", "labels": [], "entities": []}, {"text": "Formally: \u03c0 1 \ud97b\udf59 t \u03c0 2 = \u03c0 3 iff \u03c0 1 \ud97b\udf59 t \u03c0 3 and \u03c0 2 \ud97b\udf59 t \u03c0 3 , and for all \u03c0 4 such that \u03c0 1 \ud97b\udf59 t \u03c0 4 and \u03c0 2 \ud97b\udf59 t \u03c0 4 it holds that \u03c0 3 \ud97b\udf59 t \u03c0 4 . Tree unification collects the information from two trees into a single result if they are consistent, and detects an inconsistency otherwise.", "labels": [], "entities": [{"text": "Tree unification", "start_pos": 144, "end_pos": 160, "type": "TASK", "confidence": 0.7797389030456543}]}, {"text": "In case of an inconsistency, as is the casein the functional trees (6a) and (6b), we cannot unify the structures due to a conflict concerning the internal division of an expression into phrases.", "labels": [], "entities": []}, {"text": "However, we still want to generalize these two trees into one tree that contains all and only the information that they share.", "labels": [], "entities": []}, {"text": "For that we define the tree generalization operation.", "labels": [], "entities": []}, {"text": "T-Generalization, denoted \ud97b\udf59 t , is the operation that returns the most specific tree that is more general than both trees.", "labels": [], "entities": []}, {"text": "Formally, \u03c0 1 \ud97b\udf59 t \u03c0 2 = \u03c0 3 iff \u03c0 3 \ud97b\udf59 t \u03c0 1 and \u03c0 3 \ud97b\udf59 t \u03c0 2 , and for every \u03c0 4 such that \u03c0 4 \ud97b\udf59 t \u03c0 1 and \u03c0 4 \ud97b\udf59 t \u03c0 2 it holds that \u03c0 4 \ud97b\udf59 t \u03c0 3 . Unlike unification, generalization can never fail.", "labels": [], "entities": []}, {"text": "For every pair of trees there exists a tree that is more general than both: in the extreme case, pick the completely flat structure over the yield, which is more general than any other structure.", "labels": [], "entities": []}, {"text": "For (6a)-(6b), for instance, we get that (6a)\ud97b\udf59 t (6b) is a flat tree over pre-terminals where \"would\" and \"have\" are labeled with 'vg' and \"worked\" is the head, labeled with '*'.", "labels": [], "entities": []}, {"text": "The generalization of two functional trees provides us with one structure that reflects the common and consistent content of the two trees.", "labels": [], "entities": []}, {"text": "These structures thus provide us with a formally well-defined gold standard for cross-treebank evaluation.", "labels": [], "entities": []}, {"text": "Step 3: Measuring Distances.", "labels": [], "entities": []}, {"text": "Our functional trees superficially look like constituency-based trees, so a simple proposal would be to use Parseval measures) for comparing the parsed trees against the new generalized gold trees.", "labels": [], "entities": []}, {"text": "Parseval scores, however, have two significant drawbacks.", "labels": [], "entities": [{"text": "Parseval", "start_pos": 0, "end_pos": 8, "type": "TASK", "confidence": 0.46034014225006104}]}, {"text": "First, they are known to be too restrictive with respect to some errors and too permissive with respect to others.", "labels": [], "entities": []}, {"text": "Secondly, F 1 scores would still penalize structures that are correct with respect to the original gold, but are not therein the generalized structure.", "labels": [], "entities": [{"text": "F 1 scores", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9713685115178426}]}, {"text": "Here we propose to adopt measures that are based on tree edit distance (TED) instead.", "labels": [], "entities": [{"text": "tree edit distance (TED)", "start_pos": 52, "end_pos": 76, "type": "METRIC", "confidence": 0.7456647008657455}]}, {"text": "TEDbased measures are, in fact, an extension of attachment scores for dependency trees.", "labels": [], "entities": []}, {"text": "Consider, for instance, the following operations on dependency arcs.", "labels": [], "entities": []}, {"text": "reattach-arc remove arc (u, v) \u2208 A d and add an arc Assuming that each operation is assigned a cost, the attachment score of comparing two dependency trees is simply the cost of all edit operations that are required to turn a parse tree into its gold standard, normalized with respect to the overall size of the dependency tree and subtracted from a unity.", "labels": [], "entities": [{"text": "attachment score", "start_pos": 105, "end_pos": 121, "type": "METRIC", "confidence": 0.9568892419338226}]}, {"text": "Here we apply the idea of defining scores by TED costs normalized relative to the size of the tree and substracted from a unity, and extend it from fixed-size dependency trees to ordered trees of arbitrary size.", "labels": [], "entities": [{"text": "TED", "start_pos": 45, "end_pos": 48, "type": "METRIC", "confidence": 0.9222668409347534}]}, {"text": "Our formalization follows closely the formulation of the T-Dice measure of, building on his thorough investigation of the formal and empirical differences between TED-based measures and Parseval.", "labels": [], "entities": []}, {"text": "We first define for any ordered and labeled tree \u03c0 the following operations.", "labels": [], "entities": []}, {"text": "relabel-node change the label of node v in \u03c0 delete-node delete a non-root node v in \u03c0 with parent u, making the children of v the children of u, inserted in the place of v as a subsequence in the left-to-right order of the children of u. insert-node insert anode v as a child of u in \u03c0 making it the parent of a consecutive subsequence of the children of u.", "labels": [], "entities": []}, {"text": "An edit script ES(\u03c0 1 , \u03c0 2 ) = {e 0 , e 1 ....e k } between \u03c0 1 and \u03c0 2 is a set of edit operations required for turning \u03c0 1 into \u03c0 2 . Now, assume that we are given a cost function defined for each edit operation.", "labels": [], "entities": []}, {"text": "The cost of ES(\u03c0 1 , \u03c0 2 ) is the sum of the costs of the operations in the script.", "labels": [], "entities": [{"text": "ES", "start_pos": 12, "end_pos": 14, "type": "METRIC", "confidence": 0.7467478513717651}]}, {"text": "An optimal edit script is an edit script between \u03c0 1 and \u03c0 2 of minimum cost.", "labels": [], "entities": []}, {"text": "The tree edit distance problem is defined to be the problem of finding the optimal edit script and computing the corresponding distance.", "labels": [], "entities": []}, {"text": "A simple way to calculate the error \u03b4 of a parse would be to define it as the edit distance between the parse hypothesis \u03c0 1 and the gold standard \u03c0 2 . However, in such cases the parser may still get penalized for recovering nodes that are lacking in the generalization.", "labels": [], "entities": []}, {"text": "To solve this, we refine the distance between a parse tree and the generalized gold tree to discard edit operations on nodes that are therein the native gold tree but are eliminated through generalization.", "labels": [], "entities": []}, {"text": "We compute the intersection of the edit script turning the parse tree into the generalize gold with the edit script turning the native gold tree into the generalized gold, and discard its cost.", "labels": [], "entities": []}, {"text": "That is, if parse1 and parse2 are compared against gold1 and gold2 respectively, and if we set gold3 to be the result of gold1\ud97b\udf59 t gold2, then \u03b4 new is defined as: 390 Now, if gold1 and gold3 are identical, then ES * (gold1,gold3)=\u2205 and we fallback on the simple tree edit distance score \u03b4 new (parse1,gold1,gold3)=\u03b4(parse1, gold3).", "labels": [], "entities": [{"text": "ES", "start_pos": 211, "end_pos": 213, "type": "METRIC", "confidence": 0.9934543371200562}]}, {"text": "When parse1 and gold1 are identical, i.e., the parser produced perfect output with respect to its own scheme, then \u03b4 new (parse1,gold1,gold3)=\u03b4 new (gold1,gold1,gold3) =\u03b4(gold1,gold3) \u2212 cost(ES * (gold1,gold3))=0, and the parser does not get penalized for recovering a correct structure in gold1 that is lacking in gold3.", "labels": [], "entities": [{"text": "ES", "start_pos": 191, "end_pos": 193, "type": "METRIC", "confidence": 0.9603964686393738}]}, {"text": "In order to turn distances into accuracy measures we have to normalize distances relative to the maximal number of operations that is conceivable.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.9977008700370789}]}, {"text": "In the worst case, we would have to remove all the internal nodes in the parse tree and add all the internal nodes of the generalized gold, so our normalization factor \u03b9 is defined as follows, where |\u03c0| is the size 4 of \u03c0. \u03b9(parse1,gold3) = |parse1| + |gold3| We now define the score of parse1 as follows: 1 \u2212 \u03b4 new (parse1,gold1,gold3) \u03b9(parse1,gold3) summarizes the steps in the evaluation procedure we defined so far.", "labels": [], "entities": []}, {"text": "We start off with two versions of the treebank, TB1 and TB2, which are parsed separately and provide their own gold standards and parse hypotheses in a labeled dependencies format.", "labels": [], "entities": []}, {"text": "All dependency trees are then converted into functional trees, and we compute the generalization of each pair of gold trees for each sentence in the data.", "labels": [], "entities": []}, {"text": "This provides the generalized gold standard for all experiments, here marked as gold3.", "labels": [], "entities": []}, {"text": "We finally compute the distances \u03b4 new (parse1,gold1,gold3) and \u03b4 new (parse2,gold2,gold3) using the different tree edit distances that are now available, and we repeat the procedure for each sentence in the test set.", "labels": [], "entities": []}, {"text": "To normalize the scores for an entire test set of size n we can take the arithmetic mean of the scores.", "labels": [], "entities": []}, {"text": "Alternatively we can globally average of all edit distance costs, normalized by the maximally possible edits on parse trees turned into generalized trees.", "labels": [], "entities": []}, {"text": "The latter score, global averaging over the entire test set, is the metric we use in our evaluation procedure.", "labels": [], "entities": []}, {"text": "We demonstrate the application of our procedure to comparing dependency parsing results on different versions of the Penn Treebank ().", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 61, "end_pos": 79, "type": "TASK", "confidence": 0.7161241620779037}, {"text": "Penn Treebank", "start_pos": 117, "end_pos": 130, "type": "DATASET", "confidence": 0.996012955904007}]}, {"text": "The Data We use data from the PTB, converted into dependency structures using the LTH software, a general purpose tool for constituency-todependency conversion.", "labels": [], "entities": [{"text": "PTB", "start_pos": 30, "end_pos": 33, "type": "DATASET", "confidence": 0.9414377808570862}, {"text": "constituency-todependency conversion", "start_pos": 123, "end_pos": 159, "type": "TASK", "confidence": 0.8307365477085114}]}, {"text": "We use LTH to implement the five different annotation standards detailed in.", "labels": [], "entities": []}, {"text": "Generalization is an associative and commutative operation, so it can be extended for n experiments in any order.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Cross-experiment dependency parsing evaluation for MaltParser trained on multiple schemes. We report stan- dard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results reported  in the same row. The  \u2020 sign marks pairwise results where the difference is not statistically significant.", "labels": [], "entities": [{"text": "Cross-experiment dependency parsing", "start_pos": 10, "end_pos": 45, "type": "TASK", "confidence": 0.6878387331962585}, {"text": "stan- dard LAS scores", "start_pos": 111, "end_pos": 132, "type": "METRIC", "confidence": 0.6734908103942872}, {"text": "TEDEVAL global average metrics", "start_pos": 137, "end_pos": 167, "type": "METRIC", "confidence": 0.918579950928688}]}, {"text": " Table 2: Cross-experiment dependency parsing evaluation for the MST parser trained on multiple schemes. We  report standard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results  reported in the same row. The  \u2020 sign marks pairwise results where the difference is not statistically significant.", "labels": [], "entities": [{"text": "Cross-experiment dependency parsing", "start_pos": 10, "end_pos": 45, "type": "TASK", "confidence": 0.6916316052277883}, {"text": "MST parser", "start_pos": 65, "end_pos": 75, "type": "TASK", "confidence": 0.8769615590572357}, {"text": "LAS", "start_pos": 125, "end_pos": 128, "type": "METRIC", "confidence": 0.9845027327537537}, {"text": "TEDEVAL global average metrics", "start_pos": 140, "end_pos": 170, "type": "METRIC", "confidence": 0.9208311438560486}]}]}