{"title": [], "abstractContent": [{"text": "This paper introduces Chart Inference (CI), an algorithm for deriving a CCG category for an unknown word from a partial parse chart.", "labels": [], "entities": [{"text": "Chart Inference (CI)", "start_pos": 22, "end_pos": 42, "type": "TASK", "confidence": 0.796889066696167}]}, {"text": "It is shown to be faster and more precise than a baseline brute-force method, and to achieve wider coverage than a rule-based system.", "labels": [], "entities": [{"text": "coverage", "start_pos": 99, "end_pos": 107, "type": "METRIC", "confidence": 0.9779071807861328}]}, {"text": "In addition, we show the application of CI to a domain adaptation task for question words, which are largely missing in the Penn Treebank.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 124, "end_pos": 137, "type": "DATASET", "confidence": 0.9958735704421997}]}, {"text": "When used in combination with self-training, CI increases the precision of the baseline StatCCG parser over subject-extraction questions by 50%.", "labels": [], "entities": [{"text": "precision", "start_pos": 62, "end_pos": 71, "type": "METRIC", "confidence": 0.9995879530906677}, {"text": "StatCCG parser", "start_pos": 88, "end_pos": 102, "type": "TASK", "confidence": 0.6308506578207016}]}, {"text": "An error analysis shows that CI contributes to the increase by expanding the number of category types available to the parser, while self-training adjusts the counts.", "labels": [], "entities": []}], "introductionContent": [{"text": "Unseen lexical items area major cause of error in strongly lexicalised parsers such as those based on CCG.", "labels": [], "entities": []}, {"text": "The problem is especially acute for less privileged languages, but even in the case of English, we are aware of many category types entirely missing from the Penn Treebank ().", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 158, "end_pos": 171, "type": "DATASET", "confidence": 0.9950215220451355}]}, {"text": "In the case of totally unseen words, the standard method used by and many other treebank parsers is part-of-speech backoff, which is quite effective, affording an F-score of 93% over dependencies in \u00a700 in the optimal configuration.", "labels": [], "entities": [{"text": "F-score", "start_pos": 163, "end_pos": 170, "type": "METRIC", "confidence": 0.9986997842788696}]}, {"text": "It is difficult to say how backing off affects dependency errors, but when we examine category match accuracy of the CCGBank-trained parser, we find that POS backoff has been used on 19.6% of tokens, which means that those tokens are unseen, or too infrequent in the training data to be included in the lexicon.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 101, "end_pos": 109, "type": "METRIC", "confidence": 0.6459957361221313}]}, {"text": "Of the 3320 items the parser labelled incorrectly, 675 (20.3%) are words that are missing from the lexicon entirely.", "labels": [], "entities": []}, {"text": "In the best case, if we were able to learn lexical entries for those 675, we could transfer them to lexical treatment, which is 93.5% accurate, rather than POS backoff, which is 89.3% accurate.", "labels": [], "entities": []}, {"text": "Under these conditions, we predict a further 631 word/category pairs to be tagged correctly by the parser, reducing the error rate from 7.4% to 6% on \u00a700.", "labels": [], "entities": [{"text": "error rate", "start_pos": 120, "end_pos": 130, "type": "METRIC", "confidence": 0.9879350364208221}]}, {"text": "Further to reducing parsing error, a robust method for learning words from unlabelled data would result in the recovery of interesting and important category types that are missing from our standard lexical resources.", "labels": [], "entities": [{"text": "parsing", "start_pos": 20, "end_pos": 27, "type": "TASK", "confidence": 0.9651535749435425}]}, {"text": "This paper introduces Chart Inference (CI) as a strategy for deducing a ranked set of possible categories for an unknown word using the partial chart formed from the known words that surround it.", "labels": [], "entities": [{"text": "Chart Inference (CI)", "start_pos": 22, "end_pos": 42, "type": "TASK", "confidence": 0.7913757562637329}]}, {"text": "CCG) is particularly suited to this problem, because category types can be inferred from the types of the surrounding constituents.", "labels": [], "entities": []}, {"text": "CI is designed to take advantage of this property of generative CCGBank-trained parser, and of access to the full inventory of CCG combinators and noncombinatory unary rules from the trained model.", "labels": [], "entities": []}, {"text": "It is capable of learning category types that are completely missing from the lexicon, and is superior to existing learning systems in both precision and efficiency.", "labels": [], "entities": [{"text": "precision", "start_pos": 140, "end_pos": 149, "type": "METRIC", "confidence": 0.9994151592254639}]}, {"text": "Four experiments are discussed in this paper.", "labels": [], "entities": []}, {"text": "The first compares three word-learning methods for their ability to converge to a toy target lexicon.", "labels": [], "entities": []}, {"text": "The sec-ond and third compare the three methods based on their ability to correctly tag the all the words in a small natural language corpus.", "labels": [], "entities": []}, {"text": "The final experiment shows how Chart Induction can be effectively used in a domain adaptation task where a small number of category types are known to be missing from the lexicon.", "labels": [], "entities": [{"text": "Chart Induction", "start_pos": 31, "end_pos": 46, "type": "TASK", "confidence": 0.7300036549568176}, {"text": "domain adaptation task", "start_pos": 76, "end_pos": 98, "type": "TASK", "confidence": 0.795946846405665}]}], "datasetContent": [{"text": "In the following experiments, we compare Chart Inference to the two baseline methods: Brute Force (BF), derived from Watkinson and Manandhar, and Rule-Based (RB), derived from Yao et al.", "labels": [], "entities": [{"text": "Chart Inference", "start_pos": 41, "end_pos": 56, "type": "TASK", "confidence": 0.8087144196033478}, {"text": "Brute Force (BF)", "start_pos": 86, "end_pos": 102, "type": "METRIC", "confidence": 0.9732838153839112}, {"text": "Rule-Based (RB)", "start_pos": 146, "end_pos": 161, "type": "METRIC", "confidence": 0.7556688636541367}]}, {"text": "This section investigates how robust the three systems are to changes in theoriginal seed lexicon.", "labels": [], "entities": []}, {"text": "Next, we compare the three learning methods on a larger corpus of natural language, to investigate how well they perform at recovering a wide range of category types in complex settings.", "labels": [], "entities": []}, {"text": "In this experiment, we examine how close we can get to those results by using Chart Inference to learn WH-question words from the unlabelled question corpus.", "labels": [], "entities": []}, {"text": "If successful, this would eliminate the human-annotation step for domain adaptation of the kind investigated by).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Exp. II: Category match results for the three systems on the McGuffey corpus, training and testing on MG1.", "labels": [], "entities": [{"text": "McGuffey corpus", "start_pos": 71, "end_pos": 86, "type": "DATASET", "confidence": 0.9736613929271698}, {"text": "MG1", "start_pos": 112, "end_pos": 115, "type": "DATASET", "confidence": 0.837880551815033}]}, {"text": " Table 2: Exp. III: Category match results for the three systems on the McGuffey corpus, training on MG1 and testing  on MG2. Common categories (N, NP, N/N) are overrepresented in the test data, leading to higher BF scores. POS  tags not available for MG2, so no POS baseline is reported.", "labels": [], "entities": [{"text": "McGuffey corpus", "start_pos": 72, "end_pos": 87, "type": "DATASET", "confidence": 0.9790872931480408}, {"text": "BF scores", "start_pos": 213, "end_pos": 222, "type": "METRIC", "confidence": 0.9810936152935028}, {"text": "POS baseline", "start_pos": 263, "end_pos": 275, "type": "METRIC", "confidence": 0.9540510177612305}]}, {"text": " Table 3: F-score over individual category matches. Bold  means significantly different from the Baseline.", "labels": [], "entities": [{"text": "F-score", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9976568222045898}, {"text": "Baseline", "start_pos": 97, "end_pos": 105, "type": "DATASET", "confidence": 0.8986536860466003}]}, {"text": " Table 4: Exp. IV: Lexical category distribution for the word What in the baseline  \u00a702-21 of CCGBank (BL), after  Chart Inference (CI), and after first applying Chart Inference, then self-training (CI+ST). Column 1 classifies low- frequency categories as rare (R), spurious (*) or duplicate (D). Cateogories above the middle line are present in the  Baseline lexicon; below are induced.", "labels": [], "entities": [{"text": "What", "start_pos": 62, "end_pos": 66, "type": "METRIC", "confidence": 0.9434432983398438}]}]}