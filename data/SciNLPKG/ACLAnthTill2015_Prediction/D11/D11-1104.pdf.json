{"title": [{"text": "Efficient Subsampling for Training Complex Language Models", "labels": [], "entities": []}], "abstractContent": [{"text": "We propose an efficient way to train maximum entropy language models (MELM) and neural network language models (NNLM).", "labels": [], "entities": []}, {"text": "The advantage of the proposed method comes from a more robust and efficient subsampling technique.", "labels": [], "entities": []}, {"text": "The original multi-class language mod-eling problem is transformed into a set of binary problems where each binary classifier predicts whether or not a particular word will occur.", "labels": [], "entities": []}, {"text": "We show that the binarized model is as powerful as the standard model and allows us to aggressively subsample negative training examples without sacrificing predictive performance.", "labels": [], "entities": []}, {"text": "Empirical results show that we can train MELM and NNLM at 1% \u223c 5% of the standard complexity with no loss in performance .", "labels": [], "entities": [{"text": "MELM", "start_pos": 41, "end_pos": 45, "type": "DATASET", "confidence": 0.5940578579902649}, {"text": "NNLM", "start_pos": 50, "end_pos": 54, "type": "DATASET", "confidence": 0.7654117345809937}]}], "introductionContent": [{"text": "Language models (LM) assign probabilities to sequences of words.", "labels": [], "entities": []}, {"text": "They are widely used in many natural language processing applications.", "labels": [], "entities": []}, {"text": "The probability of a sequence can be modeled as a product of local probabilities, as shown in, where w i is the i th word, and hi is the word history preceding w i . P (w 1 , w 2 , ..., w l ) = l i=1 P (w i |h i ) Therefore the task of language modeling reduces to estimating a set of conditional distributions {P (w|h)}.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 236, "end_pos": 253, "type": "TASK", "confidence": 0.7161622047424316}]}, {"text": "The n-gram LM is a dominant way to parametrize P (w|h), where it is assumed that w only depends on the previous n\u22121 words.", "labels": [], "entities": []}, {"text": "More complex models have also been proposed-MELM) and NNLM () are two examples.", "labels": [], "entities": [{"text": "NNLM", "start_pos": 54, "end_pos": 58, "type": "DATASET", "confidence": 0.8343343734741211}]}, {"text": "Modeling P (w|h) can be seen as a multi-class classification problem.", "labels": [], "entities": []}, {"text": "Given the history, we have to choose a word in the vocabulary, which can easily be a few hundred thousand words in size.", "labels": [], "entities": []}, {"text": "For complex models such as MELM and NNLM, this poses a computational challenge for learning, because the resulting objective functions are expensive to normalize.", "labels": [], "entities": [{"text": "NNLM", "start_pos": 36, "end_pos": 40, "type": "DATASET", "confidence": 0.8479551076889038}]}, {"text": "In contrast, n-gram LMs do not suffer from this computational challenge.", "labels": [], "entities": []}, {"text": "In the web era, language modelers have access to virtually unlimited amounts of data, while the computing power available to process this data is limited.", "labels": [], "entities": []}, {"text": "Therefore, despite the demonstrated effectiveness of complex LMs, the n-gram is still the predominant approach for most real world applications.", "labels": [], "entities": []}, {"text": "Subsampling is a simple solution to get around the constraint of computing resources.", "labels": [], "entities": []}, {"text": "For the purpose of language modeling, it amounts to taking only part of the text corpus to train the LM.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 19, "end_pos": 36, "type": "TASK", "confidence": 0.7345131486654282}]}, {"text": "For complex models such as NNLM, it has been shown that subsampling can speedup training greatly, at the cost of some degradation in predictive performance, allowing for trade-off between computational cost and LM quality.", "labels": [], "entities": [{"text": "NNLM", "start_pos": 27, "end_pos": 31, "type": "DATASET", "confidence": 0.8842860460281372}]}, {"text": "Our contribution is a novel way to train complex LMs such as MELM and NNLM which allows much more aggressive subsampling without incurring as high a cost in predictive performance.", "labels": [], "entities": [{"text": "MELM", "start_pos": 61, "end_pos": 65, "type": "DATASET", "confidence": 0.8551360964775085}, {"text": "NNLM", "start_pos": 70, "end_pos": 74, "type": "DATASET", "confidence": 0.8535643219947815}]}, {"text": "The key to our approach is reducing the multi-class LM problem into a set of binary problems.", "labels": [], "entities": []}, {"text": "Instead of training a V -class classifier, where V is the size of 1128 the vocabulary, we train V binary classifiers, each one of which performs a one-against-all classification.", "labels": [], "entities": []}, {"text": "The V trained binary probabilities are then renormalized to obtain a valid distribution over the V words.", "labels": [], "entities": []}, {"text": "Subsampling here can be done in the negative examples.", "labels": [], "entities": []}, {"text": "Since the majority of training examples are negative for each of the binary classifiers, we can achieve substantial computational saving by only keeping subsets of them.", "labels": [], "entities": []}, {"text": "We will show that the binarized LM is as powerful as its multi-class counterpart, while being able to sustain much more aggressive subsampling.", "labels": [], "entities": []}, {"text": "For certain types of LMs such as MELM, there are more benefits-the binarization leads to a set of completely independent classifiers to train, which allows easy parallelization and significantly lowers the memory requirement.", "labels": [], "entities": []}, {"text": "Similar one-against-all approaches are often used in the machine learning community, especially by SVM (support vector machine) practitioners to solve multi-class problems ().", "labels": [], "entities": []}, {"text": "The goal of this paper is to show that a similar technique can also be used for language modeling and that it enables us to subsample data much more efficiently.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 80, "end_pos": 97, "type": "TASK", "confidence": 0.7700507342815399}]}, {"text": "We show that the proposed approach is useful when the dominant modeling constraint is computing power as opposed to training data.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "In section 2, we describe our binarization and subsampling techniques for language models with MELM and NNLM as two specific examples.", "labels": [], "entities": [{"text": "NNLM", "start_pos": 104, "end_pos": 108, "type": "DATASET", "confidence": 0.8872959613800049}]}, {"text": "Experimental results are presented in Section 3, followed by discussion in Section 4.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Binary Subsampled MELM on WSJ", "labels": [], "entities": [{"text": "Binary Subsampled MELM", "start_pos": 10, "end_pos": 32, "type": "TASK", "confidence": 0.7763412992159525}, {"text": "WSJ", "start_pos": 36, "end_pos": 39, "type": "DATASET", "confidence": 0.8822798132896423}]}, {"text": " Table 3: WSJ WER improvement. Binary MELM are  interpolated with KN 4-gram", "labels": [], "entities": [{"text": "WSJ", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.6917955875396729}, {"text": "WER", "start_pos": 14, "end_pos": 17, "type": "METRIC", "confidence": 0.8839173316955566}]}, {"text": " Table 4: Binary NNLM vs. Standard NNLM. Fixed ran- dom subset.", "labels": [], "entities": []}, {"text": " Table 5: Binary NNLM vs. Standard NNLM. Fixed ran- dom subset. Interpolated with KN trigram.", "labels": [], "entities": []}, {"text": " Table 6: Binary NNLM vs. Standard NNLM. Variable  random subset.", "labels": [], "entities": []}, {"text": " Table 7: Binary NNLM vs. Standard NNLM. Variable  random subset. Interpolated with KN trigram.", "labels": [], "entities": []}]}