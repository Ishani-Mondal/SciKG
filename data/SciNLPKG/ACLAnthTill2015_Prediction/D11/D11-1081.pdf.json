{"title": [{"text": "Fast Generation of Translation Forest for Large-Scale SMT Discriminative Training", "labels": [], "entities": [{"text": "SMT Discriminative Training", "start_pos": 54, "end_pos": 81, "type": "TASK", "confidence": 0.9412866036097208}]}], "abstractContent": [{"text": "Although discriminative training guarantees to improve statistical machine translation by incorporating a large amount of overlapping features , it is hard to scale up to large data due to decoding complexity.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 55, "end_pos": 86, "type": "TASK", "confidence": 0.7427971462408701}]}, {"text": "We propose anew algorithm to generate translation forest of training data in linear time with the help of word alignment.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 106, "end_pos": 120, "type": "TASK", "confidence": 0.7427569031715393}]}, {"text": "Our algorithm also alleviates the oracle selection problem by ensuring that a forest always contains derivations that exactly yield the reference translation.", "labels": [], "entities": []}, {"text": "With millions of features trained on 519K sentences in 0.03 second per sentence, our system achieves significant improvement by 0.84 BLEU over the baseline system on the NIST Chinese-English test sets.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 133, "end_pos": 137, "type": "METRIC", "confidence": 0.9990804195404053}, {"text": "NIST Chinese-English test sets", "start_pos": 170, "end_pos": 200, "type": "DATASET", "confidence": 0.9513748586177826}]}], "introductionContent": [{"text": "Discriminative model) can easily incorporate non-independent and overlapping features, and has been dominating the research field of statistical machine translation (SMT) in the last decade.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 133, "end_pos": 170, "type": "TASK", "confidence": 0.8210161427656809}]}, {"text": "Recent work have shown that SMT benefits a lot from exploiting large amount of features (;.", "labels": [], "entities": [{"text": "SMT", "start_pos": 28, "end_pos": 31, "type": "TASK", "confidence": 0.9949658513069153}]}, {"text": "However, the training of the large number of features was always restricted in fairly small data sets.", "labels": [], "entities": []}, {"text": "Some systems limit the number of training examples, while others use short sentences to maintain efficiency.", "labels": [], "entities": []}, {"text": "Overfitting problem often comes when training many features on a small data ().", "labels": [], "entities": []}, {"text": "Obviously, using much more data can alleviate such problem.", "labels": [], "entities": []}, {"text": "Furthermore, large data also enables us to globally train millions of sparse lexical features which offer accurate clues for SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 125, "end_pos": 128, "type": "TASK", "confidence": 0.9959502220153809}]}, {"text": "Despite these advantages, to the best of our knowledge, no previous discriminative training paradigms scale up to use a large amount of training data.", "labels": [], "entities": []}, {"text": "The main obstacle comes from the complexity of packed forests or n-best lists generation which requires to search through all possible translations of each training example, which is computationally prohibitive in practice for SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 227, "end_pos": 230, "type": "TASK", "confidence": 0.9933429956436157}]}, {"text": "To make normalization efficient, contrastive estimation ( introduce neighborhood for unsupervised log-linear model, and has presented positive results in various tasks.", "labels": [], "entities": []}, {"text": "Motivated by these work, we use a translation forest (Section 3) which contains both \"reference\" derivations that potentially yield the reference translation and also neighboring \"non-reference\" derivations that fail to produce the reference translation.", "labels": [], "entities": []}, {"text": "However, the complexity of generating this translation forest is up to O(n 6 ), because we still need biparsing to create the reference derivations.", "labels": [], "entities": [{"text": "O", "start_pos": 71, "end_pos": 72, "type": "METRIC", "confidence": 0.9613327383995056}]}, {"text": "Consequently, we propose a method to fast generate a subset of the forest.", "labels": [], "entities": []}, {"text": "The key idea (Section 4) is to initialize a reference derivation tree with maximum score by the help of word alignment, and then traverse the tree to generate the subset forest in linear time.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 104, "end_pos": 118, "type": "TASK", "confidence": 0.7311627268791199}]}, {"text": "Besides the efficiency improvement, such a forest allows us to train the model without resort- hyperedge rule e 1 r 1 X \u21d2 \u27e8X 1 bei X 2 , X 1 was X 2 \u27e9 e 2 r 2 X \u21d2 \u27e8qiangshou bei X 1 , the gunman was X 1 \u27e9 e 3 r 3 X \u21d2 \u27e8jingfang X 1 , X 1 by the police\u27e9 e 4 r 4 X \u21d2 \u27e8jingfang X 1 , police X 1 \u27e9 e 5 r 5 X \u21d2 \u27e8qiangshou, the gunman\u27e9 e 6 r 6 X \u21d2 \u27e8jibi, shot dead\u27e9: A translation forest which is the running example throughout this paper.", "labels": [], "entities": []}, {"text": "The reference translation is \"the gunman was killed by the police\".", "labels": [], "entities": []}, {"text": "(1) Solid hyperedges denote a \"reference\" derivation tree t 1 which exactly yields the reference translation.", "labels": [], "entities": []}, {"text": "(2) Replacing e 3 int 1 withe 4 results a competing non-reference derivation t 2 , which fails to swap the order of X 3,4 . (3) Removing e 1 and e 5 int 1 and adding e 2 leads to another reference derivation t 3 . Generally, this is done by deleting anode X 0,1 . ing to constructing the oracle reference (, which is non-trivial for SMT and needs to be determined experimentally.", "labels": [], "entities": [{"text": "SMT", "start_pos": 333, "end_pos": 336, "type": "TASK", "confidence": 0.9933344125747681}]}, {"text": "Given such forests, we globally learn a log-linear model using stochastic gradient descend (Section 5).", "labels": [], "entities": []}, {"text": "Overall, both the generation of forests and the training algorithm are scalable, enabling us to train millions of features on large-scale data.", "labels": [], "entities": []}, {"text": "To show the effect of our framework, we globally train millions of word level context features motivated byword sense disambiguation ( together with the features used in traditional SMT system (Section 6).", "labels": [], "entities": [{"text": "SMT", "start_pos": 182, "end_pos": 185, "type": "TASK", "confidence": 0.9828718900680542}]}, {"text": "Training on 519K sentence pairs in 0.03 seconds per sentence, we achieve significantly improvement over the traditional pipeline by 0.84 BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 137, "end_pos": 141, "type": "METRIC", "confidence": 0.9983905553817749}]}], "datasetContent": [{"text": "Our method is able to train a large number of features on large data.", "labels": [], "entities": []}, {"text": "We use a set of word context features motivated byword sense disambiguation () to test scalability.", "labels": [], "entities": []}, {"text": "A word level context feature is a triple (f, e, f +1 ), which counts the number of time that f is aligned toe and f +1 occurs to the right off . Triple (f, e, f \u22121 ) is similar except that f \u22121 locates to the left off . We retain word alignment information in the extracted rules to exploit such features.", "labels": [], "entities": [{"text": "Triple", "start_pos": 145, "end_pos": 151, "type": "METRIC", "confidence": 0.9607144594192505}]}, {"text": "To demonstrate the importance of scaling up the size of training data and the effect of our method, we compare three types of training configurations which differ in the size of features and data.", "labels": [], "entities": []}, {"text": "We use MERT to training 8 features on a small data.", "labels": [], "entities": []}, {"text": "The 8 features is the same as Chiang (2007) including 4 rule scores (direct and reverse translation scores; direct and reverse lexical translation scores); 1 target side language model score; 3 penalties for word counts, extracted rules and glue rule.", "labels": [], "entities": []}, {"text": "Actually, traditional pipeline often uses such configuration.", "labels": [], "entities": []}, {"text": "We also learn thousands of context word features together with the 8 traditional features on a small data using perceptron.", "labels": [], "entities": []}, {"text": "coder to generate n-best lists for training.", "labels": [], "entities": []}, {"text": "The complexity of CKY decoding limits the training data into a small size.", "labels": [], "entities": []}, {"text": "We fix the 8 traditional feature weights as MERT to get a comparable results as MERT.", "labels": [], "entities": [{"text": "MERT", "start_pos": 80, "end_pos": 84, "type": "DATASET", "confidence": 0.8582668900489807}]}, {"text": "Finally, we use our method to train millions of features on large data.", "labels": [], "entities": []}, {"text": "The use of large data promises us to use full vocabulary of training data for the context word features, which results millions of fully lexicalized context features.", "labels": [], "entities": []}, {"text": "During decoding, when a context feature does not exit, we simply ignore it.", "labels": [], "entities": []}, {"text": "The weights of 8 traditional features are fixed the same as MERT also.", "labels": [], "entities": [{"text": "MERT", "start_pos": 60, "end_pos": 64, "type": "DATASET", "confidence": 0.5366853475570679}]}, {"text": "We fix these weights because the translation feature weights fluctuate intensely during online learning.", "labels": [], "entities": []}, {"text": "The main reason may come from the degeneration solution mentioned in Section 4.2, where rare rules with very high translation probability are selected as the reference derivations.", "labels": [], "entities": []}, {"text": "Another reason could be the fact that translation features are dense intensify the fluctuation.", "labels": [], "entities": []}, {"text": "We leave learning without fixing the 8 feature weights to future work.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Corpus statistics of Chinese side, where Sent.,  Avg., Lon., and Len. are short for sentence, longest,  average, and length respectively. RTRAIN denotes the  reachable (given rule table without added rules) subset of  TRAIN data.", "labels": [], "entities": [{"text": "RTRAIN", "start_pos": 148, "end_pos": 154, "type": "METRIC", "confidence": 0.7920669913291931}, {"text": "TRAIN data", "start_pos": 228, "end_pos": 238, "type": "DATASET", "confidence": 0.8864048719406128}]}, {"text": " Table 2: Effect of our method comparing with MERT and perceptron in terms of BLEU. We also compare our fast  generation method with different data (only reachable or full data). #Data is the size of data for training the feature  weights. * means significantly", "labels": [], "entities": [{"text": "MERT", "start_pos": 46, "end_pos": 50, "type": "METRIC", "confidence": 0.8691566586494446}, {"text": "BLEU", "start_pos": 78, "end_pos": 82, "type": "METRIC", "confidence": 0.99891197681427}]}]}