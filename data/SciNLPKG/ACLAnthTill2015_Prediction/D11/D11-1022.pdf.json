{"title": [{"text": "Dual Decomposition with Many Overlapping Components", "labels": [], "entities": [{"text": "Dual Decomposition", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8028790354728699}]}], "abstractContent": [{"text": "Dual decomposition has been recently proposed as away of combining complementary models, with a boost in predictive power.", "labels": [], "entities": [{"text": "Dual decomposition", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8343092799186707}]}, {"text": "However, in cases where lightweight decom-positions are not readily available (e.g., due to the presence of rich features or logical constraints), the original subgradient algorithm is inefficient.", "labels": [], "entities": []}, {"text": "We sidestep that difficulty by adopting an augmented Lagrangian method that accelerates model consensus by regular-izing towards the averaged votes.", "labels": [], "entities": []}, {"text": "We show how first-order logical constraints can be handled efficiently, even though the corresponding subproblems are no longer combinatorial, and report experiments in dependency parsing , with state-of-the-art results.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 169, "end_pos": 187, "type": "TASK", "confidence": 0.7800774574279785}]}], "introductionContent": [{"text": "The last years have witnessed increasingly accurate models for syntax, semantics, and machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 86, "end_pos": 105, "type": "TASK", "confidence": 0.7811527252197266}]}, {"text": "The predictive power of such models stems from their ability to break locality assumptions.", "labels": [], "entities": []}, {"text": "The resulting combinatorial explosion typically demands some form of approximate decoding, such as sampling, heuristic search, or variational inference.", "labels": [], "entities": []}, {"text": "In this paper, we focus on parsers built from linear programming relaxations, the so-called \"turbo parsers\".", "labels": [], "entities": []}, {"text": "applied dual decomposition as away of combining models which alone permit efficient decoding, but whose combination is intractable.", "labels": [], "entities": [{"text": "dual decomposition", "start_pos": 8, "end_pos": 26, "type": "TASK", "confidence": 0.7608462572097778}]}, {"text": "This results in a relaxation of the original problem that is elegantly solved with the subgradient algorithm.", "labels": [], "entities": []}, {"text": "While this technique has proven quite effective in parsing () as well as machine translation (), we show here that its success is strongly tied to the ability of finding a \"good\" decomposition, i.e., one involving few overlapping components (or slaves).", "labels": [], "entities": [{"text": "parsing", "start_pos": 51, "end_pos": 58, "type": "TASK", "confidence": 0.9795595407485962}, {"text": "machine translation", "start_pos": 73, "end_pos": 92, "type": "TASK", "confidence": 0.8133743703365326}]}, {"text": "With many components, the subgradient algorithm exhibits extremely slow convergence (cf.).", "labels": [], "entities": [{"text": "convergence", "start_pos": 72, "end_pos": 83, "type": "METRIC", "confidence": 0.9582813382148743}]}, {"text": "Unfortunately, a lightweight decomposition is not always at hand, either because the problem does not factor in a natural way, or because one would like to incorporate features that cannot be easily absorbed in few tractable components.", "labels": [], "entities": []}, {"text": "Examples include features generated by statements in first-order logic, features that violate Markov assumptions, or history features such as the ones employed in transition-based parsers.", "labels": [], "entities": []}, {"text": "To tackle the kind of problems above, we adopt DD-ADMM (Alg.", "labels": [], "entities": []}, {"text": "1), a recently proposed algorithm that accelerates dual decomposition).", "labels": [], "entities": [{"text": "dual decomposition", "start_pos": 51, "end_pos": 69, "type": "TASK", "confidence": 0.9241053462028503}]}, {"text": "DD-ADMM retains the modularity of the subgradient-based method, but it speeds up consensus by regularizing each slave subproblem towards the averaged votes obtained in the previous round (cf..", "labels": [], "entities": []}, {"text": "While this yields more involved subproblems (with a quadratic term), we show that exact solutions can still be efficiently computed for all cases of interest, by using sort operations.", "labels": [], "entities": []}, {"text": "As a result, we obtain parsers that can handle very rich features, do not require specifying a decomposition, and can be heavily parallelized.", "labels": [], "entities": []}, {"text": "We demonstrate the success of the approach by presenting experiments in dependency parsing with state-of-the-art results.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 72, "end_pos": 90, "type": "TASK", "confidence": 0.8356926143169403}]}], "datasetContent": [{"text": "We used 14 datasets with non-projective dependencies from the.", "labels": [], "entities": []}, {"text": "We also used a projective English dataset derived from the Penn Treebank by applying the standard head rules of.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 59, "end_pos": 72, "type": "DATASET", "confidence": 0.9943879544734955}]}, {"text": "We did not force the parser to output projective trees or unique roots for any of the datasets; everything is learned from the data.", "labels": [], "entities": []}, {"text": "We trained by running 10 iterations of the cost-augmented MIRA algorithm () with LP-relaxed decoding, as in.", "labels": [], "entities": [{"text": "MIRA", "start_pos": 58, "end_pos": 62, "type": "METRIC", "confidence": 0.6451385617256165}]}, {"text": "Following common practice, we employed a coarse-to-fine procedure to prune away unlikely candidate arcs, as described by . To ensure valid parse trees attest time, we rounded fractional solutions as described in) (yet, solutions were integral most of the time).", "labels": [], "entities": []}, {"text": "The parts used in our full model are the ones depicted in.", "labels": [], "entities": []}, {"text": "Note that a subgradient-based method could handle some of those parts efficiently (arcs, consecutive siblings, grandparents, and head bigrams) by composing arc-factored models, head automata, and a sequence labeler.", "labels": [], "entities": []}, {"text": "However, no lightweight decomposition seems possible for incorporating parts for all siblings, directed paths, and non-projective arcs.", "labels": [], "entities": []}, {"text": "1 shows the first-order logical formulae that encode the constraints in our model.", "labels": [], "entities": []}, {"text": "Each formula gives rise to a subproblem which is efficiently solvable (see \u00a74).", "labels": [], "entities": []}, {"text": "By ablating some of rows of Tab.", "labels": [], "entities": []}, {"text": "1 we recover known methods: \u2022 Resorting to the tree and consecutive sibling formulae gives one of the models in , with the same linear relaxation (a proof of this fact is included in App.", "labels": [], "entities": [{"text": "App", "start_pos": 183, "end_pos": 186, "type": "DATASET", "confidence": 0.9530147314071655}]}, {"text": "F); \u2022 Resorting to tree, all siblings, grandparent, and non-projective arcs, recovers a multi-commodity flow configuration proposed by; the relaxation is also the same.", "labels": [], "entities": []}, {"text": "The experimental results are shown in Tab.", "labels": [], "entities": [{"text": "Tab.", "start_pos": 38, "end_pos": 42, "type": "DATASET", "confidence": 0.9701259434223175}]}, {"text": "2. For comparison, we include the best published results for each dataset (at the best of our knowledge), among transition-based parsers (, graph-based parsers, hybrid methods (, and turbo parsers).", "labels": [], "entities": []}, {"text": "Our full model achieved the best reported scores for 7 datasets.", "labels": [], "entities": []}, {"text": "The last two columns show a consistent improvement (with the exceptions of Chinese and Arabic) when using the full set of features over a second order model with grandparent and consecutive siblings, which is our reproduction of the model of . 13 Although also incorporated consecutive siblings in one of their configurations, our constraints are tighter than theirs.", "labels": [], "entities": []}, {"text": "F. Note however that the actual results of  are higher than our reproduction, as can be seen in the second column.", "labels": [], "entities": [{"text": "reproduction", "start_pos": 64, "end_pos": 76, "type": "METRIC", "confidence": 0.9540273547172546}]}, {"text": "The differences are due to the features that were used and on the way the models were trained.", "labels": [], "entities": []}, {"text": "The cause is not search error: exact decoding with an ILP solver (CPLEX) revealed no significant difference with respect to our G+CS column.", "labels": [], "entities": []}, {"text": "We leave further analysis for future work.", "labels": [], "entities": []}, {"text": "In columns 3-4, \"Full\" is our full model, and \"G+CS\" is our reproduction of the model of , i.e., the same as \"Full\" but with all features ablated excepted for grandparents and consecutive siblings.", "labels": [], "entities": []}, {"text": "Feature ablation and error analysis.", "labels": [], "entities": [{"text": "Feature ablation and error analysis", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.6546912550926208}]}, {"text": "We conducted a simple ablation study by training several models on the English PTB with different sets of features.", "labels": [], "entities": [{"text": "English PTB", "start_pos": 71, "end_pos": 82, "type": "DATASET", "confidence": 0.908082515001297}]}, {"text": "As expected, performance keeps increasing as we use models with greater expressive power.", "labels": [], "entities": []}, {"text": "We show some concrete examples in App.", "labels": [], "entities": [{"text": "App.", "start_pos": 34, "end_pos": 38, "type": "DATASET", "confidence": 0.974320262670517}]}, {"text": "G of sentences that the full model parsed correctly, unlike less expressive models.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Unlabeled attachment scores, excluding punc- tuation. In the second column, [Ma08] denotes Martins  et al. (2008), [KC10] is Koo and Collins (2010), [Ma10]  is Martins et al. (2010), and [Ko10] is", "labels": [], "entities": []}, {"text": " Table 3: Feature ablation experiments. AF is an arc- factored model; +G+CS adds grandparent and consec- utive siblings; +AS adds all-siblings; +NP adds non- projective arcs; Full adds the bigram and directed paths.", "labels": [], "entities": []}]}