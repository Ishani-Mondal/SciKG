{"title": [], "abstractContent": [{"text": "We present an automatic method which leverages word lengthening to adapt a sentiment lexicon specifically for Twitter and similar social messaging networks.", "labels": [], "entities": [{"text": "word lengthening", "start_pos": 47, "end_pos": 63, "type": "TASK", "confidence": 0.7183785438537598}]}, {"text": "The contributions of the paper are as follows.", "labels": [], "entities": []}, {"text": "First, we call attention to lengthening as a widespread phenomenon in microblogs and social messag-ing, and demonstrate the importance of handling it correctly.", "labels": [], "entities": []}, {"text": "We then show that lengthening is strongly associated with subjectivity and sentiment.", "labels": [], "entities": [{"text": "lengthening", "start_pos": 18, "end_pos": 29, "type": "METRIC", "confidence": 0.876218855381012}]}, {"text": "Finally, we present an automatic method which leverages this association to detect domain-specific sentiment-and emotion-bearing words.", "labels": [], "entities": []}, {"text": "We evaluate our method by comparison to human judgments, and analyze its strengths and weaknesses.", "labels": [], "entities": []}, {"text": "Our results are of interest to anyone analyzing sentiment in microblogs and social networks, whether for research or commercial purposes.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recently, there has been a surge of interest in sentiment analysis of Twitter messages.", "labels": [], "entities": [{"text": "sentiment analysis of Twitter messages", "start_pos": 48, "end_pos": 86, "type": "TASK", "confidence": 0.9263904809951782}]}, {"text": "Many researchers (e.g.,; Kivran-Swaine and Naaman 2011) are interested in studying structure and interactions in social networks, where sentiment can play an important role.", "labels": [], "entities": []}, {"text": "Others use Twitter as a barometer for public mood and opinion in diverse areas such as entertainment, politics and economics.", "labels": [], "entities": []}, {"text": "For example, use Twitter messages posted in conjunction with the live presidential debate between Barack Obama and John McCain to gauge public opinion, measure public mood on Twitter and use it to predict upcoming stock market fluctuations, and O' connect public opinion data from polls to sentiment expressed in Twitter messages along a timeline.", "labels": [], "entities": []}, {"text": "A prerequisite of all such research is an effective method for measuring the sentiment of a post or tweet.", "labels": [], "entities": [{"text": "measuring the sentiment of a post or tweet", "start_pos": 63, "end_pos": 105, "type": "TASK", "confidence": 0.6989391073584557}]}, {"text": "Due to the extremely informal nature of the medium, and the length restriction 1 , the language and jargon which is used in Twitter varies significantly from that of commonly studied text corpora.", "labels": [], "entities": []}, {"text": "In addition, Twitter is a quickly evolving domain, and new terms are constantly being introduced.", "labels": [], "entities": []}, {"text": "These factors pose difficulties to methods designed for conventional domains, such as news.", "labels": [], "entities": []}, {"text": "One solution is to use human annotation.", "labels": [], "entities": []}, {"text": "For example, Kivran-Swaine and Naaman (2011) use manual coding of tweets in several emotion categories (e.g., joy, sadness) for their research.", "labels": [], "entities": []}, {"text": "use crowd sourcing via Amazon's Mechanical Turk.", "labels": [], "entities": [{"text": "crowd sourcing", "start_pos": 4, "end_pos": 18, "type": "TASK", "confidence": 0.7679023742675781}]}, {"text": "Manual encoding usually offers a deeper understanding and correspondingly higher accuracy than shallow automatic methods.", "labels": [], "entities": [{"text": "Manual encoding", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.6624416559934616}, {"text": "understanding", "start_pos": 40, "end_pos": 53, "type": "METRIC", "confidence": 0.9543483257293701}, {"text": "accuracy", "start_pos": 81, "end_pos": 89, "type": "METRIC", "confidence": 0.9982870221138}]}, {"text": "However, it is expensive and labor intensive and cannot be applied in real time.", "labels": [], "entities": []}, {"text": "Crowd-sourcing carries additional caveats of its own, such as issues of annotator expertise and reliability (see.", "labels": [], "entities": []}, {"text": "The automatic approach to sentiment analysis is commonly used for processing data from social networks and microblogs, where there is often a huge quantity of information and a need for low latency.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 26, "end_pos": 44, "type": "TASK", "confidence": 0.9659235775470734}]}, {"text": "Many automatic approaches (including all those used in the work mentioned above) have at their core a sentiment lexicon, containing a list of words la-beled with specific associated emotions (joy, happiness) or a polarity value (positive, neutral, negative).", "labels": [], "entities": []}, {"text": "The overall sentiment of apiece of text is calculated as a function of the labels of the component words.", "labels": [], "entities": []}, {"text": "Because Twitter messages are short, shallow approaches are sometimes considered sufficient.", "labels": [], "entities": []}, {"text": "There are also approaches that use deeper machine learning techniques to train sentiment classifiers on examples that have been labeled for sentiment, either manually or automatically, as described above.", "labels": [], "entities": [{"text": "sentiment classifiers", "start_pos": 79, "end_pos": 100, "type": "TASK", "confidence": 0.762726366519928}]}, {"text": "Recent examples of this approach are and.", "labels": [], "entities": []}, {"text": "Most established sentiment lexicons (e.g.,, see Section 5) were created fora general domain, and suffer from limited coverage and inaccuracies when applied to the highly informal domain of social networks communication.", "labels": [], "entities": []}, {"text": "By creating a sentiment lexicon which is specifically tailored to the microblogging domain, or adapting an existing one, we can expect to achieve higher accuracy and increased coverage.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 153, "end_pos": 161, "type": "METRIC", "confidence": 0.998258650302887}, {"text": "coverage", "start_pos": 176, "end_pos": 184, "type": "METRIC", "confidence": 0.965264618396759}]}, {"text": "Recent work in this area includes, who developed a method for automatically deriving an extensive sentiment lexicon from the web as a whole.", "labels": [], "entities": [{"text": "automatically deriving an extensive sentiment lexicon from the web", "start_pos": 62, "end_pos": 128, "type": "TASK", "confidence": 0.7188005480501387}]}, {"text": "The resulting lexicon has greatly increased coverage compared to existing dictionaries and can handle spelling errors and web-specific jargon.", "labels": [], "entities": [{"text": "coverage", "start_pos": 44, "end_pos": 52, "type": "METRIC", "confidence": 0.967264711856842}]}, {"text": "expand an existing well-vetted psychometric instrument -Profile of Mood States (POMS)) that associates terms with moods (e.g. calm, happy).", "labels": [], "entities": []}, {"text": "The authors use co-occurrence information from the Google n-gram corpus) to enlarge the original list of 72 terms to 964.", "labels": [], "entities": [{"text": "Google n-gram corpus", "start_pos": 51, "end_pos": 71, "type": "DATASET", "confidence": 0.7085208296775818}]}, {"text": "They use this expanded emotion lexicon (named GPOS) in conjunction with the lexicon of to estimate public mood from Twitter posts 2 . The method we present in this paper leverages a phenomenon that is specific to informal social communication to enable the extension of an existing lexicon in a domain specific manner.", "labels": [], "entities": []}], "datasetContent": [{"text": "To detect and analyze lengthened words, we employ the procedure described in.", "labels": [], "entities": []}, {"text": "We find sets of words in our data which share a common form and differ only in the number of times each letter is repeated (.", "labels": [], "entities": []}, {"text": "In Step 3 we remove sets where all the different forms are likely to be the result of misspelling, rather than lengthening.", "labels": [], "entities": []}, {"text": "Finally, in Step 4, we associate all the forms in a single set with a canonical form, which is the most common one observed in the data.", "labels": [], "entities": []}, {"text": "The procedure resulted in 4,359 sets of size > 1.", "labels": [], "entities": []}, {"text": "To reduce noise resulting from typos and misspellings, we do not consider words containing nonalphabetic characters, or sets where the canonical form is a single character or occurs less than 10 times.", "labels": [], "entities": []}, {"text": "This left us with 3,727 sets.", "labels": [], "entities": []}, {"text": "Analysis lists the canonical forms of the 20 largest sets in our list (in terms of the number of variations).", "labels": [], "entities": []}, {"text": "Most of the examples are used to express emotion or emphasis.", "labels": [], "entities": []}, {"text": "Onomatopoeic words expressing emotion (e.g., ow, ugh, yay) are often lengthened and, for some, the combined frequency of the different lengthened forms is actually greater than that of the canonical (single most frequent) one.", "labels": [], "entities": []}, {"text": "Lengthening is a common phenomenon in our dataset.", "labels": [], "entities": [{"text": "Lengthening", "start_pos": 0, "end_pos": 11, "type": "METRIC", "confidence": 0.81364905834198}]}, {"text": "Out of half-a-million tweets, containing roughly 6.5 million words, our procedure identifies 108,762 word occurrences which are lengthenings of a canonical form.", "labels": [], "entities": []}, {"text": "These words occur in 87,187 tweets (17.44% or approximately one out of every six, on average).", "labels": [], "entities": []}, {"text": "The wide-spread use of lengthening is surprising in light of the length restriction of Twitter.", "labels": [], "entities": []}, {"text": "point out several conventions that are used in text messages specifically to deal with this restriction.", "labels": [], "entities": []}, {"text": "The fact that lengthening is used in spite of the need for brevity suggests that it conveys important information.", "labels": [], "entities": [{"text": "lengthening", "start_pos": 14, "end_pos": 25, "type": "METRIC", "confidence": 0.988030731678009}]}, {"text": "Canonical Assumption We validate the assumption that the most frequent form in the set is the canonical form by examining sets containing one or more word forms that were identified in a standard English dictionary 3 . This was the case for 2,092 of the sets (56.13%).", "labels": [], "entities": []}, {"text": "Of these, in only 55 (2.63%) the most frequent form was not recognized by the dictionary.", "labels": [], "entities": []}, {"text": "This indicates that the strategy of choosing the most frequent form as the canonical one is reliable and highly accurate (> 97%).", "labels": [], "entities": []}, {"text": "Implications for NLP To examine the effects of lengthening on analyzing Twitter data, we look at the difference in coverage of a standard English dictionary when we explicitly handle lengthened words by mapping them to the canonical form.", "labels": [], "entities": []}, {"text": "Coverage with a standard dictionary is important for many NLP applications, such as information retrieval, translation, part-of-speech tagging and parsing.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 84, "end_pos": 105, "type": "TASK", "confidence": 0.8495180606842041}, {"text": "translation", "start_pos": 107, "end_pos": 118, "type": "TASK", "confidence": 0.9585890769958496}, {"text": "part-of-speech tagging", "start_pos": 120, "end_pos": 142, "type": "TASK", "confidence": 0.7287033200263977}]}, {"text": "The canonical form for 2,037 word-sets are identified by our dictionary.", "labels": [], "entities": []}, {"text": "We searched for occurrences of these words which were lengthened by two or more characters, meaning they would not be identified using standard lemmatization methods or spell-correction techniques that are based on edit At the beginning of Section 2 we presented the hypothesis that lengthening represents a textual substitute for prosodic indicators in speech.", "labels": [], "entities": []}, {"text": "As such, it is not used arbitrarily, but rather applied to subjective words to strengthen the sentiment or emotion they convey.", "labels": [], "entities": []}, {"text": "The examples presented in in the previous section appear to support this hypothesis.", "labels": [], "entities": []}, {"text": "In this section we wish to provide experimental evidence for our hypothesis, by demonstrating a significant degree of association between lengthening and subjectivity.", "labels": [], "entities": []}, {"text": "For this purpose we use an existing sentiment lexicon (), which is commonly used in the literature (see Section 1) and is at the core of OpinionFinder 4 , a popular sentiment analysis tool designed to determine opinion in a general domain.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 165, "end_pos": 183, "type": "TASK", "confidence": 0.8029848635196686}]}, {"text": "The lexicon provides a list of subjective words, each annotated with its degree of subjectivity (strongly subjective, weakly subjective), as well as its sentiment polarity (positive, negative, or neutral).", "labels": [], "entities": []}, {"text": "In these experiments, we use the presence of a word (canonical form) in the lexicon as an indicator of subjectivity.", "labels": [], "entities": []}, {"text": "It should be noted that the reverse is not true, i.e., the fact that a word is absent from the lexicon does not indicate it is objective.", "labels": [], "entities": []}, {"text": "As a measure of tendency to lengthen a word, we look at the number of distinct forms of that word appearing in our dataset (the cardinality of the set to which it belongs).", "labels": [], "entities": []}, {"text": "We group the words according to this statistic, and compare to the vocabulary of our dataset (all words appearing in our data ten times or more, and consisting of two or more alphabetic characters, see Section 4).", "labels": [], "entities": []}, {"text": "shows the percentage of subjective words (those in the lexicon) in each of the groups.", "labels": [], "entities": []}, {"text": "As noted previously, this is a lower bound, since it is possible (in fact, very likely) that other words in the group are subjective, despite being absent from the lexicon.", "labels": [], "entities": []}, {"text": "The graph shows a clear trend -the more lengthening forms a word has, the more likely it is to be subjective (as measured by the percentage of words in the lexicon).", "labels": [], "entities": []}, {"text": "The reverse also holds -if a word is used to convey sentiment, it is more likely to be lengthened.", "labels": [], "entities": []}, {"text": "We can verify this by calculating the average number of distinct forms for words in our data that are subjective and comparing to the rest.", "labels": [], "entities": []}, {"text": "This calculation yields an average of 2.41 forms for words appearing in our sentiment lexicon (our proxy for subjectivity), compared to an average of 1.79 for those that aren't 5 . This difference is statistically significant at p < 0.01%, using a student t-test.", "labels": [], "entities": []}, {"text": "The lexicon we use was designed fora general domain, and suffers from limited coverage (see below) and inaccuracies (see O'Connor et al.", "labels": [], "entities": []}, {"text": "2010 and below Section 6.2 for examples), due to the domain shift.", "labels": [], "entities": []}, {"text": "The sentiment lexicon contains 6,878 words, but only 4,939 occur in our data, and only 2,446 appear more than 10 times.", "labels": [], "entities": []}, {"text": "Of those appearing in our data, only 485 words (7% of the lexicon vocabulary) are lengthened (the bar for group 2+ in), but these are extremely salient.", "labels": [], "entities": []}, {"text": "They encompass 701,607 instances (79% of total instances of words from the lexicon), and 339,895 tweets.", "labels": [], "entities": []}, {"text": "This provides further evidence that lengthening is used with salient sentiment words.", "labels": [], "entities": []}, {"text": "These results also demonstrates the limitations of using a sentiment lexicon which is not tailored to the domain.", "labels": [], "entities": []}, {"text": "Only a small fraction of the lexicon is represented in our data, and it is likely that there are many sentiment words that are commonly used but are absent from it.", "labels": [], "entities": []}, {"text": "We address this issue in the next section.", "labels": [], "entities": []}, {"text": "The previous experiment showed the connection between lengthening and sentiment-bearing words.", "labels": [], "entities": []}, {"text": "It also demonstrated some of the shortcomings of a lexicon which is not specifically tailored to our domain.", "labels": [], "entities": []}, {"text": "There are two steps we can take to use the lengthening phenomenon to adapt an existing sentiment lexicon.", "labels": [], "entities": []}, {"text": "The first of these is simply to take lengthening into account when identifying sentiment-bearing words in our corpus.", "labels": [], "entities": []}, {"text": "The second This, too, is a conservative estimate, since the later group also includes subjective words, as mentioned. is to exploit the connection between lengthening and sentiment to expand the lexicon itself.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The canonical forms of the 20 largest sets (in  terms of cardinality), with the number of occurrences of  the canonical and non-canonical forms.", "labels": [], "entities": []}]}