{"title": [{"text": "Optimizing Semantic Coherence in Topic Models", "labels": [], "entities": []}], "abstractContent": [{"text": "Latent variable models have the potential to add value to large document collections by discovering interpretable, low-dimensional subspaces.", "labels": [], "entities": []}, {"text": "In order for people to use such models, however, they must trust them.", "labels": [], "entities": []}, {"text": "Unfortunately , typical dimensionality reduction methods for text, such as latent Dirichlet allocation , often produce low-dimensional sub-spaces (topics) that are obviously flawed to human domain experts.", "labels": [], "entities": [{"text": "dimensionality reduction", "start_pos": 24, "end_pos": 48, "type": "TASK", "confidence": 0.7247438728809357}, {"text": "latent Dirichlet allocation", "start_pos": 75, "end_pos": 102, "type": "TASK", "confidence": 0.568387359380722}]}, {"text": "The contributions of this paper are threefold: (1) An analysis of the ways in which topics can be flawed; (2) an automated evaluation metric for identifying such topics that does not rely on human annotators or reference collections outside the training data; (3) a novel statistical topic model based on this metric that significantly improves topic quality in a large-scale document collection from the National Institutes of Health (NIH).", "labels": [], "entities": []}], "introductionContent": [{"text": "Statistical topic models such as latent Dirichlet allocation (LDA) () provide a powerful framework for representing and summarizing the contents of large document collections.", "labels": [], "entities": [{"text": "latent Dirichlet allocation (LDA)", "start_pos": 33, "end_pos": 66, "type": "TASK", "confidence": 0.6840225259462992}, {"text": "summarizing the contents of large document collections", "start_pos": 120, "end_pos": 174, "type": "TASK", "confidence": 0.8372178162847247}]}, {"text": "In our experience, however, the primary obstacle to acceptance of statistical topic models by users the outside machine learning community is the presence of poor quality topics.", "labels": [], "entities": []}, {"text": "Topics that mix unrelated or looselyrelated concepts substantially reduce users' confidence in the utility of such automated systems.", "labels": [], "entities": []}, {"text": "In general, users prefer models with larger numbers of topics because such models have greater resolution and are able to support finer-grained distinctions.", "labels": [], "entities": []}, {"text": "Unfortunately, we have observed that there is a strong relationship between the size of topics and the probability of topics being nonsensical as judged by domain experts: as the number of topics increases, the smallest topics (number of word tokens assigned to each topic) are almost always poor quality.", "labels": [], "entities": []}, {"text": "The common practice of displaying only a small number of example topics hides the fact that as many as 10% of topics maybe so bad that they cannot be shown without reducing users' confidence.", "labels": [], "entities": []}, {"text": "The evaluation of statistical topic models has traditionally been dominated by either extrinsic methods (i.e., using the inferred topics to perform some external task such as information retrieval)) or quantitative intrinsic methods, such as computing the probability of held-out documents (.", "labels": [], "entities": []}, {"text": "Recent work has focused on evaluation of topics as semanticallycoherent concepts.", "labels": [], "entities": []}, {"text": "For example, found that the probability of held-out documents is not always a good predictor of human judgments.", "labels": [], "entities": []}, {"text": "showed that an automated evaluation metric based on word co-occurrence statistics gathered from Wikipedia could predict human evaluations of topic quality.", "labels": [], "entities": []}, {"text": "used differences between topic-specific distributions over words and the corpus-wide distribution over words to identify overly-general \"vacuous\" topics.", "labels": [], "entities": []}, {"text": "Finally, developed semi-supervised methods that avoid specific user-labeled semantic coherence problems.", "labels": [], "entities": []}, {"text": "The contributions of this paper are threefold: (1) To identify distinct classes of low-quality topics, some of which are not flagged by existing evaluation methods; (2) to introduce anew topic \"coherence\" score that corresponds well with human coherence judgments and makes it possible to identify 262 specific semantic problems in topic models without human evaluations or external reference corpora; to present an example of anew topic model that learns latent topics by directly optimizing a metric of topic coherence.", "labels": [], "entities": []}, {"text": "With little additional computational cost beyond that of LDA, this model exhibits significant gains in average topic coherence score.", "labels": [], "entities": []}, {"text": "Although the model does not result in a statisticallysignificant reduction in the number of topics marked \"bad\", the model consistently improves the topic coherence score of the ten lowest-scoring topics (i.e., results in bad topics that are \"less bad\" than those found using LDA) while retaining the ability to identify low-quality topics without human interaction.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated the new model on a corpus of NIH grant abstracts.", "labels": [], "entities": [{"text": "NIH grant abstracts", "start_pos": 42, "end_pos": 61, "type": "DATASET", "confidence": 0.8405771652857462}]}, {"text": "Details are given in table 3.", "labels": [], "entities": []}, {"text": "shows the performance of the generalized P\u00f3lya urn model relative to LDA.", "labels": [], "entities": []}, {"text": "Two metrics-our new topic coherence metric and the log probability of held-out documents-are shown over 1000 iterations at 50 iteration intervals.", "labels": [], "entities": []}, {"text": "Each model was run over five folds of cross validation, each with three random initializations.", "labels": [], "entities": []}, {"text": "For each model we calculated an overall coherence score by calculating the topic coherence for each topic individually and then averaging these values.", "labels": [], "entities": []}, {"text": "We report the average overall 15 models in each plot.", "labels": [], "entities": []}, {"text": "Held-out probabilities were calculated using the left-to-right method of, with each cross-validation fold using its own schema A.", "labels": [], "entities": []}, {"text": "The generalized P\u00f3lya model performs very well in average topic coherence, reaching levels within the first 50 iterations that match the final score.", "labels": [], "entities": []}, {"text": "This model has an early advantage for held-out probability as well, but is eventually overtaken by LDA.", "labels": [], "entities": []}, {"text": "This trend is consistent with Chang et al.'s observation that held-out probabilities are not always good predictors of human judgments (.", "labels": [], "entities": []}, {"text": "Results are consistent over T \u2208 {100, 200, 300}.", "labels": [], "entities": []}, {"text": "In section 4.2, we demonstrated that our topic coherence metric correlates with expert opinions of topic quality for standard LDA.", "labels": [], "entities": []}, {"text": "The generalizedP\u00f3lya urn model was therefore designed with the goal of directly optimizing that metric.", "labels": [], "entities": []}, {"text": "It is possible, however, that optimizing for coherence directly could break the association between coherence metric and topic quality.", "labels": [], "entities": []}, {"text": "We therefore repeated the expert-driven evaluation protocol described in section 3.1.", "labels": [], "entities": []}, {"text": "We trained one standard LDA model and one generalized P\u00f3lya urn model, each with T = 200, and randomly shuffled the 400 resulting topics.", "labels": [], "entities": [{"text": "T", "start_pos": 81, "end_pos": 82, "type": "METRIC", "confidence": 0.9845594763755798}]}, {"text": "The topics were then presented to the experts from NINDS, with no indication as to the identity of the model from which each topic came.", "labels": [], "entities": [{"text": "NINDS", "start_pos": 51, "end_pos": 56, "type": "DATASET", "confidence": 0.9011932611465454}]}, {"text": "As these evaluations are time consuming, the experts evaluated the only the first 200 topics, which consisted of 103 generalized P\u00f3lya urn topics and 97 LDA topics.", "labels": [], "entities": []}, {"text": "AUC values predicting bad topics given coherence were 0.83 and 0.80, respectively.", "labels": [], "entities": [{"text": "AUC", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8155642151832581}]}, {"text": "Coherence effectively predicts topic quality in both models.", "labels": [], "entities": []}, {"text": "Although we were able to improve the average overall quality of topics and the average quality of the ten lowest-scoring topics, we found that the generalized P\u00f3lya urn model was less successful reducing the overall number of bad topics.", "labels": [], "entities": []}, {"text": "Ignoring one \"unbalanced\" topic from each model, 16.5% of the LDA topics and 13.5% from the generalized P\u00f3lya urn model were marked as \"bad.\"", "labels": [], "entities": [{"text": "P\u00f3lya urn model", "start_pos": 104, "end_pos": 119, "type": "DATASET", "confidence": 0.8030407428741455}]}, {"text": "While this result is an improvement, it is not significant at p = 0.05.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Co-document frequency matrix for the top words in a low-quality topic (according to our coherence metric),", "labels": [], "entities": []}, {"text": " Table 3: Data set statistics.", "labels": [], "entities": []}]}