{"title": [{"text": "A Weakly-supervised Approach to Argumentative Zoning of Scientific Documents", "labels": [], "entities": [{"text": "Argumentative Zoning of Scientific Documents", "start_pos": 32, "end_pos": 76, "type": "TASK", "confidence": 0.8326772391796112}]}], "abstractContent": [{"text": "Argumentative Zoning (AZ)-analysis of the argumentative structure of a scientific paper-has proved useful fora number of information access tasks.", "labels": [], "entities": [{"text": "Argumentative Zoning (AZ)-analysis of the argumentative structure of a scientific paper-has", "start_pos": 0, "end_pos": 91, "type": "TASK", "confidence": 0.8788566291332245}]}, {"text": "Current approaches to AZ rely on supervised machine learning (ML).", "labels": [], "entities": [{"text": "AZ", "start_pos": 22, "end_pos": 24, "type": "TASK", "confidence": 0.965986430644989}]}, {"text": "Requiring large amounts of annotated data, these approaches are expensive to develop and port to different domains and tasks.", "labels": [], "entities": []}, {"text": "A potential solution to this problem is to use weakly-supervised ML instead.", "labels": [], "entities": []}, {"text": "We investigate the performance of four weakly-supervised clas-sifiers on scientific abstract data annotated for multiple AZ classes.", "labels": [], "entities": []}, {"text": "Our best classifier based on the combination of active learning and self-training outperforms our best supervised clas-sifier, yielding a high accuracy of 81% when using just 10% of the labeled data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 143, "end_pos": 151, "type": "METRIC", "confidence": 0.9989590644836426}]}, {"text": "This result suggests that weakly-supervised learning could be employed to improve the practical applicability and portability of AZ across different information access tasks.", "labels": [], "entities": []}], "introductionContent": [{"text": "Many practical tasks require accessing specific types of information in scientific literature.", "labels": [], "entities": []}, {"text": "For example, a reader of scientific literature maybe looking for information about the objective of the study in question, the methods used in the study, the results obtained, or the conclusions drawn by authors.", "labels": [], "entities": []}, {"text": "Similarly, many Natural Language Processing (NLP) tasks focus on the extraction of specific types of information in documents only.", "labels": [], "entities": []}, {"text": "To date, a number of approaches have been proposed for sentence-based classification of scientific literature according to categories of information structure (or discourse, rhetorical, argumentative or conceptual structure, depending on the framework in question).", "labels": [], "entities": [{"text": "sentence-based classification of scientific literature", "start_pos": 55, "end_pos": 109, "type": "TASK", "confidence": 0.7504375457763672}]}, {"text": "Some of these classify sentences according to typical section names seen in scientific documents (, while others are based e.g. on argumentative zones (), qualitative dimensions () or conceptual structure (  of documents.", "labels": [], "entities": []}, {"text": "The best of current approaches have yielded promising results and proved useful for information retrieval, information extraction and summarization tasks (.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 84, "end_pos": 105, "type": "TASK", "confidence": 0.8282877802848816}, {"text": "information extraction", "start_pos": 107, "end_pos": 129, "type": "TASK", "confidence": 0.8273841440677643}, {"text": "summarization", "start_pos": 134, "end_pos": 147, "type": "TASK", "confidence": 0.9322648644447327}]}, {"text": "However, relying on fully supervised machine learning (ML) and a large body of annotated data, existing approaches are expensive to develop and port to different scientific domains and tasks.", "labels": [], "entities": []}, {"text": "A potential solution to this bottleneck is to develop techniques based on weakly-supervised ML.", "labels": [], "entities": []}, {"text": "Relying on a small amount of labeled data and a large pool of unlabeled data, weakly-supervised techniques (e.g. semi-supervision, active learning, co/tri-training, self-training) aim to keep the advantages of fully supervised approaches.", "labels": [], "entities": []}, {"text": "They have been applied to a wide range of NLP tasks, including named-entity recognition, question answering, information extraction, text classification and many others, yielding performance levels similar or equivalent to those of fully supervised techniques.", "labels": [], "entities": [{"text": "named-entity recognition", "start_pos": 63, "end_pos": 87, "type": "TASK", "confidence": 0.7391069084405899}, {"text": "question answering", "start_pos": 89, "end_pos": 107, "type": "TASK", "confidence": 0.8719997107982635}, {"text": "information extraction", "start_pos": 109, "end_pos": 131, "type": "TASK", "confidence": 0.8152801990509033}, {"text": "text classification", "start_pos": 133, "end_pos": 152, "type": "TASK", "confidence": 0.7909161746501923}]}, {"text": "To the best of our knowledge, such techniques have not yet been applied to the analysis of information structure of scientific documents by aforementioned approaches.", "labels": [], "entities": [{"text": "analysis of information structure of scientific documents", "start_pos": 79, "end_pos": 136, "type": "TASK", "confidence": 0.8134144800049918}]}, {"text": "Recent experiments have demonstrated the usefulness of weakly-supervised learning for classifying discourse relations in scientific texts, e.g.).", "labels": [], "entities": [{"text": "classifying discourse relations in scientific texts", "start_pos": 86, "end_pos": 137, "type": "TASK", "confidence": 0.8352258702119192}]}, {"text": "However, focusing on local (rather than global) structure of documents and being much more fine-grained in nature, this related task differs from ours considerably.", "labels": [], "entities": []}, {"text": "In this paper, we investigate the potential of weakly-supervised learning for Argumentative Zoning (AZ) of scientific abstracts.", "labels": [], "entities": [{"text": "Argumentative Zoning (AZ) of scientific abstracts", "start_pos": 78, "end_pos": 127, "type": "TASK", "confidence": 0.811341755092144}]}, {"text": "AZ is an approach to information structure which provides an analysis of the rhetorical progression of the scientific argument in a document (), law, (), biology () and chemistry () -and has proved useful for NLP tasks such as summarization ().", "labels": [], "entities": [{"text": "summarization", "start_pos": 227, "end_pos": 240, "type": "TASK", "confidence": 0.9826167225837708}]}, {"text": "Although the basic scheme is said to be disciplineindependent (, its application to different domains has resulted in various modifications and laborious annotation exercises.", "labels": [], "entities": []}, {"text": "This suggests that a weakly-supervised approach would be more practical than a fully supervised one for the real-world application of AZ.", "labels": [], "entities": []}, {"text": "Taking two supervised classifiers as a comparison point -Support Vector Machines (SVM) and Conditional Random Fields (CRF) -we investigate the performance of four weakly-supervised classifiers on the AZ task: two based on semi-supervised learning (transductive SVM and semi-supervised CRF) and two on active learning (Active SVM alone and in combination with self-training).", "labels": [], "entities": []}, {"text": "Our best weaklysupervised classifier (Active SVM with selftraining) outperforms the best supervised classifier (SVM), yielding high accuracy of 81% when using just 10% of the labeled data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 132, "end_pos": 140, "type": "METRIC", "confidence": 0.9990812540054321}]}, {"text": "When using just one third of the labeled data, it performs equally well as a fully supervised SVM which uses 100% of the labeled data.", "labels": [], "entities": []}, {"text": "Our investigation suggests that weaklysupervised learning could be employed to improve the practical applicability and portability of AZ to different information access tasks.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated the ML results in terms of accuracy, precision, recall, and F-measure against manual AZ annotations in the corpus: acc = no. of correctly classified sentences total no. of sentences in the corpus p = no. of sentences correctly identified as Class i total no. of sentences identified as Class i r = no. of sentences correctly identified as Class i total no. of sentences in Class if = 2 * p * r p+r We used 10-fold cross validation for all the methods to avoid the possible bias introduced by relying on any particular split of the data.", "labels": [], "entities": [{"text": "ML", "start_pos": 17, "end_pos": 19, "type": "TASK", "confidence": 0.8853915929794312}, {"text": "accuracy", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9993746876716614}, {"text": "precision", "start_pos": 50, "end_pos": 59, "type": "METRIC", "confidence": 0.9994091987609863}, {"text": "recall", "start_pos": 61, "end_pos": 67, "type": "METRIC", "confidence": 0.9996001124382019}, {"text": "F-measure", "start_pos": 73, "end_pos": 82, "type": "METRIC", "confidence": 0.9996480941772461}]}, {"text": "More specifically, the data was randomly assigned to ten folds of roughly the same size.", "labels": [], "entities": []}, {"text": "Each fold was used once as test data and the remaining nine folds as training data.", "labels": [], "entities": []}, {"text": "The results were then averaged.", "labels": [], "entities": []}, {"text": "Following, we used McNemar's test to measure the statistical significance between the results of different ML methods.", "labels": [], "entities": [{"text": "McNemar's test", "start_pos": 19, "end_pos": 33, "type": "DATASET", "confidence": 0.6972848375638326}, {"text": "ML", "start_pos": 107, "end_pos": 109, "type": "TASK", "confidence": 0.966420590877533}]}, {"text": "The chosen significance level was .05.", "labels": [], "entities": [{"text": "significance level", "start_pos": 11, "end_pos": 29, "type": "METRIC", "confidence": 0.9812926054000854}]}, {"text": "shows the results for the four weaklysupervised and two supervised methods when 10% of the training data (i.e. \u223c700 sentences) has been labeled.", "labels": [], "entities": []}, {"text": "We can see that ASSVM is the best performing method with an accuracy of 81% and the macro  F-score of .76 (the macro F-score is calculated for the 5 scheme categories which are found by all the methods).", "labels": [], "entities": [{"text": "ASSVM", "start_pos": 16, "end_pos": 21, "type": "METRIC", "confidence": 0.45005086064338684}, {"text": "accuracy", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.9993485808372498}, {"text": "F-score", "start_pos": 91, "end_pos": 98, "type": "METRIC", "confidence": 0.7956854104995728}, {"text": "F-score", "start_pos": 117, "end_pos": 124, "type": "METRIC", "confidence": 0.8269425630569458}]}, {"text": "ASVM performs nearly as well, with an accuracy of 80% and F-score of .75.", "labels": [], "entities": [{"text": "ASVM", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8903154134750366}, {"text": "accuracy", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.9996964931488037}, {"text": "F-score", "start_pos": 58, "end_pos": 65, "type": "METRIC", "confidence": 0.9996998310089111}]}, {"text": "Both methods outperform supervised SVM with a statistically significant difference (p < .001).", "labels": [], "entities": []}, {"text": "TSVM is the lowest performing SVM-based method.", "labels": [], "entities": [{"text": "TSVM", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.6300414204597473}]}, {"text": "Yielding an accuracy of 76% and F-score of .73 its performance is lower than that of the supervised SVM.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9997407793998718}, {"text": "F-score", "start_pos": 32, "end_pos": 39, "type": "METRIC", "confidence": 0.9996933937072754}]}, {"text": "However, it does outperform both CRFbased methods.", "labels": [], "entities": []}, {"text": "SSCRF performs better than CRF with 3% higher accuracy and .02 higher F-score.", "labels": [], "entities": [{"text": "SSCRF", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.7987993359565735}, {"text": "accuracy", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.9995502829551697}, {"text": "F-score", "start_pos": 70, "end_pos": 77, "type": "METRIC", "confidence": 0.9988596439361572}]}, {"text": "The difference inaccuracy is statistically significant (p < .001).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Distribution of sentences in the AZ-annotated  corpus", "labels": [], "entities": [{"text": "AZ-annotated  corpus", "start_pos": 43, "end_pos": 63, "type": "DATASET", "confidence": 0.8692120611667633}]}, {"text": " Table 3: Results when using 10% of the labeled data", "labels": [], "entities": []}, {"text": " Table 4: Leaving one feature out results for ASSVM when  using 10% of the labeled data", "labels": [], "entities": [{"text": "ASSVM", "start_pos": 46, "end_pos": 51, "type": "TASK", "confidence": 0.6505678296089172}]}]}