{"title": [{"text": "Learning the Information Status of Noun Phrases in Spoken Dialogues", "labels": [], "entities": [{"text": "Learning the Information Status of Noun Phrases in Spoken Dialogues", "start_pos": 0, "end_pos": 67, "type": "TASK", "confidence": 0.7703129261732101}]}], "abstractContent": [{"text": "An entity in a dialogue maybe old, new, or mediated/inferrable with respect to the hearer's beliefs.", "labels": [], "entities": []}, {"text": "Knowing the information status of the entities participating in a dialogue can therefore facilitate its interpretation.", "labels": [], "entities": []}, {"text": "We address the under-investigated problem of automatically determining the information status of discourse entities.", "labels": [], "entities": []}, {"text": "Specifically, we extend Nissim's (2006) machine learning approach to information-status determination with lexical and structured features, and exploit learned knowledge of the information status of each discourse entity for coreference resolution.", "labels": [], "entities": [{"text": "information-status determination", "start_pos": 69, "end_pos": 101, "type": "TASK", "confidence": 0.7069614380598068}, {"text": "coreference resolution", "start_pos": 225, "end_pos": 247, "type": "TASK", "confidence": 0.946999728679657}]}, {"text": "Experimental results on a set of Switchboard dialogues reveal that (1) incorporating our proposed features into Nissim's feature set enables our system to achieve state-of-the-art performance on information-status classification, and (2) the resulting information can be used to improve the performance of learning-based coreference resolvers.", "labels": [], "entities": [{"text": "information-status classification", "start_pos": 195, "end_pos": 228, "type": "TASK", "confidence": 0.7232331186532974}, {"text": "coreference resolvers", "start_pos": 321, "end_pos": 342, "type": "TASK", "confidence": 0.7251612544059753}]}], "introductionContent": [{"text": "Information status is not a term unfamiliar to researchers working on discourse processing problems.", "labels": [], "entities": []}, {"text": "It describes the extent to which a discourse entity, which is typically a noun phrase (NP), is available to the hearer given the speaker's assumptions about the hearer's beliefs.", "labels": [], "entities": []}, {"text": "According to, a discourse entity can be new, old, or mediated.", "labels": [], "entities": []}, {"text": "Informally, a discourse entity is (1) old to the hearer if it is known to the hearer and has previously been referred to in the dialogue, (2) new if it is unknown to her and has not been previously referred to; and (3) mediated if it is newly mentioned in the dialogue but she can infer its identity from a previously-mentioned entity.", "labels": [], "entities": []}, {"text": "Information status is a subject that has received a lot of attention in theoretical linguistics.", "labels": [], "entities": [{"text": "Information status", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8298349380493164}]}, {"text": "Knowing the information status of discourse entities can potentially benefit many NLP applications.", "labels": [], "entities": []}, {"text": "One such task is anaphora resolution.", "labels": [], "entities": [{"text": "anaphora resolution", "start_pos": 17, "end_pos": 36, "type": "TASK", "confidence": 0.8550264537334442}]}, {"text": "While there is general belief that definite descriptions are mostly anaphoric, empirically show that only 30% of these NPs are anaphoric.", "labels": [], "entities": []}, {"text": "Without being able to determine whether an NP is anaphoric, an anaphora resolver will attempt to resolve every NP, potentially damaging its precision.", "labels": [], "entities": [{"text": "anaphora resolver", "start_pos": 63, "end_pos": 80, "type": "TASK", "confidence": 0.7031799554824829}, {"text": "precision", "start_pos": 140, "end_pos": 149, "type": "METRIC", "confidence": 0.9967777132987976}]}, {"text": "Since new entities are by definition new to the hearer and therefore cannot refer to a previously-introduced NP, knowledge of information status could be used to improve anaphora resolution.", "labels": [], "entities": [{"text": "anaphora resolution", "start_pos": 170, "end_pos": 189, "type": "TASK", "confidence": 0.7404620945453644}]}, {"text": "Despite the potential usefulness of information status in NLP tasks, there has been little work on learning the information status of discourse entities.", "labels": [], "entities": []}, {"text": "To investigate the plausibility of learning information status, annotate a set of Switchboard dialogues with such information 1 , and subsequently present a rule-based approach and a learning-based approach to acquiring such knowledge from the manual annotations).", "labels": [], "entities": []}, {"text": "Our goals in this paper are two-fold.", "labels": [], "entities": []}, {"text": "First, we describe a learning approach to the under-studied problem of determining the information status of discourse entities that extends feature set with two novel types of features: lexical features and structured features based on syntactic parse trees.", "labels": [], "entities": []}, {"text": "Second, we employ the automatically acquired knowledge of information status for coreference resolution.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 81, "end_pos": 103, "type": "TASK", "confidence": 0.968729704618454}]}, {"text": "Experimental results on corpus of Switchboard dialogues show that (1) adding our linguistic features to Nissim's feature set enables our system to outperform her system by 8.1% in F-measure, and (2) learned knowledge of information status can be used to improve coreference resolvers by 1.1-2.6% in F-measure.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 180, "end_pos": 189, "type": "METRIC", "confidence": 0.9845871925354004}, {"text": "coreference resolvers", "start_pos": 262, "end_pos": 283, "type": "TASK", "confidence": 0.8943141102790833}, {"text": "F-measure", "start_pos": 299, "end_pos": 308, "type": "METRIC", "confidence": 0.879784882068634}]}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "We first illustrate with examples the concepts of new, old, and mediated entities.", "labels": [], "entities": []}, {"text": "Then, we describe the dataset and the feature set that used in her approach.", "labels": [], "entities": []}, {"text": "After that, we introduce our lexical and structured features.", "labels": [], "entities": []}, {"text": "Finally, we evaluate the determination of information status as a standalone task and in the context of coreference resolution.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 104, "end_pos": 126, "type": "TASK", "confidence": 0.9675650894641876}]}], "datasetContent": [{"text": "We employ dataset, which comprises 147 Switchboard dialogues.", "labels": [], "entities": []}, {"text": "A total of 68,992 NPs are annotated with information status: 51.2% of them are labeled as old, 34.5% as mediated (henceforth med), and 14.3% as new.", "labels": [], "entities": []}, {"text": "randomly split the instances created from these NPs into a training set (for classifier training), a development set (for feature development), and an evaluation set (for testing).", "labels": [], "entities": []}, {"text": "Hence, the NPs from the same 1070, we partition the 147 dialogues (rather than the instances) into a training set (117 dialogues) and a test set (30 dialogues).", "labels": [], "entities": [{"text": "NPs from the same 1070", "start_pos": 11, "end_pos": 33, "type": "DATASET", "confidence": 0.877196216583252}]}, {"text": "In other words, we do not randomize the instances, as we believe that it represents an unrealistic evaluation setting, for the following reasons.", "labels": [], "entities": []}, {"text": "First, in practice, the test dialogues may not be available until test time.", "labels": [], "entities": []}, {"text": "Second, we may want to examine how a system performs on a given dialogue.", "labels": [], "entities": []}, {"text": "Finally, randomizing the instances does not allow us to apply learned knowledge of information status to coreference resolution, which needs to be performed for each dialogue.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 105, "end_pos": 127, "type": "TASK", "confidence": 0.9468711912631989}]}, {"text": "The information status distribution of the NPs in the training and test sets are shown in.", "labels": [], "entities": []}, {"text": "Next, we evaluate the effectiveness of our features in improving information-status classification.", "labels": [], "entities": [{"text": "information-status classification", "start_pos": 65, "end_pos": 98, "type": "TASK", "confidence": 0.8442846536636353}]}, {"text": "The training/test split we use in the coreference experiments is the same as that in the informationstatus experiments.", "labels": [], "entities": []}, {"text": "Specifically, we use the training set to train both the information-status classifier and our coreference models, apply the informationstatus classifier to each discourse entity in the test set, and have the coreference models resolve all and only those NPs that are labeled as old by the information-status classifier.", "labels": [], "entities": []}, {"text": "Our decision to allow the coreference models to resolve only the old entities is motivated by the fact that med and new entities have not been previously introduced in the conversation and therefore do not have antecedents.", "labels": [], "entities": []}, {"text": "The NPs used by the coreference models are the same as those accessible to the information-status classifier.", "labels": [], "entities": []}, {"text": "We employ two scoring programs, B 3 (Bagga and Baldwin, 1998) and \u03c6 3 -CEAF (, to score the output of a coreference model.", "labels": [], "entities": [{"text": "\u03c6 3 -CEAF", "start_pos": 66, "end_pos": 75, "type": "METRIC", "confidence": 0.9079265296459198}]}, {"text": "Given a goldstandard (i.e., key) partition, KP , and a systemgenerated (i.e., response) partition, RP , B 3 computes the recall and precision of each NP and averages these values at the end.", "labels": [], "entities": [{"text": "recall", "start_pos": 121, "end_pos": 127, "type": "METRIC", "confidence": 0.9976945519447327}, {"text": "precision", "start_pos": 132, "end_pos": 141, "type": "METRIC", "confidence": 0.9342193603515625}]}, {"text": "Specifically, for each NP, NP j , B 3 first computes the number of NPs that appear in both KP j and RP j , the clusters containing NP j in KP and RP , respectively, and then divides this number by |KP j | and |RP j | to obtain the recall and precision of NP j , respectively.", "labels": [], "entities": [{"text": "recall", "start_pos": 231, "end_pos": 237, "type": "METRIC", "confidence": 0.999359667301178}, {"text": "precision", "start_pos": 242, "end_pos": 251, "type": "METRIC", "confidence": 0.9979966282844543}]}, {"text": "On the other hand, CEAF finds the best one-to-one alignment between the key clusters and the response clusters using the Kuhn-Munkres algorithm, where the weight of an edge connecting two clusters is equal to the number of NPs that appear in both clusters.", "labels": [], "entities": []}, {"text": "Precision and recall are equal to the sum of the weights of the edges in the alignment divided by the total number of NPs in the response and the key, respectively.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9909690022468567}, {"text": "recall", "start_pos": 14, "end_pos": 20, "type": "METRIC", "confidence": 0.9995158910751343}]}], "tableCaptions": [{"text": " Table 3: Per-class performance of four information-status classifiers.", "labels": [], "entities": []}, {"text": " Table 3. As we can see, adding lexical features  to the baseline features improves accuracy by 2.2%,  and adding structured features further improves ac- curacy by 5.9%. Our two types of features, when  used in combination with Nissim's features, improve  the baseline substantially by an accuracy of 8.1%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 84, "end_pos": 92, "type": "METRIC", "confidence": 0.9992120265960693}, {"text": "ac- curacy", "start_pos": 151, "end_pos": 161, "type": "METRIC", "confidence": 0.9110578497250875}, {"text": "accuracy", "start_pos": 290, "end_pos": 298, "type": "METRIC", "confidence": 0.9987666606903076}]}, {"text": " Table 5: Impact of knowledge of anaphoricity on the information-status classifiers.", "labels": [], "entities": []}, {"text": " Table 6: B 3 and CEAF coreference results.", "labels": [], "entities": [{"text": "CEAF coreference", "start_pos": 18, "end_pos": 34, "type": "DATASET", "confidence": 0.9109128415584564}]}]}