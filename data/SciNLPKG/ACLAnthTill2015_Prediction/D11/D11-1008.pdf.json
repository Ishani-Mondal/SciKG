{"title": [{"text": "Accurate Parsing with Compact Tree-Substitution Grammars: Double-DOP", "labels": [], "entities": [{"text": "Accurate", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.894584059715271}]}], "abstractContent": [{"text": "We present a novel approach to Data-Oriented Parsing (DOP).", "labels": [], "entities": [{"text": "Data-Oriented Parsing (DOP)", "start_pos": 31, "end_pos": 58, "type": "TASK", "confidence": 0.8270449876785279}]}, {"text": "Like other DOP models, our parser utilizes syntactic fragments of arbitrary size from a treebank to analyze new sentences, but, crucially, it uses only those which are encountered at least twice.", "labels": [], "entities": []}, {"text": "This criterion allows us to work with a relatively small but representative set of fragments, which can be employed as the symbolic backbone of several probabilistic generative models.", "labels": [], "entities": []}, {"text": "For parsing we define a transform-backtransform approach that allows us to use standard PCFG technology, making our results easily replica-ble.", "labels": [], "entities": [{"text": "parsing", "start_pos": 4, "end_pos": 11, "type": "TASK", "confidence": 0.9805230498313904}]}, {"text": "According to standard Parseval metrics, our best model is on par with many state-of-the-art parsers, while offering some complementary benefits: a simple generative probability model, and an explicit representation of the larger units of grammar.", "labels": [], "entities": []}], "introductionContent": [{"text": "Data-oriented Parsing (DOP) is an approach to wide-coverage parsing based on assigning structures to new sentences using fragments of variable size extracted from a treebank.", "labels": [], "entities": [{"text": "Data-oriented Parsing (DOP)", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.8018537044525147}, {"text": "wide-coverage parsing", "start_pos": 46, "end_pos": 67, "type": "TASK", "confidence": 0.7110436260700226}]}, {"text": "It was first proposed by Scha in 1990 and formalized by, and preceded many developments in statistical parsing (e.g., the \"treebank grammars\" of Charniak 1997) and linguistic theory (e.g., the current popularity of \"constructions\", Jackendoff 2002).", "labels": [], "entities": [{"text": "statistical parsing", "start_pos": 91, "end_pos": 110, "type": "TASK", "confidence": 0.7657325565814972}, {"text": "linguistic theory", "start_pos": 164, "end_pos": 181, "type": "TASK", "confidence": 0.7330027371644974}]}, {"text": "A rich literature on DOP has emerged since, yielding stateof-the-art results on the Penn treebank benchmark test and inspiring developments in related frameworks including tree kernels (), reranking) and Bayesian adaptor and fragment grammars (e.g.,.", "labels": [], "entities": [{"text": "Penn treebank benchmark test", "start_pos": 84, "end_pos": 112, "type": "DATASET", "confidence": 0.9820296615362167}]}, {"text": "By formalizing the idea of using large fragments of earlier language experience to analyze new sentences, DOP captures an important property of language cognition that has shaped natural language.", "labels": [], "entities": []}, {"text": "It therefore complements approaches that have focused on properties like lexicalization or incrementality, and might bring supplementary strengths in other NLP tasks.", "labels": [], "entities": []}, {"text": "Early versions of DOP (e.g.,  aimed at extracting all subtrees of all trees in the treebank.", "labels": [], "entities": []}, {"text": "The total number of constructions, however, is prohibitively large for non-trivial treebanks: it grows exponentially with the length of the sentences, yielding the astronomically large number of approximately 10 48 for section 2-21 of the Penn WSJ corpus.", "labels": [], "entities": [{"text": "Penn WSJ corpus", "start_pos": 239, "end_pos": 254, "type": "DATASET", "confidence": 0.9795336325963339}]}, {"text": "These models thus rely on a big sample of fragments, which inevitably includes a substantial portion of overspecialized constructions.", "labels": [], "entities": []}, {"text": "Later DOP models have used the Goodman transformation to obtain a compact representation of all fragments in the treebank.", "labels": [], "entities": []}, {"text": "In this case the grammatical constructions are no longer explicitly represented, and substantial engineering effort is needed to optimally tune the models and make them efficient.", "labels": [], "entities": []}, {"text": "In this paper we present a novel DOP model (Double-DOP) in which we extract a restricted yet representative subset of fragments: those recurring at least twice in the treebank.", "labels": [], "entities": []}, {"text": "The explicit representation of the fragments allows us to derive simpleways of estimating probabilistic models on top of the symbolic grammar.", "labels": [], "entities": []}, {"text": "This and other implementation choices aim at making the methodology transparent and easily replicable.", "labels": [], "entities": []}, {"text": "The accuracy of Double-DOP is well within the range of state-of-the-art parsers currently used in other NLP-tasks, while offering the additional benefits of a simple generative probability model and an explicit representation of grammatical constructions.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9995546936988831}]}, {"text": "The contributions of this paper are summarized as follows: (i) we describe an efficient tree-kernel algorithm which allows us to extract all recurring fragments, reducing the set of potential elementary units from the astronomical 10 48 to around 10 6 . (ii) We implement and compare different DOP estimation techniques to induce a probability model (PTSG) on top of the extracted symbolic grammar.", "labels": [], "entities": [{"text": "PTSG", "start_pos": 351, "end_pos": 355, "type": "METRIC", "confidence": 0.7704176902770996}]}, {"text": "(iii) We present a simple transformation of the extracted fragments into CFG-rules that allows us to use offthe-shelf PCFG parsing and inference.", "labels": [], "entities": [{"text": "CFG-rules", "start_pos": 73, "end_pos": 82, "type": "DATASET", "confidence": 0.9030762910842896}, {"text": "PCFG parsing", "start_pos": 118, "end_pos": 130, "type": "TASK", "confidence": 0.7302316129207611}]}, {"text": "(iv) We integrate Double-DOP with recent state-splitting approaches (), yielding an even more accurate parser and a better understanding of the relation between DOP and state-splitting.", "labels": [], "entities": []}, {"text": "The rest of the paper is structured as follows.", "labels": [], "entities": []}, {"text": "In section 2 we describe the symbolic backbone of the grammar formalism that we will use for parsing.", "labels": [], "entities": []}, {"text": "In section 3 we illustrate the probabilistic extension of the grammar, including our transformation of PTSGs to PCFGs that allows us to use a standard PCFG parser, and a different transform that allows us to use a standard implementation of the insideoutside algorithm.", "labels": [], "entities": []}, {"text": "In section 4 we present the experimental setup and the results.", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to build and test our Double-DOP model , we employ the Penn WSJ Treebank ().", "labels": [], "entities": [{"text": "Penn WSJ Treebank", "start_pos": 64, "end_pos": 81, "type": "DATASET", "confidence": 0.9540167252222697}]}, {"text": "We use sections 2-21 for training, section 24 for development and section 23 for testing.", "labels": [], "entities": []}, {"text": "Treebank binarization We start with some preprocessing of the treebank, following standard prac- tice in WSJ parsing.", "labels": [], "entities": [{"text": "WSJ parsing", "start_pos": 105, "end_pos": 116, "type": "TASK", "confidence": 0.7794234454631805}]}, {"text": "We remove traces and functional tags.", "labels": [], "entities": []}, {"text": "We apply a left binarization of the training treebank as in and, setting the horizontal history H=1 and the parent labeling P=1.", "labels": [], "entities": []}, {"text": "This means that when anode has more than 2 children, the i th child (for i \u2265 3) is conditioned on child i \u2212 1.", "labels": [], "entities": []}, {"text": "Moreover the labels of all non-lexical nodes are enriched with the labels of their parent node.", "labels": [], "entities": []}, {"text": "shows the binarized version of the tree structure in.", "labels": [], "entities": []}, {"text": "Unknown words We replace words appearing less than 5 times in the training data by one of 50 unknown word categories based on the presence of lexical features as implemented in.", "labels": [], "entities": []}, {"text": "In some of the experiments we also perform a smoothing over the lexical elements assigning low counts ( = 0.01) to open-class words, PoS-tags pairs not encountered in the training corpus . Fragment extraction We extract the symbolic grammar and fragment frequencies from this preprocessed treebank as explained in section 2.", "labels": [], "entities": []}, {"text": "This is the the most time-consuming step (around 160 CPU hours 11 ).", "labels": [], "entities": []}, {"text": "In the extracted grammar we have in total 1,029,342 recurring fragments and 17,768 unseen CFG rules.", "labels": [], "entities": []}, {"text": "We test several probability distributions over the fragments (section 3.2) and various maximization objectives (section 3.3).", "labels": [], "entities": []}, {"text": "Parsing We convert our PTSG into a PCFG (section 3.1) and use Bitpar 12 for parsing.", "labels": [], "entities": [{"text": "PCFG", "start_pos": 35, "end_pos": 39, "type": "DATASET", "confidence": 0.8936979174613953}, {"text": "parsing", "start_pos": 76, "end_pos": 83, "type": "TASK", "confidence": 0.9621781706809998}]}, {"text": "For approximating MPP and other objectives we marginalize probabilities from the 1,000 best derivations.", "labels": [], "entities": [{"text": "MPP", "start_pos": 18, "end_pos": 21, "type": "TASK", "confidence": 0.656511127948761}]}], "tableCaptions": [{"text": " Table 1: Comparison of the performance (per-category  F1 score) on the development set between the Berkeley  parser and the best Double-DOP model.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 55, "end_pos": 63, "type": "METRIC", "confidence": 0.9746221601963043}]}, {"text": " Table 2: Summary of the results of different parsers  on the test set (sec 23). Double-DOP experiments use  RFE, MCP with \u03bb = 1.15, H=1, P=1; those on state- splitting (Double-DOP-Sp) use Berkeley cycle 4, H=0,  P=0. Results from Petrov and Klein (2007) already in- clude smoothing which is performed similarly to our  smoothing technique (see section 4). (* Results on a de- velopment set, with sentences up to length 20.)", "labels": [], "entities": [{"text": "RFE", "start_pos": 109, "end_pos": 112, "type": "METRIC", "confidence": 0.9595438838005066}]}]}