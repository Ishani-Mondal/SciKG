{"title": [{"text": "Generalizing Case Frames Using a Thesaurus and the MDL Principle", "labels": [], "entities": [{"text": "Generalizing Case Frames", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8732485969861349}]}], "abstractContent": [{"text": "A new method for automatically acquiring case frame patterns from large corpora is proposed.", "labels": [], "entities": []}, {"text": "In particular, the problem of generalizing values of a case frame slot fora verb is viewed as that of estimating a conditional probability distribution over a partition of words, and anew generalization method based on the Minimum Description Length (MDL) principle is proposed.", "labels": [], "entities": []}, {"text": "In order to assist with efficiency, the proposed method makes use of an existing thesaurus and restricts its attention to those partitions that are present as \"cuts\" in the thesaurus tree, thus reducing the generalization problem to that of estimating a \"tree cut model\" of the thesaurus tree.", "labels": [], "entities": []}, {"text": "An efficient algorithm is given, which provably obtains the optimal tree cut model for the given frequency data of a case slot, in the sense of MDL.", "labels": [], "entities": []}, {"text": "Case frame patterns obtained by the method were used to resolve PP-attachment ambiguity.", "labels": [], "entities": []}, {"text": "Experimental results indicate that the proposed method improves upon or is at least comparable with existing methods.", "labels": [], "entities": []}], "introductionContent": [{"text": "We address the problem of automatically acquiring case frame patterns (selectional patterns, subcategorization patterns) from large corpora.", "labels": [], "entities": []}, {"text": "A satisfactory solution to this problem would have a great impact on various tasks in natural language processing, including the structural disambiguation problem in parsing.", "labels": [], "entities": []}, {"text": "The acquired knowledge would also be helpful for building a lexicon, as it would provide lexicographers with word usage descriptions.", "labels": [], "entities": []}, {"text": "In our view, the problem of acquiring case frame patterns involves the following two issues: (a) acquiring patterns of individual case frame slots; and (b) learning dependencies that may exist between different slots.", "labels": [], "entities": []}, {"text": "In this paper, we confine ourselves to the former issue, and refer the interested reader to, which deals with the latter issue.", "labels": [], "entities": []}, {"text": "The case frame (case slot) pattern acquisition process consists of two phases: extraction of case frame instances from corpus data, and generalization of those instances to case frame patterns.", "labels": [], "entities": [{"text": "case frame (case slot) pattern acquisition", "start_pos": 4, "end_pos": 46, "type": "TASK", "confidence": 0.6229277700185776}]}, {"text": "The generalization step is needed in order to represent the input case frame instances more compactly as well as to judge the (degree of) acceptability of unseen case frame instances.", "labels": [], "entities": []}, {"text": "For the extraction problem, there have been various methods proposed to date, which are quite adequate.", "labels": [], "entities": [{"text": "extraction", "start_pos": 8, "end_pos": 18, "type": "TASK", "confidence": 0.9564498066902161}]}, {"text": "The generalization problem, in contrast, is a more challenging one and has not been solved completely.", "labels": [], "entities": [{"text": "generalization", "start_pos": 4, "end_pos": 18, "type": "TASK", "confidence": 0.9815982580184937}]}, {"text": "A number of methods for generalizing values of a case frame slot fora verb have been", "labels": [], "entities": []}], "datasetContent": [{"text": "We applied our generalization method to large corpora and inspected the obtained tree cut models to see if they agreed with human intuition.", "labels": [], "entities": []}, {"text": "In our experiments, we extracted verbs and their case frame slots (verb, slot_name, slot_value triples) from the tagged texts of the Wall Street Journal corpus (ACL/DCI CD-ROM1) consisting of 126,084 sentences, using existing techniques (specifically, those in Smadja), then 9 There are several possible measures that one could take to address this issue, including the incorporation of absolute frequencies of the words (inside and outside the particular slot in question).", "labels": [], "entities": [{"text": "Wall Street Journal corpus (ACL/DCI CD-ROM1)", "start_pos": 133, "end_pos": 177, "type": "DATASET", "confidence": 0.9532093167304992}]}, {"text": "This is outside the scope of the present paper, and we simply refer the interested reader to one possible approach (Abe and Li 1996).", "labels": [], "entities": []}, {"text": "applied our method to generalize the slot_values.", "labels": [], "entities": []}, {"text": "shows some example triple data for the direct object slot of the verb eat.", "labels": [], "entities": []}, {"text": "There were some extraction errors present in the data, but we chose not to remove them, because in general there will always be extraction errors and realistic evaluation should leave them in.", "labels": [], "entities": []}, {"text": "When generalizing, we used the noun taxonomy of WordNet (version 1.4) (Miller 1995) as our thesaurus.", "labels": [], "entities": [{"text": "WordNet (version 1.4) (Miller 1995)", "start_pos": 48, "end_pos": 83, "type": "DATASET", "confidence": 0.8972953889105055}]}, {"text": "The noun taxonomy of WordNet has a structure of directed acyclic graph (DAG), and its nodes stand fora word sense (a concept) and often contain several words having the same word sense.", "labels": [], "entities": []}, {"text": "WordNet thus deviates from our notion of thesaurus--a tree in which each leaf node stands fora noun, each internal node stands for the class of nouns below it, and a noun is uniquely represented by a leaf node--so we took a few measures to deal with this.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9082726240158081}]}, {"text": "First, we modified our algorithm FInd-MDL so that it can be applied to a DAG; now, Find-MDL effectively copies each subgraph having multiple parents (and its associated data) so that the DAG is transformed to a tree structure.", "labels": [], "entities": [{"text": "FInd-MDL", "start_pos": 33, "end_pos": 41, "type": "DATASET", "confidence": 0.5170778632164001}]}, {"text": "Note that with this modification it is no longer guaranteed that the output model is optimal.", "labels": [], "entities": []}, {"text": "Next, we dealt heuristically with the issue of word-sense ambiguity by equally dividing the observed frequency of a noun between all the nodes containing that noun.", "labels": [], "entities": []}, {"text": "Finally, when an internal node contained nouns actually occurring in the data, we assigned the .frequencies of all the nodes below it to that internal node, and excised the whole subtree (subgraph) below it.", "labels": [], "entities": []}, {"text": "The last of these measures, in effect, defines the \"starting cut\" of the thesaurus from which to begin generalizing.", "labels": [], "entities": []}, {"text": "Since (word senses of) nouns that occur in natural language tend to concentrate in the middle of a taxonomy, the starting cut given by this method usually falls around the middle of the thesaurus.", "labels": [], "entities": []}, {"text": "1\u00b0 shows the starting cut and the resulting cut in WordNet for the direct object slot of eat with respect to the data in :mushroom> <lobster> <horse> <lobster> <pizza> <rope>  Case frame patterns obtained by our method can be used in various tasks in natural language processing.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 51, "end_pos": 58, "type": "DATASET", "confidence": 0.9590506553649902}]}, {"text": "In this paper, we test its effectiveness in a structural (PPattachment) disambiguation experiment.", "labels": [], "entities": [{"text": "PPattachment) disambiguation", "start_pos": 58, "end_pos": 86, "type": "TASK", "confidence": 0.5504679878552755}]}, {"text": "It has been empirically verified that the use of lexical semantic knowledge is effective in structural disambiguation, such as the PP-attachment problem.", "labels": [], "entities": []}, {"text": "There have been  many probabilistic methods proposed in the literature to address the PP-attachment problem using lexical semantic knowledge which, in our view, can be classified into three types.", "labels": [], "entities": []}, {"text": "The first approach) takes doubles of the form (verb, prep) and (nounl, prep), like those in, as training data to acquire semantic knowledge and judges the attachment sites of the prepositional phrases in quadruples of the form (verb, nounl, prep, noun2) e.g., (see, girl, with, telescope)--based on the acquired knowledge.", "labels": [], "entities": []}, {"text": "proposed the use of the lexical association measure calculated based on such doubles.", "labels": [], "entities": []}, {"text": "More specifically, they estimate P(prep I verb) and P(prep [ noun1), and calculate the so-called t-score, which is a measure of the statistical significance of the difference between P(prep I verb) and P(prep [ nounl).", "labels": [], "entities": []}, {"text": "If the t-score indicates that the former probability is significantly larger, Example input data as doubles.", "labels": [], "entities": []}, {"text": "see in see with girl with man with Example input data as triples.", "labels": [], "entities": []}, {"text": "see in park see with telescope girl with scarf see with friend man with hat Example input data as quadruples and labels.", "labels": [], "entities": []}, {"text": "see girl in park ADV see man with telescope ADV see girl with scarf ADN then the prepositional phrase is attached to verb, if the latter probability is significantly larger, it is attached to nounl, and otherwise no decision is made.", "labels": [], "entities": []}, {"text": "The second approach ( takes triples (verb, prep, noun2) and (nounl, prep, noun2), like those in, as training data for acquiring semantic knowledge and performs PP-attachment disambiguation on quadruples.", "labels": [], "entities": [{"text": "PP-attachment disambiguation", "start_pos": 160, "end_pos": 188, "type": "TASK", "confidence": 0.7254343628883362}]}, {"text": "For example, proposes the use of the selectional association measure calculated based on such triples, as described in Section 2.", "labels": [], "entities": []}, {"text": "More specifically, his method compares maxclassi~noun2 A(Classi and maxclassi~no,m2 A(Classi I nounl,prep) to make disambiguation decisions.", "labels": [], "entities": []}, {"text": "The third approach Collins and Brooks 1995) receives quadruples (verb, noun1, prep, noun2) and labels indicating which way the PP-attachment goes, like those in, and learns a disambiguation rule for resolving PP-attachment ambiguities.", "labels": [], "entities": []}, {"text": "For example, propose a method they call transformation-based error-driven learning (see also).", "labels": [], "entities": []}, {"text": "Their method first learns IF-THEN type rules, where the IF parts represent conditions like (prep is with) and (verb is see), and the THEN parts represent transformations from (attach to verb) to (attach to nounl), or vice versa.", "labels": [], "entities": []}, {"text": "The first rule is always a default decision, and all the other rules indicate transformations (changes of attachment sites) subject to various IF conditions.", "labels": [], "entities": []}, {"text": "We note that, for the disambiguation problem, the first two approaches are basically unsupervised learning methods, in the sense that the training data are merely positive examples for both types of attachments, which could in principle be extracted from pure corpus data with no human intervention.", "labels": [], "entities": [{"text": "disambiguation problem", "start_pos": 22, "end_pos": 44, "type": "TASK", "confidence": 0.9557363986968994}]}, {"text": "(For example, one could just use unambiguous sentences.)", "labels": [], "entities": []}, {"text": "The third approach, on the other hand, is a supervised learning method, which requires labeled data prepared by a human being.", "labels": [], "entities": []}, {"text": "Average number of quadruples per data set 820.4 The generalization method we propose falls into the second category, although it can also be used as a component in a combined scheme with many of the above methods (see, Alshawi and Carter).", "labels": [], "entities": []}, {"text": "We estimate P(noun2 I verb, prep) and P(noun2 I nount, prep) from training data consisting of triples, and compare them: If the former exceeds the latter (by a certain margin) we attach it to verb, else if the latter exceeds the former (by the same margin) we attach it to noun1.", "labels": [], "entities": []}, {"text": "In our experiments, described below, we compare the performance of our proposed method, which we refer to as MDL, against the methods proposed by,, and, referred to respectively as LA, SA, and TEL.", "labels": [], "entities": [{"text": "LA", "start_pos": 181, "end_pos": 183, "type": "METRIC", "confidence": 0.9909130334854126}, {"text": "SA", "start_pos": 185, "end_pos": 187, "type": "METRIC", "confidence": 0.748005747795105}, {"text": "TEL", "start_pos": 193, "end_pos": 196, "type": "METRIC", "confidence": 0.9832811951637268}]}, {"text": "We used the bracketed corpus of the Penn Treebank (Wall Street Journal corpus) as our data.", "labels": [], "entities": [{"text": "bracketed corpus of the Penn Treebank (Wall Street Journal corpus)", "start_pos": 12, "end_pos": 78, "type": "DATASET", "confidence": 0.8386209706465403}]}, {"text": "First we randomly selected one of the 26 directories of the WSJ files as the test data and what remains as the training data.", "labels": [], "entities": [{"text": "WSJ files", "start_pos": 60, "end_pos": 69, "type": "DATASET", "confidence": 0.9658671319484711}]}, {"text": "We repeated this process 10 times and obtained 10 sets of data consisting of different training data and test data.", "labels": [], "entities": []}, {"text": "We used these 10 data sets to conduct cross-validation as described below.", "labels": [], "entities": []}, {"text": "From the test data in each data set, we extracted (verb, noun1, prep, noun2) quadruples using the extraction tool provided by the Penn Treebank called \"tgrep.\"", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 130, "end_pos": 143, "type": "DATASET", "confidence": 0.9951878190040588}]}, {"text": "At the same time, we obtained the answer for the PP-attachment site for each quadruple.", "labels": [], "entities": []}, {"text": "We did not double-check if the answers provided in the Penn Treebank were actually corrector not.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 55, "end_pos": 68, "type": "DATASET", "confidence": 0.9742147624492645}]}, {"text": "Then from the training data of each data set, we extracted and and (nounl,prep, noun2) triples using tools we developed ourselves.", "labels": [], "entities": []}, {"text": "We also extracted quadruples from the training data as before.", "labels": [], "entities": []}, {"text": "We then applied 12 heuristic rules to further preprocess the data, which include (1) changing the inflected form of a word to its stem form, (2) replacing numerals with the word number, (3) replacing integers between 1,900 and 2,999 with the word year, (4) replacing co., ltd., etc. with the words company, limited, etc.", "labels": [], "entities": []}, {"text": "11 After preprocessing there still remained some minor errors, which we did not remove further, due to the lack of a good method for doing so automatically.", "labels": [], "entities": []}, {"text": "shows the number of different types of data obtained by the above process.", "labels": [], "entities": []}, {"text": "We first compared the accuracy and coverage for each of the three disambiguation methods based on unsupervised learning: MDL, SA, and LA.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9995861649513245}, {"text": "coverage", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9876055717468262}, {"text": "LA", "start_pos": 134, "end_pos": 136, "type": "METRIC", "confidence": 0.9824416041374207}]}], "tableCaptions": [{"text": " Table 4  Description length of the five tree cut models.", "labels": [], "entities": []}, {"text": " Table 8  Required computation time and number of generalized levels.", "labels": [], "entities": []}, {"text": " Table 13  Results of PP-attachment disambiguation.", "labels": [], "entities": [{"text": "PP-attachment disambiguation", "start_pos": 22, "end_pos": 50, "type": "TASK", "confidence": 0.8314360976219177}]}, {"text": " Table 14  Example generalization results for SA and MDL.", "labels": [], "entities": []}]}