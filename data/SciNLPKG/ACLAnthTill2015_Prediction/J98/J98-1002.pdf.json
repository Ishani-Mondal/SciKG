{"title": [], "abstractContent": [{"text": "We describe a method for automatic word sense disambiguation using a text corpus and a machine-readable dictionary (MRD).", "labels": [], "entities": [{"text": "automatic word sense disambiguation", "start_pos": 25, "end_pos": 60, "type": "TASK", "confidence": 0.658335454761982}]}, {"text": "The method is based on word similarity and context similarity measures.", "labels": [], "entities": []}, {"text": "Words are considered similar if they appear in similar contexts; contexts are similar if they contain similar words.", "labels": [], "entities": []}, {"text": "The circularity of this definition is resolved by an iterative, converging process, in which the system learns from the corpus a set of typical usages for each of the senses of the polysemous word listed in the MRD.", "labels": [], "entities": []}, {"text": "A new instance of a polysemous word is assigned the sense associated with the typical usage most similar to its context.", "labels": [], "entities": []}, {"text": "Experiments show that this method can learn even from very sparse training data, achieving over 92% correct disambiguation performance.", "labels": [], "entities": [{"text": "correct", "start_pos": 100, "end_pos": 107, "type": "METRIC", "confidence": 0.9688876271247864}]}], "introductionContent": [{"text": "Word sense disambiguation (WSD) is the problem of assigning a sense to an ambiguous word, using its context.", "labels": [], "entities": [{"text": "Word sense disambiguation (WSD)", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.8322321623563766}]}, {"text": "We assume that different senses of a word correspond to different entries in its dictionary definition.", "labels": [], "entities": []}, {"text": "For example, suit has two senses listed in a dictionary: 'an action in court,' and 'suit of clothes.'", "labels": [], "entities": []}, {"text": "Given the sentence The union's lawyers are reviewing the suit, we would like the system to decide automatically that suit is used therein its court-related sense (we assume that the part of speech of the polysemous word is known).", "labels": [], "entities": []}, {"text": "In recent years, text corpora have been the main source of information for learning automatic WSD (see, for example,).", "labels": [], "entities": [{"text": "WSD", "start_pos": 94, "end_pos": 97, "type": "TASK", "confidence": 0.7506612539291382}]}, {"text": "A typical corpus-based algorithm constructs a training set from all contexts of a polysemous word W in the corpus, and uses it to learn a classifier that maps instances of W (each supplied with its context) into the senses.", "labels": [], "entities": []}, {"text": "Because learning requires that the examples in the training set be partitioned into the different senses, and because sense information is not available in the corpus explicitly, this approach depends critically on manual sense tagging--a laborious and time-consuming process that has to be repeated for every word, in every language, and, more likely than not, for every topic of discourse or source of information.", "labels": [], "entities": [{"text": "sense tagging", "start_pos": 222, "end_pos": 235, "type": "TASK", "confidence": 0.7294121384620667}]}, {"text": "The need for tagged examples creates a problem referred to in previous works as the knowledge acquisition bottleneck: training a disambiguator for W requires that the examples in the corpus be partitioned into senses, which, in turn, requires a fully operational disambiguator.", "labels": [], "entities": [{"text": "knowledge acquisition", "start_pos": 84, "end_pos": 105, "type": "TASK", "confidence": 0.7693758010864258}]}, {"text": "The method we propose circumvents this problem by automatically tagging the training set examples for W using other examples, that do not contain W, but do contain related words extracted from its dictionary definition.", "labels": [], "entities": []}, {"text": "For instance, in the training set for suit, we would use, in addition to the contexts of suit, all the contexts of court and of clothes in the corpus, because court and clothes appear in the machine-readable dictionary (MRD) entry of suit that defines its two senses.", "labels": [], "entities": []}, {"text": "Note that, unlike the contexts of suit, which may discuss either court action or clothing, the contexts of court are not likely to be especially related to clothing, and, similarly, those of clothes will normally have little to do with lawsuits.", "labels": [], "entities": []}, {"text": "We will use this observation to tag the original contexts of suit.", "labels": [], "entities": []}, {"text": "Another problem that affects the corpus-based WSD methods is the sparseness of data: these methods typically rely on the statistics of co-occurrences of words, while many of the possible co-occurrences are not observed even in a very large corpus.", "labels": [], "entities": []}, {"text": "We address this problem in several ways.", "labels": [], "entities": []}, {"text": "First, instead of tallying word statistics from the examples for each sense (which maybe unreliable when the examples are few), we collect sentence-level statistics, representing each sentence by the set of features it contains (for more on features, see Section 4.2).", "labels": [], "entities": []}, {"text": "Second, we define a similarity measure on the feature space, which allows us to pool the statistics of similar features.", "labels": [], "entities": []}, {"text": "Third, in addition to the examples of the polysemous word 142 in the corpus, we learn also from the examples of all the words in the dictionary definition of W.", "labels": [], "entities": []}, {"text": "In our experiments, this resulted in a training set that could be up to 20 times larger than the set of original examples.", "labels": [], "entities": []}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes the approach we have developed.", "labels": [], "entities": []}, {"text": "In Section 3, we report the results of tests we have conducted on the Treebank-2 corpus.", "labels": [], "entities": [{"text": "Treebank-2 corpus", "start_pos": 70, "end_pos": 87, "type": "DATASET", "confidence": 0.9867905974388123}]}, {"text": "Section 4 concludes with a discussion of related methods and a summary.", "labels": [], "entities": []}, {"text": "Proofs and other details of our scheme can be found in the appendix.", "labels": [], "entities": []}], "datasetContent": [{"text": "We tested the algorithm on the Treebank-2 corpus, which contains one million words from the Wall Street Journal, 1989, and is considered a small corpus for the present task.", "labels": [], "entities": [{"text": "Treebank-2 corpus", "start_pos": 31, "end_pos": 48, "type": "DATASET", "confidence": 0.9153450131416321}, {"text": "Wall Street Journal, 1989", "start_pos": 92, "end_pos": 117, "type": "DATASET", "confidence": 0.9275473475456237}]}, {"text": "During the development and the tuning of the algorithm, we used the method of pseudowords (Gale, Church, and Yarowsky 1992; Schitze 1992), to avoid the need for manual verification of the resulting sense tags.", "labels": [], "entities": []}, {"text": "The method of pseudowords is based on the observation that a disambiguation process designed to distinguish between two meanings of the same word should also The four polysemous test words, and the seed words they generated with the use of the MRD.", "labels": [], "entities": [{"text": "MRD", "start_pos": 244, "end_pos": 247, "type": "DATASET", "confidence": 0.7522878050804138}]}, {"text": "We now present in detail several of the results obtained with the word drug.", "labels": [], "entities": []}, {"text": "Consider first the effects of iteration.", "labels": [], "entities": []}, {"text": "A plot of the improvement in the performance vs. iteration number appears in.", "labels": [], "entities": []}, {"text": "The success rate is plotted for each sense, and for the weighted average of both senses we considered (the weights are proportional to the number of examples of each sense).", "labels": [], "entities": []}, {"text": "Iterations 2 and 5 can be seen to yield the best performance; iteration 5 is to be preferred, because of the smaller difference between the success rates for the two senses of the target word.", "labels": [], "entities": []}, {"text": "shows how the similarity values develop with iteration number.", "labels": [], "entities": []}, {"text": "For each example S of the 'narcotic' sense of drug, the value of simn(S, narcotic) increases with n. compares the similarities of a 'narcotic'-sense example to the 'narcotic' sense and to the 'medicine' sense, for each iteration.", "labels": [], "entities": []}, {"text": "One can see that the 'medicine' sense assignment, made in the first iteration, is gradually suppressed.", "labels": [], "entities": []}, {"text": "The word menace, which is a hint for the 'narcotic' sense in the sentence used in this example, did not help in the first iteration, because it did not appear in the 'narcotic' feedback set at all.", "labels": [], "entities": []}, {"text": "Thus, in iteration 1, the similarity of the sentence to the 'medicine' sense was 0.15, vs. a similarity of 0.1 to the 'narcotic' sense.", "labels": [], "entities": []}, {"text": "In iteration 2, menace was learned to be similar to other 'narcotic'-related words, yielding a small advantage for the 'narcotic' sense.", "labels": [], "entities": []}, {"text": "In iteration 3, further similarity values were updated, and there was a clear advantage to the 'narcotic' sense (0.93, vs. 0.89 for 'medicine').", "labels": [], "entities": []}, {"text": "Eventually, all similarity values become close to 1, and, because they are bounded by 1, they cannot change significantly with further iterations.", "labels": [], "entities": []}, {"text": "The decision is, therefore, best made after relatively few iterations, as we just saw.", "labels": [], "entities": []}, {"text": "shows the most similar words found for the words with the highest weights in the drug example (low-similarity words have been omitted).", "labels": [], "entities": []}, {"text": "Note that the similarity is contextual, and is affected by the polysemous target word.", "labels": [], "entities": []}, {"text": "For example, trafficking was found to be similar to crime, because in drug contexts the expressions drug trafficking and crime are highly related.", "labels": [], "entities": []}, {"text": "In general, trafficking and crime need not be similar, of course.", "labels": [], "entities": []}, {"text": "The drug experiment; the nearest neighbors of the highest-weight words.", "labels": [], "entities": []}], "tableCaptions": []}