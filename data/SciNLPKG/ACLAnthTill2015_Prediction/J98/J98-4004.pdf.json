{"title": [{"text": "PCFG Models of Linguistic Tree Representations", "labels": [], "entities": [{"text": "Linguistic Tree Representations", "start_pos": 15, "end_pos": 46, "type": "TASK", "confidence": 0.6894439955552419}]}], "abstractContent": [{"text": "The kinds of tree representations used in a treebank corpus can have a dramatic effect on performance of a parser based on the PCFG estimated from that corpus, causing the estimated likelihood of a tree to differ substantially from its frequency in the training corpus.", "labels": [], "entities": []}, {"text": "This paper points out that the Penn 1I treebank representations are of the kind predicted to have such an effect, and describes a simple node relabeling transformation that improves a treebank PCFG-based parser's average precision and recall by around 8%, or approximately half of the performance difference between a simple PCFG model and the best broad-coverage parsers available today.", "labels": [], "entities": [{"text": "Penn 1I treebank", "start_pos": 31, "end_pos": 47, "type": "DATASET", "confidence": 0.9550052483876547}, {"text": "precision", "start_pos": 221, "end_pos": 230, "type": "METRIC", "confidence": 0.9959108829498291}, {"text": "recall", "start_pos": 235, "end_pos": 241, "type": "METRIC", "confidence": 0.9976007342338562}]}, {"text": "This performance variation comes about because any PCFG, and hence the corpus of trees from which the PCFG is induced, embodies independence assumptions about the distribution of words and phrases.", "labels": [], "entities": []}, {"text": "The particular independence assumptions implicit in a tree representation can be studied theoretically and investigated empirically by means of a tree transformation / detransformation process.", "labels": [], "entities": []}], "introductionContent": [{"text": "Probabalistic context-free grammars (PCFGs) provide simple statistical models of natural languages.", "labels": [], "entities": []}, {"text": "The relative frequency estimator provides a straightforward way of inducing these grammars from treebank corpora, and a broad-coverage parsing system can be obtained by using a parser to find a maximum-likelihood parse tree for the input string with respect to such a treebank gram_mar.", "labels": [], "entities": [{"text": "broad-coverage parsing", "start_pos": 120, "end_pos": 142, "type": "TASK", "confidence": 0.6194383651018143}]}, {"text": "PCFG parsing systems often perform as well as other simple broad-coverage parsing system for predicting tree structure from part-of-speech (POS) tag sequences.", "labels": [], "entities": [{"text": "PCFG parsing", "start_pos": 0, "end_pos": 12, "type": "TASK", "confidence": 0.8122929632663727}, {"text": "broad-coverage parsing", "start_pos": 59, "end_pos": 81, "type": "TASK", "confidence": 0.6977294981479645}, {"text": "predicting tree structure from part-of-speech (POS) tag sequences", "start_pos": 93, "end_pos": 158, "type": "TASK", "confidence": 0.8323102474212647}]}, {"text": "While PCFG models do not perform as well as models that are sensitive to a wider range of dependencies, their simplicity makes them straightforward to analyze both theoretically and empirically.", "labels": [], "entities": []}, {"text": "Moreover, since more sophisticated systems can be viewed as refinements of the basic PCFG model, it seems reasonable to first attempt to better understand the properties of PCFG models themselves.", "labels": [], "entities": []}, {"text": "It is well known that natural language exhibits dependencies that context-free grammars (CFGs) cannot describe.", "labels": [], "entities": []}, {"text": "But the statistical independence assumptions embodied in a particular PCFG description of a particular natural language construction are in general much stronger than the requirement that the construction be generated by a CFG.", "labels": [], "entities": []}, {"text": "We show below that the PCFG extension of what seems to bean adequate CFG description of PP attachment constructions performs no better than PCFG models estimated from non-CFG accounts of the same constructions.", "labels": [], "entities": []}, {"text": "More specifically, this paper studies the effect of varying the tree structure representation of PP modification from both a theoretical and an empirical point of view.", "labels": [], "entities": []}, {"text": "It compares PCFG models induced from treebanks using several different tree repre-sentations, including the representation used in the Penn II treebank corpora and the \"Chomsky adjunction\" representation now standardly assumed in generative linguistics.", "labels": [], "entities": [{"text": "Penn II treebank corpora", "start_pos": 135, "end_pos": 159, "type": "DATASET", "confidence": 0.9379454851150513}, {"text": "generative linguistics", "start_pos": 230, "end_pos": 252, "type": "TASK", "confidence": 0.9420196413993835}]}, {"text": "One of the weaknesses of a PCFG model is that it is insensitive to nonlocal relationships between nodes.", "labels": [], "entities": []}, {"text": "If these relationships are significant then a PCFG will be a poor language model.", "labels": [], "entities": []}, {"text": "Indeed, the sense in which the set of trees generated by a CFG is \"context free\" is precisely that the label on anode completely characterizes the relationships between the subtree dominated by the node and the nodes that properly dominate this subtree.", "labels": [], "entities": []}, {"text": "Roughly speaking, the more nodes in the trees of the training corpus, the stronger the independence assumptions in the PCFG language model induced from those trees.", "labels": [], "entities": []}, {"text": "For example, a PCFG induced from a corpus of completely flat trees (i.e., consisting of the root node immediately dominating a string of terminals) generates precisely the strings of training corpus with likelihoods equal to their relative frequencies in that corpus.", "labels": [], "entities": []}, {"text": "Thus the location and labeling on the nonroot nonterminal nodes determine how a PCFG induced from a treebank generalizes from that training data.", "labels": [], "entities": []}, {"text": "Generally, one might expect that the fewer the nodes in the training corpus trees, the weaker the independence assumptions in the induced language model.", "labels": [], "entities": []}, {"text": "For this reason, a \"flat\" tree representation of PP modification is investigated here as well.", "labels": [], "entities": [{"text": "PP modification", "start_pos": 49, "end_pos": 64, "type": "TASK", "confidence": 0.8477579653263092}]}, {"text": "A second method of relaxing the independence assumptions implicit in a PCFG is to encode more information in each node's label.", "labels": [], "entities": []}, {"text": "Here the intuition is that the label on anode is a \"communication channel\" that conveys information between the subtree dominated by the node and the part of the tree not dominated by this node, so all other things being equal, appending to the node's label additional information about the context in which the node appears should make the independence assumptions implicit in the PCFG model weaker.", "labels": [], "entities": []}, {"text": "The effect of adding a particularly simple kind of contextual information--the category of the node's parent--is also studied in this paper.", "labels": [], "entities": []}, {"text": "Whether either of these two PCFG models outperforms a PCFG induced from the original treebank is a separate question.", "labels": [], "entities": []}, {"text": "We face a classical \"bias versus variance\" dilemma here: as the independence assumptions implicit in the PCFG model are weakened, the number of parameters that must be estimated (i.e., the number of productions) increases.", "labels": [], "entities": []}, {"text": "Thus while moving to a class of models with weaker independence assumptions permits us to more accurately describe a wider class of distributions (i.e., it reduces the bias implicit in the estimator), in general our estimate of these parameters will be less accurate simply because there are more of them to estimate from the same data (i.e., the variance in the estimator increases).", "labels": [], "entities": []}, {"text": "This paper studies the effects of these differing tree representations of PP modification theoretically by considering their effect on very simple corpora, and empirically by means of a tree transformation/detransformation methodology introduced below.", "labels": [], "entities": []}, {"text": "The corpus used as the source for the empirical study is version II of the Wall Street Journal (WSJ) corpus constructed at the University of Pennsylvania, modified as described in, in that: \u2022 root nodes (labeled ROOT) were inserted, \u2022 the terminal or lexical items were deleted (i.e., the terminal items in the trees were POS tags), \u2022 node labels consisted solely of syntactic category information (e.g., grammatical function and coindexation information was removed), \u2022 the POS tag of auxiliary verbs was replaced with AUX, \u2022 empty nodes (i.e., nodes dominating the empty string) were deleted, and \u2022 any resulting unary branching nodes dominating a single child with the same node label (i.e., which are expanded by a production X ~ X) were deleted.", "labels": [], "entities": [{"text": "Wall Street Journal (WSJ) corpus", "start_pos": 75, "end_pos": 107, "type": "DATASET", "confidence": 0.9187793220792498}]}], "datasetContent": [{"text": "It is straightforward to estimate PCFGs using the relative frequency estimator from the sequences of trees produced by applying these transforms to the WSJ corpus.", "labels": [], "entities": [{"text": "WSJ corpus", "start_pos": 152, "end_pos": 162, "type": "DATASET", "confidence": 0.9869115352630615}]}, {"text": "We turn now to the question of evaluating the different PCFGs so obtained.", "labels": [], "entities": []}, {"text": "None of the PCFGs induced from the various tree representations discussed here reliably identifies the correct tree representations on sentences from held-out data.", "labels": [], "entities": []}, {"text": "It is standard to evaluate broad-coverage parsers using less-stringent criteria that measure how similiar the trees produced by the parser are to the \"correct\" analysis trees in a portion of the treebank held out for testing purposes.", "labels": [], "entities": []}, {"text": "This study uses the 1,578 sentences in section 22 of the WSJ corpus of length 40 or less for this purpose.", "labels": [], "entities": [{"text": "WSJ corpus", "start_pos": 57, "end_pos": 67, "type": "DATASET", "confidence": 0.9495361149311066}]}, {"text": "The labeled precision and recall figures are obtained by regarding the sequence of trees e produced by a parser as a multiset or bag E(e) of edges, i.e., triples IN, 1, r / where N is a nonterminal label and 1 and rare left and right string positions in yield of the entire corpus.", "labels": [], "entities": [{"text": "precision", "start_pos": 12, "end_pos": 21, "type": "METRIC", "confidence": 0.9529221653938293}, {"text": "recall", "start_pos": 26, "end_pos": 32, "type": "METRIC", "confidence": 0.9984045624732971}]}, {"text": "(Root nodes and preterminal nodes are not included in these edge sets, as they are given as input to the parser).", "labels": [], "entities": []}, {"text": "Relative to a test sequence of trees er (here section 22 of the WSJ corpus) the labeled precision and recall of a sequence of trees e with the same yield as e tare calculated as follows, where then operation denotes multiset intersection.", "labels": [], "entities": [{"text": "WSJ corpus", "start_pos": 64, "end_pos": 74, "type": "DATASET", "confidence": 0.9780322909355164}, {"text": "precision", "start_pos": 88, "end_pos": 97, "type": "METRIC", "confidence": 0.9454631805419922}, {"text": "recall", "start_pos": 102, "end_pos": 108, "type": "METRIC", "confidence": 0.9993845224380493}]}, {"text": "Thus, precision is the fraction of edges in the tree sequence to be evaluated that also appear in the test tree sequence, and recall is the fraction of edges in the test tree sequence that also appear in tree sequence to be evaluated.", "labels": [], "entities": [{"text": "precision", "start_pos": 6, "end_pos": 15, "type": "METRIC", "confidence": 0.999519944190979}, {"text": "recall", "start_pos": 126, "end_pos": 132, "type": "METRIC", "confidence": 0.9995545744895935}]}, {"text": "It is straightforward to use the PCFG estimation techniques described in Section 2 to estimate PCFGs from the result of applying these transformations to sections 2-21 of the Penn II WSJ corpus.", "labels": [], "entities": [{"text": "PCFG estimation", "start_pos": 33, "end_pos": 48, "type": "TASK", "confidence": 0.5678521543741226}, {"text": "Penn II WSJ corpus", "start_pos": 175, "end_pos": 193, "type": "DATASET", "confidence": 0.9775703698396683}]}, {"text": "The resulting PCFGs can be used with a parser to obtain maximum-likelihood parse trees for the POS tag yields of the trees of the heldout test corpus (section 22 of the WSJ corpus).", "labels": [], "entities": [{"text": "WSJ corpus", "start_pos": 169, "end_pos": 179, "type": "DATASET", "confidence": 0.9741387963294983}]}, {"text": "While the resulting parse trees can be compared to the trees in the test corpus using the precision and recall measures described above, the results would not be meaningful as the parse trees reflect a different tree representation to that used in the test corpus, and thus are not directly comparable with the test corpus trees.", "labels": [], "entities": [{"text": "precision", "start_pos": 90, "end_pos": 99, "type": "METRIC", "confidence": 0.9989620447158813}, {"text": "recall", "start_pos": 104, "end_pos": 110, "type": "METRIC", "confidence": 0.9822608828544617}]}, {"text": "For example, the node labels used in the PCFG induced from trees produced by applying the parent transform are pairs of categories from the original Penn II WSJ tree bank, and so the labeled precision and recall measures obtained by comparing the parse trees obtained using this PCFG with the trees from the tree bank would be close to zero.", "labels": [], "entities": [{"text": "Penn II WSJ tree bank", "start_pos": 149, "end_pos": 170, "type": "DATASET", "confidence": 0.9724984288215637}, {"text": "precision", "start_pos": 191, "end_pos": 200, "type": "METRIC", "confidence": 0.9380783438682556}, {"text": "recall", "start_pos": 205, "end_pos": 211, "type": "METRIC", "confidence": 0.9964128136634827}]}, {"text": "One might try to overcome this by applying the same transformation to the test trees as was used to obtain the training trees for the PCFG, but then the resulting precision and recall measures would not be comparable across transformations.", "labels": [], "entities": [{"text": "PCFG", "start_pos": 134, "end_pos": 138, "type": "DATASET", "confidence": 0.8900068998336792}, {"text": "precision", "start_pos": 163, "end_pos": 172, "type": "METRIC", "confidence": 0.9992037415504456}, {"text": "recall", "start_pos": 177, "end_pos": 183, "type": "METRIC", "confidence": 0.9978869557380676}]}, {"text": "For example, as two different Penn II format trees may map to the same flattened tree, the flatten transformation is in general not invertible.", "labels": [], "entities": [{"text": "Penn II format trees", "start_pos": 30, "end_pos": 50, "type": "DATASET", "confidence": 0.9202893674373627}]}, {"text": "Thus a parsing system that produces perfect flat tree representations provides less information than one that produces perfect Penn II tree representations, and one might expect that all else being equal, a The tree transformation/detransformation process.", "labels": [], "entities": []}], "tableCaptions": []}