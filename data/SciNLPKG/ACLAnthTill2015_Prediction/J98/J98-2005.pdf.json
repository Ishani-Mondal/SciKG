{"title": [{"text": "Squibs and Discussions Estimation of Probabilistic Context-Free Grammars I", "labels": [], "entities": [{"text": "Squibs and Discussions Estimation of Probabilistic Context-Free Grammars I", "start_pos": 0, "end_pos": 74, "type": "TASK", "confidence": 0.7426806224717034}]}], "abstractContent": [{"text": "The assignment of probabilities to the productions of a context-free grammar may generate an improper distribution: the probability of all finite parse trees is less than one.", "labels": [], "entities": []}, {"text": "The condition for proper assignment is rather subtle.", "labels": [], "entities": [{"text": "proper assignment", "start_pos": 18, "end_pos": 35, "type": "TASK", "confidence": 0.758369117975235}]}, {"text": "Production probabilities can be estimated from parsed or unparsed sentences, and the question arises as to whether or not an estimated system is automatically proper.", "labels": [], "entities": []}, {"text": "We show here that estimated production probabilities always yield proper distributions.", "labels": [], "entities": []}], "introductionContent": [{"text": "Context-free grammars (CFG's) are useful because of their relatively broad coverage and because of the availability of efficient parsing algorithms.", "labels": [], "entities": [{"text": "Context-free grammars (CFG's)", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.7890464762846628}]}, {"text": "Furthermore, CFG's are readily fit with a probability distribution (to make probabilistic CFG's--or PCFG's), rendering them suitable for ambiguous languages through the maximum a posteriori rule of choosing the most probable parse.", "labels": [], "entities": []}, {"text": "For each nonterminal symbol, a (normalized) probability is placed on the set of all productions from that symbol.", "labels": [], "entities": []}, {"text": "Unfortunately, this simple procedure runs into an unexpected complication: the language generated by the grammar may have probability less than one.", "labels": [], "entities": []}, {"text": "The reason is that the derivation tree may have probability greater than zero of never terminating--some mass can be lost to infinity.", "labels": [], "entities": []}, {"text": "This phenomenon is well known and well understood, and there are tests for \"tightness\" (by which we mean total probability mass equal to one) involving a matrix derived from the expected growth in numbers of symbols generated by the probabilistic rules (see for example, Grenander, and Harris).", "labels": [], "entities": []}, {"text": "What if the production probabilities are estimated from data?", "labels": [], "entities": []}, {"text": "Suppose, for example, that we have a parsed corpus that we treat as a collection of (independent) samples from a grammar.", "labels": [], "entities": []}, {"text": "It is reasonable to hope that if the trees in the sample are finite, then an estimate of production probabilities based upon the sample will produce a system that assigns probability zero to the set of infinite trees.", "labels": [], "entities": []}, {"text": "For example, there is a simple maximum-likelihood prescription for estimating the production probabilities from a corpus of trees (see Section 2), resulting in a PCFG.", "labels": [], "entities": [{"text": "PCFG", "start_pos": 162, "end_pos": 166, "type": "DATASET", "confidence": 0.9145586490631104}]}, {"text": "If the corpus is unparsed then there is an iterative approach to maximum-likelihood estimation (the EM or Baum-Welsh algorithm--again, see Section 2) and the same question arises: do we get actual probabilities or do the estimated PCFG's assign some mass to infinite trees?", "labels": [], "entities": [{"text": "maximum-likelihood estimation", "start_pos": 65, "end_pos": 94, "type": "TASK", "confidence": 0.6756452471017838}]}, {"text": "We will show that in both cases the estimated probability is tight.", "labels": [], "entities": []}, {"text": "2 has asked a similar question: a scheme (different from maximum likelihood) is introduced for estimating production probabilities from an unparsed corpus, and it is conjectured that the resulting system is tight.", "labels": [], "entities": []}, {"text": "(Wetherell and others use the designation \"consistent\" instead of \"tight,\" but in statistics, consistency refers to the asymptotic correctness of an estimator.)", "labels": [], "entities": [{"text": "consistency", "start_pos": 94, "end_pos": 105, "type": "METRIC", "confidence": 0.9798086285591125}]}, {"text": "A trivial example is the CFG with one nonterminal and one terminal symbol, in Chomsky normal form: where a is the only terminal symbol.", "labels": [], "entities": [{"text": "CFG", "start_pos": 25, "end_pos": 28, "type": "DATASET", "confidence": 0.9660149216651917}]}, {"text": "Assign probability p to the first production (A ~ AA) and q = 1 -p to the second (A ~ a).", "labels": [], "entities": []}, {"text": "Let Sh be the total probability of all trees with depth less than or equal to h.", "labels": [], "entities": []}, {"text": "For example, $2 = q corresponding to A ~ a, and $3 = q + pq2 corresponding to {A ~ a} tO {A ~ AA, A --~ a,A --~ a}.", "labels": [], "entities": []}, {"text": "In general, Sh+l = q + pSi.", "labels": [], "entities": []}, {"text": "(Condition on the first production: with probability q the tree terminates and with probability pit produces two nonterminal symbols, each of which must now terminate with depth less than or equal to h.)", "labels": [], "entities": []}, {"text": "It is not hard to show that Sh is nondecreasing and converges to min(1, I), meaning that a proper probability is a obtained if and only if p < ~.", "labels": [], "entities": []}, {"text": "What if p is estimated from data?", "labels": [], "entities": []}, {"text": "Given a set of finite parse trees wl, w2 .....", "labels": [], "entities": []}, {"text": "w,, the maximum-likelihood estimator for p (see Section 2) is, sensibly enough, the \"relative frequency\" estimator where f(.;w) is the number of occurrences of the production \".\" in the tree w.", "labels": [], "entities": [{"text": "relative frequency\" estimator", "start_pos": 85, "end_pos": 114, "type": "METRIC", "confidence": 0.8551310300827026}]}, {"text": "The sentence am, although ambiguous (there are multiple parses when m > 2), always involves m -1 of the A ~ AA productions and m of the A ~ a productions.", "labels": [], "entities": []}, {"text": "Hence f(A ~ AA; Odi) < f(A ~ a; odi) for each wi.", "labels": [], "entities": []}, {"text": "Consequently: for each wi, and ~ < \u00bd.", "labels": [], "entities": []}, {"text": "The maximum-likelihood probability is tight.", "labels": [], "entities": []}, {"text": "If only the yields (left-to-right sequence of terminals) Y(o;1), Y(w2) .....", "labels": [], "entities": []}, {"text": "Y(wn) are available, the EM algorithm can be used to iteratively \"climb\" the likelihood surface (see Section 2).", "labels": [], "entities": []}, {"text": "In the simple example here, the estimator converges in one step and is the same ~ as if we had observed the entire parse tree for each wi.", "labels": [], "entities": []}, {"text": "Thus, ~ is again less than \u00bd and the distribution is again tight.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}