{"title": [{"text": "A Corpus-based Investigation of Definite Description Use", "labels": [], "entities": []}], "abstractContent": [{"text": "We present the results of a study of the use of definite descriptions in written texts aimed at assessing the feasibility of annotating corpora with information about definite description interpretation.", "labels": [], "entities": [{"text": "definite description interpretation", "start_pos": 167, "end_pos": 202, "type": "TASK", "confidence": 0.6942643622557322}]}, {"text": "We ran two experiments, in which subjects were asked to classi~ the uses of definite descriptions in a corpus of 33 newspaper articles, containing a total ofl,412 definite descriptions.", "labels": [], "entities": []}, {"text": "We measured the agreement among annotators about the classes assigned to definite descriptions, as well as the agreement about the antecedent assigned to those definites that the annotators classified as being related to an antecedent in the text.", "labels": [], "entities": []}, {"text": "The most interesting result of this study from a corpus annotation perspective was the rather low agreement (K = 0.63) that we obtained using versions of Hawkins's and Prince's classification schemes; better results (K = 0.76) were obtained using the simplified scheme proposed by Fraurud that includes only two classes, first-mention and subsequent-mention.", "labels": [], "entities": [{"text": "agreement", "start_pos": 98, "end_pos": 107, "type": "METRIC", "confidence": 0.9852759838104248}]}, {"text": "The agreement about antecedents was also not complete.", "labels": [], "entities": []}, {"text": "These findings raise questions concerning the strategy of evaluating systems for definite description interpretation by comparing their results with a standardized annotation.", "labels": [], "entities": [{"text": "definite description interpretation", "start_pos": 81, "end_pos": 116, "type": "TASK", "confidence": 0.7314209143320719}]}, {"text": "From a linguistic point of view, the most interesting observations were the great number of discourse-new definites in our corpus (in one of our experiments, about 50% of the definites in the collection were classified as discourse-new, 30% as anaphoric, and 18% as associative~bridging) and the presence of definites that did not seem to require a complete disambiguation.", "labels": [], "entities": []}], "introductionContent": [{"text": "The work presented in this paper was inspired by the growing realization in the field of computational linguistics of the need for experimental evaluation of linguistic theories--semantic theories, in our case.", "labels": [], "entities": []}, {"text": "The evaluation we are considering typically takes the form of experiments in which human subjects are asked to annotate texts from a corpus (or recordings of spoken conversations) according to a given classification scheme, and the agreement among their annotations is measured (see, for example, Passonneau and Litman 1993 or the papers in Moore and.", "labels": [], "entities": []}, {"text": "These attempts at evaluation are, in part, motivated by the desire to put these theories on a more \"scientific\" footing by ensuring that the semantic judgments on which they are based reflect the intuitions of a large number of speakers; 1 but experimental evaluation is also seen as a necessary precondition for the kind of system evaluation done, for example, in the Message Understanding initiative (MUC), where the performance of a system is evaluated by comparing its output on a collection of texts with a standardized annotation of those texts produced by humans (.", "labels": [], "entities": [{"text": "Message Understanding initiative (MUC)", "start_pos": 369, "end_pos": 407, "type": "TASK", "confidence": 0.6936317731936773}]}, {"text": "Clearly, to avoid dealing with deictic uses of definite descriptions and with phenomena such as reference failure and repair.", "labels": [], "entities": []}, {"text": "A second reason was that we intended to use computer simulations of the classification task to supplement the results of our experiments, and we needed a parsed corpus for this purpose; the articles we chose were all part of the Penn Treebank.", "labels": [], "entities": [{"text": "classification task", "start_pos": 72, "end_pos": 91, "type": "TASK", "confidence": 0.9027631878852844}, {"text": "Penn Treebank", "start_pos": 229, "end_pos": 242, "type": "DATASET", "confidence": 0.9917291402816772}]}, {"text": "In the remainder of the paper, we review two existing classification schemes in Section 2 and then discuss our two classification experiments in Sections 3 and 4.", "labels": [], "entities": []}], "datasetContent": [{"text": "For our first experiment evaluating subjects' performance at the classification task, we developed a taxonomy of definite description uses based on the schemes discussed in the previous section, preliminarily tested the taxonomy by annotating the corpus ourselves, and then asked two annotators to do the same task.", "labels": [], "entities": [{"text": "classification task", "start_pos": 65, "end_pos": 84, "type": "TASK", "confidence": 0.8915703892707825}]}, {"text": "This first experiment is described in the rest of this section.", "labels": [], "entities": []}, {"text": "We explain first the classification we developed for this experiment, then the experimental conditions, and, finally, discuss the results.", "labels": [], "entities": []}, {"text": "We used three subjects for Experiment 2.", "labels": [], "entities": []}, {"text": "Our subjects were English native speakers, graduate students of mathematics, geography, and mechanical engineering at the University of Edinburgh; we will refer to them as C, D, and E below.", "labels": [], "entities": []}, {"text": "They were asked to annotate 14 randomly selected Wall Street Journal articles, all but one of them dif- ferent from those used in Experiment 1, and containing 464 definite descriptions in total.", "labels": [], "entities": [{"text": "Wall Street Journal articles", "start_pos": 49, "end_pos": 77, "type": "DATASET", "confidence": 0.9561895430088043}]}, {"text": "16 Unlike in our first experiment, we did not suggest any relation between the classes and the syntactic form of the definite descriptions in the instructions.", "labels": [], "entities": []}, {"text": "The subjects were asked to indicate whether the entity referred to by a definite description (i) had been mentioned previously in the text, else if (ii) it was new but related to an entity already mentioned in the text, else (iii) it was new but presumably known to the average reader, or, finally, (iv) it was new in the text and presumably new to the average reader.", "labels": [], "entities": []}, {"text": "When the description was indicated as discourse-old (i) or related to some other entity (ii), the subjects were asked to locate the previous mention of the related entity in the text.", "labels": [], "entities": []}, {"text": "Unlike the first experiment, the subjects did not have the option of classifying a definite description as Idiom; we instructed them to make a choice and write down their doubts.", "labels": [], "entities": []}, {"text": "The written instructions and the script given to the subjects can be found in Appendix B. As in Experiment 1, the subjects were given one text to practice before starting with the analysis of the corpus.", "labels": [], "entities": []}, {"text": "They took, on average, eight hours to complete the task.", "labels": [], "entities": []}, {"text": "In order to address some of the questions raised by Experiment 1 we setup a second experiment.", "labels": [], "entities": []}, {"text": "In this second experiment we modified both the classification scheme and what we asked the annotators to do.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1  Classification by the authors of the definite descriptions in  the first corpus.", "labels": [], "entities": []}, {"text": " Table 2  Classification of definit'e descriptions according to Annotator A.", "labels": [], "entities": [{"text": "Annotator A", "start_pos": 64, "end_pos": 75, "type": "DATASET", "confidence": 0.8864881992340088}]}, {"text": " Table 3  Classification of definite descriptions according to Annotator B.", "labels": [], "entities": [{"text": "Classification of definite descriptions", "start_pos": 10, "end_pos": 49, "type": "TASK", "confidence": 0.6703703328967094}]}, {"text": " Table 4  Confusion matrix of A and B's classifications.", "labels": [], "entities": []}, {"text": " Table 5  An example of the Kappa test.", "labels": [], "entities": []}, {"text": " Table 7  Per-class agreement in Experiment 1.", "labels": [], "entities": []}, {"text": " Table 9  Per-class agreement in Experiment 2.", "labels": [], "entities": []}]}