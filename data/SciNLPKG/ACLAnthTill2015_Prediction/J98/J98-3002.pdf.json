{"title": [{"text": "Collaborative Response Generation in Planning Dialogues", "labels": [], "entities": [{"text": "Collaborative Response Generation in Planning Dialogues", "start_pos": 0, "end_pos": 55, "type": "TASK", "confidence": 0.6125096082687378}]}], "abstractContent": [{"text": "In collaborative planning dialogues, the agents have different beliefs about the domain and about each other; thus, it is inevitable that conflicts arise during the planning process.", "labels": [], "entities": []}, {"text": "In this paper , we present a plan-based model for response generation during collaborative planning, based on a recursive Propose-Evaluate-Modify framework for modeling collaboration.", "labels": [], "entities": [{"text": "response generation", "start_pos": 50, "end_pos": 69, "type": "TASK", "confidence": 0.7527460157871246}]}, {"text": "We focus on identifying strategies for content selection when 1) the system initiates information-sharing to gather further information in order to make an informed decision about whether to accept a proposal from the user, and 2) the system initiates collaborative negotiation to negotiate with the user to resolve a detected conflict in the user's proposal.", "labels": [], "entities": []}, {"text": "When our model determines that information-sharing should be pursued, it selects a focus o fin formation-sharing from among multiple uncertainties that might be addressed, chooses an appropriate information-sharing strategy, and formulates a response that initiates an information-sharing subdialogue.", "labels": [], "entities": []}, {"text": "When our model determines that conflicts must be resolved, it selects the most effective conflicts to address in resolving disagreement about the user's proposal, identifies appropriate justification for the system's claims, and formulates a response that initiates a negotiation subdialogue.", "labels": [], "entities": []}], "introductionContent": [{"text": "In task-oriented collaborative planning dialogues, two agents work together to develop a plan for achieving their shared goal.", "labels": [], "entities": []}, {"text": "Such a goal maybe for one agent to obtain a Bachelor's degree in Computer Science or for both agents to go to a mutually desirable movie.", "labels": [], "entities": []}, {"text": "Since the two agents each have private beliefs about the domain and about one another, it is inevitable that conflicts will arise between them during the planning process.", "labels": [], "entities": []}, {"text": "In order for the agents to effectively collaborate with one another, each agent must attempt to detect such conflicts as soon as they arise, and to resolve them in an efficient manner so that the agents can continue with their task.", "labels": [], "entities": []}, {"text": "Our analysis of naturally occurring collaborative planning dialogues shows that agents initiate two types of subdialogues for the purpose of resolving (potential) conflicts between the agents.", "labels": [], "entities": []}, {"text": "First, an agent may initiate information-sharing subdialogues when she does not have sufficient information to determine whether to accept or reject a proposal made by the other agent.", "labels": [], "entities": []}, {"text": "The purpose of such information-sharing subdialogues is for the two agents to share their knowledge regarding the proposal so that each agent can then knowledgeably reevaluate the proposal and come to an informed decision about its acceptance.", "labels": [], "entities": []}, {"text": "Second, an agent may initiate collaborative negotiation subdialogues when she detects a conflict between the agents with respect to a proposal.", "labels": [], "entities": []}, {"text": "The purpose of such collaborative negotiation subdialogues is for the two agents to resolve the detected conflict and agree on accepting the original proposal or perhaps some modification of it.", "labels": [], "entities": []}, {"text": "For example, consider the following dialogue segment between a travel agent (T) and a customer (C) who is making reservations for two other agents T: Can we put them on American?", "labels": [], "entities": [{"text": "American", "start_pos": 169, "end_pos": 177, "type": "DATASET", "confidence": 0.974049985408783}]}, {"text": "(2) C: Why?", "labels": [], "entities": []}, {"text": "(3) T: We're having a lot of problems on the USAir seat maps so we may not be able to get the seats they want.", "labels": [], "entities": [{"text": "T", "start_pos": 4, "end_pos": 5, "type": "METRIC", "confidence": 0.9915542006492615}, {"text": "USAir seat maps", "start_pos": 45, "end_pos": 60, "type": "DATASET", "confidence": 0.9636266827583313}]}, {"text": "(4) But American whatever we request pretty much we get.", "labels": [], "entities": []}, {"text": "(5) C: I don't know if they care about seats.", "labels": [], "entities": [{"text": "I", "start_pos": 7, "end_pos": 8, "type": "METRIC", "confidence": 0.9500068426132202}]}, {"text": "(6) Let's go with USAir.", "labels": [], "entities": [{"text": "USAir", "start_pos": 18, "end_pos": 23, "type": "DATASET", "confidence": 0.985952615737915}]}, {"text": "(7) T: Are you sure they won't mind if they don't get seats next to each other?", "labels": [], "entities": []}, {"text": "(8) C: I don't think they would care.", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to obtain an initial assessment of the quality of CORE's responses, we performed an evaluation to determine whether or not the strategies adopted by CORE are reasonable strategies that a system should employ when participating in collaborative planning dialogues and whether other options should be considered.", "labels": [], "entities": []}, {"text": "The evaluation, however, was not intended to address the completeness of the types of responses generated by CORE, nor was it intended to be a full scale evaluation such as would be provided by integrating CORE's strategies into an actual interactive advisement system.", "labels": [], "entities": []}, {"text": "The evaluation was conducted via a questionnaire in which human judges ranked CORE's responses to EA's utterances among a set of alternative responses, and also rated their level of satisfaction with each individual response.", "labels": [], "entities": [{"text": "CORE's responses to EA's utterances", "start_pos": 78, "end_pos": 113, "type": "TASK", "confidence": 0.437168687582016}]}, {"text": "The questionnaire contained a total of five dialogue segments that demonstrated CORE's ability to pursue information-sharing and to resolve detected conflicts in the agents' beliefs; other dialogue segments included in the questionnaire addressed aspects of CORE's performance that are not the topic of this paper.", "labels": [], "entities": []}, {"text": "Each dialogue segment was selected to evaluate a particular algorithm used in the response generation process.", "labels": [], "entities": [{"text": "response generation process", "start_pos": 82, "end_pos": 109, "type": "TASK", "confidence": 0.8478238185246786}]}, {"text": "For each dialogue segment, the judges were given the following information: Input to CORE: this included EA's utterances (for illustrative purposes), the beliefs that would be inferred from each of these utterances and the relationships among them.", "labels": [], "entities": [{"text": "Input", "start_pos": 76, "end_pos": 81, "type": "METRIC", "confidence": 0.9743339419364929}, {"text": "CORE", "start_pos": 85, "end_pos": 89, "type": "METRIC", "confidence": 0.5067682862281799}]}, {"text": "In effect, this is a textual description of the belief level of the dialogue model that would be inferred from EA's utterances.", "labels": [], "entities": []}, {"text": "CORE's relevant knowledge: CORE's knowledge relevant to its evaluation of each belief given in the input, along with CORE's strength of belief in each piece of knowledge.", "labels": [], "entities": []}, {"text": "Responses: for each dialogue segment, five alternative responses were given, one of which was the actual response generated by CORE (the responses were presented in random order so that the judges were not aware of which response was actually generated by the system).", "labels": [], "entities": []}, {"text": "The other four responses were obtained by altering CORE's response generation strategies.", "labels": [], "entities": [{"text": "CORE's response generation", "start_pos": 51, "end_pos": 77, "type": "TASK", "confidence": 0.650358185172081}]}, {"text": "For instance, instead of invoking our Select-Justification algorithm, an alternative response can be generated by including every piece of evidence that CORE believes will provide support for its claim.", "labels": [], "entities": []}, {"text": "Alternatively, the preference for addressing rejected evidence in Select-Focus-Modification can be altered to allow CORE to consider directly refuting a parent belief before considering refuting its rejected child beliefs.", "labels": [], "entities": []}, {"text": "Appendix A shows a sample dialogue segment in the questionnaire, annotated based on how CORE's response generation mechanism was altered to produce each of the four alternative responses.", "labels": [], "entities": [{"text": "CORE's response generation", "start_pos": 88, "end_pos": 114, "type": "TASK", "confidence": 0.6046169027686119}]}, {"text": "In evaluating alternative responses, the judges were explicitly instructed not to pay attention to the phrasing of CORE's responses, but to evaluate the responses based on their conciseness, coherence, and effectiveness, since it was the quality of the content of CORE's responses that was of interest in this  assess the level of satisfaction that a user interacting with CORE is likely to have based on CORE's responses.", "labels": [], "entities": []}, {"text": "Each alternative response was rated on a scale of very good, good, fair, poor, and terrible.", "labels": [], "entities": []}, {"text": "Ranking: the goal of this ranking was to compare our response generation strategies with other alternative strategies that might be adopted in designing a response generation system.", "labels": [], "entities": [{"text": "response generation", "start_pos": 155, "end_pos": 174, "type": "TASK", "confidence": 0.8376911580562592}]}, {"text": "The judges were asked to rank in numerical order the five responses based on their order of preference.", "labels": [], "entities": []}, {"text": "Twelve judges, all of whom were undergraduate or graduate students in computer science or linguistics, were asked to participate in this evaluation; evaluation forms were returned anonymously by 10 judges by the established deadline date.", "labels": [], "entities": []}, {"text": "Note that the judges had not been taught about the CORE system and its processing mechanisms prior to the evaluation.", "labels": [], "entities": []}, {"text": "In order to assess the judges' level of satisfaction with CORE's responses, we assigned a value of Ito 5 to each of the satisfaction ratings where I is terrible and 5 is very good.", "labels": [], "entities": [{"text": "CORE's responses", "start_pos": 58, "end_pos": 74, "type": "DATASET", "confidence": 0.7492858568827311}, {"text": "Ito", "start_pos": 99, "end_pos": 102, "type": "METRIC", "confidence": 0.9905053973197937}]}, {"text": "The mean and median of CORE's actual response in each dialogue segment were then computed, as well as the mean of all alternative responses provided for each dialogue segment, which was used as a basis for comparison.", "labels": [], "entities": []}, {"text": "Table 3(a) shows that in the two dialogue segments in which CORE initiated information-sharing (IS1 and IS2), the means of CORE's responses are both approximately one level of satisfaction higher than the average score given to all other responses (columns 1 and 3 in(a)).", "labels": [], "entities": []}, {"text": "Furthermore, in both cases the median of the score is 4, indicating that at least half of the judges considered CORE's responses to be good or very good.", "labels": [], "entities": []}, {"text": "The three dialogue segments in which CORE initiated collaborative negotiation (CN1, CN2, and CN3), however, yielded less uniform results.", "labels": [], "entities": []}, {"text": "The means of CORE's responses range from being slightly above the average score for other responses to being one satisfaction level higher.", "labels": [], "entities": []}, {"text": "However, in two out of the three responses, at least half of the judges considered CORE's responses to be either good or very good.", "labels": [], "entities": []}, {"text": "To assess the ranking of CORE's responses as compared with alternative responses, we again computed the means and medians of the rankings given to CORE's responses, as well as the mean of the rankings given to each alternative response.", "labels": [], "entities": []}, {"text": "The first column in(b) shows the mean rankings of CORE's responses.", "labels": [], "entities": []}, {"text": "This set of results is consistent with that in(a) in that the dialogue segments where CORE's responses received a higher mean satisfaction rating also received a lower mean ranking (thus indicating a higher preference).", "labels": [], "entities": []}, {"text": "The last column in(b) shows how the mean of CORE's response in a dialogue segment ranks when compared to the means of the alternative responses in the same dialogue segment.", "labels": [], "entities": []}, {"text": "The second column, on the other hand, shows the medians of the rankings for CORE's responses.", "labels": [], "entities": []}, {"text": "A comparison of these two columns indicates that they agree in all but one case.", "labels": [], "entities": []}, {"text": "The disagreement occurs in dialogue IS2; although more than half of the judges consider an alternative response better than CORE's actual response (because the median of CORE's response is 2), the judges do not agree on what this better response is (because the mean of CORE's response ranks highest among all alternatives).", "labels": [], "entities": []}, {"text": "Thus, CORE's response in IS2 can be considered the most preferred response among all judges.", "labels": [], "entities": [{"text": "CORE", "start_pos": 6, "end_pos": 10, "type": "METRIC", "confidence": 0.5543147325515747}, {"text": "IS2", "start_pos": 25, "end_pos": 28, "type": "TASK", "confidence": 0.669048547744751}]}, {"text": "Next, we examine the alternative responses that are consistently ranked higher than CORE's responses in the dialogue segments.", "labels": [], "entities": []}, {"text": "In dialogue IS1, EA proposed a main belief and provided supporting evidence for it.", "labels": [], "entities": []}, {"text": "CORE initiated information sharing using the Ask-Why strategy, focusing on an uncertain child belief.", "labels": [], "entities": [{"text": "information sharing", "start_pos": 15, "end_pos": 34, "type": "TASK", "confidence": 0.7450368106365204}]}, {"text": "The preferred alternative response also adopted the Ask-Why strateg.~ but focused on the main belief.", "labels": [], "entities": []}, {"text": "We tentatively assumed that this was because of the judges' preference for addressing the main belief directly instead of being less direct by addressing the uncertain evidence.", "labels": [], "entities": []}, {"text": "However, this assumption was shown to be invalid by the result in IS2 where the most preferred response (which is CORE's actual response) addresses an uncertain child belief.", "labels": [], "entities": []}, {"text": "A factor that further complicates the problem is the fact that EA has already proposed evidence to support the main belief in IS1; thus applying Ask-Why to the main belief would seem to be ineffective.", "labels": [], "entities": [{"text": "EA", "start_pos": 63, "end_pos": 65, "type": "DATASET", "confidence": 0.8379652500152588}]}, {"text": "To evaluate our collaborative negotiation strategies, we analyzed the responses in dialogues CN1, CN2, and CN3 that were ranked higher than CORE's actual responses.", "labels": [], "entities": []}, {"text": "We compared these preferred responses to CORE's responses based on their agreement on the outcome of the Evaluate-Belief, Select-Focus-Modification, and Select-Justification processes, as shown in.", "labels": [], "entities": []}, {"text": "For instance, the second row in the table shows that the second preferred response in dialogue CN1 (listed as CN1.2) was produced as a result of Evaluate-Belief having rejected the proposal (which is in agreement with CORE), of Select-Focus-Modification having selected a child belief as its focus (again in agreement with CORE), and of Select-Justification having selected all available evidence to present as justification (as opposed to CORE, which selected a subset of such evidence).", "labels": [], "entities": []}, {"text": "These results indicate that, in the examples we tested, all judges agreed with the \"outcome of CORE's proposal evaluation mechanism, and in all but one case, the judges agreed with the belief(s) CORE chose to refute.", "labels": [], "entities": []}, {"text": "However, disagreements arose with respect to CORE's process for selecting justification.", "labels": [], "entities": []}, {"text": "In dialogue CN1.2, the judges preferred providing all available evidence, which maybe the result of one of two assumptions.", "labels": [], "entities": [{"text": "dialogue CN1.2", "start_pos": 3, "end_pos": 17, "type": "DATASET", "confidence": 0.5648525059223175}]}, {"text": "First, the judges may believe that providing all available evidence is a better strategy in general, or second, they may have reasoned about the impact that potential pieces of evidence have on EA's beliefs and concluded that the subset of evidence that CORE selected is insufficient to convince EA of its claims.", "labels": [], "entities": []}, {"text": "In dialogue CN2, the judges preferred a response of the form B ~ A, while CORE generated a response of the form C --~ B ~ A, even though the judges were explicitly given CORE's belief that EA believes -~B.", "labels": [], "entities": [{"text": "EA believes -~B", "start_pos": 189, "end_pos": 204, "type": "METRIC", "confidence": 0.6197245121002197}]}, {"text": "This result invalidates the second assumption above, since if that assumption were true, it is very unlikely that the judges would have concluded that no further evidence for B is needed in this case.", "labels": [], "entities": [{"text": "B", "start_pos": 175, "end_pos": 176, "type": "METRIC", "confidence": 0.9813732504844666}]}, {"text": "However, the first assumption above is also invalidated because an alternative response in dialogue CN2, which enumerated all available pieces of evidence, was ranked second last.", "labels": [], "entities": [{"text": "dialogue CN2", "start_pos": 91, "end_pos": 103, "type": "DATASET", "confidence": 0.8208124339580536}]}, {"text": "This, along with the fact that in dialogue CN3, the judges preferred a response that includes a subset of the evidence selected by CORE, leads us to conclude that further research is needed to determine the reasons that led the judges to make seemingly contradictory judgments, and how these factors can be incorporated into CORE's algorithms to improve its performance.", "labels": [], "entities": [{"text": "CORE", "start_pos": 131, "end_pos": 135, "type": "DATASET", "confidence": 0.842602550983429}]}, {"text": "Although the best measure of performance would be to evaluate how our response generation strategies contribute to task success within a robust natural language advisement system, which is beyond our current capability, note that CORE's current collaborative negotiation and information-sharing strategies result in responses that most of our judges consider concise, coherent, and effective, and thus provide an excellent basis for future work.", "labels": [], "entities": []}, {"text": "In this section, we include a sample dialogue from the questionnaire given to our judges for the evaluation of CORE, discussed in Section 7.2.", "labels": [], "entities": [{"text": "CORE", "start_pos": 111, "end_pos": 115, "type": "DATASET", "confidence": 0.4556979537010193}]}, {"text": "The dialogue is annotated to indicate the primary purpose for its inclusion in the questionnaire, CORE's response in each dialogue segment, as well as how CORE's response generation strategies are modified to generate each alternative response.", "labels": [], "entities": [{"text": "CORE's response generation", "start_pos": 155, "end_pos": 181, "type": "TASK", "confidence": 0.5298736393451691}]}, {"text": "These annotations are included as comments (surrounded by/* and */) and were not available to the judges during the evaluation process.", "labels": [], "entities": []}], "tableCaptions": []}