{"title": [{"text": "Selective Sampling for Example-based Word Sense Disambiguation", "labels": [], "entities": [{"text": "Example-based Word Sense Disambiguation", "start_pos": 23, "end_pos": 62, "type": "TASK", "confidence": 0.6756212264299393}]}], "abstractContent": [{"text": "This paper proposes an efficient example sampling method for example-based word sense disam-biguation systems.", "labels": [], "entities": []}, {"text": "To construct a database of practical size, a considerable overhead for manual sense disambiguation (overhead for supervision) is required.", "labels": [], "entities": [{"text": "manual sense disambiguation", "start_pos": 71, "end_pos": 98, "type": "TASK", "confidence": 0.5740894873936971}]}, {"text": "In addition, the time complexity of searching a large-sized database poses a considerable problem (overhead for search).", "labels": [], "entities": []}, {"text": "To counter these problems, our method selectively samples a smaller-sized effective subset from a given example set for use in word sense disambiguation.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 127, "end_pos": 152, "type": "TASK", "confidence": 0.691240668296814}]}, {"text": "Our method is characterized by the reliance on the notion of training utility: the degree to which each example is informative for future example sampling when used for the training of the system.", "labels": [], "entities": []}, {"text": "The system progressively collects examples by selecting those with greatest utility.", "labels": [], "entities": []}, {"text": "The paper reports the effectiveness of our method through experiments on about one thousand sentences.", "labels": [], "entities": []}, {"text": "Compared to experiments with other example sampling methods, our method reduced both the overhead for supervision and the overhead for search, without the degeneration of the performance of the system.", "labels": [], "entities": []}], "introductionContent": [{"text": "Word sense disambiguation is a potentially crucial task in many NLP applications, such as machine translation, parsing and text retrieval (.", "labels": [], "entities": [{"text": "Word sense disambiguation", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.6738725105921427}, {"text": "machine translation", "start_pos": 90, "end_pos": 109, "type": "TASK", "confidence": 0.8091091215610504}, {"text": "parsing", "start_pos": 111, "end_pos": 118, "type": "TASK", "confidence": 0.9513795971870422}, {"text": "text retrieval", "start_pos": 123, "end_pos": 137, "type": "TASK", "confidence": 0.7330231964588165}]}, {"text": "Various corpus-based approaches to word sense disambiguation have been proposed.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 35, "end_pos": 60, "type": "TASK", "confidence": 0.7575007081031799}]}, {"text": "The use of corpus-based approaches has grown with the use of machine-readable text, because unlike conventional rule-based approaches relying on hand-crafted selectional rules (some of which are reviewed, for example, by Hirst), corpus-based approaches release us from the task of generalizing observed phenomena through a set of rules.", "labels": [], "entities": [{"text": "generalizing observed phenomena", "start_pos": 281, "end_pos": 312, "type": "TASK", "confidence": 0.881816029548645}]}, {"text": "Our verb sense disambiguation system is based on such an approach, that is, an example-based approach.", "labels": [], "entities": [{"text": "verb sense disambiguation", "start_pos": 4, "end_pos": 29, "type": "TASK", "confidence": 0.6527044177055359}]}, {"text": "A preliminary experiment showed that our system performs well when compared with systems based on other approaches, and motivated These problems suggest a different approach, namely to select a small number of optimally informative examples from given corpora.", "labels": [], "entities": []}, {"text": "Hereafter we will call these examples samples.", "labels": [], "entities": []}, {"text": "Our example sampling method, based on the utility maximization principle, decides on the preference for including a given example in the database.", "labels": [], "entities": []}, {"text": "This decision procedure is usually called selective sampling.", "labels": [], "entities": []}, {"text": "The overall control flow of selective sampling systems can be depicted as in, where \"system\" refers to our verb sense disambiguation system, and \"examples\" refers to an unsupervised example set.", "labels": [], "entities": []}, {"text": "The sampling process basically cycles between the word sense disambiguation (WSD) and training phases.", "labels": [], "entities": [{"text": "word sense disambiguation (WSD)", "start_pos": 50, "end_pos": 81, "type": "TASK", "confidence": 0.7802576422691345}]}, {"text": "During the WSD phase, the system generates an interpretation for each polysemous verb contained in the input example (\"WSD outputs\" of).", "labels": [], "entities": [{"text": "WSD phase", "start_pos": 11, "end_pos": 20, "type": "TASK", "confidence": 0.8711185157299042}]}, {"text": "This phase is equivalent to normal word sense disambiguation execution.", "labels": [], "entities": [{"text": "word sense disambiguation execution", "start_pos": 35, "end_pos": 70, "type": "TASK", "confidence": 0.7488141655921936}]}, {"text": "During the training phase, the system selects samples for training from the previously produced outputs.", "labels": [], "entities": []}, {"text": "During this phase, a human expert supervises samples, that is, provides the correct interpretation for the verbs appearing in the samples.", "labels": [], "entities": []}, {"text": "Thereafter, samples are simply incorporated into the database without any computational overhead (as would be associated with globally reestimating parameters in statistics-based systems), meaning that the system can be trained on the remaining examples (the \"residue\") for the next iteration.", "labels": [], "entities": []}, {"text": "Iterating between these two Fujii, Inui, Tokunaga, and Tanaka Selective Sampling sampling ~WSD~sD out ut~~ :(~~", "labels": [], "entities": [{"text": "WSD", "start_pos": 91, "end_pos": 94, "type": "METRIC", "confidence": 0.6425603032112122}]}], "datasetContent": [{"text": "We estimated the performance of our verb sense disambiguation method through an experiment, in which we compared the following five methods: \u2022 lower bound (LB), in which the system systematically chooses the most frequently appearing verb sense in the database (Gale, Church, and Yarowsky 1992), \u2022 rule-based method (RB), in which the system uses a thesaurus to (automatically) identify appropriate semantic classes as selectional restrictions for each verb complement, \u2022 Naive-Bayes method (NB), in which the system interprets a given verb based on the probability that it takes each verb sense, \u2022 example-based method using the vector space model (VSM), in which the system uses the above mentioned co-occurrence data extracted from the RWC text base, \u2022 example-based method using the Bunruigoihyo thesaurus (BGH), in which the system uses for the similarity computation.", "labels": [], "entities": [{"text": "verb sense disambiguation", "start_pos": 36, "end_pos": 61, "type": "TASK", "confidence": 0.6652991573015848}, {"text": "lower bound (LB)", "start_pos": 143, "end_pos": 159, "type": "METRIC", "confidence": 0.829908573627472}, {"text": "RWC text base", "start_pos": 739, "end_pos": 752, "type": "DATASET", "confidence": 0.906123697757721}]}, {"text": "In the rule-based method, selectional restrictions are represented by thesaurus classes, and allow only those nouns dominated by the given class in the thesaurus structure as verb complements.", "labels": [], "entities": []}, {"text": "In order to identify appropriate thesaurus classes, we used the association measure proposed by, which computes the information-theoretic association degree between case fillers and thesaurus classes, for each verb sense (Equation).", "labels": [], "entities": [{"text": "Equation", "start_pos": 222, "end_pos": 230, "type": "METRIC", "confidence": 0.9415032863616943}]}, {"text": "6 P(rls, c) A(s,c,r) = P(rls, c ) \u2022 log p(rlc) Fujii, Inui, Tokunaga, and Tanaka   ~n order to investigate the effectiveness of our example sampling method, we conducted an experiment in which we compared the following four sampling methods: \u2022 a control (random), in which a certain proportion of a given corpus is randomly selected for training, \u2022 uncertainty sampling (US), in which examples with minimum interpretation certainty are selected (Lewis and Gale 1994), \u2022 committee-based sampling (CBS) (Engelson and Dagan 1996), \u2022 our method based on the notion of training utility (TU).", "labels": [], "entities": []}, {"text": "We elaborate on uncertainty sampling and committee-based sampling in Section 4.2.", "labels": [], "entities": [{"text": "uncertainty sampling", "start_pos": 16, "end_pos": 36, "type": "TASK", "confidence": 0.7508963942527771}]}, {"text": "We compared these sampling methods by evaluating the relation between the number of training examples sampled and the performance of the system.", "labels": [], "entities": []}, {"text": "We conducted sixfold cross-validation and carried out sampling on the training set.", "labels": [], "entities": []}, {"text": "With regard to the training/test data set, we used the same corpus as that used for the experiment described in Section 2.3.", "labels": [], "entities": []}, {"text": "Each sampling method uses examples from IPAL to initialize the system, with the number of example case fillers for each case being an average of about 3.7.", "labels": [], "entities": []}, {"text": "For each sampling method, the system uses the Bunruigoihyo thesaurus for the similarity computation.", "labels": [], "entities": [{"text": "Bunruigoihyo thesaurus", "start_pos": 46, "end_pos": 68, "type": "DATASET", "confidence": 0.9109172224998474}]}, {"text": "In (in Section 2.3), the column of \"accuracy\" for \"BGH\" denotes the accuracy of the system with the entire set of training data contained in the database.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.9993395209312439}, {"text": "BGH", "start_pos": 51, "end_pos": 54, "type": "METRIC", "confidence": 0.8641448020935059}, {"text": "accuracy", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.9994032382965088}]}, {"text": "Each of the four sampling methods achieved this figure at the conclusion of training.", "labels": [], "entities": []}, {"text": "We evaluated each system performance according to its accuracy, that is the ratio of the number of correct outputs, compared to the total number of inputs.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.9994391798973083}]}, {"text": "For the purpose of this experiment, we set the sample size to 1 for each iteration, A = 0.5 for Equation, and k = 1 for Equation.", "labels": [], "entities": [{"text": "A", "start_pos": 84, "end_pos": 85, "type": "METRIC", "confidence": 0.9725832939147949}, {"text": "Equation", "start_pos": 96, "end_pos": 104, "type": "METRIC", "confidence": 0.9612464308738708}, {"text": "Equation", "start_pos": 120, "end_pos": 128, "type": "METRIC", "confidence": 0.8627755641937256}]}, {"text": "Based on a preliminary experiment, increasing the value of k either did not improve the performance over that fork = 1, or lowered the overall performance.", "labels": [], "entities": []}, {"text": "shows the relation between the number of training data sampled and the accuracy of the system.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 71, "end_pos": 79, "type": "METRIC", "confidence": 0.9995887875556946}]}, {"text": "In, zero on the x-axis represents the system using only the examples provided by 1PAL.", "labels": [], "entities": [{"text": "1PAL", "start_pos": 81, "end_pos": 85, "type": "DATASET", "confidence": 0.924018919467926}]}, {"text": "Looking atone can see that compared with random sampling and committee-based sampling, our sampling method reduced the number of the training data required to achieve any given accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 177, "end_pos": 185, "type": "METRIC", "confidence": 0.9892287254333496}]}, {"text": "For example, to achieve an accuracy of 80%, the number of training data required for our method was roughly one-third of that for random sampling.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9991114735603333}]}, {"text": "Although the accuracy of our method was surpassed by that of uncertainty sampling for larger sizes of training data, this minimal difference for larger data sizes is overshadowed by the considerable performance gain attained by our method for smaller data sizes.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.9995946288108826}]}, {"text": "Since IPAL has, in a sense, been manually selectively sampled in an attempt to model the maximum verb sense coverage, the performance of each method is biased by the initial contents of the database.", "labels": [], "entities": []}, {"text": "To counter this effect, we also conducted an experiment involving the construction of the database from scratch, without using examples from IPAL.", "labels": [], "entities": [{"text": "IPAL", "start_pos": 141, "end_pos": 145, "type": "DATASET", "confidence": 0.9277907609939575}]}, {"text": "During the initial phase, the system randomly selected one example for each verb sense from the training set, and a human expert provided the correct interpretation to initialize the system.", "labels": [], "entities": []}, {"text": "shows the performance of the various methods, from which the same general tendency as seen in is observable.", "labels": [], "entities": []}, {"text": "However, in this case, our method was generally superior to other methods.", "labels": [], "entities": []}, {"text": "Through these comparative experiments, we can conclude that our example sampling method is able to decrease the number of training data, i.e., the overhead for both supervision and searching, without degrading the system performance.", "labels": [], "entities": []}], "tableCaptions": []}