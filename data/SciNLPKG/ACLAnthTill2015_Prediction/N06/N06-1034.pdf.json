{"title": [{"text": "Modelling User Satisfaction and Student Learning in a Spoken Dialogue Tutoring System with Generic, Tutoring, and User Affect Parameters", "labels": [], "entities": [{"text": "User Satisfaction", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.7626871466636658}]}], "abstractContent": [{"text": "We investigate using the PARADISE framework to develop predictive models of system performance in our spoken dialogue tutoring system.", "labels": [], "entities": []}, {"text": "We represent performance with two metrics: user satisfaction and student learning.", "labels": [], "entities": []}, {"text": "We train and test predictive models of these met-rics in our tutoring system corpora.", "labels": [], "entities": []}, {"text": "We predict user satisfaction with 2 parameter types: 1) system-generic, and 2) tutoring-specific.", "labels": [], "entities": []}, {"text": "To predict student learning, we also use a third type: 3) user affect.", "labels": [], "entities": []}, {"text": "Al-hough generic parameters are useful pre-dictors of user satisfaction in other PARADISE applications, overall our parameters produce less useful user satisfaction models in our system.", "labels": [], "entities": []}, {"text": "However, generic and tutoring-specific parameters do produce useful models of student learning in our system.", "labels": [], "entities": []}, {"text": "User affect parameters can increase the usefulness of these models.", "labels": [], "entities": []}], "introductionContent": [{"text": "In recent years the development of spoken dialogue tutoring systems has become more prevalent, in an attempt to close the performance gap between human and computer tutors).", "labels": [], "entities": []}, {"text": "Student learning is a primary metric for evaluating the performance of these systems; it can be measured, e.g., by comparing student pretests taken prior to system use with posttests taken after system use.", "labels": [], "entities": []}, {"text": "In other types of spoken dialogue systems, the user's subjective judgments about using the system are often considered a primary system performance metric; e.g., user satisfaction has been measured via surveys which ask users to rate systems during use along dimensions such as task ease, speech input/output quality, user expectations and expertise, and user future use; Bonneau-Maynard et al.,;.", "labels": [], "entities": []}, {"text": "However, it is expensive to run experiments overlarge numbers of users to obtain reliable system performance measures.", "labels": [], "entities": []}, {"text": "The PARADISE model () proposes instead to predict system performance, using parameters representing interaction costs and benefits between system and user, including task success, dialogue efficiency, and dialogue quality.", "labels": [], "entities": [{"text": "PARADISE", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.7975299954414368}]}, {"text": "More formally, a set of interaction parameters are measured in a spoken dialogue system corpus, then used in a multivariate linear regression to predict the target performance variable.", "labels": [], "entities": []}, {"text": "The resulting model is described by the formula below, where there are n interaction parameters, pi , each weighted by the analysis with a coefficient, w i , which will be negative or positive, depending on whether the model treats pi as a cost or benefit, respectively.", "labels": [], "entities": []}, {"text": "The model can then be used to estimate performance during system design, with the design goals of minimizing costs and maximizing benefits.", "labels": [], "entities": []}, {"text": "System Performance = \ud97b\udf59 n i=1 w i * pi We investigate using PARADISE to develop predictive models of performance in our spoken dialogue tutoring system.", "labels": [], "entities": [{"text": "PARADISE", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.6034088730812073}]}, {"text": "Although to our knowledge,  prior PARADISE applications have only used user satisfaction to represent performance, we hypothesize that other metrics maybe more relevant when PARADISE is applied to tasks that are not optimized for user satisfaction, such as our spoken dialogue tutoring system.", "labels": [], "entities": []}, {"text": "We thus use 2 metrics to represent performance: 1) a generic metric of user satisfaction computed via user survey, 2) a tutoring-specific metric of student learning computed via student pretest and posttest scores.", "labels": [], "entities": []}, {"text": "We train and test predictive models of these metrics on multiple system corpora.", "labels": [], "entities": []}, {"text": "To predict user satisfaction, we use 2 types of interaction parameters: 1) system-generic parameters such as used in other PARADISE applications, e.g. speech recognition performance, and 2) tutoringspecific parameters, e.g. student correctness.", "labels": [], "entities": []}, {"text": "To predict student learning, we also use a third type of parameter: 3) manually annotated user affect.", "labels": [], "entities": []}, {"text": "Although prior PARADISE applications have tended to use system-generic parameters, we hypothesize that task-specific and user affect parameters may also prove useful.", "labels": [], "entities": []}, {"text": "We emphasize that user affect parameters are still system-generic; user affect has been annotated and/or automatically predicted in other types of spoken dialogue systems, e.g. as in ().", "labels": [], "entities": []}, {"text": "Our results show that, although generic parameters were useful predictors of user satisfaction in other PARADISE applications, overall our parameters produce less useful user satisfaction models in our tutoring system.", "labels": [], "entities": []}, {"text": "However, generic and tutoringspecific parameters do produce useful models of student learning in our system.", "labels": [], "entities": []}, {"text": "Generic user affect parameters increase the usefulness of these models.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Summary of our 3 ITSPOKE Corpora", "labels": [], "entities": [{"text": "ITSPOKE", "start_pos": 27, "end_pos": 34, "type": "METRIC", "confidence": 0.8379840850830078}]}, {"text": " Table 2: Testing the Predictive Power of User Satisfaction Models", "labels": [], "entities": [{"text": "User Satisfaction", "start_pos": 42, "end_pos": 59, "type": "TASK", "confidence": 0.6871493309736252}]}, {"text": " Table 3: Testing the Predictive Power of Student Learning Models with the Same Datasets", "labels": [], "entities": []}, {"text": " Table 4: Testing the Predictive Power of Student Learning Models with User Affect Parameters", "labels": [], "entities": []}]}