{"title": [{"text": "A Machine Learning based Approach to Evaluating Retrieval Systems", "labels": [], "entities": [{"text": "Evaluating Retrieval", "start_pos": 37, "end_pos": 57, "type": "TASK", "confidence": 0.8716138005256653}]}], "abstractContent": [{"text": "Test collections are essential to evaluate Information Retrieval (IR) systems.", "labels": [], "entities": [{"text": "Information Retrieval (IR)", "start_pos": 43, "end_pos": 69, "type": "TASK", "confidence": 0.8394331574440003}]}, {"text": "The relevance assessment set has been recognized as the key bottleneck in test collection building, especially on very large sized document collections.", "labels": [], "entities": [{"text": "test collection building", "start_pos": 74, "end_pos": 98, "type": "TASK", "confidence": 0.6742893854777018}]}, {"text": "This paper addresses the problem of efficiently selecting documents to be included in the assessment set.", "labels": [], "entities": []}, {"text": "We will show how machine learning techniques can fit this task.", "labels": [], "entities": []}, {"text": "This leads to smaller pools than traditional round robin pooling, thus reduces significantly the manual assessment work-load.", "labels": [], "entities": []}, {"text": "Experimental results on TREC collections 1 consistently demonstrate the effectiveness of our approach according to different evaluation criteria.", "labels": [], "entities": [{"text": "TREC collections 1", "start_pos": 24, "end_pos": 42, "type": "DATASET", "confidence": 0.8376631339391073}]}], "introductionContent": [{"text": "The effectiveness of retrieval systems is often justified by benchmark test collections.", "labels": [], "entities": []}, {"text": "A standard test collection consists of lots of documents, a set of information needs, called topics and human judgment about the relevance status of each document fora topic.", "labels": [], "entities": []}, {"text": "Nowadays, it is relatively easy to gather huge set of millions of documents and hundreds of topics.", "labels": [], "entities": []}, {"text": "The key obstacle for forming large sized test collections lies therefore in the topic assessment procedure.", "labels": [], "entities": []}, {"text": "Assessing the whole document sets is unfeasible, even for small sized collection of 800,000 documents (.", "labels": [], "entities": []}, {"text": "In order to keep the assessment process practical, one often selects a certain number of documents for judgment.", "labels": [], "entities": []}, {"text": "This is called (document) pooling and the outcome the pool or the qrels (query relevance set).", "labels": [], "entities": []}, {"text": "The collected documents are then judged by humans, documents outside the pool are assumed non relevant.", "labels": [], "entities": []}, {"text": "A representative pool is therefore essential to the whole evaluation process.", "labels": [], "entities": []}, {"text": "This paper proposes a method to form the assessment set with the support of machine learning algorithms.", "labels": [], "entities": []}, {"text": "Based on relevance judgments of relatively shallow pools, a ranking algorithm will attempt to give priority for relevant documents so that the assessment set can be fixed at a feasible size without skewing the system evaluation result.", "labels": [], "entities": []}, {"text": "The judgment process is indeed kept as much subjective-free as possible: the first relevance feeback step is designed appropriately so that the assessor cannot give any bias towards any particular rank or any system, the learning process is completely transparent to the assessors and parameters of the ranking function are collection-tailored rather than exported from previous collections.", "labels": [], "entities": []}, {"text": "The method will then be evaluated on TREC ad-hoc collections.", "labels": [], "entities": [{"text": "TREC ad-hoc collections", "start_pos": 37, "end_pos": 60, "type": "DATASET", "confidence": 0.6947278181711832}]}, {"text": "Results from our comprehensive experiment confirm that the qrels generated by our method are much more representative than those of the same size by the TREC method.", "labels": [], "entities": []}, {"text": "The outcome qrels is substantially smaller, so much cheaper to produce than the official TREC qrels, yet their conclusions about system effectiveness are quite compatible.", "labels": [], "entities": [{"text": "TREC qrels", "start_pos": 89, "end_pos": 99, "type": "DATASET", "confidence": 0.8382067680358887}]}, {"text": "The remaining of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "We review related work in Section 2.", "labels": [], "entities": []}, {"text": "Section 3 presents the general framework of applying machine learning techniques to forming test collections.", "labels": [], "entities": []}, {"text": "We also give a brief introduction about RankBoost ( and Ranking SVM, the two learning algorithms used in our experiment.", "labels": [], "entities": []}, {"text": "Section 4 introduces data sets and experimental setup.", "labels": [], "entities": []}, {"text": "Section 5 is dedicated to present experimental results according to different evaluation criteria.", "labels": [], "entities": []}, {"text": "Precisely, Section 5.1 shows the capacity of small pools on identifying relevant documents and Section 5.2 illustrates their impact on system comparison; Section 5.3 presents statistical validation tests.", "labels": [], "entities": [{"text": "system comparison", "start_pos": 135, "end_pos": 152, "type": "TASK", "confidence": 0.6701332926750183}]}, {"text": "We conclude and discuss perspectives in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our method is general enough to be applicable to any ad-hoc retrieval information task where pooling could be useful.", "labels": [], "entities": []}, {"text": "In this paper, we will however focus on TREC traditional ad-hoc retrieval collections.", "labels": [], "entities": [{"text": "TREC", "start_pos": 40, "end_pos": 44, "type": "TASK", "confidence": 0.7624314427375793}]}, {"text": "Experiments have been performed on the three corpora TREC-6, TREC-7 and TREC-8.", "labels": [], "entities": [{"text": "TREC-6", "start_pos": 53, "end_pos": 59, "type": "DATASET", "confidence": 0.8083405494689941}, {"text": "TREC-7", "start_pos": 61, "end_pos": 67, "type": "DATASET", "confidence": 0.6788060665130615}, {"text": "TREC-8", "start_pos": 72, "end_pos": 78, "type": "DATASET", "confidence": 0.835871160030365}]}, {"text": "Statistics about the number of runs, of judgments, of relevant documents are shown in Tab.: Information about three TREC ad-hoc collections.", "labels": [], "entities": [{"text": "Tab.", "start_pos": 86, "end_pos": 90, "type": "DATASET", "confidence": 0.9611429572105408}, {"text": "TREC ad-hoc collections", "start_pos": 116, "end_pos": 139, "type": "DATASET", "confidence": 0.6056231359640757}]}, {"text": "The three last columns are averaged over the topic set size (50 topics/collection).", "labels": [], "entities": []}, {"text": "Training data is gathered from the top five answers of each run.", "labels": [], "entities": []}, {"text": "The pool depth of five has been arbitrarily chosen to have both sufficient training data and to eliminate potential bias from assessors towards a particular system or towards early identified answers while judging a shallow pool.", "labels": [], "entities": []}, {"text": "Furthermore, this training data set is large enough for testing the ranking algorithm efficiency.", "labels": [], "entities": []}, {"text": "Each document is described by an N-dimensional feature vector where N is the number of participating systems.", "labels": [], "entities": []}, {"text": "The j th feature value fora document is a function of its position in the retrieved list, ties are arbitrary broken.", "labels": [], "entities": []}, {"text": "A document at rank i is assigned a feature value of (L + 1 \u2212 i) where L is the TREC limit of submission run (L is usually setup at 1000).", "labels": [], "entities": [{"text": "TREC limit", "start_pos": 79, "end_pos": 89, "type": "METRIC", "confidence": 0.9744149446487427}]}, {"text": "Documents outside submission runs receive the zero feature value (i.e. it is assumed to beat rank (L + 1)).", "labels": [], "entities": []}, {"text": "For implementation speed, the input for rSVM is further scaled down to the interval.", "labels": [], "entities": []}, {"text": "Due to the small topic set size, we use a leaveone-out training strategy: a model will be trained for each topic by using judgments of all other topics.", "labels": [], "entities": []}, {"text": "The training data set size is presented in the last column of Tab.", "labels": [], "entities": [{"text": "Tab", "start_pos": 62, "end_pos": 65, "type": "DATASET", "confidence": 0.9287817478179932}]}, {"text": "1. The workload for training dataset does not exceed the effort for assessing 5 topics in the full pool of TREC.", "labels": [], "entities": []}, {"text": "We employ SVM light package 4 for rSVM.", "labels": [], "entities": []}, {"text": "We adopt the efficient RBoost version for binary feedback and binary base functions ht (cf.), boosting is iterated 100 times and we impose positive weighting for all coefficients \u03b1 t . The non-interpolated average precision (MAP) has been chosen to measure system performance . This metric has been shown to be highly stable and reliable with both small topic set size) and very large document collections.", "labels": [], "entities": [{"text": "average precision (MAP)", "start_pos": 206, "end_pos": 229, "type": "METRIC", "confidence": 0.8436537981033325}]}, {"text": "RBoost and rSVM pools will be compared to the TREC-style pools of the same size.", "labels": [], "entities": []}, {"text": "We also include \"local MTF\") in the experiment.", "labels": [], "entities": []}, {"text": "The \"global MTF\" has been shown to slightly outperform the local version in the aforementioned paper.", "labels": [], "entities": [{"text": "MTF", "start_pos": 12, "end_pos": 15, "type": "TASK", "confidence": 0.6917097568511963}]}, {"text": "However, we believe that the global mode is merely for demonstration but unlikely practical of online judgment since it insists that all queries are judged simultaneously with a strict synchronisation among all assessors.", "labels": [], "entities": []}, {"text": "Hereafter, for simplicity, the TREC-style pool of the first n documents retrieved by each submission will be denoted by Depth-n, the equivalent pool (with the same average final pool size mover the topic set) produced by RBoost, rSVM or MTF will be RBoost-m, rSVM-m or MTF-m respectively.", "labels": [], "entities": [{"text": "MTF", "start_pos": 237, "end_pos": 240, "type": "DATASET", "confidence": 0.7410939335823059}, {"text": "MTF-m", "start_pos": 269, "end_pos": 274, "type": "DATASET", "confidence": 0.8492785692214966}]}, {"text": "In all figures in the next section, the abscissa denotes the pool size m and values of n will be present along the Depth-n curve.", "labels": [], "entities": [{"text": "abscissa", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9798250794410706}]}, {"text": "This section will examine small pools produced either by the TREC method or by RBoost/rSVM/MTF from two angles: their pooling performance and their influence on system comparison result.", "labels": [], "entities": [{"text": "RBoost/rSVM/MTF", "start_pos": 79, "end_pos": 94, "type": "DATASET", "confidence": 0.6792851209640502}]}, {"text": "shows the ratio of relevant documents retrieved by different pooling methods (i.e. the recall).", "labels": [], "entities": [{"text": "recall", "start_pos": 87, "end_pos": 93, "type": "METRIC", "confidence": 0.9980902075767517}]}, {"text": "The curves obtained by RBoost and rSVM are quite similar and much higher than that by TREC methodology.", "labels": [], "entities": [{"text": "rSVM", "start_pos": 34, "end_pos": 38, "type": "DATASET", "confidence": 0.6614173650741577}]}, {"text": "The curve of MTF is in the middle of RBoost/rSVM and Depth-n at the beginning and then catches that of RBoost at the pools of about 600 documents.", "labels": [], "entities": [{"text": "MTF", "start_pos": 13, "end_pos": 16, "type": "TASK", "confidence": 0.39771267771720886}, {"text": "rSVM", "start_pos": 44, "end_pos": 48, "type": "METRIC", "confidence": 0.4084374010562897}, {"text": "RBoost", "start_pos": 103, "end_pos": 109, "type": "DATASET", "confidence": 0.915622889995575}]}], "tableCaptions": [{"text": " Table 1: Information about three TREC ad-hoc col- lections. The three last columns are averaged over  the topic set size (50 topics/collection).", "labels": [], "entities": []}, {"text": " Table 2: Kendall's \u03c4 obtained on small qrels. D-n: TREC-style Depth-n qrels, SVM: rSVM-m; RBst:  RBoost-m.", "labels": [], "entities": [{"text": "RBst", "start_pos": 91, "end_pos": 95, "type": "METRIC", "confidence": 0.977007269859314}]}]}