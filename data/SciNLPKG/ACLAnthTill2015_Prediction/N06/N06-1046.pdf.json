{"title": [{"text": "Aggregation via Set Partitioning for Natural Language Generation", "labels": [], "entities": [{"text": "Aggregation", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.9116109013557434}, {"text": "Set Partitioning", "start_pos": 16, "end_pos": 32, "type": "TASK", "confidence": 0.7116147428750992}]}], "abstractContent": [{"text": "The role of aggregation in natural language generation is to combine two or more linguistic structures into a single sentence.", "labels": [], "entities": [{"text": "natural language generation", "start_pos": 27, "end_pos": 54, "type": "TASK", "confidence": 0.692724347114563}]}, {"text": "The task is crucial for generating concise and readable texts.", "labels": [], "entities": []}, {"text": "We present an efficient algorithm for automatically learning aggregation rules from a text and its related database.", "labels": [], "entities": []}, {"text": "The algorithm treats aggregation as a set partitioning problem and uses a global inference procedure to find an optimal solution.", "labels": [], "entities": []}, {"text": "Our experiments show that this approach yields substantial improvements over a clustering-based model which relies exclusively on local information.", "labels": [], "entities": []}], "introductionContent": [{"text": "Aggregation is an essential component of many natural language generation systems).", "labels": [], "entities": []}, {"text": "The task captures a mechanism for merging together two or more linguistic structures into a single sentence.", "labels": [], "entities": []}, {"text": "Aggregated texts tend to be more concise, coherent, and more readable overall).", "labels": [], "entities": []}, {"text": "Compare, for example, sentence (2) in and its nonaggregated counterpart in sentences (1a)-(1d).", "labels": [], "entities": []}, {"text": "The difference between the fluent aggregated sentence and its abrupt and redundant alternative is striking.", "labels": [], "entities": []}, {"text": "The benefits of aggregation go beyond making texts less stilted and repetitive.", "labels": [], "entities": []}, {"text": "Researchers in psycholinguistics have shown that by eliminating re- and the references therein).", "labels": [], "entities": []}, {"text": "Furthermore, Di demonstrate that aggregation can improve learning in the context of an intelligent tutoring application.", "labels": [], "entities": []}, {"text": "In existing generation systems, aggregation typically comprises two processes: semantic grouping and sentence structuring.", "labels": [], "entities": [{"text": "semantic grouping", "start_pos": 79, "end_pos": 96, "type": "TASK", "confidence": 0.7186095714569092}]}, {"text": "The first process involves partitioning semantic content (usually the output of a content selection component) into disjoint sets, each corresponding to a single sentence.", "labels": [], "entities": []}, {"text": "The second process is concerned with syntactic or lexical decisions that affect the realization of an aggregated sentence.", "labels": [], "entities": []}, {"text": "To date, this task has involved human analysis of a domain-relevant corpus and manual development of aggregation rules).", "labels": [], "entities": []}, {"text": "The corpus analysis and knowledge engineering work in such an approach is substantial, prohibitively so in large domains.", "labels": [], "entities": [{"text": "corpus analysis", "start_pos": 4, "end_pos": 19, "type": "TASK", "confidence": 0.7150178849697113}]}, {"text": "But since corpus data is already used in building aggregation components, an appealing alternative is to try and learn the rules of semantic grouping directly from the data.", "labels": [], "entities": []}, {"text": "Clearly, this would greatly reduce the human effort involved and ease porting generation systems to new domains.", "labels": [], "entities": []}, {"text": "In this paper, we present an automatic method for performing the semantic grouping task.", "labels": [], "entities": [{"text": "semantic grouping task", "start_pos": 65, "end_pos": 87, "type": "TASK", "confidence": 0.8436514933904012}]}, {"text": "We address the following problem: given an aligned parallel corpus of sentences and their underlying semantic representations, how can we learn grouping constraints automatically?", "labels": [], "entities": []}, {"text": "In our case the semantic content corresponds to entries from a database; however, our algorithm could be also applied to other representations such as propositions or sentence plans.", "labels": [], "entities": []}, {"text": "We formalize semantic grouping as a set partitioning problem, where each partition corresponds to a sentence.", "labels": [], "entities": [{"text": "semantic grouping", "start_pos": 13, "end_pos": 30, "type": "TASK", "confidence": 0.6998797357082367}]}, {"text": "The strength of our approach lies in its ability to capture global partitioning constraints by performing collective inference over local pairwise assignments.", "labels": [], "entities": []}, {"text": "This design allows us to integrate important constraints developed in symbolic approaches into an automatic aggregation framework.", "labels": [], "entities": []}, {"text": "At a local level, pairwise constraints capture the semantic compatibility between pairs of database entries.", "labels": [], "entities": []}, {"text": "For example, if two entries share multiple attributes, then they are likely to be aggregated.", "labels": [], "entities": []}, {"text": "Local constraints are learned using a binary classifier that considers all pairwise combinations attested in our corpus.", "labels": [], "entities": []}, {"text": "At a global level, we search fora semantic grouping that maximally agrees with the pairwise preferences while simultaneously satisfying constraints on the partitioning as a whole.", "labels": [], "entities": []}, {"text": "Global constraints, for instance, could prevent the creation of overly long sentences, and, in general, control the compression rate achieved during aggregation.", "labels": [], "entities": []}, {"text": "We encode the global inference task as an integer linear program (ILP) that can be solved using standard optimization tools.", "labels": [], "entities": []}, {"text": "We evaluate our approach in a sports domain represented by large real-world databases containing a wealth of interrelated facts.", "labels": [], "entities": []}, {"text": "Our aggregation algorithm model achieves an 11% F-score increase on grouping entry pairs over a greedy clusteringbased model which does not utilize global information for the partitioning task.", "labels": [], "entities": [{"text": "F-score", "start_pos": 48, "end_pos": 55, "type": "METRIC", "confidence": 0.9992721676826477}]}, {"text": "Furthermore, these results demonstrate that aggregation is amenable to an automatic treatment that does not require human involvement.", "labels": [], "entities": []}, {"text": "In the following section, we provide an overview of existing work on aggregation.", "labels": [], "entities": []}, {"text": "Then, we define the learning task and introduce our approach to content grouping.", "labels": [], "entities": [{"text": "content grouping", "start_pos": 64, "end_pos": 80, "type": "TASK", "confidence": 0.7008313685655594}]}, {"text": "Next, we present our experimental framework and data.", "labels": [], "entities": []}, {"text": "We conclude the paper by presenting and discussing our results.", "labels": [], "entities": []}], "datasetContent": [{"text": "The model presented in the previous section was evaluated in the context of generating summary reports for American football games.", "labels": [], "entities": []}, {"text": "In this section we describe the corpus used in our experiments, our procedure for estimating the parameters of our models, and the baseline method used for comparison with our approach.", "labels": [], "entities": []}, {"text": "Data For training and testing our algorithm, we employed a corpus of football game summaries collected by.", "labels": [], "entities": []}, {"text": "The corpus contains 468 game summaries from the official site of the American National Football League 6 (NFL).", "labels": [], "entities": [{"text": "American National Football League 6 (NFL)", "start_pos": 69, "end_pos": 110, "type": "DATASET", "confidence": 0.7851080484688282}]}, {"text": "Each summary has an associated database containing statistics about individual players and events.", "labels": [], "entities": []}, {"text": "In total, the corpus contains 73,400 database entries, 7.1% of which are verbalized; each entry is characterized by a type and a set of attributes (see).", "labels": [], "entities": []}, {"text": "Database entries are automatically aligned with their corresponding sentences in the game summaries by a procedure that considers anchor overlap between entity attributes and sentence tokens.", "labels": [], "entities": []}, {"text": "Although the alignment procedure is relatively accurate, there is unavoidably some noise in the data.", "labels": [], "entities": [{"text": "alignment", "start_pos": 13, "end_pos": 22, "type": "TASK", "confidence": 0.967351496219635}]}, {"text": "The distribution of database entries per sentence is shown in.", "labels": [], "entities": []}, {"text": "As can be seen, most aggregated sentences correspond to two or three database entries.", "labels": [], "entities": []}, {"text": "Each game summary contained 14.3 entries and 9.1 sentences on average.", "labels": [], "entities": []}, {"text": "The training and test data were generated as described in Section 4.1.", "labels": [], "entities": []}, {"text": "We used 96,434 instances (300 summaries) for training, 59,082 instances (68 summaries) for testing, and 53,776 instances (100 summaries) for development purposes.", "labels": [], "entities": []}, {"text": "Parameter Estimation As explained in Section 4, we infer a partitioning over a set of database entries in a two-stage process.", "labels": [], "entities": [{"text": "Parameter Estimation", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.7663563191890717}]}, {"text": "We first determine how likely all entry pairs are to be aggregated using a local classifier, and then infer a valid global partitioning for all entries.", "labels": [], "entities": []}, {"text": "The set of shared attributes A consists of five features that capture overlap in players, time (measured by game quarters), action type, outcome type, and number of yards.", "labels": [], "entities": []}, {"text": "The maximum cardinality of the set of conjunctive features is five.", "labels": [], "entities": []}, {"text": "See http://www.nfl.com/scores.", "labels": [], "entities": []}, {"text": "Overall, our local classifier used 28 features, including 23 conjunctive ones.", "labels": [], "entities": []}, {"text": "The maximum entropy classifier was trained for 100 iterations.", "labels": [], "entities": []}, {"text": "The global constraints for our ILP models are parametrized (see equations and) by m and k which are estimated separately for every test document.", "labels": [], "entities": []}, {"text": "The values of m ranged from 2 to 130 and fork from 2 to 9.", "labels": [], "entities": [{"text": "fork", "start_pos": 41, "end_pos": 45, "type": "METRIC", "confidence": 0.9910287261009216}]}, {"text": "Baseline Clustering is a natural baseline model for our partitioning problem.", "labels": [], "entities": []}, {"text": "In our experiments, we a employ a single-link agglomerative clustering algorithm that uses the scores returned by the maximum entropy classifier as a pairwise distance measure.", "labels": [], "entities": []}, {"text": "Initially, the algorithm creates a separate cluster for each sentence.", "labels": [], "entities": []}, {"text": "During each iteration, the two closest clusters are merged.", "labels": [], "entities": []}, {"text": "Again, we do not know in advance the appropriate number of clusters fora given document.", "labels": [], "entities": []}, {"text": "This number is estimated from the training data by averaging the number of sentences in documents of the same size.", "labels": [], "entities": []}, {"text": "We evaluate the performance of the ILP and clustering models by measuring F-score over pairwise label assignments.", "labels": [], "entities": [{"text": "F-score", "start_pos": 74, "end_pos": 81, "type": "METRIC", "confidence": 0.9918544292449951}]}, {"text": "We compute F-score individually for each document and report the average.", "labels": [], "entities": [{"text": "F-score", "start_pos": 11, "end_pos": 18, "type": "METRIC", "confidence": 0.9960136413574219}]}, {"text": "In addition, we compute partition accuracy in order to determine how many sentencelevel aggregations our model predicts correctly.: Results on pairwise label assignment (precision, recall, and F-score are averaged over documents); comparison between clustering and ILP models", "labels": [], "entities": [{"text": "accuracy", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9265702962875366}, {"text": "precision", "start_pos": 170, "end_pos": 179, "type": "METRIC", "confidence": 0.9989872574806213}, {"text": "recall", "start_pos": 181, "end_pos": 187, "type": "METRIC", "confidence": 0.9935018420219421}, {"text": "F-score", "start_pos": 193, "end_pos": 200, "type": "METRIC", "confidence": 0.995894193649292}]}], "tableCaptions": [{"text": " Table 2: Excerpt of database and (simplified) example of aggregated entries taken from a football domain.  This fragment will give rise to 6 sentences in the final text.", "labels": [], "entities": []}, {"text": " Table 3: Results on pairwise label assignment (pre- cision, recall, and F-score are averaged over doc- uments); comparison between clustering and ILP  models", "labels": [], "entities": [{"text": "recall", "start_pos": 61, "end_pos": 67, "type": "METRIC", "confidence": 0.9960765242576599}, {"text": "F-score", "start_pos": 73, "end_pos": 80, "type": "METRIC", "confidence": 0.9896802306175232}]}]}