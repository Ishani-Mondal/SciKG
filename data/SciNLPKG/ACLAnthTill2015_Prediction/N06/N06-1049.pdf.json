{"title": [{"text": "Will Pyramids Built of Nuggets Topple Over?", "labels": [], "entities": []}], "abstractContent": [{"text": "The present methodology for evaluating complex questions at TREC analyzes answers in terms of facts called \"nuggets\".", "labels": [], "entities": []}, {"text": "The official F-score metric represents the harmonic mean between recall and precision at the nugget level.", "labels": [], "entities": [{"text": "F-score metric", "start_pos": 13, "end_pos": 27, "type": "METRIC", "confidence": 0.9696970880031586}, {"text": "recall", "start_pos": 65, "end_pos": 71, "type": "METRIC", "confidence": 0.9927623867988586}, {"text": "precision", "start_pos": 76, "end_pos": 85, "type": "METRIC", "confidence": 0.9822299480438232}]}, {"text": "There is an implicit assumption that some facts are more important than others, which is implemented in a binary split between \"vi-tal\" and \"okay\" nuggets.", "labels": [], "entities": []}, {"text": "This distinction holds important implications for the TREC scoring model-essentially, systems only receive credit for retrieving vital nuggets-and is a source of evaluation instability.", "labels": [], "entities": [{"text": "TREC scoring", "start_pos": 54, "end_pos": 66, "type": "TASK", "confidence": 0.6749378144741058}]}, {"text": "The upshot is that for many questions in the TREC testsets, the median score across all submitted runs is zero.", "labels": [], "entities": [{"text": "TREC testsets", "start_pos": 45, "end_pos": 58, "type": "DATASET", "confidence": 0.8250174820423126}]}, {"text": "In this work, we introduce a scoring model based on judgments from multiple assessors that captures a more refined notion of nugget importance.", "labels": [], "entities": []}, {"text": "We demonstrate on TREC 2003, 2004, and 2005 data that our \"nugget pyramids\" address many shortcomings of the present methodology, while introducing only minimal additional overhead on the evaluation flow.", "labels": [], "entities": [{"text": "TREC 2003, 2004, and 2005 data", "start_pos": 18, "end_pos": 48, "type": "DATASET", "confidence": 0.7443209290504456}]}], "introductionContent": [{"text": "The field of question answering has been moving away from simple \"factoid\" questions such as \"Who invented the paper clip?\" to more complex information needs such as \"Who is Aaron Copland?\" and \"How have South American drug cartels been using banks in Liechtenstein to launder money?\", which cannot be answered by simple named-entities.", "labels": [], "entities": [{"text": "question answering", "start_pos": 13, "end_pos": 31, "type": "TASK", "confidence": 0.8478620052337646}]}, {"text": "Over the past few years, NIST through the TREC QA tracks has implemented an evaluation methodology based on the notion of \"information nuggets\" to assess the quality of answers to such complex questions.", "labels": [], "entities": [{"text": "NIST", "start_pos": 25, "end_pos": 29, "type": "DATASET", "confidence": 0.8652475476264954}, {"text": "TREC QA tracks", "start_pos": 42, "end_pos": 56, "type": "DATASET", "confidence": 0.8971737225850424}]}, {"text": "This paradigm has gained widespread acceptance in the research community, and is currently being applied to evaluate answers to so-called \"definition\", \"relationship\", and \"opinion\" questions.", "labels": [], "entities": []}, {"text": "Since quantitative evaluation is arguably the single biggest driver of advances in language technologies, it is important to closely examine the characteristics of a scoring model to ensure its fairness, reliability, and stability.", "labels": [], "entities": [{"text": "quantitative evaluation", "start_pos": 6, "end_pos": 29, "type": "TASK", "confidence": 0.6855810284614563}]}, {"text": "In this work, we identify a potential source of instability in the nugget evaluation paradigm, develop anew scoring method, and demonstrate that our new model addresses some of the shortcomings of the original method.", "labels": [], "entities": [{"text": "nugget evaluation paradigm", "start_pos": 67, "end_pos": 93, "type": "TASK", "confidence": 0.7930601437886556}]}, {"text": "It is our hope that this more-refined evaluation model can better guide the development of technology for answering complex questions.", "labels": [], "entities": []}, {"text": "This paper is organized as follows: Section 2 provides a brief overview of the nugget evaluation methodology.", "labels": [], "entities": [{"text": "nugget evaluation", "start_pos": 79, "end_pos": 96, "type": "TASK", "confidence": 0.8732779026031494}]}, {"text": "Section 3 draws attention to the vital/okay nugget distinction and the problems it creates.", "labels": [], "entities": []}, {"text": "Section 4 outlines our proposal for building \"nugget pyramids\", a more-refined model of nugget importance that combines judgments from multiple assessors.", "labels": [], "entities": [{"text": "nugget importance", "start_pos": 88, "end_pos": 105, "type": "TASK", "confidence": 0.7315224707126617}]}, {"text": "Section 5 describes the methodology for evaluating this new model, and Section 6 presents our results.", "labels": [], "entities": []}, {"text": "A discussion of related issues appears in Section 7, and the paper concludes with Section 8.", "labels": [], "entities": []}], "datasetContent": [{"text": "To date, NIST has conducted three large-scale evaluations of complex questions using a nugget-based evaluation methodology: \"definition\" questions in.", "labels": [], "entities": [{"text": "NIST", "start_pos": 9, "end_pos": 13, "type": "DATASET", "confidence": 0.876671552658081}]}, {"text": "Since relatively few teams participated in the 2005 evaluation of \"relationship\" questions, this work focuses on the three years' worth of \"definition/other\" questions.", "labels": [], "entities": []}, {"text": "The nugget-based paradigm has been previously detailed in a number of papers); here, we present only a short summary.", "labels": [], "entities": []}, {"text": "System responses to complex questions consist of an unordered set of passages.", "labels": [], "entities": []}, {"text": "To evaluate answers, NIST pools answer strings from all participants, removes their association with the runs that produced them, and presents them to a human assessor.", "labels": [], "entities": []}, {"text": "Using these responses and research performed during the original development of the question, the assessor creates an \"answer key\" comprised of a list of \"nuggets\"-essentially, facts about the target.", "labels": [], "entities": []}, {"text": "According to TREC guidelines, a nugget is defined as a fact for which the assessor could make a binary decision as to whether a response contained that nugget.", "labels": [], "entities": []}, {"text": "As an example, relevant nuggets for the target \"AARP\" are shown in.", "labels": [], "entities": [{"text": "AARP", "start_pos": 48, "end_pos": 52, "type": "METRIC", "confidence": 0.7492344975471497}]}, {"text": "In addition to creating the nuggets, the assessor also manually classifies each as either \"vital\" or \"okay\".", "labels": [], "entities": []}, {"text": "Vital nuggets represent concepts that must be in a \"good\" definition; on the other hand, okay nuggets contribute worthwhile information about the target but are not essential.", "labels": [], "entities": []}, {"text": "The distinction has important implications, described below.", "labels": [], "entities": []}, {"text": "Once the answer key of vital/okay nuggets is created, the assessor goes back and manually scores each run.", "labels": [], "entities": []}, {"text": "For each system response, he or she decides whether or not each nugget is present.", "labels": [], "entities": []}, {"text": "The final F-score for an answer is computed in the manner described in, and the final score of a system run is the mean of scores across all questions.", "labels": [], "entities": [{"text": "F-score", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9756307601928711}]}, {"text": "The per-question F-score is a harmonic mean between nugget precision and nugget recall, where recall is heavily favored (controlled by the \u03b2 parameter, set to five in 2003 and three in    (which means no credit is given for returning okay nuggets), while nugget precision is approximated by a length allowance based on the number of both vital and okay nuggets returned.", "labels": [], "entities": [{"text": "F-score", "start_pos": 17, "end_pos": 24, "type": "METRIC", "confidence": 0.9803243279457092}, {"text": "precision", "start_pos": 59, "end_pos": 68, "type": "METRIC", "confidence": 0.8940120339393616}, {"text": "recall", "start_pos": 80, "end_pos": 86, "type": "METRIC", "confidence": 0.9591329097747803}, {"text": "recall", "start_pos": 94, "end_pos": 100, "type": "METRIC", "confidence": 0.9985499978065491}, {"text": "precision", "start_pos": 262, "end_pos": 271, "type": "METRIC", "confidence": 0.918452799320221}]}, {"text": "Early in a pilot study, researchers discovered that it was impossible for assessors to enumerate the total set of nuggets contained in a system response, which corresponds to the denominator in the precision calculation.", "labels": [], "entities": [{"text": "precision", "start_pos": 198, "end_pos": 207, "type": "METRIC", "confidence": 0.9973258972167969}]}, {"text": "Thus, a penalty for verbosity serves as a surrogate for precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 56, "end_pos": 65, "type": "METRIC", "confidence": 0.9923266172409058}]}, {"text": "Note that while a question's answer key only needs to be created once, assessors must manually determine if each nugget is present in a system's response.", "labels": [], "entities": []}, {"text": "This human involvement has been identified as a bottleneck in the evaluation process, although we have recently developed an automatic scoring metric called POURPRE that correlates well with human judgments).", "labels": [], "entities": [{"text": "POURPRE", "start_pos": 157, "end_pos": 164, "type": "METRIC", "confidence": 0.9851716160774231}]}, {"text": "Previously, we have argued that the vital/okay distinction is a source of instability in the nuggetbased evaluation methodology, especially given the manner in which F-score is calculated).", "labels": [], "entities": [{"text": "F-score", "start_pos": 166, "end_pos": 173, "type": "METRIC", "confidence": 0.9917077422142029}]}, {"text": "Since only vital nuggets figure into the calculation of nugget recall, there is a large \"quantization effect\" for system scores on topics that have few vital nuggets.", "labels": [], "entities": [{"text": "recall", "start_pos": 63, "end_pos": 69, "type": "METRIC", "confidence": 0.9535226821899414}]}, {"text": "For example, on a question that has only one vital nugget, a system cannot obtain a non-zero score unless that vital nugget is retrieved.", "labels": [], "entities": []}, {"text": "In reality, whether or not a system returned a passage containing that single vital nugget is often a matter of luck, which is compounded by assessor judgment errors.", "labels": [], "entities": []}, {"text": "Furthermore, there does not appear to be any reliable indicators for predicting the importance of a nugget, which makes the task of developing systems even more challenging.", "labels": [], "entities": []}, {"text": "The polarizing effect of the vital/okay distinction brings into question the stability of TREC evaluations.", "labels": [], "entities": [{"text": "TREC evaluations", "start_pos": 90, "end_pos": 106, "type": "TASK", "confidence": 0.8155501484870911}]}, {"text": "shows statistics about the number of questions that have only one or two vital nuggets.", "labels": [], "entities": []}, {"text": "Compared to the size of the testset, these numbers are relatively large.", "labels": [], "entities": []}, {"text": "As a concrete example, \"F16\" is the target for question 71.7 from TREC 2005.", "labels": [], "entities": [{"text": "F16", "start_pos": 24, "end_pos": 27, "type": "METRIC", "confidence": 0.9913323521614075}, {"text": "TREC 2005", "start_pos": 66, "end_pos": 75, "type": "DATASET", "confidence": 0.7607008218765259}]}, {"text": "The only vital nugget is \"First F16s builtin 1974\".", "labels": [], "entities": []}, {"text": "The practical effect of the vital/okay distinction in its current form is the number of questions for which the median system score across all submitted runs is zero: 22 in.", "labels": [], "entities": []}, {"text": "An evaluation in which the median score for many questions is zero has many shortcomings.", "labels": [], "entities": []}, {"text": "For one, it is difficult to tell if a particular run is \"better\" than another-even though they maybe very different in other salient properties such as length, for example.", "labels": [], "entities": [{"text": "length", "start_pos": 152, "end_pos": 158, "type": "METRIC", "confidence": 0.9841023087501526}]}, {"text": "The discriminative power of the present F-score measure is called into question: are present systems that bad, or is the current scoring model insufficient to discriminate between different (poorly performing) systems?", "labels": [], "entities": [{"text": "F-score measure", "start_pos": 40, "end_pos": 55, "type": "METRIC", "confidence": 0.9174584150314331}]}, {"text": "Also, as pointed out by, a score distribution heavily skewed towards zero makes meta-analysis of evaluation stability hard to perform.", "labels": [], "entities": []}, {"text": "Since such studies depend on variability in scores, evaluations would appear more stable than they really are.", "labels": [], "entities": []}, {"text": "While there are obviously shortcomings to the current scheme of labeling nuggets as either \"vital\" or \"okay\", the distinction does start to capture the intuition that \"not all nuggets are created equal\".", "labels": [], "entities": []}, {"text": "Some nuggets are inherently more important than others, and this should be reflected in the evaluation methodology.", "labels": [], "entities": []}, {"text": "The solution, we believe, is to solicit judgments from multiple assessors and develop a more refined sense of nugget importance.", "labels": [], "entities": []}, {"text": "However, given finite resources, it is important to balance the amount of additional manual effort required with the gains derived from those efforts.", "labels": [], "entities": []}, {"text": "We present the idea of building \"nugget pyramids\", which addresses the shortcomings noted here, and then assess the implications of this new scoring model against data from  We evaluate our methodology for building \"nugget pyramids\" using runs submitted to the, and 2005 question answering tracks.", "labels": [], "entities": []}, {"text": "There were 50 questions in the 2003 testset, 64 in 2004, and 75 in 2005.", "labels": [], "entities": []}, {"text": "In total, there were 54 runs submitted to.", "labels": [], "entities": []}, {"text": "NIST assessors have manually annotated nuggets found in a given system's response, and this allows us to calculate the final Fscore under different scoring models.", "labels": [], "entities": [{"text": "NIST assessors", "start_pos": 0, "end_pos": 14, "type": "DATASET", "confidence": 0.9246328175067902}, {"text": "Fscore", "start_pos": 125, "end_pos": 131, "type": "METRIC", "confidence": 0.9401914477348328}]}, {"text": "We recruited a total of nine different assessors for this study.", "labels": [], "entities": []}, {"text": "Assessors consisted of graduate students in library and information science and computer science at the University of Maryland as well as volunteers from the question answering community (obtained via a posting to NIST's TREC QA mailing list).", "labels": [], "entities": [{"text": "question answering community", "start_pos": 158, "end_pos": 186, "type": "TASK", "confidence": 0.8361533880233765}, {"text": "NIST's TREC QA mailing list", "start_pos": 214, "end_pos": 241, "type": "DATASET", "confidence": 0.8509699006875356}]}, {"text": "Each assessor was given the reference nuggets along with the original questions and asked to classify each nugget as vital or okay.", "labels": [], "entities": []}, {"text": "They were purposely asked to make these judgments without reference to documents in the corpus in order to expedite the assessment process-our goal is to propose a refinement to the current nugget evaluation methodology that addresses shortcomings while minimizing the amount of additional effort required.", "labels": [], "entities": []}, {"text": "Combined with the answer key created by the original NIST assessors, we obtained a total often judgments for every single nugget in the three testsets.: Kendall's \u03c4 correlation between system scores generated using \"official\" vital/okay judgments and each assessor's judgments.", "labels": [], "entities": [{"text": "NIST assessors", "start_pos": 53, "end_pos": 67, "type": "DATASET", "confidence": 0.9432550370693207}]}, {"text": "(Assessor 0 represents the original NIST assessors.)", "labels": [], "entities": [{"text": "NIST assessors", "start_pos": 36, "end_pos": 50, "type": "DATASET", "confidence": 0.9345276653766632}]}], "tableCaptions": [{"text": " Table 2: Number of questions with few vital nuggets  in the different testsets.", "labels": [], "entities": [{"text": "Number", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9403824210166931}]}, {"text": " Table 3: Kendall's \u03c4 correlation between system scores generated using \"official\" vital/okay judgments and  each assessor's judgments. (Assessor 0 represents the original NIST assessors.)", "labels": [], "entities": [{"text": "Kendall's \u03c4 correlation", "start_pos": 10, "end_pos": 33, "type": "METRIC", "confidence": 0.5787579715251923}, {"text": "NIST assessors", "start_pos": 172, "end_pos": 186, "type": "DATASET", "confidence": 0.9064474999904633}]}, {"text": " Table 4: Kendall's \u03c4 correlation between system  rankings generated using the ten-assessor nugget  pyramid and those generated using each individual  assessor's judgments. (Assessor 0 represents the  original NIST assessors.)", "labels": [], "entities": [{"text": "Kendall's \u03c4 correlation", "start_pos": 10, "end_pos": 33, "type": "METRIC", "confidence": 0.5713498219847679}, {"text": "NIST assessors", "start_pos": 210, "end_pos": 224, "type": "DATASET", "confidence": 0.9127860069274902}]}]}