{"title": [], "abstractContent": [{"text": "We present an unsupervised approach to symmetric word alignment in which two simple asymmetric models are trained jointly to maximize a combination of data likelihood and agreement between the models.", "labels": [], "entities": [{"text": "symmetric word alignment", "start_pos": 39, "end_pos": 63, "type": "TASK", "confidence": 0.6342000861962637}]}, {"text": "Compared to the standard practice of intersecting predictions of independently-trained models, joint training provides a 32% reduction in AER.", "labels": [], "entities": [{"text": "AER", "start_pos": 138, "end_pos": 141, "type": "METRIC", "confidence": 0.9983786344528198}]}, {"text": "Moreover, a simple and efficient pair of HMM aligners provides a 29% reduction in AER over symmetrized IBM model 4 predictions.", "labels": [], "entities": [{"text": "AER", "start_pos": 82, "end_pos": 85, "type": "METRIC", "confidence": 0.9994107484817505}]}], "introductionContent": [{"text": "Word alignment is an important component of a complete statistical machine translation pipeline (.", "labels": [], "entities": [{"text": "Word alignment", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7357750833034515}, {"text": "statistical machine translation pipeline", "start_pos": 55, "end_pos": 95, "type": "TASK", "confidence": 0.7052310407161713}]}, {"text": "The classic approaches to unsupervised word alignment are based on IBM models 1-5 () and the HMM model (see fora systematic comparison).", "labels": [], "entities": [{"text": "word alignment", "start_pos": 39, "end_pos": 53, "type": "TASK", "confidence": 0.7101959884166718}]}, {"text": "One can classify these six models into two groups: sequence-based models (models 1, 2, and HMM) and fertility-based models (models 3, 4, and 5).", "labels": [], "entities": []}, {"text": "Whereas the sequence-based models are tractable and easily implemented, the more accurate fertility-based models are intractable and thus require approximation methods which are difficult to implement.", "labels": [], "entities": []}, {"text": "As a result, many practitioners use the complex GIZA++ software package) as a black box, selecting model 4 as a good compromise between alignment quality and efficiency.", "labels": [], "entities": [{"text": "GIZA++ software package", "start_pos": 48, "end_pos": 71, "type": "DATASET", "confidence": 0.8576686978340149}]}, {"text": "Even though the fertility-based models are more accurate, there are several reasons to consider avenues for improvement based on the simpler and faster sequence-based models.", "labels": [], "entities": []}, {"text": "First, even with the highly optimized implementations in GIZA++, models 3 and above are still very slow to train.", "labels": [], "entities": []}, {"text": "Second, we seem to have hit a point of diminishing returns with extensions to the fertility-based models.", "labels": [], "entities": []}, {"text": "For example, gains from the new model 6 of are modest.", "labels": [], "entities": []}, {"text": "When models are too complex to reimplement, the barrier to improvement is raised even higher.", "labels": [], "entities": []}, {"text": "Finally, the fertility-based models are asymmetric, and symmetrization is commonly employed to improve alignment quality by intersecting alignments induced in each translation direction.", "labels": [], "entities": []}, {"text": "It is therefore natural to explore models which are designed from the start with symmetry in mind.", "labels": [], "entities": []}, {"text": "In this paper, we introduce anew method for word alignment that addresses the three issues above.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 44, "end_pos": 58, "type": "TASK", "confidence": 0.8165068328380585}]}, {"text": "Our development is motivated by the observation that intersecting the predictions of two directional models outperforms each model alone.", "labels": [], "entities": []}, {"text": "Viewing intersection as away of finding predictions that both models agree on, we take the agreement idea one step further.", "labels": [], "entities": []}, {"text": "The central idea of our approach is to not only make the predictions of the models agree attest time, but also encourage agreement during training.", "labels": [], "entities": []}, {"text": "We define an intuitive objective function which incor-porates both data likelihood and a measure of agreement between models.", "labels": [], "entities": []}, {"text": "Then we derive an EM-like algorithm to maximize this objective function.", "labels": [], "entities": []}, {"text": "Because the E-step is intractable in our case, we use a heuristic approximation which nonetheless works well in practice.", "labels": [], "entities": []}, {"text": "By jointly training two simple HMM models, we obtain 4.9% AER on the standard English-French Hansards task.", "labels": [], "entities": [{"text": "AER", "start_pos": 58, "end_pos": 61, "type": "METRIC", "confidence": 0.9997618794441223}]}, {"text": "To our knowledge, this is the lowest published unsupervised AER result, and it is competitive with supervised approaches.", "labels": [], "entities": [{"text": "AER", "start_pos": 60, "end_pos": 63, "type": "METRIC", "confidence": 0.8516614437103271}]}, {"text": "Furthermore, our approach is very practical: it is no harder to implement than a standard HMM model, and joint training is no slower than the standard training of two HMM models.", "labels": [], "entities": []}, {"text": "Finally, we show that word alignments from our system can be used in a phrasebased translation system to modestly improve BLEU score.", "labels": [], "entities": [{"text": "phrasebased translation", "start_pos": 71, "end_pos": 94, "type": "TASK", "confidence": 0.7682068347930908}, {"text": "BLEU score", "start_pos": 122, "end_pos": 132, "type": "METRIC", "confidence": 0.9798054397106171}]}], "datasetContent": [{"text": "We tested our approach on the English-French Hansards data from the NAACL 2003 Shared Task, which includes a training set of 1.1 million sentences, a validation set of 37 sentences, and a test set of 447 sentences.", "labels": [], "entities": [{"text": "Hansards data from the NAACL 2003 Shared Task", "start_pos": 45, "end_pos": 90, "type": "DATASET", "confidence": 0.8666487708687782}]}, {"text": "The validation and test sentences have been hand-aligned (see) and are marked with both sure and possible alignments.", "labels": [], "entities": []}, {"text": "Using these alignments, alignment error rate (AER) is calculated as: where A is a set of proposed edges, S is the sure gold edges, and P is the possible gold edges.", "labels": [], "entities": [{"text": "alignment error rate (AER)", "start_pos": 24, "end_pos": 50, "type": "METRIC", "confidence": 0.9217796127001444}]}, {"text": "As a preprocessing step, we lowercased all words.", "labels": [], "entities": []}, {"text": "Then we used the validation set and the first 100 sentences of the test set as our development set to tune our models.", "labels": [], "entities": []}, {"text": "Lastly, we ran our models on the last 347 sentences of the test set to get final AER results.", "labels": [], "entities": [{"text": "AER", "start_pos": 81, "end_pos": 84, "type": "METRIC", "confidence": 0.9367570877075195}]}, {"text": "To see whether our improvement in AER also improves BLEU score, we aligned 100K EnglishFrench sentences from the Europarl corpus and tested on 3000 sentences of length 5-15.", "labels": [], "entities": [{"text": "AER", "start_pos": 34, "end_pos": 37, "type": "METRIC", "confidence": 0.9986289739608765}, {"text": "BLEU score", "start_pos": 52, "end_pos": 62, "type": "METRIC", "confidence": 0.9776073396205902}, {"text": "Europarl corpus", "start_pos": 113, "end_pos": 128, "type": "DATASET", "confidence": 0.9484491348266602}]}, {"text": "Using GIZA++ model 4 alignments and Pharaoh (, we achieved a BLEU score of 0.3035.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 61, "end_pos": 71, "type": "METRIC", "confidence": 0.982426643371582}]}, {"text": "By using alignments from our jointly trained HMMs instead, we get a BLEU score of 0.3051.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 68, "end_pos": 78, "type": "METRIC", "confidence": 0.9848705530166626}]}, {"text": "While this improvement is very modest, we are currently investigating alternative ways of interfacing with phrase table construction to make a larger impact on translation quality.", "labels": [], "entities": [{"text": "phrase table construction", "start_pos": 107, "end_pos": 132, "type": "TASK", "confidence": 0.6995975772539774}]}], "tableCaptions": [{"text": " Table 1: Comparison of AER between independent  and joint training across different size training sets  and different models, evaluated on the development  set. The last column shows the relative reduction in  AER.", "labels": [], "entities": [{"text": "AER", "start_pos": 24, "end_pos": 27, "type": "METRIC", "confidence": 0.9532465934753418}, {"text": "AER", "start_pos": 211, "end_pos": 214, "type": "METRIC", "confidence": 0.9958053827285767}]}, {"text": " Table 2: Comparison of test set AER between vari- ous models trained on the full 1.1 million sentences.", "labels": [], "entities": [{"text": "AER", "start_pos": 33, "end_pos": 36, "type": "METRIC", "confidence": 0.9925287961959839}]}, {"text": " Table 3: Contributions of using joint training versus  independent training and posterior decoding (with  the optimal threshold) instead of Viterbi decoding,  evaluated on the development set.", "labels": [], "entities": []}]}