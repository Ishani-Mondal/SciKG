{"title": [{"text": "Nuggeteer: Automatic Nugget-Based Evaluation using Descriptions and Judgements", "labels": [], "entities": []}], "abstractContent": [{"text": "The TREC Definition and Relationship questions are evaluated on the basis of information nuggets that maybe contained in system responses.", "labels": [], "entities": [{"text": "TREC Definition", "start_pos": 4, "end_pos": 19, "type": "TASK", "confidence": 0.43473130464553833}]}, {"text": "Human evalua-tors provide informal descriptions of each nugget, and judgements (assignments of nuggets to responses) for each response submitted by participants.", "labels": [], "entities": []}, {"text": "While human evaluation is the most accurate way to compare systems, approximate automatic evaluation becomes critical during system development.", "labels": [], "entities": []}, {"text": "We present Nuggeteer, anew automatic evaluation tool for nugget-based tasks.", "labels": [], "entities": []}, {"text": "Like the first such tool, Pourpre, Nugge-teer uses words in common between candidate answer and answer key to approximate human judgements.", "labels": [], "entities": []}, {"text": "Unlike Pour-pre, but like human assessors, Nuggeteer creates a judgement for each candidate-nugget pair, and can use existing judgements instead of guessing.", "labels": [], "entities": []}, {"text": "This creates a more readily interpretable aggregate score, and allows developers to track individual nuggets through the variants of their system.", "labels": [], "entities": []}, {"text": "Nuggeteer is quantitatively comparable in performance to Pourpre, and provides qualitatively better feedback to developers.", "labels": [], "entities": [{"text": "Nuggeteer", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.8679282665252686}]}], "introductionContent": [{"text": "The TREC Definition and Relationship questions are evaluated on the basis of information nuggets, abstract pieces of knowledge that, taken together, comprise an answer.", "labels": [], "entities": [{"text": "TREC Definition", "start_pos": 4, "end_pos": 19, "type": "TASK", "confidence": 0.5419625639915466}]}, {"text": "Nuggets are described informally, with abbreviations, misspellings, etc., and each is associated with an importance judgement: 'vital' or 'okay'.", "labels": [], "entities": []}, {"text": "1 In some sense, nuggets are like WordNet synsets, and their descriptions are like glosses.", "labels": [], "entities": []}, {"text": "Responses may contain more than one nugget-when they contain more than one piece of knowledge from the answer.", "labels": [], "entities": []}, {"text": "The median scores of today's systems are frequently zero; most responses contain no nuggets.", "labels": [], "entities": []}, {"text": "Human assessors decide what nuggets makeup an answer based on some initial research and on pools of top system responses for each question.", "labels": [], "entities": []}, {"text": "Answer keys list, for each nugget, its id, importance, and description; two example answer keys are shown in Figures 1 and 2.", "labels": [], "entities": []}, {"text": "Assessors make binary decisions about each response, whether it contains each nugget.", "labels": [], "entities": []}, {"text": "When multiple responses contain a nugget, the assessor gives credit only to the (subjectively) best response.", "labels": [], "entities": []}, {"text": "Using the judgements of the assessors, the final score combines the recall of the available vital nuggets, and the length (discounting whitespace) of the system response as a proxy for precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 68, "end_pos": 74, "type": "METRIC", "confidence": 0.9991388320922852}, {"text": "length", "start_pos": 115, "end_pos": 121, "type": "METRIC", "confidence": 0.9816727638244629}, {"text": "precision", "start_pos": 185, "end_pos": 194, "type": "METRIC", "confidence": 0.9971218705177307}]}, {"text": "Nuggets valued 'okay' contribute to precision by increasing the length allowance, but do not contribute to recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 36, "end_pos": 45, "type": "METRIC", "confidence": 0.999506950378418}, {"text": "length allowance", "start_pos": 64, "end_pos": 80, "type": "METRIC", "confidence": 0.9768643379211426}, {"text": "recall", "start_pos": 107, "end_pos": 113, "type": "METRIC", "confidence": 0.9977644681930542}]}, {"text": "The scoring formula is shown in.", "labels": [], "entities": [{"text": "scoring", "start_pos": 4, "end_pos": 11, "type": "METRIC", "confidence": 0.9490785002708435}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: For each data set (D=\"definition\", O=\"other\", R=\"relationship\"), the number of questions, the  numbers of vital and okay nuggets, the average total number of nuggets per question, the number of par- ticipating systems, the average number of responses per system, and the average number of responses per  question over all systems.", "labels": [], "entities": []}, {"text": " Table 3: Correlation (R 2 ) and Root Mean Squared  Error (  \u221a mse) between scores generated by Pour-", "labels": [], "entities": [{"text": "Correlation (R 2 )", "start_pos": 10, "end_pos": 28, "type": "METRIC", "confidence": 0.8079731225967407}, {"text": "Root Mean Squared  Error (  \u221a mse)", "start_pos": 33, "end_pos": 67, "type": "METRIC", "confidence": 0.9535239934921265}]}, {"text": " Table 5: Nuggeteer agreement with official judge- ments, under best settings for each year, and under  the default settings.", "labels": [], "entities": []}]}