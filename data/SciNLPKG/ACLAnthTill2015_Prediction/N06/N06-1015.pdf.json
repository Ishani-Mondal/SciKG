{"title": [], "abstractContent": [{"text": "Recently, discriminative word alignment methods have achieved state-of-the-art accuracies by extending the range of information sources that can be easily incorporated into aligners.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 25, "end_pos": 39, "type": "TASK", "confidence": 0.7068174481391907}]}, {"text": "The chief advantage of a discriminative framework is the ability to score alignments based on arbitrary features of the matching word tokens, including orthographic form, predictions of other models, lexical context and soon.", "labels": [], "entities": []}, {"text": "However, the proposed bipartite matching model of Taskar et al.", "labels": [], "entities": []}, {"text": "(2005), despite being tractable and effective, has two important limitations.", "labels": [], "entities": []}, {"text": "First, it is limited by the restriction that words have fertility of at most one.", "labels": [], "entities": []}, {"text": "More importantly , first order correlations between consecutive words cannot be directly captured by the model.", "labels": [], "entities": []}, {"text": "In this work, we address these limitations by enriching the model form.", "labels": [], "entities": []}, {"text": "We give estimation and inference algorithms for these enhancements.", "labels": [], "entities": []}, {"text": "Our best model achieves a relative AER reduction of 25% over the basic matching formulation, outperform-ing intersected IBM Model 4 without using any overly compute-intensive features.", "labels": [], "entities": [{"text": "AER reduction", "start_pos": 35, "end_pos": 48, "type": "METRIC", "confidence": 0.9892134368419647}]}, {"text": "By including predictions of other models as features, we achieve AER of 3.8 on the standard Hansards dataset.", "labels": [], "entities": [{"text": "AER", "start_pos": 65, "end_pos": 68, "type": "METRIC", "confidence": 0.999809205532074}, {"text": "Hansards dataset", "start_pos": 92, "end_pos": 108, "type": "DATASET", "confidence": 0.9890380501747131}]}], "introductionContent": [{"text": "Word alignment is a key component of most endto-end statistical machine translation systems.", "labels": [], "entities": [{"text": "Word alignment", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7565138638019562}, {"text": "endto-end statistical machine translation", "start_pos": 42, "end_pos": 83, "type": "TASK", "confidence": 0.5662540048360825}]}, {"text": "The standard approach to word alignment is to construct directional generative models (, which produce a sentence in one language given the sentence in another language.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 25, "end_pos": 39, "type": "TASK", "confidence": 0.7896718084812164}]}, {"text": "While these models require sentence-aligned bitexts, they can be trained with no further supervision, using EM.", "labels": [], "entities": []}, {"text": "Generative alignment models do, however, have serious drawbacks.", "labels": [], "entities": [{"text": "Generative alignment", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.6233989298343658}]}, {"text": "First, they require extensive tuning and processing of large amounts of data which, for the better-performing models, is a non-trivial resource requirement.", "labels": [], "entities": []}, {"text": "Second, conditioning on arbitrary features of the input is difficult; for example, we would like to condition on the orthographic similarity of a word pair (for detecting cognates), the presence of that pair in various dictionaries, the similarity of the frequency of its two words, choices made by other alignment systems, and soon.", "labels": [], "entities": []}, {"text": "Recently, proposed a discriminative model in which pairs of sentences (e, f ) and proposed alignments a are scored using a linear combination of arbitrary features computed from the tuples (a, e, f ).", "labels": [], "entities": []}, {"text": "While there are no restrictions on the form of the model features, the problem of finding the highest scoring alignment is very difficult and involves heuristic search.", "labels": [], "entities": []}, {"text": "Moreover, the parameters of the model must be estimated using averaged perceptron training), which can be unstable.", "labels": [], "entities": []}, {"text": "In contrast, cast word alignment as a maximum weighted matching problem, in which each pair of words (e j , f k ) in a sentence pair (e, f ) is associated with a score s jk (e, f ) reflecting the desirability of the alignment of that pair.", "labels": [], "entities": [{"text": "cast word alignment", "start_pos": 13, "end_pos": 32, "type": "TASK", "confidence": 0.7314575513203939}]}, {"text": "Importantly, this problem is computationally tractable.", "labels": [], "entities": []}, {"text": "The alignment for the sentence pair is the highest scoring matching under constraints (such as the constraint that matchings be one-to-one).", "labels": [], "entities": []}, {"text": "The scoring model s jk (e, f ) can be based on a rich feature set defined on word pairs (e j , f k ) and their context, including measures of association, orthography, relative position, predictions of generative models, etc.", "labels": [], "entities": []}, {"text": "The parameters of the model are estimated within the framework of large-margin estimation; in particular, the problem turns out to reduce to the solution of a (relatively) small quadratic program (QP).", "labels": [], "entities": []}, {"text": "The authors show that large-margin estimation is both more stable and more accurate than perceptron training.", "labels": [], "entities": []}, {"text": "While the bipartite matching approach is a useful first step in the direction of discriminative word alignment, for discriminative approaches to compete with and eventually surpass the most sophisticated generative models, it is necessary to consider more realistic underlying statistical models.", "labels": [], "entities": [{"text": "discriminative word alignment", "start_pos": 81, "end_pos": 110, "type": "TASK", "confidence": 0.6518514653046926}]}, {"text": "Note in particular two substantial limitations of the bipartite matching model of: words have fertility of at most one, and there is noway to incorporate pairwise interactions among alignment decisions.", "labels": [], "entities": []}, {"text": "Moving beyond these limitations-while retaining computational tractability-is the next major challenge for discriminative word alignment.", "labels": [], "entities": [{"text": "discriminative word alignment", "start_pos": 107, "end_pos": 136, "type": "TASK", "confidence": 0.5902350544929504}]}, {"text": "In this paper, we show how to overcome both limitations.", "labels": [], "entities": []}, {"text": "First, we introduce a parameterized model that penalizes different levels of fertility.", "labels": [], "entities": []}, {"text": "While this extension adds very useful expressive power to the model, it turns out not to increase the computational complexity of the aligner, for either the prediction or the parameter estimation problem.", "labels": [], "entities": []}, {"text": "Second, we introduce a more thoroughgoing extension which incorporates first-order interactions between alignments of consecutive words into the model.", "labels": [], "entities": []}, {"text": "We do this by formulating the alignment problem as a quadratic assignment problem (QAP), wherein addition to scoring individual edges, we also define scores of pairs of edges that connect consecutive words in an alignment.", "labels": [], "entities": []}, {"text": "The predicted alignment is the highest scoring quadratic assignment.", "labels": [], "entities": []}, {"text": "QAP is an NP-hard problem, but in the range of problem sizes that we need to tackle the problem can be solved efficiently.", "labels": [], "entities": [{"text": "QAP", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.5987601280212402}]}, {"text": "In particular, using standard off-the-shelf integer program solvers, we are able to solve the QAP problems in our experiments in under a second.", "labels": [], "entities": [{"text": "integer program solvers", "start_pos": 44, "end_pos": 67, "type": "TASK", "confidence": 0.7111291686693827}, {"text": "QAP", "start_pos": 94, "end_pos": 97, "type": "TASK", "confidence": 0.7623414397239685}]}, {"text": "Moreover, the parameter estimation problem can also be solved efficiently by making use of a linear relaxation of QAP for the min-max formulation of large-margin estimation).", "labels": [], "entities": [{"text": "parameter estimation", "start_pos": 14, "end_pos": 34, "type": "TASK", "confidence": 0.5641853660345078}, {"text": "QAP", "start_pos": 114, "end_pos": 117, "type": "METRIC", "confidence": 0.9674760103225708}]}, {"text": "We show that these two extensions yield significant improvements in error rates when compared to the bipartite matching model.", "labels": [], "entities": [{"text": "error rates", "start_pos": 68, "end_pos": 79, "type": "METRIC", "confidence": 0.9754642844200134}]}, {"text": "The addition of a fertility model improves the AER by 0.4.", "labels": [], "entities": [{"text": "AER", "start_pos": 47, "end_pos": 50, "type": "METRIC", "confidence": 0.9995272159576416}]}, {"text": "Modeling first-order interactions improves the AER by 1.8.", "labels": [], "entities": [{"text": "AER", "start_pos": 47, "end_pos": 50, "type": "METRIC", "confidence": 0.9994028806686401}]}, {"text": "Combining the two extensions results in an improvement in AER of 2.3, yielding alignments of better quality than intersected IBM Model 4.", "labels": [], "entities": [{"text": "AER", "start_pos": 58, "end_pos": 61, "type": "METRIC", "confidence": 0.9997602105140686}]}, {"text": "Moreover, including predictions of bi-directional IBM Model 4 and model of as features, we achieve an absolute AER of 3.8 on the EnglishFrench Hansards alignment task-the best AER result published on this task to date.", "labels": [], "entities": [{"text": "AER", "start_pos": 111, "end_pos": 114, "type": "METRIC", "confidence": 0.9485891461372375}, {"text": "EnglishFrench Hansards alignment task-the", "start_pos": 129, "end_pos": 170, "type": "DATASET", "confidence": 0.8667348921298981}, {"text": "AER", "start_pos": 176, "end_pos": 179, "type": "METRIC", "confidence": 0.9975922703742981}]}], "datasetContent": [{"text": "We applied our algorithms to word-level alignment using the English-French Hansards data from the 2003 NAACL shared task (.", "labels": [], "entities": [{"text": "word-level alignment", "start_pos": 29, "end_pos": 49, "type": "TASK", "confidence": 0.7896711826324463}, {"text": "English-French Hansards data from the 2003 NAACL shared task", "start_pos": 60, "end_pos": 120, "type": "DATASET", "confidence": 0.8362665077050527}]}, {"text": "This corpus consists of 1.1M automatically aligned sentences, and comes with a validation set of 37 sentence pairs and a test set of 447 sentences.", "labels": [], "entities": []}, {"text": "The validation and test sentences have been hand-aligned (see) and are marked with both sure and possible alignments.", "labels": [], "entities": []}, {"text": "Using these alignments, alignment error rate (AER) is calculated as: Here, A is a set of proposed index pairs, S is the sure gold pairs, and P is the possible gold pairs.", "labels": [], "entities": [{"text": "alignment error rate (AER)", "start_pos": 24, "end_pos": 50, "type": "METRIC", "confidence": 0.9121596415837606}]}, {"text": "For example, in, proposed alignments are shown against gold alignments, with open squares for sure alignments, rounded open squares for possible alignments, and filled black squares for proposed alignments.", "labels": [], "entities": []}, {"text": "The input to our algorithm is a small number of labeled examples.", "labels": [], "entities": []}, {"text": "In order to make our results more comparable with, we split the original set into 200 training examples and 247 test examples.", "labels": [], "entities": []}, {"text": "We also trained on only the first 100 to make our results more comparable with the experiments of, in which IBM model 4 was tuned using 100 sentences.", "labels": [], "entities": []}, {"text": "In all our experiments, we used a structured loss function that penalized false negatives 10 times more than false positives, where the value of 10 was picked by using a validation set.", "labels": [], "entities": []}, {"text": "The regularization parameter \u03b3 was also chosen using the validation set.", "labels": [], "entities": []}], "tableCaptions": []}