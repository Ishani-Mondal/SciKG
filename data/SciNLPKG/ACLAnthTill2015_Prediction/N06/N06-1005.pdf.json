{"title": [{"text": "Effectively Using Syntax for Recognizing False Entailment", "labels": [], "entities": [{"text": "Recognizing False Entailment", "start_pos": 29, "end_pos": 57, "type": "TASK", "confidence": 0.8804630835851034}]}], "abstractContent": [{"text": "Recognizing textual entailment is a challenging problem and a fundamental component of many applications in natural language processing.", "labels": [], "entities": [{"text": "Recognizing textual entailment", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.8837054769198099}]}, {"text": "We present a novel framework for recognizing textual entail-ment that focuses on the use of syntactic heuristics to recognize false entailment.", "labels": [], "entities": []}, {"text": "We give a thorough analysis of our system , which demonstrates state-of-the-art performance on a widely-used test set.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recognizing the semantic equivalence of two fragments of text is a fundamental component of many applications in natural language processing.", "labels": [], "entities": [{"text": "Recognizing the semantic equivalence of two fragments of text", "start_pos": 0, "end_pos": 61, "type": "TASK", "confidence": 0.8273167543941073}, {"text": "natural language processing", "start_pos": 113, "end_pos": 140, "type": "TASK", "confidence": 0.6421675086021423}]}, {"text": "Recognizing textual entailment, as formulated in the recent PASCAL Challenge 1 , is the problem of determining whether some text sentence T entails some hypothesis sentence H.", "labels": [], "entities": [{"text": "PASCAL Challenge 1", "start_pos": 60, "end_pos": 78, "type": "DATASET", "confidence": 0.7168026169141134}]}, {"text": "The motivation for this formulation was to isolate and evaluate the application-independent component of semantic inference shared across many application areas, reflected in the division of the PAS-CAL RTE dataset into seven distinct tasks: Information Extraction (IE), Comparable Documents (CD), Reading Comprehension (RC), Machine Translation (MT), Information Retrieval (IR), Question Answering (QA), and Paraphrase Acquisition (PP).", "labels": [], "entities": [{"text": "PAS-CAL RTE dataset", "start_pos": 195, "end_pos": 214, "type": "DATASET", "confidence": 0.7217061122258505}, {"text": "Machine Translation (MT)", "start_pos": 326, "end_pos": 350, "type": "TASK", "confidence": 0.8443233907222748}, {"text": "Information Retrieval (IR)", "start_pos": 352, "end_pos": 378, "type": "TASK", "confidence": 0.8264331340789794}, {"text": "Question Answering (QA)", "start_pos": 380, "end_pos": 403, "type": "TASK", "confidence": 0.8515048742294311}]}, {"text": "The RTE problem as presented in the PASCAL RTE dataset is particularly attractive in that it is a reasonably simple task for human annotators with high inter-annotator agreement (95.1% in one independent labeling), but an extremely challenging task for automated systems.", "labels": [], "entities": [{"text": "RTE problem", "start_pos": 4, "end_pos": 15, "type": "TASK", "confidence": 0.9162580966949463}, {"text": "PASCAL RTE dataset", "start_pos": 36, "end_pos": 54, "type": "DATASET", "confidence": 0.8677428960800171}]}, {"text": "The highest accuracy systems on the RTE test set are still much closer in performance to a random baseline accuracy of 50% than to the inter-annotator agreement.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9936216473579407}, {"text": "RTE test set", "start_pos": 36, "end_pos": 48, "type": "DATASET", "confidence": 0.9406213363011678}, {"text": "random baseline accuracy", "start_pos": 91, "end_pos": 115, "type": "METRIC", "confidence": 0.6723464528719584}]}, {"text": "For example, two high-accuracy systems are those described in (), achieving 60.4% accuracy with no task-specific information, and), which achieves 61.2% task-dependent accuracy, i.e. when able to use the specific task labels as input.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 82, "end_pos": 90, "type": "METRIC", "confidence": 0.9966539144515991}, {"text": "accuracy", "start_pos": 168, "end_pos": 176, "type": "METRIC", "confidence": 0.931374728679657}]}, {"text": "Previous systems for RTE have attempted a wide variety of strategies.", "labels": [], "entities": [{"text": "RTE", "start_pos": 21, "end_pos": 24, "type": "TASK", "confidence": 0.8756656646728516}]}, {"text": "Many previous approaches have used a logical form representation of the text and hypothesis sentences, focusing on deriving a proof by which one can infer the hypothesis logical form from the text logical form ().", "labels": [], "entities": []}, {"text": "These papers often cite that a major obstacle to accurate theorem proving for the task of textual entailment is the lack of world knowledge, which is frequently difficult and costly to obtain and encode.", "labels": [], "entities": [{"text": "theorem proving", "start_pos": 58, "end_pos": 73, "type": "TASK", "confidence": 0.7099234461784363}, {"text": "textual entailment", "start_pos": 90, "end_pos": 108, "type": "TASK", "confidence": 0.7000119537115097}]}, {"text": "Attempts have been made to remedy this deficit through various techniques, including modelbuilding) and the addition of semantic axioms ().", "labels": [], "entities": []}, {"text": "Our system diverges from previous approaches most strongly by focusing upon false entailments; rather than assuming that a given entailment is false until proven true, we make the opposite assump-tion, and instead focus on applying knowledge-free heuristics that can act locally on a subgraph of syntactic dependencies to determine with high confidence that the entailment is false.", "labels": [], "entities": []}, {"text": "Our approach is inspired by an analysis of the RTE dataset that suggested a syntax-based approach should be approximately twice as effective at predicting false entailment as true entailment).", "labels": [], "entities": [{"text": "RTE dataset", "start_pos": 47, "end_pos": 58, "type": "DATASET", "confidence": 0.9124485552310944}]}, {"text": "The analysis implied that a great deal of syntactic information remained unexploited by existing systems, but gave few explicit suggestions on how syntactic information should be applied; this paper provides a starting point for creating the heuristics capable of obtaining the bound they suggest 2 .", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Summary of accuracies and confidence- weighted scores, by task", "labels": [], "entities": [{"text": "Summary", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.8825492858886719}, {"text": "accuracies", "start_pos": 21, "end_pos": 31, "type": "METRIC", "confidence": 0.7929660081863403}, {"text": "confidence- weighted scores", "start_pos": 36, "end_pos": 63, "type": "METRIC", "confidence": 0.8924674093723297}]}, {"text": " Table 3: Feature ablation study; quantity is the ac- curacy loss obtained by removal of single feature", "labels": [], "entities": [{"text": "ac- curacy loss", "start_pos": 50, "end_pos": 65, "type": "METRIC", "confidence": 0.8807208687067032}]}]}