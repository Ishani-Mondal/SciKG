{"title": [{"text": "A fast finite-state relaxation method for enforcing global constraints on sequence decoding", "labels": [], "entities": []}], "abstractContent": [{"text": "We describe finite-state constraint relaxation, a method for applying global constraints, expressed as automata, to sequence model decoding.", "labels": [], "entities": [{"text": "finite-state constraint relaxation", "start_pos": 12, "end_pos": 46, "type": "TASK", "confidence": 0.6498386164506277}]}, {"text": "We present algorithms for both hard constraints and binary soft constraints.", "labels": [], "entities": []}, {"text": "On the CoNLL-2004 semantic role labeling task, we report a speedup of at least 16x over a previous method that used integer linear programming.", "labels": [], "entities": [{"text": "CoNLL-2004 semantic role labeling task", "start_pos": 7, "end_pos": 45, "type": "TASK", "confidence": 0.7761998534202575}]}], "introductionContent": [{"text": "Many tasks in natural language processing involve sequence labeling.", "labels": [], "entities": [{"text": "sequence labeling", "start_pos": 50, "end_pos": 67, "type": "TASK", "confidence": 0.6527861952781677}]}, {"text": "If one models long-distance or global properties of labeled sequences, it can become intractable to find (\"decode\") the best labeling of an unlabeled sequence.", "labels": [], "entities": []}, {"text": "Nonetheless, such global properties can improve the accuracy of a model, so recent NLP papers have considered practical techniques for decoding with them.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.9987357258796692}]}, {"text": "Such techniques include Gibbs sampling (), a general-purpose Monte Carlo method, and integer linear programming (ILP),), a general-purpose exact framework for NP-complete problems.", "labels": [], "entities": []}, {"text": "Under generative models such as hidden Markov models, the probability of a labeled sequence depends only on its local properties.", "labels": [], "entities": []}, {"text": "The situation improves with discriminatively trained models, such as conditional random fields (), which do efficiently allow features that are functions of the entire observation sequence.", "labels": [], "entities": []}, {"text": "However, these features can still only look locally at the label sequence.", "labels": [], "entities": []}, {"text": "That is a significant shortcoming, because in many domains, hard or soft global constraints on the label sequence are motivated by common sense: \u2022 For named entity recognition, a phrase that appears multiple times should tend to get the same label each time ().", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 151, "end_pos": 175, "type": "TASK", "confidence": 0.6248948474725088}]}, {"text": "\u2022 In bibliography entries (), a given field (author, title, etc.) should be filled by at most one substring of the input, and there are strong preferences on the cooccurrence and order of certain fields.", "labels": [], "entities": []}, {"text": "\u2022 In seminar announcements, a given field (speaker, start time, etc.) should appear with at most one value in each announcement, although the field and value maybe repeated ().", "labels": [], "entities": []}, {"text": "\u2022 For semantic role labeling, each argument should be instantiated only once fora given verb.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 6, "end_pos": 28, "type": "TASK", "confidence": 0.731860876083374}]}, {"text": "There are several other constraints that we will describe later.", "labels": [], "entities": []}, {"text": "A popular approximate technique is to hypothesize a list of possible answers by decoding without any global constraints, and then rerank (or prune) this n-best list using the full model with all constraints.", "labels": [], "entities": []}, {"text": "Reranking relies on the local model being \"good enough\" that the globally best answer appears in its n-best list.", "labels": [], "entities": [{"text": "Reranking", "start_pos": 0, "end_pos": 9, "type": "TASK", "confidence": 0.8610275387763977}]}, {"text": "Otherwise, reranking can't find it.", "labels": [], "entities": []}, {"text": "In this paper, we propose \"constraint relaxation,\" a simple exact alternative to reranking.", "labels": [], "entities": []}, {"text": "As in reranking, we start with a weighted lattice of hypotheses proposed by the local model.", "labels": [], "entities": []}, {"text": "But rather than restrict to then best of these according to the local model, we aim to directly extract the one best according to the global model.", "labels": [], "entities": []}, {"text": "As in reranking, we hope that the local constraints alone will work well, but if they do not, the penalty is not incorrect decoding, but longer runtime as we gradually fold the global constraints into the lattice.", "labels": [], "entities": []}, {"text": "Constraint relaxation can be used whenever the global constraints can be expressed as regular languages over the label sequence.", "labels": [], "entities": []}, {"text": "In the worst case, our runtime maybe exponential in the number of constraints, since we are considering an intractable class of problems.", "labels": [], "entities": []}, {"text": "However, we show that in practice, the method is quite effective at rapid decoding under global hard constraints.", "labels": [], "entities": []}, {"text": "The remainder of the paper is organized as follows: In \u00a72 we describe how finite-state automata can be used to apply global constraints.", "labels": [], "entities": []}, {"text": "We then give a brute-force decoding algorithm ( \u00a73).", "labels": [], "entities": []}, {"text": "In \u00a74, we present a more efficient algorithm for the case of hard constraints.", "labels": [], "entities": []}, {"text": "We report results for the semantic role labeling task in \u00a75.", "labels": [], "entities": [{"text": "semantic role labeling task", "start_pos": 26, "end_pos": 53, "type": "TASK", "confidence": 0.7922233045101166}]}, {"text": "\u00a76 treats soft constraints.", "labels": [], "entities": []}], "datasetContent": [{"text": "We implemented our hard constraint relaxation algorithm, using the FSA toolkit () for finite-state operations.", "labels": [], "entities": [{"text": "hard constraint relaxation", "start_pos": 19, "end_pos": 45, "type": "TASK", "confidence": 0.7105508248011271}, {"text": "FSA toolkit", "start_pos": 67, "end_pos": 78, "type": "DATASET", "confidence": 0.9600479602813721}]}, {"text": "FSA is an opensource C++ library providing a useful set of algorithms on weighted finite-state acceptors and transducers.", "labels": [], "entities": [{"text": "FSA", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9159759879112244}]}, {"text": "For each example we decoded, we chose a random order in which to apply the constraints.", "labels": [], "entities": []}, {"text": "Lattices are generated from what amounts to a unigram model-the voted perceptron classifier of Roth and Yih.", "labels": [], "entities": []}, {"text": "The features used area subset of those commonly applied to the task.", "labels": [], "entities": []}, {"text": "Our system produces output identical to that of Roth and Yih.", "labels": [], "entities": []}, {"text": "shows F-measure on the core arguments.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 6, "end_pos": 15, "type": "METRIC", "confidence": 0.9971117973327637}]}, {"text": "The ILP runtime was provided by the authors (personal communication).", "labels": [], "entities": []}, {"text": "Because the systems were run under different conditions, the times are not directly comparable.", "labels": [], "entities": []}, {"text": "However, constraint relaxation is more than sixteen times faster than ILP despite running on a slower platform.", "labels": [], "entities": [{"text": "constraint relaxation", "start_pos": 9, "end_pos": 30, "type": "TASK", "confidence": 0.6853968948125839}]}, {"text": "Using the ten constraints from \u00a75.1, weighted naively by their log odds of violation, the soft constraint relaxation algorithm runs in a time of 58.40 seconds.", "labels": [], "entities": [{"text": "soft constraint relaxation", "start_pos": 90, "end_pos": 116, "type": "TASK", "confidence": 0.6127089659372965}]}, {"text": "It is, as expected, slower than hard constraint relaxation, but only by a factor of about two.", "labels": [], "entities": []}, {"text": "As aside note, softening these particular constraints in this particular way did not improve decoding quality in this case.", "labels": [], "entities": []}, {"text": "It might help to jointly train the relative weights of these constraints and the local model-e.g., using a perceptron algorithm, which repeatedly extracts the best global path (using our algorithm), compares it to the gold standard, and adjusts the constraint weights.", "labels": [], "entities": []}, {"text": "An obvious alternative is maximumentropy training, but the partition function would have to be computed using the large brute-force lattices, or else approximated by a sampling method.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: F-measure on core arguments.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9913301467895508}]}, {"text": " Table 3: Violations of constraints by y  *   0 .", "labels": [], "entities": [{"text": "Violations of constraints", "start_pos": 10, "end_pos": 35, "type": "TASK", "confidence": 0.8778368234634399}]}, {"text": " Table 4: Number of y  *   0 with each violation count.", "labels": [], "entities": [{"text": "Number of y  *   0", "start_pos": 10, "end_pos": 28, "type": "METRIC", "confidence": 0.9416899442672729}, {"text": "violation count", "start_pos": 39, "end_pos": 54, "type": "METRIC", "confidence": 0.9046794772148132}]}, {"text": " Table 5: Violations of constraints by\u02c6yby\u02c6 by\u02c6y, measured over the de- velopment set.", "labels": [], "entities": [{"text": "Violations", "start_pos": 10, "end_pos": 20, "type": "TASK", "confidence": 0.9810226559638977}]}]}