{"title": [{"text": "Improved Affinity Graph Based Multi-Document Summarization", "labels": [], "entities": [{"text": "Affinity", "start_pos": 9, "end_pos": 17, "type": "METRIC", "confidence": 0.8436278104782104}, {"text": "Summarization", "start_pos": 45, "end_pos": 58, "type": "TASK", "confidence": 0.7728306651115417}]}], "abstractContent": [{"text": "This paper describes an affinity graph based approach to multi-document sum-marization.", "labels": [], "entities": []}, {"text": "We incorporate a diffusion process to acquire semantic relationships between sentences, and then compute information richness of sentences by a graph rank algorithm on differentiated in-tra-document links and inter-document links between sentences.", "labels": [], "entities": []}, {"text": "A greedy algorithm is employed to impose diversity penalty on sentences and the sentences with both high information richness and high information novelty are chosen into the summary.", "labels": [], "entities": []}, {"text": "Experimental results on task 2 of DUC 2002 and task 2 of DUC 2004 demonstrate that the proposed approach outperforms existing state-of-the-art systems.", "labels": [], "entities": [{"text": "DUC 2002", "start_pos": 34, "end_pos": 42, "type": "DATASET", "confidence": 0.9149578511714935}, {"text": "DUC 2004", "start_pos": 57, "end_pos": 65, "type": "DATASET", "confidence": 0.8956163227558136}]}], "introductionContent": [{"text": "Automated multi-document summarization has drawn much attention in recent years.", "labels": [], "entities": [{"text": "Automated multi-document summarization", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.5553926726182302}]}, {"text": "Multidocument summary is usually used to provide concise topic description about a cluster of documents and facilitate the users to browse the document cluster.", "labels": [], "entities": []}, {"text": "A particular challenge for multi-document summarization is that the information stored in different documents inevitably overlaps with each other, and hence we need effective summarization methods to merge information stored in different documents, and if possible, contrast their differences.", "labels": [], "entities": [{"text": "multi-document summarization", "start_pos": 27, "end_pos": 55, "type": "TASK", "confidence": 0.625450849533081}, {"text": "summarization", "start_pos": 175, "end_pos": 188, "type": "TASK", "confidence": 0.9639090299606323}]}, {"text": "A variety of multi-document summarization methods have been developed recently.", "labels": [], "entities": [{"text": "multi-document summarization", "start_pos": 13, "end_pos": 41, "type": "TASK", "confidence": 0.6050237417221069}]}, {"text": "In this study, we focus on extractive summarization, which involves assigning saliency scores to some units (e.g. sentences, paragraphs) of the documents and extracting t\ud97b\udf59e sentences with highest scores.", "labels": [], "entities": [{"text": "extractive summarization", "start_pos": 27, "end_pos": 51, "type": "TASK", "confidence": 0.6577797532081604}]}, {"text": "MEAD is an implementation of the centroid-based method () that scores sentences based on sentence-level and inter-sentence features, including cluster centroids, position, TF*IDF, etc.", "labels": [], "entities": [{"text": "MEAD", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.556808590888977}, {"text": "TF*IDF", "start_pos": 172, "end_pos": 178, "type": "METRIC", "confidence": 0.8483228882153829}]}, {"text": "NeATS () selects important content using \ud97b\udf59entence position, term frequency, topic signature and term clustering, and then uses MMR () to remove redundancy.) identifies the most salient themes within the set by passage clustering and then composes an extraction summary, which reflects these main themes.", "labels": [], "entities": []}, {"text": "investigate different topic representations and extraction methods.\ud97b\udf59 Graph-based methods have been proposed to rank sentences or passages.", "labels": [], "entities": []}, {"text": "Websumm) uses a graph-connectivity model and operates under the assumption that nodes which are connected to many other nodes are likely to carry salient information.) is an approach for computing sentence importance based on the concept of eigenvector centrality.", "labels": [], "entities": [{"text": "computing sentence importance", "start_pos": 187, "end_pos": 216, "type": "TASK", "confidence": 0.6999018788337708}]}, {"text": "also propose similar algorithms based on PageRank and HITS to compute sentence importance for document summarization.", "labels": [], "entities": [{"text": "document summarization", "start_pos": 94, "end_pos": 116, "type": "TASK", "confidence": 0.6497810184955597}]}, {"text": "In this study, we extend the above graph-based works by proposing an integrated framework for considering both information richness and information novelty of a sentence based on sentence affinity graph.", "labels": [], "entities": []}, {"text": "First, a diffusion process is imposed on sentence affinity graph in order to make the affinity graph reflect true semantic relationships between sentences.", "labels": [], "entities": []}, {"text": "Second, intra-document links and inter-document links between sentences are differentiated to attach more importance to interdocument links for sentence information richness computation.", "labels": [], "entities": []}, {"text": "Lastly, a diversity penalty process is imposed on sentences to penalize redundant sentences.", "labels": [], "entities": []}, {"text": "Experiments on DUC 2002 and DUC 2004 data are performed and we obtain encouraging results and conclusions.", "labels": [], "entities": [{"text": "DUC 2002", "start_pos": 15, "end_pos": 23, "type": "DATASET", "confidence": 0.9636958539485931}, {"text": "DUC 2004 data", "start_pos": 28, "end_pos": 41, "type": "DATASET", "confidence": 0.8513345321019491}]}], "datasetContent": [{"text": "We compare our system with top 3 performing systems and two baseline systems on task 2 of DUC 2002 and task 4 of DUC 2004 respectively.", "labels": [], "entities": [{"text": "DUC 2002", "start_pos": 90, "end_pos": 98, "type": "DATASET", "confidence": 0.9309284687042236}, {"text": "DUC 2004", "start_pos": 113, "end_pos": 121, "type": "DATASET", "confidence": 0.8995649814605713}]}, {"text": "ROUGE ( metrics is used for evaluation and we mainly concern about ROUGE-1.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.954376220703125}, {"text": "ROUGE-1", "start_pos": 67, "end_pos": 74, "type": "METRIC", "confidence": 0.9179210662841797}]}, {"text": "The parameters of our system are tuned on DUC 2001 as follows: \ud97b\udf59=7, \ud97b\udf59=0.3 and \ud97b\udf59=1.", "labels": [], "entities": [{"text": "DUC 2001", "start_pos": 42, "end_pos": 50, "type": "DATASET", "confidence": 0.9877485632896423}]}, {"text": "We can see from the tables that our system outperforms the top performing systems and baseline systems on both DUC 2002 and DUC 2004 tasks overall three metrics.", "labels": [], "entities": [{"text": "DUC 2002 and DUC 2004", "start_pos": 111, "end_pos": 132, "type": "DATASET", "confidence": 0.8920812964439392}]}, {"text": "The performance improvement achieved by our system results from three factors: diversity penalty imposition, intradocument and inter-document link differentiation and diffusion process incorporation.", "labels": [], "entities": [{"text": "diffusion process incorporation", "start_pos": 167, "end_pos": 198, "type": "TASK", "confidence": 0.6941761573155721}]}, {"text": "The ROUGE-1 contributions of the above three factors are 0.02200, 0.00268 and 0.00043 respectively.", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 4, "end_pos": 11, "type": "METRIC", "confidence": 0.997079610824585}]}, {"text": "show the influence of the parameters in our system.", "labels": [], "entities": []}, {"text": "Note that \ud97b\udf59: \ud97b\udf59 denotes the real values \ud97b\udf59 and \ud97b\udf59 are set to.", "labels": [], "entities": []}, {"text": "\"w/ diffusion\" is the system with the diffusion process (our system) and \"w/o diffusion\" is the system without the diffusion proc-ess.", "labels": [], "entities": []}, {"text": "The observations demonstrate that \"w/ diffusion\" performs better than \"w/o diffusion\" for most parameter settings.", "labels": [], "entities": []}, {"text": "Meanwhile, \"w/ diffusion\" is more robust than \"w/o diffusion\" because the ROUGE-1 value of \"w/ diffusion\" changes less when the parameter values vary.", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 74, "end_pos": 81, "type": "METRIC", "confidence": 0.995452880859375}]}, {"text": "Note that in and 4 the performance decreases sharply with the decrease of the weight \ud97b\udf59 of interdocument links and it is the worst case when interdocument links are not taken into account (i.e. \ud97b\udf59: \ud97b\udf59=1:0), while if intra-document links are not taken into account (i.e. \ud97b\udf59:\ud97b\udf59=0:1), the performance is still good, which demonstrates the great importance of inter-document links.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. System comparison on task 2 of DUC 2002", "labels": [], "entities": [{"text": "DUC 2002", "start_pos": 41, "end_pos": 49, "type": "DATASET", "confidence": 0.9388816952705383}]}, {"text": " Table 2. System comparison on task 2 of DUC 2004", "labels": [], "entities": [{"text": "DUC 2004", "start_pos": 41, "end_pos": 49, "type": "DATASET", "confidence": 0.9382911026477814}]}]}