{"title": [{"text": "Language Model-Based Document Clustering Using Random Walks", "labels": [], "entities": [{"text": "Language Model-Based Document Clustering", "start_pos": 0, "end_pos": 40, "type": "TASK", "confidence": 0.6414932236075401}]}], "abstractContent": [{"text": "We propose anew document vector representation specifically designed for the document clustering task.", "labels": [], "entities": [{"text": "document clustering task", "start_pos": 77, "end_pos": 101, "type": "TASK", "confidence": 0.7943409283955892}]}, {"text": "Instead of the traditional term-based vectors, a document is represented as an-dimensional vector, where is the number of documents in the cluster.", "labels": [], "entities": []}, {"text": "The value at each dimension of the vector is closely related to the generation probability based on the language model of the corresponding document.", "labels": [], "entities": []}, {"text": "Inspired by the recent graph-based NLP methods, we reinforce the generation probabilities by iterating random walks on the underlying graph representation.", "labels": [], "entities": []}, {"text": "Experiments with k-means and hierarchical clustering algorithms show significant improvements over the alternative \u00a1 \u00a2 \u00a3 \u00a4 \u00a5 \u00a2 vector representation.", "labels": [], "entities": []}], "introductionContent": [{"text": "Document clustering is one of the oldest and most studied problems of information retrieval.", "labels": [], "entities": [{"text": "Document clustering", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.9106388688087463}, {"text": "information retrieval", "start_pos": 70, "end_pos": 91, "type": "TASK", "confidence": 0.7853040993213654}]}, {"text": "Almost all document clustering approaches to date have represented documents as vectors in a bag-of-words vector space model, where each dimension of a document vector corresponds to a term in the corpus).", "labels": [], "entities": [{"text": "document clustering", "start_pos": 11, "end_pos": 30, "type": "TASK", "confidence": 0.7203806787729263}]}, {"text": "General clustering algorithms are then applied to these vectors to cluster the given corpus.", "labels": [], "entities": []}, {"text": "There have been attempts to use bigrams or even higher-order ngrams to represent documents in text categorization, the supervised counterpart of document clustering, with little success ().", "labels": [], "entities": [{"text": "document clustering", "start_pos": 145, "end_pos": 164, "type": "TASK", "confidence": 0.7127716541290283}]}, {"text": "Clustering can be viewed as partitioning a set of data objects into groups such that the similarities between the objects in a same group is high while inter-group similarities are weaker.", "labels": [], "entities": []}, {"text": "The fundamental assumption in this work is that the documents that are likely to have been generated from similar language models are likely to be in the same cluster.", "labels": [], "entities": []}, {"text": "Under this assumption, we propose anew representation for document vectors specifically designed for clustering purposes.", "labels": [], "entities": []}, {"text": "Given a corpus, we are interested in the generation probabilities of a document based on the language models induced by other documents in the corpus.", "labels": [], "entities": []}, {"text": "Using these probabilities, we propose a vector representation where each dimension of a document vector corresponds to a document in the corpus instead of a term in the classical representation.", "labels": [], "entities": []}, {"text": "In other words, our document vectors are -dimensional, where is the number of documents in the corpus to be clustered.", "labels": [], "entities": []}, {"text": "The main steps of our method are as follows: for general spatial data represented as undirected graphs.", "labels": [], "entities": []}, {"text": "We have extended their model to the directed graph case.", "labels": [], "entities": []}, {"text": "We use new probabilities derived from random walks as the vector representation of the documents.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated our new vector representation by comparing it against the traditional \u00a1 \u00a2 \u00a3 \u00a4 \u00a5 \u00a2 vector space representation.", "labels": [], "entities": []}, {"text": "We ran k-means, single-link, average-link, and complete-link clustering algorithms on various data sets using both representations.", "labels": [], "entities": []}, {"text": "These algorithms are among the most popular ones that are used in document clustering.", "labels": [], "entities": [{"text": "document clustering", "start_pos": 66, "end_pos": 85, "type": "TASK", "confidence": 0.774560272693634}]}, {"text": "Given a corpus, we stemmed all the documents, removed the stopwords and constructed the \u00a1 \u00a2 \u00a3 \u00a4 \u00a5 \u00a2 vector for each document by using the bow toolkit , the random walk goes \"out of the cluster\" and I \u0089 \u0090 q vectors become very dense.", "labels": [], "entities": []}, {"text": "In other words, almost all of the graph is reachable from a given node with 4-step or longer random walks (assuming Wis around 80), which is an indication of a \"small world\" effect in generation graphs (", "labels": [], "entities": [{"text": "Wis", "start_pos": 116, "end_pos": 119, "type": "METRIC", "confidence": 0.9335721731185913}]}], "tableCaptions": [{"text": " Table 1 summarizes the corpora we used together with  the sizes of the smallest and largest class in each of them.", "labels": [], "entities": []}, {"text": " Table 3: The corpora used in the hierarchical clustering exper-", "labels": [], "entities": [{"text": "hierarchical clustering", "start_pos": 34, "end_pos": 57, "type": "TASK", "confidence": 0.690718799829483}]}, {"text": " Table 2: Performances of different vector representations using k-means (average of 30 runs  \u009a  \u009b  \u009c  \u009d", "labels": [], "entities": []}, {"text": " Table 4: Performances (F-measure  \u00a5  \u00a6  x  x", "labels": [], "entities": [{"text": "F-measure", "start_pos": 24, "end_pos": 33, "type": "METRIC", "confidence": 0.9696603417396545}]}]}