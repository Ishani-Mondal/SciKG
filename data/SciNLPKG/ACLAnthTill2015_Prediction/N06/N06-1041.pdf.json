{"title": [], "abstractContent": [{"text": "We investigate prototype-driven learning for primarily unsupervised sequence modeling.", "labels": [], "entities": []}, {"text": "Prior knowledge is specified declaratively, by providing a few canonical examples of each target annotation label.", "labels": [], "entities": []}, {"text": "This sparse prototype information is then propagated across a corpus using distri-butional similarity features in a log-linear gener-ative model.", "labels": [], "entities": []}, {"text": "On part-of-speech induction in En-glish and Chinese, as well as an information extraction task, prototype features provide substantial error rate reductions over competitive baselines and outperform previous work.", "labels": [], "entities": [{"text": "part-of-speech induction", "start_pos": 3, "end_pos": 27, "type": "TASK", "confidence": 0.7005850672721863}, {"text": "information extraction", "start_pos": 67, "end_pos": 89, "type": "TASK", "confidence": 0.8308990895748138}]}, {"text": "For example, we can achieve an English part-of-speech tagging accuracy of 80.5% using only three examples of each tag and no dictionary constraints.", "labels": [], "entities": [{"text": "English part-of-speech tagging", "start_pos": 31, "end_pos": 61, "type": "TASK", "confidence": 0.487486074368159}, {"text": "accuracy", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.9361395835876465}]}, {"text": "We also compare to semi-supervised learning and discuss the system's error trends.", "labels": [], "entities": []}], "introductionContent": [{"text": "Learning, broadly taken, involves choosing a good model from a large space of possible models.", "labels": [], "entities": []}, {"text": "In supervised learning, model behavior is primarily determined by labeled examples, whose production requires a certain kind of expertise and, typically, a substantial commitment of resources.", "labels": [], "entities": []}, {"text": "In unsupervised learning, model behavior is largely determined by the structure of the model.", "labels": [], "entities": []}, {"text": "Designing models to exhibit a certain target behavior requires another, rare kind of expertise and effort.", "labels": [], "entities": []}, {"text": "Unsupervised learning, while minimizing the usage of labeled data, does not necessarily minimize total effort.", "labels": [], "entities": []}, {"text": "We therefore consider here how to learn models with the least effort.", "labels": [], "entities": []}, {"text": "In particular, we argue fora certain kind of semi-supervised learning, which we call prototype-driven learning.", "labels": [], "entities": []}, {"text": "In prototype-driven learning, we specify prototypical examples for each target label or label configuration, but do not necessarily label any documents or sentences.", "labels": [], "entities": []}, {"text": "For example, when learning a model for Penn treebank-style part-of-speech tagging in English, we may list the 45 target tags and a few examples of each tag (see fora concrete prototype list for this task).", "labels": [], "entities": [{"text": "Penn treebank-style", "start_pos": 39, "end_pos": 58, "type": "DATASET", "confidence": 0.9277300238609314}, {"text": "part-of-speech tagging", "start_pos": 59, "end_pos": 81, "type": "TASK", "confidence": 0.6911806166172028}]}, {"text": "This manner of specifying prior knowledge about the task has several advantages.", "labels": [], "entities": [{"text": "specifying prior knowledge about the task", "start_pos": 15, "end_pos": 56, "type": "TASK", "confidence": 0.8520117302735647}]}, {"text": "First, is it certainly compact (though it remains to be proven that it is effective).", "labels": [], "entities": []}, {"text": "Second, it is more or less the minimum one would have to provide to a human annotator in order to specify anew annotation task and policy (compare, for example, with the list in figure 2, which suggests an entirely different task).", "labels": [], "entities": []}, {"text": "Indeed, prototype lists have been used pedagogically to summarize tagsets to students).", "labels": [], "entities": []}, {"text": "Finally, natural language does exhibit proform and prototype effects, which suggests that learning by analogy to prototypes maybe effective for language tasks.", "labels": [], "entities": []}, {"text": "In this paper, we consider three sequence modeling tasks: part-of-speech tagging in English and Chinese and a classified ads information extraction task.", "labels": [], "entities": [{"text": "sequence modeling", "start_pos": 33, "end_pos": 50, "type": "TASK", "confidence": 0.7276394963264465}, {"text": "part-of-speech tagging", "start_pos": 58, "end_pos": 80, "type": "TASK", "confidence": 0.7975559830665588}, {"text": "classified ads information extraction", "start_pos": 110, "end_pos": 147, "type": "TASK", "confidence": 0.6004140451550484}]}, {"text": "Our general approach is to use distributional similarity to link any given word to similar prototypes.", "labels": [], "entities": []}, {"text": "For example, the word reported maybe linked to said, which is in turn a prototype for the part-of-speech VBD.", "labels": [], "entities": [{"text": "VBD", "start_pos": 105, "end_pos": 108, "type": "DATASET", "confidence": 0.8036453127861023}]}, {"text": "We then encode these prototype links as features in a log-linear generative model, which is trained to fit unlabeled data (see section 4.1).", "labels": [], "entities": []}, {"text": "Distributional prototype features provide substantial error rate reductions on all three tasks.", "labels": [], "entities": []}, {"text": "For example, on English part-of-speech tagging with three prototypes per tag, adding prototype features to the baseline raises per-position accuracy from 41.3% to 80.5%.", "labels": [], "entities": [{"text": "English part-of-speech tagging", "start_pos": 16, "end_pos": 46, "type": "TASK", "confidence": 0.5343965291976929}, {"text": "accuracy", "start_pos": 140, "end_pos": 148, "type": "METRIC", "confidence": 0.9842908382415771}]}], "datasetContent": [{"text": "We experimented with prototype-driven learning in three domains: English and Chinese part-of-speech tagging and classified advertisement field segmentation.", "labels": [], "entities": [{"text": "English and Chinese part-of-speech tagging", "start_pos": 65, "end_pos": 107, "type": "TASK", "confidence": 0.5706842422485352}, {"text": "classified advertisement field segmentation", "start_pos": 112, "end_pos": 155, "type": "TASK", "confidence": 0.6439793631434441}]}, {"text": "At inference time, we used maximum posterior decoding, 2 which we found to be uniformly but slightly superior to Viterbi decoding.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: English POS results measured by per- position accuracy", "labels": [], "entities": [{"text": "English", "start_pos": 10, "end_pos": 17, "type": "DATASET", "confidence": 0.5498687624931335}, {"text": "POS", "start_pos": 18, "end_pos": 21, "type": "METRIC", "confidence": 0.4029727578163147}, {"text": "per- position accuracy", "start_pos": 42, "end_pos": 64, "type": "METRIC", "confidence": 0.6680274903774261}]}]}