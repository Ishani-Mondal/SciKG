{"title": [], "abstractContent": [{"text": "We present a simple, but surprisingly effective , method of self-training a two-phase parser-reranker system using readily available unlabeled data.", "labels": [], "entities": []}, {"text": "We show that this type of bootstrapping is possible for parsing when the bootstrapped parses are processed by a discriminative reranker.", "labels": [], "entities": [{"text": "parsing", "start_pos": 56, "end_pos": 63, "type": "TASK", "confidence": 0.9736601710319519}]}, {"text": "Our improved model achieves an f-score of 92.1%, an absolute 1.1% improvement (12% error reduction) over the previous best result for Wall Street Journal parsing.", "labels": [], "entities": [{"text": "f-score", "start_pos": 31, "end_pos": 38, "type": "METRIC", "confidence": 0.9968389272689819}, {"text": "error reduction", "start_pos": 83, "end_pos": 98, "type": "METRIC", "confidence": 0.9742513597011566}, {"text": "Wall Street Journal parsing", "start_pos": 134, "end_pos": 161, "type": "TASK", "confidence": 0.7925731986761093}]}, {"text": "Finally, we provide some analysis to better understand the phenomenon.", "labels": [], "entities": []}], "introductionContent": [{"text": "In parsing, we attempt to uncover the syntactic structure from a string of words.", "labels": [], "entities": []}, {"text": "Much of the challenge of this lies in extracting the appropriate parsing decisions from textual examples.", "labels": [], "entities": []}, {"text": "Given sufficient labelled data, there are several \"supervised\" techniques of training high-performance parsers).", "labels": [], "entities": []}, {"text": "Other methods are \"semi-supervised\" where they use some labelled data to annotate unlabeled data.", "labels": [], "entities": []}, {"text": "Examples of this include self-training) and co-training).", "labels": [], "entities": []}, {"text": "Finally, there are \"unsupervised\" strategies where no data is labeled and all annotations (including the grammar itself) must be discovered ().", "labels": [], "entities": []}, {"text": "Semi-supervised and unsupervised methods are important because good labeled data is expensive, whereas there is no shortage of unlabeled data.", "labels": [], "entities": []}, {"text": "While some domain-language pairs have quite a bit of labelled data (e.g. news text in English), many other categories are not as fortunate.", "labels": [], "entities": []}, {"text": "Less unsupervised methods are more likely to be portable to these new domains, since they do not rely as much on existing annotations.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our parsing model consists of two phases.", "labels": [], "entities": []}, {"text": "First, we use a generative parser to produce a list of the top n parses.", "labels": [], "entities": [{"text": "generative parser", "start_pos": 16, "end_pos": 33, "type": "TASK", "confidence": 0.8729528486728668}]}, {"text": "Next, a discriminative reranker reorders the n-best list.", "labels": [], "entities": []}, {"text": "These components constitute two views of the data, though the reranker's view is restricted to the parses suggested by the first-stage parser.", "labels": [], "entities": []}, {"text": "The reranker is notable to suggest new parses and, moreover, uses the probability of each parse tree according to the parser as a feature to perform the reranking.", "labels": [], "entities": []}, {"text": "Nevertheless, the reranker's value comes from its ability to make use of more powerful features.", "labels": [], "entities": []}, {"text": "We use the reranking parser to produce 50-best parses of unlabeled news articles from NANC.", "labels": [], "entities": [{"text": "parses of unlabeled news articles from NANC", "start_pos": 47, "end_pos": 90, "type": "TASK", "confidence": 0.6034579532487052}]}, {"text": "Next, we produce two sets of one-best lists from these 50-best lists.", "labels": [], "entities": []}, {"text": "The parser-best and reranker-best lists represent the best parse for each sentence according to the parser and reranker, respectively.", "labels": [], "entities": []}, {"text": "Finally, we mix a portion of parser-best or rerankerbest lists with the standard Wall Street Journal training data (sections 2-21) to retrain anew parser (but not reranker 1 ) model.", "labels": [], "entities": [{"text": "Wall Street Journal training data", "start_pos": 81, "end_pos": 114, "type": "DATASET", "confidence": 0.9454005837440491}]}, {"text": "The Wall Street Journal training data is combined with the NANC data in the following way: The count of each parsing event is the (optionally weighted) sum of the counts of that event in Wall Street Journal and NANC.", "labels": [], "entities": [{"text": "Wall Street Journal training data", "start_pos": 4, "end_pos": 37, "type": "DATASET", "confidence": 0.9589529633522034}, {"text": "NANC data", "start_pos": 59, "end_pos": 68, "type": "DATASET", "confidence": 0.9510757625102997}, {"text": "parsing event", "start_pos": 109, "end_pos": 122, "type": "TASK", "confidence": 0.8709494471549988}, {"text": "Wall Street Journal", "start_pos": 187, "end_pos": 206, "type": "DATASET", "confidence": 0.9680931568145752}, {"text": "NANC", "start_pos": 211, "end_pos": 215, "type": "DATASET", "confidence": 0.7598373293876648}]}, {"text": "show that count merging is more effective than creating multiple models and calculating weights for each model (model interpolation).", "labels": [], "entities": [{"text": "count merging", "start_pos": 10, "end_pos": 23, "type": "TASK", "confidence": 0.8302951753139496}]}, {"text": "Intuitively, this corresponds to concatenating our training sets, possibly with multiple copies of each to account for weighting.", "labels": [], "entities": []}, {"text": "Some notes regarding evaluations: All numbers reported are f -scores 2 . In some cases, we evaluate only the parser's performance to isolate it from the reranker.", "labels": [], "entities": []}, {"text": "In other cases, we evaluate the reranking parser as a whole.", "labels": [], "entities": []}, {"text": "In these cases, we will use the term reranking parser.", "labels": [], "entities": []}, {"text": "shows the difference in parser's (not reranker's) performance when trained on parser-best We attempted to retrain the reranker using the self-trained sentences, but found no significant improvement.", "labels": [], "entities": []}, {"text": "The harmonic mean of labeled precision (P) and labeled recall (R), i.e. f = 2\u00d7P \u00d7R P +R Sentences added Parser-best Reranker-best 0 (baseline) 90: f -scores after adding either parser-best or reranker-best sentences from NANC to WSJ training data.", "labels": [], "entities": [{"text": "labeled precision (P)", "start_pos": 21, "end_pos": 42, "type": "METRIC", "confidence": 0.7503268659114838}, {"text": "recall (R)", "start_pos": 55, "end_pos": 65, "type": "METRIC", "confidence": 0.9214332848787308}, {"text": "Parser-best Reranker-best 0", "start_pos": 104, "end_pos": 131, "type": "METRIC", "confidence": 0.817017674446106}, {"text": "NANC to WSJ training data", "start_pos": 221, "end_pos": 246, "type": "DATASET", "confidence": 0.7675784945487976}]}, {"text": "While the reranker was used to produce the reranker-best sentences, we performed this evaluation using only the first-stage parser to parse all sentences from section 22.", "labels": [], "entities": []}, {"text": "We did not train a model where we added 2,000k parser-best sentences.", "labels": [], "entities": []}, {"text": "Adding parserbest sentences recreates previous self-training experiments and confirms that it is not beneficial.", "labels": [], "entities": []}, {"text": "However, we see a large improvement from adding reranker-best sentences.", "labels": [], "entities": []}, {"text": "One may expect to see a monotonic improvement from this technique, but this is not quite the case, as seen when we add 1,000k sentences.", "labels": [], "entities": []}, {"text": "This maybe due to some sections of NANC being less similar to WSJ or containing more noise.", "labels": [], "entities": [{"text": "NANC", "start_pos": 35, "end_pos": 39, "type": "DATASET", "confidence": 0.907729983329773}, {"text": "WSJ", "start_pos": 62, "end_pos": 65, "type": "DATASET", "confidence": 0.887825071811676}]}, {"text": "Another possibility is that these sections contains harder sentences which we cannot parse as accurately and thus are not as useful for self-training.", "labels": [], "entities": []}, {"text": "For our remaining experiments, we will only use reranker-best lists.", "labels": [], "entities": []}, {"text": "We also attempt to discover the optimal number of sentences to add from NANC.", "labels": [], "entities": [{"text": "NANC", "start_pos": 72, "end_pos": 76, "type": "DATASET", "confidence": 0.9147968888282776}]}, {"text": "Much of the improvement comes from the addition of the initial 50,000 sentences, showing that even small amounts of new data can have a significant effect.", "labels": [], "entities": []}, {"text": "As we add more data, it becomes clear that the maximum benefit to parsing accuracy by strictly adding rerankerbest sentences is about 0.7% and that f -scores will asymptote around 91.0%.", "labels": [], "entities": [{"text": "parsing", "start_pos": 66, "end_pos": 73, "type": "TASK", "confidence": 0.9742276668548584}, {"text": "accuracy", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.9193273186683655}, {"text": "f -scores", "start_pos": 148, "end_pos": 157, "type": "METRIC", "confidence": 0.9121829072634379}]}, {"text": "We will return to this when we consider the relative weightings of WSJ and NANC data.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 67, "end_pos": 70, "type": "DATASET", "confidence": 0.9183650612831116}, {"text": "NANC data", "start_pos": 75, "end_pos": 84, "type": "DATASET", "confidence": 0.9383143484592438}]}, {"text": "One hypothesis we consider is that the reranked NANC data incorporated some of the features from the reranker.", "labels": [], "entities": [{"text": "NANC data", "start_pos": 48, "end_pos": 57, "type": "DATASET", "confidence": 0.9166398346424103}]}, {"text": "If this were the case, we would not see an improvement when evaluating a reranking parser  on the same models.", "labels": [], "entities": []}, {"text": "In, we see that the new NANC data contains some information orthogonal to the reranker and improves parsing accuracy of the reranking parser.", "labels": [], "entities": [{"text": "NANC data", "start_pos": 24, "end_pos": 33, "type": "DATASET", "confidence": 0.8914949893951416}, {"text": "parsing", "start_pos": 100, "end_pos": 107, "type": "TASK", "confidence": 0.9581186771392822}, {"text": "accuracy", "start_pos": 108, "end_pos": 116, "type": "METRIC", "confidence": 0.9575965404510498}]}, {"text": "Up to this point, we have only considered giving our true training data a relative weight of one.", "labels": [], "entities": []}, {"text": "Increasing the weight of the Wall Street Journal data should improve, or at least not hurt, parsing performance.", "labels": [], "entities": [{"text": "Wall Street Journal data", "start_pos": 29, "end_pos": 53, "type": "DATASET", "confidence": 0.9675719887018204}, {"text": "parsing", "start_pos": 92, "end_pos": 99, "type": "TASK", "confidence": 0.9839624166488647}]}, {"text": "Indeed, this is the case for both the parser (figure not shown) and reranking parser.", "labels": [], "entities": []}, {"text": "Adding more weight to the Wall Street Journal data ensures that the counts of our events will be closer to our more accurate data source while still incorporating new data from NANC.", "labels": [], "entities": [{"text": "Wall Street Journal data", "start_pos": 26, "end_pos": 50, "type": "DATASET", "confidence": 0.9821345508098602}, {"text": "NANC", "start_pos": 177, "end_pos": 181, "type": "DATASET", "confidence": 0.9448257684707642}]}, {"text": "While it appears that the performance still levels off after adding about one million sentences from NANC, the curves corresponding to higher WSJ weights achieve a higher asymptote.", "labels": [], "entities": [{"text": "NANC", "start_pos": 101, "end_pos": 105, "type": "DATASET", "confidence": 0.934773325920105}]}, {"text": "Looking at the performance of various weights across sections 1, 22, and 24, we decided that the best combination of training data is to give WSJ a relative weight of 5 and use the first 1,750k reranker-best sentences from NANC.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 142, "end_pos": 145, "type": "DATASET", "confidence": 0.8962125778198242}, {"text": "NANC", "start_pos": 223, "end_pos": 227, "type": "DATASET", "confidence": 0.9829587340354919}]}, {"text": "Finally, we evaluate our new model on the test section of Wall Street Journal.", "labels": [], "entities": [{"text": "Wall Street Journal", "start_pos": 58, "end_pos": 77, "type": "DATASET", "confidence": 0.9457617600758871}]}, {"text": "In, we note that baseline system (i.e. the parser and reranker trained purely on Wall Street Journal) has improved by 0.3% over.", "labels": [], "entities": [{"text": "Wall Street Journal", "start_pos": 81, "end_pos": 100, "type": "DATASET", "confidence": 0.9706715941429138}]}, {"text": "The 92.1% f -score is the 1.1% absolute improvement mentioned in the abstract.", "labels": [], "entities": [{"text": "f -score", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9892436265945435}]}, {"text": "The improvement from self-training is significant in both macro and micro tests (p < 10 \u22125 ).: f -scores on WSJ section 23.", "labels": [], "entities": [{"text": "f -", "start_pos": 95, "end_pos": 98, "type": "METRIC", "confidence": 0.9662094712257385}, {"text": "WSJ section 23", "start_pos": 108, "end_pos": 122, "type": "DATASET", "confidence": 0.9349580407142639}]}, {"text": "f parser and f reranker are the evaluation of the parser and reranking parser on all sentences, respectively.", "labels": [], "entities": []}, {"text": "\"WSJ + NANC\" represents the system trained on WSJ training (with a relative weight of 5) and 1,750k sentences from the reranker-best list of NANC.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 1, "end_pos": 4, "type": "DATASET", "confidence": 0.7474091649055481}, {"text": "NANC", "start_pos": 141, "end_pos": 145, "type": "DATASET", "confidence": 0.7836707830429077}]}], "tableCaptions": [{"text": " Table 1: f -scores after adding either parser-best or  reranker-best sentences from NANC to WSJ training  data. While the reranker was used to produce the  reranker-best sentences, we performed this evalua- tion using only the first-stage parser to parse all sen- tences from section 22. We did not train a model  where we added 2,000k parser-best sentences.", "labels": [], "entities": [{"text": "NANC to WSJ training  data", "start_pos": 85, "end_pos": 111, "type": "DATASET", "confidence": 0.8390941500663758}]}, {"text": " Table 2: f -scores from evaluating the rerank- ing parser on three held-out sections after adding  reranked sentences from NANC to WSJ training.  These evaluations were performed on all sentences.", "labels": [], "entities": [{"text": "f -scores", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9471085468928019}, {"text": "NANC to WSJ training", "start_pos": 124, "end_pos": 144, "type": "DATASET", "confidence": 0.72577765583992}]}, {"text": " Table 3: f -scores on WSJ section 23. f parser and  f reranker are the evaluation of the parser and rerank- ing parser on all sentences, respectively. \"WSJ +  NANC\" represents the system trained on WSJ train- ing (with a relative weight of 5) and 1,750k sen- tences from the reranker-best list of NANC.", "labels": [], "entities": [{"text": "WSJ section 23", "start_pos": 23, "end_pos": 37, "type": "DATASET", "confidence": 0.8973516225814819}, {"text": "WSJ +  NANC", "start_pos": 153, "end_pos": 164, "type": "DATASET", "confidence": 0.6810656984647115}]}, {"text": " Table 4: Oracle f -scores of top n parses produced  by baseline, a small self-trained parser, and the  \"best\" parser", "labels": [], "entities": []}, {"text": " Table 5: Factor analysis for the question: does the  self-trained parser improve the parse with the high- est probability", "labels": [], "entities": [{"text": "high- est probability", "start_pos": 101, "end_pos": 122, "type": "METRIC", "confidence": 0.7647126466035843}]}]}