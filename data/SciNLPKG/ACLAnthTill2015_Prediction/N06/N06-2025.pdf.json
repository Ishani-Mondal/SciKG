{"title": [{"text": "Syntactic Kernels for Natural Language Learning: the Semantic Role Labeling Case", "labels": [], "entities": [{"text": "Syntactic Kernels", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8697899580001831}, {"text": "Semantic Role Labeling", "start_pos": 53, "end_pos": 75, "type": "TASK", "confidence": 0.6857419411341349}]}], "abstractContent": [{"text": "In this paper, we use tree kernels to exploit deep syntactic parsing information for natural language applications.", "labels": [], "entities": []}, {"text": "We study the properties of different kernels and we provide algorithms for their computation in linear average time.", "labels": [], "entities": []}, {"text": "The experiments with SVMs on the task of predicate argument classification provide empirical data that validates our methods.", "labels": [], "entities": [{"text": "predicate argument classification", "start_pos": 41, "end_pos": 74, "type": "TASK", "confidence": 0.8693675001462301}]}], "introductionContent": [{"text": "Recently, several tree kernels have been applied to natural language learning, e.g. ().", "labels": [], "entities": []}, {"text": "Despite their promising results, three general objections against kernel methods are raised: (1) only a subset of the dual space features are relevant, thus, it maybe possible to design features in the primal space that produce the same accuracy with a faster computation time; (2) in some cases the high number of features (substructures) of the dual space can produce overfitting with a consequent accuracy decrease (; and (3) the computation time of kernel functions maybe too high and prevent their application in real scenarios.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 237, "end_pos": 245, "type": "METRIC", "confidence": 0.9969723224639893}, {"text": "accuracy", "start_pos": 400, "end_pos": 408, "type": "METRIC", "confidence": 0.9971230626106262}]}, {"text": "In this paper, we study the impact of the subtree (ST) (), subset tree (SST) () and partial tree (PT) kernels on Semantic Role Labeling (SRL).", "labels": [], "entities": [{"text": "Semantic Role Labeling (SRL)", "start_pos": 113, "end_pos": 141, "type": "TASK", "confidence": 0.8376363019148508}]}, {"text": "The PT kernel is anew function that we have designed to generate larger substructure spaces.", "labels": [], "entities": [{"text": "PT kernel", "start_pos": 4, "end_pos": 13, "type": "DATASET", "confidence": 0.6707297712564468}]}, {"text": "Moreover, to solve the computation problems, we propose algorithms which evaluate the above kernels in linear average running time.", "labels": [], "entities": []}, {"text": "We experimented such kernels with Support Vector Machines (SVMs) on the classification of semantic roles of PropBank ( and FrameNet data sets.", "labels": [], "entities": [{"text": "PropBank", "start_pos": 108, "end_pos": 116, "type": "DATASET", "confidence": 0.7629947662353516}, {"text": "FrameNet data sets", "start_pos": 123, "end_pos": 141, "type": "DATASET", "confidence": 0.8516397873560587}]}, {"text": "The results show that: (1) the kernel approach provides the same accuracy of the manually designed features.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.9993467926979065}]}, {"text": "(2) The overfitting problem does not occur although the richer space of PTs does not provide better accuracy than the one based on SST.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 100, "end_pos": 108, "type": "METRIC", "confidence": 0.9990271329879761}]}, {"text": "The average running time of our tree kernel computation is linear.", "labels": [], "entities": []}, {"text": "In the remainder of this paper, Section 2 introduces the different tree kernel spaces.", "labels": [], "entities": []}, {"text": "Section 3 describes the kernel functions and our fast algorithms for their evaluation.", "labels": [], "entities": []}, {"text": "Section 4 shows the comparative performance in terms of execution time and accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.9985978007316589}]}], "datasetContent": [{"text": "In these experiments, we study tree kernel performance in terms of average running time and accuracy on the classification of predicate arguments.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 92, "end_pos": 100, "type": "METRIC", "confidence": 0.9995156526565552}]}, {"text": "As shown in, we can label semantic roles by classifying the smallest subtree that includes the predicate with one of its arguments, i.e. the so called PAF structure.", "labels": [], "entities": []}, {"text": "The experiments were carried outwith the SVM-light-TK software available at http://ai-nlp.info.uniroma2.it/moschitti/ which encodes the fast tree kernels in the SVM-light software).", "labels": [], "entities": []}, {"text": "The multiclassifiers were obtained by training an SVM for each class in the ONE-vs.-ALL fashion.", "labels": [], "entities": [{"text": "ONE-vs.-ALL", "start_pos": 76, "end_pos": 87, "type": "METRIC", "confidence": 0.7850750088691711}]}, {"text": "In the testing phase, we selected the class associated with the maximum SVM score.", "labels": [], "entities": []}, {"text": "For the ST, SST and PT kernels, we found that the best \u03bb values (see Section 3) on the development set were 1, 0.4 and 0.8, respectively, whereas the best \u00b5 was 0.4.", "labels": [], "entities": [{"text": "PT kernels", "start_pos": 20, "end_pos": 30, "type": "DATASET", "confidence": 0.7635883092880249}]}, {"text": "To study the FTK running time, we extracted from the Penn Treebank several samples of 500 trees containing exactly n nodes.", "labels": [], "entities": [{"text": "FTK", "start_pos": 13, "end_pos": 16, "type": "DATASET", "confidence": 0.6687533855438232}, {"text": "Penn Treebank", "start_pos": 53, "end_pos": 66, "type": "DATASET", "confidence": 0.9963709712028503}]}, {"text": "Each point of shows the average computation time 1 of the kernel function applied to the 250,000 pairs of trees of size n.", "labels": [], "entities": []}, {"text": "It clearly appears that the FTK-SST and FTK-PT (i.e. FTK applied to the SST and PT kernels) average running time has linear behavior whereas, as expected, the na\u00a8\u0131vena\u00a8\u0131ve SST algorithm shows a quadratic curve.", "labels": [], "entities": [{"text": "FTK-SST", "start_pos": 28, "end_pos": 35, "type": "DATASET", "confidence": 0.8298513293266296}, {"text": "FTK-PT", "start_pos": 40, "end_pos": 46, "type": "DATASET", "confidence": 0.87343829870224}]}, {"text": "We used two different corpora: PropBank (www.cis.upenn.edu/\u223cace) along with Penn Treebank 2 ( and FrameNet.", "labels": [], "entities": [{"text": "PropBank", "start_pos": 31, "end_pos": 39, "type": "DATASET", "confidence": 0.9059069752693176}, {"text": "Penn Treebank 2", "start_pos": 76, "end_pos": 91, "type": "DATASET", "confidence": 0.9860414067904154}]}, {"text": "PropBank contains about 53,700 sentences and a fixed split between training and testing used in other researches.", "labels": [], "entities": [{"text": "PropBank", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.954056441783905}]}, {"text": "In this split, sections from 02 to 21 are used for training, section 23 for testing and section 22 as development set.", "labels": [], "entities": []}, {"text": "We considered a total of 122,774 and 7,359 arguments (from Arg0 to Arg5, ArgA and ArgM) in training and testing, respectively.", "labels": [], "entities": [{"text": "Arg0", "start_pos": 59, "end_pos": 63, "type": "METRIC", "confidence": 0.9023553133010864}, {"text": "Arg5", "start_pos": 67, "end_pos": 71, "type": "METRIC", "confidence": 0.9184056520462036}, {"text": "ArgA", "start_pos": 73, "end_pos": 77, "type": "METRIC", "confidence": 0.9838162064552307}, {"text": "ArgM", "start_pos": 82, "end_pos": 86, "type": "METRIC", "confidence": 0.969494104385376}]}, {"text": "The tree structures were extracted from the Penn Treebank.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 44, "end_pos": 57, "type": "DATASET", "confidence": 0.9962286651134491}]}, {"text": "24,558 sentences of the 40 Frames selected for the Automatic Labeling of Semantic Roles task of Senseval 3 (www.senseval.org).", "labels": [], "entities": [{"text": "Automatic Labeling of Semantic Roles task", "start_pos": 51, "end_pos": 92, "type": "TASK", "confidence": 0.7986589272816976}]}, {"text": "We considered the 18 most frequent roles, fora total of 37,948 examples (30% of the sentences for testing and 70% for training/validation).", "labels": [], "entities": []}, {"text": "The sentences were processed with the Collins' parser to generate automatic parse trees.", "labels": [], "entities": [{"text": "Collins'", "start_pos": 38, "end_pos": 46, "type": "DATASET", "confidence": 0.9592500329017639}]}, {"text": "We run ST, SST and PT kernels along with the linear kernel of standard features) on PropBank training sets of different size.", "labels": [], "entities": [{"text": "PT", "start_pos": 19, "end_pos": 21, "type": "METRIC", "confidence": 0.8048169612884521}, {"text": "PropBank training sets", "start_pos": 84, "end_pos": 106, "type": "DATASET", "confidence": 0.9555955529212952}]}, {"text": "illustrates the learning curves associated with the above kernels for the SVM multiclassifiers.", "labels": [], "entities": []}, {"text": "The tables 1 and 2 report the results, using all available training data, on PropBank and FrameNet test sets, respectively.", "labels": [], "entities": [{"text": "PropBank", "start_pos": 77, "end_pos": 85, "type": "DATASET", "confidence": 0.9604805111885071}, {"text": "FrameNet test sets", "start_pos": 90, "end_pos": 108, "type": "DATASET", "confidence": 0.9203144709269205}]}, {"text": "We note that: (1) the accuracy of PTs is almost equal to the one produced by SSTs as the PT space is a hyperset of SSTs.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9995835423469543}]}, {"text": "The small difference is due to the poor relevance of the substructures in the PT \u2212 SST set, which degrade the PT space.", "labels": [], "entities": [{"text": "PT \u2212 SST set", "start_pos": 78, "end_pos": 90, "type": "DATASET", "confidence": 0.780453160405159}]}, {"text": "(2) The high F 1 measures of tree kernels on FrameNet suggest that they are robust with respect to automatic parse trees.", "labels": [], "entities": [{"text": "F 1", "start_pos": 13, "end_pos": 16, "type": "METRIC", "confidence": 0.9772698283195496}, {"text": "FrameNet", "start_pos": 45, "end_pos": 53, "type": "DATASET", "confidence": 0.932793140411377}]}, {"text": "Moreover, the learning time of SVMs using FTK for the classification of one large argument (Arg 0) is much lower than the one required by na\u00a8\u0131vena\u00a8\u0131ve algorithm.", "labels": [], "entities": [{"text": "FTK", "start_pos": 42, "end_pos": 45, "type": "DATASET", "confidence": 0.8622590899467468}, {"text": "Arg 0)", "start_pos": 92, "end_pos": 98, "type": "METRIC", "confidence": 0.9640537103017172}]}, {"text": "With all the training data FTK terminated in 6 hours whereas the na\u00a8\u0131vena\u00a8\u0131ve approach required more than 1 week.", "labels": [], "entities": [{"text": "FTK", "start_pos": 27, "end_pos": 30, "type": "METRIC", "confidence": 0.5433339476585388}]}, {"text": "However, the complexity burden of working in the dual space can be alleviated with recent approaches proposed in ().", "labels": [], "entities": []}, {"text": "Finally, we carried out some experiments with the combination between linear and tree kernels and we found that tree kernels improve the models based on manually designed features by 2/3 percent points, thus they can be seen as a useful tactic to boost system accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 260, "end_pos": 268, "type": "METRIC", "confidence": 0.9881282448768616}]}], "tableCaptions": []}