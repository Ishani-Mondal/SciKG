{"title": [{"text": "2. Beyond EM: Bayesian Techniques for Human Language Technology Researchers", "labels": [], "entities": []}], "abstractContent": [{"text": "The Expectation-Maximization (EM) algorithm has proved to be a great and useful technique for unsu-pervised learning problems in natural language, but, unfortunately, its range of applications is largely limited by intractable E-or M-steps, and its reliance on the maximum likelihood estimator.", "labels": [], "entities": []}, {"text": "The natural language processing community typically resorts to ad-hoc approximation methods to get (some reduced form of) EM to apply to our tasks.", "labels": [], "entities": []}, {"text": "However, many of the problems that plague EM can be solved with Bayesian methods, which are theoretically more well justified.", "labels": [], "entities": [{"text": "EM", "start_pos": 42, "end_pos": 44, "type": "TASK", "confidence": 0.9680947661399841}]}, {"text": "This tutorial will cover Bayesian methods as they can be used in natural language processing.", "labels": [], "entities": []}, {"text": "The two primary foci of this tutorial are specifying prior distributions and performing the necessary computations to perform inference in Bayesian models.", "labels": [], "entities": []}, {"text": "The focus of the tutorial will be primarily on unsupervised techniques (for which EM is the obvious choice).", "labels": [], "entities": []}, {"text": "Supervised and discriminative techniques will also be mentioned at the conclusion of the tutorial, and pointers to relevant literature will be provided.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [], "tableCaptions": []}