{"title": [{"text": "Investigating Cross-Language Speech Retrieval fora Spontaneous Conversational Speech Collection", "labels": [], "entities": [{"text": "Investigating Cross-Language Speech Retrieval", "start_pos": 0, "end_pos": 45, "type": "TASK", "confidence": 0.7048579007387161}]}], "abstractContent": [{"text": "Cross-language retrieval of spontaneous speech combines the challenges of working with noisy automated transcription and language translation.", "labels": [], "entities": [{"text": "Cross-language retrieval of spontaneous speech", "start_pos": 0, "end_pos": 46, "type": "TASK", "confidence": 0.7978245615959167}, {"text": "language translation", "start_pos": 121, "end_pos": 141, "type": "TASK", "confidence": 0.721728041768074}]}, {"text": "The CLEF 2005 Cross-Language Speech Retrieval (CL-SR) task provides a standard test collection to investigate these challenges.", "labels": [], "entities": [{"text": "CLEF 2005 Cross-Language Speech Retrieval (CL-SR) task", "start_pos": 4, "end_pos": 58, "type": "TASK", "confidence": 0.8573596411281161}]}, {"text": "We show that we can improve retrieval performance: by careful selection of the term weighting scheme; by decomposing automated transcripts into phonetic substrings to help ameliorate transcription errors; and by combining automatic transcriptions with manually-assigned metadata.", "labels": [], "entities": []}, {"text": "We further show that topic translation with online machine translation resources yields effective CL-SR.", "labels": [], "entities": [{"text": "topic translation", "start_pos": 21, "end_pos": 38, "type": "TASK", "confidence": 0.8351956009864807}, {"text": "machine translation", "start_pos": 51, "end_pos": 70, "type": "TASK", "confidence": 0.7361242473125458}]}], "introductionContent": [{"text": "The emergence of large collections of digitized spoken data has encouraged research in speech retrieval.", "labels": [], "entities": [{"text": "speech retrieval", "start_pos": 87, "end_pos": 103, "type": "TASK", "confidence": 0.7413040399551392}]}, {"text": "Previous studies, notably those at TREC (), have focused mainly on well-structured news documents.", "labels": [], "entities": [{"text": "TREC", "start_pos": 35, "end_pos": 39, "type": "DATASET", "confidence": 0.807864248752594}]}, {"text": "In this paper we report on work carried out for the Cross-Language Evaluation Forum (CLEF) 2005 Cross-Language Speech Retrieval (CL-SR) track (White et).", "labels": [], "entities": [{"text": "Cross-Language Evaluation Forum (CLEF) 2005 Cross-Language Speech Retrieval (CL-SR)", "start_pos": 52, "end_pos": 135, "type": "TASK", "confidence": 0.6396796451165125}]}, {"text": "The document collection for the CL-SR task is apart of the oral testimonies collected by the USC Shoah Foundation Institute for Visual History and Education (VHI) for which some Automatic Speech Recognition (ASR) transcriptions are available ().", "labels": [], "entities": [{"text": "USC Shoah Foundation Institute for Visual History and Education (VHI)", "start_pos": 93, "end_pos": 162, "type": "DATASET", "confidence": 0.8112046172221502}, {"text": "Automatic Speech Recognition (ASR)", "start_pos": 178, "end_pos": 212, "type": "TASK", "confidence": 0.7509187757968903}]}, {"text": "The data is conversional spontaneous speech lacking clear topic boundaries; it is thus a more challenging speech retrieval task than those explored previously.", "labels": [], "entities": [{"text": "conversional spontaneous speech", "start_pos": 12, "end_pos": 43, "type": "TASK", "confidence": 0.9184930324554443}, {"text": "speech retrieval", "start_pos": 106, "end_pos": 122, "type": "TASK", "confidence": 0.7445662021636963}]}, {"text": "The CLEF data is also annotated with a range of automatic and manually generated sets of metadata.", "labels": [], "entities": [{"text": "CLEF data", "start_pos": 4, "end_pos": 13, "type": "DATASET", "confidence": 0.9503365457057953}]}, {"text": "While the complete VHI dataset contains interviews in many languages, the CLEF 2005 CL-SR task focuses on English speech.", "labels": [], "entities": [{"text": "VHI dataset", "start_pos": 19, "end_pos": 30, "type": "DATASET", "confidence": 0.9501632153987885}, {"text": "CLEF 2005 CL-SR task", "start_pos": 74, "end_pos": 94, "type": "DATASET", "confidence": 0.8984811007976532}]}, {"text": "Cross-language searching is evaluated by making the topic statements (from which queries are automatically formed) available in several languages.", "labels": [], "entities": [{"text": "Cross-language searching", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.7744007408618927}]}, {"text": "This task raises many interesting research questions; in this paper we explore alternative term weighting methods and content indexing strategies.", "labels": [], "entities": [{"text": "content indexing", "start_pos": 118, "end_pos": 134, "type": "TASK", "confidence": 0.6728325337171555}]}, {"text": "The remainder of this paper is structured as follows: Section 2 briefly reviews details of the CLEF 2005 CL-SR task; Section 3 describes the system we used to investigate this task; Section 4 reports our experimental results; and Section 5 gives conclusions and details for our ongoing work.", "labels": [], "entities": [{"text": "CLEF 2005 CL-SR task", "start_pos": 95, "end_pos": 115, "type": "DATASET", "confidence": 0.921484112739563}]}], "datasetContent": [{"text": "In this section we report results from our experimental investigation of the CLEF 2005 CL-SR task.", "labels": [], "entities": [{"text": "CLEF 2005 CL-SR task", "start_pos": 77, "end_pos": 97, "type": "DATASET", "confidence": 0.9331334382295609}]}, {"text": "For each set of experiments we report Mean uninterpolated Average Precision (MAP) computed using the trec_eval script.", "labels": [], "entities": [{"text": "Mean uninterpolated Average Precision (MAP)", "start_pos": 38, "end_pos": 81, "type": "METRIC", "confidence": 0.9506078277315412}]}, {"text": "The topic fields used are indicated as: T for title only, TD for title + description, TDN for title + description + narrative.", "labels": [], "entities": []}, {"text": "The first experiment shows results for different term weighting schemes; we then give cross-language retrieval results.", "labels": [], "entities": []}, {"text": "For both sets of experiments, \"documents\" are represented by combining the ASR transcription with the AK1 and AK2 fields.", "labels": [], "entities": []}, {"text": "Thus each document representation is generated completely automatically.", "labels": [], "entities": []}, {"text": "Later experiments explore two alternative indexing strategies.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. MAP, 25 English test topics. Bold=best scores.", "labels": [], "entities": [{"text": "MAP", "start_pos": 10, "end_pos": 13, "type": "DATASET", "confidence": 0.4139614999294281}, {"text": "Bold", "start_pos": 39, "end_pos": 43, "type": "METRIC", "confidence": 0.9681487083435059}]}, {"text": " Table 2. MAP, cross-language, 25 test topics", "labels": [], "entities": [{"text": "MAP", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.6996326446533203}]}]}