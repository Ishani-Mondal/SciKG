{"title": [], "abstractContent": [], "introductionContent": [{"text": "Although the natural-language processing community has dedicated much of its focus to text, faceto-face spoken language is ubiquitous, and offers the potential for breakthrough applications in domains such as meetings, lectures, and presentations.", "labels": [], "entities": []}, {"text": "Because spontaneous spoken language is typically more disfluent and less structured than written text, it maybe critical to identify features from additional modalities that can aid in language understanding.", "labels": [], "entities": [{"text": "language understanding", "start_pos": 185, "end_pos": 207, "type": "TASK", "confidence": 0.741530567407608}]}, {"text": "However, due to the long-standing emphasis on text datasets, there has been relatively little work on nontextual features in unconstrained natural language (prosody being the most studied non-textual modality, e.g. ().", "labels": [], "entities": []}, {"text": "There are many non-verbal modalities that may contribute to face-to-face communication, including body posture, hand gesture, facial expression, prosody, and free-hand drawing.", "labels": [], "entities": [{"text": "free-hand drawing", "start_pos": 158, "end_pos": 175, "type": "TASK", "confidence": 0.7109196782112122}]}, {"text": "Hand gesture maybe more expressive than any non-verbal modality besides drawing, since it serves as the foundation for sign languages in hearing-disabled communities.", "labels": [], "entities": []}, {"text": "While non-deaf speakers rarely use any such systematized language as American Sign Language (ASL) while gesturing, the existence of ASL speaks to the potential of gesture for communicative expressivity.", "labels": [], "entities": []}, {"text": "Hand gesture relates to spoken language in several ways: \u2022 Hand gesture communicates meaning.", "labels": [], "entities": [{"text": "Hand gesture communicates meaning", "start_pos": 59, "end_pos": 92, "type": "TASK", "confidence": 0.6739817410707474}]}, {"text": "For example, () describe a model of how hand gesture is used to convey spatial properties of its referents when speakers give navigational directions.", "labels": [], "entities": []}, {"text": "This model both explains observed behavior of human speakers, and serves as the basis for an implemented embodied agent.", "labels": [], "entities": []}, {"text": "\u2022 Hand gesture communicates discourse structure.) and describe how the structure of discourse is mirrored by the the structure of the gestures, when speakers describe sequences of events in cartoon narratives.", "labels": [], "entities": []}, {"text": "\u2022 Hand gesture segments in unison with speech, suggesting possible applications to speech recognition and syntactic processing.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 83, "end_pos": 101, "type": "TASK", "confidence": 0.7356972843408585}]}, {"text": "show a strong correlation between the onset and duration of gestures, and their \"lexical affiliates\" -the phrase that is thought to relate semantically to the gesture.", "labels": [], "entities": [{"text": "duration", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.9114843010902405}]}, {"text": "Also, ( show that gesture features may improve sentence segmentation.", "labels": [], "entities": [{"text": "sentence segmentation", "start_pos": 47, "end_pos": 68, "type": "TASK", "confidence": 0.7276001423597336}]}, {"text": "These examples area subset of abroad literature on gesture that suggests that this modality could play an important role in improving the performance of NLP systems on spontaneous spoken language.", "labels": [], "entities": []}, {"text": "However, the existence of significant relationships between gesture and speech does not prove that gesture will improve NLP; gesture features could be redundant with existing textual features, or they maybe simply too noisy or speaker-dependant to be useful.", "labels": [], "entities": []}, {"text": "To test this, my thesis research will identify specific, objective NLP tasks, and attempt to show that automatically-detected gestural features improve performance beyond what is attainable using textual features.", "labels": [], "entities": []}, {"text": "The relationship between gesture and meaning is particularly intriguing, since gesture seems to offer a unique, spatial representation of meaning to sup-plement verbal expression.", "labels": [], "entities": []}, {"text": "However, the expression of meaning through gesture is likely to be highly variable and speaker dependent, as the set of possible mappings between meaning and gestural form is large, if not infinite.", "labels": [], "entities": []}, {"text": "For this reason, I take the point of view that it is too difficult to attempt to decode individual gestures.", "labels": [], "entities": []}, {"text": "A more feasible approach is to identify similarities between pairs or groups of gestures.", "labels": [], "entities": []}, {"text": "If gestures do communicate semantics, then similar gestures should predict semantic similarity.", "labels": [], "entities": []}, {"text": "Thus, gestures can help computers understand speech by providing a set of \"back pointers\" between moments that are semantically related.", "labels": [], "entities": []}, {"text": "Using this model, my dissertation will explore measures of gesture similarity and applications of gesture similarity to NLP.", "labels": [], "entities": []}, {"text": "A set of semantic \"back pointers\" decoded from gestural features could be relevant to a number of NLP benchmark problems.", "labels": [], "entities": []}, {"text": "I will investigate two: coreference resolution and disfluency detection.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 24, "end_pos": 46, "type": "TASK", "confidence": 0.980924129486084}, {"text": "disfluency detection", "start_pos": 51, "end_pos": 71, "type": "TASK", "confidence": 0.7492308020591736}]}, {"text": "In coreference resolution, we seek to identify whether two noun phrases refer to the same semantic entity.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 3, "end_pos": 25, "type": "TASK", "confidence": 0.9669068157672882}]}, {"text": "A similarity in the gestural features observed during two different noun phrases might suggest a similarity in meaning.", "labels": [], "entities": []}, {"text": "This problem has the advantage of permitting a quantitative evaluation of the relationship between gesture and semantics, without requiring the construction of a domain ontology.", "labels": [], "entities": []}, {"text": "Restarts are disfluencies that occur when a speaker begins an utterance, and then stops and starts over again.", "labels": [], "entities": []}, {"text": "It is thought that the gesture may return to its state at the beginning of the utterance, providing a back-pointer to the restart insertion point (.", "labels": [], "entities": []}, {"text": "If so, then a similar training procedure and set of gestural features can be used for both coreference resolution and restart correction.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 91, "end_pos": 113, "type": "TASK", "confidence": 0.9516489505767822}, {"text": "restart correction", "start_pos": 118, "end_pos": 136, "type": "TASK", "confidence": 0.7882198691368103}]}, {"text": "Both of these problems have objective, quantifiable success measures, and both may play an important role in bringing to spontaneous spoken language useful NLP applications such as summarization, segmentation, and question answering.", "labels": [], "entities": [{"text": "summarization", "start_pos": 181, "end_pos": 194, "type": "TASK", "confidence": 0.9892375469207764}, {"text": "segmentation", "start_pos": 196, "end_pos": 208, "type": "TASK", "confidence": 0.9206975698471069}, {"text": "question answering", "start_pos": 214, "end_pos": 232, "type": "TASK", "confidence": 0.899434894323349}]}], "datasetContent": [], "tableCaptions": []}