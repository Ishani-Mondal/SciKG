{"title": [{"text": "Arabic Preprocessing Schemes for Statistical Machine Translation", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 33, "end_pos": 64, "type": "TASK", "confidence": 0.8728049794832865}]}], "abstractContent": [{"text": "In this paper, we study the effect of different word-level preprocessing decisions for Arabic on SMT quality.", "labels": [], "entities": [{"text": "SMT quality", "start_pos": 97, "end_pos": 108, "type": "TASK", "confidence": 0.880146712064743}]}, {"text": "Our results show that given large amounts of training data, splitting off only proclitics performs best.", "labels": [], "entities": []}, {"text": "However, for small amounts of training data, it is best to apply English-like to-kenization using part-of-speech tags, and sophisticated morphological analysis and disambiguation.", "labels": [], "entities": []}, {"text": "Moreover, choosing the appropriate preprocessing produces a significant increase in BLEU score if there is a change in genre between training and test data.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 84, "end_pos": 94, "type": "METRIC", "confidence": 0.9805479049682617}]}], "introductionContent": [{"text": "Approaches to statistical machine translation (SMT) are robust when it comes to the choice of their input representation: the only requirement is consistency between training and evaluation.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 14, "end_pos": 51, "type": "TASK", "confidence": 0.8216399252414703}]}, {"text": "This leaves a wide range of possible preprocessing choices, even more so for morphologically rich languages such as Arabic.", "labels": [], "entities": []}, {"text": "We use the term \"preprocessing\" to describe various input modifications that can be applied to raw training and evaluation texts for SMT to make them suitable for model training and decoding, including different kinds of tokenization, stemming, part-of-speech (POS) tagging and lemmatization.", "labels": [], "entities": [{"text": "SMT", "start_pos": 133, "end_pos": 136, "type": "TASK", "confidence": 0.9899535775184631}, {"text": "part-of-speech (POS) tagging", "start_pos": 245, "end_pos": 273, "type": "TASK", "confidence": 0.573539525270462}]}, {"text": "We refer to a specific kind of preprocessing as a \"scheme\" and differentiate it from the \"technique\" used to obtain it.", "labels": [], "entities": []}, {"text": "Since we wish to study the effect of word-level preprocessing, we do not utilize any syntactic information.", "labels": [], "entities": []}, {"text": "We define the word (and by extension its morphology) to be limited to written Modern Standard Arabic (MSA) strings separated by white space, punctuation and numbers.", "labels": [], "entities": [{"text": "Modern Standard Arabic (MSA) strings", "start_pos": 78, "end_pos": 114, "type": "DATASET", "confidence": 0.8206783277647836}]}, {"text": "Thus, some prepositional particles and conjunctions are considered part of the word morphology.", "labels": [], "entities": []}, {"text": "In this paper, we report on an extensive study of the effect on SMT quality of six preprocessing schemes 2 , applied to text disambiguated in three different techniques and across a learning curve.", "labels": [], "entities": [{"text": "SMT", "start_pos": 64, "end_pos": 67, "type": "TASK", "confidence": 0.9968908429145813}]}, {"text": "Our results are as follows: (a) for large amounts of training data, splitting off only proclitics performs best; (b) for small amount of training data, following an English-like tokenization and using part-of-speech tags performs best; (c) suitable choice of preprocessing yields a significant increase in BLEU score if there is little training data and/or there is a change in genre between training and test data; (d) sophisticated morphological analysis and disambiguation help significantly in the absence of large amounts of data.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 306, "end_pos": 316, "type": "METRIC", "confidence": 0.9783237874507904}]}, {"text": "Section 2 presents previous relevant research.", "labels": [], "entities": []}, {"text": "Section 3 presents some relevant background on Arabic linguistics to motivate the schemes discussed in Section 4.", "labels": [], "entities": []}, {"text": "Section 5 presents the tools and data sets used, along with the results of our experiments.", "labels": [], "entities": []}, {"text": "Section 6 contains a discussion of the results.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use the phrase-based SMT system, Portage).", "labels": [], "entities": [{"text": "SMT", "start_pos": 24, "end_pos": 27, "type": "TASK", "confidence": 0.8978832364082336}]}, {"text": "For training, Portage uses IBM word alignment models (models 1 and 2) trained in both directions to extract phrase tables.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 31, "end_pos": 45, "type": "TASK", "confidence": 0.6790416240692139}]}, {"text": "Maximum phrase size used is 8.", "labels": [], "entities": []}, {"text": "Trigram language models are implemented using the SRILM toolkit).", "labels": [], "entities": [{"text": "SRILM toolkit", "start_pos": 50, "end_pos": 63, "type": "DATASET", "confidence": 0.864099383354187}]}, {"text": "Decoding weights are optimized using Och's algorithm to set weights for the four components of the log-linear model: language model, phrase translation model, distortion model, and word-length feature.", "labels": [], "entities": [{"text": "phrase translation", "start_pos": 133, "end_pos": 151, "type": "TASK", "confidence": 0.8080732226371765}]}, {"text": "The weights are optimized over the BLEU metric ().", "labels": [], "entities": [{"text": "BLEU", "start_pos": 35, "end_pos": 39, "type": "METRIC", "confidence": 0.9956515431404114}]}, {"text": "The Portage decoder, Canoe, is a dynamic-programming beam search algorithm, resembling the algorithm described in.", "labels": [], "entities": [{"text": "Portage decoder, Canoe", "start_pos": 4, "end_pos": 26, "type": "DATASET", "confidence": 0.7741037905216217}]}, {"text": "All of the training data we use is available from the Linguistic Data Consortium (LDC).", "labels": [], "entities": [{"text": "Linguistic Data Consortium (LDC)", "start_pos": 54, "end_pos": 86, "type": "DATASET", "confidence": 0.8550355484088262}]}, {"text": "We use an Arabic-English parallel corpus of about 5 million words for translation model training data.", "labels": [], "entities": [{"text": "translation model training", "start_pos": 70, "end_pos": 96, "type": "TASK", "confidence": 0.896505614121755}]}, {"text": "We created the English language model from the English side of the parallel corpus together with 116 million words from the English Gigaword Corpus (LDC2005T12) and 128 million words from the English side of the UN Parallel corpus (LDC2004E13).", "labels": [], "entities": [{"text": "English Gigaword Corpus (LDC2005T12)", "start_pos": 124, "end_pos": 160, "type": "DATASET", "confidence": 0.905870646238327}, {"text": "UN Parallel corpus (LDC2004E13)", "start_pos": 212, "end_pos": 243, "type": "DATASET", "confidence": 0.9030814965566}]}, {"text": "English preprocessing comprised down-casing, separating punctuation from words and splitting off \"'s\".", "labels": [], "entities": []}, {"text": "Arabic preprocessing was varied using the proposed schemes and techniques.", "labels": [], "entities": []}, {"text": "Decoding weight optimization was done on 200 sentences from the 2003 NIST MT evaluation test set.", "labels": [], "entities": [{"text": "Decoding weight optimization", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8892012635866801}, {"text": "NIST MT evaluation test set", "start_pos": 69, "end_pos": 96, "type": "DATASET", "confidence": 0.8375911235809326}]}, {"text": "We used two different test sets: (a) the 2004 NIST MT evaluation test set (MT04) and (b) the 2005 NIST MT evaluation test set (MT05).", "labels": [], "entities": [{"text": "NIST MT evaluation test set (MT04", "start_pos": 46, "end_pos": 79, "type": "DATASET", "confidence": 0.8454605170658657}, {"text": "NIST MT evaluation test set (MT05)", "start_pos": 98, "end_pos": 132, "type": "DATASET", "confidence": 0.8356706500053406}]}, {"text": "MT04 is a mix of news, editorials and speeches, whereas MT05, like the training data, is purely news.", "labels": [], "entities": [{"text": "MT04", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9696933627128601}, {"text": "MT05", "start_pos": 56, "end_pos": 60, "type": "DATASET", "confidence": 0.9149273037910461}]}, {"text": "We use the evaluation metric BLEU-4 ().", "labels": [], "entities": [{"text": "BLEU-4", "start_pos": 29, "end_pos": 35, "type": "METRIC", "confidence": 0.9926202297210693}]}, {"text": "We conducted all possible combinations of schemes and techniques discussed in Section 4 with different training corpus sizes: 1%, 10% and 100%.", "labels": [], "entities": []}, {"text": "The results of the experiments are summarized in /a n/a n/a 12.7 29.6 35.9 15.7 29.5 34.3 n/a n/a n/a EN 17.5 28.4 34.5 16.3 27.9 34.0 n/a n/a n/a 18.3 30.4 36.0 17.6 30.4 34.8 n/a n/a n/a.", "labels": [], "entities": []}, {"text": "All reported scores must have over 1.1% BLEU-4 difference to be significant at the 95% confidence level for 1% training.", "labels": [], "entities": [{"text": "BLEU-4 difference", "start_pos": 40, "end_pos": 57, "type": "METRIC", "confidence": 0.9848013520240784}]}, {"text": "For all other training sizes, the difference must be over 1.7% BLEU-4.", "labels": [], "entities": [{"text": "BLEU-4", "start_pos": 63, "end_pos": 69, "type": "METRIC", "confidence": 0.9978487491607666}]}, {"text": "Error intervals were computed using bootstrap resampling (Koehn, 2004b).", "labels": [], "entities": [{"text": "Error intervals", "start_pos": 0, "end_pos": 15, "type": "METRIC", "confidence": 0.9640578925609589}]}], "tableCaptions": []}