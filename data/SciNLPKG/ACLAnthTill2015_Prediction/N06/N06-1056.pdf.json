{"title": [{"text": "Learning for Semantic Parsing with Statistical Machine Translation", "labels": [], "entities": [{"text": "Semantic Parsing", "start_pos": 13, "end_pos": 29, "type": "TASK", "confidence": 0.7448716461658478}, {"text": "Statistical Machine Translation", "start_pos": 35, "end_pos": 66, "type": "TASK", "confidence": 0.6352922817071279}]}], "abstractContent": [{"text": "We present a novel statistical approach to semantic parsing, WASP, for constructing a complete, formal meaning representation of a sentence.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 43, "end_pos": 59, "type": "TASK", "confidence": 0.7761747241020203}, {"text": "WASP", "start_pos": 61, "end_pos": 65, "type": "TASK", "confidence": 0.7965336441993713}]}, {"text": "A semantic parser is learned given a set of sentences annotated with their correct meaning representations.", "labels": [], "entities": []}, {"text": "The main innovation of WASP is its use of state-of-the-art statistical machine translation techniques.", "labels": [], "entities": [{"text": "WASP", "start_pos": 23, "end_pos": 27, "type": "TASK", "confidence": 0.9326299428939819}, {"text": "statistical machine translation", "start_pos": 59, "end_pos": 90, "type": "TASK", "confidence": 0.6303402086098989}]}, {"text": "A word alignment model is used for lexical acquisition , and the parsing model itself can be seen as a syntax-based translation model.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 2, "end_pos": 16, "type": "TASK", "confidence": 0.7403927147388458}, {"text": "lexical acquisition", "start_pos": 35, "end_pos": 54, "type": "TASK", "confidence": 0.7559248507022858}]}, {"text": "We show that WASP performs favorably in terms of both accuracy and coverage compared to existing learning methods requiring similar amount of supervision, and shows better robustness to variations in task complexity and word order.", "labels": [], "entities": [{"text": "WASP", "start_pos": 13, "end_pos": 17, "type": "TASK", "confidence": 0.9564212560653687}, {"text": "accuracy", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.9992762207984924}, {"text": "coverage", "start_pos": 67, "end_pos": 75, "type": "METRIC", "confidence": 0.9687641263008118}]}], "introductionContent": [{"text": "Recent work on natural language understanding has mainly focused on shallow semantic analysis, such as semantic role labeling and word-sense disambiguation.", "labels": [], "entities": [{"text": "natural language understanding", "start_pos": 15, "end_pos": 45, "type": "TASK", "confidence": 0.6581420600414276}, {"text": "semantic role labeling", "start_pos": 103, "end_pos": 125, "type": "TASK", "confidence": 0.6455769141515096}, {"text": "word-sense disambiguation", "start_pos": 130, "end_pos": 155, "type": "TASK", "confidence": 0.7313187718391418}]}, {"text": "This paper considers a more ambitious task of semantic parsing, which is the construction of a complete, formal, symbolic, meaning representation (MR) of a sentence.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 46, "end_pos": 62, "type": "TASK", "confidence": 0.7334643453359604}, {"text": "construction of a complete, formal, symbolic, meaning representation (MR) of a sentence", "start_pos": 77, "end_pos": 164, "type": "TASK", "confidence": 0.7579170763492584}]}, {"text": "Semantic parsing has found its way in practical applications such as natural-language (NL) interfaces to databases and advice taking ().", "labels": [], "entities": [{"text": "Semantic parsing", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8155717849731445}, {"text": "advice taking", "start_pos": 119, "end_pos": 132, "type": "TASK", "confidence": 0.7836804986000061}]}, {"text": "shows a sample MR written in a meaning-representation language (MRL) called CLANG, which is used for (do our {6} (pos (left (half our))))) If our player 4 has the ball, then our player 6 should stay in the left side of our half.", "labels": [], "entities": []}, {"text": "encoding coach advice given to simulated soccerplaying agents ().", "labels": [], "entities": []}, {"text": "Prior research in semantic parsing has mainly focused on relatively simple domains such as ATIS (Air Travel Information Service) (), in which a typcial MR is only a single semantic frame.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 18, "end_pos": 34, "type": "TASK", "confidence": 0.8225374221801758}, {"text": "ATIS (Air Travel Information Service)", "start_pos": 91, "end_pos": 128, "type": "DATASET", "confidence": 0.6837072329861777}]}, {"text": "Learning methods have been devised that can generate MRs with a complex, nested structure (cf.).", "labels": [], "entities": [{"text": "MRs", "start_pos": 53, "end_pos": 56, "type": "TASK", "confidence": 0.9636924266815186}]}, {"text": "However, these methods are mostly based on deterministic parsing (), which lack the robustness that characterizes recent advances in statistical NLP.", "labels": [], "entities": [{"text": "deterministic parsing", "start_pos": 43, "end_pos": 64, "type": "TASK", "confidence": 0.6334248483181}]}, {"text": "Other learning methods involve the use of fullyannotated augmented parse trees) or prior knowledge of the NL syntax) in training, and hence require extensive human efforts when porting to anew domain or language.", "labels": [], "entities": []}, {"text": "In this paper, we present a novel statistical approach to semantic parsing which can handle MRs with a nested structure, based on previous work on semantic parsing using transformation rules ().", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 58, "end_pos": 74, "type": "TASK", "confidence": 0.8281658589839935}, {"text": "semantic parsing", "start_pos": 147, "end_pos": 163, "type": "TASK", "confidence": 0.7360501289367676}]}, {"text": "The algorithm learns a semantic parser given a set of NL sentences annotated with their correct MRs.", "labels": [], "entities": []}, {"text": "It requires no prior knowledge of the NL syntax, although it assumes that an unambiguous, context-free grammar (CFG) of the target MRL is available.", "labels": [], "entities": []}, {"text": "The main innovation of this al-answer(count(city(loc 2(countryid(usa))))) How many cities are therein the US?: A meaning representation in GEOQUERY gorithm is its integration with state-of-the-art statistical machine translation techniques.", "labels": [], "entities": [{"text": "GEOQUERY gorithm", "start_pos": 139, "end_pos": 155, "type": "DATASET", "confidence": 0.7753177881240845}, {"text": "statistical machine translation", "start_pos": 197, "end_pos": 228, "type": "TASK", "confidence": 0.6548742453257242}]}, {"text": "More specifically, a statistical word alignment model) is used to acquire a bilingual lexicon consisting of NL substrings coupled with their translations in the target MRL.", "labels": [], "entities": [{"text": "statistical word alignment", "start_pos": 21, "end_pos": 47, "type": "TASK", "confidence": 0.6228499213854471}]}, {"text": "Complete MRs are then formed by combining these NL substrings and their translations under a parsing framework called the synchronous CFG (, which forms the basis of most existing statistical syntax-based translation models).", "labels": [], "entities": []}, {"text": "Our algorithm is called WASP, short for Word Alignment-based Semantic Parsing.", "labels": [], "entities": [{"text": "Word Alignment-based Semantic Parsing", "start_pos": 40, "end_pos": 77, "type": "TASK", "confidence": 0.8143703117966652}]}, {"text": "In initial evaluation on several real-world data sets, we show that WASP performs favorably in terms of both accuracy and coverage compared to existing learning methods requiring the same amount of supervision, and shows better robustness to variations in task complexity and word order.", "labels": [], "entities": [{"text": "WASP", "start_pos": 68, "end_pos": 72, "type": "TASK", "confidence": 0.955634355545044}, {"text": "accuracy", "start_pos": 109, "end_pos": 117, "type": "METRIC", "confidence": 0.9992231130599976}, {"text": "coverage", "start_pos": 122, "end_pos": 130, "type": "METRIC", "confidence": 0.9236735105514526}]}, {"text": "Section 2 provides a brief overview of the domains being considered.", "labels": [], "entities": []}, {"text": "In Section 3, we present the semantic parsing model of WASP.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 29, "end_pos": 45, "type": "TASK", "confidence": 0.7179738432168961}, {"text": "WASP", "start_pos": 55, "end_pos": 59, "type": "TASK", "confidence": 0.9273321032524109}]}, {"text": "Section 4 outlines the algorithm for acquiring a bilingual lexicon through the use of word alignments.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 86, "end_pos": 101, "type": "TASK", "confidence": 0.714902713894844}]}, {"text": "Section 5 describes a probabilistic model for semantic parsing.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 46, "end_pos": 62, "type": "TASK", "confidence": 0.8656658232212067}]}, {"text": "Finally, we report on experiments that show the robustness of WASP in Section 6, followed by the conclusion in Section 7.", "labels": [], "entities": [{"text": "WASP", "start_pos": 62, "end_pos": 66, "type": "TASK", "confidence": 0.8960874676704407}]}], "datasetContent": [{"text": "We evaluated WASP in the ROBOCUP and GEO-QUERY domains (see Section 2).", "labels": [], "entities": [{"text": "WASP", "start_pos": 13, "end_pos": 17, "type": "TASK", "confidence": 0.9147968888282776}, {"text": "ROBOCUP", "start_pos": 25, "end_pos": 32, "type": "METRIC", "confidence": 0.5648762583732605}, {"text": "GEO-QUERY", "start_pos": 37, "end_pos": 46, "type": "DATASET", "confidence": 0.9084869623184204}]}, {"text": "To build a corpus for ROBOCUP, 300 pieces of coach advice were randomly selected from the log files of the 2003 ROBOCUP Coach Competition, which were manually translated into English ().", "labels": [], "entities": [{"text": "ROBOCUP Coach Competition", "start_pos": 112, "end_pos": 137, "type": "DATASET", "confidence": 0.6939074794451395}]}, {"text": "The average sentence length is 22.52.", "labels": [], "entities": []}, {"text": "To build a corpus for GEOQUERY, 880 English questions were gathered from various sources, which were manually translated into the functional GEOQUERY language).", "labels": [], "entities": [{"text": "GEOQUERY", "start_pos": 22, "end_pos": 30, "type": "DATASET", "confidence": 0.914505124092102}]}, {"text": "The average sentence length is 7.48, much shorter than ROBOCUP.", "labels": [], "entities": [{"text": "ROBOCUP", "start_pos": 55, "end_pos": 62, "type": "METRIC", "confidence": 0.571723997592926}]}, {"text": "250 of the queries were also translated into Spanish, Japanese and Turkish, resulting in a smaller, multilingual data set.", "labels": [], "entities": []}, {"text": "For each domain, there was a minimal set of initial rules representing knowledge needed for translating basic domain entities.", "labels": [], "entities": []}, {"text": "These rules were always included in a lexicon.", "labels": [], "entities": []}, {"text": "For example, in GEO-QUERY, the initial rules were: NUM \u2192 x, x, for all x \u2208 \u00ca; CITY \u2192 c, cityid('c', ), for all city names c (e.g. new york); and similar rules for other types of names (e.g. rivers).", "labels": [], "entities": [{"text": "GEO-QUERY", "start_pos": 16, "end_pos": 25, "type": "DATASET", "confidence": 0.8810083270072937}]}, {"text": "Name translations were provided for the multilingual data set (e.g. CITY \u2192 nyuu yooku, cityid('new york', ) for Japanese).", "labels": [], "entities": []}, {"text": "Standard 10-fold cross validation was used in our experiments.", "labels": [], "entities": []}, {"text": "A semantic parser was learned from the training set.", "labels": [], "entities": []}, {"text": "Then the learned parser was used to translate the test sentences into MRs.", "labels": [], "entities": [{"text": "MRs", "start_pos": 70, "end_pos": 73, "type": "METRIC", "confidence": 0.6477794051170349}]}, {"text": "Translation failed when there were constructs that the parser did not cover.", "labels": [], "entities": [{"text": "Translation", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.9610267877578735}]}, {"text": "We counted the number of sentences that were translated into an MR, and the number of translations that were correct.", "labels": [], "entities": []}, {"text": "For ROBOCUP, a translation was correct if it exactly matched the correct MR.", "labels": [], "entities": [{"text": "ROBOCUP", "start_pos": 4, "end_pos": 11, "type": "METRIC", "confidence": 0.42828652262687683}, {"text": "MR", "start_pos": 73, "end_pos": 75, "type": "METRIC", "confidence": 0.9270554780960083}]}, {"text": "For GEOQUERY, a translation was correct if it retrieved the same answer as the correct query.", "labels": [], "entities": [{"text": "GEOQUERY", "start_pos": 4, "end_pos": 12, "type": "DATASET", "confidence": 0.7020235657691956}]}, {"text": "Using these counts, we measured the performance of the parser in terms of precision (percentage of translations that were correct) and recall (percentage of test sentences that were correctly translated).", "labels": [], "entities": [{"text": "precision", "start_pos": 74, "end_pos": 83, "type": "METRIC", "confidence": 0.9994581341743469}, {"text": "recall", "start_pos": 135, "end_pos": 141, "type": "METRIC", "confidence": 0.9995856881141663}]}, {"text": "For ROBOCUP, it took 47 minutes to learn a parser using IIS.", "labels": [], "entities": [{"text": "ROBOCUP", "start_pos": 4, "end_pos": 11, "type": "TASK", "confidence": 0.5018453598022461}, {"text": "IIS", "start_pos": 56, "end_pos": 59, "type": "DATASET", "confidence": 0.8375349044799805}]}, {"text": "For GEOQUERY, it took 83 minutes.", "labels": [], "entities": [{"text": "GEOQUERY", "start_pos": 4, "end_pos": 12, "type": "TASK", "confidence": 0.3714214265346527}]}, {"text": "shows the performance of WASP compared to four other algorithms: SILT (), COCKTAIL), SCIS-SOR (Ge and ) and.", "labels": [], "entities": [{"text": "WASP", "start_pos": 25, "end_pos": 29, "type": "TASK", "confidence": 0.8383857607841492}]}, {"text": "Experimental results clearly show the advantage of extra supervision in SCISSOR and Zettlemoyer and Collins's parser (see Section 1).", "labels": [], "entities": []}, {"text": "However, WASP performs quite favorably compared to SILT and COCKTAIL, which use the same training data.", "labels": [], "entities": [{"text": "WASP", "start_pos": 9, "end_pos": 13, "type": "TASK", "confidence": 0.9594779014587402}, {"text": "COCKTAIL", "start_pos": 60, "end_pos": 68, "type": "DATASET", "confidence": 0.8320534229278564}]}, {"text": "In particular, COCKTAIL, a deterministic shift-reduce parser based on inductive logic programming, fails to scale up to the ROBOCUP domain where sentences are much longer, and crashes on larger training sets due to memory overflow.", "labels": [], "entities": []}, {"text": "WASP also outperforms SILT in terms of recall, where lexical learning is done by a local bottom-up search, which is much less effective than the wordalignment-based algorithm in WASP.", "labels": [], "entities": [{"text": "WASP", "start_pos": 0, "end_pos": 4, "type": "TASK", "confidence": 0.6910685896873474}, {"text": "recall", "start_pos": 39, "end_pos": 45, "type": "METRIC", "confidence": 0.9956623911857605}]}, {"text": "shows the performance of WASP on the multilingual GEOQUERY data set.", "labels": [], "entities": [{"text": "WASP", "start_pos": 25, "end_pos": 29, "type": "TASK", "confidence": 0.8226791024208069}, {"text": "GEOQUERY data set", "start_pos": 50, "end_pos": 67, "type": "DATASET", "confidence": 0.9677601854006449}]}, {"text": "The languages being considered differ in terms of word order: Subject-Verb-Object for English and Spanish, and Subject-Object-Verb for Japanese and Turkish.", "labels": [], "entities": []}, {"text": "WASP's performance is consistent across these languages despite some slight differences, most probably due to factors other than word order (e.g. lower recall for Turkish due to a much larger vocabulary).", "labels": [], "entities": [{"text": "WASP", "start_pos": 0, "end_pos": 4, "type": "TASK", "confidence": 0.9481357336044312}, {"text": "recall", "start_pos": 152, "end_pos": 158, "type": "METRIC", "confidence": 0.9987826943397522}]}, {"text": "Details can be found in a longer version of this paper ().", "labels": [], "entities": []}], "tableCaptions": []}