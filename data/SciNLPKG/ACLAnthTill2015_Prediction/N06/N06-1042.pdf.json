{"title": [{"text": "Learning Morphological Disambiguation Rules for Turkish", "labels": [], "entities": [{"text": "Learning Morphological Disambiguation", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.6963881452878317}]}], "abstractContent": [{"text": "In this paper, we present a rule based model for morphological disambiguation of Turkish.", "labels": [], "entities": [{"text": "morphological disambiguation of Turkish", "start_pos": 49, "end_pos": 88, "type": "TASK", "confidence": 0.7760913670063019}]}, {"text": "The rules are generated by a novel decision list learning algorithm using supervised training.", "labels": [], "entities": []}, {"text": "Morphological ambiguity (e.g. lives = live+s or life+s) is a challenging problem for agglutinative languages like Turkish where close to half of the words in running text are morphologically ambiguous.", "labels": [], "entities": []}, {"text": "Furthermore, it is possible fora word to take an unlimited number of suffixes, therefore the number of possible morphological tags is unlimited.", "labels": [], "entities": []}, {"text": "We attempted to cope with these problems by training a separate model for each of the 126 morphological features recognized by the morphological analyzer.", "labels": [], "entities": []}, {"text": "The resulting decision lists independently vote on each of the potential parses of a word and the final parse is selected based on our confidence on these votes.", "labels": [], "entities": []}, {"text": "The accuracy of our model (96%) is slightly above the best previously reported results which use statistical models.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9997934699058533}]}, {"text": "For comparison , when we train a single decision list on full tags instead of using separate models on each feature we get 91% accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 127, "end_pos": 135, "type": "METRIC", "confidence": 0.9990791082382202}]}], "introductionContent": [{"text": "Morphological disambiguation is the task of selecting the correct morphological parse fora given word in a given context.", "labels": [], "entities": [{"text": "Morphological disambiguation", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.9158738851547241}]}, {"text": "The possible parses of a word are generated by a morphological analyzer.", "labels": [], "entities": []}, {"text": "In Turkish, close to half the words in running text are morphologically ambiguous.", "labels": [], "entities": []}, {"text": "Below is atypical word \"masal\u0131\" with three possible parses.", "labels": [], "entities": []}, {"text": "masal+Noun+A3sg+Pnon+Acc (= the story) masal+Noun+A3sg+P3sg+Nom (= his story) masa+Noun+A3sg+Pnon+Nom\u02c6DB+Adj+With (= with tables): Three parses of the word \"masal\u0131\" The first two parses start with the same root, masal (= story, fable), but the interpretation of the following +\u0131 suffix is the Accusative marker in one case, and third person possessive agreement in the other.", "labels": [], "entities": [{"text": "Accusative", "start_pos": 293, "end_pos": 303, "type": "METRIC", "confidence": 0.986236572265625}]}, {"text": "The third parse starts with a different root, masa (= table) followed by a derivational suffix +l\u0131 (= with) which turns the noun into an adjective.", "labels": [], "entities": []}, {"text": "The symbo\u00eeDB represents a derivational boundary and splits the parse into chunks called inflectional groups (IGs).", "labels": [], "entities": []}, {"text": "We will use the term feature to refer to individual morphological features like +Acc and +With; the term IG to refer to groups of features split by derivational boundaries (\u02c6DB), and the term tag to refer to the sequence of IGs following the root.", "labels": [], "entities": []}, {"text": "Morphological disambiguation is a useful first step for higher level analysis of any language but it is especially critical for agglutinative languages like Turkish, Czech, Hungarian, and Finnish.", "labels": [], "entities": [{"text": "Morphological disambiguation", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8663991987705231}]}, {"text": "These languages have a relatively free constituent order, and syntactic relations are partly determined by morphological features.", "labels": [], "entities": []}, {"text": "Many applications including syntactic parsing, word sense disambiguation, text to speech synthesis and spelling correction depend on accurate analyses of words.", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 28, "end_pos": 45, "type": "TASK", "confidence": 0.7857730984687805}, {"text": "word sense disambiguation", "start_pos": 47, "end_pos": 72, "type": "TASK", "confidence": 0.7143877744674683}, {"text": "text to speech synthesis", "start_pos": 74, "end_pos": 98, "type": "TASK", "confidence": 0.6256841868162155}, {"text": "spelling correction", "start_pos": 103, "end_pos": 122, "type": "TASK", "confidence": 0.8479816913604736}]}, {"text": "An important qualitative difference between part of speech tagging in English and morphological disambiguation in an agglutinative language like Turkish is the number of possible tags that can be assigned to a word.", "labels": [], "entities": [{"text": "speech tagging", "start_pos": 52, "end_pos": 66, "type": "TASK", "confidence": 0.7208832204341888}]}, {"text": "Typical English tag sets include less than a hundred tag types representing syntactic and morphological information.", "labels": [], "entities": []}, {"text": "The number of potential morphological tags in Turkish is theoretically unlimited.", "labels": [], "entities": []}, {"text": "We have observed more than ten thousand tag types in our training corpus of a million words.", "labels": [], "entities": []}, {"text": "The high number of possible tags poses a data sparseness challenge for the typical machine learning approach, somewhat akin to what we observe in word sense disambiguation.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 146, "end_pos": 171, "type": "TASK", "confidence": 0.6256880660851797}]}, {"text": "One way out of this dilemma could be to ignore the detailed morphological structure of the word and focus on determining only the major and minor parts of speech.", "labels": [], "entities": []}, {"text": "However () observes that the modifier words in Turkish can have dependencies to anyone of the inflectional groups of a derived word.", "labels": [], "entities": []}, {"text": "For example, in \"mavi masal\u0131 oda\" (= the room with a blue table) the adjective \"mavi\" (= blue) modifies the noun root \"masa\" (= table) even though the final part of speech of \"masal\u0131\" is an adjective.", "labels": [], "entities": []}, {"text": "Therefore, the final part of speech and inflection of a word do not carry sufficient information for the identification of the syntactic dependencies it is involved in.", "labels": [], "entities": []}, {"text": "One needs the full morphological analysis.", "labels": [], "entities": []}, {"text": "Our approach to the data sparseness problem is to consider each morphological feature separately.", "labels": [], "entities": []}, {"text": "Even though the number of potential tags is unlimited, the number of morphological features is small: The Turkish morphological analyzer we use produces tags that consist of 126 unique features.", "labels": [], "entities": [{"text": "Turkish morphological analyzer", "start_pos": 106, "end_pos": 136, "type": "TASK", "confidence": 0.6530067324638367}]}, {"text": "For each unique feature f , we take the subset of the training data in which one of the parses for each instance contain f . We then split this subset into positive and negative examples depending on whether the correct parse contains the feature f . These examples are used to learn rules using the Greedy Prepend Algorithm (GPA), a novel decision list learner.", "labels": [], "entities": []}, {"text": "To predict the tag of an unknown word, first the morphological analyzer is used to generate all its possible parses.", "labels": [], "entities": []}, {"text": "The decision lists are then used to predict the presence or absence of each of the features contained in the candidate parses.", "labels": [], "entities": []}, {"text": "The results are probabilistically combined taking into account the accuracy of each decision list to select the best parse.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 67, "end_pos": 75, "type": "METRIC", "confidence": 0.9991378784179688}]}, {"text": "The resulting tagging accuracy is 96% on a hand tagged test set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.971847653388977}]}, {"text": "A more direct approach would be to train a single decision list using the full tags as the target classification.", "labels": [], "entities": []}, {"text": "Given a word in context, such a decision list assigns a complete morphological tag instead of predicting individual morphological features.", "labels": [], "entities": []}, {"text": "As such, it does not need the output of a morphological analyzer and should be considered a tagger rather than a disambiguator.", "labels": [], "entities": []}, {"text": "For comparison, such a decision list was built, and its accuracy was determined to be 91% on the same test set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.9996464252471924}]}, {"text": "The main reason we chose to work with decision lists and the GPA algorithm is their robustness to irrelevant or redundant features.", "labels": [], "entities": []}, {"text": "The input to the decision lists include the suffixes of all possible lengths and character type information within a five word window.", "labels": [], "entities": []}, {"text": "Each instance ends up with 40 attributes on average which are highly redundant and mostly irrelevant.", "labels": [], "entities": []}, {"text": "GPA is able to sort out the relevant features automatically and build a fairly accurate model.", "labels": [], "entities": [{"text": "GPA", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9490801095962524}]}, {"text": "Our experiments with Naive Bayes resulted in a significantly worse performance.", "labels": [], "entities": []}, {"text": "Typical statistical approaches include the tags of the previous words as inputs in the model.", "labels": [], "entities": []}, {"text": "GPA was able to deliver good performance without using the previous tags as inputs, because it was able to extract equivalent information implicit in the surface attributes.", "labels": [], "entities": [{"text": "GPA", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9468861222267151}]}, {"text": "Finally, unlike most statistical approaches, the resulting models of GPA are human readable and open to interpretation as Section 3.1 illustrates.", "labels": [], "entities": [{"text": "GPA", "start_pos": 69, "end_pos": 72, "type": "TASK", "confidence": 0.9252843260765076}]}, {"text": "The next section will review related work.", "labels": [], "entities": []}, {"text": "Section 3 introduces decision lists and the GPA training algorithm.", "labels": [], "entities": [{"text": "GPA training", "start_pos": 44, "end_pos": 56, "type": "TASK", "confidence": 0.6777324378490448}]}, {"text": "Section 4 presents the experiments and the results.", "labels": [], "entities": []}, {"text": "rule-based approach a large number of hand crafted rules are used to select the correct morphological parse or POS tag of a given word in a given context ().", "labels": [], "entities": []}, {"text": "In the statistical approach a hand tagged corpus is used to train a probabilistic model which is then used to select the best tags in unseen text).", "labels": [], "entities": []}, {"text": "Examples of statistical and machine learning approaches that have been used for tagging include transformation based learning, memory based learning, and maximum entropy models.", "labels": [], "entities": [{"text": "tagging", "start_pos": 80, "end_pos": 87, "type": "TASK", "confidence": 0.9701980352401733}]}, {"text": "It is also possible to train statistical models using unlabeled data with the expectation maximization algorithm (.", "labels": [], "entities": []}, {"text": "Van Halteren (1999) gives a comprehensive overview of syntactic word-class tagging.", "labels": [], "entities": [{"text": "syntactic word-class tagging", "start_pos": 54, "end_pos": 82, "type": "TASK", "confidence": 0.6110134720802307}]}, {"text": "Previous work on morphological disambiguation of inflectional or agglutinative languages include unsupervised learning for of Hebrew (, maximum entropy modeling for Czech), combination of statistical and rule-based disambiguation methods for Basque (, transformation based tagging for Hungarian.", "labels": [], "entities": []}, {"text": "Early work on Turkish used a constraint-based approach with hand crafted rules.", "labels": [], "entities": []}, {"text": "A purely statistical morphological disambiguation model was recently introduced).", "labels": [], "entities": []}, {"text": "To counter the data sparseness problem the morphological parses are split across their derivational boundaries and certain independence assumptions are made in the prediction of each inflectional group.", "labels": [], "entities": []}, {"text": "A combination of three ideas makes our approach unique in the field: (1) the use of decision lists and a novel learning algorithm that combine the statistical and rule based techniques, (2) the treatment of each individual feature separately to address the data sparseness problem, and (3) the lack of dependence on previous tags and relying on surface attributes alone.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we present the details of the data, the training and testing procedures, the surface attributes used, and the accuracy results.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 126, "end_pos": 134, "type": "METRIC", "confidence": 0.9993347525596619}]}, {"text": "Our training data consists of about 1 million words of semi-automatically disambiguated Turkish news text.", "labels": [], "entities": []}, {"text": "For each one of the 126 unique morphological features, we used the subset of the training data in which instances have the given feature in at least one of their generated parses.", "labels": [], "entities": []}, {"text": "We then split this subset into positive and negative examples depending on whether the correct parse contains the given feature.", "labels": [], "entities": []}, {"text": "A decision list specific to that feature is created using GPA based on these examples.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: A five rule decision list for +Det", "labels": [], "entities": []}]}