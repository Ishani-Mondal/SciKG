{"title": [{"text": "Summarizing Speech Without Text Using Hidden Markov Models", "labels": [], "entities": [{"text": "Summarizing Speech Without Text", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.9032680690288544}]}], "abstractContent": [{"text": "We present a method for summarizing speech documents without using any type of transcript/text in a Hidden Markov Model framework.", "labels": [], "entities": [{"text": "summarizing speech documents", "start_pos": 24, "end_pos": 52, "type": "TASK", "confidence": 0.9132192532221476}]}, {"text": "The hidden variables or states in the model represent whether a sentence is to be included in a summary or not, and the acoustic/prosodic features are the observation vectors.", "labels": [], "entities": []}, {"text": "The model predicts the optimal sequence of segments that best summarize the document.", "labels": [], "entities": [{"text": "summarize the document", "start_pos": 62, "end_pos": 84, "type": "TASK", "confidence": 0.8540363907814026}]}, {"text": "We evaluate our method by comparing the predicted summary with one generated by a human summarizer.", "labels": [], "entities": []}, {"text": "Our results indicate that we can generate 'good' summaries even when using only acous-tic/prosodic information, which points toward the possibility of text-independent summarization for spoken documents.", "labels": [], "entities": []}], "introductionContent": [{"text": "The goal of single document text or speech summarization is to identify information from a text or spoken document that summarizes, or conveys the essence of a document.", "labels": [], "entities": [{"text": "single document text or speech summarization", "start_pos": 12, "end_pos": 56, "type": "TASK", "confidence": 0.5912481447060903}]}, {"text": "EXTRACTIVE SUM-MARIZATION identifies portions of the original document and concatenates these segments to form a summary.", "labels": [], "entities": []}, {"text": "How these segments are selected is thus critical to the summarization adequacy.", "labels": [], "entities": [{"text": "summarization", "start_pos": 56, "end_pos": 69, "type": "TASK", "confidence": 0.9552461504936218}]}, {"text": "Many classifier-based methods have been examined for extractive summarization of text and of speech.", "labels": [], "entities": [{"text": "extractive summarization of text", "start_pos": 53, "end_pos": 85, "type": "TASK", "confidence": 0.8505670726299286}]}, {"text": "These approaches attempt to classify segments as to whether they should or should not be included in a summary.", "labels": [], "entities": []}, {"text": "However, the classifiers used in these methods implicitly assume that the posterior probability for the inclusion of a sentence in the summary is only dependent on the observations for that sentence, and is not affected by previous decisions.", "labels": [], "entities": []}, {"text": "Some of these) also assume that the features themselves are independent.", "labels": [], "entities": []}, {"text": "Such an independence assumption simplifies the training procedure of the models, but it does not appear to model the factors human beings appear to use in generating summaries.", "labels": [], "entities": []}, {"text": "In particular, human summarizers seem to take previous decisions into account when deciding if a sentence in the source document should be in the document's summary.", "labels": [], "entities": []}, {"text": "In this paper, we examine a Hidden Markov Model (HMM) approach to the selection of segments to be included in a summary that we believe better models the interaction between extracted segments and their features, for the domain of Broadcast News (BN).", "labels": [], "entities": [{"text": "Broadcast News (BN)", "start_pos": 231, "end_pos": 250, "type": "DATASET", "confidence": 0.90303555727005}]}, {"text": "In Section 2 we describe related work on the use of HMMs in summarization.", "labels": [], "entities": [{"text": "summarization", "start_pos": 60, "end_pos": 73, "type": "TASK", "confidence": 0.9796199202537537}]}, {"text": "We present our own approach in Section 3 and discuss our results in Section 3.1.", "labels": [], "entities": []}, {"text": "We conclude in Section 5 and discuss future research.", "labels": [], "entities": []}], "datasetContent": [{"text": "We tested our resulting model on a held-out test set of 19 stories.", "labels": [], "entities": []}, {"text": "For each sentence in the test set we extracted the 12 acoustic/prosodic features.", "labels": [], "entities": []}, {"text": "We built a 12XN matrix using these features for N sentences in the story where N was the total length of the story.", "labels": [], "entities": []}, {"text": "We then computed the optimal sequence of sentences to include in the summary by decoding our sentence state lattice using the Viterbi algorithm.", "labels": [], "entities": []}, {"text": "For all the even states in this sequence we extracted the corresponding segments and concatenated them to produce the summary.", "labels": [], "entities": []}, {"text": "Evaluating summarizers is a difficult problem, since there is great disagreement between humans over what should be included in a summary.", "labels": [], "entities": [{"text": "Evaluating summarizers", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.832573413848877}]}, {"text": "Speech summaries are even harder to evaluate because most objective evaluation metrics are based on word overlap.", "labels": [], "entities": [{"text": "Speech summaries", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.6382840424776077}]}, {"text": "The metric we will use here is the standard information retrieval measure of Precision, Recall and F-measure on sentences.", "labels": [], "entities": [{"text": "Precision", "start_pos": 77, "end_pos": 86, "type": "METRIC", "confidence": 0.9875983595848083}, {"text": "Recall", "start_pos": 88, "end_pos": 94, "type": "METRIC", "confidence": 0.9241464734077454}, {"text": "F-measure", "start_pos": 99, "end_pos": 108, "type": "METRIC", "confidence": 0.9830422401428223}]}, {"text": "This is a strict metric, since it requires exact matching with sentences in the human summary; we are penalized if we identify sentences similar in meaning but not identical to the gold standard.", "labels": [], "entities": []}, {"text": "We first computed the F-measure of a baseline system which randomly extracts sentences for the summary; this method produces an F-measure of 0.24.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 22, "end_pos": 31, "type": "METRIC", "confidence": 0.9887567162513733}, {"text": "F-measure", "start_pos": 128, "end_pos": 137, "type": "METRIC", "confidence": 0.9900318384170532}]}, {"text": "To determine whether the positional information captured in our position-sensitive HMM model was useful, we first built a 2-state HMM that models only inclusion/exclusion of sentences from a summary, without modeling sentence position in the document.", "labels": [], "entities": []}, {"text": "We trained this HMM on the train-ing corpus described above.", "labels": [], "entities": []}, {"text": "We then trained a position-sensitive HMM by first discretizing position into 4 bins, such that each bin includes onequarter of the sentences in the story.", "labels": [], "entities": []}, {"text": "We built an 8-state HMM that captures this positional information.", "labels": [], "entities": []}, {"text": "We tested both on our held-out test set.", "labels": [], "entities": []}, {"text": "Note that recall for the 8-state position-sensitive HMM is 16% better than recall for the 2-state HMM, although precision for the 2-state model is slightly (1%) better than for the 8-state model.", "labels": [], "entities": [{"text": "recall", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9996217489242554}, {"text": "recall", "start_pos": 75, "end_pos": 81, "type": "METRIC", "confidence": 0.9989499449729919}, {"text": "precision", "start_pos": 112, "end_pos": 121, "type": "METRIC", "confidence": 0.9994308352470398}]}, {"text": "The F-measure for the 8-state position-sensitive model represents a slight improvement over the 2-state model, of 1%.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9989312291145325}]}, {"text": "These results are encouraging, since, in skewed datasets like documents with their summaries, only a few sentences from a document are usually included in the summary; thus, recall is generally more important than precision in extractive summarization.", "labels": [], "entities": [{"text": "recall", "start_pos": 174, "end_pos": 180, "type": "METRIC", "confidence": 0.9978832602500916}, {"text": "precision", "start_pos": 214, "end_pos": 223, "type": "METRIC", "confidence": 0.9983654618263245}]}, {"text": "And, compared to the baseline, the position-sensitive 8-state HMM obtains an F-measure of 0.41, which is 17% higher than the baseline.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 77, "end_pos": 86, "type": "METRIC", "confidence": 0.9994120597839355}]}], "tableCaptions": [{"text": " Table 1. Note that recall for  the 8-state position-sensitive HMM is 16% better  than recall for the 2-state HMM, although precision  for the 2-state model is slightly (1%) better than  for the 8-state model. The F-measure for the 8- state position-sensitive model represents a slight im- provement over the 2-state model, of 1%. These re- sults are encouraging, since, in skewed datasets like  documents with their summaries, only a few sen- tences from a document are usually included in the  summary; thus, recall is generally more important  than precision in extractive summarization. And,  compared to the baseline, the position-sensitive 8- state HMM obtains an F-measure of 0.41, which is  17% higher than the baseline.", "labels": [], "entities": [{"text": "recall", "start_pos": 20, "end_pos": 26, "type": "METRIC", "confidence": 0.999249279499054}, {"text": "recall", "start_pos": 87, "end_pos": 93, "type": "METRIC", "confidence": 0.9977282881736755}, {"text": "precision", "start_pos": 124, "end_pos": 133, "type": "METRIC", "confidence": 0.9989902377128601}, {"text": "F-measure", "start_pos": 214, "end_pos": 223, "type": "METRIC", "confidence": 0.9975340366363525}, {"text": "im- provement", "start_pos": 286, "end_pos": 299, "type": "METRIC", "confidence": 0.8315383394559225}, {"text": "re-", "start_pos": 337, "end_pos": 340, "type": "METRIC", "confidence": 0.954715371131897}, {"text": "recall", "start_pos": 511, "end_pos": 517, "type": "METRIC", "confidence": 0.9990565180778503}, {"text": "precision", "start_pos": 552, "end_pos": 561, "type": "METRIC", "confidence": 0.9968846440315247}, {"text": "F-measure", "start_pos": 670, "end_pos": 679, "type": "METRIC", "confidence": 0.9976717829704285}]}, {"text": " Table 1: Speech Summarization Results", "labels": [], "entities": [{"text": "Speech Summarization", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.8158273696899414}]}]}