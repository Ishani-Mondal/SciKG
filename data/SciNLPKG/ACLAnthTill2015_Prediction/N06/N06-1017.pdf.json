{"title": [{"text": "Unknown word sense detection as outlier detection", "labels": [], "entities": [{"text": "Unknown word sense detection", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.6658089905977249}]}], "abstractContent": [{"text": "We address the problem of unknown word sense detection: the identification of corpus occurrences that are not covered by a given sense inventory.", "labels": [], "entities": [{"text": "unknown word sense detection", "start_pos": 26, "end_pos": 54, "type": "TASK", "confidence": 0.6606980338692665}]}, {"text": "We model this as an instance of outlier detection, using a simple nearest neighbor-based approach to measuring the resemblance of anew item to a training set.", "labels": [], "entities": [{"text": "outlier detection", "start_pos": 32, "end_pos": 49, "type": "TASK", "confidence": 0.7333478778600693}]}, {"text": "In combination with a method that alleviates data sparseness by sharing training data across lemmas, the approach achieves a precision of 0.77 and recall of 0.82.", "labels": [], "entities": [{"text": "precision", "start_pos": 125, "end_pos": 134, "type": "METRIC", "confidence": 0.9991115927696228}, {"text": "recall", "start_pos": 147, "end_pos": 153, "type": "METRIC", "confidence": 0.999716579914093}]}], "introductionContent": [{"text": "If a system has seen only positive examples, how does it recognize a negative example?", "labels": [], "entities": []}, {"text": "This is the problem addressed by outlier detection, also called novelty detection 1 (: to detect novel or unknown items that differ from all the seen training data.", "labels": [], "entities": [{"text": "outlier detection", "start_pos": 33, "end_pos": 50, "type": "TASK", "confidence": 0.6925663203001022}, {"text": "novelty detection", "start_pos": 64, "end_pos": 81, "type": "TASK", "confidence": 0.7779116332530975}]}, {"text": "Outlier detection approaches typically derive some model of \"normal\" objects from the training set and use a distance measure and a threshold to detect abnormal items.", "labels": [], "entities": [{"text": "Outlier detection", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8672901093959808}]}, {"text": "In this paper, we apply outlier detection techniques to the task of unknown sense detection: the identification of corpus occurrences that are not covered by a given sense inventory.", "labels": [], "entities": [{"text": "outlier detection", "start_pos": 24, "end_pos": 41, "type": "TASK", "confidence": 0.7318932712078094}, {"text": "unknown sense detection", "start_pos": 68, "end_pos": 91, "type": "TASK", "confidence": 0.638739675283432}]}, {"text": "The training set The term novelty detection is also used for the distinction of novel and repeated information in information retrieval, a different if related topic.", "labels": [], "entities": [{"text": "novelty detection", "start_pos": 26, "end_pos": 43, "type": "TASK", "confidence": 0.716554656624794}]}, {"text": "Unknown sense detection is related to word sense disambiguation (WSD) and to word sense discrimination, but differs from both.", "labels": [], "entities": [{"text": "Unknown sense detection", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.7129682898521423}, {"text": "word sense disambiguation (WSD)", "start_pos": 38, "end_pos": 69, "type": "TASK", "confidence": 0.7697925666968027}, {"text": "word sense discrimination", "start_pos": 77, "end_pos": 102, "type": "TASK", "confidence": 0.7486777901649475}]}, {"text": "In WSD all senses are assumed known, and the task is to select one of them, while in unknown sense detection the task is to decide whether a given occurrence matches any of the known senses or none of them, and all training instances, regardless of the sense to which they belong, are modeled as one group of known data.", "labels": [], "entities": [{"text": "WSD", "start_pos": 3, "end_pos": 6, "type": "TASK", "confidence": 0.9602799415588379}, {"text": "unknown sense detection", "start_pos": 85, "end_pos": 108, "type": "TASK", "confidence": 0.6793507734934489}]}, {"text": "Unknown sense detection also differs from word sense discrimination, where no sense inventory is given and the task is to group occurrences into senses.", "labels": [], "entities": [{"text": "Unknown sense detection", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.757219135761261}, {"text": "word sense discrimination", "start_pos": 42, "end_pos": 67, "type": "TASK", "confidence": 0.7411221265792847}]}, {"text": "In unknown sense detection the model respects the given word senses.", "labels": [], "entities": [{"text": "unknown sense detection", "start_pos": 3, "end_pos": 26, "type": "TASK", "confidence": 0.7042075594266256}]}, {"text": "The main motivation for this study comes from shallow semantic parsing, by which we mean a combination of WSD and the automatic assignment of semantic roles to free text.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 54, "end_pos": 70, "type": "TASK", "confidence": 0.7260896265506744}, {"text": "WSD", "start_pos": 106, "end_pos": 109, "type": "METRIC", "confidence": 0.45953086018562317}]}, {"text": "In cases where a sense is missing from the inventory, WSD will wrongly assign one of the existing senses.", "labels": [], "entities": [{"text": "WSD", "start_pos": 54, "end_pos": 57, "type": "TASK", "confidence": 0.8416303396224976}]}, {"text": "shows an example, a sentence from the Hound of the Baskervilles, analyzed by the SHALMANESER) shallow semantic parser.", "labels": [], "entities": [{"text": "Hound of the Baskervilles", "start_pos": 38, "end_pos": 63, "type": "DATASET", "confidence": 0.771838903427124}]}, {"text": "The analysis is based on FrameNet (), a resource that lists senses and semantic roles for English expressions.", "labels": [], "entities": []}, {"text": "FrameNet is lacking a sense of \"expectation\" or \"being mentally prepared\" for the verb prepare, so prepared has been assigned the sense COOKING CREATION, a possible but improbable analysis 2 . Such erroneous labels can be fatal when further processing builds on the results of shallow semantic parsing, e.g. for drawing inferences.", "labels": [], "entities": [{"text": "COOKING CREATION", "start_pos": 136, "end_pos": 152, "type": "METRIC", "confidence": 0.8101258277893066}]}, {"text": "Unknown sense detection can prevent such mistakes.", "labels": [], "entities": [{"text": "Unknown sense detection", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.6663691798845927}]}, {"text": "All sense inventories face the problem of missing senses, either because of their small overall size (as is the case for some non-English WordNets) or when they encounter domain-specific senses.", "labels": [], "entities": []}, {"text": "Our study will be evaluated on FrameNet because of our main aim of improving shallow semantic parsing, but the method we propose is applicable to any sense inventory that has annotated data; in particular, it is also applicable to WordNet.", "labels": [], "entities": [{"text": "FrameNet", "start_pos": 31, "end_pos": 39, "type": "DATASET", "confidence": 0.8320593237876892}, {"text": "shallow semantic parsing", "start_pos": 77, "end_pos": 101, "type": "TASK", "confidence": 0.6743556559085846}, {"text": "WordNet", "start_pos": 231, "end_pos": 238, "type": "DATASET", "confidence": 0.9533056020736694}]}, {"text": "In this paper we model unknown sense detection as outlier detection, using a simple Nearest Neighbor-based method) that compares the local probability density at each test item with that of its nearest training item.", "labels": [], "entities": [{"text": "unknown sense detection", "start_pos": 23, "end_pos": 46, "type": "TASK", "confidence": 0.8094878792762756}, {"text": "outlier detection", "start_pos": 50, "end_pos": 67, "type": "TASK", "confidence": 0.7479154765605927}]}, {"text": "To our knowledge, there exists no other approach to date to the problem of detecting unknown senses.", "labels": [], "entities": []}, {"text": "There are, however, approaches to the complementary problem of determining the closest known sense for unknown words), which can be viewed as the logical next step after unknown sense detection.", "labels": [], "entities": [{"text": "unknown sense detection", "start_pos": 170, "end_pos": 193, "type": "TASK", "confidence": 0.6619477768739065}]}, {"text": "After a brief sketch of FrameNet in Section 2, we describe the experimental setup used throughout this paper in Section 3.", "labels": [], "entities": []}, {"text": "Section 4 tests whether a very simple model suffices for detecting unknown senses: a threshold on confidence scores returned by the SHALMANESER WSD system.", "labels": [], "entities": [{"text": "SHALMANESER WSD", "start_pos": 132, "end_pos": 147, "type": "TASK", "confidence": 0.4986608028411865}]}, {"text": "The result is that recall is much too low.", "labels": [], "entities": [{"text": "recall", "start_pos": 19, "end_pos": 25, "type": "METRIC", "confidence": 0.9996370077133179}]}, {"text": "Section 5 introduces the NN-based outlier detection approach that we use in section 6 for unknown sense detection, with better results than in the first experiment but still low recall.", "labels": [], "entities": [{"text": "NN-based outlier detection", "start_pos": 25, "end_pos": 51, "type": "TASK", "confidence": 0.5989635686079661}, {"text": "unknown sense detection", "start_pos": 90, "end_pos": 113, "type": "TASK", "confidence": 0.737540046374003}, {"text": "recall", "start_pos": 178, "end_pos": 184, "type": "METRIC", "confidence": 0.9984787106513977}]}, {"text": "Section 7 repeats the experiment of section 6 with added training data, making use of the fact that one semantic class in FrameNet typically pertains to several lemmas and achieving a marked improvement in results.", "labels": [], "entities": [{"text": "FrameNet", "start_pos": 122, "end_pos": 130, "type": "DATASET", "confidence": 0.8715958595275879}]}], "datasetContent": [{"text": "To evaluate an unknown sense detection system, we need occurrences that are guaranteed not to belong to any of the seen senses.", "labels": [], "entities": [{"text": "sense detection", "start_pos": 23, "end_pos": 38, "type": "TASK", "confidence": 0.7651864588260651}]}, {"text": "To that end we use sense-annotated data, in our case the FrameNet annotated sentences, simulating unknown senses by designating one sense of each ambiguous lemma as unknown.", "labels": [], "entities": []}, {"text": "All occurrences of that sense are placed in the test set, while occurrences of all other senses are split randomly between training and test set, using 5-fold cross-validation.", "labels": [], "entities": []}, {"text": "We repeat the experiment with each of the senses of an ambiguous lemma playing the part of the unknown sense once.", "labels": [], "entities": []}, {"text": "Viewing each cross-validation run for each unknown sense as a separate experiment, we then report precision and recall averaged over unknown senses and cross-validation runs.", "labels": [], "entities": [{"text": "precision", "start_pos": 98, "end_pos": 107, "type": "METRIC", "confidence": 0.9995438456535339}, {"text": "recall", "start_pos": 112, "end_pos": 118, "type": "METRIC", "confidence": 0.9984242916107178}]}, {"text": "It may seem questionable that in this experimental setup, the unknown sense occurrences of each lemma all belong to the same sense.", "labels": [], "entities": []}, {"text": "However, this does not bias the experiment since none of the models we study take advantage of the shape of the test set in anyway.", "labels": [], "entities": []}, {"text": "Rather, each test item is classified individually, without recourse to the other test items.", "labels": [], "entities": []}, {"text": "All experiments in this paper were performed on the FrameNet 1.2 annotated data pertaining to ambiguous lemmas.", "labels": [], "entities": [{"text": "FrameNet 1.2 annotated data", "start_pos": 52, "end_pos": 79, "type": "DATASET", "confidence": 0.9338636696338654}]}, {"text": "After removal of instances that were annotated with more than one sense, we obtain 26,496 annotated sentences for the 1,031 ambiguous lemmas.", "labels": [], "entities": []}, {"text": "They were parsed with Minipar; named entities were computed using Heart of).", "labels": [], "entities": []}, {"text": "In this section we test a very simple model of unknown sense detection: Classifiers often return a confidence score along with the assigned label.", "labels": [], "entities": [{"text": "unknown sense detection", "start_pos": 47, "end_pos": 70, "type": "TASK", "confidence": 0.7172854940096537}]}, {"text": "We will try to detect unknown senses by a threshold on confidence scores, declaring anything below the threshold as unknown.", "labels": [], "entities": []}, {"text": "Note that this method can only be applied to lemmas that have more than one sense, since for single-sense lemmas the system will always return the maximum confidence score.", "labels": [], "entities": []}, {"text": "While the approach that we follow in this section is applicable to all lemmas with at least two (1): subj, obj, mod (since sand subj corefer, we use only one of them) (2): she, hand, outwards (3): subj-she, obj-hand, mod-outwards (4): mod-obj-subj senses, we need lemmas with at least three senses to evaluate it: One of the senses of each lemma is treated as unknown, which for lemmas with three or more senses leaves at least two senses for the training set.", "labels": [], "entities": []}, {"text": "This reduces our data set to 125 lemmas with 7,435 annotated sentences.", "labels": [], "entities": []}, {"text": "We test whether the WSD system built into SHALMANESER () can distinguish known sense items from unknown sense items reliably by its confidence scores.", "labels": [], "entities": [{"text": "WSD", "start_pos": 20, "end_pos": 23, "type": "TASK", "confidence": 0.908653199672699}]}, {"text": "The system extracts a rich feature set, which forms the basis of all three experiments in this paper: \u2022 a bag-of-words context, with a window size of one sentence; \u2022 bi-and trigrams centered on the target word; \u2022 grammatical function information: for each dependent of the target, (1) its function label, (2) its headword, and (3) a combination of both are used as features.", "labels": [], "entities": []}, {"text": "The concatenation of all function labels constitutes another feature.", "labels": [], "entities": []}, {"text": "For PPs, function labels are extended by the preposition.", "labels": [], "entities": []}, {"text": "As an example, shows a BNC sentence and its grammatical function features.", "labels": [], "entities": []}, {"text": "\u2022 for verb targets, the target voice.", "labels": [], "entities": []}, {"text": "all system parameters were set to their default settings.", "labels": [], "entities": []}, {"text": "To detect unknown senses building on this WSD system, we use a fixed confidence threshold and label all items below the threshold as unknown.", "labels": [], "entities": [{"text": "WSD", "start_pos": 42, "end_pos": 45, "type": "TASK", "confidence": 0.8610725998878479}]}, {"text": "shows precision and recall for labeling instances as unknown using different confidence thresholds \u03b8, averaged over unknown senses and 5-fold cross-validation . We see that while the precision of this method is acceptable at 0.74 to 0.765, recall is extremely low, i.e. almost no items were labeled unknown, even at a threshold of 0.98.", "labels": [], "entities": [{"text": "precision", "start_pos": 6, "end_pos": 15, "type": "METRIC", "confidence": 0.9993205070495605}, {"text": "recall", "start_pos": 20, "end_pos": 26, "type": "METRIC", "confidence": 0.9990991353988647}, {"text": "precision", "start_pos": 183, "end_pos": 192, "type": "METRIC", "confidence": 0.9991852641105652}, {"text": "recall", "start_pos": 240, "end_pos": 246, "type": "METRIC", "confidence": 0.9987213015556335}]}, {"text": "However, SHALMANESER has very high confidence values overall: Only 14.5% of all instances in this study had a confidence value of 0.98 or below (7,697 of 53,206).", "labels": [], "entities": [{"text": "SHALMANESER", "start_pos": 9, "end_pos": 20, "type": "TASK", "confidence": 0.7945393919944763}]}, {"text": "We conclude that with the given WSD system and (rather standard) features, this simple method cannot detect items with an unknown sense reliably.", "labels": [], "entities": [{"text": "WSD", "start_pos": 32, "end_pos": 35, "type": "TASK", "confidence": 0.900941789150238}]}, {"text": "This maybe due to the indiscriminately high confidence scores; or it could indicate that classifiers, which are geared at distinguishing between known classes rather than detecting objects that differ from all seen data, are not optimally suited to the task.", "labels": [], "entities": []}, {"text": "However, one further disadvantage of this approach is that, as mentioned above, it can only be applied to lemmas with more than one annotated sense.", "labels": [], "entities": []}, {"text": "For FrameNet 1.2, this comprises only 19% of the lemmas.", "labels": [], "entities": [{"text": "FrameNet 1.2", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.8081609606742859}]}, {"text": "In this section we use the NN-based outlier detection approach of the previous section for an experiment in unknown sense detection.", "labels": [], "entities": [{"text": "NN-based outlier detection", "start_pos": 27, "end_pos": 53, "type": "TASK", "confidence": 0.5669975280761719}, {"text": "unknown sense detection", "start_pos": 108, "end_pos": 131, "type": "TASK", "confidence": 0.6722470919291178}]}, {"text": "Experimental setup and data are as described in Section 3.", "labels": [], "entities": []}, {"text": "We model unknown sense detection as an outlier detection task, using Tax and Duin's outlier detection approach that we have outlined in the previous section.", "labels": [], "entities": [{"text": "unknown sense detection", "start_pos": 9, "end_pos": 32, "type": "TASK", "confidence": 0.7365373571713766}]}, {"text": "Nearest neighbors (by Euclidean distance) were computed using the ANN tool).", "labels": [], "entities": []}, {"text": "We compute one outlier detection model per lemma.", "labels": [], "entities": []}, {"text": "With training and test sets constructed as described in Section 3, the average training set comprises 22.5 sentences.", "labels": [], "entities": []}, {"text": "We use the same features as in Section 4, with feature vector entries of 1 for present and 0 for absent features.", "labels": [], "entities": []}, {"text": "For a more detailed analysis of the contribution of different feature types, we test on reduced as well as full feature vectors:  While the NN-based outlier detection model we used in the previous experiment showed better re-    sults than the WSD confidence model, its recall is still low.", "labels": [], "entities": [{"text": "NN-based outlier detection", "start_pos": 140, "end_pos": 166, "type": "TASK", "confidence": 0.5864172677199045}, {"text": "re-    sults", "start_pos": 222, "end_pos": 234, "type": "METRIC", "confidence": 0.9108365774154663}, {"text": "recall", "start_pos": 270, "end_pos": 276, "type": "METRIC", "confidence": 0.9997114539146423}]}, {"text": "We have suggested that data sparseness maybe responsible for the low performance.", "labels": [], "entities": []}, {"text": "Consequently, we repeat the experiment of the previous section with more, but less specific, training data.", "labels": [], "entities": []}, {"text": "Like WordNet synsets, FrameNet frames are semantic classes that typically comprise several lemmas or expressions.", "labels": [], "entities": []}, {"text": "So, assuming that words with similar meaning occur in similar contexts, the context features for lemmas in the same frame should be similar.", "labels": [], "entities": []}, {"text": "Following this idea, we supplement the training data fora lemma by all the other annotated data for the senses that are present in the training set, whereby \"other data\" we mean data with other target lemmas.", "labels": [], "entities": []}, {"text": "shows an example 4 . Modeling.", "labels": [], "entities": [{"text": "Modeling", "start_pos": 21, "end_pos": 29, "type": "TASK", "confidence": 0.9692491292953491}]}, {"text": "Again, we use Tax and Duin's outlier detection approach for unknown sense detection.", "labels": [], "entities": [{"text": "unknown sense detection", "start_pos": 60, "end_pos": 83, "type": "TASK", "confidence": 0.75669397910436}]}, {"text": "The experimental design and evaluation are the same as in Experiment 2, the only difference being the training set extension.", "labels": [], "entities": []}, {"text": "Training set extension raises the average training set size from 22.5 to 374.", "labels": [], "entities": []}, {"text": "Results.: Experiments 2 and 3: Results by the number of senses of a lemma, condition All, \u03b8 = 1.0 and 5-fold cross-validation.", "labels": [], "entities": []}, {"text": "In comparison to Experiment 2, precision has risen slightly, and for conditions All, Cx and Syn-hw, recall has risen steeply; the maximum recall is achieved by Cx at 0.82.", "labels": [], "entities": [{"text": "precision", "start_pos": 31, "end_pos": 40, "type": "METRIC", "confidence": 0.9997860789299011}, {"text": "All", "start_pos": 80, "end_pos": 83, "type": "METRIC", "confidence": 0.9655376076698303}, {"text": "recall", "start_pos": 100, "end_pos": 106, "type": "METRIC", "confidence": 0.9997289776802063}, {"text": "recall", "start_pos": 138, "end_pos": 144, "type": "METRIC", "confidence": 0.9994111061096191}]}, {"text": "As before, increasing the distance quotient threshold leads to little change in precision but a sharp drop in recall.", "labels": [], "entities": [{"text": "distance quotient threshold", "start_pos": 26, "end_pos": 53, "type": "METRIC", "confidence": 0.8597172498703003}, {"text": "precision", "start_pos": 80, "end_pos": 89, "type": "METRIC", "confidence": 0.999631404876709}, {"text": "recall", "start_pos": 110, "end_pos": 116, "type": "METRIC", "confidence": 0.9987975358963013}]}, {"text": "For All vectors, recall is 0.72 for threshold 1.0, 0.56 for \u03b8 = 1.1, and 0.41 for \u03b8 = 1.2.", "labels": [], "entities": [{"text": "recall", "start_pos": 17, "end_pos": 23, "type": "METRIC", "confidence": 0.9998180270195007}]}, {"text": "shows system performance by training set size.", "labels": [], "entities": []}, {"text": "As the average training set in this experiment is much larger than in Experiment 2, we are now inspecting sets of minimum size 50 and 200 rather than 10 and 20.", "labels": [], "entities": []}, {"text": "We find the same effect as in Experiment 2, with noticeably higher recall for lemmas with larger training sets, but slightly lower precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 67, "end_pos": 73, "type": "METRIC", "confidence": 0.999271810054779}, {"text": "precision", "start_pos": 131, "end_pos": 140, "type": "METRIC", "confidence": 0.9985032081604004}]}, {"text": "breaks down system performance by the degree of ambiguity of a lemma.", "labels": [], "entities": []}, {"text": "Here, too, we seethe same effect as in Experiment 2: the more senses a lemma has, the lower the precision and the higher the recall of unknown label assignment.", "labels": [], "entities": [{"text": "precision", "start_pos": 96, "end_pos": 105, "type": "METRIC", "confidence": 0.999578058719635}, {"text": "recall", "start_pos": 125, "end_pos": 131, "type": "METRIC", "confidence": 0.9992920160293579}]}, {"text": "In comparison to Experiment 2, Experiment 3 shows a dramatic increase in recall, and even some increase in precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 73, "end_pos": 79, "type": "METRIC", "confidence": 0.9996969699859619}, {"text": "precision", "start_pos": 107, "end_pos": 116, "type": "METRIC", "confidence": 0.9991470575332642}]}, {"text": "Precision and recall for conditions All and Cx are good enough for the system to be usable in practice.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9960454106330872}, {"text": "recall", "start_pos": 14, "end_pos": 20, "type": "METRIC", "confidence": 0.9997386336326599}, {"text": "All", "start_pos": 36, "end_pos": 39, "type": "METRIC", "confidence": 0.9795283079147339}, {"text": "Cx", "start_pos": 44, "end_pos": 46, "type": "METRIC", "confidence": 0.8782588839530945}]}, {"text": "Of the four conditions, the three that involve context words, All, Cx and Syn-hw, show considerably higher recall than Syn.", "labels": [], "entities": [{"text": "recall", "start_pos": 107, "end_pos": 113, "type": "METRIC", "confidence": 0.9988359808921814}]}, {"text": "Furthermore, the two conditions that do not involve syntactic features, All and Cx, have markedly higher results than Syn-hw.", "labels": [], "entities": [{"text": "All", "start_pos": 72, "end_pos": 75, "type": "METRIC", "confidence": 0.9596120715141296}]}, {"text": "This could mean that syntactic features are not as helpful as context features in detecting unknown senses; however in Experiment 2 the performance difference between Syn and the other conditions was not by far as large as in this experiment.", "labels": [], "entities": []}, {"text": "It could also mean that frames are not as uniform in their syntactic structure as they are in their context words.", "labels": [], "entities": []}, {"text": "This seems plausible as FrameNet frames are constructed mostly on semantic grounds, without recourse to similarity in syntactic structure.", "labels": [], "entities": []}, {"text": "points to a sparse data problem, even with training sets extended by additional items.", "labels": [], "entities": []}, {"text": "It also shows that the more a test condition relies on context word information, the more it profits from additional data.", "labels": [], "entities": []}, {"text": "So it maybe worthwhile to explore methods fora further alleviation of data sparseness, e.g. by generalizing over context words.", "labels": [], "entities": []}, {"text": "underscores the large influence of training set uniformity: the more senses a lemma has, the more likely the model is to classify a test instance as unknown.", "labels": [], "entities": []}, {"text": "This is the case even for extended training sets.", "labels": [], "entities": []}, {"text": "One possible way of addressing this problem would be to take into account more than a single nearest neighbor in NN-based outlier detection in order to compute more precise boundaries between known and unknown instances.", "labels": [], "entities": [{"text": "NN-based outlier detection", "start_pos": 113, "end_pos": 139, "type": "TASK", "confidence": 0.6275934974352518}]}], "tableCaptions": [{"text": " Table 1: Experiment 1: Results for label unknown  sense, WSD confidence level approach. \u03b8: confi- dence threshold. \u03c3: std. dev.", "labels": [], "entities": [{"text": "WSD confidence level", "start_pos": 58, "end_pos": 78, "type": "METRIC", "confidence": 0.5094076693058014}]}, {"text": " Table 2: Experiment 2: Results for label unknown  sense, NN-based outlier detection, \u03b8 = 1.0. \u03c3: stan- dard deviation", "labels": [], "entities": [{"text": "NN-based outlier detection", "start_pos": 58, "end_pos": 84, "type": "TASK", "confidence": 0.5543395777543386}, {"text": "stan- dard deviation", "start_pos": 98, "end_pos": 118, "type": "METRIC", "confidence": 0.6899166256189346}]}, {"text": " Table 3: Experiment 2: Results by training set size,  \u03b8 = 1.0", "labels": [], "entities": []}, {"text": " Table 6: Experiment 3: Results by training set size,  \u03b8 = 1.0", "labels": [], "entities": []}, {"text": " Table 7: Experiments 2 and 3: Results by the num- ber of senses of a lemma, condition All, \u03b8 = 1.0", "labels": [], "entities": []}]}