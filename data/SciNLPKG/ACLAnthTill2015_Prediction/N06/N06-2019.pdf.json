{"title": [{"text": "Early Deletion of Fillers In Processing Conversational Speech", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper evaluates the benefit of deleting fillers (e.g. you know, like) early in parsing conversational speech.", "labels": [], "entities": [{"text": "parsing conversational speech", "start_pos": 84, "end_pos": 113, "type": "TASK", "confidence": 0.8832414348920187}]}, {"text": "Readability studies have shown that disfluencies (fillers and speech repairs) maybe deleted from transcripts without compromising meaning (Jones et al., 2003), and deleting repairs prior to parsing has been shown to improve its accuracy (Charniak and Johnson, 2001).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 228, "end_pos": 236, "type": "METRIC", "confidence": 0.9983859062194824}]}, {"text": "We explore whether this strategy of early deletion is also beneficial with regard to fillers.", "labels": [], "entities": []}, {"text": "Reported experiments measure the effect of early deletion under in-domain and out-of-domain parser training conditions using a state-of-the-art parser (Charniak, 2000).", "labels": [], "entities": []}, {"text": "While early deletion is found to yield only modest benefit for in-domain parsing, significant improvement is achieved for out-of-domain adaptation.", "labels": [], "entities": []}, {"text": "This suggests a potentially broader role for disfluency modeling in adapting text-based tools for processing conversational speech.", "labels": [], "entities": [{"text": "disfluency modeling", "start_pos": 45, "end_pos": 64, "type": "TASK", "confidence": 0.7599238157272339}]}], "introductionContent": [{"text": "This paper evaluates the benefit of deleting fillers early in parsing conversational speech.", "labels": [], "entities": [{"text": "parsing conversational speech", "start_pos": 62, "end_pos": 91, "type": "TASK", "confidence": 0.8871071139971415}]}, {"text": "We follow LDC (2004) conventions in using the term filler to encompass abroad set of vocalized space-fillers that can introduce syntactic (and semantic) ambiguity.", "labels": [], "entities": []}, {"text": "For example, in the questions Did you know I do that?", "labels": [], "entities": []}, {"text": "Is it like that one?", "labels": [], "entities": []}, {"text": "colloquial use of fillers, indicated below through use of commas, can yield alternative readings Did, you know, I do that?", "labels": [], "entities": []}, {"text": "Is it, like, that one?", "labels": [], "entities": []}, {"text": "Readings of the first example differ in querying listener knowledge versus speaker action, while readings of the second differ in querying similarity versus exact match.", "labels": [], "entities": []}, {"text": "Though an engaged listener rarely has difficulty distinguishing between such alternatives, studies show that deleting disfluencies from transcripts improves readability with no reduction in reading comprehension (.", "labels": [], "entities": []}, {"text": "The fact that disfluencies can be completely removed without compromising meaning is important.", "labels": [], "entities": []}, {"text": "Earlier work had already made this claim regarding speech repairs 1 and argued that there was consequently little value in syntactically analyzing repairs or evaluating our ability to do so).", "labels": [], "entities": [{"text": "speech repairs", "start_pos": 51, "end_pos": 65, "type": "TASK", "confidence": 0.7958850860595703}]}, {"text": "Moreover, this work showed that collateral damage to parse accuracy caused by repairs could be averted by deleting them prior to parsing, and this finding has been confirmed in subsequent studies).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.4615689814090729}, {"text": "parsing", "start_pos": 129, "end_pos": 136, "type": "TASK", "confidence": 0.9601876139640808}]}, {"text": "But whereas speech repairs have received significant attention in the parsing literature, fillers have been relatively neglected.", "labels": [], "entities": [{"text": "speech repairs", "start_pos": 12, "end_pos": 26, "type": "TASK", "confidence": 0.7595718502998352}]}, {"text": "While one study has shown that the presence of interjection and parenthetical constituents in conversational speech reduces parse accuracy (), these constituent types are defined to cover both fluent and disfluent speech phenomena, leaving the impact of fillers alone unclear.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 130, "end_pos": 138, "type": "METRIC", "confidence": 0.8085285425186157}]}, {"text": "In our study, disfluency annotations are leveraged to identify fillers precisely, and these annotations are merged with treebank syntax.", "labels": [], "entities": []}, {"text": "Extending the arguments of Charniak and Johnson with regard to repairs (2001), we argue there is little value in recovering the syntactic structure of fillers, and we relax evaluation metrics accordingly ( \u00a73.2).", "labels": [], "entities": []}, {"text": "Experiments performed ( \u00a73.3) use a state-of-the-art parser to study the impact of early filler deletion under in-domain and out-of-domain (i.e. adaptation) training conditions.", "labels": [], "entities": []}, {"text": "In terms of adaptation, there is tremendous potential in applying textual tools and training data to processing transcribed speech (e.g. machine translation, information extraction, etc.), and bleaching speech data to more closely resemble text has been shown to improve accuracy with some text-based processing tasks ().", "labels": [], "entities": [{"text": "machine translation", "start_pos": 137, "end_pos": 156, "type": "TASK", "confidence": 0.7381339073181152}, {"text": "information extraction", "start_pos": 158, "end_pos": 180, "type": "TASK", "confidence": 0.6929362565279007}, {"text": "accuracy", "start_pos": 271, "end_pos": 279, "type": "METRIC", "confidence": 0.9982328414916992}]}, {"text": "For our study, a state-of-the-art filler detector ) is employed to delete fillers prior to parsing.", "labels": [], "entities": []}, {"text": "Results show parse accuracy improves significantly, suggesting disfluency filtering may have abroad role in enabling text-based processing of speech data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9264779090881348}]}], "datasetContent": [{"text": "This section reports parsing experiments studying the effect of early deletion under in-domain and outof-domain parser training conditions using the August 2005 release of the Charniak parser (2000).", "labels": [], "entities": [{"text": "parsing", "start_pos": 21, "end_pos": 28, "type": "TASK", "confidence": 0.9650962352752686}]}, {"text": "We describe data and evaluation metrics used, then proceed to describe the experiments.", "labels": [], "entities": []}, {"text": "As discussed earlier ( \u00a71), Charniak and Johnson (2001) have argued that speech repairs do not contribute to meaning and so there is little value in syntactically analyzing repairs or evaluating our ability to do so.", "labels": [], "entities": []}, {"text": "Consequently, they relaxed standard PARSEVAL to treat EDITED constituents like punctuation: adjacent EDITED constituents are merged, and the internal structure and attachment of EDITED constituents is not evaluated.", "labels": [], "entities": [{"text": "PARSEVAL", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.5665134191513062}]}, {"text": "We propose generalizing this approach to disfluency at large, i.e. fillers as well as repairs.", "labels": [], "entities": []}, {"text": "Note that the details of appropriate evaluation metrics for parsed speech data is orthogonal to the parsing methods proposed here: however parsing is performed, we should avoid wasting metric attention evaluating syntax of words that do not contribute toward meaning and instead evaluate only how well such words can be identified.", "labels": [], "entities": []}, {"text": "Relaxed metric treatment of disfluency was achieved via simple parameterization of the SParseval tool).", "labels": [], "entities": []}, {"text": "SParseval also has the added benefit of calculating a dependencybased evaluation alongside PARSEVAL's bracketbased measure.", "labels": [], "entities": []}, {"text": "The dependency metric performs syntactic head-matching for each word using a set of given head percolation rules (derived from Charniak's parser), and its relaxed formulation ignores terminals spanned by FILLER and EDITED constituents.", "labels": [], "entities": [{"text": "FILLER", "start_pos": 204, "end_pos": 210, "type": "METRIC", "confidence": 0.9452205300331116}]}, {"text": "We found this metric offered additional insights in analyzing some of our results.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: F-scores on Switchboard when trained in- domain. LB and Dep refer to relaxed labelled- bracket and dependency parse metrics ( \u00a73.2). Edit  and filler word detection f-scores are also shown.", "labels": [], "entities": []}, {"text": " Table 2: F-scores parsing Switchboard when trained  on WSJ. Edit word detection varies between parser  and oracle, and filler word detection varies between  none, system (Johnson et al., 2004), and oracle.  Filler F, LB, and Dep are defined as in Table 1.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9699556231498718}, {"text": "parsing", "start_pos": 19, "end_pos": 26, "type": "TASK", "confidence": 0.6784030199050903}, {"text": "WSJ", "start_pos": 56, "end_pos": 59, "type": "DATASET", "confidence": 0.836441159248352}, {"text": "Edit word detection", "start_pos": 61, "end_pos": 80, "type": "TASK", "confidence": 0.5340867042541504}, {"text": "filler word detection", "start_pos": 120, "end_pos": 141, "type": "TASK", "confidence": 0.6224114894866943}, {"text": "Dep", "start_pos": 226, "end_pos": 229, "type": "METRIC", "confidence": 0.9689190983772278}]}]}