{"title": [{"text": "Backoff Model Training using Partially Observed Data: Application to Dialog Act Tagging", "labels": [], "entities": [{"text": "Dialog Act Tagging", "start_pos": 69, "end_pos": 87, "type": "TASK", "confidence": 0.7654528220494589}]}], "abstractContent": [{"text": "Dialog act (DA) tags are useful for many applications in natural language processing and automatic speech recognition.", "labels": [], "entities": [{"text": "Dialog act (DA) tags", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.5963668525218964}, {"text": "automatic speech recognition", "start_pos": 89, "end_pos": 117, "type": "TASK", "confidence": 0.6080884536107382}]}, {"text": "In this work, we introduce hidden backoff models (HBMs) where a large generalized backoff model is trained, using an embedded expectation-maximization (EM) procedure , on data that is partially observed.", "labels": [], "entities": []}, {"text": "We use HBMs as word models conditioned on both DAs and (hidden) DA-segments.", "labels": [], "entities": []}, {"text": "Experimental results on the ICSI meeting recorder dialog act corpus show that our procedure can strictly increase likelihood on training data and can effectively reduce errors on test data.", "labels": [], "entities": [{"text": "ICSI meeting recorder dialog act corpus", "start_pos": 28, "end_pos": 67, "type": "DATASET", "confidence": 0.9404193858305613}, {"text": "likelihood", "start_pos": 114, "end_pos": 124, "type": "METRIC", "confidence": 0.986240804195404}]}, {"text": "In the best case, test error can be reduced by 6.1% relative to our baseline, an improvement on previously reported models that also use prosody.", "labels": [], "entities": [{"text": "error", "start_pos": 23, "end_pos": 28, "type": "METRIC", "confidence": 0.6070232391357422}]}, {"text": "We also compare with our own prosody-based model, and show that our HBM is competitive even without the use of prosody.", "labels": [], "entities": []}, {"text": "We have not yet succeeded , however, in combining the benefits of both prosody and the HBM.", "labels": [], "entities": []}], "introductionContent": [{"text": "Discourse patterns in natural conversations and meetings are well known to provide interesting and useful information about human conversational behavior.", "labels": [], "entities": []}, {"text": "They thus attract research from many different and beneficial perspectives.", "labels": [], "entities": []}, {"text": "Dialog acts (DAs), which reflect the functions that utterances serve in a discourse, are one type of such patterns.", "labels": [], "entities": []}, {"text": "Detecting and understanding dialog act patterns can provide benefit to systems such as automatic speech recognition (ASR) (, machine dialog translation (, and general natural language processing (NLP) (.", "labels": [], "entities": [{"text": "automatic speech recognition (ASR)", "start_pos": 87, "end_pos": 121, "type": "TASK", "confidence": 0.8019230763117472}, {"text": "machine dialog translation", "start_pos": 125, "end_pos": 151, "type": "TASK", "confidence": 0.6543019314606985}, {"text": "general natural language processing (NLP)", "start_pos": 159, "end_pos": 200, "type": "TASK", "confidence": 0.7142961323261261}]}, {"text": "DA pattern recognition is an instance of \"tagging.\"", "labels": [], "entities": [{"text": "DA pattern recognition", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8129587968190511}]}, {"text": "Many different techniques have been quite successful in this endeavor, including hidden Markov models (), semantic classification trees and polygrams, maximum entropy models (), and other language models (.", "labels": [], "entities": []}, {"text": "Like other tagging tasks, DA recognition can also be achieved using conditional random fields () and general discriminative modeling on structured outputs ().", "labels": [], "entities": [{"text": "tagging tasks", "start_pos": 11, "end_pos": 24, "type": "TASK", "confidence": 0.9056063294410706}, {"text": "DA recognition", "start_pos": 26, "end_pos": 40, "type": "TASK", "confidence": 0.9901836514472961}]}, {"text": "In many sequential data analysis tasks (speech, language, or DNA sequence analysis), standard dynamic Bayesian networks (DBNs)) have shown great flexibility and are widely used.", "labels": [], "entities": [{"text": "sequential data analysis tasks (speech, language, or DNA sequence analysis", "start_pos": 8, "end_pos": 82, "type": "TASK", "confidence": 0.6959052360974826}]}, {"text": "In, for example, an analysis of DA tagging using DBNs is performed, where the models avoid label bias by structural changes and avoid data sparseness by using a generalized backoff procedures).", "labels": [], "entities": [{"text": "DA tagging", "start_pos": 32, "end_pos": 42, "type": "TASK", "confidence": 0.8886639475822449}]}, {"text": "Most DA classification procedures assume that within a sentence of a particular fixed DA type, there is a fixed word distribution over the entire sentence.", "labels": [], "entities": [{"text": "DA classification", "start_pos": 5, "end_pos": 22, "type": "TASK", "confidence": 0.9825681746006012}]}, {"text": "Similar to () (and see citations therein), we have found, however, that intra-sentence discourse patterns are inherently dynamic.", "labels": [], "entities": []}, {"text": "Moreover, the patterns are specific to each type of DA, meaning a sentence will go through a DAspecific sequence of sub-DA phases or \"states.\"", "labels": [], "entities": []}, {"text": "A generative description of this phenomena is that a DA is first chosen, and then words are generated according to both the DA and to the relative position of the word in that sentence.", "labels": [], "entities": []}, {"text": "For example, a \"statement\" (one type of DA) can consist of a subject (noun phrase), verb phrase, and object (noun phrase).", "labels": [], "entities": []}, {"text": "This particular sequence might be different fora different DA (e.g., a \"back-channel\").", "labels": [], "entities": []}, {"text": "Our belief is that explicitly modeling these internal states can help a DA-classification system in conversational meetings or dialogs.", "labels": [], "entities": []}, {"text": "In this work, we describe an approach that is motivated by several aspects of the typical DAclassification procedure.", "labels": [], "entities": []}, {"text": "First, it is rare to have subDAs labeled in training data, and indeed this is true of the corpus () that we use.", "labels": [], "entities": []}, {"text": "Therefore, some form of unsupervised clustering or pre-shallow-parsing of sub-DAs must be performed.", "labels": [], "entities": []}, {"text": "In such a model, these sub-DAs are essentially unknown hidden variables that ideally could be trained with an expectation-maximization (EM) procedure.", "labels": [], "entities": []}, {"text": "Second, when training models of language, it is necessary to employ some form of smoothing methodology since otherwise data-sparseness would render standard maximum-likelihood trained models useless.", "labels": [], "entities": []}, {"text": "Third, discrete conditional probability distributions formed using backoff models that have been smoothed (particularly using modified) have been extremely successful in many language modeling tasks.", "labels": [], "entities": []}, {"text": "Training backoff models, however, requires that all data is observed so that data counts can be formed.", "labels": [], "entities": []}, {"text": "Indeed, our DA-specific word models (implemented via backoff) will also need to condition on the current sub-DA, which at training time is unknown.", "labels": [], "entities": []}, {"text": "We therefore have developed a procedure that allows us to train generalized backoff models, even when some or all of the variables involved in the model are hidden.", "labels": [], "entities": []}, {"text": "We thus call our models hidden backoff models (HBMs).", "labels": [], "entities": []}, {"text": "Our method is indeed a form of embedded EM training, and more generally is a specific form of EM (.", "labels": [], "entities": []}, {"text": "Our approach is similar to (), except our underlying language models are backoff-based and thus retain the benefits of advanced smoothing methods, and we utilize both a normal and a backoff EM step as will be seen.", "labels": [], "entities": []}, {"text": "We moreover wrap up the above ideas in the framework of dynamic Bayesian networks, which are used to represent and train all of our models.", "labels": [], "entities": []}, {"text": "We evaluate our methods on the ICSI meeting recorder dialog act (MRDA) () corpus, and find that our novel hidden backoff model can significantly improve dialog tagging accuracy.", "labels": [], "entities": [{"text": "ICSI meeting recorder dialog act (MRDA) () corpus", "start_pos": 31, "end_pos": 80, "type": "DATASET", "confidence": 0.7216612577438355}, {"text": "dialog tagging", "start_pos": 153, "end_pos": 167, "type": "TASK", "confidence": 0.7992893755435944}, {"text": "accuracy", "start_pos": 168, "end_pos": 176, "type": "METRIC", "confidence": 0.8234773278236389}]}, {"text": "With a different number of hidden states for each DA, a relative reduction in tagging error rate as much as 6.1% can be achieved.", "labels": [], "entities": [{"text": "tagging error rate", "start_pos": 78, "end_pos": 96, "type": "METRIC", "confidence": 0.8210398952166239}]}, {"text": "Our best HBM result shows an accuracy that improves on the best known (to our knowledge) result on this corpora which is one that uses acoustic prosody as a feature.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.9996191263198853}]}, {"text": "We have moreover developed our own prosody model and while we have not been able to usefully employ both prosody and the HBM technique together, our HBM is competitive in this case as well.", "labels": [], "entities": []}, {"text": "Furthermore, our results show the effectiveness of our embedded EM procedure, as we demonstrate that it increases training log likelihoods, while simultaneously reducing error rate.", "labels": [], "entities": [{"text": "training log likelihoods", "start_pos": 114, "end_pos": 138, "type": "METRIC", "confidence": 0.797887404759725}, {"text": "error rate", "start_pos": 170, "end_pos": 180, "type": "METRIC", "confidence": 0.9690096080303192}]}, {"text": "Section 2 briefly summarizes our baseline DBNbased models for DA tagging tasks.", "labels": [], "entities": [{"text": "DA tagging tasks", "start_pos": 62, "end_pos": 78, "type": "TASK", "confidence": 0.9347036878267924}]}, {"text": "In Section 3, we introduce our HBMs.", "labels": [], "entities": []}, {"text": "Section 4 contains experimental evaluations on the MRDA corpus and finally Section 5 concludes.", "labels": [], "entities": [{"text": "MRDA corpus", "start_pos": 51, "end_pos": 62, "type": "DATASET", "confidence": 0.8961696922779083}]}], "datasetContent": [{"text": "We evaluated our hidden backoff model on the ICSI meeting recorder dialog act (MRDA) corpus).", "labels": [], "entities": [{"text": "ICSI meeting recorder dialog act (MRDA) corpus", "start_pos": 45, "end_pos": 91, "type": "DATASET", "confidence": 0.7588262127505409}]}, {"text": "MRDA is a rich data set that contains 75 natural meetings on different topics with each meeting involving about 6 participants.", "labels": [], "entities": [{"text": "MRDA", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.7230120897293091}]}, {"text": "DA annotations from ICSI were based on a previous approach in () with some adaptation for meetings in a number of ways described in (.", "labels": [], "entities": [{"text": "DA annotations", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.6586645245552063}, {"text": "ICSI", "start_pos": 20, "end_pos": 24, "type": "DATASET", "confidence": 0.7521958947181702}]}, {"text": "Each DA contains a main tag, several optional special tags and an optional \"disruption\" form.", "labels": [], "entities": []}, {"text": "The total number of distinct DAs in the corpus is as large as 1260.", "labels": [], "entities": []}, {"text": "In order to make the problem comparable to other work (), a DA tag sub-set is used in our experiments that contains back channels (b), place holders (h), questions (q), statements (s), and disruptions (x).", "labels": [], "entities": []}, {"text": "In our evaluations, among the entire 75 conversations, 51 are used as the training set, 11 are used as the development set, 11 are used as test set, and the remaining 3 are not used.", "labels": [], "entities": []}, {"text": "For each experiment, we used a genetic algorithm to search for the best factored language model structure on the development set and we report the best results.", "labels": [], "entities": []}, {"text": "Our baseline system is the generative model shown in and uses a backoff implementation of the word model, and is optimized on the development set.", "labels": [], "entities": []}, {"text": "We use the SRILM toolkit with extensions ( to train, and use GMTK () for decoding.", "labels": [], "entities": [{"text": "SRILM toolkit", "start_pos": 11, "end_pos": 24, "type": "DATASET", "confidence": 0.8469593226909637}, {"text": "GMTK", "start_pos": 61, "end_pos": 65, "type": "DATASET", "confidence": 0.6685448288917542}]}, {"text": "Our baseline system has an error rate of 19.7% on the test set, which is comparable to other approaches on the same task).", "labels": [], "entities": [{"text": "error rate", "start_pos": 27, "end_pos": 37, "type": "METRIC", "confidence": 0.9857219159603119}]}], "tableCaptions": [{"text": " Table 2: Length statistics of different DAs.", "labels": [], "entities": [{"text": "Length", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9750609397888184}]}, {"text": " Table 3: Number of hidden states for different DAs.", "labels": [], "entities": []}]}