{"title": [{"text": "Cross-Entropy and Estimation of Probabilistic Context-Free Grammars", "labels": [], "entities": []}], "abstractContent": [{"text": "We investigate the problem of training probabilistic context-free grammars on the basis of a distribution defined over an infinite set of trees, by minimizing the cross-entropy.", "labels": [], "entities": []}, {"text": "This problem can be seen as a generalization of the well-known maximum likelihood estimator on (finite) tree banks.", "labels": [], "entities": []}, {"text": "We prove an unexpected theoretical property of grammars that are trained in this way, namely, we show that the derivational entropy of the grammar takes the same value as the cross-entropy between the input distribution and the grammar itself.", "labels": [], "entities": []}, {"text": "We show that the result also holds for the widely applied maximum likelihood estimator on tree banks.", "labels": [], "entities": []}], "introductionContent": [{"text": "Probabilistic context-free grammars are able to describe hierarchical, tree-shaped structures underlying sentences, and are widely used in statistical natural language processing; see for instance and references therein.", "labels": [], "entities": [{"text": "statistical natural language processing", "start_pos": 139, "end_pos": 178, "type": "TASK", "confidence": 0.6371529251337051}]}, {"text": "Probabilistic contextfree grammars seem also more suitable than finitestate devices for language modeling, and several language models based on these grammars have been recently proposed in the literature; see for instance (, and.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 88, "end_pos": 105, "type": "TASK", "confidence": 0.714205414056778}]}, {"text": "Empirical estimation of probabilistic context-free grammars is usually carried out on tree banks, that is, finite samples of parse trees, through the maximization of the likelihood of the sample itself.", "labels": [], "entities": [{"text": "Empirical estimation of probabilistic context-free grammars", "start_pos": 0, "end_pos": 59, "type": "TASK", "confidence": 0.8007707049449285}]}, {"text": "It is well-known that this method also minimizes the cross-entropy between the probability distribution induced by the tree bank, also called the empirical distribution, and the tree probability distribution induced by the estimated grammar.", "labels": [], "entities": []}, {"text": "In this paper we generalize the maximum likelihood method, proposing an estimation technique that works on any unrestricted tree distribution defined over an infinite set of trees.", "labels": [], "entities": []}, {"text": "This generalization is theoretically appealing, and allows us to prove unexpected properties of the already mentioned maximum likelihood estimator for tree banks, that were not previously known in the literature on statistical natural language parsing.", "labels": [], "entities": [{"text": "statistical natural language parsing", "start_pos": 215, "end_pos": 251, "type": "TASK", "confidence": 0.6759036928415298}]}, {"text": "More specifically, we investigate the following information theoretic quantities \u2022 the cross-entropy between the unrestricted tree distribution given as input and the tree distribution induced by the estimated probabilistic context-free grammar; and \u2022 the derivational entropy of the estimated probabilistic context-free grammar.", "labels": [], "entities": []}, {"text": "These two quantities are usually unrelated.", "labels": [], "entities": []}, {"text": "We show that these two quantities take the same value when the probabilistic context-free grammar is trained using the minimal cross-entropy criterion.", "labels": [], "entities": []}, {"text": "We then translate back this property to the method of maximum likelihood estimation.", "labels": [], "entities": []}, {"text": "Our general estimation method also has practical applications in cases one uses a probabilistic context-free grammar to approximate strictly more powerful rewriting systems, as for instance probabilistic tree adjoining grammars.", "labels": [], "entities": []}, {"text": "Not much is found in the literature about the estimation of probabilistic grammars from infinite distributions.", "labels": [], "entities": []}, {"text": "This line of research was started in, investigating the problem of training an input probabilistic finite automaton from an infinite tree distribution specified by means of an input probabilistic context-free grammar.", "labels": [], "entities": []}, {"text": "The problem we consider in this paper can then be seen as a generalization of the above problem, where the input model to be trained is a probabilistic context-free grammar and the input distribution is an unrestricted tree distribution.", "labels": [], "entities": []}, {"text": "In) an estimator that maximizes the likelihood of a probability distribution defined over a finite set of trees is introduced, as a generalization of the maximum likelihood estimator.", "labels": [], "entities": []}, {"text": "Again, the problems we consider here can bethought of as generalizations of such estimator to the case of distributions over infinite sets of trees or sentences.", "labels": [], "entities": []}, {"text": "The remainder of this paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 introduces the basic notation and definitions and Section 3 discusses our new estimation method.", "labels": [], "entities": [{"text": "estimation", "start_pos": 88, "end_pos": 98, "type": "TASK", "confidence": 0.9361974596977234}]}, {"text": "Section 4 presents our main result, which is transferred in Section 5 to the method of maximum likelihood estimation.", "labels": [], "entities": [{"text": "maximum likelihood estimation", "start_pos": 87, "end_pos": 116, "type": "TASK", "confidence": 0.5790804723898569}]}, {"text": "Section 6 discusses some simple examples, and Section 7 closes with some further discussion.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}