{"title": [{"text": "Accurate Parsing of the Proposition Bank", "labels": [], "entities": [{"text": "Accurate", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9827813506126404}]}], "abstractContent": [{"text": "We integrate PropBank semantic role labels to an existing statistical parsing model producing richer output.", "labels": [], "entities": []}, {"text": "We show conclusive results on joint learning and inference of syntactic and semantic representations .", "labels": [], "entities": []}], "introductionContent": [{"text": "Recent successes in statistical syntactic parsing based on supervised techniques trained on a large corpus of syntactic trees have brought the hope that the same approach could be applied to the more ambitious goal of recovering the propositional content and the frame semantics of a sentence.", "labels": [], "entities": [{"text": "statistical syntactic parsing", "start_pos": 20, "end_pos": 49, "type": "TASK", "confidence": 0.6473334034283956}]}, {"text": "Moving towards a shallow semantic level of representation has immediate applications in question-answering and information extraction.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 111, "end_pos": 133, "type": "TASK", "confidence": 0.8615774214267731}]}, {"text": "For example, an automatic flight reservation system processing the sentence I want to book a flight from Geneva to New York will need to know that from Geneva indicates the origin of the flight and to New York the destination.", "labels": [], "entities": [{"text": "automatic flight reservation system processing the sentence I want to book a flight from Geneva to New York", "start_pos": 16, "end_pos": 123, "type": "TASK", "confidence": 0.6155061191982694}]}, {"text": "() define this shallow semantic task as a classification problem where the semantic role to be assigned to each constituent is inferred on the basis of probability distributions of syntactic features extracted from parse trees.", "labels": [], "entities": []}, {"text": "They use learning features such as phrase type, position, voice, and parse tree path.", "labels": [], "entities": []}, {"text": "Consider, for example, a sentence such as The authority dropped at midnight Tuesday to $ 2.80 trillion (taken from section 00 of).", "labels": [], "entities": []}, {"text": "The fact that to $ 2.80 trillion receives a direction semantic label is highly correlated to the fact that it is a Prepositional Phrase (PP), that it follows the verb dropped, a verb of change of state requiring an endpoint, that the verb is in the active voice, and that the PP is in a certain tree configuration with the governing verb.", "labels": [], "entities": []}, {"text": "All the recent systems proposed for semantic role labelling (SRL) follow this same assumption).", "labels": [], "entities": [{"text": "semantic role labelling (SRL)", "start_pos": 36, "end_pos": 65, "type": "TASK", "confidence": 0.7900612155596415}]}, {"text": "The assumption that syntactic distributions will be predictive of semantic role assignments is based on linking theory.", "labels": [], "entities": [{"text": "semantic role assignments", "start_pos": 66, "end_pos": 91, "type": "TASK", "confidence": 0.6204065481821696}]}, {"text": "Linking theory assumes the existence of a hierarchy of semantic roles which are mapped by default on a hierarchy of syntactic positions.", "labels": [], "entities": [{"text": "Linking theory", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.9768394529819489}]}, {"text": "It also shows that regular mappings from the semantic to the syntactic level can be posited even for those verbs whose arguments can take several syntactic positions, such as psychological verbs, locatives, or datives, requiring a more complex theory.) among many others.)", "labels": [], "entities": []}, {"text": "If the internal semantics of a predicate determines the syntactic expressions of constituents bearing a semantic role, it is then reasonable to expect that knowledge about semantic roles in a sentence will be informative of its syntactic structure, and that learning semantic role labels at the same time as parsing will be beneficial to parsing accuracy.", "labels": [], "entities": [{"text": "parsing", "start_pos": 338, "end_pos": 345, "type": "TASK", "confidence": 0.9556871652603149}, {"text": "accuracy", "start_pos": 346, "end_pos": 354, "type": "METRIC", "confidence": 0.9036601781845093}]}, {"text": "We present work to test the hypothesis that a current statistical parser can output rich information comprising both a parse tree and semantic role labels robustly, that is without any significant degradation of the parser's accuracy on the original parsing task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 225, "end_pos": 233, "type": "METRIC", "confidence": 0.9975770115852356}]}, {"text": "We achieve promising results both on the simple parsing task, where the accuracy of the parser is measured on the standard Parseval measures, and also on the parsing task where more complex labels comprising both syntactic labels and semantic roles are taken into account.", "labels": [], "entities": [{"text": "parsing task", "start_pos": 48, "end_pos": 60, "type": "TASK", "confidence": 0.7844121754169464}, {"text": "accuracy", "start_pos": 72, "end_pos": 80, "type": "METRIC", "confidence": 0.9993058443069458}, {"text": "parsing task", "start_pos": 158, "end_pos": 170, "type": "TASK", "confidence": 0.8996738791465759}]}, {"text": "These results have several consequences.", "labels": [], "entities": []}, {"text": "First, we show that it is possible to build a single integrated system successfully.", "labels": [], "entities": []}, {"text": "This is a meaningful achievement, as a task combining semantic role labelling and parsing is more complex than simple syntactic parsing.", "labels": [], "entities": [{"text": "semantic role labelling", "start_pos": 54, "end_pos": 77, "type": "TASK", "confidence": 0.6270702183246613}, {"text": "syntactic parsing", "start_pos": 118, "end_pos": 135, "type": "TASK", "confidence": 0.7782970666885376}]}, {"text": "While the shallow semantics of a constituent and its structural position are often correlated, they sometimes diverge.", "labels": [], "entities": []}, {"text": "For example, some nominal temporal modifiers occupy an object position without being objects, like Tuesday in the Penn Treebank representation of the sentence above.", "labels": [], "entities": [{"text": "Penn Treebank representation", "start_pos": 114, "end_pos": 142, "type": "DATASET", "confidence": 0.9779118498166403}]}, {"text": "The indirectness of the relation is also confirmed by the difficulty in exploiting semantic information for parsing.", "labels": [], "entities": []}, {"text": "Previous attempts have not been successful.", "labels": [], "entities": []}, {"text": "() report a reduction in parsing accuracy of an unlexicalised PCFG from 77.8% to 72.9% in using Penn Treebank function labels in training.", "labels": [], "entities": [{"text": "parsing", "start_pos": 25, "end_pos": 32, "type": "TASK", "confidence": 0.9696980714797974}, {"text": "accuracy", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.9477302432060242}, {"text": "Penn Treebank function labels", "start_pos": 96, "end_pos": 125, "type": "DATASET", "confidence": 0.9770753979682922}]}, {"text": "The two existing systems that use function labels sucessfully, either inherit Collins' modelling of the notion of complement) or model function labels directly ().", "labels": [], "entities": []}, {"text": "Furthermore, our results indicate that the proposed models are robust.", "labels": [], "entities": []}, {"text": "To model our task accurately, additional parameters must be estimated.", "labels": [], "entities": []}, {"text": "However, given the current limited availability of annotated treebanks, this more complex task will have to be solved with the same overall amount of data, aggravating the difficulty of estimating the model's parameters due to sparse data.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our extended semantic role SSN parser was trained on sections 2-21 and validated on section 24 from the PropBank.", "labels": [], "entities": [{"text": "SSN parser", "start_pos": 27, "end_pos": 37, "type": "TASK", "confidence": 0.8988626897335052}, {"text": "PropBank", "start_pos": 104, "end_pos": 112, "type": "DATASET", "confidence": 0.9769574403762817}]}, {"text": "Testing data are section 23 from the CoNLL-2005 shared task (Carreras and . We perform two different evaluations on our model trained on PropBank data.", "labels": [], "entities": [{"text": "CoNLL-2005 shared task", "start_pos": 37, "end_pos": 59, "type": "DATASET", "confidence": 0.7782594362894694}, {"text": "PropBank data", "start_pos": 137, "end_pos": 150, "type": "DATASET", "confidence": 0.9676458239555359}]}, {"text": "We distinguish between two parsing tasks: the PropBank parsing task and the PTB parsing task.", "labels": [], "entities": [{"text": "parsing", "start_pos": 27, "end_pos": 34, "type": "TASK", "confidence": 0.967103123664856}, {"text": "PropBank parsing task", "start_pos": 46, "end_pos": 67, "type": "TASK", "confidence": 0.7752668261528015}, {"text": "PTB parsing task", "start_pos": 76, "end_pos": 92, "type": "TASK", "confidence": 0.7751259207725525}]}, {"text": "To evaluate the former parsing task, we compute the standard Parseval measures of labelled recall and precision of constituents, taking into account not only the 33 original labels, but also the newly introduced PropBank labels.", "labels": [], "entities": [{"text": "parsing", "start_pos": 23, "end_pos": 30, "type": "TASK", "confidence": 0.9794953465461731}, {"text": "recall", "start_pos": 91, "end_pos": 97, "type": "METRIC", "confidence": 0.883358359336853}, {"text": "precision", "start_pos": 102, "end_pos": 111, "type": "METRIC", "confidence": 0.9988184571266174}, {"text": "PropBank labels", "start_pos": 212, "end_pos": 227, "type": "DATASET", "confidence": 0.9135764241218567}]}, {"text": "This evaluation gives us an indication of how accurately and exhaustively we can recover this richer set of non-terminal labels.", "labels": [], "entities": []}, {"text": "The results, computed on the testing data set from the PropBank, are shown in the PropBank column of, first line.", "labels": [], "entities": [{"text": "testing data set", "start_pos": 29, "end_pos": 45, "type": "DATASET", "confidence": 0.7795192996660868}, {"text": "PropBank", "start_pos": 55, "end_pos": 63, "type": "DATASET", "confidence": 0.8498544096946716}, {"text": "PropBank", "start_pos": 82, "end_pos": 90, "type": "DATASET", "confidence": 0.8107433319091797}]}, {"text": "To evaluate the PTB task, we ignore the set of PropBank semantic role labels that our model assigns to constituents (PTB column of, first line to be compared to the third line of the same column).", "labels": [], "entities": [{"text": "PTB task", "start_pos": 16, "end_pos": 24, "type": "TASK", "confidence": 0.7768935263156891}]}, {"text": "To our knowledge, no results have yet been published on parsing the PropBank.", "labels": [], "entities": [{"text": "PropBank", "start_pos": 68, "end_pos": 76, "type": "DATASET", "confidence": 0.9443086385726929}]}, {"text": "1 Accordingly, it is not possible to draw a straightforward quantitative comparison between our PropBank SSN parser and other PropBank parsers.", "labels": [], "entities": [{"text": "PropBank SSN parser", "start_pos": 96, "end_pos": 115, "type": "DATASET", "confidence": 0.8956371744473776}]}, {"text": "However, state-of-theart semantic role labelling systems) use parse trees output by state-of-the-art parsers), both for training and testing, and return partial trees annotated with semantic role labels.", "labels": [], "entities": []}, {"text": "An indirect way of comparing our parser with semantic role labellers suggests itself.", "labels": [], "entities": []}, {"text": "We merge the partial trees output by a semantic role labeller with the output of the parser on which it was trained, and compute PropBank parsing performance measures on the resulting parse trees.", "labels": [], "entities": [{"text": "PropBank parsing", "start_pos": 129, "end_pos": 145, "type": "TASK", "confidence": 0.6490333080291748}]}, {"text": "The third line, PropBank column of reports such measures summarised for the five best semantic role labelling systems) in the CoNLL 2005 shared task.", "labels": [], "entities": [{"text": "CoNLL 2005 shared task", "start_pos": 126, "end_pos": 148, "type": "DATASET", "confidence": 0.9274854958057404}]}, {"text": "These systems all use)'s parse trees both for training and testing, as well as various other information sources including sets of n-best parse trees, chunks, or named entities.", "labels": [], "entities": []}, {"text": "Thus, the partial trees output by these systems were merged with the parse trees returned by Charniak's parser (second line, PropBank column).", "labels": [], "entities": []}, {"text": "These results jointly confirm our initial hypothe- sis.", "labels": [], "entities": []}, {"text": "The performance on the parsing task (PTB column) does not appreciably deteriorate compared to a current state-of-the-art parser, even if our learner can output a much richer set of labels, and therefore solves a considerably more complex problem, suggesting that the relationship between syntactic PTB parsing and semantic PropBank parsing is strict enough that an integrated approach to the problem of semantic role labelling is beneficial.", "labels": [], "entities": [{"text": "parsing task", "start_pos": 23, "end_pos": 35, "type": "TASK", "confidence": 0.8945408165454865}, {"text": "syntactic PTB parsing", "start_pos": 288, "end_pos": 309, "type": "TASK", "confidence": 0.6553601821263632}, {"text": "semantic PropBank parsing", "start_pos": 314, "end_pos": 339, "type": "TASK", "confidence": 0.6447599728902181}]}, {"text": "Moreover, the results indicate that we can perform the more complex PropBank parsing task at levels of accuracy comparable to those achieved by the best semantic role labellers (PropBank column).", "labels": [], "entities": [{"text": "PropBank parsing task", "start_pos": 68, "end_pos": 89, "type": "TASK", "confidence": 0.7989412744839987}, {"text": "accuracy", "start_pos": 103, "end_pos": 111, "type": "METRIC", "confidence": 0.9980472326278687}]}, {"text": "This indicates that the model is robust, as it has been extended to a richer set of labels successfully, without increase in training data.", "labels": [], "entities": []}, {"text": "In fact, the limited availability of data is increased further by the high variability of the argumental labels A0-A5 whose semantics is specific to a given verb or a given verb sense.", "labels": [], "entities": []}, {"text": "Methodologically, these initial results on a joint solution to parsing and semantic role labelling provide the first direct test of whether parsing is necessary for semantic role labelling ().", "labels": [], "entities": [{"text": "parsing", "start_pos": 63, "end_pos": 70, "type": "TASK", "confidence": 0.9676896333694458}, {"text": "semantic role labelling", "start_pos": 75, "end_pos": 98, "type": "TASK", "confidence": 0.6072908341884613}, {"text": "semantic role labelling", "start_pos": 165, "end_pos": 188, "type": "TASK", "confidence": 0.6554326017697653}]}, {"text": "Comparing semantic role labelling based on chunked input to the better semantic role labels retrieved based on parsed trees, () conclude that parsing is necessary.", "labels": [], "entities": []}, {"text": "In an extensive experimental investigation of the different learning stages usually involved in semantic role labelling,) find instead that sophisticated chunking can achieve state-of-the-art results.", "labels": [], "entities": [{"text": "semantic role labelling", "start_pos": 96, "end_pos": 119, "type": "TASK", "confidence": 0.6519848903020223}]}, {"text": "Neither of these pieces of work actually used a parser to do SRL.", "labels": [], "entities": [{"text": "SRL", "start_pos": 61, "end_pos": 64, "type": "TASK", "confidence": 0.9873028993606567}]}, {"text": "Their investigation was therefore limited to establishing the usefulness of syntactic features for the SRL task.", "labels": [], "entities": [{"text": "SRL task", "start_pos": 103, "end_pos": 111, "type": "TASK", "confidence": 0.9479154050350189}]}, {"text": "Our results do not yet indicate that parsing is beneficial to SRL, but they show that the joint task can be performed successfully.", "labels": [], "entities": [{"text": "parsing", "start_pos": 37, "end_pos": 44, "type": "TASK", "confidence": 0.9872467517852783}, {"text": "SRL", "start_pos": 62, "end_pos": 65, "type": "TASK", "confidence": 0.9887112379074097}]}], "tableCaptions": [{"text": " Table 1: Percentage F-measure of our SSN parser on  PTB and PropBank parsing, compared to the origi- nal SSN parser and to the best CoNLL 2005 SR la- bellers.", "labels": [], "entities": [{"text": "Percentage", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9566511511802673}, {"text": "F-measure", "start_pos": 21, "end_pos": 30, "type": "METRIC", "confidence": 0.8119109869003296}, {"text": "PTB", "start_pos": 53, "end_pos": 56, "type": "DATASET", "confidence": 0.9696604013442993}, {"text": "PropBank", "start_pos": 61, "end_pos": 69, "type": "DATASET", "confidence": 0.776660680770874}, {"text": "CoNLL 2005 SR la- bellers", "start_pos": 133, "end_pos": 158, "type": "DATASET", "confidence": 0.9323184291521708}]}]}