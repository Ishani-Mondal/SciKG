{"title": [], "abstractContent": [{"text": "We present a novel parser combination scheme that works by reparsing input sentences once they have already been parsed by several different parsers.", "labels": [], "entities": []}, {"text": "We apply this idea to dependency and constituent parsing, generating results that surpass state-of-the-art accuracy levels for individual parsers.", "labels": [], "entities": [{"text": "constituent parsing", "start_pos": 37, "end_pos": 56, "type": "TASK", "confidence": 0.6242323666810989}, {"text": "accuracy", "start_pos": 107, "end_pos": 115, "type": "METRIC", "confidence": 0.9963326454162598}]}], "introductionContent": [{"text": "Over the past decade, remarkable progress has been made in data-driven parsing.", "labels": [], "entities": [{"text": "data-driven parsing", "start_pos": 59, "end_pos": 78, "type": "TASK", "confidence": 0.5001579821109772}]}, {"text": "Much of this work has been fueled by the availability of large corpora annotated with syntactic structures, especially the Penn Treebank (.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 123, "end_pos": 136, "type": "DATASET", "confidence": 0.9956513941287994}]}, {"text": "In fact, years of extensive research on training and testing parsers on the Wall Street Journal (WSJ) corpus of the Penn Treebank have resulted in the availability of several high-accuracy parsers.", "labels": [], "entities": [{"text": "Wall Street Journal (WSJ) corpus of the Penn Treebank", "start_pos": 76, "end_pos": 129, "type": "DATASET", "confidence": 0.947587332942269}]}, {"text": "We present a framework for combining the output of several different accurate parsers to produce results that are superior to those of each of the individual parsers.", "labels": [], "entities": []}, {"text": "This is done in a two stage process of reparsing.", "labels": [], "entities": []}, {"text": "In the first stage, m different parsers analyze an input sentence, each producing a syntactic structure.", "labels": [], "entities": []}, {"text": "In the second stage, a parsing algorithm is applied to the original sentence, taking into account the analyses produced by each parser in the first stage.", "labels": [], "entities": []}, {"text": "Our approach produces results with accuracy above those of the best individual parsers on both dependency and constituent parsing of the standard WSJ test set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9981529116630554}, {"text": "WSJ test set", "start_pos": 146, "end_pos": 158, "type": "DATASET", "confidence": 0.9356449047724406}]}], "datasetContent": [{"text": "In our dependency parsing experiments we used unlabeled dependencies extracted from the Penn Treebank using the same head-table as, using sections 02-21 as training data and section 23 as test data, following.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 7, "end_pos": 25, "type": "TASK", "confidence": 0.7899315059185028}, {"text": "Penn Treebank", "start_pos": 88, "end_pos": 101, "type": "DATASET", "confidence": 0.9964850544929504}]}, {"text": "Dependencies extracted from section 00 were used as held-out data, and section 22 was used as additional development data.", "labels": [], "entities": []}, {"text": "For constituent parsing, we used the section splits of the Penn Treebank as described above, as has become standard in statistical parsing research.", "labels": [], "entities": [{"text": "constituent parsing", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.6569474041461945}, {"text": "Penn Treebank", "start_pos": 59, "end_pos": 72, "type": "DATASET", "confidence": 0.9741423428058624}, {"text": "statistical parsing", "start_pos": 119, "end_pos": 138, "type": "TASK", "confidence": 0.7621085345745087}]}, {"text": "Six dependency parsers were used in our combination experiments, as described below.", "labels": [], "entities": []}, {"text": "The deterministic shift-reduce parsing algorithm of) was used to create two parsers 2 , one that processes the input sentence from left-to-right (LR), and one that goes from right-toleft (RL).", "labels": [], "entities": [{"text": "deterministic shift-reduce parsing", "start_pos": 4, "end_pos": 38, "type": "TASK", "confidence": 0.6244186758995056}]}, {"text": "Because this deterministic algorithm makes a single pass over the input string with no back-tracking, making decisions based on the parser's state and history, the order in which input tokens are considered affects the result.", "labels": [], "entities": []}, {"text": "Therefore, we achieve additional parser diversity with the same algorithm, simply by varying the direction of parsing.", "labels": [], "entities": []}, {"text": "We refer to the two parsers as LR and RL.", "labels": [], "entities": []}, {"text": "The deterministic parser of Yamada and Matsumoto uses an algorithm similar to Nivre and Scholz's, but it makes several successive leftto-right passes over the input instead of keeping a stack.", "labels": [], "entities": []}, {"text": "To increase parser diversity, we used aversion of Yamada and Matsumoto's algorithm where the direction of each of the consecutive passes over the input string alternates from left-to-right and right-to-left.", "labels": [], "entities": []}, {"text": "We refer to this parser as LRRL.", "labels": [], "entities": [{"text": "LRRL", "start_pos": 27, "end_pos": 31, "type": "METRIC", "confidence": 0.7056710124015808}]}, {"text": "The large-margin parser described in () was used with no alterations.", "labels": [], "entities": []}, {"text": "Unlike the deterministic parsers above, this parser uses a dynamic programming algorithm to determine the best tree, so there is no difference between presenting the input from left-to-right or right-to-left.", "labels": [], "entities": []}, {"text": "Three different weight configurations were considered: (1) giving all dependencies the same weight; (2) giving dependencies different weights, depending only on which parser generated the dependency; and (3) giving dependencies different weights, depending on which parser generated the dependency, and the part-of-speech of the dependent word.", "labels": [], "entities": []}, {"text": "Option 2 takes into consideration that parsers may have different levels of accuracy, and dependencies proposed by more accurate parsers should be counted more heavily.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 76, "end_pos": 84, "type": "METRIC", "confidence": 0.9988805651664734}]}, {"text": "Option 3 goes a step further, attempting to capitalize on the specific strengths of the different parsers.", "labels": [], "entities": []}, {"text": "The weights in option 2 are determined by computing the accuracy of each parser on the held-out set (WSJ section 00).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.999203622341156}, {"text": "WSJ section 00", "start_pos": 101, "end_pos": 115, "type": "DATASET", "confidence": 0.7815967798233032}]}, {"text": "The weights are simply the corresponding parser's accuracy (number of correct dependencies divided by the total number of dependencies).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 50, "end_pos": 58, "type": "METRIC", "confidence": 0.9963867664337158}]}, {"text": "The weights in option 3 are determined in a similar manner, but different accuracy figures are computed for each part-of-speech.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.999030351638794}]}, {"text": "shows the dependency accuracy and root accuracy (number of times the root of the dependency tree was identified correctly divided by the number of sentences) for each of the parsers, and for each of the different weight settings in the reparsing experiments (numbered according to their descriptions above  The parsers that were used in the constituent reparsing experiments are: (1) Charniak and Johnson's (2005) reranking parser; (2) Henderson's (2004) synchronous neural network parser; (3) Bikel's (2002) implementation of the Collins (1999) model 2 parser; and (4) two versions of Sagae and Lavie's (2005) shift-reduce parser, one using a maximum entropy classifier, and one using support vector machines.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.6917148232460022}, {"text": "accuracy", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.5715622901916504}, {"text": "synchronous neural network parser", "start_pos": 455, "end_pos": 488, "type": "TASK", "confidence": 0.6379417404532433}]}, {"text": "Henderson and Brill's voting scheme mentioned in section 3 can be emulated by our reparsing approach by setting all weights to 1.0 and t to (m + 1)/2, but better results can be obtained by setting appropriate weights and adjusting the precision/recall tradeoff.", "labels": [], "entities": [{"text": "precision", "start_pos": 235, "end_pos": 244, "type": "METRIC", "confidence": 0.9988793730735779}, {"text": "recall", "start_pos": 245, "end_pos": 251, "type": "METRIC", "confidence": 0.8762658834457397}]}, {"text": "Weights for different types of constituents from each parser can beset in a similar way to configuration 3 in the dependency experiments.", "labels": [], "entities": []}, {"text": "However, instead of measuring accuracy for each part-of-speech tag of dependents, we measure precision for each non-terminal label.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9992828965187073}, {"text": "precision", "start_pos": 93, "end_pos": 102, "type": "METRIC", "confidence": 0.9995468258857727}]}, {"text": "The parameter t is set using held-out data (from WSJ section 22) and a simple hill-climbing procedure.", "labels": [], "entities": [{"text": "WSJ section 22", "start_pos": 49, "end_pos": 63, "type": "DATASET", "confidence": 0.9263231754302979}]}, {"text": "First we sett to (m + 1)/2 (which heavily favors precision).", "labels": [], "entities": [{"text": "precision", "start_pos": 49, "end_pos": 58, "type": "METRIC", "confidence": 0.9992376565933228}]}, {"text": "We then repeatedly evaluate the combination of parsers, each time decreasing the value oft (by 0.01, say).", "labels": [], "entities": []}, {"text": "We record the values oft for which precision and recall were closest, and for which f-score was highest.", "labels": [], "entities": [{"text": "precision", "start_pos": 35, "end_pos": 44, "type": "METRIC", "confidence": 0.9995061159133911}, {"text": "recall", "start_pos": 49, "end_pos": 55, "type": "METRIC", "confidence": 0.9988904595375061}, {"text": "f-score", "start_pos": 84, "end_pos": 91, "type": "METRIC", "confidence": 0.9694046974182129}]}, {"text": "shows the accuracy of each individual parser and for three reparsing settings.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9995377063751221}]}, {"text": "Setting 1 is the emulation of Henderson and Brill's voting.", "labels": [], "entities": []}, {"text": "In setting 2, t is set for balancing precision and recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 37, "end_pos": 46, "type": "METRIC", "confidence": 0.9983654618263245}, {"text": "recall", "start_pos": 51, "end_pos": 57, "type": "METRIC", "confidence": 0.9984108209609985}]}, {"text": "In setting 3, t is set for highest f-score.: Precision, recall and f-score of each constituent parser and their combination under three different reparsing settings.", "labels": [], "entities": [{"text": "Precision", "start_pos": 45, "end_pos": 54, "type": "METRIC", "confidence": 0.9952303171157837}, {"text": "recall", "start_pos": 56, "end_pos": 62, "type": "METRIC", "confidence": 0.9968529343605042}]}], "tableCaptions": [{"text": " Table 1: Dependency accuracy and root accuracy of  individual dependency parsers and their combination  under three different weighted reparsing settings.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.8631748557090759}, {"text": "accuracy", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.7364360094070435}]}, {"text": " Table 2: Precision, recall and f-score of each constituent  parser and their combination under three different  reparsing settings.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9983880519866943}, {"text": "recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9980575442314148}, {"text": "f-score", "start_pos": 32, "end_pos": 39, "type": "METRIC", "confidence": 0.9694784283638}]}]}