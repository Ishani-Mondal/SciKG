{"title": [{"text": "Evaluation of Utility of LSA for Word Sense Discrimination", "labels": [], "entities": [{"text": "Word Sense Discrimination", "start_pos": 33, "end_pos": 58, "type": "TASK", "confidence": 0.697288324435552}]}], "abstractContent": [{"text": "The goal of the ongoing project described in this paper is evaluation of the utility of Latent Semantic Analysis (LSA) for unsupervised word sense discrimination.", "labels": [], "entities": [{"text": "word sense discrimination", "start_pos": 136, "end_pos": 161, "type": "TASK", "confidence": 0.6640090048313141}]}, {"text": "The hypothesis is that LSA can be used to compute context vectors for ambiguous words that can be clustered together with each cluster corresponding to a different sense of the word.", "labels": [], "entities": []}, {"text": "In this paper we report first experimental result on tightness, separation and purity of sense-based clusters as a function of vector space dimensionality and using different distance metrics.", "labels": [], "entities": [{"text": "purity", "start_pos": 79, "end_pos": 85, "type": "METRIC", "confidence": 0.9897221922874451}]}], "introductionContent": [{"text": "Latent semantic analysis (LSA) is a mathematical technique used in natural language processing for finding complex and hidden relations of meaning among words and the various contexts in which they are found).", "labels": [], "entities": [{"text": "Latent semantic analysis (LSA)", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.8435452580451965}]}, {"text": "LSA is based on the idea of association of elements (words) with contexts and similarity in word meaning is defined by similarity in shared contexts.", "labels": [], "entities": []}, {"text": "The starting point for LSA is the construction of a co-occurrence matrix, where the columns represent the different contexts in the corpus, and the rows the different word tokens.", "labels": [], "entities": []}, {"text": "An entry ij in the matrix corresponds to the count of the number of times the word token i appeared in context j.", "labels": [], "entities": []}, {"text": "Often the co-occurrence matrix is normalized for document length and word entropy.", "labels": [], "entities": []}, {"text": "The critical step of the LSA algorithm is to compute the singular value decomposition (SVD) of the normalized co-occurrence matrix.", "labels": [], "entities": [{"text": "singular value decomposition (SVD)", "start_pos": 57, "end_pos": 91, "type": "METRIC", "confidence": 0.6610994140307108}]}, {"text": "If the matrices comprising the SVD are permuted such that the singular values are in decreasing order, they can be truncated to a much lower rank.", "labels": [], "entities": []}, {"text": "According to, it is this dimensionality reduction step, the combining of surface information into a deeper abstraction that captures the mutual implications of words and passages and uncovers important structural aspects of a problem while filtering out noise.", "labels": [], "entities": []}, {"text": "The singular vectors reflect principal components, or axes of greatest variance in the data, constituting the hidden abstract concepts of the semantic space, and each word and each document is represented as a linear combination of these concepts.", "labels": [], "entities": []}, {"text": "Within the LSA framework discreet entities such as words and documents are mapped into the same continuous low-dimensional parameter space, revealing the underlying semantic structure of these entities and making it especially efficient for variety of machine-learning algorithms.", "labels": [], "entities": []}, {"text": "Following successful application of LSA to information retrieval other areas of application of the same methodology have been explored, including language modeling, word and document clustering, call routing and semantic inference for spoken interface control).", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 43, "end_pos": 64, "type": "TASK", "confidence": 0.7805369198322296}, {"text": "language modeling", "start_pos": 146, "end_pos": 163, "type": "TASK", "confidence": 0.7073831409215927}, {"text": "word and document clustering", "start_pos": 165, "end_pos": 193, "type": "TASK", "confidence": 0.623512752354145}, {"text": "call routing", "start_pos": 195, "end_pos": 207, "type": "TASK", "confidence": 0.8377382457256317}]}, {"text": "The ultimate goal of the project described here is to explore the use of LSA for unsupervised identification of word senses and for estimating word sense frequencies from application relevant corpora following context-group discrimination paradigm.", "labels": [], "entities": []}, {"text": "In this paper we describe a first set of experiments investigating the tightness, separation and purity properties of sensebased clusters.", "labels": [], "entities": [{"text": "purity", "start_pos": 97, "end_pos": 103, "type": "METRIC", "confidence": 0.956684947013855}]}], "datasetContent": [{"text": "The basic idea of the context-group discrimination paradigm adopted in this investigation is to induce senses of ambiguous word from their contextual similarity.", "labels": [], "entities": []}, {"text": "The occurrences of an ambiguous word represented by their context vectors are grouped into clusters, where clusters consist of contextually similar occurrences.", "labels": [], "entities": []}, {"text": "The context vectors in our experiments are LSA-based representation of the documents in which the ambiguous word appears.", "labels": [], "entities": []}, {"text": "Context vectors from the training portion of the corpus are grouped into clusters and the centroid of the cluster-the sense vector-is computed.", "labels": [], "entities": []}, {"text": "Ambiguous words from the test portion of the corpus are disambiguated by finding the closest sense vector (cluster centroid) to its context vector representation.", "labels": [], "entities": []}, {"text": "If sense labels are available for the ambiguous words in the corpus, sense vectors are given a label that corresponds to the majority sense in their cluster, and sense discrimination accuracy can be evaluated by computing the percentage of ambiguous words from the test portion that were mapped to the sense vector whose label corresponds to the ambiguous word's sense label.", "labels": [], "entities": [{"text": "sense discrimination", "start_pos": 162, "end_pos": 182, "type": "TASK", "confidence": 0.6606866270303726}, {"text": "accuracy", "start_pos": 183, "end_pos": 191, "type": "METRIC", "confidence": 0.9289035797119141}]}, {"text": "Our goal is to investigate how well the different senses of ambiguous words are separated in the LSA-based vector space.", "labels": [], "entities": []}, {"text": "With an ideal representation the clusters of context vectors would be tight (the vectors in the cluster close to each other and close to centroid of the cluster), and faraway from each other, and each cluster would be pure, i.e., consisting of vectors corresponding to words with the same sense.", "labels": [], "entities": []}, {"text": "Since we don't want the evaluation of the LSA-based representation to be influenced by the choice of clustering algorithm, or the algorithm's initialization and its parameter settings that determine the resulting grouping, we took an orthogonal approach to the problem: Instead of evaluating the purity of the clusters based on geometrical position of vectors, we evaluate how wellformed the clusters based on sense labels are, how separated from each other and tight they are.", "labels": [], "entities": []}, {"text": "As will be discussed below, performance evaluation of such sense-based clusters results in an upper bound on the performance that can be obtained by clustering algorithms such as EM or K-means.", "labels": [], "entities": []}], "tableCaptions": []}