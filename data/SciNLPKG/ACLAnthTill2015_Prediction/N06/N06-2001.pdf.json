{"title": [], "abstractContent": [{"text": "We present anew type of neural proba-bilistic language model that learns a mapping from both words and explicit word features into a continuous space that is then used for word prediction.", "labels": [], "entities": [{"text": "word prediction", "start_pos": 172, "end_pos": 187, "type": "TASK", "confidence": 0.7808208167552948}]}, {"text": "Additionally , we investigate several ways of deriving continuous word representations for unknown words from those of known words.", "labels": [], "entities": []}, {"text": "The resulting model significantly reduces perplexity on sparse-data tasks when compared to standard backoff models , standard neural language models, and factored language models.", "labels": [], "entities": []}], "introductionContent": [{"text": "Neural language models (NLMs) () map words into a continuous representation space and then predict the probability of a word given the continuous representations of the preceding words in the history.", "labels": [], "entities": []}, {"text": "They have previously been shown to outperform standard back-off models in terms of perplexity and word error rate on medium and large speech recognition tasks (.", "labels": [], "entities": [{"text": "word error rate", "start_pos": 98, "end_pos": 113, "type": "METRIC", "confidence": 0.6893015305201212}, {"text": "speech recognition tasks", "start_pos": 134, "end_pos": 158, "type": "TASK", "confidence": 0.786008616288503}]}, {"text": "Their main drawbacks are computational complexity and the fact that only distributional information (word context) is used to generalize over words, whereas other word properties (e.g. spelling, morphology etc.) are ignored for this purpose.", "labels": [], "entities": []}, {"text": "Thus, there is also no principled way of handling out-of-vocabulary (OOV) words.", "labels": [], "entities": []}, {"text": "Though this maybe sufficient for applications that use a closed vocabulary, the current trend of porting systems to a wider range of languages (esp.", "labels": [], "entities": []}, {"text": "highlyinflected languages such as Arabic) calls for dynamic dictionary expansion and the capability of assigning probabilities to newly added words without having seen them in the training data.", "labels": [], "entities": [{"text": "dictionary expansion", "start_pos": 60, "end_pos": 80, "type": "TASK", "confidence": 0.7117424607276917}]}, {"text": "Here, we introduce a novel type of NLM that improves generalization by using vectors of word features (stems, affixes, etc.) as input, and we investigate deriving continuous representations for unknown words from those of known words.", "labels": [], "entities": []}, {"text": "A standard NLM ( takes as input the previous n \u2212 1 words, which select rows from a continuous word representation matrix M . The next layer's input i is the concatenation of the rows in M corresponding to the input words.", "labels": [], "entities": []}, {"text": "Backpropagation (BKP) is used to learn model parame-ters, including the M matrix, which is shared across input words.", "labels": [], "entities": [{"text": "Backpropagation (BKP)", "start_pos": 0, "end_pos": 21, "type": "METRIC", "confidence": 0.8638605177402496}]}, {"text": "The training criterion maximizes the regularized log-likelihood of the training data.", "labels": [], "entities": []}], "datasetContent": [{"text": "We first investigated how the different OOV handling methods affect the average probability assigned to words with OOVs in their context.", "labels": [], "entities": [{"text": "OOV handling", "start_pos": 40, "end_pos": 52, "type": "TASK", "confidence": 0.847100168466568}]}, {"text": "shows that average probabilities increase compared to the strategy described in Section 3 as well as other baseline models (standard backoff trigrams and FLM, further described below), with the strongest increase observed for the scheme using the least frequent factor as an OOV factor model.", "labels": [], "entities": [{"text": "FLM", "start_pos": 154, "end_pos": 157, "type": "METRIC", "confidence": 0.7351022362709045}, {"text": "OOV", "start_pos": 275, "end_pos": 278, "type": "METRIC", "confidence": 0.9564884305000305}]}, {"text": "This strategy is used for the models in the following perplexity experiments.", "labels": [], "entities": []}, {"text": "We compare the perplexity of word-based and factor-based NLMs with standard backoff trigrams, class-based trigrams, FLMs, and interpolated models.", "labels": [], "entities": []}, {"text": "Evaluation was done with (the \"w/unk\" column in) and without (the \"no unk\" column) scoring of OOVs, in order to assess the usefulness of our approach to applications using closed vs. open vocabularies.", "labels": [], "entities": []}, {"text": "The baseline Model 1 is a standard backoff 3-gram using modified Kneser-Ney smoothing (model orders beyond 3 did not improve perplexity).", "labels": [], "entities": []}, {"text": "Model 2 is a class-based trigram model with Brown clustering (256 classes), which, when interpolated with the baseline 3-gram, reduces the perplexity (see row 3).", "labels": [], "entities": []}, {"text": "Model 3 is a 3-gram word-based NLM (with output unit clustering).", "labels": [], "entities": []}, {"text": "For NLMs, higher model orders gave improvements, demonstrating their better scalability: for ECA, a 6-gram (w/o unk) and a 5-gram (w/unk) were used; for Turkish, a 7-gram (w/o unk) and a 5-gram (w/unk) were used.", "labels": [], "entities": [{"text": "ECA", "start_pos": 93, "end_pos": 96, "type": "DATASET", "confidence": 0.9254535436630249}]}, {"text": "Though worse in isolation, the word-based NLMs reduce perplexity considerably when interpolated with Model 1.", "labels": [], "entities": []}, {"text": "The FLM baseline is a handoptimized 3-gram FLM (Model 5); we also tested an FLM optimized with a genetic algorithm as de-", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Average probability (scaled by 10 2 ) of known words", "labels": [], "entities": [{"text": "Average probability", "start_pos": 10, "end_pos": 29, "type": "METRIC", "confidence": 0.9390688240528107}]}]}