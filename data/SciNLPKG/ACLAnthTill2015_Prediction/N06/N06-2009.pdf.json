{"title": [{"text": "Answering the Question You Wish They Had Asked: The Impact of Paraphrasing for Question Answering", "labels": [], "entities": [{"text": "Answering the Question You Wish They", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.9009844859441122}, {"text": "Question Answering", "start_pos": 79, "end_pos": 97, "type": "TASK", "confidence": 0.7116037011146545}]}], "abstractContent": [{"text": "State-of-the-art Question Answering (QA) systems are very sensitive to variations in the phrasing of an information need.", "labels": [], "entities": [{"text": "Question Answering (QA)", "start_pos": 17, "end_pos": 40, "type": "TASK", "confidence": 0.8525577247142792}]}, {"text": "Finding the preferred language for such a need is a valuable task.", "labels": [], "entities": []}, {"text": "We investigate that claim by adopting a simple MT-based paraphrasing technique and evaluating QA system performance on paraphrased questions.", "labels": [], "entities": [{"text": "MT-based paraphrasing", "start_pos": 47, "end_pos": 68, "type": "TASK", "confidence": 0.8745140731334686}]}, {"text": "We found a potential increase of 35% in MRR with respect to the original question.", "labels": [], "entities": [{"text": "MRR", "start_pos": 40, "end_pos": 43, "type": "METRIC", "confidence": 0.6444239616394043}]}], "introductionContent": [{"text": "Ina typical Question Answering system, an input question is analyzed to formulate a query to retrieve relevant documents from a target corpus ().", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 12, "end_pos": 30, "type": "TASK", "confidence": 0.7807847559452057}]}, {"text": "This analysis of the input question affects the subset of documents that will be examined and ultimately plays a key role in determining the answers the system chooses to produce.", "labels": [], "entities": []}, {"text": "However, most existing QA systems, whether they adopt knowledge-based, statistical, or hybrid methods, are very sensitive to small variations in the question form, often yielding substantially different answers for questions that are semantically equivalent.", "labels": [], "entities": []}, {"text": "For example, our system's answer to \"Who invented the telephone?\" is \"Alexander Graham Bell;\" however, its top answer to a paraphrase of the above question \"Who is credited with the invention of the telephone?\" is \"Gutenberg,\" who is credited with the invention of the printing press, while \"Alexander Graham Bell,\" who is credited with the invention of the telephone, appears in rank four.", "labels": [], "entities": []}, {"text": "To demonstrate the ubiquity of this phenomenon, we asked the aforementioned two questions to several QA systems on the web, including LCC's PowerAnswer system, 1 MIT's START system, 2 AnswerBus, and Ask Jeeves.", "labels": [], "entities": []}, {"text": "All systems exhibited different behavior for the two phrasings of the question, ranging from minor variations in documents presented to justify an answer, to major differences such as the presence of correct answers in the answer list.", "labels": [], "entities": []}, {"text": "For some systems, the more complex question form posed sufficient difficulty that they chose not to answer it.", "labels": [], "entities": []}, {"text": "In this paper we focus on investigating a high risk but potentially high payoff approach, that of improving system performance by replacing the user question with a paraphrased version of it.", "labels": [], "entities": []}, {"text": "To obtain candidate paraphrases, we adopt a simple yet powerful technique based on machine translation, which we describe in the next section.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 83, "end_pos": 102, "type": "TASK", "confidence": 0.7094326317310333}]}, {"text": "Our experimental results show that we can potentially achieve a 35% relative improvement in system performance if we have an oracle that always picks the optimal paraphrase for each question.", "labels": [], "entities": []}, {"text": "Our ultimate goal is to automatically select from the set of candidates a high potential paraphrase using a component trained against the QA system.", "labels": [], "entities": []}, {"text": "In Section 3, we present our initial approach to paraphrase selection which shows that, despite the tremendous odds against selecting performance-improving paraphrases, our conservative selection algorithm resulted in marginal improvement in system performance.", "labels": [], "entities": [{"text": "paraphrase selection", "start_pos": 49, "end_pos": 69, "type": "TASK", "confidence": 0.9718234241008759}]}], "datasetContent": [{"text": "We trained the paraphrase selection module using our QA system, PIQUANT).", "labels": [], "entities": [{"text": "paraphrase selection", "start_pos": 15, "end_pos": 35, "type": "TASK", "confidence": 0.8809188604354858}, {"text": "PIQUANT", "start_pos": 64, "end_pos": 71, "type": "METRIC", "confidence": 0.6641031503677368}]}, {"text": "Our target corpus is the AQUAINT corpus, employed in the TREC QA track since 2002.", "labels": [], "entities": [{"text": "AQUAINT corpus", "start_pos": 25, "end_pos": 39, "type": "DATASET", "confidence": 0.9153416752815247}, {"text": "TREC QA track", "start_pos": 57, "end_pos": 70, "type": "DATASET", "confidence": 0.8530435959498087}]}, {"text": "As for MT engines, we employed Babelfish and Google MT, 8 rule-based systems developed by SY ST R A N and Google, respectively.", "labels": [], "entities": [{"text": "MT engines", "start_pos": 7, "end_pos": 17, "type": "TASK", "confidence": 0.9285571277141571}, {"text": "Google MT", "start_pos": 45, "end_pos": 54, "type": "DATASET", "confidence": 0.854114443063736}]}, {"text": "We adopted different MT engines based on the hypothesis that differences in their translation rules will improve the effectiveness of the paraphrasing module.", "labels": [], "entities": [{"text": "MT", "start_pos": 21, "end_pos": 23, "type": "TASK", "confidence": 0.9671776294708252}]}, {"text": "To measure performance, we trained and tested by cross-validation over 712 questions from the TREC 9 and 10 datasets.", "labels": [], "entities": [{"text": "TREC 9 and 10 datasets", "start_pos": 94, "end_pos": 116, "type": "DATASET", "confidence": 0.9233531355857849}]}, {"text": "We paraphrased the questions using the four possible combinations of MT engines with up to 11 intermediate languages, obtaining a total of 15,802 paraphrases.", "labels": [], "entities": []}, {"text": "These questions were then fed to our system and evaluated per TREC answer key.", "labels": [], "entities": [{"text": "TREC answer key", "start_pos": 62, "end_pos": 77, "type": "METRIC", "confidence": 0.838932991027832}]}, {"text": "We obtained a baseline MRR (top five answers) of 0.345 running over the original questions.", "labels": [], "entities": [{"text": "MRR (top five answers)", "start_pos": 23, "end_pos": 45, "type": "METRIC", "confidence": 0.8335473040739695}]}, {"text": "An oracle run, in which the best paraphrase (or the original question) is always picked would yield a MRR of 0.48.", "labels": [], "entities": [{"text": "MRR", "start_pos": 102, "end_pos": 105, "type": "METRIC", "confidence": 0.998130738735199}]}, {"text": "This potential increase is substantial, taking into account that a 35% improvement separated the tenth participant from the second in TREC-9.", "labels": [], "entities": [{"text": "TREC-9", "start_pos": 134, "end_pos": 140, "type": "DATASET", "confidence": 0.5960558652877808}]}, {"text": "Our three-fold cross validation using the features and algorithm described in Section 3 yielded a MRR of 0.347.", "labels": [], "entities": [{"text": "MRR", "start_pos": 98, "end_pos": 101, "type": "METRIC", "confidence": 0.9815059900283813}]}, {"text": "Over 712 questions, it replaced 14, two of which improved performance, the rest stayed the same.", "labels": [], "entities": []}, {"text": "On the other hand, random selection of paraphrases decreased performance to 0.156, clearly showing the importance of selecting a good paraphrase.", "labels": [], "entities": []}], "tableCaptions": []}