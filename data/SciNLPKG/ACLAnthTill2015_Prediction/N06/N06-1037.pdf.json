{"title": [{"text": "Exploring Syntactic Features for Relation Extraction using a Convolution Tree Kernel", "labels": [], "entities": [{"text": "Relation Extraction", "start_pos": 33, "end_pos": 52, "type": "TASK", "confidence": 0.9751091599464417}]}], "abstractContent": [{"text": "This paper proposes to use a convolution kernel over parse trees to model syntactic structure information for relation extraction.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 110, "end_pos": 129, "type": "TASK", "confidence": 0.84157794713974}]}, {"text": "Our study reveals that the syntactic structure features embedded in a parse tree are very effective for relation extraction and these features can be well captured by the convolution tree kernel.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 104, "end_pos": 123, "type": "TASK", "confidence": 0.8644795417785645}]}, {"text": "Evaluation on the ACE 2003 corpus shows that the convolution kernel over parse trees can achieve comparable performance with the previous best-reported feature-based methods on the 24 ACE relation subtypes.", "labels": [], "entities": [{"text": "ACE 2003 corpus", "start_pos": 18, "end_pos": 33, "type": "DATASET", "confidence": 0.9651485880215963}]}, {"text": "It also shows that our method significantly outperforms the previous two dependency tree kernels on the 5 ACE relation major types.", "labels": [], "entities": []}], "introductionContent": [{"text": "Relation extraction is a subtask of information extraction that finds various predefined semantic relations, such as location, affiliation, rival, etc., between pairs of entities in text.", "labels": [], "entities": [{"text": "Relation extraction", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.9243430495262146}, {"text": "information extraction that finds various predefined semantic relations, such as location, affiliation, rival, etc., between pairs of entities in text", "start_pos": 36, "end_pos": 186, "type": "Description", "confidence": 0.8015958571434021}]}, {"text": "For example, the sentence \"George Bush is the president of the United States.\" conveys the semantic relation \"President\" between the entities \"George Bush\" (PER) and \"the United States\" (GPE: a Geo-Political Entity ---an entity with land and a government).", "labels": [], "entities": []}, {"text": "Prior feature-based methods for this task) employed a large amount of diverse linguistic features, varying from lexical knowledge, entity mention information to syntactic parse trees, dependency trees and semantic features.", "labels": [], "entities": []}, {"text": "Since a parse tree contains rich syntactic structure information, in principle, the features extracted from a parse tree should contribute much more to performance improvement for relation extraction.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 180, "end_pos": 199, "type": "TASK", "confidence": 0.8530543148517609}]}, {"text": "However it is reported () that hierarchical structured syntactic features contributes less to performance improvement.", "labels": [], "entities": []}, {"text": "This maybe mainly due to the fact that the syntactic structure information in a parse tree is hard to explicitly describe by a vector of linear features.", "labels": [], "entities": []}, {"text": "As an alternative, kernel methods) provide an elegant solution to implicitly explore tree structure features by directly computing the similarity between two trees.", "labels": [], "entities": []}, {"text": "But to our surprise, the sole two-reported dependency tree kernels for relation extraction on the ACE corpus () showed much lower performance than the feature-based methods.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 71, "end_pos": 90, "type": "TASK", "confidence": 0.8755272924900055}, {"text": "ACE corpus", "start_pos": 98, "end_pos": 108, "type": "DATASET", "confidence": 0.9703567624092102}]}, {"text": "One may ask: are the syntactic tree features very useful for relation extraction?", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 61, "end_pos": 80, "type": "TASK", "confidence": 0.9230431914329529}]}, {"text": "Can tree kernel methods effectively capture the syntactic tree features and other various features that have been proven useful in the feature-based methods?", "labels": [], "entities": []}, {"text": "In this paper, we demonstrate the effectiveness of the syntactic tree features for relation extraction and study how to capture such features via a convolution tree kernel.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 83, "end_pos": 102, "type": "TASK", "confidence": 0.898140549659729}]}, {"text": "We also study how to select the optimal feature space (e.g. the set of sub-trees to represent relation instances) to optimize the system performance.", "labels": [], "entities": []}, {"text": "The experimental results show that the convolution tree kernel plus entity features achieves slightly better performance than the previous best-reported feature-based methods.", "labels": [], "entities": []}, {"text": "It also shows that our method significantly outperforms the two dependency tree kernels () on the 5 ACE relation types.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we review the previous work.", "labels": [], "entities": []}, {"text": "Section 3 discusses our tree kernel based learning algorithm.", "labels": [], "entities": []}, {"text": "Section 4 shows the experimental results and compares our work with the related work.", "labels": [], "entities": []}, {"text": "We conclude our work in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "The aim of our experiment is to verify the effectiveness of using richer syntactic structures and the convolution tree kernel for relation extraction.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 130, "end_pos": 149, "type": "TASK", "confidence": 0.8528595864772797}]}, {"text": "Corpus: we use the official ACE corpus for 2003 evaluation from LDC as our test corpus.", "labels": [], "entities": [{"text": "ACE corpus for 2003 evaluation from LDC", "start_pos": 28, "end_pos": 67, "type": "DATASET", "confidence": 0.8531958290508815}]}, {"text": "The ACE corpus is gathered from various newspaper, newswire and broadcasts.", "labels": [], "entities": [{"text": "ACE corpus", "start_pos": 4, "end_pos": 14, "type": "DATASET", "confidence": 0.8487052917480469}]}, {"text": "The same as previous work, our experiments are carried out on explicit relations due to the poor inter-annotator agreement in annotation of implicit relations and their limited numbers.", "labels": [], "entities": []}, {"text": "The training set consists of 674 annotated text documents and 9683 relation instances.", "labels": [], "entities": []}, {"text": "The test set consists of 97 documents and 1386 relation instances.", "labels": [], "entities": []}, {"text": "The 2003 evaluation defined 5 types of entities: Persons, Organizations, Locations, Facilities and GPE.", "labels": [], "entities": [{"text": "GPE", "start_pos": 99, "end_pos": 102, "type": "METRIC", "confidence": 0.8560907244682312}]}, {"text": "Each mention of an entity is associated with a mention type: proper name, nominal or pronoun.", "labels": [], "entities": []}, {"text": "They further defined 5 major relation types and 24 subtypes: AT (Base-In, Located\u2026), NEAR (Relative-Location), PART (Part-of, Subsidiary \u2026), ROLE (Member, Owner \u2026) and SOCIAL (Associate, Parent\u2026).", "labels": [], "entities": [{"text": "AT", "start_pos": 61, "end_pos": 63, "type": "METRIC", "confidence": 0.9934542179107666}, {"text": "NEAR", "start_pos": 85, "end_pos": 89, "type": "METRIC", "confidence": 0.9908223152160645}, {"text": "PART", "start_pos": 111, "end_pos": 115, "type": "METRIC", "confidence": 0.9812881946563721}, {"text": "ROLE", "start_pos": 141, "end_pos": 145, "type": "METRIC", "confidence": 0.9927771091461182}]}, {"text": "As previous work, we explicitly model the argument order of the two mentions involved.", "labels": [], "entities": []}, {"text": "We thus model relation extraction as a multi-class classification problem with 10 classes on the major types (2 for each relation major type and a \"NONE\" class for nonrelation (except 1 symmetric type)) and 43 classes on the subtypes (2 for each relation subtype and a \"NONE\" class for non-relation (except 6 symmetric subtypes)).", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 14, "end_pos": 33, "type": "TASK", "confidence": 0.7912757396697998}]}, {"text": "In this paper, we only measure the performance of relation extraction models on \"true\" mentions with \"true\" chaining of coreference (i.e. as annotated by LDC annotators).", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 50, "end_pos": 69, "type": "TASK", "confidence": 0.825912594795227}]}, {"text": "Classifier: we select SVM as the classifier used in this paper since SVM can naturally work with kernel methods and it also represents the state-of-theart machine learning algorithm.", "labels": [], "entities": []}, {"text": "We adopt the one vs. others strategy and select the one with largest margin as the final answer.", "labels": [], "entities": []}, {"text": "The training parameters are chosen using cross-validation (C=2.4 (SVM); \u03bb =0.4(tree kernel)).", "labels": [], "entities": []}, {"text": "In our implementation, we use the binary SVMLight developed by and Tree Kernel Toolkits developed by.", "labels": [], "entities": []}, {"text": "Kernel Normalization: since the size of a parse tree is not constant, we normalize 1 2 ( , ) Evaluation Method: we parse the sentence using Charniak parser and iterate overall pair of mentions occurring in the same sentence to generate potential instances.", "labels": [], "entities": []}, {"text": "We find the negative samples are 10 times more than the positive samples.", "labels": [], "entities": []}, {"text": "Thus data imbalance and sparseness are potential problems.", "labels": [], "entities": []}, {"text": "Recall (R), Precision (P) and F-measure (F) are adopted as the performance measure.", "labels": [], "entities": [{"text": "Recall (R)", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9557188749313354}, {"text": "Precision (P)", "start_pos": 12, "end_pos": 25, "type": "METRIC", "confidence": 0.9529773890972137}, {"text": "F-measure (F)", "start_pos": 30, "end_pos": 43, "type": "METRIC", "confidence": 0.9659851640462875}]}, {"text": "In order to study the impact of the sole syntactic structure information embedded in parse trees on relation extraction, we remove the entity information from parse trees by replacing the entity-related phrase type (\"E1-O-PER\", etc., in) with \"NP\".", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 100, "end_pos": 119, "type": "TASK", "confidence": 0.7886222302913666}]}, {"text": "Then we carryout a couple of preliminary experiments on the test set using parse trees regardless of entity information..", "labels": [], "entities": []}, {"text": "Performance of seven relation feature spaces over the 5 ACE major types using parse tree information only reports the performance of our defined seven relation feature spaces over the 5 ACE major types using parse tree information regardless of any entity information.", "labels": [], "entities": []}, {"text": "This preliminary experiments show that:", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. Performance of seven relation feature  spaces over the 5 ACE major types using parse  tree information only", "labels": [], "entities": []}, {"text": " Table 2. Performance of Path-enclosed Trees  with different setups over the 5 ACE major types", "labels": [], "entities": []}, {"text": " Table 4. Error Distribution  Finally,", "labels": [], "entities": [{"text": "Error", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9388949871063232}]}, {"text": " Table 2. It shows  that 85.9% (587/684) of the errors result from rela- tion detection and only 14.1% (97/684) of the er- rors result from relation characterization. This is  mainly due to the imbalance of the posi- tive/negative instances and the sparseness of some  relation types on the ACE corpus.", "labels": [], "entities": [{"text": "rela- tion detection", "start_pos": 67, "end_pos": 87, "type": "TASK", "confidence": 0.591682955622673}, {"text": "relation characterization", "start_pos": 140, "end_pos": 165, "type": "TASK", "confidence": 0.8423730134963989}, {"text": "ACE corpus", "start_pos": 291, "end_pos": 301, "type": "DATASET", "confidence": 0.9693500399589539}]}]}