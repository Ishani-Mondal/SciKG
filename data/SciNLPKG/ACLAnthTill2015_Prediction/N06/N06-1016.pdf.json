{"title": [{"text": "An Empirical Study of the Behavior of Active Learning for Word Sense Disambiguation", "labels": [], "entities": [{"text": "Word Sense Disambiguation", "start_pos": 58, "end_pos": 83, "type": "TASK", "confidence": 0.7431159118811289}]}], "abstractContent": [{"text": "This paper shows that two uncertainty-based active learning methods, combined with a maximum entropy model, work well on learning English verb senses.", "labels": [], "entities": []}, {"text": "Data analysis on the learning process, based on both instance and feature levels, suggests that a careful treatment of feature extraction is important for the active learning to be useful for WSD.", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 119, "end_pos": 137, "type": "TASK", "confidence": 0.7202353179454803}, {"text": "WSD", "start_pos": 192, "end_pos": 195, "type": "TASK", "confidence": 0.9801691174507141}]}, {"text": "The overfitting phenomena that occurred during the active learning process are identified as classic overfitting in machine learning based on the data analysis.", "labels": [], "entities": []}], "introductionContent": [{"text": "Corpus-based methods for word sense disambiguation (WSD) have gained popularity in recent years.", "labels": [], "entities": [{"text": "word sense disambiguation (WSD)", "start_pos": 25, "end_pos": 56, "type": "TASK", "confidence": 0.8395580848058065}]}, {"text": "As evidenced by the SENSEVAL exercises (http://www.senseval.org), machine learning models supervised by sense-tagged training corpora tend to perform better on the lexical sample tasks than unsupervised methods.", "labels": [], "entities": []}, {"text": "However, WSD tasks typically have very limited amounts of training data due to the fact that creating large-scale high-quality sense-tagged corpora is difficult and time-consuming.", "labels": [], "entities": [{"text": "WSD tasks", "start_pos": 9, "end_pos": 18, "type": "TASK", "confidence": 0.9203744232654572}]}, {"text": "Therefore, the lack of sufficient labeled training data has become a major hurdle to improving the performance of supervised WSD.", "labels": [], "entities": [{"text": "WSD", "start_pos": 125, "end_pos": 128, "type": "TASK", "confidence": 0.9282568693161011}]}, {"text": "A promising method for solving this problem could be the use of active learning.", "labels": [], "entities": []}, {"text": "Researchers use active learning methods to minimize the labeling of examples by human annotators.", "labels": [], "entities": []}, {"text": "A decrease in overall labeling occurs because active learners (the machine learning models used in active learning) pick more informative examples for the target word (a word whose senses need to be learned) than those that would be picked randomly.", "labels": [], "entities": []}, {"text": "Active learning requires human labeling of the newly selected training data to ensure high quality.", "labels": [], "entities": []}, {"text": "We focus hereon pool-based active learning where there is an abundant supply of unlabeled data, but where the labeling process is expensive.", "labels": [], "entities": []}, {"text": "In NLP problems such as text classification (, statistical parsing), information extraction (, and named entity recognition), pool-based active learning has produced promising results.", "labels": [], "entities": [{"text": "text classification", "start_pos": 24, "end_pos": 43, "type": "TASK", "confidence": 0.8206357359886169}, {"text": "statistical parsing)", "start_pos": 47, "end_pos": 67, "type": "TASK", "confidence": 0.8585895498593649}, {"text": "information extraction", "start_pos": 69, "end_pos": 91, "type": "TASK", "confidence": 0.8431751728057861}, {"text": "named entity recognition", "start_pos": 99, "end_pos": 123, "type": "TASK", "confidence": 0.617652048667272}]}, {"text": "This paper presents our experiments in applying two active learning methods, a min-margin based method and a Shannon-entropy based one, to the task of the disambiguation of English verb senses.", "labels": [], "entities": [{"text": "disambiguation of English verb senses", "start_pos": 155, "end_pos": 192, "type": "TASK", "confidence": 0.8123696565628051}]}, {"text": "The contribution of our work is not only in demonstrating that these methods work well for the active learning of coarse-grained verb senses, but also analyzing the behavior of the active learning process on two levels: the instance level and the feature level.", "labels": [], "entities": []}, {"text": "The analysis suggests that a careful treatment of feature design and feature generation is important fora successful application of active learning to WSD.", "labels": [], "entities": [{"text": "feature generation", "start_pos": 69, "end_pos": 87, "type": "TASK", "confidence": 0.7229782789945602}, {"text": "WSD", "start_pos": 151, "end_pos": 154, "type": "TASK", "confidence": 0.9582690000534058}]}, {"text": "We also accounted for the overfitting phenomena that occurred in the learning process based on our data analysis.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we introduce two uncertainty sampling methods used in our active learning experiments and review related work in using active learning for WSD.", "labels": [], "entities": [{"text": "WSD", "start_pos": 153, "end_pos": 156, "type": "TASK", "confidence": 0.946809709072113}]}, {"text": "We then present our active learning experiments on coarse-grained English verb senses in Section 3 and analyze the active learning process in Section 4.", "labels": [], "entities": []}, {"text": "Section 5 presents conclusions of our study.", "labels": [], "entities": []}], "datasetContent": [{"text": "We experimented with the two uncertainty sampling methods on 5 English verbs that had coarse-grained senses (see), as described below.", "labels": [], "entities": []}, {"text": "By using coarse-grained senses, we limit the impact of noisy data due to unclear sense boundaries and therefore can get a clearer observation of the effects of the active learning methods themselves..", "labels": [], "entities": []}, {"text": "The number of senses, the baseline accuracy, the number of instances used for active learning and for held-out evaluation for each verb The coarse-grained senses are produced by grouping together the original WordNet senses using syntactic and semantic criteria).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.97236168384552}]}, {"text": "Double-blind tagging is applied to 50 instances of the target word.", "labels": [], "entities": [{"text": "Double-blind tagging", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.6116698384284973}]}, {"text": "If the ITA < 90%, the sense entry is revised by adding examples and explanations of distinguishing criteria.", "labels": [], "entities": [{"text": "ITA", "start_pos": 7, "end_pos": 10, "type": "METRIC", "confidence": 0.5662209391593933}]}, {"text": "summarizes the statistics of the data.", "labels": [], "entities": []}, {"text": "The baseline accuracy was computed by using the \"most frequent sense\" heuristic to assign sense labels to verb instances (examples).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.9824175238609314}]}, {"text": "The data used in active learning (Column 4 in) include two parts: an initial labeled training set and a pool of unlabeled training data.", "labels": [], "entities": []}, {"text": "We experimented with sizes 20, 50 and 100 for the initial training set.", "labels": [], "entities": []}, {"text": "The pool of unlabeled data had actually been annotated in advance, as inmost pool-based active learning experiments.", "labels": [], "entities": []}, {"text": "Each time an example was selected from the pool by the active learner, its label was returned to the learner.", "labels": [], "entities": []}, {"text": "This simulates the process of asking human annotators to tag the selected unlabeled example at each time.", "labels": [], "entities": []}, {"text": "The advantage of using such a simulation is that we can experiment with different settings (different sizes of the initial training set and different sampling methods).", "labels": [], "entities": []}, {"text": "The data sets used for active learning and for held-out evaluation were randomly sampled from a large data pool for each round of the active learning experiment.", "labels": [], "entities": []}, {"text": "We ran ten rounds of the experiments for each verb and averaged the learning curves for the ten rounds.", "labels": [], "entities": []}, {"text": "In the experiments, we used random sampling (picking up an unlabeled example randomly at each time) as a lower bound.", "labels": [], "entities": []}, {"text": "Another control (ultimate-maxent) was the learner's performance on the test set when it was trained on a set of labeled data that were randomly sampled from a large data pool and equaled the amount of data used in the whole active learning process (e.g., 400 training data for the verb add).", "labels": [], "entities": []}, {"text": "The machine learning model we used for active learning was a regularized maximum entropy (MaxEnt) model).", "labels": [], "entities": []}, {"text": "The features used for disambiguating the verb senses included topical, collocation, syntactic (e.g., the subject, object, and preposition phrases taken by a target verb), and semantic (e.g., the WordNet synsets and hypernyms of the head nouns of a verb's NP arguments) features).", "labels": [], "entities": []}, {"text": "Due to space limits, only shows the learning curves for 4 verbs do, feel, see, and work (size of the initial training set = 20).", "labels": [], "entities": []}, {"text": "The curve for the verb add is similar to that for feel.", "labels": [], "entities": []}, {"text": "These curves clearly show that the two uncertainty sampling methods, the entropy-based (called entropy-maxent in the and the margin-based (called min_margin-maxent), work very well for active learning of the senses of these verbs.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2. The number of senses, the baseline  accuracy, the number of instances used for active  learning and for held-out evaluation for each verb", "labels": [], "entities": [{"text": "accuracy", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.9789115190505981}]}]}