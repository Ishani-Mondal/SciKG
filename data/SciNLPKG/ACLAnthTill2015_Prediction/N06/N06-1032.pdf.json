{"title": [], "abstractContent": [{"text": "We present an approach to statistical machine translation that combines ideas from phrase-based SMT and traditional grammar-based MT.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 26, "end_pos": 57, "type": "TASK", "confidence": 0.6733945409456888}, {"text": "phrase-based SMT", "start_pos": 83, "end_pos": 99, "type": "TASK", "confidence": 0.5553731024265289}]}, {"text": "Our system incorporates the concept of multi-word translation units into transfer of dependency structure snippets, and models and trains statistical components according to state-of-the-art SMT systems.", "labels": [], "entities": [{"text": "SMT", "start_pos": 191, "end_pos": 194, "type": "TASK", "confidence": 0.9884873628616333}]}, {"text": "Compliant with classical transfer-based MT, target dependency structure snippets are input to a grammar-based generator.", "labels": [], "entities": [{"text": "MT", "start_pos": 40, "end_pos": 42, "type": "TASK", "confidence": 0.9462499618530273}]}, {"text": "An experimental evaluation shows that the incorporation of a grammar-based generator into an SMT framework provides improved gram-maticality while achieving state-of-the-art quality on in-coverage examples, suggesting a possible hybrid framework.", "labels": [], "entities": [{"text": "SMT", "start_pos": 93, "end_pos": 96, "type": "TASK", "confidence": 0.9700503349304199}]}], "introductionContent": [{"text": "Recent approaches to statistical machine translation (SMT) piggyback on the central concepts of phrasebased SMT ( and at the same time attempt to improve some of its shortcomings by incorporating syntactic knowledge in the translation process.", "labels": [], "entities": [{"text": "statistical machine translation (SMT", "start_pos": 21, "end_pos": 57, "type": "TASK", "confidence": 0.7707530796527863}, {"text": "phrasebased SMT", "start_pos": 96, "end_pos": 111, "type": "TASK", "confidence": 0.6455068290233612}]}, {"text": "Phrase-based translation with multi-word units excels at modeling local ordering and short idiomatic expressions, however, it lacks a mechanism to learn long-distance dependencies and is unable to generalize to unseen phrases that share non-overt linguistic information.", "labels": [], "entities": [{"text": "Phrase-based translation", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.7593682706356049}]}, {"text": "Publicly available statistical parsers can provide the syntactic information that is necessary for linguistic generalizations and for the resolution of non-local dependencies.", "labels": [], "entities": [{"text": "linguistic generalizations", "start_pos": 99, "end_pos": 125, "type": "TASK", "confidence": 0.6833665370941162}, {"text": "resolution of non-local dependencies", "start_pos": 138, "end_pos": 174, "type": "TASK", "confidence": 0.8256908059120178}]}, {"text": "This information source is deployed in recent work either for pre-ordering source sentences before they are input to to a phrase-based system (), or for re-ordering the output of translation models by statistical ordering models that access linguistic information on dependencies and part-of-speech) . While these approaches deploy dependency-style grammars for parsing source and/or target text, a utilization of grammar-based generation on the output of translation models has not yet been attempted in dependency-based SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 522, "end_pos": 525, "type": "TASK", "confidence": 0.7377966642379761}]}, {"text": "Instead, simple target language realization models that can easily be trained to reflect the ordering of the reference translations in the training corpus are preferred.", "labels": [], "entities": []}, {"text": "The advantage of such models over grammar-based generation seems to be supported, for example, by improvements over phrase-based SMT as well as over an SMT system that deploys a grammar-based generator) on ngram based automatic evaluation scores ().", "labels": [], "entities": [{"text": "SMT", "start_pos": 129, "end_pos": 132, "type": "TASK", "confidence": 0.6282883882522583}]}, {"text": "Another data point, however, is given by who show that parsing-based language modeling can improve grammaticality of translations, even if these improvements are not recorded under n-gram based evaluation measures.", "labels": [], "entities": [{"text": "parsing-based language modeling", "start_pos": 55, "end_pos": 86, "type": "TASK", "confidence": 0.9052176276842753}]}, {"text": "In this paper we would like to step away from n-gram based automatic evaluation scores fora moment, and investigate the possible contributions of incorporating a grammar-based generator into a dependency-based SMT system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 210, "end_pos": 213, "type": "TASK", "confidence": 0.9011561870574951}]}, {"text": "We present a dependency-based SMT model that integrates the idea of multi-word translation units from phrasebased SMT into a transfer system for dependency structure snippets.", "labels": [], "entities": [{"text": "SMT", "start_pos": 30, "end_pos": 33, "type": "TASK", "confidence": 0.8169901371002197}]}, {"text": "The statistical components of our system are modeled on the phrase-based system of, and component weights are adjusted by minimum error rate training.", "labels": [], "entities": []}, {"text": "In contrast to phrase-based SMT and to the above cited dependency-based SMT approaches, our system feeds dependency-structure snippets into a grammar-based generator, and determines target language ordering by applying n-gram and distortion models after grammar-based generation.", "labels": [], "entities": [{"text": "phrase-based SMT", "start_pos": 15, "end_pos": 31, "type": "TASK", "confidence": 0.5249757319688797}, {"text": "SMT", "start_pos": 72, "end_pos": 75, "type": "TASK", "confidence": 0.6602090001106262}]}, {"text": "The goal of this ordering model is thus not foremost to reflect the ordering of the reference translations, but to improve the grammaticality of translations.", "labels": [], "entities": []}, {"text": "Since our system uses standard SMT techniques to learn about correct lexical choice and idiomatic expressions, it allows us to investigate the contribution of grammar-based generation to dependencybased SMT 2 . In an experimental evaluation on the test-set that was used in we show that for examples that are in coverage of the grammar-based system, we can achieve stateof-the-art quality on n-gram based evaluation measures.", "labels": [], "entities": [{"text": "SMT", "start_pos": 31, "end_pos": 34, "type": "TASK", "confidence": 0.9870960712432861}, {"text": "SMT", "start_pos": 203, "end_pos": 206, "type": "TASK", "confidence": 0.7185816764831543}]}, {"text": "To discern the factors of grammaticality and translational adequacy, we conducted a manual evaluation on 500 in-coverage and 500 out-ofcoverage examples.", "labels": [], "entities": []}, {"text": "This showed that an incorporation of a grammar-based generator into an SMT framework provides improved grammaticality over phrase-based SMT on in-coverage examples.", "labels": [], "entities": [{"text": "SMT", "start_pos": 71, "end_pos": 74, "type": "TASK", "confidence": 0.9821676015853882}]}, {"text": "Since in our system it is determinable whether an example is in-coverage, this opens the possibility fora hybrid system that achieves improved grammaticality at state-of-the-art translation quality.", "labels": [], "entities": []}, {"text": "Our method for extracting transfer rules for dependency structure snippets operates on the paired sentences of a sentence-aligned bilingual corpus.", "labels": [], "entities": [{"text": "extracting transfer", "start_pos": 15, "end_pos": 34, "type": "TASK", "confidence": 0.8489182889461517}]}, {"text": "Similar to phrase-based SMT, our approach starts with an improved word-alignment that is created by intersecting alignment matrices for both translation directions, and refining the intersection alignment by adding directly adjacent alignment points, and alignment points that align previously unaligned words (see).", "labels": [], "entities": [{"text": "phrase-based SMT", "start_pos": 11, "end_pos": 27, "type": "TASK", "confidence": 0.5738125741481781}]}, {"text": "Next, source and target sentences are parsed using source and target LFG grammars to produce a set of possible f(unctional) dependency structures for each side (see for the English grammar and parser; for German).", "labels": [], "entities": []}, {"text": "The two f-structures that most preserve dependencies are selected for further consideration.", "labels": [], "entities": []}, {"text": "Selecting the most similar instead of the most probable f-structures is advantageous for rule induction since it provides for higher coverage with simpler rules.", "labels": [], "entities": [{"text": "rule induction", "start_pos": 89, "end_pos": 103, "type": "TASK", "confidence": 0.9061558544635773}]}, {"text": "In the third step, the manyto-many word alignment created in the first step is used to define many-to-many correspondences between the substructures of the f-structures selected in the second step.", "labels": [], "entities": []}, {"text": "The parsing process maintains an association between words in the string and particular predicate features in the f-structure, and thus the predicates on the two sides are implicitly linked by virtue of the original word alignment.", "labels": [], "entities": []}, {"text": "The word alignment is extended to f-structures by setting into correspondence the f-structure units that immediately contain linked predicates.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 4, "end_pos": 18, "type": "TASK", "confidence": 0.6950349062681198}]}, {"text": "These f-structure correspondences are the basis for hypothesizing candidate transfer rules.", "labels": [], "entities": []}, {"text": "To illustrate, suppose our corpus contains the following aligned sentences (this example is taken from our experiments on German-to-English translation): Daf\u00fcr bin ich zutiefst dankbar.", "labels": [], "entities": [{"text": "German-to-English translation", "start_pos": 122, "end_pos": 151, "type": "TASK", "confidence": 0.7040969133377075}]}, {"text": "I have a deep appreciation for that.", "labels": [], "entities": []}, {"text": "Suppose further that we have created the many-tomany bi-directional word alignment Daf\u00fcr{6 7} bin{2} ich{1} zutiefst{3 4 5} dankbar{5} indicating for example that Daf\u00fcrDaf\u00a8Daf\u00fcr is aligned with words 6 and 7 of the English sentence (for and that).: F-structure alignment for induction of German-to-English transfer rules.", "labels": [], "entities": [{"text": "induction of German-to-English transfer", "start_pos": 275, "end_pos": 314, "type": "TASK", "confidence": 0.8188250660896301}]}, {"text": "This results in the links between the predicates of the source and target f-structures shown in.", "labels": [], "entities": []}, {"text": "From these source-target f-structure alignments transfer rules are extracted in two steps.", "labels": [], "entities": []}, {"text": "In the first step, primitive transfer rules are extracted directly from the alignment of f-structure units.", "labels": [], "entities": []}, {"text": "These include simple rules for mapping lexical predicates such as: and somewhat more complicated rules for mapping local f-structure configurations.", "labels": [], "entities": []}, {"text": "For example, the rule shown below is derived from the alignment of the outermost f-structures.", "labels": [], "entities": []}, {"text": "It maps any f-structure whose pred is sein to an f-structure with pred have, and in addition interprets the subj-to-subj link as an indication to map the subject of a source with this predicate into the subject of the target and the xcomp of the source into the object of the target.", "labels": [], "entities": []}, {"text": "Features denoting number, person, type, etc. are not shown; variables %X denote f-structure values.", "labels": [], "entities": []}, {"text": "The following rule shows how a single source fstructure can be mapped to a local configuration of several units on the target side, in this case the single f-structure headed by daf\u00fcrdaf\u00a8daf\u00fcr into one that corresponds to an English preposition+object f-structure.", "labels": [], "entities": []}, {"text": "Transfer rules are required to only operate on contiguous units of the f-structure that are consistent with the word alignment.", "labels": [], "entities": []}, {"text": "This transfer contiguity constraint states that 1.", "labels": [], "entities": []}, {"text": "source and target f-structures are each connected.", "labels": [], "entities": []}, {"text": "2. f-structures in the transfer source can only be aligned with f-structures in the transfer target, and vice versa.", "labels": [], "entities": []}, {"text": "This constraint on f-structures is analogous to the constraint on contiguous and alignment-consistent phrases employed in phrase-based SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 135, "end_pos": 138, "type": "TASK", "confidence": 0.6992030143737793}]}, {"text": "It prevents the extraction of a transfer rule that would translate dankbar directly into appreciation since appreciation is aligned also to zutiefst and its f-structure would also have to be included in the transfer.", "labels": [], "entities": []}, {"text": "Thus, the primitive transfer rule for these predicates must be: In the second step, rules for more complex mappings are created by combining primitive transfer rules that are adjacent in the source and target fstructures.", "labels": [], "entities": []}, {"text": "For instance, we can combine the primitive transfer rule that maps sein to have with the primitive transfer rule that maps ich to Ito produce the complex transfer rule: In the worst case, there can bean exponential number of combinations of primitive transfer rules, so we only allow at most three primitive transfer rules to be combined.", "labels": [], "entities": []}, {"text": "This produces O(n 2 ) trans-fer rules in the worst case, where n is the number of f-structures in the source.", "labels": [], "entities": []}, {"text": "Other points where linguistic information comes into play is in morphological stemming in fstructures, and in the optional filtering of f-structure phrases based on consistency of linguistic types.", "labels": [], "entities": []}, {"text": "For example, the extraction of a phrase-pair that translates zutiefst dankbar into a deep appreciation is valid in the string-based world, but would be prevented in the f-structure world because of the incompatibility of the types A and N for adjectival dankbar and nominal appreciation.", "labels": [], "entities": []}, {"text": "Similarly, a transfer rule translating sein to have could be dispreferred because of a mismatch in the the verbal types V/A and V/N.", "labels": [], "entities": []}, {"text": "However, the transfer of sein zutiefst dankbar to have a deep appreciation is licensed by compatible head types V.", "labels": [], "entities": []}], "datasetContent": [{"text": "The setup for our experimental comparison is German-to-English translation on the Europarl parallel data set 3 . For quick experimental turnaround we restricted our attention to sentences with 5 to 15 words, resulting in a training set of 163,141 sentences and a development set of 1967 sentences.", "labels": [], "entities": [{"text": "Europarl parallel data set", "start_pos": 82, "end_pos": 108, "type": "DATASET", "confidence": 0.9559607654809952}]}, {"text": "Final results are reported on the test set of 1,755 sentences of length 5-15 that was used in.", "labels": [], "entities": []}, {"text": "To extract transfer rules, an improved bidirectional word alignment was created for the training data from the word alignment of IBM model 4 as implemented by GIZA++ ().", "labels": [], "entities": []}, {"text": "Training sentences were parsed using German and English LFG grammars ().", "labels": [], "entities": []}, {"text": "The grammars obtain 100% coverage on unseen data.", "labels": [], "entities": [{"text": "coverage", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9935152530670166}]}, {"text": "80% are parsed as full parses; 20% receive FRAGMENT parses.", "labels": [], "entities": [{"text": "FRAGMENT parses", "start_pos": 43, "end_pos": 58, "type": "METRIC", "confidence": 0.9442707896232605}]}, {"text": "Around 700,000 transfer rules were extracted from f-structures pairs chosen according to a dependency similarity measure.", "labels": [], "entities": []}, {"text": "For language modeling, we used the trigram model of.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.8458493947982788}]}, {"text": "When applied to translating unseen text, the system operates on n-best lists of parses, transferred f-structures, and generated strings.", "labels": [], "entities": [{"text": "translating unseen text", "start_pos": 16, "end_pos": 39, "type": "TASK", "confidence": 0.8602093458175659}]}, {"text": "For minimumerror-rate training on the development set, and for translating the test set, we considered 1 German parse for each source sentence, 10 transferred fstructures for each source parse, and 1,000 generated strings for each transferred f-structure.", "labels": [], "entities": []}, {"text": "Selection of most probable translations proceeds in two steps: First, the most probable transferred f-structure is computed by abeam search on the transfer chart using the first 10 features described above.", "labels": [], "entities": []}, {"text": "These features include tests on source and target f-structure snippets related via transfer rules (features 1-7) as well as language model and distortion features on the target c-and f-structures (features 8-10).", "labels": [], "entities": []}, {"text": "In our experiments, the beam size was set to 20 hypotheses.", "labels": [], "entities": []}, {"text": "The second step is based on features 11-13, which are computed on the strings that were actually generated from the selected n-best f-structures.", "labels": [], "entities": []}, {"text": "We compared our system to IBM model 4 as produced by GIZA++ () and a phrasebased SMT model as provided by.", "labels": [], "entities": [{"text": "SMT", "start_pos": 81, "end_pos": 84, "type": "TASK", "confidence": 0.8648033142089844}]}, {"text": "The same improved word alignment matrix and the same training data were used for phrase-extraction for phrase-based SMT as well as for transfer-rule extraction for LFG-based SMT.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 18, "end_pos": 32, "type": "TASK", "confidence": 0.761175274848938}, {"text": "SMT", "start_pos": 116, "end_pos": 119, "type": "TASK", "confidence": 0.6643459796905518}, {"text": "transfer-rule extraction", "start_pos": 135, "end_pos": 159, "type": "TASK", "confidence": 0.7261742800474167}, {"text": "LFG-based SMT", "start_pos": 164, "end_pos": 177, "type": "TASK", "confidence": 0.5586591362953186}]}, {"text": "Minimum-error-rate training was done using Koehn's implementation of Och's (2003) minimum-error-rate model.", "labels": [], "entities": []}, {"text": "To train the weights for phrase-based SMT we used the first 500 sentences of the development set; the weights of the LFG-based translator were adjusted on the 750 sentences that were in coverage of our grammars.", "labels": [], "entities": [{"text": "SMT", "start_pos": 38, "end_pos": 41, "type": "TASK", "confidence": 0.8212224841117859}]}, {"text": "For automatic evaluation, we use the NIST metric) combined with the approximate randomization test, providing the desired combination of a sensitive evaluation metric and an accurate significance test (see Riezler and phrase-based SMT (P), and the LFG-based SMT (LFG) on the full test set and on in-coverage examples for LFG.", "labels": [], "entities": [{"text": "NIST", "start_pos": 37, "end_pos": 41, "type": "DATASET", "confidence": 0.8217235207557678}]}, {"text": "Results in the same row that are not statistically significant from each other are marked with a * .", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Preference ratings of two human judges for transla-", "labels": [], "entities": [{"text": "Preference", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9663318395614624}, {"text": "transla-", "start_pos": 53, "end_pos": 61, "type": "TASK", "confidence": 0.8192735910415649}]}]}