{"title": [{"text": "Incorporating Speaker and Discourse Features into Speech Summarization", "labels": [], "entities": [{"text": "Summarization", "start_pos": 57, "end_pos": 70, "type": "TASK", "confidence": 0.8288931846618652}]}], "abstractContent": [{"text": "We have explored the usefulness of incorporating speech and discourse features in an automatic speech summarization system applied to meeting recordings from the ICSI Meetings corpus.", "labels": [], "entities": [{"text": "automatic speech summarization", "start_pos": 85, "end_pos": 115, "type": "TASK", "confidence": 0.7183804710706075}, {"text": "ICSI Meetings corpus", "start_pos": 162, "end_pos": 182, "type": "DATASET", "confidence": 0.9511269330978394}]}, {"text": "By analyzing speaker activity, turn-taking and discourse cues, we hypothesize that such a system can out-perform solely text-based methods inherited from the field of text summarization.", "labels": [], "entities": [{"text": "text summarization", "start_pos": 167, "end_pos": 185, "type": "TASK", "confidence": 0.7017931044101715}]}, {"text": "The summariza-tion methods are described, two evaluation methods are applied and compared, and the results clearly show that utilizing such features is advantageous and efficient.", "labels": [], "entities": []}, {"text": "Even simple methods relying on discourse cues and speaker activity can outper-form text summarization approaches.", "labels": [], "entities": [{"text": "text summarization", "start_pos": 83, "end_pos": 101, "type": "TASK", "confidence": 0.6981898844242096}]}], "introductionContent": [{"text": "The task of summarizing spontaneous spoken dialogue from meetings presents many challenges: information is sparse; speech is disfluent and fragmented; automatic speech recognition is imperfect.", "labels": [], "entities": [{"text": "summarizing spontaneous spoken dialogue from meetings", "start_pos": 12, "end_pos": 65, "type": "TASK", "confidence": 0.9179957807064056}, {"text": "automatic speech recognition", "start_pos": 151, "end_pos": 179, "type": "TASK", "confidence": 0.6544274787108103}]}, {"text": "However, there are numerous speech-specific characteristics to be explored and taken advantage of.", "labels": [], "entities": []}, {"text": "Previous research on summarizing speech has concentrated on utilizing prosodic features.", "labels": [], "entities": [{"text": "summarizing speech", "start_pos": 21, "end_pos": 39, "type": "TASK", "confidence": 0.9267207682132721}]}, {"text": "We have examined the usefulness of additional speech-specific characteristics such as discourse cues, speaker activity, and listener feedback.", "labels": [], "entities": []}, {"text": "This speech features approach is contrasted with a second summarization approach using only textual features-a centroid method using a latent semantic representation of utterances.", "labels": [], "entities": [{"text": "summarization", "start_pos": 58, "end_pos": 71, "type": "TASK", "confidence": 0.9658161997795105}]}, {"text": "These individual approaches are compared to a combined approach as well as random baseline summaries.", "labels": [], "entities": []}, {"text": "This paper also introduces anew evaluation scheme for automatic summaries of meeting recordings, using a weighted precision score based on multiple human annotations of each meeting transcript.", "labels": [], "entities": [{"text": "summaries of meeting recordings", "start_pos": 64, "end_pos": 95, "type": "TASK", "confidence": 0.8239664286375046}, {"text": "precision score", "start_pos": 114, "end_pos": 129, "type": "METRIC", "confidence": 0.9220702946186066}]}, {"text": "This evaluation scheme is described in detail below and is motivated by previous findings suggesting that n-gram based metrics like ROUGE do not correlate well in this domain.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 132, "end_pos": 137, "type": "METRIC", "confidence": 0.9398820400238037}]}], "datasetContent": [{"text": "All summaries were 350 words in length, much shorter than the compression rate used in (10% of dialogue acts).", "labels": [], "entities": []}, {"text": "The ICSI meetings themselves average around 10,000 words in length.", "labels": [], "entities": []}, {"text": "The reasons for choosing a shorter length for summaries are that shorter summaries are more likely to be useful to a user wanting to quickly overview and browse a meeting, they present a greater summarization challenge in that the summarizer must be more exact in pinpointing the important aspects of the meeting, and shorter summaries make it more feasible to enlist human evaluators to judge the numerous summaries on various criteria in the future.", "labels": [], "entities": [{"text": "summaries", "start_pos": 46, "end_pos": 55, "type": "TASK", "confidence": 0.9753643274307251}, {"text": "summarization", "start_pos": 195, "end_pos": 208, "type": "TASK", "confidence": 0.960781455039978}]}, {"text": "Summaries were created on both manual transcripts and speech recognizer output.", "labels": [], "entities": [{"text": "speech recognizer output", "start_pos": 54, "end_pos": 78, "type": "TASK", "confidence": 0.7388269603252411}]}, {"text": "The unit of extraction for these summaries was the dialogue act, and these experiments used human segmented and labeled dialogue acts rather than try to detect them automatically.", "labels": [], "entities": []}, {"text": "In future work, we intend to incorporate dialogue act detection and labeling as part of one complete automatic summarization system.", "labels": [], "entities": [{"text": "dialogue act detection", "start_pos": 41, "end_pos": 63, "type": "TASK", "confidence": 0.770327607790629}]}, {"text": "The many-to-many mapping of dialogue acts to summary sentences described in the previous section allows us to evaluate our extractive summaries according to how often each annotator linked a given extracted dialogue act to a summary sentence.", "labels": [], "entities": []}, {"text": "This is somewhat analogous to Pyramid weighting, but with dialogue acts as the SCUs.", "labels": [], "entities": [{"text": "Pyramid weighting", "start_pos": 30, "end_pos": 47, "type": "TASK", "confidence": 0.7833310067653656}]}, {"text": "In fact, we can calculate weighted precision, recall and f-score using these annotations, but because the summaries created are so short, we focus on weighted precision as our central metric.", "labels": [], "entities": [{"text": "precision", "start_pos": 35, "end_pos": 44, "type": "METRIC", "confidence": 0.9088565707206726}, {"text": "recall", "start_pos": 46, "end_pos": 52, "type": "METRIC", "confidence": 0.9993687272071838}, {"text": "f-score", "start_pos": 57, "end_pos": 64, "type": "METRIC", "confidence": 0.9424451589584351}, {"text": "precision", "start_pos": 159, "end_pos": 168, "type": "METRIC", "confidence": 0.8574903607368469}]}, {"text": "For each dialogue act that the summarizer extracts, we count the number of times that each annotator links that dialogue act to a summary sentence.", "labels": [], "entities": []}, {"text": "For a given dialogue act, it maybe that one annotator links it 0 times, one annotator links it 1 time, and the third annotator links it two times, resulting in an average score of 1 for that dialogue act.", "labels": [], "entities": []}, {"text": "The scores for all of the summary dialogue acts can be calculated and averaged to create an overall summary score.", "labels": [], "entities": [{"text": "summary score", "start_pos": 100, "end_pos": 113, "type": "METRIC", "confidence": 0.961502730846405}]}, {"text": "ROUGE scores, based on n-gram overlap between human abstracts and automatic extracts, were also calculated for comparison.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9819613099098206}]}, {"text": "ROUGE-2, based on bigram overlap, is considered the most stable as far as correlating with human judgments, and this was therefore our ROUGE metric of interest.", "labels": [], "entities": [{"text": "ROUGE-2", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.5627343654632568}]}, {"text": "ROUGE-SU4, which evaluates bigrams with intervening material between the two elements of the bigram, has recently been shown in the context of the Document Understanding Conference (DUC) 2 to bring no significant additional information as compared with ROUGE-2.", "labels": [], "entities": [{"text": "ROUGE-SU4", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.7313891053199768}, {"text": "Document Understanding Conference (DUC) 2", "start_pos": 147, "end_pos": 188, "type": "TASK", "confidence": 0.6473557991640908}]}, {"text": "Results from and from DUC 2005 also show that ROUGE does not always correlate well with human judgments.", "labels": [], "entities": [{"text": "DUC 2005", "start_pos": 22, "end_pos": 30, "type": "DATASET", "confidence": 0.9483548104763031}, {"text": "ROUGE", "start_pos": 46, "end_pos": 51, "type": "METRIC", "confidence": 0.9561055302619934}]}, {"text": "It is therefore included in this research in the hope of further determining how reliable the ROUGE metric is for our domain of meeting summarization.", "labels": [], "entities": [{"text": "ROUGE metric", "start_pos": 94, "end_pos": 106, "type": "METRIC", "confidence": 0.8956078588962555}, {"text": "summarization", "start_pos": 136, "end_pos": 149, "type": "TASK", "confidence": 0.827440619468689}]}], "tableCaptions": []}