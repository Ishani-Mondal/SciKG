{"title": [{"text": "Exploiting Variant Corpora for Machine Translation", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 31, "end_pos": 50, "type": "TASK", "confidence": 0.744179904460907}]}], "abstractContent": [{"text": "This paper proposes the usage of variant corpora, i.e., parallel text corpora that are equal in meaning but use different ways to express content, in order to improve corpus-based machine translation.", "labels": [], "entities": [{"text": "corpus-based machine translation", "start_pos": 167, "end_pos": 199, "type": "TASK", "confidence": 0.6351702809333801}]}, {"text": "The usage of multiple training corpora of the same content with different sources results invariant models that focus on specific linguistic phenomena covered by the respective corpus.", "labels": [], "entities": []}, {"text": "The proposed method applies each variant model separately resulting in multiple translation hypotheses which are selectively combined according to statistical models.", "labels": [], "entities": []}, {"text": "The proposed method outperforms the conventional approach of merging all variants by reducing translation ambiguities and exploiting the strengths of each variant model.", "labels": [], "entities": []}], "introductionContent": [{"text": "Corpus-based approaches to machine translation (MT) have achieved much progress over the last decades.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 27, "end_pos": 51, "type": "TASK", "confidence": 0.8661874532699585}]}, {"text": "Despite a high performance on average, these approaches can often produce translations with severe errors.", "labels": [], "entities": [{"text": "translations", "start_pos": 74, "end_pos": 86, "type": "TASK", "confidence": 0.9583770036697388}]}, {"text": "Input sentences featuring linguistic phenomena that are not sufficiently covered by the utilized models cannot be translated accurately.", "labels": [], "entities": []}, {"text": "This paper proposes to use multiple variant corpora, i.e., parallel text corpora that are equal in meaning, but use different vocabulary and grammatical constructions in order to express the same content.", "labels": [], "entities": []}, {"text": "Using training corpora of the same content with different sources result in translation models that focus on specific linguistic phenomena, thus reducing translation ambiguities compared to models trained on a larger corpus obtained by merging all variant corpora.", "labels": [], "entities": [{"text": "translation ambiguities", "start_pos": 154, "end_pos": 177, "type": "TASK", "confidence": 0.8590018451213837}]}, {"text": "The proposed method applies each variant model separately to an input sentence resulting in multiple translation hypotheses.", "labels": [], "entities": []}, {"text": "The best translation is selected according to statistical models.", "labels": [], "entities": [{"text": "translation", "start_pos": 9, "end_pos": 20, "type": "TASK", "confidence": 0.9610002636909485}]}, {"text": "We show that the combination of variant translation models is effective and outperforms not only all single variant models, but also is superior to translation models trained on the union of all variant corpora.", "labels": [], "entities": []}, {"text": "In addition, we extend the proposed method to multi-engine MT.", "labels": [], "entities": [{"text": "MT", "start_pos": 59, "end_pos": 61, "type": "TASK", "confidence": 0.8351757526397705}]}, {"text": "Combining multiple MT engines can boost the system performance further by exploiting the strengths of each MT engine.", "labels": [], "entities": []}, {"text": "For each variant, all MT engines are trained on the same corpus and used in parallel to translate the input.", "labels": [], "entities": [{"text": "MT", "start_pos": 22, "end_pos": 24, "type": "TASK", "confidence": 0.9676085710525513}]}, {"text": "We first select the best translation hypotheses created by all MT engines trained on the same variant and then verify the translation quality of the translation hypotheses selected for each variant.", "labels": [], "entities": [{"text": "MT", "start_pos": 63, "end_pos": 65, "type": "TASK", "confidence": 0.916133463382721}]}, {"text": "The outline of the proposed system is given in.", "labels": [], "entities": []}, {"text": "For the experiments described in this paper we are using two variants of a parallel text corpus for Chinese (C) and English (E) from the travel domain (cf. Section 2).", "labels": [], "entities": []}, {"text": "These variant corpora are used to acquire the translation knowledge for seven corpus-based MT engines.", "labels": [], "entities": [{"text": "MT", "start_pos": 91, "end_pos": 93, "type": "TASK", "confidence": 0.8863951563835144}]}, {"text": "The method to select the best translation hypotheses of MT engines trained on the same variant is described in Section 3.1.", "labels": [], "entities": [{"text": "MT", "start_pos": 56, "end_pos": 58, "type": "TASK", "confidence": 0.943263590335846}]}, {"text": "Finally, the selected translations of different variants are combined according to a statistical significance test as described in Section 3.2.", "labels": [], "entities": []}, {"text": "The effectiveness of the proposed method is verified in Section 4 for the Chinese-English translation task of last year's IWSLT 1 evaluation campaign.", "labels": [], "entities": [{"text": "Chinese-English translation task", "start_pos": 74, "end_pos": 106, "type": "TASK", "confidence": 0.7060392598311106}, {"text": "IWSLT 1 evaluation campaign", "start_pos": 122, "end_pos": 149, "type": "DATASET", "confidence": 0.7232195287942886}]}], "datasetContent": [{"text": "The effectiveness of the proposed method is verified for the CE translation task (500 sentences) of last year's IWSLT evaluation campaign.", "labels": [], "entities": [{"text": "CE translation task", "start_pos": 61, "end_pos": 80, "type": "TASK", "confidence": 0.6971670488516489}, {"text": "IWSLT evaluation campaign", "start_pos": 112, "end_pos": 137, "type": "DATASET", "confidence": 0.7713060776392618}]}, {"text": "For the experiments, we used the four statistical (SMT) and three example-based (EBMT) MT engines described in detail in ().", "labels": [], "entities": []}, {"text": "For evaluation, we used the BLEU metrics, which calculates the geometric mean of n-gram precision for the MT outputs found in reference translations ().", "labels": [], "entities": [{"text": "BLEU", "start_pos": 28, "end_pos": 32, "type": "METRIC", "confidence": 0.997678816318512}]}, {"text": "Higher BLEU scores indicate better translations.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 7, "end_pos": 11, "type": "METRIC", "confidence": 0.9994136095046997}]}, {"text": "the EBMT engines whereby the best performing system is marked with bold-face.", "labels": [], "entities": [{"text": "EBMT", "start_pos": 4, "end_pos": 8, "type": "DATASET", "confidence": 0.8419275879859924}]}], "tableCaptions": [{"text": " Table 1: Statistics of variant corpora", "labels": [], "entities": []}, {"text": " Table 2: BLEU evaluation of element MT engines", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.997352123260498}, {"text": "MT", "start_pos": 37, "end_pos": 39, "type": "TASK", "confidence": 0.8630975484848022}]}, {"text": " Table 4: BLEU evaluation of variant selection", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9949659705162048}]}]}