{"title": [{"text": "Initial Study on Automatic Identification of Speaker Role in Broadcast News Speech", "labels": [], "entities": [{"text": "Automatic Identification of Speaker Role in Broadcast News Speech", "start_pos": 17, "end_pos": 82, "type": "TASK", "confidence": 0.7143537269698249}]}], "abstractContent": [{"text": "Identifying a speaker's role (anchor, reporter, or guest speaker) is important for finding the structural information in broadcast news speech.", "labels": [], "entities": [{"text": "Identifying a speaker's role (anchor, reporter, or guest speaker)", "start_pos": 0, "end_pos": 65, "type": "TASK", "confidence": 0.6418216526508331}]}, {"text": "We present an HMM-based approach and a maximum entropy model for speaker role labeling using Mandarin broadcast news speech.", "labels": [], "entities": [{"text": "speaker role labeling", "start_pos": 65, "end_pos": 86, "type": "TASK", "confidence": 0.727469801902771}]}, {"text": "The algorithms achieve classification accuracy of about 80% (compared to the base-line of around 50%) using the human transcriptions and manually labeled speaker turns.", "labels": [], "entities": [{"text": "classification", "start_pos": 23, "end_pos": 37, "type": "TASK", "confidence": 0.917287290096283}, {"text": "accuracy", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.9732130765914917}]}, {"text": "We found that the maximum entropy model performs slightly better than the HMM, and that the combination of them outperforms any model alone.", "labels": [], "entities": []}, {"text": "The impact of the contextual role information is also examined in this study.", "labels": [], "entities": []}], "introductionContent": [{"text": "More effective information access is beneficial to deal with the increasing amount of broadcast news speech.", "labels": [], "entities": []}, {"text": "Many attempts have been made in the past decade to build news browser, spoken document retrieval system, and summarization or question answering system to effectively handle the large volume of news broadcast speech (e.g., the recent DARPA GALE program).", "labels": [], "entities": [{"text": "spoken document retrieval", "start_pos": 71, "end_pos": 96, "type": "TASK", "confidence": 0.6203482945760092}, {"text": "summarization or question answering", "start_pos": 109, "end_pos": 144, "type": "TASK", "confidence": 0.7517375499010086}, {"text": "DARPA GALE", "start_pos": 234, "end_pos": 244, "type": "TASK", "confidence": 0.4765997529029846}]}, {"text": "Structural information, such as story segmentation or speaker clustering, is critical for all of these applications.", "labels": [], "entities": [{"text": "story segmentation", "start_pos": 32, "end_pos": 50, "type": "TASK", "confidence": 0.7964804172515869}, {"text": "speaker clustering", "start_pos": 54, "end_pos": 72, "type": "TASK", "confidence": 0.819264143705368}]}, {"text": "In this paper, we investigate automatic identification of the speakers' roles in broadcast news speech.", "labels": [], "entities": [{"text": "automatic identification of the speakers' roles in broadcast news speech", "start_pos": 30, "end_pos": 102, "type": "TASK", "confidence": 0.766563680768013}]}, {"text": "A speaker's role (such as anchor, reporter or journalist, interviewee, or some soundbites) can provide useful structural information of broadcast news.", "labels": [], "entities": []}, {"text": "For example, anchors appear through the entire program and generally introduce news stories.", "labels": [], "entities": []}, {"text": "Reporters typically report a specific news story, in which there maybe other guest speakers.", "labels": [], "entities": []}, {"text": "The transition between anchors and reporters is usually a good indicator of story structure.", "labels": [], "entities": [{"text": "story structure", "start_pos": 76, "end_pos": 91, "type": "TASK", "confidence": 0.6873527020215988}]}, {"text": "Speaker role information was shown to be useful for summarizing broadcast news.", "labels": [], "entities": [{"text": "summarizing broadcast news", "start_pos": 52, "end_pos": 78, "type": "TASK", "confidence": 0.9367793599764506}]}, {"text": "Anchor information has also been used for video segmentation, such as the systems in the TRECVID evaluations.", "labels": [], "entities": [{"text": "video segmentation", "start_pos": 42, "end_pos": 60, "type": "TASK", "confidence": 0.7530246078968048}, {"text": "TRECVID evaluations", "start_pos": 89, "end_pos": 108, "type": "TASK", "confidence": 0.5851060152053833}]}, {"text": "In this paper, we develop algorithms for speaker role identification in broadcast news speech.", "labels": [], "entities": [{"text": "speaker role identification", "start_pos": 41, "end_pos": 68, "type": "TASK", "confidence": 0.7207350929578146}]}, {"text": "Human transcription and manual speaker turn labels are used in this initial study.", "labels": [], "entities": []}, {"text": "The task is then to classify each speaker's turn as anchor, reporter, or other.", "labels": [], "entities": []}, {"text": "We use about 170 hours of speech for training and testing.", "labels": [], "entities": []}, {"text": "Two approaches are evaluated, an HMM and a maximum entropy classifier.", "labels": [], "entities": []}, {"text": "Our methods achieve about 80% accuracy for the three-way classification task, compared to around 50% when every speaker is labeled with the majority class label, i.e., anchor.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9995489716529846}, {"text": "three-way classification task", "start_pos": 47, "end_pos": 76, "type": "TASK", "confidence": 0.7045408884684244}]}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Related work is introduced in Section 2.", "labels": [], "entities": []}, {"text": "We describe our approaches in Section 3.", "labels": [], "entities": []}, {"text": "Experimental setup and results are presented in Section 4.", "labels": [], "entities": []}, {"text": "Summary and future work appear in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "We used the TDT4 Mandarin broadcast news data in this study.", "labels": [], "entities": [{"text": "TDT4 Mandarin broadcast news data", "start_pos": 12, "end_pos": 45, "type": "DATASET", "confidence": 0.9765735983848571}]}, {"text": "The data set consists of about 170 hours (336 shows) of news speech from different sources.", "labels": [], "entities": []}, {"text": "In the original transcripts provided by LDC, stories are segmented; however, speaker information (segmentation or identity) is not provided.", "labels": [], "entities": []}, {"text": "Using the reference transcripts and the audio files, we manually labeled the data with speaker turns and the role tag for each turn.", "labels": [], "entities": []}, {"text": "3 Speaker segmentation is generally very reliable; however, the role annotation is ambiguous in some cases.", "labels": [], "entities": [{"text": "3 Speaker segmentation", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.5558481514453888}]}, {"text": "The interannotator agreement will be evaluated in our future work.", "labels": [], "entities": []}, {"text": "In this initial study, we just treat the data as noisy data.", "labels": [], "entities": []}, {"text": "We preprocessed the transcriptions by removing some bad codes and also did text normalization.", "labels": [], "entities": [{"text": "text normalization", "start_pos": 75, "end_pos": 93, "type": "TASK", "confidence": 0.7829302847385406}]}, {"text": "We used punctuation (period, question mark, and exclamation) available from the transcriptions (though not very accurate) to generate sentences, and a left-to-right longest word match approach to segment sentences into words.", "labels": [], "entities": []}, {"text": "These words/sentences are then used for feature extraction in the Maxent model, and LM training and perplexity calculation in the HMM as described in Section 3.", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 40, "end_pos": 58, "type": "TASK", "confidence": 0.7151294499635696}]}, {"text": "Note that the word segmentation approach we used may not be the-state-of-art, which might have some effect on our experiments.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 14, "end_pos": 31, "type": "TASK", "confidence": 0.640963613986969}]}, {"text": "10-fold cross validation is used in our experiments.", "labels": [], "entities": []}, {"text": "The entire data set is split into ten subsets.", "labels": [], "entities": []}, {"text": "Each time one subset is used as the test set, another one is used as the development set, and the rest are used for training.", "labels": [], "entities": []}, {"text": "The average number of segments (i.e., speaker turns) in the ten subsets is 1591, among which 50.8% are anchors.", "labels": [], "entities": []}, {"text": "Parameters (e.g., weighting factor) are tuned based on the average performance over the ten development sets, and the same weights are applied to all the splits during testing.", "labels": [], "entities": [{"text": "Parameters", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9789469242095947}, {"text": "weighting factor", "start_pos": 18, "end_pos": 34, "type": "METRIC", "confidence": 0.8948234021663666}]}], "tableCaptions": [{"text": " Table 1: Automatic role labeling results (%) using the  HMM and Maxent classifiers.", "labels": [], "entities": [{"text": "role labeling", "start_pos": 20, "end_pos": 33, "type": "TASK", "confidence": 0.7814943492412567}]}, {"text": " Table 2: Impact of role sequence information on the  HMM and Maxent classifiers. The combination results  of the HMM and Maxent are also provided.", "labels": [], "entities": []}]}