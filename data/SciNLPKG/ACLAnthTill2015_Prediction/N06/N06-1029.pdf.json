{"title": [{"text": "Unsupervised and Semi-supervised Learning of Tone and Pitch Accent", "labels": [], "entities": []}], "abstractContent": [{"text": "Recognition of tone and intonation is essential for speech recognition and language understanding.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 52, "end_pos": 70, "type": "TASK", "confidence": 0.7817246615886688}, {"text": "language understanding", "start_pos": 75, "end_pos": 97, "type": "TASK", "confidence": 0.7267573028802872}]}, {"text": "However, most approaches to this recognition task have relied upon extensive collections of manually tagged data obtained at substantial time and financial cost.", "labels": [], "entities": [{"text": "recognition task", "start_pos": 33, "end_pos": 49, "type": "TASK", "confidence": 0.8240780532360077}]}, {"text": "In this paper, we explore two approaches to tone learning with substantially reductions in training data.", "labels": [], "entities": [{"text": "tone learning", "start_pos": 44, "end_pos": 57, "type": "TASK", "confidence": 0.7411980926990509}]}, {"text": "We employ both unsupervised clustering and semi-supervised learning to recognize pitch accent in English and tones in Mandarin Chinese.", "labels": [], "entities": []}, {"text": "In unsu-pervised Mandarin tone clustering experiments , we achieve 57-87% accuracy on materials ranging from broadcast news to clean lab speech.", "labels": [], "entities": [{"text": "Mandarin tone clustering", "start_pos": 17, "end_pos": 41, "type": "TASK", "confidence": 0.6344463229179382}, {"text": "accuracy", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.9990498423576355}]}, {"text": "For English pitch accent in broadcast news materials, results reach 78%.", "labels": [], "entities": []}, {"text": "In the semi-supervised framework, we achieve Mandarin tone recognition accuracies ranging from 70% for broadcast news speech to 94% for read speech, out-performing both Support Vector Machines (SVMs) trained on only the labeled data and the 25% most common class assignment level.", "labels": [], "entities": [{"text": "Mandarin tone recognition", "start_pos": 45, "end_pos": 70, "type": "TASK", "confidence": 0.6279284656047821}]}, {"text": "These results indicate that the intrinsic structure of tone and pitch accent acoustics can be exploited to reduce the need for costly labeled training data for tone learning and recognition.", "labels": [], "entities": [{"text": "tone learning and recognition", "start_pos": 160, "end_pos": 189, "type": "TASK", "confidence": 0.7219101190567017}]}], "introductionContent": [{"text": "Tone and intonation play a crucial role across many languages.", "labels": [], "entities": []}, {"text": "However, the use and structure of tone varies widely, ranging from lexical tone which determines word identity to pitch accent signalling information status.", "labels": [], "entities": []}, {"text": "Here we consider the recognition of lexical tones in Mandarin Chinese syllables and pitch accent in English.", "labels": [], "entities": [{"text": "recognition of lexical tones in Mandarin Chinese syllables", "start_pos": 21, "end_pos": 79, "type": "TASK", "confidence": 0.8046369180083275}]}, {"text": "Although intonation is an integral part of language and is requisite for understanding, recognition of tone and pitch accent remains a challenging problem.", "labels": [], "entities": []}, {"text": "The majority of current approaches to tone recognition in Mandarin and other East Asian tone languages integrate tone identification with the general task of speech recognition within a Hidden Markov Model framework.", "labels": [], "entities": [{"text": "tone recognition", "start_pos": 38, "end_pos": 54, "type": "TASK", "confidence": 0.7547980844974518}, {"text": "tone identification", "start_pos": 113, "end_pos": 132, "type": "TASK", "confidence": 0.7784848213195801}, {"text": "speech recognition", "start_pos": 158, "end_pos": 176, "type": "TASK", "confidence": 0.7388162165880203}]}, {"text": "In some cases tone recognition is done only implicitly when a word or syllable is constrained jointly by the segmental acoustics and a higher level language model and the word identity determines tone identity.", "labels": [], "entities": [{"text": "tone recognition", "start_pos": 14, "end_pos": 30, "type": "TASK", "confidence": 0.7307660281658173}]}, {"text": "Other strategies build explicit and distinct models for the syllable final region, the vowel and optionally a final nasal, for each tone.", "labels": [], "entities": []}, {"text": "Recent research has demonstrated the importance of contextual and coarticulatory influences on the surface realization of tones.( The overall shape of the tone or accent can be substantially modified by the local effects of adjacent tone and intonational elements.", "labels": [], "entities": []}, {"text": "Furthermore, broad scale phenomena such as topic and phrase structure can affect pitch height, and pitch shape maybe variably affected by the presence of boundary tones.", "labels": [], "entities": []}, {"text": "These findings have led to explicit modeling of tonal context within the HMM framework.", "labels": [], "entities": []}, {"text": "In addition to earlier approaches that employed phrase structure, several recent approaches to tone recognition in East Asian languages (;) have incorporated elements of local and broad range contextual influence on tone.", "labels": [], "entities": [{"text": "tone recognition", "start_pos": 95, "end_pos": 111, "type": "TASK", "confidence": 0.7729175686836243}]}, {"text": "Many of these techniques create explicit context-dependent models of the phone, tone, or accent for each context in which they appear, either using the tone sequence for left or right context or using a simplified high-low contrast, as is natural for integration in a Hidden Markov Model speech recognition framework.", "labels": [], "entities": []}, {"text": "In pitch accent recognition, recent work by) has integrated pitch accent and boundary tone recognition with speech recognition using prosodically conditioned models within an HMM framework, improving both speech and prosodic recognition.", "labels": [], "entities": [{"text": "pitch accent recognition", "start_pos": 3, "end_pos": 27, "type": "TASK", "confidence": 0.6964680254459381}, {"text": "boundary tone recognition", "start_pos": 77, "end_pos": 102, "type": "TASK", "confidence": 0.7575170000394186}, {"text": "speech recognition", "start_pos": 108, "end_pos": 126, "type": "TASK", "confidence": 0.7170166969299316}, {"text": "prosodic recognition", "start_pos": 216, "end_pos": 236, "type": "TASK", "confidence": 0.7433512210845947}]}, {"text": "Since these approaches are integrated with HMM speech recognition models, standard HMM training procedures which rely upon large labeled training sets are used for tone recognition as well.", "labels": [], "entities": [{"text": "HMM speech recognition", "start_pos": 43, "end_pos": 65, "type": "TASK", "confidence": 0.730030337969462}, {"text": "tone recognition", "start_pos": 164, "end_pos": 180, "type": "TASK", "confidence": 0.7306606620550156}]}, {"text": "Other tone and pitch accent recognition approaches using other classification frameworks such as support vector machines) and decision trees with boosting and bagging) have relied upon large labeled training setsthousands of instances -for classifier learning.", "labels": [], "entities": [{"text": "tone and pitch accent recognition", "start_pos": 6, "end_pos": 39, "type": "TASK", "confidence": 0.6967883110046387}, {"text": "classifier learning", "start_pos": 240, "end_pos": 259, "type": "TASK", "confidence": 0.8907056152820587}]}, {"text": "This labelled training data is costly to construct, both in terms of time and money, with estimates for some intonation annotation tasks reaching tens of times realtime.", "labels": [], "entities": []}, {"text": "This annotation bottleneck as well as a theoretical interest in the learning of tone motivates the use of unsupervised or semi-supervised approaches to tone recognition whereby the reliance on this often scarce resource can be reduced.", "labels": [], "entities": [{"text": "tone recognition", "start_pos": 152, "end_pos": 168, "type": "TASK", "confidence": 0.737935334444046}]}, {"text": "Little research has been done in the application of unsupervised and semi-supervised techniques for tone and pitch accent recognition.", "labels": [], "entities": [{"text": "tone and pitch accent recognition", "start_pos": 100, "end_pos": 133, "type": "TASK", "confidence": 0.6723793745040894}]}, {"text": "Some preliminary work by () employs selforganizing maps and measures of f0 velocity for tone learning.", "labels": [], "entities": [{"text": "tone learning", "start_pos": 88, "end_pos": 101, "type": "TASK", "confidence": 0.7109712809324265}]}, {"text": "In this paper we explore the use of spectral and standard k-means clustering for unsupervised acquisition of tone, and the framework of manifold regularization for semi-supervised tone learning.", "labels": [], "entities": []}, {"text": "We find that in clean read speech, unsupervised techniques can identify the underlying Mandarin tone categories with high accuracy, while even on noisier broadcast news speech, Mandarin tones can be recognized well above chance levels, with English pitch accent recognition at near the levels achieved with fully supervised Support Vector Machine (SVM) classifiers.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 122, "end_pos": 130, "type": "METRIC", "confidence": 0.9903692603111267}, {"text": "English pitch accent recognition", "start_pos": 241, "end_pos": 273, "type": "TASK", "confidence": 0.5778590217232704}]}, {"text": "Likewise in the semi-supervised framework, tone classification outperforms both most common class assignment and a comparable SVM trained on only the same small set of labeled instances, without recourse to the unlabeled instances.", "labels": [], "entities": [{"text": "tone classification", "start_pos": 43, "end_pos": 62, "type": "TASK", "confidence": 0.7328395843505859}]}, {"text": "The remainder of paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes the data sets on which English pitch accent and Mandarin tone learning are performed and the feature extraction process.", "labels": [], "entities": [{"text": "Mandarin tone learning", "start_pos": 68, "end_pos": 90, "type": "TASK", "confidence": 0.5872615476449331}, {"text": "feature extraction", "start_pos": 113, "end_pos": 131, "type": "TASK", "confidence": 0.7059369832277298}]}, {"text": "Section 3 describes the unsupervised and semisupervised techniques employed.", "labels": [], "entities": []}, {"text": "Sections 4 and 5 describe the experiments and results in unsupervised and semi-supervised frameworks respectively.", "labels": [], "entities": []}, {"text": "Section 6 presents conclusions and future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "We executed four sets of experiments in unsupervised clustering using the) asymmetric clustering algorithm.", "labels": [], "entities": []}, {"text": "We again conduct contrastive experiments using both the clean focused read speech and the more challenging broadcast news data.", "labels": [], "entities": []}, {"text": "In each Mandarin case, for each class, we use only a small set (40) of labeled training instances in conjunction with an additional sixty unlabeled instances, testing on 40 instances.", "labels": [], "entities": []}, {"text": "For English pitch accent, we restricted the task to the binary classification of syllables as accented or unaccented.", "labels": [], "entities": [{"text": "English pitch accent", "start_pos": 4, "end_pos": 24, "type": "TASK", "confidence": 0.5818471709887186}]}, {"text": "For the one thousand samples we proportionally labeled 200 unaccented examples and 100 accented examples.", "labels": [], "entities": []}, {"text": "We configure the Laplacian SVM classification with binary neighborhood weights, radial basis function kernel, and cosine distance measure typically with 6 nearest neighbors.", "labels": [], "entities": [{"text": "cosine distance measure", "start_pos": 114, "end_pos": 137, "type": "METRIC", "confidence": 0.7405134638150533}]}, {"text": "Following (C-C.), for -class classification we train \u00a1 \u00a3 \u00a2 \u00a4 \u00a1 \u00a3 \u00a5 \u00a7 \u00a6 \u00a9 \u00a8 binary classifiers.", "labels": [], "entities": []}, {"text": "We then classify each test instance using all of the classifiers and assign the most frequent prediction, with ties broken randomly.", "labels": [], "entities": []}, {"text": "We contrast these results both with conventional SVM classification with a radial basis function kernel excluding the unlabeled training examples and with most common class assignment, which gives a 25% baseline.", "labels": [], "entities": [{"text": "SVM classification", "start_pos": 49, "end_pos": 67, "type": "TASK", "confidence": 0.9663504958152771}]}, {"text": "For the Mandarin focused read syllables, we achieve 94% accuracy on the four-way classification task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.9996185302734375}]}, {"text": "For the noisier broadcast news data, the accuracy is 70% for the comparable task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.9996892213821411}]}, {"text": "These results all substantially outperform the 25% most common class assignment level.", "labels": [], "entities": []}, {"text": "The semi-supervised classifier also reliably outperforms an SVM classifier with an RBF kernel trained on the same labeled training instances.", "labels": [], "entities": []}, {"text": "This baseline SVM classifier with a very small training set achieves 81% accuracy on clean read speech, but only 35% on the broadcast news speech.", "labels": [], "entities": [{"text": "SVM classifier", "start_pos": 14, "end_pos": 28, "type": "TASK", "confidence": 0.8843929469585419}, {"text": "accuracy", "start_pos": 73, "end_pos": 81, "type": "METRIC", "confidence": 0.9994267225265503}]}, {"text": "Finally, for English pitch accent recognition in broadcast news data, the classifier achieves 81.5%, relative to 84% accuracy in the fully supervised case.", "labels": [], "entities": [{"text": "English pitch accent recognition", "start_pos": 13, "end_pos": 45, "type": "TASK", "confidence": 0.6683162227272987}, {"text": "accuracy", "start_pos": 117, "end_pos": 125, "type": "METRIC", "confidence": 0.998745322227478}]}, {"text": "We further contrast the use of different unsupervised learners, comparing the three spectral techniques and k-means with Euclidean distance.", "labels": [], "entities": []}, {"text": "All contrasts are presented for English pitch accent classification, ranging over different numbers of clusters, with the best parameter setting of neighborhood size.", "labels": [], "entities": [{"text": "English pitch accent classification", "start_pos": 32, "end_pos": 67, "type": "TASK", "confidence": 0.6423438861966133}]}, {"text": "The results are illustrated in.", "labels": [], "entities": []}, {"text": "K-means and the asymmetric clustering technique are presented for the clean focal Mandarin speech under the standard two stage clustering, in.", "labels": [], "entities": []}, {"text": "The asymmetric k-lines clustering approach consistently outperforms the corresponding symmetric clustering learner, as well as Laplacian Eigenmaps with binary weights for pitch accent classification.", "labels": [], "entities": [{"text": "pitch accent classification", "start_pos": 171, "end_pos": 198, "type": "TASK", "confidence": 0.651440312465032}]}, {"text": "Somewhat surprisingly, k-means clustering outperforms all of the other approaches when producing 3-14 clusters.", "labels": [], "entities": []}, {"text": "Accuracy for the optimal choice of clusters and parameters is comparable for asymmetric k-lines clustering and k-means, and somewhat better than all other techniques considered.", "labels": [], "entities": []}, {"text": "The careful feature selection process for tone and pitch accent modeling may reduce the difference between the spectral and k-means approaches.", "labels": [], "entities": [{"text": "tone and pitch accent modeling", "start_pos": 42, "end_pos": 72, "type": "TASK", "confidence": 0.6942413866519928}]}, {"text": "In contrast, for the four tone classification task in Mandarin using two stage clustering with 2 or 3 initial clusters, the best clustering using asymmetric k-lines strongly outperforms k-means.", "labels": [], "entities": [{"text": "four tone classification", "start_pos": 21, "end_pos": 45, "type": "TASK", "confidence": 0.7427430550257365}]}, {"text": "We also performed a contrastive experiment in pitch accent recognition in which we excluded contextual information from both types of contextual features.", "labels": [], "entities": [{"text": "pitch accent recognition", "start_pos": 46, "end_pos": 70, "type": "TASK", "confidence": 0.6970529158910116}]}, {"text": "We find little difference for the majority of Asymm.", "labels": [], "entities": [{"text": "Asymm", "start_pos": 46, "end_pos": 51, "type": "DATASET", "confidence": 0.835778534412384}]}, {"text": "K-means Clear speech 87% 74.75%: Clustering effectiveness for asymmetric k-lines and k-means on clear focused speech.", "labels": [], "entities": []}, {"text": "the unsupervised clustering algorithms, with results from symmetric, asymmetric and k-means clustering differing by less than 1% in absolute accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 141, "end_pos": 149, "type": "METRIC", "confidence": 0.9292135238647461}]}, {"text": "It is, however, worth noting that exclusion of these features from experiments using supervised learning led to a 4% absolute reduction inaccuracy.", "labels": [], "entities": []}], "tableCaptions": []}