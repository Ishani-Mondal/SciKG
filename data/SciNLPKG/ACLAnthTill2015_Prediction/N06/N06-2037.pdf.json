{"title": [{"text": "Selecting relevant text subsets from web-data for building topic specific language models", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper we present a scheme to select relevant subsets of sentences from a large generic corpus such as text acquired from the web.", "labels": [], "entities": []}, {"text": "A relative entropy (R.E) based criterion is used to incrementally select sentences whose distribution matches the domain of interest.", "labels": [], "entities": []}, {"text": "Experimental results show that by using the proposed subset selection scheme we can get significant performance improvement in both Word Error Rate (WER) and Perplexity (PPL) over the models built from the entire web-corpus by using just 10% of the data.", "labels": [], "entities": [{"text": "Word Error Rate (WER) and Perplexity (PPL)", "start_pos": 132, "end_pos": 174, "type": "METRIC", "confidence": 0.842565422708338}]}, {"text": "In addition incremental data selection enables us to achieve significant reduction in the vocabulary size as well as number of n-grams in the adapted language model.", "labels": [], "entities": []}, {"text": "To demonstrate the gains from our method we provide a comparative analysis with a number of methods proposed in recent language modeling literature for cleaning up text.", "labels": [], "entities": []}], "introductionContent": [{"text": "One of the main challenges in the rapid deployment of NLP applications is the lack of in-domain data required for training statistical models.", "labels": [], "entities": []}, {"text": "Language models, especially n-gram based, are key components of most NLP applications, such as speech recognition and machine translation, where they serve as priors in the decoding process.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 95, "end_pos": 113, "type": "TASK", "confidence": 0.7920113503932953}, {"text": "machine translation", "start_pos": 118, "end_pos": 137, "type": "TASK", "confidence": 0.8088952898979187}]}, {"text": "To estimate a n-gram language model we require examples of in-domain transcribed utterances, which in absence of readily available relevant corpora have to be collected manually.", "labels": [], "entities": []}, {"text": "This poses severe constraints in terms of both system turnaround time and cost.", "labels": [], "entities": []}, {"text": "This led to a growing interest in using the World Wide Web (WWW) as a corpus for NLP.", "labels": [], "entities": []}, {"text": "The web can serve as a good resource for automatically gathering data for building task-specific language models.", "labels": [], "entities": []}, {"text": "Webpages of interest can be identified by generating query terms either manually or automatically from an initial set of in-domain sentences by measures such as TFIDF or Relative Entropy (R.E).", "labels": [], "entities": [{"text": "TFIDF", "start_pos": 161, "end_pos": 166, "type": "METRIC", "confidence": 0.9504054188728333}]}, {"text": "These webpages can then be converted to a text corpus (which we will refer to as web-data) by appropriate preprocessing.", "labels": [], "entities": []}, {"text": "However text gathered from the web will rarely fit the demands or the nature of the domain of interest completely.", "labels": [], "entities": []}, {"text": "Even with the best queries and web crawling schemes, both the style and content of the web-data will usually differ significantly from the specific needs.", "labels": [], "entities": []}, {"text": "For example, a speech recognition system requires conversational style text whereas most of the data on the web is literary.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 15, "end_pos": 33, "type": "TASK", "confidence": 0.7313646674156189}]}, {"text": "The mismatch between in-domain data and webdata can be seen as a semi-supervised learning problem.", "labels": [], "entities": []}, {"text": "We can model the web-data as a mix of sentences from two classes: in-domain (I) and noise (N) (or out-of-domain).", "labels": [], "entities": []}, {"text": "The labels I and N are latent and unknown for the sentences in web-data but we usually have a small number of examples of indomain examples I.", "labels": [], "entities": []}, {"text": "Selecting the right labels for the unlabeled set is important for benefiting from it.", "labels": [], "entities": []}, {"text": "Recent research on semi-supervised learning shows that in many cases (;) poor preprocessing of unlabeled data might actually lower the performance of classifiers.", "labels": [], "entities": []}, {"text": "We found similar results in our language modeling experiments where the presence of a large set of noisy N examples in training actually lowers the performance slightly in both perplexity and WER terms.", "labels": [], "entities": [{"text": "WER", "start_pos": 192, "end_pos": 195, "type": "METRIC", "confidence": 0.9453837275505066}]}, {"text": "Recent literature on building language models from text acquired from the web addresses this issue partly by using various rank-and-select schemes for identifying the set I ().", "labels": [], "entities": []}, {"text": "However we believe that similar to the question of balance () in semisupervised learning for classification, we need to address the question of distributional similarity while selecting the appropriate utterances for building a language model from noisy data.", "labels": [], "entities": []}, {"text": "The subset of sentences from web-data which are selected to build the adaptation language should have a distribution similar to the in-domain data model.", "labels": [], "entities": []}, {"text": "To address the issue of distributional similarity we present an incremental algorithm which compares the distribution of the selected set and the in-domain examples by using a relative entropy (R.E) criterion.", "labels": [], "entities": [{"text": "relative entropy (R.E)", "start_pos": 176, "end_pos": 198, "type": "METRIC", "confidence": 0.6625703036785126}]}, {"text": "We will review in section 2 some of the ranking schemes which provide baselines for performance comparison and in section 3 we describe the proposed algorithm.", "labels": [], "entities": []}, {"text": "Experimental results are provided in section 4, before we conclude with a summary of this work and directions for the future.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our experiments were conducted on medical domain data collected for building the English ASR of our English-Persian Speech to Speech translation project (.", "labels": [], "entities": [{"text": "English-Persian Speech to Speech translation", "start_pos": 100, "end_pos": 144, "type": "TASK", "confidence": 0.6320119142532349}]}, {"text": "We have 50K indomain sentences for this task available.", "labels": [], "entities": []}, {"text": "We downloaded around 60GB data from the web using automatically generated queries which after filtering and normalization amount to 150M words.", "labels": [], "entities": []}, {"text": "The test set for perplexity evaluations consists of 5000 sentences(35K words) and the heldout set had 2000 sentences (12K words).", "labels": [], "entities": []}, {"text": "The test set for word error rate evaluation consisted of 520 utterances.", "labels": [], "entities": [{"text": "word error rate evaluation", "start_pos": 17, "end_pos": 43, "type": "TASK", "confidence": 0.5609873309731483}]}, {"text": "A generic conversational speech language model was built from the WSJ, Fisher and SWB corpora interpolated with the CMU LM.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 66, "end_pos": 69, "type": "DATASET", "confidence": 0.9634912014007568}, {"text": "CMU LM", "start_pos": 116, "end_pos": 122, "type": "DATASET", "confidence": 0.9047095477581024}]}, {"text": "All language models built from web-data and in-domain data were interpolated with this language model with the interpolation weight determined on the heldout set.", "labels": [], "entities": []}, {"text": "We first compare our proposed algorithm against baselines based on perplexity(PPL), BLEU and LPU classification in terms of test set perplexity.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 84, "end_pos": 88, "type": "METRIC", "confidence": 0.9985944628715515}]}, {"text": "As the comparison shows the proposed algorithm outperforms the rank-and-select schemes with just 1/10th of data.", "labels": [], "entities": []}, {"text": "shows the test set perplexity with different amounts of initial in-domain data.", "labels": [], "entities": []}, {"text": "shows the number of sentences selected for the best perplexity on the heldout set by the above schemes.", "labels": [], "entities": []}, {"text": "The average relative perplexity reduction is around 6%.", "labels": [], "entities": [{"text": "perplexity reduction", "start_pos": 21, "end_pos": 41, "type": "METRIC", "confidence": 0.8553979098796844}]}, {"text": "In addition to the PPL and WER improvements we were able to acheive a factor of 5 reduction in the number of estimated language model parameters (bigram+trigram) and a 30% reduction in the vocab- show that adding data from the web without proper filtering can actually harm the performance of the speech recognition system when the initial in-domain data size increases.", "labels": [], "entities": [{"text": "WER", "start_pos": 27, "end_pos": 30, "type": "METRIC", "confidence": 0.7227166891098022}, {"text": "speech recognition", "start_pos": 297, "end_pos": 315, "type": "TASK", "confidence": 0.7526691854000092}]}, {"text": "This can be attributed to the large increase in vocabulary size which increases the acoustic decoder perplexity.", "labels": [], "entities": []}, {"text": "The average reduction in WER using the proposed scheme is close to 3% relative.", "labels": [], "entities": [{"text": "WER", "start_pos": 25, "end_pos": 28, "type": "METRIC", "confidence": 0.7677240967750549}]}, {"text": "It is interesting to note that for our data selection scheme the perplexity improvments correlate surprisingly well with WER improvments.", "labels": [], "entities": []}, {"text": "A plausible explanation is that the perplexity improvments are accompanied by a significant reduction in the number of language model parameters.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Perplexity of testdata with the web adapted  model for different number of initial sentences.", "labels": [], "entities": []}, {"text": " Table 2: Percentage of web-data selected for differ- ent number of initial sentences.", "labels": [], "entities": []}, {"text": " Table 3: Word Error Rate (WER) with web adapted  models for different number of initial sentences.", "labels": [], "entities": [{"text": "Word Error Rate (WER)", "start_pos": 10, "end_pos": 31, "type": "METRIC", "confidence": 0.7902739097674688}]}]}