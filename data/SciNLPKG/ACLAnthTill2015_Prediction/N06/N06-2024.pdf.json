{"title": [{"text": "NER Systems that Suit User's Preferences: Adjusting the Recall-Precision Trade-off for Entity Extraction", "labels": [], "entities": [{"text": "Entity Extraction", "start_pos": 87, "end_pos": 104, "type": "TASK", "confidence": 0.7099170684814453}]}], "abstractContent": [{"text": "We describe a method based on \"tweak-ing\" an existing learned sequential classi-fier to change the recall-precision tradeoff, guided by a user-provided performance criterion.", "labels": [], "entities": [{"text": "recall-precision", "start_pos": 99, "end_pos": 115, "type": "METRIC", "confidence": 0.9899032115936279}]}, {"text": "This method is evaluated on the task of recognizing personal names in email and newswire text, and proves to be both simple and effective.", "labels": [], "entities": []}], "introductionContent": [{"text": "Named entity recognition (NER) is the task of identifying named entities in free text-typically personal names, organizations, gene-protein entities, and soon.", "labels": [], "entities": [{"text": "Named entity recognition (NER)", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.8157857060432434}]}, {"text": "Recently, sequential learning methods, such as hidden Markov models (HMMs) and conditional random fields (CRFs), have been used successfully fora number of applications, including NER (.", "labels": [], "entities": [{"text": "NER", "start_pos": 180, "end_pos": 183, "type": "TASK", "confidence": 0.8951966762542725}]}, {"text": "In practice, these methods provide imperfect performance: precision and recall, even for well-studied problems on clean wellwritten text, reach at most the mid-90's.", "labels": [], "entities": [{"text": "precision", "start_pos": 58, "end_pos": 67, "type": "METRIC", "confidence": 0.999483585357666}, {"text": "recall", "start_pos": 72, "end_pos": 78, "type": "METRIC", "confidence": 0.9994327425956726}]}, {"text": "While performance of NER systems is often evaluated in terms of F 1 measure (a harmonic mean of precision and recall), this measure may not match user preferences regarding precision and recall.", "labels": [], "entities": [{"text": "NER", "start_pos": 21, "end_pos": 24, "type": "TASK", "confidence": 0.9486681818962097}, {"text": "F 1 measure", "start_pos": 64, "end_pos": 75, "type": "METRIC", "confidence": 0.986499547958374}, {"text": "precision", "start_pos": 96, "end_pos": 105, "type": "METRIC", "confidence": 0.9868344664573669}, {"text": "recall", "start_pos": 110, "end_pos": 116, "type": "METRIC", "confidence": 0.9649520516395569}, {"text": "precision", "start_pos": 173, "end_pos": 182, "type": "METRIC", "confidence": 0.9972350001335144}, {"text": "recall", "start_pos": 187, "end_pos": 193, "type": "METRIC", "confidence": 0.9578635692596436}]}, {"text": "Furthermore, learned NER models maybe sub-optimal also in terms of F1, as they are trained to optimize other measures (e.g., loglikelihood of the training data for CRFs).", "labels": [], "entities": [{"text": "NER", "start_pos": 21, "end_pos": 24, "type": "TASK", "confidence": 0.9468432664871216}, {"text": "F1", "start_pos": 67, "end_pos": 69, "type": "METRIC", "confidence": 0.9991651773452759}]}, {"text": "Obviously, different applications of NER have different requirements for precision and recall.", "labels": [], "entities": [{"text": "NER", "start_pos": 37, "end_pos": 40, "type": "TASK", "confidence": 0.8748427033424377}, {"text": "precision", "start_pos": 73, "end_pos": 82, "type": "METRIC", "confidence": 0.9994908571243286}, {"text": "recall", "start_pos": 87, "end_pos": 93, "type": "METRIC", "confidence": 0.9971798658370972}]}, {"text": "A system might require high precision if it is designed to extract entities as one stage of fact-extraction, where facts are stored directly into a database.", "labels": [], "entities": [{"text": "precision", "start_pos": 28, "end_pos": 37, "type": "METRIC", "confidence": 0.994450569152832}]}, {"text": "On the other hand, a system that generates candidate extractions which are passed to a semi-automatic curation system might prefer higher recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 138, "end_pos": 144, "type": "METRIC", "confidence": 0.9989084005355835}]}, {"text": "In some domains, such as anonymization of medical records, high recall is essential.", "labels": [], "entities": [{"text": "anonymization of medical records", "start_pos": 25, "end_pos": 57, "type": "TASK", "confidence": 0.8499332815408707}, {"text": "recall", "start_pos": 64, "end_pos": 70, "type": "METRIC", "confidence": 0.9987809062004089}]}, {"text": "One way to manipulate an extractor's precisionrecall tradeoff is to assign a confidence score to each extracted entity and then apply a global threshold to confidence level.", "labels": [], "entities": [{"text": "precisionrecall", "start_pos": 37, "end_pos": 52, "type": "METRIC", "confidence": 0.9929234981536865}]}, {"text": "However, confidence thresholding of this sort cannot increase recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 62, "end_pos": 68, "type": "METRIC", "confidence": 0.9984074234962463}]}, {"text": "Also, while confidence scores are straightforward to compute in many classification settings, there is no inherent mechanism for computing confidence of a sequential extractor.", "labels": [], "entities": []}, {"text": "suggest several methods for doing this with CRFs.", "labels": [], "entities": []}, {"text": "In this paper, we suggest an alternative simple method for exploring and optimizing the relationship between precision and recall for NER systems.", "labels": [], "entities": [{"text": "precision", "start_pos": 109, "end_pos": 118, "type": "METRIC", "confidence": 0.9989387392997742}, {"text": "recall", "start_pos": 123, "end_pos": 129, "type": "METRIC", "confidence": 0.995012104511261}, {"text": "NER", "start_pos": 134, "end_pos": 137, "type": "TASK", "confidence": 0.9588744640350342}]}, {"text": "In particular, we describe and evaluate a technique called \"extractor tweaking\" that optimizes a learned extractor with respect to a specific evaluation metric.", "labels": [], "entities": []}, {"text": "Ina nutshell, we directly tweak the threashold term that is part of any linear classifier, including sequential extractors.", "labels": [], "entities": []}, {"text": "Though simple, this approach has not been empirically evaluated before, to our knowledge.", "labels": [], "entities": []}, {"text": "Further, although sequential extractors such as HMMs and CRFs are state-of-the-art methods for tasks like NER, there has been little prior research about tuning these extractors' performance to suit user preferences.", "labels": [], "entities": []}, {"text": "The suggested algorithm optimizes the system performance per a user-provided evaluation criterion, using a linear search procedure.", "labels": [], "entities": []}, {"text": "Applying this procedure is not trivial, since the underlying function is not smooth.", "labels": [], "entities": []}, {"text": "However, we show that the system's precision-recall rate can indeed be tuned to user preferences given labelled data using this method.", "labels": [], "entities": [{"text": "precision-recall rate", "start_pos": 35, "end_pos": 56, "type": "METRIC", "confidence": 0.9851761758327484}]}, {"text": "Empirical results are presented fora particular NER task-recognizing person names, for three corpora, including email and newswire text.", "labels": [], "entities": [{"text": "NER task-recognizing person names", "start_pos": 48, "end_pos": 81, "type": "TASK", "confidence": 0.897800549864769}]}], "datasetContent": [{"text": "We experiment with three datasets, of both email and newswire text.", "labels": [], "entities": []}, {"text": "gives summary statistics for all datasets.", "labels": [], "entities": []}, {"text": "The widely-used MUC-6 dataset includes news articles drawn from the Wall Street Journal.", "labels": [], "entities": [{"text": "MUC-6 dataset", "start_pos": 16, "end_pos": 29, "type": "DATASET", "confidence": 0.8981940746307373}, {"text": "Wall Street Journal", "start_pos": 68, "end_pos": 87, "type": "DATASET", "confidence": 0.9557554721832275}]}, {"text": "The Enron dataset is a collection of emails extracted from the Enron corpus (), where we use a subcollection of the messages located in folders named \"meetings\" or \"calendar\".", "labels": [], "entities": [{"text": "Enron dataset", "start_pos": 4, "end_pos": 17, "type": "DATASET", "confidence": 0.9488306045532227}, {"text": "Enron corpus", "start_pos": 63, "end_pos": 75, "type": "DATASET", "confidence": 0.9039891958236694}]}, {"text": "The Mgmt-Groups dataset is a second email collection, extracted from the CSpace email corpus, which contains email messages sent by MBA students taking a management course conducted at Carnegie Mellon University in 1997.", "labels": [], "entities": [{"text": "Mgmt-Groups dataset", "start_pos": 4, "end_pos": 23, "type": "DATASET", "confidence": 0.9257416427135468}, {"text": "CSpace email corpus", "start_pos": 73, "end_pos": 92, "type": "DATASET", "confidence": 0.8505358497301737}]}, {"text": "This data was split such that its test set contains a different mix of entity names comparing to training exmaples.", "labels": [], "entities": []}, {"text": "Further details about these datasets are available elsewhere ( We used an implementation of Collins' votedpercepton method for discriminatively training HMMs (henceforth, VP-HMM)) as well as CRF () to learn a NER.", "labels": [], "entities": []}, {"text": "Both VP-HMM and CRF were trained for 20 epochs on every dataset, using a simple set of features such as word identity and capitalization patterns fora window of three words around each word being classified.", "labels": [], "entities": []}, {"text": "Each word is classified as either inside or outside a person name.", "labels": [], "entities": []}, {"text": "evaluates the effectiveness of the optimization process used by \"extractor tweaking\" on the Enron dataset.", "labels": [], "entities": [{"text": "Enron dataset", "start_pos": 92, "end_pos": 105, "type": "DATASET", "confidence": 0.9700578451156616}]}, {"text": "We optimized models for F \u03b2 with different values of \u03b2, and also evaluated each optimized model with different F \u03b2 metrics.", "labels": [], "entities": []}, {"text": "The top graph shows the results for token-level F \u03b2 , and the bottom graph shows entity-level F \u03b2 behavior.", "labels": [], "entities": []}, {"text": "The graph illustates that the optimized model does indeed roughly maximize performance for the target \u03b2 value: for example, the token-level F \u03b2 curve for the model optimized for \u03b2 = 0.5 indeed peaks at \u03b2 = 0.5 on the test set data.", "labels": [], "entities": [{"text": "token-level F \u03b2 curve", "start_pos": 128, "end_pos": 149, "type": "METRIC", "confidence": 0.6969506740570068}]}, {"text": "The optimization is only roughly accurate 5 for several possible reasons: first, there are differences between train and test sets; in addition, the line search assumes that the performance metric is smooth and convex, which need not be true.", "labels": [], "entities": []}, {"text": "Note that evaluation-metric optimization is less successful for entity-level performance, which behaves less smoothly than token-level performance.", "labels": [], "entities": [{"text": "evaluation-metric optimization", "start_pos": 10, "end_pos": 40, "type": "TASK", "confidence": 0.7346921265125275}]}, {"text": "Similar results were obtained optimizing baseline CRF classifiers.", "labels": [], "entities": []}, {"text": "Sample results (for MUC-6 only, due to space limitations) are given in, optimizing a CRF baseline for entity-level F \u03b2 . Note that as \u03b2 increases, recall monotonically increases and precision monotonically falls.", "labels": [], "entities": [{"text": "recall", "start_pos": 147, "end_pos": 153, "type": "METRIC", "confidence": 0.9985993504524231}, {"text": "precision", "start_pos": 182, "end_pos": 191, "type": "METRIC", "confidence": 0.9990573525428772}]}], "tableCaptions": [{"text": " Table 1: Summary of the corpora used in the experiments", "labels": [], "entities": []}, {"text": " Table 2: Sample optimized CRF results, for the MUC-6", "labels": [], "entities": [{"text": "Sample optimized CRF", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.8688321709632874}, {"text": "MUC-6", "start_pos": 48, "end_pos": 53, "type": "DATASET", "confidence": 0.8197178244590759}]}]}