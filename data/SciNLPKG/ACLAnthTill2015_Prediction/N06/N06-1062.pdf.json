{"title": [{"text": "Unlimited vocabulary speech recognition for agglutinative languages", "labels": [], "entities": [{"text": "Unlimited vocabulary speech recognition", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.625744603574276}]}], "abstractContent": [{"text": "It is practically impossible to build a word-based lexicon for speech recognition in agglutinative languages that would coverall the relevant words.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 63, "end_pos": 81, "type": "TASK", "confidence": 0.7353332191705704}]}, {"text": "The problem is that words are generally built by concatenating several prefixes and suffixes to the word roots.", "labels": [], "entities": []}, {"text": "Together with compounding and inflections this leads to millions of different, but still frequent word forms.", "labels": [], "entities": []}, {"text": "Due to inflections, ambiguity and other phenomena, it is also not trivial to automatically split the words into meaningful parts.", "labels": [], "entities": []}, {"text": "Rule-based morphological analyzers can perform this splitting, but due to the handcrafted rules, they also suffer from an out-of-vocabulary problem.", "labels": [], "entities": [{"text": "Rule-based morphological analyzers", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.6017734011014303}, {"text": "splitting", "start_pos": 52, "end_pos": 61, "type": "TASK", "confidence": 0.9618668556213379}]}, {"text": "In this paper we apply a recently proposed fully automatic and rather language and vocabulary independent way to build sub-word lexica for three different agglutina-tive languages.", "labels": [], "entities": []}, {"text": "We demonstrate the language portability as well by building a successful large vocabulary speech recog-nizer for each language and show superior recognition performance compared to the corresponding word-based reference systems .", "labels": [], "entities": [{"text": "language portability", "start_pos": 19, "end_pos": 39, "type": "TASK", "confidence": 0.6669451296329498}]}], "introductionContent": [{"text": "Speech recognition for dictation or prepared radio and television broadcasts has had huge advances during the last decades.", "labels": [], "entities": [{"text": "Speech recognition", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8339502513408661}]}, {"text": "For example, broadcast news (BN) in English can now be recognized with about ten percent word error rate (WER)) which results in mostly quite understandable text.", "labels": [], "entities": [{"text": "word error rate (WER))", "start_pos": 89, "end_pos": 111, "type": "METRIC", "confidence": 0.9040098687012991}]}, {"text": "Some rare and new words maybe missing but the result has proven to be sufficient for many important applications, such as browsing and retrieval of recorded speech and information retrieval from the speech ().", "labels": [], "entities": [{"text": "retrieval of recorded speech", "start_pos": 135, "end_pos": 163, "type": "TASK", "confidence": 0.813579723238945}, {"text": "information retrieval from the speech", "start_pos": 168, "end_pos": 205, "type": "TASK", "confidence": 0.8295552372932434}]}, {"text": "However, besides the development of powerful computers and new algorithms, a crucial factor in this development is the vast amount of transcribed speech and suitable text data that has been collected for training the models.", "labels": [], "entities": []}, {"text": "The problem faced in porting the BN recognition systems to conversational speech or to other languages is that almost as much new speech and text data have to be collected again for the new task.", "labels": [], "entities": [{"text": "BN recognition", "start_pos": 33, "end_pos": 47, "type": "TASK", "confidence": 0.8821878433227539}]}, {"text": "The reason for the need fora vast amount of training texts is that the state-of-the-art statistical language models contain a huge amount of parameters to be estimated in order to provide a proper probability for any possible word sequence.", "labels": [], "entities": []}, {"text": "The main reason for the huge model size is that for an acceptable coverage in an English BN task, the vocabulary must be very large, at least 50,000 words, or more.", "labels": [], "entities": []}, {"text": "For languages with a higher degree of word inflections than English, even larger vocabularies are required.", "labels": [], "entities": []}, {"text": "This paper focuses on the agglutinative languages in which words are frequently formed by concatenating one or more stems, prefixes, and suffixes.", "labels": [], "entities": []}, {"text": "For these languages in which the words are often highly inflected as well as formed from several morphemes, even a vocabulary of 100,000 most common words would not give sufficient coverage).", "labels": [], "entities": []}, {"text": "Thus, the solution to the language modeling clearly has to involve splitting of words into smaller modeling units that could then be adequately modeled.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 26, "end_pos": 43, "type": "TASK", "confidence": 0.7394096553325653}]}, {"text": "This paper focuses on solving the vocabulary problem for several languages in which the speech and text database resources are much smaller than for the world's main languages.", "labels": [], "entities": []}, {"text": "A common feature for the agglutinative languages, such as Finnish, Estonian, Hungarian and Turkish is that the large vocabulary continuous speech recognition (LVCSR) attempts so far have not resulted comparable performance to the English systems.", "labels": [], "entities": [{"text": "large vocabulary continuous speech recognition (LVCSR)", "start_pos": 111, "end_pos": 165, "type": "TASK", "confidence": 0.7030209414660931}]}, {"text": "The reason for this is not only the language modeling difficulties, but, of course, the lack of suitable speech and text training data resources.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 36, "end_pos": 53, "type": "TASK", "confidence": 0.7885801494121552}]}, {"text": "In) the systems aim at reducing the active vocabulary and language models to a feasible size by clustering and focusing.", "labels": [], "entities": []}, {"text": "In () the words are split into morphemes by languagedependent hand-crafted morphological rules.", "labels": [], "entities": []}, {"text": "In) different combinations of words, grammatical morphemes and endings are utilized to decrease the OOV rate and optimize the speech recognition accuracy.", "labels": [], "entities": [{"text": "OOV rate", "start_pos": 100, "end_pos": 108, "type": "METRIC", "confidence": 0.9879611432552338}, {"text": "speech recognition", "start_pos": 126, "end_pos": 144, "type": "TASK", "confidence": 0.6765281558036804}, {"text": "accuracy", "start_pos": 145, "end_pos": 153, "type": "METRIC", "confidence": 0.9379213452339172}]}, {"text": "However, constant large improvements over the conventional word-based language models in LVCSR have been rare.", "labels": [], "entities": []}, {"text": "The approach presented in this paper relies on a data-driven algorithm called Morfessor () which is a language independent unsupervised machine learning method to find morpheme-like units (called statistical morphs) from a large text corpus.", "labels": [], "entities": []}, {"text": "This method has several advantages over the rule-based grammatical morphemes, e.g. that no hand-crafted rules are needed and all words can be processed, even the foreign ones.", "labels": [], "entities": []}, {"text": "Even if good grammatical morphemes are available, the language modeling results by the statistical morphs seem to beat least as good, if not better ().", "labels": [], "entities": []}, {"text": "In this paper we evaluate the statistical morphs for three agglutinative languages and describe three different speech recognition systems that successfully utilize the n-gram language models trained for these units in the corresponding LVCSR tasks.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 112, "end_pos": 130, "type": "TASK", "confidence": 0.7351642847061157}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: The LVCSR performance for the speaker- independent Estonian task consisting of read sen- tences recorded via telephone (see Section 4.3). For  a reference (word-based) language model a 60k lex- icon was used here.", "labels": [], "entities": []}, {"text": " Table 3: The LVCSR performance for the speaker- independent Turkish task consisting of read news- paper sentences (see Section 4.4). For the refer- ence 50k (word-based) language model the accuracy  given by 4 and 5-grams did not improve from that of  3-grams.", "labels": [], "entities": [{"text": "LVCSR", "start_pos": 14, "end_pos": 19, "type": "METRIC", "confidence": 0.7244974374771118}, {"text": "accuracy", "start_pos": 190, "end_pos": 198, "type": "METRIC", "confidence": 0.9995251893997192}]}, {"text": " Table 4: The amount of different n-grams in each  language model based on statistical morphs. Note  that the Turkish language model was not prepared  by the growing n-gram algorithm as the others and  the model was limited to 6-grams.", "labels": [], "entities": []}]}