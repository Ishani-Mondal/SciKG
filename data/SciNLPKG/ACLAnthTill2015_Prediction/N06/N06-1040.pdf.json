{"title": [{"text": "Probabilistic Context-Free Grammar Induction Based on Structural Zeros", "labels": [], "entities": []}], "abstractContent": [{"text": "We present a method for induction of concise and accurate probabilistic context-free grammars for efficient use in early stages of a multi-stage parsing technique.", "labels": [], "entities": []}, {"text": "The method is based on the use of statistical tests to determine if a non-terminal combination is unobserved due to sparse data or hard syntactic constraints.", "labels": [], "entities": []}, {"text": "Experimental results show that, using this method, high accuracies can be achieved with a non-terminal set that is orders of magnitude smaller than in typically induced probabilistic context-free grammars , leading to substantial speed-ups in parsing.", "labels": [], "entities": []}, {"text": "The approach is further used in combination with an existing reranker to provide competitive WSJ parsing results.", "labels": [], "entities": [{"text": "WSJ parsing", "start_pos": 93, "end_pos": 104, "type": "TASK", "confidence": 0.854743242263794}]}], "introductionContent": [{"text": "There is a very severe speed vs. accuracy tradeoff in stochastic context-free parsing, which can be explained by the grammar factor in the running-time complexity of standard parsing algorithms such as the CYK algorithm.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.9821933507919312}, {"text": "stochastic context-free parsing", "start_pos": 54, "end_pos": 85, "type": "TASK", "confidence": 0.5090931355953217}]}, {"text": "That algorithm has complexity O(n 3 |P |), where n is the length in words of the sentence parsed, and |P | is the number of grammar productions.", "labels": [], "entities": [{"text": "O", "start_pos": 30, "end_pos": 31, "type": "METRIC", "confidence": 0.5882046818733215}]}, {"text": "Grammar nonterminals can be split to encode richer dependencies in a stochastic model and improve parsing accuracy.", "labels": [], "entities": [{"text": "parsing", "start_pos": 98, "end_pos": 105, "type": "TASK", "confidence": 0.960148811340332}, {"text": "accuracy", "start_pos": 106, "end_pos": 114, "type": "METRIC", "confidence": 0.8766208291053772}]}, {"text": "For example, the parent of the left-hand side (LHS) can be annotated onto the label of the LHS category, hence differentiating, for instance, between expansions of a VP with parent Sand parent VP.", "labels": [], "entities": []}, {"text": "Such annotations, however, tend to substantially increase the number of grammar productions as well as the ambiguity of the grammar, thereby significantly slowing down the parsing algorithm.", "labels": [], "entities": []}, {"text": "In the case of bilexical grammars, where categories in binary grammars are annotated with their lexical heads, the grammar factor contributes an additional O(n 2 |V D | 3 ) complexity, leading to an overall O(n 5 |V D | 3 ) parsing complexity, where |V D | is the number of delexicalized non-terminals.", "labels": [], "entities": [{"text": "O", "start_pos": 156, "end_pos": 157, "type": "METRIC", "confidence": 0.9589763283729553}]}, {"text": "Even with special modifications to the basic CYK algorithm, such as those presented by, improvements to the stochastic model are obtained at the expense of efficiency.", "labels": [], "entities": []}, {"text": "In addition to the significant cost in efficiency, increasing the non-terminal set impacts parameter estimation for the stochastic model.", "labels": [], "entities": []}, {"text": "With more productions, much fewer observations per production are available and one is left with the hope that a subsequent smoothing technique can effectively deal with this problem, regardless of the number of non-terminals created.", "labels": [], "entities": []}, {"text": "showed that, by making certain linguistically-motivated node label annotations, but avoiding certain other kinds of state splits (mainly lexical annotations) models of relatively high accuracy can be built without resorting to smoothing.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 184, "end_pos": 192, "type": "METRIC", "confidence": 0.9757843017578125}]}, {"text": "The resulting grammars were small enough to allow for exhaustive CYK parsing; even so, parsing speed was significantly impacted by the state splits: the test-set parsing time reported was about 3s for average length sentences, with a memory usage of 1GB.", "labels": [], "entities": [{"text": "CYK parsing", "start_pos": 65, "end_pos": 76, "type": "TASK", "confidence": 0.6601502895355225}, {"text": "parsing", "start_pos": 87, "end_pos": 94, "type": "TASK", "confidence": 0.9586116075515747}]}, {"text": "This paper presents an automatic method for deciding which state to split in order to create concise and accurate unsmoothed probabilistic context-free grammars (PCFGs) for efficient use in early stages of a multi-stage parsing technique.", "labels": [], "entities": []}, {"text": "The method is based on the use of statistical tests to determine if a non-terminal combination is unobserved due to the limited size of the sample (sampling zero) or because it is grammatically impossible (structural zero).", "labels": [], "entities": []}, {"text": "This helps introduce a relatively small number of new non-terminals with little additional parsing  overhead.", "labels": [], "entities": []}, {"text": "Experimental results show that, using this method, high accuracies can be achieved with orders of magnitude fewer non-terminals than in typically induced PCFGs, leading to substantial speed-ups in parsing.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 56, "end_pos": 66, "type": "METRIC", "confidence": 0.9799599647521973}, {"text": "parsing", "start_pos": 197, "end_pos": 204, "type": "TASK", "confidence": 0.9714741110801697}]}, {"text": "The approach can further be used in combination with an existing reranker to provide competitive WSJ parsing results.", "labels": [], "entities": [{"text": "WSJ parsing", "start_pos": 97, "end_pos": 108, "type": "TASK", "confidence": 0.8708934187889099}]}, {"text": "The remainder of the paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 gives a brief description of PCFG induction from treebanks, including non-terminal label-splitting, factorization, and relative frequency estimation.", "labels": [], "entities": [{"text": "PCFG induction", "start_pos": 39, "end_pos": 53, "type": "TASK", "confidence": 0.943987637758255}]}, {"text": "Section 3 discusses the statistical criteria that we explored to determine structural zeros and thus select non-terminals for the factored PCFG.", "labels": [], "entities": [{"text": "PCFG", "start_pos": 139, "end_pos": 143, "type": "DATASET", "confidence": 0.8728299736976624}]}, {"text": "Finally, Section 4 reports the results of parsing experiments using our exhaustive k-best CYK parser with the concise PCFGs induced from the Penn WSJ treebank ().", "labels": [], "entities": [{"text": "parsing", "start_pos": 42, "end_pos": 49, "type": "TASK", "confidence": 0.981421709060669}, {"text": "Penn WSJ treebank", "start_pos": 141, "end_pos": 158, "type": "DATASET", "confidence": 0.9503675897916158}]}], "datasetContent": [{"text": "We used the log-likelihood ratio statistic G 2 to rank unobserved events ab, where a \u2282 P and b \u2208 V . Let V o be the original, unfactored non-terminal set, and let \u03b1 \u2208 (V o :) * be a sequence of zero or more nonterminal/colon symbol pairs.", "labels": [], "entities": []}, {"text": "Suppose we have a frequent factored non-terminal X :\u03b1B for X, B \u2208 V o . Then, if the set of productions X \u2192 Y X :\u03b1A with A \u2208 V o is also frequent, but X \u2192 Y X:\u03b1B is unobserved, this is a candidate structural zero.", "labels": [], "entities": []}, {"text": "Similar splits can be considered with non-factored nonterminals.", "labels": [], "entities": []}, {"text": "There are two state split scenarios we consider in this paper.", "labels": [], "entities": []}, {"text": "Scenario 1 is for factored non-terminals, which are always the second child of a binary production.", "labels": [], "entities": []}, {"text": "For use in Equation 7, Scenario 2 is for non-factored non-terminals, which we will split using the leftmost child, the left-corner POS-tag, and the left-corner lexical item, which are easily incorporated into our grammar factorization approach.", "labels": [], "entities": []}, {"text": "In this scenario, the non-terminal to be split can be either the left or right child in the binary production.", "labels": [], "entities": []}, {"text": "Here we show the counts for the left child case for use in Equation 7: ) In this case, the possible splits are more complicated than just non-terminals as used in factoring.", "labels": [], "entities": []}, {"text": "Here, the first possible split is the left child category, along with an indication of whether it is a unary production.", "labels": [], "entities": []}, {"text": "One can further split by including the left-corner tag, and even further by including the left-corner word.", "labels": [], "entities": []}, {"text": "For example, a unary S category might be split as follows: first to S[1:VP] if the single child of the S is a VP; next to S[1:VP:VBD] if the left-corner POS-tag is VBD; finally to S[1:VP:VBD:went] if the VBD verb was 'went'.", "labels": [], "entities": []}, {"text": "Note that, once non-terminals are split by annotating such information, the base non-terminals, e.g., S, implicitly encode contexts other than the ones that were split.", "labels": [], "entities": []}, {"text": "shows the unobserved rules with the largest G 2 score, along with the ten non-terminals: Top ten non-terminals to add, and the unobserved productions leading to their addition to the non-terminal set. that these productions suggest for inclusion in our non-terminal set.", "labels": [], "entities": [{"text": "G 2 score", "start_pos": 44, "end_pos": 53, "type": "METRIC", "confidence": 0.8825285832087199}]}, {"text": "The highest scoring unobserved production is PP \u2192 IN[that] NP.", "labels": [], "entities": [{"text": "IN", "start_pos": 50, "end_pos": 52, "type": "METRIC", "confidence": 0.8886886835098267}]}, {"text": "It receives such a high score because the base production (PP \u2192 IN NP) is very frequent, and so is 'IN\u2192that', but they jointly never occur, since 'IN\u2192that' is a complementizer.", "labels": [], "entities": []}, {"text": "This split non-terminal also shows up in the second-highest ranked zero, an SBAR with 'that' complementizer and an S child that consists of a unary VP.", "labels": [], "entities": []}, {"text": "The unary S\u2192VP production is very common, but never with a 'that' complementizer in an SBAR.", "labels": [], "entities": []}, {"text": "Note that the fourth-ranked production uses two split non-terminals.", "labels": [], "entities": []}, {"text": "The fifth ranked rule presumably does not add much information to aid parsing disambiguation, since the AUX MD tag sequence is unlikely . The eighth ranked production is the first with a factored category, ruling out coordination between NN and NP.", "labels": [], "entities": [{"text": "parsing disambiguation", "start_pos": 70, "end_pos": 92, "type": "TASK", "confidence": 0.9360405206680298}, {"text": "AUX MD tag sequence", "start_pos": 104, "end_pos": 123, "type": "METRIC", "confidence": 0.918033555150032}]}, {"text": "Before presenting experimental results, we will mention some practical issues related to the approach described.", "labels": [], "entities": []}, {"text": "First, we independently parameterized the number of factored categories to select and the number of non-factored categories to select.", "labels": [], "entities": []}, {"text": "This was done to allow for finer control of the amount of splitting of non-terminals of each type.", "labels": [], "entities": []}, {"text": "To choose 100 of each, every non-terminal was assigned the score of the highest scoring unobserved production within which it occurred.", "labels": [], "entities": []}, {"text": "Then the 100 highest scoring non-terminals of each type were added to the base non-terminal list, which originally consisted of the atomic treebank non-terminals and Markov order-0 factored non-terminals.", "labels": [], "entities": []}, {"text": "Once the desired non-terminals are selected, the training corpus is factored, and non-terminals are split if they were among the selected set.", "labels": [], "entities": []}, {"text": "Note, how- ever, that some of the information in a selected nonterminal may not be fully available, requiring some number of additional splits.", "labels": [], "entities": []}, {"text": "Any non-terminal that is required by a selected non-terminal will be selected itself.", "labels": [], "entities": []}, {"text": "For example, suppose that NP:CC:NP was chosen as a factored non-terminal.", "labels": [], "entities": []}, {"text": "Then the second child of any local tree with that non-terminal on the LHS must either bean NP or a factored non-terminal with at least the first child identified as an NP, i.e., NP:NP.", "labels": [], "entities": []}, {"text": "If that factored non-terminal was not selected to be in the set, it must be added.", "labels": [], "entities": []}, {"text": "The same situation occurs with left-corner tags and words, which maybe arbitrarily far below the category.", "labels": [], "entities": []}, {"text": "After factoring and selective splitting of nonterminals, the resulting treebank corpus is used to train a PCFG.", "labels": [], "entities": [{"text": "PCFG", "start_pos": 106, "end_pos": 110, "type": "DATASET", "confidence": 0.7975051999092102}]}, {"text": "Recall that we use the k-best output of a POS-tagger to parse.", "labels": [], "entities": []}, {"text": "For each POS-tag and lexical item pair from the output of the tagger, we reduce the word to lowercase and check to see if the combination is in the set of split POS-tags, in which case we split the tag, e.g., IN.", "labels": [], "entities": [{"text": "IN", "start_pos": 209, "end_pos": 211, "type": "METRIC", "confidence": 0.9919098019599915}]}, {"text": "shows the F-measure accuracy for our trials on the development set versus the number of non-factored splits parameterized for the trial.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9953124523162842}, {"text": "accuracy", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.8721345663070679}]}, {"text": "From this plot, we can see that 500 non-factored splits provides the best F-measure accuracy on the dev set.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 74, "end_pos": 83, "type": "METRIC", "confidence": 0.9732992053031921}, {"text": "accuracy", "start_pos": 84, "end_pos": 92, "type": "METRIC", "confidence": 0.8995718955993652}]}, {"text": "Presumably, as more than 500 splits are made, sparse data becomes more problematic.", "labels": [], "entities": []}, {"text": "shows the development set F-measure accuracy versus the number of words-per-second it takes to parse the development set, for non-factored splits of 0 and 500, at a range of factored split parameterizations.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 26, "end_pos": 35, "type": "METRIC", "confidence": 0.9320274591445923}, {"text": "accuracy", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.8528438806533813}]}, {"text": "With 0 non-factored splits, efficiency is substantially impacted by increasing the factored splits, whereas it can be seen that with 500 non-factored splits, that impact is much less, so that the best performance (1) no non-factored splits (i.e., only factored categories selected); (2) 500 non-factored splits, which was the best performing; and (3) four baseline results. is reached with both relatively few factored nonterminal splits, and a relatively small efficiency impact.", "labels": [], "entities": []}, {"text": "The non-factored splits provide substantial accuracy improvements at relatively small efficiency cost.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 44, "end_pos": 52, "type": "METRIC", "confidence": 0.9981326460838318}]}, {"text": "shows the 1-best and reranked 50-best results for the baseline Markov order-2 model, and the best-performing model using factored and nonfactored non-terminal splits.", "labels": [], "entities": []}, {"text": "We present the efficiency of the model in terms of words-per-second over the entire dev set, including the longer strings (maximum length 116 words) . We used the k-best decoding algorithm of with our CYK parser, using on-demand k-best backpointer calculation.", "labels": [], "entities": [{"text": "CYK", "start_pos": 201, "end_pos": 204, "type": "DATASET", "confidence": 0.9405214786529541}]}, {"text": "We then trained a MaxEnt reranker on sections 2-21, using the approach outlined in, via the publicly available reranking code from that paper.", "labels": [], "entities": []}, {"text": "We used the default features that come with that package.", "labels": [], "entities": []}, {"text": "The processing time in the table includes the time to parse and rerank.", "labels": [], "entities": []}, {"text": "As can be seen from the trials, there is some overhead to these processes, but the time is still dominated by the base parsing.", "labels": [], "entities": []}, {"text": "We present the k-best results to demonstrate the benefits of using a better model, such as the one we have presented, for producing candidates for downstream processing.", "labels": [], "entities": []}, {"text": "Even with severe pruning to only the top 50 candidate parses per string, which results in low oracle and reranked accuracy for the Markov order-2 model, the best-performing model based on structural zeros achieves a relatively high oracle accuracy, and reaches 88.0 and 87.5 percent F-measure accuracy on the dev (f24) and eval (f23) sets respectively.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 114, "end_pos": 122, "type": "METRIC", "confidence": 0.8315644860267639}, {"text": "accuracy", "start_pos": 239, "end_pos": 247, "type": "METRIC", "confidence": 0.9630889892578125}, {"text": "F-measure", "start_pos": 283, "end_pos": 292, "type": "METRIC", "confidence": 0.9956639409065247}, {"text": "accuracy", "start_pos": 293, "end_pos": 301, "type": "METRIC", "confidence": 0.7884162664413452}]}, {"text": "Note that the well-known Char-: Parsing results on the development set (f24) and the evaluation set (f23) for the baseline Markov order-2 model and the best-performing structural zero model, with 200 factored and 500 non-factored non-terminal splits.", "labels": [], "entities": []}, {"text": "1-best results, plus reranking using a trained version of an existing reranker with 50 candidates.", "labels": [], "entities": [{"text": "reranking", "start_pos": 21, "end_pos": 30, "type": "TASK", "confidence": 0.9403286576271057}]}, {"text": "niak parser) uses a Markov order-3 baseline PCFG in the initial pass, with a best-first algorithm that is run past the first parse to populate the chart for use by the richer model.", "labels": [], "entities": []}, {"text": "While we have demonstrated exhaustive parsing efficiency, our model could be used with any of the efficient search best-first approaches documented in the literature, from those used in the Charniak parser) to A * parsing (.", "labels": [], "entities": [{"text": "parsing", "start_pos": 38, "end_pos": 45, "type": "TASK", "confidence": 0.8107587099075317}, {"text": "A * parsing", "start_pos": 210, "end_pos": 221, "type": "TASK", "confidence": 0.5109216868877411}]}, {"text": "By using a richer grammar of the sort we present, far fewer edges would be required in the chart to include sufficient quality candidates for the richer model, leading to further downstream savings of processing time.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Baseline results of exhaustive CYK parsing using different probabilistic context-free grammars. Grammars are trained", "labels": [], "entities": [{"text": "CYK parsing", "start_pos": 41, "end_pos": 52, "type": "TASK", "confidence": 0.7227492779493332}]}, {"text": " Table 2: Top ten non-terminals to add, and the unobserved", "labels": [], "entities": []}, {"text": " Table 3: Parsing results on the development set (f24) and the evaluation set (f23) for the baseline Markov order-2 model and the", "labels": [], "entities": []}]}