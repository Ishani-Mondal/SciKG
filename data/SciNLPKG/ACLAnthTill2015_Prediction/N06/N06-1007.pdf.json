{"title": [{"text": "Acquisition of Verb Entailment from Text", "labels": [], "entities": [{"text": "Acquisition", "start_pos": 0, "end_pos": 11, "type": "METRIC", "confidence": 0.7871797680854797}, {"text": "Verb Entailment", "start_pos": 15, "end_pos": 30, "type": "TASK", "confidence": 0.6153615266084671}]}], "abstractContent": [{"text": "The study addresses the problem of automatic acquisition of entailment relations between verbs.", "labels": [], "entities": [{"text": "automatic acquisition of entailment relations between verbs", "start_pos": 35, "end_pos": 94, "type": "TASK", "confidence": 0.8039525406701225}]}, {"text": "While this task has much in common with paraphrases acquisition which aims to discover semantic equivalence between verbs, the main challenge of entailment acquisition is to capture asymmetric, or directional, relations.", "labels": [], "entities": [{"text": "paraphrases acquisition", "start_pos": 40, "end_pos": 63, "type": "TASK", "confidence": 0.7885391712188721}, {"text": "entailment acquisition", "start_pos": 145, "end_pos": 167, "type": "TASK", "confidence": 0.7521406412124634}]}, {"text": "Motivated by the intuition that it often underlies the local structure of coherent text, we develop a method that discovers verb en-tailment using evidence about discourse relations between clauses available in a parsed corpus.", "labels": [], "entities": []}, {"text": "In comparison with earlier work, the proposed method covers a much wider range of verb entailment types and learns the mapping between verbs with highly varied argument structures.", "labels": [], "entities": []}], "introductionContent": [{"text": "The entailment relations between verbs area natural language counterpart of the commonsense knowledge that certain events and states give rise to other events and states.", "labels": [], "entities": []}, {"text": "For example, there is an entailment relation between the verbs buy and belong, which reflects the commonsense notion that if someone has bought an object, this object belongs to that person.", "labels": [], "entities": []}, {"text": "A lexical resource encoding entailment can serve as a useful tool in many tasks where automatic inferencing over natural language text is required.", "labels": [], "entities": [{"text": "lexical resource encoding entailment", "start_pos": 2, "end_pos": 38, "type": "TASK", "confidence": 0.8199422508478165}]}, {"text": "In Question Answering, it has been used to establish that a certain sentence found in the corpus can serve as a suitable, albeit implicit answer to a query),,).", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 3, "end_pos": 21, "type": "TASK", "confidence": 0.80106520652771}]}, {"text": "In Information Extraction, it can similarly help to recognize relations between named entities in cases when the entities in the text are linked by a linguistic construction that entails a known extraction pattern, but not by the pattern itself.", "labels": [], "entities": [{"text": "Information Extraction", "start_pos": 3, "end_pos": 25, "type": "TASK", "confidence": 0.7320792973041534}]}, {"text": "A lexical entailment resource can contribute to information retrieval tasks via integration into a textual entailment system that aims to recognize entailment between two larger text fragments ( ).", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 48, "end_pos": 69, "type": "TASK", "confidence": 0.736547127366066}]}, {"text": "Since entailment is known to systematically interact with the discourse organization of text, an entailment resource can be of interest to tasks that deal with structuring a set of individual facts into coherent text.", "labels": [], "entities": []}, {"text": "In Natural Language Generation) and Multi-Document Summarization () it can be used to order sentences coming from multiple, possibly unrelated sources to produce a coherent document.", "labels": [], "entities": [{"text": "Natural Language Generation", "start_pos": 3, "end_pos": 30, "type": "TASK", "confidence": 0.6479578018188477}, {"text": "Multi-Document Summarization", "start_pos": 36, "end_pos": 64, "type": "TASK", "confidence": 0.7748229503631592}]}, {"text": "The knowledge is essential for compiling answers for procedural questions in a QA system, when sentences containing relevant information are spread across the corpus ().", "labels": [], "entities": []}, {"text": "The present paper is concerned with the problem of automatic acquisition of verb entailment from text.", "labels": [], "entities": [{"text": "automatic acquisition of verb entailment from text", "start_pos": 51, "end_pos": 101, "type": "TASK", "confidence": 0.8259609256471906}]}, {"text": "In the next section we set the background for the study by describing previous work.", "labels": [], "entities": []}, {"text": "We then define the goal of the study and describe our method for verb entailment acquisition.", "labels": [], "entities": [{"text": "verb entailment acquisition", "start_pos": 65, "end_pos": 92, "type": "TASK", "confidence": 0.8114231626192728}]}, {"text": "After that we present results of its experimental evaluation.", "labels": [], "entities": []}, {"text": "Finally, we draw conclusions and outline future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "Once the best parameter settings for the method were found, we compared its performance against human judges as well as the DIRT inference rules and the verb entailment encoded in the WordNet 2.0 database.", "labels": [], "entities": [{"text": "WordNet 2.0 database", "start_pos": 184, "end_pos": 204, "type": "DATASET", "confidence": 0.9226722717285156}]}, {"text": "To elicit human judgments on the evaluation data, we automatically converted the templates into a natural language form using a number of simple rules to arrange words in the correct grammatical order.", "labels": [], "entities": []}, {"text": "In cases where an obligatory syntactic position near a verb was missing, we supplied the pronouns someone or something in that position.", "labels": [], "entities": []}, {"text": "In each template pair, the premise was turned into a statement, and the consequence into a question.", "labels": [], "entities": []}, {"text": "illustrates the result of converting the test item from the previous example into the natural language form.", "labels": [], "entities": []}, {"text": "During the experiment, two judges were asked to mark those statement-question pairs in each test item, where, considering the statement, they could answer the question affirmatively.", "labels": [], "entities": []}, {"text": "The judges' decisions coincided in 95 of 129 test items.", "labels": [], "entities": []}, {"text": "The Kappa statistic is \u03ba=0.725, which provides some indication about the upper bound of performance on this task.", "labels": [], "entities": [{"text": "Kappa statistic", "start_pos": 4, "end_pos": 19, "type": "METRIC", "confidence": 0.8693369925022125}]}, {"text": "We also experimented with the inference rules contained in the DIRT database).", "labels": [], "entities": [{"text": "DIRT database", "start_pos": 63, "end_pos": 76, "type": "DATASET", "confidence": 0.9583428204059601}]}, {"text": "According to), an inference rule is a relation between two verbs which are more loosely related than typical paraphrases, but nonetheless can be useful for performing inferences over natural language texts.", "labels": [], "entities": []}, {"text": "We were interested to see how these inference rules perform on the entailment recognition task.", "labels": [], "entities": [{"text": "entailment recognition task", "start_pos": 67, "end_pos": 94, "type": "TASK", "confidence": 0.8921139438947042}]}, {"text": "For each dependency tree path (a graph linking a verb with two slots for its arguments), DIRT contains a list of the most similar tree paths along with the similarity scores.", "labels": [], "entities": []}, {"text": "To decide which is the most likely consequence in each test item, we looked up the DIRT database for the corresponding two dependency tree paths.", "labels": [], "entities": [{"text": "DIRT database", "start_pos": 83, "end_pos": 96, "type": "DATASET", "confidence": 0.9093903303146362}]}, {"text": "The template pair with the greatest similarity was output as the correct answer.", "labels": [], "entities": []}, {"text": "WordNet 2.0 contains manually encoded entailment relations between verb synsets, which are labeled as \"cause\", \"troponymy\", or \"entailment\".", "labels": [], "entities": [{"text": "WordNet", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9066947102546692}]}, {"text": "To identify the template pair satisfying entailment in a test item, we checked whether the two verbs in each pair are linked in WordNet in terms of one of these three labels.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 128, "end_pos": 135, "type": "DATASET", "confidence": 0.9546669721603394}]}, {"text": "Because WordNet does not encode the information as to the relative plausibility of relations, all template pairs where verbs were linked in WordNet, were output as correct answers.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 8, "end_pos": 15, "type": "DATASET", "confidence": 0.9505277276039124}, {"text": "WordNet", "start_pos": 140, "end_pos": 147, "type": "DATASET", "confidence": 0.9527056217193604}]}, {"text": "describes the accuracy scores achieved by our entailment acquisition algorithm, the two human judges, DIRT and WordNet.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.999202311038971}, {"text": "entailment acquisition", "start_pos": 46, "end_pos": 68, "type": "TASK", "confidence": 0.7152110934257507}, {"text": "DIRT", "start_pos": 102, "end_pos": 106, "type": "DATASET", "confidence": 0.890443742275238}, {"text": "WordNet", "start_pos": 111, "end_pos": 118, "type": "DATASET", "confidence": 0.941569983959198}]}, {"text": "For comparison purposes, the random baseline is also shown.", "labels": [], "entities": []}, {"text": "Our algorithm outperformed WordNet by 0.38 and DIRT by 0.15.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 27, "end_pos": 34, "type": "DATASET", "confidence": 0.9078089594841003}]}, {"text": "The improvement is significant vs. WordNet (73+, 27-, 29 ties: p<0.001) as well as vs. DIRT (37+, 20-, 72 ties: p=0.034).", "labels": [], "entities": [{"text": "WordNet", "start_pos": 35, "end_pos": 42, "type": "DATASET", "confidence": 0.96319180727005}, {"text": "DIRT", "start_pos": 87, "end_pos": 91, "type": "DATASET", "confidence": 0.917000949382782}]}, {"text": "We examined whether the improvement on DIRT was due to the fact that DIRT had less extensive coverage, encoding only verb pairs with similarity above a certain threshold.", "labels": [], "entities": []}, {"text": "We re-computed the accuracy scores for the two methods, ignoring cases where DIRT did not make any decision, i.e. where the database contained none of the five verb pairs of the test item.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9995155334472656}]}, {"text": "On the resulting 102 items, our method was again at an advantage, 0.735 vs. 0.647, but the significance of the difference could not be established (21+, 12-, 69 ties: p=0.164).", "labels": [], "entities": [{"text": "significance", "start_pos": 91, "end_pos": 103, "type": "METRIC", "confidence": 0.9591958522796631}]}, {"text": "The difference in the performance between our algorithm and the human judges is quite large (0.103 vs. Judge 1 and 0.088 vs Judge 2), but significance to the 0.05 level could not be found (vs. Judge 1: 17-, 29+, 83 ties: p=0.105; vs. Judge 2: 15-, 27+, ties 87: p=0.09).", "labels": [], "entities": []}], "tableCaptions": []}