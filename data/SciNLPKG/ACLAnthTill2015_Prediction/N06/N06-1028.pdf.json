{"title": [{"text": "Towards Automatic Scoring of Non-Native Spontaneous Speech", "labels": [], "entities": [{"text": "Automatic Scoring of Non-Native Spontaneous Speech", "start_pos": 8, "end_pos": 58, "type": "TASK", "confidence": 0.7287679016590118}]}], "abstractContent": [{"text": "This paper investigates the feasibility of automated scoring of spoken English proficiency of non-native speakers.", "labels": [], "entities": []}, {"text": "Unlike existing automated assessments of spoken English, our data consists of spontaneous spoken responses to complex test items.", "labels": [], "entities": []}, {"text": "We perform both a quantitative and a qualitative analysis of these features using two different machine learning approaches.", "labels": [], "entities": []}, {"text": "(1) We use support vector machines to produce a score and evaluate it with respect to a mode baseline and to human rater agreement.", "labels": [], "entities": []}, {"text": "We find that scoring based on support vector machines yields accuracies approaching inter-rater agreement in some cases.", "labels": [], "entities": []}, {"text": "(2) We use classification and regression trees to understand the role of different features and feature classes in the characterization of speaking proficiency by human scorers.", "labels": [], "entities": []}, {"text": "Our analysis shows that across all the test items most or all the feature classes are used in the nodes of the trees suggesting that the scores are, appropriately, a combination of multiple components of speaking proficiency.", "labels": [], "entities": []}, {"text": "Future research will concentrate on extending the set of features and introducing new feature classes to arrive at a scoring model that comprises additional relevant aspects of speaking proficiency.", "labels": [], "entities": []}], "introductionContent": [{"text": "While automated scoring of open-ended written discourse has been approached by several groups recently, automated scoring of spontaneous spoken language has proven to be more challenging and complex.", "labels": [], "entities": [{"text": "automated scoring of open-ended written discourse", "start_pos": 6, "end_pos": 55, "type": "TASK", "confidence": 0.7388854523499807}, {"text": "automated scoring of spontaneous spoken language", "start_pos": 104, "end_pos": 152, "type": "TASK", "confidence": 0.7517928083737692}]}, {"text": "Spoken language tests are still mostly scored by human raters.", "labels": [], "entities": []}, {"text": "However, several systems exist that score different aspects of spoken language;;).", "labels": [], "entities": []}, {"text": "Our work departs from previous research in that our goal is to study the feasibility of automating scoring for spontaneous speech, that is, when the spoken text is not known in advance.", "labels": [], "entities": []}, {"text": "We approach scoring here as the characterization of a speaker's oral proficiency based on features that can be extracted from a spoken response to a well defined test question by means of automatic speech recognition (ASR).", "labels": [], "entities": [{"text": "automatic speech recognition (ASR)", "start_pos": 188, "end_pos": 222, "type": "TASK", "confidence": 0.7689239978790283}]}, {"text": "We further approach scoring as the construction of a mapping from a set of features to a score scale, in our case five discrete scores from 1 (least proficient) to 5 (most proficient).", "labels": [], "entities": []}, {"text": "The set of features and the specific mapping are motivated by the concept of communicative competence.", "labels": [], "entities": []}, {"text": "This means that the features in the scoring system we are developing are meant to characterize specific components of communicative competence, such as mastery of pronunciation, fluency, prosodic, lexical, grammatical and pragmatical subskills.", "labels": [], "entities": []}, {"text": "The selection of features is guided by an understanding of the nature of speaking proficiency.", "labels": [], "entities": []}, {"text": "We rely on the scoring behavior of judges to evaluate the features (section 8) as well as a convenient criterion for evaluating the feasibility of automated scoring based on those features (section 7).", "labels": [], "entities": []}, {"text": "That is, the role of human scorers in this context is to provide a standard for system evaluations (see section 7), as well as to validate specific features and feature classes chosen by the authors (section 8).", "labels": [], "entities": []}, {"text": "We use support vector machines to determine how well the features recover human scores.", "labels": [], "entities": []}, {"text": "We collect performance data under three different conditions, where features are either based on actual recognizer output or on forced alignment.", "labels": [], "entities": []}, {"text": "(Forced alignment describes a procedure in speech recognition where the recognizer is looking for the most likely path through the Hidden Markov Models given a transcription of the speech file by an experienced transcriber.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 43, "end_pos": 61, "type": "TASK", "confidence": 0.7224358767271042}]}, {"text": "This helps, e.g., in finding start and end times of words or phonemes.)", "labels": [], "entities": [{"text": "finding start and end times of words or phonemes", "start_pos": 21, "end_pos": 69, "type": "TASK", "confidence": 0.736559722158644}]}, {"text": "We then use classification and regression trees (CART) as a means to evaluate the relative importance and salience of our features.", "labels": [], "entities": []}, {"text": "When the classification criterion is a human score, as is the casein this study, an inspection of the CART tree can give us insights into the feature preferences a human judge might have in deciding on a score.", "labels": [], "entities": []}, {"text": "The organization of this paper is as follows: first, we discuss related work in spoken language scoring.", "labels": [], "entities": [{"text": "spoken language scoring", "start_pos": 80, "end_pos": 103, "type": "TASK", "confidence": 0.6383510728677114}]}, {"text": "Next, we introduce the data of our study and the speech recognizer used.", "labels": [], "entities": [{"text": "speech recognizer", "start_pos": 49, "end_pos": 66, "type": "TASK", "confidence": 0.700551226735115}]}, {"text": "In section 5 we describe features we used for this study.", "labels": [], "entities": []}, {"text": "Section 6 describes the agreement among raters for this data.", "labels": [], "entities": []}, {"text": "Section 7 describes the SVM analysis, section 8 the CART analysis.", "labels": [], "entities": []}, {"text": "This is followed by a discussion and then finally by conclusions and an outlook on future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "We built five SVM models based on the train data, one for each of the five test items.", "labels": [], "entities": []}, {"text": "Each model has two versions: (a) based on forced alignment with the true reference, representing the case with 100% word accuracy (align), and (b) based on the actual recognition output hypotheses (hypo).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 121, "end_pos": 129, "type": "METRIC", "confidence": 0.6107812523841858}]}, {"text": "The SVM models were tested on the eval data set and there were three test conditions: (1) both training and test conditions derived from forced alignment (alignalign); (2) models trained on forced alignment and evaluated based on actual recognition hypotheses (align-hypo; this represented the realistic situation that while human transcriptions are made for the training set, they would turnout to be too costly when the system is running continuously); and (3) both training and evaluation are based on ASR output in recognition mode (hypo-hypo).", "labels": [], "entities": [{"text": "eval data set", "start_pos": 34, "end_pos": 47, "type": "DATASET", "confidence": 0.7663777073224386}]}, {"text": "We identified the best models by running a set of SVMs with varying cost factors, ranging from 0.01 to 15, and three different kernels: radial basis function, and polynomial, of second degree and of third degree.", "labels": [], "entities": []}, {"text": "We selected the best performing models measured on the train set and report results with these models on the eval set.", "labels": [], "entities": []}, {"text": "The cost factor for all three configurations varied between 5 and 15 among the five test items, and as best kernel we found the radial basis function in almost all cases, except for some polynomial kernels in the hypo-hypo configuration Mode: Speech scoring: Mode baseline, SVM performance on forced alignment and standard recognition data, and human agreement for all five test items (ind=independent task; int=integrated task).", "labels": [], "entities": []}, {"text": "shows the results for the SVM analysis as well as a baseline measure of agreement and the inter rater agreement.", "labels": [], "entities": []}, {"text": "The baseline refers to the expected level of agreement with Rater1 by simply assigning the mode of the distribution of scores fora given question, i.e., to always assign the most frequently occurring score on the train set.", "labels": [], "entities": [{"text": "Rater1", "start_pos": 60, "end_pos": 66, "type": "DATASET", "confidence": 0.9053990244865417}]}, {"text": "also reports the agreement between trained raters.", "labels": [], "entities": []}, {"text": "As can be seen the human agreement is consistently higher than the mode agreement but the difference is less for the integrated questions suggesting that humans scorers found those questions more challenging to score consistently.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Speech scoring: Mode baseline, SVM performance on forced alignment and standard recogni- tion data, and human agreement for all five test items (ind=independent task; int=integrated task).", "labels": [], "entities": []}, {"text": " Table 3: Distribution of features from the nodes of five CART trees (rows) into feature classes (columns). The \"to- tals\" in the last colunmn and row count first the number of classes with at least one feature and then sums the fea- tures (in parentheses).", "labels": [], "entities": []}]}