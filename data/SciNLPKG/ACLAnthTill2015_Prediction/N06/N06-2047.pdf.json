{"title": [{"text": "A Maximum Entropy Framework that Integrates Word Dependencies and Grammatical Relations for Reading Comprehension", "labels": [], "entities": []}], "abstractContent": [{"text": "Automatic reading comprehension (RC) systems can analyze a given passage and generate/extract answers in response to questions about the passage.", "labels": [], "entities": [{"text": "Automatic reading comprehension (RC)", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.6296053379774094}]}, {"text": "The RC passages are often constrained in their lengths and the target answer sentence usually occurs very few times.", "labels": [], "entities": [{"text": "lengths", "start_pos": 47, "end_pos": 54, "type": "METRIC", "confidence": 0.9590378403663635}]}, {"text": "In order to generate/extract a specific precise answer , this paper proposes the integration of two types of \"deep\" linguistic features, namely word dependencies and grammatical relations, in a maximum entropy (ME) framework to handle the RC task.", "labels": [], "entities": []}, {"text": "The proposed approach achieves 44.7% and 73.2% HumSent accuracy on the Reme-dia and ChungHwa corpora respectively.", "labels": [], "entities": [{"text": "HumSent", "start_pos": 47, "end_pos": 54, "type": "METRIC", "confidence": 0.95485919713974}, {"text": "accuracy", "start_pos": 55, "end_pos": 63, "type": "METRIC", "confidence": 0.8434373736381531}, {"text": "Reme-dia", "start_pos": 71, "end_pos": 79, "type": "DATASET", "confidence": 0.972913920879364}, {"text": "ChungHwa corpora", "start_pos": 84, "end_pos": 100, "type": "DATASET", "confidence": 0.8720532655715942}]}, {"text": "This result is competitive with other results reported thus far.", "labels": [], "entities": []}], "introductionContent": [{"text": "Automatic reading comprehension (RC) systems can analyze a given passage and generate/extract answers in response to questions about the passage.", "labels": [], "entities": [{"text": "Automatic reading comprehension (RC)", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.6296053379774094}]}, {"text": "The RC passages are often constrained in their lengths and the target answer sentence usually occurs only once (or very few times).", "labels": [], "entities": []}, {"text": "This differentiates the RC task from other tasks such as open-domain question answering (QA) in the Text Retrieval Conference ().", "labels": [], "entities": [{"text": "open-domain question answering (QA) in the Text Retrieval Conference", "start_pos": 57, "end_pos": 125, "type": "TASK", "confidence": 0.7218425382267345}]}, {"text": "In order to generate/extract a specific precise answer to a given question from a short passage, \"deep\" linguistic analysis of sentences in a passage is needed.", "labels": [], "entities": []}, {"text": "Previous efforts in RC often use the bag-of-words (BOW) approach as the baseline, which is further augmented with techniques such as shallow syntactic analysis, the use of named entities (NE) and pronoun references.", "labels": [], "entities": [{"text": "shallow syntactic analysis", "start_pos": 133, "end_pos": 159, "type": "TASK", "confidence": 0.6345903972784678}]}, {"text": "For example, have augmented the BOW approach with stemming, NE recognition, NE filtering, semantic class identification and pronoun resolution to achieve 36% HumSent 1 accuracy in the Remedia test set.", "labels": [], "entities": [{"text": "BOW", "start_pos": 32, "end_pos": 35, "type": "METRIC", "confidence": 0.7436723709106445}, {"text": "NE recognition", "start_pos": 60, "end_pos": 74, "type": "TASK", "confidence": 0.8512915670871735}, {"text": "NE filtering", "start_pos": 76, "end_pos": 88, "type": "TASK", "confidence": 0.8126615881919861}, {"text": "semantic class identification", "start_pos": 90, "end_pos": 119, "type": "TASK", "confidence": 0.7010297377904257}, {"text": "pronoun resolution", "start_pos": 124, "end_pos": 142, "type": "TASK", "confidence": 0.7150511741638184}, {"text": "HumSent 1", "start_pos": 158, "end_pos": 167, "type": "METRIC", "confidence": 0.5254630744457245}, {"text": "accuracy", "start_pos": 168, "end_pos": 176, "type": "METRIC", "confidence": 0.6557270884513855}, {"text": "Remedia test set", "start_pos": 184, "end_pos": 200, "type": "DATASET", "confidence": 0.881627102692922}]}, {"text": "Based on these technologies, improved the HumSent accuracy to 40% by applying a set of heuristic rules that assign handcrafted weights to matching words and NE.", "labels": [], "entities": [{"text": "HumSent", "start_pos": 42, "end_pos": 49, "type": "DATASET", "confidence": 0.5138550400733948}, {"text": "accuracy", "start_pos": 50, "end_pos": 58, "type": "METRIC", "confidence": 0.9529046416282654}]}, {"text": "used additional strategies for different question types to achieve 41%.", "labels": [], "entities": []}, {"text": "An example strategy for why questions is that if the first word of the matching sentence is \"this,\" \"that,\" \"these\" or \"those,\" the system should select the previous sentence as an answer.", "labels": [], "entities": []}, {"text": "also introduced an approach to estimate the performance upper bound of the BOW approach.", "labels": [], "entities": [{"text": "BOW", "start_pos": 75, "end_pos": 78, "type": "METRIC", "confidence": 0.6782836318016052}]}, {"text": "When we apply the same approach to the Remedia test set, we obtained the upper bound of 48.3% HumSent accuracy.", "labels": [], "entities": [{"text": "Remedia test set", "start_pos": 39, "end_pos": 55, "type": "DATASET", "confidence": 0.9320841828982035}, {"text": "HumSent", "start_pos": 94, "end_pos": 101, "type": "METRIC", "confidence": 0.7540203332901001}, {"text": "accuracy", "start_pos": 102, "end_pos": 110, "type": "METRIC", "confidence": 0.64656662940979}]}, {"text": "The state-of-art performance reached 42% with answer patterns derived from web (.", "labels": [], "entities": []}, {"text": "This paper investigates the possibility of enhancing RC performance by applying \"deep\" linguistic analysis for every sentence in the passage.", "labels": [], "entities": [{"text": "RC", "start_pos": 53, "end_pos": 55, "type": "TASK", "confidence": 0.9617618322372437}]}, {"text": "We refer to the use of two types of features, namely word dependencies and grammatical relations, that are integrated in a maximum entropy framework.", "labels": [], "entities": []}, {"text": "Word dependencies refer to the headword dependencies in lexicalized syntactic parse trees, together with part-of-speech (POS) information.", "labels": [], "entities": []}, {"text": "Grammatical relations (GR) refer to linkages such as subject, object, modifier, etc.", "labels": [], "entities": [{"text": "Grammatical relations (GR)", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.6546832084655761}]}, {"text": "The ME framework has shown its effectiveness in solving QA tasks.", "labels": [], "entities": [{"text": "solving QA tasks", "start_pos": 48, "end_pos": 64, "type": "TASK", "confidence": 0.7225452860196432}]}, {"text": "In comparison with previous approaches mentioned earlier, the current approach involves richer syntactic information that cover longer-distance relationships.", "labels": [], "entities": []}], "datasetContent": [{"text": "We selected the features used in Quarc () to establish the reference performance level.", "labels": [], "entities": []}, {"text": "In our experiments, the 24 rules in Quarc are transferred 6 to ME features: \"If contains(Q,{start, begin}) and contains(S,{start, begin, since, year}) Then Score(S)+=20\" \u2192 f j (x, y) = 1 (0< j <25) if Q is a when question that contains \"start\" or \"begin\" and C contains \"start,\" \"begin,\" \"since\" or \"year\"; f j (x, y) = 0 otherwise.", "labels": [], "entities": []}, {"text": "In addition to the Quarc features, we resolved five pronouns (he, him, his, she and her) in the stories based on the annotation in the corpora.", "labels": [], "entities": []}, {"text": "The result of using Quarc features in the ME framework is 38.3% HumSent accuracy on the Remedia test set.", "labels": [], "entities": [{"text": "HumSent", "start_pos": 64, "end_pos": 71, "type": "METRIC", "confidence": 0.9827122688293457}, {"text": "accuracy", "start_pos": 72, "end_pos": 80, "type": "METRIC", "confidence": 0.7907914519309998}, {"text": "Remedia test set", "start_pos": 88, "end_pos": 104, "type": "DATASET", "confidence": 0.9320649107297262}]}, {"text": "This is lower than the result (40%) obtained by our re-implementation of Quarc that uses handcrafted scores.", "labels": [], "entities": []}, {"text": "A possible explanation is that handcrafted scores are more reliable than ME, since humans can generalize the score even for sparse data.", "labels": [], "entities": [{"text": "ME", "start_pos": 73, "end_pos": 75, "type": "METRIC", "confidence": 0.9848237037658691}]}, {"text": "Therefore, we refined our reference performance level by combining the ME models (MEM) and handcrafted models (HCM).", "labels": [], "entities": []}, {"text": "Suppose the score of a question-answer pair is score(Q, Ci), the conditional probability that Ci answers Q in HCM is: . We combined the probabilities from MEM and HCM in the following manner: To obtain the optimal \u03b1, we partitioned the training set into four bins.", "labels": [], "entities": []}, {"text": "The ME models are trained on three different bins; the optimal \u03b1 is determined on the other bins.", "labels": [], "entities": []}, {"text": "By trying different bins combinations and different \u03b1 such that 0 < \u03b1 < 1 with interval 0.1, we obtained the average optimal \u03b1 = 0.15 and 0.9 from the Remedia and ChungHwa training sets respectively 7 . Our baseline used the combined ME models and handcrafted models to achieve 40.3% and 70.6% HumSent accuracy in the Remedia and ChungHwa test sets respectively.", "labels": [], "entities": [{"text": "ChungHwa training sets", "start_pos": 163, "end_pos": 185, "type": "DATASET", "confidence": 0.8942594528198242}, {"text": "HumSent", "start_pos": 294, "end_pos": 301, "type": "METRIC", "confidence": 0.7750552296638489}, {"text": "accuracy", "start_pos": 302, "end_pos": 310, "type": "METRIC", "confidence": 0.805395245552063}, {"text": "Remedia and ChungHwa test sets", "start_pos": 318, "end_pos": 348, "type": "DATASET", "confidence": 0.7880891323089599}]}, {"text": "We setup our experiments such that the linguistic features are applied incrementally -(i) First , we use only POS tags of matching words among questions and candidate answer sentences.", "labels": [], "entities": []}, {"text": "(ii) Then we add POS tags of the matching dependencies.", "labels": [], "entities": []}, {"text": "(iii) We apply only GR features from MINIPAR.", "labels": [], "entities": [{"text": "MINIPAR", "start_pos": 37, "end_pos": 44, "type": "DATASET", "confidence": 0.8896723985671997}]}, {"text": "(iv) All features are used.", "labels": [], "entities": []}, {"text": "These four feature sets are denoted as \"+wp,\" \"+wp+dp,\" \"+mini\" and \"+wp+dp+mini\" respectively.", "labels": [], "entities": []}, {"text": "The results are shown in for the Remedia and ChungHwa test sets.", "labels": [], "entities": [{"text": "Remedia", "start_pos": 33, "end_pos": 40, "type": "DATASET", "confidence": 0.9092427492141724}, {"text": "ChungHwa test sets", "start_pos": 45, "end_pos": 63, "type": "DATASET", "confidence": 0.910818338394165}]}, {"text": "With the significance level 0.05, the pairwise ttest (for every question) to the statistical significance of the improvements shows that the p-value is 0.009 and 0.025 for the Remedia and ChungHwa test sets respectively.", "labels": [], "entities": [{"text": "significance", "start_pos": 9, "end_pos": 21, "type": "METRIC", "confidence": 0.9570258259773254}, {"text": "Remedia and ChungHwa test sets", "start_pos": 176, "end_pos": 206, "type": "DATASET", "confidence": 0.7956838011741638}]}, {"text": "The \"deep\" syntactic features significantly improve the performance over the baseline system on the Remedia and ChungHwa test sets 8 .", "labels": [], "entities": [{"text": "Remedia and ChungHwa test sets 8", "start_pos": 100, "end_pos": 132, "type": "DATASET", "confidence": 0.8453346093495687}]}], "tableCaptions": []}