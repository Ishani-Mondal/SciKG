{"title": [{"text": "Document Representation and Multilevel Measures of Document Similarity", "labels": [], "entities": [{"text": "Document Representation", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.8743762969970703}, {"text": "Multilevel Measures of Document Similarity", "start_pos": 28, "end_pos": 70, "type": "TASK", "confidence": 0.5558800578117371}]}], "abstractContent": [{"text": "We present our work on combining large-scale statistical approaches with local linguistic analysis and graph-based machine learning techniques to compute a combined measure of semantic similarity between terms and documents for application in information extraction, question answering, and summarisation.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 243, "end_pos": 265, "type": "TASK", "confidence": 0.8084035217761993}, {"text": "question answering", "start_pos": 267, "end_pos": 285, "type": "TASK", "confidence": 0.9182472825050354}, {"text": "summarisation", "start_pos": 291, "end_pos": 304, "type": "TASK", "confidence": 0.9917396306991577}]}], "introductionContent": [{"text": "Document indexing and representation of termdocument relations are crucial for document classification, clustering and retrieval.", "labels": [], "entities": [{"text": "Document indexing", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8860075771808624}, {"text": "document classification", "start_pos": 79, "end_pos": 102, "type": "TASK", "confidence": 0.7430455088615417}]}, {"text": "In the traditional bag-of-words vector space representation of documents ( words represent orthogonal dimensions which makes an unrealistic assumption about their independence.", "labels": [], "entities": []}, {"text": "Since document vectors are constructed in a very high dimensional vocabulary space, there has been a considerable interest in low-dimensional document representations to overcome the drawbacks of the bag-of-words document vectors.", "labels": [], "entities": []}, {"text": "Latent Semantic Analysis (LSA)) is one of the best known dimensionality reduction algorithms in information retrieval.", "labels": [], "entities": [{"text": "Latent Semantic Analysis (LSA))", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.7265812853972117}, {"text": "dimensionality reduction", "start_pos": 57, "end_pos": 81, "type": "TASK", "confidence": 0.7256495356559753}, {"text": "information retrieval", "start_pos": 96, "end_pos": 117, "type": "TASK", "confidence": 0.8441213071346283}]}, {"text": "In my research, I consider different notions of similarity measure between documents.", "labels": [], "entities": [{"text": "similarity measure", "start_pos": 48, "end_pos": 66, "type": "METRIC", "confidence": 0.903919905424118}]}, {"text": "I use dimensionality reduction and statistical co-occurrence information to define representations that support them.", "labels": [], "entities": [{"text": "dimensionality reduction", "start_pos": 6, "end_pos": 30, "type": "TASK", "confidence": 0.7287647873163223}]}], "datasetContent": [{"text": "We used the TOEFL, TS1 and TS2 synonymy tests to demonstrate that the GLSA vector space representation for terms captures their semantic relations, see (Matveeva et al.,) for details.", "labels": [], "entities": [{"text": "TOEFL", "start_pos": 12, "end_pos": 17, "type": "METRIC", "confidence": 0.6963974237442017}]}, {"text": "Our results demonstrate that similarities between GLSA term vectors achieve better results than PMI scores and outperform the related PMI-IR approach.", "labels": [], "entities": []}, {"text": "On the TOEFL test GLSA achieves the best precision of 0.86, which is much better than our PMI baseline as well as the highest precision of 0.81 reported in.", "labels": [], "entities": [{"text": "TOEFL test GLSA", "start_pos": 7, "end_pos": 22, "type": "DATASET", "confidence": 0.7135372360547384}, {"text": "precision", "start_pos": 41, "end_pos": 50, "type": "METRIC", "confidence": 0.998764157295227}, {"text": "PMI baseline", "start_pos": 90, "end_pos": 102, "type": "DATASET", "confidence": 0.6951277852058411}, {"text": "precision", "start_pos": 126, "end_pos": 135, "type": "METRIC", "confidence": 0.9889931082725525}]}, {"text": "GLSA achieves the same maximum precision as in) for TS1 (0.73) and higher precision on TS2 (0.82 compared to 0.75 in).", "labels": [], "entities": [{"text": "GLSA", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.7088859677314758}, {"text": "precision", "start_pos": 31, "end_pos": 40, "type": "METRIC", "confidence": 0.9964138865470886}, {"text": "TS1", "start_pos": 52, "end_pos": 55, "type": "DATASET", "confidence": 0.7514388561248779}, {"text": "precision", "start_pos": 74, "end_pos": 83, "type": "METRIC", "confidence": 0.999173104763031}, {"text": "TS2", "start_pos": 87, "end_pos": 90, "type": "DATASET", "confidence": 0.8693141937255859}]}, {"text": "We also conducted document classification experiments to demonstrate the advantage of the GLSA document vectors ().", "labels": [], "entities": [{"text": "document classification", "start_pos": 18, "end_pos": 41, "type": "TASK", "confidence": 0.8093989789485931}, {"text": "GLSA document vectors", "start_pos": 90, "end_pos": 111, "type": "DATASET", "confidence": 0.7977567315101624}]}, {"text": "We used a k-nearest neighbors classifier fora set of 5300 documents from 6 dissimilar groups from the 20 news groups data set.", "labels": [], "entities": [{"text": "news groups data set", "start_pos": 105, "end_pos": 125, "type": "DATASET", "confidence": 0.8467922210693359}]}, {"text": "The k-nn classifier achieved higher accuracy with the GLSA document vectors than with the traditional tf-idf document vectors, especially with fewer training examples.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.9990608096122742}, {"text": "GLSA document vectors", "start_pos": 54, "end_pos": 75, "type": "DATASET", "confidence": 0.8276263475418091}]}, {"text": "With 100 training examples, the k-nn classifier with GLSA had 0.75 accuracy vs. 0.58 with the tf-idf document vectors.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 67, "end_pos": 75, "type": "METRIC", "confidence": 0.9993515610694885}]}, {"text": "With 1000 training examples the numbers were 0.81 vs. 0.75.", "labels": [], "entities": []}, {"text": "The inner product between the GLSA document vectors can be used as input to other algorithms.", "labels": [], "entities": []}, {"text": "The language modelling approach (Berger and Lafferty, 1999) proved very effective for the information retrieval task.", "labels": [], "entities": [{"text": "language modelling", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.7818634212017059}, {"text": "information retrieval task", "start_pos": 90, "end_pos": 116, "type": "TASK", "confidence": 0.8681325117746989}]}, {"text": "Berger et. al) used translation probabilities between the document and query terms to account for synonymy and polysemy.", "labels": [], "entities": []}, {"text": "We proposed to use low dimensional term vectors for inducing the translation probabilities between terms ().", "labels": [], "entities": []}, {"text": "We used the same k-nn classification task as above.", "labels": [], "entities": []}, {"text": "With 100 training examples, the k-nn accuracy based on tf-idf document vectors was 0.58 and with the similarity based on the language modelling with GLSA term translation probabilities the accuracy was 0.69.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.9590522050857544}, {"text": "GLSA term translation", "start_pos": 149, "end_pos": 170, "type": "TASK", "confidence": 0.5352494815985361}, {"text": "accuracy", "start_pos": 189, "end_pos": 197, "type": "METRIC", "confidence": 0.9994294047355652}]}, {"text": "With larger training sets the difference in performance was less significant.", "labels": [], "entities": []}, {"text": "These results illustrate that the pair-wise similarities between the GLSA term vectors add important semantic information which helps to go beyond term matching and deal with synonymy and polysemy.", "labels": [], "entities": [{"text": "term matching", "start_pos": 147, "end_pos": 160, "type": "TASK", "confidence": 0.7075356692075729}]}], "tableCaptions": []}