{"title": [], "abstractContent": [{"text": "Coreference resolution, like many problems in natural language processing, has most often been explored using datasets of written text.", "labels": [], "entities": [{"text": "Coreference resolution", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9426993131637573}]}, {"text": "While spontaneous spoken language poses well-known challenges, it also offers additional modalities that may help disambiguate some of the inherent disfluency.", "labels": [], "entities": []}, {"text": "We explore features of hand gesture that are correlated with coreference.", "labels": [], "entities": [{"text": "coreference", "start_pos": 61, "end_pos": 72, "type": "TASK", "confidence": 0.9505215883255005}]}, {"text": "Combining these features with a traditional textual model yields a statistically significant improvement in overall performance .", "labels": [], "entities": []}], "introductionContent": [{"text": "Although the natural language processing community has traditionally focused largely on text, face-to-face spoken language is ubiquitous, and offers the potential for breakthrough applications in domains such as meetings, lectures, and presentations.", "labels": [], "entities": []}, {"text": "We believe that in face-to-face discourse, it is important to consider the possibility that non-verbal communication may offer features that are critical to language understanding.", "labels": [], "entities": [{"text": "language understanding", "start_pos": 157, "end_pos": 179, "type": "TASK", "confidence": 0.7188903093338013}]}, {"text": "However, due to the long-standing emphasis on text datasets, there has been relatively little work on non-textual features in unconstrained natural language (prosody being the most notable exception).", "labels": [], "entities": []}, {"text": "Multimodal research in NLP has typically focused on dialogue systems for human-computer interaction (e.g.,); in contrast, we are interested in the applicability of multimodal features to unconstrained human-human dialogues.", "labels": [], "entities": []}, {"text": "We believe that such features will play an essential role in bringing NLP applications such as automatic summarization and segmentation to multimedia documents, such as lectures and meetings.", "labels": [], "entities": [{"text": "summarization and segmentation", "start_pos": 105, "end_pos": 135, "type": "TASK", "confidence": 0.5938437978426615}]}, {"text": "More specifically, in this paper we explore the possibility of applying hand gesture features to the problem of coreference resolution, which is thought to be fundamental to these more ambitious applications.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 112, "end_pos": 134, "type": "TASK", "confidence": 0.9854474663734436}]}, {"text": "To motivate the need for multimodal features in coreference resolution, consider the following transcript: \"[This circle (1)] is rotating clockwise and [this piece of wood] is attached at [this point (3)] and [this point (4)] but [it (5)] can rotate.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 48, "end_pos": 70, "type": "TASK", "confidence": 0.9819412529468536}]}, {"text": "So as [the circle (6)] rotates, [this (7)] moves in and out.", "labels": [], "entities": []}, {"text": "So [this whole thing (8)] is just going back and forth.\"", "labels": [], "entities": []}, {"text": "Even given a high degree of domain knowledge (e.g., that \"circles\" often \"rotate\" but \"points\" rarely do), determining the coreference in this excerpt seems difficult.", "labels": [], "entities": []}, {"text": "The word \"this\" accompanied by a gesture is frequently used to introduce anew entity, so it is difficult to determine from the text alone whether \"[this (7)]\" refers to \"[this piece of wood (2)],\" or to an entirely different part of the diagram.", "labels": [], "entities": []}, {"text": "In addition, \"[this whole thing (8)]\" could be anaphoric, or it might refer to anew entity, perhaps some superset of predefined parts.", "labels": [], "entities": []}, {"text": "The example text was drawn from a small corpus of dialogues, which has been annotated for coreference.", "labels": [], "entities": [{"text": "coreference", "start_pos": 90, "end_pos": 101, "type": "TASK", "confidence": 0.9755942225456238}]}, {"text": "Participants in the study had little difficulty understanding what was communicated.", "labels": [], "entities": []}, {"text": "While this does not prove that human listeners are using gesture or other multimodal features, it suggests that these features merit further investigation.", "labels": [], "entities": []}, {"text": "We extracted hand positions from the videos in the corpus, using computer vision.", "labels": [], "entities": []}, {"text": "From the raw hand positions, we derived gesture features that were used to supplement traditional textual features for coreference resolution.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 119, "end_pos": 141, "type": "TASK", "confidence": 0.9667298495769501}]}, {"text": "For a description of the study's protocol, automatic hand tracking, and a fuller examination of the gesture features, see).", "labels": [], "entities": [{"text": "hand tracking", "start_pos": 53, "end_pos": 66, "type": "TASK", "confidence": 0.7608934938907623}]}, {"text": "In this paper, we present results showing that these features yield a significant improvement in performance.", "labels": [], "entities": []}], "datasetContent": [{"text": "The results of our experiments are computed using mention-based CEAF scoring (, and are reported in Individual features for each of the four most common pronouns: \"this\", \"it\", \"that\", and \"they\" FOCUS DIST Distance between the position of the in-focus hand during j and i (see text) WHICH HAND Whether the hand in focus during j is the same as in i (see text): Results for the boosted decision trees (t(15) = 2.48, p < .02), though not for the voted perceptron (t(15) = 1.07, p = .15).", "labels": [], "entities": [{"text": "FOCUS DIST Distance", "start_pos": 196, "end_pos": 215, "type": "METRIC", "confidence": 0.8206343253453573}, {"text": "WHICH", "start_pos": 284, "end_pos": 289, "type": "METRIC", "confidence": 0.7583823204040527}, {"text": "HAND", "start_pos": 290, "end_pos": 294, "type": "METRIC", "confidence": 0.7377588152885437}]}, {"text": "In the \"all corefer\" baseline, all NPs are grouped into a single cluster; in the \"none corefer\", each NP gets its own cluster.", "labels": [], "entities": []}, {"text": "In the \"EXACT MATCH\" baseline, two NPs corefer when their surface forms are identical.", "labels": [], "entities": [{"text": "EXACT MATCH\"", "start_pos": 8, "end_pos": 20, "type": "TASK", "confidence": 0.47057350476582843}]}, {"text": "All experimental systems outperform all baselines by a statistically significant amount.", "labels": [], "entities": []}, {"text": "There are few other reported results for coreference resolution on spontaneous, unconstrained speech;) similarly finds low overall scores for pronoun resolution on the Switchboard Corpus, albeit by a different scoring metric.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 41, "end_pos": 63, "type": "TASK", "confidence": 0.9662837386131287}, {"text": "pronoun resolution", "start_pos": 142, "end_pos": 160, "type": "TASK", "confidence": 0.7333610951900482}, {"text": "Switchboard Corpus", "start_pos": 168, "end_pos": 186, "type": "DATASET", "confidence": 0.9076672196388245}]}, {"text": "Unfortunately, they do not compare performance to equivalent baselines.", "labels": [], "entities": []}, {"text": "For the AdaBoost method, 50 iterations of boosting are performed on shallow decision trees, with a maximum tree depth of three.", "labels": [], "entities": []}, {"text": "For the voted perceptron, 50 training iterations were performed.", "labels": [], "entities": []}, {"text": "The performance of the voted perceptron on this task was somewhat unstable, varying depending on the order in which the documents were presented.", "labels": [], "entities": []}, {"text": "This maybe because a small change in the weights can lead to a very different partitioning, which in turn affects the setting of the weights in the next perceptron iteration.", "labels": [], "entities": []}, {"text": "For these results, the order of presentation of the documents was randomized, and the scores for the voted perceptron are the average of 10 different runs (\u03c3 = 0.32% with gestures, 0.40% without).", "labels": [], "entities": []}, {"text": "Although the AdaBoost method minimizes pairwise error rather than the overall error of the partitioning, its performance was superior to the voted perceptron.", "labels": [], "entities": []}, {"text": "One possible explanation is that by boosting small decision trees, AdaBoost was able to take advantage of non-linear combinations of features.", "labels": [], "entities": []}, {"text": "We tested the voted perceptron using all pairwise combinations of features, but this did not improve performance.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The feature set", "labels": [], "entities": []}, {"text": " Table 3: Top 12 Features By Chi-Squared", "labels": [], "entities": []}]}