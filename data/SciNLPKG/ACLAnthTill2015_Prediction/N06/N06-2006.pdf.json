{"title": [{"text": "Class Model Adaptation for Speech Summarisation", "labels": [], "entities": [{"text": "Speech Summarisation", "start_pos": 27, "end_pos": 47, "type": "TASK", "confidence": 0.7391958236694336}]}], "abstractContent": [{"text": "The performance of automatic speech summarisation has been improved in previous experiments by using linguistic model adaptation.", "labels": [], "entities": [{"text": "automatic speech summarisation", "start_pos": 19, "end_pos": 49, "type": "TASK", "confidence": 0.5863785445690155}]}, {"text": "We extend such adaptation to the use of class models, whose ro-bustness further improves summarisation performance on a wider variety of objective evaluation metrics such as ROUGE-2 and ROUGE-SU4 used in the text sum-marisation literature.", "labels": [], "entities": [{"text": "summarisation", "start_pos": 89, "end_pos": 102, "type": "TASK", "confidence": 0.9809756278991699}]}, {"text": "Summaries made from automatic speech recogniser transcriptions benefit from relative improvements ranging from 6.0% to 22.2% on all investigated metrics.", "labels": [], "entities": [{"text": "speech recogniser transcriptions", "start_pos": 30, "end_pos": 62, "type": "TASK", "confidence": 0.765252153078715}]}], "introductionContent": [{"text": "Techniques for automatically summarising written text have been actively investigated in the field of natural language processing, and more recently new techniques have been developed for speech summarisation ().", "labels": [], "entities": [{"text": "summarising written text", "start_pos": 29, "end_pos": 53, "type": "TASK", "confidence": 0.8024856646855673}, {"text": "natural language processing", "start_pos": 102, "end_pos": 129, "type": "TASK", "confidence": 0.6650276978810629}, {"text": "speech summarisation", "start_pos": 188, "end_pos": 208, "type": "TASK", "confidence": 0.7036592066287994}]}, {"text": "However it is still very hard to obtain good quality summaries.", "labels": [], "entities": []}, {"text": "Moreover, recognition accuracy is still around 30% on spontaneous speech tasks, in contrast to speech read from text such as broadcast news.", "labels": [], "entities": [{"text": "recognition", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.869137167930603}, {"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9724445343017578}]}, {"text": "Spontaneous speech is characterised by disfluencies, repetitions, repairs, and fillers, all of which make recognition and consequently speech summarisation more difficult).", "labels": [], "entities": [{"text": "speech summarisation", "start_pos": 135, "end_pos": 155, "type": "TASK", "confidence": 0.6251001358032227}]}, {"text": "Ina previous study), linguistic model (LiM) adaptation using different types of word models has proved useful in order to improve summary quality.", "labels": [], "entities": [{"text": "linguistic model (LiM) adaptation", "start_pos": 21, "end_pos": 54, "type": "TASK", "confidence": 0.6520932664473852}]}, {"text": "However sparsity of the data available for adaptation makes it difficult to obtain reliable estimates of word n-gram probabilities.", "labels": [], "entities": [{"text": "adaptation", "start_pos": 43, "end_pos": 53, "type": "TASK", "confidence": 0.9599789381027222}]}, {"text": "In speech recognition, class models are often used in such cases to improve model robustness.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 3, "end_pos": 21, "type": "TASK", "confidence": 0.8212935328483582}]}, {"text": "In this paper we extend the work previously done on adapting the linguistic model of the speech summariser by investigating class models.", "labels": [], "entities": []}, {"text": "We also use a wider variety of objective evaluation metrics to corroborate results.", "labels": [], "entities": []}], "datasetContent": [{"text": "Experiments were performed on spontaneous speech, using 9 talks taken from the Translanguage English Database (TED) corpus (), each transcribed and manually summarised by nine different humans for both 10% and 30% summarization ratios.", "labels": [], "entities": [{"text": "Translanguage English Database (TED) corpus", "start_pos": 79, "end_pos": 122, "type": "DATASET", "confidence": 0.7334498294762203}]}, {"text": "Speech recognition transcriptions (ASR) were obtained for each talk, with an average word error rate of 33.3%.", "labels": [], "entities": [{"text": "Speech recognition transcriptions (ASR)", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.7226555496454239}, {"text": "word error rate", "start_pos": 85, "end_pos": 100, "type": "METRIC", "confidence": 0.8104087313016256}]}, {"text": "A corpus consisting of around ten years of conference proceedings (17.8M words) on the subject of speech and signal processing is used to generate the LiM B and word classes using the clustering algorithm in (.", "labels": [], "entities": []}, {"text": "Different types of component LiM are built and combined for adaptation as described in Section 2.", "labels": [], "entities": []}, {"text": "The first type of component linguistic models are built on the small corpus of hand-made summaries described above, made for the same summarisation ratio as the one we are generating.", "labels": [], "entities": []}, {"text": "For each talk the hand-made summaries of the other eight talks (i.e. 72 summaries) were used as the LiM training corpus.", "labels": [], "entities": [{"text": "LiM training corpus", "start_pos": 100, "end_pos": 119, "type": "DATASET", "confidence": 0.7737860282262167}]}, {"text": "This type of LiM is expected to help generate automatic summaries in the same style as those made manually.", "labels": [], "entities": [{"text": "summaries", "start_pos": 56, "end_pos": 65, "type": "TASK", "confidence": 0.8712772727012634}]}, {"text": "The second type of component linguistic models are built from the papers in the conference proceedings for the talk we want to summarise.", "labels": [], "entities": []}, {"text": "This type of LiM, used for topic adaptation, is investigated because key words and important sentences that appear in the associated paper are expected to have a high information value and should be selected during the summarisation process.", "labels": [], "entities": [{"text": "topic adaptation", "start_pos": 27, "end_pos": 43, "type": "TASK", "confidence": 0.8593372106552124}]}, {"text": "Three sets of experiments were made: in the first experiment (referred to as Word), LiM B and both component models are word models, as introduced in ().", "labels": [], "entities": [{"text": "Word", "start_pos": 77, "end_pos": 81, "type": "DATASET", "confidence": 0.917514979839325}]}, {"text": "For the second one (Class), both LiM B and the component models are class models built using exactly the same data as the word models.", "labels": [], "entities": []}, {"text": "For the third experiment (Mixed), the LiM B is an interpolation of class and word models, while the component LiMs are class models.", "labels": [], "entities": []}, {"text": "To optimise use of the available data, a rotating form of cross-validation () is used: all talks but one are used for development, the remaining talk being used for testing.", "labels": [], "entities": []}, {"text": "Summaries from the development talks are generated automatically by the system using different sets of parameters and the LiM B . These summaries are evaluated and the set of parameters which maximises the development score for the LiM B is selected for the remaining talk.", "labels": [], "entities": []}, {"text": "The purpose of the development phase is to choose the most effective combination of weights \u03b1 C , \u03b1 I and \u03b1 L . The summary generated for each talk using its set of optimised parameters is then evaluated using the same metric, which gives us our baseline for this talk.", "labels": [], "entities": []}, {"text": "Using the same parameters as those that were selected for the baseline, we generate summaries for the lectures in the development set for different LiM interpolation weights \u03bb k . Values between 0 and 1 in steps of 0.1, were investigated for the latter, and an optimal set of \u03bb k is selected.", "labels": [], "entities": []}, {"text": "Using these interpolation weights, as well as the set of parameters determined for the baseline, we generate a summary of the test talk, which is evaluated using the same evaluation metric, giving us our final adapted result for this talk.", "labels": [], "entities": []}, {"text": "Averaging those results over the test set (i.e. all talks) gives us our final adapted result.", "labels": [], "entities": []}, {"text": "This process is repeated for all evaluation metrics, and all three experiments (Word, Class, and Mixed).", "labels": [], "entities": []}, {"text": "Lower bound results are given by random summarisation (Random) i.e. randomly extracting sentences and words, without use of the scores present in Equation (1) for appropriate summarisation ratios.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: TRS baseline and adapted results.", "labels": [], "entities": [{"text": "TRS baseline", "start_pos": 10, "end_pos": 22, "type": "METRIC", "confidence": 0.3735280930995941}]}, {"text": " Table 2: ASR baseline and adapted results.", "labels": [], "entities": [{"text": "ASR", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.6677319407463074}]}]}