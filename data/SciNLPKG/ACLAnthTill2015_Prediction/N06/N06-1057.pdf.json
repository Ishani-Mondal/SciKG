{"title": [{"text": "ParaEval: Using Paraphrases to Evaluate Summaries Automatically", "labels": [], "entities": []}], "abstractContent": [{"text": "ParaEval is an automated evaluation method for comparing reference and peer summaries.", "labels": [], "entities": []}, {"text": "It facilitates a tiered-comparison strategy where recall-oriented global optimal and local greedy searches for paraphrase matching are enabled in the top tiers.", "labels": [], "entities": [{"text": "recall-oriented", "start_pos": 50, "end_pos": 65, "type": "METRIC", "confidence": 0.9265315532684326}]}, {"text": "We utilize a domain-independent paraphrase table extracted from a large bilingual parallel corpus using methods from Machine Translation (MT).", "labels": [], "entities": [{"text": "Machine Translation (MT)", "start_pos": 117, "end_pos": 141, "type": "TASK", "confidence": 0.8421798825263977}]}, {"text": "We show that the quality of ParaE-val's evaluations, measured by correlating with human judgments, closely resembles that of ROUGE's.", "labels": [], "entities": []}], "introductionContent": [{"text": "Content coverage is commonly measured in summary comparison to assess how much information from the reference summary is included in a peer summary.", "labels": [], "entities": []}, {"text": "Both manual and automatic methodologies have been used.", "labels": [], "entities": []}, {"text": "Naturally, there is a great amount of confidence in manual evaluation since humans can infer, paraphrase, and use world knowledge to relate text units with similar meanings, but which are worded differently.", "labels": [], "entities": []}, {"text": "Human efforts are preferred if the evaluation task is easily conducted and managed, and does not need to be performed repeatedly.", "labels": [], "entities": []}, {"text": "However, when resources are limited, automated evaluation methods become more desirable.", "labels": [], "entities": []}, {"text": "For years, the summarization community has been actively seeking an automatic evaluation methodology that can be readily applied to various summarization tasks.", "labels": [], "entities": [{"text": "summarization", "start_pos": 15, "end_pos": 28, "type": "TASK", "confidence": 0.9845505356788635}, {"text": "summarization tasks", "start_pos": 140, "end_pos": 159, "type": "TASK", "confidence": 0.9231800734996796}]}, {"text": "ROUGE () has gained popularity due to its simplicity and high correlation with human judgments.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.622867226600647}]}, {"text": "Even though validated by high correlations with human judgments gathered from previous Document Understanding Conference (DUC) experiments, current automatic procedures () only employ lexical n-gram matching.", "labels": [], "entities": [{"text": "Document Understanding Conference (DUC)", "start_pos": 87, "end_pos": 126, "type": "TASK", "confidence": 0.5491160005331039}]}, {"text": "The lack of support for word or phrase matching that stretches beyond strict lexical matches has limited the expressiveness and utility of these methods.", "labels": [], "entities": [{"text": "word or phrase matching", "start_pos": 24, "end_pos": 47, "type": "TASK", "confidence": 0.6104567348957062}]}, {"text": "We need a mechanism that supplements literal matching-i.e. paraphrase and synonym-and approximates semantic closeness.", "labels": [], "entities": [{"text": "literal matching-i.e.", "start_pos": 37, "end_pos": 58, "type": "TASK", "confidence": 0.7178986966609955}]}, {"text": "In this paper we present ParaEval, an automatic summarization evaluation method, which facilitates paraphrase matching in an overall three-level comparison strategy.", "labels": [], "entities": [{"text": "summarization evaluation", "start_pos": 48, "end_pos": 72, "type": "TASK", "confidence": 0.8955652713775635}, {"text": "paraphrase matching", "start_pos": 99, "end_pos": 118, "type": "TASK", "confidence": 0.7740648984909058}]}, {"text": "At the top level, favoring higher coverage in reference, we perform an optimal search via dynamic programming to find multi-word to multi-word paraphrase matches between phrases in the reference summary (usually human-written) and those in the peer summary (system-generated).", "labels": [], "entities": []}, {"text": "The non-matching fragments from the previous level are then searched by a greedy algorithm to find single-word paraphrase/synonym matches.", "labels": [], "entities": []}, {"text": "At the third and the lowest level, we perform literal lexical unigram matching on the remaining texts.", "labels": [], "entities": [{"text": "literal lexical unigram matching", "start_pos": 46, "end_pos": 78, "type": "TASK", "confidence": 0.6498855501413345}]}, {"text": "This tiered design for summary comparison guarantees at least a ROUGE-1 level of summary content matching if no paraphrases are found.", "labels": [], "entities": [{"text": "summary comparison", "start_pos": 23, "end_pos": 41, "type": "TASK", "confidence": 0.9135346412658691}, {"text": "ROUGE-1", "start_pos": 64, "end_pos": 71, "type": "METRIC", "confidence": 0.9965643286705017}, {"text": "summary content matching", "start_pos": 81, "end_pos": 105, "type": "TASK", "confidence": 0.6177561283111572}]}, {"text": "The first two levels employ a paraphrase table.", "labels": [], "entities": []}, {"text": "Since manually created multi-word paraphrases--phrases determined by humans to be paraphrases of one another-are not available in sufficient quantities, we automatically build a paraphrase table using methods from the Machine Translation (MT) field.", "labels": [], "entities": [{"text": "Machine Translation (MT)", "start_pos": 218, "end_pos": 242, "type": "TASK", "confidence": 0.8404705166816712}]}, {"text": "The assumption made in creating this table is that if two English phrases are translated into the same foreign phrase with high probability (shown in the alignment results from a statistically trained alignment algorithm), then the two English phrases are paraphrases of each other.", "labels": [], "entities": []}, {"text": "This paper is organized in the following way: Section 2 introduces previous work in summarization evaluation; Section 3 describes the motivation behind this work; paraphrase acquisition is discussed in Section 4; Section 5 explains in detail our summary comparison mechanism; Section 6 validates ParaEval with human summary judgments; and we conclude and discuss future work in Section 7.", "labels": [], "entities": [{"text": "summarization evaluation", "start_pos": 84, "end_pos": 108, "type": "TASK", "confidence": 0.9691020250320435}, {"text": "paraphrase acquisition", "start_pos": 163, "end_pos": 185, "type": "TASK", "confidence": 0.7944738864898682}]}], "datasetContent": [{"text": "To evaluate and validate the effectiveness of an automatic evaluation metric, it is necessary to show that automatic evaluations correlate with human assessments highly, positively, and consistently (.", "labels": [], "entities": []}, {"text": "In other words, an automatic evaluation procedure should be able to distinguish good and bad summarization systems by assigning scores with close resemblance to humans' assessments.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. Correlation with DUC 2003 MDS results.", "labels": [], "entities": [{"text": "Correlation", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9279791712760925}, {"text": "DUC 2003 MDS", "start_pos": 27, "end_pos": 39, "type": "DATASET", "confidence": 0.9301140308380127}]}]}