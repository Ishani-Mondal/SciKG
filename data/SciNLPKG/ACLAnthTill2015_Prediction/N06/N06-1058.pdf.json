{"title": [], "abstractContent": [{"text": "This paper studies the impact of paraphrases on the accuracy of automatic evaluation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.9967676401138306}]}, {"text": "Given a reference sentence and a machine-generated sentence, we seek to find a paraphrase of the reference sentence that is closer in wording to the machine output than the original reference.", "labels": [], "entities": []}, {"text": "We apply our paraphrasing method in the context of machine translation evaluation.", "labels": [], "entities": [{"text": "machine translation evaluation", "start_pos": 51, "end_pos": 81, "type": "TASK", "confidence": 0.8770199418067932}]}, {"text": "Our experiments show that the use of a paraphrased synthetic reference refines the accuracy of automatic evaluation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 83, "end_pos": 91, "type": "METRIC", "confidence": 0.9992610812187195}]}, {"text": "We also found a strong connection between the quality of automatic paraphrases as judged by humans and their contribution to automatic evaluation.", "labels": [], "entities": []}], "introductionContent": [{"text": "The use of automatic methods for evaluating machine-generated text is quickly becoming mainstream in natural language processing.", "labels": [], "entities": []}, {"text": "The most notable examples in this category include measures such as BLEU and ROUGE which drive research in the machine translation and text summarization communities.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 68, "end_pos": 72, "type": "METRIC", "confidence": 0.9980813264846802}, {"text": "ROUGE", "start_pos": 77, "end_pos": 82, "type": "METRIC", "confidence": 0.9894270896911621}, {"text": "machine translation and text summarization", "start_pos": 111, "end_pos": 153, "type": "TASK", "confidence": 0.7369498372077942}]}, {"text": "These methods assess the quality of a machine-generated output by considering its similarity to a reference text written by a human.", "labels": [], "entities": []}, {"text": "Ideally, the similarity would reflect the semantic proximity between the two.", "labels": [], "entities": [{"text": "similarity", "start_pos": 13, "end_pos": 23, "type": "METRIC", "confidence": 0.9546079635620117}]}, {"text": "In practice, this comparison breaks down to n-gram overlap between the reference and the machine output.", "labels": [], "entities": []}, {"text": "However, Israel's reply failed to completely clear the U.S. suspicions.", "labels": [], "entities": []}, {"text": "However, Israeli answer unable to fully remove the doubts.: A reference sentence and corresponding machine translation from the NIST 2004 MT evaluation.", "labels": [], "entities": [{"text": "NIST 2004 MT evaluation", "start_pos": 128, "end_pos": 151, "type": "DATASET", "confidence": 0.9110513925552368}]}, {"text": "Consider the human-written translation and the machine translation of the same Chinese sentence shown in.", "labels": [], "entities": []}, {"text": "While the two translations convey the same meaning, they share only auxiliary words.", "labels": [], "entities": []}, {"text": "Clearly, any measure based on word overlap will penalize a system for generating such a sentence.", "labels": [], "entities": []}, {"text": "The question is whether such cases are common phenomena or infrequent exceptions.", "labels": [], "entities": []}, {"text": "Empirical evidence supports the former.", "labels": [], "entities": []}, {"text": "Analyzing 10,728 reference translation pairs 1 used in the NIST 2004 machine translation evaluation, we found that only 21 (less than 0.2%) of them are identical.", "labels": [], "entities": [{"text": "NIST 2004", "start_pos": 59, "end_pos": 68, "type": "DATASET", "confidence": 0.9029420912265778}, {"text": "machine translation", "start_pos": 69, "end_pos": 88, "type": "TASK", "confidence": 0.7982638776302338}]}, {"text": "Moreover, 60% of the pairs differ in at least 11 words.", "labels": [], "entities": []}, {"text": "These statistics suggest that without accounting for paraphrases, automatic evaluation measures may never reach the accuracy of human evaluation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 116, "end_pos": 124, "type": "METRIC", "confidence": 0.999267041683197}]}, {"text": "As a solution to this problem, researchers use multiple references to refine automatic evaluation.", "labels": [], "entities": []}, {"text": "shows that expanding the number of references reduces the gap between automatic and human evaluation.", "labels": [], "entities": []}, {"text": "However, very few human annotated sets are augmented with multiple references and those that are available are relatively small in size.", "labels": [], "entities": []}, {"text": "Moreover, access to several references does not guarantee that the references will include the same words that appear in machine-generated sentences.", "labels": [], "entities": []}, {"text": "In this paper, we explore the use of paraphrasing methods for refinement of automatic evaluation techniques.", "labels": [], "entities": []}, {"text": "Given a reference sentence and a machine-generated sentence, we seek to find a paraphrase of the reference sentence that is closer in wording to the machine output than the original reference.", "labels": [], "entities": []}, {"text": "For instance, given the pair of sentences in, we automatically transform the reference sentence (1a.) into However, Israel's answer failed to completely remove the U.S. suspicions.", "labels": [], "entities": []}, {"text": "Thus, among many possible paraphrases of the reference, we are interested only in those that use words appearing in the system output.", "labels": [], "entities": []}, {"text": "Our paraphrasing algorithm is based on the substitute in context strategy.", "labels": [], "entities": []}, {"text": "First, the algorithm identifies pairs of words from the reference and the system output that could potentially form paraphrases.", "labels": [], "entities": []}, {"text": "We select these candidates using existing lexico-semantic resources such as WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 76, "end_pos": 83, "type": "DATASET", "confidence": 0.9673423171043396}]}, {"text": "Next, the algorithm tests whether the candidate paraphrase is admissible in the context of the reference sentence.", "labels": [], "entities": []}, {"text": "Since even synonyms cannot be substituted in any context (), this filtering step is necessary.", "labels": [], "entities": []}, {"text": "We predict whether a word is appropriate in anew context by analyzing its distributional properties in a large body of text.", "labels": [], "entities": []}, {"text": "Finally, paraphrases that pass the filtering stage are used to rewrite the reference sentence.", "labels": [], "entities": []}, {"text": "We apply our paraphrasing method in the context of machine translation evaluation.", "labels": [], "entities": [{"text": "machine translation evaluation", "start_pos": 51, "end_pos": 81, "type": "TASK", "confidence": 0.8770199418067932}]}, {"text": "Using this strategy, we generate anew sentence for every pair of human and machine translated sentences.", "labels": [], "entities": []}, {"text": "This synthetic reference then replaces the original human reference in automatic evaluation.", "labels": [], "entities": []}, {"text": "The key findings of our work are as follows: (1) Automatically generated paraphrases improve the accuracy of the automatic evaluation methods.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 97, "end_pos": 105, "type": "METRIC", "confidence": 0.9988784193992615}]}, {"text": "Our experiments show that evaluation based on paraphrased references gives a better approximation of human judgments than evaluation that uses original references.", "labels": [], "entities": []}, {"text": "(2) The quality of automatic paraphrases determines their contribution to automatic evaluation.", "labels": [], "entities": []}, {"text": "By analyzing several paraphrasing resources, we found that the accuracy and coverage of a paraphrasing method correlate with its utility for automatic MT evaluation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 63, "end_pos": 71, "type": "METRIC", "confidence": 0.9993239641189575}, {"text": "coverage", "start_pos": 76, "end_pos": 84, "type": "METRIC", "confidence": 0.9621071219444275}, {"text": "MT evaluation", "start_pos": 151, "end_pos": 164, "type": "TASK", "confidence": 0.9661888182163239}]}, {"text": "Our results suggest that researchers may find it useful to augment standard measures such as BLEU and ROUGE with paraphrasing information thereby taking more semantic knowledge into account.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 93, "end_pos": 97, "type": "METRIC", "confidence": 0.998781144618988}, {"text": "ROUGE", "start_pos": 102, "end_pos": 107, "type": "METRIC", "confidence": 0.9899415373802185}]}, {"text": "In the following section, we provide an overview of existing work on automatic paraphrasing.", "labels": [], "entities": []}, {"text": "We then describe our paraphrasing algorithm and explain how it can be used in an automatic evaluation setting.", "labels": [], "entities": []}, {"text": "Next, we present our experimental framework and data and conclude by presenting and discussing our results.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our primary goal is to investigate the impact of machine-generated paraphrases on the accuracy of automatic evaluation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 86, "end_pos": 94, "type": "METRIC", "confidence": 0.9975549578666687}]}, {"text": "We focus on automatic evaluation of machine translation due to the availability of human annotated data in that domain.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 36, "end_pos": 55, "type": "TASK", "confidence": 0.7501430511474609}]}, {"text": "The hypothesis is that by using a synthetic reference translation, automatic measures approximate better human evaluation.", "labels": [], "entities": []}, {"text": "In section 4.2, we test this hypothesis by comparing the performance of BLEU scores with and without synthetic references.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 72, "end_pos": 76, "type": "METRIC", "confidence": 0.9974545836448669}]}, {"text": "Our secondary goal is to study the relationship between the quality of paraphrases and their contribution to the performance of automatic machine translation evaluation.", "labels": [], "entities": [{"text": "machine translation evaluation", "start_pos": 138, "end_pos": 168, "type": "TASK", "confidence": 0.7868603666623434}]}, {"text": "In section 4.3, we present a manual evaluation of several paraphrasing methods and show a close connection between intrinsic and extrinsic assessments of these methods.", "labels": [], "entities": []}, {"text": "We begin by describing relevant background information, including the BLEU evaluation method, the test data set, and the alternative paraphrasing methods considered in our experiments.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 70, "end_pos": 74, "type": "METRIC", "confidence": 0.9943939447402954}]}, {"text": "The standard way to analyze the performance of an evaluation metric in machine translation is to compute the Pearson correlation between the automatic metric and human scores ().", "labels": [], "entities": [{"text": "machine translation", "start_pos": 71, "end_pos": 90, "type": "TASK", "confidence": 0.743545264005661}, {"text": "Pearson correlation", "start_pos": 109, "end_pos": 128, "type": "METRIC", "confidence": 0.9849838316440582}]}, {"text": "Pearson correlation estimates how linearly dependent two sets of values are.", "labels": [], "entities": []}, {"text": "The Pearson correlation values range from 1, when the scores are perfectly linearly correlated, to -1, in the case of inversely correlated scores.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 4, "end_pos": 23, "type": "METRIC", "confidence": 0.9064627289772034}]}, {"text": "To calculate the Pearson correlation, we create a document by concatenating 300 segments.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 17, "end_pos": 36, "type": "METRIC", "confidence": 0.9453442692756653}]}, {"text": "This strategy is commonly used in MT evaluation, because of BLEU's well-known problems with documents of small size ().", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 34, "end_pos": 47, "type": "TASK", "confidence": 0.9810375869274139}, {"text": "BLEU", "start_pos": 60, "end_pos": 64, "type": "METRIC", "confidence": 0.9161701798439026}]}, {"text": "For each of the ten MT system translations, Reference: The monthly magazine \"Choices\" has won the deep trust of the residents.", "labels": [], "entities": [{"text": "MT system translations", "start_pos": 20, "end_pos": 42, "type": "TASK", "confidence": 0.7747674981753031}]}, {"text": "The current Internet edition of \"Choices\" will give full play to its functions and will help consumers get quick access to market information.", "labels": [], "entities": []}, {"text": "System: The public has a lot of faith in the \"Choice\" monthly magazine and the Council is now working on a web version.", "labels": [], "entities": []}, {"text": "This will enhance the magazine's function and help consumer to acquire more up-to-date market information.", "labels": [], "entities": []}, {"text": "WordNet The monthly magazine \"Choices\" has won the deep faith of the residents.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9729793071746826}]}, {"text": "The current Internet version of \"Choices\" will give full play to its functions and will help consumers acquire quick access to market information.", "labels": [], "entities": []}, {"text": "ContextWN The monthly magazine \"Choices\" has won the deep trust of the residents.", "labels": [], "entities": []}, {"text": "The current Internet version of \"Choices\" will give full play to its functions and will help consumers acquire quick access to market information.", "labels": [], "entities": []}, {"text": "LSA The monthly magazine \"Choice\" has won the deep trust of the residents.", "labels": [], "entities": [{"text": "LSA", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8898967504501343}]}, {"text": "The current web edition of \"Choice\" will give full play to its functions and will help consumer get quick access to market information.", "labels": [], "entities": []}, {"text": "In the last section, we saw significant variations in MT evaluation performance when different paraphrasing methods were used to generate a synthetic reference.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 54, "end_pos": 67, "type": "TASK", "confidence": 0.8989849090576172}]}, {"text": "In this section, we examine the correlation between the quality of automatically generated paraphrases and their contribution to automatic evaluation.", "labels": [], "entities": []}, {"text": "We analyze how the substitution frequency and the accuracy of those substitutions contributes to a method's performance.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 50, "end_pos": 58, "type": "METRIC", "confidence": 0.9990135431289673}]}, {"text": "We compute the substitution frequency of an automatic paraphrasing method by counting the number of words it rewrites in a set of reference sentences.", "labels": [], "entities": []}, {"text": "shows the substitution frequency and the corresponding BLEU score.", "labels": [], "entities": [{"text": "substitution frequency", "start_pos": 10, "end_pos": 32, "type": "METRIC", "confidence": 0.9018904268741608}, {"text": "BLEU score", "start_pos": 55, "end_pos": 65, "type": "METRIC", "confidence": 0.9819014668464661}]}, {"text": "The substitution frequency varies greatly across different methods -LSA is by far the most prolific rewriter, while Brown produces very few substitutions.", "labels": [], "entities": []}, {"text": "As expected, the more paraphrases identified, the higher the BLEU score for the method.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 61, "end_pos": 71, "type": "METRIC", "confidence": 0.9767795205116272}]}, {"text": "However, this increase does   Accuracy measures the correctness of the proposed substitutions in the context of a reference sentence.", "labels": [], "entities": []}, {"text": "To evaluate the accuracy of different paraphrasing methods, we randomly extracted 200 paraphrasing examples from each method.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.9990569949150085}]}, {"text": "A paraphrase example consists of a reference sentence, a reference word to be paraphrased and a proposed paraphrase of that reference (that actually occurred in a corresponding system translation).", "labels": [], "entities": []}, {"text": "The judge was instructed to mark a substitution as correct only if the substitution was both semantically and grammatically correct in the context of the original reference sentence.", "labels": [], "entities": []}, {"text": "Paraphrases produced by the four methods were judged by two native English speakers.", "labels": [], "entities": []}, {"text": "The pairs were presented in random order, and the judges were not told which system produced a given pair.", "labels": [], "entities": []}, {"text": "We employ a commonly used measure, Kappa, to assess agreement between the judges.", "labels": [], "entities": [{"text": "Kappa", "start_pos": 35, "end_pos": 40, "type": "METRIC", "confidence": 0.9794788956642151}]}, {"text": "We found that negative positive filtered 40 27 non-filtered 33 100 on all the four sets the Kappa value was around 0.7, which corresponds to substantial agreement (.", "labels": [], "entities": [{"text": "agreement", "start_pos": 153, "end_pos": 162, "type": "METRIC", "confidence": 0.9697895646095276}]}, {"text": "As shows, the ranking between the accuracy of the different paraphrasing methods mirrors the ranking of the corresponding MT evaluation methods shown in.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9988749623298645}, {"text": "MT evaluation", "start_pos": 122, "end_pos": 135, "type": "TASK", "confidence": 0.9020004868507385}]}, {"text": "The paraphrasing method with the highest accuracy, ContextWN, contributes most significantly to the evaluation performance of BLEU.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.9984155893325806}, {"text": "BLEU", "start_pos": 126, "end_pos": 130, "type": "METRIC", "confidence": 0.8875530362129211}]}, {"text": "Interestingly, even methods with moderate accuracy, i.e. 63% for WordNet, have a positive influence on the BLEU metric.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.9982883334159851}, {"text": "WordNet", "start_pos": 65, "end_pos": 72, "type": "DATASET", "confidence": 0.9615374803543091}, {"text": "BLEU", "start_pos": 107, "end_pos": 111, "type": "METRIC", "confidence": 0.9967766404151917}]}, {"text": "At the same time, poor paraphrasing accuracy, such as LSA with 30%, does hurt the performance of automatic evaluation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.9951252341270447}, {"text": "LSA", "start_pos": 54, "end_pos": 57, "type": "METRIC", "confidence": 0.9914259910583496}]}, {"text": "To further understand the contribution of contextual filtering, we compare the substitutions made by WordNet and ContextWN on the same set of sentences.", "labels": [], "entities": [{"text": "contextual filtering", "start_pos": 42, "end_pos": 62, "type": "TASK", "confidence": 0.7098737210035324}, {"text": "WordNet", "start_pos": 101, "end_pos": 108, "type": "DATASET", "confidence": 0.975846529006958}]}, {"text": "Among the 200 paraphrases proposed by WordNet, 73 (36.5%) were identified as incorrect by human judges.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 38, "end_pos": 45, "type": "DATASET", "confidence": 0.9404124617576599}]}, {"text": "As the confusion matrix in shows, 40 (54.5%) were eliminated during the filtering step.", "labels": [], "entities": []}, {"text": "At the same time, the filtering erroneously eliminates 27 positive examples (21%).", "labels": [], "entities": []}, {"text": "Even at this level of false negatives, the filtering has an overall positive effect.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: Pearson adequacy correlation scores for  rewriting using one and two references, averaged  over ten runs.", "labels": [], "entities": [{"text": "Pearson adequacy correlation", "start_pos": 10, "end_pos": 38, "type": "METRIC", "confidence": 0.8324412107467651}]}, {"text": " Table 6: Scores and the number of substitutions  made for all 1788 segments, averaged over the dif- ferent MT system translations", "labels": [], "entities": [{"text": "MT", "start_pos": 108, "end_pos": 110, "type": "TASK", "confidence": 0.8774306178092957}]}, {"text": " Table 7: Accuracy scores by two human judges as  well as the Kappa coefficient of agreement.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9992064833641052}, {"text": "Kappa coefficient of agreement", "start_pos": 62, "end_pos": 92, "type": "METRIC", "confidence": 0.869971752166748}]}]}