{"title": [{"text": "Relabeling Syntax Trees to Improve Syntax-Based Machine Translation Quality", "labels": [], "entities": [{"text": "Improve Syntax-Based Machine Translation", "start_pos": 27, "end_pos": 67, "type": "TASK", "confidence": 0.7748731374740601}]}], "abstractContent": [{"text": "We identify problems with the Penn Tree-bank that render it imperfect for syntax-based machine translation and propose methods of relabeling the syntax trees to improve translation quality.", "labels": [], "entities": [{"text": "Penn Tree-bank", "start_pos": 30, "end_pos": 44, "type": "DATASET", "confidence": 0.992338240146637}, {"text": "syntax-based machine translation", "start_pos": 74, "end_pos": 106, "type": "TASK", "confidence": 0.6535059014956156}]}, {"text": "We develop a system incorporating a handful of relabel-ing strategies that yields a statistically significant improvement of 2.3 BLEU points over a baseline syntax-based system.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 129, "end_pos": 133, "type": "METRIC", "confidence": 0.9989321827888489}]}], "introductionContent": [{"text": "Recent work in statistical machine translation (MT) has sought to overcome the limitations of phrasebased models () by making use of syntactic information.", "labels": [], "entities": [{"text": "statistical machine translation (MT)", "start_pos": 15, "end_pos": 51, "type": "TASK", "confidence": 0.8164892047643661}]}, {"text": "Syntax-based MT offers the potential advantages of enforcing syntaxmotivated constraints in translation and capturing long-distance/non-contiguous dependencies.", "labels": [], "entities": [{"text": "MT", "start_pos": 13, "end_pos": 15, "type": "TASK", "confidence": 0.8055253624916077}]}, {"text": "Some approaches have used syntax at the core (Wu, 1997;) while others have integrated syntax into existing phrase-based frameworks (.", "labels": [], "entities": []}, {"text": "In this work, we employ a syntax-based model that applies a series of tree/string (xRS) rules () to a source language string to produce a target language phrase structure tree.  is translated as the NP-C the gunman by rule \ud97b\udf59.", "labels": [], "entities": []}, {"text": "Finally, rule 4 \ud97b\udf59 combines the sequence of NP-C VP . into an S, denoting a complete tree.", "labels": [], "entities": []}, {"text": "The yield of this tree gives the target translation: the gunman was killed by police . The Penn English Treebank (PTB)) is our source of syntactic information, largely due to the availability of reliable parsers.", "labels": [], "entities": [{"text": "Penn English Treebank (PTB))", "start_pos": 91, "end_pos": 119, "type": "DATASET", "confidence": 0.9680885374546051}]}, {"text": "It is not clear, however, whether this resource is suitable, as is, for the task of MT.", "labels": [], "entities": [{"text": "MT", "start_pos": 84, "end_pos": 86, "type": "TASK", "confidence": 0.9917803406715393}]}, {"text": "In this paper, we argue that the overly-general tagset of the PTB is problematic for MT because it fails to capture important grammatical distinctions that are critical in translation.", "labels": [], "entities": [{"text": "MT", "start_pos": 85, "end_pos": 87, "type": "TASK", "confidence": 0.9895069003105164}]}, {"text": "As a solution, we propose methods of relabeling the syntax trees that effectively improve translation quality.", "labels": [], "entities": []}, {"text": "The output translation has two salient errors: determiner/noun number disagreement (*this Turkish positions) and auxiliary/verb tense disagreement (*has demonstrate).", "labels": [], "entities": []}, {"text": "The first problem arises because the DT tag, which does not distinguish between singular and plural determiners, allows singular this to be used with plural NNS positions.", "labels": [], "entities": []}, {"text": "In the second problem, the VP-C tag fails to communicate that it is headed by the base verb (VB) demonstrate, which should prevent it from being used with the auxiliary VBZ has.", "labels": [], "entities": []}, {"text": "Information-poor tags like DT and VP-C can be relabeled to encourage more fluent translations, which is the thrust of this paper.", "labels": [], "entities": []}, {"text": "Section 2 describes our data and experimental procedure.", "labels": [], "entities": []}, {"text": "Section 3 explores different relabeling approaches and their impact on translation quality.", "labels": [], "entities": [{"text": "translation quality", "start_pos": 71, "end_pos": 90, "type": "TASK", "confidence": 0.9097840189933777}]}, {"text": "Section 4 reports a substantial improvement in BLEU achieved by combining the most effective relabeling methods.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 47, "end_pos": 51, "type": "METRIC", "confidence": 0.9970263838768005}]}], "datasetContent": [{"text": "Our training data consists of 164M+167M words of parallel Chinese/English text.", "labels": [], "entities": []}, {"text": "The English half was parsed with a reimplementation of Collins' Model 2) and the two halves were wordaligned using GIZA++).", "labels": [], "entities": [{"text": "Collins' Model 2", "start_pos": 55, "end_pos": 71, "type": "DATASET", "confidence": 0.9666173656781515}]}, {"text": "These three components -Chinese strings, English parse trees, and their word alignments -were inputs to our experimental procedure, which involved five steps: (1) tree relabeling, (2) rule extraction, (3) decoding, (4) n-best reranking, (5) evaluation.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 72, "end_pos": 87, "type": "TASK", "confidence": 0.7334882020950317}, {"text": "rule extraction", "start_pos": 184, "end_pos": 199, "type": "TASK", "confidence": 0.7419004142284393}]}, {"text": "This paper focuses on step 1, in which the original English parse trees are transformed by one or more relabeling strategies.", "labels": [], "entities": []}, {"text": "Step 2 involves extracting minimal xRS rules () from the set of string/tree/alignments triplets.", "labels": [], "entities": []}, {"text": "These rules are then used in a CKY-type parser-decoder to translate the 878-sentence 2002 NIST MT evaluation test set (step 3).", "labels": [], "entities": [{"text": "NIST MT evaluation test set", "start_pos": 90, "end_pos": 117, "type": "DATASET", "confidence": 0.7978967666625977}]}, {"text": "In step 4, the output 2,500-sentence nbest list is reranked using an n-gram language model trained on 800M words of English news text.", "labels": [], "entities": []}, {"text": "In the final step, we score our translations with 4-gram BLEU ().", "labels": [], "entities": [{"text": "translations", "start_pos": 32, "end_pos": 44, "type": "TASK", "confidence": 0.9481496214866638}, {"text": "BLEU", "start_pos": 57, "end_pos": 61, "type": "METRIC", "confidence": 0.9975340366363525}]}, {"text": "Separately for each relabeling method, we ran these five steps and compared the resulting BLEU score with that of a baseline system with no relabeling.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 90, "end_pos": 100, "type": "METRIC", "confidence": 0.9767242670059204}]}, {"text": "To determine if a BLEU score increase or decrease is meaningful, we calculate statistical significance at 95% using paired bootstrap resampling) on 1,000 samples.", "labels": [], "entities": [{"text": "BLEU score increase", "start_pos": 18, "end_pos": 37, "type": "METRIC", "confidence": 0.977255622545878}]}, {"text": "shows the results from each relabeling experiment.", "labels": [], "entities": []}, {"text": "The second column indicates the change in the number of unique rules from the baseline number of 16.7M rules.", "labels": [], "entities": []}, {"text": "The third column gives the BLEU score along with an indication whether it is a statistically significant increase (v), a statistically significant decrease (w), or neither (?) over the baseline BLEU score.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 27, "end_pos": 37, "type": "METRIC", "confidence": 0.9860683679580688}, {"text": "statistically significant increase (v)", "start_pos": 79, "end_pos": 117, "type": "METRIC", "confidence": 0.7999981741110483}, {"text": "statistically significant decrease (w)", "start_pos": 121, "end_pos": 159, "type": "METRIC", "confidence": 0.7604643106460571}, {"text": "BLEU", "start_pos": 194, "end_pos": 198, "type": "METRIC", "confidence": 0.9904772043228149}]}, {"text": "To maximize the benefit of relabeling, we incorporated five of the most promising relabeling strategies into one additive system: LEX_%, LEX_DT variant Figure 11: Relabelings in the additive system and their individual/cumulative effects over the baseline.", "labels": [], "entities": []}, {"text": "2, TAG_VP, LEX_PREP variant 2, and SISTERHOOD variant 1.", "labels": [], "entities": [{"text": "TAG_VP", "start_pos": 3, "end_pos": 9, "type": "METRIC", "confidence": 0.728879710038503}]}, {"text": "These relabelings contributed to a 2.3 absolute (11.6% relative) BLEU point increase over the baseline, with a score of 22.38.", "labels": [], "entities": [{"text": "relabelings", "start_pos": 6, "end_pos": 17, "type": "METRIC", "confidence": 0.9644725322723389}, {"text": "BLEU point increase", "start_pos": 65, "end_pos": 84, "type": "METRIC", "confidence": 0.9762631058692932}]}, {"text": "lists these relabelings in the order they were added.", "labels": [], "entities": []}], "tableCaptions": []}