{"title": [{"text": "Reducing Weight Undertraining in Structured Discriminative Learning", "labels": [], "entities": []}], "abstractContent": [{"text": "Discriminative probabilistic models are very popular in NLP because of the latitude they afford in designing features.", "labels": [], "entities": []}, {"text": "But training involves complex trade-offs among weights, which can be dangerous: a few highly-indicative features can swamp the contribution of many individually weaker features, causing their weights to be undertrained.", "labels": [], "entities": []}, {"text": "Such a model is less robust, for the highly-indicative features maybe noisy or missing in the test data.", "labels": [], "entities": []}, {"text": "To ameliorate this weight undertraining, we introduce several new feature bagging methods, in which separate models are trained on subsets of the original features, and combined using a mixture model or a product of experts.", "labels": [], "entities": []}, {"text": "These methods include the logarithmic opinion pools used by Smith et al.", "labels": [], "entities": []}, {"text": "We evaluate feature bagging on linear-chain conditional random fields for two natural-language tasks.", "labels": [], "entities": [{"text": "feature bagging", "start_pos": 12, "end_pos": 27, "type": "TASK", "confidence": 0.7182437628507614}]}, {"text": "On both tasks, the feature-bagged CRF performs better than simply training a single CRF on all the features.", "labels": [], "entities": []}], "introductionContent": [{"text": "Discriminative methods for training probabilistic models have enjoyed wide popularity in natural language processing, such as in part-of-speech tagging (, chunking, namedentity recognition (, and most recently parsing (.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 129, "end_pos": 151, "type": "TASK", "confidence": 0.6655306220054626}, {"text": "namedentity recognition", "start_pos": 165, "end_pos": 188, "type": "TASK", "confidence": 0.6610089391469955}]}, {"text": "A discriminative probabilistic model is trained to maximize the conditional probability p(y|x) of output labels y given input variables x, as opposed to modeling the joint probability p(y, x), as in generative models such as the Naive Bayes classifier and hidden Markov models.", "labels": [], "entities": []}, {"text": "The popularity of discriminative models stems from the great flexibility they allow in defining features: because the distribution over input features p(x) is not modeled, it can contain rich, highly overlapping features without making the model intractable for training and inference.", "labels": [], "entities": []}, {"text": "In NLP, for example, useful features include word bigrams and trigrams, prefixes and suffixes, membership in domain-specific lexicons, and information from semantic databases such as WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 183, "end_pos": 190, "type": "DATASET", "confidence": 0.9359933137893677}]}, {"text": "It is not uncommon to have hundreds of thousands or even millions of features.", "labels": [], "entities": []}, {"text": "But not all features, even ones that are carefully engineered, improve performance.", "labels": [], "entities": []}, {"text": "Adding more features to a model can hurt its accuracy on unseen testing data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.9991531372070312}]}, {"text": "One well-known reason for this is overfitting: a model with more features has more capacity to fit chance regularities in the training data.", "labels": [], "entities": []}, {"text": "In this paper, however, we focus on another, more subtle effect: adding new features can cause existing ones to be underfit.", "labels": [], "entities": []}, {"text": "Training of discriminative models, such as regularized logistic regression, involves complex trade-offs among weights.", "labels": [], "entities": []}, {"text": "A few highlyindicative features can swamp the contribution of many individually weaker features, even if the weaker features, taken together, are just as indicative of the output.", "labels": [], "entities": []}, {"text": "Such a model is less robust, for the few strong features maybe noisy or missing in the test data.", "labels": [], "entities": []}, {"text": "This effect was memorably observed by Dean Pomerleau when training neural networks to drive vehicles autonomously.", "labels": [], "entities": []}, {"text": "Pomerleau reports one example when the system was learning to drive on a dirt road: The network had no problem learning and then driving autonomously in one direction, but when driving the other way, the network was erratic, swerving from one side of the road to the other.", "labels": [], "entities": []}, {"text": "It turned out that the network was basing most of its predictions on an easilyidentifiable ditch, which was always on the right in the training set, but was on the left when the vehicle turned around.", "labels": [], "entities": []}, {"text": "The network had features to detect the sides of the road, and these features were active at training and test time, although weakly, because the dirt road was difficult to detect.", "labels": [], "entities": []}, {"text": "But the ditch was so highly indicative that the network did not learn the dependence between the road edge and the desired steering direction.", "labels": [], "entities": []}, {"text": "A natural way of avoiding undertraining is to train separate models for groups of competing features-in the driving example, one model with the ditch features, and one with the side-of-the-road features-and then average them into a single model.", "labels": [], "entities": []}, {"text": "This is same idea behind logarithmic opinion pools, used by to reduce overfitting in CRFs.", "labels": [], "entities": []}, {"text": "In this paper, we tailor our ensemble to reduce undertraining rather than overfitting, and we introduce several new combination methods, based on whether the mixture is taken additively or geometrically, and on a per-sequence or pertransition basis.", "labels": [], "entities": []}, {"text": "We call this general class of methods feature bagging, by analogy to the well-known bagging algorithm for ensemble learning.", "labels": [], "entities": []}, {"text": "We test these methods on conditional random fields (CRFs) (), which are discriminatively-trained undirected models.", "labels": [], "entities": []}, {"text": "On two natural-language tasks, we show that feature bagging performs significantly better than training a single CRF with all available features.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Comparison of various bagging methods on the  CoNLL 2003 Named Entity Task.", "labels": [], "entities": [{"text": "CoNLL 2003 Named Entity Task", "start_pos": 56, "end_pos": 84, "type": "DATASET", "confidence": 0.913429319858551}]}, {"text": " Table 2: Results for the CoNLL 2003 Named Entity  Task. The bagged CRF performs significantly better than  a single CRF with all available features (McNemar's test;  p < 0.01).", "labels": [], "entities": [{"text": "CoNLL 2003 Named Entity  Task", "start_pos": 26, "end_pos": 55, "type": "TASK", "confidence": 0.6821015000343322}]}, {"text": " Table 3: Results for the CoNLL 2000 Chunking Task.  The bagged CRF performs significantly better than a sin- gle CRF (McNemar's test; p < 0.01), and equals the re- sults of (Ando and Zhang, 2005), who use a large amount  of unlabeled data.", "labels": [], "entities": [{"text": "CoNLL 2000 Chunking Task", "start_pos": 26, "end_pos": 50, "type": "DATASET", "confidence": 0.743872195482254}]}]}