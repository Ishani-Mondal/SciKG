{"title": [{"text": "Creating a Test Collection for Citation-based IR Experiments", "labels": [], "entities": [{"text": "IR Experiments", "start_pos": 46, "end_pos": 60, "type": "TASK", "confidence": 0.9068407118320465}]}], "abstractContent": [{"text": "We present an approach to building a test collection of research papers.", "labels": [], "entities": []}, {"text": "The approach is based on the Cranfield 2 tests but uses as its vehicle a current conference; research questions and relevance judgements of all cited papers are elicited from conference authors.", "labels": [], "entities": [{"text": "Cranfield 2 tests", "start_pos": 29, "end_pos": 46, "type": "DATASET", "confidence": 0.8984302679697672}]}, {"text": "The resultant test collection is different from TREC's in that it comprises scientific articles rather than newspaper text and, thus, allows for IR experiments that include citation information.", "labels": [], "entities": [{"text": "IR", "start_pos": 145, "end_pos": 147, "type": "TASK", "confidence": 0.977493166923523}]}, {"text": "The test collection currently consists of 170 queries with relevance judgements ; the document collection is the ACL Anthology.", "labels": [], "entities": [{"text": "ACL Anthology", "start_pos": 113, "end_pos": 126, "type": "DATASET", "confidence": 0.9728263914585114}]}, {"text": "We describe properties of our queries and relevance judgements, and demonstrate the use of the test collection in an experimental setup.", "labels": [], "entities": []}, {"text": "One potentially problematic property of our collection is that queries have a low number of relevant documents; we discuss ways of alleviating this.", "labels": [], "entities": []}], "introductionContent": [{"text": "We present a methodology for creating a test collection of scientific papers that is based on the Cranfield 2 methodology but uses a current conference as the main vehicle for eliciting relevance judgements from users, i.e., the authors.", "labels": [], "entities": []}, {"text": "Building a test collection is along and expensive process but was necessary as no ready-made test collection existed on which the kinds of experiments with citation information that we envisage could be run.", "labels": [], "entities": []}, {"text": "We aim to improve term-based IR on scientific articles with citation information, by using index terms from the citing article to additionally describe the cited document.", "labels": [], "entities": [{"text": "IR", "start_pos": 29, "end_pos": 31, "type": "TASK", "confidence": 0.8727080225944519}]}, {"text": "Exactly how to do this is the research question that our test collection should help to address.", "labels": [], "entities": []}, {"text": "This paper is structured as follows: Section 2 motivates our proposed experiments and, thereby, our test collection.", "labels": [], "entities": []}, {"text": "Section 3 discusses the how test collections are built and, in particular, our own.", "labels": [], "entities": []}, {"text": "Section 4 briefly describes the practicalities of compiling the document collection and the processing we perform to prepare the documents for our experiments.", "labels": [], "entities": []}, {"text": "In Section 5, we show that our test collection can be used with standard IR tools.", "labels": [], "entities": [{"text": "IR", "start_pos": 73, "end_pos": 75, "type": "TASK", "confidence": 0.9448527693748474}]}, {"text": "Finally, Section 6 discusses the problem of the low number of relevant documents judged so far and two ways of alleviating this problem.", "labels": [], "entities": []}], "datasetContent": [{"text": "We expect that our test collection, built for our citation experiments, will be of wider value and we intend to make it publicly available.", "labels": [], "entities": []}, {"text": "As a sanity check on our data so far, we carried out some preliminary experimentation, using standard IR tools: the Lemur Toolkit 8 , specifically Indri (), its integrated language-model based search engine, and the TREC evaluation software, trec eval 9 .  We indexed around 4200 Anthology documents.", "labels": [], "entities": [{"text": "Anthology documents", "start_pos": 280, "end_pos": 299, "type": "DATASET", "confidence": 0.9312084317207336}]}, {"text": "This is the total number of documents that have, at the time of writing, been processed by our pipeline (24 years of CL journal, 25 years of ACL proceedings, 14 years of assorted workshops), plus another \u223c90 documents for which we have relevance judgements that are not currently available through the Anthology website but should be incorporated into the archive in the future.", "labels": [], "entities": []}, {"text": "The indexed documents do not yet contain annotation of the reference list or citations in text.", "labels": [], "entities": []}, {"text": "19 of our 170 queries have no relevant references in the indexed documents and were not included in these experiments.", "labels": [], "entities": []}, {"text": "Thus, shows the distribution of queries over number of relevant Anthology references, fora total of 151 queries.", "labels": [], "entities": []}, {"text": "Our Indri index was built using default parameters with no optional processing, e.g., stopping or stemming, resulting in a total of 20117410 terms, 218977 unique terms and 2263 'frequent' 10 terms.", "labels": [], "entities": []}, {"text": "We then prepared an Indri-style query file from the conference research questions.", "labels": [], "entities": [{"text": "Indri-style query file from the conference research questions", "start_pos": 20, "end_pos": 81, "type": "DATASET", "confidence": 0.8259194269776344}]}, {"text": "The Indri query language is designed to handle highly complex queries but, for our very basic purposes, we created simple bag-of-words queries by stripping all punctuation from the natural language questions and using Indri's #combine operator overall the terms.", "labels": [], "entities": []}, {"text": "This means Indri ranks documents in accordance with query likelihood.", "labels": [], "entities": []}, {"text": "Again, no stopping or stemming was applied.", "labels": [], "entities": [{"text": "stopping or stemming", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.5436917940775553}]}, {"text": "Next, the query file was run against the Anthology index using IndriRunQuery with default parameters and, thus, retrieving 1000 documents for each query.", "labels": [], "entities": [{"text": "Anthology index", "start_pos": 41, "end_pos": 56, "type": "DATASET", "confidence": 0.93144890666008}]}, {"text": "Finally, for evaluation, we converted the Indri's ranked document lists to TREC-style top results file and the conference relevance judgements compiled into a TREC-style qrels file, including only judgements corresponding to references within the indexed documents.", "labels": [], "entities": [{"text": "Indri's ranked document lists", "start_pos": 42, "end_pos": 71, "type": "DATASET", "confidence": 0.9593172311782837}, {"text": "TREC-style top results file", "start_pos": 75, "end_pos": 102, "type": "DATASET", "confidence": 0.8071007579565048}]}, {"text": "These files were then input to trec eval, to calculate precision and recall metrics.", "labels": [], "entities": [{"text": "precision", "start_pos": 55, "end_pos": 64, "type": "METRIC", "confidence": 0.9985848665237427}, {"text": "recall", "start_pos": 69, "end_pos": 75, "type": "METRIC", "confidence": 0.9956282377243042}]}, {"text": "Terms that occur in over 1000 documents.", "labels": [], "entities": [{"text": "Terms", "start_pos": 0, "end_pos": 5, "type": "TASK", "confidence": 0.8609528541564941}]}], "tableCaptions": []}