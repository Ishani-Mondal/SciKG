{"title": [], "abstractContent": [{"text": "There are two main methodologies for constructing the knowledge base of a natural language analyser: the linguistic and the data-driven.", "labels": [], "entities": []}, {"text": "Recent state-of-the-art part-of-speech taggers are based on the data-driven approach.", "labels": [], "entities": [{"text": "part-of-speech taggers", "start_pos": 24, "end_pos": 46, "type": "TASK", "confidence": 0.7459566593170166}]}, {"text": "Because of the known feasibility of the linguistic rule-based approach at related levels of description, the success of the data-driven approach in part-of-speech analysis may appear surprising.", "labels": [], "entities": [{"text": "part-of-speech analysis", "start_pos": 148, "end_pos": 171, "type": "TASK", "confidence": 0.7089399993419647}]}, {"text": "In this paper, a case is made for the syntactic nature of part-of-speech tagging.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 58, "end_pos": 80, "type": "TASK", "confidence": 0.7575482130050659}]}, {"text": "A new tagger of English that uses only linguistic dis-tributional rules is outlined and empirically evaluated.", "labels": [], "entities": []}, {"text": "Tested against a benchmark corpus of 38,000 words of previously unseen text, this syntax-based system reaches an accuracy of above 99%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 113, "end_pos": 121, "type": "METRIC", "confidence": 0.9993952512741089}]}, {"text": "Compared to the 95-97% accuracy of its best competitors, this result suggests the feasibility of the linguistic approach also in part-of-speech analysis.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.99779212474823}]}], "introductionContent": [{"text": "Part-of-speech analysis usually consists of (i) introduction of ambiguity (lexical analysis) and (ii) disambiguation (elimination of illegitimate alternatives).", "labels": [], "entities": [{"text": "Part-of-speech analysis", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.7905820310115814}]}, {"text": "While introducing ambiguity is regarded as relatively straightforward, disambiguation is known to be a difficult and controversial problem.", "labels": [], "entities": [{"text": "disambiguation", "start_pos": 71, "end_pos": 85, "type": "TASK", "confidence": 0.9614028334617615}]}, {"text": "There are two main methodologies: the linguistic and the data-driven.", "labels": [], "entities": []}, {"text": "\u2022 In the linguistic approach, the generalisations are based on the linguist's (potentially corpus-based) abstractions about the paradigms and syntagms of the language.", "labels": [], "entities": []}, {"text": "Distributional generalisations are manually coded as a grammar, a system of constraint rules used for discarding contextually illegitimate analyses.", "labels": [], "entities": [{"text": "Distributional generalisations", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.7821543514728546}]}, {"text": "The linguistic approach is labour-intensive: skill and effort is needed for writing an exhaustive grammar.", "labels": [], "entities": []}, {"text": "\u2022 In the data-driven approach, frequency-based information is automatically derived from corpora.", "labels": [], "entities": []}, {"text": "The learning corpus can consist of plain text, but the best results seem achievable with annotated corpora.", "labels": [], "entities": []}, {"text": "This corpus-based information typically concerns sequences of 1-3 tags or words (with some well-known exceptions, e.g.).", "labels": [], "entities": []}, {"text": "Corpus-based information can be represented e.g. as neural networks (, local rules (Brill 1992), or collocational matrices (.", "labels": [], "entities": []}, {"text": "In the data-driven approach, no human effort is needed for rulewriting.", "labels": [], "entities": []}, {"text": "However, considerable effort maybe needed for determining a workable tag set (cf. and annotating the training corpus.", "labels": [], "entities": []}, {"text": "At the first flush, the linguistic approach may seem an obvious choice.", "labels": [], "entities": []}, {"text": "A part-of-speech tagger's task is often illustrated with a noun-verb ambiguous word directly preceded by an unambiguous determiner (e.g. This ambiguity can reliably be resolved with a simple and obvious grammar rule that disallows verbs after determiners.", "labels": [], "entities": [{"text": "part-of-speech tagger's task", "start_pos": 2, "end_pos": 30, "type": "TASK", "confidence": 0.8173264116048813}]}, {"text": "Indeed, few contest the fact that reliable linguistic rules can be written for resolving some partof-speech ambiguities.", "labels": [], "entities": []}, {"text": "The main problem with this approach seems to be that resolving part-ofspeech ambiguities on a large scale, without introducing a considerable error margin, is very difficult at best.", "labels": [], "entities": []}, {"text": "At least, no rule-based system with a convincing accuracy has been reported so far.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.9977123737335205}]}, {"text": "1 As a rule, data-driven systems rely on statistical generalisations about short sequences of words or tags.", "labels": [], "entities": []}, {"text": "Though these systems do not usually employ information about long-distance phenom1There is one potential exception: the rule-based morphological disambiguator used in the English Constraint Grammar Parser ENGCG.", "labels": [], "entities": [{"text": "English Constraint Grammar Parser ENGCG", "start_pos": 171, "end_pos": 210, "type": "DATASET", "confidence": 0.6666086852550507}]}, {"text": "Its recall is very high (99.7% of all words receive the correct morphological analysis), but this system leaves 3-7% of all words ambiguous, trading precision for recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 4, "end_pos": 10, "type": "METRIC", "confidence": 0.9993191957473755}, {"text": "precision", "start_pos": 149, "end_pos": 158, "type": "METRIC", "confidence": 0.9991381168365479}, {"text": "recall", "start_pos": 163, "end_pos": 169, "type": "METRIC", "confidence": 0.993637204170227}]}, {"text": "ena or the linguist's abstraction capabilities (e.g. knowledge about what is relevant in the context), they tend to reach a 95-97% accuracy in the analysis of several languages, in particular English.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 131, "end_pos": 139, "type": "METRIC", "confidence": 0.9982512593269348}]}, {"text": "Interestingly, no significant improvement beyond the 97% \"barrier\" by means of purely data-driven systems has been reported so far.", "labels": [], "entities": []}, {"text": "In terms of the accuracy of known systems, the data-driven approach seems then to provide the best model of part-of-speech distribution.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.9979900121688843}]}, {"text": "This should appear a little curious because very competitive results have been achieved using the linguistic approach at related levels of description.", "labels": [], "entities": []}, {"text": "With respect to computational morphology, witness for instance the success of the Two-Level paradigm introduced by: extensive morphological descriptions have been made of more than 15 typologically different languages.", "labels": [], "entities": []}, {"text": "With regard t.o computational syntax, see for instance (.", "labels": [], "entities": []}, {"text": "The present success of the statistical approach in part-of-speech analysis seems then to form an exception to the general feasibility of the rule-based linguistic approach.", "labels": [], "entities": [{"text": "part-of-speech analysis", "start_pos": 51, "end_pos": 74, "type": "TASK", "confidence": 0.7135189175605774}]}, {"text": "Is the level of parts of speech somehow different, perhaps less rulegoverned, than related levels?", "labels": [], "entities": []}, {"text": "2 We do not need to assume this idiosyncratic status entirely.", "labels": [], "entities": []}, {"text": "The rest of this paper argues that also parts of speech can be viewed as a rule-governed phenomenon, possible to model using the linguistic approach.", "labels": [], "entities": []}, {"text": "However, it will also be argued that though the distribution of parts of speech canto some extent be described with rules specific to this level of representation, a more natural account could be given using rules overtly about the form and function of essentially syntactic categories.", "labels": [], "entities": []}, {"text": "A syntactic grammar appears to predict the distribution of parts of speech as a \"side effect\".", "labels": [], "entities": []}, {"text": "In this sense parts of speech seem to differ from morphology and syntax: their status as an independent level of linguistic description appears doubtful.", "labels": [], "entities": []}, {"text": "Before proceeding further with the main argument, consider three very recent hybrids -systems that employ linguistic rules for resolving some of the ambiguities before using automatically generated corpus-based information: collocation matrices, Hidden Markov Models, or syntactic patterns (Tapanainen and 2For related discussion, cf. and..", "labels": [], "entities": []}, {"text": "What is interesting in these hybrids is that they, unlike purely data-driven taggers, seem capable of exceeding the 97% barrier: all three report an accuracy of about 98.5%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 149, "end_pos": 157, "type": "METRIC", "confidence": 0.9994611144065857}]}, {"text": "3 The success of these hybrids could be regarded as evidence for the syntactic aspects of parts of speech.", "labels": [], "entities": []}, {"text": "However, the above hybrids still contain a datadriven component, i.e. it remains an open question whether a tagger entirely based on the linguistic approach can compare with a data-driven system.", "labels": [], "entities": []}, {"text": "Next, anew system with the following properties is outlined and evaluated: \u2022 The tagger uses only linguistic distributional rules.", "labels": [], "entities": []}, {"text": "\u2022 Tested agMnst a 38,000-word corpus of previously unseen text, the tagger reaches a better accuracy than previous systems (over 99%).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 92, "end_pos": 100, "type": "METRIC", "confidence": 0.9989858269691467}]}, {"text": "\u2022 At the level of linguistic abstraction, the grammar rules are essentially syntactic.", "labels": [], "entities": []}, {"text": "Ideally, part-of-speech disambiguation should fallout as a \"side effect\" of syntactic analysis.", "labels": [], "entities": [{"text": "part-of-speech disambiguation", "start_pos": 9, "end_pos": 38, "type": "TASK", "confidence": 0.7142368108034134}, {"text": "syntactic analysis", "start_pos": 76, "end_pos": 94, "type": "TASK", "confidence": 0.771021693944931}]}, {"text": "Section 2 outlines a rule-based system consisting of the ENGCG tagger followed by a finitestate syntactic parser) that resolves remaining part-of-speech ambiguities as aside effect.", "labels": [], "entities": []}, {"text": "In Section 3, this rule-based system is tested against a 38,000-word corpus of previously unseen text.", "labels": [], "entities": []}, {"text": "Currently tagger evaluation is only becoming standardised; the evaluation method is accordingly reported in detail.", "labels": [], "entities": [{"text": "tagger evaluation", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.9649501144886017}]}], "datasetContent": [{"text": "part-of-speech disambiguation The system was tested against a 38,202-word test corpus consisting of previously unseen journalistic, scientific and manual texts.", "labels": [], "entities": [{"text": "part-of-speech disambiguation", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.7078536152839661}]}, {"text": "The finite-state parser, the last module in the system, can in principle be \"forced\" to produce an unambiguous analysis for each input sentence, even for ungrammatical ones.", "labels": [], "entities": []}, {"text": "In practice, the present implementation sometimes fails to give an analysis to heavily ambiguous inputs, regardless of their grammaticality.", "labels": [], "entities": []}, {"text": "5 Therefore two kinds of output were accepted for the evaluation: (i) the unambiguous analyses actually proposed by the finite-state parser, and (ii) the ENGCG analysis of those sentences for which the finite-state parser gave no analyses.", "labels": [], "entities": []}, {"text": "From this nearly unambiguous combined output, the success of the hybrid was measured, by automatically comparing it with a benchmark version of the test corpus at the level. of morphological (including part-of-speech) analysis (i.e. the syntax tags were ignored).", "labels": [], "entities": []}], "tableCaptions": []}