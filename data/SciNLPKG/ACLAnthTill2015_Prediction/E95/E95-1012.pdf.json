{"title": [], "abstractContent": [{"text": "In this paper we provide a probabilis-tic interpretation for typed feature structures very similar to those used by Pol-lard and Sag.", "labels": [], "entities": []}, {"text": "We begin with aversion of the interpretation which lacks a treatment of re-entrant feature structures , then provide an extended interpretation which allows them.", "labels": [], "entities": []}, {"text": "We sketch algorithms allowing the numerical parameters of our probabilistic interpretations of HPSG to be estimated from corpora.", "labels": [], "entities": [{"text": "HPSG", "start_pos": 95, "end_pos": 99, "type": "DATASET", "confidence": 0.9196387529373169}]}], "introductionContent": [{"text": "The purpose of our paper is to develop a principled technique for attaching a probabilistic interpretation to feature structures.", "labels": [], "entities": []}, {"text": "Our techniques apply to the feature structures described by.", "labels": [], "entities": []}, {"text": "Since these structures are the ones which are used in by) their relevance to computational grammars is apparent.", "labels": [], "entities": []}, {"text": "On the basis of the usefulness of probabilistic context-free grammars, it is plausible to assume that that the extension of probabilistic techniques to such structures will allow the application of known and new techniques of parse ranking and grammar induction to more interesting grammars than has hitherto been the case.", "labels": [], "entities": [{"text": "parse ranking", "start_pos": 226, "end_pos": 239, "type": "TASK", "confidence": 0.8817239105701447}, {"text": "grammar induction", "start_pos": 244, "end_pos": 261, "type": "TASK", "confidence": 0.732392281293869}]}, {"text": "The paper is structured as follows.", "labels": [], "entities": []}, {"text": "We start by reviewing the training and use of probabilistic context-free grammars (PCFGs).", "labels": [], "entities": []}, {"text": "We then de: velop a technique to allow analogous probabilistic annotations on type hierarchies.", "labels": [], "entities": []}, {"text": "This gives us a clear account of the relationship between a large class of feature structures and their probabilities, but does not treat re-entrancy.", "labels": [], "entities": []}, {"text": "We conclude by sketching a technique which does treat such structures.", "labels": [], "entities": []}, {"text": "While we know of previous work which associates scores with feature structures are not aware of any previous treatment which makes explicit the link to classical probability theory.", "labels": [], "entities": []}, {"text": "We take a slightly unconventional perspective on feature structures, because it is easier to cast our theory within the more general framework of incremental description refinement than to exploit the usual metaphors of constraint-based grammar.", "labels": [], "entities": []}, {"text": "In fact we can afford to remain entirely agnostic about the means by which the HPSG grammar associates signs with linguistic strings, because all that we need in order to train our stochastic procedures is a corpus of signs which are known to be valid descriptions of strings.", "labels": [], "entities": []}], "datasetContent": [{"text": "Even a crude account of re-entrancy is better than completely ignoring the issue, and the one proposed gets the right result for cases of double counting such as those discussed above, but it should be obvious that there is room for improvement in the treatment which we provide.", "labels": [], "entities": [{"text": "double counting", "start_pos": 138, "end_pos": 153, "type": "TASK", "confidence": 0.7022010535001755}]}, {"text": "Intuitively what is required is a parametrisable means of distributing probability mass among the distinct equivalence relations which extend the current structure.", "labels": [], "entities": []}, {"text": "One attractive possibility would be to enumerate the relations which can be obtained by adding the current node to the various different equivalence classes which are available, apply some scoring function to each class, and then normalize such that the total score overall alternatives is one.", "labels": [], "entities": []}, {"text": "But this might introduce unpleasant dependencies of the probabilities of feature structures on the order in which the stochastic procedure chooses to expand nodes, because the normalisation is carried out before we have full knowledge of the equivalence classes with which the current node might become associated.", "labels": [], "entities": []}, {"text": "It maybe that an appropriate choice of scoring function will circumvent this difficulty, but this is left as a matter for further research.", "labels": [], "entities": []}], "tableCaptions": []}