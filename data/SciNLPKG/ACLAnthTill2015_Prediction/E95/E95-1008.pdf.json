{"title": [{"text": "Collocation Map for Overcoming Data Sparseness", "labels": [], "entities": []}], "abstractContent": [{"text": "Statistical language models are useful because they can provide probabilis-tic information upon uncertain decision making.", "labels": [], "entities": []}, {"text": "The most common statistic is n-grams measuring word cooccurrences in texts.", "labels": [], "entities": []}, {"text": "The method suffers from data shortage problem, however.", "labels": [], "entities": []}, {"text": "In this paper , we suggest Bayesian networks be used in approximating the statistics of insufficient occurrences and of those that do not occur in the sample texts with graceful degradation.", "labels": [], "entities": []}, {"text": "Collocation map is a sigmoid belief network that can be constructed from bigrams.", "labels": [], "entities": []}, {"text": "We compared the conditional probabilities and mutual information computed from bigrams and Collocation map.", "labels": [], "entities": []}, {"text": "The results show that the variance of the values from Colloca-tion map is smaller than that from frequency measure for the infrequent pairs by 48%.", "labels": [], "entities": []}, {"text": "The predictive power of Col-location map for arbitrary associations not observed from sample texts is also demonstrated.", "labels": [], "entities": []}], "introductionContent": [{"text": "In statistical language processing, n-grams are bar sic to many probabilistic models including Hidden Markov models that work on the limited dependency of linguistic events.", "labels": [], "entities": [{"text": "statistical language processing", "start_pos": 3, "end_pos": 34, "type": "TASK", "confidence": 0.7125628590583801}]}, {"text": "In this regard, Bayesian models (Bayesian network, Belief network, Inference diagram to name a few) are not very different from ItMMs.", "labels": [], "entities": []}, {"text": "Bayesian models capture the conditional independence among probabilistic variables, and can compute the conditional distribution of the variables, which is known as a probabilistic inferencing.", "labels": [], "entities": []}, {"text": "The pure n-gram statistic, however, is somewhat crude in that it cannot do anything about unobserved events and its approximation on infrequent events can be unreliable.", "labels": [], "entities": []}, {"text": "In this paper we show byway of extensive experiments that the Bayesian method that also can be composed from bigrams can overcome the data sparseness problem that is inherent in frequency counting methods.", "labels": [], "entities": []}, {"text": "According to the empirical results, Collocation map that is a Bayesian model for lexical variables induced graceful approximation over unobserved and infrequent events.", "labels": [], "entities": []}, {"text": "There are two known methods to deal with the data sparseness problem.", "labels": [], "entities": []}, {"text": "They are smoothing and class based methods.", "labels": [], "entities": []}, {"text": "Smoothing methods (Church and Gale 1991) readjust the distribution of frequencies of word occurrences obtained from sample texts, and verify the distribution through held-out texts.", "labels": [], "entities": []}, {"text": "As Dagan (1992) pointed out, however, the values from the smoothing methods closely agree with the probability of a bigram consisting of two independent words.", "labels": [], "entities": []}, {"text": "Class based methods () approximate the likelihood of unobserved words based on similar words.", "labels": [], "entities": []}, {"text": "proposed a non-hierarchical class based method.", "labels": [], "entities": []}, {"text": "The two approaches report limited successes of purely experimental nature.", "labels": [], "entities": []}, {"text": "This is so because they are based on strong assumptions.", "labels": [], "entities": []}, {"text": "In the case of smoothing methods, frequency readjustment is somewhat arbitrary and will not be good for heavily dependent bigrams.", "labels": [], "entities": []}, {"text": "As to the class based methods, the notion of similar words differs across different methods, and the association of probabilistic dependency with the similarity (class) of words is too strong to assume in generM.", "labels": [], "entities": []}, {"text": "Collocation map that is first suggested in (Itan 1993) is a sigmoid belief network with words as probabilistic variables.", "labels": [], "entities": []}, {"text": "Sigmoid belief network is extensively studied by, and has an efficient inferencing algorithm.", "labels": [], "entities": [{"text": "Sigmoid belief network", "start_pos": 0, "end_pos": 22, "type": "DATASET", "confidence": 0.7124256690343221}]}, {"text": "Unlike other Bayesian models, the inferencing on sigmoid belief network is not NP-hard, and inference methods by reducing the network and sampling are discussed in).", "labels": [], "entities": []}, {"text": "Bayesian models constructed from local dependencies provide formal approximation among the variables, thus using Collocation map does not require strong assumption or intuition to justify the associations among words produced by the map.", "labels": [], "entities": []}, {"text": "The results of inferencing on Collocation map are probabilities among any combinations of words represented in the map, which is not found in other models.", "labels": [], "entities": []}, {"text": "One significant shortcoming of Bayesian models lies in the heavy cost of inferencing.", "labels": [], "entities": []}, {"text": "Our implementation of Collocation map includes 988 nodes, and takes 2 to 3 minutes to compute an association between words.", "labels": [], "entities": []}, {"text": "The purpose of experiments is to find out how gracefully Collocation map deals with the unobserved cooccurrences in comparison with a naive bigram statistic.", "labels": [], "entities": []}, {"text": "In the next section, Collocation map is reviewed following the definition in.", "labels": [], "entities": []}, {"text": "In section 3, mutual information and conditional probabilities computed using bigrams and Collocation map are compared.", "labels": [], "entities": []}, {"text": "Section 4 concludes the paper by summarizing the good and bad points of the Collocation map and other methods.", "labels": [], "entities": []}], "datasetContent": [{"text": "The goal of our experiment is first to find how data sparseness is related with the frequency based statistics and to show Collocation map based method gives more reliable approximations.", "labels": [], "entities": []}, {"text": "In particular, from the experiments we observed the variances of statistics might suggest the level of data sparseness.", "labels": [], "entities": []}, {"text": "The less frequent data tended to have higher variances though the values of statistics (mutual information for instance) did not distinguish the level of occurrences.", "labels": [], "entities": []}, {"text": "The predictive account of Collocation map is demonstrated by observing the variances of approximations on the infrequent events.", "labels": [], "entities": []}, {"text": "The tagged Wall Street Journal articles of Penn Tree corpus were used that contain about 2.6 million word units.", "labels": [], "entities": [{"text": "Wall Street Journal articles of Penn Tree corpus", "start_pos": 11, "end_pos": 59, "type": "DATASET", "confidence": 0.9294379502534866}]}, {"text": "In the experiments, about 1.2 million of them was used.", "labels": [], "entities": []}, {"text": "Programs were coded in C language, and run on a Sun Spare 10 workstation.", "labels": [], "entities": [{"text": "Sun Spare 10 workstation", "start_pos": 48, "end_pos": 72, "type": "DATASET", "confidence": 0.9385576695203781}]}, {"text": "For the first 1.2 million words, the bigrams consisting of four types of categories (NN, NNS, IN, J J) were obtained, and mutual information of each bigram (order insensitive) was computed.", "labels": [], "entities": []}, {"text": "The bigrams were classified into 200 sets according to their occurrences.", "labels": [], "entities": []}, {"text": "summarizes the the average MI value and the variance of each frequency range.", "labels": [], "entities": [{"text": "MI", "start_pos": 27, "end_pos": 29, "type": "METRIC", "confidence": 0.9968723654747009}]}, {"text": "From that shows the occurrence distribution of 378,888 unique bigrams, about 70% of them occur only onetime.", "labels": [], "entities": []}, {"text": "One interesting and important observation is that those of 1 to 3 frequency range that take about 90% of the population have very high MI values.", "labels": [], "entities": [{"text": "MI", "start_pos": 135, "end_pos": 137, "type": "METRIC", "confidence": 0.9953333735466003}]}, {"text": "This results also agree with Dunning's argument about overestimation on the infrequent occurrences in which many infrequent pairs tend to get higher estimation).", "labels": [], "entities": [{"text": "estimation", "start_pos": 149, "end_pos": 159, "type": "METRIC", "confidence": 0.9515618085861206}]}, {"text": "The problem is due to the assumption of normality in naive frequency based statistics according to.", "labels": [], "entities": []}, {"text": "Approximated values, thus, do not indicate the level of data quality.", "labels": [], "entities": [{"text": "Approximated", "start_pos": 0, "end_pos": 12, "type": "METRIC", "confidence": 0.9871783256530762}]}, {"text": "shows variances can suggest the level of data sufficiency.", "labels": [], "entities": []}, {"text": "From this observation we propose the following definition on the notion of data sparseness.", "labels": [], "entities": []}, {"text": "A set of units belonging to a sample of ordered word units (texts) is cz datasparse if and only if the variance of measurements on the set is greater than ~.", "labels": [], "entities": []}, {"text": "The definition sets the concept of sparseness within the context of a focused set of linguistic units.", "labels": [], "entities": []}, {"text": "For a set of units unoberved from a sample, the given sample text is for sure data-sparse.", "labels": [], "entities": []}, {"text": "The above definition then gives away to judge with respect to observed units.", "labels": [], "entities": []}, {"text": "The measurement of data sparseness can be a good issue to study where it may depend on the contexts of research.", "labels": [], "entities": []}, {"text": "Here we suggest a simple method perhaps for the first time in the literature.", "labels": [], "entities": []}, {"text": "compares the results from using Collocation map and simple frequency statistic.", "labels": [], "entities": []}, {"text": "The variances are smaller and the pairs in frequency 1 class have nonzero approximations.", "labels": [], "entities": []}, {"text": "Because computation on Collocation map is very high, we have chosen 2000 unique pairs at random.", "labels": [], "entities": []}, {"text": "The network consists of 988 nodes.", "labels": [], "entities": []}, {"text": "Computing an approximation (inferencing) took about 3 minutes.", "labels": [], "entities": []}, {"text": "The test size of 2000 pairs may not be sufficient, but it showed the consistent tendency of graceful degradation of variances.", "labels": [], "entities": []}, {"text": "The overestimation problem was not significant in the approximations by Collocation map.", "labels": [], "entities": [{"text": "Collocation map", "start_pos": 72, "end_pos": 87, "type": "DATASET", "confidence": 0.8472209870815277}]}, {"text": "The average value of zero frequency class to which 50 unobserved pairs belong was also on the line of smooth degradation, and shows only the variance.", "labels": [], "entities": []}, {"text": "summarizes the details of performance gain by using Collocation map.", "labels": [], "entities": [{"text": "Collocation map", "start_pos": 52, "end_pos": 67, "type": "DATASET", "confidence": 0.7334617376327515}]}], "tableCaptions": [{"text": " Table 1: Comparison of variances between frequency based and Collocation map based MI computations.", "labels": [], "entities": []}]}