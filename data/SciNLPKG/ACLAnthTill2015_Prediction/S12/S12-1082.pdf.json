{"title": [{"text": "DeepPurple: Estimating Sentence Semantic Similarity using N-gram Regression Models and Web Snippets", "labels": [], "entities": []}], "abstractContent": [{"text": "We estimate the semantic similarity between two sentences using regression models with features: 1) n-gram hit rates (lexical matches) between sentences, 2) lexical semantic similarity between non-matching words, and 3) sentence length.", "labels": [], "entities": []}, {"text": "Lexical semantic similarity is computed via co-occurrence counts on a corpus harvested from the web using a modified mutual information metric.", "labels": [], "entities": [{"text": "Lexical semantic similarity", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.8268887797991434}]}, {"text": "State-of-the-art results are obtained for semantic similarity computation at the word level, however, the fusion of this information at the sentence level provides only moderate improvement on Task 6 of SemEval'12.", "labels": [], "entities": [{"text": "semantic similarity computation", "start_pos": 42, "end_pos": 73, "type": "TASK", "confidence": 0.7494223713874817}]}, {"text": "Despite the simple features used, regression models provide good performance , especially for shorter sentences, reaching correlation of 0.62 on the SemEval test set.", "labels": [], "entities": [{"text": "correlation", "start_pos": 122, "end_pos": 133, "type": "METRIC", "confidence": 0.9844361543655396}, {"text": "SemEval test set", "start_pos": 149, "end_pos": 165, "type": "DATASET", "confidence": 0.8013070523738861}]}], "introductionContent": [{"text": "Recently, there has been significant research activity on the area of semantic similarity estimation motivated both by abundance of relevant web data and linguistic resources for this task.", "labels": [], "entities": [{"text": "semantic similarity estimation", "start_pos": 70, "end_pos": 100, "type": "TASK", "confidence": 0.7899442712465922}]}, {"text": "Algorithms for computing semantic textual similarity (STS) are relevant fora variety of applications, including information extraction, question answering () and machine translation (.", "labels": [], "entities": [{"text": "computing semantic textual similarity (STS)", "start_pos": 15, "end_pos": 58, "type": "TASK", "confidence": 0.7598108095782143}, {"text": "information extraction", "start_pos": 112, "end_pos": 134, "type": "TASK", "confidence": 0.8282438218593597}, {"text": "question answering", "start_pos": 136, "end_pos": 154, "type": "TASK", "confidence": 0.9094782471656799}, {"text": "machine translation", "start_pos": 162, "end_pos": 181, "type": "TASK", "confidence": 0.832229346036911}]}, {"text": "Wordor term-level STS (a special case of sentence level STS) has also been successfully applied to the problem of grammar induction) and affective text categorization.", "labels": [], "entities": [{"text": "Wordor term-level STS", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.5265094935894012}, {"text": "sentence level STS", "start_pos": 41, "end_pos": 59, "type": "TASK", "confidence": 0.6791404485702515}, {"text": "grammar induction", "start_pos": 114, "end_pos": 131, "type": "TASK", "confidence": 0.7307880520820618}]}, {"text": "In this work, we built on previous research on word-level semantic similarity estimation to design and implement a system for sentence-level STS for Task6 of the SemEval'12 campaign.", "labels": [], "entities": [{"text": "word-level semantic similarity estimation", "start_pos": 47, "end_pos": 88, "type": "TASK", "confidence": 0.6675073206424713}, {"text": "sentence-level STS", "start_pos": 126, "end_pos": 144, "type": "TASK", "confidence": 0.6981661319732666}]}, {"text": "Semantic similarity between words can be regarded as the graded semantic equivalence at the lexeme level and is tightly related with the tasks of word sense discovery and disambiguation.", "labels": [], "entities": [{"text": "word sense discovery", "start_pos": 146, "end_pos": 166, "type": "TASK", "confidence": 0.7065078715483347}]}, {"text": "Metrics of word semantic similarity can be divided into: (i) knowledge-based metrics) and (ii) corpus-based metrics (.", "labels": [], "entities": [{"text": "word semantic similarity", "start_pos": 11, "end_pos": 35, "type": "TASK", "confidence": 0.7196526030699412}]}, {"text": "When more complex structures, such as phrases and sentences, are considered, it is much harder to estimate semantic equivalence due to the noncompositional nature of sentence-level semantics and the exponential explosion of possible interpretations.", "labels": [], "entities": []}, {"text": "STS is closely related to the problems of paraphrasing, which is bidirectional and based on semantic equivalence and textual entailment, which is directional and based on relations between semantics ().", "labels": [], "entities": [{"text": "STS", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.9581413269042969}]}, {"text": "Related methods incorporate measurements of similarity at various levels: lexical, syntactic, and semantic ().", "labels": [], "entities": []}, {"text": "Measures from machine translation evaluation are often used to evaluate lexical level approaches (), including BLEU (), a metric based on word ngram hit rates.", "labels": [], "entities": [{"text": "machine translation evaluation", "start_pos": 14, "end_pos": 44, "type": "TASK", "confidence": 0.8627614180246989}, {"text": "BLEU", "start_pos": 111, "end_pos": 115, "type": "METRIC", "confidence": 0.9981347322463989}]}, {"text": "Motivated by BLEU, we use n-gram hit rates and word-level semantic similarity scores as features in a linear regression model to estimate sentence level semantic similarity.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 13, "end_pos": 17, "type": "METRIC", "confidence": 0.9788186550140381}]}, {"text": "We also propose sigmoid scaling of similarity scores and sentence-length dependent modeling.", "labels": [], "entities": []}, {"text": "The models are evaluated on the SemEval'12 sentence similarity task.", "labels": [], "entities": [{"text": "SemEval'12 sentence similarity task", "start_pos": 32, "end_pos": 67, "type": "TASK", "confidence": 0.8671261519193649}]}], "datasetContent": [{"text": "Initially all sentences are pre-processed by the CoreNLP ( suite of tools, a process that includes named entity recognition, normalization, part of speech tagging, lemmatization and stemming.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 99, "end_pos": 123, "type": "TASK", "confidence": 0.6189519166946411}, {"text": "part of speech tagging", "start_pos": 140, "end_pos": 162, "type": "TASK", "confidence": 0.6404164880514145}]}, {"text": "The exact type of pre-processing used depends on the metric used.", "labels": [], "entities": []}, {"text": "For the plain lexical BLEU, we use lemmatization, stemming (of lemmas) and remove all non-content words, keeping only nouns, adjectives, verbs and adverbs.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 22, "end_pos": 26, "type": "METRIC", "confidence": 0.9758684635162354}]}, {"text": "For computing semantic similarity scores, we don't use stemming and keep only noun words, since we only have similarities between non-noun words.", "labels": [], "entities": []}, {"text": "For the computation of semantic similarity we have created a dictionary containing all the single-word nouns included in WordNet (approx. 60K) and then downloaded snippets of the 500 top-ranked documents for each word by formulating single-word queries and submitting them to the Yahoo!", "labels": [], "entities": [{"text": "semantic similarity", "start_pos": 23, "end_pos": 42, "type": "TASK", "confidence": 0.6755254864692688}, {"text": "WordNet", "start_pos": 121, "end_pos": 128, "type": "DATASET", "confidence": 0.9312941431999207}]}, {"text": "Next, results are reported in terms of correlation between the automatically computed scores and the ground truth, for each of the corpora in Task 6 of SemEval'12 (paraphrase, video, europarl, WordNet, news).", "labels": [], "entities": [{"text": "WordNet", "start_pos": 193, "end_pos": 200, "type": "DATASET", "confidence": 0.782471239566803}]}, {"text": "Overall correlation (\"Ovrl\") computed on the join of the dataset, as well as, average (\"Mean\") correlation across all task is also reported.", "labels": [], "entities": [{"text": "correlation (\"Ovrl\")", "start_pos": 8, "end_pos": 28, "type": "METRIC", "confidence": 0.8396174013614655}, {"text": "Mean\") correlation", "start_pos": 88, "end_pos": 106, "type": "METRIC", "confidence": 0.9007054964701334}]}, {"text": "Training is performed on a subset of the first three corpora and testing on all five corpora.", "labels": [], "entities": []}, {"text": "Baseline BLEU: The first set of results in Table 1, shows the correlation performance of the plain BLEU hit rates (per training data set and overall/average).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 9, "end_pos": 13, "type": "METRIC", "confidence": 0.9654779434204102}, {"text": "BLEU hit rates", "start_pos": 99, "end_pos": 113, "type": "METRIC", "confidence": 0.9216007987658182}]}, {"text": "The best performing hit rate is the one calculated using unigrams.", "labels": [], "entities": []}, {"text": "Semantic Similarity BLEU (Purple): The performance of the modified version of BLEU that incorporates various word-level similarity metrics is shown in.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 20, "end_pos": 24, "type": "METRIC", "confidence": 0.9117277264595032}, {"text": "BLEU", "start_pos": 78, "end_pos": 82, "type": "METRIC", "confidence": 0.9899893999099731}]}, {"text": "Here the BLEU hits (exact matches) are summed together with the normalized similarity scores (approximate matches) to obtain a single B 1 +M 1 (Purple) score . As we can see, there are definite benefits to using the modified version, particularly with regards to mean correlation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 9, "end_pos": 13, "type": "METRIC", "confidence": 0.9987332224845886}, {"text": "B 1 +M 1 (Purple) score", "start_pos": 134, "end_pos": 157, "type": "METRIC", "confidence": 0.8914535774124993}, {"text": "correlation", "start_pos": 268, "end_pos": 279, "type": "METRIC", "confidence": 0.5589858293533325}]}, {"text": "Overall the best performers, when taking into account both mean and overall correlation, are the WordNetbased and Ia metrics, with the Ia metric winning by a slight margin, earning a place in the final models.", "labels": [], "entities": [{"text": "mean", "start_pos": 59, "end_pos": 63, "type": "METRIC", "confidence": 0.9553658366203308}, {"text": "correlation", "start_pos": 76, "end_pos": 87, "type": "METRIC", "confidence": 0.5192666053771973}, {"text": "WordNetbased", "start_pos": 97, "end_pos": 109, "type": "DATASET", "confidence": 0.9747982621192932}]}, {"text": "both with and without semantic similarity.", "labels": [], "entities": []}, {"text": "The baseline in this case is the Purple metric (corresponding to no fusion).", "labels": [], "entities": []}, {"text": "Clearly the use of regression models significantly improves performance compared to the 1-gram BLEU and Purple baselines for almost all datasets, and especially for the combined dataset (overall).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 95, "end_pos": 99, "type": "METRIC", "confidence": 0.9914914965629578}]}, {"text": "Among the fusion schemes, the hierarchical models perform the best.", "labels": [], "entities": []}, {"text": "Following fusion, the performance gain from incorporating semantic similarity (SS) is much smaller.", "labels": [], "entities": [{"text": "incorporating semantic similarity (SS)", "start_pos": 44, "end_pos": 82, "type": "TASK", "confidence": 0.666527216633161}]}, {"text": "Finally, in, correlation performance of our submissions on the official SemEval test set is: Correlation performance of regression model with (SS) and without semantic similarities on the training set (using 10-fold cross-validation).", "labels": [], "entities": [{"text": "SemEval test set", "start_pos": 72, "end_pos": 88, "type": "DATASET", "confidence": 0.8173124889532725}]}], "tableCaptions": [{"text": " Table 1: Correlation performance of BLEU hit rates.", "labels": [], "entities": [{"text": "BLEU hit rates", "start_pos": 37, "end_pos": 51, "type": "METRIC", "confidence": 0.9212353229522705}]}, {"text": " Table 2: Correlation performance of 1-gram BLEU  scores with semantic similarity metrics (nouns-only).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 44, "end_pos": 48, "type": "METRIC", "confidence": 0.9910076260566711}]}, {"text": " Table 3: Correlation performance of regression model  with (SS) and without semantic similarities on the train- ing set (using 10-fold cross-validation).", "labels": [], "entities": []}, {"text": " Table 4: Correlation performance on test set.", "labels": [], "entities": []}]}