{"title": [{"text": "SemEval-2012 Task 6: A Pilot on Semantic Textual Similarity", "labels": [], "entities": [{"text": "SemEval-2012 Task 6", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8706931471824646}, {"text": "Semantic Textual Similarity", "start_pos": 32, "end_pos": 59, "type": "TASK", "confidence": 0.5912917256355286}]}], "abstractContent": [{"text": "Semantic Textual Similarity (STS) measures the degree of semantic equivalence between two texts.", "labels": [], "entities": [{"text": "Semantic Textual Similarity (STS)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7829715758562088}]}, {"text": "This paper presents the results of the STS pilot task in Semeval.", "labels": [], "entities": [{"text": "STS pilot task", "start_pos": 39, "end_pos": 53, "type": "TASK", "confidence": 0.6712040106455485}]}, {"text": "The training data contained 2000 sentence pairs from previously existing paraphrase datasets and machine translation evaluation resources.", "labels": [], "entities": [{"text": "machine translation evaluation", "start_pos": 97, "end_pos": 127, "type": "TASK", "confidence": 0.7969241539637247}]}, {"text": "The test data also comprised 2000 sentences pairs for those datasets, plus two surprise datasets with 400 pairs from a different machine translation evaluation corpus and 750 pairs from a lexical resource mapping exercise.", "labels": [], "entities": [{"text": "machine translation evaluation", "start_pos": 129, "end_pos": 159, "type": "TASK", "confidence": 0.7672393222649893}]}, {"text": "The similarity of pairs of sentences was rated on a 0-5 scale (low to high similarity) by human judges using Amazon Mechanical Turk, with high Pearson correlation scores, around 90%.", "labels": [], "entities": [{"text": "similarity", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.965060830116272}, {"text": "Amazon Mechanical Turk", "start_pos": 109, "end_pos": 131, "type": "DATASET", "confidence": 0.9194350838661194}, {"text": "Pearson correlation", "start_pos": 143, "end_pos": 162, "type": "METRIC", "confidence": 0.9755171239376068}]}, {"text": "35 teams participated in the task, submitting 88 runs.", "labels": [], "entities": []}, {"text": "The best results scored a Pearson correlation >80%, well above a simple lexical baseline that only scored a 31% correlation.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 26, "end_pos": 45, "type": "METRIC", "confidence": 0.9227540791034698}]}, {"text": "This pilot task opens an exciting way ahead, although there are still open issues, specially the evaluation metric.", "labels": [], "entities": []}], "introductionContent": [{"text": "Semantic Textual Similarity (STS) measures the degree of semantic equivalence between two sentences.", "labels": [], "entities": [{"text": "Semantic Textual Similarity (STS)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7572402705748876}]}, {"text": "STS is related to both Textual Entailment (TE) and Paraphrase (PARA).", "labels": [], "entities": [{"text": "Textual Entailment (TE)", "start_pos": 23, "end_pos": 46, "type": "TASK", "confidence": 0.7450945794582366}, {"text": "Paraphrase (PARA", "start_pos": 51, "end_pos": 67, "type": "METRIC", "confidence": 0.9003874460856119}]}, {"text": "STS is more directly applicable in a number of NLP tasks than TE and PARA such as Machine Translation and evaluation, Summarization, Machine Reading, Deep Question Answering, etc.", "labels": [], "entities": [{"text": "STS", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.7260485291481018}, {"text": "TE", "start_pos": 62, "end_pos": 64, "type": "METRIC", "confidence": 0.859552800655365}, {"text": "PARA", "start_pos": 69, "end_pos": 73, "type": "METRIC", "confidence": 0.9671533107757568}, {"text": "Machine Translation and evaluation", "start_pos": 82, "end_pos": 116, "type": "TASK", "confidence": 0.8836729824542999}, {"text": "Summarization", "start_pos": 118, "end_pos": 131, "type": "TASK", "confidence": 0.990211546421051}, {"text": "Machine Reading", "start_pos": 133, "end_pos": 148, "type": "TASK", "confidence": 0.8372069001197815}, {"text": "Deep Question Answering", "start_pos": 150, "end_pos": 173, "type": "TASK", "confidence": 0.6595510641733805}]}, {"text": "STS differs from TE in as much as it assumes symmetric graded equivalence between the pair of textual snippets.", "labels": [], "entities": [{"text": "STS", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.7233350872993469}]}, {"text": "In the case of TE the equivalence is directional, e.g. a car is a vehicle, but a vehicle is not necessarily a car.", "labels": [], "entities": []}, {"text": "Additionally, STS differs from both TE and PARA in that, rather than being a binary yes/no decision (e.g. a vehicle is not a car), STS incorporates the notion of graded semantic similarity (e.g. a vehicle and a car are more similar than a wave and a car).", "labels": [], "entities": [{"text": "STS", "start_pos": 14, "end_pos": 17, "type": "TASK", "confidence": 0.9338934421539307}, {"text": "TE", "start_pos": 36, "end_pos": 38, "type": "METRIC", "confidence": 0.6984094977378845}, {"text": "PARA", "start_pos": 43, "end_pos": 47, "type": "METRIC", "confidence": 0.863105058670044}]}, {"text": "STS provides a unified framework that allows for an extrinsic evaluation of multiple semantic components that otherwise have tended to be evaluated independently and without broad characterization of their impact on NLP applications.", "labels": [], "entities": []}, {"text": "Such components include word sense disambiguation and induction, lexical substitution, semantic role labeling, multiword expression detection and handling, anaphora and coreference resolution, time and date resolution, named-entity handling, underspecification, hedging, semantic scoping and discourse analysis.", "labels": [], "entities": [{"text": "word sense disambiguation and induction", "start_pos": 24, "end_pos": 63, "type": "TASK", "confidence": 0.740537965297699}, {"text": "lexical substitution", "start_pos": 65, "end_pos": 85, "type": "TASK", "confidence": 0.7608446180820465}, {"text": "semantic role labeling", "start_pos": 87, "end_pos": 109, "type": "TASK", "confidence": 0.6502544184525808}, {"text": "multiword expression detection and handling", "start_pos": 111, "end_pos": 154, "type": "TASK", "confidence": 0.7939265012741089}, {"text": "coreference resolution", "start_pos": 169, "end_pos": 191, "type": "TASK", "confidence": 0.7409378886222839}, {"text": "time and date resolution", "start_pos": 193, "end_pos": 217, "type": "TASK", "confidence": 0.5865044444799423}, {"text": "named-entity handling", "start_pos": 219, "end_pos": 240, "type": "TASK", "confidence": 0.7073060274124146}, {"text": "discourse analysis", "start_pos": 292, "end_pos": 310, "type": "TASK", "confidence": 0.7541531622409821}]}, {"text": "Though not in the scope of the current pilot task, we plan to explore building an open source toolkit for integrating and applying diverse linguistic analysis modules to the STS task.", "labels": [], "entities": [{"text": "STS task", "start_pos": 174, "end_pos": 182, "type": "TASK", "confidence": 0.8769672513008118}]}, {"text": "While the characterization of STS is still preliminary, we observed that there was no comparable existing dataset extensively annotated for pairwise semantic sentence similarity.", "labels": [], "entities": [{"text": "STS", "start_pos": 30, "end_pos": 33, "type": "TASK", "confidence": 0.9173718690872192}, {"text": "pairwise semantic sentence similarity", "start_pos": 140, "end_pos": 177, "type": "TASK", "confidence": 0.6049778461456299}]}, {"text": "We approached the construction of the first STS dataset with the following goals: (1) To set a definition of STS as a graded notion which can be easily communicated to non-expert annotators beyond the likert-scale; (2) To gather a substantial amount of sentence pairs from diverse datasets, and to annotate them with high quality; (3) To explore evaluation measures for STS; (4) To explore the relation of STS to PARA and Machine Translation Evaluation exercises.", "labels": [], "entities": [{"text": "Machine Translation Evaluation", "start_pos": 422, "end_pos": 452, "type": "TASK", "confidence": 0.7984551986058553}]}, {"text": "In the next section we present the various sources of the STS data and the annotation procedure used.", "labels": [], "entities": [{"text": "STS data", "start_pos": 58, "end_pos": 66, "type": "DATASET", "confidence": 0.8019469976425171}]}, {"text": "Section 4 investigates the evaluation of STS systems.", "labels": [], "entities": []}, {"text": "Section 5 summarizes the resources and tools used by participant systems.", "labels": [], "entities": []}, {"text": "Finally, Section 6 draws the conclusions.", "labels": [], "entities": []}], "datasetContent": [{"text": "Datasets for STS are scarce.", "labels": [], "entities": [{"text": "STS", "start_pos": 13, "end_pos": 16, "type": "TASK", "confidence": 0.975233793258667}]}, {"text": "Existing datasets include () and ().", "labels": [], "entities": []}, {"text": "The first dataset includes 65 sentence pairs which correspond to the dictionary definitions for the 65 word pairs in Similarity(.", "labels": [], "entities": []}, {"text": "The authors asked human informants to assess the meaning of the sentence pairs on a scale from 0.0 (minimum similarity) to 4.0 (maximum similarity).", "labels": [], "entities": []}, {"text": "While the dataset is very relevant to STS, it is too small to train, develop and test typical machine learning based systems.", "labels": [], "entities": [{"text": "STS", "start_pos": 38, "end_pos": 41, "type": "TASK", "confidence": 0.9855522513389587}]}, {"text": "The second dataset comprises 50 documents on news, ranging from 51 to 126 words.", "labels": [], "entities": []}, {"text": "Subjects were asked to judge the similarity of document pairs on a five-point scale (with 1.0 indicating \"highly unrelated\" and 5.0 indicating \"highly related\").", "labels": [], "entities": []}, {"text": "This second dataset comprises a larger number of document pairs, but it goes beyond sentence similarity into textual similarity.", "labels": [], "entities": []}, {"text": "When constructing our datasets, gathering naturally occurring pairs of sentences with different degrees of semantic equivalence was a challenge in itself.", "labels": [], "entities": []}, {"text": "If we took pairs of sentences at random, the vast majority of them would be totally unrelated, and only a very small fragment would show some sort of semantic equivalence.", "labels": [], "entities": []}, {"text": "Accordingly, we investigated reusing a collection of existing datasets from tasks that are related to STS.", "labels": [], "entities": []}, {"text": "We first studied the pairs of text from the Recognizing TE challenge.", "labels": [], "entities": [{"text": "Recognizing TE challenge", "start_pos": 44, "end_pos": 68, "type": "TASK", "confidence": 0.6239770849545797}]}, {"text": "The first editions of the challenge included pairs of sentences as the following: T: The Christian Science Monitor named a US journalist kidnapped in Iraq as freelancer Jill Carroll.", "labels": [], "entities": [{"text": "T", "start_pos": 82, "end_pos": 83, "type": "METRIC", "confidence": 0.9811562299728394}, {"text": "The Christian Science Monitor", "start_pos": 85, "end_pos": 114, "type": "DATASET", "confidence": 0.8448241055011749}]}, {"text": "H: Jill Carroll was abducted in Iraq.", "labels": [], "entities": []}, {"text": "The first sentence is the text, and the second is the hypothesis.", "labels": [], "entities": []}, {"text": "The organizers of the challenge annotated several pairs with a binary tag, indicating whether the hypothesis could be entailed from the text.", "labels": [], "entities": []}, {"text": "Although these pairs of text are interesting we decided to discard them from this pilot because the length of the hypothesis was typically much shorter than the text, and we did not want to bias the STS task in this respect.", "labels": [], "entities": []}, {"text": "We may, however, explore using TE pairs for STS in the future.", "labels": [], "entities": [{"text": "TE", "start_pos": 31, "end_pos": 33, "type": "METRIC", "confidence": 0.7483115196228027}, {"text": "STS", "start_pos": 44, "end_pos": 47, "type": "TASK", "confidence": 0.9521359205245972}]}, {"text": "Microsoft Research (MSR) has pioneered the acquisition of paraphrases with two manually annotated datasets.", "labels": [], "entities": [{"text": "Microsoft Research (MSR)", "start_pos": 0, "end_pos": 24, "type": "DATASET", "confidence": 0.938863468170166}]}, {"text": "The first, called MSR Paraphrase (MSRpar for short) has been widely used to evaluate text similarity algorithms.", "labels": [], "entities": [{"text": "text similarity algorithms", "start_pos": 85, "end_pos": 111, "type": "TASK", "confidence": 0.778584768374761}]}, {"text": "It contains 5801 pairs of sentences gleaned over a period of 18 months from thousands of news sources on the web ().", "labels": [], "entities": []}, {"text": "67% of the pairs were tagged as paraphrases.", "labels": [], "entities": []}, {"text": "The inter annotator agreement is between 82% and 84%.", "labels": [], "entities": []}, {"text": "Complete meaning equivalence is not required, and the annotation guidelines allowed for some relaxation.", "labels": [], "entities": []}, {"text": "The pairs which were annotated as not being paraphrases ranged from completely unrelated semantically, to partially overlapping, to those that were almost-but-not-quite semantically equivalent.", "labels": [], "entities": []}, {"text": "In this sense our graded annotations enrich the dataset with more nuanced tags, as we will see in the following section.", "labels": [], "entities": []}, {"text": "We followed the original split of 70% for training and 30% for testing.", "labels": [], "entities": []}, {"text": "A sample pair from the dataset follows: The Senate Select Committee on Intelligence is preparing a blistering report on prewar intelligence on Iraq.", "labels": [], "entities": []}, {"text": "American intelligence leading up to the war on Iraq will be criticized by a powerful US Congressional committee due to report soon, officials said today.", "labels": [], "entities": []}, {"text": "In order to construct a dataset which would reflect a uniform distribution of similarity ranges, we sampled the MSRpar dataset at certain ranks of string similarity.", "labels": [], "entities": [{"text": "MSRpar dataset", "start_pos": 112, "end_pos": 126, "type": "DATASET", "confidence": 0.9483575522899628}]}, {"text": "We used the implementation readily accessible at CPAN 1 of a well-known metric.", "labels": [], "entities": [{"text": "CPAN 1", "start_pos": 49, "end_pos": 55, "type": "DATASET", "confidence": 0.9174182116985321}]}, {"text": "We sampled equal numbers of pairs from five bands of similarity in the [0.4 ..", "labels": [], "entities": []}, {"text": "0.8] range separately from the paraphrase and non-paraphrase pairs.", "labels": [], "entities": []}, {"text": "We sampled 1500 pairs overall, which we split 50% for training and 50% for testing.", "labels": [], "entities": []}, {"text": "The second dataset from MSR is the MSR Video Paraphrase Corpus (MSRvid for short).", "labels": [], "entities": [{"text": "MSR Video Paraphrase Corpus (MSRvid", "start_pos": 35, "end_pos": 70, "type": "DATASET", "confidence": 0.6858641058206558}]}, {"text": "The authors showed brief video segments to Annotators from Amazon Mechanical Turk (AMT) and were asked to provide a one-sentence description of the main action or event in the video (.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk (AMT)", "start_pos": 59, "end_pos": 87, "type": "DATASET", "confidence": 0.8803495665391287}]}, {"text": "Nearly 120 thousand sentences were collected for 2000 videos.", "labels": [], "entities": []}, {"text": "The sentences can betaken to be roughly parallel descriptions, and they included sentences for many languages.", "labels": [], "entities": []}, {"text": "shows a video and corresponding descriptions.", "labels": [], "entities": []}, {"text": "The sampling procedure from this dataset is similar to that for MSRpar.", "labels": [], "entities": []}, {"text": "We construct two bags of data to draw samples.", "labels": [], "entities": []}, {"text": "The first includes all possible pairs for the same video, and the second includes pairs taken from different videos.", "labels": [], "entities": []}, {"text": "Note that not all sentences from the same video were equivalent, as some descriptions were contradictory or unrelated.", "labels": [], "entities": []}, {"text": "Conversely, not all sentences coming from different videos were necessarily unrelated, as many videos were on similar topics.", "labels": [], "entities": []}, {"text": "We took an equal number of samples from each of these two sets, in an attempt to provide a balanced dataset between equivalent and non-equivalent pairs.", "labels": [], "entities": []}, {"text": "The sampling was also done according to string similarity, but in four bands in the [0.5 ..", "labels": [], "entities": []}, {"text": "0.8] range, as sentences from the same video had a usually higher string similarity than those in the MSRpar dataset.", "labels": [], "entities": [{"text": "MSRpar dataset", "start_pos": 102, "end_pos": 116, "type": "DATASET", "confidence": 0.9631330966949463}]}, {"text": "We sampled 1500 pairs overall, which we split 50% for training and 50% for testing.", "labels": [], "entities": []}, {"text": "Given the strong connection between STS systems and Machine Translation evaluation metrics, we also sampled pairs of segments that had been part of human evaluation exercises.", "labels": [], "entities": [{"text": "Machine Translation evaluation", "start_pos": 52, "end_pos": 82, "type": "TASK", "confidence": 0.8786613742510477}]}, {"text": "Those pairs included a reference translation and a automatic Machine Translation system submission, as follows: The only instance in which no tax is levied is when the supplier is in a non-EU country and the recipient is in a Member State of the EU.", "labels": [], "entities": [{"text": "reference translation", "start_pos": 23, "end_pos": 44, "type": "TASK", "confidence": 0.7589381635189056}, {"text": "Machine Translation system submission", "start_pos": 61, "end_pos": 98, "type": "TASK", "confidence": 0.806392602622509}]}, {"text": "The only case for which no tax is still perceived \"is an example of supply in the European Community from a third country.", "labels": [], "entities": []}, {"text": "We selected pairs from the translation shared task of the 2007 and 2008 ACL Workshops on Statistical Machine Translation (WMT)).", "labels": [], "entities": [{"text": "translation shared task of the 2007 and 2008 ACL Workshops on Statistical Machine Translation (WMT))", "start_pos": 27, "end_pos": 127, "type": "TASK", "confidence": 0.8035427998093998}]}, {"text": "For consistency, we only used French to English system submissions.", "labels": [], "entities": []}, {"text": "The training data includes all of the Europarl human ranked fr-en system submissions from WMT 2007, with each machine translation being paired with the correct reference translation.", "labels": [], "entities": [{"text": "Europarl human ranked fr-en system submissions from WMT 2007", "start_pos": 38, "end_pos": 98, "type": "DATASET", "confidence": 0.9319287074936761}]}, {"text": "This resulted in 729 unique training pairs.", "labels": [], "entities": []}, {"text": "The test data is comprised of all Europarl human evaluated fr-en pairs from WMT 2008 that contain 16 white space delimited tokens or less.", "labels": [], "entities": [{"text": "Europarl human evaluated fr-en pairs from WMT 2008", "start_pos": 34, "end_pos": 84, "type": "DATASET", "confidence": 0.9463540762662888}]}, {"text": "In addition, we selected two other datasets that were used as out-of-domain testing.", "labels": [], "entities": []}, {"text": "One of them comprised of all the human ranked fr-en system submissions from the WMT 2007 news conversation test set, resulting in 351 unique system reference pairs.", "labels": [], "entities": [{"text": "WMT 2007 news conversation test set", "start_pos": 80, "end_pos": 115, "type": "DATASET", "confidence": 0.9142488439877828}]}, {"text": "The second set is radically different as it comprised 750 pairs of glosses from OntoNotes 4.0 () and WordNet 3.1 senses.", "labels": [], "entities": []}, {"text": "The mapping of the senses of both resources comprised 110K sense pairs.", "labels": [], "entities": []}, {"text": "The similarity between the sense pairs was generated using simple word overlap.", "labels": [], "entities": []}, {"text": "50% of the pairs were sampled from senses which were deemed as equivalent senses, the rest from senses which did not map to one another.", "labels": [], "entities": []}, {"text": "lected at random from the three main datasets in the training set.", "labels": [], "entities": []}, {"text": "We did the annotation, and the pairwise Pearson ranged from 84% to 87% among ourselves.", "labels": [], "entities": [{"text": "Pearson", "start_pos": 40, "end_pos": 47, "type": "METRIC", "confidence": 0.6382468938827515}]}, {"text": "The agreement of each annotator with the average scores of the other was between 87% and 89%.", "labels": [], "entities": [{"text": "agreement", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9912723898887634}]}, {"text": "In the future, we would like to explore whether the definitions improve the consistency of the tagging with respect to a likert scale without definitions.", "labels": [], "entities": [{"text": "consistency", "start_pos": 76, "end_pos": 87, "type": "METRIC", "confidence": 0.9895819425582886}]}, {"text": "Note also that in the assessment of the quality and evaluation of the systems performances, we just took the resulting SS scores and their averages.", "labels": [], "entities": [{"text": "SS", "start_pos": 119, "end_pos": 121, "type": "METRIC", "confidence": 0.9835848808288574}]}, {"text": "Using the qualitative descriptions for each score in analysis and evaluation is left for future work.", "labels": [], "entities": []}, {"text": "Given the good results of the pilot we decided to deploy the task in Amazon Mechanical Turk (AMT) in order to crowd source the annotation task.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk (AMT)", "start_pos": 69, "end_pos": 97, "type": "DATASET", "confidence": 0.9301107625166575}]}, {"text": "The turkers were required to have achieved a 95% of approval rating in their previous HITs, and had to pass a qualification task which included 6 example pairs.", "labels": [], "entities": [{"text": "approval rating", "start_pos": 52, "end_pos": 67, "type": "METRIC", "confidence": 0.9372027218341827}, {"text": "HITs", "start_pos": 86, "end_pos": 90, "type": "DATASET", "confidence": 0.8596106171607971}]}, {"text": "Each HIT included 5 pairs of sentences, and was paid at 0.20$ each.", "labels": [], "entities": []}, {"text": "We collected 5 annotations per HIT.", "labels": [], "entities": [{"text": "HIT", "start_pos": 31, "end_pos": 34, "type": "DATASET", "confidence": 0.9584753513336182}]}, {"text": "In the latest data collection, each HIT required 114.9 second for completion.", "labels": [], "entities": [{"text": "HIT", "start_pos": 36, "end_pos": 39, "type": "DATASET", "confidence": 0.5613930225372314}]}, {"text": "In order to ensure the quality, we also performed post-hoc validation.", "labels": [], "entities": []}, {"text": "Each HIT contained one pair from our pilot.", "labels": [], "entities": []}, {"text": "After the tagging was completed we checked the correlation of each individual turker with our scores, and removed annotations of turkers which had low correlations (below 50%).", "labels": [], "entities": []}, {"text": "Given the high quality of the annotations among the turkers, we could alternatively use the correlation between the turkers itself to detect poor quality annotators.", "labels": [], "entities": []}, {"text": "Given two sentences, s1 and s2, an STS system would need to return a similarity score.", "labels": [], "entities": [{"text": "STS", "start_pos": 35, "end_pos": 38, "type": "METRIC", "confidence": 0.8132370114326477}, {"text": "similarity", "start_pos": 69, "end_pos": 79, "type": "METRIC", "confidence": 0.9681729674339294}]}, {"text": "Participants can also provide a confidence score indicating their confidence level for the result returned for each pair, but this confidence is not used for the main results.", "labels": [], "entities": []}, {"text": "The output of the systems performance is evaluated using the Pearson product-moment correlation coefficient between the system scores and the human scores, as customary in text similarity.", "labels": [], "entities": [{"text": "Pearson product-moment correlation coefficient", "start_pos": 61, "end_pos": 107, "type": "METRIC", "confidence": 0.9204311817884445}]}, {"text": "We calculated Pearson for each evaluation dataset separately.", "labels": [], "entities": [{"text": "Pearson", "start_pos": 14, "end_pos": 21, "type": "METRIC", "confidence": 0.9968386888504028}]}, {"text": "In order to have a single Pearson measure for each system we concatenated the gold standard (and system outputs) for all 5 datasets into a single gold standard file (and single system output).", "labels": [], "entities": []}, {"text": "The first version of the results were published using this method, but the overall score did not correspond well to the individual scores in the datasets, and participants proposed two additional evaluation metrics, both of them based on Pearson correlation.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 238, "end_pos": 257, "type": "METRIC", "confidence": 0.9249093532562256}]}, {"text": "The organizers of the task decided that it was more informative, and on the benefit of the community, to also adopt those evaluation metrics, and the idea of having a single main evaluation metric was dropped.", "labels": [], "entities": []}, {"text": "This decision was not without controversy, but the organizers gave more priority to openness and inclusiveness and to the involvement of participants.", "labels": [], "entities": []}, {"text": "The final result table thus included three evaluation metrics.", "labels": [], "entities": []}, {"text": "For the future we plan to analyze the evaluation metrics, including non-parametric metrics like Spearman.", "labels": [], "entities": []}, {"text": "The first evaluation metric is the Pearson correlation for the concatenation of all five datasets, as described above.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 35, "end_pos": 54, "type": "METRIC", "confidence": 0.9657372832298279}]}, {"text": "We will use overall Pearson or simply ALL to refer to this measure.", "labels": [], "entities": [{"text": "Pearson", "start_pos": 20, "end_pos": 27, "type": "METRIC", "confidence": 0.7163699269294739}, {"text": "ALL", "start_pos": 38, "end_pos": 41, "type": "METRIC", "confidence": 0.9021140336990356}]}, {"text": "The second evaluation metric normalizes the output for each dataset separately, using the linear least squares method.", "labels": [], "entities": []}, {"text": "We concatenated the system results for five datasets and then computed a single Pearson correlation.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 80, "end_pos": 99, "type": "METRIC", "confidence": 0.9443242847919464}]}, {"text": "Given Y = {y i } and X = {x i } (the gold standard scores and the system scores, respectively), we transform the system scores into X = {x i } in order to minimize the squared error The linear transformation is given by xi = xi * \u03b2 1 + \u03b2 2 , where \u03b2 1 and \u03b2 2 are found analytically.", "labels": [], "entities": []}, {"text": "We refer to this measure as Normalized Pearson or simply ALLnorm.", "labels": [], "entities": [{"text": "Pearson", "start_pos": 39, "end_pos": 46, "type": "METRIC", "confidence": 0.5287132859230042}, {"text": "ALLnorm", "start_pos": 57, "end_pos": 64, "type": "METRIC", "confidence": 0.9398424625396729}]}, {"text": "This metric was suggested by one of the participants, Sergio Jimenez.", "labels": [], "entities": []}, {"text": "The third evaluation metric is the weighted mean of the Pearson correlations on individual datasets.", "labels": [], "entities": [{"text": "Pearson correlations", "start_pos": 56, "end_pos": 76, "type": "METRIC", "confidence": 0.8939726054668427}]}, {"text": "The Pearson returned for each dataset is weighted according to the number of sentence pairs in that dataset.", "labels": [], "entities": [{"text": "Pearson returned", "start_pos": 4, "end_pos": 20, "type": "METRIC", "confidence": 0.9784189462661743}]}, {"text": "Given r i the five Pearson scores for each dataset, and n i the number of pairs in each dataset, the weighted mean is given as i=1..5 (r i * n i )/ i=1..5 n i We refer to this measure as weighted mean of Pearson or Mean for short.", "labels": [], "entities": [{"text": "Pearson scores", "start_pos": 19, "end_pos": 33, "type": "METRIC", "confidence": 0.9303800761699677}, {"text": "Pearson", "start_pos": 204, "end_pos": 211, "type": "DATASET", "confidence": 0.8621575236320496}, {"text": "Mean", "start_pos": 215, "end_pos": 219, "type": "METRIC", "confidence": 0.7739115953445435}]}], "tableCaptions": [{"text": " Table 1: The first row corresponds to the baseline. ALL for overall Pearson, ALLnorm for Pearson after normaliza- tion, and Mean for mean of Pearsons. We also show the ranks for each measure. Rightmost columns show Pearson for  each individual dataset. Note:  *  system submitted past the 120 hour window, post-deadline fixes,  \u2020 team involving  one of the organizers.", "labels": [], "entities": [{"text": "ALL", "start_pos": 53, "end_pos": 56, "type": "METRIC", "confidence": 0.9979217648506165}, {"text": "Pearson", "start_pos": 69, "end_pos": 76, "type": "METRIC", "confidence": 0.9156628251075745}, {"text": "ALLnorm", "start_pos": 78, "end_pos": 85, "type": "METRIC", "confidence": 0.9964718818664551}, {"text": "Mean", "start_pos": 125, "end_pos": 129, "type": "METRIC", "confidence": 0.9796091914176941}, {"text": "Pearson", "start_pos": 216, "end_pos": 223, "type": "METRIC", "confidence": 0.979099452495575}]}, {"text": " Table 2: Results according to weighted correlation for the systems that provided non-uniform confidence alongside  their scores.", "labels": [], "entities": []}]}