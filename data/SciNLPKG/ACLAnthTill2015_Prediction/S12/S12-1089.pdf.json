{"title": [{"text": "Saarland: Vector-based models of semantic textual similarity", "labels": [], "entities": [{"text": "semantic textual similarity", "start_pos": 33, "end_pos": 60, "type": "TASK", "confidence": 0.6457937757174174}]}], "abstractContent": [{"text": "This paper describes our system for the Se-meval 2012 Sentence Textual Similarity task.", "labels": [], "entities": [{"text": "Se-meval 2012 Sentence Textual Similarity task", "start_pos": 40, "end_pos": 86, "type": "TASK", "confidence": 0.8068062961101532}]}, {"text": "The system is based on a combination of few simple vector space-based methods for word meaning similarity.", "labels": [], "entities": [{"text": "word meaning similarity", "start_pos": 82, "end_pos": 105, "type": "TASK", "confidence": 0.6889378229777018}]}, {"text": "Evaluation results show that a simple combination of these unsuper-vised data-driven methods can be quite successful.", "labels": [], "entities": []}, {"text": "The simple vector space components achieve high performance on short sentences; on longer, more complex sentences, they are outperformed by a surprisingly competitive word overlap baseline, but they still bring improvements over this baseline when incorporated into a mixture model.", "labels": [], "entities": []}], "introductionContent": [{"text": "Vector space models are widely-used methods for word meaning similarity which exploit the so-called distributional hypothesis, stating that semantically similar words tend to occur in similar contexts.", "labels": [], "entities": [{"text": "word meaning similarity", "start_pos": 48, "end_pos": 71, "type": "TASK", "confidence": 0.6747947533925375}]}, {"text": "Word meaning is represented by the contexts in which a word occurs, and similarity is computed by comparing these contexts in a high-dimensional vector space.", "labels": [], "entities": [{"text": "similarity", "start_pos": 72, "end_pos": 82, "type": "METRIC", "confidence": 0.9554997086524963}]}, {"text": "Distributional models of word meaning are attractive because they are simple, have wide coverage, and can be easily acquired at virtually no cost in an unsupervised way.", "labels": [], "entities": [{"text": "word meaning", "start_pos": 25, "end_pos": 37, "type": "TASK", "confidence": 0.7237752825021744}]}, {"text": "Furthermore, recent research has shown that, at least to some extent, these models can be generalized to capture similarity beyond the (isolated) word level, either as lexical meaning modulated by context, or as vectorial meaning representations for phrases and sentences.", "labels": [], "entities": []}, {"text": "In this paper we evaluate the use of some of these models for the Semantic Textual Similarity (STS) task, which measures the degree of semantic equivalence between two sentences.", "labels": [], "entities": [{"text": "Semantic Textual Similarity (STS) task", "start_pos": 66, "end_pos": 104, "type": "TASK", "confidence": 0.7742888757160732}]}, {"text": "In recent work has drawn the attention to the question of building vectorial meaning representations for sentences by combining individual word vectors.", "labels": [], "entities": []}, {"text": "They propose a family of simple \"compositional\" models that compute a vector fora phrase or a sentence by combining vectors of the constituent words, using different operations such as vector addition or component-wise multiplication.", "labels": [], "entities": []}, {"text": "More refined models have been proposed recently by and. and others take a slightly different perspective on the problem: Instead of computing a vector representation fora complete phrase or sentence, they focus on the problem of \"disambiguating\" the vector representation of a target word based on distributional information about the words in the target's context.", "labels": [], "entities": []}, {"text": "While this approach is not \"compositional\" in the sense described above, it still captures some meaning of the complete phrase in which a target word occurs.", "labels": [], "entities": []}, {"text": "In this paper, we report on the system we used in the Semeval 2012 Sentence Textual Similarity shared task and describe an approach that uses a combination of few simple vector-based components.", "labels": [], "entities": [{"text": "Semeval 2012 Sentence Textual Similarity shared task", "start_pos": 54, "end_pos": 106, "type": "TASK", "confidence": 0.753115713596344}]}, {"text": "We extend the model of, which has been shown to perform well on a closely related paraphrase ranking task, with an additive composition operation along the lines of, and compare it with a simple alignment-based approach which in turn uses vector-based similarity scores.", "labels": [], "entities": [{"text": "paraphrase ranking task", "start_pos": 82, "end_pos": 105, "type": "TASK", "confidence": 0.8480914235115051}]}, {"text": "Results show that in particular the alignmentbased approach can achieve good performance on the Microsoft Research Video Description dataset.", "labels": [], "entities": [{"text": "Microsoft Research Video Description dataset", "start_pos": 96, "end_pos": 140, "type": "DATASET", "confidence": 0.9314387321472168}]}, {"text": "On the other datasets, all vector-based components are outperformed by a surprisingly competitive word overlap baseline, but they still bring improvements over this baseline when incorporated into a mixture model.", "labels": [], "entities": []}, {"text": "On the test dataset, the mixture model ranks 10th and 13th on the Microsoft Research Paraphrase and Video Description datasets, respectively, which we take this to be a quite promising result given that we use only few relatively simple vector based components to compute similarity scores for sentences.", "labels": [], "entities": [{"text": "Microsoft Research Paraphrase and Video Description datasets", "start_pos": 66, "end_pos": 126, "type": "DATASET", "confidence": 0.8371322665895734}]}, {"text": "The rest of the paper is structured as follows: Section 2 presents the individual vector-based components used by our system.", "labels": [], "entities": []}, {"text": "In Section 3 we present detailed evaluation results on the training set, as well as results for our system on the test set, while Section 4 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we present our experimental results.", "labels": [], "entities": []}, {"text": "In addition to the models described in Section 2, we define a baseline model which simply computes the word overlap between two sentences as: 2 Note that this can result in some words not being aligned while pairs not empty do w, w \u2190 highest cosine pair in pairs if w / \u2208 marked and w / \u2208 marked then alignment \u2190 w, w \u222a alignment marked \u2190 {w, w } \u222a marked end if pairs \u2190 pairs \\ {{w, w } end while return alignment end function The score assigned by this method is simply the number of words that the two sentences have in common divided by their total number of words.", "labels": [], "entities": []}, {"text": "Finally, we also propose a straightforward mixture model which combines all of the above methods.", "labels": [], "entities": []}, {"text": "We use the training data to fit a degree two polynomial over these individual predictors using least squares regression.", "labels": [], "entities": []}, {"text": "The vector space used in all experiments is a bag-ofwords space containing word co-occurrence counts.", "labels": [], "entities": []}, {"text": "We use the GigaWord (1.7 billion tokens) as input corpus and extract word co-occurrences within asymmetric 5-word context window.", "labels": [], "entities": [{"text": "GigaWord", "start_pos": 11, "end_pos": 19, "type": "DATASET", "confidence": 0.892303466796875}]}, {"text": "Co-occurrence counts smaller than three are set to 0 and we further apply (positive) pmi weighting.", "labels": [], "entities": []}], "tableCaptions": []}