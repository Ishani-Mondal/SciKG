{"title": [{"text": "A Probabilistic Lexical Model for Ranking Textual Inferences", "labels": [], "entities": [{"text": "Ranking Textual Inferences", "start_pos": 34, "end_pos": 60, "type": "TASK", "confidence": 0.7290156682332357}]}], "abstractContent": [{"text": "Identifying textual inferences, where the meaning of one text follows from another, is a general underlying task within many natural language applications.", "labels": [], "entities": [{"text": "Identifying textual inferences", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.9087406992912292}]}, {"text": "Commonly, it is approached either by generative syntactic-based methods or by \"lightweight\" heuristic lexical models.", "labels": [], "entities": []}, {"text": "We suggest a model which is confined to simple lexical information, but is formulated as a principled generative probabilistic model.", "labels": [], "entities": []}, {"text": "We focus our attention on the task of ranking textual inferences and show substantially improved results on a recently investigated question answering data set.", "labels": [], "entities": [{"text": "ranking textual inferences", "start_pos": 38, "end_pos": 64, "type": "TASK", "confidence": 0.7594687143961588}, {"text": "question answering data set", "start_pos": 132, "end_pos": 159, "type": "TASK", "confidence": 0.7431021183729172}]}], "introductionContent": [{"text": "The task of identifying texts which share semantic content arises as a general need in many natural language processing applications.", "labels": [], "entities": []}, {"text": "For instance, a paraphrasing application has to recognize texts which convey roughly the same content, and a summarization application needs to single out texts which contain the content stated by other texts.", "labels": [], "entities": [{"text": "summarization", "start_pos": 109, "end_pos": 122, "type": "TASK", "confidence": 0.9733739495277405}]}, {"text": "We refer to this general task as textual inference similar to prior use of this term ().", "labels": [], "entities": []}, {"text": "In many textual inference scenarios the setting requires a classification decision of whether the inference relation holds or not.", "labels": [], "entities": []}, {"text": "But in other scenarios ranking according to inference likelihood would be the natural task.", "labels": [], "entities": []}, {"text": "In this work we focus on ranking textual inferences; given a sentence and a corpus, the task is to rank the corpus passages by their plausibility to imply as much of the sentence meaning as possible.", "labels": [], "entities": []}, {"text": "Most naturally, this is the casein question answering (QA), where systems search for passages that cover the semantic components of the question.", "labels": [], "entities": [{"text": "casein question answering (QA)", "start_pos": 28, "end_pos": 58, "type": "TASK", "confidence": 0.7755043208599091}]}, {"text": "A recent line of research was dedicated to this task (.", "labels": [], "entities": []}, {"text": "A related scenario is the task of Recognizing Textual Entailment (RTE) within a corpus ( . In this task, inference systems should identify, fora given hypothesis, the sentences which entail it in a given corpus.", "labels": [], "entities": [{"text": "Recognizing Textual Entailment (RTE)", "start_pos": 34, "end_pos": 70, "type": "TASK", "confidence": 0.815798262755076}]}, {"text": "Even though RTE was presented as a classification task, it has an appealing potential as a ranking task as well.", "labels": [], "entities": [{"text": "RTE", "start_pos": 12, "end_pos": 15, "type": "TASK", "confidence": 0.7712530493736267}, {"text": "classification task", "start_pos": 35, "end_pos": 54, "type": "TASK", "confidence": 0.8972061574459076}]}, {"text": "For instance, one may want to find texts that validate a claim such as cellular radiation is dangerous for children, or to learn more about it from a newswire corpus.", "labels": [], "entities": []}, {"text": "To that end, one should look for additional mentions of this claim such as extensive usage of cell phones maybe harmful for youngsters.", "labels": [], "entities": []}, {"text": "This can be done by ranking the corpus passages by their likelihood to entail the claim, where the top ranked passages are likely to contain additional relevant information.", "labels": [], "entities": []}, {"text": "Two main approaches have been used to address textual inference (for either ranking or classification).", "labels": [], "entities": [{"text": "ranking or classification", "start_pos": 76, "end_pos": 101, "type": "TASK", "confidence": 0.705832819143931}]}, {"text": "One is based on transformations over syntactic parse trees (.", "labels": [], "entities": []}, {"text": "Some works in this line describe a probabilistic generative process in which the parse tree of the question is generated from the passage (.", "labels": [], "entities": []}, {"text": "In the second approach, lexical models have been employed for textual inference.", "labels": [], "entities": [{"text": "textual inference", "start_pos": 62, "end_pos": 79, "type": "TASK", "confidence": 0.7584029734134674}]}, {"text": "Typi-cally, lexical models consider a text fragment as a bag of terms and split the inference decision into two steps.", "labels": [], "entities": []}, {"text": "The first is a term-level estimation of the inference likelihood for each term independently, based on direct lexical match and on lexical knowledge resources.", "labels": [], "entities": []}, {"text": "Some commonly used resources are WordNet, distributional-similarity thesauri, and web knowledge resources such as.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 33, "end_pos": 40, "type": "DATASET", "confidence": 0.9501186609268188}]}, {"text": "The second step is making a final sentence-level decision based on these estimations for the component terms.", "labels": [], "entities": []}, {"text": "Lexical models have the advantage of being fast and easy to utilize (e.g. no dependency on parsing tools) while being highly competitive with top performing systems, e.g. the system of.", "labels": [], "entities": []}, {"text": "In this work, we investigate how well such lexical models can perform in textual inference ranking scenarios.", "labels": [], "entities": [{"text": "textual inference ranking", "start_pos": 73, "end_pos": 98, "type": "TASK", "confidence": 0.6602413356304169}]}, {"text": "However, while lexical models usually apply heuristic methods, we would like to pursue a principled learning-based generative framework, in analogy to the approaches for syntactic-based inference.", "labels": [], "entities": []}, {"text": "An attractive work in this spirit is presented in, that propose a model which is both lexical and probabilistic.", "labels": [], "entities": []}, {"text": "Later, improved this model and reported results that outperformed previous lexical models and were on par with state-of-the-art RTE models.", "labels": [], "entities": []}, {"text": "Whereas their term-level model provides means to integrate lexical knowledge in a probabilistic manner, their sentence-level model depends to a great extent on heuristic normalizations which were introduced to incorporate prominent aspects of the sentence-level decision.", "labels": [], "entities": []}, {"text": "This deviates their model from a pure probabilistic methodology.", "labels": [], "entities": []}, {"text": "Our work aims at amending this deficiency and proposes anew probabilistic sentence-level model based on a Markovian process.", "labels": [], "entities": []}, {"text": "In that model, all parameters are estimated by an EM algorithm.", "labels": [], "entities": []}, {"text": "We evaluate this model on the tasks of ranking passages for QA and ranking textual entailments within a corpus, and show that eliminating the need for heuristic normalizations greatly improves state-of-the-art performance.", "labels": [], "entities": []}, {"text": "The full implementation of our model is available for download and can be used as an easy-to-install and highly competitive inference en-gine that operates only on lexical knowledge, or as a lexical component integrated within a more complex inference system.", "labels": [], "entities": []}, {"text": "provided an annotated data set, based on the Text REtrieval Conference (TREC) QA tracks 3 , specifically for the task of ranking candidate answer passages.", "labels": [], "entities": [{"text": "Text REtrieval Conference (TREC) QA tracks", "start_pos": 45, "end_pos": 87, "type": "DATASET", "confidence": 0.620278749614954}]}, {"text": "We adopt their experimental setup and next review the line of syntactic-based works which reported results on this data set.", "labels": [], "entities": []}, {"text": "propose a quasi-synchronous grammar formulation which specifies the generation of the question parse tree, loosely conditioned on the parse tree of the candidate answer passage.", "labels": [], "entities": []}, {"text": "Their model showed improvement over previous syntactic models for QA:, who computed similarity between question-answer pairs with a generalized tree-edit distance, and, who developed an information measure for sentence similarity based on dependency paths of aligned words.", "labels": [], "entities": []}, {"text": "reproduced these methods and extended them to utilize WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 54, "end_pos": 61, "type": "DATASET", "confidence": 0.949837327003479}]}], "datasetContent": [{"text": "To evaluate the performance of M-PLM for ranking textual inferences we focused on the task of ranking candidate answer passages for question answering (QA) as presented in Section 5.1.", "labels": [], "entities": [{"text": "question answering (QA)", "start_pos": 132, "end_pos": 155, "type": "TASK", "confidence": 0.8416732490062714}]}, {"text": "Additionally, we demonstrate the added value of our sentence-level model in another ranking experiment based on RTE data sets, described in Section 5.2.", "labels": [], "entities": [{"text": "RTE data sets", "start_pos": 112, "end_pos": 125, "type": "DATASET", "confidence": 0.8888939619064331}]}, {"text": "To assess the added value of our model on an additional ranking evaluation, we utilize the search task data sets of the recent Recognizing Textual Entailment (RTE) benchmarks, which were originally con-structed for the task of entailment classification.", "labels": [], "entities": [{"text": "Recognizing Textual Entailment (RTE)", "start_pos": 127, "end_pos": 163, "type": "TASK", "confidence": 0.574455921848615}, {"text": "entailment classification", "start_pos": 227, "end_pos": 252, "type": "TASK", "confidence": 0.7151841670274734}]}, {"text": "In that task a hypothesis is given with a corpus and the goal is to identify which sentences of the corpus entail the hypothesis.", "labels": [], "entities": []}, {"text": "This setting naturally lends itself to a ranking scenario, in which the desired output is a list of the corpus sentences ranked by their probability to entail the given hypothesis.", "labels": [], "entities": []}, {"text": "To that end, we employed the same methodology as described in the previous section.", "labels": [], "entities": []}, {"text": "Table 3 presents the improvement of our model over HN-PLM, whose classification performance was reported to be on par with best-performing systems on these data sets . As can be seen, the improvement is substantial for both measures on both data sets.", "labels": [], "entities": []}, {"text": "These results further assess the contribution of our Markovian sentence-level model.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results (in %) for the task of answer ranking for  question answering (sorted by MAP).", "labels": [], "entities": [{"text": "answer ranking", "start_pos": 41, "end_pos": 55, "type": "TASK", "confidence": 0.7190185189247131}, {"text": "question answering", "start_pos": 61, "end_pos": 79, "type": "TASK", "confidence": 0.7288320362567902}, {"text": "MAP", "start_pos": 91, "end_pos": 94, "type": "METRIC", "confidence": 0.9576020240783691}]}, {"text": " Table 3: Improvements of our sentence-level model over  HN-PLM. Results (in %) are shown for the last RTE and  for the search task in RTE-5.", "labels": [], "entities": [{"text": "RTE", "start_pos": 103, "end_pos": 106, "type": "DATASET", "confidence": 0.7782055139541626}, {"text": "RTE-5", "start_pos": 135, "end_pos": 140, "type": "DATASET", "confidence": 0.8819189071655273}]}]}