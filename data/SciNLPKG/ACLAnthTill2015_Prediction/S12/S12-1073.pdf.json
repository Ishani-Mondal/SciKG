{"title": [{"text": "ICT:A System Combination for Chinese Semantic Dependency Parsing", "labels": [], "entities": [{"text": "ICT", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.9049201607704163}, {"text": "Chinese Semantic Dependency Parsing", "start_pos": 29, "end_pos": 64, "type": "TASK", "confidence": 0.5183738172054291}]}], "abstractContent": [{"text": "The goal of semantic dependency parsing is to build dependency structure and label semantic relation between ahead and its modifier.", "labels": [], "entities": [{"text": "semantic dependency parsing", "start_pos": 12, "end_pos": 39, "type": "TASK", "confidence": 0.6587371230125427}]}, {"text": "To attain this goal, we concentrate on obtaining better dependency structure to predict better semantic relations, and propose a method to combine the results of three state-of-the-art dependency parsers.", "labels": [], "entities": []}, {"text": "Unfortunately, we made a mistake when we generate the final output that results in a lower score of 56.31% in term of Labeled Attachment Score (LAS), reported by organizers.", "labels": [], "entities": [{"text": "Labeled Attachment Score (LAS)", "start_pos": 118, "end_pos": 148, "type": "METRIC", "confidence": 0.8911281824111938}]}, {"text": "After giving golden testing set, we fix the bug and rerun the evaluation script, this time we obtain the score of 62.8% which is consistent with the results on developing set.", "labels": [], "entities": []}, {"text": "We will report detailed experimental results with correct program as a comparison standard for further research.", "labels": [], "entities": []}], "introductionContent": [{"text": "In this year's Semantic Evaluation Task, the organizers hold a task for Chinese Semantic Dependency Parsing.", "labels": [], "entities": [{"text": "Semantic Evaluation Task", "start_pos": 15, "end_pos": 39, "type": "TASK", "confidence": 0.9208735426266988}, {"text": "Chinese Semantic Dependency Parsing", "start_pos": 72, "end_pos": 107, "type": "TASK", "confidence": 0.5384980142116547}]}, {"text": "The semantic dependency parsing (SDP) is a kind of dependency parsing.", "labels": [], "entities": [{"text": "semantic dependency parsing (SDP)", "start_pos": 4, "end_pos": 37, "type": "TASK", "confidence": 0.7989165882269541}, {"text": "dependency parsing", "start_pos": 51, "end_pos": 69, "type": "TASK", "confidence": 0.7330733835697174}]}, {"text": "It builds a dependency structure fora sentence and labels the semantic relation between ahead and its modifier.", "labels": [], "entities": []}, {"text": "The semantic relations are different from syntactic relations.", "labels": [], "entities": []}, {"text": "They are position independent, e.g., the patient can be before or behind a predicate.", "labels": [], "entities": []}, {"text": "On the other hand, their grains are finer than syntactic relations, e.g., the syntactic subject can be agent or experiencer.", "labels": [], "entities": []}, {"text": "Readers can refer to) for detailed introduction.", "labels": [], "entities": []}, {"text": "Different from most methods proposed in CoNLL-2008 1 and 2009 2 , in which some researchers build a joint model to simultaneously generate dependency structure and its syntactic relations (), here, we first employ several parsers to generate dependency structure and then propose a method to combine their outputs.", "labels": [], "entities": []}, {"text": "After that, we label relation between each head and its modifier via the traversal of this refined parse tree.", "labels": [], "entities": []}, {"text": "The reason why we use a pipeline model while not a joint model is that the number of semantic relations annotated by organizers is more than 120 types, while in the former task is only 21 types.", "labels": [], "entities": []}, {"text": "Compared to the former task, the large number of types will obviously drop the performance of classifier.", "labels": [], "entities": []}, {"text": "On the other hand, the performance of syntactic dependency parsing is approaching to perfect, intuitively, that better dependency structure does help to semantic parsing, thus we can concentrate on improving the accuracy of dependency structure construction.", "labels": [], "entities": [{"text": "syntactic dependency parsing", "start_pos": 38, "end_pos": 66, "type": "TASK", "confidence": 0.6189447641372681}, {"text": "semantic parsing", "start_pos": 153, "end_pos": 169, "type": "TASK", "confidence": 0.7179040312767029}, {"text": "accuracy", "start_pos": 212, "end_pos": 220, "type": "METRIC", "confidence": 0.9973630309104919}, {"text": "dependency structure construction", "start_pos": 224, "end_pos": 257, "type": "TASK", "confidence": 0.6641996204853058}]}, {"text": "The overall framework of our system is illustrated in, where three dependency parsers are employed to generate the dependency structure, and a maximum entropy classifier is used to predict relation for head and its modifier over combined parse tree.", "labels": [], "entities": []}, {"text": "Final experimental results show that our system achieves 80.45% in term of unlabeled attachment score (UAS), and 62.8 % in term of LAS.", "labels": [], "entities": [{"text": "unlabeled attachment score (UAS)", "start_pos": 75, "end_pos": 107, "type": "METRIC", "confidence": 0.8395813902219137}, {"text": "LAS", "start_pos": 131, "end_pos": 134, "type": "METRIC", "confidence": 0.9713882803916931}]}, {"text": "Both of them are higher than the baseline without using system combinational techniques.", "labels": [], "entities": []}, {"text": "In the following of this paper, we will demonstrate the detailed information of our system, and report several experimental results.", "labels": [], "entities": []}], "datasetContent": [{"text": "The given corpus consists of 8301 sentences for training(TR), and 569 sentences for developing(DE).", "labels": [], "entities": [{"text": "DE", "start_pos": 95, "end_pos": 97, "type": "METRIC", "confidence": 0.8836860656738281}]}, {"text": "For tuning parameters, we just use TR portion, while for testing, we combine two parts and retrain the parser to obtain better results.", "labels": [], "entities": [{"text": "TR", "start_pos": 35, "end_pos": 37, "type": "METRIC", "confidence": 0.9888947010040283}]}, {"text": "Surely, we also give results of testing set trained on TR portion for comparison.", "labels": [], "entities": [{"text": "TR", "start_pos": 55, "end_pos": 57, "type": "METRIC", "confidence": 0.6112889647483826}]}, {"text": "In the following of this section, we will report the detailed experimental results both on  developing and testing set.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Syntactic precision of different parsers on devel- oping set.", "labels": [], "entities": [{"text": "precision", "start_pos": 20, "end_pos": 29, "type": "METRIC", "confidence": 0.9110432267189026}]}, {"text": " Table 4: LAS of semantic relations over different parses  on developing set.", "labels": [], "entities": [{"text": "LAS", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9118360280990601}]}, {"text": " Table 5: LAS and UAS on testing set trained on TR.", "labels": [], "entities": [{"text": "LAS", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9899779558181763}, {"text": "UAS", "start_pos": 18, "end_pos": 21, "type": "METRIC", "confidence": 0.5592779517173767}, {"text": "TR", "start_pos": 48, "end_pos": 50, "type": "TASK", "confidence": 0.5683016777038574}]}, {"text": " Table 6: LAS and UAS on testing set trained on TR and  DE.", "labels": [], "entities": [{"text": "LAS", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9911736845970154}, {"text": "UAS", "start_pos": 18, "end_pos": 21, "type": "METRIC", "confidence": 0.6817676424980164}, {"text": "TR", "start_pos": 48, "end_pos": 50, "type": "METRIC", "confidence": 0.49991217255592346}, {"text": "DE", "start_pos": 56, "end_pos": 58, "type": "METRIC", "confidence": 0.9883850812911987}]}]}