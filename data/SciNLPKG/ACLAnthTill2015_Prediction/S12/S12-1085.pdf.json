{"title": [{"text": "sranjans : Semantic Textual Similarity using Maximal Weighted Bipartite Graph Matching", "labels": [], "entities": [{"text": "Maximal Weighted Bipartite Graph Matching", "start_pos": 45, "end_pos": 86, "type": "TASK", "confidence": 0.49530559182167055}]}], "abstractContent": [{"text": "The paper aims to come up with a system that examines the degree of semantic equivalence between two sentences.", "labels": [], "entities": []}, {"text": "At the core of the paper is the attempt to grade the similarity of two sentences by finding the maximal weighted bipartite match between the tokens of the two sentences.", "labels": [], "entities": []}, {"text": "The tokens include single words, or multi-words in case of Named Entitites, adjectivally and numerically modified words.", "labels": [], "entities": []}, {"text": "Two token similarity measures are used for the task-WordNet based similarity, and a statistical word similarity measure which overcomes the shortcomings of WordNet based similarity.", "labels": [], "entities": []}, {"text": "As part of three systems created for the task, we explore a simple bag of words tokenization scheme, a more careful tokenization scheme which captures named entities, times, dates, monetary entities etc., and finally try to capture context around tokens using grammatical dependencies.", "labels": [], "entities": []}], "introductionContent": [{"text": "Semantic Textual Similarity (STS) measures the degree of semantic equivalence between texts.", "labels": [], "entities": [{"text": "Semantic Textual Similarity (STS)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7575670729080836}]}, {"text": "The goal of this task is to create a unified framework for the evaluation of semantic textual similarity modules and to characterize their impact on NLP applications.", "labels": [], "entities": []}, {"text": "The task is part of the Semantic Evaluation 2012 Workshop ().", "labels": [], "entities": [{"text": "Semantic Evaluation 2012 Workshop", "start_pos": 24, "end_pos": 57, "type": "TASK", "confidence": 0.8661247342824936}]}, {"text": "STS is related to both Textual Entailment and Paraphrase, but differs in a number of ways and it is more directly applicable to a number of NLP tasks.", "labels": [], "entities": [{"text": "STS", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.7607860565185547}, {"text": "Textual Entailment", "start_pos": 23, "end_pos": 41, "type": "TASK", "confidence": 0.7815744280815125}]}, {"text": "Also, STS is a graded similarity notionthis graded bidirectional nature of STS is useful for NLP tasks such as MT evaluation, information extraction, question answering, and summarization.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 111, "end_pos": 124, "type": "TASK", "confidence": 0.982590913772583}, {"text": "information extraction", "start_pos": 126, "end_pos": 148, "type": "TASK", "confidence": 0.8460899591445923}, {"text": "question answering", "start_pos": 150, "end_pos": 168, "type": "TASK", "confidence": 0.9002718031406403}, {"text": "summarization", "start_pos": 174, "end_pos": 187, "type": "TASK", "confidence": 0.9908798933029175}]}, {"text": "We propose a lexical similarity approach to grade the similarity of two sentences, where a maximal weighted bipartite match is found between the tokens of the two sentences.", "labels": [], "entities": []}, {"text": "The approach is robust enough to apply across different datasets.", "labels": [], "entities": []}, {"text": "The results on the STS test datasets are encouraging to say the least.", "labels": [], "entities": [{"text": "STS test datasets", "start_pos": 19, "end_pos": 36, "type": "DATASET", "confidence": 0.7905670901139578}]}, {"text": "The tokens are single word tokens in case of the first system, while in the second system, named and monetary entities, percentages, dates and times are handled too.", "labels": [], "entities": []}, {"text": "A token-token similarity measure is integral to the approach and we use both a statistical similarity measure and a WordNet based word similarity measure for the same.", "labels": [], "entities": []}, {"text": "In the final run of the task, apart from capturing the aforementioned entities, we heuristically extract adjectivally and numerically modified words.", "labels": [], "entities": []}, {"text": "Also, the last run naively attempts to capture the context around the tokens using grammatical dependencies, which in turn is used to measure context similarity.", "labels": [], "entities": []}, {"text": "Section 2 discusses the previous work done in this area.", "labels": [], "entities": []}, {"text": "Section 3 describes the datasets, the baseline system and the evaluation measures used by the task organizers.", "labels": [], "entities": []}, {"text": "Section 4, 5 and 6 introduce the systems developed and discuss the results of each system.", "labels": [], "entities": []}, {"text": "Finally, section 7 con-cludes the work and section 8 offers suggestions for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "The development datasets are drawn from the following sources : \u2022 MSR Paraphrase : This dataset consists of pairs of sentences which have been extracted from news sources on the web.", "labels": [], "entities": []}, {"text": "\u2022 MSR Video : This dataset consists of pairs of sentences where each sentence of a pair tries to summarize the action in a short video snippet.", "labels": [], "entities": [{"text": "MSR Video", "start_pos": 2, "end_pos": 11, "type": "DATASET", "confidence": 0.7064413577318192}]}, {"text": "\u2022 SMT Europarl : This dataset consists of pairs sentences drawn from the proceedings of the European Parliament, where each sentence of a pair is a translation from a European language to English.", "labels": [], "entities": [{"text": "SMT", "start_pos": 2, "end_pos": 5, "type": "TASK", "confidence": 0.6901450753211975}, {"text": "Europarl", "start_pos": 6, "end_pos": 14, "type": "DATASET", "confidence": 0.4494287371635437}]}, {"text": "In addition to the above sources, the test datasets also contained the following sources : \u2022 SMT News : This dataset consists of machine translated news conversation sentence pairs.", "labels": [], "entities": [{"text": "SMT News", "start_pos": 93, "end_pos": 101, "type": "TASK", "confidence": 0.802416056394577}]}, {"text": "\u2022 On WN : This dataset consists of pairs of sentences where the first comes from Ontonotes() and the second from a WordNet definition.", "labels": [], "entities": [{"text": "Ontonotes", "start_pos": 81, "end_pos": 90, "type": "DATASET", "confidence": 0.7963030934333801}, {"text": "WordNet definition", "start_pos": 115, "end_pos": 133, "type": "DATASET", "confidence": 0.9255671799182892}]}, {"text": "Hence, the sentences are rather phrases.", "labels": [], "entities": []}, {"text": "The scores obtained by the participating systems are evaluated against the gold standard of the datasets using a pearson correlation measure.", "labels": [], "entities": [{"text": "pearson correlation measure", "start_pos": 113, "end_pos": 140, "type": "METRIC", "confidence": 0.8039709726969401}]}, {"text": "In order to evaluate the overall performance of the systems on all the five datasets, the organizers use three evaluation measures : \u2022 ALL : This measure takes the union of all the test datasets, and finds the Pearson correlation of the system scores with the gold standard of the union.", "labels": [], "entities": [{"text": "ALL", "start_pos": 135, "end_pos": 138, "type": "METRIC", "confidence": 0.9983097314834595}, {"text": "Pearson correlation", "start_pos": 210, "end_pos": 229, "type": "METRIC", "confidence": 0.9686600565910339}]}, {"text": "\u2022 ALL Normalized : In this measure, a linear fit is found for the system scores on each dataset using a least squared error criterion, and then the union of the linearly fitted scores is used to calculate the Pearson correlation against the gold standard union.", "labels": [], "entities": [{"text": "ALL", "start_pos": 2, "end_pos": 5, "type": "METRIC", "confidence": 0.9964985847473145}, {"text": "Pearson correlation", "start_pos": 209, "end_pos": 228, "type": "METRIC", "confidence": 0.9656806886196136}]}, {"text": "\u2022 Weighted Mean : The average of the Pearson correlation scores of the systems on the individual datasets is taken, weighted by the number of test instances in each dataset.", "labels": [], "entities": [{"text": "Weighted Mean", "start_pos": 2, "end_pos": 15, "type": "METRIC", "confidence": 0.9242084622383118}, {"text": "Pearson correlation scores", "start_pos": 37, "end_pos": 63, "type": "METRIC", "confidence": 0.9349311192830404}]}], "tableCaptions": []}