{"title": [{"text": "Selecting Corpus-Semantic Models for Neurolinguistic Decoding", "labels": [], "entities": []}], "abstractContent": [{"text": "Neurosemantics aims to learn the mapping between concepts and the neural activity which they elicit during neuroimaging experiments.", "labels": [], "entities": []}, {"text": "Different approaches have been used to represent individual concepts, but current state-of-the-art techniques require extensive manual intervention to scale to arbitrary words and domains.", "labels": [], "entities": []}, {"text": "To overcome this challenge, we initiate a systematic comparison of automatically-derived corpus representations, based on various types of textual co-occurrence.", "labels": [], "entities": []}, {"text": "We find that dependency parse-based features are the most effective, achieving accuracies similar to the leading semi-manual approaches and higher than any published fora corpus-based model.", "labels": [], "entities": [{"text": "dependency parse-based", "start_pos": 13, "end_pos": 35, "type": "TASK", "confidence": 0.7736511826515198}, {"text": "accuracies", "start_pos": 79, "end_pos": 89, "type": "METRIC", "confidence": 0.9724134206771851}]}, {"text": "We also find that simple word features enriched with directional information provide a close-to-optimal solution at much lower computational cost.", "labels": [], "entities": []}], "introductionContent": [{"text": "The cognitive plausibility of computational models of word meaning has typically been tested using behavioural benchmarks, such as identification of synonyms among close associates (the TOEFL task for language learners, see e.g.; emulating elicited judgments of pairwise similarity (such as; judgments of category membership (e.g.; and word priming effects.", "labels": [], "entities": [{"text": "identification of synonyms among close associates", "start_pos": 131, "end_pos": 180, "type": "TASK", "confidence": 0.8087925016880035}, {"text": "TOEFL", "start_pos": 186, "end_pos": 191, "type": "METRIC", "confidence": 0.8026449084281921}]}, {"text": "introduced anew task in neurosemantic decoding -using models of semantics to learn the mapping between concepts and the neural activity which they elicit during neuroimaging experiments.", "labels": [], "entities": []}, {"text": "This was achieved with a linear model which used training data to find neural basis images that correspond to the assumed semantic dimensions (for instance, one such basis image might be the activity of the brain for words representing animate concepts), and subsequently used these general patterns and known semantic dimensions to infer the fMRI activity that should be elicited by an unseen stimulus concept.", "labels": [], "entities": []}, {"text": "Follow-on work has experimented with other neuroimaging modalities (, and with a range of semantic models including elicited property norms, corpus derived models) and structured ontologies (.", "labels": [], "entities": []}, {"text": "The current state-of-the-art performance on this task is achieved using models that are handtailored in some respect, whether using manual annotation tasks (), use of a domain-appropriate curated corpus), or selection of particular collocates to suit the concepts to be described (.", "labels": [], "entities": []}, {"text": "While these approaches are clearly very successful, it is questionable whether they area general solution to describe the various parts-of-speech and semantic domains that makeup a speaker's vocabulary.", "labels": [], "entities": []}, {"text": "The 25-verb model would probably have to be extended to describe the lexicon at large, and it is unclear whether such a compact model could be maintained.", "labels": [], "entities": []}, {"text": "While) has very broad and increasing cov-erage, it is possible that it will remain inadequate for specialist vocabularies, or for lessstudied languages.", "labels": [], "entities": []}, {"text": "And while the method used by distributes the annotation task efficiently by crowd-sourcing, it still requires that appropriate questions are compiled by researchers, a task that is both difficult to perform in a systematic way, and which may not generalize to more abstract concepts.", "labels": [], "entities": []}, {"text": "In this paper we examine a representative set of corpus-derived models of meaning, that require no manual intervention, and are applicable to any syntactic and semantic domain.", "labels": [], "entities": []}, {"text": "We concentrate on which types of basic corpus pattern perform well on the neurosemantic decoding task: LSA-style word-region co-occurrences, and various HAL-style word-collocate features including raw tokens, POS tags, and a full dependency parse.", "labels": [], "entities": []}, {"text": "Otherwise a common feature extraction and preprocessing pipeline is used: a co-occurrence frequency cutoff, application of a frequency normalization weighting, and dimensionality reduction with SVD.", "labels": [], "entities": []}, {"text": "The following section describes how the brain activity data was gathered and processed; the construction of several corpus-derived models of meaning; and the regression-based methods used to predict one from the other, evaluated with a brain-image matching task ().", "labels": [], "entities": []}, {"text": "In section 3 we report the results, and in the Conclusion we discuss both the practical implications, and what this works suggests for the cognitive plausibility of distributional models of meaning.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Brain activity prediction accuracy on leave- 2-out pair-matching task. A frequency cutoff of 20  was used for all 1000 dimensional models.", "labels": [], "entities": [{"text": "Brain activity prediction", "start_pos": 10, "end_pos": 35, "type": "TASK", "confidence": 0.6224807302157084}, {"text": "accuracy", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.9451815485954285}]}, {"text": " Table 2: Effect of SVD dimensionality in the leave- 2-out pair-matching setting; frequency cutoff of 20.", "labels": [], "entities": [{"text": "frequency cutoff", "start_pos": 82, "end_pos": 98, "type": "METRIC", "confidence": 0.9224830269813538}]}, {"text": " Table 3: Effect of frequency cutoff in the leave-2-out  pair-matching setting; 300 SVD dimensions.", "labels": [], "entities": []}]}