{"title": [{"text": "An Exact Dual Decomposition Algorithm for Shallow Semantic Parsing with Constraints", "labels": [], "entities": [{"text": "Semantic Parsing with Constraints", "start_pos": 50, "end_pos": 83, "type": "TASK", "confidence": 0.7634562999010086}]}], "abstractContent": [{"text": "We present a novel technique for jointly predicting semantic arguments for lexical predicates.", "labels": [], "entities": [{"text": "predicting semantic arguments for lexical predicates", "start_pos": 41, "end_pos": 93, "type": "TASK", "confidence": 0.8045958578586578}]}, {"text": "The task is to find the best matching between semantic roles and sentential spans, subject to structural constraints that come from expert linguistic knowledge (e.g., in the FrameNet lexicon).", "labels": [], "entities": [{"text": "FrameNet lexicon", "start_pos": 174, "end_pos": 190, "type": "DATASET", "confidence": 0.8908843994140625}]}, {"text": "We formulate this task as an integer linear program (ILP); instead of using an off-the-shelf tool to solve the ILP, we employ a dual decomposition algorithm, which we adapt for exact decoding via a branch-and-bound technique.", "labels": [], "entities": []}, {"text": "Compared to a baseline that makes local predictions, we achieve better argument identification scores and avoid all structural violations.", "labels": [], "entities": [{"text": "argument identification", "start_pos": 71, "end_pos": 94, "type": "TASK", "confidence": 0.6863072663545609}]}, {"text": "Runtime is nine times faster than a proprietary ILP solver.", "labels": [], "entities": [{"text": "ILP solver", "start_pos": 48, "end_pos": 58, "type": "TASK", "confidence": 0.6670628786087036}]}], "introductionContent": [{"text": "Semantic knowledge is often represented declaratively in resources created by linguistic experts.", "labels": [], "entities": []}, {"text": "In this work, we strive to exploit such knowledge in a principled, unified, and intuitive way.", "labels": [], "entities": []}, {"text": "An example resource where a wide variety of knowledge has been encoded over along period of time is the FrameNet lexicon (, which suggests an analysis based on frame semantics.", "labels": [], "entities": [{"text": "FrameNet lexicon", "start_pos": 104, "end_pos": 120, "type": "DATASET", "confidence": 0.9128227233886719}]}, {"text": "This resource defines hundreds of semantic frames.", "labels": [], "entities": []}, {"text": "Each frame represents a gestalt event or scenario, and is associated with several semantic roles, which serve as participants in the event that the frame signifies (see for an example).", "labels": [], "entities": []}, {"text": "Along with storing the above data, FrameNet also provides a hierarchy of relationships between frames, and semantic relationships between pairs of roles.", "labels": [], "entities": []}, {"text": "In prior NLP research using FrameNet, these interactions have been largely ignored, though they have the potential to improve the quality and consistency of semantic analysis.", "labels": [], "entities": [{"text": "semantic analysis", "start_pos": 157, "end_pos": 174, "type": "TASK", "confidence": 0.7722796499729156}]}, {"text": "In this paper, we present an algorithm that finds the full collection of arguments of a predicate given its semantic frame.", "labels": [], "entities": []}, {"text": "Although we work within the conventions of FrameNet, our approach is generalizable to other semantic role labeling (SRL) frameworks.", "labels": [], "entities": [{"text": "semantic role labeling (SRL)", "start_pos": 92, "end_pos": 120, "type": "TASK", "confidence": 0.7942893405755361}]}, {"text": "We model this argument identification task as constrained optimization, where the constraints come from expert knowledge encoded in a lexicon.", "labels": [], "entities": [{"text": "argument identification task", "start_pos": 14, "end_pos": 42, "type": "TASK", "confidence": 0.8066713015238444}]}, {"text": "Following prior work on PropBank-style SRL) that dealt with similar constrained problems (, inter alia), we incorporate this declarative knowledge in an integer linear program (ILP).", "labels": [], "entities": [{"text": "PropBank-style SRL", "start_pos": 24, "end_pos": 42, "type": "DATASET", "confidence": 0.8012820482254028}]}, {"text": "Because general-purpose ILP solvers are proprietary and do not fully exploit the structure of the problem, we turn to a class of optimization techniques called dual decomposition ().", "labels": [], "entities": [{"text": "ILP solvers", "start_pos": 24, "end_pos": 35, "type": "TASK", "confidence": 0.902826189994812}, {"text": "dual decomposition", "start_pos": 160, "end_pos": 178, "type": "TASK", "confidence": 0.7503982782363892}]}, {"text": "We derive a modular, extensible, parallelizable approach in which semantic constraints map not just to declarative components of the algorithm, but also to procedural ones, in the form of \"workers.\"", "labels": [], "entities": []}, {"text": "While dual decomposition algorithms only solve a relaxation of the original problem, we make a novel contribution by wrapping the algorithm in a branch-andbound search procedure, resulting inexact solutions.", "labels": [], "entities": []}, {"text": "We experimentally find that our algorithm achieves accuracy comparable to a state-of-the-art system, while respecting all imposed linguistic constraints.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.999196469783783}]}, {"text": "In comparison to inexact beam search that violates many of these constraints, our exact decoder has less than twice the runtime; furthermore, it decodes nine times faster than CPLEX, a state-of-theart, proprietary, general-purpose exact ILP solver.", "labels": [], "entities": [{"text": "ILP solver", "start_pos": 237, "end_pos": 247, "type": "TASK", "confidence": 0.6263529658317566}]}, {"text": "Austria , once expected to waltz smoothly into the European Union , is elbowing its partners , treading on toes and pogo-dancing in a most un-Viennese manner .", "labels": [], "entities": []}], "datasetContent": [{"text": "In our experiments, we use FrameNet 1.5, which contains a lexicon of 877 frames and 1,068 role labels, and 78 documents with multiple predicateargument annotations (a superset of the SemEval shared task dataset;.", "labels": [], "entities": [{"text": "SemEval shared task dataset", "start_pos": 183, "end_pos": 210, "type": "DATASET", "confidence": 0.63141680508852}]}, {"text": "We used the same split as, with 55 documents for training (containing 19,582 frame annotations) and 23 for testing (with 4,458 annotations).", "labels": [], "entities": []}, {"text": "We randomly selected 4,462 predicates in the training set as development data.", "labels": [], "entities": []}, {"text": "The raw sentences in all the training and test documents were preprocessed using MXPOST and the MST dependency parser).", "labels": [], "entities": [{"text": "MXPOST", "start_pos": 81, "end_pos": 87, "type": "DATASET", "confidence": 0.7980409860610962}]}, {"text": "The state-of-the-art system for this task is SE-MAFOR, an open source tool () that provides a baseline benchmark for our new algorithm.", "labels": [], "entities": [{"text": "SE-MAFOR", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.7615470290184021}]}, {"text": "We use the components of SEMAFOR as-is to define the features hand train the weights \u03c8 used in the scoring function c.", "labels": [], "entities": [{"text": "SEMAFOR", "start_pos": 25, "end_pos": 32, "type": "DATASET", "confidence": 0.4746326506137848}]}, {"text": "We also use its heuristic mechanism to find potential spans St fora given predicate t.", "labels": [], "entities": []}, {"text": "SEMAFOR learns weights using 2 -penalized log-likelihood; we augmented its dev set-tuning procedure to tune both the regularization strength and the AD 3 penalty strength \u03c1.", "labels": [], "entities": [{"text": "AD 3 penalty strength", "start_pos": 149, "end_pos": 170, "type": "METRIC", "confidence": 0.9119395762681961}]}, {"text": "We initialize \u03c1 = 0.1 and follow in dynamically adjusting it.", "labels": [], "entities": []}, {"text": "Note that we do not use SEMAFOR's automatic frame identification component in our presented experiments, as we assume that we have gold frames on each predicate.", "labels": [], "entities": [{"text": "SEMAFOR's automatic frame identification", "start_pos": 24, "end_pos": 64, "type": "TASK", "confidence": 0.4989235162734985}]}, {"text": "This lets us compare the different argument identification methods in a controlled fashion.", "labels": [], "entities": [{"text": "argument identification", "start_pos": 35, "end_pos": 58, "type": "TASK", "confidence": 0.7390857338905334}]}], "tableCaptions": [{"text": " Table 1: Comparison of decoding strategies in  \u00a74.2. We evaluate in terms of precision, recall and F 1 score on a test  set containing 4,458 predicates. We also compute the number of structural violations each model makes: number  of overlapping arguments and violations of the \"requires\" and \"excludes\" constraints of  \u00a72. Finally decoding time  (without feature computation steps) on the whole test set is shown in the last column averaged over 5 runs.", "labels": [], "entities": [{"text": "precision", "start_pos": 78, "end_pos": 87, "type": "METRIC", "confidence": 0.9996482133865356}, {"text": "recall", "start_pos": 89, "end_pos": 95, "type": "METRIC", "confidence": 0.9994961023330688}, {"text": "F 1 score", "start_pos": 100, "end_pos": 109, "type": "METRIC", "confidence": 0.9892553687095642}]}]}