{"title": [{"text": "Modelling selectional preferences in a lexical hierarchy", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper describes Bayesian selectional preference models that incorporate knowledge from a lexical hierarchy such as WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 120, "end_pos": 127, "type": "DATASET", "confidence": 0.9678998589515686}]}, {"text": "Inspired by previous work on modelling with WordNet, these approaches are based either on \"cutting\" the hierarchy at an appropriate level of generalisation or on a \"walking\" model that selects a path from the root to a leaf.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 44, "end_pos": 51, "type": "DATASET", "confidence": 0.9174569249153137}]}, {"text": "In an evaluation comparing against human plau-sibility judgements, we show that the models presented here outperform previously proposed comparable WordNet-based models, are competitive with state-of-the-art selectional preference models and are particularly well-suited to estimating plausibility for items that were not seen in training.", "labels": [], "entities": []}], "introductionContent": [{"text": "The concept of selectional preference captures the intuitive fact that predicates in language have a better semantic \"fit\" for certain arguments than others.", "labels": [], "entities": []}, {"text": "For example, the direct object argument slot of the verb eat is more plausibly filled by a type of food (I ate a pizza) than by a type of vehicle (I ate a car), while the subject slot of the verb laugh is more plausibly filled by a person than by a vegetable.", "labels": [], "entities": []}, {"text": "Human language users' knowledge about selectional preferences has been implicated in analyses of metaphor processing and in psycholinguistic studies of comprehension).", "labels": [], "entities": [{"text": "metaphor processing", "start_pos": 97, "end_pos": 116, "type": "TASK", "confidence": 0.9153709709644318}]}, {"text": "In Natural Language Processing, automatically acquired preference models have been shown to aid a number of tasks, including semantic role labelling (), parsing (Zhou et al., 2011) and lexical disambiguation).", "labels": [], "entities": [{"text": "semantic role labelling", "start_pos": 125, "end_pos": 148, "type": "TASK", "confidence": 0.6609051724274954}, {"text": "parsing", "start_pos": 153, "end_pos": 160, "type": "TASK", "confidence": 0.9651058316230774}]}, {"text": "It is tempting to assume that with a large enough corpus, preference learning reduces to a simple language modelling task that can be solved by counting predicate-argument co-occurrences.", "labels": [], "entities": [{"text": "preference learning", "start_pos": 58, "end_pos": 77, "type": "TASK", "confidence": 0.8063845932483673}]}, {"text": "Indeed, show that relatively good performance at plausibility estimation can be attained by submitting queries to a Web search engine.", "labels": [], "entities": []}, {"text": "However, there are many scenarios where this approach is insufficient: for languages and language domains where Web-scale data is unavailable, for predicate types (e.g., inference rules or semantic roles) that cannot be retrieved by keyword search and for applications where accurate models of rarer words are required.", "labels": [], "entities": []}, {"text": "shows that the Webbased approach is reliably outperformed by more complex models trained on smaller corpora for less frequent predicate-argument combinations.", "labels": [], "entities": []}, {"text": "Models that induce a level of semantic representation, such as probabilistic latent variable models, have a further advantage in that they can provide rich structured information for downstream tasks such as lexical disambiguation) and semantic relation mining).", "labels": [], "entities": [{"text": "semantic relation mining", "start_pos": 236, "end_pos": 260, "type": "TASK", "confidence": 0.6795158386230469}]}, {"text": "Recent research has investigated the potential of Bayesian probabilistic models such as Latent Dirichlet Allocation (LDA) for modelling selectional preferences).", "labels": [], "entities": [{"text": "Latent Dirichlet Allocation (LDA", "start_pos": 88, "end_pos": 120, "type": "METRIC", "confidence": 0.859981894493103}]}, {"text": "These models are flexible and robust, yielding superior performance compared to previous approaches.", "labels": [], "entities": []}, {"text": "In this paper we present a preliminary study of analogous models that make use of a lexical hierarchy (in our case the WordNet hierarchy).", "labels": [], "entities": []}, {"text": "We describe two broad classes of probabilistic models over WordNet and how they can be implemented in a Bayesian framework.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 59, "end_pos": 66, "type": "DATASET", "confidence": 0.9425627589225769}]}, {"text": "The two main potential advantages of incorporating WordNet information are: (a) improved predictions about rare and out-of-vocabulary arguments; (b) the ability to perform syntactic word sense disambiguation with a principled probabilistic model and without the need for an additional step that heuristically maps latent variables onto WordNet senses.", "labels": [], "entities": [{"text": "syntactic word sense disambiguation", "start_pos": 172, "end_pos": 207, "type": "TASK", "confidence": 0.6245959624648094}]}, {"text": "Focussing hereon (a), we demonstrate that our models attain better performance than previously-proposed WordNet-based methods on a plausibility estimation task and are particularly wellsuited to estimating plausibility for arguments that were not seen in training and for which LDA cannot make useful predictions.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our methods by comparing their predictions to human judgements of predicate-argument plausibility.", "labels": [], "entities": []}, {"text": "This is a standard approach to selectional preference evaluation ( and arguably yields a better appraisal of a model's intrinsic semantic quality than other evaluations such as pseudo-disambiguation or held-out likelihood prediction.", "labels": [], "entities": [{"text": "selectional preference evaluation", "start_pos": 31, "end_pos": 64, "type": "TASK", "confidence": 0.7026504079500834}]}, {"text": "We use a set of plausibility judgements collected by.", "labels": [], "entities": []}, {"text": "This dataset comprises 180 predicateargument combinations for each of three syntactic relations: verb-object, noun-noun modification and adjective-noun modification.", "labels": [], "entities": []}, {"text": "The data for each relation is divided into a \"seen\" portion containing 90 combinations that were observed in the British National Corpus and an \"unseen\" portion containing 90 combinations that do not appear (though the predicates and arguments do appear separately).", "labels": [], "entities": [{"text": "British National Corpus", "start_pos": 113, "end_pos": 136, "type": "DATASET", "confidence": 0.918314258257548}]}, {"text": "Plausibility judgements were elicited from a large group of human subjects, then normalised and logtransformed.", "labels": [], "entities": []}, {"text": "gives a representative illustration of the data.", "labels": [], "entities": []}, {"text": "Following the evaluation in\u00b4Oin\u00b4 in\u00b4O S\u00e9aghdha (2010), with which we wish to compare, we use Pearson rand Spearman \u03c1 correlation coefficients as performance measures.", "labels": [], "entities": []}, {"text": "All models were trained on the 90-million word written component of the British National Corpus, 3 lemmatised, POS-tagged and parsed with the RASP toolkit ().", "labels": [], "entities": [{"text": "British National Corpus", "start_pos": 72, "end_pos": 95, "type": "DATASET", "confidence": 0.9331953326861063}, {"text": "RASP toolkit", "start_pos": 142, "end_pos": 154, "type": "DATASET", "confidence": 0.8773392736911774}]}, {"text": "We removed predicates occurring with just one argument type and all tokens containing non-alphabetic characters.", "labels": [], "entities": []}, {"text": "The resulting datasets consist of 3,587,172 verbobject observations (7,954 predicate types, 80,107 argument types), 3,732,470 noun-noun observations (68,303 predicate types, 105,425 argument types) and 3,843,346 adjective-noun observations (29,975 predicate types, 62,595 argument types).", "labels": [], "entities": []}, {"text": "All the Bayesian models were trained by Gibbs sampling, as outlined above.", "labels": [], "entities": []}, {"text": "For each model we run three sampling chains for 1,000 iterations and average the plausibility predictions for each to produce a final prediction P (w|p) for each predicate-argument item.", "labels": [], "entities": []}, {"text": "As the evaluation demands an estimate of the joint probability P (w, p) we multiply the predicted P (w|p) by a predicate probability P (p|r) estimated from relative corpus frequencies.", "labels": [], "entities": []}, {"text": "In training we use a burn-in period of 200 iterations, after which hyperparameters are reestimated and P (p|r) predictions are sampled every 50 iterations.", "labels": [], "entities": [{"text": "P (p|r) predictions", "start_pos": 103, "end_pos": 122, "type": "METRIC", "confidence": 0.8243227260453361}]}, {"text": "All probability estimates are log-transformed to match the gold standard judgements.", "labels": [], "entities": []}, {"text": "In order to compare against previously proposed selectional preference approaches based on WordNet we also reimplemented the methods that performed best in the evaluation of: and.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 91, "end_pos": 98, "type": "DATASET", "confidence": 0.9769758582115173}]}, {"text": "For Resnik's model we used WordNet 2.1 rather than WordNet 3.0 as the former has multiple roots, a property that turns out to be necessary for good performance.", "labels": [], "entities": []}, {"text": "Clark and Weir's method requires that the user specify a significance threshold \u03b1 to be used in deciding whereto cut; to give it the best possible chance we tested with a range of values (0.05, 0.3, 0.6, 0.9) and report results for the best-performing setting, which consistently was \u03b1 = 0.9.", "labels": [], "entities": []}, {"text": "One can also use different statistical hypothesis tests; again we choose the test giving the best results, which was Pearson's chi-squared test.", "labels": [], "entities": []}, {"text": "As this method produces a probability estimate conditioned on the predicate p we multiply by a MLE estimate of P (p|r) and log-transform the result.", "labels": [], "entities": [{"text": "MLE estimate", "start_pos": 95, "end_pos": 107, "type": "METRIC", "confidence": 0.9574152827262878}]}, {"text": "eat food#n#1, aliment#n#1, entity#n#1, solid#n#1, food#n#2 drink fluid#n#1, liquid#n#1, entity#n#1, alcohol#n#1, beverage#n#1 appoint individual#n#1, entity#n#1, chief#n#1, being#n#2, expert#n#1 publish abstract entity#n#1, piece of writing#n#1, communication#n#2, publication#n#1   Bayesian models do follow close behind.", "labels": [], "entities": []}, {"text": "This may suggest that the incorporation of WordNet structure into the model in itself provides much of the clustering benefit provided by an additional layer of \"topic\" latent variables.", "labels": [], "entities": []}, {"text": "In order to test the ability of the WordNet-based models to make predictions about arguments that are absent from the training vocabulary, we created an artificial out-of-vocabulary dataset by removing each of the Keller and Lapata argument words from the input corpus and retraining.", "labels": [], "entities": []}, {"text": "An LDA selectional preference model will completely fail here, but we hope that the WordNet models can still make relatively accurate predictions by leveraging the additional lexical knowledge provided by the hierarchy.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 84, "end_pos": 91, "type": "DATASET", "confidence": 0.9283802509307861}]}, {"text": "For example, if one knows that a tomatillo is classed as a vegetable in WordNet, one can predict a relatively high probability that it can be eaten, even though the word tomatillo does not appear in the BNC.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 72, "end_pos": 79, "type": "DATASET", "confidence": 0.9555630683898926}, {"text": "BNC", "start_pos": 203, "end_pos": 206, "type": "DATASET", "confidence": 0.9617077112197876}]}, {"text": "As a baseline we use a BNC-trained model that  predicts P (w, p) proportional to the MLE predicate probability P (p); a distributional LDA model will make essentially the same prediction.", "labels": [], "entities": [{"text": "MLE predicate probability P (p)", "start_pos": 85, "end_pos": 116, "type": "METRIC", "confidence": 0.8995830842426845}]}, {"text": "Clark and Weir's method does not have full coverage; if no sense s of an argument appears in the data then P (s|p) is zero for all senses and the resulting prediction is zero, which cannot be log-transformed.", "labels": [], "entities": []}, {"text": "To sidestep this issue, unseen senses are assigned a pseudofrequency of 0.1.", "labels": [], "entities": []}, {"text": "Results for this \"forced-OOV\" task are presented in.", "labels": [], "entities": []}, {"text": "WN-CUT proves the most adept at generalising to unseen arguments, attaining the best performance on 7 of 12 dataset/evaluation conditions and a statistically significant improvement over the baseline on 6.", "labels": [], "entities": [{"text": "WN-CUT", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.931334376335144}, {"text": "generalising to unseen arguments", "start_pos": 32, "end_pos": 64, "type": "TASK", "confidence": 0.861870750784874}]}, {"text": "We observe that estimating the plausibility of unseen arguments for noun-noun modifiers is particularly difficult.", "labels": [], "entities": []}, {"text": "One obvious explanation is that the training data for this relation has fewer tokens per predicate, making it more difficult to learn their preferences.", "labels": [], "entities": []}, {"text": "A second, more hypothetical, explanation is that the ontological structure of WordNet is a relatively poor fit for the preferences of nominal modifiers; it is well-known that almost any pair of nouns can combine to produce a minimally plausible nounnoun compound and it maybe that this behaviour is ill-suited by the assumption that preferences are sparse distributions over regions of WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 78, "end_pos": 85, "type": "DATASET", "confidence": 0.9570257067680359}, {"text": "WordNet", "start_pos": 386, "end_pos": 393, "type": "DATASET", "confidence": 0.9652462601661682}]}], "tableCaptions": [{"text": " Table 1: Extract from the noun-noun section of Keller and  Lapata's (2003) dataset, with human plausibility scores", "labels": [], "entities": [{"text": "Keller and  Lapata's (2003) dataset", "start_pos": 48, "end_pos": 83, "type": "DATASET", "confidence": 0.5867566987872124}]}, {"text": " Table 2: Most probable cuts learned by WN-CUT for the object argument of selected verbs", "labels": [], "entities": []}, {"text": " Table 3: Results (Pearson r and Spearman \u03c1 correlations) on Keller and Lapata's (2003) plausibility data; underlining  denotes the best-performing WordNet-based model, boldface denotes the overall best performance", "labels": [], "entities": [{"text": "Pearson r and Spearman \u03c1 correlations", "start_pos": 19, "end_pos": 56, "type": "METRIC", "confidence": 0.7995954503615698}]}, {"text": " Table 4: Forced-OOV results (Pearson r and Spearman \u03c1 correlations) on Keller and Lapata's (2003) plausibility data", "labels": [], "entities": [{"text": "Pearson r and Spearman \u03c1 correlations", "start_pos": 30, "end_pos": 67, "type": "METRIC", "confidence": 0.8205637882153193}]}]}