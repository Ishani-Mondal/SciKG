{"title": [{"text": "Penn: Using Word Similarities to better Estimate Sentence Similarity", "labels": [], "entities": [{"text": "Penn", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9227116703987122}, {"text": "Estimate Sentence Similarity", "start_pos": 40, "end_pos": 68, "type": "TASK", "confidence": 0.6975179513295492}]}], "abstractContent": [{"text": "We present the Penn system for SemEval-2012 Task 6, computing the degree of semantic equivalence between two sentences.", "labels": [], "entities": [{"text": "SemEval-2012 Task 6", "start_pos": 31, "end_pos": 50, "type": "TASK", "confidence": 0.8651313980420431}]}, {"text": "We explore the contributions of different vector models for computing sentence and word similarity: Collobert and Weston embeddings as well as two novel approaches, namely eigen-words and selectors.", "labels": [], "entities": [{"text": "computing sentence and word similarity", "start_pos": 60, "end_pos": 98, "type": "TASK", "confidence": 0.5493457198143006}]}, {"text": "These embeddings provide different measures of distributional similarity between words, and their contexts.", "labels": [], "entities": []}, {"text": "We used regression to combine the different similarity measures, and found that each provides partially independent predictive signal above baseline models.", "labels": [], "entities": []}], "introductionContent": [{"text": "We compute the semantic similarity between pairs of sentences by combining a set of similarity metrics at various levels of depth, from surface word similarity to similarities derived from vector models of word or sentence meaning.", "labels": [], "entities": []}, {"text": "Regression is then used to determine optimal weightings of the different similarity measures.", "labels": [], "entities": [{"text": "Regression", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9810701608657837}]}, {"text": "We use this setting to assess the contributions from several different word embeddings.", "labels": [], "entities": []}, {"text": "Our system is based on similarities computed using multiple sets of features: (a) naive lexical features, (b) similarity between vector representations of sentences, and (c) similarity between constituent words computed using WordNet, using the eigenword vector representations of words , and using selectors, which generalize words to a set of words that appear in the same context.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 226, "end_pos": 233, "type": "DATASET", "confidence": 0.9623216390609741}]}], "datasetContent": [{"text": "We combine the similarity metrics discussed previously via regression).", "labels": [], "entities": []}, {"text": "We included the following sets of features: \u2022 System-baseline: surface metrics, knowledgebased metrics.", "labels": [], "entities": []}, {"text": "(discussed in section 2.4).", "labels": [], "entities": []}, {"text": "\u2022 Neu: Neural Model similarity (section 2.1) \u2022 Ew: Eigenword similarity (section 2.2) \u2022 Sel: Selector similarity (section 2.3) To capture possible non-linear relations, we added a squared and square-rooted column corresponding to each feature in the feature matrix.", "labels": [], "entities": []}, {"text": "We also tried to combine all the features to form composite measures by defining multiple interaction terms.", "labels": [], "entities": []}, {"text": "Both these sets of additional features improved the performance of our regression model.", "labels": [], "entities": []}, {"text": "We used all features to train both a linear regression model and a regularized model based on ridge regression.", "labels": [], "entities": []}, {"text": "The regularization parameter for ridge regression was set via cross-validation over the training set.", "labels": [], "entities": [{"text": "ridge regression", "start_pos": 33, "end_pos": 49, "type": "TASK", "confidence": 0.8595971167087555}]}, {"text": "All predictions of similarity values were capped within the range.", "labels": [], "entities": [{"text": "similarity", "start_pos": 19, "end_pos": 29, "type": "METRIC", "confidence": 0.9670135974884033}]}, {"text": "Our systems were trained on the following data sets: \u2022 MSR-Paraphrase, Microsoft Research Paraphrase Corpus-750 pairs of sentences.", "labels": [], "entities": [{"text": "Microsoft Research Paraphrase Corpus-750", "start_pos": 71, "end_pos": 111, "type": "DATASET", "confidence": 0.8846582323312759}]}, {"text": "\u2022 MSR-Video, Microsoft Research Video Description Corpus-750 pairs of sentences.", "labels": [], "entities": [{"text": "Microsoft Research Video Description Corpus-750", "start_pos": 13, "end_pos": 60, "type": "DATASET", "confidence": 0.8308968663215637}]}, {"text": "\u2022 SMT-Europarl, WMT2008 development data set (Europarl section)-734 pairs of sentences.", "labels": [], "entities": [{"text": "SMT-Europarl", "start_pos": 2, "end_pos": 14, "type": "TASK", "confidence": 0.7154837250709534}, {"text": "WMT2008 development data set", "start_pos": 16, "end_pos": 44, "type": "DATASET", "confidence": 0.8762296587228775}, {"text": "Europarl section)-734", "start_pos": 46, "end_pos": 67, "type": "DATASET", "confidence": 0.9073921740055084}]}, {"text": "Our performance in the official submission for the SemEval task can be seen in.", "labels": [], "entities": [{"text": "SemEval task", "start_pos": 51, "end_pos": 63, "type": "TASK", "confidence": 0.9275383949279785}]}, {"text": "LReg indicates the run with linear regression, ELReg adds the eigenwords feature and ERReg also uses eigenwords but with ridge regression.", "labels": [], "entities": [{"text": "ELReg", "start_pos": 47, "end_pos": 52, "type": "DATASET", "confidence": 0.8672859072685242}]}, {"text": "At the time of submission, we were not ready to test with the selector features yet.", "labels": [], "entities": []}, {"text": "Ridge regression consistently outperformed linear regression for every run of our system, but overall Pearson score for our system using linear regression scored the highest.", "labels": [], "entities": [{"text": "Ridge regression", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.7287393808364868}, {"text": "Pearson score", "start_pos": 102, "end_pos": 115, "type": "METRIC", "confidence": 0.9869672656059265}]}, {"text": "In the aggregate, we see that each of the similarity metrics has the ability to improve results when used with the right combination of other features.", "labels": [], "entities": []}, {"text": "For example, while selector similarity by itself does not seem to help overall, using this metric in conjunction with the neural model of similarity gives us our best results.", "labels": [], "entities": []}, {"text": "Interestingly, the opposite is true of eigenword similarity, where the best results are seen when they are independent of selectors or the neural models.", "labels": [], "entities": [{"text": "eigenword similarity", "start_pos": 39, "end_pos": 59, "type": "TASK", "confidence": 0.800212025642395}]}, {"text": "The decreased correlations can be accounted for by the new features introducing over fitting, and one should note that no such reductions in performance are significant compared to the baseline, whereas our best performance is a significant (p < 0.05) improvement.", "labels": [], "entities": []}, {"text": "There area few potential directions for future improvements.", "labels": [], "entities": []}, {"text": "We did not tune our system differently for different data sets although there is evidence of specific features favoring certain data sets.", "labels": [], "entities": []}, {"text": "In the case of the neural model of similarity we expect that deriving phrase level representations from the sentences and utilizing the dynamic pooling layer should give us a more thorough measure of similarity beyond the sentence-level vectors we used in this work.", "labels": [], "entities": []}, {"text": "For eigenwords, we would like to experiment with context-aware vectors as was described in).", "labels": [], "entities": []}, {"text": "Lastly, we were only able to acquire selectors for nouns, but we believe introducing selectors for other parts of speech will increase the power of the selector similarity metric.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Pearson's r scores for the official submission. ALLnrm: Pearson correlation after the system outputs for each  dataset are fitted to the gold standard using least squares, and corresponding rank. Mean: Weighted mean across the  5 datasets, where the weight depends on the number of pairs in the dataset. ALL: Pearson correlation with the gold  standard for the five datasets, and corresponding rank. Parentheses indicate official rank out of 87 systems.", "labels": [], "entities": [{"text": "ALLnrm", "start_pos": 58, "end_pos": 64, "type": "METRIC", "confidence": 0.9871131777763367}, {"text": "Pearson correlation", "start_pos": 66, "end_pos": 85, "type": "METRIC", "confidence": 0.894649475812912}, {"text": "Mean", "start_pos": 206, "end_pos": 210, "type": "METRIC", "confidence": 0.9740111827850342}, {"text": "ALL", "start_pos": 314, "end_pos": 317, "type": "METRIC", "confidence": 0.9791416525840759}, {"text": "Pearson correlation", "start_pos": 319, "end_pos": 338, "type": "METRIC", "confidence": 0.9404052197933197}]}, {"text": " Table 2: Pearson's r scores for runs based on various combinations of features. Mean: Weighted mean across the 5  datasets, where the weight depends on the number of pairs in the dataset. ALL: Pearson correlation with the gold  standard for the five datasets, and corresponding rank.", "labels": [], "entities": [{"text": "Pearson's r scores", "start_pos": 10, "end_pos": 28, "type": "METRIC", "confidence": 0.7158916145563126}, {"text": "Mean", "start_pos": 81, "end_pos": 85, "type": "METRIC", "confidence": 0.992463231086731}, {"text": "ALL", "start_pos": 189, "end_pos": 192, "type": "METRIC", "confidence": 0.9971309304237366}, {"text": "Pearson correlation", "start_pos": 194, "end_pos": 213, "type": "METRIC", "confidence": 0.8249085247516632}]}]}