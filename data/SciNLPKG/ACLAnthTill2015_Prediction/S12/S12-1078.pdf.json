{"title": [{"text": "LIMSI: Learning Semantic Similarity by Selecting Random Word Subsets", "labels": [], "entities": [{"text": "Learning Semantic Similarity", "start_pos": 7, "end_pos": 35, "type": "TASK", "confidence": 0.6571632325649261}]}], "abstractContent": [{"text": "We propose a semantic similarity learning method based on Random Indexing (RI) and ranking with boosting.", "labels": [], "entities": [{"text": "semantic similarity learning", "start_pos": 13, "end_pos": 41, "type": "TASK", "confidence": 0.7256218393643697}, {"text": "Random Indexing (RI", "start_pos": 58, "end_pos": 77, "type": "METRIC", "confidence": 0.8605965971946716}]}, {"text": "Unlike classical RI, we use only those context vector features that are informative for the semantics modeled.", "labels": [], "entities": []}, {"text": "Despite ignoring text preprocessing and dispensing with semantic resources, the approach was ranked as high as 22nd among 89 participants in the SemEval-2012 Task6: Semantic Textual Similarity.", "labels": [], "entities": [{"text": "SemEval-2012 Task6: Semantic Textual Similarity", "start_pos": 145, "end_pos": 192, "type": "TASK", "confidence": 0.8041417002677917}]}], "introductionContent": [{"text": "One of the popular and flexible tools of semantics modeling are vector distributional representations of texts (also known as vector space models, semantic word spaces or distributed representations).", "labels": [], "entities": [{"text": "semantics modeling", "start_pos": 41, "end_pos": 59, "type": "TASK", "confidence": 0.9038305580615997}]}, {"text": "The principle idea behind vector space models is to use word usage statistics in different contexts to generate a high-dimensional vector representations for each word.", "labels": [], "entities": []}, {"text": "Words are represented by context vectors whose closeness in the vector space is postulated to reflect semantic similarity).", "labels": [], "entities": []}, {"text": "The approach rests upon the distributional hypothesis: words with similar meanings or functions tend to appear in similar contexts.", "labels": [], "entities": []}, {"text": "The prominent examples of vector space models are Latent Semantic Analysis (or Indexing)) and Random Indexing ().", "labels": [], "entities": [{"text": "Latent Semantic Analysis", "start_pos": 50, "end_pos": 74, "type": "TASK", "confidence": 0.5663142204284668}]}, {"text": "Because of the heuristic nature of distributional methods, they are often designed with a specific semantic relation in mind (synonymy, paraphrases, contradiction, etc.).", "labels": [], "entities": []}, {"text": "This complicates their adaption to other application domains and tasks, requiring manual trial-and-error feature redesigns and tailored preprocessing steps to remove morphology/syntax variations that are not supposed to contribute to the semantics facet in question (e.g., stemming, stopwords).", "labels": [], "entities": []}, {"text": "Further, assessing closeness of semantic vectors is usually based on a fixed simple similarity function between distributed representations (often, the cosine function).", "labels": [], "entities": []}, {"text": "The cosine function implicitly assigns equal weights to each component of the semantic vectors regardless of its importance for the particular semantic relation and task.", "labels": [], "entities": []}, {"text": "Finally, during production of training and evaluation sets, the continuum of possible grades of semantic similarity is usually substituted with several integer values, although often only the relative grade order matters and not their absolute values.", "labels": [], "entities": []}, {"text": "Trying to reproduce the same values or the same gaps between grades when designing a semantic representation scheme may introduce an unnecessary bias.", "labels": [], "entities": []}, {"text": "In this paper we address all of the above drawbacks and present a semantic similarity learning method based on Random Indexing.", "labels": [], "entities": [{"text": "semantic similarity learning", "start_pos": 66, "end_pos": 94, "type": "TASK", "confidence": 0.7000723878542582}]}, {"text": "It does not require manual feature design, and is automatically adapted to the specific semantic relations by selecting needed important features and/or learning necessary feature transformations before calculating similarity.", "labels": [], "entities": []}, {"text": "In the proof-of-concept experiments on the SemEval-2012 data we deliberately ignored all routine preprocessing steps, that are often considered obligatory in semantic text processing, we did not use any of the semantic resources (like WordNet) nor trained different models for different data domains/types.", "labels": [], "entities": [{"text": "SemEval-2012 data", "start_pos": 43, "end_pos": 60, "type": "DATASET", "confidence": 0.7942103743553162}]}, {"text": "Despite such over-constrained setting, the method showed very positive performance and was ranked as high as 22nd among 89 participants.", "labels": [], "entities": []}], "datasetContent": [{"text": "We learned context vectors on the GigaWord English corpus.", "labels": [], "entities": [{"text": "GigaWord English corpus", "start_pos": 34, "end_pos": 57, "type": "DATASET", "confidence": 0.9485906362533569}]}, {"text": "The only preprocessing of the cor-: Mean performance of the transformation and boosting methods for N = 100 on train data.", "labels": [], "entities": []}, {"text": "pus was stripping all tag data, removing punctuation and lowercasing.", "labels": [], "entities": []}, {"text": "Context vectors were built with the JavaSDM package 3 of dimensionality N = 100 and N = 10 5 , resp., for preliminary and final experiments, with random degree 10 (five +1s and -1s in each initial vector), right and left context window size of 4 words 4 and constant weighting scheme.", "labels": [], "entities": []}, {"text": "Training and test data provided in the SemEval-2012 Task 6 contained 5 training and 5 testing text sets each of different domains or types of sentences (short video descriptions, pairs of outputs of a machine translation system, etc.).", "labels": [], "entities": []}, {"text": "Although the 5 sets had very different characteristics, we concatenated all training files and trained a single model.", "labels": [], "entities": []}, {"text": "The principal evaluation metrics was Pearson correlation coefficient, that we report here.", "labels": [], "entities": [{"text": "Pearson correlation coefficient", "start_pos": 37, "end_pos": 68, "type": "METRIC", "confidence": 0.8746180534362793}]}, {"text": "Two related other measures were also used (.", "labels": [], "entities": []}, {"text": "Obtained sentence vectors v(s) for were transformed into vectors \u00af x with several methods: Methods 'concat' and 'sumdiff' were proposed by: Mean performance of the best-performing two transformation and two boosting methods for N = 10 5 . tering.", "labels": [], "entities": []}, {"text": "Comparison of mean performance of different transformation and learning methods on the 5-fold splitting of the training set is given in for short context vectors (N = 100).", "labels": [], "entities": []}, {"text": "The correlation is given for the optimal algorithms' parameters (T for RankBoost and, additionally, tree depth and random ratio for RtRank), found with cross-validation on 5 folds.", "labels": [], "entities": [{"text": "correlation", "start_pos": 4, "end_pos": 15, "type": "METRIC", "confidence": 0.9556766152381897}]}, {"text": "With these results for small N , two transformation methods were preselected ('sumdiff' and 'product') for testing and submission with N = 10 5, as increasing N usually increased performance.", "labels": [], "entities": []}, {"text": "Yet, only about 10 3 features were actually selected by RankBoost, meaning that a relatively few random word subsets were informative for approximating semantic textual similarity.", "labels": [], "entities": [{"text": "RankBoost", "start_pos": 56, "end_pos": 65, "type": "DATASET", "confidence": 0.926621675491333}]}, {"text": "In result, RtRank showed better performance, most likely because of more powerful learners, that depend on several features (word subsets) simultaneously.", "labels": [], "entities": []}, {"text": "Performance on machine translation test sets was the lowest that can be explained by very poor quality of the training data : models for these subsets should have been trained separately.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 15, "end_pos": 34, "type": "TASK", "confidence": 0.777648001909256}]}], "tableCaptions": [{"text": " Table 1: Mean performance of the transformation and  boosting methods for N = 100 on train data.", "labels": [], "entities": []}, {"text": " Table 2: Mean performance of the best-performing two transformation and two boosting methods for N = 10 5 .", "labels": [], "entities": []}]}