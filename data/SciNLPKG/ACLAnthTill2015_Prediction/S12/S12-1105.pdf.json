{"title": [{"text": "FBK: Cross-Lingual Textual Entailment Without Translation", "labels": [], "entities": [{"text": "FBK", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.899321436882019}, {"text": "Cross-Lingual Textual Entailment Without Translation", "start_pos": 5, "end_pos": 57, "type": "TASK", "confidence": 0.8021295070648193}]}], "abstractContent": [{"text": "This paper overviews FBK's participation in the Cross-Lingual Textual Entailment for Content Synchronization task organized within SemEval-2012.", "labels": [], "entities": [{"text": "FBK", "start_pos": 21, "end_pos": 24, "type": "DATASET", "confidence": 0.8137235045433044}, {"text": "Cross-Lingual Textual Entailment for Content Synchronization task organized within SemEval-2012", "start_pos": 48, "end_pos": 143, "type": "TASK", "confidence": 0.6905864387750625}]}, {"text": "Our participation is characterized by using cross-lingual matching features extracted from lexical and semantic phrase tables and dependency relations.", "labels": [], "entities": []}, {"text": "The features are used for multi-class and binary classification using SVMs.", "labels": [], "entities": []}, {"text": "Using a combination of lexical, syntactic, and semantic features to create a cross-lingual textual entail-ment system, we report on experiments over the provided dataset.", "labels": [], "entities": []}, {"text": "Our best run achieved an accuracy of 50.4% on the Spanish-English dataset (with the average score and the median system respectively achieving 40.7% and 34.6%), demonstrating the effectiveness of a \"pure\" cross-lingual approach that avoids intermediate translations.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9994122982025146}, {"text": "Spanish-English dataset", "start_pos": 50, "end_pos": 73, "type": "DATASET", "confidence": 0.7644244134426117}]}], "introductionContent": [{"text": "So far, cross-lingual textual entailment (CLTE) ( ) has been applied to: i) available TE datasets (\"YES\"/\"NO\" uni-directional relations between monolingual pairs) transformed into their cross-lingual counterpart by translating the hypotheses into other languages , and ii) machine translation evaluation datasets).", "labels": [], "entities": [{"text": "cross-lingual textual entailment (CLTE)", "start_pos": 8, "end_pos": 47, "type": "TASK", "confidence": 0.6814467509587606}, {"text": "YES", "start_pos": 100, "end_pos": 103, "type": "METRIC", "confidence": 0.9500654935836792}, {"text": "machine translation evaluation", "start_pos": 273, "end_pos": 303, "type": "TASK", "confidence": 0.7924066583315531}]}, {"text": "The content synchronization task represents a challenging application scenario to test the capabilities of CLTE systems, by proposing a richer inventory of phenomena (i.e. \"Bidirectional\"/\"Forward\"/\"Backward\"/\"No entailment\" multi-directional entailment relations).", "labels": [], "entities": []}, {"text": "Multi-directional CLTE recognition can be seen as the identification of semantic equivalence and information disparity between two topically related sentences, at the cross-lingual level.", "labels": [], "entities": [{"text": "CLTE recognition", "start_pos": 18, "end_pos": 34, "type": "TASK", "confidence": 0.7863321304321289}]}, {"text": "This is a core aspect of the multilingual content synchronization task, which represents a challenging application scenario fora variety of NLP technologies, and a shared research framework for the integration of semantics and MT technology.", "labels": [], "entities": [{"text": "multilingual content synchronization task", "start_pos": 29, "end_pos": 70, "type": "TASK", "confidence": 0.6919006854295731}, {"text": "MT", "start_pos": 227, "end_pos": 229, "type": "TASK", "confidence": 0.9673435688018799}]}, {"text": "The CLTE methods proposed so far adopt either a \"pivoting approach\" (translation of the two input texts into the same language, as in ( ), or an \"integrated solution\" that exploits bilingual phrase tables to capture lexical relations and contextual information ).", "labels": [], "entities": []}, {"text": "The promising results achieved with the integrated approach still rely on phrasal matching techniques that disregard relevant semantic aspects of the problem.", "labels": [], "entities": []}, {"text": "By filling this gap integrating linguistically motivated features, in our participation, we propose an approach that combines lexical, syntactic and semantic features within a machine learning framework ().", "labels": [], "entities": []}, {"text": "Our submitted runs have been produced by training and optimizing multiclass and binary SVM classifiers, over the Spanish-English (Spa-Eng) development set.", "labels": [], "entities": [{"text": "Spa-Eng) development set", "start_pos": 130, "end_pos": 154, "type": "DATASET", "confidence": 0.6962626725435257}]}, {"text": "In both cases, our results were positive, showing significant improvements over the median systems and average scores obtained by participants.", "labels": [], "entities": []}, {"text": "The overall results confirm the difficulty of the task, and the potential of our approach in combining linguistically motivated features in a \"pure\" cross-lingual approach that avoids the recourse to external MT components.", "labels": [], "entities": [{"text": "MT", "start_pos": 209, "end_pos": 211, "type": "TASK", "confidence": 0.9783172011375427}]}], "datasetContent": [{"text": "In our experiment we used the Spa-Eng portion of the dataset described in (), consisting of 500 multi-directional entailment pairs which was provided to train the systems and 500 pairs for the submission.", "labels": [], "entities": []}, {"text": "Each pair in the dataset is annotated with \"Bidirectional\", \"Forward\", \"Backward\" or \"No entailment\" judgements.", "labels": [], "entities": [{"text": "Bidirectional", "start_pos": 44, "end_pos": 57, "type": "METRIC", "confidence": 0.9797284007072449}, {"text": "Forward", "start_pos": 61, "end_pos": 68, "type": "METRIC", "confidence": 0.9310113787651062}, {"text": "Backward", "start_pos": 72, "end_pos": 80, "type": "METRIC", "confidence": 0.9362585544586182}]}], "tableCaptions": [{"text": " Table 1: Summary of the submitted runs and results for Spa-Eng dataset.", "labels": [], "entities": [{"text": "Summary", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.857573926448822}, {"text": "Spa-Eng dataset", "start_pos": 56, "end_pos": 71, "type": "DATASET", "confidence": 0.9779107868671417}]}, {"text": " Table 2: Best run's Precision/Recall/F1 scores.", "labels": [], "entities": [{"text": "Precision/Recall/", "start_pos": 21, "end_pos": 38, "type": "METRIC", "confidence": 0.7729754000902176}, {"text": "F1", "start_pos": 38, "end_pos": 40, "type": "METRIC", "confidence": 0.8248946666717529}]}]}