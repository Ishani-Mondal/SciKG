{"title": [{"text": "UKP: Computing Semantic Textual Similarity by Combining Multiple Content Similarity Measures", "labels": [], "entities": [{"text": "UKP", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9262775778770447}, {"text": "Computing Semantic Textual Similarity", "start_pos": 5, "end_pos": 42, "type": "TASK", "confidence": 0.7017529085278511}, {"text": "Combining Multiple Content Similarity Measures", "start_pos": 46, "end_pos": 92, "type": "TASK", "confidence": 0.6988541960716248}]}], "abstractContent": [{"text": "We present the UKP system which performed best in the Semantic Textual Similarity (STS) task at SemEval-2012 in two out of three met-rics.", "labels": [], "entities": [{"text": "UKP", "start_pos": 15, "end_pos": 18, "type": "DATASET", "confidence": 0.8873755931854248}, {"text": "Semantic Textual Similarity (STS) task at SemEval-2012", "start_pos": 54, "end_pos": 108, "type": "TASK", "confidence": 0.8061403996414609}]}, {"text": "It uses a simple log-linear regression model, trained on the training data, to combine multiple text similarity measures of varying complexity.", "labels": [], "entities": []}, {"text": "These range from simple character and word n-grams and common sub-sequences to complex features such as Explicit Semantic Analysis vector comparisons and aggregation of word similarity based on lexical-semantic resources.", "labels": [], "entities": []}, {"text": "Further, we employ a lexical substitution system and statistical machine translation to add additional lex-emes, which alleviates lexical gaps.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 53, "end_pos": 84, "type": "TASK", "confidence": 0.6070413788159689}]}, {"text": "Our final models, one per dataset, consist of a log-linear combination of about 20 features, out of the possible 300+ features implemented.", "labels": [], "entities": []}], "introductionContent": [{"text": "The goal of the pilot Semantic Textual Similarity (STS) task at SemEval-2012 is to measure the degree of semantic equivalence between pairs of sentences.", "labels": [], "entities": [{"text": "Semantic Textual Similarity (STS) task at SemEval-2012", "start_pos": 22, "end_pos": 76, "type": "TASK", "confidence": 0.7653501464260949}]}, {"text": "STS is fundamental to a variety of tasks and applications such as question answering), text reuse detection () or automatic essay grading).", "labels": [], "entities": [{"text": "STS", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.9023083448410034}, {"text": "question answering", "start_pos": 66, "end_pos": 84, "type": "TASK", "confidence": 0.8669633269309998}, {"text": "text reuse detection", "start_pos": 87, "end_pos": 107, "type": "TASK", "confidence": 0.8253522117932638}]}, {"text": "STS is also closely related to textual entailment (TE) () and paraphrase recognition ().", "labels": [], "entities": [{"text": "STS", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.8428918719291687}, {"text": "textual entailment (TE)", "start_pos": 31, "end_pos": 54, "type": "TASK", "confidence": 0.6702096939086915}, {"text": "paraphrase recognition", "start_pos": 62, "end_pos": 84, "type": "TASK", "confidence": 0.8942203223705292}]}, {"text": "It differs from both tasks, though, insofar as those operate on binary similarity decisions while STS is defined as a graded notion of similarity.", "labels": [], "entities": [{"text": "STS", "start_pos": 98, "end_pos": 101, "type": "METRIC", "confidence": 0.8625723123550415}]}, {"text": "STS further requires a bidirectional similarity relationship to hold between a pair of sentences rather than a unidirectional entailment relation as for the TE task.", "labels": [], "entities": [{"text": "STS", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.7289804220199585}]}, {"text": "A multitude of measures for computing similarity between texts have been proposed in the past based on surface-level and/or semantic content features (.", "labels": [], "entities": []}, {"text": "The existing measures exhibit two major limitations, though: Firstly, measures are typically used in separation.", "labels": [], "entities": [{"text": "separation", "start_pos": 101, "end_pos": 111, "type": "TASK", "confidence": 0.9624393582344055}]}, {"text": "Thereby, the assumption is made that a single measure inherently captures all text characteristics which are necessary for computing similarity.", "labels": [], "entities": []}, {"text": "Secondly, existing measures typically exclude similarity features beyond content per se, thereby implying that similarity can be computed by comparing text content exclusively, leaving out any other text characteristics.", "labels": [], "entities": []}, {"text": "While we can only briefly tackle the second issue here, we explicitly address the first one by combining several measures using a supervised machine learning approach.", "labels": [], "entities": []}, {"text": "With this, we hope to take advantage of the different facets and intuitions that are captured in the single measures.", "labels": [], "entities": []}, {"text": "In the following section, we describe the feature space in detail.", "labels": [], "entities": []}, {"text": "Section 3 describes the machine learning setup.", "labels": [], "entities": []}, {"text": "After describing our submitted runs, we discuss the results and conclude.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Best results for single measures, grouped by di- mension, on the training datasets MSRpar, MSRvid, and  SMTeuroparl, using 10-fold cross-validation", "labels": [], "entities": [{"text": "training datasets MSRpar", "start_pos": 75, "end_pos": 99, "type": "DATASET", "confidence": 0.7196083863576254}, {"text": "MSRvid", "start_pos": 101, "end_pos": 107, "type": "DATASET", "confidence": 0.7509979009628296}]}, {"text": " Table 3: Official results on the test data for the top 5  participating runs out of 89 which were achieved on the  known datasets MSRpar, MSRvid, and SMTeuroparl, as  well as on the surprise datasets OnWN and SMTnews. We  report the ranks (# 1 : ALL, # 2 : ALLnrm, # 3 : Mean) and  the corresponding Pearson correlation r according to the  three offical evaluation metrics (see Sec. 6). The provided  baseline is shown at the bottom of this table.", "labels": [], "entities": [{"text": "MSRpar", "start_pos": 131, "end_pos": 137, "type": "DATASET", "confidence": 0.5398311614990234}, {"text": "MSRvid", "start_pos": 139, "end_pos": 145, "type": "DATASET", "confidence": 0.7879360914230347}, {"text": "OnWN", "start_pos": 201, "end_pos": 205, "type": "DATASET", "confidence": 0.612282931804657}, {"text": "SMTnews", "start_pos": 210, "end_pos": 217, "type": "DATASET", "confidence": 0.767812967300415}, {"text": "Mean", "start_pos": 272, "end_pos": 276, "type": "METRIC", "confidence": 0.9932503700256348}, {"text": "Pearson correlation r", "start_pos": 301, "end_pos": 322, "type": "METRIC", "confidence": 0.977751096089681}]}]}