{"title": [{"text": "Soft Cardinality + ML: Learning Adaptive Similarity Functions for Cross-lingual Textual Entailment", "labels": [], "entities": [{"text": "Soft Cardinality + ML", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.7236990630626678}, {"text": "Cross-lingual Textual Entailment", "start_pos": 66, "end_pos": 98, "type": "TASK", "confidence": 0.6861924529075623}]}], "abstractContent": [{"text": "This paper presents a novel approach for building adaptive similarity functions based on cardinality using machine learning.", "labels": [], "entities": []}, {"text": "Unlike current approaches that build feature sets using similarity scores, we have developed these feature sets with the cardinal-ities of the commonalities and differences between pairs of objects being compared.", "labels": [], "entities": []}, {"text": "This approach allows the machine-learning algorithm to obtain an asymmetric similarity function suitable for directional judgments.", "labels": [], "entities": []}, {"text": "Besides using the classic set cardi-nality, we used soft cardinality to allow flexibility in the comparison between words.", "labels": [], "entities": []}, {"text": "Our approach used only the information from the surface of the text, a stop-word remover and a stemmer to address the cross-lingual textual entailment task 8 at SEMEVAL 2012.", "labels": [], "entities": [{"text": "cross-lingual textual entailment task 8 at SEMEVAL 2012", "start_pos": 118, "end_pos": 173, "type": "TASK", "confidence": 0.6593106053769588}]}, {"text": "We have the third best result among the 29 systems submitted by 10 teams.", "labels": [], "entities": []}, {"text": "Additionally, this paper presents better results compared with the best official score.", "labels": [], "entities": []}], "introductionContent": [{"text": "Adaptive similarity functions are those functions that, beyond using the information of two objects being compared, use information from a broader set of objects ( . Therefore, the same similarity function may return different results for the same pair of objects, depending on the context of where the objects are.", "labels": [], "entities": []}, {"text": "Adaptability is intended to improve the performance of the similarity function in relation to the task in question associated with the entire set of objects.", "labels": [], "entities": [{"text": "Adaptability", "start_pos": 0, "end_pos": 12, "type": "TASK", "confidence": 0.8659611344337463}]}, {"text": "For example, adaptiveness improves relevance of documents retrieved fora query in an information retrieval task fora particular document collection.", "labels": [], "entities": [{"text": "information retrieval task fora particular document collection", "start_pos": 85, "end_pos": 147, "type": "TASK", "confidence": 0.7127268740109035}]}, {"text": "In text applications there are mainly three methods to provide adaptiveness to similarity functions: term weighting, adjustment or learning the parameters of the similarity function, and machine learning.", "labels": [], "entities": []}, {"text": "Term weighting is a common practice that assigns a degree of importance to each occurrence of a term in a text collection ().", "labels": [], "entities": [{"text": "Term weighting", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.9405560493469238}]}, {"text": "Secondly, if a similarity function has parameters, these can be adjusted or learned to adapt to a particular data set.", "labels": [], "entities": []}, {"text": "Depending on the size of the search space defined by these parameters, they can be adjusted either manually or using a technique of AI.", "labels": [], "entities": []}, {"text": "For instance, Jimenez et al. manually adjusted a single parameter in the generalized measure of Monge-Elkan (1996) ( and learned the costs of editing operations between particular characters for the Levenshtein distance (1966) using HMMs.", "labels": [], "entities": []}, {"text": "Thirdly, the machine-learning approach aims to learn a similarity function based on a vector representation of texts using a subset of texts for training and a learning function ( . The three methods of adaptability can also be used in a variety of combinations, e.g. term weighting in combination with machine learning).", "labels": [], "entities": []}, {"text": "Finally, to achieve adaptability, other approaches use data sets considerably larger, such as large corpora or the Web, e.g. distributional similarity).", "labels": [], "entities": []}, {"text": "In the machine-learning approach, a vector representation of texts is used in conjunction with an algorithm of classification or regression.", "labels": [], "entities": []}, {"text": "Each vector of features f 1 , f 2 , . .", "labels": [], "entities": []}, {"text": ", f m is associated to each pair Ti , T j of texts.", "labels": [], "entities": []}, {"text": "Thus,  proposed a set of features indexed by the data set vocabulary, similar to who used fragments of parse trees.", "labels": [], "entities": []}, {"text": "However, a more common approach is to select as features the scores of different similarity functions.", "labels": [], "entities": []}, {"text": "Using these features, the machine-learning algorithm discovers the relative importance of each feature and a combination mechanism that maximizes the alignment of the final result with a gold standard for the particular task.", "labels": [], "entities": []}, {"text": "In this paper, we propose a novel approach to extract feature sets fora machine-learning algorithm using car-dinalities rather than scores of similarity functions.", "labels": [], "entities": []}, {"text": "For instance, instead of using as a feature the score obtained by the Dice's coefficient (i.e. 2\u00d7|Ti\u2229Tj | /|Ti|+|Tj|), we use |T i |, |T j | and |T i \u2229 T j | as features.", "labels": [], "entities": []}, {"text": "The rationale behind this idea is that despite the similarity scores being suitable for learning a combined function of similarity, they hide the information imbalance between the original pair of texts.", "labels": [], "entities": []}, {"text": "Our hypothesis is that the information coded in this imbalance could provide the machine-learning algorithm with better information to generate a combined similarity score.", "labels": [], "entities": [{"text": "similarity score", "start_pos": 155, "end_pos": 171, "type": "METRIC", "confidence": 0.9387860596179962}]}, {"text": "For instance, consider these pairs of texts: \"The beach house is white.\", \"The house was completely empty.\" and \"The house\", \"The beach house was completely empty and isolated\" . Both pairs have the same similarity score using the Dice coefficient, but it is evident that the latter has an imbalance of information lost in that single score.", "labels": [], "entities": []}, {"text": "This imbalance of information is even more important if the task requires to identify directional similarities, such as \"T 1 is more similar to T 2 , than T 2 is to T 1 \".", "labels": [], "entities": []}, {"text": "However, unlike the similarity functions, which are numerous, there is only one set cardinality.", "labels": [], "entities": []}, {"text": "This issue can be addressed using the soft cardinality proposed by, which uses an auxiliary function of similarity between elements to make a soft count of the elements in a set.", "labels": [], "entities": []}, {"text": "For instance, the classic cardinality of the set A = { \"Sunday\", \"Saturday\" } is |A| = 2; and the soft cardinality of the same set, using a normalized editdistance as auxiliary similarity function, is |A| sim = 1.23 because of the commonalities between both words.", "labels": [], "entities": []}, {"text": "Furthermore, soft cardinality allows weighting of elements giving it additional capacity to adapt.", "labels": [], "entities": []}, {"text": "We used the proposed approach to participate in the cross-lingual textual-entailment task 8 at SEMEVAL 2012.", "labels": [], "entities": [{"text": "cross-lingual textual-entailment task 8 at SEMEVAL 2012", "start_pos": 52, "end_pos": 107, "type": "TASK", "confidence": 0.5179969838687352}]}, {"text": "The task was to recognize bidirectional, forward, backward or lack of entailment in pairs of texts written in five languages.", "labels": [], "entities": []}, {"text": "We built a system based on the proposed method and the use of surface information of the text, a stop-word remover and a stemmer.", "labels": [], "entities": []}, {"text": "Our system achieved the third best result in official classification and, after some debugging, we are reporting better results than the best official scores.", "labels": [], "entities": []}, {"text": "This paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 briefly describes soft cardinality and other cardinalities for text applications.", "labels": [], "entities": []}, {"text": "Section 3 presents the proposed method.", "labels": [], "entities": []}, {"text": "Experimental validation is presented in Section 4.", "labels": [], "entities": []}, {"text": "A brief discussion is presented in Section 5.", "labels": [], "entities": []}, {"text": "Finally, conclusions are drawn in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "Given that each pair of texts T 1 , T 2 are in different languages, a pair of translations T t 1 , T t 2 were provided using Google Translate service.", "labels": [], "entities": []}, {"text": "Thus, each one of the text pairs T 1 , T t 2 and T t 1 , T 2 were in the same language.", "labels": [], "entities": []}, {"text": "Then, all produced pairs were pre-processed by removing stop-words in their respective languages.", "labels": [], "entities": []}, {"text": "Finally, all texts were lemmatized using Porter's stemmer (1980) for English and Snowball stemmers for other languages using an implementation provided by the NLTK).", "labels": [], "entities": [{"text": "Snowball stemmers", "start_pos": 81, "end_pos": 98, "type": "DATASET", "confidence": 0.9203400015830994}, {"text": "NLTK", "start_pos": 159, "end_pos": 163, "type": "DATASET", "confidence": 0.9309207797050476}]}, {"text": "Then, different set of features were generated using similarity scores or cardinalities.", "labels": [], "entities": []}, {"text": "While each symmetric similarity function generates 2 features i)sim(T 1 , T t 2 ) and ii)sim(T t 1 , T 2 ), asymmetric functions generate two additional features iii)sim(T t 2 , T 1 ) and iv)sim(T 2 , T t 1 ).", "labels": [], "entities": []}, {"text": "On the other hand, each cardinality function generates 12 features: i) , and xii) |T 2 \u2212 T t 1 |.", "labels": [], "entities": []}, {"text": "Various combinations of cardinalities, symmetric and asymmetric functions were used to generate the following feature sets: Sym.simScores: scores of the following symmetric similarity functions: Jaccard, Dice, and cosine coefficients using classical cardinality and soft cardinality (edit-distance as auxiliar sim. function).", "labels": [], "entities": []}, {"text": "In addition, cosine similarity, softTFIDF  and editdistance (total 18 features).", "labels": [], "entities": [{"text": "softTFIDF", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.7967784404754639}]}, {"text": "Asym.LCS.sim: scores of the following asymmetric similarity functions: sim(T 1 , T 2 ) = lcs(T1,T2) /len(T1) and sim(T 1 , T 2 ) = lcs(T1,T2) /len(T2) at character level (4 features).", "labels": [], "entities": []}, {"text": "Classic.card: cardinalities using classical set cardinality (12 features).", "labels": [], "entities": []}, {"text": "Dot.card.w: dot-product cardinality using idf weights as described in Section 2.4, using p = 1 (12 features).", "labels": [], "entities": []}, {"text": "LCS.card: LCS cardinality at word-level using idf weights as described in Section 2.1 (12 features).", "labels": [], "entities": [{"text": "LCS.card", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.8258110284805298}]}, {"text": "SimScores: combined features sets from Sym.SimScores, Asym.LCS.sim and the generalized Monge-Elkan measure () using p = 1, 2, 3 (30 features).", "labels": [], "entities": []}, {"text": "Dot.card.w.0.5: same as Dot.card.w using p = 0.5.", "labels": [], "entities": []}, {"text": "Classic.card.w: classical cardinality using idf weights (12 features).", "labels": [], "entities": []}, {"text": "Soft.card.w: soft cardinality using idf weights as described in Section 2.3 using p = 1, 2, 3, 4, 5 (60 features).", "labels": [], "entities": []}, {"text": "The machine-learning classification algorithm for all feature sets was SVM () with the complexity parameter C = 1.5 and a linear polynomial kernel.", "labels": [], "entities": [{"text": "machine-learning classification", "start_pos": 4, "end_pos": 35, "type": "TASK", "confidence": 0.7715960443019867}]}, {"text": "All experiments were conducted using WEKA ().", "labels": [], "entities": [{"text": "WEKA", "start_pos": 37, "end_pos": 41, "type": "DATASET", "confidence": 0.5452512502670288}]}], "tableCaptions": [{"text": " Table 1: Accuracy results for Semeval2012 task 8", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9994988441467285}, {"text": "Semeval2012 task", "start_pos": 31, "end_pos": 47, "type": "TASK", "confidence": 0.7710563540458679}]}, {"text": " Table 2: Average accuracy comparison vs. Soft.card.w in 100  runs", "labels": [], "entities": [{"text": "Average", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9698680639266968}, {"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9429848194122314}]}]}