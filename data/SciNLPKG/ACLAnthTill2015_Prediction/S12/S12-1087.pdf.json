{"title": [{"text": "UNIBA: Distributional Semantics for Textual Similarity", "labels": [], "entities": [{"text": "Distributional Semantics", "start_pos": 7, "end_pos": 31, "type": "TASK", "confidence": 0.8506368398666382}, {"text": "Similarity", "start_pos": 44, "end_pos": 54, "type": "TASK", "confidence": 0.716203510761261}]}], "abstractContent": [{"text": "We report the results of UNIBA participation in the first SemEval-2012 Semantic Textual Similarity task.", "labels": [], "entities": [{"text": "SemEval-2012 Semantic Textual Similarity task", "start_pos": 58, "end_pos": 103, "type": "TASK", "confidence": 0.8887862682342529}]}, {"text": "Our systems rely on distribu-tional models of words automatically inferred from a large corpus.", "labels": [], "entities": []}, {"text": "We exploit three different semantic word spaces: Random Indexing (RI), Latent Semantic Analysis (LSA) over RI, and vector permutations in RI.", "labels": [], "entities": [{"text": "Latent Semantic Analysis (LSA)", "start_pos": 71, "end_pos": 101, "type": "METRIC", "confidence": 0.7021530717611313}]}, {"text": "Runs based on these spaces consistently outperform the base-line on the proposed datasets.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "SemEval-2012 STS is a first attempt to provide a \"unified framework for the evaluation of modular semantic components.\"", "labels": [], "entities": [{"text": "SemEval-2012 STS", "start_pos": 0, "end_pos": 16, "type": "DATASET", "confidence": 0.6482108235359192}]}, {"text": "ALL Pearson correlation with the gold standard for the five datasets.", "labels": [], "entities": [{"text": "Pearson", "start_pos": 4, "end_pos": 11, "type": "METRIC", "confidence": 0.8543731570243835}, {"text": "correlation", "start_pos": 12, "end_pos": 23, "type": "METRIC", "confidence": 0.5554574728012085}]}, {"text": "ALLnrm Pearson correlation after the system outputs for each dataset are fitted to the gold standard using least squares.", "labels": [], "entities": [{"text": "ALLnrm Pearson correlation", "start_pos": 0, "end_pos": 26, "type": "METRIC", "confidence": 0.763097087542216}]}, {"text": "Mean Weighted mean across the five datasets, where the weight depends on the number of pairs in the dataset.", "labels": [], "entities": [{"text": "Mean Weighted mean", "start_pos": 0, "end_pos": 18, "type": "METRIC", "confidence": 0.9051862160364786}]}, {"text": "For the evaluation, we built Distributional Spaces using the WaCkypedia_EN corpus 2 . WaCkypedia_EN is based on a 2009 dump of the English Wikipedia (about 800 million tokens) and includes information about: part-ofspeech, lemma and a full dependency parsing performed by).", "labels": [], "entities": [{"text": "WaCkypedia_EN corpus 2", "start_pos": 61, "end_pos": 83, "type": "DATASET", "confidence": 0.8072575449943542}]}, {"text": "The three spaces described in Section 2 are built exploiting information about term windows and dependency parsing supplied by WaCkypedia.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 96, "end_pos": 114, "type": "TASK", "confidence": 0.7195729464292526}, {"text": "WaCkypedia", "start_pos": 127, "end_pos": 137, "type": "DATASET", "confidence": 0.9232076406478882}]}, {"text": "The total number of dependencies amounts to about 200 million.", "labels": [], "entities": []}, {"text": "The RI system is implemented in Java and relies on some portions of code publicly available in the Semantic Vectors package, while for LSA we exploited the publicly available C library SVDLIBC 3 . We restricted the vocabulary to the 50,000 most frequent terms, with stop words removal and forcing the system to include terms which occur in the dataset.", "labels": [], "entities": []}, {"text": "Hence, the dimension of the original matrix would have been 50,000\u00d750,000.", "labels": [], "entities": []}, {"text": "Our approach involves some parameters.", "labels": [], "entities": []}, {"text": "In particular, each semantic space needs to setup the dimension k of the space.", "labels": [], "entities": []}, {"text": "All spaces use a dimension of 500 (resulting in a 50,000\u00d7500 matrix).", "labels": [], "entities": []}, {"text": "The number of non-zero elements in the random vector is set to 10.", "labels": [], "entities": []}, {"text": "When we apply LSA to the output space generated by the Random Indexing we holdall the 500 dimensions since during the tuning we observed a drop in performance when a lower dimension was set.", "labels": [], "entities": [{"text": "LSA", "start_pos": 14, "end_pos": 17, "type": "METRIC", "confidence": 0.8595901727676392}]}, {"text": "The co-occurrence distance w between terms was setup to 4.", "labels": [], "entities": []}, {"text": "In order to compute the similarity between the vector representations of sentences we used the cosine similarity, and then we multiplied by 5 the obtained value.", "labels": [], "entities": [{"text": "similarity", "start_pos": 24, "end_pos": 34, "type": "METRIC", "confidence": 0.9633039832115173}]}, {"text": "shows the overall results obtained exploiting the different semantic spaces.", "labels": [], "entities": []}, {"text": "We report the three proposed evaluation measures with the corresponding overall ranks with respect to the 89 runs submitted by participants.", "labels": [], "entities": []}, {"text": "We submitted three different runs, each exploring a different semantic space: UNIBA-RI (based on Random Indexing), UNIBA-LSARI (based on LSA performed over RI outcome), and UNIBA-DEPRI (based on Random Indexing and vector permutations).", "labels": [], "entities": [{"text": "UNIBA-RI", "start_pos": 78, "end_pos": 86, "type": "DATASET", "confidence": 0.9193674921989441}, {"text": "UNIBA-LSARI", "start_pos": 115, "end_pos": 126, "type": "DATASET", "confidence": 0.9168593883514404}, {"text": "UNIBA-DEPRI", "start_pos": 173, "end_pos": 184, "type": "DATASET", "confidence": 0.8990941643714905}]}, {"text": "Each proposed measure stresses different aspects.", "labels": [], "entities": []}, {"text": "ALL is the Pearson's correlation computed over the concatenated dataset.", "labels": [], "entities": [{"text": "ALL", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.954239547252655}, {"text": "Pearson's correlation", "start_pos": 11, "end_pos": 32, "type": "METRIC", "confidence": 0.7822926243146261}]}, {"text": "As a consequence this measure ranks higher systems which obtain consistent better results.", "labels": [], "entities": []}, {"text": "Conversely, ALLNrm normalizes results by scaling values obtained from each dataset, in this way it tries to give emphasis to systems trained on each dataset.", "labels": [], "entities": [{"text": "ALLNrm normalizes", "start_pos": 12, "end_pos": 29, "type": "TASK", "confidence": 0.6928151398897171}]}, {"text": "The result of these different perspective is that our three spaces rank differently according to each measure.", "labels": [], "entities": []}, {"text": "It seems that UNIBA-RI is able to work better across all datasets, while UNIBA-LSARI gives the best results on specific datasets, even though all our methods are unsupervised and do not need training steps.", "labels": [], "entities": [{"text": "UNIBA-RI", "start_pos": 14, "end_pos": 22, "type": "DATASET", "confidence": 0.8782203793525696}, {"text": "UNIBA-LSARI", "start_pos": 73, "end_pos": 84, "type": "DATASET", "confidence": 0.8620725274085999}]}, {"text": "A deeper analysis on each dataset is reported on.", "labels": [], "entities": []}, {"text": "Here results seem to beat odds with.", "labels": [], "entities": []}, {"text": "Considering individual datasets, UNIBA-RI gives only once the best result, while UNIBA-LSARI and UNIBA-DEPRI are able to provide the best results twice.", "labels": [], "entities": [{"text": "UNIBA-RI", "start_pos": 33, "end_pos": 41, "type": "DATASET", "confidence": 0.845698893070221}, {"text": "UNIBA-LSARI", "start_pos": 81, "end_pos": 92, "type": "DATASET", "confidence": 0.9374347925186157}, {"text": "UNIBA-DEPRI", "start_pos": 97, "end_pos": 108, "type": "DATASET", "confidence": 0.9407038688659668}]}, {"text": "Generally, all results outperform the baseline, based on a simple keyword overlap.", "labels": [], "entities": []}, {"text": "Lower results are obtained in MSRpar, we ascribe this result to the notably long sentences here involved.", "labels": [], "entities": [{"text": "MSRpar", "start_pos": 30, "end_pos": 36, "type": "DATASET", "confidence": 0.8725235462188721}]}, {"text": "In particular, UNIBA-LSARI gives a result lower than the baseline, and inline with the one obtained by LSA during the tuning.", "labels": [], "entities": [{"text": "UNIBA-LSARI", "start_pos": 15, "end_pos": 26, "type": "DATASET", "confidence": 0.8591906428337097}]}, {"text": "Hence, we ascribe this low performance to the application of LSA method to this specific dataset.", "labels": [], "entities": []}, {"text": "Only UNIBA-DEPRI was able to outperform the baseline in this dataset.", "labels": [], "entities": [{"text": "UNIBA-DEPRI", "start_pos": 5, "end_pos": 16, "type": "DATASET", "confidence": 0.9015169739723206}]}, {"text": "This shows the usefulness of encoding syntactic features in semantic word space where longer sentences are involved.", "labels": [], "entities": []}, {"text": "Generally, it is interesting to be noticed that our spaces perform rather well on short and similarly structured sentences, such as MSRvid and On-WN.", "labels": [], "entities": [{"text": "MSRvid", "start_pos": 132, "end_pos": 138, "type": "DATASET", "confidence": 0.894325315952301}]}], "tableCaptions": [{"text": " Table 1: Evaluation results of Pearson's correlation.", "labels": [], "entities": [{"text": "Pearson's correlation", "start_pos": 32, "end_pos": 53, "type": "METRIC", "confidence": 0.6326845089594523}]}, {"text": " Table 2: Evaluation results of Pearson's correlation for individual datasets.", "labels": [], "entities": [{"text": "Pearson's correlation", "start_pos": 32, "end_pos": 53, "type": "METRIC", "confidence": 0.6945624550183614}]}]}