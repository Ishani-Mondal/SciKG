{"title": [{"text": "ATA-Sem: Chunk-based Determination of Semantic Text Similarity", "labels": [], "entities": [{"text": "ATA-Sem", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.8292601108551025}, {"text": "Semantic Text Similarity", "start_pos": 38, "end_pos": 62, "type": "TASK", "confidence": 0.587541381518046}]}], "abstractContent": [{"text": "This paper describes investigations into using syntactic chunk information as the basis for determining the similarity of candidate texts at the semantic level.", "labels": [], "entities": []}, {"text": "The first was a corpus-based method that extracted lexical and semantic features from pairs of chunks from each sentence that were associated through a chunk alignment algorithm.", "labels": [], "entities": []}, {"text": "The features were used as input to a classifier trained on the same features extracted from a corpus of gold standard training data.", "labels": [], "entities": []}, {"text": "The second approach involved breadth-first chunk association and the application of a rule-based scoring algorithm.", "labels": [], "entities": [{"text": "breadth-first chunk association", "start_pos": 29, "end_pos": 60, "type": "TASK", "confidence": 0.7347880601882935}]}, {"text": "Both approaches were evaluated against the test data for the SemEval 2012 Semantic Text Similarity task.", "labels": [], "entities": [{"text": "SemEval 2012 Semantic Text Similarity task", "start_pos": 61, "end_pos": 103, "type": "TASK", "confidence": 0.902339925368627}]}, {"text": "The results show that the rule-based chunk approach is superior.", "labels": [], "entities": []}], "introductionContent": [{"text": "The task of determining whether two texts are similar in some sense has important applications in the field of natural language processing, including but not limited to document summarization), plagiarism detection () and large corpus document retrieval.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 111, "end_pos": 138, "type": "TASK", "confidence": 0.6802223324775696}, {"text": "document summarization", "start_pos": 169, "end_pos": 191, "type": "TASK", "confidence": 0.6394585520029068}, {"text": "plagiarism detection", "start_pos": 194, "end_pos": 214, "type": "TASK", "confidence": 0.7215699553489685}, {"text": "large corpus document retrieval", "start_pos": 222, "end_pos": 253, "type": "TASK", "confidence": 0.6397939994931221}]}, {"text": "While textual similarity can be performed at the purely surface lexical level, as in the \"simhash\" clustering method described in, similarity also applies at the semantic level, where conceptually similar texts may nevertheless be entirely dissimilar at the surface lexical level.", "labels": [], "entities": []}, {"text": "For example, the phrases \"restrict or confine\" and \"place limits on (extent or access)\" share no words or morphological roots, yet mean very nearly the same thing at the semantic level.", "labels": [], "entities": []}, {"text": "The Semantic Textual Similarity (STS) task (Task #6) at SemEval-2012) provided a forum for exploring these issues by furnishing training and evaluation data, and also a common standard for describing degrees of similarity, shown in.", "labels": [], "entities": [{"text": "Semantic Textual Similarity (STS) task", "start_pos": 4, "end_pos": 42, "type": "TASK", "confidence": 0.8053293142999921}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 4. Results against STS test suite. 1", "labels": [], "entities": [{"text": "STS test suite", "start_pos": 26, "end_pos": 40, "type": "DATASET", "confidence": 0.9079898198445638}]}]}