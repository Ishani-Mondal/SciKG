{"title": [{"text": "FBK: Machine Translation Evaluation and Word Similarity metrics for Semantic Textual Similarity", "labels": [], "entities": [{"text": "FBK", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9196275472640991}, {"text": "Machine Translation Evaluation", "start_pos": 5, "end_pos": 35, "type": "TASK", "confidence": 0.8592960039774576}, {"text": "Word Similarity", "start_pos": 40, "end_pos": 55, "type": "TASK", "confidence": 0.7071259617805481}, {"text": "Semantic Textual Similarity", "start_pos": 68, "end_pos": 95, "type": "TASK", "confidence": 0.682355801264445}]}], "abstractContent": [{"text": "This paper describes the participation of FBK in the Semantic Textual Similarity (STS) task organized within Semeval 2012.", "labels": [], "entities": [{"text": "FBK", "start_pos": 42, "end_pos": 45, "type": "DATASET", "confidence": 0.893690288066864}, {"text": "Semantic Textual Similarity (STS) task organized within Semeval 2012", "start_pos": 53, "end_pos": 121, "type": "TASK", "confidence": 0.835536003112793}]}, {"text": "Our approach explores lexical, syntactic and semantic machine translation evaluation metrics combined with distributional and knowledge-based word similarity metrics.", "labels": [], "entities": [{"text": "semantic machine translation evaluation", "start_pos": 45, "end_pos": 84, "type": "TASK", "confidence": 0.7603357136249542}]}, {"text": "Our best model achieves 60.77% correlation with human judgements (Mean score) and ranked 20 out of 88 submitted runs in the Mean ranking , where the average correlation across all the sub-portions of the test set is considered.", "labels": [], "entities": [{"text": "Mean score)", "start_pos": 66, "end_pos": 77, "type": "METRIC", "confidence": 0.9812726974487305}, {"text": "Mean", "start_pos": 124, "end_pos": 128, "type": "METRIC", "confidence": 0.95635986328125}]}], "introductionContent": [{"text": "The Semantic Textual Similarity (STS) task proposed at SemEval 2012 consists of examining the degree of semantic equivalence between two sentences and assigning a score to quantify such similarity ranging from 0 (the two texts are about different topics) to 5 (the two texts are semantically equivalent).", "labels": [], "entities": [{"text": "Semantic Textual Similarity (STS)", "start_pos": 4, "end_pos": 37, "type": "TASK", "confidence": 0.7822643965482712}, {"text": "SemEval 2012", "start_pos": 55, "end_pos": 67, "type": "TASK", "confidence": 0.73199662566185}]}, {"text": "The complete description of the task, the datasets and the evaluation methodology adopted can be found in (.", "labels": [], "entities": []}, {"text": "Typical approaches to measure semantic textual similarity exploit information at the lexical level.", "labels": [], "entities": []}, {"text": "The proposed solutions range from calculating the overlap of common words between the two text segments () to the application of knowledge-based and corpus-based word similarity metrics to cope with the low recall achieved by on simple lexical matching ().", "labels": [], "entities": [{"text": "recall", "start_pos": 207, "end_pos": 213, "type": "METRIC", "confidence": 0.9950906038284302}]}, {"text": "Our participation in the STS task is inspired by previous work on paraphrase recognition, in which machine translation (MT) evaluation metrics are used to identify whether a pair of sentences are semantically equivalent or not).", "labels": [], "entities": [{"text": "STS task", "start_pos": 25, "end_pos": 33, "type": "TASK", "confidence": 0.9194135665893555}, {"text": "paraphrase recognition", "start_pos": 66, "end_pos": 88, "type": "TASK", "confidence": 0.9354536831378937}, {"text": "machine translation (MT) evaluation", "start_pos": 99, "end_pos": 134, "type": "TASK", "confidence": 0.8250428636868795}]}, {"text": "Our approach to semantic textual similarity makes use of not only lexical information but also syntactic and semantic information.", "labels": [], "entities": [{"text": "semantic textual similarity", "start_pos": 16, "end_pos": 43, "type": "TASK", "confidence": 0.6202100912729899}]}, {"text": "To this aim, our metrics are based on different natural language processing tools that provide syntactic and semantic annotation.", "labels": [], "entities": []}, {"text": "These include shallow parsing, constituency parsing, dependency parsing, semantic roles labeling, discourse representation analyzer, and named entities recognition.", "labels": [], "entities": [{"text": "shallow parsing", "start_pos": 14, "end_pos": 29, "type": "TASK", "confidence": 0.6825797408819199}, {"text": "constituency parsing", "start_pos": 31, "end_pos": 51, "type": "TASK", "confidence": 0.9258846342563629}, {"text": "dependency parsing", "start_pos": 53, "end_pos": 71, "type": "TASK", "confidence": 0.8065221905708313}, {"text": "semantic roles labeling", "start_pos": 73, "end_pos": 96, "type": "TASK", "confidence": 0.6291847427686056}, {"text": "discourse representation analyzer", "start_pos": 98, "end_pos": 131, "type": "TASK", "confidence": 0.7273340225219727}, {"text": "named entities recognition", "start_pos": 137, "end_pos": 163, "type": "TASK", "confidence": 0.7131979862848917}]}, {"text": "In addition, we employed distributional and knowledgebased word similarity metrics in an attempt to improve the results given by the MT metrics.", "labels": [], "entities": [{"text": "MT", "start_pos": 133, "end_pos": 135, "type": "TASK", "confidence": 0.8559161424636841}]}, {"text": "The computed scores are used as features to train a regression model in a supervised learning framework.", "labels": [], "entities": []}, {"text": "Our best run model achieves 60.77% correlation with human judgements when evaluating the semantic similarity of texts from the entire test set and was ranked in the 20th position (out of 88 submitted runs) in the Mean ranking.", "labels": [], "entities": [{"text": "Mean", "start_pos": 213, "end_pos": 217, "type": "METRIC", "confidence": 0.9705253839492798}]}], "datasetContent": [{"text": "MT evaluation metrics are designed to assess whether the output of a MT system is semantically equivalent to a set of reference translations.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.9213741719722748}, {"text": "MT", "start_pos": 69, "end_pos": 71, "type": "TASK", "confidence": 0.9624965786933899}]}, {"text": "The MT evaluation metrics described in this section, implemented in the Asiya Open Toolkit for Automatic Machine Translation (Meta-) Evaluation 1 () are used to extract features at different linguistic levels: lexical, syntactic and semantic.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 4, "end_pos": 17, "type": "TASK", "confidence": 0.9113883078098297}, {"text": "Automatic Machine Translation (Meta-) Evaluation 1", "start_pos": 95, "end_pos": 145, "type": "TASK", "confidence": 0.7737079709768295}]}, {"text": "For the syntactic and semantic levels, Asiya calculates similarity measures based on the linguistic elements provided by each kind of annotation.", "labels": [], "entities": [{"text": "similarity", "start_pos": 56, "end_pos": 66, "type": "METRIC", "confidence": 0.9509375095367432}]}, {"text": "Linguistic elements are defined as \"the linguistic units, structures, or relationships\") (e.g. dependency relations, discourse relations, named entities, part-of-speech tags, among others).) defines two simple measures using the linguistic elements of a given linguistic level: overlapping and matching.", "labels": [], "entities": []}, {"text": "Overlapping is a measure of the proportion of items inside the linguistic elements of a certain type shared by both texts.", "labels": [], "entities": []}, {"text": "Matching is defined in the same way with the difference that the order between the items inside a linguistic element is taken into consideration.", "labels": [], "entities": []}, {"text": "That is, the items of a linguistic element are concatenated in a single unit from left to right.", "labels": [], "entities": []}, {"text": "In this section we present our experiments settings, the configuration of the runs submitted and discuss the results obtained.", "labels": [], "entities": []}, {"text": "All our experiments were made using half of the training set for training and half for testing (development).", "labels": [], "entities": []}, {"text": "Ten different randomizations were run over the training data in order to obtain ten different pairs of train/development sets and reduce overfitting.", "labels": [], "entities": []}, {"text": "We tried several different regression algorithms and the best performance was achieved with the implementation of Support Vector Machines (SVM) of the SVMLight package.", "labels": [], "entities": []}, {"text": "We used the radial basis function kernel with default parameters without any special tuning for the different datasets.", "labels": [], "entities": []}, {"text": "After the evaluation period, as a first step towards the required error analysis and a better comprehension of the potential of our approach, we performed an experiment to assess the impact of having models trained for specific datasets.", "labels": [], "entities": []}, {"text": "In this experiment, each training dataset was used to train a model.", "labels": [], "entities": []}, {"text": "Each dataset's model was tested on its respective test dataset.", "labels": [], "entities": []}, {"text": "The model for the surprise datasets (OnWn and SMTnews) were trained using the whole training dataset.", "labels": [], "entities": [{"text": "OnWn", "start_pos": 37, "end_pos": 41, "type": "DATASET", "confidence": 0.9411420226097107}]}, {"text": "We used the Run 3 feature set (the best run in the official evaluation).", "labels": [], "entities": [{"text": "Run 3 feature set", "start_pos": 12, "end_pos": 29, "type": "DATASET", "confidence": 0.9313020259141922}]}, {"text": "The results of the experiment are reported in the column \"Exp\" of table 1.", "labels": [], "entities": [{"text": "Exp", "start_pos": 58, "end_pos": 61, "type": "METRIC", "confidence": 0.9438443183898926}]}, {"text": "The impact of having specific models for each dataset is high.", "labels": [], "entities": []}, {"text": "The Mean score goes from .607 to .829 and improvements are also observed in the All score (0.789).", "labels": [], "entities": [{"text": "Mean score", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9821414947509766}, {"text": "All score", "start_pos": 80, "end_pos": 89, "type": "METRIC", "confidence": 0.9742040038108826}]}, {"text": "These scores would rank our system at the 7th position in the Mean rank.", "labels": [], "entities": [{"text": "Mean", "start_pos": 62, "end_pos": 66, "type": "METRIC", "confidence": 0.9591244459152222}]}, {"text": "However, it is important to notice that in a real-world setting, knowledge about the source of data is not always available.", "labels": [], "entities": []}, {"text": "We consider that having a general model that does not rely on this kind of information represents a more realistic way to confront with real-world applications.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results of each run for each dataset (MSRpar,  MSRvid, SMTeuroparl, OnWn, SMTnews) calculated  with the Pearson correlation between the system's out- puts and the gold standard annotation. Official scores ob- tained using the three evaluation scores All, Allnrm and  Mean. Development row presents the average results for  each run in the whole training dataset. Base is the of- ficial baseline system. Post Evaluation is the experiment  ran after the evaluation period with models trained for the  specific datasets.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 114, "end_pos": 133, "type": "METRIC", "confidence": 0.964566558599472}, {"text": "All", "start_pos": 260, "end_pos": 263, "type": "METRIC", "confidence": 0.9429100751876831}, {"text": "Allnrm", "start_pos": 265, "end_pos": 271, "type": "METRIC", "confidence": 0.860195517539978}, {"text": "Mean", "start_pos": 277, "end_pos": 281, "type": "METRIC", "confidence": 0.8456488847732544}, {"text": "Post Evaluation", "start_pos": 413, "end_pos": 428, "type": "METRIC", "confidence": 0.9257252216339111}]}]}