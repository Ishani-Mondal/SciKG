{"title": [{"text": "Towards a Flexible Semantics: Colour Terms in Collaborative Reference Tasks", "labels": [], "entities": []}], "abstractContent": [{"text": "We report ongoing work on the development of agents that can implicitly coordinate with their partners in referential tasks, taking as a case study colour terms.", "labels": [], "entities": []}, {"text": "We describe algorithms for generation and resolution of colour descriptions and report results of experiments on how humans use colour terms for reference in production and comprehension.", "labels": [], "entities": [{"text": "generation and resolution of colour descriptions", "start_pos": 27, "end_pos": 75, "type": "TASK", "confidence": 0.8162001371383667}]}], "introductionContent": [{"text": "Speakers do not always share identical semantic representations nor identical lexicons.", "labels": [], "entities": []}, {"text": "For instance, a subject may refer to a shape as a diamond while another subject may call that same shape a square (which just happens to be tilted sidewise); or someone may refer to a particular colour with 'light pink' while a different speaker may refer to it as 'salmon'.", "labels": [], "entities": []}, {"text": "Regardless of these differences, which seem commonplace, speakers in dialogue are able to communicate successfully most of the time.", "labels": [], "entities": []}, {"text": "Successful communication exploits interlocutors' abilities to negotiate referring expressions interactively through grounding, but in many cases interlocutors can already make a good guess at their partners' intentions by relaxing the interpretation of their utterances and looking for the referent that best matches this looser interpretation.", "labels": [], "entities": []}, {"text": "We are interested in modelling this second kind of behaviour computationally, to get a better understanding of it and to contribute to the development of dialogue systems that are able to better coordinate with their human partners.", "labels": [], "entities": []}, {"text": "In this paper we focus on collaborative referential tasks (akin to the classic matching tasks introduced by and) and take as a case study colour terms.", "labels": [], "entities": []}, {"text": "Our focus here is not on the explicit joint negotiation of effective terms, but rather on the deployment of flexible semantic representations that can adapt to the constraints imposed by the context and to the dialogue partner's language use.", "labels": [], "entities": []}, {"text": "We start by describing our algorithms for generation and resolution of colour descriptions in the next section.", "labels": [], "entities": [{"text": "generation and resolution of colour descriptions", "start_pos": 42, "end_pos": 90, "type": "TASK", "confidence": 0.8267691632111868}]}, {"text": "In sections 3 and 4, we present results of experiments that investigate how humans use colour terms for reference in production and comprehension.", "labels": [], "entities": []}, {"text": "Section 5 compares our model against the experimental data we have collected so far and discusses some directions for future work.", "labels": [], "entities": []}, {"text": "We end with a short conclusion in section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conducted two small experiments to collect data about how speakers and addressees use colour terms in referential tasks.", "labels": [], "entities": []}, {"text": "We created 12 different scenes, each consisting of four solid coloured squares, one of them the target (see for sample scenes).", "labels": [], "entities": []}, {"text": "Scenes were designed to take into account two parameters: basic and non-basic target colours, and without or with a competitor -a colour at a distance threshold from the target.", "labels": [], "entities": []}, {"text": "The target basic colours used were 'brown' and 'magenta' and the non-basic ones, 'rose' and 'sea blue'.", "labels": [], "entities": []}, {"text": "6 Each target colour appeared at least in one scene where there were no competitors.", "labels": [], "entities": []}, {"text": "We run a generation experiment (ExpA) and a resolution experiment (ExpB).", "labels": [], "entities": [{"text": "resolution experiment (ExpB)", "start_pos": 44, "end_pos": 72, "type": "METRIC", "confidence": 0.9042753458023072}]}, {"text": "In ExpA, participants were shown our 12 scenes and were asked to refer to the target with a colour term that would allow a potential addressee to identify it in the current context, but without reference to the other colours in the scene (to avoid comparatives such as 'the bluer square').", "labels": [], "entities": [{"text": "ExpA", "start_pos": 3, "end_pos": 7, "type": "DATASET", "confidence": 0.8089013695716858}]}, {"text": "In ExpB, participants were shown a scene and a colour term and were asked to pickup the intended referent.", "labels": [], "entities": []}, {"text": "The colour terms used in this second experiment were selected from those produced in ExpA -29 scene-term pairs in total.", "labels": [], "entities": []}, {"text": "Each scene appeared at least twice, once with a term with high occurrence frequency in ExpA, and once or twice with one or two terms that had been produced with low frequency.", "labels": [], "entities": [{"text": "ExpA", "start_pos": 87, "end_pos": 91, "type": "DATASET", "confidence": 0.8711062669754028}]}, {"text": "To minimize chances that subjects recognize the same scene more than once, we rotated and dispersed them evenly throughout.", "labels": [], "entities": []}, {"text": "Any colour within a Euclidean distance of 125 from the target was considered a competitor.", "labels": [], "entities": []}, {"text": "Compositional phrases may introduce more sophisticated effects.", "labels": [], "entities": []}, {"text": "However, the data on which our lexicon is based abstracted away from such details, treating them as simples.", "labels": [], "entities": []}, {"text": "A total of 36 native-English participants took part in the experiments: 19 in ExpA and 17 in ExpB.", "labels": [], "entities": [{"text": "ExpA", "start_pos": 78, "end_pos": 82, "type": "DATASET", "confidence": 0.7964571714401245}, {"text": "ExpB", "start_pos": 93, "end_pos": 97, "type": "DATASET", "confidence": 0.9036056995391846}]}, {"text": "Subjects for both experiments included undergraduate students, graduates students, and university faculty.", "labels": [], "entities": []}, {"text": "Both experiments were run online.", "labels": [], "entities": []}, {"text": "ExpA revealed there is high variability in the terms produced to refer to a single colour.", "labels": [], "entities": []}, {"text": "As expected, variability of terms generated for non-basic colours was higher than for basic colours.", "labels": [], "entities": []}, {"text": "For non-basic colours, variability of terms in scenes with competitors was higher.", "labels": [], "entities": []}, {"text": "shows the different terms produced fora basic colour ('brown') and a non-basic colour ('rose') in scenes without and with competitors, together with the proportional frequency of each term.", "labels": [], "entities": []}, {"text": "For the brown square target in a scene without competitors, the basic-colour term 'brown' was used with high frequency (72% of the time) while any other terms were used 1 or 2 times only.", "labels": [], "entities": []}, {"text": "In scenes with competitors, 'dark brown' had highest frequency with 'brown' almost as much (43% vs. 40%).", "labels": [], "entities": []}, {"text": "For the rose square target in a scene without competitors, there was also one term that stood out as the most frequent, 'pink', although its frequency (30%) is substantially lower to that of the basic-colour 'brown'.", "labels": [], "entities": [{"text": "frequency", "start_pos": 141, "end_pos": 150, "type": "METRIC", "confidence": 0.9826582074165344}]}, {"text": "In scenes with competitors there is an explosion in variation, with 'pink' still standing out but only with a proportional frequency of 21%.", "labels": [], "entities": [{"text": "variation", "start_pos": 52, "end_pos": 61, "type": "METRIC", "confidence": 0.9869644045829773}]}, {"text": "Overall, ExpA showed that speakers attempt to adapt their colour descriptions to the context and that there is high variability in the terms they choose to do this.", "labels": [], "entities": [{"text": "ExpA", "start_pos": 9, "end_pos": 13, "type": "DATASET", "confidence": 0.8519461750984192}]}, {"text": "ExpB showed that reference resolution is almost always successful despite the variation in colour terms observed in ExpA.", "labels": [], "entities": [{"text": "ExpB", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9474469423294067}, {"text": "reference resolution", "start_pos": 17, "end_pos": 37, "type": "TASK", "confidence": 0.8528462648391724}, {"text": "ExpA", "start_pos": 116, "end_pos": 120, "type": "DATASET", "confidence": 0.9224672317504883}]}, {"text": "For the basic colours in scenes with no competitors, participants successfully identified the targets in all cases, while in scenes with competitors they did so 98% of the time.", "labels": [], "entities": []}, {"text": "This was the case for both terms with proportionally high and low frequency.", "labels": [], "entities": []}, {"text": "For the non-basic colours in scenes with no competitors, the success rate in identifying the target was again 100% for both high and low frequency terms.", "labels": [], "entities": []}, {"text": "For scenes with competitors, there were differences depending on the frequency of the terms used: for high frequency terms there were once more no resolution errors, while the resolution success rate dropped to 78% where we used terms with low proportional frequency scores.", "labels": [], "entities": [{"text": "resolution errors", "start_pos": 147, "end_pos": 164, "type": "METRIC", "confidence": 0.9823208153247833}, {"text": "resolution success", "start_pos": 176, "end_pos": 194, "type": "METRIC", "confidence": 0.9722351729869843}]}, {"text": "A summary of these results is shown in", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1, together with the success  rate of our resolution algorithm ALIN.", "labels": [], "entities": [{"text": "ALIN", "start_pos": 70, "end_pos": 74, "type": "METRIC", "confidence": 0.829574704170227}]}]}