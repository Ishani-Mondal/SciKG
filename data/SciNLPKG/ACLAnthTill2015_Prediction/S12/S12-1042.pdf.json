{"title": [{"text": "UiO 2 : Sequence-Labeling Negation Using Dependency Features", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper describes the second of two systems submitted from the University of Oslo (UiO) to the 2012 *SEM Shared Task on resolving negation.", "labels": [], "entities": [{"text": "SEM Shared Task on resolving negation", "start_pos": 104, "end_pos": 141, "type": "TASK", "confidence": 0.661160409450531}]}, {"text": "The system combines SVM cue classification with CRF sequence labeling of events and scopes.", "labels": [], "entities": [{"text": "SVM cue classification", "start_pos": 20, "end_pos": 42, "type": "TASK", "confidence": 0.8857951362927755}, {"text": "CRF sequence labeling of events and scopes", "start_pos": 48, "end_pos": 90, "type": "TASK", "confidence": 0.7580127418041229}]}, {"text": "Models for scopes and events are created using lexical and syntactic features, together with a fine-grained set of labels that capture the scopal behavior of certain tokens.", "labels": [], "entities": []}, {"text": "Following labeling, negated tokens are assigned to their respective cues using simple post-processing heuristics.", "labels": [], "entities": []}, {"text": "The system was ranked first in the open track and third in the closed track, and was one of the top performers in the scope resolution sub-task overall.", "labels": [], "entities": [{"text": "scope resolution", "start_pos": 118, "end_pos": 134, "type": "TASK", "confidence": 0.8184934258460999}]}], "introductionContent": [{"text": "Negation Resolution (NR) is the task of determining, fora given sentence, which tokens are affected by a negation cue.", "labels": [], "entities": [{"text": "Negation Resolution (NR)", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8869097709655762}]}, {"text": "The data set most prominently used for the development of systems for automatic NR is the BioScope Corpus (), a collection of clinical reports and papers in the biomedical domain annotated with negation and speculation cues and their scopes.", "labels": [], "entities": [{"text": "NR", "start_pos": 80, "end_pos": 82, "type": "TASK", "confidence": 0.857279360294342}, {"text": "BioScope Corpus", "start_pos": 90, "end_pos": 105, "type": "DATASET", "confidence": 0.6918221265077591}]}, {"text": "The data sets released in conjunction with the 2012 shared task on NR hosted by The First Joint Conference on Lexical and Computational Semantics (*SEM 2012) are comprised of the following negation annotated stories of Conan Doyle (CD): a training set of 3644 sentences drawn from The Hound of the Baskervilles (CDT), a development set of 787 sentences taken from Wisteria Lodge (CDD; we will refer to the combination of CDT and CDD as CDTD), and a held-out test set of 1089 sentences from The Cardboard Box and The Red Circle (CDE).", "labels": [], "entities": [{"text": "NR hosted by The First Joint Conference on Lexical and Computational Semantics (*SEM 2012)", "start_pos": 67, "end_pos": 157, "type": "TASK", "confidence": 0.7520096767693758}, {"text": "The Cardboard Box and The Red Circle (CDE)", "start_pos": 490, "end_pos": 532, "type": "DATASET", "confidence": 0.916775381565094}]}, {"text": "In these sets, the concept of negation scope extends on the one adopted in the BioScope corpus in several aspects: Negation cues are not part of the scope, morphological (affixal) cues are annotated and scopes can be discontinuous.", "labels": [], "entities": [{"text": "negation scope", "start_pos": 30, "end_pos": 44, "type": "TASK", "confidence": 0.8746000230312347}, {"text": "BioScope corpus", "start_pos": 79, "end_pos": 94, "type": "DATASET", "confidence": 0.8949794471263885}]}, {"text": "Moreover, in-scope states or events are marked as negated if they are factual and presented as events that did not happen).", "labels": [], "entities": []}, {"text": "Examples (1) and (2) below are examples of affixal negation and discontinuous scope respectively: The cues are bold, the tokens contained within their scopes are underlined and the negated event is italicized.", "labels": [], "entities": []}, {"text": "(1) Since we have been so unfortunate as to miss him (2) If he was in the hospital and yet not on the staff he could only have been a house-surgeon or a house-physician: little more than a senior student.", "labels": [], "entities": []}, {"text": "Example (2) has no negated events because the sentence is non-factual.", "labels": [], "entities": []}, {"text": "The *SEM shared task thus comprises three subtasks: cue identification, scope resolution and event detection.", "labels": [], "entities": [{"text": "SEM shared task", "start_pos": 5, "end_pos": 20, "type": "TASK", "confidence": 0.8127044041951498}, {"text": "cue identification", "start_pos": 52, "end_pos": 70, "type": "TASK", "confidence": 0.7903097867965698}, {"text": "scope resolution", "start_pos": 72, "end_pos": 88, "type": "TASK", "confidence": 0.7556577920913696}, {"text": "event detection", "start_pos": 93, "end_pos": 108, "type": "TASK", "confidence": 0.7281049937009811}]}, {"text": "It is furthermore divided into two separate tracks: one closed track, where only the data supplied by the organizers (word form, lemma, PoS-tag and syntactic constituent for each token) maybe employed, and an open track, where participants may employ any additional tools or resources.", "labels": [], "entities": []}, {"text": "Pragmatically speaking, a token can be either out of scope or assigned to one or more of the three remaining classes: negation cue, in scope and negated event.", "labels": [], "entities": []}, {"text": "Additionally, in-scope tokens and negated events are paired to the cues they are negated by.", "labels": [], "entities": []}, {"text": "Our system achieves this by remodeling the task as a sequence labeling task.", "labels": [], "entities": [{"text": "sequence labeling task", "start_pos": 53, "end_pos": 75, "type": "TASK", "confidence": 0.7362077434857687}]}, {"text": "With annotations converted to sequences of labels, we train a Conditional Random Field (CRF) classifier with a range of different feature types, including features defined over dependency graphs.", "labels": [], "entities": []}, {"text": "This article presents two submissions for the *SEM shared task, differing only with respect to how these dependency graphs were derived.", "labels": [], "entities": [{"text": "SEM shared task", "start_pos": 47, "end_pos": 62, "type": "TASK", "confidence": 0.8556306958198547}]}, {"text": "For our open track submission, the dependency representations are produced by a state-of-the-art dependency parser, whereas the closed track submission employs dependencies derived from the constituent analyses supplied with the shared task data sets through a process of constituent-to-dependency conversion.", "labels": [], "entities": []}, {"text": "In both systems, labeling of test data is performed in two stages.", "labels": [], "entities": [{"text": "labeling of test", "start_pos": 17, "end_pos": 33, "type": "TASK", "confidence": 0.8654387791951498}]}, {"text": "First, cues are detected using a token classifier, 1 and secondly, scope and event resolution is achieved by post-processing the output of the sequence labeler.", "labels": [], "entities": [{"text": "scope", "start_pos": 67, "end_pos": 72, "type": "METRIC", "confidence": 0.9872819781303406}]}, {"text": "The two systems described in this paper have been developed using CDT for training and CDD for testing, and differ only with regard to the source of syntactic information.", "labels": [], "entities": []}, {"text": "All reported scores are generated using an evaluation script provided by the task organizers.", "labels": [], "entities": []}, {"text": "In addition to providing a full end-to-end evaluation, the script breaks down results with respect to identification of cues, events, scope tokens, and two variants of scope-level exact match; one requiring exact match also of cues and another only partial cue match.", "labels": [], "entities": []}, {"text": "For our system these two scopelevel scores are identical and so are not duplicated in our reporting.", "labels": [], "entities": []}, {"text": "Additionally we chose not to optimize for the scope tokens measure, and hence this is also not reported as a development result.", "labels": [], "entities": []}, {"text": "Note also that the official evaluation actually includes two different variants of the metrics mentioned above; a set of primary measures with precision computed as P=TP/(TP+FP) and a set of B measures where precision is rather computed as P=TP/SYS, where SYS is the total number of predictions made by the system.", "labels": [], "entities": [{"text": "precision", "start_pos": 143, "end_pos": 152, "type": "METRIC", "confidence": 0.9966591596603394}, {"text": "TP/(TP+FP", "start_pos": 167, "end_pos": 176, "type": "METRIC", "confidence": 0.7585885047912597}, {"text": "precision", "start_pos": 208, "end_pos": 217, "type": "METRIC", "confidence": 0.99797123670578}]}, {"text": "The reason why SYS is not identical with TP+FP is that partial matches are Note that the cue classifier applied in the current paper is the same as that used in the other shared task submission from the University of Oslo ( , and the two system descriptions will therefore have much overlap on this particular point.", "labels": [], "entities": [{"text": "FP", "start_pos": 44, "end_pos": 46, "type": "METRIC", "confidence": 0.7469920516014099}]}, {"text": "For all other components the architectures of the two system are completely different, however.", "labels": [], "entities": []}, {"text": "only counted as FNs (and not FPs) in order to avoid double penalties.", "labels": [], "entities": [{"text": "FNs", "start_pos": 16, "end_pos": 19, "type": "DATASET", "confidence": 0.8003949522972107}]}, {"text": "We do not report the B measures for development testing as they were introduced for the final evaluation and hence were not considered in our system optimization.", "labels": [], "entities": [{"text": "B", "start_pos": 21, "end_pos": 22, "type": "METRIC", "confidence": 0.9886894822120667}]}, {"text": "We note though, that the relative-ranking of participating systems for the primary and B measures is identical, and that the correlation between the paired lists of scores is nearly perfect (r=0.997).", "labels": [], "entities": []}, {"text": "The rest of the paper is structured as follows.", "labels": [], "entities": []}, {"text": "First, the cue classifier, its features and results are described in Section 2.", "labels": [], "entities": []}, {"text": "Section 3 presents the system for scope and event resolution and details different features, the model-internal representation used for sequence-labeling, as well as the post-processing component.", "labels": [], "entities": [{"text": "event resolution", "start_pos": 44, "end_pos": 60, "type": "TASK", "confidence": 0.6912524551153183}]}, {"text": "Error analyses for the cue, scope and event components are provided in the respective sections.", "labels": [], "entities": []}, {"text": "Section 4 and 5 provide developmental and held-out results, respectively.", "labels": [], "entities": []}, {"text": "Finally, we provide conclusions and some reflections regarding future work in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "Final results on held-out data for both closed and open track submissions are reported in.", "labels": [], "entities": []}, {"text": "For the final run, we trained our systems on CDTD.", "labels": [], "entities": [{"text": "CDTD", "start_pos": 45, "end_pos": 49, "type": "DATASET", "confidence": 0.9716159701347351}]}, {"text": "We observe a similar relative performance to our development results, with the open track system outperforming the closed track one, albeit by a smaller margin than what we saw in development.", "labels": [], "entities": []}, {"text": "We are also surprised to see that despite not addressing discontinuous scopes directly, our system obtained the best score on scope resolution (according to the metric dubbed \"Scopes (cue match)\").", "labels": [], "entities": [{"text": "scope resolution", "start_pos": 126, "end_pos": 142, "type": "TASK", "confidence": 0.642079770565033}]}], "tableCaptions": [{"text": " Table 1: Cue classification results for the final classifier  and the majority-usage baseline, showing test scores for  the development set (training on CDT) and the final held- out set (training on CDTD).", "labels": [], "entities": [{"text": "Cue classification", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.8108593225479126}]}, {"text": " Table 3: Frequency distribution of parts of speech over  the S, MCUE and CUE labels in CDTD.", "labels": [], "entities": []}, {"text": " Table 4: Full negation results on CDD with gold cues.", "labels": [], "entities": [{"text": "negation", "start_pos": 15, "end_pos": 23, "type": "TASK", "confidence": 0.7618420720100403}]}, {"text": " Table 5: Results for scope resolution on CDD with gold cues.", "labels": [], "entities": [{"text": "scope resolution", "start_pos": 22, "end_pos": 38, "type": "TASK", "confidence": 0.9555475115776062}]}, {"text": " Table 6: End-to-end results on the held-out data.", "labels": [], "entities": [{"text": "End-to-end", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.974277138710022}]}]}