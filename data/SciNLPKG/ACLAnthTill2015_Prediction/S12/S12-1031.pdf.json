{"title": [{"text": "The Effects of Semantic Annotations on Precision Parse Ranking", "labels": [], "entities": [{"text": "Precision Parse", "start_pos": 39, "end_pos": 54, "type": "TASK", "confidence": 0.8319981098175049}]}], "abstractContent": [{"text": "We investigate the effects of adding semantic annotations including word sense hypernyms to the source text for use as an extra source of information in HPSG parse ranking for the English Resource Grammar.", "labels": [], "entities": [{"text": "HPSG parse ranking", "start_pos": 153, "end_pos": 171, "type": "TASK", "confidence": 0.6683990756670634}, {"text": "English Resource Grammar", "start_pos": 180, "end_pos": 204, "type": "DATASET", "confidence": 0.8168070316314697}]}, {"text": "The semantic annotations are coarse semantic categories or entries from a distributional thesaurus, assigned either heuristically or by a pre-trained tagger.", "labels": [], "entities": []}, {"text": "We test this using two test corpora in different domains with various sources of training data.", "labels": [], "entities": []}, {"text": "The best reduces error rate in dependency F-score by 1% on average, while some methods produce substantial decreases in performance.", "labels": [], "entities": [{"text": "error rate", "start_pos": 17, "end_pos": 27, "type": "METRIC", "confidence": 0.9855038821697235}, {"text": "F-score", "start_pos": 42, "end_pos": 49, "type": "METRIC", "confidence": 0.9511797428131104}]}], "introductionContent": [{"text": "Most start-of-the-art natural language parsers use lexicalised features for parse ranking.", "labels": [], "entities": [{"text": "parse ranking", "start_pos": 76, "end_pos": 89, "type": "TASK", "confidence": 0.8832729160785675}]}, {"text": "These are important to achieve optimal parsing accuracy, and yet these are also the features which by their nature suffer from data-sparseness problems in the training data.", "labels": [], "entities": [{"text": "parsing", "start_pos": 39, "end_pos": 46, "type": "TASK", "confidence": 0.9824336767196655}, {"text": "accuracy", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.892875075340271}]}, {"text": "In the absence of reliable fine-grained statistics fora given token, various strategies are possible.", "labels": [], "entities": []}, {"text": "There will often be statistics available for coarser categories, such as the POS of the particular token.", "labels": [], "entities": [{"text": "POS", "start_pos": 77, "end_pos": 80, "type": "METRIC", "confidence": 0.9689809083938599}]}, {"text": "However, it is possible that these coarser representations discard too much, missing out information which could be valuable to the parse ranking.", "labels": [], "entities": []}, {"text": "assume we wish to correctly attach the prepositional phrases in the following examples: (1) I saw a tree with my telescope (2) I saw a tree with no leaves The most obvious interpretation in each case has the prepositional phrase headed by with attaching in different places: to the verb phrase in the first example, and to the noun tree in the second.", "labels": [], "entities": []}, {"text": "Such distinctions are difficult fora parser to make when the training data is sparse, but imagine we had seen examples such as the following in the training corpus:", "labels": [], "entities": []}], "datasetContent": [{"text": "Our primary evaluation metric is Elementary Dependency Match ().", "labels": [], "entities": [{"text": "Elementary Dependency Match", "start_pos": 33, "end_pos": 60, "type": "METRIC", "confidence": 0.6773826579252878}]}, {"text": "This converts the semantic output of the ERG into a set of dependency-like triples, and scores these triples using precision, recall and F-score as is conventional for other dependency evaluation.", "labels": [], "entities": [{"text": "precision", "start_pos": 115, "end_pos": 124, "type": "METRIC", "confidence": 0.9993481040000916}, {"text": "recall", "start_pos": 126, "end_pos": 132, "type": "METRIC", "confidence": 0.9963023662567139}, {"text": "F-score", "start_pos": 137, "end_pos": 144, "type": "METRIC", "confidence": 0.9925629496574402}]}, {"text": "Following, we use the EDM NA mode of evaluation, which provides a good level of comparability while still reflecting most the semantically salient information from the grammar.", "labels": [], "entities": []}, {"text": "Other work on the ERG and related grammars has tended to focus on exact tree match, but the granular EDM metric is a better fit for our needs hereamong other reasons, it is more sensitive in terms of error rate reduction to changes in parse selection models).", "labels": [], "entities": [{"text": "exact tree match", "start_pos": 66, "end_pos": 82, "type": "METRIC", "confidence": 0.8226274450620016}]}, {"text": "Additionally, it is desirable to be able to choose between two different parses which do not match the gold standard exactly but when one of the parses is a closer match than the other; this is not possible with exact match accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 224, "end_pos": 232, "type": "METRIC", "confidence": 0.9084838032722473}]}], "tableCaptions": [{"text": " Table 1: Corpora used in our experiments, with total sen- tences, how many of those can be parsed, how many of  the parseable sentences have a single gold parse (and are  used in these experiments), and average sentence length", "labels": [], "entities": []}, {"text": " Table 1.  With these corpora, we are able to investigate in- domain and cross-domain effects, by testing on a", "labels": [], "entities": []}, {"text": " Table 2: Results for SS (WNF) (supersense from first WordNet sense), evaluated on 23k tokens (approx 1500  sentences) of either WESCIENCE or LOGON, and trained on various sizes of in-domain and cross-domain training  data. Subscript ' p ' indicates mappings were applied to leaf parents rather than leaves.", "labels": [], "entities": [{"text": "WordNet sense", "start_pos": 54, "end_pos": 67, "type": "DATASET", "confidence": 0.915513664484024}, {"text": "LOGON", "start_pos": 142, "end_pos": 147, "type": "METRIC", "confidence": 0.8257498741149902}]}, {"text": " Table 3: Results for SS (SST) (supersense from SuperSense Tagger)", "labels": [], "entities": [{"text": "SS (SST)", "start_pos": 22, "end_pos": 30, "type": "TASK", "confidence": 0.8161175549030304}, {"text": "SuperSense Tagger", "start_pos": 48, "end_pos": 65, "type": "TASK", "confidence": 0.6902927756309509}]}, {"text": " Table 4: Results for HPWNF (hypernym path from first WordNet sense)", "labels": [], "entities": [{"text": "HPWNF", "start_pos": 22, "end_pos": 27, "type": "DATASET", "confidence": 0.9199384450912476}, {"text": "WordNet sense", "start_pos": 54, "end_pos": 67, "type": "DATASET", "confidence": 0.9264577627182007}]}, {"text": " Table 5: Results for LDT (5) (Lin-style distributional thesaurus, expanding each term with the top-5 most similar)", "labels": [], "entities": []}]}