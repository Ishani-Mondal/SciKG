{"title": [{"text": "Monolingual Distributional Similarity for Text-to-Text Generation", "labels": [], "entities": [{"text": "Monolingual Distributional Similarity", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.7149803737799326}]}], "abstractContent": [{"text": "Previous work on paraphrase extraction and application has relied on either parallel datasets, or on distributional similarity met-rics overlarge text corpora.", "labels": [], "entities": [{"text": "paraphrase extraction", "start_pos": 17, "end_pos": 38, "type": "TASK", "confidence": 0.9683084189891815}]}, {"text": "Our approach combines these two orthogonal sources of information and directly integrates them into our paraphrasing system's log-linear model.", "labels": [], "entities": []}, {"text": "We compare different distributional similarity feature-sets and show significant improvements in grammaticality and meaning retention on the example text-to-text generation task of sentence compression, achieving state-of-the-art quality.", "labels": [], "entities": [{"text": "text-to-text generation", "start_pos": 149, "end_pos": 172, "type": "TASK", "confidence": 0.7528026103973389}, {"text": "sentence compression", "start_pos": 181, "end_pos": 201, "type": "TASK", "confidence": 0.7103869467973709}]}], "introductionContent": [{"text": "A wide variety of applications in natural language processing can be cast in terms of text-to-text generation.", "labels": [], "entities": [{"text": "text-to-text generation", "start_pos": 86, "end_pos": 109, "type": "TASK", "confidence": 0.7165705114603043}]}, {"text": "Given input in the form of natural language, a text-to-text generation system produces natural language output that is subject to a set of constraints.", "labels": [], "entities": []}, {"text": "Compression systems, for instance, produce shorter sentences.", "labels": [], "entities": []}, {"text": "Paraphrases, i.e. differing textual realizations of the same meaning, area crucial components of text-to-text generation systems, and have been successfully applied to tasks such as multi-document summarization (, query expansion, question answering), sentence compression, and simplification.", "labels": [], "entities": [{"text": "text-to-text generation", "start_pos": 97, "end_pos": 120, "type": "TASK", "confidence": 0.748638391494751}, {"text": "multi-document summarization", "start_pos": 182, "end_pos": 210, "type": "TASK", "confidence": 0.6686312854290009}, {"text": "query expansion", "start_pos": 214, "end_pos": 229, "type": "TASK", "confidence": 0.7154505550861359}, {"text": "question answering", "start_pos": 231, "end_pos": 249, "type": "TASK", "confidence": 0.806033581495285}, {"text": "sentence compression", "start_pos": 252, "end_pos": 272, "type": "TASK", "confidence": 0.7928816676139832}]}, {"text": "Paraphrase collections for text-to-text generation have been extracted from a variety of different corpora.", "labels": [], "entities": [{"text": "text-to-text generation", "start_pos": 27, "end_pos": 50, "type": "TASK", "confidence": 0.722023993730545}]}, {"text": "Several approaches rely on bilingual parallel data), while others leverage distributional methods on monolingual text corpora (.", "labels": [], "entities": []}, {"text": "So far, however, only preliminary studies have been undertaken to combine the information from these two sources.", "labels": [], "entities": []}, {"text": "In this paper, we describe an extension of's bilingual data-based approach.", "labels": [], "entities": []}, {"text": "We augment the bilingually-sourced paraphrases using features based on monolingual distributional similarity.", "labels": [], "entities": []}, {"text": "More specifically: \u2022 We show that using monolingual distributional similarity features improves paraphrase quality beyond what we can achieve with features estimated from bilingual data.", "labels": [], "entities": []}, {"text": "\u2022 We define distributional similarity for paraphrase patterns that contain constituent-level gaps, e.g. sim(one JJ instance of NP , a JJ case of NP ).", "labels": [], "entities": []}, {"text": "This generalizes over distributional similarity for contiguous phrases.", "labels": [], "entities": []}, {"text": "\u2022 We compare different types of monolingual distributional information and show that they can be used to achieve significant improvements in grammaticality.", "labels": [], "entities": []}, {"text": "\u2022 Finally, we compare our method to several strong baselines on the text-to-text generation task of sentence compression.", "labels": [], "entities": [{"text": "text-to-text generation", "start_pos": 68, "end_pos": 91, "type": "TASK", "confidence": 0.7261083573102951}, {"text": "sentence compression", "start_pos": 100, "end_pos": 120, "type": "TASK", "confidence": 0.7238511890172958}]}, {"text": "Our method shows state-of-the-art results, beating a purely bilingually sourced paraphrasing system.", "labels": [], "entities": []}], "datasetContent": [{"text": "To rate the quality of our output, we solicit human judgments of the compressions along two five-point scales: grammaticality and meaning preservation.", "labels": [], "entities": [{"text": "meaning preservation", "start_pos": 130, "end_pos": 150, "type": "TASK", "confidence": 0.7578285038471222}]}, {"text": "Judges are instructed to decide how much the meaning from a reference translation is retained in the compressed sentence, with a score of 5 indicating that all of the important information is present, and 1 being that the compression does not retain any of the original meaning.", "labels": [], "entities": []}, {"text": "Similarly, a grammar score of 5 indicates perfect grammaticality, while a score of 1 is assigned to sentences that are entirely ungrammatical.", "labels": [], "entities": []}, {"text": "We ran our evaluation on Mechanical Turk, where a total of 126 judges provided 3 redundant judgments for each system output.", "labels": [], "entities": [{"text": "Mechanical Turk", "start_pos": 25, "end_pos": 40, "type": "DATASET", "confidence": 0.77144455909729}]}, {"text": "To provide additional quality control, our HITs were augmented with both positive and negative control compressions.", "labels": [], "entities": []}, {"text": "For the positive control we used the reference compressions from our test set.", "labels": [], "entities": []}, {"text": "Negative control was provided by adding a compression model based on random word deletions to the mix.", "labels": [], "entities": []}, {"text": "In we compare our distributional similarity-augmented systems to the plain pivotingbased baseline and the ILP approach.", "labels": [], "entities": []}, {"text": "The compression ratios of the paraphrasing systems are tuned to match the average compression ratio seen on the development and test set.", "labels": [], "entities": [{"text": "compression", "start_pos": 4, "end_pos": 15, "type": "METRIC", "confidence": 0.9818294048309326}]}, {"text": "The ILP system is config-ured to loosely match this ratio, as to not overly constrain its search space.", "labels": [], "entities": []}, {"text": "Our results indicate that the paraphrase approach significantly outperforms ILP on meaning retention.", "labels": [], "entities": [{"text": "meaning retention", "start_pos": 83, "end_pos": 100, "type": "TASK", "confidence": 0.7904181480407715}]}, {"text": "However, the baseline system shows notable weaknesses in grammaticality.", "labels": [], "entities": []}, {"text": "Adding the n-gram distributional similarity model to the paraphraser recovers some of the difference in grammaticality while simultaneously yielding some gain in the compressions' meaning retention.", "labels": [], "entities": []}, {"text": "Moving to distributional similarity estimated on the syntactic feature-set yields additional improvement, despite the model's lower coverage.", "labels": [], "entities": []}, {"text": "It is known that human evaluation scores correlate linearly with the compression ratio produced by a sentence compression system ).", "labels": [], "entities": []}, {"text": "Thus, to ensure fairness in our comparisons, we produce a pairwise comparison breakdown that only takes into account compressions of almost identical length.", "labels": [], "entities": []}, {"text": "2 shows the results of this analysis, detailing the number of wins and ties in the human judgements.", "labels": [], "entities": []}, {"text": "We note that the gains in meaning retention over both the baseline and the ILP system are still present in the pairwise breakdown.", "labels": [], "entities": []}, {"text": "The gains over the paraphrasing baseline, as well as the improvement in meaning over ILP are statistically significant at p < 0.05 (using the sign test).", "labels": [], "entities": []}, {"text": "We can observe that there is substantial overlap between the baseline paraphraser and the n-gram model, while the syntax model appears to yield noticeably different output far more often.", "labels": [], "entities": []}, {"text": "shows two example sentences drawn from our test set and the compressions produced by the different systems.", "labels": [], "entities": []}, {"text": "It can be seen that both the paraphrase-based and ILP systems produce good quality results, with the paraphrase system retaining the meaning of the source sentence more accurately.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results of the human evaluation on longer  compressions: pairwise compression rates (CR),  meaning and grammaticality scores. Bold indicates  a statistically significance difference at p < 0.05.", "labels": [], "entities": [{"text": "pairwise compression rates (CR)", "start_pos": 67, "end_pos": 98, "type": "METRIC", "confidence": 0.8793635070323944}]}]}