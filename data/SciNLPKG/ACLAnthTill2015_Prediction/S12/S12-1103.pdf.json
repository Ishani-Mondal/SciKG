{"title": [{"text": "JU_CSE_NLP: Language Independent Cross-lingual Textual Entailment System", "labels": [], "entities": [{"text": "JU_CSE_NLP", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.7612384915351867}, {"text": "Language Independent Cross-lingual Textual Entailment", "start_pos": 12, "end_pos": 65, "type": "TASK", "confidence": 0.5276775777339935}]}], "abstractContent": [{"text": "This article presents the experiments carried out at Jadavpur University as part of the participation in Cross-lingual Textual Entailment for Content Synchronization (CLTE) of task 8 @ Semantic Evaluation Exercises (SemEval-2012).", "labels": [], "entities": [{"text": "Cross-lingual Textual Entailment for Content Synchronization (CLTE)", "start_pos": 105, "end_pos": 172, "type": "TASK", "confidence": 0.6971363061004214}]}, {"text": "The work explores cross-lingual textual entailment as a relation between two texts in different languages and proposes different measures for entailment decision in a four way classification tasks (forward, backward, bidi-rectional and no-entailment).", "labels": [], "entities": [{"text": "cross-lingual textual entailment", "start_pos": 18, "end_pos": 50, "type": "TASK", "confidence": 0.6206964353720347}]}, {"text": "We setup different heuristics and measures for evaluating the entailment between two texts based on lexical relations.", "labels": [], "entities": []}, {"text": "Experiments have been carried outwith both the text and hypothesis converted to the same language using the Microsoft Bing translation system.", "labels": [], "entities": [{"text": "Microsoft Bing translation", "start_pos": 108, "end_pos": 134, "type": "TASK", "confidence": 0.556030809879303}]}, {"text": "The entailment system considers Named Entity, Noun Chunks, Part of speech, N-Gram and some text similarity measures of the text pair to decide the en-tailment judgments.", "labels": [], "entities": []}, {"text": "Rules have been developed to encounter the multi way entailment issue.", "labels": [], "entities": []}, {"text": "Our system decides on the entailment judgment after comparing the entailment scores for the text pairs.", "labels": [], "entities": []}, {"text": "Four different rules have been developed for the four different classes of entailment.", "labels": [], "entities": []}, {"text": "The best run is submitted for Italian-English language with accuracy 0.326.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.9992455244064331}]}], "introductionContent": [{"text": "Textual Entailment (TE)) is one of the recent challenges of Natural Language Processing (NLP).", "labels": [], "entities": [{"text": "Textual Entailment (TE))", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8187819063663483}, {"text": "Natural Language Processing (NLP)", "start_pos": 60, "end_pos": 93, "type": "TASK", "confidence": 0.7315278947353363}]}, {"text": "The Task 8 of SemEval-2012 1 defines a textual entailment system that specifies two major aspects: the task is based on cross-lingual corpora and the entailment decision must be four ways.", "labels": [], "entities": []}, {"text": "Given a pair of topically related text fragments (T1 and T2) in different languages, the CLTE task consists of automatically annotating it with one of the following entailment judgments: i.", "labels": [], "entities": []}, {"text": "Bidirectional (T1 ->T2 & T1 <-T2): the two fragments entail each other (semantic equivalence) ii.", "labels": [], "entities": []}, {"text": "Forward (T1 -> T2 & T1!<-T2): unidirectional entailment from T1 to T2 . iii.", "labels": [], "entities": []}, {"text": "Backward (T1! -> T2 & T1 <-T2): unidirectional entailment from T2 to T1. iv. No Entailment (T1!", "labels": [], "entities": []}, {"text": "<-T2): there is no entailment between T1 and T2.", "labels": [], "entities": []}, {"text": "CLTE (Cross Lingual Textual Entailment) task consists of 1,000 CLTE dataset pairs (500 for training and 500 for test) available for the following language combinations: -Spanish/English (spa-eng) -German/English (deu-eng).", "labels": [], "entities": [{"text": "CLTE (Cross Lingual Textual Entailment)", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.6935129335948399}]}, {"text": "-Italian/English (ita-eng) -French/English (fra-eng) Seven Recognizing Textual Entailment (RTE) evaluation tracks have already been held: RTE-1 in 2005, RTE-2 in 2006, RTE-3 in 2007, RTE-4 in 2008, RTE-5 in 2009, RTE-6 in 2010 and RTE-7 in 2011.", "labels": [], "entities": [{"text": "RTE-6", "start_pos": 213, "end_pos": 218, "type": "DATASET", "confidence": 0.8080282807350159}, {"text": "RTE-7", "start_pos": 231, "end_pos": 236, "type": "DATASET", "confidence": 0.791167140007019}]}, {"text": "RTE task produces a generic framework for entailment task across NLP applications.", "labels": [], "entities": []}, {"text": "The RTE challenges have moved from 2 -way entailment task (YES, NO) to 3 -way task.", "labels": [], "entities": [{"text": "RTE", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.7614058256149292}, {"text": "YES, NO)", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.7330925166606903}]}, {"text": "EVALITA/IRTE task is similar to the RTE challenge for the Italian language.", "labels": [], "entities": [{"text": "EVALITA", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.763822615146637}, {"text": "IRTE", "start_pos": 8, "end_pos": 12, "type": "METRIC", "confidence": 0.4374336302280426}, {"text": "RTE challenge", "start_pos": 36, "end_pos": 49, "type": "TASK", "confidence": 0.5917137563228607}]}, {"text": "So far, TE has been applied only in a monolingual setting.", "labels": [], "entities": [{"text": "TE", "start_pos": 8, "end_pos": 10, "type": "TASK", "confidence": 0.685118556022644}]}, {"text": "Cross-lingual Textual Entailment (CLTE) has been proposed (,,) as an extension of Textual Entailment.", "labels": [], "entities": [{"text": "Cross-lingual Textual Entailment (CLTE)", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.7538877377907435}]}, {"text": "In 2010, Parser Training and Evaluation using Textual Entailment was organized by SemEval-2.", "labels": [], "entities": [{"text": "Parser Training and Evaluation", "start_pos": 9, "end_pos": 39, "type": "TASK", "confidence": 0.7905929684638977}]}, {"text": "Recognizing Inference in Text (RITE) 2 organized by NTCIR-9 in 2011 is the first to expand TE as a 5-way entailment task (forward, backward, bi-directional, contradiction and independent) in a monolingual scenario.", "labels": [], "entities": [{"text": "Recognizing Inference in Text (RITE) 2 organized by NTCIR-9 in 2011", "start_pos": 0, "end_pos": 67, "type": "TASK", "confidence": 0.7877319821944604}, {"text": "TE", "start_pos": 91, "end_pos": 93, "type": "TASK", "confidence": 0.933777928352356}]}, {"text": "We have participated in RTE-5, RTE-6, RTE-7, SemEval-2 Parser Training and Evaluation using Textual Entailment Task and RITE.", "labels": [], "entities": [{"text": "RTE-5", "start_pos": 24, "end_pos": 29, "type": "DATASET", "confidence": 0.5903736352920532}, {"text": "RTE-6", "start_pos": 31, "end_pos": 36, "type": "DATASET", "confidence": 0.6052075624465942}, {"text": "RTE-7", "start_pos": 38, "end_pos": 43, "type": "DATASET", "confidence": 0.7050725817680359}, {"text": "RITE", "start_pos": 120, "end_pos": 124, "type": "METRIC", "confidence": 0.6691377758979797}]}, {"text": "Section 2 describes our Cross-lingual Textual Entailment system.", "labels": [], "entities": [{"text": "Cross-lingual Textual Entailment", "start_pos": 24, "end_pos": 56, "type": "TASK", "confidence": 0.6396559178829193}]}, {"text": "The various experiments carried out on the development and test data sets are described in Section 3 along with the results.", "labels": [], "entities": []}, {"text": "The conclusions are drawn in Section 4.", "labels": [], "entities": []}], "datasetContent": [{"text": "Three runs (Run 1, Run 2 and Run 3) for each language were submitted for the SemEval-3 Task 8.", "labels": [], "entities": [{"text": "SemEval-3 Task 8", "start_pos": 77, "end_pos": 93, "type": "TASK", "confidence": 0.8168423970540365}]}, {"text": "The descriptions of submissions for the CLTE task are as follows: \u2022 Run1: Lexical matching between text pairs (Based on system Architecture -1).", "labels": [], "entities": [{"text": "Lexical matching between text pairs", "start_pos": 74, "end_pos": 109, "type": "TASK", "confidence": 0.8990408778190613}]}, {"text": "\u2022 Run2: Lexical matching between text pairs (Based on System Architecture -2).", "labels": [], "entities": [{"text": "Lexical matching between text pairs", "start_pos": 8, "end_pos": 43, "type": "TASK", "confidence": 0.9141039729118348}]}, {"text": "\u2022 Run3: ANDing Module between Run1 and Run2.", "labels": [], "entities": []}, {"text": "(Based on System Architecture -3).", "labels": [], "entities": []}, {"text": "The CLTE dataset consists of 500 training CLTE pairs and 500 test CLTE pairs.", "labels": [], "entities": [{"text": "CLTE dataset", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.8799658119678497}]}, {"text": "The results for Run 1, Run 2 and Run 3 for each language on CLTE Development set are shown in.: Results on Development set", "labels": [], "entities": [{"text": "CLTE Development set", "start_pos": 60, "end_pos": 80, "type": "DATASET", "confidence": 0.926792323589325}]}], "tableCaptions": [{"text": " Table 1: Results on Development set", "labels": [], "entities": []}, {"text": " Table 2: Results on Test Set", "labels": [], "entities": []}]}