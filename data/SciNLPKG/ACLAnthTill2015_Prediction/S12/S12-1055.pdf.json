{"title": [{"text": "UTD: Determining Relational Similarity Using Lexical Patterns", "labels": [], "entities": [{"text": "Determining Relational Similarity", "start_pos": 5, "end_pos": 38, "type": "TASK", "confidence": 0.819084882736206}]}], "abstractContent": [{"text": "In this paper we present our approach for assigning degrees of relational similarity to pairs of words in the SemEval-2012 Task 2.", "labels": [], "entities": [{"text": "SemEval-2012 Task 2", "start_pos": 110, "end_pos": 129, "type": "TASK", "confidence": 0.8034007350603739}]}, {"text": "To measure relational similarity we employed lexical patterns that can match against word pairs within a large corpus of 12 million documents.", "labels": [], "entities": []}, {"text": "Patterns are weighted by obtaining statistically estimated lower bounds on their precision for extracting word pairs from a given relation.", "labels": [], "entities": [{"text": "precision", "start_pos": 81, "end_pos": 90, "type": "METRIC", "confidence": 0.998559296131134}]}, {"text": "Finally, word pairs are ranked based on a model predicting the probability that they belong to the relation of interest.", "labels": [], "entities": []}, {"text": "This approach achieved the best results on the Se-mEval 2012 Task 2, obtaining a Spearman correlation of 0.229 and an accuracy on reproducing human answers to MaxDiff questions of 39.4%.", "labels": [], "entities": [{"text": "Se-mEval 2012 Task 2", "start_pos": 47, "end_pos": 67, "type": "DATASET", "confidence": 0.7455432116985321}, {"text": "Spearman correlation", "start_pos": 81, "end_pos": 101, "type": "METRIC", "confidence": 0.9513565003871918}, {"text": "accuracy", "start_pos": 118, "end_pos": 126, "type": "METRIC", "confidence": 0.9995110034942627}]}], "introductionContent": [{"text": "Considerable prior research has examined and elaborated upon a wide variety of semantic relations between concepts along with techniques for automatically discovering pairs of concepts for which a relation holds (;).", "labels": [], "entities": []}, {"text": "However, most previous work has considered membership assignment fora semantic relation as a binary property.", "labels": [], "entities": [{"text": "membership assignment", "start_pos": 43, "end_pos": 64, "type": "TASK", "confidence": 0.9389829337596893}]}, {"text": "In this paper we discuss an approach which assigns a degree of membership to a pair of concepts fora given relation.", "labels": [], "entities": []}, {"text": "For example, for the semantic relation CLASS-INCLUSION (Taxonomic), the concept pairs weapon:spear and bird:robin are stronger members Consider the following word pairs: millionaire:money, author:copyright, robin:nest.", "labels": [], "entities": []}, {"text": "These X:Y pairs share a relation \"X R Y\".", "labels": [], "entities": []}, {"text": "Now consider the following word pairs: (1) teacher:students (2) farmer:crops (3) homeowner:door (4) shrubs:roots Which of the numbered word pairs is the MOST illustrative example of the same relation \"X R Y\"?", "labels": [], "entities": []}, {"text": "Which of the above numbered word pairs is the LEAST illustrative example of the same relation \"X R Y\"? of the relationship than hair:brown, because brown may describe many things other than hair, and brown is also used much less frequently as a noun than the words in the first two word pairs.", "labels": [], "entities": [{"text": "LEAST", "start_pos": 46, "end_pos": 51, "type": "METRIC", "confidence": 0.8903712034225464}]}, {"text": "Task 2 of SemEval 2012) was designed to evaluate the effectiveness of automatic approaches for determining the similarity of a pair of concepts to a specific semantic relation.", "labels": [], "entities": [{"text": "determining the similarity of a pair of concepts to a specific semantic relation", "start_pos": 95, "end_pos": 175, "type": "TASK", "confidence": 0.6150861657582797}]}, {"text": "The task focused on 79 semantic relations from which broadly fall into the ten categories enumerated in Table 1.", "labels": [], "entities": []}, {"text": "The data for the task was collected in two phases using Amazon Mechanical Turk . During Phase 1, Turkers were asked to provide pairs of words which fit a relation template, such as \"X possesses/owns/has Y\".", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 56, "end_pos": 78, "type": "DATASET", "confidence": 0.9258636633555094}]}, {"text": "Turkers provided word pairs such as expert:experience, mall:shops, letters:words, and doctor:degree.", "labels": [], "entities": []}, {"text": "across 79 relations were provided by Turkers in Phase 1.", "labels": [], "entities": [{"text": "Turkers", "start_pos": 37, "end_pos": 44, "type": "DATASET", "confidence": 0.9270939826965332}]}, {"text": "Some of these word pairs are naturally more representative of the relationship than others.", "labels": [], "entities": []}, {"text": "Therefore, in the second phase, each word pair was presented to a different set of Turkers for ranking in the form of questions.", "labels": [], "entities": []}, {"text": "shows an example MaxDiff question for the relation 2h PART-WHOLE: Creature:Possession (\"X possesses/owns/has Y\").", "labels": [], "entities": []}, {"text": "In each MaxDiff question, Turkers were simply asked to select the word pair which was the most illustrative of the relation and the word pair which was the least illustrative of the relation.", "labels": [], "entities": []}, {"text": "For the example in, most Turkers chose either shrubs:roots or farmer:crops as the most illustrative of the Creature:Possession relation, and homeowner:door as the least illustrative.", "labels": [], "entities": []}, {"text": "When Turkers select a pair of words they are performing a semantic inference that we wanted to also perform in a computational manner.", "labels": [], "entities": []}, {"text": "In this paper we present a method for automatically ranking word pairs according to their relatedness to a given semantic relation.", "labels": [], "entities": []}], "datasetContent": [{"text": "SemEval-2012 Task 2 had two official evaluation metrics.", "labels": [], "entities": []}, {"text": "The first directly measured the accuracy of automatically choosing the most and least illustrative word pairs among a set of four word pairs taken from responses during Phase 1.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.9992820620536804}]}, {"text": "The accuracy of choosing the most illustrative word pair and the  accuracy of choosing the least illustrative word pair were calculated separately and averaged to produce the MaxDiff accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9991099238395691}, {"text": "accuracy", "start_pos": 66, "end_pos": 74, "type": "METRIC", "confidence": 0.9993886947631836}, {"text": "MaxDiff", "start_pos": 175, "end_pos": 182, "type": "DATASET", "confidence": 0.6379109621047974}, {"text": "accuracy", "start_pos": 183, "end_pos": 191, "type": "METRIC", "confidence": 0.7680971026420593}]}, {"text": "The second evaluation metric measured the correlation between an automatic ranking of word pairs fora relation and a ranking induced by the Turkers' responses to the MaxDiff questions.", "labels": [], "entities": [{"text": "MaxDiff questions", "start_pos": 166, "end_pos": 183, "type": "DATASET", "confidence": 0.8373221755027771}]}, {"text": "The word pairs were given scores equal to the percentage of times they were chosen by Turkers as the most illustrative example fora relation minus the percentage of times they were chosen as the least illustrative.", "labels": [], "entities": [{"text": "Turkers", "start_pos": 86, "end_pos": 93, "type": "DATASET", "confidence": 0.9463943839073181}]}, {"text": "Systems were then evaluated according to their Spearman rank correlation with the ranking of word pairs induced by that score.", "labels": [], "entities": [{"text": "Spearman rank correlation", "start_pos": 47, "end_pos": 72, "type": "METRIC", "confidence": 0.6881984372933706}]}, {"text": "Spearman correlations range from -1 fora negative correlation to 1.0 fora perfect correlation.", "labels": [], "entities": []}, {"text": "shows the results for the six systems which participated in SemEval-2012 Task 2, along with the results fora baseline which ranks each word pair randomly.", "labels": [], "entities": [{"text": "SemEval-2012 Task 2", "start_pos": 60, "end_pos": 79, "type": "TASK", "confidence": 0.8543968796730042}]}, {"text": "Our two approaches achieved the best results on both evaluation metrics.", "labels": [], "entities": []}, {"text": "Our UTD-NB approach achieves much better performance than our UTD-SVM approach, likely due to the unconventional use of the SVM to classify its own training data.", "labels": [], "entities": []}, {"text": "That said, the results are still significantly higher than those of other participants.", "labels": [], "entities": []}, {"text": "This maybe attributed to our incorporation of better patterns or our use of a large corpus.", "labels": [], "entities": []}, {"text": "It might also be a consequence of our approaches considering all of the testing word pairs simultaneously.", "labels": [], "entities": []}, {"text": "shows the results for each of the ten categories of relations.", "labels": [], "entities": []}, {"text": "The best results are achieved on SPACE-TIME relations, while the lowest performance is on the: Spearman correlation results for the best system from each team, across all ten categories of relations.", "labels": [], "entities": [{"text": "Spearman correlation", "start_pos": 95, "end_pos": 115, "type": "METRIC", "confidence": 0.6238870620727539}]}], "tableCaptions": [{"text": " Table 3: Results for all systems participating in SemEval  2012 Task 2 on relational similarity, including a random  baseline.", "labels": [], "entities": [{"text": "SemEval  2012 Task 2", "start_pos": 51, "end_pos": 71, "type": "TASK", "confidence": 0.8569596856832504}]}, {"text": " Table 4: Spearman correlation results for the best system  from each team, across all ten categories of relations.", "labels": [], "entities": [{"text": "Spearman", "start_pos": 10, "end_pos": 18, "type": "TASK", "confidence": 0.8528279066085815}, {"text": "correlation", "start_pos": 19, "end_pos": 30, "type": "METRIC", "confidence": 0.6088341474533081}]}]}