{"title": [{"text": "TakeLab: Systems for Measuring Semantic Text Similarity", "labels": [], "entities": [{"text": "Semantic Text Similarity", "start_pos": 31, "end_pos": 55, "type": "TASK", "confidence": 0.5985033313433329}]}], "abstractContent": [{"text": "This paper describes the two systems for determining the semantic similarity of short texts submitted to the SemEval 2012 Task 6.", "labels": [], "entities": [{"text": "determining the semantic similarity of short texts submitted to the SemEval 2012 Task 6", "start_pos": 41, "end_pos": 128, "type": "TASK", "confidence": 0.7131185552903584}]}, {"text": "Most of the research on semantic similarity of textual content focuses on large documents.", "labels": [], "entities": []}, {"text": "However, a fair amount of information is condensed into short text snippets such as social media posts, image captions, and scientific abstracts.", "labels": [], "entities": []}, {"text": "We predict the human ratings of sentence similarity using a support vector regression model with multiple features measuring word-overlap similarity and syntax similarity.", "labels": [], "entities": []}, {"text": "Out of 89 systems submitted, our two systems ranked in the top 5, for the three overall evaluation metrics used (overall Pearson-2nd and 3rd, normalized Pearson-1st and 3rd, weighted mean-2nd and 5th).", "labels": [], "entities": [{"text": "Pearson-2nd", "start_pos": 121, "end_pos": 132, "type": "METRIC", "confidence": 0.9818037748336792}, {"text": "Pearson-1st", "start_pos": 153, "end_pos": 164, "type": "METRIC", "confidence": 0.822740375995636}]}], "introductionContent": [{"text": "Natural language processing tasks such as text classification), text summarization (), information retrieval), and word sense disambiguation) rely on a measure of semantic similarity of textual documents.", "labels": [], "entities": [{"text": "text classification", "start_pos": 42, "end_pos": 61, "type": "TASK", "confidence": 0.7520982921123505}, {"text": "text summarization", "start_pos": 64, "end_pos": 82, "type": "TASK", "confidence": 0.7566523551940918}, {"text": "information retrieval", "start_pos": 87, "end_pos": 108, "type": "TASK", "confidence": 0.7603841722011566}, {"text": "word sense disambiguation", "start_pos": 115, "end_pos": 140, "type": "TASK", "confidence": 0.6627411743005117}]}, {"text": "Research predominantly focused either on the document similarity () or the word similarity (.", "labels": [], "entities": []}, {"text": "Evaluating the similarity of short texts such as sentences or paragraphs) received less attention from the research community.", "labels": [], "entities": []}, {"text": "The task of recognizing paraphrases) is sufficiently similar to reuse some of the techniques.", "labels": [], "entities": []}, {"text": "This paper presents the two systems for automated measuring of semantic similarity of short texts which we submitted to the).", "labels": [], "entities": []}, {"text": "We propose several sentence similarity measures built upon knowledge-based and corpus-based similarity of individual words as well as similarity of dependency parses.", "labels": [], "entities": []}, {"text": "Our two systems, simple and syntax, use supervised machine learning, more specifically the support vector regression (SVR), to combine a large amount of features computed from pairs of sentences.", "labels": [], "entities": []}, {"text": "The two systems differ in the set of features they employ.", "labels": [], "entities": []}, {"text": "Our systems placed in the top 5 (out of 89 submitted systems) for all three aggregate correlation measures: 2nd (syntax) and 3rd (simple) for overall Pearson, 1st (simple) and 3rd (syntax) for normalized Pearson, and 2nd (simple) and 5th (syntax) for weighted mean.", "labels": [], "entities": [{"text": "Pearson", "start_pos": 150, "end_pos": 157, "type": "METRIC", "confidence": 0.8411055207252502}]}, {"text": "The rest of the paper is structured as follows.", "labels": [], "entities": []}, {"text": "In Section 2 we describe both knowledge-based and corpus-based word similarity measures.", "labels": [], "entities": []}, {"text": "In Section 3 we describe in detail the features used by our systems.", "labels": [], "entities": []}, {"text": "In Section 4 we report the experimental results cross-validated on the development set as well as the official results on all test sets.", "labels": [], "entities": []}, {"text": "Conclusions and ideas for future work are given in Section 5.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Evaluation of word similarity measures", "labels": [], "entities": []}, {"text": " Table 3: Cross-validated results on train sets", "labels": [], "entities": []}]}