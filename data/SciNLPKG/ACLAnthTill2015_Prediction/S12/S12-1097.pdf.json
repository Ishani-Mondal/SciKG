{"title": [{"text": "University Of Sheffield: Two Approaches to Semantic Text Similarity", "labels": [], "entities": [{"text": "Semantic Text Similarity", "start_pos": 43, "end_pos": 67, "type": "TASK", "confidence": 0.6670476297537485}]}], "abstractContent": [{"text": "This paper describes the University of Sheffield's submission to SemEval-2012 Task 6: Semantic Text Similarity.", "labels": [], "entities": [{"text": "SemEval-2012 Task 6", "start_pos": 65, "end_pos": 84, "type": "TASK", "confidence": 0.8999843200047811}, {"text": "Semantic Text Similarity", "start_pos": 86, "end_pos": 110, "type": "TASK", "confidence": 0.6650942166646322}]}, {"text": "The first is an unsupervised technique based on the widely used vector space model and information from WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 104, "end_pos": 111, "type": "DATASET", "confidence": 0.9507715702056885}]}, {"text": "The second method relies on supervised machine learning and represents each sentence as a set of n-grams.", "labels": [], "entities": []}, {"text": "This approach also makes use of information from WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 49, "end_pos": 56, "type": "DATASET", "confidence": 0.9703596830368042}]}, {"text": "Results from the formal evaluation show that both approaches are useful for determining the similarity in meaning between pairs of sentences with the best performance being obtained by the supervised approach.", "labels": [], "entities": []}, {"text": "Incorporating information from WordNet also improves performance for both approaches.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 31, "end_pos": 38, "type": "DATASET", "confidence": 0.9274884462356567}]}], "introductionContent": [{"text": "This paper describes the University of Sheffield's submission to SemEval-2012 Task 6: Semantic Text Similarity ().", "labels": [], "entities": [{"text": "SemEval-2012 Task 6", "start_pos": 65, "end_pos": 84, "type": "TASK", "confidence": 0.8978299101193746}, {"text": "Semantic Text Similarity", "start_pos": 86, "end_pos": 110, "type": "TASK", "confidence": 0.6727617084980011}]}, {"text": "The task is concerned with determining the degree of semantic equivalence between a pair of sentences.", "labels": [], "entities": []}, {"text": "Measuring the similarity between sentences is an important problem that is relevant to many areas of language processing, including the identification of text reuse, textual entailment (, paraphrase detection (), Information Extraction/Question Answering (), Information Retrieval), short answer grading, recommendation () and evaluation ().", "labels": [], "entities": [{"text": "identification of text reuse", "start_pos": 136, "end_pos": 164, "type": "TASK", "confidence": 0.8699995279312134}, {"text": "paraphrase detection", "start_pos": 188, "end_pos": 208, "type": "TASK", "confidence": 0.7411918938159943}, {"text": "Information Extraction/Question Answering", "start_pos": 213, "end_pos": 254, "type": "TASK", "confidence": 0.7862647175788879}, {"text": "Information Retrieval", "start_pos": 259, "end_pos": 280, "type": "TASK", "confidence": 0.8056881725788116}, {"text": "short answer grading", "start_pos": 283, "end_pos": 303, "type": "TASK", "confidence": 0.6671608487764994}]}, {"text": "Many of the previous approaches to measuring the similarity between texts have relied purely on lexical matching techniques, for example).", "labels": [], "entities": []}, {"text": "In these approaches the similarity of texts is computed as a function of the number of matching tokens, or sequences of tokens, they contain.", "labels": [], "entities": []}, {"text": "However, this approach fails to identify similarities when the same meaning is conveyed using synonymous terms or phrases (for example, \"The dog sat on the mat\" and \"The hound sat on the mat\") or when the meanings of the texts are similar but not identical (for example, \"The cat sat on the mat\" and \"A dog sat on the chair\").", "labels": [], "entities": []}, {"text": "Significant amounts of previous work on text similarity have focussed on comparing the meanings of texts longer than a single sentence, such as paragraphs or documents.", "labels": [], "entities": [{"text": "text similarity", "start_pos": 40, "end_pos": 55, "type": "TASK", "confidence": 0.7189071327447891}]}, {"text": "The size of these texts means that there is a reasonable amount of lexical items in each document that can be used to determine similarity and failing to identify connections between related terms may not be problematic.", "labels": [], "entities": []}, {"text": "The situation is different for the problem of semantic text similarity where the texts are short (single sentences).", "labels": [], "entities": [{"text": "semantic text similarity", "start_pos": 46, "end_pos": 70, "type": "TASK", "confidence": 0.6773759325345358}]}, {"text": "There are fewer lexical items to match in this case, making it more important that connections between related terms are identified.", "labels": [], "entities": []}, {"text": "One way in which this information has been incorporated in NLP systems has been to make use of WordNet to provide information about similarity between word meanings, and this approach has been shown to be useful for computing text similarity (.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 95, "end_pos": 102, "type": "DATASET", "confidence": 0.9586688876152039}, {"text": "text similarity", "start_pos": 226, "end_pos": 241, "type": "TASK", "confidence": 0.6890572011470795}]}, {"text": "This paper describes two approaches to the semantic text similarity problem that use WordNet () to provide information about relations between word meanings.", "labels": [], "entities": [{"text": "semantic text similarity problem", "start_pos": 43, "end_pos": 75, "type": "TASK", "confidence": 0.7348045855760574}, {"text": "WordNet", "start_pos": 85, "end_pos": 92, "type": "DATASET", "confidence": 0.9457854628562927}]}, {"text": "The two approaches are based on commonly used techniques for computing semantic similarity based on lexical matching.", "labels": [], "entities": []}, {"text": "The first is unsupervised while the other requires annotated data to train a learning algorithm.", "labels": [], "entities": []}, {"text": "Results of the SemEval evaluation show that the supervised approach produces the best overall results and that using the information provided by WordNet leads to an improvement in performance.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 145, "end_pos": 152, "type": "DATASET", "confidence": 0.9486104846000671}]}, {"text": "The remainder of this paper is organised as follows.", "labels": [], "entities": []}, {"text": "The next section describes the two approaches for computing semantic similarity between pairs of sentences that were developed.", "labels": [], "entities": [{"text": "computing semantic similarity between pairs of sentences", "start_pos": 50, "end_pos": 106, "type": "TASK", "confidence": 0.8464914304869515}]}, {"text": "The system submitted for the task is described in Section 3 and its performance in the official evaluation in Section 4.", "labels": [], "entities": []}, {"text": "Section 5 contains the conclusions and suggestions for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "The overall performance (ALLnrm) of NG, VG and the hybrid systems is significantly higher than the).", "labels": [], "entities": [{"text": "ALLnrm)", "start_pos": 25, "end_pos": 32, "type": "METRIC", "confidence": 0.9904356598854065}]}, {"text": "The table also includes separate results for each of the evaluation corpora (rows three to seven): the unsupervised VS model performance is significantly higher than the baseline (p-value of 0.06) overall corpus types, as is that of the hybrid model.", "labels": [], "entities": []}, {"text": "However, the performance of the supervised NG model is below the baseline for the (unseen in training data) SMT-news corpus.", "labels": [], "entities": [{"text": "training data) SMT-news corpus", "start_pos": 93, "end_pos": 123, "type": "DATASET", "confidence": 0.7336714148521424}]}, {"text": "Given a pair of sentences from an unknown source, the algorithm employs a model trained on all data combined (i.e., omits the corpus information), which may resemble the input (On-WN) or it may not (SMT-news).", "labels": [], "entities": []}, {"text": "After stoplist removal, the average sentence length within MSRvid is 4.5, whereas it is 6.0 and 6.9 in MSRpar and SMT-eur respectively, and thus the last two corpora are expected to form better training data for each other.", "labels": [], "entities": []}, {"text": "The overall performance on the MSRvid data is higher than for the other corpora, which maybe due to the small number of adjectives and the simpler structure of the shorter sentences within the corpus.", "labels": [], "entities": [{"text": "MSRvid data", "start_pos": 31, "end_pos": 42, "type": "DATASET", "confidence": 0.9407965540885925}]}, {"text": "The hybrid system, which selects the supervised system (NG)'s output when the test sentence pair is drawn from a corpus within the training data   and selects the unsupervised system (VS)'s answer otherwise, outperforms both systems in combination.", "labels": [], "entities": []}, {"text": "Contrary to expectations, the supervised system did not always outperform VS on phrases based on training data -the performance of VS on MSRpar, with its long and complex sentences, proved to be slightly higher than that of NG.", "labels": [], "entities": []}, {"text": "However, the unsupervised system was clearly the correct choice when the source was unknown.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Performance of Vector Space Model us- ing various disambiguation strategies and similarity  measures", "labels": [], "entities": []}, {"text": " Table 2: Correlation scores across individual cor- pora using Path Distance and Most Frequent Sense.", "labels": [], "entities": []}, {"text": " Table 3: Correlation scores from official SemEval results", "labels": [], "entities": [{"text": "Correlation", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.8875932097434998}, {"text": "SemEval", "start_pos": 43, "end_pos": 50, "type": "TASK", "confidence": 0.8788033723831177}]}, {"text": " Table 4: Ranks from official SemEval results", "labels": [], "entities": [{"text": "SemEval", "start_pos": 30, "end_pos": 37, "type": "TASK", "confidence": 0.8814805746078491}]}]}