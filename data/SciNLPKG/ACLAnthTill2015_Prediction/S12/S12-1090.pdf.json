{"title": [{"text": "UMCC_DLSI: Multidimensional Lexical-Semantic Textual Similarity", "labels": [], "entities": [{"text": "UMCC_DLSI", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.7859542568524679}, {"text": "Multidimensional Lexical-Semantic Textual Similarity", "start_pos": 11, "end_pos": 63, "type": "TASK", "confidence": 0.5348547399044037}]}], "abstractContent": [{"text": "This paper describes the specifications and results of UMCC_DLSI system, which participated in the first Semantic Textual Similarity task (STS) of SemEval-2012.", "labels": [], "entities": [{"text": "Semantic Textual Similarity task (STS) of SemEval-2012", "start_pos": 105, "end_pos": 159, "type": "TASK", "confidence": 0.7757561670409309}]}, {"text": "Our supervised system uses different kinds of semantic and lexical features to train classifiers and it uses a voting process to select the correct option.", "labels": [], "entities": []}, {"text": "Related to the different features we can highlight the resource ISR-WN 1 used to extract semantic relations among words and the use of different algorithms to establish semantic and lexical similarities.", "labels": [], "entities": []}, {"text": "In order to establish which features are the most appropriate to improve STS results we participated with three runs using different set of features.", "labels": [], "entities": [{"text": "STS", "start_pos": 73, "end_pos": 76, "type": "TASK", "confidence": 0.974003255367279}]}, {"text": "Our best approach reached the position 18 of 89 runs, obtaining a general correlation coefficient up to 0.72.", "labels": [], "entities": [{"text": "general correlation coefficient", "start_pos": 66, "end_pos": 97, "type": "METRIC", "confidence": 0.7517474889755249}]}], "introductionContent": [{"text": "SemEval 2012 competition for evaluating Natural Language Processing (NLP) systems presents anew task called Semantic Textual Similarity (STS).", "labels": [], "entities": [{"text": "Semantic Textual Similarity (STS)", "start_pos": 108, "end_pos": 141, "type": "TASK", "confidence": 0.7376357366641363}]}, {"text": "In STS the participating systems must examine the degree of semantic equivalence between two sentences.", "labels": [], "entities": [{"text": "STS", "start_pos": 3, "end_pos": 6, "type": "TASK", "confidence": 0.9651363492012024}]}, {"text": "The goal of this task is to create a unified framework for the evaluation of semantic textual similarity modules and to characterize their impact on NLP applications.", "labels": [], "entities": []}, {"text": "STS is related to Textual Entailment (TE) and Paraphrase tasks.", "labels": [], "entities": [{"text": "Textual Entailment (TE)", "start_pos": 18, "end_pos": 41, "type": "TASK", "confidence": 0.7778303980827331}]}, {"text": "The main difference is that STS 1 Integration of Semantic Resource based on assumes bidirectional graded equivalence between the pair of textual snippets.", "labels": [], "entities": []}, {"text": "In the case of TE the equivalence is directional (e.g. a student is a person, but a person is not necessarily a student).", "labels": [], "entities": []}, {"text": "In addition, STS differs from TE and Paraphrase in that, rather than being a binary yes/no decision, STS is a similarity-graded notion (e.g. a student and a person are more similar than a dog and a person).", "labels": [], "entities": []}, {"text": "This bidirectional gradation is useful for NLP tasks such as Machine Translation, Information Extraction, Question Answering, and Summarization.", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 61, "end_pos": 80, "type": "TASK", "confidence": 0.8490514159202576}, {"text": "Information Extraction", "start_pos": 82, "end_pos": 104, "type": "TASK", "confidence": 0.8156563341617584}, {"text": "Question Answering", "start_pos": 106, "end_pos": 124, "type": "TASK", "confidence": 0.8637941479682922}, {"text": "Summarization", "start_pos": 130, "end_pos": 143, "type": "TASK", "confidence": 0.9898030757904053}]}, {"text": "Several semantic tasks could be added as modules in the STS framework, \"such as Word Sense Disambiguation and Induction, Lexical Substitution, Semantic Role Labeling, Multiword Expression detection and handling, Anaphora and Co-reference resolution, Time and Date resolution and Named Entity Recognition, among others\" 2", "labels": [], "entities": [{"text": "Word Sense Disambiguation and Induction", "start_pos": 80, "end_pos": 119, "type": "TASK", "confidence": 0.6920827388763428}, {"text": "Semantic Role Labeling", "start_pos": 143, "end_pos": 165, "type": "TASK", "confidence": 0.6976223488648733}, {"text": "Multiword Expression detection and handling", "start_pos": 167, "end_pos": 210, "type": "TASK", "confidence": 0.8965343832969666}, {"text": "Anaphora and Co-reference resolution", "start_pos": 212, "end_pos": 248, "type": "TASK", "confidence": 0.6090617254376411}, {"text": "Time and Date resolution", "start_pos": 250, "end_pos": 274, "type": "TASK", "confidence": 0.5315711200237274}, {"text": "Named Entity Recognition", "start_pos": 279, "end_pos": 303, "type": "TASK", "confidence": 0.6396921277046204}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 2. Features extracted from the analyzed sentences.", "labels": [], "entities": []}, {"text": " Table 3. Distances between the groups of nouns.", "labels": [], "entities": []}, {"text": " Table 5. Correlation of individual features over all training sets.", "labels": [], "entities": []}, {"text": " Table 7. Official SemEval 2012 results.", "labels": [], "entities": [{"text": "Official SemEval 2012 results", "start_pos": 10, "end_pos": 39, "type": "DATASET", "confidence": 0.8580427467823029}]}, {"text": " Table 8. Ranking position of our runs in SemEval 2012.", "labels": [], "entities": [{"text": "Ranking", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9652812480926514}, {"text": "SemEval 2012", "start_pos": 42, "end_pos": 54, "type": "DATASET", "confidence": 0.6731160879135132}]}, {"text": " Table 9. The best run of SemEval 2012.", "labels": [], "entities": [{"text": "SemEval 2012", "start_pos": 26, "end_pos": 38, "type": "TASK", "confidence": 0.6442567408084869}]}, {"text": " Table 10. Comparison of our distance with the best.", "labels": [], "entities": []}]}