{"title": [{"text": "UOW: Semantically Informed Text Similarity", "labels": [], "entities": [{"text": "UOW", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.6882680058479309}, {"text": "Semantically Informed Text Similarity", "start_pos": 5, "end_pos": 42, "type": "TASK", "confidence": 0.6149142906069756}]}], "abstractContent": [{"text": "The UOW submissions to the Semantic Tex-tual Similarity task at SemEval-2012 use a supervised machine learning algorithm along with features based on lexical, syntactic and semantic similarity metrics to predict the semantic equivalence between a pair of sentences.", "labels": [], "entities": [{"text": "Semantic Tex-tual Similarity task at SemEval-2012", "start_pos": 27, "end_pos": 76, "type": "TASK", "confidence": 0.7728730390469233}]}, {"text": "The lexical metrics are based on word-overlap.", "labels": [], "entities": []}, {"text": "A shallow syntactic metric is based on the overlap of base-phrase labels.", "labels": [], "entities": []}, {"text": "The semantically informed metrics are based on the preservation of named entities and on the alignment of verb predicates and the overlap of argument roles using inexact matching.", "labels": [], "entities": []}, {"text": "Our submissions outperformed the official base-line, with our best system ranked above average , but the contribution of the semantic met-rics was not conclusive.", "labels": [], "entities": []}], "introductionContent": [{"text": "We describe the UOW submissions to the Semantic Textual Similarity (STS) task at SemEval-2012.", "labels": [], "entities": [{"text": "Semantic Textual Similarity (STS) task at SemEval-2012", "start_pos": 39, "end_pos": 93, "type": "TASK", "confidence": 0.8026095430056254}]}, {"text": "Our systems are based on combining similarity scores as features using a regression algorithm to predict the degree of semantic equivalence between a pair of sentences.", "labels": [], "entities": []}, {"text": "We train the regression algorithm with different classes of similarity metrics: i) lexical, ii) syntactic and iii) semantic.", "labels": [], "entities": []}, {"text": "The lexical similarity metrics are: i) cosine similarity using a bag-ofwords representation, and ii) precision, recall and F-measure of content words.", "labels": [], "entities": [{"text": "precision", "start_pos": 101, "end_pos": 110, "type": "METRIC", "confidence": 0.9994288086891174}, {"text": "recall", "start_pos": 112, "end_pos": 118, "type": "METRIC", "confidence": 0.9988594055175781}, {"text": "F-measure", "start_pos": 123, "end_pos": 132, "type": "METRIC", "confidence": 0.9977338314056396}]}, {"text": "The syntactic metric computes BLEU (), a machine translation evaluation metric, over a labels of basephrases (chunks).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 30, "end_pos": 34, "type": "METRIC", "confidence": 0.9973570704460144}]}, {"text": "Two semantic metrics are used: a metric based on the preservation of Named Entities and TINE ().", "labels": [], "entities": [{"text": "TINE", "start_pos": 88, "end_pos": 92, "type": "METRIC", "confidence": 0.932449221611023}]}, {"text": "Named entities are matched by type and content: while the type has to match exactly, the content is compared with the assistance of a distributional thesaurus.", "labels": [], "entities": []}, {"text": "TINE is a metric proposed to measure adequacy in machine translation and favors similar semantic frames.", "labels": [], "entities": [{"text": "TINE", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.6886805295944214}, {"text": "machine translation", "start_pos": 49, "end_pos": 68, "type": "TASK", "confidence": 0.7199853509664536}]}, {"text": "TINE attempts to align verb predicates, assuming a oneto-one correspondence between semantic roles, and considering ontologies for inexact alignment.", "labels": [], "entities": []}, {"text": "The surface realization of the arguments is compared using a distributional thesaurus and the cosine similarity metric.", "labels": [], "entities": []}, {"text": "Finally, we use METEOR, also a common metric for machine translation evaluation, that also computes inexact word overlap as at way of measuring the impact of our semantic metrics.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 16, "end_pos": 22, "type": "METRIC", "confidence": 0.9635230302810669}, {"text": "machine translation evaluation", "start_pos": 49, "end_pos": 79, "type": "TASK", "confidence": 0.8694068789482117}]}, {"text": "The lexical and syntactic metrics complement the semantic metrics in dealing with the phenomena observed in the task's dataset.", "labels": [], "entities": []}, {"text": "For instance, from the MSRvid dataset: S1 Two men are playing football.", "labels": [], "entities": [{"text": "MSRvid dataset", "start_pos": 23, "end_pos": 37, "type": "DATASET", "confidence": 0.9836438894271851}]}, {"text": "S2 Two men are practicing football.", "labels": [], "entities": []}, {"text": "In this case, as typical of paraphrasing, the situation and participants are the same while the surface realization differs, but playing can be considered similar to practicing.", "labels": [], "entities": []}, {"text": "From the SMT-eur dataset: S3 The Council of Europe, along with the Court of Human Rights, has a wealth of experience of such forms of supervision, and we can build on these.", "labels": [], "entities": [{"text": "SMT-eur dataset", "start_pos": 9, "end_pos": 24, "type": "DATASET", "confidence": 0.7477741539478302}]}, {"text": "S4 Just as the European Court of Human Rights, the Council of Europe has also considerable experience with regard to these forms of control; we can take as a basis.", "labels": [], "entities": [{"text": "European Court of Human Rights", "start_pos": 15, "end_pos": 45, "type": "DATASET", "confidence": 0.7699283003807068}]}, {"text": "Similarly, here although with different realizations, the Court of Human Rights and the European Court of Human Rights represent the same entity.", "labels": [], "entities": [{"text": "European Court of Human Rights", "start_pos": 88, "end_pos": 118, "type": "DATASET", "confidence": 0.9592136144638062}]}, {"text": "Semantic metrics based on predicate-argument structure can play a role in cases when different realization have similar semantic roles: S5 The right of a government arbitrarily to set aside its own constitution is the defining characteristic of a tyranny.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use the following state-of-the-art tools to preprocess the data for feature extraction: i) TreeTagger 3 for lemmas and ii) SENNA for Part-of-Speech tagging, Chunking, Named Entity Recognition and Semantic Role Labeling.", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 71, "end_pos": 89, "type": "TASK", "confidence": 0.7212724387645721}, {"text": "SENNA", "start_pos": 126, "end_pos": 131, "type": "METRIC", "confidence": 0.9680777788162231}, {"text": "Part-of-Speech tagging", "start_pos": 136, "end_pos": 158, "type": "TASK", "confidence": 0.7759295105934143}, {"text": "Named Entity Recognition", "start_pos": 170, "end_pos": 194, "type": "TASK", "confidence": 0.643341064453125}, {"text": "Semantic Role Labeling", "start_pos": 199, "end_pos": 221, "type": "TASK", "confidence": 0.6968166530132294}]}, {"text": "SENNA has been reported to achieve an Fmeasure of 75.79% for tagging semantic roles on the CoNLL-2005 2 benchmark.", "labels": [], "entities": [{"text": "Fmeasure", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.9996925592422485}, {"text": "tagging semantic roles", "start_pos": 61, "end_pos": 83, "type": "TASK", "confidence": 0.8947162429491679}, {"text": "CoNLL-2005 2 benchmark", "start_pos": 91, "end_pos": 113, "type": "DATASET", "confidence": 0.8710851271947225}]}, {"text": "The final feature set includes: \u2022 Lexical metrics -Cosine metric over bag-of-words -Precision over content words -Recall over content words -F-measure over content words \u2022 BLEU metric over chunks \u2022 METEOR metric over words (with stems, synonyms and paraphrases) \u2022 Named Entity metric \u2022 Semantic Role Labeling metric The Machine Learning algorithm used for regression is the LIBSVM 5 Support Vector Machine (SVM) implementation using the radial basis kernel function.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 141, "end_pos": 150, "type": "METRIC", "confidence": 0.9546144008636475}, {"text": "BLEU", "start_pos": 172, "end_pos": 176, "type": "METRIC", "confidence": 0.9974218606948853}, {"text": "Semantic Role Labeling", "start_pos": 286, "end_pos": 308, "type": "TASK", "confidence": 0.7202858924865723}]}, {"text": "We used a simple genetic algorithm () to tune the parameters of the SVM.", "labels": [], "entities": []}, {"text": "The configuration of the genetic algorithm is as follows: \u2022 Fitness function: minimize the mean squared error found by cross-validation \u2022 Chromosome: real numbers for SVM parameters \u03b3, cost and \u2022 Number of individuals: 80 \u2022 Number of generations: 100 \u2022 Selection method: roulette \u2022 Crossover probability: 0.9 \u2022 Mutation probability: 0.01 We submitted three system runs, each is a variation of the above feature set.", "labels": [], "entities": []}, {"text": "For the official submission we used the systems with optimized SVM parameters.", "labels": [], "entities": []}, {"text": "We trained SVM models with each of the following task datasets: MSRpar, MSRvid, SMTeur and the combination of MSRpar+MSRvid.", "labels": [], "entities": []}, {"text": "For each test dataset we applied their respective training models, except for the new test sets, not covered by any training set: for On-WN we used the combination MSRpar+MSRvid, and for SMT-news we used SMT-eur.", "labels": [], "entities": [{"text": "On-WN", "start_pos": 134, "end_pos": 139, "type": "DATASET", "confidence": 0.9317578077316284}, {"text": "SMT-news", "start_pos": 187, "end_pos": 195, "type": "TASK", "confidence": 0.8298305869102478}]}, {"text": "focus on the Pearson correlation of our three systems/runs for individual datasets of the predicted scores against human annotation, compared against the official baseline, which uses a simple word overlap metric.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 13, "end_pos": 32, "type": "METRIC", "confidence": 0.9366531074047089}]}, {"text": "shows the average results overall five datasets, where ALL stands for the Pearson correlation with the gold standard for the five dataset, Rank is the absolute rank among all submissions, ALLnrm is the Pearson correlation when each dataset is fitted to the gold standard using least squares, RankNrm is the corresponding rank and Mean is the weighted mean across the five datasets, where the weight depends on the number of sentence pairs in the dataset.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 74, "end_pos": 93, "type": "METRIC", "confidence": 0.9682475626468658}, {"text": "Rank", "start_pos": 139, "end_pos": 143, "type": "METRIC", "confidence": 0.965239942073822}, {"text": "ALLnrm", "start_pos": 188, "end_pos": 194, "type": "METRIC", "confidence": 0.9697356224060059}, {"text": "Pearson correlation", "start_pos": 202, "end_pos": 221, "type": "METRIC", "confidence": 0.9838736951351166}, {"text": "Mean", "start_pos": 330, "end_pos": 334, "type": "METRIC", "confidence": 0.9746683835983276}]}], "tableCaptions": [{"text": " Table 1: Results for Run 1 using lexical, chunking,  named entities and METEOR as features. A is the non- optimized version, B are the official results", "labels": [], "entities": [{"text": "METEOR", "start_pos": 73, "end_pos": 79, "type": "METRIC", "confidence": 0.9751445651054382}]}, {"text": " Table 2: Results for Run 2 using the SRL feature only. A  is the non-optimized version, B are the official results", "labels": [], "entities": []}, {"text": " Table 3: Results for Run 3 using all features. A is the  non-optimized version, B are the official results", "labels": [], "entities": []}, {"text": " Table 4: Official results and ranking over the test set for Runs 1-3 with SVM parameters optimized", "labels": [], "entities": []}]}