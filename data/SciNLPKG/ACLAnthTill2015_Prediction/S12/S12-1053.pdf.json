{"title": [{"text": "Semeval-2012 Task 8: Cross-lingual Textual Entailment for Content Synchronization", "labels": [], "entities": [{"text": "Cross-lingual Textual Entailment", "start_pos": 21, "end_pos": 53, "type": "TASK", "confidence": 0.644758939743042}, {"text": "Content Synchronization", "start_pos": 58, "end_pos": 81, "type": "TASK", "confidence": 0.7089360654354095}]}], "abstractContent": [{"text": "This paper presents the first round of the task on Cross-lingual Textual Entailment for Content Synchronization, organized within SemEval-2012.", "labels": [], "entities": [{"text": "Cross-lingual Textual Entailment", "start_pos": 51, "end_pos": 83, "type": "TASK", "confidence": 0.7144177158673605}, {"text": "Content Synchronization", "start_pos": 88, "end_pos": 111, "type": "TASK", "confidence": 0.6782897859811783}]}, {"text": "The task was designed to promote research on semantic inference over texts written in different languages, targeting at the same time areal application scenario.", "labels": [], "entities": [{"text": "semantic inference", "start_pos": 45, "end_pos": 63, "type": "TASK", "confidence": 0.7993273735046387}]}, {"text": "Participants were presented with datasets for different language pairs, where multi-directional entailment relations (\"forward\", \"backward\", \"bidirectional\", \"no entailment\") had to be identified.", "labels": [], "entities": []}, {"text": "We report on the training and test data used for evaluation, the process of their creation, the participating systems (10 teams, 92 runs), the approaches adopted and the results achieved.", "labels": [], "entities": []}], "introductionContent": [{"text": "The cross-lingual textual entailment task) addresses textual entailment (TE) recognition () under the new dimension of cross-linguality, and within the new challenging application scenario of content synchronization.", "labels": [], "entities": [{"text": "textual entailment (TE) recognition", "start_pos": 53, "end_pos": 88, "type": "TASK", "confidence": 0.6641526619593302}]}, {"text": "Cross-linguality represents a dimension of the TE recognition problem that has been so far only partially investigated.", "labels": [], "entities": [{"text": "TE recognition", "start_pos": 47, "end_pos": 61, "type": "TASK", "confidence": 0.9931449294090271}]}, {"text": "The great potential for integrating monolingual TE recognition components into NLP architectures has been reported in several areas, including question answering, information retrieval, information extraction, and document summarization.", "labels": [], "entities": [{"text": "TE recognition", "start_pos": 48, "end_pos": 62, "type": "TASK", "confidence": 0.9370822906494141}, {"text": "question answering", "start_pos": 143, "end_pos": 161, "type": "TASK", "confidence": 0.8819476068019867}, {"text": "information retrieval", "start_pos": 163, "end_pos": 184, "type": "TASK", "confidence": 0.8116381764411926}, {"text": "information extraction", "start_pos": 186, "end_pos": 208, "type": "TASK", "confidence": 0.8434910178184509}, {"text": "document summarization", "start_pos": 214, "end_pos": 236, "type": "TASK", "confidence": 0.6289373338222504}]}, {"text": "However, mainly due to the absence of cross-lingual textual entailment (CLTE) recognition components, similar improvements have not been achieved yet in any cross-lingual application.", "labels": [], "entities": [{"text": "cross-lingual textual entailment (CLTE) recognition", "start_pos": 38, "end_pos": 89, "type": "TASK", "confidence": 0.6172235906124115}]}, {"text": "The CLTE task aims at prompting research to fill this gap.", "labels": [], "entities": []}, {"text": "Along such direction, research can now benefit from recent advances in other fields, especially machine translation (MT), and the availability of: i) large amounts of parallel and comparable corpora in many languages, ii) open source software to compute word-alignments from parallel corpora, and iii) open source software to setup MT systems.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 96, "end_pos": 120, "type": "TASK", "confidence": 0.8645887494087219}, {"text": "MT", "start_pos": 332, "end_pos": 334, "type": "TASK", "confidence": 0.978726863861084}]}, {"text": "We believe that all these resources can positively contribute to develop inference mechanisms for multilingual data.", "labels": [], "entities": []}, {"text": "Content synchronization represents a challenging application scenario to test the capabilities of advanced NLP systems.", "labels": [], "entities": [{"text": "Content synchronization", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.7774316370487213}]}, {"text": "Given two documents about the same topic written in different languages (e.g. Wiki pages), the task consists of automatically detecting and resolving differences in the information they provide, in order to produce aligned, mutually enriched versions of the two documents.", "labels": [], "entities": []}, {"text": "Towards this objective, a crucial requirement is to identify the information in one page that is either equivalent or novel (more informative) with respect to the content of the other.", "labels": [], "entities": []}, {"text": "The task can be naturally cast as an entailment recognition problem, where bidirectional and unidirectional entailment judgments for two text fragments are respectively mapped into judgments about semantic equivalence and novelty.", "labels": [], "entities": [{"text": "entailment recognition", "start_pos": 37, "end_pos": 59, "type": "TASK", "confidence": 0.734592080116272}]}, {"text": "Alternatively, the task can be seen as a machine translation evaluation problem, where judgments about semantic equivalence and novelty depend on the possibility to fully or partially translate a text fragment into the other.", "labels": [], "entities": [{"text": "machine translation evaluation", "start_pos": 41, "end_pos": 71, "type": "TASK", "confidence": 0.7868891457716624}]}, {"text": "The recent advances on monolingual TE on the one hand, and the methodologies used in Statistical Machine Translation (SMT) on the other, offer promising solutions to approach the CLTE task.", "labels": [], "entities": [{"text": "TE", "start_pos": 35, "end_pos": 37, "type": "TASK", "confidence": 0.7193036675453186}, {"text": "Statistical Machine Translation (SMT)", "start_pos": 85, "end_pos": 122, "type": "TASK", "confidence": 0.8461583256721497}]}, {"text": "In line with a number of systems that model the RTE task as a similarity problem (i.e. handling similarity scores between T and H as useful evidence to draw entailment decisions), the standard sentence and word alignment programs used in SMT offer a strong baseline for CLTE.", "labels": [], "entities": [{"text": "RTE task", "start_pos": 48, "end_pos": 56, "type": "TASK", "confidence": 0.9275798499584198}, {"text": "word alignment", "start_pos": 206, "end_pos": 220, "type": "TASK", "confidence": 0.6901110708713531}, {"text": "SMT", "start_pos": 238, "end_pos": 241, "type": "TASK", "confidence": 0.9923220276832581}]}, {"text": "However, although representing a solid starting point to approach the problem, similarity-based techniques are just approximations, open to significant improvements coming from semantic inference at the multilingual level (e.g. cross-lingual entailment rules such as \"perro\"\u2192\"animal\").", "labels": [], "entities": []}, {"text": "Taken in isolation, similaritybased techniques clearly fall short of providing an effective solution to the problem of assigning directions to the entailment relations (especially in the complex CLTE scenario, where entailment relations are multi-directional).", "labels": [], "entities": []}, {"text": "Thanks to the contiguity between CLTE, TE and SMT, the proposed task provides an interesting scenario to approach the issues outlined above from different perspectives, and large room for mutual improvement.", "labels": [], "entities": [{"text": "TE", "start_pos": 39, "end_pos": 41, "type": "METRIC", "confidence": 0.6790804266929626}, {"text": "SMT", "start_pos": 46, "end_pos": 49, "type": "TASK", "confidence": 0.9305018186569214}]}], "datasetContent": [{"text": "Each dataset consists of 1,000 pairs (500 for training and 500 for test), balanced across the four entailment judgments (bidirectional, forward, backward, and no entailment).", "labels": [], "entities": []}, {"text": "For each language combination, the distribution of the four entailment judgments according to length diff is shown in.", "labels": [], "entities": []}, {"text": "Vertical bars represent, for each length diff value, the proportion of pairs belonging to the four entailment classes.", "labels": [], "entities": []}, {"text": "As can be seen, the length diff constraint applied to the length difference in the monolingual English Such constraint has been applied in order to focus as much as possible on semantic aspects of the problem, by reducing the applicability of simple association rules such as IF length(T1)>length(T2) THEN T1\u2192T2.", "labels": [], "entities": [{"text": "IF length(T1)>length(T2) THEN", "start_pos": 276, "end_pos": 305, "type": "METRIC", "confidence": 0.8216208100318909}]}, {"text": "The cross-lingual datasets are already available for research purposes at http://www.celct.it/resourcesList. php.", "labels": [], "entities": []}, {"text": "The monolingual English dataset will be publicly released to non participants in July 2012.", "labels": [], "entities": [{"text": "English dataset", "start_pos": 16, "end_pos": 31, "type": "DATASET", "confidence": 0.6391803175210953}]}, {"text": "pairs (step 3 of the creation process) is substantially reflected in the cross-lingual datasets for all language combinations.", "labels": [], "entities": []}, {"text": "In fact, as shown in, the majority of the pairs is always included in the same length diff range (approximately) and, within this range, the distribution of the four classes is substantially uniform.", "labels": [], "entities": []}, {"text": "Our assumption is that such data distribution makes entailment judgments based on mere surface features such as sentence length ineffective, thus encouraging the development of alternative, deeper processing strategies.", "labels": [], "entities": []}, {"text": "Evaluation results have been automatically computed by comparing the entailment judgments returned by each system with those manually assigned by human annotators.", "labels": [], "entities": []}, {"text": "The metric used for systems' ranking is accuracy over the whole test set, i.e. the number of correct judgments out of the total number of judgments in the test set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9991443157196045}]}, {"text": "Additionally, we calculated precision, recall, and F1 measures for each of the four entailment judgment categories taken separately.", "labels": [], "entities": [{"text": "precision", "start_pos": 28, "end_pos": 37, "type": "METRIC", "confidence": 0.999701201915741}, {"text": "recall", "start_pos": 39, "end_pos": 45, "type": "METRIC", "confidence": 0.9995406866073608}, {"text": "F1", "start_pos": 51, "end_pos": 53, "type": "METRIC", "confidence": 0.999862551689148}]}, {"text": "These scores aim at giving participants the possibility to gain clearer insights into their system's behavior on the entailment phenomena relevant to the task.", "labels": [], "entities": []}, {"text": "For each language combination, two baselines considering the length difference between T1 and T2 have been calculated (besides the trivial 0.25 accuracy score obtained by assigning each test pair in the balanced dataset to one of the four classes): \u2022 Composition of binary judgments (Binary).", "labels": [], "entities": [{"text": "accuracy score", "start_pos": 144, "end_pos": 158, "type": "METRIC", "confidence": 0.9788562059402466}]}, {"text": "To calculate this baseline an SVM classifier is trained to take binary entailment decisions (\"YES\", \"NO\").", "labels": [], "entities": [{"text": "YES", "start_pos": 94, "end_pos": 97, "type": "METRIC", "confidence": 0.9961744546890259}, {"text": "NO", "start_pos": 101, "end_pos": 103, "type": "METRIC", "confidence": 0.8567237257957458}]}, {"text": "The classifier uses length(T1)/length(T2) as a single feature to check for entailment from T1 to T2, and length(T2)/length(T1) for the opposite direction.", "labels": [], "entities": [{"text": "length(T1)/length(T2)", "start_pos": 20, "end_pos": 41, "type": "METRIC", "confidence": 0.927072711288929}, {"text": "length(T2)/length(T1)", "start_pos": 105, "end_pos": 126, "type": "METRIC", "confidence": 0.8670257702469826}]}, {"text": "For each test pair, the unidirectional judgments returned by the two classifiers are composed into a single multi-directional judgment (\"YES-YES\"=\"bidirectional\", \"YES-NO\"=\"forward\", \"NO-YES\"=\"backward\", \"NO-NO\"=\"no entailment\"); \u2022 Multi-class classification (Multi-class).", "labels": [], "entities": []}, {"text": "A single SVM classifier is trained with the same features to directly assign to each pair one of the four entailment judgments.", "labels": [], "entities": []}, {"text": "Both the baselines have been calculated with the LIBSVM package (Chang and Lin, 2011), using a linear kernel with default parameters.", "labels": [], "entities": []}, {"text": "Baseline results are reported in.", "labels": [], "entities": [{"text": "Baseline", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9524142742156982}]}, {"text": "Although the four CLTE datasets are derived from the same monolingual EN-EN corpus, baseline results present slight differences due to the effect of translation into different languages.", "labels": [], "entities": [{"text": "CLTE datasets", "start_pos": 18, "end_pos": 31, "type": "DATASET", "confidence": 0.8860260546207428}]}], "tableCaptions": [{"text": " Table 1: CLTE pairs distribution within the -5/+5  length diff range.", "labels": [], "entities": []}, {"text": " Table 2: Baseline accuracy results.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9836674928665161}]}, {"text": " Table 3: Accuracy results (92 runs) over the 4 lan- guage combinations. Highest, average, median and low- est scores are calculated considering the best run for each  team (*task organizers' system).", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.999258816242218}]}, {"text": " Table 4: precision, recall and F1 scores, calculated for each team's best run for all the language combinations.", "labels": [], "entities": [{"text": "precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9997466206550598}, {"text": "recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9993076324462891}, {"text": "F1 scores", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.9776809811592102}]}]}