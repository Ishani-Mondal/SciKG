{"title": [{"text": "SRIUBC: Simple Similarity Features for Semantic Textual Similarity", "labels": [], "entities": [{"text": "SRIUBC", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.664714515209198}, {"text": "Semantic Textual Similarity", "start_pos": 39, "end_pos": 66, "type": "TASK", "confidence": 0.5900949637095133}]}], "abstractContent": [{"text": "We describe the systems submitted by SRI International and the University of the Basque Country for the Semantic Textual Similarity (STS) SemEval-2012 task.", "labels": [], "entities": [{"text": "SRI International", "start_pos": 37, "end_pos": 54, "type": "DATASET", "confidence": 0.736629068851471}, {"text": "Semantic Textual Similarity (STS) SemEval-2012 task", "start_pos": 104, "end_pos": 155, "type": "TASK", "confidence": 0.8091871105134487}]}, {"text": "Our systems fo-cused on using a simple set of features, featuring a mix of semantic similarity resources, lexical match heuristics, and part of speech (POS) information.", "labels": [], "entities": []}, {"text": "We also incorporate precision focused scores over lexical and POS information derived from the BLEU measure, and lexical and POS features computed over split-bigrams from the ROUGE-S measure.", "labels": [], "entities": [{"text": "precision focused scores", "start_pos": 20, "end_pos": 44, "type": "METRIC", "confidence": 0.946116586526235}, {"text": "BLEU", "start_pos": 95, "end_pos": 99, "type": "METRIC", "confidence": 0.9962365031242371}]}, {"text": "These were used to train support vector re-gressors over the pairs in the training data.", "labels": [], "entities": []}, {"text": "From the three systems we submitted, two performed well in the overall ranking, with split-bigrams improving performance over pairs drawn from the MSR Research Video Description Corpus.", "labels": [], "entities": [{"text": "MSR Research Video Description Corpus", "start_pos": 147, "end_pos": 184, "type": "DATASET", "confidence": 0.8714757204055786}]}, {"text": "Our third system maintained three separate regressors, each trained specifically for the STS dataset they were drawn from.", "labels": [], "entities": [{"text": "STS dataset", "start_pos": 89, "end_pos": 100, "type": "DATASET", "confidence": 0.7547630965709686}]}, {"text": "It used a multinomial classifier to predict which dataset regressor would be most appropriate to score a given pair, and used it to score that pair.", "labels": [], "entities": []}, {"text": "This system underperformed, primarily due to errors in the dataset predictor.", "labels": [], "entities": []}], "introductionContent": [{"text": "Previous semantic similarity tasks, such as paraphrase identification or recognizing textual entailment, have focused on performing binary decisions.", "labels": [], "entities": [{"text": "paraphrase identification", "start_pos": 44, "end_pos": 69, "type": "TASK", "confidence": 0.924512505531311}, {"text": "recognizing textual entailment", "start_pos": 73, "end_pos": 103, "type": "TASK", "confidence": 0.7222708861033121}]}, {"text": "These problems are usually framed in terms of identifying whether a pair of texts exhibit the needed similarity or entailment relationship or not.", "labels": [], "entities": []}, {"text": "In many cases, such as producing a ranking over similarity scores, a soft measure of similarity between a pair of texts would be more desirable.", "labels": [], "entities": []}, {"text": "We contributed three systems for the 2012 Semantic Textual Similarity (STS) task).", "labels": [], "entities": [{"text": "Semantic Textual Similarity (STS) task", "start_pos": 42, "end_pos": 80, "type": "TASK", "confidence": 0.789602300950459}]}, {"text": "System 1, which used a combination of semantic similarity, lexical similarity, and precision focused part-of-speech (POS) features.", "labels": [], "entities": [{"text": "precision", "start_pos": 83, "end_pos": 92, "type": "METRIC", "confidence": 0.9799304008483887}]}, {"text": "2. System 2, which used features from System 1, with the addition of skip-bigram features derived from the ROUGE-S (Lin, 2004) measure.", "labels": [], "entities": [{"text": "ROUGE-S", "start_pos": 107, "end_pos": 114, "type": "METRIC", "confidence": 0.9794171452522278}]}, {"text": "POS variants of skip-bigrams were incorporated as well.", "labels": [], "entities": []}, {"text": "3. System 3, used the features from above to first classify the dataset the pair was drawn from, and then applied regressors trained for that dataset.", "labels": [], "entities": []}, {"text": "Our systems characterize sentence pairs as feature vectors, populated by a variety of scorers that will be described below.", "labels": [], "entities": []}, {"text": "During training, we used support vector regression (SVR) to train regressors against these vectors and their associated similarity scores.", "labels": [], "entities": []}, {"text": "The STS training data is divided into three datasets, reflecting their origin: Microsoft Research Paraphrase Corpus (MSRpar), MSR Research Video Description Corpus (MSRvid), and WMT2008 Development dataset (SMTeuroparl).", "labels": [], "entities": [{"text": "Microsoft Research Paraphrase Corpus (MSRpar)", "start_pos": 79, "end_pos": 124, "type": "DATASET", "confidence": 0.8445670008659363}, {"text": "MSR Research Video Description Corpus (MSRvid)", "start_pos": 126, "end_pos": 172, "type": "DATASET", "confidence": 0.7242822125554085}, {"text": "WMT2008 Development dataset (SMTeuroparl)", "start_pos": 178, "end_pos": 219, "type": "DATASET", "confidence": 0.8665421307086945}]}, {"text": "We trained individual regressors for each of these datasets, and applied them to their counterparts in the testing set.", "labels": [], "entities": []}, {"text": "Both Systems 1 and 2 used the following types of features: 1.", "labels": [], "entities": []}, {"text": "Resource based word to word semantic similarities.", "labels": [], "entities": [{"text": "Resource based word to word semantic similarities", "start_pos": 0, "end_pos": 49, "type": "TASK", "confidence": 0.5636714356286185}]}, {"text": "2. Cosine-based lexical similarity measure.", "labels": [], "entities": []}, {"text": "3. Bilingual Evaluation Understudy (BLEU)) lexical overlap.", "labels": [], "entities": [{"text": "Bilingual Evaluation Understudy (BLEU)) lexical overlap", "start_pos": 3, "end_pos": 58, "type": "METRIC", "confidence": 0.7365105003118515}]}, {"text": "4. Precision focused Part of Speech (POS) features.", "labels": [], "entities": [{"text": "Precision focused Part of Speech (POS)", "start_pos": 3, "end_pos": 41, "type": "TASK", "confidence": 0.7122786082327366}]}, {"text": "System 2 added the following features: 1.", "labels": [], "entities": []}, {"text": "2. Precision focused skip-bigram POS features.", "labels": [], "entities": []}, {"text": "One of the primary motivations for our the choice of features was to use relatively simple and fast features, which can be scaled up to large datasets, given appropriate caching and pre-generated lookups.", "labels": [], "entities": []}, {"text": "As the test phase included surprise datasets, whose origin was not disclosed, we also trained a fourth model using all of the training data from all three datasets.", "labels": [], "entities": []}, {"text": "Systems 1 and 2 employed this strategy for the surprise data.", "labels": [], "entities": []}, {"text": "Since the statistics for each of the training datasets varied, directly pooling them together may not be the best strategy when scoring the surprise data, whose origins were unknown.", "labels": [], "entities": []}, {"text": "To account for this, System 3 treated this as a gated regression problem, where pairs are considered to originate strictly from one dataset, and to score using a model specifically tailored for that dataset.", "labels": [], "entities": []}, {"text": "We first trained regressors on each of the datasets separately.", "labels": [], "entities": []}, {"text": "Then we trained a classifier to predict which dataset a given pair is likeliest to have been drawn from, and then applied the matching trained regressor to obtain its score.", "labels": [], "entities": []}, {"text": "This team included one of the organizers.", "labels": [], "entities": []}, {"text": "We want to stress that we took all measures to make our participation on the same conditions as the rest of participants.", "labels": [], "entities": []}, {"text": "In particular, the organizer did not allow the other member of the team to access any data or information which was not already available for the rest of participants.", "labels": [], "entities": []}, {"text": "For the rest of this system description, we first outline the scorers used to populate the feature vectors used for Systems 1 and 2.", "labels": [], "entities": []}, {"text": "We then describe the setup for performing the regression.", "labels": [], "entities": []}, {"text": "We follow with an explanation of our strategies for dealing with the surprise data, including a description of System 3.", "labels": [], "entities": []}, {"text": "We then summarize performance over the the datasets, and discuss future avenues of investigation.", "labels": [], "entities": []}], "datasetContent": [{"text": "For all three systems, we used the Stanford CoreNLP () package to perform lemmatization and POS tagging of the input sentences.", "labels": [], "entities": [{"text": "Stanford CoreNLP", "start_pos": 35, "end_pos": 51, "type": "DATASET", "confidence": 0.8718119561672211}, {"text": "POS tagging of the input sentences", "start_pos": 92, "end_pos": 126, "type": "TASK", "confidence": 0.8192708194255829}]}, {"text": "For regressors, we used LibSVM's () support vector regression capability, using radial basis kernels.", "labels": [], "entities": []}, {"text": "Based off of tuning on the training set, we set \u03b3 = 1 and the default slack value.", "labels": [], "entities": []}, {"text": "From previous experience with paraphrase identification over the MSR Paraphrase Corpus, we retained stop words in all of our experiments.", "labels": [], "entities": [{"text": "paraphrase identification", "start_pos": 30, "end_pos": 55, "type": "TASK", "confidence": 0.9593266248703003}, {"text": "MSR Paraphrase Corpus", "start_pos": 65, "end_pos": 86, "type": "DATASET", "confidence": 0.9050301313400269}]}], "tableCaptions": [{"text": " Table 1: Means and standard deviations of similarity  scores for each of the training datasets.", "labels": [], "entities": [{"text": "Means", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.992902934551239}, {"text": "standard deviations of similarity  scores", "start_pos": 20, "end_pos": 61, "type": "METRIC", "confidence": 0.86213538646698}]}, {"text": " Table 2: Pearson correlation of described systems against test data, by dataset. Overall measures are All indicates the  combined Pearson, Allnorm the normalized variant, and Mean the macro average of Pearson correlations. Rank for  the system in the overall measure is given after the slash.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 10, "end_pos": 29, "type": "METRIC", "confidence": 0.8993750512599945}, {"text": "Pearson", "start_pos": 131, "end_pos": 138, "type": "METRIC", "confidence": 0.9553387761116028}, {"text": "Allnorm", "start_pos": 140, "end_pos": 147, "type": "METRIC", "confidence": 0.966651201248169}]}, {"text": " Table 3: Confusion for the dataset predictor, used to pre- dict which dataset a pair was drawn from. This was  ddrawn using five-fold cross validation over the training  set, with columns representing golds and guesses as rows.", "labels": [], "entities": []}, {"text": " Table 4: Results on classifying pairs by source dataset,  using five-fold cross validation over training data.", "labels": [], "entities": []}]}