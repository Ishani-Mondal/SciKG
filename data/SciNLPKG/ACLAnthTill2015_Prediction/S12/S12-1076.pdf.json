{"title": [{"text": "ETS: Discriminative Edit Models for Paraphrase Scoring", "labels": [], "entities": [{"text": "Paraphrase Scoring", "start_pos": 36, "end_pos": 54, "type": "TASK", "confidence": 0.9327940046787262}]}], "abstractContent": [{"text": "Many problems in natural language processing can be viewed as variations of the task of measuring the semantic textual similarity between short texts.", "labels": [], "entities": []}, {"text": "However, many systems that address these tasks focus on a single task and mayor may not generalize well.", "labels": [], "entities": []}, {"text": "In this work, we extend an existing machine translation metric, TERp (Snover et al., 2009a), by adding support for more detailed feature types and by implementing a discriminative learning algorithm.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 36, "end_pos": 55, "type": "TASK", "confidence": 0.6861682832241058}, {"text": "TERp", "start_pos": 64, "end_pos": 68, "type": "METRIC", "confidence": 0.983496904373169}]}, {"text": "These additions facilitate applications of our system, called PERP, to similarity tasks other than machine translation evaluation , such as paraphrase recognition.", "labels": [], "entities": [{"text": "machine translation evaluation", "start_pos": 99, "end_pos": 129, "type": "TASK", "confidence": 0.7930923799673716}, {"text": "paraphrase recognition", "start_pos": 140, "end_pos": 162, "type": "TASK", "confidence": 0.8849407136440277}]}, {"text": "In the SemEval 2012 Semantic Textual Similarity task, PERP performed competitively, particularly at the two surprise subtasks revealed shortly before the submission deadline.", "labels": [], "entities": [{"text": "SemEval 2012 Semantic Textual Similarity task", "start_pos": 7, "end_pos": 52, "type": "TASK", "confidence": 0.9065888226032257}]}], "introductionContent": [{"text": "Techniques for measuring the similarity of two sentences have various potential applications: automated short answer scoring, question answering (, machine translation evaluation (), etc.", "labels": [], "entities": [{"text": "automated short answer scoring", "start_pos": 94, "end_pos": 124, "type": "TASK", "confidence": 0.5803008303046227}, {"text": "question answering", "start_pos": 126, "end_pos": 144, "type": "TASK", "confidence": 0.8978776931762695}, {"text": "machine translation evaluation", "start_pos": 148, "end_pos": 178, "type": "TASK", "confidence": 0.8058424194653829}]}, {"text": "An important aspect of this problem is that similarity is not binary.", "labels": [], "entities": []}, {"text": "Sentences can be very semantically similar, such that they might be called paraphrases of each other.", "labels": [], "entities": []}, {"text": "They might be completely different.", "labels": [], "entities": []}, {"text": "Or, they might be somewhere in between.", "labels": [], "entities": []}, {"text": "Indeed, it is arguable that all sentence pairs (except exact duplicates) lie somewhere on a continuum of similarity.", "labels": [], "entities": []}, {"text": "Therefore, it is desirable to develop methods that model sentence pair similarity on a continuous, or at least ordinal, scale.", "labels": [], "entities": [{"text": "sentence pair similarity", "start_pos": 57, "end_pos": 81, "type": "TASK", "confidence": 0.6416837573051453}]}, {"text": "In this paper, we describe a system for measuring the semantic similarity of pairs of short texts.", "labels": [], "entities": []}, {"text": "As a starting point, we use the Translation Error Rate Plus (), or TERp, system, which was specifically developed for machine translation evaluation.", "labels": [], "entities": [{"text": "Translation Error Rate Plus", "start_pos": 32, "end_pos": 59, "type": "METRIC", "confidence": 0.7921225130558014}, {"text": "TERp", "start_pos": 67, "end_pos": 71, "type": "METRIC", "confidence": 0.7057170867919922}, {"text": "machine translation evaluation", "start_pos": 118, "end_pos": 148, "type": "TASK", "confidence": 0.8706118663152059}]}, {"text": "TERp takes two sentences as input, finds a set of weighted edits that convert one into the other with low overall weight, and then produces a lengthnormalized score.", "labels": [], "entities": []}, {"text": "TERp also has a greedy, heuristic learning algorithm for inducing weights from labeled sentence pairs in order to increase correlations with human similarity scores.", "labels": [], "entities": [{"text": "TERp", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.7835971117019653}]}, {"text": "Some features of the original TERp make adaptation to other semantic similarity tasks difficult, including its largely one-to-one mapping of features to edits and its heuristic, greedy learning algorithm.", "labels": [], "entities": []}, {"text": "For example, there is a single feature for lexical substitution, even though it is clear that different types of substitutions have different effects on similarity (e.g., substituting \"43.6\" with \"17\" versus substituting \"a\" for \"an\").", "labels": [], "entities": [{"text": "lexical substitution", "start_pos": 43, "end_pos": 63, "type": "TASK", "confidence": 0.6949458867311478}]}, {"text": "In addition, the heuristic learning algorithm, which involves perturbing the weight vector by small amounts as in grid search, seems unscalable to larger sets of overlapping features.", "labels": [], "entities": []}, {"text": "Therefore, here, we use TERp's inference algorithms that find low cost edit sequences but use a discriminative learning algorithm based on the Perceptron) to estimate edit cost parameters, along with an expanded feature set for broader coverage of the phenomena that are relevant to sentence-to-sentence similarity.", "labels": [], "entities": []}, {"text": "We refer to this new approach as Paraphrase Edit Rate with the Perceptron (PERP).", "labels": [], "entities": [{"text": "Paraphrase Edit Rate", "start_pos": 33, "end_pos": 53, "type": "METRIC", "confidence": 0.7069156169891357}]}, {"text": "In addition to describing PERP, we discuss how it was applied for the SemEval 2012 Semantic Textual Similarity (STS) task.", "labels": [], "entities": [{"text": "SemEval 2012 Semantic Textual Similarity (STS) task", "start_pos": 70, "end_pos": 121, "type": "TASK", "confidence": 0.9111753702163696}]}], "datasetContent": [{"text": "In this section, we report results for the STS shared task.", "labels": [], "entities": [{"text": "STS shared task", "start_pos": 43, "end_pos": 58, "type": "TASK", "confidence": 0.6425071557362875}]}, {"text": "For a full description of the task, see.", "labels": [], "entities": []}, {"text": "The task consisted of three known subtasks (MSRpar, MSRvid, and SMT-eur) and two surprise subtasks (On-WN, SMT-news).", "labels": [], "entities": [{"text": "MSRpar", "start_pos": 44, "end_pos": 50, "type": "DATASET", "confidence": 0.8825790882110596}, {"text": "MSRvid", "start_pos": 52, "end_pos": 58, "type": "DATASET", "confidence": 0.7425220012664795}, {"text": "On-WN", "start_pos": 100, "end_pos": 105, "type": "DATASET", "confidence": 0.9498271346092224}]}, {"text": "For the known subtasks, we trained models with task-specific data only.", "labels": [], "entities": []}, {"text": "For the On-WN subtask, we used the model trained for MSRpar.", "labels": [], "entities": [{"text": "MSRpar", "start_pos": 53, "end_pos": 59, "type": "DATASET", "confidence": 0.7883262634277344}]}, {"text": "For SMT-news, we used the model trained for SMT-eur.", "labels": [], "entities": [{"text": "SMT-news", "start_pos": 4, "end_pos": 12, "type": "TASK", "confidence": 0.9311828017234802}, {"text": "SMT-eur", "start_pos": 44, "end_pos": 51, "type": "TASK", "confidence": 0.7704374194145203}]}, {"text": "Our submissions to the task included results from two variations, one using the full system (PERPphrases) and one with the paraphrase substitution edits disabled (PERP), in order to isolate the effect of including phrasal paraphrases.", "labels": [], "entities": [{"text": "paraphrase substitution edits disabled (PERP)", "start_pos": 123, "end_pos": 168, "type": "METRIC", "confidence": 0.6210366061755589}]}, {"text": "In our original submission, the PERPphrases system included a minor bug that affected the calculation of the phrasal paraphrasing features.", "labels": [], "entities": []}, {"text": "Here, we report both the original results and a corrected version (\"PERPphrases (fix)\"), though the correction only minimally affected performance.", "labels": [], "entities": [{"text": "PERPphrases (fix)\")", "start_pos": 68, "end_pos": 87, "type": "METRIC", "confidence": 0.8924328088760376}]}, {"text": "We also tested two variations of the original TERp system: one with the weights set as reported by (\"TERp (default)\"), and one tuned in the same task-specific manner as PERP (\"TERp (tuned)\").", "labels": [], "entities": [{"text": "TERp", "start_pos": 101, "end_pos": 105, "type": "METRIC", "confidence": 0.9848306179046631}, {"text": "PERP", "start_pos": 169, "end_pos": 173, "type": "METRIC", "confidence": 0.9167618155479431}]}, {"text": "We multiplied TERp's predictions by \u22121 since it produces costs rather than similarities.", "labels": [], "entities": [{"text": "TERp", "start_pos": 14, "end_pos": 18, "type": "METRIC", "confidence": 0.694232165813446}]}, {"text": "The results, in terms of Pearson correlations with test set gold standard scores, are shown in.", "labels": [], "entities": [{"text": "Pearson correlations", "start_pos": 25, "end_pos": 45, "type": "METRIC", "confidence": 0.9600139856338501}]}, {"text": "In addition to correlations for each subtask, we include the three aggregated measures used for the task.", "labels": [], "entities": []}, {"text": "The \"ALL\" measure is the Pearson correlations on the concatenation of all the data for all five subtasks.", "labels": [], "entities": [{"text": "ALL\" measure", "start_pos": 5, "end_pos": 17, "type": "METRIC", "confidence": 0.964292585849762}, {"text": "Pearson correlations", "start_pos": 25, "end_pos": 45, "type": "METRIC", "confidence": 0.9383030533790588}]}, {"text": "It was the original measured used to aggregate the results for the different subtasks.", "labels": [], "entities": []}, {"text": "The second aggregated measure is the \"Allnrm\" measure, which we view as an oracle because it uses the gold standard similarity values from the test set to adjust system predictions.", "labels": [], "entities": [{"text": "Allnrm\" measure", "start_pos": 38, "end_pos": 53, "type": "METRIC", "confidence": 0.9526075919469198}]}, {"text": "The final aggregate measure is the mean of the correlations for the subtasks, weighted by the number of examples in each subtask's test set (\"Mean\").", "labels": [], "entities": []}, {"text": "(2012) fora full description of the metrics.", "labels": [], "entities": []}, {"text": "For comparison, the table also includes the results from the top-ranked submission according to the \"ALL\" measure, the results for the word-overlap: Pearson correlations between predictions about the test data and gold standard scores.", "labels": [], "entities": [{"text": "ALL\" measure", "start_pos": 101, "end_pos": 113, "type": "METRIC", "confidence": 0.9370848933855692}]}, {"text": "\" \u2020\" marks experiments that were not parts of the official SemEval task 6 evaluation.", "labels": [], "entities": [{"text": "SemEval task 6 evaluation", "start_pos": 59, "end_pos": 84, "type": "TASK", "confidence": 0.8772676736116409}]}, {"text": "The highest correlation in each column is given in bold.", "labels": [], "entities": [{"text": "correlation", "start_pos": 12, "end_pos": 23, "type": "METRIC", "confidence": 0.9967411160469055}]}, {"text": "ALLnrm results are not included for all runs because we did not have an implementation of that measure.", "labels": [], "entities": [{"text": "ALLnrm", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.5248540043830872}]}, {"text": "baseline from the organizers (, and the means across all 88 submissions (not including the baseline).", "labels": [], "entities": []}, {"text": "shows the rankings in the official results of the PERPphrases submission, for each subtask and overall, along with Pearson correlations from PERP and the best submission for each subtask.: The ranking and correlation (\u03c1) obtained by PERPphrases for each of the five datasets as well for all datasets combined.", "labels": [], "entities": [{"text": "Pearson correlations", "start_pos": 115, "end_pos": 135, "type": "METRIC", "confidence": 0.9732712805271149}, {"text": "PERP", "start_pos": 141, "end_pos": 145, "type": "DATASET", "confidence": 0.7749887108802795}, {"text": "correlation (\u03c1)", "start_pos": 205, "end_pos": 220, "type": "METRIC", "confidence": 0.9253401309251785}]}, {"text": "The STS task had a total of 88 submissions.", "labels": [], "entities": [{"text": "STS task", "start_pos": 4, "end_pos": 12, "type": "TASK", "confidence": 0.5811649858951569}]}, {"text": "\u03c1 best shows the correlation for the best submission, across all submissions, for each dataset.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Pearson correlations between predictions about the test data and gold standard scores. \" \u2020\" marks experiments  that were not parts of the official SemEval task 6 evaluation. The highest correlation in each column is given in bold.  ALLnrm results are not included for all runs because we did not have an implementation of that measure.", "labels": [], "entities": [{"text": "Pearson correlations", "start_pos": 10, "end_pos": 30, "type": "METRIC", "confidence": 0.946427047252655}, {"text": "SemEval task 6 evaluation", "start_pos": 157, "end_pos": 182, "type": "TASK", "confidence": 0.5963838249444962}, {"text": "ALLnrm", "start_pos": 242, "end_pos": 248, "type": "METRIC", "confidence": 0.8802812695503235}]}, {"text": " Table 3: The ranking and correlation (\u03c1) obtained by  PERPphrases for each of the five datasets as well for all  datasets combined. The STS task had a total of 88 sub- missions. \u03c1 best shows the correlation for the best submis- sion, across all submissions, for each dataset.", "labels": [], "entities": [{"text": "correlation (\u03c1)", "start_pos": 26, "end_pos": 41, "type": "METRIC", "confidence": 0.9473037123680115}, {"text": "PERPphrases", "start_pos": 55, "end_pos": 66, "type": "METRIC", "confidence": 0.9684823751449585}]}]}