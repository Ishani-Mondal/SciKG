{"title": [{"text": "Regular polysemy: A distributional model", "labels": [], "entities": []}], "abstractContent": [{"text": "Many types of polysemy are not word specific, but are instances of general sense alternations such as ANIMAL-FOOD.", "labels": [], "entities": [{"text": "ANIMAL-FOOD", "start_pos": 102, "end_pos": 113, "type": "METRIC", "confidence": 0.8038879036903381}]}, {"text": "Despite their perva-siveness, regular alternations have been mostly ignored in empirical computational semantics.", "labels": [], "entities": []}, {"text": "This paper presents (a) a general framework which grounds sense alternations in corpus data, generalizes them above individual words, and allows the prediction of alternations for new words; and (b) a concrete unsupervised implementation of the framework, the Cen-troid Attribute Model.", "labels": [], "entities": []}, {"text": "We evaluate this model against a set of 2,400 ambiguous words and demonstrate that it outperforms two baselines.", "labels": [], "entities": []}], "introductionContent": [{"text": "One of the biggest challenges in computational semantics is the fact that many words are polysemous.", "labels": [], "entities": []}, {"text": "For instance, lamb can refer to an animal (as in The lamb squeezed through the gap) or to a food item (as in Sue had lamb for lunch).", "labels": [], "entities": []}, {"text": "Polysemy is pervasive inhuman language and is a problem in almost all applications of NLP, ranging from Machine Translation (as word senses can translate differently) to Textual Entailment (as most lexical entailments are sense-specific).", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 104, "end_pos": 123, "type": "TASK", "confidence": 0.837152749300003}, {"text": "Textual Entailment", "start_pos": 170, "end_pos": 188, "type": "TASK", "confidence": 0.7504861354827881}]}, {"text": "The field has thus devoted a large amount of effort to the representation and modeling of word senses.", "labels": [], "entities": [{"text": "representation and modeling of word senses", "start_pos": 59, "end_pos": 101, "type": "TASK", "confidence": 0.7078166753053665}]}, {"text": "The arguably most prominent effort is Word Sense Disambiguation, WSD, an in-vitro task whose goal is to identify which, of a set of predefined senses, is the one used in a given context.", "labels": [], "entities": [{"text": "Word Sense Disambiguation", "start_pos": 38, "end_pos": 63, "type": "TASK", "confidence": 0.7415220936139425}]}, {"text": "In work on WSD and other tasks related to polysemy, such as word sense induction, sense alternations are treated as word-specific.", "labels": [], "entities": [{"text": "WSD", "start_pos": 11, "end_pos": 14, "type": "TASK", "confidence": 0.9850629568099976}, {"text": "word sense induction", "start_pos": 60, "end_pos": 80, "type": "TASK", "confidence": 0.7595011790593466}]}, {"text": "As a result, a model for the meaning of lamb that accounts for the relation between the animal and food senses cannot predict that the same relation holds between instances of chicken or salmon in the same type of contexts.", "labels": [], "entities": []}, {"text": "A large number of studies in linguistics and cognitive science show evidence that there are regularities in the way words vary in their meaning), due to general analogical processes such as regular polysemy, metonymy and metaphor.", "labels": [], "entities": []}, {"text": "Most work in theoretical linguistics has focused on regular, systematic, or logical polysemy, which accounts for alternations like ANIMAL-FOOD.", "labels": [], "entities": []}, {"text": "Sense alternations also arise from metaphorical use of words, as dark in dark glass-dark mood, and also from metonymy when, for instance, using the name of a place fora representative (as in Germany signed the treatise).", "labels": [], "entities": [{"text": "Sense alternations", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7074525058269501}]}, {"text": "Disregarding this evidence is empirically inadequate and leads to the well-known lexical bottleneck of current word sense models, which have serious problems in achieving high coverage.", "labels": [], "entities": []}, {"text": "We believe that empirical computational semantics could profit from a model of polysemy 1 which (a) is applicable across individual words, and thus capable of capturing general patterns and generalizing to new words, and (b) is induced in an unsupervised fashion from corpus data.", "labels": [], "entities": []}, {"text": "This is a long-term goal with many unsolved subproblems.", "labels": [], "entities": []}, {"text": "The current paper presents two contributions towards this goal.", "labels": [], "entities": []}, {"text": "First, since we are working on a relatively unexplored area, we introduce a formal framework that can encompass different approaches (Section 2).", "labels": [], "entities": []}, {"text": "Second, we implement a concrete instantiation of this framework, the unsupervised Centroid Attribute Model (Section 3), and evaluate it on anew task, namely, to detect which of a set of words instantiate a given type of polysemy (Sections 4 and 5).", "labels": [], "entities": []}, {"text": "We finish with some conclusions and future work (Section 7).", "labels": [], "entities": []}], "datasetContent": [{"text": "We test CAM on the task of identifying which lemmas of a given set instantiate a specific meta alternation.", "labels": [], "entities": [{"text": "CAM", "start_pos": 8, "end_pos": 11, "type": "TASK", "confidence": 0.9183048009872437}]}, {"text": "We let the model rank the lemmas through the score function (cf. Table and Eq.) and evaluate the ranked list using Average Precision.", "labels": [], "entities": [{"text": "Average Precision", "start_pos": 115, "end_pos": 132, "type": "METRIC", "confidence": 0.8109034895896912}]}, {"text": "While an alternative would be to rank meta alternations fora given polysemous lemma, the method chosen here has the benefit of providing data on the performance of individual meta senses and meta alternations.", "labels": [], "entities": []}, {"text": "To measure success on this task, we use Average Precision (AP), an evaluation measure from IR that reaches its maximum value of 1 when all correct items are ranked at the top (.", "labels": [], "entities": [{"text": "Average Precision (AP)", "start_pos": 40, "end_pos": 62, "type": "METRIC", "confidence": 0.9716181874275207}, {"text": "IR", "start_pos": 91, "end_pos": 93, "type": "TASK", "confidence": 0.8338747024536133}]}, {"text": "It interpolates the precision values of the top-n prediction lists for all positions n in the list that contain a target.", "labels": [], "entities": [{"text": "precision", "start_pos": 20, "end_pos": 29, "type": "METRIC", "confidence": 0.9977676868438721}]}, {"text": "Let T = q 1 , . .", "labels": [], "entities": [{"text": "T", "start_pos": 4, "end_pos": 5, "type": "METRIC", "confidence": 0.9710270166397095}]}, {"text": ", q m be the list of targets, and let P = p 1 , . .", "labels": [], "entities": []}, {"text": ", p n be the list of predictions as ranked by the model.", "labels": [], "entities": []}, {"text": "Let I(x i ) = 1 if pi \u2208 T , and zero otherwise.", "labels": [], "entities": []}, {"text": "Then AP (P, . AP measures the quality of the ranked list fora single meta alternation.", "labels": [], "entities": [{"text": "AP", "start_pos": 5, "end_pos": 7, "type": "METRIC", "confidence": 0.9986245632171631}, {"text": "AP", "start_pos": 14, "end_pos": 16, "type": "METRIC", "confidence": 0.9845207333564758}]}, {"text": "The overall quality of a model is given by Mean Average Precision (MAP), the mean of the AP values for all meta alternations.", "labels": [], "entities": [{"text": "Mean Average Precision (MAP)", "start_pos": 43, "end_pos": 71, "type": "METRIC", "confidence": 0.9799820681413015}, {"text": "AP", "start_pos": 89, "end_pos": 91, "type": "METRIC", "confidence": 0.9679043292999268}]}, {"text": "We consider two baselines: (1) A random baseline that ranks all lemmas in random order.", "labels": [], "entities": []}, {"text": "This baseline is the same for all meta alternations, since the distribution is identical.", "labels": [], "entities": []}, {"text": "We estimate it by sampling.", "labels": [], "entities": []}, {"text": "(2) A meta alternation-specific frequency baseline which orders the lemmas by their corpus frequencies.", "labels": [], "entities": []}, {"text": "This amphibian (anm-art) mousse (art-fod) appropriation (act-mea) duckling ape (anm-hum) parsley (fod-plt) scissors (act-art) eel leopard (anm-sub) pickle (fod-sta) showman (agt-hum) hare lizard (anm-hum) pork (fod-mea) upholstery (act-art) baseline uses the intuition that frequent words will tend to exhibit more typical alternations.", "labels": [], "entities": []}], "tableCaptions": []}