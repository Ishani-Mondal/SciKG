{"title": [{"text": "SemEval-2012 Task 3: Spatial Role Labeling", "labels": [], "entities": [{"text": "SemEval-2012 Task", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.876594215631485}, {"text": "Spatial Role Labeling", "start_pos": 21, "end_pos": 42, "type": "TASK", "confidence": 0.8580687642097473}]}], "abstractContent": [{"text": "This SemEval2012 shared task is based on a recently introduced spatial annotation scheme called Spatial Role Labeling.", "labels": [], "entities": [{"text": "SemEval2012 shared task", "start_pos": 5, "end_pos": 28, "type": "TASK", "confidence": 0.8319402933120728}, {"text": "Spatial Role Labeling", "start_pos": 96, "end_pos": 117, "type": "TASK", "confidence": 0.8113571604092916}]}, {"text": "The Spatial Role Labeling task concerns the extraction of main components of the spatial semantics from natural language: trajectors, landmarks and spatial indicators.", "labels": [], "entities": [{"text": "Spatial Role Labeling task", "start_pos": 4, "end_pos": 30, "type": "TASK", "confidence": 0.9071842133998871}]}, {"text": "In addition to these major components, the links between them and the general-type of spatial relationships including region, direction and distance are targeted.", "labels": [], "entities": []}, {"text": "The annotated dataset contains about 1213 sentences which describe 612 images of the CLEF IAPR TC-12 Image Benchmark.", "labels": [], "entities": [{"text": "CLEF IAPR TC-12 Image Benchmark", "start_pos": 85, "end_pos": 116, "type": "DATASET", "confidence": 0.9085929155349731}]}, {"text": "We have one participant system with two runs.", "labels": [], "entities": []}, {"text": "The participant's runs are compared to the system in (Kordjamshidi et al., 2011c) which is provided by task organizers.", "labels": [], "entities": []}], "introductionContent": [{"text": "One of the essential functions of natural language is to talk about spatial relationships between objects.", "labels": [], "entities": []}, {"text": "The sentence \"Give me the book on AI on the big table behind the wall.\" expresses information about the spatial configuration of the objects (book, table, wall) in some space.", "labels": [], "entities": []}, {"text": "Particularly, it explains the region occupied by the book with respect to the table and the direction (orientation) of the table with respect to the wall.", "labels": [], "entities": []}, {"text": "Understanding such spatial utterances is a problem in many areas, including robotics, navigation, traffic management, and query answering systems.", "labels": [], "entities": [{"text": "traffic management", "start_pos": 98, "end_pos": 116, "type": "TASK", "confidence": 0.7365757972002029}, {"text": "query answering", "start_pos": 122, "end_pos": 137, "type": "TASK", "confidence": 0.7164532244205475}]}, {"text": "Linguistic constructs can express highly complex, relational structures of objects, spatial relations between them, and patterns of motion through space relative to some reference point.", "labels": [], "entities": []}, {"text": "Compared to natural language, formal spatial models focus on one particular spatial aspect such as orientation, topology or distance and specify its underlying spatial logic in detail.", "labels": [], "entities": []}, {"text": "These formal models enable spatial reasoning that is difficult to perform on natural language expressions.", "labels": [], "entities": []}, {"text": "Learning how to map natural language spatial information onto a formal representation is a challenging problem.", "labels": [], "entities": []}, {"text": "The complexity of spatial semantics from the cognitive-linguistic point of view on the one hand, the diversity of formal spatial representation models in different applications on the other hand and the gap between the specification level of the two sides has led to the present situation that no well-defined framework for automatic spatial information extraction exists that can handle all of these aspects.", "labels": [], "entities": [{"text": "automatic spatial information extraction", "start_pos": 324, "end_pos": 364, "type": "TASK", "confidence": 0.7241808474063873}]}, {"text": "Ina previous paper (), we introduced the task of spatial role labeling (SpRL) and proposed an annotation scheme that is language-independent and practically facilitates the application of machine learning techniques.", "labels": [], "entities": [{"text": "spatial role labeling (SpRL)", "start_pos": 49, "end_pos": 77, "type": "TASK", "confidence": 0.7940125564734141}]}, {"text": "Our framework consists of a set of spatial roles based on the theory of holistic spatial semantics) with the intent of covering the main aspects of spatial concepts at a course level, including both static and dynamic spatial semantics.", "labels": [], "entities": []}, {"text": "This shared task is defined on the basis of that annotation scheme.", "labels": [], "entities": []}, {"text": "Since this is the first shared task on the spatial information and this particular data, we proposed a simplified version of the original scheme.", "labels": [], "entities": []}, {"text": "The intention of this simplification was to make this practice feasible in the given timeframe.", "labels": [], "entities": []}, {"text": "However, the current task is very challenging particularly for learning the spatial links and relations.", "labels": [], "entities": []}, {"text": "The core problem of SpRL is: i) the identification of the words that play a role in describing spatial concepts, and ii) the classification of the relational role that these words play in the spatial configuration.", "labels": [], "entities": [{"text": "SpRL", "start_pos": 20, "end_pos": 24, "type": "TASK", "confidence": 0.9184696078300476}]}, {"text": "For example, consider again the sentence \"Give me the book on AI on the big table behind the wall.\".", "labels": [], "entities": []}, {"text": "The phrase headed by the token book is referring to a trajector object.", "labels": [], "entities": []}, {"text": "The trajector (TR) is an entity whose location is described in the sentence.", "labels": [], "entities": []}, {"text": "The phrase headed by the token table is referring to the role of a landmark (LM).", "labels": [], "entities": []}, {"text": "The landmark is a reference object for describing the location of a trajector.", "labels": [], "entities": []}, {"text": "These two spatial entities are related by the spatial expression on denoted as spatial indicator (SP).", "labels": [], "entities": [{"text": "spatial indicator (SP)", "start_pos": 79, "end_pos": 101, "type": "METRIC", "confidence": 0.832957124710083}]}, {"text": "The spatial indicator (often a preposition in English, but sometimes a verb, noun, adjective, or adverb) indicates the existence of spatial information in the sentence and establishes the type of a spatial relation.", "labels": [], "entities": []}, {"text": "The spatial relations that can be extracted from the whole sentence are <on SP book T R table LM > and <behind SP table T R wall LM >.", "labels": [], "entities": []}, {"text": "One could also use spatial reasoning to infer that the statement <behind book wall> holds, however, such inferred relations are not considered in this task.", "labels": [], "entities": []}, {"text": "Although the spatial indicators are mostly prepositions, the reverse may not hold-for example, the first preposition on only states the topic of the book, so <on book AI> is not a spatial relation.", "labels": [], "entities": []}, {"text": "For each of the true spatial relations, a general type is assigned.", "labels": [], "entities": []}, {"text": "The <on SP book T R table LM > relation expresses a kind of topological relationship between the two objects and we assign it a general type named region.", "labels": [], "entities": []}, {"text": "The <behind SP table T R wall LM > relation expresses directional information and we assign it a general type named direction.", "labels": [], "entities": []}, {"text": "In general we assume two main abstraction layers for the extraction of spatial information: (a) a linguistic layer, corresponding to the annotation scheme described above, which starts with unrestricted natural language and predicts the existence of spatial information at the sentence level by identifying the words that play a particular spatial role as well as their spatial relationship; (b) a formal layer, in which the spatial roles are mapped onto a spatial calculus model.", "labels": [], "entities": []}, {"text": "For example, the linguistic layer recognizes that the spatial relation (on) holds between book and table, and the formal layer maps this to a specific, formal spatial representation, e.g., a logical representation like AboveExternallyConnected(book, table) or a formal qualitative spatial representation like EC (externally connected) in the RCC model (Regional Connection Calculus).", "labels": [], "entities": []}, {"text": "In this shared task we focus on the first (linguistic) level which is a necessary step for mapping natural language to any formal spatial calculus.", "labels": [], "entities": []}, {"text": "The main roles that are considered here are trajector, landmark, spatial indicator, their links and the general type of their spatial relation.", "labels": [], "entities": []}, {"text": "The general type of a relation can be direction, region or distance.", "labels": [], "entities": []}], "datasetContent": [{"text": "The annotated corpus that we used for this shared task is a subset of IAPR TC-12 image Benchmark ().", "labels": [], "entities": [{"text": "IAPR TC-12 image Benchmark", "start_pos": 70, "end_pos": 96, "type": "DATASET", "confidence": 0.906634971499443}]}, {"text": "It contains 613 text files that include 1213 sentences in total.", "labels": [], "entities": []}, {"text": "This is an extension of the dataset used in ().", "labels": [], "entities": []}, {"text": "The original corpus was available free of charge and without copyright restrictions.", "labels": [], "entities": []}, {"text": "The corpus contains images taken by tourists with descriptions in different languages.", "labels": [], "entities": []}, {"text": "The texts describe objects, and their absolute and relative positions in the image.", "labels": [], "entities": []}, {"text": "This makes the corpus a rich resource for spatial information.", "labels": [], "entities": []}, {"text": "However the descriptions are not always limited to spatial information.", "labels": [], "entities": []}, {"text": "Therefore they are less domain-specific and contain free explanations about the images.", "labels": [], "entities": []}, {"text": "shows the detailed statistics of this data.", "labels": [], "entities": []}, {"text": "The average length of the sentences in this data is about 15 words including punctuation marks with a standard deviation of 8.", "labels": [], "entities": []}, {"text": "The spatial roles are assigned both to phrases and their headwords, but only the headwords are evaluated for this task.", "labels": [], "entities": []}, {"text": "The spatial relations indicate a triplet of these roles.", "labels": [], "entities": []}, {"text": "The general-type is assigned to each triplet of spatial indicator, trajector and landmark.", "labels": [], "entities": []}, {"text": "At the starting point two annotators including one task-organizer and another non-expert annotator, annotated 325 sentences for the spatial roles and relations.", "labels": [], "entities": []}, {"text": "The purpose was to realize the disagreement points and prepare a set of instructions in away to achieve highest-possible agreement.", "labels": [], "entities": []}, {"text": "From the first effort an inter-annotator agreement of 0.89 for Cohen's kappa was obtained.", "labels": [], "entities": []}, {"text": "We continued with the a third annotator for the remaining 888 sentences.", "labels": [], "entities": []}, {"text": "The annotator had an explanatory session and received a set of instructions and annotated examples to decrease the ambiguity in the annotations.", "labels": [], "entities": []}, {"text": "To avoid complexity only the relations that are directly expressed in the sentence are annotated and spatial reasoning was avoided during the annotations.", "labels": [], "entities": []}, {"text": "Sometimes the trajectors and landmarks or both are implicit, meaning that there is no word in the sentence to represent them.", "labels": [], "entities": []}, {"text": "For example in the sentence Come over here, the trajector you is only implicitly present.", "labels": [], "entities": []}, {"text": "To be consistent with the number of arguments in spatial relations, in these cases we use the term undefined for the implicit roles.", "labels": [], "entities": []}, {"text": "Therefore, the spatial relation in the above example is <over SP undefined T R here LM >.", "labels": [], "entities": []}, {"text": "According to the usual setting of the shared tasks our evaluation setting was based on splitting the data set into a training and a testing set.", "labels": [], "entities": []}, {"text": "Each set contained about 50% of the whole data.", "labels": [], "entities": []}, {"text": "The test set re-leased without the ground-truth labels.", "labels": [], "entities": []}, {"text": "However, after the systems submission deadline the ground-truth test was released.", "labels": [], "entities": []}, {"text": "Hence the participant group performed an additional 10-fold cross validation evaluation too.", "labels": [], "entities": []}, {"text": "We report the results of both evaluation settings.", "labels": [], "entities": []}, {"text": "Prediction of each component including TRAJECTORs, LANDMARKs and SPATIAL-INDICATORs is evaluated on the test set using their individual spatial element XML tags.", "labels": [], "entities": [{"text": "TRAJECTORs", "start_pos": 39, "end_pos": 49, "type": "METRIC", "confidence": 0.8432721495628357}]}, {"text": "The evaluation metrics of precision, recall and F1-measure are used, which are defined as: where: TP = the number of system-produced XML tags that match an annotated XML tag, FP = the number of system-produced XML tags that do not match an annotated tag, FN = the number of annotated XML tags that do not match a system-produced tag.", "labels": [], "entities": [{"text": "precision", "start_pos": 26, "end_pos": 35, "type": "METRIC", "confidence": 0.9996296167373657}, {"text": "recall", "start_pos": 37, "end_pos": 43, "type": "METRIC", "confidence": 0.9995933175086975}, {"text": "F1-measure", "start_pos": 48, "end_pos": 58, "type": "METRIC", "confidence": 0.9992306232452393}, {"text": "TP", "start_pos": 98, "end_pos": 100, "type": "METRIC", "confidence": 0.9917045831680298}, {"text": "FP", "start_pos": 175, "end_pos": 177, "type": "METRIC", "confidence": 0.9948861002922058}, {"text": "FN", "start_pos": 255, "end_pos": 257, "type": "METRIC", "confidence": 0.9435847401618958}]}, {"text": "For the roles evaluation two XML tags match when they have exactly same identifier.", "labels": [], "entities": []}, {"text": "In fact, when the identifiers are the same then the role and the word index are the same.", "labels": [], "entities": []}, {"text": "In addition, systems are evaluated on how well they are able to retrieve triplets of (trajector, spatial-indicator, landmark), in terms of precision, recall and F1-measure.", "labels": [], "entities": [{"text": "precision", "start_pos": 139, "end_pos": 148, "type": "METRIC", "confidence": 0.9995935559272766}, {"text": "recall", "start_pos": 150, "end_pos": 156, "type": "METRIC", "confidence": 0.9994944334030151}, {"text": "F1-measure", "start_pos": 161, "end_pos": 171, "type": "METRIC", "confidence": 0.9982197880744934}]}, {"text": "The TP, FP, FN are counted in a similar way but two RELA-TION tags match if the combination of their TR, LM and SP is exactly the same.", "labels": [], "entities": [{"text": "FP", "start_pos": 8, "end_pos": 10, "type": "METRIC", "confidence": 0.9809938669204712}, {"text": "FN", "start_pos": 12, "end_pos": 14, "type": "METRIC", "confidence": 0.8440601825714111}, {"text": "RELA-TION", "start_pos": 52, "end_pos": 61, "type": "METRIC", "confidence": 0.9970284104347229}]}, {"text": "In other words a true prediction requires all the three elements are correctly predicted at the same time.", "labels": [], "entities": []}, {"text": "The last evaluation is on how well the systems are able to retrieve the relations and their general type i.e {region, direction, distance} at the same time.", "labels": [], "entities": []}, {"text": "To evaluate the GENERAL-TYPE similarly the RELA-TION tag is checked.", "labels": [], "entities": [{"text": "GENERAL-TYPE", "start_pos": 16, "end_pos": 28, "type": "METRIC", "confidence": 0.7072795033454895}, {"text": "RELA-TION", "start_pos": 43, "end_pos": 52, "type": "METRIC", "confidence": 0.9978877902030945}]}, {"text": "For a true prediction, an exact match between the ground-truth and all the elements of the predicted RELATION tag including TR, LM,SP and GENERAL-TYPE is required.", "labels": [], "entities": [{"text": "RELATION", "start_pos": 101, "end_pos": 109, "type": "METRIC", "confidence": 0.9959515333175659}, {"text": "TR", "start_pos": 124, "end_pos": 126, "type": "METRIC", "confidence": 0.98447185754776}, {"text": "GENERAL-TYPE", "start_pos": 138, "end_pos": 150, "type": "METRIC", "confidence": 0.9360166192054749}]}], "tableCaptions": [{"text": " Table 1: Number of annotated components in the data set.", "labels": [], "entities": []}, {"text": " Table 2: UTDSPRL-SUPERVISED1: The University  of Texas-Dallas system with a larger number of fea- tures,test/train one split.", "labels": [], "entities": [{"text": "UTDSPRL-SUPERVISED1", "start_pos": 10, "end_pos": 29, "type": "DATASET", "confidence": 0.9317677617073059}]}, {"text": " Table 3: UTDSPRL-SUPERVISED2: The University of  Texas-Dallas system with a smaller number of features,  test/train one split.", "labels": [], "entities": [{"text": "UTDSPRL-SUPERVISED2", "start_pos": 10, "end_pos": 29, "type": "DATASET", "confidence": 0.9182697534561157}]}, {"text": " Table 4: KUL-SKIP-CHAIN-CRF: The organizers' sys- tem (Kordjamshidi et al., 2011c)-test/train one split.", "labels": [], "entities": [{"text": "KUL-SKIP-CHAIN-CRF", "start_pos": 10, "end_pos": 28, "type": "METRIC", "confidence": 0.6347957849502563}]}, {"text": " Table 5: The RELATION extraction of KUL-SKIP-CHAIN-CRF (Kordjamshidi et al., 2011c) vs. UTDSPRL- SUPERVISED2 evaluated with 10-fold cross validation", "labels": [], "entities": [{"text": "RELATION", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9956493973731995}]}]}