{"title": [{"text": "UTD-SpRL: A Joint Approach to Spatial Role Labeling", "labels": [], "entities": [{"text": "UTD-SpRL", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.8132133483886719}, {"text": "Spatial Role Labeling", "start_pos": 30, "end_pos": 51, "type": "TASK", "confidence": 0.7604682445526123}]}], "abstractContent": [{"text": "We present a joint approach for recognizing spatial roles in SemEval-2012 Task 3.", "labels": [], "entities": [{"text": "SemEval-2012 Task 3", "start_pos": 61, "end_pos": 80, "type": "TASK", "confidence": 0.8434879382451376}]}, {"text": "Candidate spatial relations, in the form of triples, are heuristically extracted from sentences with high recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 106, "end_pos": 112, "type": "METRIC", "confidence": 0.9899542331695557}]}, {"text": "The joint classification of spatial roles is then cast as a binary classification over the candidates.", "labels": [], "entities": []}, {"text": "This joint approach allows fora rich feature set based on the complete relation instead of individual relation arguments.", "labels": [], "entities": []}, {"text": "Our best official submission achieves an F 1-measure of 0.573 on relation recognition, best in the task and outperforming the previous best result on the same data set (0.500).", "labels": [], "entities": [{"text": "F 1-measure", "start_pos": 41, "end_pos": 52, "type": "METRIC", "confidence": 0.9931114315986633}, {"text": "relation recognition", "start_pos": 65, "end_pos": 85, "type": "TASK", "confidence": 0.9305072128772736}]}], "introductionContent": [{"text": "A significant amount of spatial information in natural language is encoded in spatial relationships between objects.", "labels": [], "entities": []}, {"text": "In this paper, we present our approach for detecting the special case of spatial relations evaluated in SemEval-2012 Task 3, Spatial Role Labeling (SpRL) (.", "labels": [], "entities": [{"text": "Spatial Role Labeling (SpRL)", "start_pos": 125, "end_pos": 153, "type": "TASK", "confidence": 0.7749981880187988}]}, {"text": "This task considers the most common type of spatial relationships between objects, namely those described with a spatial preposition (e.g., in, on, over) or a spatial phrase (e.g., in front of, on the left), referred to as the spatial INDICATOR.", "labels": [], "entities": []}, {"text": "A spatial INDI-CATOR connects an object of interest (the TRAJEC-TOR) with a grounding location (the LANDMARK).", "labels": [], "entities": [{"text": "TRAJEC-TOR", "start_pos": 57, "end_pos": 67, "type": "METRIC", "confidence": 0.9921768307685852}, {"text": "LANDMARK", "start_pos": 100, "end_pos": 108, "type": "METRIC", "confidence": 0.8332164883613586}]}, {"text": "Examples of this type of spatial relationship include: (1) SpRL is a type of semantic role labeling (SRL), where the spatial INDICA-TOR is the predicate (or trigger) and the TRAJEC-TOR and LANDMARK are its two arguments.", "labels": [], "entities": [{"text": "semantic role labeling (SRL)", "start_pos": 77, "end_pos": 105, "type": "TASK", "confidence": 0.7816827595233917}, {"text": "TRAJEC-TOR", "start_pos": 174, "end_pos": 184, "type": "METRIC", "confidence": 0.9757730960845947}]}, {"text": "Previous approaches to) have largely followed the commonly employed SRL pipeline: (1) find predicates (i.e., the INDICATOR), (2) recognize the predicate's syntactic constituents, and (3) classify the constituent's role (i.e., TRA-JECTOR, LANDMARK, or neither).", "labels": [], "entities": [{"text": "SRL pipeline", "start_pos": 68, "end_pos": 80, "type": "TASK", "confidence": 0.8902269899845123}, {"text": "TRA-JECTOR", "start_pos": 226, "end_pos": 236, "type": "METRIC", "confidence": 0.9080187678337097}]}, {"text": "The problem with this approach is that arguments are considered largely in isolation.", "labels": [], "entities": []}, {"text": "Consider the following: (5) there is a picture on the wall above the bed.", "labels": [], "entities": []}, {"text": "This sentence contains three objects (picture, wall, and bed) and two INDICATORs (on and above).", "labels": [], "entities": [{"text": "INDICATORs", "start_pos": 70, "end_pos": 80, "type": "METRIC", "confidence": 0.9952927827835083}]}, {"text": "Since the most common spatial relation pattern is simply trajector-indicator-landmark (as in Examples and), the triple wall-above-bed is a likely candidate relation.", "labels": [], "entities": []}, {"text": "However, the semantics of these objects invalidates the relation (i.e., walls are beside beds, ceilings are above them).", "labels": [], "entities": []}, {"text": "Instead the correct relation is picture-above-bed because the preposition above syntactically attaches to picture instead of wall.", "labels": [], "entities": []}, {"text": "Prepositional attachment, however, is a difficult syntactic problem solved largely through the use of semantics, so an understanding of the consistency of spatial relationships plays an important role in their recognition.", "labels": [], "entities": [{"text": "Prepositional attachment", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.9032489955425262}]}, {"text": "Consistency checking is not possible under a pipeline approach that classifies whether wall as the TRAJECTOR without any knowledge of its LANDMARK.", "labels": [], "entities": [{"text": "Consistency checking", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.7744004428386688}, {"text": "TRAJECTOR", "start_pos": 99, "end_pos": 108, "type": "METRIC", "confidence": 0.9172407388687134}]}, {"text": "We therefore propose an alternative to this pipeline approach that jointly decides whether a given TRAJECTOR-INDICATOR-LANDMARK triple expresses a spatial relation.", "labels": [], "entities": [{"text": "TRAJECTOR-INDICATOR-LANDMARK", "start_pos": 99, "end_pos": 127, "type": "METRIC", "confidence": 0.9272431135177612}]}, {"text": "We utilize a high recall heuristic for recognizing objects capable of participating in a spatial relation as well as a lexicon of INDICATORs.", "labels": [], "entities": [{"text": "recall", "start_pos": 18, "end_pos": 24, "type": "METRIC", "confidence": 0.9913334250450134}]}, {"text": "All possible combinations of these arguments (including undefined LANDMARKs) are considered by a binary classifier in order to make a joint decision.", "labels": [], "entities": []}, {"text": "This allows us to incorporate features based on all three relation elements such as the relation's semantic consistency.", "labels": [], "entities": []}], "datasetContent": [{"text": "The heuristics described in Section 2.1 that enable joint classification were tuned for the training data, but their recall on the test data places a strict upper bound on the recall to our overall approach.", "labels": [], "entities": [{"text": "joint classification", "start_pos": 52, "end_pos": 72, "type": "TASK", "confidence": 0.5731068849563599}, {"text": "recall", "start_pos": 117, "end_pos": 123, "type": "METRIC", "confidence": 0.9983056783676147}, {"text": "recall", "start_pos": 176, "end_pos": 182, "type": "METRIC", "confidence": 0.9945554733276367}]}, {"text": "It is therefore important to understand the performance loss that occurs at this step.", "labels": [], "entities": []}, {"text": "shows the performance of our heuristics on the training and test data.", "labels": [], "entities": []}, {"text": "The spatial INDICA-TOR lexicon has perfect recall on the training data because it was built from this data set.", "labels": [], "entities": [{"text": "recall", "start_pos": 43, "end_pos": 49, "type": "METRIC", "confidence": 0.9994787573814392}]}, {"text": "However, it performs at only 0.951 recall on the test data, as almost 5% of the INDICATORs in the test data were not seen in the training data.", "labels": [], "entities": [{"text": "recall", "start_pos": 35, "end_pos": 41, "type": "METRIC", "confidence": 0.9956198334693909}, {"text": "INDICATORs", "start_pos": 80, "end_pos": 90, "type": "METRIC", "confidence": 0.9401143193244934}]}, {"text": "Most of these are phrasal verbs (e.g., sailing over) or include the modifier very (e.g., to the very left).", "labels": [], "entities": []}, {"text": "Our spatial object recognizer performed better, only dropping from 0.998 (2 errors) to 0.989 (16 errors).", "labels": [], "entities": [{"text": "spatial object recognizer", "start_pos": 4, "end_pos": 29, "type": "TASK", "confidence": 0.6159691313902537}]}, {"text": "Some of these errors resulted from mis-spellings (e.g., housed instead of houses), non-head spatial objects (mountain from the NP mountain landscape), NPs containing conjunctions (trees in two palm trees, lamps and flags, which gets marked as one simple NP), as well as parser errors.", "labels": [], "entities": []}, {"text": "The significant drop in precision for both spatial indicators and objects is an additional concern.", "labels": [], "entities": [{"text": "precision", "start_pos": 24, "end_pos": 33, "type": "METRIC", "confidence": 0.9996151924133301}]}, {"text": "This does not indicate the extracted items were not valid as potential indicators or objects, but rather that no gold relation contained them.", "labels": [], "entities": []}, {"text": "As explained in Section 4, this is likely caused by the disparity in sentence length: longer sentences result in more matches, but not necessarily more relations.", "labels": [], "entities": []}, {"text": "As evidence of this, despite the training and test data containing almost the same number of sentences, there are 36% more spatial indicators and 20% more spatial objects in the test set.", "labels": [], "entities": []}, {"text": "After the evaluation deadline, the task organizers provided the gold test data, allowing us to perform additional experiments.", "labels": [], "entities": []}, {"text": "In this process we found several annotation errors which we needed to fix in order to process our gold results.", "labels": [], "entities": []}, {"text": "These errors were largely annotations that were given an incorrect token index, resulting in the annotation text not matching the referenced text.", "labels": [], "entities": []}, {"text": "These fixes increased our performance, shown on: Additional experiments on corrected test data using the supervised2 data set.", "labels": [], "entities": [{"text": "supervised2 data set", "start_pos": 105, "end_pos": 125, "type": "DATASET", "confidence": 0.9889542460441589}]}, {"text": "-NSI indicates that the gold spatial INDICATORs that are not in the lexicon are removed.", "labels": [], "entities": [{"text": "NSI", "start_pos": 1, "end_pos": 4, "type": "METRIC", "confidence": 0.9170986413955688}, {"text": "INDICATORs", "start_pos": 37, "end_pos": 47, "type": "METRIC", "confidence": 0.8741982579231262}]}, {"text": "CV indicates 10-fold cross validation.", "labels": [], "entities": []}, {"text": "We use this updated data set for the following experiments.", "labels": [], "entities": []}, {"text": "While the results aren't comparable to other methods, the goal of these experiments is to analyze our system under various configurations by their relative performance.", "labels": [], "entities": []}, {"text": "also shows a 10-fold cross validation performance on 3 data sets: (1) the training data, (2) the test data, and (3) both the training and test data.", "labels": [], "entities": []}, {"text": "While our feature set is tuned to the training data, the test data is clearly more difficult.", "labels": [], "entities": []}, {"text": "Section 4 discusses the differences between the training and test data that may lead to such a performance reduction.", "labels": [], "entities": []}, {"text": "Since our lexicon of spatial INDICATORs was built from the training data, our method will not recognize any relations that use unseen INDICATORs.", "labels": [], "entities": []}, {"text": "To differentiate between how our method performs on the full test data and just those INDICATORs that are in the lexicon, we removed the 39 gold relations with unseen INDICATORs and re-tested the system.", "labels": [], "entities": []}, {"text": "As can be seen in (under -NSI), this improves recall by 2.6 points.", "labels": [], "entities": [{"text": "recall", "start_pos": 46, "end_pos": 52, "type": "METRIC", "confidence": 0.9996705055236816}]}, {"text": "To estimate the contribution of our features, we performed an additive experiment to see how each feature contributes to the overall test score.", "labels": [], "entities": []}, {"text": "shows the feature contributions based on the order they were added by the feature selector.", "labels": [], "entities": []}, {"text": "For many of the features the score goes down when added.", "labels": [], "entities": []}, {"text": "However, without these features, the final score would drop to 0.578, indicating they still provide valuable information in the context of the other features.", "labels": [], "entities": []}, {"text": "Table 5 shows performance on the updated test set   when individual features are removed.", "labels": [], "entities": []}, {"text": "Here, six features that were useful on the training data did not prove useful on the test data.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Official results for submissions.", "labels": [], "entities": []}, {"text": " Table 2: Results of relation candidate selection heuristics.", "labels": [], "entities": [{"text": "relation candidate selection heuristics", "start_pos": 21, "end_pos": 60, "type": "TASK", "confidence": 0.7336634248495102}]}, {"text": " Table 3: Additional experiments on corrected test data  using the supervised2 data set. -NSI indicates that the  gold spatial INDICATORs that are not in the lexicon are  removed. CV indicates 10-fold cross validation.", "labels": [], "entities": [{"text": "supervised2 data set", "start_pos": 67, "end_pos": 87, "type": "DATASET", "confidence": 0.964647630850474}, {"text": "NSI", "start_pos": 90, "end_pos": 93, "type": "METRIC", "confidence": 0.8202810883522034}]}, {"text": " Table 4: Additive feature experiment results using the su- pervised2 features. Bold indicates increases in F 1 over  the previous feature set.", "labels": [], "entities": [{"text": "F 1", "start_pos": 108, "end_pos": 111, "type": "METRIC", "confidence": 0.9930832087993622}]}, {"text": " Table 5: Results when individual features from the super- vised2 submission are removed. Bold indicates improve- ment when the feature is removed.", "labels": [], "entities": [{"text": "improve- ment", "start_pos": 105, "end_pos": 118, "type": "METRIC", "confidence": 0.9034410715103149}]}]}