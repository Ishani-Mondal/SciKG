{"title": [{"text": "Sbdlrhmn: A Rule-based Human Interpretation System for Semantic Textual Similarity Task", "labels": [], "entities": [{"text": "Semantic Textual Similarity", "start_pos": 55, "end_pos": 82, "type": "TASK", "confidence": 0.6704303820927938}]}], "abstractContent": [{"text": "In this paper, we describe the system architecture used in the Semantic Textual Similarity (STS) task 6 pilot challenge.", "labels": [], "entities": [{"text": "Semantic Textual Similarity (STS) task 6 pilot challenge", "start_pos": 63, "end_pos": 119, "type": "TASK", "confidence": 0.8356801718473434}]}, {"text": "The goal of this challenge is to accurately identify five levels of semantic similarity between two sentences: equivalent, mostly equivalent, roughly equivalent , not equivalent but sharing the same topic and no equivalence.", "labels": [], "entities": []}, {"text": "Our participations were two systems.", "labels": [], "entities": []}, {"text": "The first system (rule-based) combines both semantic and syntax features to arrive at the overall similarity.", "labels": [], "entities": []}, {"text": "The proposed rules enable the system to adequately handle domain knowledge gaps that are inherent when working with knowledge resources.", "labels": [], "entities": []}, {"text": "As such one of its main goals, the system suggests a set of domain-free rules to help the human annotator in scoring semantic equivalence of two sentences.", "labels": [], "entities": []}, {"text": "The second system is our baseline in which we use the Cosine Similarity between the words in each sentence pair.", "labels": [], "entities": []}], "introductionContent": [{"text": "Accurately establishing sentence semantic similarity would provide one of the key ingredients for solutions to many text-related applications, such as automatic grading systems, paraphrasing), text entailment () and summarization ().", "labels": [], "entities": [{"text": "sentence semantic similarity", "start_pos": 24, "end_pos": 52, "type": "TASK", "confidence": 0.7012012799580892}, {"text": "text entailment", "start_pos": 193, "end_pos": 208, "type": "TASK", "confidence": 0.733590841293335}, {"text": "summarization", "start_pos": 216, "end_pos": 229, "type": "TASK", "confidence": 0.9863515496253967}]}, {"text": "Current approaches for computing semantic similarity between a pair of sentences focus on analyzing their shared words, structures (, semantics (;; Hatzivassiloglou, 1999) or any of their combinations ().", "labels": [], "entities": [{"text": "computing semantic similarity between a pair of sentences", "start_pos": 23, "end_pos": 80, "type": "TASK", "confidence": 0.8506966754794121}]}, {"text": "The goal is to arrive at a score which increases proportionally with the relatedness between the two sentences.", "labels": [], "entities": []}, {"text": "Yet, they are not concerned with scoring the interpretations of such relatedness (.", "labels": [], "entities": []}, {"text": "Semantic Textual Similarity (STS), SEMEVAL-12 Task 6 (), measures the degree of semantic equivalence between a pair of sentences by comparing meaningful contents within a sentence.", "labels": [], "entities": [{"text": "Semantic Textual Similarity (STS)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7912820825974146}]}, {"text": "The assigned scores range from 0 to 5 for each sentence pair with the following interpretations: (5) completely equivalent, (4) mostly equivalent pair with missing unimportant information, (3) roughly equivalent with missing important information, (2) not equivalent, but sharing some details, (1) not equivalent but sharing the same topic and (0) not equivalent and on different topics.", "labels": [], "entities": []}, {"text": "The goal of developing our rule-based system was to identify knowledge representations which have possibly all task human interpretations.", "labels": [], "entities": []}, {"text": "Meanwhile, the system domain-free rules aim to help the human annotator in scoring semantic equivalence of sentence pair.", "labels": [], "entities": []}, {"text": "The proposed rule-based solution exploits both sentence syntax and semantics.", "labels": [], "entities": []}, {"text": "First, it uses Stanford parser () to expose the sentence structure, part-of-speech (POS) word tags, parse tree and Subject-Verb-Object (S-V-O) dependencies.", "labels": [], "entities": []}, {"text": "Second, Illinois Coreference Package) is used to extract sentence named entities resolving possible men-tions. and Adapted Lesk Algorithm for word sense disambiguation ( are used to compute each sentence word semantic relatedness to the other sentence.) augments WordNet in case of uncovered words and helps us to discriminate the topics of sentences.", "labels": [], "entities": [{"text": "Illinois Coreference Package", "start_pos": 8, "end_pos": 36, "type": "DATASET", "confidence": 0.9277171492576599}, {"text": "word sense disambiguation", "start_pos": 142, "end_pos": 167, "type": "TASK", "confidence": 0.708204040924708}, {"text": "WordNet", "start_pos": 263, "end_pos": 270, "type": "DATASET", "confidence": 0.9611037969589233}]}, {"text": "We use thought to compare the sentence pair words with each other.", "labels": [], "entities": []}, {"text": "Finally, we evolve a rule-based module to present the human heuristics when he interprets the relatedness of the sentence pair meaningful contents.", "labels": [], "entities": []}, {"text": "Throughout our training and testing experiments, we used  The reminder of this paper is organized as follows: Section 2 describes our two participations; Section 3 discusses their official results; Section 4 draws our conclusion for both systems.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2 provides in the official results of our  system Pearson-Correlation measure.", "labels": [], "entities": [{"text": "Pearson-Correlation", "start_pos": 57, "end_pos": 76, "type": "METRIC", "confidence": 0.9357805252075195}]}, {"text": " Table 3. Run 2 Official Person-Correlation measure", "labels": [], "entities": []}]}