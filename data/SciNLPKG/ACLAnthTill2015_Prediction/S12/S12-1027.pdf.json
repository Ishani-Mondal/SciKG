{"title": [{"text": "An Evaluation of Graded Sense Disambiguation using Word Sense Induction", "labels": [], "entities": [{"text": "Graded Sense Disambiguation", "start_pos": 17, "end_pos": 44, "type": "TASK", "confidence": 0.6982389092445374}, {"text": "Word Sense Induction", "start_pos": 51, "end_pos": 71, "type": "TASK", "confidence": 0.5278239647547404}]}], "abstractContent": [{"text": "Word Sense Disambiguation aims to label the sense of a word that best applies in a given context.", "labels": [], "entities": [{"text": "Word Sense Disambiguation", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.6119081974029541}]}, {"text": "Graded word sense disambiguation relaxes the single label assumption, allowing for multiple sense labels with varying degrees of applicability.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 7, "end_pos": 32, "type": "TASK", "confidence": 0.6330932776133219}]}, {"text": "Training multi-label classifiers for such a task requires substantial amounts of annotated data, which is currently not available.", "labels": [], "entities": []}, {"text": "We consider an alternate method of annotating graded senses using Word Sense Induction, which automatically learns the senses and their features from corpus properties.", "labels": [], "entities": []}, {"text": "Our work proposes three objective to evaluate performance on the graded sense annotation task, and two new methods for mapping between sense inventories using parallel graded sense annotations.", "labels": [], "entities": []}, {"text": "We demonstrate that sense induction offers significant promise for accurate graded sense annotation.", "labels": [], "entities": [{"text": "sense induction", "start_pos": 20, "end_pos": 35, "type": "TASK", "confidence": 0.8404470384120941}]}], "introductionContent": [{"text": "Word Sense Disambiguation (WSD) aims to identify the sense of a word in a given context, using a predefined sense inventory containing the word's different meanings.", "labels": [], "entities": [{"text": "Word Sense Disambiguation (WSD)", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.7833953698476156}]}, {"text": "Traditionally, WSD approaches have assumed that each occurrence of a word is best labeled with a single sense.", "labels": [], "entities": [{"text": "WSD", "start_pos": 15, "end_pos": 18, "type": "TASK", "confidence": 0.9848065972328186}]}, {"text": "However, human annotators often disagree about which sense is present (, especially in cases where some of the possible senses are closely related (.", "labels": [], "entities": []}, {"text": "Recently,  have shown that in cases of sense ambiguity, a graded notion of sense labeling maybe most appropriate and help reduce the ambiguity.", "labels": [], "entities": [{"text": "sense labeling", "start_pos": 75, "end_pos": 89, "type": "TASK", "confidence": 0.6848248094320297}]}, {"text": "Specifically, within a given context, multiple senses of a word maybe salient to the reader, with different levels of applicability.", "labels": [], "entities": []}, {"text": "For example, in the sentence \u2022 The athlete won the gold metal due to her hard work and dedication.", "labels": [], "entities": []}, {"text": "multiple senses could be considered applicable for \"won\" according to the WordNet 3.0 sense inventory): 1.", "labels": [], "entities": [{"text": "WordNet 3.0 sense inventory", "start_pos": 74, "end_pos": 101, "type": "DATASET", "confidence": 0.8927911818027496}]}, {"text": "win (be the winner in a contest or competition; be victorious) 2.", "labels": [], "entities": []}, {"text": "acquire, win, gain (win something through one's efforts) 3.", "labels": [], "entities": []}, {"text": "gain, advance, win, pull ahead, make headway, get ahead, gain ground (obtain advantages, such as points, etc.)", "labels": [], "entities": []}, {"text": "4. succeed, win, come through, bring home the bacon, deliver the goods (attain successor reach a desired goal) In this context, many annotators would agree that the athlete has both won an object (the gold metal itself) and won a competition (signified by the gold medal).", "labels": [], "entities": []}, {"text": "Although contexts can be constructed to elicit only one of these senses, in the example above, a graded annotation best matches human perception.", "labels": [], "entities": []}, {"text": "Graded word sense (GWS) annotation offers significant advantages for sense annotation with a finegrained sense inventory.", "labels": [], "entities": [{"text": "Graded word sense (GWS) annotation", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.68556490114757}]}, {"text": "However, creating a sufficiently large annotated corpus for training supervised GWS disambiguation models presents a significant challenge, i.e., the laborious task of gathering annotations for all combinations of a word's senses, along with variation in those senses applicabilities.", "labels": [], "entities": []}, {"text": "To our knowledge,  have provided the only data set with GWS annotations for 11 terms.", "labels": [], "entities": [{"text": "GWS", "start_pos": 56, "end_pos": 59, "type": "DATASET", "confidence": 0.7916419506072998}]}, {"text": "Therefore, we consider the use of Word Sense Induction (WSI) for GWS annotation.", "labels": [], "entities": [{"text": "Word Sense Induction (WSI)", "start_pos": 34, "end_pos": 60, "type": "TASK", "confidence": 0.6620602011680603}, {"text": "GWS annotation", "start_pos": 65, "end_pos": 79, "type": "TASK", "confidence": 0.5462879687547684}]}, {"text": "WSI removes the need for substantial training data by automatically deriving a word's senses and associated sense features through examining its contextual uses.", "labels": [], "entities": [{"text": "WSI", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.5618815422058105}]}, {"text": "Furthermore, the data-driven sense discovery defines senses as they are present in the corpus, which may identify usages not present in traditional sense inventories ().", "labels": [], "entities": []}, {"text": "Last, many WSI models represent senses loosely as abstractions over usages, which potentially may transfer well to expressing GWS annotations as a blend of their sense usages.", "labels": [], "entities": []}, {"text": "In this paper, we consider the performance of WSI models on a GWS task.", "labels": [], "entities": [{"text": "WSI", "start_pos": 46, "end_pos": 49, "type": "TASK", "confidence": 0.9549272656440735}]}, {"text": "The contributions of this paper are as follows.", "labels": [], "entities": []}, {"text": "2, we motivate three GWS annotation objectives and propose corresponding measures that provide fine-grained analysis of the capabilities of different WSI models.", "labels": [], "entities": [{"text": "WSI", "start_pos": 150, "end_pos": 153, "type": "TASK", "confidence": 0.9317426681518555}]}, {"text": "4, we propose two new sense mapping procedures for converting an induced sense inventory to a reference sense inventory when GWS annotations are present, and demonstrate significant performance improvement using these procedures on GWS annotation.", "labels": [], "entities": [{"text": "sense mapping", "start_pos": 22, "end_pos": 35, "type": "TASK", "confidence": 0.7194738835096359}]}, {"text": "5, we demonstrate a complete evaluation framework using three graph-based WSI models as examples, generating several insights for how to better evaluate GWS disambiguation systems.", "labels": [], "entities": []}], "datasetContent": [{"text": "Directly comparing GWS annotations from the induced and gold standard sense inventories requires first creating a mapping from the induced senses to the gold standard inventory.", "labels": [], "entities": []}, {"text": "propose a sense-mapping procedure, which was used in the previous two SemEval WSI Tasks ().", "labels": [], "entities": [{"text": "SemEval WSI Tasks", "start_pos": 70, "end_pos": 87, "type": "TASK", "confidence": 0.6525157888730367}]}, {"text": "We consider this procedure and two extensions of it to support learning a mapping from graded sense annotations.", "labels": [], "entities": []}, {"text": "The procedure of Agirre et al.", "labels": [], "entities": []}, {"text": "(2006) uses three corpora: (1) abase corpus from which the senses are derived, (2) a mapping corpus annotated with both gold standard senses, denoted gs, and induced senses, denoted is, and (3) a test corpus annotated with is senses that will be converted togs senses.", "labels": [], "entities": []}, {"text": "Once the senses are induced from the base corpus, the mapping corpus is annotated with is senses and a matrix M is built where cell i, j initially contains the counts of each time gs j and is i were used to label the same instance.", "labels": [], "entities": []}, {"text": "The rows of this matrix are then normalized such that each cell now represents p(gs j |is i ).", "labels": [], "entities": []}, {"text": "The final mapping selects the most probable gs sense for each is sense.", "labels": [], "entities": []}, {"text": "To label the test corpus, each instance that is labeled with is i is relabeled with the gs sense with the highest conditional probability given is i . When a context c is annotated by a set of labels L = {is i , . .", "labels": [], "entities": []}, {"text": ", is j }, the final sense labeling contains the set of all gs to which the is senses were mapped, weighted by their mapping frequencies:  We adapt the supervised WSD setting used in prior SemEval WSI Tasks ( to evaluation the models according to the three proposed objectives.", "labels": [], "entities": [{"text": "SemEval WSI Tasks", "start_pos": 188, "end_pos": 205, "type": "TASK", "confidence": 0.8262467583020529}]}, {"text": "In the supervised setting, WSI systems provide GWS annotation of their induced senses for the test corpus, which is already labeled with the gold-standard GWS annotations.", "labels": [], "entities": []}, {"text": "Then, a portion of the test corpus with gold standard annotations is used to build a mapping from induced senses to the reference sense inven-: The terms from the GWS dataset  used in this evaluation tory using one of the three algorithms described in Section 4.", "labels": [], "entities": [{"text": "GWS dataset", "start_pos": 163, "end_pos": 174, "type": "DATASET", "confidence": 0.9772416353225708}]}, {"text": "The remaining, held-out test corpus instances have their induced senses converted to the gold standard sense inventory and the sense labelings are evaluated for the three objectives from Section 2.", "labels": [], "entities": []}, {"text": "In our experiments we divide the reference corpus into five evenly-sized segments and then use four segments (80% of the test corpus) for constructing the mapping and then evaluate the converted GWS annotations of the remaining segment.", "labels": [], "entities": [{"text": "GWS", "start_pos": 195, "end_pos": 198, "type": "DATASET", "confidence": 0.820469856262207}]}], "tableCaptions": [{"text": " Table 1: The terms from the GWS dataset (", "labels": [], "entities": [{"text": "GWS dataset", "start_pos": 29, "end_pos": 40, "type": "DATASET", "confidence": 0.9806722402572632}]}, {"text": " Table 2: Average performance of the three WSI models according to Detection, Ranking, and Percetion", "labels": [], "entities": [{"text": "Detection", "start_pos": 67, "end_pos": 76, "type": "METRIC", "confidence": 0.9437317848205566}, {"text": "Percetion", "start_pos": 91, "end_pos": 100, "type": "METRIC", "confidence": 0.9684377908706665}]}, {"text": " Table 3: Average performance of the six baselines", "labels": [], "entities": [{"text": "Average", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9869311451911926}]}]}