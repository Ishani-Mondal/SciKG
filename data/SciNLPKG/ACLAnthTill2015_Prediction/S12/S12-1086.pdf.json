{"title": [{"text": "Weiwei: A Simple Unsupervised Latent Semantics based Approach for Sentence Similarity", "labels": [], "entities": [{"text": "Sentence Similarity", "start_pos": 66, "end_pos": 85, "type": "TASK", "confidence": 0.8990858793258667}]}], "abstractContent": [{"text": "The Semantic Textual Similarity (STS) shared task (Agirre et al., 2012) computes the degree of semantic equivalence between two sentences.", "labels": [], "entities": [{"text": "Semantic Textual Similarity (STS) shared", "start_pos": 4, "end_pos": 44, "type": "TASK", "confidence": 0.7724764347076416}]}, {"text": "1 We show that a simple unsupervised latent semantics based approach, Weighted Textual Matrix Factorization that only exploits bag-of-words features, can outperform most systems for this task.", "labels": [], "entities": []}, {"text": "The key to the approach is to carefully handle missing words that are not in the sentence, and thus rendering it superior to Latent Semantic Analysis (LSA) and Latent Dirichlet Allocation (LDA).", "labels": [], "entities": []}, {"text": "Our system ranks 20 out of 89 systems according to the official evaluation metric for the task, Pear-son correlation, and it ranks 10/89 and 19/89 in the other two evaluation metrics employed by the organizers.", "labels": [], "entities": [{"text": "Pear-son correlation", "start_pos": 96, "end_pos": 116, "type": "METRIC", "confidence": 0.8048439919948578}]}], "introductionContent": [{"text": "Identifying the degree of semantic similarity between two sentences is helpful for many NLP topics.", "labels": [], "entities": []}, {"text": "In Machine Translation) and Text Summarization (), results are automatically evaluated based on sentence comparison.", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 3, "end_pos": 22, "type": "TASK", "confidence": 0.8169618844985962}, {"text": "Text Summarization", "start_pos": 28, "end_pos": 46, "type": "TASK", "confidence": 0.805098831653595}]}, {"text": "In Text Coherence Detection (Lapata and), sentences are linked together by similar or related words.", "labels": [], "entities": [{"text": "Text Coherence Detection", "start_pos": 3, "end_pos": 27, "type": "TASK", "confidence": 0.6711800197760264}]}, {"text": "For Word Sense Disambiguation, researchers) construct a sense similarity measure from the sentence similarity of the sense definitions.", "labels": [], "entities": [{"text": "Word Sense Disambiguation", "start_pos": 4, "end_pos": 29, "type": "TASK", "confidence": 0.7540876865386963}]}, {"text": "Almost all SS approaches decompose the task into word pairwise similarity problems.", "labels": [], "entities": []}, {"text": "For example, create a matrix for each sentence pair, where columns are the words in the first sentence and rows are the words in the second sentence, and each cell stores the distributional similarity of the two words.", "labels": [], "entities": []}, {"text": "Then they create an alignment between words in two sentences, and sentence similarity is calculated based on the sum of the similarity of aligned word pairs.", "labels": [], "entities": []}, {"text": "There are two disadvantages with word similarity based approaches: 1.", "labels": [], "entities": []}, {"text": "lexical ambiguity as the word pairwise similarity ignores the semantic interaction between the word and sentence/context.", "labels": [], "entities": []}, {"text": "2. word co-occurrence information is not as sufficiently exploited as they are in latent variable models such as Latent Semantic Analysis (LSA) () and Latent Dirichilet Allocation (LDA) ().", "labels": [], "entities": [{"text": "Latent Semantic Analysis (LSA)", "start_pos": 113, "end_pos": 143, "type": "TASK", "confidence": 0.7344282666842142}]}, {"text": "On the other hand, latent variable models can solve the two issues naturally by modeling the semantics of words and sentences simultaneously in the low-dimensional latent space.", "labels": [], "entities": []}, {"text": "However, attempts at addressing SS using LSA perform significantly below word similarity based models (.", "labels": [], "entities": [{"text": "addressing SS", "start_pos": 21, "end_pos": 34, "type": "TASK", "confidence": 0.8210433423519135}]}, {"text": "We believe the reason is that the observed words in a sentence are too few for latent variable models to learn robust semantics.", "labels": [], "entities": []}, {"text": "For example, given the two sentences of WordNet sense definitions for bank#n#1 and stock#n#1: bank#n#1: a financial institution that accepts deposits and channels the money into lending activities stock#n#1: the capital raised by a corporation through the issue of shares entitling holders to an ownership interest (equity) LDA can only find the dominant topic (the financial topic) based on the observed words without further discernibility.", "labels": [], "entities": []}, {"text": "In this case, many sen-tences will share the same latent semantics profile, as long as they are in the same topic/domain.", "labels": [], "entities": []}, {"text": "In our work (, we propose to model the missing words (words that are not in the sentence) to address the sparseness issue for the SS task.", "labels": [], "entities": [{"text": "SS task", "start_pos": 130, "end_pos": 137, "type": "TASK", "confidence": 0.8707419335842133}]}, {"text": "Our intuition is since observed words in a sentence are too few to tell us what the sentence is about, missing words can be used to tell us what the sentence is not about.", "labels": [], "entities": []}, {"text": "We assume that the semantic space of both the observed and missing words makeup the complete semantic profile of a sentence.", "labels": [], "entities": []}, {"text": "We implement our idea using a weighted matrix factorization approach, which allows us to treat observed words and missing words differently.", "labels": [], "entities": []}, {"text": "It should be noted that our approach is very general (similar to LSA/LDA) in that it can be applied to any genre of short texts, in a manner different from existing work that models short texts by using additional data, e.g., model tweets using their metadata (author, hashtag, etc).", "labels": [], "entities": []}, {"text": "Also we do not extract additional features such as multiwords expression or syntax from sentences -all we use is bag-of-words feature.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Performance of LDA and WTMF on each individual test set of Task 6 STS data", "labels": [], "entities": [{"text": "WTMF", "start_pos": 33, "end_pos": 37, "type": "METRIC", "confidence": 0.663171648979187}]}]}