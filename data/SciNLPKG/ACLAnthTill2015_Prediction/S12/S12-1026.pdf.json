{"title": [{"text": "Unsupervised Induction of a Syntax-Semantics Lexicon Using Iterative Refinement", "labels": [], "entities": []}], "abstractContent": [{"text": "We present a method for learning syntax-semantics mappings for verbs from unanno-tated corpora.", "labels": [], "entities": []}, {"text": "We learn linkings, i.e., map-pings from the syntactic arguments and adjuncts of a verb to its semantic roles.", "labels": [], "entities": []}, {"text": "By learning such linkings, we do not need to model individual semantic roles independently of one another, and we can exploit the relation between different mappings for the same verb, or between mappings for different verbs.", "labels": [], "entities": []}, {"text": "We present an evaluation on a standard test set for semantic role labeling.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 52, "end_pos": 74, "type": "TASK", "confidence": 0.7244307001431783}]}], "introductionContent": [{"text": "A verb can have several ways of mapping its semantic arguments to syntax (\"diathesis alternations\"): (1) a.", "labels": [], "entities": []}, {"text": "We increased the response rate with SHK. b. SHK increased the response rate. c. The response rate increased.", "labels": [], "entities": []}, {"text": "The subject of increase can be the agent (1a), the instrument (1b), or the theme (what is being increased) (1c).", "labels": [], "entities": []}, {"text": "Other verbs that show this pattern include break or melt.", "labels": [], "entities": [{"text": "break", "start_pos": 43, "end_pos": 48, "type": "METRIC", "confidence": 0.9589117765426636}, {"text": "melt", "start_pos": 52, "end_pos": 56, "type": "METRIC", "confidence": 0.6641435623168945}]}, {"text": "Much theoretical and lexicographic (descriptive) work has been devoted to determining how verbs map their lexical predicate-argument structure to syntactic arguments.", "labels": [], "entities": []}, {"text": "The last decades have seen a surge in activity on the computational front, spurred in part by efforts to annotate large corpora for lexical semantics ().", "labels": [], "entities": []}, {"text": "Initially, we have seen computational efforts devoted to finding classes of verbs that share similar syntax-semantics mappings from annotated and unannotated corpora).", "labels": [], "entities": []}, {"text": "More recently, there has been an explosion of interest in semantic role labeling (with too many recent publications to cite).", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 58, "end_pos": 80, "type": "TASK", "confidence": 0.7865248719851176}]}, {"text": "In this paper, we explore learning syntaxsemantics mappings for verbs from unannotated corpora.", "labels": [], "entities": []}, {"text": "We are specifically interested in learning linkings.", "labels": [], "entities": []}, {"text": "A linking is a mapping for one verb from its syntactic arguments and adjuncts to all of its semantic roles, so that individual semantic roles are not modeled independently of one another and so that we can exploit the relation between different mappings for the same verb (as in (1) above), or between mappings for different verbs.", "labels": [], "entities": []}, {"text": "We therefore follow in treating linkings as first-class objects; however, we differ from their work in two important respects.", "labels": [], "entities": []}, {"text": "First, we use semantic clustering of head words of arguments in an approach that resembles topic modeling, rather than directly modeling the subcategorization of verbs with a distribution over words.", "labels": [], "entities": [{"text": "semantic clustering of head words of arguments", "start_pos": 14, "end_pos": 60, "type": "TASK", "confidence": 0.8306704163551331}]}, {"text": "Second and most importantly, we do not make any assumptions about the linkings, as do.", "labels": [], "entities": []}, {"text": "They list a small set of rules from which they derive all linkings possible in their model; in contrast, we are able to learn any linking observed in the data.", "labels": [], "entities": []}, {"text": "Therefore, our approach is languageindependent.", "labels": [], "entities": []}, {"text": "claim that their rules represent \"a weak form of Universal Grammar\", but their rules lack such common linking operations as the addition of an accusative reflexive for the unaccusative (Romance) or case marking (many languages), and they include a specific (English) preposition.", "labels": [], "entities": []}, {"text": "We have no objection to using linguistic knowledge, but we do not feel that we have the empirical basis as of now to provide a set of Universal Grammar rules relevant for our task.", "labels": [], "entities": []}, {"text": "A complete syntax-semantics lexicon describes how lexemes syntactically realize their semantic arguments, and provides selectional preferences on these dependents.", "labels": [], "entities": []}, {"text": "Though rich lexical resources exist (such as the PropBank rolesets, the FrameNet lexicon, or VerbNet, which relates and extends these sources), none of them is complete, not even for English, on which most of the efforts have focused.", "labels": [], "entities": []}, {"text": "However, if a complete syntax-semantics lexicon did exist, it would bean extremely useful resource: the task of shallow semantic parsing (semantic argument detection and semantic role labeling) could be reduced to determining the best analysis according to this lexicon.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 120, "end_pos": 136, "type": "TASK", "confidence": 0.7704806625843048}, {"text": "semantic argument detection", "start_pos": 138, "end_pos": 165, "type": "TASK", "confidence": 0.6466944813728333}, {"text": "semantic role labeling", "start_pos": 170, "end_pos": 192, "type": "TASK", "confidence": 0.619330624739329}]}, {"text": "In fact, the learning model we present in this paper is itself a semantic role labeling model, since we can simply apply it to the data we want to label semantically.", "labels": [], "entities": []}, {"text": "This paper is a step towards the unsupervised induction of a complete syntax-semantics lexicon.", "labels": [], "entities": []}, {"text": "We present a unified procedure for associating verbs with linkings and for associating the discovered semantic roles with selectional preferences.", "labels": [], "entities": []}, {"text": "As input, we assume a syntactic representation scheme and a parser which can produce syntactic representations of unseen sentences in the chosen scheme reasonably well, as well as unlabeled text.", "labels": [], "entities": []}, {"text": "We do not assume a specific theory of lexical semantics, nor a specific set of semantic roles.", "labels": [], "entities": []}, {"text": "We induce a set of linkings, which are mappings from semantic role symbols to syntactic functions.", "labels": [], "entities": []}, {"text": "We also induce a lexicon, which associates a verb lemma with a distribution over the linkings, and which associates the sematic role symbols with verb-specific selectional preferences (which are distributions over distributions of words).", "labels": [], "entities": []}, {"text": "We evaluate on the task of semantic role labeling using PropBank () as a gold standard.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 27, "end_pos": 49, "type": "TASK", "confidence": 0.7076061566670736}, {"text": "PropBank", "start_pos": 56, "end_pos": 64, "type": "DATASET", "confidence": 0.9285043478012085}]}, {"text": "We focus on semantic arguments, as they are defined specifically for each verb and thus have verbspecific mappings to syntactic arguments, which may further be subject to diathesis alternations.", "labels": [], "entities": []}, {"text": "In contrast, semantic adjuncts (modifiers) apply (in principle) to all verbs, and do not participate in diathesis alternations.", "labels": [], "entities": []}, {"text": "For this reason, the PropBank lexicon includes arguments but not adjuncts in its framesets.", "labels": [], "entities": [{"text": "PropBank lexicon", "start_pos": 21, "end_pos": 37, "type": "DATASET", "confidence": 0.9592811763286591}]}, {"text": "The method we present in this paper is designed to find verb-specific arguments, and we therefore take the results on semantic arguments (Argn) as our primary result.", "labels": [], "entities": [{"text": "Argn)", "start_pos": 138, "end_pos": 143, "type": "METRIC", "confidence": 0.9677045047283173}]}, {"text": "On these, we achieve a 20% F-measure error reduction over a high syntactic baseline (which maps each syntactic relation to a single semantic argument).", "labels": [], "entities": [{"text": "F-measure error reduction", "start_pos": 27, "end_pos": 52, "type": "METRIC", "confidence": 0.9385004043579102}]}], "datasetContent": [{"text": "We train and evaluate our linking model on the data set produced for the CoNLL-08 Shared Task on Joint Parsing of Syntactic and Semantic Dependencies (, which is based on the PropBank corpus ().", "labels": [], "entities": [{"text": "CoNLL-08 Shared Task on Joint Parsing of Syntactic and Semantic Dependencies", "start_pos": 73, "end_pos": 149, "type": "TASK", "confidence": 0.733061365105889}, {"text": "PropBank corpus", "start_pos": 175, "end_pos": 190, "type": "DATASET", "confidence": 0.9626640975475311}]}, {"text": "This data set includes part-of-speech tags, lemmatized tokens, and syntactic dependencies, which have been converted from the manual syntactic annotation of the underlying Penn Treebank ().", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 172, "end_pos": 185, "type": "DATASET", "confidence": 0.9945355355739594}]}, {"text": "As explained above, our model does not predict specific role labels, such as those annotated in PropBank, but rather aims at clustering like argument instances together.", "labels": [], "entities": [{"text": "PropBank", "start_pos": 96, "end_pos": 104, "type": "DATASET", "confidence": 0.9240756630897522}]}, {"text": "Since the (numbered) labels of these clusters are arbitrary, we cannot evaluate the predictions of our model against the PropBank gold annotation directly.", "labels": [], "entities": [{"text": "PropBank gold annotation", "start_pos": 121, "end_pos": 145, "type": "DATASET", "confidence": 0.9575097759564718}]}, {"text": "We follow in measuring the quality of our clustering in terms of cluster purity and collocation instead.", "labels": [], "entities": []}, {"text": "Cluster purity is a measure of the degree to which the predicted clusters meet the goal of containing only instances with the same gold standard class label.", "labels": [], "entities": []}, {"text": "Given predicted clusters C 1 , . .", "labels": [], "entities": []}, {"text": ", C n C and gold clusters G 1 , . .", "labels": [], "entities": []}, {"text": ", G n G over a set of n argument instances, it is defined as Similarly, cluster collocation measures how well the clustering meets the goal of clustering all gold instances with the same label into a single predicted cluster, formally: We determine purity and collocation separately for each predicate type and then compute their microaverage, i.e., weighting each score by the number of argument instances of this precidate.", "labels": [], "entities": []}, {"text": "Just as precision and recall, purity and collocation stand in tradeoff.", "labels": [], "entities": [{"text": "precision", "start_pos": 8, "end_pos": 17, "type": "METRIC", "confidence": 0.9987321496009827}, {"text": "recall", "start_pos": 22, "end_pos": 28, "type": "METRIC", "confidence": 0.9982048273086548}, {"text": "purity", "start_pos": 30, "end_pos": 36, "type": "METRIC", "confidence": 0.997805655002594}]}, {"text": "In the next section, we therefore report their F 1 score, i.e., their harmonic mean 2\u00b7P u\u00b7Co P u+Co .", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 47, "end_pos": 56, "type": "METRIC", "confidence": 0.9805930654207865}, {"text": "harmonic mean 2\u00b7P u\u00b7Co P u+Co", "start_pos": 70, "end_pos": 99, "type": "METRIC", "confidence": 0.8579933345317841}]}], "tableCaptions": [{"text": " Table 1: Purity (Pu), collocation (Co), and F 1 scores of our model and the syntactic baseline in percent. Performance  on arguments (Argn), adjuncts (ArgM), and overall results (Arg*) are shown separately.", "labels": [], "entities": [{"text": "F 1", "start_pos": 45, "end_pos": 48, "type": "METRIC", "confidence": 0.9921734631061554}, {"text": "Argn), adjuncts (ArgM)", "start_pos": 135, "end_pos": 157, "type": "METRIC", "confidence": 0.6773778327873775}, {"text": "Arg", "start_pos": 180, "end_pos": 183, "type": "METRIC", "confidence": 0.993278443813324}]}]}