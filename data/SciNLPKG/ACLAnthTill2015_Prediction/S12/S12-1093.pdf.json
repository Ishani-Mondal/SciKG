{"title": [{"text": "BUAP: Three Approaches for Semantic Textual Similarity", "labels": [], "entities": [{"text": "BUAP", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.597001314163208}, {"text": "Semantic Textual Similarity", "start_pos": 27, "end_pos": 54, "type": "TASK", "confidence": 0.5739037791887919}]}], "abstractContent": [{"text": "In this paper we describe the three approaches we submitted to the Semantic Textual Similarity task of SemEval 2012.", "labels": [], "entities": [{"text": "Semantic Textual Similarity task of SemEval 2012", "start_pos": 67, "end_pos": 115, "type": "TASK", "confidence": 0.8484255756650653}]}, {"text": "The first approach considers to calculate the semantic similarity by using the Jaccard coefficient with term expansion using synonyms.", "labels": [], "entities": []}, {"text": "The second approach uses the semantic similarity reported by Mihalcea in (Mihalcea et al., 2006).", "labels": [], "entities": []}, {"text": "The third approach employs Random Indexing and Bag of Concepts based on context vectors.", "labels": [], "entities": [{"text": "Random Indexing", "start_pos": 27, "end_pos": 42, "type": "TASK", "confidence": 0.5849842131137848}]}, {"text": "We consider that the first and third approaches obtained a comparable performance, meanwhile the second approach got a very poor behavior.", "labels": [], "entities": []}, {"text": "The best ALL result was obtained with the third approach, with a Pearson correlation equal to 0.663.", "labels": [], "entities": [{"text": "ALL", "start_pos": 9, "end_pos": 12, "type": "METRIC", "confidence": 0.6428787112236023}, {"text": "Pearson correlation", "start_pos": 65, "end_pos": 84, "type": "METRIC", "confidence": 0.936920553445816}]}], "introductionContent": [{"text": "Finding the semantic similarity between two sentences is very important in applications of natural language processing such as information retrieval and related areas.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 127, "end_pos": 148, "type": "TASK", "confidence": 0.7796017229557037}]}, {"text": "The problem is complex due to the small number of terms involved in sentences which are tipically less than 10 or 15.", "labels": [], "entities": []}, {"text": "Additionally, it is required to \"understand\" the meaning of the sentences in order to determine the \"semantic\" similarity of texts, which is quite different of finding the lexical similarity.", "labels": [], "entities": []}, {"text": "There exist different works at literature dealing with semantic similarity, but the problem is far to be solved because of the aforementioned issues.", "labels": [], "entities": [{"text": "semantic similarity", "start_pos": 55, "end_pos": 74, "type": "TASK", "confidence": 0.7502571046352386}]}, {"text": "In (), for instance, it is presented a method for measuring the semantic similarity of texts, using corpus-based and knowledgebased measures of similarity.", "labels": [], "entities": []}, {"text": "The approaches presented in) are based on the Vector Space Model, with the aim to capture the contextual behavior, senses and correlation, of terms.", "labels": [], "entities": []}, {"text": "The performance of the method is better than the baseline method that uses vector based cosine similarity measure.", "labels": [], "entities": []}, {"text": "In this paper, we present three different approaches for the Textual Semantic Similarity task of Semeval 2012).", "labels": [], "entities": [{"text": "Textual Semantic Similarity task", "start_pos": 61, "end_pos": 93, "type": "TASK", "confidence": 0.7619543597102165}]}, {"text": "The task is described as follows: Given two sentences s 1 and s 2 , the aim is to compute how similar s 1 and s 2 are, returning a similarity score, and an optional confidence score.", "labels": [], "entities": [{"text": "similarity score", "start_pos": 131, "end_pos": 147, "type": "METRIC", "confidence": 0.9692312180995941}, {"text": "confidence score", "start_pos": 165, "end_pos": 181, "type": "METRIC", "confidence": 0.8893037140369415}]}, {"text": "The approaches should provide values between 0 and 5 for each pair of sentences.", "labels": [], "entities": []}, {"text": "These values roughly correspond to the following considerations, even when the system should output real values: 5: The two sentences are completely equivalent, as they mean the same thing.", "labels": [], "entities": []}, {"text": "4: The two sentences are mostly equivalent, but some unimportant details differ.", "labels": [], "entities": []}, {"text": "3: The two sentences are roughly equivalent, but some important information differs/missing.", "labels": [], "entities": []}, {"text": "2: The two sentences are not equivalent, but share some details.", "labels": [], "entities": []}, {"text": "1: The two sentences are not equivalent, but are on the same topic.", "labels": [], "entities": []}, {"text": "0: The two sentences are on different topics.", "labels": [], "entities": []}, {"text": "The description of the runs submitted to the competition follows.", "labels": [], "entities": []}], "datasetContent": [{"text": "The three runs submitted to the competition use completely different mechanisms to find the degree of semantic similarity between two sentences.", "labels": [], "entities": []}, {"text": "The approaches are described as follows:  In we show the results obtained by the three approaches submitted to the competition.", "labels": [], "entities": []}, {"text": "The columns of stand for: \u2022 ALL: Pearson correlation with the gold standard for the five datasets, and corresponding rank.", "labels": [], "entities": [{"text": "ALL", "start_pos": 28, "end_pos": 31, "type": "METRIC", "confidence": 0.9990837574005127}, {"text": "Pearson", "start_pos": 33, "end_pos": 40, "type": "METRIC", "confidence": 0.9793475270271301}, {"text": "correlation", "start_pos": 41, "end_pos": 52, "type": "METRIC", "confidence": 0.5265389084815979}]}, {"text": "\u2022 ALLnrm: Pearson correlation after the system outputs for each dataset are fitted to the gold standard using least squares, and corresponding rank.", "labels": [], "entities": [{"text": "ALLnrm", "start_pos": 2, "end_pos": 8, "type": "METRIC", "confidence": 0.995549201965332}, {"text": "Pearson correlation", "start_pos": 10, "end_pos": 29, "type": "METRIC", "confidence": 0.8174958825111389}]}, {"text": "\u2022 Mean: Weighted mean across the 5 datasets, where the weight depends on the number of pairs in the dataset.", "labels": [], "entities": [{"text": "Mean", "start_pos": 2, "end_pos": 6, "type": "METRIC", "confidence": 0.9986157417297363}]}], "tableCaptions": [{"text": " Table 1: Results of approaches of BUAP in Task 6.", "labels": [], "entities": [{"text": "BUAP", "start_pos": 35, "end_pos": 39, "type": "DATASET", "confidence": 0.4318334460258484}]}]}