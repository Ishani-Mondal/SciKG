{"title": [{"text": "Soft Cardinality: A Parameterized Similarity Function for Text Comparison", "labels": [], "entities": [{"text": "Soft Cardinality", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.6912945061922073}]}], "abstractContent": [{"text": "We present an approach for the construction of text similarity functions using a parameterized resemblance coefficient in combination with a softened cardinality function called soft cardinality.", "labels": [], "entities": []}, {"text": "Our approach provides a consistent and recursive model, varying levels of granularity from sentences to characters.", "labels": [], "entities": []}, {"text": "Therefore, our model was used to compare sentences divided into words, and in turn, words divided into q-grams of characters.", "labels": [], "entities": []}, {"text": "Experimentally, we observed that a performance correlation function in a space defined by all parameters was relatively smooth and had a single maximum achievable by \"hill climbing.\"", "labels": [], "entities": []}, {"text": "Our approach used only surface text information, a stop-word remover, and a stemmer to tackle the semantic text similarity task 6 at SEMEVAL 2012.", "labels": [], "entities": [{"text": "semantic text similarity task 6 at SEMEVAL 2012", "start_pos": 98, "end_pos": 145, "type": "TASK", "confidence": 0.7036307193338871}]}, {"text": "The proposed method ranked 3rd (average), 5th (normalized correlation), and 15th (aggregated correlation) among 89 systems submitted by 31 teams.", "labels": [], "entities": [{"text": "aggregated correlation)", "start_pos": 82, "end_pos": 105, "type": "METRIC", "confidence": 0.8628973563512167}]}], "introductionContent": [{"text": "Similarity is the intrinsic ability of humans and some animals to balance commonalities and differences when comparing objects that are not identical.", "labels": [], "entities": []}, {"text": "Although there is no direct evidence of how this process works in living organisms, some models have been proposed from the cognitive perspective).", "labels": [], "entities": []}, {"text": "On the other hand, several similarity models have been proposed in mathematics, statistics, and computer science among other fields.", "labels": [], "entities": []}, {"text": "Particularly in AI, similarity measures play an important role in the construction of intelligent systems that are required to exhibit behavior similar to humans.", "labels": [], "entities": []}, {"text": "For instance, in the field of natural language processing, text similarity functions provide estimates of the human similarity judgments related to language.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 30, "end_pos": 57, "type": "TASK", "confidence": 0.6935531298319498}]}, {"text": "In this paper, we combine elements from the perspective of cognitive psychology and computer science to propose a model for building similarity functions suitable for the task of semantic text similarity.", "labels": [], "entities": [{"text": "semantic text similarity", "start_pos": 179, "end_pos": 203, "type": "TASK", "confidence": 0.6582652032375336}]}, {"text": "We identify four main families of text similarity functions: i) resemblance coefficients based on sets (e.g. and coefficients) ii) functions in metric spaces (e.g. cosine tf-idf similarity (); iii) the edit distance family of measures (e.g. distance, LCS); and iv) hybrid approaches ().", "labels": [], "entities": []}, {"text": "All of these measures use a subdivision of the texts in different granularity levels, such as q-grams of words, words, q-grams of characters, syllables, and characters.", "labels": [], "entities": []}, {"text": "Among hybrid approaches, Monge-Elkan's measure and soft cardinality methods are recursive and can be used to build similarity functions at any arbitrary range of granularity.", "labels": [], "entities": []}, {"text": "For instance, it is possible to construct a similarity function to compare sentences based on a function that compares words, which in turn can be constructed based on a function that compares bigrams of characters.", "labels": [], "entities": []}, {"text": "Furthermore, hybrid approaches can integrate similarity functions that are not based on the representation of the surface of text, such as semantic relatedness measures ().", "labels": [], "entities": []}, {"text": "Text similarity measures can be static or adaptive whether they are binary functions using only surface information of the two texts, or are functions that suit to a wider set of texts.", "labels": [], "entities": []}, {"text": "For instance, measures using tf-idf weights adapt their results to the set of texts in which those weights were obtained.", "labels": [], "entities": []}, {"text": "Other approaches learn parameters of the similarity function from a set of texts to optimize a particular task.", "labels": [], "entities": []}, {"text": "For instance, and learned the costs of edit operations for all characters for an edit-distance function in a name-matching task.", "labels": [], "entities": []}, {"text": "Other machine-learning approaches have also been proposed to build adaptive measures in name-matching and textual-entailment tasks.", "labels": [], "entities": []}, {"text": "However, those machine-learning-based methods for adaptive similarity suffer from sparseness and the \"curse of dimensionality\".", "labels": [], "entities": []}, {"text": "For example, the method of Ristad and Yianilos learns n 2 + 2n parameters, where n is the size of the character set.", "labels": [], "entities": []}, {"text": "Similarly, dimensionality in the method of Bilenko and Mooney is the size of the data set vocabulary.", "labels": [], "entities": []}, {"text": "This issue is addressed primarily through machine-learning algorithms, which reduce the dimensionality of the problem regularizating to achieve enough generalization to get an acceptable performance difference between training and test data.", "labels": [], "entities": []}, {"text": "Although machinelearning solutions have proven effective for many applications, the principle of Occam's razor suggests that it should be preferable to have a model that explains the data with a smaller number of significant parameters.", "labels": [], "entities": [{"text": "Occam's razor", "start_pos": 97, "end_pos": 110, "type": "DATASET", "confidence": 0.8612448970476786}]}, {"text": "In this paper, we seek a simpler adaptive similarity model with few meaningful parameters.", "labels": [], "entities": []}, {"text": "Our proposed similarity model starts with a cardinality-based resemblance coefficient (i.e. Dice's coefficient 2|A\u2229B| /|A|+|B|) and generalizes it to model the effect of asymmetric selection of the referent.", "labels": [], "entities": [{"text": "cardinality-based resemblance coefficient", "start_pos": 44, "end_pos": 85, "type": "METRIC", "confidence": 0.6915824015935262}]}, {"text": "This effect is a human factor discovered by that affects judgments of similarity, i.e. humans tends to select the more prominent stimulus as the referent and the less salient stimulus as the object.", "labels": [], "entities": []}, {"text": "Some of Tversky's examples are \"the son resembles the father\" rather than \"the father resembles the son\", \"an ellipse is like a circle\" not \"a circle is like an ellipse\", and \"North Korea is like Red China\" rather than \"Red China is like North Korea\".", "labels": [], "entities": []}, {"text": "Generally speaking, \"the variant is more similar to the prototype than vice versa\".", "labels": [], "entities": []}, {"text": "In the previous example, stimulus salience is associated with the prominence of the country; for text comparison we associate word salience with tf-idf weights.", "labels": [], "entities": []}, {"text": "At the text level, we associate salience with a combination of word-salience, inter-word similarity, and text length provided by soft cardinality.", "labels": [], "entities": []}, {"text": "Experimentally, we observed that this effect also occurs when comparing texts, but not necessarily in the same direction suggested by Tversky.", "labels": [], "entities": []}, {"text": "We used this effect to improve the performance of our similarity model.", "labels": [], "entities": []}, {"text": "In addition, we proposed a parameter that biases the function to generate greater or lower similarity scores.", "labels": [], "entities": [{"text": "similarity scores", "start_pos": 91, "end_pos": 108, "type": "METRIC", "confidence": 0.9513664841651917}]}, {"text": "Finally, in our model we used a soft cardinality function () instead of the classical set cardinality.", "labels": [], "entities": []}, {"text": "Just as classical cardinality counts the number of elements which are not identical in a set, soft cardinality uses an auxiliary inter-element similarity function to make a soft count.", "labels": [], "entities": []}, {"text": "For instance, the soft cardinality of a set with two very similar (but not identical) elements should be areal number closer to 1.0 instead of 2.0.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2 we briefly present soft cardinality.", "labels": [], "entities": []}, {"text": "In Section 3 the proposed parameterized similarity model is presented.", "labels": [], "entities": []}, {"text": "In Section 4 experimental validation is provided using 8 data sets annotated with human similarity judgments from the \"Semantic-Text-Similarity\" task at SEMEVAL-2012.", "labels": [], "entities": []}, {"text": "Finally, a brief discussion is provided in Section 5 and conclusions are presented in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "The aim of these experiments is to observe the behavior of the parameters of our similarity model and verify if the hypothesis that motivated these parameters can be confirmed experimentally.", "labels": [], "entities": []}, {"text": "The experimental data are 8 data sets (3 for training and 5 for test) proposed in the \"Semantic Text Similarity\" task at SEMEVAL-2012.", "labels": [], "entities": [{"text": "Semantic Text Similarity\" task at SEMEVAL-2012", "start_pos": 87, "end_pos": 133, "type": "TASK", "confidence": 0.8024088314601353}]}, {"text": "Each data set consist of a set of pairs of text annotated with humansimilarity judgments on a scale of 0 to 5.", "labels": [], "entities": []}, {"text": "Each similarity judgment is the average of the judgments provided by 5 human judges.", "labels": [], "entities": [{"text": "similarity", "start_pos": 5, "end_pos": 15, "type": "METRIC", "confidence": 0.976984441280365}]}, {"text": "For a comprehensible description of the task see(.", "labels": [], "entities": []}, {"text": "For the experiments, all data sets were pre-processed by converting to lowercase characters, English stopwords removal and stemming using Porter stemmer.", "labels": [], "entities": [{"text": "English stopwords removal", "start_pos": 93, "end_pos": 118, "type": "TASK", "confidence": 0.6034029722213745}]}, {"text": "The performance measure used for all experiments was the Pearson correlation r.", "labels": [], "entities": [{"text": "Pearson correlation r", "start_pos": 57, "end_pos": 78, "type": "METRIC", "confidence": 0.9587214787801107}]}], "tableCaptions": []}