{"title": [{"text": "PolyUCOMP: Combining Semantic Vectors with Skip bigrams for Semantic Textual Similarity", "labels": [], "entities": [{"text": "Semantic Textual Similarity", "start_pos": 60, "end_pos": 87, "type": "TASK", "confidence": 0.6446678042411804}]}], "abstractContent": [{"text": "This paper presents the work of the Hong Kong Polytechnic University (PolyUCOMP) team which has participated in the Semantic Textual Similarity task of SemEval-2012.", "labels": [], "entities": [{"text": "Hong Kong Polytechnic University (PolyUCOMP) team", "start_pos": 36, "end_pos": 85, "type": "DATASET", "confidence": 0.8318237140774727}, {"text": "Semantic Textual Similarity task of SemEval-2012", "start_pos": 116, "end_pos": 164, "type": "TASK", "confidence": 0.7764506340026855}]}, {"text": "The PolyUCOMP system combines semantic vectors with skip bigrams to determine sentence similarity.", "labels": [], "entities": []}, {"text": "The semantic vector is used to compute similarities between sentence pairs using the lexical database WordNet and the Wikipedia corpus.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 102, "end_pos": 109, "type": "DATASET", "confidence": 0.9417964220046997}, {"text": "Wikipedia corpus", "start_pos": 118, "end_pos": 134, "type": "DATASET", "confidence": 0.9236021041870117}]}, {"text": "The use of skip bigram is to introduce the order of words in measuring sentence similarity.", "labels": [], "entities": []}], "introductionContent": [{"text": "Sentence similarity computation plays an important role in text summarization, classification, question answering and social network applications ().", "labels": [], "entities": [{"text": "Sentence similarity computation", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.9106245835622152}, {"text": "text summarization", "start_pos": 59, "end_pos": 77, "type": "TASK", "confidence": 0.7706178426742554}, {"text": "question answering", "start_pos": 95, "end_pos": 113, "type": "TASK", "confidence": 0.8931728303432465}]}, {"text": "The SemEval 2012 competition includes a task targeted at Semantic Textual Similarity (STS) between sentence pairs (.", "labels": [], "entities": [{"text": "SemEval 2012 competition", "start_pos": 4, "end_pos": 28, "type": "TASK", "confidence": 0.871408998966217}, {"text": "Semantic Textual Similarity (STS) between sentence pairs", "start_pos": 57, "end_pos": 113, "type": "TASK", "confidence": 0.7928158409065671}]}, {"text": "Given a set of sentence pairs, participants are required to assign to each sentence pair a similarity score.", "labels": [], "entities": [{"text": "similarity score", "start_pos": 91, "end_pos": 107, "type": "METRIC", "confidence": 0.9662757217884064}]}, {"text": "Because a sentence has only a limited amount of content words, it is not easy to determine sentence similarities because of the sparseness issue.", "labels": [], "entities": []}, {"text": "proposed to use linguistic features as indicators of text similarity to address the problem of sparse representation of sentences.", "labels": [], "entities": []}, {"text": "measured sentence similarity using component words in sentences.", "labels": [], "entities": []}, {"text": "proposed to incorporate the semantic vector and word order to calculate sentence similarity.", "labels": [], "entities": []}, {"text": "In our approach to the STS task, semantic vector is used and the semantic relatedness between words is derived from two sources: WordNet and Wikipedia.", "labels": [], "entities": [{"text": "STS task", "start_pos": 23, "end_pos": 31, "type": "TASK", "confidence": 0.9299090504646301}, {"text": "WordNet", "start_pos": 129, "end_pos": 136, "type": "DATASET", "confidence": 0.9503812193870544}]}, {"text": "Because WordNet is limited in its coverage, Wikipedia is used as a candidate for determining word similarity.", "labels": [], "entities": []}, {"text": "Word order, however, is not considered in semantic vector.", "labels": [], "entities": []}, {"text": "As semantic information are coded in sentences according to its order of writing, and in our systems, content words may not be adjacent to each other, we proposed to use skip bigrams to represent the structure of sentences.", "labels": [], "entities": []}, {"text": "Skip bigrams, generally speaking, are pairs of words in a sentence order with arbitrary gap (.", "labels": [], "entities": []}, {"text": "Different from the previous skip bigram statistics which compare sentence similarities through overlapping skip bigrams (, the skip bigrams we used are weighted by a decaying factor of the skipping gap in a sentence, giving higher scores to closer occurrences of skip bigrams.", "labels": [], "entities": []}, {"text": "It is reasonable to assume that similar sentences should have more overlapping skip bigrams, and the gaps in their shared skip bigrams should also be similar.", "labels": [], "entities": []}, {"text": "The rest of this paper is organized as followed.", "labels": [], "entities": []}, {"text": "Section 2 describes sentence similarity using semantic vectors and the order-sensitive skip bigrams.", "labels": [], "entities": [{"text": "sentence similarity", "start_pos": 20, "end_pos": 39, "type": "TASK", "confidence": 0.7111014127731323}]}, {"text": "Section 3 gives the performance evaluation.", "labels": [], "entities": []}, {"text": "Section 4 is the conclusion.", "labels": [], "entities": []}], "datasetContent": [{"text": "In the STS task, three training datasets are available: MSR-Paraphrase, MSR-Video and SMTeuroparl ().", "labels": [], "entities": [{"text": "STS task", "start_pos": 7, "end_pos": 15, "type": "TASK", "confidence": 0.8595397770404816}]}, {"text": "The number of sentence pairs for three dataset is 750, 750 and 734.", "labels": [], "entities": []}, {"text": "In the following experiments, Let S WN , S WIKI and S SKIP denote similarity measures of the vector space representation using WordNet, Wikipedia and skip bigrams, respectively.", "labels": [], "entities": [{"text": "SKIP", "start_pos": 54, "end_pos": 58, "type": "METRIC", "confidence": 0.8108487129211426}, {"text": "WordNet", "start_pos": 127, "end_pos": 134, "type": "DATASET", "confidence": 0.9710515141487122}]}, {"text": "The three similarity measures are linearly combined as S COMB : where \u03b1 and \u03b2 are weight factors for S WN and S WIKI in the range.", "labels": [], "entities": []}, {"text": "If \u03b1 is set to 1, only the WordNet-based similarity measure is used; if \u03b1 is 0, the Wikipedia and skip bigram measures are used.", "labels": [], "entities": [{"text": "WordNet-based similarity measure", "start_pos": 27, "end_pos": 59, "type": "METRIC", "confidence": 0.7073199152946472}]}, {"text": "Because each dataset has a different representation for sentences, the parameter configurations for them are different.", "labels": [], "entities": []}, {"text": "For the word similarity using the lexical resource WordNet, the path measure is used in experiments.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 51, "end_pos": 58, "type": "DATASET", "confidence": 0.9648346304893494}]}, {"text": "To get word relatedness from the English Wikipedia, the Wikipedia Miner tool 2 is used.", "labels": [], "entities": [{"text": "word relatedness", "start_pos": 7, "end_pos": 23, "type": "TASK", "confidence": 0.7321425080299377}, {"text": "Wikipedia Miner tool 2", "start_pos": 56, "end_pos": 78, "type": "DATASET", "confidence": 0.9352949410676956}]}, {"text": "When computing sentence similarity based on the skip bigrams, the decaying factor (DF) must be specified beforehand.", "labels": [], "entities": [{"text": "computing sentence similarity", "start_pos": 5, "end_pos": 34, "type": "TASK", "confidence": 0.6218660275141398}, {"text": "decaying factor (DF)", "start_pos": 66, "end_pos": 86, "type": "METRIC", "confidence": 0.9440873861312866}]}, {"text": "Hence, parameter configurations for the three datasets are listed in: In the testing phase, five testing dataset are provided.", "labels": [], "entities": []}, {"text": "In addition to three test datasets drawn from the publicly available datasets used in the training phase, two surprise datasets are given.", "labels": [], "entities": []}, {"text": "SMTnews has 399 pairs of sentences and OnWN contains 750 sentence pairs.", "labels": [], "entities": [{"text": "SMTnews", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.8081681132316589}, {"text": "OnWN", "start_pos": 39, "end_pos": 43, "type": "DATASET", "confidence": 0.8953262567520142}]}, {"text": "The parameter configurations for these two surprise datasets are the same as those for the dataset MSR-Paraphrase.", "labels": [], "entities": []}, {"text": "The official scoring is based on Pearson correlation.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 33, "end_pos": 52, "type": "METRIC", "confidence": 0.8490822315216064}]}, {"text": "If the system gives the similarity scores close to the reference answers, the system will attain a high correlation value.", "labels": [], "entities": [{"text": "similarity", "start_pos": 24, "end_pos": 34, "type": "METRIC", "confidence": 0.971123456954956}, {"text": "correlation", "start_pos": 104, "end_pos": 115, "type": "METRIC", "confidence": 0.9893892407417297}]}, {"text": "Besides, three other evaluation metrics (ALL, ALLnrm, Mean) based on the Pearson correlation are used ().", "labels": [], "entities": [{"text": "ALL", "start_pos": 41, "end_pos": 44, "type": "METRIC", "confidence": 0.9183034896850586}, {"text": "ALLnrm", "start_pos": 46, "end_pos": 52, "type": "METRIC", "confidence": 0.8813012838363647}, {"text": "Mean", "start_pos": 54, "end_pos": 58, "type": "METRIC", "confidence": 0.9872013330459595}, {"text": "Pearson correlation", "start_pos": 73, "end_pos": 92, "type": "METRIC", "confidence": 0.9223105311393738}]}, {"text": "Among the 89 submitted systems, the results of our system are given in  Using the ALL metric, our system ranks 31, but for ALLnrm and Mean metrics, our system ranking is decreased to 59 and 51.", "labels": [], "entities": [{"text": "Mean", "start_pos": 134, "end_pos": 138, "type": "METRIC", "confidence": 0.9848644733428955}]}, {"text": "In terms of ALL metric, our system achieves a medium performance, implying that our system correlates well with human assessments.", "labels": [], "entities": [{"text": "ALL metric", "start_pos": 12, "end_pos": 22, "type": "METRIC", "confidence": 0.7543528974056244}]}, {"text": "In terms of ALLnrm and Mean metrics, our system performance degrades a lot, implying that our system is not well correlated with the reference answer when each dataset is normalized into the aggregated dataset using the least square error or the weighted mean across the datasets.", "labels": [], "entities": [{"text": "ALLnrm", "start_pos": 12, "end_pos": 18, "type": "METRIC", "confidence": 0.9389952421188354}, {"text": "Mean", "start_pos": 23, "end_pos": 27, "type": "METRIC", "confidence": 0.9950675964355469}]}, {"text": "To see how well each of the individual vector space models performed on the evaluation sets, we experiment on the five datasets using vectors based on WordNet, Wikipedia (Wiki), SkipBigram and PolyuCOMP (a combination of the three vectors).", "labels": [], "entities": [{"text": "WordNet", "start_pos": 151, "end_pos": 158, "type": "DATASET", "confidence": 0.984117865562439}, {"text": "SkipBigram", "start_pos": 178, "end_pos": 188, "type": "DATASET", "confidence": 0.9174291491508484}]}, {"text": "gives detailed results of each dataset.", "labels": [], "entities": []}, {"text": "shows that after combining three vector representations, each dataset obtains the best performance.", "labels": [], "entities": []}, {"text": "The WordNet-based approach gives a better performance than Wikipedia-based approach in MSRvid dataset.", "labels": [], "entities": [{"text": "MSRvid dataset", "start_pos": 87, "end_pos": 101, "type": "DATASET", "confidence": 0.9136406183242798}]}, {"text": "The two approaches, however, give similar performance in other four datasets.", "labels": [], "entities": []}, {"text": "This is because the sentences in the MSRvid dataset are too short with limited amount of content words.", "labels": [], "entities": [{"text": "MSRvid dataset", "start_pos": 37, "end_pos": 51, "type": "DATASET", "confidence": 0.9768874943256378}]}, {"text": "It is difficult to capture the meaning of a sentence without distinguishing words in consecutive positions.", "labels": [], "entities": []}, {"text": "This is why the order-sensitive SkipBigram approach gives better performance than the other two approaches.", "labels": [], "entities": [{"text": "SkipBigram", "start_pos": 32, "end_pos": 42, "type": "DATASET", "confidence": 0.8450167179107666}]}, {"text": "For example, A woman is playing a game with a man.", "labels": [], "entities": []}, {"text": "A man is playing piano.", "labels": [], "entities": []}, {"text": "Using the semantic vectors, we will get high similarity scores, but the two sentences are dissimilar.", "labels": [], "entities": [{"text": "similarity", "start_pos": 45, "end_pos": 55, "type": "METRIC", "confidence": 0.9727774262428284}]}, {"text": "If the skip bigram approach is used, the similarity score between sentences will be 0, which correlates with human judgment.", "labels": [], "entities": [{"text": "similarity score", "start_pos": 41, "end_pos": 57, "type": "METRIC", "confidence": 0.9708185791969299}]}, {"text": "In parameter configurations for the MSRvid dataset, higher weight (1-0.123-0.01=0.867) is also given to skip bigrams.", "labels": [], "entities": [{"text": "MSRvid dataset", "start_pos": 36, "end_pos": 50, "type": "DATASET", "confidence": 0.9608031213283539}]}, {"text": "It is interesting to note that the decaying factor for this dataset is 1.4 and is not in the range from 0 to 1 inclusive.", "labels": [], "entities": []}, {"text": "This is because higher decaying factor helps to capture semantic meaning between words that span afar.", "labels": [], "entities": []}, {"text": "For example, A man is playing a flute.", "labels": [], "entities": []}, {"text": "A man is playing a bamboo flute.", "labels": [], "entities": []}, {"text": "In this sentence pair, the second sentence is entailed by the first one.", "labels": [], "entities": []}, {"text": "The similarity can be captured by assigned larger decay factor to weigh the skip bigram \"playing flute\" in two sentences.", "labels": [], "entities": [{"text": "similarity", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9550853371620178}]}, {"text": "Hence, if the value of the decay factor is greater than 1, the two sentences will become much more similar.", "labels": [], "entities": []}, {"text": "After careful investigation, these two sentences are similar to a large extent.", "labels": [], "entities": []}, {"text": "In this sense, a higher decaying factor would help capture the meaning between sentence pairs.", "labels": [], "entities": []}, {"text": "This is quite different from the other four datasets which focus on shared skip bigrams with smaller decaying factor.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Skip bigrams and their Weights in S", "labels": [], "entities": [{"text": "Skip bigrams", "start_pos": 10, "end_pos": 22, "type": "TASK", "confidence": 0.3729492127895355}]}, {"text": " Table 3: Performance using Different Metrics", "labels": [], "entities": []}]}