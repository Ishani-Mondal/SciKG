{"title": [{"text": "Casting Implicit Role Linking as an Anaphora Resolution Task", "labels": [], "entities": [{"text": "Casting Implicit Role Linking", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.8928092569112778}, {"text": "Anaphora Resolution", "start_pos": 36, "end_pos": 55, "type": "TASK", "confidence": 0.7259280532598495}]}], "abstractContent": [{"text": "Linking implicit semantic roles is a challenging problem in discourse processing.", "labels": [], "entities": [{"text": "Linking implicit semantic roles", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.9015720337629318}, {"text": "discourse processing", "start_pos": 60, "end_pos": 80, "type": "TASK", "confidence": 0.7153117507696152}]}, {"text": "Unlike prior work inspired by SRL, we cast this problem as an anaphora resolution task and embed it in an entity-based coreference resolution (CR) architecture.", "labels": [], "entities": [{"text": "SRL", "start_pos": 30, "end_pos": 33, "type": "TASK", "confidence": 0.9439191818237305}, {"text": "anaphora resolution task", "start_pos": 62, "end_pos": 86, "type": "TASK", "confidence": 0.8495548963546753}, {"text": "coreference resolution (CR)", "start_pos": 119, "end_pos": 146, "type": "TASK", "confidence": 0.8123867034912109}]}, {"text": "Our experiments clearly show that CR-oriented features yield strongest performance exceeding a strong baseline.", "labels": [], "entities": []}, {"text": "We address the problem of data sparsity by applying heuristic labeling techniques, guided by the anaphoric nature of the phenomenon.", "labels": [], "entities": []}, {"text": "We achieve performance beyond state-of-the art.", "labels": [], "entities": []}], "introductionContent": [{"text": "A widespread phenomenon that is still poorly studied in NLP is the meaning contribution of unfilled semantic roles of predicates in discourse interpretation.", "labels": [], "entities": [{"text": "discourse interpretation", "start_pos": 132, "end_pos": 156, "type": "TASK", "confidence": 0.7512625455856323}]}, {"text": "Such roles, while linguistically unexpressed, can often be anaphorically bound to antecedent referents in the discourse context.", "labels": [], "entities": []}, {"text": "Capturing such implicit semantic roles and linking them to their antecedents is a challenging problem.", "labels": [], "entities": []}, {"text": "But it bears immense potential for establishing discourse coherence and forgetting closer to the aim of true NLU.", "labels": [], "entities": []}, {"text": "Linking of implicit semantic roles in discourse has recently been introduced as a shared task in the SemEval 2010 competition Linking Events and Their Participants in.", "labels": [], "entities": [{"text": "Linking of implicit semantic roles in discourse", "start_pos": 0, "end_pos": 47, "type": "TASK", "confidence": 0.8886553985731942}, {"text": "SemEval 2010 competition Linking Events and Their Participants", "start_pos": 101, "end_pos": 163, "type": "TASK", "confidence": 0.8829927518963814}]}, {"text": "The task consists in detecting unfilled semantic roles of events and determining antecedents in the discourse context that these roles * The work reported in this paper is based on a Master's Thesis conducted at Heidelberg University can be understood to refer to.", "labels": [], "entities": []}, {"text": "In (1), e.g., the predicate jealousy introduces two implicit roles, one for the experiencer, the other for the object of jealousy involved.", "labels": [], "entities": [{"text": "predicate jealousy", "start_pos": 18, "end_pos": 36, "type": "TASK", "confidence": 0.823566347360611}]}, {"text": "These roles can be bound to Watson and the speaker (I) in the non-local preceding context.", "labels": [], "entities": []}, {"text": "(1) Watson won't allow that I know anything of art but that is mere jealousy because our views upon the subject differ.", "labels": [], "entities": [{"text": "Watson", "start_pos": 4, "end_pos": 10, "type": "METRIC", "confidence": 0.8217904567718506}]}, {"text": "(2) I Reader was sitting reading in the chair P lace . In contrast to implicit roles that can be discoursebound to an antecedent as in (1), roles can be interpreted existentially, as in, with an unfilled TEXT role of the READING frame that cannot be anchored in prior discourse.", "labels": [], "entities": [{"text": "TEXT", "start_pos": 204, "end_pos": 208, "type": "METRIC", "confidence": 0.9425712823867798}]}, {"text": "The FrameNet paradigm) that was used for annotation in the SemEval task classifies these interpretation differences as definite (DNI) vs. indefinite (INI) null instantiations (NI) of roles, respectively.", "labels": [], "entities": [{"text": "SemEval task", "start_pos": 59, "end_pos": 71, "type": "TASK", "confidence": 0.9061416387557983}]}], "datasetContent": [{"text": "We adopt the precision (P), recall (R) and F 1 measures in.", "labels": [], "entities": [{"text": "precision (P)", "start_pos": 13, "end_pos": 26, "type": "METRIC", "confidence": 0.9494805485010147}, {"text": "recall (R)", "start_pos": 28, "end_pos": 38, "type": "METRIC", "confidence": 0.962203249335289}, {"text": "F 1", "start_pos": 43, "end_pos": 46, "type": "METRIC", "confidence": 0.9836240112781525}]}, {"text": "A true positive is a DNI which has been linked to the correct entity as given by the gold data.", "labels": [], "entities": [{"text": "DNI", "start_pos": 21, "end_pos": 24, "type": "METRIC", "confidence": 0.953460156917572}]}, {"text": "For DNI linking, we use BayesNet ( as classifier, implemented in Weka).", "labels": [], "entities": [{"text": "DNI linking", "start_pos": 4, "end_pos": 15, "type": "TASK", "confidence": 0.9373021721839905}, {"text": "BayesNet", "start_pos": 24, "end_pos": 32, "type": "DATASET", "confidence": 0.8259836435317993}, {"text": "Weka", "start_pos": 65, "end_pos": 69, "type": "DATASET", "confidence": 0.980841875076294}]}, {"text": "For each parameter combination, we perform feature selection by means of leave-oneout 10-fold cross-validation on the SemEval training data with successively removing/determining the We experimented with different learners and selected the algorithm that performed best for the different subtasks.", "labels": [], "entities": [{"text": "SemEval training data", "start_pos": 118, "end_pos": 139, "type": "DATASET", "confidence": 0.6445894340674082}]}, {"text": "The resulting models M i are then evaluated on the SemEval test data in different setups: Exp1: Linking DNIs.", "labels": [], "entities": [{"text": "SemEval test data", "start_pos": 51, "end_pos": 68, "type": "DATASET", "confidence": 0.785731703042984}]}, {"text": "Exp1 evaluates our models on the DNI linking task proper (NI-only step (iii)).", "labels": [], "entities": [{"text": "DNI linking task", "start_pos": 33, "end_pos": 49, "type": "TASK", "confidence": 0.7941137949625651}]}, {"text": "This setting uses the gold coreference, SRL and DNI information in the test data.", "labels": [], "entities": [{"text": "SRL", "start_pos": 40, "end_pos": 43, "type": "METRIC", "confidence": 0.8426123857498169}, {"text": "DNI", "start_pos": 48, "end_pos": 51, "type": "METRIC", "confidence": 0.881842851638794}]}, {"text": "For benchmarking on the SemEval task, we perform the complete NI-only task.", "labels": [], "entities": [{"text": "SemEval task", "start_pos": 24, "end_pos": 36, "type": "TASK", "confidence": 0.8469513654708862}]}, {"text": "Here, the test data is only enriched w/ SRL labeling.", "labels": [], "entities": [{"text": "SRL labeling", "start_pos": 40, "end_pos": 52, "type": "TASK", "confidence": 0.5699574649333954}]}, {"text": "Each frame fin the test corpus is processed, involving the following steps: (i) Recognition of NIs is performed by consulting the FN database and determining the FN core roles that are unfilled.", "labels": [], "entities": [{"text": "Recognition of NIs", "start_pos": 80, "end_pos": 98, "type": "TASK", "confidence": 0.8719387054443359}, {"text": "FN database", "start_pos": 130, "end_pos": 141, "type": "DATASET", "confidence": 0.9420912563800812}]}, {"text": "From this NI set, roles that are conceptually redundant or competing with f's overt roles are rejected as they don't need to or must not be linked, respectively.", "labels": [], "entities": []}, {"text": "(ii) For predicting the interpretation of an NI, we use LibSVM () as classifier which further assigns each NI a probability estimate of the NI being definite.", "labels": [], "entities": [{"text": "predicting the interpretation of an NI", "start_pos": 9, "end_pos": 47, "type": "TASK", "confidence": 0.8545728623867035}]}, {"text": "We use a small set of features: the FN semantic type of the NI and a boolean feature indicating whether the target is in passive voice and the agent (object) not realized.", "labels": [], "entities": []}, {"text": "Further, we use a statistical feature which gives the relative model add.", "labels": [], "entities": []}, {"text": "frequency of the role's realization as DNI and INI, respectively, in the training data.", "labels": [], "entities": [{"text": "DNI", "start_pos": 39, "end_pos": 42, "type": "METRIC", "confidence": 0.5866180658340454}, {"text": "INI", "start_pos": 47, "end_pos": 50, "type": "METRIC", "confidence": 0.9124630689620972}]}, {"text": "(iii) DNI linking is performed for each of f's predicted DNIs D fin descending order of their probability estimates.", "labels": [], "entities": [{"text": "DNI linking", "start_pos": 6, "end_pos": 17, "type": "TASK", "confidence": 0.8753980100154877}]}, {"text": "If an antecedent em can be determined fora predicted DNI, the role is labeled as such and linked toe m . As the DNI's role has been filled now, competing or redundant DNIs are removed from D f before moving to the next predicted DNI.", "labels": [], "entities": []}, {"text": "Only DNIs for which an antecedent is found are labeled as such.", "labels": [], "entities": []}, {"text": "Exp2 is evaluated on both gold coreference annotation and automatically assigned coreference chains, using the CR system of. shows the best performing models for DNI linking for each parameter setting 8 . We compare them to a strong baseline Prom (last row) that links each DNI to the antecedent candidate with highest prominence score.", "labels": [], "entities": [{"text": "CR", "start_pos": 111, "end_pos": 113, "type": "METRIC", "confidence": 0.9537480473518372}]}, {"text": "Its F 1 -score is beaten by the other models, with again of 7.2 points for model M 1 . The high performance of the baseline can betaken as evidence that salience factors are crucial for this task.", "labels": [], "entities": [{"text": "F 1 -score", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9923232942819595}]}, {"text": "The best performing model M 1 (27.7 F 1 ) uses about a fifth of the ON data with Chains+Win.", "labels": [], "entities": [{"text": "F 1", "start_pos": 36, "end_pos": 39, "type": "METRIC", "confidence": 0.9231546521186829}, {"text": "ON", "start_pos": 68, "end_pos": 70, "type": "METRIC", "confidence": 0.9809532761573792}]}, {"text": "When using SentWin as entity set, F 1 drops to 18.5 (not shown).", "labels": [], "entities": [{"text": "F 1", "start_pos": 34, "end_pos": 37, "type": "METRIC", "confidence": 0.9944791793823242}]}, {"text": "The best performing model using SentWin (M 1 ) performs 4.9 points below M 1 . Hence, reliance on the Chains+Win set seems beneficial.", "labels": [], "entities": []}, {"text": "Performance of the AllChains setting varies over the We consider the 3 types of entity sets and different training setups \u00b1 additional data (Section 4.3); additional data with gold, projected or automatic frame annotations.", "labels": [], "entities": [{"text": "AllChains setting", "start_pos": 19, "end_pos": 36, "type": "DATASET", "confidence": 0.867331475019455}]}, {"text": "The ON data was also evaluated with roughly a fifth of ON to evaluate the effect of different amounts of data of the same type of data.", "labels": [], "entities": [{"text": "ON", "start_pos": 4, "end_pos": 6, "type": "METRIC", "confidence": 0.9470522403717041}, {"text": "ON", "start_pos": 55, "end_pos": 57, "type": "METRIC", "confidence": 0.9972335696220398}]}, {"text": "30.8 25.1 27.7 -1-4,8 (CR) 21.6 8.1 11.8 -10,13 (SRL) 31.0 25.9 28.2 -5-7,9,11-12 (mixed) 20.6 20.5 20.5: Results of ablation study.", "labels": [], "entities": []}, {"text": "different data sets: the strongest model is M 0 without additional data.", "labels": [], "entities": []}, {"text": "An explanation could be the different data domains (story vs. news), leading to a different nature (length and number) of the entities.", "labels": [], "entities": []}, {"text": "In general, the models seem to profit from heuristically labeled training data.", "labels": [], "entities": []}, {"text": "We note strong gains (up to 10 pts) in precision for 3 of these 5 best models, compared to M 0 . Finally, we observe higher performance when using additional data with gold/ projected semantic frame annotations (M 1 , M 1 ).", "labels": [], "entities": [{"text": "precision", "start_pos": 39, "end_pos": 48, "type": "METRIC", "confidence": 0.9992555975914001}]}, {"text": "Analysis of the best model.", "labels": [], "entities": []}, {"text": "states the results for M 1 when leaving out one of the feature types at a time.", "labels": [], "entities": []}, {"text": "The serious drop of F 1 from 27.7% to 11.8% when omitting CR features clearly demonstrates that this feature type has by far the greatest impact on the task performance.", "labels": [], "entities": [{"text": "F 1", "start_pos": 20, "end_pos": 23, "type": "METRIC", "confidence": 0.9928367733955383}]}, {"text": "Rejection of the mixed features decreases F 1 to a score equal to the prominence baseline, whereas leaving out the SRLfeatures even slightly increases F 1 . The weakness of Feature 13 could still be attributed to data sparsity.", "labels": [], "entities": [{"text": "F 1", "start_pos": 42, "end_pos": 45, "type": "METRIC", "confidence": 0.9950584769248962}, {"text": "SRLfeatures", "start_pos": 115, "end_pos": 126, "type": "METRIC", "confidence": 0.4967395067214966}, {"text": "F 1", "start_pos": 151, "end_pos": 154, "type": "METRIC", "confidence": 0.9906371831893921}]}, {"text": "lists the results for the full NI-only task obtained with the presented models with different additional training data sets (lines 2-5).", "labels": [], "entities": []}, {"text": "When performing all three steps, the F 1 -score of the best model M 1 drops to 10.1% (-17.6 pts, col.", "labels": [], "entities": [{"text": "F 1 -score", "start_pos": 37, "end_pos": 47, "type": "METRIC", "confidence": 0.991847425699234}]}, {"text": "10) under usage of automatic coreference annotations in the test data (i.e. under the real task conditions).", "labels": [], "entities": []}, {"text": "When using gold coreference annotations, the F 1 -score is at 18.1% (col.", "labels": [], "entities": [{"text": "F 1 -score", "start_pos": 45, "end_pos": 55, "type": "METRIC", "confidence": 0.9893244057893753}]}, {"text": "11), which can be seen as an upper bound for our current models on this task.", "labels": [], "entities": []}, {"text": "The difference of 9.6 points between only performing DNI linking) and the full NI-only task reflects the fact that recognizing (step i) and interpreting (step ii) NIs bear difficulties on their own.", "labels": [], "entities": [{"text": "DNI linking", "start_pos": 53, "end_pos": 64, "type": "TASK", "confidence": 0.7623472213745117}]}, {"text": "Comparison of our models with the two SemEval: Exp2 results obtained for our models (lines 1-5) and comparable systems (lines 6-8).", "labels": [], "entities": [{"text": "Exp2", "start_pos": 47, "end_pos": 51, "type": "METRIC", "confidence": 0.9684869647026062}]}, {"text": "Column 5 gives the score for correctly recognized NIs.", "labels": [], "entities": []}, {"text": "6 and 7 report precision for correctly interpreted NIs on the basis of the correctly recognized (relative) vs. all gold NIs to be recognized (absolute).", "labels": [], "entities": [{"text": "precision", "start_pos": 15, "end_pos": 24, "type": "METRIC", "confidence": 0.9994137287139893}]}, {"text": "The scores in the last column (F 1 (crf)) were obtained with gold CR annotations.", "labels": [], "entities": [{"text": "F 1 (crf))", "start_pos": 31, "end_pos": 41, "type": "METRIC", "confidence": 0.9351008176803589}, {"text": "CR annotations", "start_pos": 66, "end_pos": 80, "type": "METRIC", "confidence": 0.9146600365638733}]}, {"text": "task participants 10 (lines 7-8) shows that our models clearly outperform these systems -with again of +5.7 and +8.89 points in F 1 -score in DNI linking.", "labels": [], "entities": [{"text": "F 1 -score", "start_pos": 128, "end_pos": 138, "type": "METRIC", "confidence": 0.9793663769960403}]}, {"text": "Compared to Tonelli and Delmonte (2011) (T&D), M 1 has a higher F 1 -score in linking of +2.1 points.", "labels": [], "entities": [{"text": "F 1 -score", "start_pos": 64, "end_pos": 74, "type": "METRIC", "confidence": 0.9915855526924133}, {"text": "linking", "start_pos": 78, "end_pos": 85, "type": "METRIC", "confidence": 0.9800241589546204}]}, {"text": "In contrast to our method, their linking approach is (admittedly) heavily lexicalized and strongly tailored to the domain of the used data.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: SemEval vs. heuristically acquired data", "labels": [], "entities": [{"text": "SemEval", "start_pos": 10, "end_pos": 17, "type": "TASK", "confidence": 0.9557188153266907}]}, {"text": " Table 4: Exp1: Best performing models for different en- tity and data settings. Test data contain gold CR chains.", "labels": [], "entities": [{"text": "Exp1", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9261033535003662}]}, {"text": " Table 6: Exp2 results obtained for our models (lines 1-5) and comparable systems (lines 6-8). Column 5 gives the  score for correctly recognized NIs. Cols. 6 and 7 report precision for correctly interpreted NIs on the basis of the  correctly recognized (relative) vs. all gold NIs to be recognized (absolute). The scores in the last column (F 1 (crf))  were obtained with gold CR annotations.", "labels": [], "entities": [{"text": "precision", "start_pos": 172, "end_pos": 181, "type": "METRIC", "confidence": 0.9963728189468384}, {"text": "F 1 (crf))", "start_pos": 342, "end_pos": 352, "type": "METRIC", "confidence": 0.9547320485115052}]}]}