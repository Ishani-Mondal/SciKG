{"title": [], "abstractContent": [{"text": "Given a set of images with related captions, our goal is to show how visual features can improve the accuracy of unsupervised word sense disambiguation when the textual context is very small, as this sort of data is common in news and social media.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 101, "end_pos": 109, "type": "METRIC", "confidence": 0.9981874823570251}, {"text": "word sense disambiguation", "start_pos": 126, "end_pos": 151, "type": "TASK", "confidence": 0.5958667695522308}]}, {"text": "We extend previous work in unsupervised text-only dis-ambiguation with methods that integrate text and images.", "labels": [], "entities": []}, {"text": "We construct a corpus by using Amazon Mechanical Turk to caption sense-tagged images gathered from ImageNet.", "labels": [], "entities": [{"text": "caption sense-tagged images gathered from ImageNet", "start_pos": 57, "end_pos": 107, "type": "TASK", "confidence": 0.7060246219237646}]}, {"text": "Using a Yarowsky-inspired algorithm, we show that gains can be made over text-only disam-biguation, as well as multimodal approaches such as Latent Dirichlet Allocation.", "labels": [], "entities": []}], "introductionContent": [{"text": "We examine the problem of performing unsupervised word sense disambiguation (WSD) in situations with little text, but where additional information is available in the form of an image.", "labels": [], "entities": [{"text": "word sense disambiguation (WSD)", "start_pos": 50, "end_pos": 81, "type": "TASK", "confidence": 0.8082099407911301}]}, {"text": "Such situations include captioned newswire photos, and pictures in social media where the textual context is often no larger than a tweet.", "labels": [], "entities": []}, {"text": "Unsupervised WSD has been shown to work very well when the target word is embedded in a large We thank NSERC and U. Toronto for financial support.", "labels": [], "entities": [{"text": "WSD", "start_pos": 13, "end_pos": 16, "type": "TASK", "confidence": 0.9029461145401001}, {"text": "NSERC", "start_pos": 103, "end_pos": 108, "type": "DATASET", "confidence": 0.7648048400878906}, {"text": "U. Toronto", "start_pos": 113, "end_pos": 123, "type": "DATASET", "confidence": 0.8126771152019501}]}, {"text": "Fidler and Dickinson were sponsored by the Army Research Laboratory and this research was accomplished in part under Cooperative Agreement Number W911NF-10-2-0060.", "labels": [], "entities": [{"text": "W911NF-10-2-0060", "start_pos": 146, "end_pos": 162, "type": "DATASET", "confidence": 0.608782947063446}]}, {"text": "The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either express or implied, of the Army Research Laboratory or the U.S. Government.", "labels": [], "entities": [{"text": "Army Research Laboratory", "start_pos": 181, "end_pos": 205, "type": "DATASET", "confidence": 0.8281903664271036}]}, {"text": "However, if the only available text is \"The crane was so massive it blocked the sun\" (see), then text-only disambiguation becomes much more difficult; a human could do little more than guess.", "labels": [], "entities": [{"text": "text-only disambiguation", "start_pos": 97, "end_pos": 121, "type": "TASK", "confidence": 0.6878920793533325}]}, {"text": "But if an image is available, the intended sense is much clearer.", "labels": [], "entities": []}, {"text": "We develop an unsupervised WSD algorithm based on Yarowsky's that uses words in a short caption along with \"visual words\" from the captioned image to choose the best of two possible senses of an ambiguous keyword describing the content of the image.", "labels": [], "entities": []}, {"text": "Language-vision integration is a quickly developing field, and a number of researchers have explored the possibility of combining text and visual features in various multimodal tasks.", "labels": [], "entities": [{"text": "Language-vision integration", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.7175934910774231}]}, {"text": "explored semantic relatedness between words and images to better exploit multimodal content. and combined text and vision to perform effective image annotation.", "labels": [], "entities": []}, {"text": "Barnard and colleagues showed that supervised WSD by could be improved with visual features.", "labels": [], "entities": [{"text": "WSD", "start_pos": 46, "end_pos": 49, "type": "TASK", "confidence": 0.9642871618270874}]}, {"text": "Here we show that unsupervised WSD can similarly be improved.", "labels": [], "entities": [{"text": "WSD", "start_pos": 31, "end_pos": 34, "type": "TASK", "confidence": 0.8546433448791504}]}, {"text": "Loeff, and combined visual and textual information to solve a related task, image sense disambiguation, in an unsupervised fashion.", "labels": [], "entities": [{"text": "image sense disambiguation", "start_pos": 76, "end_pos": 102, "type": "TASK", "confidence": 0.6451886097590128}]}, {"text": "In Loeff et al.'s work, little gain was realized when visual features were added to a great deal of text.", "labels": [], "entities": []}, {"text": "We show that these features have more utility with small textual contexts, and that, when little text is available, our method is more suitable than Saenko and Darrell's.", "labels": [], "entities": []}], "datasetContent": [{"text": "We require a collection of images with associated captions.", "labels": [], "entities": []}, {"text": "We also require sense annotations for the keyword for each image to use for evaluation.", "labels": [], "entities": []}, {"text": "ImCor dataset by associating images from the Corel database with text from the SemCor corpus. and used Yahoo!'s image search to gather images with their associated web pages.", "labels": [], "entities": [{"text": "ImCor dataset", "start_pos": 0, "end_pos": 13, "type": "DATASET", "confidence": 0.9048081934452057}, {"text": "SemCor corpus.", "start_pos": 79, "end_pos": 93, "type": "DATASET", "confidence": 0.7446949183940887}]}, {"text": "While these datasets contain images paired with text, the textual contexts are much larger than typical captions.", "labels": [], "entities": []}, {"text": "To show that the addition of visual features improves the accuracy of sense disambiguation for imagecaption pairs, we run our algorithm both with and without the visual features.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.9988558292388916}]}, {"text": "We also compare our results to three different baseline methods: K-means (K-M), Latent Dirichlet Allocation (LDA) (, and an unsupervised WSD algorithm (PBP) explained below.", "labels": [], "entities": [{"text": "Latent Dirichlet Allocation (LDA)", "start_pos": 80, "end_pos": 113, "type": "METRIC", "confidence": 0.9411547780036926}]}, {"text": "We use accuracy to measure performance as it is commonly used by the WSD community (See).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 7, "end_pos": 15, "type": "METRIC", "confidence": 0.9993333220481873}, {"text": "WSD", "start_pos": 69, "end_pos": 72, "type": "TASK", "confidence": 0.8921442031860352}]}, {"text": "For K-means, we set k = 2 as we have two senses, and represent each document with a V -dimensional vector, where the ith element is the proportion of word V i in the document.", "labels": [], "entities": []}, {"text": "We run K-means both with and without visual features.", "labels": [], "entities": []}, {"text": "For LDA, we use the dictionary sense model from.", "labels": [], "entities": [{"text": "LDA", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.8013826012611389}]}, {"text": "A topic model is learned where the relatedness of a topic to a sense is based on the probabilities of that topic generating the seed words from its dictionary definitions.", "labels": [], "entities": []}, {"text": "Analogously to k-means, we learn a model for text alone, and a model for text augmented with visual information.", "labels": [], "entities": []}, {"text": "For unsupervised WSD (applied to text only), we use WordNet::SenseRelate::TargetWord, hereafter PBP (, the highest scoring unsupervised lexical sample word sense disambiguation algorithm at SemEval07 (.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 52, "end_pos": 59, "type": "DATASET", "confidence": 0.9534667134284973}, {"text": "word sense disambiguation", "start_pos": 151, "end_pos": 176, "type": "TASK", "confidence": 0.675081729888916}]}, {"text": "PBP treats the nearby words around the target word as a bag, and uses the WordNet hierarchy to assign a similarity score between the possible senses of words in the context, and possible senses of the target word.", "labels": [], "entities": [{"text": "PBP", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8524082899093628}, {"text": "similarity score", "start_pos": 104, "end_pos": 120, "type": "METRIC", "confidence": 0.9747306108474731}]}, {"text": "As our captions are fairly short, we use the entire caption as context.", "labels": [], "entities": []}, {"text": "The most important result is the gain inaccuracy after adding visual features.", "labels": [], "entities": []}, {"text": "While the average gain across all words is slight, it is significant at p < 0.02 (using a paired t-test).", "labels": [], "entities": []}, {"text": "For 12 of the 20 words, the visual features improve performance, and in 6 of those, the improvement is 5-11%.", "labels": [], "entities": []}, {"text": "For some words there is no significant improvement inaccuracy, or even a slight decrease.", "labels": [], "entities": []}, {"text": "With words like \"bass\" or \"chip\" there is little room to improve upon the text-only result.", "labels": [], "entities": []}, {"text": "For words like \"plant\" or \"press\" it seems the text-only result is not strong enough to help bootstrap the visual features in any useful way.", "labels": [], "entities": []}, {"text": "In other cases where little improvement is seen, the problem may lie with high intra-class variation, as our visual words are not very robust features, or with alack of orthogonality between the lexical and visual information.", "labels": [], "entities": []}, {"text": "Our algorithm also performs significantly better than the baseline measurements.", "labels": [], "entities": []}, {"text": "K-means performs surprisingly well compared to the other baselines, but seems unable to make much sense of the visual information present.", "labels": [], "entities": []}, {"text": "LDA model makes substansial gains by using visual features, but does not perform as well on this task.", "labels": [], "entities": []}, {"text": "We suspect that a strict adherence to the seed words maybe to blame: while both this LDA model and our algorithm use the same seed definitions initially, our algorithm is free to change its mind about the usefulness of the words in the definitions as it progresses, whereas the LDA model has no such capacity.", "labels": [], "entities": []}, {"text": "Indeed, words that are intuitively nondiscriminative, such as \"carry\", \"lack\", or \"late\", are not uncommon in the definitions we use.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results (Average accuracy across all five sets of  data). Bold indicates best performance for that word.  Ours Ours K-M K-M LDA LDA PBP  text w/vis text w/vis text w/vis text  band  .80 .82 .66 .65  .64 .56 .73  bank  .77 .78 .71 .59  .52 .67 .62  bass  .94 .94 .90 .88  .61 .62 .49  chip  .90 .90 .73 .58  .57 .66 .75  clip  .70 .79 .65 .58  .48 .53 .65  club  .80 .84 .80 .81  .61 .73 .63  court  .79 .79 .61 .53  .62 .82 .57  crane .62 .67 .76 .76  .52 .54 .66  game .78 .78 .60 .66  .60 .66 .70  hood  .74 .73 .73 .70  .51 .45 .55  jack  .76 .74 .62 .53  .58 .66 .47  key  .81 .92 .79 .54  .57 .70 .50  mold  .67 .68 .59 .67  .57 .66 .54  mouse .84 .84 .71 .62  .62 .69 .68  plant  .54 .54 .56 .53  .52 .50 .72  press  .60 .59 .60 .54  .58 .62 .48  seal  .70 .80 .61 .67  .55 .53 .62  speaker .70 .69 .57 .53  .55 .62 .63  squash .89 .95 .84 .92  .55 .67 .79  track  .78 .85 .71 .66  .51 .54 .69  avg.  .76 .78 .69 .65  .56 .63 .62", "labels": [], "entities": [{"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9219032526016235}]}]}