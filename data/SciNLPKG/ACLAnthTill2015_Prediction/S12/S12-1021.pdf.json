{"title": [{"text": "An Unsupervised Ranking Model for Noun-Noun Compositionality", "labels": [], "entities": [{"text": "Noun-Noun Compositionality", "start_pos": 34, "end_pos": 60, "type": "TASK", "confidence": 0.6925076246261597}]}], "abstractContent": [{"text": "We propose an unsupervised system that learns continuous degrees of lexicality for noun-noun compounds, beating a strong base-line on several tasks.", "labels": [], "entities": []}, {"text": "We demonstrate that the distributional representations of compounds and their parts can be used to learn a fine-grained representation of semantic contribution.", "labels": [], "entities": []}, {"text": "Finally, we argue such a representation captures compositionality better than the current status-quo which treats compositionality as a binary classification problem.", "labels": [], "entities": []}], "introductionContent": [{"text": "A Multiword Expressions (MWE) can be defined as a sequence of words whose meaning cannot necessarily be derived from the meaning of the words making up that sequence, for example: Rat Race -self-defeating or pointless pursuit MWEs are considered a \"key problem for the development of large-scale, linguistically sound natural language processing technology\" ().", "labels": [], "entities": [{"text": "Multiword Expressions (MWE)", "start_pos": 2, "end_pos": 29, "type": "TASK", "confidence": 0.775361692905426}, {"text": "Rat Race -self-defeating or pointless pursuit MWEs", "start_pos": 180, "end_pos": 230, "type": "TASK", "confidence": 0.612999252974987}]}, {"text": "The challenge posed by MWEs is threefold, consisting of MWE identification, classification and interpretation.", "labels": [], "entities": [{"text": "MWE identification", "start_pos": 56, "end_pos": 74, "type": "TASK", "confidence": 0.9253562390804291}]}, {"text": "Following the identification of a MWE, it needs to be established whether the expression should be treated as lexical (idiomatic) or as compositional.", "labels": [], "entities": [{"text": "identification of a MWE", "start_pos": 14, "end_pos": 37, "type": "TASK", "confidence": 0.6410226672887802}]}, {"text": "The final step, learning the semantics of the MWE, strongly depends on this decision.", "labels": [], "entities": [{"text": "learning the semantics of the MWE", "start_pos": 16, "end_pos": 49, "type": "TASK", "confidence": 0.546937515338262}]}, {"text": "The problem posed by MWEs is considered hard, but at the same time it is highly relevant and interesting.", "labels": [], "entities": []}, {"text": "MWEs occur frequently in language and interpreting them correctly would directly improve results in a number of tasks in NLP such as translation and parsing . By extension this makes deciding the lexicality of MWEs an important challenge for various fields including machine translation, question answering and information retrieval.", "labels": [], "entities": [{"text": "translation and parsing", "start_pos": 133, "end_pos": 156, "type": "TASK", "confidence": 0.6058786809444427}, {"text": "deciding the lexicality of MWEs", "start_pos": 183, "end_pos": 214, "type": "TASK", "confidence": 0.6152070820331573}, {"text": "machine translation", "start_pos": 267, "end_pos": 286, "type": "TASK", "confidence": 0.828679084777832}, {"text": "question answering", "start_pos": 288, "end_pos": 306, "type": "TASK", "confidence": 0.899808257818222}, {"text": "information retrieval", "start_pos": 311, "end_pos": 332, "type": "TASK", "confidence": 0.8043900430202484}]}, {"text": "In this paper we discuss compositionality with respect to noun-noun compounds.", "labels": [], "entities": []}, {"text": "Most Computational Linguistics literature treats compositionality as a binary problem, classifying compounds as either lexical or compositional.", "labels": [], "entities": []}, {"text": "We show that this approach is too simplistic and argue for the real-valued treatment of compositionality.", "labels": [], "entities": []}, {"text": "We propose two unsupervised models that learn compositionality rankings for compounds, placing them on a scale between lexical and compositional extremes.", "labels": [], "entities": []}, {"text": "We develop a fine-grained representation of compositionality using a novel generative approach that models context as generated by compound constituents.", "labels": [], "entities": []}, {"text": "This representation differentiates between the semantic contribution of both compound constituents as well as the compound itself.", "labels": [], "entities": []}, {"text": "Comparing it with existing work in the field, we demonstrate the competitiveness of our approach.", "labels": [], "entities": []}, {"text": "We evaluate on an existing corpus of noun compounds with ranked compositionality data, as well as on a large corpus with a binary annotation for lexical and compositional compounds.", "labels": [], "entities": []}, {"text": "We analyse the impact of data sparsity and propose an interpolation approximation which significantly reduces the effect of sparsity on model performance.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our two models on the REDDY data set by comparing its scores for lexicality (Lex(c)) with the annotated gold standard.", "labels": [], "entities": [{"text": "REDDY data set", "start_pos": 34, "end_pos": 48, "type": "DATASET", "confidence": 0.9380426009496053}, {"text": "Lex(c))", "start_pos": 89, "end_pos": 96, "type": "METRIC", "confidence": 0.9140225201845169}]}, {"text": "The aim of this evaluation is to determine how accurately the models can capture gradual distinctions in lexicality.", "labels": [], "entities": []}, {"text": "The ROC analysis on the TRATZ data set furthermore informs us how precise the models are at distinguishing lexical from compositional compounds.", "labels": [], "entities": [{"text": "ROC", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.8019471168518066}, {"text": "TRATZ data set", "start_pos": 24, "end_pos": 38, "type": "DATASET", "confidence": 0.9568168123563131}]}, {"text": "Results of the REDDY evaluation are in.", "labels": [], "entities": [{"text": "REDDY", "start_pos": 15, "end_pos": 20, "type": "TASK", "confidence": 0.5899626016616821}]}, {"text": "We use Spearman's \u03c1 to measure the monotonic correlation of our data to the gold standard.", "labels": [], "entities": [{"text": "gold standard", "start_pos": 76, "end_pos": 89, "type": "DATASET", "confidence": 0.9085199236869812}]}, {"text": "Pearson's r additionally captures the linear relationship between the data, taking into account the relative differences in Lex(c) scores among noun compounds.", "labels": [], "entities": [{"text": "Lex(c) scores", "start_pos": 124, "end_pos": 137, "type": "METRIC", "confidence": 0.9625918984413147}]}, {"text": "While both models, BIN-CMPD and MULT-CMPD, clearly learn a correlation with lexicality rankings, they underperform the strong, semisupervised COSLEX baselines described earlier in this paper.", "labels": [], "entities": [{"text": "BIN-CMPD", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9922825694084167}, {"text": "MULT-CMPD", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.6903071403503418}]}, {"text": "The second evaluation, on the binary TRATZ data set shows a different picture (see.", "labels": [], "entities": [{"text": "TRATZ data set", "start_pos": 37, "end_pos": 51, "type": "DATASET", "confidence": 0.8715768853823344}]}, {"text": "The best COSLEX baseline (ADD with w = 0.2) fails to outperform random choice on this task.", "labels": [], "entities": [{"text": "COSLEX baseline", "start_pos": 9, "end_pos": 24, "type": "METRIC", "confidence": 0.9010608196258545}]}, {"text": "Both generative models clearly beat COSLEX on this task, with MULT-CMPD in particular performing very well for low sensitivity.", "labels": [], "entities": []}, {"text": "There is no clear distinction in performance between the two generative approaches.", "labels": [], "entities": []}, {"text": "Further analysis might help us to separate the two more clearly, and we will continue using both models throughout this paper.", "labels": [], "entities": []}, {"text": "It is important to note the different performance of the generative models vs. the cosine similarity approach on two tasks.", "labels": [], "entities": []}, {"text": "The REDDY data set has a nearly linear distribution of compositionality scores, while the TRATZ data set is overwhelmingly compositional, which more closely represents the real world distribution of compounds.", "labels": [], "entities": [{"text": "REDDY data set", "start_pos": 4, "end_pos": 18, "type": "DATASET", "confidence": 0.9338887532552084}, {"text": "TRATZ data set", "start_pos": 90, "end_pos": 104, "type": "DATASET", "confidence": 0.8260780771573385}]}, {"text": "The poor performance of the cosine similarity approach (COSLEX) on the TRATZ evaluation suggests the limitations of this approach when applied to more realistic data such as this data set.", "labels": [], "entities": [{"text": "cosine similarity approach (COSLEX)", "start_pos": 28, "end_pos": 63, "type": "METRIC", "confidence": 0.807842493057251}, {"text": "TRATZ evaluation", "start_pos": 71, "end_pos": 87, "type": "DATASET", "confidence": 0.6911025643348694}]}, {"text": "An additional explanation for the semi-supervised baseline's poorer result is that the effect of parameter tuning decreases on larger data.", "labels": [], "entities": []}, {"text": "Investigating the errors made by the models MULT-CMPD and BIN-CMPD gives rise to a number of possible explanations for their performance.", "labels": [], "entities": [{"text": "MULT-CMPD", "start_pos": 44, "end_pos": 53, "type": "DATASET", "confidence": 0.7534345984458923}, {"text": "BIN-CMPD", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.9765368700027466}]}, {"text": "The most promising lead is related to data sparsity, with many of the evaluated noun-noun compounds only appearing once or twice in the corpus.", "labels": [], "entities": []}, {"text": "This makes it harder for our generative approach to learn sensible context distributions for these instances.", "labels": [], "entities": [{"text": "generative", "start_pos": 29, "end_pos": 39, "type": "TASK", "confidence": 0.9713500142097473}]}, {"text": "We will next investigate how to reduce the effects encountered by sparsity.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Results on the REDDY data set, reporting Pear- son's r and Spearman's \u03c1 correlations. Values range from  -1 (negative correlation) to +1 (perfect correlation).", "labels": [], "entities": [{"text": "REDDY data set", "start_pos": 25, "end_pos": 39, "type": "DATASET", "confidence": 0.929853638013204}, {"text": "Spearman's \u03c1 correlations", "start_pos": 69, "end_pos": 94, "type": "METRIC", "confidence": 0.6130802184343338}, {"text": "perfect correlation", "start_pos": 148, "end_pos": 167, "type": "METRIC", "confidence": 0.9145881235599518}]}, {"text": " Table 3: Results on the REDDY data set, reporting  Pearson's r and Spearman's \u03c1 correlations, comparing  Ilex(c) and Clec(c) interpolations with Lex(c).", "labels": [], "entities": [{"text": "REDDY data set", "start_pos": 25, "end_pos": 39, "type": "DATASET", "confidence": 0.9311767816543579}, {"text": "Pearson's r and Spearman's \u03c1 correlations", "start_pos": 52, "end_pos": 93, "type": "METRIC", "confidence": 0.6553994156420231}]}]}