{"title": [{"text": "DirRelCond3: Detecting Textual Entailment Across Languages With Conditions On Directional Text Relatedness Scores", "labels": [], "entities": [{"text": "Detecting Textual Entailment", "start_pos": 13, "end_pos": 41, "type": "TASK", "confidence": 0.9007727305094401}, {"text": "Directional Text Relatedness Scores", "start_pos": 78, "end_pos": 113, "type": "TASK", "confidence": 0.7040037363767624}]}], "abstractContent": [{"text": "There are relatively few entailment heuristics that exploit the directional nature of the entail-ment relation.", "labels": [], "entities": []}, {"text": "Cross-Lingual Text Entailment (CLTE), besides introducing the extra dimension of cross-linguality, also requires to determine the exact direction of the entailment relation, to provide content synchronization (Negri et al., 2012).", "labels": [], "entities": [{"text": "Cross-Lingual Text Entailment (CLTE)", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.8021976550420126}]}, {"text": "Our system uses simple dictionary lookup combined with heuris-tic conditions to determine the possible directions of entailment between the two texts written in different languages.", "labels": [], "entities": []}, {"text": "The key members of the conditions were derived from (Cor-ley and Mihalcea, 2005) formula initially for text similarity, while the entailment condition used as a starting point was that from (Tatar et al., 2009).", "labels": [], "entities": [{"text": "text similarity", "start_pos": 103, "end_pos": 118, "type": "TASK", "confidence": 0.687565341591835}]}, {"text": "We show the results obtained by our implementation of this simple and fast approach at the CLTE task from the SemEval-2012 challenge.", "labels": [], "entities": [{"text": "CLTE task from the SemEval-2012 challenge", "start_pos": 91, "end_pos": 132, "type": "DATASET", "confidence": 0.7305047412713369}]}], "introductionContent": [{"text": "Recognizing textual entailment (TE) is a key task for many natural language processing (NLP) problems.", "labels": [], "entities": [{"text": "Recognizing textual entailment (TE)", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.926456888516744}]}, {"text": "It consists in determining if an entailment relation exists between two texts: the text (T) and the hypothesis (H).", "labels": [], "entities": []}, {"text": "The notation T \u2192 H says that the meaning of H can be inferred from T, in order words, H does not introduce any novel information with respect to T.", "labels": [], "entities": []}, {"text": "Even though RTE challenges lead to many approaches for finding textual entailment, fewer authors exploited the directional character of the entailment relation.", "labels": [], "entities": [{"text": "RTE", "start_pos": 12, "end_pos": 15, "type": "TASK", "confidence": 0.9521109461784363}]}, {"text": "Due to the fact that the entailment relation, unlike the equivalence relation, is not symmetric, if T \u2192 H, it is less likely that the reverse H \u2192 T can also hold ( ).", "labels": [], "entities": []}, {"text": "The novel Cross-Lingual Text Entailment (CLTE) approach increases the complexity of the traditional TE task in two way, both of which have been only partially researched and have promise for great potential (): \u2022 the two texts are no longer written in the same language (cross-linguality); \u2022 the entailment needs to be queried in both directions (content synchronization).", "labels": [], "entities": [{"text": "Cross-Lingual Text Entailment (CLTE)", "start_pos": 10, "end_pos": 46, "type": "TASK", "confidence": 0.771133542060852}]}, {"text": "presented initial research directions and experiments for the cross-lingual context and explored possible application scenarios.", "labels": [], "entities": []}], "datasetContent": [{"text": "The CLTE task provided researchers with training sets of 500 sentence pairs (one English, one foreign) already annotated with the type of entailment that exists between them ('Forward', 'Backward', 'Bidirectional', 'No entailment').", "labels": [], "entities": []}, {"text": "There was one training set for each French-English, German-English, Italian-English, Spanish-English language combination ().", "labels": [], "entities": []}, {"text": "The test set consisted in a similarly structured 500 pairs for each language pair but without annotations.", "labels": [], "entities": []}, {"text": "The mentioned entailment judgment types were uniformly distributed, both in the case of the development and the test dataset.", "labels": [], "entities": []}, {"text": "The DirRelCond3 system participated at the CLTE task with four runs for each of the above language combinations.", "labels": [], "entities": []}, {"text": "Regarding the results, the accuracies obtained are summarized in table 1 as percentages.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 27, "end_pos": 37, "type": "METRIC", "confidence": 0.9855659604072571}]}, {"text": "show the precision, recall and Fmeasure for the 'Forward', 'Backward', 'No entailment' and 'Bidirectional' judgments for each of the language pair combinations in the case of the best run that the DirRelCond3 system has obtained: The earlier figures pointed out that generally the unidirectional 'Forward' and 'Backward' judgements produced better results than the remaining System Spa-En Ita-En Fra-En Deu-En   ones that involved bi-directionality.", "labels": [], "entities": [{"text": "precision", "start_pos": 9, "end_pos": 18, "type": "METRIC", "confidence": 0.99968421459198}, {"text": "recall", "start_pos": 20, "end_pos": 26, "type": "METRIC", "confidence": 0.9995918869972229}, {"text": "Fmeasure", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.9994537234306335}]}, {"text": "This is somewhat expected because in this case it is more difficult to correctly judge since there could more possibility for error.", "labels": [], "entities": []}, {"text": "Regarding the individual runs, run 2 added slightly improved dictionary search in addition to run 1, by attempting to look for the lemma form of the word as well, that was available thanks to the  TreeTagger tool.", "labels": [], "entities": []}, {"text": "In case the word was still not found, but the language was French or Italian and the word contained apostrophe, a lookup was attempted for the part following it.", "labels": [], "entities": []}, {"text": "Run 3 added another slight improvement for German, in case there was still no match for the word, tried to see if the word was a composite containing two parts found in the dictionary, and if so, used the first one.", "labels": [], "entities": [{"text": "Run 3", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9359093606472015}]}, {"text": "The first two runs were only using the FreeDict (FreeDictProject, 2012) dictionary, while starting with run 3, Italian and French language words, in case not found, could also be searched in the WordReference (WordReference.com, 2012) online dictionary.", "labels": [], "entities": [{"text": "FreeDict (FreeDictProject, 2012) dictionary", "start_pos": 39, "end_pos": 82, "type": "DATASET", "confidence": 0.9182135888508388}, {"text": "WordReference (WordReference.com, 2012) online dictionary", "start_pos": 195, "end_pos": 252, "type": "DATASET", "confidence": 0.9218854233622551}]}, {"text": "The first three runs were using entailment conditions common to all language combinations.", "labels": [], "entities": []}, {"text": "The values of the parameters were chosen based on the CLTE development dataset) and were as follows: \u03b8 = 0.5, \u03b4 = 0.03, \u03c3 = 0.0.", "labels": [], "entities": [{"text": "CLTE development dataset", "start_pos": 54, "end_pos": 78, "type": "DATASET", "confidence": 0.9460382262865702}]}, {"text": "The final run used empirically-tuned conditions for each language pair in the dataset.", "labels": [], "entities": []}, {"text": "The \u03b8 threshold needed to be lowered for Spanish since many words were not found in FreeDict, which was the only one we had available for use, so the relatedness scores were rather smaller.", "labels": [], "entities": [{"text": "\u03b8 threshold", "start_pos": 4, "end_pos": 15, "type": "METRIC", "confidence": 0.9684195816516876}]}], "tableCaptions": [{"text": " Table 1: DirRelCond3 accuracies obtained for CLTE  task. Best results are with italic.", "labels": [], "entities": []}, {"text": " Table 2: DirRelCond3 -Run 4 condition parameters.", "labels": [], "entities": []}]}