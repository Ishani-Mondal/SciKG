{"title": [], "abstractContent": [{"text": "Detecting emotions in microblogs and social media posts has applications for industry, health, and security.", "labels": [], "entities": [{"text": "Detecting emotions", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8793877959251404}]}, {"text": "However, there exists no microblog corpus with instances labeled for emotions for developing supervised systems.", "labels": [], "entities": []}, {"text": "In this paper, we describe how we created such a corpus from Twitter posts using emotion-word hashtags.", "labels": [], "entities": []}, {"text": "We conduct experiments to show that the self-labeled hashtag annotations are consistent and match with the annotations of trained judges.", "labels": [], "entities": []}, {"text": "We also show how the Twit-ter emotion corpus can be used to improve emotion classification accuracy in a different domain.", "labels": [], "entities": [{"text": "Twit-ter emotion corpus", "start_pos": 21, "end_pos": 44, "type": "DATASET", "confidence": 0.7838386098543803}, {"text": "emotion classification", "start_pos": 68, "end_pos": 90, "type": "TASK", "confidence": 0.6768942922353745}, {"text": "accuracy", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.8922669887542725}]}, {"text": "Finally, we extract a word-emotion association lexicon from this Twitter corpus, and show that it leads to significantly better results than the manually crafted WordNet Affect lexicon in an emotion classification task.", "labels": [], "entities": [{"text": "WordNet Affect lexicon", "start_pos": 162, "end_pos": 184, "type": "DATASET", "confidence": 0.8475852409998575}, {"text": "emotion classification task", "start_pos": 191, "end_pos": 218, "type": "TASK", "confidence": 0.8053920467694601}]}], "introductionContent": [{"text": "We use language not just to convey facts, but also our emotions.", "labels": [], "entities": []}, {"text": "Automatically identifying emotions expressed in text has a number of applications, including customer relation management (, determining popularity of products and governments (, and improving human-computer interaction).", "labels": [], "entities": [{"text": "Automatically identifying emotions expressed in text", "start_pos": 0, "end_pos": 52, "type": "TASK", "confidence": 0.7915075023969015}, {"text": "customer relation management", "start_pos": 93, "end_pos": 121, "type": "TASK", "confidence": 0.7971479495366415}]}, {"text": "Twitter is an online social networking and microblogging service where users post and read messages that are up to 140 characters long.", "labels": [], "entities": []}, {"text": "The messages are called tweets.", "labels": [], "entities": []}, {"text": "Often a tweet may include one or more words immediately preceded with a hash symbol (#).", "labels": [], "entities": []}, {"text": "These words are called hashtags.", "labels": [], "entities": []}, {"text": "Hashtags serve many purposes, but most notably they are used to indicate the topic.", "labels": [], "entities": []}, {"text": "Often these words add to the information in the tweet: for example, hashtags indicating the tone of the message or their internal emotions.", "labels": [], "entities": []}, {"text": "From the perspective of one consuming tweets, hashtags play a role in search: Twitter allows people to search tweets not only through words in the tweets, but also through hashtagged words.", "labels": [], "entities": []}, {"text": "Consider the tweet below: We are fighting for the 99% that have been left behind.", "labels": [], "entities": []}, {"text": "#OWS #anger A number of people tweeting about the Occupy Wall Street movement added the hashtag #OWS to their tweets.", "labels": [], "entities": [{"text": "Occupy Wall Street movement", "start_pos": 50, "end_pos": 77, "type": "DATASET", "confidence": 0.9120164662599564}]}, {"text": "This allowed people searching for tweets about the movement to access them simply by searching for the #OWS hashtag.", "labels": [], "entities": []}, {"text": "In this particular instance, the tweeter (one who tweets) has also added an emotion-word hashtag #anger, possibly to convey that he or she is angry.", "labels": [], "entities": []}, {"text": "Currently there are more than 200 million Twitter accounts, 180 thousand tweets posted everyday, and 18 thousand Twitter search queries every second.", "labels": [], "entities": []}, {"text": "Socio-linguistic researchers point out that Twitter is primarily a means for people to converse with other individuals, groups, and the world in general (.", "labels": [], "entities": []}, {"text": "As tweets are freely accessible to all, the conversations can take on non-traditional forms such as discussions developing through many voices rather than just two interlocuters.", "labels": [], "entities": []}, {"text": "For example, the use of Twitter and Facebook has been credited with providing momentum to the 2011 Arab Spring and Occupy Wall Street movements.", "labels": [], "entities": [{"text": "Occupy Wall Street", "start_pos": 115, "end_pos": 133, "type": "DATASET", "confidence": 0.8631303906440735}]}, {"text": "Understanding how such conversations develop, how people influence one another through emotional expressions, and how news is shared to elicit certain emotional reactions, are just some of the compelling reasons to develop better models for the emotion analysis of social media.", "labels": [], "entities": []}, {"text": "Supervised methods for emotion detection tend to perform better than unsupervised ones.", "labels": [], "entities": [{"text": "emotion detection", "start_pos": 23, "end_pos": 40, "type": "TASK", "confidence": 0.8269836902618408}]}, {"text": "They use ngram features such as unigrams and bigrams (individual words and two-word sequences)).", "labels": [], "entities": []}, {"text": "However, these methods require labeled data where utterances are marked with the emotion they express.", "labels": [], "entities": []}, {"text": "Manual annotation is timeintensive and costly.", "labels": [], "entities": []}, {"text": "Thus only a small amount of such text exists.", "labels": [], "entities": []}, {"text": "Further, supervised algorithms that rely on ngram features tend to classify accurately only if trained on data from the same domain as the target sentences).", "labels": [], "entities": []}, {"text": "Thus even the limited amount of existing emotion-labeled data is unsuitable for use in microblog analysis.", "labels": [], "entities": [{"text": "microblog analysis", "start_pos": 87, "end_pos": 105, "type": "TASK", "confidence": 0.7476667165756226}]}, {"text": "In this paper, we show how we automatically created a large dataset of more than 20,000 emotionlabeled tweets using hashtags.", "labels": [], "entities": []}, {"text": "We compiled labeled data for six emotions-joy, sadness, anger, fear, disgust, and surprise-argued to be the most basic).", "labels": [], "entities": []}, {"text": "We will refer to our dataset as the Twitter Emotion Corpus (TEC).", "labels": [], "entities": []}, {"text": "We show through experiments that even though the tweets and hashtags cover a diverse array of topics and were generated by thousands of different individuals (possibly with very different educational and socio-economic backgrounds), the emotion annotations are consistent and match the intuitions of trained judges.", "labels": [], "entities": []}, {"text": "We also show how we used the TEC to improve emotion detection in a domain very different from social media.", "labels": [], "entities": [{"text": "emotion detection", "start_pos": 44, "end_pos": 61, "type": "TASK", "confidence": 0.744642049074173}]}, {"text": "Finally, we describe how we generated a large lexicon of ngrams and associated emotions from TEC.", "labels": [], "entities": []}, {"text": "This emotion lexicon can be used in many applications, including highlighting words and phrases in apiece of text to quickly convey regions of affect.", "labels": [], "entities": []}, {"text": "We show that the lexicon leads to significantly better results than that obtained using the manually crafted WordNet Affect lexicon in an emotion classification task.", "labels": [], "entities": [{"text": "WordNet Affect lexicon", "start_pos": 109, "end_pos": 131, "type": "DATASET", "confidence": 0.909014880657196}, {"text": "emotion classification task", "start_pos": 138, "end_pos": 165, "type": "TASK", "confidence": 0.7899873356024424}]}], "datasetContent": [{"text": "We applied the binary classifiers described above to the TEC.", "labels": [], "entities": [{"text": "TEC", "start_pos": 57, "end_pos": 60, "type": "DATASET", "confidence": 0.828612208366394}]}, {"text": "Observe that even though the TEC was created from tens of thousands of users, the automatic classifiers are able to predict the emotions (hashtags) with F-scores much higher than the random baseline, and also higher than those obtained on the 1000-headlines corpus.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 153, "end_pos": 161, "type": "METRIC", "confidence": 0.9905780553817749}]}, {"text": "Note also that this is despite the fact that the random baseline for the 1000-headlines corpus (F = 30.3) is higher than the random baseline for the TEC (F = 21.7).", "labels": [], "entities": [{"text": "F = 30.3)", "start_pos": 96, "end_pos": 105, "type": "METRIC", "confidence": 0.9437164217233658}, {"text": "TEC", "start_pos": 149, "end_pos": 152, "type": "DATASET", "confidence": 0.7065698504447937}, {"text": "F", "start_pos": 154, "end_pos": 155, "type": "METRIC", "confidence": 0.7404536604881287}]}, {"text": "The results suggest that emotion hashtags assigned to tweets are consistent to a degree such that they can be used for detecting emotion hashtags in other tweets.", "labels": [], "entities": []}, {"text": "Note that expectedly the Joy-NotJoy classifier  gets the best results as it has the highest number of training instances.", "labels": [], "entities": []}, {"text": "The Sadness-NotSadness classifier performed relatively poorly considering the amount of training instances available, whereas the Fear-NotFear classifier performed relatively well.", "labels": [], "entities": []}, {"text": "It is possible that people useless overt cues in tweets when they are explicitly giving it a sadness hashtag.", "labels": [], "entities": []}, {"text": "As mentioned earlier, supervised algorithms perform well when training and test data are from the same domain.", "labels": [], "entities": []}, {"text": "However, certain domain adaptation algorithms maybe used to combine training data in the target domain with large amounts of training data from a different source domain.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 17, "end_pos": 34, "type": "TASK", "confidence": 0.7323639690876007}]}, {"text": "The approach involves the transformation of the original training instance feature vector into anew space made up of three copies of the original vector.", "labels": [], "entities": []}, {"text": "The three copies correspond to the target domain, the source domain, and the general domain.", "labels": [], "entities": []}, {"text": "If X represents an original feature vector from the target domain, then it is transformed into XOX, where O is a zero vector.", "labels": [], "entities": []}, {"text": "If X represents original feature vector from the source domain, then it is transformed into OXX.", "labels": [], "entities": []}, {"text": "This data is given to the learning algorithm, which learns information specific to the target domain, specific to the source domain, as well as information that applies to both domains.", "labels": [], "entities": []}, {"text": "The test instance feature vector (which is from the target domain) is transformed to XOX.", "labels": [], "entities": []}, {"text": "Therefore, the classifier applies information specific to the target domain as well as information common to both the target and source domains, but not information specific only to the source domain.", "labels": [], "entities": []}, {"text": "In this section, we describe experiments on using the Twitter Emotion Corpus for emotion classification in the newspaper headlines domain.", "labels": [], "entities": [{"text": "emotion classification", "start_pos": 81, "end_pos": 103, "type": "TASK", "confidence": 0.7369261085987091}]}, {"text": "We applied our binary emotion classifiers on unseen test data from the newspaper headlines domain-the 250-headlines dataset-using each of the following as a training corpus: \u2022 Target-domain data: the 1000-headlines data.", "labels": [], "entities": [{"text": "newspaper headlines domain-the 250-headlines dataset-using", "start_pos": 71, "end_pos": 129, "type": "DATASET", "confidence": 0.8350959897041321}]}, {"text": "\u2022 Source-domain data: the TEC.", "labels": [], "entities": [{"text": "TEC", "start_pos": 26, "end_pos": 29, "type": "DATASET", "confidence": 0.8712009191513062}]}, {"text": "\u2022 Target and Source data: A joint corpus of the 1000-headlines dataset and the TEC.", "labels": [], "entities": [{"text": "TEC", "start_pos": 79, "end_pos": 82, "type": "DATASET", "confidence": 0.9084123969078064}]}, {"text": "Additionally, when using the 'Target and Source' data, we also tested the domain adaptation algorithm proposed in.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 74, "end_pos": 91, "type": "TASK", "confidence": 0.6955435127019882}]}, {"text": "Since the EmotionX class (the positive class) has markedly fewer instances than the NotEmotionX class, we assigned higher weight to instances of the positive class during training.", "labels": [], "entities": []}, {"text": "The rows under I in give the results.", "labels": [], "entities": []}, {"text": "(Row II results are for the experiment described in Section 6, and can be ignored for now.)", "labels": [], "entities": []}, {"text": "We see that the macro-averaged F-score when using target-domain data (row I.a.) is identical to the score obtained by the random baseline (row III).", "labels": [], "entities": [{"text": "F-score", "start_pos": 31, "end_pos": 38, "type": "METRIC", "confidence": 0.9619807600975037}]}, {"text": "However, observe that the precision of the ngram system is higher than the random system, and its recall is lower.", "labels": [], "entities": [{"text": "precision", "start_pos": 26, "end_pos": 35, "type": "METRIC", "confidence": 0.9995218515396118}, {"text": "recall", "start_pos": 98, "end_pos": 104, "type": "METRIC", "confidence": 0.9996472597122192}]}, {"text": "This suggests that the test data has many n-grams not previously seen in the training data.", "labels": [], "entities": []}, {"text": "Observe that as expected, using source-domain data produces much lower scores (row I.b.) than when using target-domain training data (row I.a.).", "labels": [], "entities": []}, {"text": "Using both target-and source-domain data produced significantly better results (row I.c.", "labels": [], "entities": []}, {"text": "We calculated SoA scores for the unigrams and bigrams in the TEC with the six basic emotions.", "labels": [], "entities": [{"text": "TEC", "start_pos": 61, "end_pos": 64, "type": "DATASET", "confidence": 0.939948320388794}]}, {"text": "All ngram-emotion pairs that obtained scores greater than zero were extracted to form the TEC emotion lexicon.", "labels": [], "entities": [{"text": "TEC emotion lexicon", "start_pos": 90, "end_pos": 109, "type": "DATASET", "confidence": 0.6230494578679403}]}, {"text": "We repeated these steps for the 1000-headlines dataset as well.", "labels": [], "entities": []}, {"text": "shows the number of word types in the two automatically generated and the two manually created lexicons.", "labels": [], "entities": []}, {"text": "Observe that the 1000-headlines dataset produces very few entries, whereas the large size of the TEC enables the creation of a substantial emotion lexicon.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Inter-annotator agreement (Pearson's correla- tion) amongst 6 annotators on the 1000-headlines dataset.", "labels": [], "entities": [{"text": "Pearson's correla- tion", "start_pos": 37, "end_pos": 60, "type": "DATASET", "confidence": 0.8770084500312805}]}, {"text": " Table 1. For our experiments, we consid- ered scores greater than 25 to indicate that the head- line expresses the corresponding emotion.", "labels": [], "entities": [{"text": "consid- ered scores", "start_pos": 34, "end_pos": 53, "type": "METRIC", "confidence": 0.9420245140790939}]}, {"text": " Table 3. In the  future, we intend to collect tweets with synonyms of  joy, sadness, fear, etc., as well. # of  % of  hashtag  instances instances  #anger  1,555  7.4  #disgust  761  3.6  #fear  2,816  13.4  #joy  8,240  39.1  #sadness  3,830  18.2  #surprise  3,849  18.3  Total tweets  21,051  100.0  # of tweeters  19,059", "labels": [], "entities": []}, {"text": " Table 4: Cross-validation results on the 1000-headlines dataset. #gold is the number of headlines expressing a partic- ular emotion. #right is the number these instances the classifier correctly marked as expressing the emotion. #guesses  is the number of instances marked as expressing an emotion by the classifier.", "labels": [], "entities": []}, {"text": " Table 5: Cross-validation results on the TEC. The highest F-score is shown in bold.", "labels": [], "entities": [{"text": "the TEC", "start_pos": 38, "end_pos": 45, "type": "DATASET", "confidence": 0.6527661979198456}, {"text": "F-score", "start_pos": 59, "end_pos": 66, "type": "METRIC", "confidence": 0.9978436231613159}]}, {"text": " Table 6: Results on the 250-headlines dataset. The highest F-scores in I and II are shown in bold.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.9916259050369263}]}]}