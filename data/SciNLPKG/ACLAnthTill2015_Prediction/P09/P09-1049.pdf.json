{"title": [{"text": "Bilingual Co-Training for Monolingual Hyponymy-Relation Acquisition", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper proposes a novel framework called bilingual co-training fora large-scale, accurate acquisition method for monolingual semantic knowledge.", "labels": [], "entities": []}, {"text": "In this framework, we combine the independent processes of monolingual semantic-knowledge acquisition for two languages using bilingual resources to boost performance.", "labels": [], "entities": [{"text": "monolingual semantic-knowledge acquisition", "start_pos": 59, "end_pos": 101, "type": "TASK", "confidence": 0.6959574321905772}]}, {"text": "We apply this framework to large-scale hyponymy-relation acquisition from Wikipedia.", "labels": [], "entities": [{"text": "hyponymy-relation acquisition", "start_pos": 39, "end_pos": 68, "type": "TASK", "confidence": 0.6856725960969925}]}, {"text": "Experimental results show that our approach improved the F-measure by 3.6-10.3%.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 57, "end_pos": 66, "type": "METRIC", "confidence": 0.9902453422546387}]}, {"text": "We also show that bilingual co-training enables us to build classi-fiers for two languages in tandem with the same combined amount of data as required for training a single classifier in isolation while achieving superior performance.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "We used the MAY 2008 version of English Wikipedia and the JUNE 2008 version of Japanese Wikipedia for our experiments.", "labels": [], "entities": [{"text": "MAY 2008 version of English Wikipedia", "start_pos": 12, "end_pos": 49, "type": "DATASET", "confidence": 0.9250345925490061}, {"text": "JUNE 2008 version of Japanese Wikipedia", "start_pos": 58, "end_pos": 97, "type": "DATASET", "confidence": 0.9103429814179739}]}, {"text": "24,000 hyponymy-relation candidates, randomly selected in both languages, were manually checked to build training, development, and test sets . Around 8,000 hyponymy relations were found in the manually checked data for both languages . 20,000 of the manually checked data were used as a training set for training the initial classifier.", "labels": [], "entities": []}, {"text": "The rest were equally divided into development and test sets.", "labels": [], "entities": []}, {"text": "The development set was used to select the optimal parameters in bilingual co-training and the test set was used to evaluate our system.", "labels": [], "entities": []}, {"text": "We used TinySVM) with a polynomial kernel of degree 2 as a classifier.", "labels": [], "entities": [{"text": "TinySVM", "start_pos": 8, "end_pos": 15, "type": "DATASET", "confidence": 0.886319100856781}]}, {"text": "The maximum iteration number in the bilingual cotraining was set as 100.", "labels": [], "entities": []}, {"text": "Two parameters, \u03b8 and T opN, were selected through experiments on the development set.", "labels": [], "entities": [{"text": "T opN", "start_pos": 22, "end_pos": 27, "type": "METRIC", "confidence": 0.8998444974422455}]}, {"text": "\u03b8 = 1 and T opN=900 showed We also used redirection links in English and Japanese Wikipedia for recognizing the variations of terms when we built a bilingual instance dictionary with Wikipedia crosslanguage links.", "labels": [], "entities": [{"text": "T opN=900", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9103493392467499}]}, {"text": "It took about two or three months to check them in each language.", "labels": [], "entities": []}, {"text": "Regarding a hyponymy relation as a positive sample and the others as a negative sample for training SVMs, \"positive sample:negative sample\" was about 8,000:16,000=1:2 the best performance and were used as the optimal parameter in the following experiments.", "labels": [], "entities": []}, {"text": "We conducted three experiments to show effects of bilingual co-training, training data size, and bilingual instance dictionaries.", "labels": [], "entities": []}, {"text": "In the first two experiments, we experimented with a bilingual instance dictionary derived from Wikipedia crosslanguage links.", "labels": [], "entities": []}, {"text": "Comparison among systems based on three different bilingual instance dictionaries is shown in the third experiment.", "labels": [], "entities": []}, {"text": "Precision (P ), recall (R), and F 1 -measure (F 1 ), as in Eq, were used as the evaluation measures, where Rel represents a set of manually checked hyponymy relations and HRbyS represents a set of hyponymy-relation candidates classified as hyponymy relations by the system: shows the comparison results of the four systems.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9880226850509644}, {"text": "recall", "start_pos": 16, "end_pos": 22, "type": "METRIC", "confidence": 0.9959779381752014}, {"text": "F 1 -measure (F 1 )", "start_pos": 32, "end_pos": 51, "type": "METRIC", "confidence": 0.9520664066076279}]}, {"text": "SYT represents the  system that we implemented and tested with the same data as ours.", "labels": [], "entities": [{"text": "SYT", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8184123039245605}]}, {"text": "INIT is a system based on initial classifier c 0 in bilingual co-training.", "labels": [], "entities": [{"text": "INIT", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.7113353610038757}]}, {"text": "We translated training data in one language by using our bilingual instance dictionary and added the translation to the existing training data in the other language like bilingual co-training did.", "labels": [], "entities": []}, {"text": "The size of the English and Japanese training data reached 20,729 and 20,486.", "labels": [], "entities": [{"text": "English and Japanese training data", "start_pos": 16, "end_pos": 50, "type": "DATASET", "confidence": 0.6623466193675995}]}, {"text": "We trained initial classifier c 0 with the new training data.", "labels": [], "entities": []}, {"text": "TRAN is a system based on the classifier.", "labels": [], "entities": [{"text": "TRAN", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.7136270999908447}]}, {"text": "BICO is a system based on bilingual co-training.", "labels": [], "entities": [{"text": "BICO", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.6435856223106384}]}], "tableCaptions": [{"text": " Table 2: Performance of different systems (%)", "labels": [], "entities": []}, {"text": " Table 3: F 1 based on training data size:  with/without bilingual co-training (%)", "labels": [], "entities": [{"text": "F", "start_pos": 10, "end_pos": 11, "type": "METRIC", "confidence": 0.9737169742584229}]}, {"text": " Table 4: F 1 based on training data size: when En- glish classifier is strong one", "labels": [], "entities": [{"text": "F", "start_pos": 10, "end_pos": 11, "type": "METRIC", "confidence": 0.9779120683670044}]}, {"text": " Table 5: F 1 based on training data size: when  Japanese classifier is strong one", "labels": [], "entities": [{"text": "F", "start_pos": 10, "end_pos": 11, "type": "METRIC", "confidence": 0.9890533089637756}]}, {"text": " Table 6: Effect of different bilingual instance dic- tionaries", "labels": [], "entities": []}]}