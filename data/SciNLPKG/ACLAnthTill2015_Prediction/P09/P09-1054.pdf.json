{"title": [{"text": "Stochastic Gradient Descent Training for L1-regularized Log-linear Models with Cumulative Penalty", "labels": [], "entities": []}], "abstractContent": [{"text": "Stochastic gradient descent (SGD) uses approximate gradients estimated from subsets of the training data and updates the parameters in an online fashion.", "labels": [], "entities": [{"text": "Stochastic gradient descent (SGD)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.8307125767072042}]}, {"text": "This learning framework is attractive because it often requires much less training time in practice than batch training algorithms.", "labels": [], "entities": []}, {"text": "However, L1-regularization, which is becoming popular in natural language processing because of its ability to produce compact models, cannot be efficiently applied in SGD training, due to the large dimensions of feature vectors and the fluctuations of approximate gradients.", "labels": [], "entities": [{"text": "SGD training", "start_pos": 168, "end_pos": 180, "type": "TASK", "confidence": 0.9160661995410919}]}, {"text": "We present a simple method to solve these problems by penalizing the weights according to cumulative values for L1 penalty.", "labels": [], "entities": []}, {"text": "We evaluate the effectiveness of our method in three applications: text chunking, named entity recognition, and part-of-speech tagging.", "labels": [], "entities": [{"text": "text chunking", "start_pos": 67, "end_pos": 80, "type": "TASK", "confidence": 0.8004202544689178}, {"text": "named entity recognition", "start_pos": 82, "end_pos": 106, "type": "TASK", "confidence": 0.6471381286780039}, {"text": "part-of-speech tagging", "start_pos": 112, "end_pos": 134, "type": "TASK", "confidence": 0.7905032336711884}]}, {"text": "Experimental results demonstrate that our method can produce compact and accurate models much more quickly than a state-of-the-art quasi-Newton method for L1-regularized log-linear models.", "labels": [], "entities": []}], "introductionContent": [{"text": "Log-linear models (a.k.a maximum entropy models) are one of the most widely-used probabilistic models in the field of natural language processing (NLP).", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 118, "end_pos": 151, "type": "TASK", "confidence": 0.7934214274088541}]}, {"text": "The applications range from simple classification tasks such as text classification and history-based tagging) to more complex structured prediction tasks such as partof-speech (POS) tagging (), syntactic parsing () and semantic role labeling ().", "labels": [], "entities": [{"text": "text classification", "start_pos": 64, "end_pos": 83, "type": "TASK", "confidence": 0.7749729752540588}, {"text": "history-based tagging", "start_pos": 88, "end_pos": 109, "type": "TASK", "confidence": 0.6545438170433044}, {"text": "partof-speech (POS) tagging", "start_pos": 163, "end_pos": 190, "type": "TASK", "confidence": 0.6088275074958801}, {"text": "syntactic parsing", "start_pos": 195, "end_pos": 212, "type": "TASK", "confidence": 0.7522896826267242}, {"text": "semantic role labeling", "start_pos": 220, "end_pos": 242, "type": "TASK", "confidence": 0.710599958896637}]}, {"text": "Loglinear models have a major advantage over other discriminative machine learning models such as support vector machines-their probabilistic output allows the information on the confidence of the decision to be used by other components in the text processing pipeline.", "labels": [], "entities": []}, {"text": "The training of log-liner models is typically performed based on the maximum likelihood criterion, which aims to obtain the weights of the features that maximize the conditional likelihood of the training data.", "labels": [], "entities": []}, {"text": "In maximum likelihood training, regularization is normally needed to prevent the model from overfitting the training data, The two most common regularization methods are called L1 and L2 regularization.", "labels": [], "entities": []}, {"text": "L1 regularization penalizes the weight vector for its L1-norm (i.e. the sum of the absolute values of the weights), whereas L2 regularization uses its L2-norm.", "labels": [], "entities": []}, {"text": "There is usually not a considerable difference between the two methods in terms of the accuracy of the resulting model ( ), but L1 regularization has a significant advantage in practice.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 87, "end_pos": 95, "type": "METRIC", "confidence": 0.9992254972457886}]}, {"text": "Because many of the weights of the features become zero as a result of L1-regularized training, the size of the model can be much smaller than that produced by L2-regularization.", "labels": [], "entities": []}, {"text": "Compact models require less space on memory and storage, and enable the application to startup quickly.", "labels": [], "entities": []}, {"text": "These merits can be of vital importance when the application is deployed in resource-tight environments such as cell-phones.", "labels": [], "entities": []}, {"text": "A common way to train a large-scale L1-regularized model is to use a quasi-Newton method.", "labels": [], "entities": []}, {"text": "describe a method for training a L1-regularized log-linear model with abound constrained version of the BFGS algorithm.", "labels": [], "entities": []}, {"text": "present an algorithm called OrthantWise Limited-memory Quasi-Newton (OWL-QN), which can work on the BFGS algorithm without bound constraints and achieve faster convergence.", "labels": [], "entities": [{"text": "BFGS algorithm", "start_pos": 100, "end_pos": 114, "type": "DATASET", "confidence": 0.8726720213890076}]}, {"text": "An alternative approach to training a log-linear model is to use stochastic gradient descent (SGD) methods.", "labels": [], "entities": [{"text": "stochastic gradient descent (SGD)", "start_pos": 65, "end_pos": 98, "type": "TASK", "confidence": 0.7293862104415894}]}, {"text": "SGD uses approximate gradients estimated from subsets of the training data and updates the weights of the features in an online fashion-the weights are updated much more frequently than batch training algorithms.", "labels": [], "entities": []}, {"text": "This learning framework is attracting attention because it often requires much less training time in practice than batch training algorithms, especially when the training data is large and redundant.", "labels": [], "entities": []}, {"text": "SGD was recently used for NLP tasks including machine translation () and syntactic parsing.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 46, "end_pos": 65, "type": "TASK", "confidence": 0.820301741361618}, {"text": "syntactic parsing", "start_pos": 73, "end_pos": 90, "type": "TASK", "confidence": 0.7590564489364624}]}, {"text": "Also, SGD is very easy to implement because it does not need to use the Hessian information on the objective function.", "labels": [], "entities": [{"text": "SGD", "start_pos": 6, "end_pos": 9, "type": "TASK", "confidence": 0.9427215456962585}]}, {"text": "The implementation could be as simple as the perceptron algorithm.", "labels": [], "entities": []}, {"text": "Although SGD is a very attractive learning framework, the direct application of L1 regularization in this learning framework does not result in efficient training.", "labels": [], "entities": []}, {"text": "The first problem is the inefficiency of applying the L1 penalty to the weights of all features.", "labels": [], "entities": []}, {"text": "In NLP applications, the dimension of the feature space tends to be very large-it can easily become several millions, so the application of L1 penalty to all features significantly slows down the weight updating process.", "labels": [], "entities": []}, {"text": "The second problem is that the naive application of L1 penalty in SGD does not always lead to compact models, because the approximate gradient used at each update is very noisy, so the weights of the features can be easily moved away from zero by those fluctuations.", "labels": [], "entities": []}, {"text": "In this paper, we present a simple method for solving these two problems in SGD learning.", "labels": [], "entities": [{"text": "SGD learning", "start_pos": 76, "end_pos": 88, "type": "TASK", "confidence": 0.9642786979675293}]}, {"text": "The main idea is to keep track of the total penalty and the penalty that has been applied to each weight, so that the L1 penalty is applied based on the difference between those cumulative values.", "labels": [], "entities": [{"text": "L1 penalty", "start_pos": 118, "end_pos": 128, "type": "METRIC", "confidence": 0.9253446459770203}]}, {"text": "That way, the application of L1 penalty is needed only for the features that are used in the current sample, and also the effect of noisy gradient is smoothed away.", "labels": [], "entities": []}, {"text": "We evaluate the effectiveness of our method by using linear-chain conditional random fields (CRFs) and three traditional NLP tasks, namely, text chunking (shallow parsing), named entity recognition, and POS tagging.", "labels": [], "entities": [{"text": "text chunking (shallow parsing", "start_pos": 140, "end_pos": 170, "type": "TASK", "confidence": 0.6714659690856933}, {"text": "named entity recognition", "start_pos": 173, "end_pos": 197, "type": "TASK", "confidence": 0.6508044302463531}, {"text": "POS tagging", "start_pos": 203, "end_pos": 214, "type": "TASK", "confidence": 0.8289592862129211}]}, {"text": "We show that our enhanced SGD learning method can produce compact and accurate models much more quickly than the OWL-QN algorithm.", "labels": [], "entities": [{"text": "SGD learning", "start_pos": 26, "end_pos": 38, "type": "TASK", "confidence": 0.9137053489685059}]}, {"text": "This paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 provides a general description of log-linear models used in NLP.", "labels": [], "entities": []}, {"text": "Section 3 describes our stochastic gradient descent method for L1-regularized loglinear models.", "labels": [], "entities": [{"text": "stochastic gradient descent", "start_pos": 24, "end_pos": 51, "type": "TASK", "confidence": 0.6560703615347544}]}, {"text": "Experimental results are presented in Section 4.", "labels": [], "entities": []}, {"text": "Some related work is discussed in Section 5.", "labels": [], "entities": []}, {"text": "Section 6 gives some concluding remarks.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate the effectiveness our training algorithm using linear-chain CRF models and three NLP tasks: text chunking, named entity recognition, and POS tagging.", "labels": [], "entities": [{"text": "text chunking", "start_pos": 104, "end_pos": 117, "type": "TASK", "confidence": 0.7758753299713135}, {"text": "named entity recognition", "start_pos": 119, "end_pos": 143, "type": "TASK", "confidence": 0.643986572821935}, {"text": "POS tagging", "start_pos": 149, "end_pos": 160, "type": "TASK", "confidence": 0.8041903376579285}]}, {"text": "To compare our algorithm with the state-of-theart, we present the performance of the OWL-QN algorithm on the same data.", "labels": [], "entities": []}, {"text": "We used the publicly available OWL-QN optimizer developed by Andrew and Gao.", "labels": [], "entities": []}, {"text": "The meta-parameters for learning were left unchanged from the default settings of the software: the convergence tolerance was 1e-4; and the L-BFGS memory parameter was 10.", "labels": [], "entities": [{"text": "convergence tolerance", "start_pos": 100, "end_pos": 121, "type": "METRIC", "confidence": 0.9794977307319641}, {"text": "L-BFGS memory parameter", "start_pos": 140, "end_pos": 163, "type": "METRIC", "confidence": 0.867246131102244}]}], "tableCaptions": [{"text": " Table 1: CoNLL-2000 Chunking task. Training time and accuracy of the trained model on the test data.", "labels": [], "entities": [{"text": "CoNLL-2000 Chunking task", "start_pos": 10, "end_pos": 34, "type": "DATASET", "confidence": 0.7200173338254293}, {"text": "accuracy", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.9994338154792786}]}, {"text": " Table 2: NLPBA 2004 Named entity recognition task. Training time and accuracy of the trained model  on the test data.", "labels": [], "entities": [{"text": "NLPBA 2004 Named entity recognition task", "start_pos": 10, "end_pos": 50, "type": "TASK", "confidence": 0.8499477307001749}, {"text": "accuracy", "start_pos": 70, "end_pos": 78, "type": "METRIC", "confidence": 0.9993878602981567}]}, {"text": " Table 3: POS tagging on the WSJ corpus. Training time and accuracy of the trained model on the test  data.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.7248872965574265}, {"text": "WSJ corpus", "start_pos": 29, "end_pos": 39, "type": "DATASET", "confidence": 0.9502373039722443}, {"text": "accuracy", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.9995132684707642}]}]}