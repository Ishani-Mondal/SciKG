{"title": [{"text": "Reducing semantic drift with bagging and distributional similarity", "labels": [], "entities": [{"text": "distributional similarity", "start_pos": 41, "end_pos": 66, "type": "TASK", "confidence": 0.6764881312847137}]}], "abstractContent": [{"text": "Iterative bootstrapping algorithms are typically compared using a single set of hand-picked seeds.", "labels": [], "entities": []}, {"text": "However, we demonstrate that performance varies greatly depending on these seeds, and favourable seeds for one algorithm can perform very poorly with others, making comparisons unreliable.", "labels": [], "entities": []}, {"text": "We exploit this wide variation with bagging, sampling from automatically extracted seeds to reduce semantic drift.", "labels": [], "entities": []}, {"text": "However, semantic drift still occurs in later iterations.", "labels": [], "entities": [{"text": "semantic drift", "start_pos": 9, "end_pos": 23, "type": "TASK", "confidence": 0.8307370841503143}]}, {"text": "We propose an integrated distributional similarity filter to identify and censor potential semantic drifts, ensuring over 10% higher precision when extracting large semantic lexicons.", "labels": [], "entities": [{"text": "precision", "start_pos": 133, "end_pos": 142, "type": "METRIC", "confidence": 0.9986860156059265}]}], "introductionContent": [{"text": "Iterative bootstrapping algorithms have been proposed to extract semantic lexicons for NLP tasks with limited linguistic resources.", "labels": [], "entities": []}, {"text": "Bootstrapping was initially proposed by, and has since been successfully applied to extracting general semantic lexicons (), biomedical entities (, facts), and coreference data.", "labels": [], "entities": [{"text": "extracting general semantic lexicons", "start_pos": 84, "end_pos": 120, "type": "TASK", "confidence": 0.8369411677122116}]}, {"text": "Bootstrapping approaches are attractive because they are domain and language independent, require minimal linguistic pre-processing and can be applied to raw text, and are efficient enough for tera-scale extraction).", "labels": [], "entities": [{"text": "tera-scale extraction", "start_pos": 193, "end_pos": 214, "type": "TASK", "confidence": 0.7960360646247864}]}, {"text": "Bootstrapping is minimally supervised, as it is initialised with a small number of seed instances of the information to extract.", "labels": [], "entities": []}, {"text": "For semantic lexicons, these seeds are terms from the category of interest.", "labels": [], "entities": []}, {"text": "The seeds identify contextual patterns that express a particular semantic category, which in turn recognise new terms ().", "labels": [], "entities": []}, {"text": "Unfortunately, semantic drift often occurs when ambiguous or erroneous terms and/or patterns are introduced into and then dominate the iterative process (.", "labels": [], "entities": [{"text": "semantic drift", "start_pos": 15, "end_pos": 29, "type": "TASK", "confidence": 0.8042157590389252}]}, {"text": "Bootstrapping algorithms are typically compared using only a single set of hand-picked seeds.", "labels": [], "entities": []}, {"text": "We first show that different seeds cause these algorithms to generate diverse lexicons which vary greatly in precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 109, "end_pos": 118, "type": "METRIC", "confidence": 0.9940676689147949}]}, {"text": "This makes evaluation unreliable -seeds which perform well on one algorithm can perform surprisingly poorly on another.", "labels": [], "entities": []}, {"text": "In fact, random gold-standard seeds often outperform seeds carefully chosen by domain experts.", "labels": [], "entities": []}, {"text": "Our second contribution exploits this diversity we have identified.", "labels": [], "entities": []}, {"text": "We present an unsupervised bagging algorithm which samples from the extracted lexicon rather than relying on existing gazetteers or hand-selected seeds.", "labels": [], "entities": []}, {"text": "Each sample is then fed back as seeds to the bootstrapper and the results combined using voting.", "labels": [], "entities": []}, {"text": "This both improves the precision of the lexicon and the robustness of the algorithms to the choice of initial seeds.", "labels": [], "entities": [{"text": "precision", "start_pos": 23, "end_pos": 32, "type": "METRIC", "confidence": 0.9989426732063293}]}, {"text": "Unfortunately, semantic drift still dominates in later iterations, since erroneous extracted terms and/or patterns eventually shift the category's direction.", "labels": [], "entities": []}, {"text": "Our third contribution focuses on detecting and censoring the terms introduced by semantic drift.", "labels": [], "entities": []}, {"text": "We integrate a distributional similarity filter directly into WMEB.", "labels": [], "entities": [{"text": "WMEB", "start_pos": 62, "end_pos": 66, "type": "DATASET", "confidence": 0.7712092399597168}]}, {"text": "This filter judges whether anew term is more similar to the earlier or most recently extracted terms, a sign of potential semantic drift.", "labels": [], "entities": []}, {"text": "We demonstrate these methods for extracting biomedical semantic lexicons using two bootstrapping algorithms.", "labels": [], "entities": [{"text": "extracting biomedical semantic lexicons", "start_pos": 33, "end_pos": 72, "type": "TASK", "confidence": 0.8446578681468964}]}, {"text": "Our unsupervised bagging approach outperforms carefully hand-picked seeds by \u223c 10% in later iterations.", "labels": [], "entities": []}, {"text": "Our distributional similarity filter gives a similar performance improvement.", "labels": [], "entities": []}, {"text": "This allows us to produce large lexicons accurately and efficiently for domain-specific language processing.", "labels": [], "entities": []}], "datasetContent": [{"text": "In our experiments we consider the task of extracting biomedical semantic lexicons from raw text using BASILISK and WMEB.", "labels": [], "entities": [{"text": "extracting biomedical semantic lexicons from raw text", "start_pos": 43, "end_pos": 96, "type": "TASK", "confidence": 0.7665900588035583}, {"text": "BASILISK", "start_pos": 103, "end_pos": 111, "type": "METRIC", "confidence": 0.8755804300308228}, {"text": "WMEB", "start_pos": 116, "end_pos": 120, "type": "DATASET", "confidence": 0.8558931350708008}]}, {"text": "The evaluation involves manually inspecting each extracted term and judging whether it was a member of the semantic class.", "labels": [], "entities": []}, {"text": "This manual evaluation is extremely time consuming and is necessary due to the limited coverage of biomedical resources.", "labels": [], "entities": []}, {"text": "To make later evaluations more efficient, all evaluators' decisions for each category are cached.", "labels": [], "entities": []}, {"text": "Unfamiliar terms were checked using online resources including MEDLINE, Medical Subject Headings (MeSH), Wikipedia.", "labels": [], "entities": [{"text": "MEDLINE", "start_pos": 63, "end_pos": 70, "type": "DATASET", "confidence": 0.9406078457832336}, {"text": "Medical Subject Headings (MeSH)", "start_pos": 72, "end_pos": 103, "type": "TASK", "confidence": 0.6650862296422323}]}, {"text": "Each ambiguous term was counted as correct if it was classified into one of its correct categories, such as lymphoma which is a TUMR and DISE.", "labels": [], "entities": [{"text": "TUMR", "start_pos": 128, "end_pos": 132, "type": "METRIC", "confidence": 0.9673604965209961}, {"text": "DISE", "start_pos": 137, "end_pos": 141, "type": "METRIC", "confidence": 0.8641447424888611}]}, {"text": "If a term was unambiguously part of a multi-word term we considered it correct.", "labels": [], "entities": []}, {"text": "Abbreviations, acronyms and typographical variations were included.", "labels": [], "entities": []}, {"text": "We also considered obvious spelling mistakes to be correct, such as nuetrophils instead of neutrophils (a type of CELL).", "labels": [], "entities": []}, {"text": "Non-specific modifiers are marked as incorrect, for example, gastrointestinal maybe incorrectly extracted for TUMR, as part of the entity gastrointestinal carcinoma.", "labels": [], "entities": [{"text": "TUMR", "start_pos": 110, "end_pos": 114, "type": "METRIC", "confidence": 0.9242547154426575}]}, {"text": "However, the modifier may also be used for DISE (gastrointestinal infection) and CELL.", "labels": [], "entities": [{"text": "DISE", "start_pos": 43, "end_pos": 47, "type": "METRIC", "confidence": 0.8022577166557312}, {"text": "CELL", "start_pos": 81, "end_pos": 85, "type": "METRIC", "confidence": 0.7687423229217529}]}, {"text": "The terms were evaluated by two domain experts.", "labels": [], "entities": []}, {"text": "Inter-annotator agreement was measured on the top-100 terms extracted by BASILISK and WMEB with the hand-picked seeds for each category.", "labels": [], "entities": [{"text": "agreement", "start_pos": 16, "end_pos": 25, "type": "METRIC", "confidence": 0.6350072622299194}, {"text": "BASILISK", "start_pos": 73, "end_pos": 81, "type": "METRIC", "confidence": 0.8215426802635193}, {"text": "WMEB", "start_pos": 86, "end_pos": 90, "type": "DATASET", "confidence": 0.871632993221283}]}, {"text": "All disagreements were discussed, and the kappa scores, before (\u03ba 1 ) and after (\u03ba 2 ) the discussions, are shown in.", "labels": [], "entities": []}, {"text": "Each score is above 0.8 which reflects an agreement strength of \"almost perfect\" (.", "labels": [], "entities": [{"text": "agreement strength", "start_pos": 42, "end_pos": 60, "type": "METRIC", "confidence": 0.9572509825229645}]}, {"text": "For comparing the accuracy of the systems we evaluated the precision of samples of the lexicons extracted for each category.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9989597797393799}, {"text": "precision", "start_pos": 59, "end_pos": 68, "type": "METRIC", "confidence": 0.9991393089294434}]}, {"text": "We report average precision over the 10 semantic categories on the 1-200, 401-600 and 801-1000 term samples, and over the first 1000 terms.", "labels": [], "entities": [{"text": "precision", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.9892802834510803}]}, {"text": "In each algorithm, each category is initialised with 5 seed terms, and the number of patterns, k, is set to 5.", "labels": [], "entities": []}, {"text": "In each iteration, 5 lexicon terms are extracted by each category.", "labels": [], "entities": []}, {"text": "Each algorithm is run for 200 iterations.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Filtered 5-gram dataset statistics.", "labels": [], "entities": []}, {"text": " Table 3: Variation in precision with random gold  seed sets", "labels": [], "entities": [{"text": "precision", "start_pos": 23, "end_pos": 32, "type": "METRIC", "confidence": 0.9966828227043152}]}, {"text": " Table 4: Bagging with 50 gold seed sets", "labels": [], "entities": [{"text": "Bagging", "start_pos": 10, "end_pos": 17, "type": "TASK", "confidence": 0.9926435351371765}]}, {"text": " Table 5: Bagging with 50 unsupervised seed sets", "labels": [], "entities": [{"text": "Bagging", "start_pos": 10, "end_pos": 17, "type": "TASK", "confidence": 0.9949789643287659}]}, {"text": " Table 6: Semantic drift detection results", "labels": [], "entities": [{"text": "Semantic drift detection", "start_pos": 10, "end_pos": 34, "type": "TASK", "confidence": 0.8927512963612875}]}, {"text": " Table 7: Final accuracy with drift detection", "labels": [], "entities": [{"text": "Final", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.907712996006012}, {"text": "accuracy", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.9421064257621765}]}]}