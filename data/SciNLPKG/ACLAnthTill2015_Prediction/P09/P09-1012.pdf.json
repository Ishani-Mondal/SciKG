{"title": [{"text": "Bayesian Unsupervised Word Segmentation with Nested Pitman-Yor Language Modeling", "labels": [], "entities": [{"text": "Bayesian Unsupervised Word Segmentation", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.4770952984690666}, {"text": "Nested Pitman-Yor Language Modeling", "start_pos": 45, "end_pos": 80, "type": "TASK", "confidence": 0.5995929315686226}]}], "abstractContent": [{"text": "In this paper, we propose anew Bayesian model for fully unsupervised word seg-mentation and an efficient blocked Gibbs sampler combined with dynamic programming for inference.", "labels": [], "entities": []}, {"text": "Our model is a nested hierarchical Pitman-Yor language model, where Pitman-Yor spelling model is embedded in the word model.", "labels": [], "entities": []}, {"text": "We confirmed that it significantly outperforms previous reported results in both phonetic transcripts and standard datasets for Chinese and Japanese word segmentation.", "labels": [], "entities": [{"text": "Chinese and Japanese word segmentation", "start_pos": 128, "end_pos": 166, "type": "TASK", "confidence": 0.6168871462345124}]}, {"text": "Our model is also considered as away to construct an accurate word n-gram language model directly from characters of arbitrary language, without any \"word\" indications.", "labels": [], "entities": []}], "introductionContent": [{"text": "\"Word\" is no trivial concept in many languages.", "labels": [], "entities": []}, {"text": "Asian languages such as Chinese and Japanese have no explicit word boundaries, thus word segmentation is a crucial first step when processing them.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 84, "end_pos": 101, "type": "TASK", "confidence": 0.7093516886234283}]}, {"text": "Even in western languages, valid \"words\" are often not identical to space-separated tokens.", "labels": [], "entities": []}, {"text": "For example, proper nouns such as \"United Kingdom\" or idiomatic phrases such as \"with respect to\" actually function as a single word, and we often condense them into the virtual words \"UK\" and \"w.r.t.\".", "labels": [], "entities": []}, {"text": "In order to extract \"words\" from text streams, unsupervised word segmentation is an important research area because the criteria for creating supervised training data could be arbitrary, and will be suboptimal for applications that rely on segmentations.", "labels": [], "entities": [{"text": "unsupervised word segmentation", "start_pos": 47, "end_pos": 77, "type": "TASK", "confidence": 0.7090917030970255}]}, {"text": "It is particularly difficult to create \"correct\" training data for speech transcripts, colloquial texts, and classics where segmentations are often ambiguous, let alone is impossible for unknown languages whose properties computational linguists might seek to uncover.", "labels": [], "entities": []}, {"text": "From a scientific point of view, it is also interesting because it can shed light on how children learn \"words\" without the explicitly given boundaries for every word, which is assumed by supervised learning approaches.", "labels": [], "entities": []}, {"text": "Lately, model-based methods have been introduced for unsupervised segmentation, in particular those based on Dirichlet processes on words;.", "labels": [], "entities": []}, {"text": "This maximizes the probability of word segmentation w given a string s : \u02c6 w = argmax w p(w|s) . This approach often implicitly includes heuristic criteria proposed so far 1 , while having a clear statistical semantics to find the most probable word segmentation that will maximize the probability of the data, here the strings.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 34, "end_pos": 51, "type": "TASK", "confidence": 0.7043507248163223}]}, {"text": "However, they are still na\u00a8\u0131vena\u00a8\u0131ve with respect to word spellings, and the inference is very slow owing to inefficient Gibbs sampling.", "labels": [], "entities": [{"text": "inference", "start_pos": 77, "end_pos": 86, "type": "METRIC", "confidence": 0.979836642742157}]}, {"text": "Crucially, since they rely on sampling a word boundary between two neighboring words, they can leverage only up to bigram word dependencies.", "labels": [], "entities": []}, {"text": "In this paper, we extend this work to propose a more efficient and accurate unsupervised word segmentation that will optimize the performance of the word n-gram Pitman-Yor (i.e. Bayesian Kneser-Ney) language model, with an accurate character \u221e-gram Pitman-Yor spelling model embedded in word models.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 89, "end_pos": 106, "type": "TASK", "confidence": 0.7688947319984436}]}, {"text": "Furthermore, it can be viewed as a method for building a high-performance n-gram language model directly from character strings of arbitrary language.", "labels": [], "entities": []}, {"text": "It is carefully smoothed and has no \"unknown words\" problem, resulting from its model structure.", "labels": [], "entities": []}, {"text": "This paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2, 1 For instance, TANGO algorithm ( essentially finds segments such that character n-gram probabilities are maximized blockwise, averaged over n.", "labels": [], "entities": [{"text": "TANGO", "start_pos": 30, "end_pos": 35, "type": "METRIC", "confidence": 0.9527892470359802}]}, {"text": "(a) Generating n-gram distributions G hierarchically from the Pitman-Yor process.", "labels": [], "entities": []}, {"text": "Here, n = 3.", "labels": [], "entities": []}, {"text": "(b) Equivalent representation using a hierarchical Chinese Restaurant process.", "labels": [], "entities": [{"text": "Equivalent representation", "start_pos": 4, "end_pos": 29, "type": "TASK", "confidence": 0.7637003660202026}]}, {"text": "Each word in a training text is a \"customer\" shown in italic, and added to the leaf of its two words context.", "labels": [], "entities": []}, {"text": "we briefly describe a language model based on the Pitman-Yor process), which is a generalization of the Dirichlet process used in previous research.", "labels": [], "entities": []}, {"text": "By embedding a character n-gram in word n-gram from a Bayesian perspective, Section 3 introduces a novel language model for word segmentation, which we call the Nested PitmanYor language model.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 124, "end_pos": 141, "type": "TASK", "confidence": 0.7434814274311066}]}, {"text": "Section 4 describes an efficient blocked Gibbs sampler that leverages dynamic programming for inference.", "labels": [], "entities": []}, {"text": "In Section 5 we describe experiments on the standard datasets in Chinese and Japanese in addition to English phonetic transcripts, and semi-supervised experiments are also explored.", "labels": [], "entities": []}, {"text": "Section 6 is a discussion and Section 7 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "To validate our model, we conducted experiments on standard datasets for Chinese and Japanese word segmentation that are publicly available, as well as the same dataset used in ().", "labels": [], "entities": [{"text": "Chinese and Japanese word segmentation", "start_pos": 73, "end_pos": 111, "type": "TASK", "confidence": 0.6377239763736725}]}, {"text": "Note that NPYLM maximizes the probability of strings, equivalently, minimizes the perplexity per character.", "labels": [], "entities": []}, {"text": "Therefore, the recovery of the \"ground truth\" that is not available for inference is a byproduct in unsupervised learning.", "labels": [], "entities": []}, {"text": "Since our implementation is based on Unicode and learns all hyperparameters from the data, we also confirmed that NPYLM segments the Arabic Gigawords equally well.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Segmentation accuracies on English pho- netic transcripts. NPY(n) means n-gram NPYLM.  Results for HDP(2) are taken from Goldwater et  al. (2009), which corrects the errors in Goldwater  et al. (2006).", "labels": [], "entities": []}, {"text": " Table 3: Accuracies and perplexities per character  (in parentheses) on actual corpora. \"ZK08\" are the  best results reported in", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9978594183921814}]}]}