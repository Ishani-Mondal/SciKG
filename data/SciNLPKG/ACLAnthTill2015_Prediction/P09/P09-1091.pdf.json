{"title": [], "abstractContent": [{"text": "This paper describes log-linear models fora general-purpose sentence realizer based on dependency structures.", "labels": [], "entities": [{"text": "general-purpose sentence realizer", "start_pos": 44, "end_pos": 77, "type": "TASK", "confidence": 0.6328369975090027}]}, {"text": "Unlike traditional realiz-ers using grammar rules, our method realizes sentences by linearizing dependency relations directly in two steps.", "labels": [], "entities": []}, {"text": "First, the relative order between head and each dependent is determined by their dependency relation.", "labels": [], "entities": []}, {"text": "Then the best linearizations compatible with the relative order are selected by log-linear models.", "labels": [], "entities": []}, {"text": "The log-linear models incorporate three types of feature functions, including dependency relations , surface words and headwords.", "labels": [], "entities": []}, {"text": "Our approach to sentence realization provides simplicity , efficiency and competitive accuracy.", "labels": [], "entities": [{"text": "sentence realization", "start_pos": 16, "end_pos": 36, "type": "TASK", "confidence": 0.8193036615848541}, {"text": "accuracy", "start_pos": 86, "end_pos": 94, "type": "METRIC", "confidence": 0.9741000533103943}]}, {"text": "Trained on 8,975 dependency structures of a Chinese Dependency Treebank, the realizer achieves a BLEU score of 0.8874.", "labels": [], "entities": [{"text": "Chinese Dependency Treebank", "start_pos": 44, "end_pos": 71, "type": "DATASET", "confidence": 0.9476186633110046}, {"text": "BLEU score", "start_pos": 97, "end_pos": 107, "type": "METRIC", "confidence": 0.9824331700801849}]}], "introductionContent": [{"text": "Sentence realization can be described as the process of converting the semantic and syntactic representation of a sentence or series of sentences into meaningful, grammatically correct and fluent text of a particular language.", "labels": [], "entities": [{"text": "Sentence realization", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.9359503090381622}]}, {"text": "Most previous general-purpose realization systems are developed via the application of a set of grammar rules based on particular linguistic theories, e.g. Lexical Functional Grammar (LFG), Head Driven Phrase Structure Grammar (HPSG), Combinatory Categorical Grammar (CCG), Tree Adjoining Grammar (TAG) etc.", "labels": [], "entities": []}, {"text": "The grammar rules are either developed by hand, such as those used in), OpenCCG) and XLE (, or extracted automatically from annotated corpora, like the HPSG (), LFG) and CCG () resources derived from the Penn-II Treebank.", "labels": [], "entities": [{"text": "OpenCCG", "start_pos": 72, "end_pos": 79, "type": "DATASET", "confidence": 0.9347049593925476}, {"text": "Penn-II Treebank", "start_pos": 204, "end_pos": 220, "type": "DATASET", "confidence": 0.9885950088500977}]}, {"text": "Over the last decade, there has been a lot of interest in a generate-and-select paradigm for surface realization.", "labels": [], "entities": [{"text": "surface realization", "start_pos": 93, "end_pos": 112, "type": "TASK", "confidence": 0.7837365567684174}]}, {"text": "The paradigm is characterized by a separation between realization and selection, in which rule-based methods are used to generate a space of possible paraphrases, and statistical methods are used to select the most likely realization from the space.", "labels": [], "entities": []}, {"text": "Usually, two statistical models are used to rank the output candidates.", "labels": [], "entities": []}, {"text": "One is n-gram model over different units, such as word-level bigram/trigram models), or factored language models integrated with syntactic tags ().", "labels": [], "entities": []}, {"text": "The other is log-linear model with different syntactic and semantic features (.", "labels": [], "entities": []}, {"text": "However, little work has been done on probabilistic models learning direct mapping from input to surface strings, without the effort to construct a grammar.", "labels": [], "entities": []}, {"text": "develop a general-purpose realizer couched in the framework of Lexical Functional Grammar based on simple n-gram models.", "labels": [], "entities": [{"text": "Lexical Functional Grammar", "start_pos": 63, "end_pos": 89, "type": "TASK", "confidence": 0.685229500134786}]}, {"text": "present a dependency-spanning tree algorithm for word ordering, which first builds dependency trees to decide linear precedence between heads and modifiers then uses an n-gram language model to order siblings.", "labels": [], "entities": [{"text": "word ordering", "start_pos": 49, "end_pos": 62, "type": "TASK", "confidence": 0.7809149026870728}]}, {"text": "Compared with n-gram model, log-linear model is more powerful in that it is easy to integrate a variety of features, and to tune feature weights to maximize the probability.", "labels": [], "entities": []}, {"text": "A few papers have presented maximum entropy models for word or phrase ordering.", "labels": [], "entities": [{"text": "word or phrase ordering", "start_pos": 55, "end_pos": 78, "type": "TASK", "confidence": 0.5971827656030655}]}, {"text": "However, those attempts have been limited to specialized applications, such as air travel reservation or ordering constituents of a main clause in German.", "labels": [], "entities": [{"text": "air travel reservation", "start_pos": 79, "end_pos": 101, "type": "TASK", "confidence": 0.6127633154392242}]}, {"text": "This paper presents a general-purpose realizer based on log-linear models for directly linearizing dependency relations given dependency structures.", "labels": [], "entities": []}, {"text": "We reduce the generation space by two techniques: the first is dividing the entire dependency tree into one-depth sub-trees and solving linearization in sub-trees; the second is the determination of relative positions between dependents and heads according to dependency relations.", "labels": [], "entities": []}, {"text": "Then the best linearization for each sub-tree is selected by the log-linear model that incorporates three types of feature functions, including dependency relations, surface words and headwords.", "labels": [], "entities": []}, {"text": "The evaluation shows that our realizer achieves competitive generation accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 71, "end_pos": 79, "type": "METRIC", "confidence": 0.991746187210083}]}, {"text": "The paper is structured as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we describe the idea of dividing the realization procedure for an entire dependency tree into a series of sub-procedures for sub-trees.", "labels": [], "entities": []}, {"text": "We describe how to determine the relative positions between dependents and heads according to dependency relations in Section 3.", "labels": [], "entities": []}, {"text": "Section 4 gives details of the log-linear model and the feature functions used for sentence realization.", "labels": [], "entities": [{"text": "sentence realization", "start_pos": 83, "end_pos": 103, "type": "TASK", "confidence": 0.7560732662677765}]}, {"text": "Section 5 explains the experiments and provides the results.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our experiments are carried out on HIT-CDT.", "labels": [], "entities": [{"text": "HIT-CDT", "start_pos": 35, "end_pos": 42, "type": "DATASET", "confidence": 0.952021598815918}]}, {"text": "We randomly select 526 sentences as the test set, and 499 sentences as the development set for optimizing the model parameters.", "labels": [], "entities": []}, {"text": "The rest 8,975 sentences of the HIT-CDT are used for training of the dependency relation model.", "labels": [], "entities": [{"text": "HIT-CDT", "start_pos": 32, "end_pos": 39, "type": "DATASET", "confidence": 0.8847384452819824}]}, {"text": "For training of word models, we use the Xinhua News part (6,879,644 words) of Chinese Gigaword Second Edition (LDC2005T14), segmented by the Language Technology Platform (LTP) . And for training the headword model, we use both the HIT-CDT and the HIT Chinese Skeletal Dependency Treebank (HIT-CSDT).", "labels": [], "entities": [{"text": "Xinhua News part (6,879,644 words) of Chinese Gigaword Second Edition (LDC2005T14)", "start_pos": 40, "end_pos": 122, "type": "DATASET", "confidence": 0.8464044769605}, {"text": "HIT Chinese Skeletal Dependency Treebank (HIT-CSDT)", "start_pos": 247, "end_pos": 298, "type": "DATASET", "confidence": 0.689719446003437}]}, {"text": "HIT-CSDT is a component of LTP and contains 49,991 sentences in dependency structure representation (without dependency relation labels).", "labels": [], "entities": [{"text": "HIT-CSDT", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.8403593301773071}, {"text": "dependency structure representation", "start_pos": 64, "end_pos": 99, "type": "TASK", "confidence": 0.6792888542016348}]}, {"text": "As the input dependency representation does not contain punctuation information, we simply remove all punctuation marks in the test and development sets.", "labels": [], "entities": []}, {"text": "In addition to BLEU score, percentage of exactly matched sentences and average NIST simple string accuracy (SSA) are adopted as evaluation metrics.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 15, "end_pos": 25, "type": "METRIC", "confidence": 0.9798356592655182}, {"text": "NIST simple string accuracy (SSA)", "start_pos": 79, "end_pos": 112, "type": "METRIC", "confidence": 0.6880393283707755}]}, {"text": "The exact match measure is percentage of the generated string that exactly matches the corresponding reference sentence.", "labels": [], "entities": [{"text": "exact match measure", "start_pos": 4, "end_pos": 23, "type": "METRIC", "confidence": 0.7475506265958151}]}, {"text": "The average NIST simple string accuracy score reflects the average number of insertion (I), deletion (D), and substitution (S) errors between the output sentence and the reference sentence.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.8542838096618652}, {"text": "insertion (I), deletion (D), and substitution (S) errors", "start_pos": 77, "end_pos": 133, "type": "METRIC", "confidence": 0.8344940207898617}]}, {"text": "Formally, SSA = 1 -(I + D + S) / R, where R is the number of tokens in the reference sentence.", "labels": [], "entities": []}, {"text": "All the evaluation results are shown in.", "labels": [], "entities": []}, {"text": "The first experiment, which is a baseline experiment, ignores the tree structure and randomly chooses position for every word.", "labels": [], "entities": []}, {"text": "From the second experiment, we begin to utilize the tree structure and apply the realization algorithm described in Section 4.4.", "labels": [], "entities": []}, {"text": "In the second experiment, predependents are distinguished from postdependents by the relative position determination method (RPD), then the orders inside predependents and postdependents are chosen randomly.", "labels": [], "entities": [{"text": "relative position determination", "start_pos": 85, "end_pos": 116, "type": "TASK", "confidence": 0.563264379898707}]}, {"text": "From the third experiments, the log-linear models are used for scoring the generated sequences, with the aid of three types of feature functions as de-: BLEU, ExMatch and SSA scores on the test set functions incrementally based on the RPD and DR model.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 153, "end_pos": 157, "type": "METRIC", "confidence": 0.9988903403282166}, {"text": "ExMatch", "start_pos": 159, "end_pos": 166, "type": "METRIC", "confidence": 0.8573666214942932}]}, {"text": "The relative position determination plays an important role in the realization algorithm.", "labels": [], "entities": [{"text": "realization", "start_pos": 67, "end_pos": 78, "type": "TASK", "confidence": 0.9805970788002014}]}, {"text": "We observe that the BLEU score is boosted from 0.1478 to 0.5943 by using the RPD method.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 20, "end_pos": 30, "type": "METRIC", "confidence": 0.9783682823181152}, {"text": "RPD", "start_pos": 77, "end_pos": 80, "type": "METRIC", "confidence": 0.7701852321624756}]}, {"text": "This can be explained by the reason that the linearizations of 72% sub-trees can be definitely determined by the RPD method.", "labels": [], "entities": []}, {"text": "All of the four feature functions we have tested achieve considerable improvement in BLEU scores.", "labels": [], "entities": [{"text": "BLEU scores", "start_pos": 85, "end_pos": 96, "type": "METRIC", "confidence": 0.9757069945335388}]}, {"text": "The dependency relation model achieves 0.7204, the bigram word model 0.8289, the trigram word model 0.8508 and the headword model achieves 0.7592.", "labels": [], "entities": []}, {"text": "While the combined models perform better than any of their individual component models.", "labels": [], "entities": []}, {"text": "On the foundation of relative position determination method, the combination of dependency relation and bigram word model achieves a BLEU score of 0.8615, and the combination of dependency relation and trigram word model achieves a BLEU score of 0.8772.", "labels": [], "entities": [{"text": "relative position determination", "start_pos": 21, "end_pos": 52, "type": "TASK", "confidence": 0.7774860262870789}, {"text": "BLEU score", "start_pos": 133, "end_pos": 143, "type": "METRIC", "confidence": 0.983964204788208}, {"text": "BLEU score", "start_pos": 232, "end_pos": 242, "type": "METRIC", "confidence": 0.9851652085781097}]}, {"text": "Finally the combination of dependency relation model, trigram word model and headword model achieves the best result 0.8874.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Numbers of pre/post-dependents for each  dependency relation", "labels": [], "entities": []}, {"text": " Table 3: BLEU, ExMatch and SSA scores on the test set", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9991084933280945}, {"text": "ExMatch", "start_pos": 16, "end_pos": 23, "type": "METRIC", "confidence": 0.7171962261199951}, {"text": "SSA", "start_pos": 28, "end_pos": 31, "type": "METRIC", "confidence": 0.7849969267845154}]}, {"text": " Table 4: Error types in the RPD+DR experiment", "labels": [], "entities": [{"text": "Error", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9589712619781494}, {"text": "RPD+DR", "start_pos": 29, "end_pos": 35, "type": "DATASET", "confidence": 0.6080870230992635}]}]}