{"title": [{"text": "DEPEVAL(summ): Dependency-based Evaluation for Automatic Summaries", "labels": [], "entities": [{"text": "Summaries", "start_pos": 57, "end_pos": 66, "type": "TASK", "confidence": 0.7561877965927124}]}], "abstractContent": [{"text": "This paper presents DEPEVAL(summ), a dependency-based metric for automatic evaluation of summaries.", "labels": [], "entities": [{"text": "DEPEVAL(summ)", "start_pos": 20, "end_pos": 33, "type": "METRIC", "confidence": 0.8520065248012543}]}, {"text": "Using a rerank-ing parser and a Lexical-Functional Grammar (LFG) annotation, we produce a set of dependency triples for each summary.", "labels": [], "entities": []}, {"text": "The dependency set for each candidate summary is then automatically compared against dependencies generated from model summaries.", "labels": [], "entities": []}, {"text": "We examine a number of variations of the method, including the addition of WordNet, partial matching, or removing relation labels from the dependencies.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 75, "end_pos": 82, "type": "DATASET", "confidence": 0.9622339010238647}]}, {"text": "Ina test on TAC 2008 and DUC 2007 data, DE-PEVAL(summ) achieves comparable or higher correlations with human judgments than the popular evaluation metrics ROUGE and Basic Elements (BE).", "labels": [], "entities": [{"text": "TAC 2008 and DUC 2007 data", "start_pos": 12, "end_pos": 38, "type": "DATASET", "confidence": 0.8518863419691721}, {"text": "DE-PEVAL(summ", "start_pos": 40, "end_pos": 53, "type": "METRIC", "confidence": 0.8012822071711222}, {"text": "ROUGE", "start_pos": 155, "end_pos": 160, "type": "METRIC", "confidence": 0.9347265362739563}, {"text": "Basic Elements (BE)", "start_pos": 165, "end_pos": 184, "type": "METRIC", "confidence": 0.6045132935047149}]}], "introductionContent": [{"text": "Evaluation is a crucial component in the area of automatic summarization; it is used both to rank multiple participant systems in shared summarization tasks, such as the Summarization track at Text Analysis Conference (TAC) 2008 and its Document Understanding Conference (DUC) predecessors, and to provide feedback to developers whose goal is to improve their summarization systems.", "labels": [], "entities": [{"text": "summarization", "start_pos": 59, "end_pos": 72, "type": "TASK", "confidence": 0.7649683952331543}, {"text": "Summarization track at Text Analysis Conference (TAC) 2008", "start_pos": 170, "end_pos": 228, "type": "TASK", "confidence": 0.8946340680122375}]}, {"text": "However, manual evaluation of a large number of documents necessary fora relatively unbiased view is often unfeasible, especially in the contexts where repeated evaluations are needed.", "labels": [], "entities": []}, {"text": "Therefore, there is a great need for reliable automatic metrics that can perform evaluation in a fast and consistent manner.", "labels": [], "entities": []}, {"text": "In this paper, we explore one such evaluation metric, DEPEVAL(summ), based on the comparison of Lexical-Functional Grammar (LFG) dependencies between a candidate summary and one or more model (reference) summaries.", "labels": [], "entities": [{"text": "DEPEVAL(summ)", "start_pos": 54, "end_pos": 67, "type": "METRIC", "confidence": 0.8362084627151489}]}, {"text": "The method is similar in nature to Basic Elements (, in that it extends beyond a simple string comparison of word sequences, reaching instead to a deeper linguistic analysis of the text.", "labels": [], "entities": []}, {"text": "Both methods use hand-written extraction rules to derive dependencies from constituent parses produced by widely available Penn II Treebank parsers.", "labels": [], "entities": [{"text": "Penn II Treebank parsers", "start_pos": 123, "end_pos": 147, "type": "DATASET", "confidence": 0.9326899945735931}]}, {"text": "The difference between DEPEVAL(summ) and BE is that in DEPE-VAL(summ) the dependency extraction is accomplished through an LFG annotation of applied to the output of the reranking parser of, whereas in BE (in the version presented here) dependencies are generated by the Minipar parser.", "labels": [], "entities": [{"text": "BE", "start_pos": 41, "end_pos": 43, "type": "METRIC", "confidence": 0.9977865219116211}, {"text": "DEPE-VAL", "start_pos": 55, "end_pos": 63, "type": "DATASET", "confidence": 0.9159556031227112}, {"text": "dependency extraction", "start_pos": 74, "end_pos": 95, "type": "TASK", "confidence": 0.7233447134494781}, {"text": "BE", "start_pos": 202, "end_pos": 204, "type": "METRIC", "confidence": 0.9761150479316711}]}, {"text": "Despite relying on a the same concept, our approach outperforms BE inmost comparisons, and it often achieves higher correlations with human judgments than the string-matching metric ROUGE.", "labels": [], "entities": [{"text": "BE", "start_pos": 64, "end_pos": 66, "type": "METRIC", "confidence": 0.9764320850372314}]}, {"text": "A more detailed description of BE and ROUGE is presented in Section 2, which also gives an account of manual evaluation methods employed at TAC 2008.", "labels": [], "entities": [{"text": "BE", "start_pos": 31, "end_pos": 33, "type": "METRIC", "confidence": 0.9656290411949158}, {"text": "ROUGE", "start_pos": 38, "end_pos": 43, "type": "METRIC", "confidence": 0.9931065440177917}, {"text": "TAC 2008", "start_pos": 140, "end_pos": 148, "type": "DATASET", "confidence": 0.888504147529602}]}, {"text": "Section 3 gives a short introduction to the LFG annotation.", "labels": [], "entities": [{"text": "LFG annotation", "start_pos": 44, "end_pos": 58, "type": "DATASET", "confidence": 0.9137888848781586}]}, {"text": "Section 4 describes in more detail DEPEVAL(summ) and its variants.", "labels": [], "entities": [{"text": "DEPEVAL", "start_pos": 35, "end_pos": 42, "type": "METRIC", "confidence": 0.8552643656730652}]}, {"text": "Section 5 presents the experiment in which we compared the perfomance of all three metrics on the TAC 2008 data (consisting of 5,952 100-words summaries) and on the DUC 2007 data 250-word summaries) and discusses the correlations these metrics achieve.", "labels": [], "entities": [{"text": "TAC 2008 data", "start_pos": 98, "end_pos": 111, "type": "DATASET", "confidence": 0.9775940179824829}, {"text": "DUC 2007 data 250-word summaries", "start_pos": 165, "end_pos": 197, "type": "DATASET", "confidence": 0.9668694496154785}]}, {"text": "Finally, Section 6 presents conclusions and some directions for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "In the first Text Analysis Conference, as well as its predecessor, the Document Understanding Conference (DUC) series, the evaluation of summarization tasks was conducted using both manual and automatic methods.", "labels": [], "entities": [{"text": "Text Analysis Conference", "start_pos": 13, "end_pos": 37, "type": "TASK", "confidence": 0.8608580430348715}, {"text": "Document Understanding Conference (DUC)", "start_pos": 71, "end_pos": 110, "type": "TASK", "confidence": 0.7047713647286097}, {"text": "summarization tasks", "start_pos": 137, "end_pos": 156, "type": "TASK", "confidence": 0.8513903617858887}]}, {"text": "Since manual evaluation is still the undisputed gold standard, both at TAC and DUC there was much effort to evaluate manually as much data as possible.", "labels": [], "entities": [{"text": "TAC", "start_pos": 71, "end_pos": 74, "type": "DATASET", "confidence": 0.8657848238945007}, {"text": "DUC", "start_pos": 79, "end_pos": 82, "type": "DATASET", "confidence": 0.854127824306488}]}, {"text": "Manual assessment, performed by human judges, usually centers around two main aspects of summary quality: content and form.", "labels": [], "entities": []}, {"text": "Similarly to Machine Translation, where these two aspects are represented by the categories of Accuracy and Fluency, in automatic summarization evaluation performed at TAC and DUC they surface as (Content), however, Content Responsiveness was replaced by Overall Responsiveness, conflating these two dimensions and reflecting the overall quality of the summary: the degree to which a summary was responding to the information need contained in the topic statement, as well as its linguistic quality.", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 13, "end_pos": 32, "type": "TASK", "confidence": 0.8015746772289276}]}, {"text": "A separate Readability score was still provided, assessing the fluency and structure independently of content, based on such aspects as grammaticality, nonredundancy, referential clarity, focus, structure, and coherence.", "labels": [], "entities": []}, {"text": "Both Overall Responsiveness and Readability were evaluated according to a fivepoint scale, ranging from \"Very Poor\" to \"Very Good\".", "labels": [], "entities": []}, {"text": "Content was evaluated manually by NIST assessors using the Pyramid framework ().", "labels": [], "entities": [{"text": "NIST assessors", "start_pos": 34, "end_pos": 48, "type": "DATASET", "confidence": 0.8532952666282654}, {"text": "Pyramid framework", "start_pos": 59, "end_pos": 76, "type": "DATASET", "confidence": 0.9105063080787659}]}, {"text": "In the Pyramid evaluation, assessors first extract all possible \"information nuggets\", or Summary Content Units (SCUs) from the four human-crafted model summaries on a given topic.", "labels": [], "entities": []}, {"text": "Each SCU is assigned a weight in proportion to the number of model summaries in which it appears, on the assumption that information which appears inmost or all human-produced model summaries is more essential to the topic.", "labels": [], "entities": []}, {"text": "Once all SCUs are harvested from the model summaries, assessors determine how many of these SCUs are present in each of the automatic peer summaries.", "labels": [], "entities": []}, {"text": "The final score for an automatic summary is its total SCU weight divided by the maximum SCU weight available to a summary of average length (where the average length is determined by the mean SCU count of the model summaries for this topic).", "labels": [], "entities": []}, {"text": "All types of manual assessment are expensive and time-consuming, which is why it can be rarely provided for all submitted runs in shared tasks such as the TAC Summarization track.", "labels": [], "entities": [{"text": "TAC Summarization track", "start_pos": 155, "end_pos": 178, "type": "TASK", "confidence": 0.5874611735343933}]}, {"text": "It is also not a viable tool for system developers who ideally would like a fast, reliable, and above all automatic evaluation method that can be used to improve their systems.", "labels": [], "entities": []}, {"text": "The creation and testing of automatic evaluation methods is, therefore, an important research venue, and the goal is to produce automatic metrics that will correlate with manual assessment as closely as possible.", "labels": [], "entities": []}, {"text": "Automatic metrics, because of their relative speed, can be applied more widely than manual evaluation.", "labels": [], "entities": []}, {"text": "In TAC 2008 Summarization track, all submitted runs were scored with the ROUGE) and Basic Elements (BE) metrics).", "labels": [], "entities": [{"text": "TAC 2008 Summarization track", "start_pos": 3, "end_pos": 31, "type": "DATASET", "confidence": 0.8600424975156784}, {"text": "ROUGE", "start_pos": 73, "end_pos": 78, "type": "METRIC", "confidence": 0.9947915077209473}, {"text": "Basic Elements (BE) metrics", "start_pos": 84, "end_pos": 111, "type": "METRIC", "confidence": 0.7183090150356293}]}, {"text": "ROUGE is a collection of string-comparison techniques, based on matching n-grams between a candidate string and a reference string.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.7154648900032043}]}, {"text": "The string in question might be a single sentence (as in the case of translation), or a set of sentences (as in the case of summaries).", "labels": [], "entities": []}, {"text": "The variations of ROUGE range from matching unigrams (i.e. single words) to matching four-grams, with or without lemmatization and stopwords, with the options of using different weights or skip-n-grams (i.e. matching n-grams despite intervening words).", "labels": [], "entities": []}, {"text": "The two versions used in TAC 2008 evaluations were ROUGE-2 and ROUGE-SU4, where ROUGE-2 calculates the proportion of matching bigrams between the candidate summary and the reference summaries, and ROUGE-SU4 is a combination of unigram match and skip-bigram match with skip distance of 4 words.", "labels": [], "entities": [{"text": "TAC 2008", "start_pos": 25, "end_pos": 33, "type": "DATASET", "confidence": 0.6297448128461838}, {"text": "ROUGE-2", "start_pos": 51, "end_pos": 58, "type": "METRIC", "confidence": 0.9619576930999756}, {"text": "ROUGE-SU4", "start_pos": 63, "end_pos": 72, "type": "METRIC", "confidence": 0.9218809008598328}]}, {"text": "BE, on the other hand, employs a certain degree of linguistic analysis in the assessment process, as it rests on comparing the \"Basic Elements\" between the candidate and the reference.", "labels": [], "entities": [{"text": "BE", "start_pos": 0, "end_pos": 2, "type": "TASK", "confidence": 0.5369282960891724}]}, {"text": "Basic Elements are syntactic in nature, and comprise the heads of major syntactic constituents in the text (noun, verb, adjective, etc.) and their modifiers in a dependency relation, expressed as a triple (head, modifier, relation type).", "labels": [], "entities": []}, {"text": "First, the input text is parsed with a syntactic parser, then Basic Elements are extracted from the resulting parse, and the candidate BEs are matched against the reference BEs.", "labels": [], "entities": [{"text": "BEs", "start_pos": 135, "end_pos": 138, "type": "METRIC", "confidence": 0.9291241765022278}]}, {"text": "In TAC 2008 and DUC 2008 evaluations the BEs were extracted with Minipar.", "labels": [], "entities": [{"text": "TAC 2008 and DUC 2008", "start_pos": 3, "end_pos": 24, "type": "DATASET", "confidence": 0.7556164145469666}, {"text": "BEs", "start_pos": 41, "end_pos": 44, "type": "METRIC", "confidence": 0.9869599938392639}]}, {"text": "Since BE, contrary to ROUGE, does not rely solely on the surface sequence of words to determine similarity between summaries, but delves into what could be called a shallow semantic structure, comprising thematic roles such as subject and object, it is likely to notice identity of meaning where such identity is obscured by variations in word order.", "labels": [], "entities": [{"text": "BE", "start_pos": 6, "end_pos": 8, "type": "METRIC", "confidence": 0.8122844099998474}, {"text": "ROUGE", "start_pos": 22, "end_pos": 27, "type": "METRIC", "confidence": 0.7994012236595154}]}, {"text": "In fact, when it comes to evaluation of automatic summaries, BE shows higher correlations with human judgments than ROUGE, although the difference is not large enough to be statistically significant.", "labels": [], "entities": [{"text": "evaluation of automatic summaries", "start_pos": 26, "end_pos": 59, "type": "TASK", "confidence": 0.7346612960100174}, {"text": "BE", "start_pos": 61, "end_pos": 63, "type": "METRIC", "confidence": 0.995415210723877}, {"text": "ROUGE", "start_pos": 116, "end_pos": 121, "type": "METRIC", "confidence": 0.993065357208252}]}, {"text": "In the TAC 2008 evaluations, BE-HM (a version of BE where the words are stemmed and the relation type is ignored) obtained a correlation of 0.911 with human assessment of overall responsiveness and 0.949 with the Pyramid score, whereas ROUGE-2 showed correlations of 0.894 and 0.946, respectively.", "labels": [], "entities": [{"text": "TAC 2008 evaluations", "start_pos": 7, "end_pos": 27, "type": "DATASET", "confidence": 0.8742555578549703}, {"text": "BE-HM", "start_pos": 29, "end_pos": 34, "type": "METRIC", "confidence": 0.9433252215385437}, {"text": "BE", "start_pos": 49, "end_pos": 51, "type": "METRIC", "confidence": 0.7916553616523743}, {"text": "ROUGE-2", "start_pos": 236, "end_pos": 243, "type": "METRIC", "confidence": 0.9071530103683472}]}, {"text": "While using dependency information is an important step towards integrating linguistic knowledge into the evaluation process, there are many ways in which this could be approached.", "labels": [], "entities": []}, {"text": "Since this type of evaluation processes information in stages (constituent parser, dependency extraction, and the method of dependency matching between a candidate and a reference), there is potential for variance in performance among dependencybased evaluation metrics that use different components.", "labels": [], "entities": [{"text": "dependency extraction", "start_pos": 83, "end_pos": 104, "type": "TASK", "confidence": 0.7569995224475861}]}, {"text": "Therefore, it is interesting to compare our method, which relies on the Charniak-Johnson parser and the LFG annotation, with BE, which uses Minipar to parse the input and produce dependencies.", "labels": [], "entities": [{"text": "BE", "start_pos": 125, "end_pos": 127, "type": "METRIC", "confidence": 0.9951286315917969}]}, {"text": "Our dependency-based evaluation method, similarly to BE, compares two unordered sets of dependencies: one bag contains dependencies harvested from the candidate summary and the other contains dependencies from one or more reference summaries.", "labels": [], "entities": [{"text": "BE", "start_pos": 53, "end_pos": 55, "type": "METRIC", "confidence": 0.6223464608192444}]}, {"text": "Overlap between the candidate bag and the reference bag is calculated in the form of precision, recall, and the f-measure (with precision and recall equally weighted).", "labels": [], "entities": [{"text": "precision", "start_pos": 85, "end_pos": 94, "type": "METRIC", "confidence": 0.9996821880340576}, {"text": "recall", "start_pos": 96, "end_pos": 102, "type": "METRIC", "confidence": 0.999151349067688}, {"text": "f-measure", "start_pos": 112, "end_pos": 121, "type": "METRIC", "confidence": 0.979267954826355}, {"text": "precision", "start_pos": 128, "end_pos": 137, "type": "METRIC", "confidence": 0.9982253909111023}, {"text": "recall", "start_pos": 142, "end_pos": 148, "type": "METRIC", "confidence": 0.9533109068870544}]}, {"text": "Since for ROUGE and BE the only reported score is recall, we present recall results here as well, calculated as in 2: where D cand are the candidate dependencies and D ref are the reference dependencies.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9617007374763489}, {"text": "BE", "start_pos": 20, "end_pos": 22, "type": "METRIC", "confidence": 0.9968631267547607}, {"text": "recall", "start_pos": 50, "end_pos": 56, "type": "METRIC", "confidence": 0.9991956353187561}, {"text": "recall", "start_pos": 69, "end_pos": 75, "type": "METRIC", "confidence": 0.9979709982872009}]}, {"text": "The dependency-based method using LFG annotation has been successfully employed in the evaluation of Machine Translation (MT).", "labels": [], "entities": [{"text": "Machine Translation (MT)", "start_pos": 101, "end_pos": 125, "type": "TASK", "confidence": 0.8410460233688355}]}, {"text": "In Owczarzak (2008), the method achieves equal or higher correlations with human judgments than METEOR (), one of the best-performing automatic MT evaluation metrics.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 96, "end_pos": 102, "type": "METRIC", "confidence": 0.9829598665237427}, {"text": "MT evaluation", "start_pos": 144, "end_pos": 157, "type": "TASK", "confidence": 0.9369464218616486}]}, {"text": "However, it is not clear that the method can be applied without change to the task of assessing automatic summaries; after all, the two tasks -of summarization and translation -produce outputs that are different in nature.", "labels": [], "entities": [{"text": "summaries", "start_pos": 106, "end_pos": 115, "type": "TASK", "confidence": 0.6458880305290222}, {"text": "summarization", "start_pos": 146, "end_pos": 159, "type": "TASK", "confidence": 0.9768387675285339}]}, {"text": "In MT, the unit of text is a sentence; text is translated, and the translation evaluated, sentence by sentence.", "labels": [], "entities": [{"text": "MT", "start_pos": 3, "end_pos": 5, "type": "TASK", "confidence": 0.9790182113647461}]}, {"text": "In automatic summarization, the output unit is a summary with length varying depending on task, but which most often consists of at least several sentences.", "labels": [], "entities": [{"text": "summarization", "start_pos": 13, "end_pos": 26, "type": "TASK", "confidence": 0.7848636507987976}]}, {"text": "This has bearing on the matching process: with several sentences on the candidate and reference side each, there is increased possibility of trivial matches, such as dependencies containing function words, which might inflate the summary score even in the absence of important content.", "labels": [], "entities": []}, {"text": "This is particularly likely if we were to employ partial matching for dependencies.", "labels": [], "entities": []}, {"text": "Partial matching (indicated in the result tables with the tag pm) \"splits\" each predicate dependency into two, replacing one or the other element with a variable, e.g. for the dependency subject(resign, John) we would obtain two partial dependencies subject(resign, x) and subject(x, John).", "labels": [], "entities": []}, {"text": "This process helps circumvent some of the syntactic and lexical variation between a candidate and a reference, and it proved very useful in MT evaluation.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 140, "end_pos": 153, "type": "TASK", "confidence": 0.9642983973026276}]}, {"text": "In summary evaluation, as will be shown in Section 5, it leads to higher correlations with human judgments only in the case of human-produced model summaries, because almost any variation between two model summaries is \"legal\", i.e. either a paraphrase or another, but equally relevant, piece of information.", "labels": [], "entities": []}, {"text": "For automatic summaries, which are of relatively poor quality, partial matching lowers our method's ability to reflect human judgment, because it results in overly generous matching in situations where the examined information is neither a paraphrase nor relevant.", "labels": [], "entities": []}, {"text": "Similarly, evaluating a summary against the union of all references, as we do in the baseline version of our method, increases the pool of possible matches, but may also produce score inflation through matching repetitive information across models.", "labels": [], "entities": []}, {"text": "To deal with this, we produce aversion of the score (marked in the result tables with the tag one) that counts only one \"hit\" for every dependency match, independent of how many instances of a given dependency are present in the comparison.", "labels": [], "entities": []}, {"text": "The use of WordNet 1 module (Rennie, 2000) did not provide a great advantage (see results tagged with wn), and sometimes even lowered our correlations, especially in evaluation of automatic systems.", "labels": [], "entities": [{"text": "WordNet 1 module (Rennie, 2000)", "start_pos": 11, "end_pos": 42, "type": "DATASET", "confidence": 0.9067613705992699}, {"text": "correlations", "start_pos": 138, "end_pos": 150, "type": "METRIC", "confidence": 0.9533848166465759}]}, {"text": "This makes sense if we take into consideration that WordNet lists all possible synonyms for all possible senses of a word, and so, given a great number of cross-sentence comparisons in multi-sentence summaries, there is an increased risk of spurious matches between words which, despite being potentially synonymous in certain contexts, are not equivalent in the text.", "labels": [], "entities": []}, {"text": "Another area of concern was the potential noise introduced by the parser and the annotation process.", "labels": [], "entities": []}, {"text": "Due to parsing errors, two otherwise equivalent expressions might be encoded as differing sets of dependencies.", "labels": [], "entities": []}, {"text": "In MT evaluation, the dependency-based method can alleviate parser noise by comparing n-best parses for the candidate and the reference (, but this is not an efficient solution for comparing multisentence summaries.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 3, "end_pos": 16, "type": "TASK", "confidence": 0.9763784110546112}]}, {"text": "We have therefore attempted to at least partially counteract this issue by removing relation labels from the dependencies (i.e. producing dependencies of the form (resign, John) instead of subject(resign, John)), which did provide some improvement (see results tagged with norel).", "labels": [], "entities": []}, {"text": "Finally, we experimented with a predicate-only version of the evaluation, where only the predicate dependencies participate in the comparison, excluding dependencies that provide purely grammatical information such as person, tense, or number (tagged in the results table as pred).", "labels": [], "entities": []}, {"text": "This move proved beneficial only in the case of system summaries, perhaps by decreasing the number of trivial matches, but decreased the method's correlation for model summaries, where such detailed information might be necessary to assess the degree of similarity between two human summaries.", "labels": [], "entities": [{"text": "system summaries", "start_pos": 48, "end_pos": 64, "type": "TASK", "confidence": 0.529266357421875}, {"text": "correlation", "start_pos": 146, "end_pos": 157, "type": "METRIC", "confidence": 0.9544360637664795}]}, {"text": "The first question we have to ask is: which of the manual evaluation categories do we want our metric to imitate?", "labels": [], "entities": []}, {"text": "It is unlikely that a single automatic measure will be able to correctly reflect both Readability and Content Responsiveness, as form and content are separate qualities and need different measures.", "labels": [], "entities": []}, {"text": "Content seems to be the more important aspect, especially given that Readability can be partially derived from Responsiveness (a summary high in content cannot be very low in readability, although some very readable summaries can have little relevant content).", "labels": [], "entities": []}, {"text": "Content Responsiveness was provided in DUC 2007 data, but not in TAC 2008, where the extrinsic Pyramid measure was used to evaluate content.", "labels": [], "entities": [{"text": "Content Responsiveness", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.6713561862707138}, {"text": "DUC 2007 data", "start_pos": 39, "end_pos": 52, "type": "DATASET", "confidence": 0.9797707001368204}, {"text": "TAC 2008", "start_pos": 65, "end_pos": 73, "type": "DATASET", "confidence": 0.9452995955944061}, {"text": "extrinsic Pyramid measure", "start_pos": 85, "end_pos": 110, "type": "METRIC", "confidence": 0.9078082044919332}]}, {"text": "It is, in fact, preferable to compare our metric against the Pyramid score rather than Content Responsiveness, because both the Pyramid and our method aim to measure the degree of similarity between a candidate and a model, whereas Content Responsiveness is a direct assessment of whether the summary's content is adequate given a topic and a source text.", "labels": [], "entities": []}, {"text": "The Pyramid is, at the same time, a costly manual evaluation method, so an automatic metric that successfully emulates it would be a useful replacement.", "labels": [], "entities": []}, {"text": "Another question is whether we focus on system-level or summary-level evaluation.", "labels": [], "entities": []}, {"text": "The correlation values at the summary-level are generally much lower than on the system-level, which means the metrics are better at evaluating system performance than the quality of individual summaries.", "labels": [], "entities": [{"text": "correlation", "start_pos": 4, "end_pos": 15, "type": "METRIC", "confidence": 0.9586402177810669}]}, {"text": "System-level evaluations are essential to shared summarization tasks; summary-level assessment might be useful to developers who want to test the effect of particular improvements in their system.", "labels": [], "entities": [{"text": "summarization tasks", "start_pos": 49, "end_pos": 68, "type": "TASK", "confidence": 0.8393041789531708}]}, {"text": "Of course, the ideal evaluation metric would show high correlations with human judgment on both levels.", "labels": [], "entities": []}, {"text": "We used the data from the TAC 2008 and DUC 2007 Summarization tracks.", "labels": [], "entities": [{"text": "TAC 2008 and DUC 2007 Summarization tracks", "start_pos": 26, "end_pos": 68, "type": "DATASET", "confidence": 0.8477291549955096}]}, {"text": "The first set comprised 58 system submissions and 4 humanproduced model summaries for each of the 96 subtopics (there were 48 topics, each of which required two summaries: a main and an update summary), as well as human-produced Overall Responsiveness and Pyramid scores for each summary.", "labels": [], "entities": [{"text": "Pyramid", "start_pos": 256, "end_pos": 263, "type": "METRIC", "confidence": 0.9279971718788147}]}, {"text": "The second set included 32 system submissions and 4 human models for each of the 45 topics.", "labels": [], "entities": []}, {"text": "For fair comparison of models and systems, we used jackknifing: while each model was evaluated against the remaining three models, each system summary was evaluated four times, each time against a different set of three models, and the four scores were averaged.", "labels": [], "entities": []}, {"text": "presents system-level Pearson's correlations between the scores provided by our dependency-based metric DEPEVAL(summ), as well as the automatic metrics ROUGE-2, ROUGE-SU4, and BE-HM used in the TAC evaluation, and the manual Pyramid scores, which measured the content quality of the systems.", "labels": [], "entities": [{"text": "Pearson's correlations", "start_pos": 22, "end_pos": 44, "type": "METRIC", "confidence": 0.8123424053192139}, {"text": "DEPEVAL(summ)", "start_pos": 104, "end_pos": 117, "type": "METRIC", "confidence": 0.8884917348623276}, {"text": "ROUGE-2", "start_pos": 152, "end_pos": 159, "type": "METRIC", "confidence": 0.7993839979171753}, {"text": "BE-HM", "start_pos": 176, "end_pos": 181, "type": "METRIC", "confidence": 0.9976726174354553}, {"text": "TAC evaluation", "start_pos": 194, "end_pos": 208, "type": "TASK", "confidence": 0.684649646282196}]}, {"text": "It also includes correlations with the manual Overall Responsiveness score, which reflected both content and linguistic quality.", "labels": [], "entities": [{"text": "Overall Responsiveness score", "start_pos": 46, "end_pos": 74, "type": "METRIC", "confidence": 0.605104019244512}]}, {"text": "shows the correlations with Content Responsiveness for DUC 2007 data for ROUGE, BE, and those few select versions of DEPEVAL(summ) which achieve optimal results on TAC 2008 data (for a more detailed discussion of the selection see Section 6).", "labels": [], "entities": [{"text": "DUC 2007 data", "start_pos": 55, "end_pos": 68, "type": "DATASET", "confidence": 0.9525304238001505}, {"text": "ROUGE", "start_pos": 73, "end_pos": 78, "type": "METRIC", "confidence": 0.4691679775714874}, {"text": "BE", "start_pos": 80, "end_pos": 82, "type": "METRIC", "confidence": 0.9187811613082886}, {"text": "DEPEVAL", "start_pos": 117, "end_pos": 124, "type": "DATASET", "confidence": 0.6631977558135986}, {"text": "TAC 2008 data", "start_pos": 164, "end_pos": 177, "type": "DATASET", "confidence": 0.9575625658035278}]}], "tableCaptions": [{"text": " Table 1: System-level Pearson's correlation between auto-", "labels": [], "entities": [{"text": "Pearson's correlation", "start_pos": 23, "end_pos": 44, "type": "METRIC", "confidence": 0.8158878286679586}]}, {"text": " Table 2: Summary-level Pearson's correlation between automatic and manual", "labels": [], "entities": [{"text": "Summary-level Pearson's correlation", "start_pos": 10, "end_pos": 45, "type": "METRIC", "confidence": 0.7733517587184906}]}, {"text": " Table 3: System-level Pearson's correlation", "labels": [], "entities": [{"text": "Pearson's correlation", "start_pos": 23, "end_pos": 44, "type": "DATASET", "confidence": 0.9437488714853922}]}, {"text": " Table 4: Summary-level Pearson's correlation", "labels": [], "entities": [{"text": "Summary-level Pearson's correlation", "start_pos": 10, "end_pos": 45, "type": "METRIC", "confidence": 0.5785540193319321}]}]}