{"title": [{"text": "A Syntax-Driven Bracketing Model for Phrase-Based Translation", "labels": [], "entities": [{"text": "Phrase-Based Translation", "start_pos": 37, "end_pos": 61, "type": "TASK", "confidence": 0.883373349905014}]}], "abstractContent": [{"text": "Syntactic analysis influences the way in which the source sentence is translated.", "labels": [], "entities": []}, {"text": "Previous efforts add syntactic constraints to phrase-based translation by directly rewarding/punishing a hypothesis whenever it matches/violates source-side constituents.", "labels": [], "entities": [{"text": "phrase-based translation", "start_pos": 46, "end_pos": 70, "type": "TASK", "confidence": 0.7332552969455719}]}, {"text": "We present anew model that automatically learns syntactic constraints, including but not limited to constituent matching/violation, from training corpus.", "labels": [], "entities": [{"text": "constituent matching/violation", "start_pos": 100, "end_pos": 130, "type": "TASK", "confidence": 0.808658055961132}]}, {"text": "The model brackets a source phrase as to whether it satisfies the learnt syntactic constraints.", "labels": [], "entities": []}, {"text": "The bracketed phrases are then translated as a whole unit by the de-coder.", "labels": [], "entities": []}, {"text": "Experimental results and analysis show that the new model outperforms other previous methods and achieves a substantial improvement over the baseline which is not syntactically informed.", "labels": [], "entities": []}], "introductionContent": [{"text": "The phrase-based approach is widely adopted in statistical machine translation (SMT).", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 47, "end_pos": 84, "type": "TASK", "confidence": 0.8125004669030508}]}, {"text": "It segments a source sentence into a sequence of phrases, then translates and reorder these phrases in the target.", "labels": [], "entities": []}, {"text": "In such a process, original phrase-based decoding () does not take advantage of any linguistic analysis, which, however, is broadly used in rule-based approaches.", "labels": [], "entities": []}, {"text": "Since it is not linguistically motivated, original phrasebased decoding might produce ungrammatical or even wrong translations.", "labels": [], "entities": []}, {"text": "Consider the following Chinese fragment with its parse tree: The output is generated from a phrase-based system which does not involve any syntactic analysis.", "labels": [], "entities": []}, {"text": "Here we use \"[]\" (straight orientation) and \"\ud97b\udf59\ud97b\udf59\" (inverted orientation) to denote the common structure of the source fragment and its translation found by the decoder.", "labels": [], "entities": []}, {"text": "We can observe that the decoder inadequately breaks up the second NP phrase and translates the two words \"\" and \"\ud97b\udf59\" separately.", "labels": [], "entities": []}, {"text": "However, the parse tree of the source fragment constrains the phrase \" \" to be translated as a unit.", "labels": [], "entities": []}, {"text": "Without considering syntactic constraints from the parse tree, the decoder makes wrong decisions not only on phrase movement but also on the lexical selection for the multi-meaning word \"\ud97b\udf59\" . To avert such errors, the decoder can fully respect linguistic structures by only allowing syntactic constituent translations and reorderings.", "labels": [], "entities": [{"text": "phrase movement", "start_pos": 109, "end_pos": 124, "type": "TASK", "confidence": 0.7088167667388916}]}, {"text": "This, unfortunately, significantly jeopardizes performance () because by integrating syntactic constraint into decoding as a hard constraint, it simply prohibits any other useful non-syntactic translations which violate constituent boundaries.", "labels": [], "entities": []}, {"text": "To better leverage syntactic constraint yet still allow non-syntactic translations, introduces a count for each hypothesis and accumulates it whenever the hypothesis exactly matches syntactic boundaries on the source side.", "labels": [], "entities": []}, {"text": "On the contrary,  and accumulate a count whenever hypotheses violate constituent boundaries.", "labels": [], "entities": []}, {"text": "These constituent matching/violation counts are used as a feature in the decoder's log-linear model and their weights are tuned via minimal error rate training (MERT).", "labels": [], "entities": [{"text": "minimal error rate training (MERT", "start_pos": 132, "end_pos": 165, "type": "METRIC", "confidence": 0.8027810603380203}]}, {"text": "In this way, syntactic constraint is integrated into decoding as a soft constraint to enable the decoder to reward hypotheses that respect syntactic analyses or to pe-nalize hypotheses that violate syntactic structures.", "labels": [], "entities": []}, {"text": "Although experiments show that this constituent matching/violation counting feature achieves significant improvements on various language-pairs, one issue is that matching syntactic analysis cannot always guarantee a good translation, and violating syntactic structure does not always induce a bad translation.", "labels": [], "entities": [{"text": "constituent matching/violation counting", "start_pos": 36, "end_pos": 75, "type": "TASK", "confidence": 0.7547919869422912}]}, {"text": "find that some constituency types favor matching the source parse while others encourage violations.", "labels": [], "entities": [{"text": "matching the source parse", "start_pos": 40, "end_pos": 65, "type": "TASK", "confidence": 0.6688647270202637}]}, {"text": "Therefore it is necessary to integrate more syntactic constraints into phrase translation, not just the constraint of constituent matching/violation.", "labels": [], "entities": [{"text": "phrase translation", "start_pos": 71, "end_pos": 89, "type": "TASK", "confidence": 0.7785298526287079}, {"text": "constituent matching/violation", "start_pos": 118, "end_pos": 148, "type": "TASK", "confidence": 0.8165554478764534}]}, {"text": "The other issue is that during decoding we are more concerned with the question of phrase cohesion, i.e. whether the current phrase can be translated as a unit or not within particular syntactic contexts) 2 , than that of constituent matching/violation.", "labels": [], "entities": [{"text": "constituent matching/violation", "start_pos": 222, "end_pos": 252, "type": "TASK", "confidence": 0.812778614461422}]}, {"text": "Phrase cohesion is one of the main reasons that we introduce syntactic constraints.", "labels": [], "entities": []}, {"text": "If a source phrase remains contiguous after translation, we refer this type of phrase bracketable, otherwise unbracketable.", "labels": [], "entities": []}, {"text": "It is more desirable to translate a bracketable phrase than an unbracketable one.", "labels": [], "entities": [{"text": "translate a bracketable phrase", "start_pos": 24, "end_pos": 54, "type": "TASK", "confidence": 0.8216293454170227}]}, {"text": "In this paper, we propose a syntax-driven bracketing (SDB) model to predict whether a phrase (a sequence of contiguous words) is bracketable or not using rich syntactic constraints.", "labels": [], "entities": [{"text": "syntax-driven bracketing (SDB)", "start_pos": 28, "end_pos": 58, "type": "TASK", "confidence": 0.8141047477722168}]}, {"text": "We parse the source language sentences in the word-aligned training corpus.", "labels": [], "entities": []}, {"text": "According to the word alignments, we define bracketable and unbracketable instances.", "labels": [], "entities": []}, {"text": "For each of these instances, we automatically extract relevant syntactic features from the source parse tree as bracketing evidences.", "labels": [], "entities": []}, {"text": "Then we tune the weights of these features using a maximum entropy (ME) trainer.", "labels": [], "entities": [{"text": "maximum entropy (ME)", "start_pos": 51, "end_pos": 71, "type": "METRIC", "confidence": 0.6809584498405457}]}, {"text": "In this way, we build two bracketing models: 1) a unary SDB model (UniSDB) which predicts whether an independent phrase is bracketable or not; and 2) a binary SDB model(BiSDB) which predicts whether two neighboring phrases are bracketable.", "labels": [], "entities": []}, {"text": "Similar to previous methods, our SDB model is integrated into the decoder's log-linear model as a feature so that we can inherit the idea of soft constraints.", "labels": [], "entities": []}, {"text": "In contrast to the constituent matching/violation counting (CMVC)), our SDB model has Here we expand the definition of phrase to include both syntactic and non-syntactic phrases.", "labels": [], "entities": [{"text": "constituent matching/violation counting (CMVC))", "start_pos": 19, "end_pos": 66, "type": "TASK", "confidence": 0.817731436342001}]}], "datasetContent": [{"text": "We carried out the MT experiments on Chineseto-English translation, using ()'s system as our baseline system.", "labels": [], "entities": [{"text": "MT", "start_pos": 19, "end_pos": 21, "type": "TASK", "confidence": 0.9246991276741028}, {"text": "Chineseto-English translation", "start_pos": 37, "end_pos": 66, "type": "TASK", "confidence": 0.7112678587436676}]}, {"text": "We modified the baseline decoder to incorporate our SDB models as descried in section 3.3.", "labels": [], "entities": []}, {"text": "In order to compare with Marton and Resnik's approach, we also adapted the baseline decoder to their XP+ feature.", "labels": [], "entities": []}, {"text": "In order to obtain syntactic trees for SDB models and XP+, we parsed source sentences using a lexicalized PCFG parser ().", "labels": [], "entities": []}, {"text": "The parser was trained on the Penn Chinese Treebank with an F1 score of 79.4%.", "labels": [], "entities": [{"text": "Penn Chinese Treebank", "start_pos": 30, "end_pos": 51, "type": "DATASET", "confidence": 0.982250432173411}, {"text": "F1 score", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.9873105585575104}]}, {"text": "All translation models were trained on the FBIS corpus.", "labels": [], "entities": [{"text": "FBIS corpus", "start_pos": 43, "end_pos": 54, "type": "DATASET", "confidence": 0.9592835903167725}]}, {"text": "We removed 15,250 sentences, for which the Chinese parser failed to produce syntactic parse trees.", "labels": [], "entities": []}, {"text": "To obtain word-level alignments, we ran GIZA++) on the remaining corpus in both directions, and applied the \"grow-diag-final\" refinement rule () to produce the final many-to-many word alignments.", "labels": [], "entities": []}, {"text": "We built our four-gram language model using Xinhua section of the English Gigaword corpus (181.1M words) with the SRILM toolkit.", "labels": [], "entities": [{"text": "English Gigaword corpus", "start_pos": 66, "end_pos": 89, "type": "DATASET", "confidence": 0.8801444371541342}, {"text": "SRILM toolkit", "start_pos": 114, "end_pos": 127, "type": "DATASET", "confidence": 0.9259749948978424}]}, {"text": "For the efficiency of MERT, we built our development set (580 sentences) using sentences not exceeding 50 characters from the NIST MT-02 set.", "labels": [], "entities": [{"text": "MERT", "start_pos": 22, "end_pos": 26, "type": "TASK", "confidence": 0.5082095861434937}, {"text": "NIST MT-02 set", "start_pos": 126, "end_pos": 140, "type": "DATASET", "confidence": 0.8978020151456197}]}, {"text": "We evaluated all models on the NIST MT-05 set using case-sensitive BLEU-4.", "labels": [], "entities": [{"text": "NIST MT-05 set", "start_pos": 31, "end_pos": 45, "type": "DATASET", "confidence": 0.8871927857398987}, {"text": "BLEU-4", "start_pos": 67, "end_pos": 73, "type": "METRIC", "confidence": 0.9804286360740662}]}, {"text": "Statistical significance in BLEU score differences was tested by paired bootstrap re-sampling).", "labels": [], "entities": [{"text": "BLEU score differences", "start_pos": 28, "end_pos": 50, "type": "METRIC", "confidence": 0.966862142086029}]}], "tableCaptions": [{"text": " Table 1: Feature weights obtained by MERT on the development set. The first 4 features are the phrase  translation probabilities in both directions and the lexical translation probabilities in both directions. P lm  = language model; P r = MaxEnt-based reordering model; Word = word bonus; Phr = phrase bonus.", "labels": [], "entities": []}, {"text": " Table 2: Results on the test set. **: significantly better than baseline (p < 0.01). + or ++: significantly  better than Marton and Resnik's XP+ (p < 0.05 or p < 0.01, respectively).", "labels": [], "entities": []}, {"text": " Table 3: Results of different feature sets. * or **:  significantly better than baseline (p < 0.05 or p <  0.01, respectively). + or ++: significantly better  than XP+ (p < 0.05 or p < 0.01, respectively).  @ \u2212 : almost significantly better than its UniSDB  counterpart (p < 0.075). @ or @@: significantly  better than its UniSDB counterpart (p < 0.05 or  p < 0.01, respectively).", "labels": [], "entities": []}, {"text": " Table 4: Consistent constituent matching rates re- ported on 1-best translation outputs.", "labels": [], "entities": [{"text": "Consistent constituent matching", "start_pos": 10, "end_pos": 41, "type": "TASK", "confidence": 0.560093512137731}]}, {"text": " Table 6: Consistent constituent matching rates for  structures with different spans.", "labels": [], "entities": [{"text": "Consistent constituent matching", "start_pos": 10, "end_pos": 41, "type": "TASK", "confidence": 0.5991787910461426}]}]}