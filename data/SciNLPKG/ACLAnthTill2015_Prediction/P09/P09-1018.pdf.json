{"title": [{"text": "Revisiting Pivot Language Approach for Machine Translation", "labels": [], "entities": [{"text": "Revisiting Pivot Language Approach", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.8047935366630554}, {"text": "Machine Translation", "start_pos": 39, "end_pos": 58, "type": "TASK", "confidence": 0.7478905022144318}]}], "abstractContent": [{"text": "This paper revisits the pivot language approach for machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 52, "end_pos": 71, "type": "TASK", "confidence": 0.8243556320667267}]}, {"text": "First, we investigate three different methods for pivot translation.", "labels": [], "entities": [{"text": "pivot translation", "start_pos": 50, "end_pos": 67, "type": "TASK", "confidence": 0.8153199553489685}]}, {"text": "Then we employ a hybrid method combining RBMT and SMT systems to fill up the data gap for pivot translation, where the source-pivot and pivot-target corpora are independent.", "labels": [], "entities": [{"text": "SMT", "start_pos": 50, "end_pos": 53, "type": "TASK", "confidence": 0.9580236077308655}, {"text": "pivot translation", "start_pos": 90, "end_pos": 107, "type": "TASK", "confidence": 0.7167282104492188}]}, {"text": "Experimental results on spoken language translation show that this hybrid method significantly improves the translation quality, which outperforms the method using a source-target corpus of the same size.", "labels": [], "entities": [{"text": "spoken language translation", "start_pos": 24, "end_pos": 51, "type": "TASK", "confidence": 0.6973661581675211}]}, {"text": "In addition, we propose a system combination approach to select better translations from those produced by various pivot translation methods.", "labels": [], "entities": []}, {"text": "This method regards system combination as a translation evaluation problem and formalizes it with a regression learning model.", "labels": [], "entities": [{"text": "translation evaluation", "start_pos": 44, "end_pos": 66, "type": "TASK", "confidence": 0.9449829459190369}]}, {"text": "Experimental results indicate that our method achieves consistent and significant improvement over individual translation outputs.", "labels": [], "entities": []}], "introductionContent": [{"text": "Current statistical machine translation (SMT) systems rely on large parallel and monolingual training corpora to produce translations of relatively higher quality.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 8, "end_pos": 45, "type": "TASK", "confidence": 0.796380857626597}]}, {"text": "Unfortunately, large quantities of parallel data are not readily available for some languages pairs, therefore limiting the potential use of current SMT systems.", "labels": [], "entities": [{"text": "SMT", "start_pos": 149, "end_pos": 152, "type": "TASK", "confidence": 0.9903801679611206}]}, {"text": "In particular, for speech translation, the translation task often focuses on a specific domain such as the travel domain.", "labels": [], "entities": [{"text": "speech translation", "start_pos": 19, "end_pos": 37, "type": "TASK", "confidence": 0.7397108376026154}]}, {"text": "It is especially difficult to obtain such a domain-specific corpus for some language pairs such as Chinese to Spanish translation.", "labels": [], "entities": [{"text": "Chinese to Spanish translation", "start_pos": 99, "end_pos": 129, "type": "TASK", "confidence": 0.7058335393667221}]}, {"text": "To circumvent the data bottleneck, some researchers have investigated to use a pivot language approach (.", "labels": [], "entities": []}, {"text": "This approach introduces a third language, named the pivot language, for which there exist large source-pivot and pivot-target bilingual corpora.", "labels": [], "entities": []}, {"text": "A pivot task was also designed for spoken language translation in the evaluation campaign of, where English is used as a pivot language for Chinese to Spanish translation.", "labels": [], "entities": [{"text": "spoken language translation", "start_pos": 35, "end_pos": 62, "type": "TASK", "confidence": 0.7071657379468282}, {"text": "Chinese to Spanish translation", "start_pos": 140, "end_pos": 170, "type": "TASK", "confidence": 0.7391523122787476}]}, {"text": "Three different pivot strategies have been investigated in the literature.", "labels": [], "entities": []}, {"text": "The first is based on phrase table multiplication.", "labels": [], "entities": [{"text": "phrase table multiplication", "start_pos": 22, "end_pos": 49, "type": "TASK", "confidence": 0.6824346582094828}]}, {"text": "It multiples corresponding translation probabilities and lexical weights in source-pivot and pivot-target translation models to induce anew source-target phrase table.", "labels": [], "entities": []}, {"text": "We name it the triangulation method.", "labels": [], "entities": []}, {"text": "The second is the sentence translation strategy, which first translates the source sentence to the pivot sentence, and then to the target sentence (.", "labels": [], "entities": [{"text": "sentence translation", "start_pos": 18, "end_pos": 38, "type": "TASK", "confidence": 0.7811855673789978}]}, {"text": "We name it the transfer method.", "labels": [], "entities": []}, {"text": "The third is to use existing models to build a synthetic source-target corpus, from which a source-target model can be trained (.", "labels": [], "entities": []}, {"text": "For example, we can obtain a source-pivot corpus by translating the pivot sentence in the source-pivot corpus into the target language with pivot-target translation models.", "labels": [], "entities": []}, {"text": "We name it the synthetic method.", "labels": [], "entities": []}, {"text": "The working condition with the pivot language approach is that the source-pivot and pivot-target parallel corpora are independent, in the sense that they are not derived from the same set of sentences, namely independently sourced corpora.", "labels": [], "entities": []}, {"text": "Thus, some linguistic phenomena in the sourcepivot corpus will lost if they do not exist in the pivot-target corpus, and vice versa.", "labels": [], "entities": []}, {"text": "In order to fill up this data gap, we make use of rule-based machine translation (RBMT) systems to translate the pivot sentences in the source-pivot or pivot-target corpus into target or source sentences.", "labels": [], "entities": [{"text": "rule-based machine translation (RBMT)", "start_pos": 50, "end_pos": 87, "type": "TASK", "confidence": 0.8019059300422668}]}, {"text": "As a result, we can build a synthetic multilingual corpus, which can be used to improve the translation quality.", "labels": [], "entities": []}, {"text": "The idea of using RBMT systems to improve the translation quality of SMT sysems has been explored in.", "labels": [], "entities": [{"text": "SMT sysems", "start_pos": 69, "end_pos": 79, "type": "TASK", "confidence": 0.8860806822776794}]}, {"text": "Here, we re-examine the hybrid method to fill up the data gap for pivot translation.", "labels": [], "entities": [{"text": "pivot translation", "start_pos": 66, "end_pos": 83, "type": "TASK", "confidence": 0.6919196844100952}]}, {"text": "Although previous studies proposed several pivot translation methods, there are no studies to combine different pivot methods for translation quality improvement.", "labels": [], "entities": []}, {"text": "In this paper, we first compare the individual pivot methods and then investigate to improve pivot translation quality by combining the outputs produced by different systems.", "labels": [], "entities": []}, {"text": "We propose to regard system combination as a translation evaluation problem.", "labels": [], "entities": [{"text": "translation evaluation", "start_pos": 45, "end_pos": 67, "type": "TASK", "confidence": 0.9597043991088867}]}, {"text": "For translations from one of the systems, this method uses the outputs from other translation systems as pseudo references.", "labels": [], "entities": []}, {"text": "A regression learning method is used to infer a function that maps a feature vector (which measures the similarity of a translation to the pseudo references) to a score that indicates the quality of the translation.", "labels": [], "entities": []}, {"text": "Scores are first generated independently for each translation, then the translations are ranked by their respective scores.", "labels": [], "entities": []}, {"text": "The candidate with the highest score is selected as the final translation.", "labels": [], "entities": []}, {"text": "This is achieved by optimizing the regression learning model's output to correlate against a set of training examples, where the source sentences are provided with several reference translations, instead of manually labeling the translations produced by various systems with quantitative assessments as described in.", "labels": [], "entities": []}, {"text": "The advantage of our method is that we do not need to manually label the translations produced by each translation system, therefore enabling our method suitable for translation selection among any systems without additional manual work.", "labels": [], "entities": [{"text": "translation selection", "start_pos": 166, "end_pos": 187, "type": "TASK", "confidence": 0.9544099569320679}]}, {"text": "We conducted experiments for spoken language translation on the pivot task in the IWSLT 2008 evaluation campaign, where Chinese sentences in travel domain need to be translated into Spanish, with English as the pivot language.", "labels": [], "entities": [{"text": "spoken language translation", "start_pos": 29, "end_pos": 56, "type": "TASK", "confidence": 0.7029711008071899}, {"text": "IWSLT 2008 evaluation campaign", "start_pos": 82, "end_pos": 112, "type": "DATASET", "confidence": 0.906121701002121}]}, {"text": "Experimental results show that (1) the performances of the three pivot methods are comparable when only SMT systems are used.", "labels": [], "entities": [{"text": "SMT", "start_pos": 104, "end_pos": 107, "type": "TASK", "confidence": 0.9746196866035461}]}, {"text": "However, the triangulation method and the transfer method significantly outperform the synthetic method when RBMT systems are used to improve the translation quality; (2) The hybrid method combining SMT and RBMT system for pivot translation greatly improves the translation quality.", "labels": [], "entities": [{"text": "pivot translation", "start_pos": 223, "end_pos": 240, "type": "TASK", "confidence": 0.7411218583583832}]}, {"text": "And this translation quality is higher than that of those produced by the system trained with areal Chinese-Spanish corpus; (3) Our sentence-level translation selection method consistently and significantly improves the translation quality over individual translation outputs in all of our experiments.", "labels": [], "entities": [{"text": "sentence-level translation selection", "start_pos": 132, "end_pos": 168, "type": "TASK", "confidence": 0.6797311206658682}]}, {"text": "Section 2 briefly introduces the three pivot translation methods.", "labels": [], "entities": []}, {"text": "Section 3 presents the hybrid method combining SMT and RBMT systems.", "labels": [], "entities": [{"text": "SMT", "start_pos": 47, "end_pos": 50, "type": "TASK", "confidence": 0.9818392992019653}]}, {"text": "Section 4 describes the translation selection method.", "labels": [], "entities": [{"text": "translation selection", "start_pos": 24, "end_pos": 45, "type": "TASK", "confidence": 0.9879533648490906}]}, {"text": "Experimental results are presented in Section 5, followed by a discussion in Section 6.", "labels": [], "entities": []}, {"text": "The last section draws conclusions.", "labels": [], "entities": []}, {"text": "2 Pivot Methods for Phrase-based SMT", "labels": [], "entities": [{"text": "SMT", "start_pos": 33, "end_pos": 36, "type": "TASK", "confidence": 0.7157711386680603}]}], "datasetContent": [{"text": "We used two commercial RBMT systems in our experiments: System A for Chinese-English bidirectional translation and System B for EnglishChinese and English-Spanish translation.", "labels": [], "entities": [{"text": "Chinese-English bidirectional translation", "start_pos": 69, "end_pos": 110, "type": "TASK", "confidence": 0.6623628536860148}, {"text": "English-Spanish translation", "start_pos": 147, "end_pos": 174, "type": "TASK", "confidence": 0.6598481088876724}]}, {"text": "For phrase-based SMT translation, we used the Moses decoder () and its support training scripts.", "labels": [], "entities": [{"text": "SMT translation", "start_pos": 17, "end_pos": 32, "type": "TASK", "confidence": 0.9247083365917206}]}, {"text": "We ran the decoder with its default settings and then used Moses' implementation of minimum error rate training to tune the feature weights on the development set.", "labels": [], "entities": [{"text": "minimum error rate", "start_pos": 84, "end_pos": 102, "type": "METRIC", "confidence": 0.7107638120651245}]}, {"text": "To select translation among outputs produced by different pivot translation systems, we used SVM-light) to perform support vector regression with the linear kernel.", "labels": [], "entities": []}, {"text": "Translation quality was evaluated using both the BLEU score proposed by and also the modified BLEU (BLEU-Fix) score 3 used in the IWSLT 2008 evaluation campaign, where the brevity calculation is modified to use closest reference length instead of shortest reference length.", "labels": [], "entities": [{"text": "Translation", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.9353938102722168}, {"text": "BLEU", "start_pos": 49, "end_pos": 53, "type": "METRIC", "confidence": 0.9976820945739746}, {"text": "BLEU (BLEU-Fix) score 3", "start_pos": 94, "end_pos": 117, "type": "METRIC", "confidence": 0.8811694979667664}, {"text": "IWSLT 2008 evaluation campaign", "start_pos": 130, "end_pos": 160, "type": "DATASET", "confidence": 0.8188516050577164}]}], "tableCaptions": [{"text": " Table 2: Training data. SW and TW represent  source words and target words, respectively.", "labels": [], "entities": []}, {"text": " Table 3: CRR/ASR translation results by using  SMT systems", "labels": [], "entities": [{"text": "CRR/ASR translation", "start_pos": 10, "end_pos": 29, "type": "TASK", "confidence": 0.7196663618087769}, {"text": "SMT", "start_pos": 48, "end_pos": 51, "type": "TASK", "confidence": 0.9653093814849854}]}, {"text": " Table 4. From the translation results, it can be  seen that, enlarging the size of training data with  RBMT systems can further improve the translation  quality.", "labels": [], "entities": [{"text": "translation", "start_pos": 19, "end_pos": 30, "type": "TASK", "confidence": 0.956596851348877}]}, {"text": " Table 4: CRR/ASR translation results by using RBMT and SMT systems", "labels": [], "entities": [{"text": "CRR/ASR translation", "start_pos": 10, "end_pos": 29, "type": "TASK", "confidence": 0.5915858522057533}, {"text": "RBMT", "start_pos": 47, "end_pos": 51, "type": "DATASET", "confidence": 0.6554373502731323}, {"text": "SMT", "start_pos": 56, "end_pos": 59, "type": "TASK", "confidence": 0.8788735270500183}]}, {"text": " Table 6: CRR translation results (BLEU scores)  by using different RBMT systems", "labels": [], "entities": [{"text": "CRR translation", "start_pos": 10, "end_pos": 25, "type": "TASK", "confidence": 0.8638584017753601}, {"text": "BLEU", "start_pos": 35, "end_pos": 39, "type": "METRIC", "confidence": 0.9988448619842529}, {"text": "RBMT", "start_pos": 68, "end_pos": 72, "type": "DATASET", "confidence": 0.6929371356964111}]}, {"text": " Table 7: CRR translation results by using multilin- gual corpus. \"/\" separates the BLEU and BLEU- fix scores.", "labels": [], "entities": [{"text": "CRR translation", "start_pos": 10, "end_pos": 25, "type": "TASK", "confidence": 0.9341655969619751}, {"text": "BLEU", "start_pos": 84, "end_pos": 88, "type": "METRIC", "confidence": 0.9976740479469299}, {"text": "BLEU- fix scores", "start_pos": 93, "end_pos": 109, "type": "METRIC", "confidence": 0.9645505547523499}]}, {"text": " Table 8: Comparison with related work", "labels": [], "entities": []}]}