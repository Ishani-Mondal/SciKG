{"title": [], "abstractContent": [{"text": "Efficient processing of tera-scale text data is an important research topic.", "labels": [], "entities": []}, {"text": "This paper proposes lossless compression of N-gram language models based on LOUDS, a succinct data structure.", "labels": [], "entities": [{"text": "LOUDS", "start_pos": 76, "end_pos": 81, "type": "METRIC", "confidence": 0.8556203246116638}]}, {"text": "LOUDS succinctly represents a trie with M nodes as a 2M + 1 bit string.", "labels": [], "entities": []}, {"text": "We compress it further for the N-gram language model structure.", "labels": [], "entities": []}, {"text": "We also use 'variable length coding' and 'block-wise compression' to compress values associated with nodes.", "labels": [], "entities": []}, {"text": "Experimental results for three large-scale N-gram compression tasks achieved a significant compression rate without any loss.", "labels": [], "entities": [{"text": "N-gram compression tasks", "start_pos": 43, "end_pos": 67, "type": "TASK", "confidence": 0.7595271269480387}, {"text": "compression rate", "start_pos": 91, "end_pos": 107, "type": "METRIC", "confidence": 0.9767621159553528}]}], "introductionContent": [{"text": "There has been an increase in available N -gram data and a large amount of web-scaled N -gram data has been successfully deployed in statistical machine translation.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 133, "end_pos": 164, "type": "TASK", "confidence": 0.7240456144014994}]}, {"text": "However, we need either a machine with hundreds of gigabytes of memory or a large computer cluster to handle them.", "labels": [], "entities": []}, {"text": "Either pruning or lossy randomizing approaches) may result in a compact representation for the application run-time.", "labels": [], "entities": []}, {"text": "However, the lossy approaches may reduce accuracy, and tuning is necessary.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.9993733763694763}]}, {"text": "A lossless approach is obviously better than a lossy one if other conditions are the same.", "labels": [], "entities": []}, {"text": "In addtion, a lossless approach can easly combined with pruning.", "labels": [], "entities": [{"text": "addtion", "start_pos": 3, "end_pos": 10, "type": "TASK", "confidence": 0.9891090393066406}]}, {"text": "Therefore, lossless representation of N -gram is a key issue even for lossy approaches.", "labels": [], "entities": []}, {"text": "showed a general Ngram language model structure and introduced a lossless algorithm that compressed a sorted integer vector by recursively shifting a certain number of bits and by emitting index-value inverted vectors.", "labels": [], "entities": []}, {"text": "However, we need more compact representation.", "labels": [], "entities": []}, {"text": "In this work, we propose a succinct way to represent the N -gram language model structure based on LOUDS).", "labels": [], "entities": [{"text": "LOUDS", "start_pos": 99, "end_pos": 104, "type": "METRIC", "confidence": 0.8012214303016663}]}, {"text": "It was first introduced by and requires only a small space close to the information-theoretic lower bound.", "labels": [], "entities": []}, {"text": "For an M node ordinal trie, its information-theoretical lower bound is 2M \u2212 O(lg M ) bits (lg(x) = log 2 (x))  and LOUDS succinctly represents it by a 2M + 1 bit string.", "labels": [], "entities": [{"text": "LOUDS", "start_pos": 115, "end_pos": 120, "type": "METRIC", "confidence": 0.9507946968078613}]}, {"text": "The space is further reduced by considering the N -gram structure.", "labels": [], "entities": []}, {"text": "We also use variable length coding and block-wise compression to compress the values associated with each node, such as word ids, probabilities or counts.", "labels": [], "entities": []}, {"text": "We experimented with English Web 1T 5-gram from LDC consisting of 25 GB of gzipped raw text N -gram counts.", "labels": [], "entities": [{"text": "English Web 1T 5-gram from LDC", "start_pos": 21, "end_pos": 51, "type": "DATASET", "confidence": 0.8284831643104553}]}, {"text": "By using 8-bit floating point quantization 1 , N -gram language models are compressed into 10 GB, which is comparable to a lossy representation).", "labels": [], "entities": []}], "datasetContent": [{"text": "We applied the proposed representation to 5-gram trained by \"English Gigaword 3rd Edition,\" \"English Web 1T 5-gram\" from LDC, and \"Japanese Web 1T 7-gram\" from GSK.", "labels": [], "entities": [{"text": "English Gigaword 3rd Edition", "start_pos": 61, "end_pos": 89, "type": "DATASET", "confidence": 0.9034310430288315}, {"text": "GSK", "start_pos": 160, "end_pos": 163, "type": "DATASET", "confidence": 0.8028803467750549}]}, {"text": "Since their tendencies are the same, we only report in this paper the results on English Web 1T 5-gram, where the size of the count data in gzipped raw text format is 25GB, the number of N-grams is 3.8G, the vocabulary size is 13.6M words, and the number of the highest order N-grams is 1.2G.", "labels": [], "entities": [{"text": "English Web 1T 5-gram", "start_pos": 81, "end_pos": 102, "type": "DATASET", "confidence": 0.9445812255144119}]}, {"text": "We implemented an N -gram indexer/estimator using MPI inspired by the MapReduce implementation of N -gram language model indexing/estimation pipeline (.", "labels": [], "entities": [{"text": "N -gram language model indexing", "start_pos": 98, "end_pos": 129, "type": "TASK", "confidence": 0.6072531392176946}]}, {"text": "We show the initial indexed counts and the final language model size by differentiating compression strategies for the pointers, namely the 4-byte raw value (Trie), the sorted integer compression (Integer) and our succinct representation (Succinct).", "labels": [], "entities": []}, {"text": "The \"block\" indicates block compression.", "labels": [], "entities": []}, {"text": "For the sake of implementation simplicity, the sorted integer compression used a fixed 8-bit shift amount, although the original paper proposed recursively determined optimum shift amounts).", "labels": [], "entities": [{"text": "sorted integer compression", "start_pos": 47, "end_pos": 73, "type": "TASK", "confidence": 0.6138827403386434}]}, {"text": "8-bit quantization was performed for probabilities and back-off coefficients using a simple binning approach.", "labels": [], "entities": []}, {"text": "N -gram counts were reduced from 23.59GB to 10.57GB by our succinct representation with block compression.", "labels": [], "entities": []}, {"text": "N -gram language models of 42.65GB were compressed to 18.37GB.", "labels": [], "entities": []}, {"text": "Finally, the 8-bit quantized N -gram language models are represented by 9.83GB of space.", "labels": [], "entities": []}, {"text": "shows the compression ratio for the pointer table alone.", "labels": [], "entities": [{"text": "compression ratio", "start_pos": 10, "end_pos": 27, "type": "METRIC", "confidence": 0.9652262926101685}]}, {"text": "Block compression employed on raw 4-byte pointers attained a large reduction that was almost comparable to sorted integer compression.", "labels": [], "entities": [{"text": "Block compression", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7733154892921448}]}, {"text": "Since large pointer value tables are sorted, even a generic compression algorithm could achieve better compression.", "labels": [], "entities": []}, {"text": "Using our succinct representation, 2.4 bits are required for each N -gram.", "labels": [], "entities": []}, {"text": "By using the \"flat\" trie structure, we approach closer to its information-theoretic lower bound beyond the LOUDS baseline.", "labels": [], "entities": [{"text": "LOUDS", "start_pos": 107, "end_pos": 112, "type": "METRIC", "confidence": 0.7718629240989685}]}, {"text": "With block compression, we achieved 1.8 bits per N -gram.", "labels": [], "entities": []}, {"text": "shows the effect of variable length coding and block compression for the word ids, counts, probabilities and back-off coefficients.", "labels": [], "entities": []}, {"text": "After variable-length coding, the word id is almost half its original size.", "labels": [], "entities": []}, {"text": "We assign a word id for each", "labels": [], "entities": []}], "tableCaptions": []}