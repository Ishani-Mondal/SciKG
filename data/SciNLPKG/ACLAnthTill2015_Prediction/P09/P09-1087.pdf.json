{"title": [{"text": "Quadratic-Time Dependency Parsing for Machine Translation", "labels": [], "entities": [{"text": "Quadratic-Time Dependency Parsing", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.6357501149177551}, {"text": "Machine Translation", "start_pos": 38, "end_pos": 57, "type": "TASK", "confidence": 0.7417918145656586}]}], "abstractContent": [{"text": "Efficiency is a prime concern in syntactic MT decoding , yet significant developments in statistical parsing with respect to asymptotic efficiency haven't yet been explored in MT.", "labels": [], "entities": [{"text": "Efficiency", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9427107572555542}, {"text": "MT decoding", "start_pos": 43, "end_pos": 54, "type": "TASK", "confidence": 0.915406346321106}, {"text": "statistical parsing", "start_pos": 89, "end_pos": 108, "type": "TASK", "confidence": 0.7174549996852875}, {"text": "MT", "start_pos": 176, "end_pos": 178, "type": "TASK", "confidence": 0.987355649471283}]}, {"text": "Recently, McDonald et al.", "labels": [], "entities": []}, {"text": "(2005b) formalized dependency parsing as a maximum spanning tree (MST) problem , which can be solved in quadratic time relative to the length of the sentence.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 19, "end_pos": 37, "type": "TASK", "confidence": 0.832132488489151}]}, {"text": "They show that MST parsing is almost as accurate as cubic-time dependency parsing in the case of English, and that it is more accurate with free word order languages.", "labels": [], "entities": [{"text": "MST parsing", "start_pos": 15, "end_pos": 26, "type": "TASK", "confidence": 0.9509573876857758}, {"text": "cubic-time dependency parsing", "start_pos": 52, "end_pos": 81, "type": "TASK", "confidence": 0.7031951347986857}]}, {"text": "This paper applies MST parsing to MT, and describes how it can be integrated into a phrase-based decoder to compute dependency language model scores.", "labels": [], "entities": [{"text": "MST parsing", "start_pos": 19, "end_pos": 30, "type": "TASK", "confidence": 0.9678646326065063}, {"text": "MT", "start_pos": 34, "end_pos": 36, "type": "TASK", "confidence": 0.974207878112793}]}, {"text": "Our results show that augmenting a state-of-the-art phrase-based system with this dependency language model leads to significant improvements in TER (0.92%) and BLEU (0.45%) scores on five NIST Chinese-English evaluation test sets.", "labels": [], "entities": [{"text": "TER", "start_pos": 145, "end_pos": 148, "type": "METRIC", "confidence": 0.9991679191589355}, {"text": "BLEU (0.45%) scores", "start_pos": 161, "end_pos": 180, "type": "METRIC", "confidence": 0.9428534030914306}, {"text": "NIST Chinese-English evaluation test sets", "start_pos": 189, "end_pos": 230, "type": "DATASET", "confidence": 0.9138822436332703}]}], "introductionContent": [{"text": "Hierarchical approaches to machine translation have proven increasingly successful in recent years, and often outperform phrase-based systems ( on target-language fluency and adequacy.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 27, "end_pos": 46, "type": "TASK", "confidence": 0.7395870983600616}]}, {"text": "However, their benefits generally come with high computational costs, particularly when chart parsing, such as CKY, is integrated with language models of high orders.", "labels": [], "entities": [{"text": "chart parsing", "start_pos": 88, "end_pos": 101, "type": "TASK", "confidence": 0.7492498457431793}]}, {"text": "Indeed, synchronous CFG parsing with m-grams runs in O(n 3m ) time, where n is the length of the sentence.", "labels": [], "entities": [{"text": "CFG parsing", "start_pos": 20, "end_pos": 31, "type": "TASK", "confidence": 0.722425639629364}, {"text": "O", "start_pos": 53, "end_pos": 54, "type": "METRIC", "confidence": 0.9859148263931274}]}, {"text": "Furthermore, synchronous CFG approaches often only marginally outperform the most com- The algorithmic complexity of is O(n 3+4(m\u22121) ), though present a more efficient factorization inspired by) that yields an overall complexity of O(n 3+3(m\u22121) ), i.e., O(n 3m ).", "labels": [], "entities": [{"text": "O", "start_pos": 120, "end_pos": 121, "type": "METRIC", "confidence": 0.9630268216133118}, {"text": "O", "start_pos": 232, "end_pos": 233, "type": "METRIC", "confidence": 0.9844338297843933}, {"text": "O", "start_pos": 254, "end_pos": 255, "type": "METRIC", "confidence": 0.9920510649681091}]}, {"text": "In comparison, phrase-based decoding can run in linear time if a distortion limit is imposed.", "labels": [], "entities": []}, {"text": "Of course, this comparison holds only for approximate algorithms.", "labels": [], "entities": []}, {"text": "Since exact MT decoding is NP complete, there is no exact search algorithm for either phrase-based or syntactic MT that runs in polynomial time (unless P = NP).", "labels": [], "entities": [{"text": "MT decoding", "start_pos": 12, "end_pos": 23, "type": "TASK", "confidence": 0.9057255685329437}]}, {"text": "petitive phrase-based systems in large-scale experiments such as NIST evaluations.", "labels": [], "entities": []}, {"text": "This lack of significant difference may not be completely surprising.", "labels": [], "entities": []}, {"text": "Indeed, researchers have shown that gigantic language models are key to state-ofthe-art performance (, and the ability of phrase-based decoders to handle large-size, high-order language models with no consequence on asymptotic running time during decoding presents a compelling advantage over CKY decoders, whose time complexity grows prohibitively large with higher-order language models.", "labels": [], "entities": []}, {"text": "While context-free decoding algorithms (CKY, Earley, etc.) may sometimes appear too computationally expensive for high-end statistical machine translation, there are many alternative parsing algorithms that have seldom been explored in the machine translation literature.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 123, "end_pos": 154, "type": "TASK", "confidence": 0.6338315705458323}, {"text": "machine translation", "start_pos": 240, "end_pos": 259, "type": "TASK", "confidence": 0.7244573682546616}]}, {"text": "The parsing literature presents faster alternatives for both phrasestructure and dependency trees, e.g., O(n) shiftreduce parsers and variants, inter alia).", "labels": [], "entities": []}, {"text": "While deterministic parsers are often deemed inadequate for dealing with ambiguities of natural language, highly accurate O(n 2 ) algorithms exist in the case of dependency parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 162, "end_pos": 180, "type": "TASK", "confidence": 0.8208814263343811}]}, {"text": "Building upon the theoretical work of (, present a quadratic-time dependency parsing algorithm that is just 0.7% less accurate than \"full-fledged\" chart parsing (which, in the case of dependency parsing, runs in time O(n 3 )).", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 66, "end_pos": 84, "type": "TASK", "confidence": 0.6912159025669098}, {"text": "full-fledged\" chart parsing", "start_pos": 133, "end_pos": 160, "type": "TASK", "confidence": 0.7599811404943466}, {"text": "dependency parsing", "start_pos": 184, "end_pos": 202, "type": "TASK", "confidence": 0.7906008362770081}]}, {"text": "In this paper, we show how to exploit syntactic dependency structure for better machine translation, under the constraint that the depen-dency structure is built as a by-product of phrasebased decoding, without reliance on a dynamicprogramming or chart parsing algorithm such as CKY or Earley.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 80, "end_pos": 99, "type": "TASK", "confidence": 0.7392438352108002}, {"text": "chart parsing", "start_pos": 247, "end_pos": 260, "type": "TASK", "confidence": 0.7354224622249603}]}, {"text": "Adapting the approach of for machine translation, we incrementally build dependency structure left-toright in time O(n 2 ) during decoding.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 29, "end_pos": 48, "type": "TASK", "confidence": 0.7864683866500854}]}, {"text": "Most interestingly, the time complexity of non-projective dependency parsing remains quadratic as the order of the language model increases.", "labels": [], "entities": [{"text": "non-projective dependency parsing", "start_pos": 43, "end_pos": 76, "type": "TASK", "confidence": 0.7461931506792704}]}, {"text": "This provides a compelling advantage over previous dependency language models for MT, which use a 5-gram LM only during reranking.", "labels": [], "entities": [{"text": "MT", "start_pos": 82, "end_pos": 84, "type": "TASK", "confidence": 0.9720379710197449}]}, {"text": "In our experiments, we build a competitive baseline ( ) incorporating a 5-gram LM trained on a large part of Gigaword and show that our dependency language model provides improvements on five different test sets, with an overall gain of 0.92 in TER and 0.45 in BLEU scores.", "labels": [], "entities": [{"text": "Gigaword", "start_pos": 109, "end_pos": 117, "type": "DATASET", "confidence": 0.9248483777046204}, {"text": "TER", "start_pos": 245, "end_pos": 248, "type": "METRIC", "confidence": 0.9985881447792053}, {"text": "BLEU", "start_pos": 261, "end_pos": 265, "type": "METRIC", "confidence": 0.9991983771324158}]}, {"text": "These results are found to be statistically very significant (p \u2264 .01).", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we compare the performance of our parsing model to the ones of McDonald et al.", "labels": [], "entities": []}, {"text": "Since our MT test sets include newswire, web, and audio, we trained our parser on different genres.", "labels": [], "entities": [{"text": "MT", "start_pos": 10, "end_pos": 12, "type": "TASK", "confidence": 0.9698495268821716}]}, {"text": "Our training data includes newswire from the English translation treebank (LDC2007T02) and the English-Arabic Treebank (LDC2006T10), which are respectively translations of sections of the Chinese treebank (CTB) and Arabic treebank (ATB).", "labels": [], "entities": [{"text": "English translation treebank (LDC2007T02", "start_pos": 45, "end_pos": 85, "type": "DATASET", "confidence": 0.6807465970516204}, {"text": "English-Arabic Treebank (LDC2006T10)", "start_pos": 95, "end_pos": 131, "type": "DATASET", "confidence": 0.8386181414127349}, {"text": "Chinese treebank (CTB) and Arabic treebank (ATB)", "start_pos": 188, "end_pos": 236, "type": "DATASET", "confidence": 0.7783754874359478}]}, {"text": "We also trained the parser on the broadcastnews treebank available in the OntoNotes corpus (LDC2008T04), and added sections 02-21 of the WSJ Penn treebank.", "labels": [], "entities": [{"text": "broadcastnews treebank", "start_pos": 34, "end_pos": 56, "type": "DATASET", "confidence": 0.9617010056972504}, {"text": "OntoNotes corpus (LDC2008T04)", "start_pos": 74, "end_pos": 103, "type": "DATASET", "confidence": 0.908677852153778}, {"text": "WSJ Penn treebank", "start_pos": 137, "end_pos": 154, "type": "DATASET", "confidence": 0.9671592513720194}]}, {"text": "Documents 001-040 of the English CTB data were set aside to constitute a test set for newswire texts.", "labels": [], "entities": [{"text": "English CTB data", "start_pos": 25, "end_pos": 41, "type": "DATASET", "confidence": 0.8959006865819296}]}, {"text": "Our other test set is the standard Section 23 of the Penn treebank.", "labels": [], "entities": [{"text": "Section 23 of the Penn treebank", "start_pos": 35, "end_pos": 66, "type": "DATASET", "confidence": 0.7996981044610342}]}, {"text": "The splits and amounts of data used for training are displayed in.", "labels": [], "entities": []}, {"text": "Parsing experiments are shown in.", "labels": [], "entities": []}, {"text": "We distinguish two experimental conditions: Parsing and MT.", "labels": [], "entities": [{"text": "Parsing", "start_pos": 44, "end_pos": 51, "type": "METRIC", "confidence": 0.6224318146705627}, {"text": "MT", "start_pos": 56, "end_pos": 58, "type": "TASK", "confidence": 0.4897305965423584}]}, {"text": "For Parsing, sentences are cased and tokenization abides to the PTB segmentation as used in the Penn treebank version 3.", "labels": [], "entities": [{"text": "Parsing", "start_pos": 4, "end_pos": 11, "type": "TASK", "confidence": 0.9348090291023254}, {"text": "PTB segmentation", "start_pos": 64, "end_pos": 80, "type": "TASK", "confidence": 0.6131269037723541}, {"text": "Penn treebank version 3", "start_pos": 96, "end_pos": 119, "type": "DATASET", "confidence": 0.9712837189435959}]}, {"text": "For the MT setting, texts are all lowercase, and tokenization was changed to improve machine translation (e.g., most hyphenated words were split).", "labels": [], "entities": [{"text": "MT", "start_pos": 8, "end_pos": 10, "type": "TASK", "confidence": 0.9529078602790833}, {"text": "machine translation", "start_pos": 85, "end_pos": 104, "type": "TASK", "confidence": 0.7102223038673401}]}, {"text": "For this setting, we also had to harmonize the four treebanks.", "labels": [], "entities": []}, {"text": "The most crucial modification was to add NP internal bracketing to the WSJ (, since the three other treebanks contain that information.", "labels": [], "entities": [{"text": "NP internal bracketing", "start_pos": 41, "end_pos": 63, "type": "TASK", "confidence": 0.5969620744387308}, {"text": "WSJ", "start_pos": 71, "end_pos": 74, "type": "DATASET", "confidence": 0.8644240498542786}]}, {"text": "Treebanks were also transformed to be consistent with MT tokenization.", "labels": [], "entities": [{"text": "MT tokenization", "start_pos": 54, "end_pos": 69, "type": "TASK", "confidence": 0.8867012560367584}]}, {"text": "We evaluate MT parsing models on CTB rather than on WSJ, since CTB contains newswire and is thus more representative of MT evaluation conditions.", "labels": [], "entities": [{"text": "MT parsing", "start_pos": 12, "end_pos": 22, "type": "TASK", "confidence": 0.9812707304954529}, {"text": "CTB", "start_pos": 33, "end_pos": 36, "type": "DATASET", "confidence": 0.9421790242195129}, {"text": "WSJ", "start_pos": 52, "end_pos": 55, "type": "DATASET", "confidence": 0.9621005058288574}, {"text": "MT evaluation", "start_pos": 120, "end_pos": 133, "type": "TASK", "confidence": 0.8859110772609711}]}, {"text": "To obtain part-of-speech tags, we use a state-of-the-art maximum-entropy (CMM) tagger ().", "labels": [], "entities": []}, {"text": "In the Parsing setting, we use its best configuration, which reaches a tagging accuracy of 97.25% on standard WSJ test data.", "labels": [], "entities": [{"text": "Parsing", "start_pos": 7, "end_pos": 14, "type": "TASK", "confidence": 0.9278314113616943}, {"text": "accuracy", "start_pos": 79, "end_pos": 87, "type": "METRIC", "confidence": 0.878717303276062}, {"text": "WSJ test data", "start_pos": 110, "end_pos": 123, "type": "DATASET", "confidence": 0.9461150566736857}]}, {"text": "In the MT setting, we need to use a less effective tagger, since we cannot afford to perform Viterbi inference as a by-product of phrase-based decoding.", "labels": [], "entities": [{"text": "MT", "start_pos": 7, "end_pos": 9, "type": "TASK", "confidence": 0.9842221140861511}]}, {"text": "Hence, we use a simpler tagging model that assigns tag ti to word xi by only using features of words x i\u22123 \u00b7 \u00b7 \u00b7 xi , and that does not condition any decision based on any preceding or next tags (t i\u22121 , etc.).", "labels": [], "entities": []}, {"text": "Its performance is 95.02% on the WSJ, and 95.30% on the English CTB.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 33, "end_pos": 36, "type": "DATASET", "confidence": 0.9101923704147339}, {"text": "English CTB", "start_pos": 56, "end_pos": 67, "type": "DATASET", "confidence": 0.9515706300735474}]}, {"text": "Additional experiments reveal two main contributing factors to this drop on WSJ: tagging uncased texts reduces tagging accuracy by about 1%, and using only wordbased features further reduces it by 0.6%.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 76, "end_pos": 79, "type": "DATASET", "confidence": 0.7496250867843628}, {"text": "tagging uncased texts", "start_pos": 81, "end_pos": 102, "type": "TASK", "confidence": 0.8956915934880575}, {"text": "accuracy", "start_pos": 119, "end_pos": 127, "type": "METRIC", "confidence": 0.9727660417556763}]}, {"text": "shows that the accuracy of our truly O(n 2 ) parser is only .25% to .34% worse than the O(n 3 ) implementation of (.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.999612033367157}]}, {"text": "Compared to the state-of-the-art projective parser as implemented in), performance is 1.28% lower on WSJ, but only 0.95% when training on all our available data and using the MT setting.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 101, "end_pos": 104, "type": "DATASET", "confidence": 0.8298860192298889}]}, {"text": "Overall, we believe that the drop of performance is a reasonable price to pay considering the computational constraints imposed by integrating the dependency parser into an MT decoder.", "labels": [], "entities": []}, {"text": "The table also shows again of more than 1% in dependency accuracy by adding ATB, OntoNotes, and WSJ to the English CTB training set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.9896321296691895}, {"text": "ATB", "start_pos": 76, "end_pos": 79, "type": "DATASET", "confidence": 0.5609455704689026}, {"text": "OntoNotes", "start_pos": 81, "end_pos": 90, "type": "DATASET", "confidence": 0.5741435289382935}, {"text": "English CTB training set", "start_pos": 107, "end_pos": 131, "type": "DATASET", "confidence": 0.8941927254199982}]}, {"text": "The four sources were assigned non-uniform weights: we set the weight of the CTB data to be 10 times larger than the other corpora, which seems to work best in our parsing experiments.", "labels": [], "entities": [{"text": "CTB data", "start_pos": 77, "end_pos": 85, "type": "DATASET", "confidence": 0.9106530249118805}]}, {"text": "While this improvement of 1% may seem relatively small considering that the amount of training data is more than 20 times larger in the latter case, it is quite consistent with previous findings in domain adaptation, which is known to be a difficult task.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 198, "end_pos": 215, "type": "TASK", "confidence": 0.7756736576557159}]}, {"text": "For example, shows that training a learning algorithm on the weighted union of different data sets (which is basically what we did) performs almost as well as more involved domain adaptation approaches.", "labels": [], "entities": []}, {"text": "In our experiments, we use a re-implementation of the Moses phrase-based decoder ( . We use the standard features implemented almost exactly as in Moses: four translation features (phrase-based translation probabilities and lexically-weighted probabilities), word penalty, phrase penalty, linear distortion, and language model score.", "labels": [], "entities": []}, {"text": "We also incorporated the lexicalized reordering features of Moses, in order to experiment with a baseline that is stronger than the default Moses configuration.", "labels": [], "entities": []}, {"text": "The language pair for our experiments is Chinese-to-English.", "labels": [], "entities": []}, {"text": "The training data consists of about 28 million English words and 23.3 million Note that our results on WSJ are not exactly the same as those reported in), since we used slightly different head finding rules.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 103, "end_pos": 106, "type": "DATASET", "confidence": 0.8271780610084534}, {"text": "head finding", "start_pos": 188, "end_pos": 200, "type": "TASK", "confidence": 0.9146949648857117}]}, {"text": "To extract dependencies from treebanks, we used the LTH Penn Converter (http:// nlp.cs.lth.se/pennconverter/), which extracts dependencies that are almost identical to those used for the CoNLL-2008 Shared Task.", "labels": [], "entities": [{"text": "LTH Penn Converter", "start_pos": 52, "end_pos": 70, "type": "DATASET", "confidence": 0.8811455368995667}]}, {"text": "We constrain the converter not to use functional tags found in the treebanks, in order to make it possible to use automatically parsed texts (i.e., perform selftraining) in future work.", "labels": [], "entities": []}, {"text": "Chinese words drawn from various news parallel corpora distributed by the Linguistic Data Consortium (LDC).", "labels": [], "entities": [{"text": "Linguistic Data Consortium (LDC)", "start_pos": 74, "end_pos": 106, "type": "DATASET", "confidence": 0.7947321136792501}]}, {"text": "In order to provide experiments comparable to previous work, we used the same corpora as (): LDC2002E18, LDC2003E07, LDC2003E14, LDC2005E83, LDC2005T06, LDC2006E26, LDC2006E8, and LDC2006G05.", "labels": [], "entities": []}, {"text": "Chinese words were automatically segmented with a conditional random field (CRF) classifier () that conforms to the Chinese Treebank (CTB) standard.", "labels": [], "entities": [{"text": "Chinese Treebank (CTB) standard", "start_pos": 116, "end_pos": 147, "type": "DATASET", "confidence": 0.9485036532084147}]}, {"text": "In order to train a competitive baseline given our computational resources, we built a large 5-gram language model using the Xinhua and AFP sections of the Gigaword corpus (LDC2007T40) in addition to the target side of the parallel data.", "labels": [], "entities": [{"text": "Gigaword corpus", "start_pos": 156, "end_pos": 171, "type": "DATASET", "confidence": 0.9240736961364746}]}, {"text": "This data represents a total of about 700 million words.", "labels": [], "entities": []}, {"text": "We manually removed documents of Gigaword that were released during periods that overlap with those of our development and test sets.", "labels": [], "entities": []}, {"text": "The language model was smoothed with the modified Kneser-Ney algorithm as implemented in), and we only kept 4-grams and 5-grams that occurred at least three times in the training data.", "labels": [], "entities": []}, {"text": "For tuning and testing, we use the official NIST MT evaluation data for Chinese from 2002 to 2008 (MT02 to MT08), which all have four English references for each input sentence.", "labels": [], "entities": [{"text": "NIST MT evaluation data", "start_pos": 44, "end_pos": 67, "type": "DATASET", "confidence": 0.9127912223339081}, {"text": "MT02", "start_pos": 99, "end_pos": 103, "type": "DATASET", "confidence": 0.8418340682983398}]}, {"text": "We used the 1082 sentences of MT05 for tuning and all other sets for testing.", "labels": [], "entities": [{"text": "MT05", "start_pos": 30, "end_pos": 34, "type": "DATASET", "confidence": 0.9287164211273193}]}, {"text": "Parameter tuning was done with minimum error rate training, which was used to maximize BLEU ().", "labels": [], "entities": [{"text": "Parameter tuning", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.808832049369812}, {"text": "BLEU", "start_pos": 87, "end_pos": 91, "type": "METRIC", "confidence": 0.9956218600273132}]}, {"text": "Since MERT is prone to search errors, especially with large numbers of parameters, we ran each tuning experiment three times with different initial conditions.", "labels": [], "entities": [{"text": "MERT", "start_pos": 6, "end_pos": 10, "type": "METRIC", "confidence": 0.42690882086753845}]}, {"text": "We used n-best lists of size 200 and abeam size of 200.", "labels": [], "entities": []}, {"text": "In the final evaluations, we report results using both TER () and the original BLEU metric as described in).", "labels": [], "entities": [{"text": "TER", "start_pos": 55, "end_pos": 58, "type": "METRIC", "confidence": 0.9989650249481201}, {"text": "BLEU metric", "start_pos": 79, "end_pos": 90, "type": "METRIC", "confidence": 0.973345011472702}]}, {"text": "All our evaluations are performed on uncased texts.", "labels": [], "entities": []}, {"text": "The results for our translation experiments are shown in.", "labels": [], "entities": []}, {"text": "We compared two systems: one with the set of features described earlier in this section.", "labels": [], "entities": []}, {"text": "The second system incorporates one additional feature, which is the dependency language: MT experiments with and without a dependency language model.", "labels": [], "entities": [{"text": "MT", "start_pos": 89, "end_pos": 91, "type": "TASK", "confidence": 0.9544286727905273}]}, {"text": "We use randomization tests () to determine significance: differences marked with a (*) are significant at the p \u2264 .05 level, and those marked as (**) are significant at the p \u2264 .01 level.", "labels": [], "entities": []}, {"text": "model score computed with the dependency parsing algorithm described in Section 2.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 30, "end_pos": 48, "type": "TASK", "confidence": 0.7105946987867355}]}, {"text": "We used the dependency model trained on the English CTB and ATB treebank, WSJ, and OntoNotes.", "labels": [], "entities": [{"text": "English CTB and ATB treebank", "start_pos": 44, "end_pos": 72, "type": "DATASET", "confidence": 0.8352695345878601}, {"text": "WSJ", "start_pos": 74, "end_pos": 77, "type": "DATASET", "confidence": 0.7801654934883118}, {"text": "OntoNotes", "start_pos": 83, "end_pos": 92, "type": "DATASET", "confidence": 0.8850405216217041}]}, {"text": "We see that the Moses decoder with integrated dependency language model systematically outperforms the Moses baseline.", "labels": [], "entities": []}, {"text": "For BLEU evaluations, differences are significant in four out of six cases, and in the case of TER, all differences are significant.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.9852421879768372}, {"text": "TER", "start_pos": 95, "end_pos": 98, "type": "METRIC", "confidence": 0.8641826510429382}]}, {"text": "Regarding the small difference in BLEU scores on MT08, we would like to point out that tuning on MT05 and testing on MT08 had a rather adverse effect with respect to translation length: while the two systems are relatively close in terms of BLEU scores, the dependency LM provides a much bigger gain when evaluated with BLEU precision, i.e., by ignoring the brevity penalty.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 34, "end_pos": 38, "type": "METRIC", "confidence": 0.9971670508384705}, {"text": "MT08", "start_pos": 49, "end_pos": 53, "type": "DATASET", "confidence": 0.9253114461898804}, {"text": "MT05", "start_pos": 97, "end_pos": 101, "type": "DATASET", "confidence": 0.9492998719215393}, {"text": "MT08", "start_pos": 117, "end_pos": 121, "type": "DATASET", "confidence": 0.9656612873077393}, {"text": "BLEU", "start_pos": 241, "end_pos": 245, "type": "METRIC", "confidence": 0.9954448938369751}, {"text": "BLEU precision", "start_pos": 320, "end_pos": 334, "type": "METRIC", "confidence": 0.9112631380558014}]}, {"text": "On the other hand, the difference on MT08 is significant in terms of TER.", "labels": [], "entities": [{"text": "MT08", "start_pos": 37, "end_pos": 41, "type": "DATASET", "confidence": 0.8601863980293274}, {"text": "TER", "start_pos": 69, "end_pos": 72, "type": "METRIC", "confidence": 0.9993813037872314}]}, {"text": "provides experimental results on the NIST test data (excluding the tuning set MT05) for each of the three genres: newswire, web data, and speech (broadcast news and conversation).", "labels": [], "entities": [{"text": "NIST test data", "start_pos": 37, "end_pos": 51, "type": "DATASET", "confidence": 0.9771124124526978}, {"text": "MT05", "start_pos": 78, "end_pos": 82, "type": "DATASET", "confidence": 0.8384562730789185}]}, {"text": "The last column displays results for all test sets combined.", "labels": [], "entities": []}, {"text": "Results do not suggest any noticeable difference between genres, and the dependency language model provides significant gains on all genres, despite the fact that this model was primarily trained on news data.", "labels": [], "entities": []}, {"text": "We wish to emphasize that our positive results are particularly noteworthy because they are achieved over a baseline incorporating a competitive 5-gram language model.", "labels": [], "entities": []}, {"text": "As is widely acknowledged in the speech community, it can be difficult to outperform high-order n-gram models in large-scale experiments.", "labels": [], "entities": []}, {"text": "Finally, we quantified the effective running time of our phrase-based decoder with and without our dependency language   model using MT05.", "labels": [], "entities": [{"text": "MT05", "start_pos": 133, "end_pos": 137, "type": "DATASET", "confidence": 0.9269076585769653}]}, {"text": "In both settings, we selected the best tuned model, which yield the performance shown in the first column of.", "labels": [], "entities": []}, {"text": "Our decoder was run on an AMD Opteron Processor 2216 with 16GB of memory, and without resorting to any rescoring method such as cube pruning.", "labels": [], "entities": []}, {"text": "In the case of English translations of 40 words and shorter, the baseline system took 6.5 seconds per sentence, whereas the dependency LM system spent 15.6 seconds per sentence, i.e., 2.4 times the baseline running time.", "labels": [], "entities": []}, {"text": "In the case of translations longer than 40 words, average speeds were respectively 17.5 and 59.5 seconds per sentence, i.e., the dependency was only 3.4 times slower.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Features for dependency parsing. It is quite similar  to the McDonald (2005a) feature set, except that it does not  include the set of all POS tags that appear between each can- didate head-modifier pair (i, j). This modification is essential  in order to make our parser run in true O(n 2 ) time, as opposed  to (McDonald et al., 2005b).", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 23, "end_pos": 41, "type": "TASK", "confidence": 0.8639302849769592}]}, {"text": " Table 3: Characteristics of our training data. The second col- umn identifies documents and sections selected for training.", "labels": [], "entities": []}, {"text": " Table 5: MT experiments with and without a dependency language model. We use randomization tests (", "labels": [], "entities": [{"text": "MT", "start_pos": 10, "end_pos": 12, "type": "TASK", "confidence": 0.9695742130279541}]}, {"text": " Table 6: Test set performances on MT02-MT04 and MT06- MT08, where the data was broken down by genre. Given  the large amount of test data involved in this table, all these  results are statistically highly significant (p \u2264 .01).", "labels": [], "entities": [{"text": "MT02-MT04", "start_pos": 35, "end_pos": 44, "type": "DATASET", "confidence": 0.9520075917243958}, {"text": "MT06- MT08", "start_pos": 49, "end_pos": 59, "type": "DATASET", "confidence": 0.7719505627950033}]}]}