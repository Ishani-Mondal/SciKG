{"title": [{"text": "Sentence Diagram Generation Using Dependency Parsing", "labels": [], "entities": [{"text": "Sentence Diagram Generation", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.9493604898452759}]}], "abstractContent": [{"text": "Dependency parsers show syntactic relations between words using a directed graph, but comparing dependency parsers is difficult because of differences in theoretical models.", "labels": [], "entities": [{"text": "Dependency parsers", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7427180707454681}, {"text": "dependency parsers", "start_pos": 96, "end_pos": 114, "type": "TASK", "confidence": 0.7030842304229736}]}, {"text": "We describe a system to convert dependency models to a structural grammar used in grammar education.", "labels": [], "entities": []}, {"text": "Doing so highlights features that are potentially overlooked in the dependency graph, as well as exposing potential weaknesses and limitations in parsing models.", "labels": [], "entities": []}, {"text": "Our system performs automated analysis of dependency relations and uses them to populate a data structure we designed to emulate sentence diagrams.", "labels": [], "entities": []}, {"text": "This is done by mapping dependency relations between words to the relative positions of those words in a sentence diagram.", "labels": [], "entities": []}, {"text": "Using an original metric for judging the accuracy of sentence diagrams, we achieve precision of 85%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.9979421496391296}, {"text": "precision", "start_pos": 83, "end_pos": 92, "type": "METRIC", "confidence": 0.9996284246444702}]}, {"text": "Multiple causes for errors are presented as potential areas for improvement in dependency parsers.", "labels": [], "entities": [{"text": "dependency parsers", "start_pos": 79, "end_pos": 97, "type": "TASK", "confidence": 0.7210851609706879}]}, {"text": "1 Dependency parsing Dependencies are generally considered a strong metric of accuracy in parse trees, as described in (Lin, 1995).", "labels": [], "entities": [{"text": "Dependency parsing", "start_pos": 2, "end_pos": 20, "type": "TASK", "confidence": 0.6810414046049118}, {"text": "accuracy", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.9982625842094421}]}, {"text": "Ina dependency parse, words are connected to each other through relations, with ahead word (the governor) being modified by a dependent word.", "labels": [], "entities": []}, {"text": "By converting parse trees to dependency representations before judging accuracy, more detailed syntactic information can be discovered.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 71, "end_pos": 79, "type": "METRIC", "confidence": 0.9971597194671631}]}, {"text": "Recently, however, a number of dependency parsers have been developed that have very different theories of a correct model of dependencies.", "labels": [], "entities": []}, {"text": "Dependency parsers define syntactic relations between words in a sentence.", "labels": [], "entities": [{"text": "Dependency parsers", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7712612450122833}]}, {"text": "This can be done either through spanning tree search as in (McDon-ald et al., 2005), which is computationally expensive , or through analysis of another modeling system , such as a phrase structure parse tree, which can introduce errors from the long pipeline.", "labels": [], "entities": [{"text": "phrase structure parse tree", "start_pos": 181, "end_pos": 208, "type": "TASK", "confidence": 0.7077261507511139}]}, {"text": "To the best of our knowledge, the first use of dependency relations as an evaluation tool for parse trees was in (Lin, 1995), which described a process for determining heads in phrase structures and assigning modifiers to those heads appropriately.", "labels": [], "entities": []}, {"text": "Because of different ways to describe relations between negations, conjunctions, and other grammatical structures, it was immediately clear that comparing different models would be difficult.", "labels": [], "entities": []}, {"text": "Research into this area of evaluation produced several new dependency parsers, each using different theories of what constitutes a correct parse.", "labels": [], "entities": [{"text": "dependency parsers", "start_pos": 59, "end_pos": 77, "type": "TASK", "confidence": 0.7121063768863678}]}, {"text": "In addition, attempts to model multiple parse trees in a single dependency relation system were often stymied by problems such as differences in tokenization systems.", "labels": [], "entities": []}, {"text": "These problems are discussed by (Lin, 1998) in greater detail.", "labels": [], "entities": []}, {"text": "An attempt to reconcile differences between parsers was described in (Marneffe et al., 2006).", "labels": [], "entities": []}, {"text": "In this paper, a dependency parser (from herein referred to as the Stanford parser) was developed and compared to two other systems: MINIPAR, described in (Lin, 1998), and the Link parser of (Sleator and Temperley, 1993), which uses a radically different approach but produces a similar, if much more fine-grained, result.", "labels": [], "entities": [{"text": "dependency parser", "start_pos": 17, "end_pos": 34, "type": "TASK", "confidence": 0.7352248132228851}]}, {"text": "Comparing dependency parsers is difficult.", "labels": [], "entities": []}, {"text": "The main problem is that there is no clearway to compare models which mark dependencies differently.", "labels": [], "entities": []}, {"text": "For instance, when clauses are linked by a conjunction , the Link parser considers the conjunction related to the subject of a clause, while the Stan-ford parser links the conjunction to the verb of a clause.", "labels": [], "entities": []}, {"text": "In (Marneffe et al., 2006), a simple comparison was used to alleviate this problem, which was based only on the presence of dependencies, without semantic information.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "In order to test our conversion algorithm, a large number of sentence diagrams were needed in order to ensure a wide range of structures.", "labels": [], "entities": []}, {"text": "We decided to use an undergraduate-level English grammar textbook that uses diagramming as a teaching tool for two reasons.", "labels": [], "entities": []}, {"text": "The first is a pragmatic matter: the sentences have already been diagrammed accurately for comparison to algorithm output.", "labels": [], "entities": []}, {"text": "Second, the breadth of examples necessary to allow students a thorough understanding of the process is beneficial in assuring the completeness of the conversion system.", "labels": [], "entities": []}, {"text": "Cases that are especially difficult for students are also likely to be stressed with multiple examples, giving more opportunities to determine the problem if parsers have similar difficulty.", "labels": [], "entities": []}, {"text": "Therefore, () was selected to be used as the source of this testing data.", "labels": [], "entities": []}, {"text": "This textbook contained 292 sentences, 152 from examples and 140 from solutions to problem sets.", "labels": [], "entities": []}, {"text": "50% of the example sentences (76 in total, chosen by selecting every other example) were set aside to use for development.", "labels": [], "entities": []}, {"text": "The remaining 216 sentences were used to gauge the accuracy of the conversion algorithm.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.9992508292198181}]}, {"text": "Our implementation of this algorithm was developed as an extension of the Stanford dependency parser.", "labels": [], "entities": []}, {"text": "We developed two metrics of precision to evaluate the accuracy of a diagram.", "labels": [], "entities": [{"text": "precision", "start_pos": 28, "end_pos": 37, "type": "METRIC", "confidence": 0.9990459084510803}, {"text": "accuracy", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.9988127946853638}]}, {"text": "The first approach, known as the inheritance metric, scored the results of the algorithm based on the parent of each word in the output sentence diagram.", "labels": [], "entities": []}, {"text": "Head words were judged on their placement in the correct slot, while modifiers were judged on whether they modified the correct parent word.", "labels": [], "entities": []}, {"text": "The second approach, known as the orientation metric, judged each word based solely on its orientation.", "labels": [], "entities": []}, {"text": "This distinction judges whether a word was correctly identified as a primary or modifying element of a sentence.", "labels": [], "entities": []}, {"text": "These scoring systems have various advantages.", "labels": [], "entities": []}, {"text": "By only scoring a word based on its immediate parent, a single mistake in the diagram does not severely impact the result of the score, even if it is at a high level in the diagram.", "labels": [], "entities": []}, {"text": "Certain mistakes are affected by one scoring system but not the other; for instance, incorrect prepositional phrase attachment will not have an effect on the orientation score, but will reduce the value of the inheritance score.", "labels": [], "entities": [{"text": "prepositional phrase attachment", "start_pos": 95, "end_pos": 126, "type": "TASK", "confidence": 0.722131590048472}, {"text": "orientation score", "start_pos": 158, "end_pos": 175, "type": "METRIC", "confidence": 0.8729162812232971}]}, {"text": "Alternatively, a mistake such as failing to label a modifying word as a participial modifier will reduce the orientation score, but will not reduce the value of the inheritance score.", "labels": [], "entities": [{"text": "orientation score", "start_pos": 109, "end_pos": 126, "type": "METRIC", "confidence": 0.9337198138237}]}, {"text": "Generally, orientation scoring is more forgiving than inheritance scoring.", "labels": [], "entities": [{"text": "orientation scoring", "start_pos": 11, "end_pos": 30, "type": "TASK", "confidence": 0.9262085556983948}, {"text": "inheritance scoring", "start_pos": 54, "end_pos": 73, "type": "TASK", "confidence": 0.8344215452671051}]}], "tableCaptions": [{"text": " Table 2: Precision of diagramming algorithm on testing data.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9678521752357483}, {"text": "diagramming", "start_pos": 23, "end_pos": 34, "type": "TASK", "confidence": 0.9709463119506836}]}]}