{"title": [{"text": "Where's the Verb? Correcting Machine Translation During Question Answering", "labels": [], "entities": [{"text": "Correcting Machine Translation", "start_pos": 18, "end_pos": 48, "type": "TASK", "confidence": 0.5802914003531138}, {"text": "Question Answering", "start_pos": 56, "end_pos": 74, "type": "TASK", "confidence": 0.6894542425870895}]}], "abstractContent": [{"text": "When a multilingual question-answering (QA) system provides an answer that has been incorrectly translated, it is very likely to be regarded as irrelevant.", "labels": [], "entities": []}, {"text": "In this paper, we propose a novel method for correcting a deletion error that affects overall understanding of the sentence.", "labels": [], "entities": []}, {"text": "Our post-editing technique uses information available at query time: examples drawn from related documents determined to be relevant to the query.", "labels": [], "entities": []}, {"text": "Our results show that 4%-7% of MT sentences are missing the main verb and on average, 79% of the modified sentences are judged to be more comprehensible.", "labels": [], "entities": [{"text": "MT sentences", "start_pos": 31, "end_pos": 43, "type": "TASK", "confidence": 0.91580730676651}]}, {"text": "The QA performance also benefits from the improved MT: 7% of irrelevant response sentences become relevant.", "labels": [], "entities": [{"text": "MT", "start_pos": 51, "end_pos": 53, "type": "METRIC", "confidence": 0.5443260073661804}]}], "introductionContent": [{"text": "We are developing a multi-lingual questionanswering (QA) system that must provide relevant English answers fora given query, drawing pieces of the answer from translated foreign source.", "labels": [], "entities": []}, {"text": "Relevance and translation quality are usually inseparable: an incorrectly translated sentence in the answer is very likely to be regarded as irrelevant even when the corresponding source language sentence is actually relevant.", "labels": [], "entities": []}, {"text": "We use a phrase-based statistical machine translation system for the MT component and thus, for us, MT serves as a black box that produces the translated documents in our corpus; we cannot change the MT system itself.", "labels": [], "entities": [{"text": "phrase-based statistical machine translation", "start_pos": 9, "end_pos": 53, "type": "TASK", "confidence": 0.5869327038526535}, {"text": "MT", "start_pos": 69, "end_pos": 71, "type": "TASK", "confidence": 0.9653726816177368}]}, {"text": "As MT is used in more and more multi-lingual applications, this situation will become quite common.", "labels": [], "entities": [{"text": "MT", "start_pos": 3, "end_pos": 5, "type": "TASK", "confidence": 0.9799137115478516}]}, {"text": "We propose a novel method which uses redundant information available at questionanswering time to correct errors.", "labels": [], "entities": []}, {"text": "We present a post-editing mechanism to both detect and correct errors in translated documents determined to be relevant for the response.", "labels": [], "entities": []}, {"text": "In this paper, we focus on cases where the main verb of a Chinese sentence has not been translated.", "labels": [], "entities": []}, {"text": "The main verb usually plays a crucial role in conveying the meaning of a sentence.", "labels": [], "entities": [{"text": "conveying the meaning of a sentence", "start_pos": 46, "end_pos": 81, "type": "TASK", "confidence": 0.8563131193319956}]}, {"text": "In cases where only the main verb is missing, an MT score relying on edit distance (e.g., TER or Bleu) maybe high, but the sentence may nonetheless be incomprehensible.", "labels": [], "entities": [{"text": "MT score", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.7409310936927795}, {"text": "TER", "start_pos": 90, "end_pos": 93, "type": "METRIC", "confidence": 0.9767593145370483}, {"text": "Bleu", "start_pos": 97, "end_pos": 101, "type": "METRIC", "confidence": 0.9278583526611328}]}, {"text": "Handling this problem at query time rather than during SMT gives us valuable information which was not available during SMT, namely, a set of related sentences and their translations which may contain the missing verb.", "labels": [], "entities": [{"text": "SMT", "start_pos": 55, "end_pos": 58, "type": "TASK", "confidence": 0.9497143030166626}, {"text": "SMT", "start_pos": 120, "end_pos": 123, "type": "TASK", "confidence": 0.9693500399589539}]}, {"text": "By using translation examples of verb phrases and alignment information in the related documents, we are able to find an appropriate English verb and embed it in the right position as the main verb in order to improve MT quality.", "labels": [], "entities": [{"text": "MT", "start_pos": 218, "end_pos": 220, "type": "TASK", "confidence": 0.9946485161781311}]}, {"text": "A missing main verb can result in an incomprehensible sentence as seen here where the Chinese verb \"\u88ab\u6355\" was not translated at all.", "labels": [], "entities": []}], "datasetContent": [{"text": "For evaluation, we used human judgments of the modified and original MT.", "labels": [], "entities": [{"text": "MT", "start_pos": 69, "end_pos": 71, "type": "TASK", "confidence": 0.8554956316947937}]}, {"text": "We did not have reference translations for the data used by our question-answering system and thus, could not use metrics such as TER or Bleu.", "labels": [], "entities": [{"text": "TER", "start_pos": 130, "end_pos": 133, "type": "METRIC", "confidence": 0.9898616075515747}, {"text": "Bleu", "start_pos": 137, "end_pos": 141, "type": "METRIC", "confidence": 0.894879937171936}]}, {"text": "Moreover, at best, TER or Bleu score would increase by a small amount and that is only if we select the same main verb in the same position as the reference.", "labels": [], "entities": [{"text": "TER", "start_pos": 19, "end_pos": 22, "type": "METRIC", "confidence": 0.9982995390892029}, {"text": "Bleu score", "start_pos": 26, "end_pos": 36, "type": "METRIC", "confidence": 0.9805699288845062}]}, {"text": "Critically, we also know that a missing main verb can cause major problems with comprehension.", "labels": [], "entities": []}, {"text": "Thus, readers could better determine if the modified sentence better captured the meaning of the source sentence.", "labels": [], "entities": []}, {"text": "We also evaluated relevance of a sentence to a query before and after modification.", "labels": [], "entities": []}, {"text": "We recruited 13 Chinese native speakers who are also proficient in English to judge MT quality.", "labels": [], "entities": [{"text": "MT", "start_pos": 84, "end_pos": 86, "type": "TASK", "confidence": 0.9892811179161072}]}, {"text": "Native English speakers cannot tell which translation is better since they do not understand the meaning of the original Chinese.", "labels": [], "entities": []}, {"text": "To judge relevance to the query, we used native English speakers.", "labels": [], "entities": []}, {"text": "Each modified sentence was evaluated by three people.", "labels": [], "entities": []}, {"text": "They were shown the Chinese sentence and two translations, the original MT and the modified one.", "labels": [], "entities": [{"text": "MT", "start_pos": 72, "end_pos": 74, "type": "TASK", "confidence": 0.6298110485076904}]}, {"text": "Evaluators did not know which MT sentence was modified.", "labels": [], "entities": [{"text": "MT sentence", "start_pos": 30, "end_pos": 41, "type": "TASK", "confidence": 0.8629034459590912}]}, {"text": "They were asked to decide which sentence is a better translation, after reading the Chinese sentence.", "labels": [], "entities": []}, {"text": "An evaluator also had the option of answering \"no difference\".", "labels": [], "entities": []}], "tableCaptions": []}