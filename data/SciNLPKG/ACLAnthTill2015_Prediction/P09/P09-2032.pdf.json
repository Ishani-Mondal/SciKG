{"title": [{"text": "A Statistical Machine Translation Model Based on a Synthetic Synchronous Grammar", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 2, "end_pos": 33, "type": "TASK", "confidence": 0.6839786370595297}]}], "abstractContent": [{"text": "Recently, various synchronous grammars are proposed for syntax-based machine translation, e.g. synchronous context-free grammar and synchronous tree (sequence) substitution grammar, either purely formal or linguistically motivated.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 69, "end_pos": 88, "type": "TASK", "confidence": 0.7230657935142517}, {"text": "synchronous tree (sequence) substitution grammar", "start_pos": 132, "end_pos": 180, "type": "TASK", "confidence": 0.6279204956122807}]}, {"text": "Aiming at combining the strengths of different grammars, we describes a synthetic synchronous grammar (SSG), which tentatively in this paper, integrates asynchronous context-free grammar (SCFG) and asynchronous tree sequence substitution grammar (STSSG) for statistical machine translation.", "labels": [], "entities": [{"text": "asynchronous tree sequence substitution grammar (STSSG", "start_pos": 198, "end_pos": 252, "type": "TASK", "confidence": 0.704264121396201}, {"text": "statistical machine translation", "start_pos": 258, "end_pos": 289, "type": "TASK", "confidence": 0.7399452229340872}]}, {"text": "The experimental results on NIST MT05 Chinese-to-English test set show that the SSG based translation system achieves significant improvement over three baseline systems.", "labels": [], "entities": [{"text": "NIST MT05 Chinese-to-English test set", "start_pos": 28, "end_pos": 65, "type": "DATASET", "confidence": 0.9294166445732117}, {"text": "SSG based translation", "start_pos": 80, "end_pos": 101, "type": "TASK", "confidence": 0.7490605513254801}]}], "introductionContent": [{"text": "The use of various synchronous grammar based formalisms has been a trend for statistical machine translation (SMT).", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 77, "end_pos": 114, "type": "TASK", "confidence": 0.8170159558455149}]}, {"text": "The grammar formalism determines the intrinsic capacities and computational efficiency of the SMT systems.", "labels": [], "entities": [{"text": "SMT", "start_pos": 94, "end_pos": 97, "type": "TASK", "confidence": 0.9867426156997681}]}, {"text": "To evaluate the capacity of a grammar formalism, two factors, i.e. generative power and expressive power are usually considered ().", "labels": [], "entities": []}, {"text": "The generative power refers to the ability to generate the strings of the language, and the expressive power to the ability to describe the same language with fewer or no extra ambiguities.", "labels": [], "entities": []}, {"text": "For the current synchronous grammars based SMT, to some extent, the generalization ability of the grammar rules (the usability of the rules for the new sentences) can be considered as a kind of the generative power of the grammar and the disambiguition ability to the rule candidates can be considered as an embodiment of expressive power.", "labels": [], "entities": [{"text": "SMT", "start_pos": 43, "end_pos": 46, "type": "TASK", "confidence": 0.9464899301528931}]}, {"text": "However, the generalization ability and the disambiguition ability often contradict each other in practice such that various grammar formalisms in SMT are actually different trade-off between them.", "labels": [], "entities": [{"text": "SMT", "start_pos": 147, "end_pos": 150, "type": "TASK", "confidence": 0.9785109758377075}]}, {"text": "For instance, in our investigations for SMT (Section 3.1), the Formally SCFG based hierarchical phrase-based model (hereinafter FSCFG) has a better generalization capability than a Linguistically motivated STSSG based model (hereinafter LSTSSG) (, with 5% rules of the former matched by NIST05 test set while only 3.5% rules of the latter matched by the same test set.", "labels": [], "entities": [{"text": "SMT", "start_pos": 40, "end_pos": 43, "type": "TASK", "confidence": 0.9959025979042053}, {"text": "NIST05 test set", "start_pos": 287, "end_pos": 302, "type": "DATASET", "confidence": 0.9698109825452169}]}, {"text": "However, from expressiveness point of view, the former usually results in more ambiguities than the latter.", "labels": [], "entities": []}, {"text": "To combine the strengths of different synchronous grammars, this paper proposes a statistical machine translation model based on a synthetic synchronous grammar (SSG) which syncretizes FSCFG and LSTSSG.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 82, "end_pos": 113, "type": "TASK", "confidence": 0.6140617529551188}, {"text": "FSCFG", "start_pos": 185, "end_pos": 190, "type": "DATASET", "confidence": 0.7575898170471191}]}, {"text": "Moreover, it is noteworthy that, from the combination point of view, our proposed scheme can be considered as a novel system combination method which goes beyond the existing post-decoding style combination of N -best hypotheses from different systems.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our system, named HITREE, is implemented in standard C++ and STL.", "labels": [], "entities": []}, {"text": "In this section we report Scored: The statistics of the counts of the rules in different phases.", "labels": [], "entities": []}, {"text": "'k' means one thousand.", "labels": [], "entities": []}, {"text": "on experiments with Chinese-to-English translation base on it.", "labels": [], "entities": [{"text": "Chinese-to-English translation", "start_pos": 20, "end_pos": 50, "type": "TASK", "confidence": 0.6660746186971664}]}, {"text": "We used FBIS Chinese-to-English parallel corpora (7.2M+9.2M words) as the training data.", "labels": [], "entities": [{"text": "FBIS Chinese-to-English parallel corpora", "start_pos": 8, "end_pos": 48, "type": "DATASET", "confidence": 0.8146297782659531}]}, {"text": "We also used SRI Language Modeling Toolkit to train a 4-gram language model on the Xinhua portion of the English Gigaword corpus(181M words).", "labels": [], "entities": [{"text": "SRI Language Modeling", "start_pos": 13, "end_pos": 34, "type": "TASK", "confidence": 0.6296941836675009}, {"text": "English Gigaword corpus", "start_pos": 105, "end_pos": 128, "type": "DATASET", "confidence": 0.8409485022226969}]}, {"text": "NIST MT2002 test set is used as the development set.", "labels": [], "entities": [{"text": "NIST MT2002 test set", "start_pos": 0, "end_pos": 20, "type": "DATASET", "confidence": 0.8973948657512665}]}, {"text": "The NIST MT2005 test set is used as the test set.", "labels": [], "entities": [{"text": "NIST MT2005 test set", "start_pos": 4, "end_pos": 24, "type": "DATASET", "confidence": 0.8818623423576355}]}, {"text": "The evaluation metric is case-sensitive BLEU4.", "labels": [], "entities": [{"text": "BLEU4", "start_pos": 40, "end_pos": 45, "type": "METRIC", "confidence": 0.983119785785675}]}, {"text": "For significant test, we used Zhang's implementation ()(confidence level of 95%).", "labels": [], "entities": []}, {"text": "For comparisons, we used the following three baseline systems: LSTSSG An in-house implementation of linguistically motivated STSSG based model similar to (.", "labels": [], "entities": []}, {"text": "FSCFG An in-house implementation of purely formally SCFG based model similar to.", "labels": [], "entities": [{"text": "FSCFG", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8664957880973816}]}, {"text": "MBR We use an in-house combination system which is an implementation of a classic sentence level combination method based on the Minimum Bayes Risk (MBR) decoding ().  has obvious more rules than LSTSSG.", "labels": [], "entities": [{"text": "MBR", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9118871092796326}, {"text": "Minimum Bayes Risk (MBR", "start_pos": 129, "end_pos": 152, "type": "METRIC", "confidence": 0.6763299942016602}]}, {"text": "However, in Scored phase, this situation reverses.", "labels": [], "entities": [{"text": "Scored phase", "start_pos": 12, "end_pos": 24, "type": "TASK", "confidence": 0.7487514913082123}]}, {"text": "Interestingly, the situation reverses again in Filtered phase.", "labels": [], "entities": [{"text": "Filtered", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.5250461101531982}]}, {"text": "The reasons for these phenomenons are that FSCFG abstract rules involves high-degree generalization.", "labels": [], "entities": [{"text": "FSCFG abstract rules", "start_pos": 43, "end_pos": 63, "type": "TASK", "confidence": 0.601240317026774}]}, {"text": "Each FSCFG abstract rule averagely have several duplicates 2 in the extracted rule set.", "labels": [], "entities": [{"text": "FSCFG", "start_pos": 5, "end_pos": 10, "type": "DATASET", "confidence": 0.6791409850120544}]}, {"text": "Then, the duplicates will be discarded during scoring.", "labels": [], "entities": []}, {"text": "However, due to the high-degree generalization , the FSCFG abstract rules are more likely to be matched by the test sentences.", "labels": [], "entities": [{"text": "FSCFG", "start_pos": 53, "end_pos": 58, "type": "DATASET", "confidence": 0.7480131983757019}]}, {"text": "Contrastively, LSTSSG rules have more diversified structures and thus weaker generalization capability than FSCFG rules.", "labels": [], "entities": [{"text": "FSCFG", "start_pos": 108, "end_pos": 113, "type": "DATASET", "confidence": 0.7450017929077148}]}, {"text": "From the ratios of two transition states, indicates that HITREE can be considered as compromise of FSCFG between LSTSSG.", "labels": [], "entities": [{"text": "HITREE", "start_pos": 57, "end_pos": 63, "type": "METRIC", "confidence": 0.9774967432022095}, {"text": "FSCFG", "start_pos": 99, "end_pos": 104, "type": "METRIC", "confidence": 0.9843937754631042}]}], "tableCaptions": [{"text": " Table 1: The statistics of the counts of the rules in  different phases. 'k' means one thousand.", "labels": [], "entities": []}, {"text": " Table 2: The Comparison of LSTSSG, FSCFG  ,HITREE and the MBR.", "labels": [], "entities": [{"text": "LSTSSG", "start_pos": 28, "end_pos": 34, "type": "METRIC", "confidence": 0.41233691573143005}, {"text": "FSCFG", "start_pos": 36, "end_pos": 41, "type": "METRIC", "confidence": 0.930837094783783}, {"text": "HITREE", "start_pos": 44, "end_pos": 50, "type": "METRIC", "confidence": 0.9642866253852844}, {"text": "MBR", "start_pos": 59, "end_pos": 62, "type": "DATASET", "confidence": 0.7549978494644165}]}]}