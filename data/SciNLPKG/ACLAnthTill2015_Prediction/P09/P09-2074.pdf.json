{"title": [], "abstractContent": [{"text": "Most approaches to topic modeling assume an independence between documents that is frequently violated.", "labels": [], "entities": [{"text": "topic modeling", "start_pos": 19, "end_pos": 33, "type": "TASK", "confidence": 0.8244128823280334}]}, {"text": "We present an topic model that makes use of one or more user-specified graphs describing relationships between documents.", "labels": [], "entities": []}, {"text": "These graph are encoded in the form of a Markov random field over topics and serve to encourage related documents to have similar topic structures.", "labels": [], "entities": []}, {"text": "Experiments on show upwards of a 10% improvement in modeling performance.", "labels": [], "entities": []}], "introductionContent": [{"text": "One often wishes to apply topic models to large document collections.", "labels": [], "entities": []}, {"text": "In these large collections, we usually have meta-information about how one document relates to another.", "labels": [], "entities": []}, {"text": "Perhaps two documents share an author; perhaps one document cites another; perhaps two documents are published in the same journal or conference.", "labels": [], "entities": []}, {"text": "We often believe that documents related in such away should have similar topical structures.", "labels": [], "entities": []}, {"text": "We encode this in a probabilistic fashion by imposing an (undirected) Markov random field (MRF) on top of a standard topic model (see Section 3).", "labels": [], "entities": [{"text": "Markov random field (MRF)", "start_pos": 70, "end_pos": 95, "type": "METRIC", "confidence": 0.685318261384964}]}, {"text": "The edge potentials in the MRF encode the fact that \"connected\" documents should share similar topic structures, measured by some parameterized distance function.", "labels": [], "entities": []}, {"text": "Inference in the resulting model is complicated by the addition of edge potentials in the MRF.", "labels": [], "entities": [{"text": "MRF", "start_pos": 90, "end_pos": 93, "type": "DATASET", "confidence": 0.7840909957885742}]}, {"text": "We demonstrate that a hybrid Gibbs/MetropolisHastings sampler is able to efficiently explore the posterior distribution (see Section 4).", "labels": [], "entities": [{"text": "MetropolisHastings sampler", "start_pos": 35, "end_pos": 61, "type": "DATASET", "confidence": 0.9215041995048523}]}, {"text": "In experiments (Section 5), we explore several variations on our basic model.", "labels": [], "entities": []}, {"text": "The first is to explore the importance of being able to tune the strength of the potentials in the MRF as part of the inference procedure.", "labels": [], "entities": [{"text": "MRF", "start_pos": 99, "end_pos": 102, "type": "DATASET", "confidence": 0.6228535175323486}]}, {"text": "This turns out to be of utmost importance.", "labels": [], "entities": []}, {"text": "The second is to study the importance of the form of the distance metric used to specify the edge potentials.", "labels": [], "entities": []}, {"text": "Again, this has a significant impact on performance.", "labels": [], "entities": []}, {"text": "Finally, we consider the use of multiple graphs fora single model and find that the power of combined graphs also leads to significantly better models.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our experiments are on a collection for 7441 document abstracts crawled from CiteSeer.", "labels": [], "entities": []}, {"text": "The crawl was seeded with a collection often documents from each of: ACL, EMNLP, SIGIR, ICML, NIPS, UAI.", "labels": [], "entities": [{"text": "EMNLP", "start_pos": 74, "end_pos": 79, "type": "DATASET", "confidence": 0.8290928602218628}, {"text": "UAI", "start_pos": 100, "end_pos": 103, "type": "DATASET", "confidence": 0.8568823337554932}]}, {"text": "This yields 650 thousand words of text after remove stop words.", "labels": [], "entities": []}, {"text": "We use the following graphs (number in parens is the number of edges): auth: shared author (47k) book: shared booktitle/journal (227k) cite: one cites the other (18k) http: source file from same domain (147k) time: published within one year (4122k) year: published in the same year (2101k) Other graph structures are of course possible, but these were the most straightforward to cull.", "labels": [], "entities": []}, {"text": "The first thing we look at is convergence of the samplers for the different graphs..", "labels": [], "entities": []}, {"text": "Here, we can see that the author graph and the citation graph provide improved perplexity to the straightforward LDA model (called \"*none*\"), and that convergence occurs in a few hundred iterations.", "labels": [], "entities": []}, {"text": "Due to their size, the final two graphs led to significantly slower inference than the first four, so results with those graphs are incomplete.", "labels": [], "entities": []}, {"text": "The next item we investigate is whether it is important to tune the graph connectivity weights (the and \u03bb variables).", "labels": [], "entities": []}, {"text": "It turns out this is incredibly important; see.", "labels": [], "entities": []}, {"text": "This is the same set of results as, but without and \u03bb tuning.", "labels": [], "entities": []}, {"text": "We see that the graphbased methods do not improve over the baseline.", "labels": [], "entities": []}, {"text": "Next, we investigate the use of different distance metrics.", "labels": [], "entities": []}, {"text": "We experiments with Bhattacharyya, Hellinger, Euclidean and logisticEuclidean.", "labels": [], "entities": []}, {"text": "See (this is just for the auth graph).", "labels": [], "entities": []}, {"text": "Here, we see that Bhattacharyya and Hellinger (well motivated distances for probability distributions) outperform the Euclidean metrics.", "labels": [], "entities": []}, {"text": "Using Multiple Graphs Finally, we compare results using combinations of graphs.", "labels": [], "entities": []}, {"text": "Here, we run every sampler for 500 iterations and compute standard deviations based on ten runs (year and time are excluded).", "labels": [], "entities": []}, {"text": "Here, we can see that adding graphs (almost) always helps and never hurts.", "labels": [], "entities": []}, {"text": "By adding all the graphs together, we are able to achieve an absolute reduction in perplexity of 9 points (roughly 10%).", "labels": [], "entities": [{"text": "perplexity", "start_pos": 83, "end_pos": 93, "type": "METRIC", "confidence": 0.9271170496940613}]}, {"text": "As discussed, this hinges on the tuning of the graph parameters to allow different graphs to have different amounts of influence.", "labels": [], "entities": []}], "tableCaptions": []}