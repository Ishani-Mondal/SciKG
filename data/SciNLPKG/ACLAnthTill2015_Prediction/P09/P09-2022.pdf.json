{"title": [{"text": "Discriminative Approach to Predicate-Argument Structure Analysis with Zero-Anaphora Resolution", "labels": [], "entities": [{"text": "Predicate-Argument Structure Analysis", "start_pos": 27, "end_pos": 64, "type": "TASK", "confidence": 0.9184780120849609}]}], "abstractContent": [{"text": "This paper presents a predicate-argument structure analysis that simultaneously conducts zero-anaphora resolution.", "labels": [], "entities": [{"text": "predicate-argument structure analysis", "start_pos": 22, "end_pos": 59, "type": "TASK", "confidence": 0.7627214193344116}, {"text": "zero-anaphora resolution", "start_pos": 89, "end_pos": 113, "type": "TASK", "confidence": 0.7594702839851379}]}, {"text": "By adding noun phrases as candidate arguments that are not only in the sentence of the target predicate but also outside of the sentence, our analyzer identifies arguments regardless of whether they appear in the sentence or not.", "labels": [], "entities": []}, {"text": "Because we adopt discrimi-native models based on maximum entropy for argument identification, we can easily add new features.", "labels": [], "entities": [{"text": "argument identification", "start_pos": 69, "end_pos": 92, "type": "TASK", "confidence": 0.7338635921478271}]}, {"text": "We add language model scores as well as contextual features.", "labels": [], "entities": []}, {"text": "We also use contextual information to restrict candidate arguments.", "labels": [], "entities": []}], "introductionContent": [{"text": "Predicate-argument structure analysis is a type of semantic role labeling, which is an important module to extract event information such as \"who did what to whom\" from a sentence.", "labels": [], "entities": [{"text": "Predicate-argument structure analysis", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.8454910318056742}, {"text": "semantic role labeling", "start_pos": 51, "end_pos": 73, "type": "TASK", "confidence": 0.6835695107777914}, {"text": "extract event information such as \"who did what to whom\" from a sentence", "start_pos": 107, "end_pos": 179, "type": "TASK", "confidence": 0.6401195049285888}]}, {"text": "There are many arguments called zero pronouns that do not appear in the surface of a sentence in Japanese.", "labels": [], "entities": []}, {"text": "In this case, predicate-argument structures cannot be constructed if we only rely on the syntactic information of a single sentence.", "labels": [], "entities": []}, {"text": "Similar phenomena also happen in English noun predicates, in which arguments of noun predicates sometimes do not exist in the sentence due to things such as ellipses ().", "labels": [], "entities": [{"text": "English noun predicates", "start_pos": 33, "end_pos": 56, "type": "TASK", "confidence": 0.6572896540164948}]}, {"text": "To correctly extract the structures from such sentences, it is necessary to resolve what zero pronouns refer to by using other information such as context.", "labels": [], "entities": []}, {"text": "Although predicate-argument structure analysis and zero-anaphora resolution are closely related, it was not until recently that these two tasks were lumped together.", "labels": [], "entities": [{"text": "predicate-argument structure analysis", "start_pos": 9, "end_pos": 46, "type": "TASK", "confidence": 0.8330402771631876}, {"text": "zero-anaphora resolution", "start_pos": 51, "end_pos": 75, "type": "TASK", "confidence": 0.7391890287399292}]}, {"text": "Due to the developments of large annotated corpora with predicate-argument and coreference relations (e.g., and with case frames, several works using statistical models have been proposed to solve these two tasks simultaneously (.", "labels": [], "entities": []}, {"text": "In this paper, we present a predicate-argument structure analysis that simultaneously resolves the anaphora of zero pronouns in Japanese, based on supervised learning.", "labels": [], "entities": [{"text": "predicate-argument structure analysis", "start_pos": 28, "end_pos": 65, "type": "TASK", "confidence": 0.741879920164744}]}, {"text": "The analyzer obtains candidate arguments not only from the sentence of the target predicate but also from the previous sentences.", "labels": [], "entities": []}, {"text": "It then identifies the most likely arguments based on discriminative models.", "labels": [], "entities": []}, {"text": "To identify arguments that appear in the sentence and are represented by zero pronouns without distinction, the analyzer introduces the following features and techniques: the language model features of noun phrases, contextual features, and restrictions of candidate arguments.", "labels": [], "entities": []}], "datasetContent": [{"text": "Corpora: We used the NAIST Text Corpus version 1.4b () and the Kyoto Text Corpus 4.0 as the annotated corpora.", "labels": [], "entities": [{"text": "NAIST Text Corpus version 1.4b", "start_pos": 21, "end_pos": 51, "type": "DATASET", "confidence": 0.9783068656921386}, {"text": "Kyoto Text Corpus 4.0", "start_pos": 63, "end_pos": 84, "type": "DATASET", "confidence": 0.9720999896526337}]}, {"text": "We could obtain dependency and predicate-argument structures because these corpora were annotated to almost the same newspaper articles.", "labels": [], "entities": []}, {"text": "We divided them into training, development, and test sets as shown in.", "labels": [], "entities": []}, {"text": "Argument Identification Models: Maximum entropy models were trained using the training set.", "labels": [], "entities": [{"text": "Argument Identification", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.6920014321804047}]}, {"text": "In these experiments, we used the Gaussian prior, and the variance was tuned using the development set.", "labels": [], "entities": []}, {"text": "Candidate argument restrictions were applied during both training and decoding.", "labels": [], "entities": []}, {"text": "Language Models: Language models were trained from twelve years of newspaper articles: Results on the Test Set 5.5M sentences) using the method described in Section 2.2.", "labels": [], "entities": []}, {"text": "However, we eliminated articles that overlap the NAIST Corpus.", "labels": [], "entities": [{"text": "NAIST Corpus", "start_pos": 49, "end_pos": 61, "type": "DATASET", "confidence": 0.9910125136375427}]}, {"text": "Evaluation: We evaluated the precision and recall rates, and F scores, all of which were computed by comparing system output and the correct answer of each argument.", "labels": [], "entities": [{"text": "precision", "start_pos": 29, "end_pos": 38, "type": "METRIC", "confidence": 0.9996015429496765}, {"text": "recall rates", "start_pos": 43, "end_pos": 55, "type": "METRIC", "confidence": 0.9776164293289185}, {"text": "F scores", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.990249365568161}]}, {"text": "We also evaluated the rate at which all arguments of a predicate were completely identified as predicate-argument accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 114, "end_pos": 122, "type": "METRIC", "confidence": 0.9695017337799072}]}], "tableCaptions": [{"text": " Table 3: Results on the Test Set", "labels": [], "entities": [{"text": "Test Set", "start_pos": 25, "end_pos": 33, "type": "DATASET", "confidence": 0.877266138792038}]}]}