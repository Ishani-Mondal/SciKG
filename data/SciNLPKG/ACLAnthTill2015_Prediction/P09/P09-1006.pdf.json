{"title": [], "abstractContent": [{"text": "We address the issue of using heterogeneous treebanks for parsing by breaking it down into two sub-problems, converting grammar formalisms of the treebanks to the same one, and parsing on these homogeneous treebanks.", "labels": [], "entities": [{"text": "parsing", "start_pos": 58, "end_pos": 65, "type": "TASK", "confidence": 0.9737493395805359}]}, {"text": "First we propose to employ an iteratively trained target grammar parser to perform grammar formalism conversion, eliminating prede-fined heuristic rules as required in previous methods.", "labels": [], "entities": [{"text": "grammar formalism conversion", "start_pos": 83, "end_pos": 111, "type": "TASK", "confidence": 0.6537428696950277}]}, {"text": "Then we provide two strategies to refine conversion results, and adopt a corpus weighting technique for parsing on homogeneous treebanks.", "labels": [], "entities": [{"text": "parsing", "start_pos": 104, "end_pos": 111, "type": "TASK", "confidence": 0.9700490832328796}]}, {"text": "Results on the Penn Treebank show that our conversion method achieves 42% error reduction over the previous best result.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 15, "end_pos": 28, "type": "DATASET", "confidence": 0.9938215017318726}, {"text": "error reduction", "start_pos": 74, "end_pos": 89, "type": "METRIC", "confidence": 0.9831943511962891}]}, {"text": "Evaluation on the Penn Chinese Treebank indicates that a converted dependency treebank helps constituency parsing and the use of unlabeled data by self-training further increases parsing f-score to 85.2%, resulting in 6% error reduction over the previous best result.", "labels": [], "entities": [{"text": "Penn Chinese Treebank", "start_pos": 18, "end_pos": 39, "type": "DATASET", "confidence": 0.9782950083414713}, {"text": "constituency parsing", "start_pos": 93, "end_pos": 113, "type": "TASK", "confidence": 0.8484814763069153}, {"text": "error reduction", "start_pos": 221, "end_pos": 236, "type": "METRIC", "confidence": 0.9803650677204132}]}], "introductionContent": [{"text": "The last few decades have seen the emergence of multiple treebanks annotated with different grammar formalisms, motivated by the diversity of languages and linguistic theories, which is crucial to the success of statistical parsing (;).", "labels": [], "entities": [{"text": "statistical parsing", "start_pos": 212, "end_pos": 231, "type": "TASK", "confidence": 0.7844778597354889}]}, {"text": "Availability of multiple treebanks creates a scenario where we have a treebank annotated with one grammar formalism, and another treebank annotated with another grammar formalism that we are interested in.", "labels": [], "entities": []}, {"text": "We call the first a source treebank, and the second a target treebank.", "labels": [], "entities": []}, {"text": "We thus encounter a problem of how to use these heterogeneous treebanks for target grammar parsing.", "labels": [], "entities": [{"text": "target grammar parsing", "start_pos": 76, "end_pos": 98, "type": "TASK", "confidence": 0.6485217809677124}]}, {"text": "Here heterogeneous treebanks refer to two or more treebanks with different grammar formalisms, e.g., one treebank annotated with dependency structure (DS) and the other annotated with phrase structure (PS).", "labels": [], "entities": []}, {"text": "It is important to acquire additional labeled data for the target grammar parsing through exploitation of existing source treebanks since there is often a shortage of labeled data.", "labels": [], "entities": [{"text": "grammar parsing", "start_pos": 66, "end_pos": 81, "type": "TASK", "confidence": 0.719602108001709}]}, {"text": "However, to our knowledge, there is no previous study on this issue.", "labels": [], "entities": []}, {"text": "Recently there have been some works on using multiple treebanks for domain adaptation of parsers, where these treebanks have the same grammar formalism (.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 68, "end_pos": 85, "type": "TASK", "confidence": 0.7319550812244415}]}, {"text": "Other related works focus on converting one grammar formalism of a treebank to another and then conducting studies on the converted treebank ().", "labels": [], "entities": []}, {"text": "These works were done either on multiple treebanks with the same grammar formalism or on only one converted treebank.", "labels": [], "entities": []}, {"text": "We see that their scenarios are different from ours as we work with multiple heterogeneous treebanks.", "labels": [], "entities": []}, {"text": "For the use of heterogeneous treebanks 1 , we propose a two-step solution: (1) converting the grammar formalism of the source treebank to the target one, (2) refining converted trees and using them as additional training data to build a target grammar parser.", "labels": [], "entities": []}, {"text": "For grammar formalism conversion, we choose the DS to PS direction for the convenience of the comparison with existing works.", "labels": [], "entities": [{"text": "grammar formalism conversion", "start_pos": 4, "end_pos": 32, "type": "TASK", "confidence": 0.7307581106821696}]}, {"text": "Specifically, we assume that the source grammar formalism is dependency grammar, and the target grammar formalism is phrase structure grammar.", "labels": [], "entities": [{"text": "phrase structure grammar", "start_pos": 117, "end_pos": 141, "type": "TASK", "confidence": 0.7996599078178406}]}, {"text": "Previous methods for DS to PS conversion () often rely on predefined heuristic rules to eliminate converison ambiguity, e.g., minimal projection for dependents, lowest attachment position for dependents, and the selection of conversion rules that add fewer number of nodes to the converted tree.", "labels": [], "entities": [{"text": "DS to PS conversion", "start_pos": 21, "end_pos": 40, "type": "TASK", "confidence": 0.6363207474350929}]}, {"text": "In addition, the validity of these heuristic rules often depends on their target grammars.", "labels": [], "entities": []}, {"text": "To eliminate the heuristic rules as required in previous methods, we propose to use an existing target grammar parser (trained on the target treebank) to generate N-best parses for each sentence in the source treebank as conversion candidates, and then select the parse consistent with the structure of the source tree as the converted tree.", "labels": [], "entities": []}, {"text": "Furthermore, we attempt to use converted trees as additional training data to retrain the parser for better conversion candidates.", "labels": [], "entities": []}, {"text": "The procedure of tree conversion and parser retraining will be run iteratively until a stopping condition is satisfied.", "labels": [], "entities": [{"text": "tree conversion", "start_pos": 17, "end_pos": 32, "type": "TASK", "confidence": 0.7443132996559143}, {"text": "parser retraining", "start_pos": 37, "end_pos": 54, "type": "TASK", "confidence": 0.8981878757476807}]}, {"text": "Since some converted trees might be imperfect from the perspective of the target grammar, we provide two strategies to refine conversion results: (1) pruning low-quality trees from the converted treebank, (2) interpolating the scores from the source grammar and the target grammar to select better converted trees.", "labels": [], "entities": []}, {"text": "Finally we adopt a corpus weighting technique to get an optimal combination of the converted treebank and the existing target treebank for parser training.", "labels": [], "entities": [{"text": "parser training", "start_pos": 139, "end_pos": 154, "type": "TASK", "confidence": 0.8989748060703278}]}, {"text": "We have evaluated our conversion algorithm on a dependency structure treebank (produced from the Penn Treebank) for comparison with previous work).", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 97, "end_pos": 110, "type": "DATASET", "confidence": 0.995575875043869}]}, {"text": "We also have investigated our two-step solution on two existing treebanks, the Penn Chinese Treebank (CTB) () and the Chinese Dependency Treebank (CDT) 2 ().", "labels": [], "entities": [{"text": "Penn Chinese Treebank (CTB)", "start_pos": 79, "end_pos": 106, "type": "DATASET", "confidence": 0.9669109284877777}, {"text": "Chinese Dependency Treebank (CDT) 2", "start_pos": 118, "end_pos": 153, "type": "DATASET", "confidence": 0.9046793239457267}]}, {"text": "Evaluation on WSJ data demonstrates that it is feasible to use a parser for grammar formalism conversion and the conversion benefits from converted trees used for parser retraining.", "labels": [], "entities": [{"text": "WSJ data", "start_pos": 14, "end_pos": 22, "type": "DATASET", "confidence": 0.864924430847168}, {"text": "grammar formalism conversion", "start_pos": 76, "end_pos": 104, "type": "TASK", "confidence": 0.6681043903032938}]}, {"text": "Our conversion method achieves 93.8% f-score on dependency trees produced from WSJ section 22, resulting in 42% error reduction over the previous best result for DS to PS conversion.", "labels": [], "entities": [{"text": "f-score", "start_pos": 37, "end_pos": 44, "type": "METRIC", "confidence": 0.9937374591827393}, {"text": "WSJ section 22", "start_pos": 79, "end_pos": 93, "type": "DATASET", "confidence": 0.9162418246269226}, {"text": "error reduction", "start_pos": 112, "end_pos": 127, "type": "METRIC", "confidence": 0.9813911020755768}, {"text": "DS to PS conversion", "start_pos": 162, "end_pos": 181, "type": "TASK", "confidence": 0.5690676718950272}]}, {"text": "Results on CTB show that score interpolation is 2 Available at http://ir.hit.edu.cn/.", "labels": [], "entities": [{"text": "CTB", "start_pos": 11, "end_pos": 14, "type": "DATASET", "confidence": 0.9738476872444153}]}, {"text": "more effective than instance pruning for the use of converted treebanks for parsing and converted CDT helps parsing on CTB.", "labels": [], "entities": [{"text": "parsing", "start_pos": 76, "end_pos": 83, "type": "TASK", "confidence": 0.9841798543930054}, {"text": "parsing", "start_pos": 108, "end_pos": 115, "type": "TASK", "confidence": 0.976504921913147}, {"text": "CTB", "start_pos": 119, "end_pos": 122, "type": "DATASET", "confidence": 0.9354338645935059}]}, {"text": "When coupled with self-training technique, a reranking parser with CTB and converted CDT as labeled data achieves 85.2% f-score on CTB test set, an absolute 1.0% improvement (6% error reduction) over the previous best result for Chinese parsing.", "labels": [], "entities": [{"text": "CTB", "start_pos": 67, "end_pos": 70, "type": "DATASET", "confidence": 0.9046458601951599}, {"text": "f-score", "start_pos": 120, "end_pos": 127, "type": "METRIC", "confidence": 0.9928036332130432}, {"text": "CTB test set", "start_pos": 131, "end_pos": 143, "type": "DATASET", "confidence": 0.8817110459009806}, {"text": "error reduction", "start_pos": 178, "end_pos": 193, "type": "METRIC", "confidence": 0.9533441066741943}, {"text": "Chinese parsing", "start_pos": 229, "end_pos": 244, "type": "TASK", "confidence": 0.5818691253662109}]}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we first describe a parser based method for DS to PS conversion, and then we discuss possible strategies to refine conversion results, and finally we adopt the corpus weighting technique for parsing on homogeneous treebanks.", "labels": [], "entities": [{"text": "DS to PS conversion", "start_pos": 58, "end_pos": 77, "type": "TASK", "confidence": 0.5699197724461555}]}, {"text": "Section 3 provides experimental results of grammar formalism conversion on a dependency treebank produced from the Penn Treebank.", "labels": [], "entities": [{"text": "grammar formalism conversion", "start_pos": 43, "end_pos": 71, "type": "TASK", "confidence": 0.6581453680992126}, {"text": "Penn Treebank", "start_pos": 115, "end_pos": 128, "type": "DATASET", "confidence": 0.9920039176940918}]}, {"text": "In Section 4, we evaluate our two-step solution on two existing heterogeneous Chinese treebanks.", "labels": [], "entities": []}, {"text": "Section 5 reviews related work and Section 6 concludes this work.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this experiment we evaluated our conversion algorithm on a larger test set, WSJ section 2\u223c18 and 20\u223c22 (totally 39688 sentences: Results of the generative parser on the development set, when trained with various weighting of CTB training set and CDT PS . this experiment are as same as that in Section 3.1, except that here we used a larger test set.", "labels": [], "entities": [{"text": "WSJ section 2", "start_pos": 79, "end_pos": 92, "type": "DATASET", "confidence": 0.9021088480949402}]}, {"text": "provides the f-scores of our method with Q equal to 0 or 10 on WSJ section 2\u223c18 and 20\u223c22.", "labels": [], "entities": [{"text": "WSJ section 2", "start_pos": 63, "end_pos": 76, "type": "DATASET", "confidence": 0.9210238059361776}]}, {"text": "With Q-10-method, DevScore reached the highest value of 91.6% when q was 1.", "labels": [], "entities": []}, {"text": "Finally Q-10-method achieved an f-score of 93.6% on WSJ section 2\u223c18 and 20\u223c22, better than that of Q-0-method and comparable with that of Q-10-method in Section 3.1.", "labels": [], "entities": [{"text": "f-score", "start_pos": 32, "end_pos": 39, "type": "METRIC", "confidence": 0.995112955570221}, {"text": "WSJ section 2", "start_pos": 52, "end_pos": 65, "type": "DATASET", "confidence": 0.9462818702061971}]}, {"text": "It confirms our previous finding that the conversion benefits from the use of converted trees for parser retraining.", "labels": [], "entities": [{"text": "parser retraining", "start_pos": 98, "end_pos": 115, "type": "TASK", "confidence": 0.9676722884178162}]}, {"text": "We investigated our two-step solution on two existing treebanks, CDT and CTB, and we used CDT as the source treebank and CTB as the target treebank.", "labels": [], "entities": []}, {"text": "CDT consists of 60k Chinese sentences, annotated with POS tag information and dependency structure information (including 28 POS tags, and 24 dependency tags) ().", "labels": [], "entities": [{"text": "CDT", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8599362969398499}]}, {"text": "We did not use POS tag information as inputs to the parser in our conversion method due to the difficulty of conversion from CDT POS tags to CTB POS tags.", "labels": [], "entities": []}, {"text": "We used a standard split of CTB for performance evaluation, articles 1-270 and 400-1151 as training set, articles 301-325 as development set, and articles 271-300 as test set.", "labels": [], "entities": [{"text": "CTB", "start_pos": 28, "end_pos": 31, "type": "DATASET", "confidence": 0.9100903868675232}]}, {"text": "We used Charniak's maximum entropy inspired parser and their reranker) for target grammar parsing, called a generative parser (GP) and a reranking parser (RP) respectively.", "labels": [], "entities": [{"text": "target grammar parsing", "start_pos": 75, "end_pos": 97, "type": "TASK", "confidence": 0.6413321395715078}]}, {"text": "We reported ParseVal measures from the EVALB tool.: Results of the generative parser (GP) and the reranking parser (RP) on the test set, when trained on only CTB training set or an optimal combination of CTB training set and CDT PS .", "labels": [], "entities": [{"text": "ParseVal", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9154860973358154}, {"text": "reranking parser (RP)", "start_pos": 98, "end_pos": 119, "type": "METRIC", "confidence": 0.7243100702762604}, {"text": "CTB training set", "start_pos": 158, "end_pos": 174, "type": "DATASET", "confidence": 0.8711834748586019}]}], "tableCaptions": [{"text": " Table 2: Comparison with the work of Xia et al.  (2008) on WSJ section 22.", "labels": [], "entities": [{"text": "WSJ section 22", "start_pos": 60, "end_pos": 74, "type": "DATASET", "confidence": 0.750528872013092}]}, {"text": " Table 3: Results of our algorithm on WSJ section  2\u223c18 and 20\u223c22.", "labels": [], "entities": [{"text": "WSJ section  2\u223c18", "start_pos": 38, "end_pos": 55, "type": "DATASET", "confidence": 0.8562778949737548}]}, {"text": " Table 4: Results of the generative parser on the de- velopment set, when trained with various weight- ing of CTB training set and CDT P S .", "labels": [], "entities": [{"text": "generative parser", "start_pos": 25, "end_pos": 42, "type": "TASK", "confidence": 0.9609433710575104}, {"text": "CTB training set", "start_pos": 110, "end_pos": 126, "type": "DATASET", "confidence": 0.9004826347033182}]}, {"text": " Table 5: Results of the generative parser (GP) and  the reranking parser (RP) on the test set, when  trained on only CTB training set or an optimal  combination of CTB training set and CDT P S .", "labels": [], "entities": [{"text": "generative parser", "start_pos": 25, "end_pos": 42, "type": "TASK", "confidence": 0.8101800680160522}, {"text": "reranking parser (RP)", "start_pos": 57, "end_pos": 78, "type": "METRIC", "confidence": 0.7858497023582458}, {"text": "CTB training set", "start_pos": 118, "end_pos": 134, "type": "DATASET", "confidence": 0.8864205280939738}, {"text": "CTB training set", "start_pos": 165, "end_pos": 181, "type": "DATASET", "confidence": 0.901028593381246}]}, {"text": " Table 6: Results of the generative parser and the  reranking parser on the test set, when trained on  an optimal combination of CTB training set and  converted CDT.", "labels": [], "entities": [{"text": "generative parser", "start_pos": 25, "end_pos": 42, "type": "TASK", "confidence": 0.9525215923786163}, {"text": "CTB training set", "start_pos": 129, "end_pos": 145, "type": "DATASET", "confidence": 0.8630102276802063}]}, {"text": " Table 7: Results of the self-trained gen- erative parser and updated reranking parser  on the test set.  10\u00d7T+10\u00d7D+P stands for  10\u00d7CTB+10\u00d7CDT P S  \u03bb +PDC.", "labels": [], "entities": [{"text": "gen- erative parser", "start_pos": 38, "end_pos": 57, "type": "TASK", "confidence": 0.6908638626337051}]}, {"text": " Table 8: Results of previous studies on CTB with CTB articles 1-270 as labeled data.", "labels": [], "entities": [{"text": "CTB articles 1-270", "start_pos": 50, "end_pos": 68, "type": "DATASET", "confidence": 0.8614656329154968}]}, {"text": " Table 9: Results of previous studies on CTB with more labeled data.", "labels": [], "entities": []}]}