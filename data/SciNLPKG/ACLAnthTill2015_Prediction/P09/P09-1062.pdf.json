{"title": [{"text": "Summarizing multiple spoken documents: finding evidence from untranscribed audio", "labels": [], "entities": [{"text": "Summarizing multiple spoken documents", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.8951897919178009}]}], "abstractContent": [{"text": "This paper presents a model for summarizing multiple untranscribed spoken documents.", "labels": [], "entities": [{"text": "summarizing multiple untranscribed spoken documents", "start_pos": 32, "end_pos": 83, "type": "TASK", "confidence": 0.8945764422416687}]}, {"text": "Without assuming the availability of transcripts, the model modifies a recently proposed unsupervised algorithm to detect re-occurring acoustic patterns in speech and uses them to estimate similarities between utterances, which are in turn used to identify salient utterances and remove redundancies.", "labels": [], "entities": []}, {"text": "This model is of interest due to its independence from spoken language transcription, an error-prone and resource-intensive process, its ability to integrate multiple sources of information on the same topic, and its novel use of acoustic patterns that extends previous work on low-level prosodic feature detection.", "labels": [], "entities": [{"text": "low-level prosodic feature detection", "start_pos": 278, "end_pos": 314, "type": "TASK", "confidence": 0.6288209035992622}]}, {"text": "We compare the performance of this model with that achieved using manual and automatic transcripts, and find that this new approach is roughly equivalent to having access to ASR transcripts with word error rates in the 33-37% range without actually having to do the ASR, plus it better handles utterances with out-of-vocabulary words.", "labels": [], "entities": []}], "introductionContent": [{"text": "Summarizing spoken documents has been extensively studied over the past several years ().", "labels": [], "entities": []}, {"text": "Conventionally called speech summarization, although speech connotes more than spoken documents themselves, it is motivated by the demand for better ways to navigate spoken content and the natural difficulty in doing sospeech is inherently more linear or sequential than text in its traditional delivery.", "labels": [], "entities": [{"text": "speech summarization", "start_pos": 22, "end_pos": 42, "type": "TASK", "confidence": 0.7090432643890381}]}, {"text": "Previous research on speech summarization has addressed several important problems in this field (see Section 2.1).", "labels": [], "entities": [{"text": "speech summarization", "start_pos": 21, "end_pos": 41, "type": "TASK", "confidence": 0.685269296169281}]}, {"text": "All of this work, however, has focused on single-document summarization and the integration of fairly simplistic acoustic features, inspired by work in descriptive linguistics.", "labels": [], "entities": [{"text": "single-document summarization", "start_pos": 42, "end_pos": 71, "type": "TASK", "confidence": 0.6087397038936615}]}, {"text": "The issues of navigating speech content are magnified when dealing with larger collectionsmultiple spoken documents on the same topic.", "labels": [], "entities": [{"text": "navigating speech content", "start_pos": 14, "end_pos": 39, "type": "TASK", "confidence": 0.843191385269165}]}, {"text": "For example, when one is browsing news broadcasts covering the same events or call-centre recordings related to the same type of customer questions, content redundancy is a prominent issue.", "labels": [], "entities": []}, {"text": "Multi-document summarization on written documents has been studied for more than a decade (see Section 2.2).", "labels": [], "entities": [{"text": "Multi-document summarization", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.7542149722576141}]}, {"text": "Unfortunately, no such effort has been made on audio documents yet.", "labels": [], "entities": []}, {"text": "An obvious way to summarize multiple spoken documents is to adopt the transcribe-andsummarize approach, in which automatic speech recognition (ASR) is first employed to acquire written transcripts.", "labels": [], "entities": [{"text": "summarize multiple spoken documents", "start_pos": 18, "end_pos": 53, "type": "TASK", "confidence": 0.8841893672943115}, {"text": "automatic speech recognition (ASR)", "start_pos": 113, "end_pos": 147, "type": "TASK", "confidence": 0.7752591470877329}]}, {"text": "Speech summarization is accordingly reduced to a text summarization task conducted on error-prone transcripts.", "labels": [], "entities": [{"text": "Speech summarization", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.7122386246919632}, {"text": "text summarization", "start_pos": 49, "end_pos": 67, "type": "TASK", "confidence": 0.7091008275747299}]}, {"text": "Such an approach, however, encounters several problems.", "labels": [], "entities": []}, {"text": "First, assuming the availability of ASR is not always valid for many languages other than English that one may want to summarize.", "labels": [], "entities": [{"text": "ASR", "start_pos": 36, "end_pos": 39, "type": "TASK", "confidence": 0.9456162452697754}]}, {"text": "Even when it is, transcription quality is often an issuetraining ASR models requires collecting and annotating corpora on specific languages, dialects, or even different domains.", "labels": [], "entities": [{"text": "ASR", "start_pos": 65, "end_pos": 68, "type": "TASK", "confidence": 0.9837794899940491}]}, {"text": "Although recognition errors do not significantly impair extractive summarizers (;), error-laden transcripts are not necessarily browseable if recognition errors are higher than certain thresholds ().", "labels": [], "entities": []}, {"text": "In such situations, audio summaries are an alternative when salient content can be identified directly from untranscribed audio.", "labels": [], "entities": [{"text": "audio summaries", "start_pos": 20, "end_pos": 35, "type": "TASK", "confidence": 0.6099244356155396}]}, {"text": "Third, the underlying paradigm of most ASR models aims to solve a classification problem, in which speech is segmented and classified into pre-existing categories (words).", "labels": [], "entities": [{"text": "ASR", "start_pos": 39, "end_pos": 42, "type": "TASK", "confidence": 0.9918375611305237}]}, {"text": "Words not in the predefined dictionary are certain to be misrecognized without exception.", "labels": [], "entities": []}, {"text": "This out-of-vocabulary (OOV) problem is unavoidable in the regular ASR framework, although it is more likely to happen on salient words such as named entities or domain-specific terms.", "labels": [], "entities": [{"text": "ASR", "start_pos": 67, "end_pos": 70, "type": "TASK", "confidence": 0.9653650522232056}]}, {"text": "Our approach uses acoustic evidence from the untranscribed audio stream.", "labels": [], "entities": []}, {"text": "Consider text summarization first: many well-known models such as MMR) and MEAD () rely on the reoccurrence statistics of words.", "labels": [], "entities": [{"text": "text summarization", "start_pos": 9, "end_pos": 27, "type": "TASK", "confidence": 0.6838121712207794}, {"text": "MEAD", "start_pos": 75, "end_pos": 79, "type": "METRIC", "confidence": 0.8123781085014343}]}, {"text": "That is, if we switch any word w 1 with another word w 2 across an entire corpus, the ranking of extracts (often sentences) will be unaffected, because no wordspecific knowledge is involved.", "labels": [], "entities": []}, {"text": "These models have achieved state-of-the-art performance in transcript-based speech summarization.", "labels": [], "entities": [{"text": "transcript-based speech summarization", "start_pos": 59, "end_pos": 96, "type": "TASK", "confidence": 0.5867307980855306}]}, {"text": "For spoken documents, such reoccurrence statistics are available directly from the speech signal.", "labels": [], "entities": []}, {"text": "In recent years, a variant of dynamic time warping (DTW) has been proposed to find reoccurring patterns in the speech signal.", "labels": [], "entities": []}, {"text": "This method has been successfully applied to tasks such as word detection) and topic boundary detection).", "labels": [], "entities": [{"text": "word detection", "start_pos": 59, "end_pos": 73, "type": "TASK", "confidence": 0.8737308382987976}, {"text": "topic boundary detection", "start_pos": 79, "end_pos": 103, "type": "TASK", "confidence": 0.7141920328140259}]}, {"text": "Motivated by the work above, this paper explores the approach to summarizing multiple spoken documents directly over an untranscribed audio stream.", "labels": [], "entities": [{"text": "summarizing multiple spoken documents", "start_pos": 65, "end_pos": 102, "type": "TASK", "confidence": 0.9058891832828522}]}, {"text": "Such a model is of interest because of its independence from ASR.", "labels": [], "entities": [{"text": "ASR", "start_pos": 61, "end_pos": 64, "type": "TASK", "confidence": 0.8399034738540649}]}, {"text": "It is directly applicable to audio recordings in languages or domains when ASR is not possible or transcription quality is low.", "labels": [], "entities": [{"text": "ASR", "start_pos": 75, "end_pos": 78, "type": "TASK", "confidence": 0.9803730249404907}]}, {"text": "In principle, this approach is free from the OOV problem inherent to ASR.", "labels": [], "entities": [{"text": "OOV", "start_pos": 45, "end_pos": 48, "type": "METRIC", "confidence": 0.7221781015396118}, {"text": "ASR", "start_pos": 69, "end_pos": 72, "type": "TASK", "confidence": 0.9560866951942444}]}, {"text": "The premise of this approach, however, is to reliably find reoccuring acoustic patterns in audio, which is challenging because of noise and pronunciation variance existing in the speech signal, as well as the difficulty of finding alignments with proper lengths corresponding to words well.", "labels": [], "entities": []}, {"text": "Therefore, our primary goal in this paper is to empirically determine the extent to which acoustic information alone can effectively replace conventional speech recognition with or without simple prosodic feature detection within the multi-document speech summarization task.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 154, "end_pos": 172, "type": "TASK", "confidence": 0.7447560727596283}, {"text": "prosodic feature detection", "start_pos": 196, "end_pos": 222, "type": "TASK", "confidence": 0.7899539271990458}, {"text": "multi-document speech summarization task", "start_pos": 234, "end_pos": 274, "type": "TASK", "confidence": 0.7163843438029289}]}, {"text": "As shown below, a modification of the Park-Glass approach amounts to the efficacy of a 33-37% WER ASR engine in the domain of multiple spoken document summarization, and also has better treatment of OOV items.", "labels": [], "entities": [{"text": "WER ASR", "start_pos": 94, "end_pos": 101, "type": "TASK", "confidence": 0.5979866534471512}, {"text": "multiple spoken document summarization", "start_pos": 126, "end_pos": 164, "type": "TASK", "confidence": 0.6411828100681305}]}, {"text": "ParkGlass similarity scores by themselves can attribute a high score to distorted paths that, in our context, ultimately leads to too many false-alarm alignments, even after applying the distortion threshold.", "labels": [], "entities": [{"text": "ParkGlass similarity scores", "start_pos": 0, "end_pos": 27, "type": "METRIC", "confidence": 0.6205431520938873}]}, {"text": "We introduce additional distortion penalty and subpath length constraints on their scoring to discourage this possibility.", "labels": [], "entities": [{"text": "distortion penalty", "start_pos": 24, "end_pos": 42, "type": "METRIC", "confidence": 0.9864585697650909}]}], "datasetContent": [{"text": "We use the TDT-4 dataset for our evaluation, which consists of annotated news broadcasts grouped into common topics.", "labels": [], "entities": [{"text": "TDT-4 dataset", "start_pos": 11, "end_pos": 24, "type": "DATASET", "confidence": 0.989302009344101}]}, {"text": "Since our aim in this paper is to study the achievable performance of the audio-based model, we grouped together news stories by their news anchors for each topic.", "labels": [], "entities": []}, {"text": "Then we selected the largest 20 groups for our experiments.", "labels": [], "entities": []}, {"text": "Each of these contained between 5 and 20 articles.", "labels": [], "entities": []}, {"text": "We compare our acoustics-only approach against transcripts produced automatically from two ASR systems.", "labels": [], "entities": [{"text": "ASR", "start_pos": 91, "end_pos": 94, "type": "TASK", "confidence": 0.9097725749015808}]}, {"text": "The first set of transcripts was obtained directly from the TDT-4 database.", "labels": [], "entities": [{"text": "TDT-4 database", "start_pos": 60, "end_pos": 74, "type": "DATASET", "confidence": 0.9688773155212402}]}, {"text": "These transcripts contain a word error rate of 12.6%, which is comparable to the best accuracies obtained in the literature on this data set.", "labels": [], "entities": [{"text": "word error rate", "start_pos": 28, "end_pos": 43, "type": "METRIC", "confidence": 0.7776768604914347}, {"text": "accuracies", "start_pos": 86, "end_pos": 96, "type": "METRIC", "confidence": 0.9813143610954285}]}, {"text": "We also run a custom ASR system designed to produce transcripts at various degrees of accuracy in order to simulate the type of performance one might expect given languages with sparser training corpora.", "labels": [], "entities": [{"text": "ASR", "start_pos": 21, "end_pos": 24, "type": "TASK", "confidence": 0.979363739490509}, {"text": "accuracy", "start_pos": 86, "end_pos": 94, "type": "METRIC", "confidence": 0.9959449172019958}]}, {"text": "These custom acoustic models consist of context-dependent tri-phone units trained on HUB-4 broadcast news data by sequential Viterbi forced alignment.", "labels": [], "entities": [{"text": "HUB-4 broadcast news data", "start_pos": 85, "end_pos": 110, "type": "DATASET", "confidence": 0.8745249062776566}]}, {"text": "During each round of forced alignment, the maximum likelihood linear regression (MLLR) transform is used on gender-dependent models to improve the alignment quality.", "labels": [], "entities": [{"text": "maximum likelihood linear regression (MLLR) transform", "start_pos": 43, "end_pos": 96, "type": "METRIC", "confidence": 0.8093222975730896}]}, {"text": "Language models are also trained on HUB-4 data.", "labels": [], "entities": [{"text": "HUB-4 data", "start_pos": 36, "end_pos": 46, "type": "DATASET", "confidence": 0.9248341917991638}]}, {"text": "Our aim in this paper is to study the achievable performance of the audio-based model.", "labels": [], "entities": []}, {"text": "Instead of evaluating the result against human generated summaries, we directly compare the performance against the summaries obtained by using manual transcripts, which we take as an upper bound to the audio-based system's performance.", "labels": [], "entities": []}, {"text": "This obviously does not preclude using the audio-based system together with other features such as utterance position, length, speaker's roles, and most others used in the literature (.", "labels": [], "entities": []}, {"text": "Here, we do not want our results to be affected by them with the hope of observing the difference accurately.", "labels": [], "entities": []}, {"text": "As such, we quantify success based on ROUGE) scores.", "labels": [], "entities": [{"text": "ROUGE) scores", "start_pos": 38, "end_pos": 51, "type": "METRIC", "confidence": 0.9603990912437439}]}, {"text": "Our goal is to evalu-ate whether the relatedness of spoken documents can reasonably be gleaned solely from the surface acoustic information.", "labels": [], "entities": []}, {"text": "We aim to empirically determine the extent to which acoustic information alone can effectively replace conventional speech recognition within the multi-document speech summarization task.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 116, "end_pos": 134, "type": "TASK", "confidence": 0.7447297871112823}, {"text": "multi-document speech summarization task", "start_pos": 146, "end_pos": 186, "type": "TASK", "confidence": 0.7081310898065567}]}, {"text": "Since ASR performance can vary greatly as we discussed above, we compare our system against automatic transcripts having word error rates of 12.6%, 20.9%, 29.2%, and 35.5% on the same speech source.", "labels": [], "entities": [{"text": "ASR", "start_pos": 6, "end_pos": 9, "type": "TASK", "confidence": 0.9890963435173035}, {"text": "word error rates", "start_pos": 121, "end_pos": 137, "type": "METRIC", "confidence": 0.7181824743747711}]}, {"text": "We changed our language models by restricting the training data so as to obtain the worst WER and then interpolated the corresponding transcripts with the TDT-4 original automatic transcripts to obtain the rest.", "labels": [], "entities": [{"text": "WER", "start_pos": 90, "end_pos": 93, "type": "METRIC", "confidence": 0.975176215171814}]}, {"text": "shows ROUGE scores for our acoustics-only system, as depicted by horizontal lines, as well as those for the extractive summaries given automatic transcripts having different WERs, as depicted by points.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 6, "end_pos": 11, "type": "METRIC", "confidence": 0.9926503300666809}, {"text": "WERs", "start_pos": 174, "end_pos": 178, "type": "METRIC", "confidence": 0.969031035900116}]}, {"text": "Dotted lines represent the 95% confidence intervals of the transcript-based models.", "labels": [], "entities": []}, {"text": "reveals that, typically, as the WERs of automatic transcripts increase to around 33%-37%, the difference between the transcript-based and the acoustics-based models is no longer significant.", "labels": [], "entities": [{"text": "WERs", "start_pos": 32, "end_pos": 36, "type": "METRIC", "confidence": 0.9283522963523865}]}, {"text": "These observations are consistent across summaries with different fixed lengths, namely 10%, 20%, and 30% of the lengths of the source documents for the top, middle, and bottom rows of, respectively.", "labels": [], "entities": []}, {"text": "The consistency of this trend is shown across both ROUGE-2 and ROUGE-SU4, which are the official measures used in the DUC evaluation.", "labels": [], "entities": [{"text": "consistency", "start_pos": 4, "end_pos": 15, "type": "METRIC", "confidence": 0.9966076612472534}, {"text": "ROUGE-2", "start_pos": 51, "end_pos": 58, "type": "METRIC", "confidence": 0.9737496972084045}, {"text": "ROUGE-SU4", "start_pos": 63, "end_pos": 72, "type": "METRIC", "confidence": 0.8292896151542664}, {"text": "DUC evaluation", "start_pos": 118, "end_pos": 132, "type": "DATASET", "confidence": 0.8131003081798553}]}, {"text": "We also varied the MMR parameter \u03bb within atypical range of 0.4-1, which yielded the same observation.", "labels": [], "entities": [{"text": "MMR parameter \u03bb", "start_pos": 19, "end_pos": 34, "type": "METRIC", "confidence": 0.76363605260849}]}, {"text": "Since the acoustics-based approach can be applied to any data domain and to any language in principle, this would be of special interest when those situations yield relatively high WER with conventional ASR.", "labels": [], "entities": [{"text": "WER", "start_pos": 181, "end_pos": 184, "type": "METRIC", "confidence": 0.9849164485931396}, {"text": "ASR", "start_pos": 203, "end_pos": 206, "type": "TASK", "confidence": 0.9436212778091431}]}, {"text": "also shows the ROUGE scores achievable by selecting utterances uniformly at random for extractive summarization, which are significantly lower than all other presented methods and corroborate the usefulness of acoustic information.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 15, "end_pos": 20, "type": "METRIC", "confidence": 0.990148663520813}, {"text": "extractive summarization", "start_pos": 87, "end_pos": 111, "type": "TASK", "confidence": 0.7077081203460693}]}, {"text": "Although our acoustics-based method performs similarly to automatic transcripts with 33-37% WER, the errors observed are not the same, which Figure 2: ROUGE scores and 95% confidence intervals for the MMR-based extractive summaries produced from our acoustics-only approach (horizontal lines), and from ASR-generated transcripts having varying WER (points).", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 151, "end_pos": 156, "type": "METRIC", "confidence": 0.9977499842643738}, {"text": "WER", "start_pos": 344, "end_pos": 347, "type": "METRIC", "confidence": 0.9457049369812012}]}, {"text": "The top, middle, and bottom rows of subfigures correspond to summaries whose lengths are fixed at 10%, 20%, and 30% the sizes of the source text, respectively.", "labels": [], "entities": []}, {"text": "\u03bb in MMR takes 1, 0.7, and 0.4 in these rows, respectively.", "labels": [], "entities": [{"text": "MMR", "start_pos": 5, "end_pos": 8, "type": "TASK", "confidence": 0.8497939109802246}]}, {"text": "we attribute to fundamental differences between these two methods.", "labels": [], "entities": []}, {"text": "presents the number of different utterances correctly selected by the acoustics-based and ASR-based methods across three categories, namely those sentences that are correctly selected by both methods, those appearing only in the acoustics-based summaries, and those appearing only in the ASR-based summaries.", "labels": [], "entities": []}, {"text": "These are shown for summaries having different proportional lengths relative to the source documents and at different WERs.", "labels": [], "entities": [{"text": "WERs", "start_pos": 118, "end_pos": 122, "type": "DATASET", "confidence": 0.6231551766395569}]}, {"text": "Again, correctness here means that the utterance is also selected when using a manual transcript, since that is our defined topline.", "labels": [], "entities": []}, {"text": "A manual analysis of the corpus shows that utterances correctly included in summaries by the acoustics-based method often contain out-ofvocabulary errors in the corresponding ASR transcripts.", "labels": [], "entities": []}, {"text": "For example, given the news topic of the bombing of the U.S. destroyer ship Cole in Yemen, the ASR-based method always mistook the word Cole, which was not in the vocabulary, for cold, khol, and called.", "labels": [], "entities": [{"text": "ASR-based", "start_pos": 95, "end_pos": 104, "type": "TASK", "confidence": 0.9457615613937378}]}, {"text": "Although named entities and domain-specific terms are often highly relevant to the documents in which they are referenced, these types of words are often not included in ASR vocabularies, due to their relative global rarity.", "labels": [], "entities": [{"text": "ASR vocabularies", "start_pos": 170, "end_pos": 186, "type": "TASK", "confidence": 0.9166693687438965}]}, {"text": "Importantly, an unsupervised acoustics-based approach such as ours does not suffer from this fundamental discord.", "labels": [], "entities": []}, {"text": "At the very least, these findings suggest that ASR-based summarization systems augmented with our type of approach might be more robust against out-of-vocabulary errors.", "labels": [], "entities": [{"text": "ASR-based summarization", "start_pos": 47, "end_pos": 70, "type": "TASK", "confidence": 0.8647083342075348}]}, {"text": "It is, however, very encouraging that an acousticsbased approach can perform to within atypical WER range within non-broadcast-news domains, although those domains can likewise be more challenging for the acoustics-based approach.", "labels": [], "entities": []}, {"text": "It is also of scientific interest to be able to quantify this WER as an acoustics-only baseline for further research on ASR-based spoken document summarizers.", "labels": [], "entities": [{"text": "ASR-based spoken document summarizers", "start_pos": 120, "end_pos": 157, "type": "TASK", "confidence": 0.9200566411018372}]}], "tableCaptions": [{"text": " Table 1: Utterances correctly selected by both  the ASR-based models and acoustics-based ap- proach, or by either of them, under different  WERs (12.6%, 20.9%, 29.2%, and 35.5%) and  summary lengths (10%, 20%, and 30% utterances  of the original documents)", "labels": [], "entities": [{"text": "Utterances", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.973511815071106}, {"text": "WERs", "start_pos": 141, "end_pos": 145, "type": "METRIC", "confidence": 0.7937724590301514}]}]}