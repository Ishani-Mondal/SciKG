{"title": [{"text": "Discovering the Discriminative Views: Measuring Term Weights for Sentiment Analysis", "labels": [], "entities": [{"text": "Sentiment Analysis", "start_pos": 65, "end_pos": 83, "type": "TASK", "confidence": 0.9824250936508179}]}], "abstractContent": [{"text": "This paper describes an approach to utilizing term weights for sentiment analysis tasks and shows how various term weight-ing schemes improve the performance of sentiment analysis systems.", "labels": [], "entities": [{"text": "sentiment analysis tasks", "start_pos": 63, "end_pos": 87, "type": "TASK", "confidence": 0.9445524017016093}, {"text": "sentiment analysis", "start_pos": 161, "end_pos": 179, "type": "TASK", "confidence": 0.8854225873947144}]}, {"text": "Previously, sentiment analysis was mostly studied under data-driven and lexicon-based frameworks.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 12, "end_pos": 30, "type": "TASK", "confidence": 0.965704083442688}]}, {"text": "Such work generally exploits tex-tual features for fact-based analysis tasks or lexical indicators from a sentiment lexicon.", "labels": [], "entities": []}, {"text": "We propose to model term weighting into a sentiment analysis system utilizing collection statistics, contextual and topic-related characteristics as well as opinion-related properties.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 42, "end_pos": 60, "type": "TASK", "confidence": 0.9293869733810425}]}, {"text": "Experiments carried out on various datasets show that our approach effectively improves previous methods.", "labels": [], "entities": []}], "introductionContent": [{"text": "With the explosion in the amount of commentaries on current issues and personal views expressed in weblogs on the Internet, the field of studying how to analyze such remarks and sentiments has been increasing as well.", "labels": [], "entities": []}, {"text": "The field of opinion mining and sentiment analysis involves extracting opinionated pieces of text, determining the polarities and strengths, and extracting holders and targets of the opinions.", "labels": [], "entities": [{"text": "opinion mining", "start_pos": 13, "end_pos": 27, "type": "TASK", "confidence": 0.7981875836849213}, {"text": "sentiment analysis", "start_pos": 32, "end_pos": 50, "type": "TASK", "confidence": 0.8290174305438995}]}, {"text": "Much research has focused on creating testbeds for sentiment analysis tasks.", "labels": [], "entities": [{"text": "sentiment analysis tasks", "start_pos": 51, "end_pos": 75, "type": "TASK", "confidence": 0.9599355657895406}]}, {"text": "Most notable and widely used are Multi-Perspective Question Answering (MPQA) and Movie-review datasets.", "labels": [], "entities": [{"text": "Multi-Perspective Question Answering (MPQA)", "start_pos": 33, "end_pos": 76, "type": "TASK", "confidence": 0.7215206126372019}, {"text": "Movie-review datasets", "start_pos": 81, "end_pos": 102, "type": "DATASET", "confidence": 0.9150535762310028}]}, {"text": "MPQA is a collection of newspaper articles annotated with opinions and private states at the subsentence level (.", "labels": [], "entities": [{"text": "MPQA", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9743527173995972}]}, {"text": "Movie-review dataset consists of positive and negative reviews from the Internet Movie Database (IMDb) archive).", "labels": [], "entities": [{"text": "Movie-review dataset", "start_pos": 0, "end_pos": 20, "type": "DATASET", "confidence": 0.9241321086883545}, {"text": "Internet Movie Database (IMDb) archive", "start_pos": 72, "end_pos": 110, "type": "DATASET", "confidence": 0.9272916231836591}]}, {"text": "Evaluation workshops such as TREC and NT-CIR have recently joined in this new trend of research and organized a number of successful meetings.", "labels": [], "entities": [{"text": "TREC", "start_pos": 29, "end_pos": 33, "type": "METRIC", "confidence": 0.5951123237609863}, {"text": "NT-CIR", "start_pos": 38, "end_pos": 44, "type": "DATASET", "confidence": 0.9138345122337341}]}, {"text": "At the TREC Blog Track meetings, researchers have dealt with the problem of retrieving topically-relevant blog posts and identifying documents with opinionated contents.", "labels": [], "entities": [{"text": "TREC Blog Track meetings", "start_pos": 7, "end_pos": 31, "type": "DATASET", "confidence": 0.7508816570043564}]}, {"text": "NTCIR Multilingual Opinion Analysis Task (MOAT) shared a similar mission, where participants are provided with a number of topics and a set of relevant newspaper articles for each topic, and asked to extract opinion-related properties from enclosed sentences (.", "labels": [], "entities": [{"text": "NTCIR Multilingual Opinion Analysis Task (MOAT)", "start_pos": 0, "end_pos": 47, "type": "TASK", "confidence": 0.808220349252224}]}, {"text": "Previous studies for sentiment analysis belong to either the data-driven approach where an annotated corpus is used to train a machine learning (ML) classifier, or to the lexicon-based approach where a pre-compiled list of sentiment terms is utilized to build a sentiment score function.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 21, "end_pos": 39, "type": "TASK", "confidence": 0.9653395414352417}]}, {"text": "This paper introduces an approach to the sentiment analysis tasks with an emphasis on how to represent and evaluate the weights of sentiment terms.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 41, "end_pos": 59, "type": "TASK", "confidence": 0.9577936232089996}]}, {"text": "We propose a number of characteristics of good sentiment terms from the perspectives of informativeness, prominence, topic-relevance, and semantic aspects using collection statistics, contextual information, semantic associations as well as opinion-related properties of terms.", "labels": [], "entities": []}, {"text": "These term weighting features constitute the sentiment analysis model in our opinion retrieval system.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 45, "end_pos": 63, "type": "TASK", "confidence": 0.9404312074184418}, {"text": "opinion retrieval", "start_pos": 77, "end_pos": 94, "type": "TASK", "confidence": 0.7057323604822159}]}, {"text": "We test our opinion retrieval system with TREC and NT-CIR datasets to validate the effectiveness of our term weighting features.", "labels": [], "entities": [{"text": "TREC", "start_pos": 42, "end_pos": 46, "type": "METRIC", "confidence": 0.8259620070457458}, {"text": "NT-CIR datasets", "start_pos": 51, "end_pos": 66, "type": "DATASET", "confidence": 0.9672828614711761}]}, {"text": "We also verify the effectiveness of the statistical features used in datadriven approaches by evaluating an ML classifier with labeled corpora.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our experiments consist of an opinion retrieval task and a sentiment classification task.", "labels": [], "entities": [{"text": "opinion retrieval", "start_pos": 30, "end_pos": 47, "type": "TASK", "confidence": 0.8415756821632385}, {"text": "sentiment classification", "start_pos": 59, "end_pos": 83, "type": "TASK", "confidence": 0.9542535543441772}]}, {"text": "We use MPQA and movie-review corpora in our experiments with an ML classifier.", "labels": [], "entities": [{"text": "ML classifier", "start_pos": 64, "end_pos": 77, "type": "TASK", "confidence": 0.6440327763557434}]}, {"text": "For the opinion retrieval task, we use the two datasets used by TREC blog track and NTCIR MOAT evaluation workshops.", "labels": [], "entities": [{"text": "opinion retrieval", "start_pos": 8, "end_pos": 25, "type": "TASK", "confidence": 0.8636283874511719}, {"text": "TREC blog track", "start_pos": 64, "end_pos": 79, "type": "DATASET", "confidence": 0.8850568731625875}, {"text": "NTCIR MOAT evaluation", "start_pos": 84, "end_pos": 105, "type": "TASK", "confidence": 0.49912895758946735}]}, {"text": "The opinion retrieval task at TREC Blog Track consists of three subtasks: topic retrieval, opinion retrieval, and polarity retrieval.", "labels": [], "entities": [{"text": "opinion retrieval", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.7770205438137054}, {"text": "TREC Blog Track", "start_pos": 30, "end_pos": 45, "type": "DATASET", "confidence": 0.8077169060707092}, {"text": "topic retrieval", "start_pos": 74, "end_pos": 89, "type": "TASK", "confidence": 0.7329857647418976}, {"text": "opinion retrieval", "start_pos": 91, "end_pos": 108, "type": "TASK", "confidence": 0.7742017209529877}]}, {"text": "Opinion and polarity retrieval subtasks use the relevant documents retrieved at the topic retrieval stage.", "labels": [], "entities": []}, {"text": "On the other hand, the NTCIR MOAT task aims to find opinionated sentences given a set of documents that are already hand-assessed to be relevant to the topic.", "labels": [], "entities": [{"text": "NTCIR MOAT task", "start_pos": 23, "end_pos": 38, "type": "TASK", "confidence": 0.8236885666847229}]}, {"text": "To test our SVM classifier, we perform the classification task.", "labels": [], "entities": []}, {"text": "Movie Review polarity dataset 7 was To closely reproduce the experiment with the best performance carried out in () using SVM, we use unigram with the presence feature.", "labels": [], "entities": [{"text": "Movie Review polarity dataset 7", "start_pos": 0, "end_pos": 31, "type": "DATASET", "confidence": 0.9262176871299743}]}, {"text": "We test various combinations of our features applicable to the task.", "labels": [], "entities": []}, {"text": "For evaluation, we use ten-fold cross-validation accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.9306212663650513}]}, {"text": "We present the sentiment classification performances in.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 15, "end_pos": 39, "type": "TASK", "confidence": 0.9299533069133759}]}, {"text": "As observed by, using the raw tf drops the accuracy of the sentiment classification (-13.92%) of movie-review data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.9997281432151794}]}, {"text": "Using the raw idf feature worsens the accuracy even more (-25.42%).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.9996751546859741}]}, {"text": "Normalized tf-variants show improvements over tf but are worse than presence.", "labels": [], "entities": [{"text": "presence", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.9507334232330322}]}, {"text": "Normalized idf features produce slightly better accuracy results than the baseline.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.999055802822113}]}, {"text": "Finally, combining any normalized tf and idf features improved the baseline (high 83% \u223c low 85%).", "labels": [], "entities": [{"text": "baseline", "start_pos": 67, "end_pos": 75, "type": "METRIC", "confidence": 0.9924609065055847}]}, {"text": "The best combination was BM25.TF\u00b7VS.IDF.", "labels": [], "entities": [{"text": "BM25.TF", "start_pos": 25, "end_pos": 32, "type": "DATASET", "confidence": 0.565924882888794}]}, {"text": "MPQA corpus reveals similar but somewhat uncertain tendency.", "labels": [], "entities": [{"text": "MPQA corpus", "start_pos": 0, "end_pos": 11, "type": "DATASET", "confidence": 0.957374781370163}]}], "tableCaptions": [{"text": " Table 1: Performance of opinion retrieval models  using Blog 08 topics. The linear combination pa- rameter \u03bb is optimized on Blog 07 topics.  \u2020 indi- cates statistical significance at the 1% level over  the baseline.  Model  MAP  R-prec P@10  TOPIC REL.  0.4052 0.4366 0.6440  BASELINE  0.4141 0.4534 0.6440  VS  0.4196 0.4542 0.6600  BM25  0.4235 \u2020 0.4579 0.6600  LM  0.4158 0.4520 0.6560  PMI  0.4177 0.4538 0.6620  LSA  0.4155 0.4526 0.6480  WP  0.4165 0.4533 0.6640  BM25\u00b7PMI  0.4238 \u2020 0.4575 0.6600  BM25\u00b7LSA  0.4237 \u2020 0.4578 0.6600  BM25\u00b7WP  0.4237 \u2020 0.4579 0.6600  BM25\u00b7PMI\u00b7WP 0.4242 \u2020 0.4574 0.6620  BM25\u00b7LSA\u00b7WP 0.4238 \u2020 0.4576 0.6580", "labels": [], "entities": [{"text": "opinion retrieval", "start_pos": 25, "end_pos": 42, "type": "TASK", "confidence": 0.7141185700893402}, {"text": "Blog 08 topics", "start_pos": 57, "end_pos": 71, "type": "DATASET", "confidence": 0.927210291226705}, {"text": "TOPIC REL", "start_pos": 244, "end_pos": 253, "type": "METRIC", "confidence": 0.49500980973243713}, {"text": "BASELINE", "start_pos": 278, "end_pos": 286, "type": "METRIC", "confidence": 0.8836226463317871}]}, {"text": " Table 2: Performance of the Sentiment Analy- sis System on NTCIR7 dataset. System parame- ters are optimized for F-measure using NTCIR6  dataset with lenient evaluations.  Opinionated  Model  Precision Recall F-Measure  BASELINE  0.305  0.866  0.451  VS  0.331  0.807  0.470  BM25  0.327  0.795  0.464  LM  0.325  0.794  0.461  LSA  0.315  0.806  0.453  PMI  0.342  0.603  0.436  DTP  0.322  0.778  0.455  VS\u00b7LSA  0.335  0.769  0.466  VS\u00b7PMI  0.311  0.833  0.453  VS\u00b7DTP  0.342  0.745  0.469  VS\u00b7LSA\u00b7DTP  0.349  0.719  0.470  VS\u00b7PMI\u00b7DTP  0.328  0.773  0.461", "labels": [], "entities": [{"text": "NTCIR7 dataset", "start_pos": 60, "end_pos": 74, "type": "DATASET", "confidence": 0.9693218171596527}, {"text": "NTCIR6  dataset", "start_pos": 130, "end_pos": 145, "type": "DATASET", "confidence": 0.9499641954898834}, {"text": "Opinionated  Model  Precision Recall F-Measure", "start_pos": 173, "end_pos": 219, "type": "METRIC", "confidence": 0.6735343754291534}, {"text": "BASELINE", "start_pos": 221, "end_pos": 229, "type": "METRIC", "confidence": 0.5137412548065186}]}, {"text": " Table 3: Average ten-fold cross-validation accura- cies of polarity classification task with SVM.  Accuracy  Features  Movie-review MPQA  PRESENCE  82.6  76.8  TF  71.1  76.5  VS.TF  81.3  76.7  BM25.TF  81.4  77.9  IDF  61.6  61.8  VS.IDF  83.6  77.9  BM25.IDF  83.6  77.8  VS.TF\u00b7VS.IDF  83.8  77.9  BM25.TF\u00b7BM25.IDF  84.1  77.7  BM25.TF\u00b7VS.IDF  85.1  77.7", "labels": [], "entities": [{"text": "polarity classification", "start_pos": 60, "end_pos": 83, "type": "TASK", "confidence": 0.6881135404109955}, {"text": "Accuracy", "start_pos": 100, "end_pos": 108, "type": "METRIC", "confidence": 0.9974874258041382}, {"text": "Movie-review MPQA  PRESENCE  82.6  76.8  TF  71.1  76.5  VS.TF  81.3  76.7  BM25.TF  81.4  77.9  IDF  61.6  61.8  VS.IDF  83.6  77.9  BM25.IDF  83.6  77.8  VS.TF\u00b7VS.IDF  83.8  77.9  BM25.TF\u00b7BM25.IDF  84.1  77.7  BM25.TF\u00b7VS.IDF  85.1  77.7", "start_pos": 120, "end_pos": 358, "type": "DATASET", "confidence": 0.8774996360665873}]}]}