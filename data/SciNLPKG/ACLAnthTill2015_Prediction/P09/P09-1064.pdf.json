{"title": [{"text": "Fast Consensus Decoding over Translation Forests", "labels": [], "entities": []}], "abstractContent": [{"text": "The minimum Bayes risk (MBR) decoding objective improves BLEU scores for machine translation output relative to the standard Viterbi objective of maximizing model score.", "labels": [], "entities": [{"text": "Bayes risk (MBR)", "start_pos": 12, "end_pos": 28, "type": "METRIC", "confidence": 0.9283141851425171}, {"text": "BLEU", "start_pos": 57, "end_pos": 61, "type": "METRIC", "confidence": 0.9991298317909241}, {"text": "machine translation", "start_pos": 73, "end_pos": 92, "type": "TASK", "confidence": 0.7903541326522827}]}, {"text": "However, MBR targeting BLEU is prohibitively slow to optimize over k-best lists for large k.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 23, "end_pos": 27, "type": "METRIC", "confidence": 0.9773962497711182}]}, {"text": "In this paper , we introduce and analyze an alternative to MBR that is equally effective at improving performance , yet is asymptotically faster-running 80 times faster than MBR in experiments with 1000-best lists.", "labels": [], "entities": [{"text": "MBR", "start_pos": 59, "end_pos": 62, "type": "DATASET", "confidence": 0.6375628113746643}, {"text": "MBR", "start_pos": 174, "end_pos": 177, "type": "DATASET", "confidence": 0.7584813237190247}]}, {"text": "Furthermore, our fast decoding procedure can select output sentences based on distributions over entire forests of translations, in addition to k-best lists.", "labels": [], "entities": []}, {"text": "We evaluate our procedure on translation forests from two large-scale, state-of-the-art hierarchical machine translation systems.", "labels": [], "entities": [{"text": "translation", "start_pos": 29, "end_pos": 40, "type": "TASK", "confidence": 0.9850949048995972}]}, {"text": "Our forest-based decoding objective consistently outperforms k-best list MBR, giving improvements of up to 1.0 BLEU.", "labels": [], "entities": [{"text": "MBR", "start_pos": 73, "end_pos": 76, "type": "METRIC", "confidence": 0.5277249813079834}, {"text": "BLEU", "start_pos": 111, "end_pos": 115, "type": "METRIC", "confidence": 0.9991243481636047}]}], "introductionContent": [{"text": "In statistical machine translation, output translations are evaluated by their similarity to human reference translations, where similarity is most often measured by BLEU (.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 3, "end_pos": 34, "type": "TASK", "confidence": 0.6404895683129629}, {"text": "BLEU", "start_pos": 166, "end_pos": 170, "type": "METRIC", "confidence": 0.998829185962677}]}, {"text": "A decoding objective specifies how to derive final translations from a system's underlying statistical model.", "labels": [], "entities": []}, {"text": "The Bayes optimal decoding objective is to minimize risk based on the similarity measure used for evaluation.", "labels": [], "entities": []}, {"text": "The corresponding minimum Bayes risk (MBR) procedure maximizes the expected similarity score of a system's translations relative to the model's distribution over possible translations ().", "labels": [], "entities": [{"text": "minimum Bayes risk (MBR)", "start_pos": 18, "end_pos": 42, "type": "METRIC", "confidence": 0.8040801038344702}, {"text": "similarity score", "start_pos": 76, "end_pos": 92, "type": "METRIC", "confidence": 0.898198813199997}]}, {"text": "Unfortunately, with a non-linear similarity measure like BLEU, we must resort to approximating the expected loss using a k-best list, which accounts for only a tiny fraction of a model's full posterior distribution.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 57, "end_pos": 61, "type": "METRIC", "confidence": 0.997002899646759}]}, {"text": "In this paper, we introduce a variant of the MBR decoding procedure that applies efficiently to translation forests.", "labels": [], "entities": []}, {"text": "Instead of maximizing expected similarity, we express similarity in terms of features of sentences, and choose translations that are similar to expected feature values.", "labels": [], "entities": []}, {"text": "Our exposition begins with algorithms over kbest lists.", "labels": [], "entities": []}, {"text": "A na\u00a8\u0131vena\u00a8\u0131ve algorithm for finding MBR translations computes the similarity between every pair of k sentences, entailing O(k 2 ) comparisons.", "labels": [], "entities": [{"text": "MBR translations", "start_pos": 37, "end_pos": 53, "type": "TASK", "confidence": 0.6672025471925735}, {"text": "similarity", "start_pos": 67, "end_pos": 77, "type": "METRIC", "confidence": 0.9642482995986938}]}, {"text": "We show that if the similarity measure is linear in features of a sentence, then computing expected similarity for all k sentences requires only k similarity evaluations.", "labels": [], "entities": []}, {"text": "Specific instances of this general algorithm have recently been proposed for two linear similarity measures (.", "labels": [], "entities": []}, {"text": "However, the sentence similarity measures we want to optimize in MT are not linear functions, and so this fast algorithm for MBR does not apply.", "labels": [], "entities": [{"text": "MT", "start_pos": 65, "end_pos": 67, "type": "TASK", "confidence": 0.89427250623703}, {"text": "MBR", "start_pos": 125, "end_pos": 128, "type": "TASK", "confidence": 0.7419131398200989}]}, {"text": "For this reason, we propose anew objective that retains the benefits of MBR, but can be optimized efficiently, even for non-linear similarity measures.", "labels": [], "entities": [{"text": "MBR", "start_pos": 72, "end_pos": 75, "type": "METRIC", "confidence": 0.48695674538612366}]}, {"text": "In experiments using BLEU over 1000-best lists, we found that our objective provided benefits very similar to MBR, only much faster.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 21, "end_pos": 25, "type": "METRIC", "confidence": 0.9963351488113403}, {"text": "MBR", "start_pos": 110, "end_pos": 113, "type": "METRIC", "confidence": 0.6074084043502808}]}, {"text": "This same decoding objective can also be computed efficiently from forest-based expectations.", "labels": [], "entities": []}, {"text": "Translation forests compactly encode distributions overmuch larger sets of derivations and arise naturally in chart-based decoding fora wide variety of hierarchical translation systems).", "labels": [], "entities": []}, {"text": "The resulting forest-based decoding procedure compares favorably in both complexity and performance to the recently proposed latticebased MBR (.", "labels": [], "entities": []}, {"text": "The contributions of this paper include a lineartime algorithm for MBR using linear similarities, a linear-time alternative to MBR using non-linear similarity measures, and a forest-based extension to this procedure for similarities based on n-gram counts.", "labels": [], "entities": [{"text": "MBR", "start_pos": 67, "end_pos": 70, "type": "TASK", "confidence": 0.8652186393737793}]}, {"text": "In experiments, we show that our fast procedure is on average 80 times faster than MBR using 1000-best lists.", "labels": [], "entities": [{"text": "MBR", "start_pos": 83, "end_pos": 86, "type": "METRIC", "confidence": 0.48781776428222656}]}, {"text": "We also show that using forests outperforms using k-best lists consistently across language pairs.", "labels": [], "entities": []}, {"text": "Finally, in the first published multi-system experiments on consensus de-coding for translation, we demonstrate that benefits can differ substantially across systems.", "labels": [], "entities": []}, {"text": "In all, we show improvements of up to 1.0 BLEU from consensus approaches for state-of-the-art largescale hierarchical translation systems.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 42, "end_pos": 46, "type": "METRIC", "confidence": 0.997154951095581}]}], "datasetContent": [{"text": "We evaluate these consensus decoding techniques on two different full-scale state-of-the-art hierarchical machine translation systems.", "labels": [], "entities": []}, {"text": "Both systems were trained for 2008 GALE evaluations, in which they outperformed a phrase-based system trained on identical data.", "labels": [], "entities": [{"text": "GALE evaluations", "start_pos": 35, "end_pos": 51, "type": "TASK", "confidence": 0.6174569427967072}]}], "tableCaptions": [{"text": " Table 1: Fast consensus decoding is orders of magnitude  faster than MBR when using BLEU as a similarity measure.  Times only include reranking, not k-best list extraction.", "labels": [], "entities": [{"text": "MBR", "start_pos": 70, "end_pos": 73, "type": "METRIC", "confidence": 0.4683195948600769}, {"text": "BLEU", "start_pos": 85, "end_pos": 89, "type": "METRIC", "confidence": 0.9959207773208618}, {"text": "list extraction", "start_pos": 157, "end_pos": 172, "type": "TASK", "confidence": 0.7407840490341187}]}]}