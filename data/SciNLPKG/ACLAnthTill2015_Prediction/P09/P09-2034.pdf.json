{"title": [{"text": "The Back-translation Score: Automatic MT Evaluation at the Sentence Level without Reference Translations", "labels": [], "entities": [{"text": "MT", "start_pos": 38, "end_pos": 40, "type": "TASK", "confidence": 0.9845101237297058}]}], "abstractContent": [{"text": "Automatic tools for machine translation (MT) evaluation such as BLEU are well established, but have the drawbacks that they do not perform well at the sentence level and that they presuppose manually translated reference texts.", "labels": [], "entities": [{"text": "machine translation (MT) evaluation", "start_pos": 20, "end_pos": 55, "type": "TASK", "confidence": 0.8762816488742828}, {"text": "BLEU", "start_pos": 64, "end_pos": 68, "type": "METRIC", "confidence": 0.9947955012321472}]}, {"text": "Assuming that the MT system to be evaluated can deal with both directions of a language pair, in this research we suggest to conduct automatic MT evaluation by determining the orthographic similarity between a back-translation and the original source text.", "labels": [], "entities": [{"text": "MT", "start_pos": 18, "end_pos": 20, "type": "TASK", "confidence": 0.975559413433075}, {"text": "MT evaluation", "start_pos": 143, "end_pos": 156, "type": "TASK", "confidence": 0.973851889371872}]}, {"text": "This way we eliminate the need for human translated reference texts.", "labels": [], "entities": []}, {"text": "By correlating BLEU and back-translation scores with human judgments , it could be shown that the back-translation score gives an improved performance at the sentence level.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 15, "end_pos": 19, "type": "METRIC", "confidence": 0.9984109401702881}]}], "introductionContent": [{"text": "The manual evaluation of the results of machine translation systems requires considerable time and effort.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 40, "end_pos": 59, "type": "TASK", "confidence": 0.7729040086269379}]}, {"text": "For this reason fast and inexpensive automatic methods were developed.", "labels": [], "entities": []}, {"text": "They are based on the comparison of a machine translation with a reference translation produced by humans.", "labels": [], "entities": []}, {"text": "The comparison is done by determining the number of matching word sequences between both translations.", "labels": [], "entities": []}, {"text": "It could be shown that such methods, of which BLEU () is the most common, can deliver evaluation results that show a high agreement with human judgments ().", "labels": [], "entities": [{"text": "BLEU", "start_pos": 46, "end_pos": 50, "type": "METRIC", "confidence": 0.9982659220695496}]}, {"text": "Disadvantages of BLEU and related methods are that a human reference translation is required, and that the results are reliable only at corpus level, i.e. when computed over many sentence pairs (see e.g.).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 17, "end_pos": 21, "type": "METRIC", "confidence": 0.8722733855247498}, {"text": "human reference translation", "start_pos": 53, "end_pos": 80, "type": "TASK", "confidence": 0.7514785726865133}]}, {"text": "However, at the sentence level, due to data sparseness the results tend to be unsatisfactory.", "labels": [], "entities": []}, {"text": "describe this as follows: \"BLEU's strength is that it correlates highly with human judgments by averaging out individual sentence judgment errors over a test corpus rather than attempting to divine the exact human judgment for every sentence: quantity leads to quality.\"", "labels": [], "entities": [{"text": "BLEU", "start_pos": 27, "end_pos": 31, "type": "METRIC", "confidence": 0.9967957139015198}]}, {"text": "Although in many scenarios the above mentioned drawbacks may not be a major problem, it is nevertheless desirable to overcome them.", "labels": [], "entities": []}, {"text": "This is what we attempt in this paper by introducing the back-translation score.", "labels": [], "entities": []}, {"text": "It is based on the assumption that the MT system considered can translate a language pair in both directions, which is usually the case.", "labels": [], "entities": [{"text": "MT", "start_pos": 39, "end_pos": 41, "type": "TASK", "confidence": 0.9472663402557373}]}, {"text": "Evaluating the quality of a machine translation now involves translating it back to the source language.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 28, "end_pos": 47, "type": "TASK", "confidence": 0.7183625400066376}]}, {"text": "The score is then computed by comparing the back-translation to the original source text.", "labels": [], "entities": []}, {"text": "Although for this comparison BLEU could be used, our experiments show that a modified version which we call OrthoBLEU is better suited for this purpose as it can deal with compounds and inflexional variants in a more appropriate way.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 29, "end_pos": 33, "type": "METRIC", "confidence": 0.9977506995201111}]}, {"text": "Its operation is based on finding matches of character-rather than word-sequences.", "labels": [], "entities": []}, {"text": "It resembles algorithms used in translation memory search for locating orthographically similar sentences.", "labels": [], "entities": [{"text": "translation memory search", "start_pos": 32, "end_pos": 57, "type": "TASK", "confidence": 0.8926401933034261}]}, {"text": "The results that we obtain in this work refute to some extend the common belief that backtranslation (sometimes also called round-trip translation) is not a suitable means for MT evaluation.", "labels": [], "entities": [{"text": "round-trip translation", "start_pos": 124, "end_pos": 146, "type": "TASK", "confidence": 0.7273780107498169}, {"text": "MT evaluation", "start_pos": 176, "end_pos": 189, "type": "TASK", "confidence": 0.9708108901977539}]}, {"text": "This belief seems to be largely based on the obvious observation that the back-translation score is highest fora trivial translation system that does nothing and simply leaves all source words in place.", "labels": [], "entities": []}, {"text": "On the other hand, according to \"until now no one as far as we know has published results demonstrating this\" (i.e. that back-translation is not useful for MT evaluation).", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 156, "end_pos": 169, "type": "TASK", "confidence": 0.9526595175266266}]}, {"text": "We would like to add that so far the inappropriateness of back-translation has only been shown by comparisons with other automatic metrics, which are also flawed.", "labels": [], "entities": []}, {"text": "therefore states: \"To be really sure of our results, we should like to replicate the experiments evaluating the translations using a more old-fashioned method involving human ratings of intelligibility.\"", "labels": [], "entities": []}, {"text": "That is, apparently nobody has ever seriously compared backtranslation scores to human judgments, so the belief about their inutility seems not sufficiently backed by facts.", "labels": [], "entities": []}, {"text": "This is a serious deficit which we try to overcome in this work.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Average BLEU, NIST and OrthoBLEU scores for the 100 test sentences.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 18, "end_pos": 22, "type": "METRIC", "confidence": 0.9621151685714722}, {"text": "NIST", "start_pos": 24, "end_pos": 28, "type": "DATASET", "confidence": 0.7842940092086792}, {"text": "OrthoBLEU", "start_pos": 33, "end_pos": 42, "type": "METRIC", "confidence": 0.9743315577507019}]}]}