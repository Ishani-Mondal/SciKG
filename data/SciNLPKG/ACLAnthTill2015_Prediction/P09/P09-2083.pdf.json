{"title": [{"text": "Do Automatic Annotation Techniques Have Any Impact on Supervised Complex Question Answering?", "labels": [], "entities": [{"text": "Supervised Complex Question Answering", "start_pos": 54, "end_pos": 91, "type": "TASK", "confidence": 0.5963175594806671}]}], "abstractContent": [{"text": "In this paper, we analyze the impact of different automatic annotation methods on the performance of supervised approaches to the complex question answering problem (defined in the DUC-2007 main task).", "labels": [], "entities": [{"text": "question answering problem", "start_pos": 138, "end_pos": 164, "type": "TASK", "confidence": 0.8308074673016866}, {"text": "DUC-2007 main task", "start_pos": 181, "end_pos": 199, "type": "DATASET", "confidence": 0.8371531367301941}]}, {"text": "Huge amount of annotated or labeled data is a prerequisite for supervised training.", "labels": [], "entities": []}, {"text": "The task of labeling can be accomplished either by humans or by computer programs.", "labels": [], "entities": [{"text": "labeling", "start_pos": 12, "end_pos": 20, "type": "TASK", "confidence": 0.962753176689148}]}, {"text": "When humans are employed , the whole process becomes time consuming and expensive.", "labels": [], "entities": []}, {"text": "So, in order to produce a large set of labeled data we prefer the automatic annotation strategy.", "labels": [], "entities": []}, {"text": "We apply five different automatic annotation techniques to produce labeled data using ROUGE similarity measure, Basic Element (BE) overlap, syntactic similarity measure, semantic similarity measure , and Extended String Subsequence Kernel (ESSK).", "labels": [], "entities": [{"text": "ROUGE similarity measure", "start_pos": 86, "end_pos": 110, "type": "METRIC", "confidence": 0.7805608113606771}, {"text": "Basic Element (BE) overlap", "start_pos": 112, "end_pos": 138, "type": "METRIC", "confidence": 0.6816992461681366}]}, {"text": "The representative supervised methods we use are Support Vector Machines (SVM), Conditional Random Fields (CRF), Hidden Markov Models (HMM), and Maximum Entropy (Max-Ent).", "labels": [], "entities": []}, {"text": "Evaluation results are presented to show the impact.", "labels": [], "entities": []}], "introductionContent": [{"text": "In this paper, we consider the complex question answering problem defined in the DUC-2007 main task . We focus on an extractive approach of summarization to answer complex questions where a subset of the sentences in the original documents are chosen.", "labels": [], "entities": [{"text": "question answering", "start_pos": 39, "end_pos": 57, "type": "TASK", "confidence": 0.6908375173807144}, {"text": "DUC-2007 main task", "start_pos": 81, "end_pos": 99, "type": "DATASET", "confidence": 0.9119515816370646}, {"text": "summarization", "start_pos": 140, "end_pos": 153, "type": "TASK", "confidence": 0.9765146970748901}]}, {"text": "For supervised learning methods, huge amount of annotated or labeled data sets are obviously required as a precondition.", "labels": [], "entities": []}, {"text": "The decision as to whether a sentence is important enough http://www-nlpir.nist.gov/projects/duc/duc2007/ to be annotated can betaken either by humans or by computer programs.", "labels": [], "entities": []}, {"text": "When humans are employed in the process, producing such a large labeled corpora becomes time consuming and expensive.", "labels": [], "entities": []}, {"text": "There comes the necessity of using automatic methods to align sentences with the intention to build extracts from abstracts.", "labels": [], "entities": []}, {"text": "In this paper, we use ROUGE similarity measure, Basic Element (BE) overlap, syntactic similarity measure, semantic similarity measure, and Extended String Subsequence Kernel (ESSK) to automatically label the corpora of sentences (DUC-2006 data) into extract summary or non-summary categories in correspondence with the document abstracts.", "labels": [], "entities": [{"text": "ROUGE similarity measure", "start_pos": 22, "end_pos": 46, "type": "METRIC", "confidence": 0.9198314150174459}, {"text": "Basic Element (BE) overlap", "start_pos": 48, "end_pos": 74, "type": "METRIC", "confidence": 0.6895507524410883}, {"text": "semantic similarity measure", "start_pos": 106, "end_pos": 133, "type": "METRIC", "confidence": 0.5931502282619476}, {"text": "DUC-2006 data)", "start_pos": 230, "end_pos": 244, "type": "DATASET", "confidence": 0.9093355735143026}]}, {"text": "We feed these 5 types of labeled data into the learners of each of the supervised approaches: SVM, CRF, HMM, and MaxEnt.", "labels": [], "entities": []}, {"text": "Then we extensively investigate the performance of the classifiers to label unseen sentences (from 25 topics of DUC-2007 data set) as summary or non-summary sentence.", "labels": [], "entities": [{"text": "DUC-2007 data set", "start_pos": 112, "end_pos": 129, "type": "DATASET", "confidence": 0.9774652123451233}]}, {"text": "The experimental results clearly show the impact of different automatic annotation methods on the performance of the candidate supervised techniques.", "labels": [], "entities": []}], "datasetContent": [{"text": "Task Description The problem definition at DUC-2007 was: \"Given a complex question (topic description) and a collection of relevant documents, the task is to synthesize a fluent, wellorganized 250-word summary of the documents that answers the question(s) in the topic\".", "labels": [], "entities": [{"text": "DUC-2007", "start_pos": 43, "end_pos": 51, "type": "DATASET", "confidence": 0.8953266143798828}]}, {"text": "We consider this task and use the five automatic annotation methods to label each sentence of the 50 document sets of DUC-2006 to produce five different versions of training data for feeding the SVM, HMM, CRF and MaxEnt learners.", "labels": [], "entities": [{"text": "DUC-2006", "start_pos": 118, "end_pos": 126, "type": "DATASET", "confidence": 0.70623379945755}]}, {"text": "We choose the top 30% sentences (based on the scores assigned by an annotation scheme) of a document set to have the label +1 and the rest to have \u22121.", "labels": [], "entities": []}, {"text": "Unlabeled sentences of 25 document sets of DUC-2007 data are used for the testing purpose.", "labels": [], "entities": [{"text": "DUC-2007 data", "start_pos": 43, "end_pos": 56, "type": "DATASET", "confidence": 0.9711833000183105}]}, {"text": "Feature Space We represent each of the document-sentences as a vector of feature-values.", "labels": [], "entities": []}, {"text": "We extract several query-related features and some other important features from each sentence.", "labels": [], "entities": []}, {"text": "We use the features: n-gram overlap, Longest Common Subsequence (LCS), Weighted LCS (WLCS), skip-bigram, exact word overlap, synonym overlap, hypernym/hyponym overlap, gloss overlap, Basic Element (BE) overlap, syntactic tree similarity measure, position of sentences, length of sentences, Named Entity (NE), cue word match, and title match.", "labels": [], "entities": [{"text": "Weighted LCS (WLCS)", "start_pos": 71, "end_pos": 90, "type": "METRIC", "confidence": 0.7617817997932435}]}, {"text": "Supervised Systems For SVM we use second order polynomial kernel for the ROUGE and ESSK labeled training.", "labels": [], "entities": [{"text": "SVM", "start_pos": 23, "end_pos": 26, "type": "TASK", "confidence": 0.9461511373519897}, {"text": "ROUGE", "start_pos": 73, "end_pos": 78, "type": "METRIC", "confidence": 0.9753251671791077}, {"text": "ESSK", "start_pos": 83, "end_pos": 87, "type": "METRIC", "confidence": 0.7534763216972351}]}, {"text": "For the BE, syntactic, and semantic labeled training third order polynomial kernel is used.", "labels": [], "entities": [{"text": "BE", "start_pos": 8, "end_pos": 10, "type": "METRIC", "confidence": 0.8113941550254822}]}, {"text": "The use of kernel is based on the accuracy we achieved during training.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9991002082824707}]}, {"text": "We apply 3-fold cross validation with randomized local-grid search for estimating the value of the trade-off parameter C.", "labels": [], "entities": []}, {"text": "We try the value of C in 2 i following heuristics, where i \u2208 {\u22125, \u22124, \u00b7 \u00b7 \u00b7 , 4, 5} and set C as the best performed value 0.125 for second order polynomial kernel and default value is used for third order kernel.", "labels": [], "entities": []}, {"text": "We use SV M light 5 package for training and testing in this research.", "labels": [], "entities": []}, {"text": "In case of HMM, we apply the Maximum Likelihood Estimation (MLE) technique by frequency counts with add-one smoothing to estimate the three HMM parameters: initial state probabilities, transition probabilities and emission probabilities.", "labels": [], "entities": []}, {"text": "We use Dr. Dekang Lin's HMM package to generate the most probable label sequence given the model parameters and the observation sequence (unlabeled DUC-2007 test data).", "labels": [], "entities": [{"text": "DUC-2007 test data", "start_pos": 148, "end_pos": 166, "type": "DATASET", "confidence": 0.9458510279655457}]}, {"text": "We use MALLET-0.4 NLP toolkit to implement the CRF.", "labels": [], "entities": [{"text": "MALLET-0.4 NLP toolkit", "start_pos": 7, "end_pos": 29, "type": "DATASET", "confidence": 0.7299759984016418}]}, {"text": "We formu-late our problem in terms of MALLET's SimpleTagger class which is a command line interface to the MALLET CRF class.", "labels": [], "entities": []}, {"text": "We modify the SimpleTagger class in order to include the provision for producing corresponding posterior probabilities of the predicted labels which are used later for ranking sentences.", "labels": [], "entities": []}, {"text": "We build the MaxEnt system using Dr. Dekang Lin's MaxEnt package 8 . To define the exponential prior of the \u03bb values in MaxEnt models, an extra parameter \u03b1 is used in the package during training.", "labels": [], "entities": []}, {"text": "We keep the value of \u03b1 as default.", "labels": [], "entities": []}, {"text": "Sentence Selection The proportion of important sentences in the training data will differ from the one in the test data.", "labels": [], "entities": [{"text": "Sentence Selection", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.893941193819046}]}, {"text": "A simple strategy is to rank the sentences in a document, then select the top N sentences.", "labels": [], "entities": []}, {"text": "In SVM systems, we use the normalized distance from the hyperplane to each sample to rank the sentences.", "labels": [], "entities": []}, {"text": "Then, we choose N sentences until the summary length (250 words for DUC-2007) is reached.", "labels": [], "entities": [{"text": "summary length", "start_pos": 38, "end_pos": 52, "type": "METRIC", "confidence": 0.9037484228610992}, {"text": "DUC-2007", "start_pos": 68, "end_pos": 76, "type": "DATASET", "confidence": 0.9016032814979553}]}, {"text": "For HMM systems, we use Maximal Marginal Relevance (MMR) based method to rank the sentences (.", "labels": [], "entities": [{"text": "Maximal Marginal Relevance (MMR)", "start_pos": 24, "end_pos": 56, "type": "METRIC", "confidence": 0.7347792685031891}]}, {"text": "In CRF systems, we generate posterior probabilities corresponding to each predicted label in the label sequence to measure the confidence of each sentence for summary inclusion.", "labels": [], "entities": [{"text": "summary inclusion", "start_pos": 159, "end_pos": 176, "type": "TASK", "confidence": 0.727875292301178}]}, {"text": "Similarly for MaxEnt, the corresponding probability values of the predicted labels are used to rank the sentences.", "labels": [], "entities": [{"text": "MaxEnt", "start_pos": 14, "end_pos": 20, "type": "DATASET", "confidence": 0.9027876257896423}]}, {"text": "Evaluation Results The multiple \"reference summaries\" given by DUC-2007 are used in the evaluation of our summary content.", "labels": [], "entities": [{"text": "DUC-2007", "start_pos": 63, "end_pos": 71, "type": "DATASET", "confidence": 0.9641096591949463}]}, {"text": "We evaluate the system generated summaries using the automatic evaluation toolkit ROUGE).", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 82, "end_pos": 87, "type": "METRIC", "confidence": 0.9289656281471252}]}, {"text": "We report the three widely adopted important ROUGE metrics in the results: ROUGE-1 (unigram), ROUGE-2 (bigram) and ROUGE-SU (skip bi-gram).", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 75, "end_pos": 82, "type": "METRIC", "confidence": 0.9209476113319397}, {"text": "ROUGE-2", "start_pos": 94, "end_pos": 101, "type": "METRIC", "confidence": 0.7941412925720215}, {"text": "ROUGE-SU", "start_pos": 115, "end_pos": 123, "type": "METRIC", "confidence": 0.8588866591453552}]}, {"text": "shows the ROUGE F-measures for SVM, HMM, CRF and MaxEnt systems.", "labels": [], "entities": [{"text": "ROUGE F-measures", "start_pos": 10, "end_pos": 26, "type": "METRIC", "confidence": 0.789696455001831}]}, {"text": "The X-axis containing ROUGE, BE, Synt (Syntactic), Sem (Semantic), and ESSK stands for the annotation scheme used.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 22, "end_pos": 27, "type": "METRIC", "confidence": 0.9862983822822571}, {"text": "BE", "start_pos": 29, "end_pos": 31, "type": "METRIC", "confidence": 0.995945155620575}, {"text": "ESSK", "start_pos": 71, "end_pos": 75, "type": "METRIC", "confidence": 0.9790275692939758}]}, {"text": "The Y-axis shows the ROUGE-1 scores at the top, ROUGE-2 scores at the bottom and ROUGE-SU scores in the middle.", "labels": [], "entities": [{"text": "Y-axis", "start_pos": 4, "end_pos": 10, "type": "METRIC", "confidence": 0.929532527923584}, {"text": "ROUGE-1", "start_pos": 21, "end_pos": 28, "type": "METRIC", "confidence": 0.966315746307373}, {"text": "ROUGE-2", "start_pos": 48, "end_pos": 55, "type": "METRIC", "confidence": 0.9151928424835205}, {"text": "ROUGE-SU", "start_pos": 81, "end_pos": 89, "type": "METRIC", "confidence": 0.9535209536552429}]}, {"text": "The supervised systems are distinguished by the line style used in the From the figure, we can see that the ESSK labeled SVM system is having the poorest ROUGE -1 score whereas the Sem labeled system performs: ROUGE F-scores for different supervised systems best.", "labels": [], "entities": [{"text": "ROUGE -1 score", "start_pos": 154, "end_pos": 168, "type": "METRIC", "confidence": 0.9754982590675354}, {"text": "ROUGE F-scores", "start_pos": 210, "end_pos": 224, "type": "METRIC", "confidence": 0.9377805590629578}]}, {"text": "The other annotation methods' impact is almost similar herein terms of ROUGE-1.", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 71, "end_pos": 78, "type": "METRIC", "confidence": 0.8398465514183044}]}, {"text": "Analyzing ROUGE-2 scores, we find that the BE performs the best for SVM, on the other hand, Sem achieves top ROUGE-SU score.", "labels": [], "entities": [{"text": "ROUGE-2", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9221883416175842}, {"text": "BE", "start_pos": 43, "end_pos": 45, "type": "METRIC", "confidence": 0.9989475607872009}, {"text": "SVM", "start_pos": 68, "end_pos": 71, "type": "DATASET", "confidence": 0.6120076775550842}, {"text": "ROUGE-SU", "start_pos": 109, "end_pos": 117, "type": "METRIC", "confidence": 0.9670731425285339}]}, {"text": "As for the two measures Sem annotation is performing the best, we can typically conclude that Sem annotation is the most suitable method for the SVM system.", "labels": [], "entities": []}, {"text": "ESSK works as the best for HMM and Sem labeling performs the worst for all ROUGE scores.", "labels": [], "entities": [{"text": "ESSK", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.7963956594467163}, {"text": "HMM", "start_pos": 27, "end_pos": 30, "type": "DATASET", "confidence": 0.6951541900634766}, {"text": "Sem labeling", "start_pos": 35, "end_pos": 47, "type": "TASK", "confidence": 0.7795588374137878}, {"text": "ROUGE", "start_pos": 75, "end_pos": 80, "type": "METRIC", "confidence": 0.9825166463851929}]}, {"text": "Synt and BE labeled HMMs perform almost similar whereas ROUGE labeled system is pretty close to that of ESSK.", "labels": [], "entities": [{"text": "BE", "start_pos": 9, "end_pos": 11, "type": "METRIC", "confidence": 0.9846864938735962}, {"text": "ROUGE", "start_pos": 56, "end_pos": 61, "type": "METRIC", "confidence": 0.9820868968963623}, {"text": "ESSK", "start_pos": 104, "end_pos": 108, "type": "DATASET", "confidence": 0.8566157221794128}]}, {"text": "Again, we see that the CRF performs best with the ESSK annotated data in terms of ROUGE -1 and ROUGE-SU scores and Sem has the highest ROUGE-2 score.", "labels": [], "entities": [{"text": "ESSK annotated data", "start_pos": 50, "end_pos": 69, "type": "DATASET", "confidence": 0.843900203704834}, {"text": "ROUGE -1", "start_pos": 82, "end_pos": 90, "type": "METRIC", "confidence": 0.9806924859682719}, {"text": "ROUGE-SU", "start_pos": 95, "end_pos": 103, "type": "METRIC", "confidence": 0.9003984332084656}]}, {"text": "But BE and Synt labeling work bad for CRF whereas the ROUGE labeling performs decently.", "labels": [], "entities": [{"text": "BE", "start_pos": 4, "end_pos": 6, "type": "METRIC", "confidence": 0.9931355714797974}, {"text": "CRF", "start_pos": 38, "end_pos": 41, "type": "TASK", "confidence": 0.4865863621234894}, {"text": "ROUGE", "start_pos": 54, "end_pos": 59, "type": "METRIC", "confidence": 0.942135751247406}]}, {"text": "So, we can typically conclude that ESSK annotation is the best method for the CRF system.", "labels": [], "entities": []}, {"text": "Analyzing further, we find that ESSK works best for MaxEnt and BE labeling is the worst for all ROUGE scores.", "labels": [], "entities": [{"text": "ESSK", "start_pos": 32, "end_pos": 36, "type": "METRIC", "confidence": 0.9675671458244324}, {"text": "MaxEnt", "start_pos": 52, "end_pos": 58, "type": "DATASET", "confidence": 0.9210936427116394}, {"text": "BE", "start_pos": 63, "end_pos": 65, "type": "METRIC", "confidence": 0.995784342288971}, {"text": "ROUGE", "start_pos": 96, "end_pos": 101, "type": "METRIC", "confidence": 0.96272212266922}]}, {"text": "We can also see that ROUGE, Synt and Sem labeled MaxEnt systems perform almost similar.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 21, "end_pos": 26, "type": "METRIC", "confidence": 0.8299463391304016}]}, {"text": "So, from this discussion we can come to a conclusion that SVM system performs best if the training data uses semantic annotation scheme and ESSK works best for HMM, CRF and MaxEnt systems.", "labels": [], "entities": [{"text": "ESSK", "start_pos": 140, "end_pos": 144, "type": "METRIC", "confidence": 0.8809106945991516}]}], "tableCaptions": []}