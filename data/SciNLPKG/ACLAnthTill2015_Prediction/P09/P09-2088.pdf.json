{"title": [{"text": "Improved Smoothing for N-gram Language Models Based on Ordinary Counts", "labels": [], "entities": []}], "abstractContent": [{"text": "Kneser-Ney (1995) smoothing and its variants are generally recognized as having the best perplexity of any known method for estimating N-gram language models.", "labels": [], "entities": []}, {"text": "Kneser-Ney smoothing, however, requires nonstandard N-gram counts for the lower-order models used to smooth the highest-order model.", "labels": [], "entities": [{"text": "Kneser-Ney smoothing", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.5431454181671143}]}, {"text": "For some applications, this makes Kneser-Ney smoothing inappropriate or inconvenient.", "labels": [], "entities": [{"text": "Kneser-Ney smoothing", "start_pos": 34, "end_pos": 54, "type": "TASK", "confidence": 0.5485714823007584}]}, {"text": "In this paper, we introduce anew smoothing method based on ordinary counts that outperforms all of the previous ordinary-count methods we have tested, with the new method eliminating most of the gap between Kneser-Ney and those methods.", "labels": [], "entities": []}], "introductionContent": [{"text": "Statistical language models are potentially useful for any language technology task that produces natural-language text as a final (or intermediate) output.", "labels": [], "entities": []}, {"text": "In particular, they are extensively used in speech recognition and machine translation.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 44, "end_pos": 62, "type": "TASK", "confidence": 0.8418120443820953}, {"text": "machine translation", "start_pos": 67, "end_pos": 86, "type": "TASK", "confidence": 0.7955729067325592}]}, {"text": "Despite the criticism that they ignore the structure of natural language, simple N-gram models, which estimate the probability of each word in a text string based on the N \u22121 preceding words, remain the most widely used type of model.", "labels": [], "entities": []}, {"text": "The simplest possible N-gram model is the maximum likelihood estimate (MLE), which takes the probability of a word w n , given the preceding context w 1 . .", "labels": [], "entities": [{"text": "maximum likelihood estimate (MLE)", "start_pos": 42, "end_pos": 75, "type": "METRIC", "confidence": 0.8311329980691274}]}, {"text": "w n\u22121 , to be the ratio of the number of occurrences in a training corpus of the Ngram w 1 . .", "labels": [], "entities": []}, {"text": "w n to the total number of occurrences of any word in the same context: p(w n |w 1 . .", "labels": [], "entities": []}, {"text": "w n\u22121 ) = C(w 1 . .", "labels": [], "entities": []}, {"text": "w n ) w C(w 1 . .", "labels": [], "entities": []}, {"text": "w n\u22121 w ) One obvious problem with this method is that it assigns a probability of zero to any N-gram that is not observed in the training corpus; hence, numerous smoothing methods have been invented that reduce the probabilities assigned to some or all observed N-grams, to provide a non-zero probability for N-grams not observed in the training corpus.", "labels": [], "entities": []}, {"text": "The best methods for smoothing N-gram language models all use a hierarchy of lower-order models to smooth the highest-order model.", "labels": [], "entities": []}, {"text": "Thus, if w 1 w 2 w 3 w 4 w 5 was not observed in the training corpus, p(w 5 |w 1 w 2 w 3 w 4 ) is estimated based on p(w 5 |w 2 w 3 w 4 ), which is estimated based on p(w 5 |w 3 w 4 ) if w 2 w 3 w 4 w 5 was not observed, etc.", "labels": [], "entities": []}, {"text": "In most smoothing methods, the lower-order models, for all N > 1, are recursively estimated in the same way as the highest-order model.", "labels": [], "entities": []}, {"text": "However, the smoothing method of and its variants are the most effective methods known), and they use a different way of computing N-gram counts for all the lower-order models used for smoothing.", "labels": [], "entities": []}, {"text": "For these lower-order models, the actual corpus counts C(w 1 . .", "labels": [], "entities": []}, {"text": "w n ) are replaced by In other words, the count used fora lower-order N-gram is the number of distinct word types that precede it in the training corpus.", "labels": [], "entities": []}, {"text": "The fact that the lower-order models are estimated differently from the highest-order model makes the use of Kneser-Ney (KN) smoothing awkward in some situations.", "labels": [], "entities": []}, {"text": "For example, coarse-to-fine search using a sequence of lowerorder to higher-order language models has been shown to bean efficient way of constraining highdimensional search spaces for speech recognition) and machine translation ().", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 185, "end_pos": 203, "type": "TASK", "confidence": 0.7372810244560242}, {"text": "machine translation", "start_pos": 209, "end_pos": 228, "type": "TASK", "confidence": 0.8244376480579376}]}, {"text": "The lower-order models used in KN smoothing, however, are very poor estimates of the probabilities for N-grams that have been observed in the training corpus, so they are Figure 1: General language model smoothing schema not suitable for use in coarse-to-fine search.", "labels": [], "entities": [{"text": "KN smoothing", "start_pos": 31, "end_pos": 43, "type": "TASK", "confidence": 0.9521581530570984}]}, {"text": "Thus, two versions of every language model below the highest-order model would be needed to use KN smoothing in this case.", "labels": [], "entities": [{"text": "KN smoothing", "start_pos": 96, "end_pos": 108, "type": "TASK", "confidence": 0.7943832874298096}]}, {"text": "Another casein which use of special KN counts is problematic is the method presented by for building and applying language models trained on very large corpora (up to 40 billion words in their experiments).", "labels": [], "entities": []}, {"text": "The scalability of their approach depends on a \"backsorted trie\", but this data structure does not support efficient computation of the special KN counts.", "labels": [], "entities": []}, {"text": "In this paper, we introduce anew smoothing method for language models based on ordinary counts.", "labels": [], "entities": []}, {"text": "In our experiments, it outperformed all of the previous ordinary-count methods we tested, and it eliminated most of the gap between KN smoothing and the other previous methods.", "labels": [], "entities": [{"text": "KN smoothing", "start_pos": 132, "end_pos": 144, "type": "TASK", "confidence": 0.8122793138027191}]}], "datasetContent": [{"text": "We trained and measured the perplexity of 4-gram language models using English data from the WMT-06 Europarl corpus ().", "labels": [], "entities": [{"text": "WMT-06 Europarl corpus", "start_pos": 93, "end_pos": 115, "type": "DATASET", "confidence": 0.8689225912094116}]}, {"text": "We took 1,003,349 sentences words) for training, and 2000 sentences each for testing and parameter optimization.", "labels": [], "entities": [{"text": "parameter optimization", "start_pos": 89, "end_pos": 111, "type": "TASK", "confidence": 0.729501485824585}]}, {"text": "We built models based on six previous approaches: (1) Katz backoff, (2) interpolated absolute discounting with Ney et al. formula discounts, backoff absolute discounting with (3) Ney et al. formula discounts and with (4) one empirically optimized discount, (5) modified interpolated KN with Chen-Goodman formula discounts, and (6) interpolated KN with one empirically optimized discount.", "labels": [], "entities": []}, {"text": "We built models based on four ways of computing the D parameters of our new model, with a fixed \u03b4 = 0.5: (7) Ney et al. formula discounts, (8) one empirically optimized discount, (9) Chen-Goodman formula discounts, and (10) Good-Turing formula discounts.", "labels": [], "entities": []}, {"text": "We also built a model (11) based on one empirically optimized discount D = 0.55 and an empircially optimized value of \u03b4 = 0.9.", "labels": [], "entities": [{"text": "empirically optimized discount D", "start_pos": 40, "end_pos": 72, "type": "METRIC", "confidence": 0.6632549613714218}]}, {"text": "shows that each of these variants of our method had better perplexity than every previous ordinary-count method tested.", "labels": [], "entities": []}, {"text": "Finally, we performed one more experiment, to see if the best variant of our model  ters of the model with the KN counts.", "labels": [], "entities": []}, {"text": "However, the best variant of our model eliminated 65% of the difference in perplexity between the best previous ordinary-count method tested and the best variant of KN smoothing tested, suggesting that it may currently be the best approach when language models based on ordinary counts are desired.", "labels": [], "entities": [{"text": "KN smoothing", "start_pos": 165, "end_pos": 177, "type": "TASK", "confidence": 0.7143680155277252}]}], "tableCaptions": [{"text": " Table 1: 4-gram perplexity results", "labels": [], "entities": []}]}