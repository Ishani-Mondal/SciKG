{"title": [{"text": "Investigations on Word Senses and Word Usages", "labels": [], "entities": [{"text": "Word Usages", "start_pos": 34, "end_pos": 45, "type": "TASK", "confidence": 0.6492776423692703}]}], "abstractContent": [{"text": "The vast majority of work on word senses has relied on predefined sense inventories and an annotation schema where each word instance is tagged with the best fitting sense.", "labels": [], "entities": []}, {"text": "This paper examines the case fora graded notion of word meaning in two experiments, one which uses WordNet senses in a graded fashion, contrasted with the \"winner takes all\" annotation, and one which asks annotators to judge the similarity of two usages.", "labels": [], "entities": []}, {"text": "We find that the graded responses correlate with annotations from previous datasets, but sense assignments are used in away that weakens the case for clear cut sense boundaries.", "labels": [], "entities": []}, {"text": "The responses from both experiments correlate with the overlap of paraphrases from the English lexical substitution task which bodes well for the use of substitutes as a proxy for word sense.", "labels": [], "entities": [{"text": "English lexical substitution task", "start_pos": 87, "end_pos": 120, "type": "TASK", "confidence": 0.6692778766155243}]}, {"text": "This paper also provides two novel datasets which can be used for evaluating computational systems.", "labels": [], "entities": []}], "introductionContent": [{"text": "The vast majority of work on word sense tagging has assumed that predefined word senses from a dictionary are an adequate proxy for the task, although of course there are issues with this enterprise both in terms of cognitive validity) and adequacy for computational linguistics applications).", "labels": [], "entities": [{"text": "word sense tagging", "start_pos": 29, "end_pos": 47, "type": "TASK", "confidence": 0.7318241397539774}]}, {"text": "Furthermore, given a predefined list of senses, annotation efforts and computational approaches to word sense disambiguation (WSD) have usually assumed that one best fitting sense should be selected for each usage.", "labels": [], "entities": [{"text": "word sense disambiguation (WSD)", "start_pos": 99, "end_pos": 130, "type": "TASK", "confidence": 0.7726608713467916}]}, {"text": "While there is usually some allowance made for multiple senses, this is typically not adopted by annotators or computational systems.", "labels": [], "entities": []}, {"text": "Research on the psychology of concepts shows that categories in the human mind are not simply sets with clearcut boundaries: Some items are perceived as more typical than others, and there are borderline cases on which people disagree more often, and on whose categorization they are more likely to change their minds.", "labels": [], "entities": []}, {"text": "Word meanings are certainly related to mental concepts.", "labels": [], "entities": []}, {"text": "This raises the question of whether there is any such thing as the one appropriate sense fora given occurrence.", "labels": [], "entities": []}, {"text": "In this paper we will explore using graded responses for sense tagging within a novel annotation paradigm.", "labels": [], "entities": [{"text": "sense tagging", "start_pos": 57, "end_pos": 70, "type": "TASK", "confidence": 0.8161675333976746}]}, {"text": "Modeling the annotation framework after psycholinguistic experiments, we do not train annotators to conform to sense distinctions; rather we assess individual differences by asking annotators to produce graded ratings instead of making a binary choice.", "labels": [], "entities": []}, {"text": "We perform two annotation studies.", "labels": [], "entities": []}, {"text": "In the first one, referred to as WSsim (Word Sense Similarity), annotators give graded ratings on the applicability of WordNet senses.", "labels": [], "entities": [{"text": "Word Sense Similarity)", "start_pos": 40, "end_pos": 62, "type": "TASK", "confidence": 0.49932029098272324}]}, {"text": "In the second one, Usim (Usage Similarity), annotators rate the similarity of pairs of occurrences (usages) of a common target word.", "labels": [], "entities": []}, {"text": "Both studies explore whether users make use of a graded scale or persist in making binary decisions even when there is the option fora graded response.", "labels": [], "entities": []}, {"text": "The first study additionally tests to what extent the judgments on WordNet senses fall into clear-cut clusters, while the second study allows us to explore meaning similarity independently of any lexicon resource.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: A sample annotation in the WSsim experiment. The senses are: 1:statement, 2:controversy,  3:debate, 4:literary argument, 5:parameter, 6:variable, 7:line of reasoning", "labels": [], "entities": [{"text": "WSsim experiment", "start_pos": 37, "end_pos": 53, "type": "DATASET", "confidence": 0.8066961169242859}]}, {"text": " Table 2: Percentage of items with multiple senses  assigned. Orig: in the original SemCor/SE-3 data.  WSsim judgment: items with judgments at or  above the specified threshold. The percentages for  WSsim are averaged over the three annotators.", "labels": [], "entities": [{"text": "Orig", "start_pos": 62, "end_pos": 66, "type": "METRIC", "confidence": 0.9888485670089722}, {"text": "SemCor/SE-3 data", "start_pos": 84, "end_pos": 100, "type": "DATASET", "confidence": 0.7089490443468094}]}, {"text": " Table 3: Percentage of sense pairs that were sig- nificantly positively (pos) or negatively (neg) cor- related at p < 0.05 and p < 0.01, shown by anno- tator.", "labels": [], "entities": []}, {"text": " Table 4: Percentage of sentences in which at least  two uncorrelated (p > 0.05) or negatively corre- lated senses have been annotated with judgments  at the specified threshold.", "labels": [], "entities": []}, {"text": " Table 5: Annotator correlation with LEXSUB sub- stitute overlap (inter)", "labels": [], "entities": []}]}