{"title": [{"text": "Modeling Morphologically Rich Languages Using Split Words and Unstructured Dependencies", "labels": [], "entities": [{"text": "Modeling Morphologically Rich Languages", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.8309335559606552}]}], "abstractContent": [{"text": "We experiment with splitting words into their stem and suffix components for mod-eling morphologically rich languages.", "labels": [], "entities": []}, {"text": "We show that using a morphological ana-lyzer and disambiguator results in a significant perplexity reduction in Turkish.", "labels": [], "entities": []}, {"text": "We present flexible n-gram models, Flex-Grams, which assume that the n\u22121 tokens that determine the probability of a given token can be chosen anywhere in the sentence rather than the preceding n \u2212 1 positions.", "labels": [], "entities": [{"text": "Flex-Grams", "start_pos": 35, "end_pos": 45, "type": "METRIC", "confidence": 0.5855180621147156}]}, {"text": "Our final model achieves 27% per-plexity reduction compared to the standard n-gram model.", "labels": [], "entities": []}], "introductionContent": [{"text": "Language models, i.e. models that assign probabilities to sequences of words, have been proven useful in a variety of applications including speech recognition and machine translation ().", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 141, "end_pos": 159, "type": "TASK", "confidence": 0.7873808443546295}, {"text": "machine translation", "start_pos": 164, "end_pos": 183, "type": "TASK", "confidence": 0.8234433531761169}]}, {"text": "More recently, good results on lexical substitution and word sense disambiguation using language models have also been reported.", "labels": [], "entities": [{"text": "lexical substitution", "start_pos": 31, "end_pos": 51, "type": "TASK", "confidence": 0.748634934425354}, {"text": "word sense disambiguation", "start_pos": 56, "end_pos": 81, "type": "TASK", "confidence": 0.724412222703298}]}, {"text": "Morphologically rich languages pose a challenge to standard modeling techniques because of their relatively large out-of-vocabulary rates and the regularities they possess at the sub-word level.", "labels": [], "entities": []}, {"text": "The standard n-gram language model ignores long-distance relationships between words and uses the independence assumption of a Markov chain of order n \u2212 1.", "labels": [], "entities": []}, {"text": "Morphemes play an important role in the syntactic dependency structure in morphologically rich languages.", "labels": [], "entities": []}, {"text": "The dependencies are not only between stems but also between stems and suffixes and if we use complete words as unit tokens, we will not be able to represent these sub-word dependencies.", "labels": [], "entities": []}, {"text": "Our working hypothesis is that the performance of a language model is correlated by how much the probabilistic dependencies mirror the syntactic dependencies.", "labels": [], "entities": []}, {"text": "We present flexible n-grams, FlexGrams, in which each token can be conditioned on tokens anywhere in the sentence, not just the preceding n \u2212 1 tokens.", "labels": [], "entities": [{"text": "FlexGrams", "start_pos": 29, "end_pos": 38, "type": "DATASET", "confidence": 0.7182416319847107}]}, {"text": "We also experiment with words split into their stem and suffix forms, and define stemsuffix FlexGrams where one set of offsets is applied to stems and another to suffixes.", "labels": [], "entities": []}, {"text": "We evaluate the performance of these models on a morphologically rich language, Turkish.", "labels": [], "entities": []}], "datasetContent": [{"text": "We used the Turkish newspaper corpus of Milliyet after removing sentences with 100 or more tokens.", "labels": [], "entities": [{"text": "Turkish newspaper corpus of Milliyet", "start_pos": 12, "end_pos": 48, "type": "DATASET", "confidence": 0.928509795665741}]}, {"text": "The dataset contains about 600 thousand sentences in the training set and 60 thousand sentences in the test set (giving a total of about 10 million words).", "labels": [], "entities": []}, {"text": "The versions of the corpus we use developed by using different word-split strategies along with a sample sentence are explained below: 1.", "labels": [], "entities": []}, {"text": "The unsplit dataset contains the raw corpus: Kasparov b\u00fckemedi\u02dcb\u00fckemedi\u02dcgi el\u00ef opecek (Kasparov is going to kiss the hand he cannot bend) 2.", "labels": [], "entities": []}, {"text": "The morfessor dataset was prepared using the Morfessor () algorithm: Kasparov b\u00fcke +medi\u02dcgi+medi\u02dcgi el\u00ef op +ecek 3.", "labels": [], "entities": [{"text": "morfessor dataset", "start_pos": 4, "end_pos": 21, "type": "DATASET", "confidence": 0.6966763436794281}]}, {"text": "The auto-split dataset is obtained after using our unsupervised morphological splitter: Kaspar +ov b\u00fck +emedi\u02dcgi+emedi\u02dcgi el\u00ef op +ecek 4.", "labels": [], "entities": []}, {"text": "The split dataset contains words that are split into their stem and suffix forms by using a highly accurate supervised morphological analyzer (): Kasparov b\u00fck +yAmA+dHk+sH el +sH\u00f6p+sH\u00f6p +yAcAk 5.", "labels": [], "entities": []}, {"text": "The split+0 version is derived from the split dataset by adding a zero-suffix to any stem that is not followed by a suffix: Kasparov +0 b\u00fck +yAmA+dHk+sH el +sH  In this section we present a number of experiments that demonstrate that when modeling a morphologically rich language like Turkish, (i) splitting words into their stem and suffix forms is beneficial when the split is performed using a morphological analyzer and (ii) allowing the model to choose stem and suffix dependencies separately and flexibly results in a perplexity reduction, however the reduction does not offset the cost of zero suffixes.", "labels": [], "entities": []}, {"text": "We used the SRILM toolkit) to simulate the behavior of FlexGram models by using count files as input.", "labels": [], "entities": [{"text": "SRILM toolkit", "start_pos": 12, "end_pos": 25, "type": "DATASET", "confidence": 0.8144561350345612}]}, {"text": "The interpolated KneserNey smoothing was used in all our experiments.", "labels": [], "entities": [{"text": "KneserNey smoothing", "start_pos": 17, "end_pos": 36, "type": "TASK", "confidence": 0.6295237243175507}]}], "tableCaptions": [{"text": " Table 1: Dataset statistics (K for thousands, M for millions)", "labels": [], "entities": []}, {"text": " Table 2: Total log probability (M for millions of bits).", "labels": [], "entities": [{"text": "Total log probability", "start_pos": 10, "end_pos": 31, "type": "METRIC", "confidence": 0.9418317079544067}, {"text": "M", "start_pos": 33, "end_pos": 34, "type": "METRIC", "confidence": 0.7704825401306152}]}, {"text": " Table 5: Best stem-suffix flexgram model combinations for", "labels": [], "entities": []}, {"text": " Table 6: Perplexity for compared models.", "labels": [], "entities": [{"text": "Perplexity", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9443211555480957}]}]}