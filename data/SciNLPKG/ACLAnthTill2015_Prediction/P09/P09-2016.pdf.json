{"title": [{"text": "A Combination of Active Learning and Semi-supervised Learning Starting with Positive and Unlabeled Examples for Word Sense \ud97b\udf59 \ud97b\udf59 Disambiguation: An Empirical Study on Japanese Web Search Query", "labels": [], "entities": [{"text": "Word Sense \ud97b\udf59 \ud97b\udf59 Disambiguation", "start_pos": 112, "end_pos": 141, "type": "TASK", "confidence": 0.8402365922927857}, {"text": "Japanese Web Search Query", "start_pos": 165, "end_pos": 190, "type": "TASK", "confidence": 0.5882538259029388}]}], "abstractContent": [{"text": "This paper proposes to solve the bottleneck of finding training data for word sense disambiguation (WSD) in the domain of web queries, where a complete set of ambiguous word senses are unknown.", "labels": [], "entities": [{"text": "word sense disambiguation (WSD)", "start_pos": 73, "end_pos": 104, "type": "TASK", "confidence": 0.8088282297054926}]}, {"text": "In this paper, we present a combination of active learning and semi-supervised learning method to treat the case when positive examples, which have an expected word sense in web search result, are only given.", "labels": [], "entities": []}, {"text": "The novelty of our approach is to use \"pseudo negative examples\" with reliable confidence score estimated by a classifier trained with positive and unlabeled examples.", "labels": [], "entities": []}, {"text": "We show experimentally that our proposed method achieves close enough WSD accuracy to the method with the manually prepared negative examples in several Japanese Web search data.", "labels": [], "entities": [{"text": "WSD", "start_pos": 70, "end_pos": 73, "type": "TASK", "confidence": 0.6930695176124573}, {"text": "accuracy", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.8561893105506897}, {"text": "Japanese Web search data", "start_pos": 153, "end_pos": 177, "type": "DATASET", "confidence": 0.633995421230793}]}], "introductionContent": [{"text": "In Web mining for sentiment or reputation analysis, it is important for reliable analysis to extract large amount of texts about certain products, shops, or persons with high accuracy.", "labels": [], "entities": [{"text": "sentiment or reputation analysis", "start_pos": 18, "end_pos": 50, "type": "TASK", "confidence": 0.8409323841333389}, {"text": "accuracy", "start_pos": 175, "end_pos": 183, "type": "METRIC", "confidence": 0.9931508302688599}]}, {"text": "When retrieving texts from Web archive, we often suffer from word sense ambiguity and WSD system is indispensable.", "labels": [], "entities": []}, {"text": "For instance, when we try to analyze reputation of \"Loft\", a name of variety store chain in Japan, we found that simple text search retrieved many unrelated texts which contain \"Loft\" with different senses such as an attic room, an angle of golf club face, a movie title, a name of a club with live music and soon.", "labels": [], "entities": []}, {"text": "The words in Web search queries are often proper nouns.", "labels": [], "entities": []}, {"text": "Then it is not trivial to discriminate these senses especially for the language like Japanese whose proper nouns are not capitalized.", "labels": [], "entities": []}, {"text": "To train WSD systems we need a large amount of positive and negative examples.", "labels": [], "entities": [{"text": "WSD", "start_pos": 9, "end_pos": 12, "type": "TASK", "confidence": 0.9778597354888916}]}, {"text": "In the real Web mining application, how to acquire training data fora various target of analysis has become a major hurdle to use supervised WSD.", "labels": [], "entities": [{"text": "WSD", "start_pos": 141, "end_pos": 144, "type": "TASK", "confidence": 0.8567834496498108}]}, {"text": "Fortunately, it is not so difficult to create positive examples.", "labels": [], "entities": []}, {"text": "We can retrieve positive examples from Web archive with high precision (but low recall) by manually augmenting queries with hypernyms or semantically related words (e.g., \"Loft AND shop\" or \"Loft AND stationary\").", "labels": [], "entities": [{"text": "precision", "start_pos": 61, "end_pos": 70, "type": "METRIC", "confidence": 0.9972301125526428}, {"text": "recall", "start_pos": 80, "end_pos": 86, "type": "METRIC", "confidence": 0.9984622001647949}]}, {"text": "On the other hand, it is often costly to create negative examples.", "labels": [], "entities": []}, {"text": "In principle, we can create negative examples in the same way as we did to create positive ones.", "labels": [], "entities": []}, {"text": "The problem is, however, that we are not sure of most of the senses of a target word.", "labels": [], "entities": []}, {"text": "Because target words are often proper nouns, their word senses are rarely listed in hand-crafted lexicon.", "labels": [], "entities": []}, {"text": "In addition, since the Web is huge and contains heterogeneous domains, we often find a large number of unexpected senses.", "labels": [], "entities": []}, {"text": "For example, all the authors did not know the music club meaning of Loft.", "labels": [], "entities": [{"text": "music club meaning of Loft", "start_pos": 46, "end_pos": 72, "type": "DATASET", "confidence": 0.6668370008468628}]}, {"text": "As the result, we often had to spend much time to find such unexpected meaning of target words.", "labels": [], "entities": []}, {"text": "This situation motivated us to study active learning for WSD starting with only positive examples.", "labels": [], "entities": [{"text": "WSD", "start_pos": 57, "end_pos": 60, "type": "TASK", "confidence": 0.9861866235733032}]}, {"text": "The previous techniques) require balanced positive and negative examples to estimate the score.", "labels": [], "entities": []}, {"text": "In our problem setting, however, we have no negative examples at the initial stage.", "labels": [], "entities": []}, {"text": "To tackle this problem, we propose a method of active learning for WSD with pseudo negative examples, which are selected from unlabeled data by a classifier trained with positive and unlabeled examples.", "labels": [], "entities": [{"text": "WSD", "start_pos": 67, "end_pos": 70, "type": "TASK", "confidence": 0.9167735576629639}]}, {"text": "combined active learning and semi-supervised learning technique by using EM with unlabeled data integrated into active learning, but it did not treat our problem setting where only positive examples are given.", "labels": [], "entities": []}, {"text": "The construction of this paper is as follows; Section 2 describes a proposed learning algorithm.", "labels": [], "entities": []}, {"text": "Section 3 shows the experimental results.", "labels": [], "entities": []}], "datasetContent": [{"text": "We select several example data sets from Japanese blog data crawled from Web.", "labels": [], "entities": [{"text": "Japanese blog data crawled from Web.", "start_pos": 41, "end_pos": 77, "type": "DATASET", "confidence": 0.819205641746521}]}, {"text": "shows the ambiguous words, the number of its senses, the number of its data instances, the number of feature, and the percentage of positive sense instances for each data set.", "labels": [], "entities": []}, {"text": "Assigning the correct labels of data instances is done by one person and 48.5% of all the labels are checked by another person.", "labels": [], "entities": []}, {"text": "The percentage of agreement between 2 persons for the assigned labels is 99.0%.", "labels": [], "entities": []}, {"text": "The average time of assigning labels is 35 minutes per 100 instances.", "labels": [], "entities": []}, {"text": "Selected instances for evaluation are randomly divided 10% test set and 90% training set. is set to empirically optimized value 50.", "labels": [], "entities": []}, {"text": "Dependency on threshold value \ud97b\udf59 will be discussed in 3.3.", "labels": [], "entities": []}, {"text": "shows the average WSD accuracy of the following 6 approaches.", "labels": [], "entities": [{"text": "WSD", "start_pos": 18, "end_pos": 21, "type": "METRIC", "confidence": 0.5349937677383423}, {"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.5216721892356873}]}, {"text": "man labeled negative examples in initial training data is the same as that of positive examples in.", "labels": [], "entities": []}, {"text": "Human labeling is considered to be the upper accuracy in the variants of selecting pseudo negative examples.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.9993413090705872}]}], "tableCaptions": [{"text": " Table 2: Selected examples for evaluation", "labels": [], "entities": []}]}