{"title": [{"text": "Sense-based Interpretation of Logical Metonymy Using a Statistical Method", "labels": [], "entities": [{"text": "Sense-based Interpretation of Logical Metonymy", "start_pos": 0, "end_pos": 46, "type": "TASK", "confidence": 0.6694789111614228}]}], "abstractContent": [{"text": "The use of figurative language is ubiquitous in natural language texts and it is a serious bottleneck in automatic text understanding.", "labels": [], "entities": [{"text": "automatic text understanding", "start_pos": 105, "end_pos": 133, "type": "TASK", "confidence": 0.6202429632345835}]}, {"text": "We address the problem of interpretation of logical metonymy, using a statistical method.", "labels": [], "entities": [{"text": "interpretation of logical metonymy", "start_pos": 26, "end_pos": 60, "type": "TASK", "confidence": 0.8902505189180374}]}, {"text": "Our approach originates from that of Lapata and Lascarides (2003), which generates a list of non-disambiguated interpretations with their likelihood derived from a corpus.", "labels": [], "entities": []}, {"text": "We propose a novel sense-based representation of the interpretation of logical metonymy and a more thorough evaluation method than that of Lapata and Lascarides (2003).", "labels": [], "entities": [{"text": "interpretation of logical metonymy", "start_pos": 53, "end_pos": 87, "type": "TASK", "confidence": 0.8649271726608276}]}, {"text": "By carrying out a human experiment we prove that such a representation is intuitive to human subjects.", "labels": [], "entities": []}, {"text": "We derive a ranking scheme for verb senses using an unan-notated corpus, WordNet sense numbering and glosses.", "labels": [], "entities": [{"text": "WordNet sense numbering", "start_pos": 73, "end_pos": 96, "type": "TASK", "confidence": 0.7181529800097147}]}, {"text": "We also provide an account of the requirements that different aspec-tual verbs impose onto the interpretation of logical metonymy.", "labels": [], "entities": []}, {"text": "We tested our system on verb-object metonymic phrases.", "labels": [], "entities": []}, {"text": "It identifies and ranks metonymic interpretations with the mean average precision of 0.83 as compared to the gold standard.", "labels": [], "entities": [{"text": "mean average precision", "start_pos": 59, "end_pos": 81, "type": "METRIC", "confidence": 0.7696176171302795}]}], "introductionContent": [{"text": "Metonymy is defined as the use of a word or a phrase to stand fora related concept which is not explicitly mentioned.", "labels": [], "entities": []}, {"text": "Here are some examples of metonymic phrases: (1) The penis mightier than the sword.", "labels": [], "entities": []}, {"text": "(5) After three martinis John was feeling well.", "labels": [], "entities": []}, {"text": "( The metonymic adage in (1) is a classical example.", "labels": [], "entities": []}, {"text": "Here the pen stands for the press and the sword for military power.", "labels": [], "entities": [{"text": "pen", "start_pos": 9, "end_pos": 12, "type": "METRIC", "confidence": 0.8436703681945801}]}, {"text": "In the following example Bach is used to refer to the composer's music and in (3) the glass stands for its content, i.e. the actual drink (beverage).", "labels": [], "entities": []}, {"text": "The sentences (4) and (5) represent a variation of this phenomenon called logical metonymy.", "labels": [], "entities": []}, {"text": "Here both the book and three martinis have eventive interpretations, i.e. the noun phrases stand for the events of reading the book and drinking three martinis respectively.", "labels": [], "entities": []}, {"text": "Such behaviour is triggered by the type requirements the verb (or the preposition) places onto its argument.", "labels": [], "entities": []}, {"text": "This is known in linguistics as a phenomenon of type coercion.", "labels": [], "entities": []}, {"text": "Many existing approaches to logical metonymy explain systematic syntactic ambiguity of metonymic verbs (such as enjoy) or prepositions (such as after) by means of type coercion.", "labels": [], "entities": []}, {"text": "Logical metonymy occurs in natural language texts relatively frequently.", "labels": [], "entities": []}, {"text": "Therefore, its automatic interpretation would significantly facilitate the task of many NLP applications that require semantic processing (e.g., machine translation, information extraction, question answering and many others).", "labels": [], "entities": [{"text": "machine translation", "start_pos": 145, "end_pos": 164, "type": "TASK", "confidence": 0.8015280067920685}, {"text": "information extraction", "start_pos": 166, "end_pos": 188, "type": "TASK", "confidence": 0.8055135011672974}, {"text": "question answering", "start_pos": 190, "end_pos": 208, "type": "TASK", "confidence": 0.8727739751338959}]}, {"text": "followed by used text corpora to automatically derive interpretations of metonymic phrases.", "labels": [], "entities": []}, {"text": "1 used a statistical model for the interpretation of general metonymies for Japanese.", "labels": [], "entities": [{"text": "interpretation of general metonymies", "start_pos": 35, "end_pos": 71, "type": "TASK", "confidence": 0.870829701423645}]}, {"text": "Given a verb-object metonymic phrase, such as read Shakespeare, they searched for entities the object could stand for, such as plays of Shakespeare.", "labels": [], "entities": []}, {"text": "They considered all the nouns cooccurring with the object noun and the Japanese equivalent of the preposition of.", "labels": [], "entities": []}, {"text": "Utiyama and his colleagues tested their approach on 75 metonymic phrases taken from the literature and reported a precision of 70.6%, whereby an interpretation was considered correct if it made sense in some imaginary context.", "labels": [], "entities": [{"text": "precision", "start_pos": 114, "end_pos": 123, "type": "METRIC", "confidence": 0.99796462059021}]}, {"text": "extend Utiyama's approach to interpretation of logical metonymies containing aspectual verbs (e.g. begin the book) and polysemous adjectives (e.g. good meal vs. good cook).", "labels": [], "entities": []}, {"text": "Their method generates a list of interpretations with their likelihood derived from a corpus.", "labels": [], "entities": []}, {"text": "Lapata and Lascarides define an interpretation of logical metonymy as a verb string, which is ambiguous with respect to word sense.", "labels": [], "entities": []}, {"text": "Some of these strings indeed correspond to paraphrases that a human would give for the metonymic phrase.", "labels": [], "entities": []}, {"text": "But they are not meaningful as such for automatic processing, since their senses still need to be disambiguated in order to obtain the actual meaning.", "labels": [], "entities": []}, {"text": "For example, compare the grab sense of take vs. its film sense for the metonymic phrase finish video.", "labels": [], "entities": []}, {"text": "It is obvious that only the latter sense is a correct interpretation.", "labels": [], "entities": []}, {"text": "We extend the experiment of Lapata and Lascarides by disambiguating the interpretations with respect to WordNet synsets (for verb-object metonymic phrases).", "labels": [], "entities": []}, {"text": "We propose a novel ranking scheme for the synsets using a non-disambiguated corpus, address the issue of sense frequency distribution and utilize information from WordNet glosses to refine the ranking.", "labels": [], "entities": []}, {"text": "We conduct and experiment to show that our representation of a metonymic interpretation as a synset is intuitive to human subjects.", "labels": [], "entities": []}, {"text": "In the discussion section we provide an overview of the constraints on logical metonymy pointed out in linguistics literature, as well as proposing some additional constraints (e.g. on the type of the metonymic verb, on the type of the reconstructed event, etc.)", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated the performance of the system against the gold standard.", "labels": [], "entities": []}, {"text": "The objective was to find out if the synsets were distributed in such away that the plausible interpretations appear at the top of the list and the incorrect ones at the bottom.", "labels": [], "entities": []}, {"text": "The evaluation was done in terms of mean average precision (MAP) at top 30 synsets.", "labels": [], "entities": [{"text": "mean average precision (MAP)", "start_pos": 36, "end_pos": 64, "type": "METRIC", "confidence": 0.9311430354913076}]}, {"text": "where M is the number of metonymic phrases, N j is the number of correct interpretations for the metonymic phrase, P ji is the precision at each correct interpretation (the number of correct interpretations among the topi ranks).", "labels": [], "entities": [{"text": "P ji", "start_pos": 115, "end_pos": 119, "type": "METRIC", "confidence": 0.7285230159759521}, {"text": "precision", "start_pos": 127, "end_pos": 136, "type": "METRIC", "confidence": 0.9993962049484253}]}, {"text": "First, the average precision was computed for each metonymic phrase independently.", "labels": [], "entities": [{"text": "precision", "start_pos": 19, "end_pos": 28, "type": "METRIC", "confidence": 0.949633777141571}]}, {"text": "Then the mean values were calculated for the development and the test sets.", "labels": [], "entities": []}, {"text": "The reasoning behind computing MAP instead of precision at a fixed number of synsets (e.g. top 30) is that the number of correct interpretations varies dramatically for different metonymic phrases.", "labels": [], "entities": [{"text": "precision", "start_pos": 46, "end_pos": 55, "type": "METRIC", "confidence": 0.9983854293823242}]}, {"text": "MAP essentially evaluates how many good interpretations appear at the top of the list, which takes this variation into account.", "labels": [], "entities": [{"text": "MAP", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.6617357730865479}]}, {"text": "We conducted an experiment with humans in order to prove that this task is intuitive to people, i.e. they agree on the task.", "labels": [], "entities": []}, {"text": "We had 8 volunteer subjects altogether.", "labels": [], "entities": []}, {"text": "All of   them were native speakers of English and nonlinguists.", "labels": [], "entities": []}, {"text": "We divided them into 2 groups: 4 and 4.", "labels": [], "entities": []}, {"text": "Subjects in each group annotated three metonymic phrases as shown in.", "labels": [], "entities": []}, {"text": "They received written guidelines, which were the only source of information on the experiment.", "labels": [], "entities": []}, {"text": "For each metonymic phrase they were presented with a list of 30 possible interpretations produced by the system.", "labels": [], "entities": []}, {"text": "For each synset in the list they had to decide whether it was a plausible interpretation of the metonymic phrase in an imaginary context.", "labels": [], "entities": []}, {"text": "We evaluated interannotator agreement in terms of and f-measure computed pairwise and then averaged across the annotators.", "labels": [], "entities": []}, {"text": "The agreement in group 1 was 0.76 (f-measure) and 0.56 (kappa); in group 2 0.68 (f-measure) and 0.51 (kappa).", "labels": [], "entities": [{"text": "agreement", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9830512404441833}]}, {"text": "This yielded the average agreement of 0.72 (f-measure) and 0.53 (kappa).", "labels": [], "entities": [{"text": "agreement", "start_pos": 25, "end_pos": 34, "type": "METRIC", "confidence": 0.9953271150588989}, {"text": "f-measure", "start_pos": 44, "end_pos": 53, "type": "METRIC", "confidence": 0.9585741758346558}]}], "tableCaptions": []}