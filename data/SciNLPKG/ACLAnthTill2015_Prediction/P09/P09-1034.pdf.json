{"title": [{"text": "Robust Machine Translation Evaluation with Entailment Features *", "labels": [], "entities": [{"text": "Robust Machine Translation Evaluation", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.8131697475910187}]}], "abstractContent": [{"text": "Existing evaluation metrics for machine translation lack crucial robustness: their correlations with human quality judgments vary considerably across languages and genres.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 32, "end_pos": 51, "type": "TASK", "confidence": 0.8027156591415405}]}, {"text": "We believe that the main reason is their inability to properly capture meaning: A good translation candidate means the same thing as the reference translation, regardless of formulation.", "labels": [], "entities": []}, {"text": "We propose a metric that evaluates MT output based on a rich set of features motivated by textual entailment, such as lexical-semantic (in-)compatibility and argument structure overlap.", "labels": [], "entities": [{"text": "MT output", "start_pos": 35, "end_pos": 44, "type": "TASK", "confidence": 0.9004075527191162}]}, {"text": "We compare this metric against a combination metric of four state-of-the-art scores (BLEU, NIST, TER, and METEOR) in two different settings.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 85, "end_pos": 89, "type": "METRIC", "confidence": 0.9980500936508179}, {"text": "NIST", "start_pos": 91, "end_pos": 95, "type": "METRIC", "confidence": 0.577776312828064}, {"text": "TER", "start_pos": 97, "end_pos": 100, "type": "METRIC", "confidence": 0.9876678586006165}, {"text": "METEOR", "start_pos": 106, "end_pos": 112, "type": "METRIC", "confidence": 0.9965779185295105}]}, {"text": "The combination metric out-performs the individual scores, but is bested by the entailment-based metric.", "labels": [], "entities": []}, {"text": "Combining the entailment and traditional features yields further improvements.", "labels": [], "entities": []}], "introductionContent": [{"text": "Constant evaluation is vital to the progress of machine translation (MT).", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 48, "end_pos": 72, "type": "TASK", "confidence": 0.8426233410835267}]}, {"text": "Since human evaluation is costly and difficult to do reliably, a major focus of research has been on automatic measures of MT quality, pioneered by BLEU () and NIST).", "labels": [], "entities": [{"text": "MT", "start_pos": 123, "end_pos": 125, "type": "TASK", "confidence": 0.9955651164054871}, {"text": "BLEU", "start_pos": 148, "end_pos": 152, "type": "METRIC", "confidence": 0.9535415172576904}, {"text": "NIST", "start_pos": 160, "end_pos": 164, "type": "DATASET", "confidence": 0.9275248646736145}]}, {"text": "BLEU and NIST measure MT quality by using the strong correlation between human judgments and the degree of n-gram overlap between a system hypothesis translation and one or more reference translations.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.7621272802352905}, {"text": "NIST", "start_pos": 9, "end_pos": 13, "type": "DATASET", "confidence": 0.8249402046203613}, {"text": "MT", "start_pos": 22, "end_pos": 24, "type": "TASK", "confidence": 0.9957602620124817}]}, {"text": "The resulting scores are cheap and objective.", "labels": [], "entities": []}, {"text": "However, studies such as Callison- have identified a number of problems with BLEU and related n-gram-based scores: (1) BLEUlike metrics are unreliable at the level of individual sentences due to data sparsity; (2) BLEU metrics can be \"gamed\" by permuting word order; (3) for some corpora and languages, the correlation to human ratings is very low even at the system level; (4) scores are biased towards statistical MT; (5) the quality gap between MT and human translations is not reflected in equally large BLEU differences.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 77, "end_pos": 81, "type": "METRIC", "confidence": 0.9815618395805359}, {"text": "BLEUlike", "start_pos": 119, "end_pos": 127, "type": "METRIC", "confidence": 0.9848783612251282}, {"text": "BLEU", "start_pos": 214, "end_pos": 218, "type": "METRIC", "confidence": 0.9421989321708679}, {"text": "MT", "start_pos": 416, "end_pos": 418, "type": "TASK", "confidence": 0.9421255588531494}, {"text": "BLEU", "start_pos": 508, "end_pos": 512, "type": "METRIC", "confidence": 0.9542067646980286}]}, {"text": "This is problematic, but not surprising: The metrics treat any divergence from the reference as a negative, while (computational) linguistics has long dealt with linguistic variation that preserves the meaning, usually called paraphrase, such as: (1) HYP: However, this was declared terrorism by observers and witnesses.", "labels": [], "entities": [{"text": "HYP", "start_pos": 251, "end_pos": 254, "type": "METRIC", "confidence": 0.6694559454917908}]}, {"text": "REF: Nevertheless, commentators as well as eyewitnesses are terming it terrorism.", "labels": [], "entities": [{"text": "REF", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9115784168243408}]}, {"text": "A number of metrics have been designed to account for paraphrase, either by making the matching more intelligent (TER, Snover et al.), or by using linguistic evidence, mostly lexical similarity (ME-TEOR,; MaxSim,), or syntactic overlap (;).", "labels": [], "entities": [{"text": "TER", "start_pos": 114, "end_pos": 117, "type": "METRIC", "confidence": 0.9874923229217529}]}, {"text": "Unfortunately, each metrics tend to concentrate on one particular type of linguistic information, none of which always correlates well with human judgments.", "labels": [], "entities": []}, {"text": "Our paper proposes two strategies.", "labels": [], "entities": []}, {"text": "We first explore the combination of traditional scores into a more robust ensemble metric with linear regression.", "labels": [], "entities": []}, {"text": "Our second, more fundamental, strategy replaces the use of loose surrogates of translation quality with a model that attempts to comprehensively assess meaning equivalence between references and MT hypotheses.", "labels": [], "entities": [{"text": "MT hypotheses", "start_pos": 195, "end_pos": 208, "type": "TASK", "confidence": 0.9010977149009705}]}, {"text": "We operationalize meaning equivalence by bidirectional textual entailment (RTE,), and thus predict the quality of MT hypotheses with a rich RTE feature set.", "labels": [], "entities": [{"text": "operationalize meaning equivalence", "start_pos": 3, "end_pos": 37, "type": "TASK", "confidence": 0.7076256573200226}, {"text": "MT hypotheses", "start_pos": 114, "end_pos": 127, "type": "TASK", "confidence": 0.9239398241043091}]}, {"text": "The entailment-based model goes beyond existing word-level \"semantic\" metrics such as METEOR by integrating phrasal and compositional aspects of meaning equivalence, such as multiword paraphrases, (in-)correct argument and modification relations, and (dis-)allowed phrase reorderings.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 86, "end_pos": 92, "type": "METRIC", "confidence": 0.5746575593948364}]}, {"text": "We demonstrate that the resulting metric beats both individual and combined traditional MT metrics.", "labels": [], "entities": [{"text": "MT", "start_pos": 88, "end_pos": 90, "type": "TASK", "confidence": 0.9819454550743103}]}, {"text": "The complementary features of both metric types can be combined into a joint, superior metric.", "labels": [], "entities": []}, {"text": "Figure 1: Entailment status between an MT system hypothesis and a reference translation for equivalent (top) and non-equivalent (bottom) translations.", "labels": [], "entities": [{"text": "MT system", "start_pos": 39, "end_pos": 48, "type": "TASK", "confidence": 0.8983836770057678}]}], "datasetContent": [{"text": "Our novel approach to MT evaluation exploits the similarity between MT evaluation and textual entailment (TE).", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 22, "end_pos": 35, "type": "TASK", "confidence": 0.9793055951595306}, {"text": "MT evaluation", "start_pos": 68, "end_pos": 81, "type": "TASK", "confidence": 0.9066665172576904}, {"text": "textual entailment (TE)", "start_pos": 86, "end_pos": 109, "type": "TASK", "confidence": 0.6129604160785675}]}, {"text": "TE was introduced by as a concept that corresponds more closely to \"common sense\" reasoning patterns than classical, strict logical entailment.", "labels": [], "entities": [{"text": "TE", "start_pos": 0, "end_pos": 2, "type": "TASK", "confidence": 0.7087627649307251}]}, {"text": "Textual entailment is defined informally as a relation between two natural language sentences (a premise P and a hypothesis H) that holds if \"a human reading P would infer that H is most likely true\".", "labels": [], "entities": [{"text": "Textual entailment", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7759960889816284}]}, {"text": "Knowledge about entailment is beneficial for NLP tasks such as Question Answering ().", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 63, "end_pos": 81, "type": "TASK", "confidence": 0.7704496383666992}]}, {"text": "The relation between textual entailment and MT evaluation is shown in.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 44, "end_pos": 57, "type": "TASK", "confidence": 0.9770978093147278}]}, {"text": "Perfect MT output and the reference translation entail each other (top).", "labels": [], "entities": [{"text": "MT", "start_pos": 8, "end_pos": 10, "type": "TASK", "confidence": 0.8941342234611511}]}, {"text": "Translation problems that impact semantic equivalence, e.g., deletion or addition of material, can break entailment in one or both directions (bottom).", "labels": [], "entities": [{"text": "Translation", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.9463757872581482}]}, {"text": "On the modelling level, there is common ground between RTE and MT evaluation: Both have to distinguish between valid and invalid variation to determine whether two texts convey the same information or not.", "labels": [], "entities": [{"text": "RTE", "start_pos": 55, "end_pos": 58, "type": "TASK", "confidence": 0.957239031791687}, {"text": "MT evaluation", "start_pos": 63, "end_pos": 76, "type": "TASK", "confidence": 0.9433316290378571}]}, {"text": "For example, to recognize the bidirectional entailment in Ex.", "labels": [], "entities": []}, {"text": "(1), RTE must account for the following reformulations: synonymy (However/Nevertheless), more general semantic relatedness (observers/commentators), phrasal replacements (and/as well as), and an active/passive alternation that implies structural change (is declared/are terming).", "labels": [], "entities": [{"text": "RTE", "start_pos": 5, "end_pos": 8, "type": "TASK", "confidence": 0.8887055516242981}]}, {"text": "This leads us to our main hypothesis: RTE features are designed to distinguish meaning-preserving variation from true divergence and are thus also good predictors in MT evaluation.", "labels": [], "entities": [{"text": "RTE", "start_pos": 38, "end_pos": 41, "type": "TASK", "confidence": 0.9246847629547119}, {"text": "MT evaluation", "start_pos": 166, "end_pos": 179, "type": "TASK", "confidence": 0.9618897140026093}]}, {"text": "However, while the original RTE task is asymmetric, MT evaluation needs to determine meaning equivalence, which is asymmetric relation.", "labels": [], "entities": [{"text": "RTE task", "start_pos": 28, "end_pos": 36, "type": "TASK", "confidence": 0.8763893842697144}, {"text": "MT evaluation", "start_pos": 52, "end_pos": 65, "type": "TASK", "confidence": 0.9254887104034424}]}, {"text": "We do this by checking for entailment in both directions (see.", "labels": [], "entities": []}, {"text": "Operationally, this ensures we detect translations which either delete or insert material.", "labels": [], "entities": []}, {"text": "Clearly, there are also differences between the two tasks.", "labels": [], "entities": []}, {"text": "An important one is that RTE assumes the well-formedness of the two sentences.", "labels": [], "entities": [{"text": "RTE", "start_pos": 25, "end_pos": 28, "type": "TASK", "confidence": 0.6628112196922302}]}, {"text": "This is not generally true in MT, and could lead to degraded linguistic analyses.", "labels": [], "entities": [{"text": "MT", "start_pos": 30, "end_pos": 32, "type": "TASK", "confidence": 0.9848758578300476}]}, {"text": "However, entailment relations are more sensitive to the contribution of individual words.", "labels": [], "entities": []}, {"text": "In Example 2, the modal modifiers break the entailment between two otherwise identical sentences: (2) HYP: Peter is certainly from Lincolnshire.", "labels": [], "entities": [{"text": "HYP", "start_pos": 102, "end_pos": 105, "type": "METRIC", "confidence": 0.9508786797523499}]}, {"text": "REF: Peter is possibly from Lincolnshire.", "labels": [], "entities": [{"text": "REF", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.723875880241394}]}, {"text": "This means that the prediction of TE hinges on correct semantic analysis and is sensitive to misanalyses.", "labels": [], "entities": [{"text": "prediction of TE", "start_pos": 20, "end_pos": 36, "type": "TASK", "confidence": 0.713378111521403}]}, {"text": "In contrast, human MT judgments behave robustly.", "labels": [], "entities": [{"text": "MT judgments", "start_pos": 19, "end_pos": 31, "type": "TASK", "confidence": 0.9297060966491699}]}, {"text": "Translations that involve individual errors, like (2), are judged lower than perfect ones, but usually not crucially so, since most aspects are still rendered correctly.", "labels": [], "entities": []}, {"text": "We thus expect even noisy RTE features to be predictive for translation quality.", "labels": [], "entities": [{"text": "translation", "start_pos": 60, "end_pos": 71, "type": "TASK", "confidence": 0.956889271736145}]}, {"text": "This allows us to use an off-the-shelf RTE system to obtain features, and to combine them using a regression model as described in Section 2.", "labels": [], "entities": []}, {"text": "Traditionally, human ratings for MT quality have been collected in the form of absolute scores on a five-or seven-point Likert scale, but low reliability numbers for this type of annotation have raised concerns).", "labels": [], "entities": [{"text": "MT", "start_pos": 33, "end_pos": 35, "type": "TASK", "confidence": 0.9946537017822266}]}, {"text": "An alternative that has been adopted by the yearly WMT evaluation shared tasks since 2008 is the collection of pairwise preference judgments between pairs of MT hypotheses which can be elicited (somewhat) more reliably.", "labels": [], "entities": [{"text": "WMT evaluation shared tasks", "start_pos": 51, "end_pos": 78, "type": "TASK", "confidence": 0.76483453810215}, {"text": "MT hypotheses", "start_pos": 158, "end_pos": 171, "type": "TASK", "confidence": 0.8598276078701019}]}, {"text": "We demonstrate that our approach works well for both types of annotation and different corpora.", "labels": [], "entities": []}, {"text": "Experiment 1 models absolute scores on Asian newswire, and Experiment 2 pairwise preferences on European speech and news data.", "labels": [], "entities": []}, {"text": "We evaluate the output of our models both on the sentence and on the system level.", "labels": [], "entities": []}, {"text": "At the sentence level, we can correlate predictions in Experiment 1 directly with human judgments with Spearman's \u03c1, a non-parametric rank correlation coefficient appropriate for non-normally distributed data.", "labels": [], "entities": [{"text": "Spearman's \u03c1", "start_pos": 103, "end_pos": 115, "type": "METRIC", "confidence": 0.7132546106974283}]}, {"text": "In Experiment 2, the predictions cannot be pooled between sentences.", "labels": [], "entities": []}, {"text": "Instead of correlation, we compute \"consistency\" (i.e., accuracy) with human preferences.", "labels": [], "entities": [{"text": "consistency", "start_pos": 36, "end_pos": 47, "type": "METRIC", "confidence": 0.9824490547180176}, {"text": "accuracy", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.9928313493728638}]}, {"text": "System-level predictions are computed in both experiments from sentence-level predictions, as the ratio of sentences for which each system provided the best translation.", "labels": [], "entities": []}, {"text": "We extend this procedure slightly because realvalued predictions cannot predict ties, while human raters decide fora significant portion of sentences (as much as 80% in absolute score annotation) to \"tie\" two systems for first place.", "labels": [], "entities": []}, {"text": "To simulate this behavior, we compute \"tie-aware\" predictions as the percentage of sentences where the system's hypothesis was assigned a score better or at most \u03b5 worse than the best system.", "labels": [], "entities": []}, {"text": "\u03b5 is set to match the frequency of ties in the training data.", "labels": [], "entities": []}, {"text": "Finally, the predictions are again correlated with human judgments using Spearman's \u03c1.", "labels": [], "entities": []}, {"text": "\"Tie awareness\" makes a considerable practical difference, improving correlation figures by 5-10 points.", "labels": [], "entities": [{"text": "Tie awareness", "start_pos": 1, "end_pos": 14, "type": "TASK", "confidence": 0.9477586150169373}, {"text": "correlation", "start_pos": 69, "end_pos": 80, "type": "METRIC", "confidence": 0.9596840739250183}]}, {"text": "It consists of data from EU-ROPARL () and various news commentaries, with five source languages (French, German, Spanish, Czech, and Hungarian).", "labels": [], "entities": [{"text": "EU-ROPARL", "start_pos": 25, "end_pos": 34, "type": "DATASET", "confidence": 0.93057781457901}]}, {"text": "As training set, we use the portions of WMT 2006 and 2007 that are annotated with absolute scores on a fivepoint scale (around 14,000 sentences produced by 40 systems).", "labels": [], "entities": [{"text": "WMT 2006", "start_pos": 40, "end_pos": 48, "type": "DATASET", "confidence": 0.8850194811820984}]}, {"text": "The test set is formed by the WMT 2008 relative rank annotation task.", "labels": [], "entities": [{"text": "WMT 2008 relative rank annotation", "start_pos": 30, "end_pos": 63, "type": "TASK", "confidence": 0.5595516741275788}]}, {"text": "As in Experiment 1, we set \u03b5 so that the incidence of ties in the training and test set is equal (60%).", "labels": [], "entities": [{"text": "incidence", "start_pos": 41, "end_pos": 50, "type": "METRIC", "confidence": 0.9900447726249695}]}, {"text": "The left result column shows consistency, i.e., the accuracy on human pairwise preference judgments.", "labels": [], "entities": [{"text": "consistency", "start_pos": 29, "end_pos": 40, "type": "METRIC", "confidence": 0.9985787868499756}, {"text": "accuracy", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.9994500279426575}]}, {"text": "The pattern of results matches our observations in Expt.", "labels": [], "entities": [{"text": "Expt", "start_pos": 51, "end_pos": 55, "type": "DATASET", "confidence": 0.9152359366416931}]}, {"text": "1: Among individual metrics, METEORR and TERR do better than BLEUR and NISTR.", "labels": [], "entities": [{"text": "METEORR", "start_pos": 29, "end_pos": 36, "type": "METRIC", "confidence": 0.9897671341896057}, {"text": "TERR", "start_pos": 41, "end_pos": 45, "type": "METRIC", "confidence": 0.9978813529014587}, {"text": "BLEUR", "start_pos": 61, "end_pos": 66, "type": "METRIC", "confidence": 0.9990851879119873}, {"text": "NISTR", "start_pos": 71, "end_pos": 76, "type": "METRIC", "confidence": 0.7016429901123047}]}, {"text": "MTR and RTER outperform individual metrics.", "labels": [], "entities": [{"text": "MTR", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.5752967596054077}, {"text": "RTER", "start_pos": 8, "end_pos": 12, "type": "METRIC", "confidence": 0.9124049544334412}]}, {"text": "The best result by a wide margin, 52.5%, is shown by MT+RTER.", "labels": [], "entities": [{"text": "MT", "start_pos": 53, "end_pos": 55, "type": "METRIC", "confidence": 0.5719567537307739}, {"text": "RTER", "start_pos": 56, "end_pos": 60, "type": "METRIC", "confidence": 0.5073965787887573}]}, {"text": "The right column shows Spearman's \u03c1 for the correlation between human judgments and tieaware system-level predictions.", "labels": [], "entities": [{"text": "Spearman's \u03c1", "start_pos": 23, "end_pos": 35, "type": "METRIC", "confidence": 0.5663253168265024}]}, {"text": "All metrics predict system scores highly significantly, partly due to the larger number of systems compared (87 systems).", "labels": [], "entities": []}, {"text": "Again, we see better results for METEORR and TERR than for BLEUR and NISTR, and the individual metrics do worse than the combination models.", "labels": [], "entities": [{"text": "METEORR", "start_pos": 33, "end_pos": 40, "type": "METRIC", "confidence": 0.9867581725120544}, {"text": "TERR", "start_pos": 45, "end_pos": 49, "type": "METRIC", "confidence": 0.9941810965538025}, {"text": "BLEUR", "start_pos": 59, "end_pos": 64, "type": "METRIC", "confidence": 0.9977990984916687}, {"text": "NISTR", "start_pos": 69, "end_pos": 74, "type": "DATASET", "confidence": 0.7859796285629272}]}, {"text": "Among the latter, the order is: MTR (worst), MT+RTER, and RTER (best at 78.3).", "labels": [], "entities": [{"text": "MTR", "start_pos": 32, "end_pos": 35, "type": "METRIC", "confidence": 0.9733223915100098}, {"text": "MT+RTER", "start_pos": 45, "end_pos": 52, "type": "METRIC", "confidence": 0.6698000828425089}, {"text": "RTER", "start_pos": 58, "end_pos": 62, "type": "METRIC", "confidence": 0.9987452030181885}]}, {"text": "2 RTER metric to the WMT 2009 shared MT evaluation task).", "labels": [], "entities": [{"text": "RTER", "start_pos": 2, "end_pos": 6, "type": "METRIC", "confidence": 0.9965797066688538}, {"text": "WMT 2009 shared MT evaluation task", "start_pos": 21, "end_pos": 55, "type": "TASK", "confidence": 0.6471088429292043}]}, {"text": "The results provide further validation for our results and our general approach.", "labels": [], "entities": []}, {"text": "At the system level, RTER made third place (avg. correlation \u03c1 = 0.79), trailing the two top metrics closely (\u03c1 = 0.80, \u03c1 = 0.83) and making the best predictions for Hungarian.", "labels": [], "entities": [{"text": "RTER", "start_pos": 21, "end_pos": 25, "type": "METRIC", "confidence": 0.6673018336296082}, {"text": "avg. correlation \u03c1", "start_pos": 44, "end_pos": 62, "type": "METRIC", "confidence": 0.842474619547526}, {"text": "Hungarian", "start_pos": 166, "end_pos": 175, "type": "DATASET", "confidence": 0.8490968942642212}]}, {"text": "It also obtained the second-best consistency score (53%, best: 54%).", "labels": [], "entities": [{"text": "consistency score", "start_pos": 33, "end_pos": 50, "type": "METRIC", "confidence": 0.9854824244976044}]}, {"text": "The pairwise preference annotation of WMT 2008 gives us the opportunity to compare the MTR and RTER models by computing consistency separately on the \"top\" (highestranked) and \"bottom\" (lowest-ranked) hypotheses for each reference.", "labels": [], "entities": [{"text": "WMT 2008", "start_pos": 38, "end_pos": 46, "type": "DATASET", "confidence": 0.8923501670360565}, {"text": "MTR", "start_pos": 87, "end_pos": 90, "type": "DATASET", "confidence": 0.7114250063896179}, {"text": "consistency", "start_pos": 120, "end_pos": 131, "type": "METRIC", "confidence": 0.9866800904273987}]}, {"text": "RTER performs about 1.5 percent better on the top than on the bottom hypotheses.", "labels": [], "entities": [{"text": "RTER", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.4329715967178345}]}, {"text": "The MTR model shows the inverse behavior, performing 2 percent worse on the top hypotheses.", "labels": [], "entities": [{"text": "MTR", "start_pos": 4, "end_pos": 7, "type": "DATASET", "confidence": 0.6052645444869995}]}, {"text": "This matches well with our intuitions: We see some noise-induced degradation for the entailment features, but not much.", "labels": [], "entities": []}, {"text": "In contrast, surface-based features are better at detecting bad translations than at discriminating among good ones.", "labels": [], "entities": []}, {"text": "further illustrates the difference between the top models on two example sentences.", "labels": [], "entities": []}, {"text": "In the top example, RTER makes a more accurate prediction than MTR.", "labels": [], "entities": [{"text": "RTER", "start_pos": 20, "end_pos": 24, "type": "METRIC", "confidence": 0.6735011339187622}, {"text": "MTR", "start_pos": 63, "end_pos": 66, "type": "DATASET", "confidence": 0.5954596996307373}]}, {"text": "The human rater's favorite translation deviates considerably from the reference in lexical choice, syntactic structure, and word order, for which it is punished by MTR (rank 3/5).", "labels": [], "entities": [{"text": "MTR", "start_pos": 164, "end_pos": 167, "type": "METRIC", "confidence": 0.9946075081825256}]}, {"text": "In contrast, RTER determines correctly that the propositional content of the reference is almost completely preserved (rank 1).", "labels": [], "entities": [{"text": "RTER", "start_pos": 13, "end_pos": 17, "type": "METRIC", "confidence": 0.7176570296287537}]}, {"text": "In the bottom example, RTER's prediction is less accurate.", "labels": [], "entities": [{"text": "RTER", "start_pos": 23, "end_pos": 27, "type": "METRIC", "confidence": 0.5789259672164917}]}, {"text": "This sentence was rated as bad by the judge, presumably due to the inappropriate main verb translation.", "labels": [], "entities": []}, {"text": "Together with the subject mismatch, MTR correctly predicts a low score (rank 5/5).", "labels": [], "entities": [{"text": "MTR", "start_pos": 36, "end_pos": 39, "type": "TASK", "confidence": 0.6838798522949219}]}, {"text": "RTER's attention to semantic overlap leads to an incorrect high score (rank 2/5).", "labels": [], "entities": [{"text": "RTER", "start_pos": 0, "end_pos": 4, "type": "TASK", "confidence": 0.5509759783744812}, {"text": "incorrect high score", "start_pos": 49, "end_pos": 69, "type": "METRIC", "confidence": 0.7062184810638428}]}, {"text": "Finally, we make two observations about feature weights in the RTER model.", "labels": [], "entities": []}, {"text": "First, the model has learned high weights not only for the overall alignment score (which behaves most similarly to traditional metrics), but also fora number of binary syntacto-semantic match and mismatch features.", "labels": [], "entities": []}, {"text": "This confirms that these features systematically confer the benefit we have shown anecdotally in.", "labels": [], "entities": []}, {"text": "Features with a consistently negative effect include dropping adjuncts, unaligned or poorly aligned root nodes, incompatible modality between the main clauses, person and location mismatches (as opposed to general mismatches) and wrongly handled passives.", "labels": [], "entities": []}, {"text": "Con-versely, higher scores result from factors such as high alignment score, matching embeddings under factive verbs, and matches between appositions.", "labels": [], "entities": []}, {"text": "Second, good MT evaluation feature weights are not good weights for RTE.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 13, "end_pos": 26, "type": "TASK", "confidence": 0.898962140083313}, {"text": "RTE", "start_pos": 68, "end_pos": 71, "type": "TASK", "confidence": 0.8345566391944885}]}, {"text": "Some differences, particularly for structural features, are caused by the low grammaticality of MT data.", "labels": [], "entities": [{"text": "MT", "start_pos": 96, "end_pos": 98, "type": "TASK", "confidence": 0.9521558284759521}]}, {"text": "For example, the feature that fires for mismatches between dependents of predicates is unreliable on the WMT data.", "labels": [], "entities": [{"text": "WMT data", "start_pos": 105, "end_pos": 113, "type": "DATASET", "confidence": 0.939216673374176}]}, {"text": "Other differences do reflect more fundamental differences between the two tasks (cf. Section 3).", "labels": [], "entities": []}, {"text": "For example, RTE puts high weights onto quantifier and polarity features, both of which have the potential of influencing entailment decisions, but are (at least currently) unimportant for MT evaluation.", "labels": [], "entities": [{"text": "RTE", "start_pos": 13, "end_pos": 16, "type": "DATASET", "confidence": 0.5232476592063904}, {"text": "MT evaluation", "start_pos": 189, "end_pos": 202, "type": "TASK", "confidence": 0.958164244890213}]}], "tableCaptions": [{"text": " Table 1: Expt. 1: Spearman's \u03c1 for correlation between human absolute scores and model predictions on  NIST OpenMT 2008. Sentence level: All correlations are highly significant. System level:  *  : p<0.05.", "labels": [], "entities": [{"text": "Spearman's \u03c1", "start_pos": 19, "end_pos": 31, "type": "METRIC", "confidence": 0.5501464406649271}, {"text": "NIST OpenMT 2008", "start_pos": 104, "end_pos": 120, "type": "DATASET", "confidence": 0.8958599170049032}]}, {"text": " Table 3: Expt. 2: Reference translations and MT output (French). Ranks are out of five (smaller is better).", "labels": [], "entities": [{"text": "MT", "start_pos": 46, "end_pos": 48, "type": "TASK", "confidence": 0.6041774153709412}]}, {"text": " Table 4: Expt. 2: Prediction of pairwise preferences  on the WMT 2008 dataset.", "labels": [], "entities": [{"text": "WMT 2008 dataset", "start_pos": 62, "end_pos": 78, "type": "DATASET", "confidence": 0.9663979411125183}]}]}