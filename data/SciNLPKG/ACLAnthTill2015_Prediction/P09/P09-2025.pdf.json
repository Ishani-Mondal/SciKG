{"title": [{"text": "Correlating Human and Automatic Evaluation of a German Surface Realiser", "labels": [], "entities": [{"text": "Correlating Human and Automatic Evaluation of a German Surface Realiser", "start_pos": 0, "end_pos": 71, "type": "TASK", "confidence": 0.5073430866003037}]}], "abstractContent": [{"text": "We examine correlations between native speaker judgements on automatically generated German text against automatic evaluation metrics.", "labels": [], "entities": []}, {"text": "We look at a number of metrics from the MT and Summarisation communities and find that fora relative ranking task, most automatic metrics perform equally well and have fairly strong correlations to the human judgements.", "labels": [], "entities": [{"text": "MT", "start_pos": 40, "end_pos": 42, "type": "TASK", "confidence": 0.8879169821739197}, {"text": "Summarisation", "start_pos": 47, "end_pos": 60, "type": "TASK", "confidence": 0.7541609406471252}]}, {"text": "In contrast, on a naturalness judgement task, the General Text Matcher (GTM) tool correlates best overall, although in general , correlation between the human judgements and the automatic metrics was quite weak.", "labels": [], "entities": [{"text": "General Text Matcher (GTM)", "start_pos": 50, "end_pos": 76, "type": "TASK", "confidence": 0.685365249713262}]}], "introductionContent": [{"text": "During the development of a surface realisation system, it is important to be able to quickly and automatically evaluate its performance.", "labels": [], "entities": [{"text": "surface realisation", "start_pos": 28, "end_pos": 47, "type": "TASK", "confidence": 0.6784639358520508}]}, {"text": "The evaluation of a string realisation system usually involves string comparisons between the output of the system and some gold standard set of strings.", "labels": [], "entities": []}, {"text": "Typically automatic metrics from the fields of Machine Translation (e.g. BLEU) or Summarisation (e.g. ROUGE) are used, but it is not clear how successful or even appropriate these are. and describe comparison experiments between the automatic evaluation of system output and human (expert and non-expert) evaluation of the same data (English weather forecasts).", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 47, "end_pos": 66, "type": "TASK", "confidence": 0.699009120464325}, {"text": "BLEU", "start_pos": 73, "end_pos": 77, "type": "METRIC", "confidence": 0.9503318667411804}, {"text": "ROUGE", "start_pos": 102, "end_pos": 107, "type": "METRIC", "confidence": 0.8263189196586609}]}, {"text": "Their findings show that the NIST metric correlates best with the human judgements, and all automatic metrics favour systems that generate based on frequency.", "labels": [], "entities": [{"text": "NIST metric", "start_pos": 29, "end_pos": 40, "type": "DATASET", "confidence": 0.8175670206546783}]}, {"text": "They conclude that automatic evaluations should be accompanied by human evaluations where possible.", "labels": [], "entities": []}, {"text": "investigate a number of automatic evaluation methods for generation in terms of adequacy and fluency on automatically generated English paraphrases.", "labels": [], "entities": []}, {"text": "They find that the automatic metrics are reasonably good at measuring adequacy, but not good measures of fluency, i.e. syntactic correctness.", "labels": [], "entities": []}, {"text": "In this paper, we carryout experiments to correlate automatic evaluation of the output of a surface realisation ranking system for German against human judgements.", "labels": [], "entities": []}, {"text": "We particularly look at correlations at the individual sentence level.", "labels": [], "entities": []}], "datasetContent": [{"text": "The data used in our experiments is the output of the German realisation ranking system.", "labels": [], "entities": [{"text": "German realisation ranking system", "start_pos": 54, "end_pos": 87, "type": "DATASET", "confidence": 0.802927702665329}]}, {"text": "That system is couched within the Lexical Functional Grammar (LFG) grammatical framework.", "labels": [], "entities": [{"text": "Lexical Functional Grammar (LFG) grammatical", "start_pos": 34, "end_pos": 78, "type": "TASK", "confidence": 0.7035586408206395}]}, {"text": "LFG has two levels of representation, C(onstituent)-Structure which is a contextfree tree representation and F(unctional)-Structure which is a recursive attribute-value matrix capturing basic predicate-argument-adjunct relations.", "labels": [], "entities": []}, {"text": "use a large-scale handcrafted grammar) to generate a number of (almost always) grammatical sentences given an input F-Structure.", "labels": [], "entities": []}, {"text": "They show that a linguistically-inspired log-linear ranking model outperforms a simple baseline tri-gram language model trained on the Huge German Corpus (HGC), a corpus of 200 million words of newspaper and other text.", "labels": [], "entities": [{"text": "Huge German Corpus (HGC)", "start_pos": 135, "end_pos": 159, "type": "DATASET", "confidence": 0.8200028638044993}]}, {"text": "Cahill and Forst (2009) describe a number of experiments where they collect judgements from native speakers about the three systems compared in: (i) the original corpus string, (ii) the string chosen by the language model, and (iii) the string chosen by the linguistically-inspired log-linear model.", "labels": [], "entities": []}, {"text": "We only take the data from 2 of those experiments since the remaining experiments would not provide any In all cases, the three strings were different.", "labels": [], "entities": []}, {"text": "In the first experiment that we consider (A), subjects are asked to rank on a scale from 1-3 (1 being the best, 3 being the worst) the output of the three systems (joint rankings were not permitted).", "labels": [], "entities": []}, {"text": "In the second experiment (B), subjects were asked to rank on a scale from 1-5 (1 being the worst, 5 being the best) how natural sounding the string chosen by the log-linear model was.", "labels": [], "entities": []}, {"text": "The goal of experiment B was to determine whether the log-linear model was choosing good or bad alternatives to the original string.", "labels": [], "entities": []}, {"text": "Judgements on the data were collected from 24 native German speakers.", "labels": [], "entities": []}, {"text": "There were 44 items in Experiment A with an average sentence length of 14.4, and there were 52 items in Experiment B with an average sentence length of 12.1.", "labels": [], "entities": []}, {"text": "Each item was judged by each native speaker at least once.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Average scores of each metric for Exper- iment A data", "labels": [], "entities": [{"text": "Exper- iment A data", "start_pos": 44, "end_pos": 63, "type": "DATASET", "confidence": 0.5634248554706573}]}, {"text": " Table 2: Correlation between human judgements  for experiment A (rank 1-3) and automatic metrics", "labels": [], "entities": []}, {"text": " Table 3: Correlation between human judgements  for experiment B (naturalness scale 1-5) and au- tomatic metrics", "labels": [], "entities": []}]}