{"title": [{"text": "Improving Automatic Speech Recognition for Lectures through Transformation-based Rules Learned from Minimal Data", "labels": [], "entities": [{"text": "Improving Automatic Speech Recognition", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.894119381904602}]}], "abstractContent": [{"text": "We demonstrate that transformation-based learning can be used to correct noisy speech recognition transcripts in the lecture domain with an average word error rate reduction of 12.9%.", "labels": [], "entities": [{"text": "correct noisy speech recognition transcripts", "start_pos": 65, "end_pos": 109, "type": "TASK", "confidence": 0.6969268202781678}, {"text": "word error rate reduction", "start_pos": 148, "end_pos": 173, "type": "METRIC", "confidence": 0.8101644068956375}]}, {"text": "Our method is distinguished from earlier related work by its robustness to small amounts of training data, and its resulting efficiency, in spite of its use of true word error rate computations as a rule scoring function.", "labels": [], "entities": []}], "introductionContent": [{"text": "Improving access to archives of recorded lectures is a task that, by its very nature, requires research efforts common to both Automatic Speech Recognition (ASR) and Human-Computer Interaction (HCI).", "labels": [], "entities": [{"text": "Improving access to archives of recorded lectures", "start_pos": 0, "end_pos": 49, "type": "TASK", "confidence": 0.7257749949182782}, {"text": "Automatic Speech Recognition (ASR)", "start_pos": 127, "end_pos": 161, "type": "TASK", "confidence": 0.7731290956338247}]}, {"text": "One of the main challenges to integrating text transcripts into archives of webcast lectures is the poor performance of ASR systems on lecture transcription.", "labels": [], "entities": [{"text": "ASR", "start_pos": 120, "end_pos": 123, "type": "TASK", "confidence": 0.9584468007087708}, {"text": "lecture transcription", "start_pos": 135, "end_pos": 156, "type": "TASK", "confidence": 0.6883923709392548}]}, {"text": "This is in part caused by the mismatch between the language used in a lecture and the predictive language models employed by most ASR systems.", "labels": [], "entities": [{"text": "ASR", "start_pos": 130, "end_pos": 133, "type": "TASK", "confidence": 0.9837155938148499}]}, {"text": "Most ASR systems achieve Word Error Rates (WERs) of about 40-45% in realistic and uncontrolled lecture conditions ().", "labels": [], "entities": [{"text": "ASR", "start_pos": 5, "end_pos": 8, "type": "TASK", "confidence": 0.9855718612670898}, {"text": "Word Error Rates (WERs)", "start_pos": 25, "end_pos": 48, "type": "METRIC", "confidence": 0.8807450334231058}]}, {"text": "Progress in ASR for this genre requires both better acoustic modelling () and better language modelling (.", "labels": [], "entities": [{"text": "ASR", "start_pos": 12, "end_pos": 15, "type": "TASK", "confidence": 0.9925066828727722}, {"text": "language modelling", "start_pos": 85, "end_pos": 103, "type": "TASK", "confidence": 0.6978815943002701}]}, {"text": "In contrast to some unsupervised approaches to language modelling that require large amounts of manual transcription, either from the same instructor or on the same topic (), the solution proposed by uses half of the lectures in a semester course to train an ASR system for the other half or for when the course is next offered, and still results in significant WER reductions.", "labels": [], "entities": [{"text": "language modelling", "start_pos": 47, "end_pos": 65, "type": "TASK", "confidence": 0.7640868425369263}, {"text": "WER", "start_pos": 362, "end_pos": 365, "type": "METRIC", "confidence": 0.9888840913772583}]}, {"text": "And yet even in this scenario, the business case for manually transcribing half of the lecture material in every recorded course is difficult to make, to say the least.", "labels": [], "entities": []}, {"text": "Manually transcribing a one-hour recorded lecture requires at least 5 hours in the hands of qualified transcribers) and roughly 10 hours by students enrolled in the course (.", "labels": [], "entities": []}, {"text": "As argued by, any ASR improvements that rely on manual transcripts need to offer a balance between the cost of producing those transcripts and the amount of improvement (i.e. WER reductions).", "labels": [], "entities": [{"text": "ASR", "start_pos": 18, "end_pos": 21, "type": "TASK", "confidence": 0.9921481013298035}, {"text": "WER reductions", "start_pos": 175, "end_pos": 189, "type": "METRIC", "confidence": 0.9663548171520233}]}, {"text": "There is some work that specializes in adaptive language modelling with extremely limited amounts of manual transcripts.", "labels": [], "entities": [{"text": "adaptive language modelling", "start_pos": 39, "end_pos": 66, "type": "TASK", "confidence": 0.6642897228399912}]}, {"text": "filters the corpus on which language models are trained in order to retain the parts that are more similar to the correct transcripts on a particular topic.", "labels": [], "entities": []}, {"text": "This technique resulted in relative WER reductions of between 7% and 10%.", "labels": [], "entities": [{"text": "WER reductions", "start_pos": 36, "end_pos": 50, "type": "METRIC", "confidence": 0.9009555578231812}]}, {"text": "use an information retrieval technique that exploits lecture presentation slides, automatically mining the World Wide Web for documents related to the topic as attested by text on the slides, and using these to build a bettermatching language model.", "labels": [], "entities": []}, {"text": "This yields about an 11% relative WER reduction for lecture-specific language models.", "labels": [], "entities": [{"text": "WER", "start_pos": 34, "end_pos": 37, "type": "METRIC", "confidence": 0.9936138987541199}]}, {"text": "Following upon other applications of computer-supported collaborative work to address shortcomings of other systems in artificial intelligence), a wikibased technique for collaboratively editing lecture transcripts has been shown to produce entirely cor-rected transcripts, given the proper motivation for students to participate (.", "labels": [], "entities": []}, {"text": "Another approach is active learning, where the goal is to selector generate a subset of the available data that would be the best candidate for ASR adaptation or training (.", "labels": [], "entities": [{"text": "ASR adaptation", "start_pos": 144, "end_pos": 158, "type": "TASK", "confidence": 0.9927210509777069}]}, {"text": "Even with all of these, however, there remains a significant gap between this WER and the threshold of 25%, at which lecture transcripts have been shown with statistical significance to improve student performance on atypical lecture browsing task ().", "labels": [], "entities": [{"text": "WER", "start_pos": 78, "end_pos": 81, "type": "METRIC", "confidence": 0.9408250451087952}]}, {"text": "People have also tried to correct ASR output in a second pass.", "labels": [], "entities": [{"text": "ASR output", "start_pos": 34, "end_pos": 44, "type": "METRIC", "confidence": 0.6721622347831726}]}, {"text": "treated ASR errors as noise produced by an auxiliary noisy channel, and tried to decode back to the perfect transcript.", "labels": [], "entities": [{"text": "ASR", "start_pos": 8, "end_pos": 11, "type": "TASK", "confidence": 0.808242917060852}]}, {"text": "This reduced WER from 41% to 35% on a corpus of train dispatch dialogues.", "labels": [], "entities": [{"text": "WER", "start_pos": 13, "end_pos": 16, "type": "METRIC", "confidence": 0.593749463558197}]}, {"text": "Others combine the transcripts or word lattices (from which transcripts are extracted) of two complementary ASR systems, a technique first proposed in the context of NIST's ROVER system (Fiscus, 1997) with a 12% relative error reduction (RER), and subsequently widely employed in many ASR systems.", "labels": [], "entities": [{"text": "ASR", "start_pos": 108, "end_pos": 111, "type": "TASK", "confidence": 0.9653705954551697}, {"text": "NIST's ROVER system (Fiscus, 1997)", "start_pos": 166, "end_pos": 200, "type": "DATASET", "confidence": 0.8155841694937812}, {"text": "relative error reduction (RER)", "start_pos": 212, "end_pos": 242, "type": "METRIC", "confidence": 0.9198833107948303}, {"text": "ASR", "start_pos": 285, "end_pos": 288, "type": "TASK", "confidence": 0.9897876977920532}]}, {"text": "This paper tries to correct ASR output using transformation-based learning (TBL).", "labels": [], "entities": [{"text": "ASR output", "start_pos": 28, "end_pos": 38, "type": "TASK", "confidence": 0.8776289820671082}]}, {"text": "This, too, has been attempted, although on a professional dictation corpus with a 35% initial WER ().", "labels": [], "entities": [{"text": "WER", "start_pos": 94, "end_pos": 97, "type": "METRIC", "confidence": 0.9307417869567871}]}, {"text": "They had access to a very large amount of manually transcribed data -so large, in fact, that the computation of true WER in the TBL rule selection loop was computationally infeasible, and so they used a set of faster heuristics instead.", "labels": [], "entities": [{"text": "WER", "start_pos": 117, "end_pos": 120, "type": "METRIC", "confidence": 0.918867290019989}, {"text": "TBL rule selection", "start_pos": 128, "end_pos": 146, "type": "TASK", "confidence": 0.6082869569460551}]}, {"text": "used TBL to improve the word lattices from which the transcripts are decoded, but this method also has efficiency problems (it begins with a reduction of the lattice to a confusion network), is poorly suited to word lattices that have already been heavily domain-adapted because of the language model's low perplexity, and even with higher perplexity models (the SWITCHBOARD corpus using a lan-1 This work generally measures progress by reduction in the size of training data rather than relative WER reduction.", "labels": [], "entities": [{"text": "WER reduction", "start_pos": 497, "end_pos": 510, "type": "METRIC", "confidence": 0.9757443964481354}]}, {"text": "achieved a 30% WER with 68% less training data than their baseline.", "labels": [], "entities": [{"text": "WER", "start_pos": 15, "end_pos": 18, "type": "METRIC", "confidence": 0.9992939233779907}]}, {"text": "worked on a small-vocabulary name-selection task that combined active learning with acoustic model adaptation.", "labels": [], "entities": [{"text": "acoustic model adaptation", "start_pos": 84, "end_pos": 109, "type": "TASK", "confidence": 0.6601913472016653}]}, {"text": "They reduced the WER from 15% to 3% with 70 syllables of acoustic adaptation, relative to a baseline that reduced the WER to 3% with 300 syllables of acoustic adaptation.", "labels": [], "entities": [{"text": "WER", "start_pos": 17, "end_pos": 20, "type": "METRIC", "confidence": 0.7397351861000061}, {"text": "WER", "start_pos": 118, "end_pos": 121, "type": "METRIC", "confidence": 0.7323695421218872}]}, {"text": "guage model trained over a diverse range of broadcast news and telephone conversation transcripts), was reported to produce only a 5% WER reduction.", "labels": [], "entities": [{"text": "WER reduction", "start_pos": 134, "end_pos": 147, "type": "METRIC", "confidence": 0.9825960099697113}]}, {"text": "What we show in this paper is that a true WER calculation is so valuable that a manual transcription of only about 10 minutes of a one-hour lecture is necessary to learn the TBL rules, and that this smaller amount of transcribed data in turn makes the true WER calculation computationally feasible.", "labels": [], "entities": [{"text": "WER calculation", "start_pos": 42, "end_pos": 57, "type": "TASK", "confidence": 0.8551149666309357}, {"text": "TBL", "start_pos": 174, "end_pos": 177, "type": "DATASET", "confidence": 0.4829947352409363}, {"text": "WER calculation", "start_pos": 257, "end_pos": 272, "type": "TASK", "confidence": 0.9226813912391663}]}, {"text": "With this combination, we achieve a greater average relative error reduction (12.9%) than that reported by on their dictation corpus (9.6%), and an RER over three times greater than that of our reimplementation of their heuristics on our lecture data (3.6%).", "labels": [], "entities": [{"text": "relative error reduction", "start_pos": 52, "end_pos": 76, "type": "METRIC", "confidence": 0.8730123440424601}, {"text": "RER", "start_pos": 148, "end_pos": 151, "type": "METRIC", "confidence": 0.9993380904197693}]}, {"text": "This is on top of the average 11% RER from language model adaptation on the same data.", "labels": [], "entities": [{"text": "RER", "start_pos": 34, "end_pos": 37, "type": "METRIC", "confidence": 0.9984254837036133}, {"text": "language model adaptation", "start_pos": 43, "end_pos": 68, "type": "TASK", "confidence": 0.6064000129699707}]}, {"text": "We also achieve the RER from TBL without the obligatory round of development-set parameter tuning required by their heuristics, and in a manner that is robust to perplexity.", "labels": [], "entities": [{"text": "RER", "start_pos": 20, "end_pos": 23, "type": "METRIC", "confidence": 0.9272536635398865}]}, {"text": "Section 2 briefly introduces TransformationBased Learning (TBL), a method used in various Natural Language Processing tasks to correct the output of a stochastic model, and then introduces a TBL-based solution for improving ASR transcripts for lectures.", "labels": [], "entities": [{"text": "TransformationBased Learning (TBL)", "start_pos": 29, "end_pos": 63, "type": "TASK", "confidence": 0.6687762141227722}, {"text": "ASR transcripts", "start_pos": 224, "end_pos": 239, "type": "TASK", "confidence": 0.883597195148468}]}, {"text": "Section 3 describes our experimental setup, and Section 4 analyses its results.", "labels": [], "entities": []}], "datasetContent": [{"text": "Several combinations of TBL parameters were tested with no tuning or modifications between tests.", "labels": [], "entities": [{"text": "TBL", "start_pos": 24, "end_pos": 27, "type": "METRIC", "confidence": 0.6447685360908508}]}, {"text": "As the proposed method was not refined during the experiments, and since one of the goals of our proposed approach is to eliminate the need for developmental data sets, the available data were partitioned only into training and test sets, with one additional hour set aside for code development and debugging.", "labels": [], "entities": []}, {"text": "It can be assumed that a one-hour lecture given by the same instructor will exhibit a strong cohesion, both in topic and in speaking style, between its parts.", "labels": [], "entities": []}, {"text": "Therefore, in contrast to typical TBL solutions, we have evaluated our TBL-based approach by partitioning each 50 minute lecture into a training and a test set, where the training set is smaller than the test set.", "labels": [], "entities": []}, {"text": "As mentioned in the introduction, it is feasible to obtain manual transcripts for the first 10 to 15 minutes of a lecture.", "labels": [], "entities": []}, {"text": "As such, the evaluation was carried outwith two values for the training size: the first fifth (T S = 20%) and the first third (T S = 33%) of the lecture being manually transcribed.", "labels": [], "entities": [{"text": "T S = 20%)", "start_pos": 95, "end_pos": 105, "type": "METRIC", "confidence": 0.9344077348709107}, {"text": "T S = 33%)", "start_pos": 127, "end_pos": 137, "type": "METRIC", "confidence": 0.9210385680198669}]}, {"text": "Besides the training size parameter, during all experimental tests a second parameter was also considered: the rule pruning threshold (RT ).", "labels": [], "entities": [{"text": "rule pruning threshold (RT )", "start_pos": 111, "end_pos": 139, "type": "METRIC", "confidence": 0.7894780735174814}]}, {"text": "As described in Section 2.2, of all the rules learned during the rule discovery step, only those that occur more often than the threshold are scored and ranked.", "labels": [], "entities": [{"text": "rule discovery step", "start_pos": 65, "end_pos": 84, "type": "TASK", "confidence": 0.874661366144816}]}, {"text": "This parameter can beset as low as 1 (consider all rules) or 2 (consider all rules that occur at least twice over the training set).", "labels": [], "entities": []}, {"text": "For largerscale tasks, the threshold serves as a pruning alternative to the computational burden of scoring several thousand rules.", "labels": [], "entities": []}, {"text": "A large threshold could potentially lead to discrediting low-frequency but high-scoring rules.", "labels": [], "entities": []}, {"text": "Due to the intentionally small size of our training data for lecture TBL, the lowest threshold was set to RT = 2.", "labels": [], "entities": [{"text": "RT", "start_pos": 106, "end_pos": 108, "type": "METRIC", "confidence": 0.9978501796722412}]}, {"text": "When a development set is available, several values for the RT parameter could be tested and the optimal one chosen for the evaluation task.", "labels": [], "entities": [{"text": "RT", "start_pos": 60, "end_pos": 62, "type": "TASK", "confidence": 0.7561683058738708}]}, {"text": "Since we used no development set, we tested two more values for the rule pruning threshold: RT = 5 and RT = 10.", "labels": [], "entities": [{"text": "RT", "start_pos": 92, "end_pos": 94, "type": "METRIC", "confidence": 0.9968246221542358}, {"text": "RT", "start_pos": 103, "end_pos": 105, "type": "METRIC", "confidence": 0.9954487681388855}]}, {"text": "Since our TBL solution is an extension of the solution proposed in, their heuristic is our baseline.", "labels": [], "entities": []}, {"text": "Their scoring function is the expected error reduction: a WER approximation computed overall instances of rules applicable to the training set which reflects the difference between true positives (the number of times a rule is correctly applied to errorful transcripts -GoodCnt) and false positives (the instances of correct text being unnecessarily \"corrected\" by a rule -BadCnt).", "labels": [], "entities": [{"text": "WER", "start_pos": 58, "end_pos": 61, "type": "METRIC", "confidence": 0.6573317646980286}, {"text": "GoodCnt", "start_pos": 270, "end_pos": 277, "type": "DATASET", "confidence": 0.9307494163513184}]}, {"text": "These are weighted by the length in words (ErrLen) of the text area that matches the left-hand side of the replacement.", "labels": [], "entities": [{"text": "ErrLen)", "start_pos": 43, "end_pos": 50, "type": "METRIC", "confidence": 0.9108920693397522}]}], "tableCaptions": [{"text": " Table 1: The evaluation data.", "labels": [], "entities": []}, {"text": " Table 4: Experimental evaluation: WER values for  instructor G using the ICSISWB language model.", "labels": [], "entities": [{"text": "WER", "start_pos": 35, "end_pos": 38, "type": "METRIC", "confidence": 0.9926482439041138}, {"text": "ICSISWB language model", "start_pos": 74, "end_pos": 96, "type": "DATASET", "confidence": 0.9111129442850748}]}, {"text": " Table 2: Experimental evaluation: WER values for instructor R using the WSJ-5K language model.", "labels": [], "entities": [{"text": "WER", "start_pos": 35, "end_pos": 38, "type": "METRIC", "confidence": 0.9834574460983276}, {"text": "WSJ-5K language model", "start_pos": 73, "end_pos": 94, "type": "DATASET", "confidence": 0.9100808898607889}]}, {"text": " Table 3: Experimental evaluation: WER values for instructor R using the WEB language models.", "labels": [], "entities": [{"text": "WER", "start_pos": 35, "end_pos": 38, "type": "METRIC", "confidence": 0.9841156601905823}, {"text": "WEB language models", "start_pos": 73, "end_pos": 92, "type": "DATASET", "confidence": 0.9162326653798422}]}, {"text": " Table 5: Experimental evaluation: WER values for instructor K using the WSJ-5K language model.", "labels": [], "entities": [{"text": "WER", "start_pos": 35, "end_pos": 38, "type": "METRIC", "confidence": 0.9818671941757202}, {"text": "WSJ-5K language model", "start_pos": 73, "end_pos": 94, "type": "DATASET", "confidence": 0.9030787944793701}]}]}