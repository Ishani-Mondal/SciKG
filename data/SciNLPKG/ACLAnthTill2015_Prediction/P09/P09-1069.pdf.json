{"title": [{"text": "Learning a Compositional Semantic Parser using an Existing Syntactic Parser", "labels": [], "entities": []}], "abstractContent": [{"text": "We present anew approach to learning a semantic parser (a system that maps natural language sentences into logical form).", "labels": [], "entities": []}, {"text": "Unlike previous methods, it exploits an existing syntactic parser to produce disam-biguated parse trees that drive the compo-sitional semantic interpretation.", "labels": [], "entities": []}, {"text": "The resulting system produces improved results on standard corpora on natural language interfaces for database querying and simulated robot control.", "labels": [], "entities": []}], "introductionContent": [{"text": "Semantic parsing is the task of mapping a natural language (NL) sentence into a completely formal meaning representation (MR) or logical form.", "labels": [], "entities": [{"text": "Semantic parsing", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8362451195716858}]}, {"text": "A meaning representation language (MRL) is a formal unambiguous language that supports automated inference, such as first-order predicate logic.", "labels": [], "entities": [{"text": "meaning representation language (MRL)", "start_pos": 2, "end_pos": 39, "type": "TASK", "confidence": 0.8390633463859558}, {"text": "first-order predicate logic", "start_pos": 116, "end_pos": 143, "type": "TASK", "confidence": 0.6357878545920054}]}, {"text": "This distinguishes it from related tasks such as semantic role labeling (SRL) () and other forms of \"shallow\" semantic analysis that do not produce completely formal representations.", "labels": [], "entities": [{"text": "semantic role labeling (SRL)", "start_pos": 49, "end_pos": 77, "type": "TASK", "confidence": 0.8015042146046957}]}, {"text": "A number of systems for automatically learning semantic parsers have been proposed.", "labels": [], "entities": []}, {"text": "Given a training corpus of NL sentences annotated with their correct MRs, these systems induce an interpreter for mapping novel sentences into the given MRL.", "labels": [], "entities": []}, {"text": "Previous methods for learning semantic parsers do not utilize an existing syntactic parser that provides disambiguated parse trees.", "labels": [], "entities": []}, {"text": "However, accurate syntactic parsers are available for many Ge and Mooney (2005) use training examples with semantically annotated parse trees, and learn a probabilistic semantic parsing model which initially requires a hand-built, ambiguous CCG grammar template.", "labels": [], "entities": []}, {"text": "(a) If our player 2 has the ball, then position our player 5 in the midfield.", "labels": [], "entities": []}, {"text": "((bowner (player our {2})) (do (player our {5}) (pos (midfield)))) (b) Which river is the longest?", "labels": [], "entities": []}, {"text": "answer(x1,longest(x1,river(x1))) languages and could potentially be used to learn more effective semantic analyzers.", "labels": [], "entities": []}, {"text": "This paper presents an approach to learning semantic parsers that uses parse trees from an existing syntactic analyzer to drive the interpretation process.", "labels": [], "entities": [{"text": "learning semantic parsers", "start_pos": 35, "end_pos": 60, "type": "TASK", "confidence": 0.6071847577889761}]}, {"text": "The learned parser uses standard compositional semantics to construct alternative MRs fora sentence based on its syntax tree, and then chooses the best MR based on a trained statistical disambiguation model.", "labels": [], "entities": []}, {"text": "The learning system first employs a word alignment method from statistical machine translation (GIZA++ () to acquire a semantic lexicon that maps words to logical predicates.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 36, "end_pos": 50, "type": "TASK", "confidence": 0.7543659210205078}, {"text": "statistical machine translation", "start_pos": 63, "end_pos": 94, "type": "TASK", "confidence": 0.6421358287334442}]}, {"text": "Then it induces rules for composing MRs and estimates the parameters of a maximumentropy model for disambiguating semantic interpretations.", "labels": [], "entities": []}, {"text": "After describing the details of our approach, we present experimental results on standard corpora demonstrating improved results on learning NL interfaces for database querying and simulated robot control.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated our approach on two standard corpora in CLANG and GEOQUERY.", "labels": [], "entities": [{"text": "CLANG", "start_pos": 53, "end_pos": 58, "type": "DATASET", "confidence": 0.5513548254966736}, {"text": "GEOQUERY", "start_pos": 63, "end_pos": 71, "type": "DATASET", "confidence": 0.8290084600448608}]}, {"text": "For CLANG, 300 instructions were randomly selected from the log files of the 2003 ROBOCUP Coach Competition and manually translated into English ().", "labels": [], "entities": [{"text": "ROBOCUP Coach Competition", "start_pos": 82, "end_pos": 107, "type": "DATASET", "confidence": 0.6153633495171865}]}, {"text": "For GEOQUERY, 880 English questions were gathered from various sources and manually translated into Prolog queries).", "labels": [], "entities": [{"text": "GEOQUERY", "start_pos": 4, "end_pos": 12, "type": "DATASET", "confidence": 0.7831076979637146}, {"text": "Prolog", "start_pos": 100, "end_pos": 106, "type": "DATASET", "confidence": 0.9370900392532349}]}, {"text": "The average sentence lengths for the CLANG and GEOQUERY corpora are 22.52 and 7.48, respectively.", "labels": [], "entities": [{"text": "GEOQUERY corpora", "start_pos": 47, "end_pos": 63, "type": "DATASET", "confidence": 0.8405832648277283}]}, {"text": "Our experiments used 10-fold cross validation and proceeded as follows.", "labels": [], "entities": []}, {"text": "First Bikel's implementation of Collins parsing model 2 was trained to generate syntactic parses.", "labels": [], "entities": [{"text": "Collins parsing", "start_pos": 32, "end_pos": 47, "type": "TASK", "confidence": 0.6246240139007568}]}, {"text": "Second, a semantic parser was learned from the training set augmented with their syntactic parses.", "labels": [], "entities": []}, {"text": "Finally, the learned semantic parser was used to generate the MRs for the test sentences using their syntactic parses.", "labels": [], "entities": []}, {"text": "If a test example contains constructs that did not occur in training, the parser may fail to return an MR.", "labels": [], "entities": [{"text": "MR", "start_pos": 103, "end_pos": 105, "type": "METRIC", "confidence": 0.8622571229934692}]}, {"text": "We measured the performance of semantic parsing using precision (percentage of returned MRs that were correct), recall (percentage of test examples with correct MRs returned), and F-measure (harmonic mean of precision and recall).", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 31, "end_pos": 47, "type": "TASK", "confidence": 0.8153540194034576}, {"text": "precision", "start_pos": 54, "end_pos": 63, "type": "METRIC", "confidence": 0.9992684721946716}, {"text": "recall", "start_pos": 112, "end_pos": 118, "type": "METRIC", "confidence": 0.9991748929023743}, {"text": "F-measure", "start_pos": 180, "end_pos": 189, "type": "METRIC", "confidence": 0.9984776377677917}, {"text": "precision", "start_pos": 208, "end_pos": 217, "type": "METRIC", "confidence": 0.8454909920692444}, {"text": "recall", "start_pos": 222, "end_pos": 228, "type": "METRIC", "confidence": 0.9863550066947937}]}, {"text": "For CLANG, an MR was correct if it exactly matched the correct MR, up to reordering of arguments of commutative predicates like and.", "labels": [], "entities": []}, {"text": "For GEO-QUERY, an MR was correct if it retrieved the same answer as the gold-standard query, thereby reflecting the quality of the final result returned to the user.", "labels": [], "entities": [{"text": "GEO-QUERY", "start_pos": 4, "end_pos": 13, "type": "DATASET", "confidence": 0.5712199211120605}, {"text": "MR", "start_pos": 18, "end_pos": 20, "type": "METRIC", "confidence": 0.9384289383888245}]}, {"text": "The performance of a syntactic parser trained only on the Wall Street Journal (WSJ) can degrade dramatically in new domains due to corpus variation.", "labels": [], "entities": [{"text": "Wall Street Journal (WSJ)", "start_pos": 58, "end_pos": 83, "type": "DATASET", "confidence": 0.9379361271858215}]}, {"text": "Experiments on CLANG and GEOQUERY showed that the performance can be greatly improved by adding a small number of treebanked examples from the corresponding training set together with the WSJ corpus.", "labels": [], "entities": [{"text": "GEOQUERY", "start_pos": 25, "end_pos": 33, "type": "DATASET", "confidence": 0.8698283433914185}, {"text": "WSJ corpus", "start_pos": 188, "end_pos": 198, "type": "DATASET", "confidence": 0.9498905837535858}]}, {"text": "Our semantic parser was evaluated using three kinds of syntactic parses.", "labels": [], "entities": []}, {"text": "Listed together with their PARSEVAL F-measures these are: gold-standard parses from the treebank (GoldSyn, 100%), a parser trained on WSJ plus a small number of in-domain training sentences required to achieve good performance, 20 for CLANG (Syn20, 88.21%) and 40 for GEOQUERY (Syn40, 91.46%), and a parser trained on no in-domain data (Syn0, 82.15% for CLANG and 76.44% for GEOQUERY).", "labels": [], "entities": [{"text": "PARSEVAL F-measures", "start_pos": 27, "end_pos": 46, "type": "METRIC", "confidence": 0.5573488473892212}, {"text": "GoldSyn", "start_pos": 98, "end_pos": 105, "type": "DATASET", "confidence": 0.844297468662262}]}, {"text": "We compared our approach to the following alternatives (where results for the given corpus were    ), an SVM-based parser using string kernels; WASP (, a system based on synchronous grammars; Z&C (Zettlemoyer and Collins, 2007) 3 , a probabilistic parser based on relaxed CCG grammars; and LU (), a generative model with discriminative reranking.", "labels": [], "entities": []}, {"text": "Note that some of these approaches require additional human supervision, knowledge, or engineered features that are unavailable to the other systems; namely, SCISSOR requires gold-standard SAPTs, Z&C requires hand-built template grammar rules, LU requires a reranking model using specially designed global features, and our approach requires an existing syntactic parser.", "labels": [], "entities": []}, {"text": "The F-measures for syntactic parses that generate correct MRs in CLANG are 85.50% for syn0 and 91.16% for syn20, showing that our method can produce correct MRs even when given imperfect syntactic parses.", "labels": [], "entities": [{"text": "F-measures", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9976353645324707}, {"text": "syntactic parses", "start_pos": 19, "end_pos": 35, "type": "TASK", "confidence": 0.7249729931354523}, {"text": "CLANG", "start_pos": 65, "end_pos": 70, "type": "DATASET", "confidence": 0.8237418532371521}]}, {"text": "The results of semantic parsers are shown in.", "labels": [], "entities": []}, {"text": "First, not surprisingly, more accurate syntactic parsers (i.e. ones trained on more in-domain data) improved our approach.", "labels": [], "entities": []}, {"text": "Second, in CLANG, all of our methods outperform WASP and KRISP, which also require no additional information during training.", "labels": [], "entities": [{"text": "WASP", "start_pos": 48, "end_pos": 52, "type": "TASK", "confidence": 0.57579106092453}, {"text": "KRISP", "start_pos": 57, "end_pos": 62, "type": "METRIC", "confidence": 0.8826441764831543}]}, {"text": "In GEOQUERY, Syn0 has significantly worse results than WASP and our other systems using better syntactic parses.", "labels": [], "entities": [{"text": "GEOQUERY", "start_pos": 3, "end_pos": 11, "type": "DATASET", "confidence": 0.8694235682487488}]}, {"text": "This is not surprising since Syn0's F-measure for syntactic parsing is only 76.44% in GEOQUERY due to alack.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 36, "end_pos": 45, "type": "METRIC", "confidence": 0.9976221919059753}, {"text": "syntactic parsing", "start_pos": 50, "end_pos": 67, "type": "TASK", "confidence": 0.7481083869934082}, {"text": "GEOQUERY", "start_pos": 86, "end_pos": 94, "type": "DATASET", "confidence": 0.8716520667076111}]}, {"text": "Third, SCISSOR performs better than our methods on CLANG, but it requires extra human supervision that is not available to the other systems.", "labels": [], "entities": []}, {"text": "Lastly, a detailed analysis showed that our improved performance on CLANG compared to WASP and KRISP is mainly for long sentences (> 20 words), while performance on shorter sentences is similar.", "labels": [], "entities": []}, {"text": "This is consistent with their relative performance on GEOQUERY, where sentences are normally short.", "labels": [], "entities": [{"text": "GEOQUERY", "start_pos": 54, "end_pos": 62, "type": "DATASET", "confidence": 0.8925298452377319}]}, {"text": "Longer sentences typically have more complex syntax, and the traditional syntactic analysis used by our approach results in better compositional semantic analysis in this situation.", "labels": [], "entities": [{"text": "compositional semantic analysis", "start_pos": 131, "end_pos": 162, "type": "TASK", "confidence": 0.682125190893809}]}, {"text": "We also ran experiments with less training data.", "labels": [], "entities": []}, {"text": "For CLANG, 40 random examples from the training sets (CLANG40) were used.", "labels": [], "entities": [{"text": "CLANG", "start_pos": 4, "end_pos": 9, "type": "DATASET", "confidence": 0.6695795059204102}, {"text": "CLANG40", "start_pos": 54, "end_pos": 61, "type": "DATASET", "confidence": 0.8670283555984497}]}, {"text": "For GEOQUERY, an existing 250-example subset (GEO250)) was used.", "labels": [], "entities": [{"text": "GEOQUERY", "start_pos": 4, "end_pos": 12, "type": "DATASET", "confidence": 0.9332205057144165}, {"text": "GEO250", "start_pos": 46, "end_pos": 52, "type": "DATASET", "confidence": 0.9416045546531677}]}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "Note the performance of our systems on GEO250 is higher than that on GEOQUERY since GEOQUERY includes more complex queries).", "labels": [], "entities": [{"text": "GEO250", "start_pos": 39, "end_pos": 45, "type": "DATASET", "confidence": 0.9800679087638855}, {"text": "GEOQUERY", "start_pos": 69, "end_pos": 77, "type": "DATASET", "confidence": 0.9706887602806091}, {"text": "GEOQUERY", "start_pos": 84, "end_pos": 92, "type": "DATASET", "confidence": 0.9453454613685608}]}, {"text": "First, all of our systems gave the best F-measures (except SYN0 compared to SCISSOR in CLANG40), and the differences are generally quite substantial.", "labels": [], "entities": [{"text": "F-measures", "start_pos": 40, "end_pos": 50, "type": "METRIC", "confidence": 0.9954877495765686}, {"text": "SYN0", "start_pos": 59, "end_pos": 63, "type": "METRIC", "confidence": 0.9459664225578308}]}, {"text": "This shows that our approach significantly improves results when limited training data is available.", "labels": [], "entities": []}, {"text": "Second, in CLANG, reducing the training data increased the difference between SYN20 and SYN0.", "labels": [], "entities": [{"text": "SYN20", "start_pos": 78, "end_pos": 83, "type": "DATASET", "confidence": 0.7873690724372864}]}, {"text": "This suggests that the quality of syntactic parsing becomes more important when less training data is available.", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 34, "end_pos": 51, "type": "TASK", "confidence": 0.7775847911834717}]}, {"text": "This demonstrates the advantage of utilizing existing syntactic parsers that are learned from large open domain treebanks instead of relying just on the training data.", "labels": [], "entities": []}, {"text": "We also evaluated the impact of the word alignment component by replacing Giza++ by goldstandard word alignments manually annotated for the CLANG corpus.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 36, "end_pos": 50, "type": "TASK", "confidence": 0.737482339143753}, {"text": "CLANG corpus", "start_pos": 140, "end_pos": 152, "type": "DATASET", "confidence": 0.9363330900669098}]}, {"text": "The results consistently showed that compared to using gold-standard word alignment, Giza++ produced lower semantic parsing accuracy when given very little training data, but similar or better results when given sufficient training data.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 69, "end_pos": 83, "type": "TASK", "confidence": 0.7149201184511185}, {"text": "semantic parsing", "start_pos": 107, "end_pos": 123, "type": "TASK", "confidence": 0.7142743617296219}, {"text": "accuracy", "start_pos": 124, "end_pos": 132, "type": "METRIC", "confidence": 0.8882815837860107}]}, {"text": "This suggests that, given sufficient data, Giza++ can produce effective word alignments, and that imperfect word alignments do not seriously impair our semantic parsers since the disambiguation model evaluates multiple possible interpretations of ambiguous words.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 72, "end_pos": 87, "type": "TASK", "confidence": 0.7076139897108078}]}, {"text": "Using multiple potential alignments from Giza++ sometimes performs even better than using a single gold-standard word alignment because it allows multiple interpretations to be evaluated by the global disambiguation model.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Performance on CLANG.", "labels": [], "entities": []}, {"text": " Table 3: Performance on GEOQUERY.", "labels": [], "entities": [{"text": "GEOQUERY", "start_pos": 25, "end_pos": 33, "type": "DATASET", "confidence": 0.8935093283653259}]}, {"text": " Table 4: Performance on CLANG40.", "labels": [], "entities": [{"text": "CLANG40", "start_pos": 25, "end_pos": 32, "type": "DATASET", "confidence": 0.9018142223358154}]}, {"text": " Table 5: Performance on GEO250 (20 in-domain  sentences are used in SYN20 to train the syntactic  parser).", "labels": [], "entities": [{"text": "GEO250", "start_pos": 25, "end_pos": 31, "type": "DATASET", "confidence": 0.9533138275146484}]}]}