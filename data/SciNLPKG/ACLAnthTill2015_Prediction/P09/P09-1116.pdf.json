{"title": [], "abstractContent": [{"text": "We present a simple and scalable algorithm for clustering tens of millions of phrases and use the resulting clusters as features in discriminative classifiers.", "labels": [], "entities": []}, {"text": "To demonstrate the power and generality of this approach, we apply the method in two very different applications: named entity recognition and query classification.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 114, "end_pos": 138, "type": "TASK", "confidence": 0.711409737666448}, {"text": "query classification", "start_pos": 143, "end_pos": 163, "type": "TASK", "confidence": 0.7943989038467407}]}, {"text": "Our results show that phrase clusters offer significant improvements over word clusters.", "labels": [], "entities": []}, {"text": "Our NER system achieves the best current result on the widely used CoNLL benchmark.", "labels": [], "entities": [{"text": "NER", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.8383592963218689}, {"text": "CoNLL benchmark", "start_pos": 67, "end_pos": 82, "type": "DATASET", "confidence": 0.9489368498325348}]}, {"text": "Our query classifier is on par with the best system in KDDCUP 2005 without resorting to labor intensive knowledge engineering efforts.", "labels": [], "entities": [{"text": "KDDCUP 2005", "start_pos": 55, "end_pos": 66, "type": "DATASET", "confidence": 0.9183679521083832}]}], "introductionContent": [{"text": "Over the past decade, supervised learning algorithms have gained widespread acceptance in natural language processing (NLP).", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 90, "end_pos": 123, "type": "TASK", "confidence": 0.7664045890172323}]}, {"text": "They have become the workhorse in almost all sub-areas and components of NLP, including part-ofspeech tagging, chunking, named entity recognition and parsing.", "labels": [], "entities": [{"text": "part-ofspeech tagging", "start_pos": 88, "end_pos": 109, "type": "TASK", "confidence": 0.6633485108613968}, {"text": "named entity recognition", "start_pos": 121, "end_pos": 145, "type": "TASK", "confidence": 0.6320246954758962}]}, {"text": "To apply supervised learning to an NLP problem, one first represents the problem as a vector of features.", "labels": [], "entities": []}, {"text": "The learning algorithm then optimizes a regularized, convex objective function that is expressed in terms of these features.", "labels": [], "entities": []}, {"text": "The performance of such learning-based solutions thus crucially depends on the informativeness of the features.", "labels": [], "entities": []}, {"text": "The majority of the features in these supervised classifiers are predicated on lexical information, such as word identities.", "labels": [], "entities": []}, {"text": "The long-tailed distribution of natural language words implies that most of the word types will be either unseen or seen very few times in the labeled training data, even if the data set is a relatively large one (e.g., the Penn Treebank).", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 224, "end_pos": 237, "type": "DATASET", "confidence": 0.9914627075195312}]}, {"text": "While the labeled data is generally very costly to obtain, there is avast amount of unlabeled textual data freely available on the web.", "labels": [], "entities": []}, {"text": "One way to alleviate the sparsity problem is to adopt a two-stage strategy: first create word clusters with unlabeled data and then use the clusters as features in supervised training.", "labels": [], "entities": []}, {"text": "Under this approach, even if a word is not found in the training data, it may still fire cluster-based features as long as it shares cluster assignments with some words in the labeled data.", "labels": [], "entities": []}, {"text": "Since the clusters are obtained without any labeled data, they may not correspond directly to concepts that are useful for decision making in the problem domain.", "labels": [], "entities": []}, {"text": "However, the supervised learning algorithms can typically identify useful clusters and assign proper weights to them, effectively adapting the clusters to the domain.", "labels": [], "entities": []}, {"text": "This method has been shown to be quite successful in named entity recognition) and dependency parsing (.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 53, "end_pos": 77, "type": "TASK", "confidence": 0.7058586279551188}, {"text": "dependency parsing", "start_pos": 83, "end_pos": 101, "type": "TASK", "confidence": 0.8808810114860535}]}, {"text": "In this paper, we present a semi-supervised learning algorithm that goes a step further.", "labels": [], "entities": []}, {"text": "In addition to word-clusters, we also use phraseclusters as features.", "labels": [], "entities": []}, {"text": "Out of context, natural language words are often ambiguous.", "labels": [], "entities": []}, {"text": "Phrases are much less so because the words in a phrase provide contexts for one another.", "labels": [], "entities": [{"text": "Phrases", "start_pos": 0, "end_pos": 7, "type": "TASK", "confidence": 0.9637354612350464}]}, {"text": "Consider the phrase \"Land of Odds\".", "labels": [], "entities": []}, {"text": "One would never have guessed that it is a company name based on the clusters containing Odds and Land.", "labels": [], "entities": [{"text": "Odds and Land", "start_pos": 88, "end_pos": 101, "type": "DATASET", "confidence": 0.7577782471974691}]}, {"text": "With phrase-based clustering, \"Land of Odds\" is grouped with many names that are labeled as company names, which is a strong indication that it is a company name as well.", "labels": [], "entities": []}, {"text": "The disambiguation power of phrases is also evidenced by the improvements of phrase-based machine translation systems) over word-based ones.", "labels": [], "entities": [{"text": "phrase-based machine translation", "start_pos": 77, "end_pos": 109, "type": "TASK", "confidence": 0.6222446660200754}]}, {"text": "Previous approaches, e.g.,) and (, have all used the Brown algorithm for clustering).", "labels": [], "entities": []}, {"text": "The main idea of the algorithm is to minimize the bigram language-model perplexity of a text corpus.", "labels": [], "entities": []}, {"text": "The algorithm is quadratic in the number of elements to be clustered.", "labels": [], "entities": []}, {"text": "It is able to cluster tens of thousands of words, but is not scalable enough to deal with tens of millions of phrases.", "labels": [], "entities": []}, {"text": "proposed a distributed clustering algorithm with a similar objective function as the Brown algorithm.", "labels": [], "entities": []}, {"text": "It substantially increases the number of elements that can be clustered.", "labels": [], "entities": []}, {"text": "However, since it still needs to load the current clustering of all elements into each of the workers in the distributed system, the memory requirement becomes a bottleneck.", "labels": [], "entities": []}, {"text": "We present a distributed version of a much simpler K-Means clustering that allows us to cluster tens of millions of elements.", "labels": [], "entities": [{"text": "K-Means clustering", "start_pos": 51, "end_pos": 69, "type": "TASK", "confidence": 0.6174156814813614}]}, {"text": "We demonstrate the advantages of phrase-based clusters over word-based ones with experimental results from two distinct application domains: named entity recognition and query classification.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 141, "end_pos": 165, "type": "TASK", "confidence": 0.6112478772799174}, {"text": "query classification", "start_pos": 170, "end_pos": 190, "type": "TASK", "confidence": 0.7276313006877899}]}, {"text": "Our named entity recognition system achieves an F1-score of 90.90 on the CoNLL 2003 English data set, which is about 1 point higher than the previous best result.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 4, "end_pos": 28, "type": "TASK", "confidence": 0.6268627643585205}, {"text": "F1-score", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.9996600151062012}, {"text": "CoNLL 2003 English data set", "start_pos": 73, "end_pos": 100, "type": "DATASET", "confidence": 0.9713940858840943}]}, {"text": "Our query classifier reaches the same level of performance as the KDDCUP 2005 winning systems, which were built with a great deal of knowledge engineering.", "labels": [], "entities": [{"text": "KDDCUP 2005 winning", "start_pos": 66, "end_pos": 85, "type": "DATASET", "confidence": 0.8596557180086771}]}], "datasetContent": [{"text": "The performance of our baseline system is rather mediocre because it has far fewer feature functions than the more competitive systems.", "labels": [], "entities": []}, {"text": "The Top CoNLL 2003 systems all employed gazetteers or other types of specialized resources (e.g., lists of words that tend to co-occur with certain named entity types) in addition to part-ofspeech tags.", "labels": [], "entities": [{"text": "Top CoNLL 2003", "start_pos": 4, "end_pos": 18, "type": "DATASET", "confidence": 0.681657483180364}]}, {"text": "Introducing the word clusters immediately brings the performance up to a very competitive level.", "labels": [], "entities": []}, {"text": "Phrasal clusters obtained from the LDC corpus give the same level of improvement as word clusters from the web corpus that is 20 times larger.", "labels": [], "entities": [{"text": "LDC corpus", "start_pos": 35, "end_pos": 45, "type": "DATASET", "confidence": 0.8792678117752075}]}, {"text": "The best F-score of 90.90, which is about 1 point higher than the previous best result, is obtained with a combination of clusters.", "labels": [], "entities": [{"text": "F-score", "start_pos": 9, "end_pos": 16, "type": "METRIC", "confidence": 0.9986068606376648}]}, {"text": "Adding POS tags to this configuration caused a small drop in F1.", "labels": [], "entities": [{"text": "F1", "start_pos": 61, "end_pos": 63, "type": "METRIC", "confidence": 0.999432384967804}]}], "tableCaptions": [{"text": " Table 4 CoNLL NER test set results", "labels": [], "entities": [{"text": "CoNLL NER test set", "start_pos": 9, "end_pos": 27, "type": "DATASET", "confidence": 0.8530444800853729}]}]}