{"title": [{"text": "A Comparative Study of Hypothesis Alignment and its Improvement for Machine Translation System Combination", "labels": [], "entities": [{"text": "Hypothesis Alignment", "start_pos": 23, "end_pos": 43, "type": "TASK", "confidence": 0.7675306499004364}, {"text": "Machine Translation System Combination", "start_pos": 68, "end_pos": 106, "type": "TASK", "confidence": 0.8401041924953461}]}], "abstractContent": [{"text": "Recently confusion network decoding shows the best performance in combining outputs from multiple machine translation (MT) systems.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 98, "end_pos": 122, "type": "TASK", "confidence": 0.8150663495063781}]}, {"text": "However, overcoming different word orders presented in multiple MT systems during hypothesis alignment still remains the biggest challenge to confusion network-based MT system combination.", "labels": [], "entities": [{"text": "MT", "start_pos": 64, "end_pos": 66, "type": "TASK", "confidence": 0.9632033109664917}, {"text": "hypothesis alignment", "start_pos": 82, "end_pos": 102, "type": "TASK", "confidence": 0.708025798201561}]}, {"text": "In this paper, we compare four commonly used word alignment methods, namely GIZA++, TER, CLA and IHMM, for hypothesis alignment.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 45, "end_pos": 59, "type": "TASK", "confidence": 0.7609026432037354}, {"text": "TER", "start_pos": 84, "end_pos": 87, "type": "METRIC", "confidence": 0.9916419982910156}, {"text": "IHMM", "start_pos": 97, "end_pos": 101, "type": "METRIC", "confidence": 0.8195032477378845}, {"text": "hypothesis alignment", "start_pos": 107, "end_pos": 127, "type": "TASK", "confidence": 0.8127045929431915}]}, {"text": "Then we propose a method to build the confusion network from intersection word alignment, which utilizes both direct and inverse word alignment between the backbone and hypothesis to improve the reliability of hypothesis alignment.", "labels": [], "entities": [{"text": "intersection word alignment", "start_pos": 61, "end_pos": 88, "type": "TASK", "confidence": 0.6429918309052786}]}, {"text": "Experimental results demonstrate that the intersection word alignment yields consistent performance improvement for all four word alignment methods on both Chi-nese-to-English spoken and written language tasks.", "labels": [], "entities": [{"text": "intersection word alignment", "start_pos": 42, "end_pos": 69, "type": "TASK", "confidence": 0.659950852394104}, {"text": "word alignment", "start_pos": 125, "end_pos": 139, "type": "TASK", "confidence": 0.7652472853660583}]}], "introductionContent": [{"text": "Machine translation (MT) system combination technique leverages on multiple MT systems to achieve better performance by combining their outputs.", "labels": [], "entities": [{"text": "Machine translation (MT) system combination", "start_pos": 0, "end_pos": 43, "type": "TASK", "confidence": 0.8831147381237575}]}, {"text": "Confusion network based system combination for machine translation has shown promising advantage compared with other techniques based system combination, such as sentence level hypothesis selection by voting and source sentence re-decoding using the phrases or translation models that are learned from the source sentences and target hypotheses pairs (.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 47, "end_pos": 66, "type": "TASK", "confidence": 0.7842793464660645}, {"text": "sentence level hypothesis selection", "start_pos": 162, "end_pos": 197, "type": "TASK", "confidence": 0.6054782792925835}]}, {"text": "In general, the confusion network based system combination method for MT consists of four steps: 1) Backbone selection: to select a backbone (also called \"skeleton\") from all hypotheses.", "labels": [], "entities": [{"text": "MT", "start_pos": 70, "end_pos": 72, "type": "TASK", "confidence": 0.993069589138031}]}, {"text": "The backbone defines the word orders of the final translation.", "labels": [], "entities": []}, {"text": "2) Hypothesis alignment: to build word-alignment between backbone and each hypothesis.", "labels": [], "entities": [{"text": "Hypothesis alignment", "start_pos": 3, "end_pos": 23, "type": "TASK", "confidence": 0.78269162774086}]}, {"text": "3) Confusion network construction: to build a confusion network based on hypothesis alignments.", "labels": [], "entities": [{"text": "Confusion network construction", "start_pos": 3, "end_pos": 33, "type": "TASK", "confidence": 0.834954559803009}]}, {"text": "4) Confusion network decoding: to decode the best translation from a confusion network.", "labels": [], "entities": [{"text": "Confusion network decoding", "start_pos": 3, "end_pos": 29, "type": "TASK", "confidence": 0.7749858697255453}]}, {"text": "Among the four steps, the hypothesis alignment presents the biggest challenge to the method due to the varying word orders between outputs from different MT systems ().", "labels": [], "entities": [{"text": "MT", "start_pos": 154, "end_pos": 156, "type": "TASK", "confidence": 0.9552575945854187}]}, {"text": "Many techniques have been studied to address this issue.", "labels": [], "entities": []}, {"text": "used the edit distance alignment algorithm which is extended to multiple strings to build confusion network, it only allows monotonic alignment.", "labels": [], "entities": [{"text": "edit distance alignment", "start_pos": 9, "end_pos": 32, "type": "TASK", "confidence": 0.6296210289001465}]}, {"text": "Jayaraman and proposed a heuristic-based matching algorithm which allows nonmonotonic alignments to align the words between the hypotheses.", "labels": [], "entities": []}, {"text": "More recently, used GIZA++ to produce word alignment for hypotheses pairs.,, and used minimum Translation Error Rate (TER)) alignment to build the confusion network.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 38, "end_pos": 52, "type": "TASK", "confidence": 0.7208307385444641}, {"text": "minimum Translation Error Rate (TER)) alignment", "start_pos": 86, "end_pos": 133, "type": "METRIC", "confidence": 0.8778942376375198}]}, {"text": "extended TER algorithm which allows a confusion network as the reference to compute word alignment.", "labels": [], "entities": [{"text": "TER", "start_pos": 9, "end_pos": 12, "type": "METRIC", "confidence": 0.9656544327735901}, {"text": "word alignment", "start_pos": 84, "end_pos": 98, "type": "TASK", "confidence": 0.7463330924510956}]}, {"text": "used ITG-based method for hypothesis alignment.", "labels": [], "entities": [{"text": "hypothesis alignment", "start_pos": 26, "end_pos": 46, "type": "TASK", "confidence": 0.8949401378631592}]}, {"text": "used Competitive Linking Algorithm (CLA)) to align the words to construct confusion network.", "labels": [], "entities": []}, {"text": "proposed to improve alignment of hypotheses using synonyms as found in WordNet) and a two-pass alignment strategy based on TER word alignment approach.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 71, "end_pos": 78, "type": "DATASET", "confidence": 0.9612607359886169}, {"text": "TER word alignment", "start_pos": 123, "end_pos": 141, "type": "TASK", "confidence": 0.6435659726460775}]}, {"text": "proposed an IHMM-based word alignment method which the parameters are estimated indirectly from a variety of sources.", "labels": [], "entities": [{"text": "IHMM-based word alignment", "start_pos": 12, "end_pos": 37, "type": "TASK", "confidence": 0.6041042606035868}]}, {"text": "Although many methods have been attempted, no systematic comparison among them has been reported.", "labels": [], "entities": []}, {"text": "A through and fair comparison among them would be of great meaning to the MT sys-tem combination research.", "labels": [], "entities": [{"text": "MT sys-tem combination", "start_pos": 74, "end_pos": 96, "type": "TASK", "confidence": 0.8550217350323995}]}, {"text": "In this paper, we implement a confusion network-based decoder.", "labels": [], "entities": []}, {"text": "Based on this decoder, we compare four commonly used word alignment methods (GIZA++, TER, CLA and IHMM) for hypothesis alignment using the same experimental data and the same multiple MT system outputs with similar features in terms of translation performance.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 53, "end_pos": 67, "type": "TASK", "confidence": 0.7625128924846649}, {"text": "TER", "start_pos": 85, "end_pos": 88, "type": "METRIC", "confidence": 0.9894413948059082}, {"text": "IHMM", "start_pos": 98, "end_pos": 102, "type": "METRIC", "confidence": 0.6965218186378479}, {"text": "hypothesis alignment", "start_pos": 108, "end_pos": 128, "type": "TASK", "confidence": 0.7506916224956512}]}, {"text": "We conduct the comparison study and other experiments in this paper on both spoken and newswire domains: Chinese-to-English spoken and written language translation tasks.", "labels": [], "entities": [{"text": "Chinese-to-English spoken and written language translation tasks", "start_pos": 105, "end_pos": 169, "type": "TASK", "confidence": 0.6557442716189793}]}, {"text": "Our comparison shows that although the performance differences between the four methods are not significant, IHMM consistently show slightly better performance than other methods.", "labels": [], "entities": []}, {"text": "This is mainly due to the fact the IHMM is able to explore more knowledge sources and Viterbi decoding used in IHMM allows more thorough search for the best alignment while other methods has to useless optimal greedy search.", "labels": [], "entities": [{"text": "IHMM", "start_pos": 35, "end_pos": 39, "type": "DATASET", "confidence": 0.9477171301841736}, {"text": "IHMM", "start_pos": 111, "end_pos": 115, "type": "DATASET", "confidence": 0.8984839916229248}]}, {"text": "In addition, for better performance, instead of only using one direction word alignment (n-to-1 from hypothesis to backbone) as in previous work, we propose to use more reliable word alignments which are derived from the intersection of two-direction hypothesis alignment to construct confusion network.", "labels": [], "entities": []}, {"text": "Experimental results show that the intersection word alignmentbased method consistently improves the performance for all four methods on both spoken and written language tasks.", "labels": [], "entities": [{"text": "intersection word alignmentbased", "start_pos": 35, "end_pos": 67, "type": "TASK", "confidence": 0.7807577053705851}]}, {"text": "This paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 presents a standard framework of confusion network based machine translation system combination.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 67, "end_pos": 86, "type": "TASK", "confidence": 0.6761172413825989}]}, {"text": "Section 3 introduces four word alignment methods, and the algorithm of computing intersection word alignment for all four word alignment methods.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 26, "end_pos": 40, "type": "TASK", "confidence": 0.7152314931154251}, {"text": "intersection word alignment", "start_pos": 81, "end_pos": 108, "type": "TASK", "confidence": 0.6277223825454712}]}, {"text": "Section 4 describes the experiments setting and results on two translation tasks.", "labels": [], "entities": [{"text": "translation tasks", "start_pos": 63, "end_pos": 80, "type": "TASK", "confidence": 0.90479776263237}]}, {"text": "Section 5 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "For each system, we used the top 10 scored hypotheses to build the confusion network.", "labels": [], "entities": []}, {"text": "Similar to (), each word in the hypothesis is assigned with a rank-based score of 1/ (1 ) r + , where r is the rank of the hypothesis.", "labels": [], "entities": []}, {"text": "And we assign the same weights to each system.", "labels": [], "entities": []}, {"text": "For selecting the backbone, only the top hypothesis from each system is considered as a candidate for the backbone.", "labels": [], "entities": []}, {"text": "Concerning the four alignment methods, we use the default setting for GIZA++; and use toolkit TERCOM) to compute the TER-based word alignment, and also use the default setting.", "labels": [], "entities": [{"text": "TERCOM", "start_pos": 94, "end_pos": 100, "type": "METRIC", "confidence": 0.9311280250549316}, {"text": "TER-based word alignment", "start_pos": 117, "end_pos": 141, "type": "TASK", "confidence": 0.5965760896603266}]}, {"text": "For fair comparison reason, we 4 LDC2003E14 5 http://www.nist.gov/speech/tests/mt/ decide to do not use any additional resource, such as target language synonym list, IBM model lexicon; therefore, only surface similarity is applied in IHMM-based and CLA-based methods.", "labels": [], "entities": []}, {"text": "We compute the distortion model by following) for IHMM and CLA-based methods.", "labels": [], "entities": [{"text": "IHMM", "start_pos": 50, "end_pos": 54, "type": "DATASET", "confidence": 0.7932071089744568}]}, {"text": "The weights for each model are optimized on held-out data.: Results (BLEU% score) of single systems involved to system combination.", "labels": [], "entities": [{"text": "BLEU% score)", "start_pos": 69, "end_pos": 81, "type": "METRIC", "confidence": 0.9731153398752213}]}, {"text": "Our evaluation metric is BLEU (), which are to perform case-insensitive matching of n-grams up ton = 4.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 25, "end_pos": 29, "type": "METRIC", "confidence": 0.9993366599082947}]}, {"text": "Performance comparison of four methods: the results based on direct word alignments are reported in, row Best is the best single systems' scores; row MBR is the scores of backbone; GIZA++, TER, CLA, IHMM stand for scores of systems for four word alignment methods.", "labels": [], "entities": [{"text": "MBR", "start_pos": 150, "end_pos": 153, "type": "METRIC", "confidence": 0.9502397179603577}, {"text": "GIZA", "start_pos": 181, "end_pos": 185, "type": "METRIC", "confidence": 0.9209082722663879}, {"text": "TER", "start_pos": 189, "end_pos": 192, "type": "METRIC", "confidence": 0.9580996632575989}, {"text": "IHMM", "start_pos": 199, "end_pos": 203, "type": "METRIC", "confidence": 0.8768340349197388}]}, {"text": "\ud97b\udf59 MBR decoding slightly improves the performance over the best single system for both tasks.", "labels": [], "entities": [{"text": "\ud97b\udf59", "start_pos": 0, "end_pos": 1, "type": "DATASET", "confidence": 0.7426225543022156}, {"text": "MBR", "start_pos": 2, "end_pos": 5, "type": "DATASET", "confidence": 0.5856236815452576}]}, {"text": "This suggests that the simple voting strategy to select backbone is workable.", "labels": [], "entities": []}, {"text": "\ud97b\udf59 For both tasks, all methods improve the performance over the backbone.", "labels": [], "entities": []}, {"text": "For IWSLT test set, the improvements are from 2.06 (CLA, 30.88-28.82) to 2.52 BLEU-score).", "labels": [], "entities": [{"text": "IWSLT test set", "start_pos": 4, "end_pos": 18, "type": "DATASET", "confidence": 0.8151731689771017}, {"text": "CLA", "start_pos": 52, "end_pos": 55, "type": "METRIC", "confidence": 0.9553179740905762}, {"text": "BLEU-score", "start_pos": 78, "end_pos": 88, "type": "METRIC", "confidence": 0.9954684972763062}]}, {"text": "For NIST test set, the improvements are from 0.63 (TER, 24.31-23.68) to 1.40 BLEUscore (IHMM, 25.08-23.68).", "labels": [], "entities": [{"text": "NIST test set", "start_pos": 4, "end_pos": 17, "type": "DATASET", "confidence": 0.9227406779925028}, {"text": "TER", "start_pos": 51, "end_pos": 54, "type": "METRIC", "confidence": 0.9974151849746704}, {"text": "BLEUscore", "start_pos": 77, "end_pos": 86, "type": "METRIC", "confidence": 0.989176332950592}, {"text": "IHMM", "start_pos": 88, "end_pos": 92, "type": "METRIC", "confidence": 0.5103251338005066}]}, {"text": "This verifies that the confusion network decoding is effective in combining outputs from multiple MT systems and the four word-alignment methods are also workable for hypothesis-to-backbone alignment.", "labels": [], "entities": [{"text": "hypothesis-to-backbone alignment", "start_pos": 167, "end_pos": 199, "type": "TASK", "confidence": 0.679244339466095}]}, {"text": "\ud97b\udf59 For IWSLT task where source sentences are shorter (12-13 words per sentence in average), the four word alignment methods achieve similar performance on both dev and test set.", "labels": [], "entities": [{"text": "IWSLT task", "start_pos": 6, "end_pos": 16, "type": "TASK", "confidence": 0.5452408492565155}]}, {"text": "The biggest difference is only 0.46 BLEU score (30.88 for CLA, vs. 31.34 for IHMM).", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 36, "end_pos": 46, "type": "METRIC", "confidence": 0.9458455145359039}, {"text": "CLA", "start_pos": 58, "end_pos": 61, "type": "DATASET", "confidence": 0.8267150521278381}, {"text": "IHMM", "start_pos": 77, "end_pos": 81, "type": "DATASET", "confidence": 0.7238798141479492}]}, {"text": "For NIST task where source sentences are longer (26-28 words per sentence in average), the difference is more significant.", "labels": [], "entities": []}, {"text": "Here IHMM method achieves the best performance, followed by GIZA++, CLA and TER.", "labels": [], "entities": [{"text": "TER", "start_pos": 76, "end_pos": 79, "type": "METRIC", "confidence": 0.9915964007377625}]}, {"text": "IHMM is significantly better than TER by 0.77 BLEU-score (from 24.31 to 25.08, p<0.05).", "labels": [], "entities": [{"text": "IHMM", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.6484405994415283}, {"text": "TER", "start_pos": 34, "end_pos": 37, "type": "METRIC", "confidence": 0.9961897730827332}, {"text": "BLEU-score", "start_pos": 46, "end_pos": 56, "type": "METRIC", "confidence": 0.9989784955978394}]}, {"text": "This is mainly because IHMM exploits more knowledge source and Viterbi decoding allows more thorough search for the best alignment while other methods useless optimal greedy search.", "labels": [], "entities": [{"text": "IHMM", "start_pos": 23, "end_pos": 27, "type": "DATASET", "confidence": 0.8370775580406189}]}, {"text": "Another reason is that TER uses hard matching in computing edit distance.", "labels": [], "entities": []}, {"text": "Performance improvement by intersection word alignment: reports the performance of the system combinations based on intersection word alignments.", "labels": [], "entities": [{"text": "intersection word alignment", "start_pos": 27, "end_pos": 54, "type": "TASK", "confidence": 0.71326212088267}]}, {"text": "It shows that:, we can see that the intersection word alignment-based expansion method improves the performance in all the dev and test sets for both tasks by 0.2-0.57 BLEUscore and the improvements are consistent under all conditions.", "labels": [], "entities": [{"text": "intersection word alignment-based expansion", "start_pos": 36, "end_pos": 79, "type": "TASK", "confidence": 0.6704205870628357}, {"text": "BLEUscore", "start_pos": 168, "end_pos": 177, "type": "METRIC", "confidence": 0.9995182752609253}]}, {"text": "This suggests that the intersection word alignment-based expansion method is more effective than the commonly used direct wordalignment-based hypothesis alignment method in confusion network-based MT system combination.", "labels": [], "entities": [{"text": "intersection word alignment-based expansion", "start_pos": 23, "end_pos": 66, "type": "TASK", "confidence": 0.7349215224385262}]}, {"text": "This is because intersection word alignments are more reliable compared with direct word alignments, and so for heuristic-based expansion which is based on the aligned words with higher scores.", "labels": [], "entities": [{"text": "intersection word alignments", "start_pos": 16, "end_pos": 44, "type": "TASK", "confidence": 0.6037306487560272}]}, {"text": "\ud97b\udf59 TER-based method achieves the biggest performance improvement by 0.4 BLEU-score in IWSLT and 0.57 in NIST.", "labels": [], "entities": [{"text": "\ud97b\udf59", "start_pos": 0, "end_pos": 1, "type": "METRIC", "confidence": 0.8596482276916504}, {"text": "TER-based", "start_pos": 2, "end_pos": 11, "type": "METRIC", "confidence": 0.843849241733551}, {"text": "BLEU-score", "start_pos": 71, "end_pos": 81, "type": "METRIC", "confidence": 0.9994946718215942}, {"text": "IWSLT", "start_pos": 85, "end_pos": 90, "type": "METRIC", "confidence": 0.4732525646686554}, {"text": "NIST", "start_pos": 103, "end_pos": 107, "type": "DATASET", "confidence": 0.9657562375068665}]}, {"text": "Our statistics shows that the TER-based word alignment generates more inconsistent links between the twodirectional word alignments than other methods.", "labels": [], "entities": [{"text": "TER-based word alignment", "start_pos": 30, "end_pos": 54, "type": "TASK", "confidence": 0.6553810934225718}]}, {"text": "This may give the intersection with heuristicbased expansion method more room to improve performance.", "labels": [], "entities": []}, {"text": "\ud97b\udf59 On the contrast, CLA-based method obtains relatively small improvement of 0.26 BLEUscore in IWSLT and 0.21 in NIST.", "labels": [], "entities": [{"text": "BLEUscore", "start_pos": 81, "end_pos": 90, "type": "METRIC", "confidence": 0.9992305040359497}, {"text": "IWSLT", "start_pos": 94, "end_pos": 99, "type": "METRIC", "confidence": 0.5281100273132324}, {"text": "NIST", "start_pos": 112, "end_pos": 116, "type": "DATASET", "confidence": 0.9488465189933777}]}, {"text": "The reason could be that the similarity functions used in the two directions are more similar.", "labels": [], "entities": []}, {"text": "Therefore, there are not so many inconsistent links between the two directions.", "labels": [], "entities": []}, {"text": "\ud97b\udf59 shows the number of links modified by intersection operation and the BLEU-score improvement.", "labels": [], "entities": [{"text": "BLEU-score", "start_pos": 71, "end_pos": 81, "type": "METRIC", "confidence": 0.999170184135437}]}, {"text": "We can see that the more the modified links, the bigger the improvement.", "labels": [], "entities": []}, {"text": "Effect of fuzzy matching in TER: the previous work on TER-based word alignment uses hard match in counting edits distance.", "labels": [], "entities": [{"text": "TER-based word alignment", "start_pos": 54, "end_pos": 78, "type": "TASK", "confidence": 0.5998086233933767}]}, {"text": "Therefore, it is notable to handle cognate words match, such as in, original TER script count the edit cost of (shoot, shot) equals to word pair (shot, the).", "labels": [], "entities": [{"text": "TER script count", "start_pos": 77, "end_pos": 93, "type": "DATASET", "confidence": 0.6791227261225382}]}, {"text": "Following (), we modified the TER script to allow fuzzy matching: change the substitution cost from 1 for any word pair to original TER, both of the two scores are equal to 1.", "labels": [], "entities": [{"text": "TER", "start_pos": 132, "end_pos": 135, "type": "METRIC", "confidence": 0.9316627979278564}]}, {"text": "Since cost of word pair (shoot, shot) is smaller than that of word pair (shot, the), word \"shot\" has higher chance to be aligned to \"shoot\") instead of \"the\").", "labels": [], "entities": []}, {"text": "This fuzzy matching mechanism is very useful to such kind of monolingual alignment task as in hypothesis-to-backbone word alignment since it can well model word variances and morphological changes.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 117, "end_pos": 131, "type": "TASK", "confidence": 0.7288991510868073}]}, {"text": "summaries the results of TER-based systems with or without fuzzy matching.", "labels": [], "entities": []}, {"text": "We can see that the fuzzy matching improves the performance for all cases.", "labels": [], "entities": []}, {"text": "This verifies the effect of fuzzy matching for TER in monolingual word alignment.", "labels": [], "entities": [{"text": "monolingual word alignment", "start_pos": 54, "end_pos": 80, "type": "TASK", "confidence": 0.6170024375120798}]}, {"text": "In addition, the improvement in NIST test set (0.36 BLEU-score for direct alignment and 0.21 BLEU-score for intersection one) are more than that in IWSLT test set (0.15 BLEUscore for direct alignment and 0.11 BLEU-score for intersection one).", "labels": [], "entities": [{"text": "NIST test set", "start_pos": 32, "end_pos": 45, "type": "DATASET", "confidence": 0.963948667049408}, {"text": "BLEU-score", "start_pos": 52, "end_pos": 62, "type": "METRIC", "confidence": 0.9951895475387573}, {"text": "BLEU-score", "start_pos": 93, "end_pos": 103, "type": "METRIC", "confidence": 0.9957695007324219}, {"text": "IWSLT test set", "start_pos": 148, "end_pos": 162, "type": "DATASET", "confidence": 0.9551838835080465}, {"text": "BLEUscore", "start_pos": 169, "end_pos": 178, "type": "METRIC", "confidence": 0.9943203926086426}, {"text": "BLEU-score", "start_pos": 209, "end_pos": 219, "type": "METRIC", "confidence": 0.9960424900054932}]}, {"text": "This is because the sentences of IWSLT test set are much shorter than that of NIST test set.: Results (BLEU% score) of TER-based combined systems with or without fuzzy match.", "labels": [], "entities": [{"text": "IWSLT test set", "start_pos": 33, "end_pos": 47, "type": "DATASET", "confidence": 0.9198763171831766}, {"text": "NIST test set.", "start_pos": 78, "end_pos": 92, "type": "DATASET", "confidence": 0.9491965174674988}, {"text": "BLEU% score)", "start_pos": 103, "end_pos": 115, "type": "METRIC", "confidence": 0.9625124335289001}]}], "tableCaptions": [{"text": " Table 1 summarizes the statistics of the train- ing, dev and test data for IWSLT and NIST tasks.", "labels": [], "entities": [{"text": "IWSLT", "start_pos": 76, "end_pos": 81, "type": "DATASET", "confidence": 0.7670597434043884}, {"text": "NIST", "start_pos": 86, "end_pos": 90, "type": "DATASET", "confidence": 0.6375177502632141}]}, {"text": " Table 2: Results (BLEU% score) of single sys- tems involved to system combination.", "labels": [], "entities": [{"text": "BLEU% score)", "start_pos": 19, "end_pos": 31, "type": "METRIC", "confidence": 0.9557336568832397}]}, {"text": " Table 3: Results (BLEU% score) of combined  systems based on direct word alignments.", "labels": [], "entities": [{"text": "BLEU% score)", "start_pos": 19, "end_pos": 31, "type": "METRIC", "confidence": 0.950236901640892}]}, {"text": " Table 4: Results (BLEU% score) of combined  systems based on intersection word alignments.", "labels": [], "entities": [{"text": "BLEU% score", "start_pos": 19, "end_pos": 30, "type": "METRIC", "confidence": 0.9638495842615763}, {"text": "intersection word alignments", "start_pos": 62, "end_pos": 90, "type": "TASK", "confidence": 0.6055704951286316}]}, {"text": " Table 5: Number of modified links and absolute  BLEU(%) score improvement on test sets.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 49, "end_pos": 53, "type": "METRIC", "confidence": 0.9628274440765381}]}, {"text": " Table 6: Results (BLEU% score) of TER-based  combined systems with or without fuzzy match.", "labels": [], "entities": [{"text": "BLEU% score)", "start_pos": 19, "end_pos": 31, "type": "METRIC", "confidence": 0.9501251578330994}]}]}