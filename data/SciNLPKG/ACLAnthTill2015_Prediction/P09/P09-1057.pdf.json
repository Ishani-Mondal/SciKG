{"title": [{"text": "Minimized Models for Unsupervised Part-of-Speech Tagging", "labels": [], "entities": [{"text": "Part-of-Speech Tagging", "start_pos": 34, "end_pos": 56, "type": "TASK", "confidence": 0.6796329915523529}]}], "abstractContent": [{"text": "We describe a novel method for the task of unsupervised POS tagging with a dictionary , one that uses integer programming to explicitly search for the smallest model that explains the data, and then uses EM to set parameter values.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 56, "end_pos": 67, "type": "TASK", "confidence": 0.7951115965843201}]}, {"text": "We evaluate our method on a standard test corpus using different standard tagsets (a 45-tagset as well as a smaller 17-tagset), and show that our approach performs better than existing state-of-the-art systems in both settings.", "labels": [], "entities": []}], "introductionContent": [{"text": "In recent years, we have seen increased interest in using unsupervised methods for attacking different NLP tasks like part-of-speech (POS) tagging.", "labels": [], "entities": [{"text": "part-of-speech (POS) tagging", "start_pos": 118, "end_pos": 146, "type": "TASK", "confidence": 0.6191076517105103}]}, {"text": "The classic Expectation Maximization (EM) algorithm has been shown to perform poorly on POS tagging, when compared to other techniques, such as Bayesian methods.", "labels": [], "entities": [{"text": "Expectation Maximization (EM)", "start_pos": 12, "end_pos": 41, "type": "TASK", "confidence": 0.815451318025589}, {"text": "POS tagging", "start_pos": 88, "end_pos": 99, "type": "TASK", "confidence": 0.8328779339790344}]}, {"text": "In this paper, we develop new methods for unsupervised part-of-speech tagging.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 55, "end_pos": 77, "type": "TASK", "confidence": 0.7110048979520798}]}, {"text": "We adopt the problem formulation of, in which we are given a raw word sequence and a dictionary of legal tags for each word type.", "labels": [], "entities": []}, {"text": "The goal is to tag each word token so as to maximize accuracy against a gold tag sequence.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.9963460564613342}]}, {"text": "Whether this is a realistic problem set-up is arguable, but an interesting collection of methods and results has accumulated around it, and these can be clearly compared with one another.", "labels": [], "entities": []}, {"text": "We use the standard test set for this task, a 24,115-word subset of the Penn Treebank, for which a gold tag sequence is available.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 72, "end_pos": 85, "type": "DATASET", "confidence": 0.9959789514541626}]}, {"text": "There are 5,878 word types in this test set.", "labels": [], "entities": []}, {"text": "We use the standard tag dictionary, consisting of 57,388 word/tag pairs derived from the entire Penn Treebank.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 96, "end_pos": 109, "type": "DATASET", "confidence": 0.995199590921402}]}, {"text": "8,910 dictionary entries are relevant to the 5,878 word types in the test set.", "labels": [], "entities": []}, {"text": "Per-token ambiguity is about 1.5 tags/token, yielding approximately 10 6425 possible ways to tag the data.", "labels": [], "entities": []}, {"text": "There are 45 distinct grammatical tags.", "labels": [], "entities": []}, {"text": "In this set-up, there are no unknown words.", "labels": [], "entities": []}, {"text": "shows prior results for this problem.", "labels": [], "entities": []}, {"text": "While the methods are quite different, they all make use of two common model elements.", "labels": [], "entities": []}, {"text": "One is a probabilistic n-gram tag model P(t i |t i\u2212n+1 ...t i\u22121 ), which we call the grammar.", "labels": [], "entities": []}, {"text": "The other is a probabilistic word-given-tag model P(w i |t i ), which we call the dictionary.", "labels": [], "entities": []}, {"text": "The classic approach) is expectation-maximization (EM), where we estimate grammar and dictionary probabilities in order to maximize the probability of the observed word sequence: P (w1...wn) = Goldwater and Griffiths (2007) report 74.5% accuracy for EM with a 3-gram tag model, which we confirm by replication.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 237, "end_pos": 245, "type": "METRIC", "confidence": 0.9992235898971558}]}, {"text": "They improve this to 83.9% by employing a fully Bayesian approach which integrates overall possible parameter values, rather than estimating a single distribution.", "labels": [], "entities": []}, {"text": "They further improve this to 86.8% by using priors that favor sparse distributions.", "labels": [], "entities": []}, {"text": "Smith and Eisner (2005) employ a contrastive estimation tech-System Tagging accuracy (%) on 24,115-word corpus 1.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 76, "end_pos": 84, "type": "METRIC", "confidence": 0.503948450088501}]}, {"text": "Random baseline (for each word, pick a random tag from the alternatives given by the word/tag dictionary) 64.6 2.", "labels": [], "entities": []}, {"text": "EM with 2-gram tag model 81.7 3.", "labels": [], "entities": [{"text": "EM", "start_pos": 0, "end_pos": 2, "type": "DATASET", "confidence": 0.8039000630378723}]}, {"text": "EM with 3-gram tag model 74.5 4a.", "labels": [], "entities": [{"text": "EM", "start_pos": 0, "end_pos": 2, "type": "DATASET", "confidence": 0.8026329278945923}]}, {"text": "Bayesian method with sparse priors 86.8 5.", "labels": [], "entities": []}, {"text": "CRF model trained using contrastive estimation 88.6 6.", "labels": [], "entities": [{"text": "CRF", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.6472609639167786}]}, {"text": "EM-HMM tagger provided with good initial conditions ( 91.4* (*uses linguistic constraints and manual adjustments to the dictionary): Previous results on unsupervised POS tagging using a dictionary on the full 45-tag set.", "labels": [], "entities": [{"text": "EM-HMM tagger", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.7211004197597504}, {"text": "POS tagging", "start_pos": 166, "end_pos": 177, "type": "TASK", "confidence": 0.7581570744514465}]}, {"text": "All other results reported in this paper (unless specified otherwise) are on the 45-tag set as well.", "labels": [], "entities": [{"text": "45-tag set", "start_pos": 81, "end_pos": 91, "type": "DATASET", "confidence": 0.6561731994152069}]}, {"text": "nique, in which they automatically generate negative examples and use CRF training.", "labels": [], "entities": []}, {"text": "In more recent work, Toutanova and Johnson (2008) propose a Bayesian LDA-based generative model that in addition to using sparse priors, explicitly groups words into ambiguity classes.", "labels": [], "entities": []}, {"text": "They show considerable improvements in tagging accuracy when using a coarser-grained version (with 17-tags) of the tag set from the Penn Treebank.", "labels": [], "entities": [{"text": "tagging", "start_pos": 39, "end_pos": 46, "type": "TASK", "confidence": 0.9577638506889343}, {"text": "accuracy", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.9687935709953308}, {"text": "Penn Treebank", "start_pos": 132, "end_pos": 145, "type": "DATASET", "confidence": 0.9621767699718475}]}, {"text": "depart from the Bayesian framework and show how EM can be used to learn good POS taggers for Hebrew and English, when provided with good initial conditions.", "labels": [], "entities": [{"text": "POS taggers", "start_pos": 77, "end_pos": 88, "type": "TASK", "confidence": 0.7002414762973785}]}, {"text": "They use language specific information (like word contexts, syntax and morphology) for learning initial P(t|w) distributions and also use linguistic knowledge to apply constraints on the tag sequences allowed by their models (e.g., the tag sequence \"V V\" is disallowed).", "labels": [], "entities": []}, {"text": "Also, they make other manual adjustments to reduce noise from the word/tag dictionary (e.g., reducing the number of tags for \"the\" from six to just one).", "labels": [], "entities": []}, {"text": "In contrast, we keep all the original dictionary entries derived from the Penn Treebank data for our experiments.", "labels": [], "entities": [{"text": "Penn Treebank data", "start_pos": 74, "end_pos": 92, "type": "DATASET", "confidence": 0.9950530926386515}]}, {"text": "The literature omits one other baseline, which is EM with a 2-gram tag model.", "labels": [], "entities": []}, {"text": "Here we obtain 81.7% accuracy, which is better than the 3-gram model.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9998058676719666}]}, {"text": "It seems that EM with a 3-gram tag model runs amok with its freedom.", "labels": [], "entities": []}, {"text": "For the rest of this paper, we will limit ourselves to a 2-gram tag model.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}