{"title": [{"text": "Automatic training of lemmatization rules that handle morphological changes in pre-, in-and suffixes alike", "labels": [], "entities": []}], "abstractContent": [{"text": "We propose a method to automatically train lemmatization rules that handle prefix, infix and suffix changes to generate the lemma from the full form of a word.", "labels": [], "entities": []}, {"text": "We explain how the lemmatization rules are created and how the lemmatizer works.", "labels": [], "entities": []}, {"text": "We trained this lemmatizer on Danish, Dutch, English, German, Greek, Icelandic, Norwegian, Polish, Slovene and Swedish full form-lemma pairs respectively.", "labels": [], "entities": []}, {"text": "We obtained significant improvements of 24 percent for Polish, 2.3 percent for Dutch, 1.5 percent for English, 1.2 percent for German and 1.0 percent for Swedish compared to plain suffix lemmatization using a suffix-only lem-matizer.", "labels": [], "entities": []}, {"text": "Icelandic deteriorated with 1.9 percent.", "labels": [], "entities": [{"text": "Icelandic", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.7174844145774841}]}, {"text": "We also made an observation regarding the number of produced lemmatization rules as a function of the number of training pairs.", "labels": [], "entities": []}], "introductionContent": [{"text": "Lemmatizers and stemmers are valuable human language technology tools to improve precision and recall in an information retrieval setting.", "labels": [], "entities": [{"text": "precision", "start_pos": 81, "end_pos": 90, "type": "METRIC", "confidence": 0.995891809463501}, {"text": "recall", "start_pos": 95, "end_pos": 101, "type": "METRIC", "confidence": 0.9937196969985962}]}, {"text": "For example, stemming and lemmatization make it possible to match a query in one morphological form with a word in a document in another morphological form.", "labels": [], "entities": []}, {"text": "Lemmatizers can also be used in lexicography to find new words in text material, including the words' frequency of use.", "labels": [], "entities": []}, {"text": "Other applications are creation of index lists for book indexes as well as keyword lists Lemmatization is the process of reducing a word to its base form, normally the dictionary look-up form (lemma) of the word.", "labels": [], "entities": []}, {"text": "A trivial way to do this is by dictionary look-up.", "labels": [], "entities": []}, {"text": "More advanced systems use hand crafted or automatically generated transformation rules that look at the surface form of the word and attempt to produce the correct base form by replacing all or parts of the word.", "labels": [], "entities": []}, {"text": "Stemming conflates a word to its stem.", "labels": [], "entities": []}, {"text": "A stem does not have to be the lemma of the word, but can be any trait that is shared between a group of words, so that even the group membership itself can be regarded as the group's stem.", "labels": [], "entities": []}, {"text": "The most famous stemmer is the Porter Stemmer for English.", "labels": [], "entities": []}, {"text": "This stemmer removes around 60 different suffixes, using rewriting rules in two steps.", "labels": [], "entities": []}, {"text": "The paper is structured as follows: section 2 discusses related work, section 3 explains what the new algorithm is supposed to do, section 4 describes some details of the new algorithm, section 5 evaluates the results, conclusions are drawn in section 6, and finally in section 7 we mention plans for further tests and improvements.", "labels": [], "entities": []}], "datasetContent": [{"text": "We trained the new lemmatizer using training material for Danish (STO), Dutch (CELEX), English (CELEX), German (CELEX),), Icelandic (IFD), Norwegian (SCARRIE), Polish (Morfologik),) and Swedish (SUC).", "labels": [], "entities": []}, {"text": "The guidelines for the construction of the training material are not always known to us.", "labels": [], "entities": []}, {"text": "In some cases, we know that the full forms have been generated automatically from the lemmas.", "labels": [], "entities": []}, {"text": "On the other hand, we know that the Icelandic data is derived from a corpus and only contains word forms occurring in that corpus.", "labels": [], "entities": [{"text": "Icelandic data", "start_pos": 36, "end_pos": 50, "type": "DATASET", "confidence": 0.7738938927650452}]}, {"text": "Because of the uncertainties, the results cannot be used fora quantitative comparison of the accuracy of lemmatization between languages.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 93, "end_pos": 101, "type": "METRIC", "confidence": 0.9988017082214355}]}, {"text": "Some of the resources were already disambiguated (one lemma per full form) when we received the data.", "labels": [], "entities": []}, {"text": "We decided to disambiguate the remaining resources as well.", "labels": [], "entities": []}, {"text": "Handling homographs wisely is important in many lemmatization tasks, but there are many pitfalls.", "labels": [], "entities": []}, {"text": "As we only wanted to investigate the improvement of the affix algorithm over the suffix algorithm, we decided to factor out ambiguity.", "labels": [], "entities": []}, {"text": "We simply chose the lemma that comes first alphabetically and discarded the other lemmas from the available data.", "labels": [], "entities": []}, {"text": "The evaluation was carried out by dividing the available material in training data and test data in seven different ratios, setting aside between 1.54% and 98.56% as training data and the remainder as OOV test data.", "labels": [], "entities": [{"text": "OOV test data", "start_pos": 201, "end_pos": 214, "type": "DATASET", "confidence": 0.6950004994869232}]}, {"text": "(See section 7).", "labels": [], "entities": []}, {"text": "To keep the sample standard deviation s for the accuracy below an acceptable level we used the evaluation method repeated random subsampling validation that is proposed in and.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.9985087513923645}]}, {"text": "We repeated the training and evaluation for each ratio with several randomly chosen sets, up to 17 times for the smallest and largest ratios, because these ratios lead to relatively small training sets and test sets respectively.", "labels": [], "entities": []}, {"text": "The same procedure was followed for the suffix lemmatizer, using the same training and test sets.", "labels": [], "entities": []}, {"text": "shows the results for the largest training sets.", "labels": [], "entities": []}, {"text": "For some languages lemmatization accuracy for OOV words improved by deleting rules that are based on very few examples from the training data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.9726630449295044}]}, {"text": "This pruning was done after the training of the rule set was completed.", "labels": [], "entities": []}, {"text": "Regarding the affix algorithm, the results for half of the languages became better with mild pruning, i.e. deleting rules with only one example.", "labels": [], "entities": []}, {"text": "For Danish, Dutch, German, Greek and Icelandic pruning did not improve accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 71, "end_pos": 79, "type": "METRIC", "confidence": 0.9971609115600586}]}, {"text": "Regarding the suffix algorithm, only English and Swedish profited from pruning..", "labels": [], "entities": []}, {"text": "Accuracy for the suffix and affix algorithms.", "labels": [], "entities": []}, {"text": "The fifth column shows the size of the available data.", "labels": [], "entities": []}, {"text": "Of these, 98.56% was used for training and 1.44% for testing.", "labels": [], "entities": []}, {"text": "The last column shows the number n of performed iterations, which was inversely proportional to \u221aN with a minimum of two.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3. Accuracy for the suffix and affix algo- rithms. The fifth column shows the size of the  available data. Of these, 98.56% was used for  training and 1.44% for testing. The last column  shows the number n of performed iterations,  which was inversely proportional to \u221aN with a  minimum of two.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9986361861228943}]}, {"text": " Table 4. Prevalence of suffix-only rules, rules  specifying a prefix, rules specifying an infix and  rules specifying infixes containing either \u00e4, \u00f6 or  \u00fc or the letter combination ge.  Almost 94% of the lemmas were created using  suffix-only rules, with an accuracy of almost  89%. Less than 3% of the lemmas were created  using rules that included at least one infix sub- pattern. Of these, about 83% were correctly  lemmatized, pulling the average down. We also  looked at two particular groups of infix-rules:  those including the letters \u00e4, \u00f6 or \u00fc and those  with the letter combination ge. The former  group applies to many words that display umlaut,  while the latter applies to past participles. The", "labels": [], "entities": [{"text": "accuracy", "start_pos": 259, "end_pos": 267, "type": "METRIC", "confidence": 0.9992620348930359}]}]}