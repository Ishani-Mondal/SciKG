{"title": [{"text": "Leveraging Structural Relations for Fluent Compressions at Multiple Compression Rates", "labels": [], "entities": []}], "abstractContent": [{"text": "Prior approaches to sentence compression have taken low level syntactic constraints into account in order to maintain grammaticality.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 20, "end_pos": 40, "type": "TASK", "confidence": 0.7497917413711548}]}, {"text": "We propose and successfully evaluate a more comprehensive, generalizable feature set that takes syntactic and structural relationships into account in order to sustain variable compression rates while making compressed sentences more coherent, grammatical and readable.", "labels": [], "entities": []}], "introductionContent": [{"text": "We present an evaluation of the effect of syntactic and structural constraints at multiple levels of granularity on the robustness of sentence compression at varying compression rates.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 134, "end_pos": 154, "type": "TASK", "confidence": 0.7198648452758789}]}, {"text": "Our evaluation demonstrates that the new feature set produces significantly improved compressions across a range of compression rates compared to existing state-of-the-art approaches.", "labels": [], "entities": []}, {"text": "Thus, we name our system for generating compressions the Adjustable Rate Compressor (ARC).", "labels": [], "entities": []}, {"text": "Knight and Marcu (2000) (K&M, henceforth) presented two approaches to the sentence compression problem: one using a noisy channel model, the other using a decision-based model.", "labels": [], "entities": [{"text": "sentence compression problem", "start_pos": 74, "end_pos": 102, "type": "TASK", "confidence": 0.8631935715675354}]}, {"text": "The performances of the two models were comparable though their experiments suggested that the noisy channel model degraded more smoothly than the decision-based model when tested on out-of-domain data.", "labels": [], "entities": []}, {"text": "applied linguistically rich LFG grammars to a sentence compression system.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 46, "end_pos": 66, "type": "TASK", "confidence": 0.7181138843297958}]}, {"text": "achieved similar performance to K&M using an unsupervised approach that induced rules from the Penn Treebank.", "labels": [], "entities": [{"text": "K&M", "start_pos": 32, "end_pos": 35, "type": "DATASET", "confidence": 0.7047354976336161}, {"text": "Penn Treebank", "start_pos": 95, "end_pos": 108, "type": "DATASET", "confidence": 0.9936611652374268}]}, {"text": "A variety of feature encodings have previously been explored for the problem of sentence compression.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 80, "end_pos": 100, "type": "TASK", "confidence": 0.7991639375686646}]}, {"text": "included discourse level features in their framework to leverage context for enhancing coherence.", "labels": [], "entities": []}, {"text": "model (M06, henceforth) is similar to K&M except that it uses discriminative online learning to train feature weights.", "labels": [], "entities": []}, {"text": "A key aspect of the M06 approach is a decoding algorithm that searches the entire space of compressions using dynamic programming to choose the best compression (details in Section 2).", "labels": [], "entities": []}, {"text": "We use M06 as a foundation for this work because its soft constraint approach allows for natural integration of additional classes of features.", "labels": [], "entities": []}, {"text": "Similar to most previous approaches, our approach compresses sentences by deleting words only.", "labels": [], "entities": []}, {"text": "The remainder of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 discusses the architectural framework.", "labels": [], "entities": []}, {"text": "Section 3 describes the innovations in the proposed model.", "labels": [], "entities": []}, {"text": "We conclude after presenting the results of our evaluation in Section 4.", "labels": [], "entities": []}], "datasetContent": [{"text": "Supervised approaches to sentence compression typically use parallel corpora consisting of original and compressed sentences (paired corpus, henceforth).", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 25, "end_pos": 45, "type": "TASK", "confidence": 0.7873306274414062}]}, {"text": "In this paper, we will refer to these pairs as a 2-tuple <x, y>, where x is the original sentence and y is the compressed sentence.", "labels": [], "entities": []}, {"text": "We implemented the M06 system as an experimental framework in which to conduct our investigation.", "labels": [], "entities": [{"text": "M06 system", "start_pos": 19, "end_pos": 29, "type": "DATASET", "confidence": 0.8631292283535004}]}, {"text": "The system uses as input the paired corpus, the corresponding POS tagged corpus, the paired corpus parsed using the Charniak parser, and dependency parses from the MST parser).", "labels": [], "entities": [{"text": "POS tagged corpus", "start_pos": 62, "end_pos": 79, "type": "DATASET", "confidence": 0.6789713104565939}]}, {"text": "Features are extracted over adjacent pairs of words in the compressed sentence and weights are learnt at training time using the MIRA algorithm.", "labels": [], "entities": [{"text": "MIRA", "start_pos": 129, "end_pos": 133, "type": "METRIC", "confidence": 0.4853260815143585}]}, {"text": "We decode as follows to find the best compression: Let the score of a compression y fora sentence x be s(x, y).", "labels": [], "entities": []}, {"text": "This score is factored using a first-order Markov assumption over the words in the compressed sentence, and is defined by the dot product between a high dimensional feature representation and a corresponding weight vector (for details, refer to).", "labels": [], "entities": []}, {"text": "The equations for decoding are as follows: where C is the dynamic programming table and C[i] represents the highest score for compressions ending at word i for the sentence x.", "labels": [], "entities": []}, {"text": "The M06 system takes the best scoring compression from the set of all possible compressions.", "labels": [], "entities": [{"text": "M06", "start_pos": 4, "end_pos": 7, "type": "DATASET", "confidence": 0.7722986936569214}]}, {"text": "In the ARC system, the model determines the compression rate and enforces a target compression length by altering the dynamic programming algorithm as suggested by M06: where C is the dynamic programming table as before and C[r] is the score for the best compression of length r that ends at position i in the sentence x.", "labels": [], "entities": [{"text": "M06", "start_pos": 164, "end_pos": 167, "type": "DATASET", "confidence": 0.9113873839378357}]}, {"text": "This algorithm runs in O (n 2 r) time.", "labels": [], "entities": [{"text": "O", "start_pos": 23, "end_pos": 24, "type": "METRIC", "confidence": 0.9075850248336792}]}, {"text": "We define the rate of human generated compressions in the training corpus as the gold standard compression rate (GSCR).", "labels": [], "entities": [{"text": "gold standard compression rate (GSCR)", "start_pos": 81, "end_pos": 118, "type": "METRIC", "confidence": 0.8017852561814445}]}, {"text": "We train a linear regression model over the training data to predict the GSCR fora sentence based on the ratio between the lengths of each compressed-original sentence pair in the training set.", "labels": [], "entities": []}, {"text": "The predicted compression rate is used to force the system to compress sentences in the test set to a specific target length.", "labels": [], "entities": []}, {"text": "Based on the computed regression, the formula for computing the Predicted Compression Rate (PCR) from the Original Sentence Length (OSL) is as follows: In our work, enforcing specific compression rates serves two purposes.", "labels": [], "entities": [{"text": "Predicted Compression Rate (PCR)", "start_pos": 64, "end_pos": 96, "type": "METRIC", "confidence": 0.9319755931695303}, {"text": "Original Sentence Length (OSL)", "start_pos": 106, "end_pos": 136, "type": "METRIC", "confidence": 0.7683233022689819}]}, {"text": "First, it allows us to make a more controlled comparison across approaches, since variation in compression rate across approaches confounds comparison of other aspects of performance.", "labels": [], "entities": []}, {"text": "Second, it allows us to investigate how alternative models work at higher compression rates.", "labels": [], "entities": []}, {"text": "Here our primary contribution is of robustness of the approach with respect to alternative feature spaces and compression rates.", "labels": [], "entities": []}, {"text": "We evaluate our model in comparison with M06.", "labels": [], "entities": [{"text": "M06", "start_pos": 41, "end_pos": 44, "type": "DATASET", "confidence": 0.8858305215835571}]}, {"text": "At training time, compression rates were not enforced on the ARC or M06 model.", "labels": [], "entities": [{"text": "compression rates", "start_pos": 18, "end_pos": 35, "type": "METRIC", "confidence": 0.9737687110900879}, {"text": "ARC", "start_pos": 61, "end_pos": 64, "type": "DATASET", "confidence": 0.9659292697906494}]}, {"text": "Our evaluation demonstrates that the proposed feature set produces more grammatical sentences across varying compression rates.", "labels": [], "entities": []}, {"text": "In this section, GSCR denotes gold standard compression rate (i.e., the compression rate found in training data), CR denotes compression rate.", "labels": [], "entities": [{"text": "GSCR", "start_pos": 17, "end_pos": 21, "type": "METRIC", "confidence": 0.9931983351707458}, {"text": "gold standard compression rate", "start_pos": 30, "end_pos": 60, "type": "METRIC", "confidence": 0.7656883895397186}, {"text": "CR", "start_pos": 114, "end_pos": 116, "type": "METRIC", "confidence": 0.9959768652915955}, {"text": "compression rate", "start_pos": 125, "end_pos": 141, "type": "METRIC", "confidence": 0.972815603017807}]}], "tableCaptions": [{"text": " Table 1 presents all the results in  terms of human ratings of Grammaticality and  Completeness as well as automatically computed  ROUGE F 1 scores (Lin and Hovy, 2003). The  scores in parentheses denote standard deviations.", "labels": [], "entities": [{"text": "Completeness", "start_pos": 84, "end_pos": 96, "type": "METRIC", "confidence": 0.9842656850814819}, {"text": "ROUGE F 1 scores", "start_pos": 132, "end_pos": 148, "type": "METRIC", "confidence": 0.9087388813495636}]}]}