{"title": [{"text": "Reducing SMT Rule Table with Monolingual Key Phrase", "labels": [], "entities": [{"text": "SMT Rule", "start_pos": 9, "end_pos": 17, "type": "TASK", "confidence": 0.7722400426864624}]}], "abstractContent": [{"text": "This paper presents an effective approach to discard most entries of the rule table for statistical machine translation.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 88, "end_pos": 119, "type": "TASK", "confidence": 0.7514992753664652}]}, {"text": "The rule table is filtered by monolingual key phrases, which are extracted from source text using a technique based on term extraction.", "labels": [], "entities": [{"text": "term extraction", "start_pos": 119, "end_pos": 134, "type": "TASK", "confidence": 0.6868184804916382}]}, {"text": "Experiments show that 78% of the rule table is reduced without worsening translation performance.", "labels": [], "entities": [{"text": "translation", "start_pos": 73, "end_pos": 84, "type": "TASK", "confidence": 0.946212887763977}]}, {"text": "In most cases, our approach results in measurable improvements in BLEU score.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 66, "end_pos": 76, "type": "METRIC", "confidence": 0.9641201496124268}]}], "introductionContent": [{"text": "In statistical machine translation (SMT) community, the state-of-the-art method is to use rules that contain hierarchical structures to model translation, such as the hierarchical phrase-based model.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 3, "end_pos": 40, "type": "TASK", "confidence": 0.7964508434136709}]}, {"text": "Rules are more powerful than conventional phrase pairs because they contain structural information for capturing long distance reorderings.", "labels": [], "entities": [{"text": "capturing long distance reorderings", "start_pos": 103, "end_pos": 138, "type": "TASK", "confidence": 0.7205907106399536}]}, {"text": "However, hierarchical translation systems often suffer from a large rule table (the collection of rules), which makes decoding slow and memory-consuming.", "labels": [], "entities": []}, {"text": "In the training procedure of SMT systems, numerous rules are extracted from the bilingual corpus.", "labels": [], "entities": [{"text": "SMT", "start_pos": 29, "end_pos": 32, "type": "TASK", "confidence": 0.9936210513114929}]}, {"text": "During decoding, however, many of them are rarely used.", "labels": [], "entities": []}, {"text": "One of the reasons is that these rules have low quality.", "labels": [], "entities": [{"text": "quality", "start_pos": 48, "end_pos": 55, "type": "METRIC", "confidence": 0.9541311860084534}]}, {"text": "The rule quality are usually evaluated by the conditional translation probabilities, which focus on the correspondence between the source and target phrases, while ignore the quality of phrases in a monolingual corpus.", "labels": [], "entities": []}, {"text": "In this paper, we address the problem of reducing the rule table with the information of monolingual corpus.", "labels": [], "entities": []}, {"text": "We use C-value, a measurement of automatic term recognition, to score source phrases.", "labels": [], "entities": [{"text": "term recognition", "start_pos": 43, "end_pos": 59, "type": "TASK", "confidence": 0.68430095911026}]}, {"text": "A source phrase is regarded as a key phrase if its score greater than a threshold.", "labels": [], "entities": []}, {"text": "Note that a source phrase is either a flat phrase consists of words, or a hierarchical phrase consists of both words and variables.", "labels": [], "entities": []}, {"text": "For rule table reduction, the rule whose source-side is not key phrase is discarded.", "labels": [], "entities": [{"text": "rule table reduction", "start_pos": 4, "end_pos": 24, "type": "TASK", "confidence": 0.772931694984436}]}, {"text": "Our approach is different from the previous research.", "labels": [], "entities": []}, {"text": "reduced the phrase table based on the significance testing of phrase pair co-occurrence in bilingual corpus.", "labels": [], "entities": []}, {"text": "The basic difference is that they used statistical information of bilingual corpus while we use that of monolingual corpus.", "labels": [], "entities": []}, {"text": "proposed a string-to-dependency model, which restricted the target-side of a rule by dependency structures.", "labels": [], "entities": []}, {"text": "Their approach greatly reduced the rule table, however, caused a slight decrease of translation quality.", "labels": [], "entities": [{"text": "translation", "start_pos": 84, "end_pos": 95, "type": "TASK", "confidence": 0.954879641532898}]}, {"text": "They obtained improvements by incorporating an additional dependency language model.", "labels": [], "entities": []}, {"text": "Different from their research, we restrict rules on the source-side.", "labels": [], "entities": []}, {"text": "Furthermore, the system complexity is not increased because no additional model is introduced.", "labels": [], "entities": []}, {"text": "The hierarchical phrase-based model) is used to build a translation system.", "labels": [], "entities": []}, {"text": "Experiments show that our approach discards 78% of the rule table without worsening the translation quality.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our experiments were carried out on two language pairs: \u2022 Chinese-English: For this task, the corpora are from the NIST evaluation.", "labels": [], "entities": [{"text": "NIST evaluation", "start_pos": 115, "end_pos": 130, "type": "DATASET", "confidence": 0.9498180747032166}]}, {"text": "The parallel corpus 1 consists of 1M sentence pairs . We trained two trigram language models: one on the Xinhua portion of the Gigaword corpus, and the other on the target-side of the parallel corpus.", "labels": [], "entities": [{"text": "Gigaword corpus", "start_pos": 127, "end_pos": 142, "type": "DATASET", "confidence": 0.8818435966968536}]}, {"text": "The test sets were NIST MT06 GALE set (06G) and NIST set (06N) and NIST MT08 test set.", "labels": [], "entities": [{"text": "NIST MT06 GALE set", "start_pos": 19, "end_pos": 37, "type": "DATASET", "confidence": 0.7204083502292633}, {"text": "NIST set (06N", "start_pos": 48, "end_pos": 61, "type": "DATASET", "confidence": 0.7991819232702255}, {"text": "NIST MT08 test set", "start_pos": 67, "end_pos": 85, "type": "DATASET", "confidence": 0.8966643065214157}]}, {"text": "\u2022 German-English: For this task, the corpora are from the WMT 2 evaluation.", "labels": [], "entities": [{"text": "WMT 2 evaluation", "start_pos": 58, "end_pos": 74, "type": "DATASET", "confidence": 0.7930904825528463}]}, {"text": "The parallel corpus contains 1.3M sentence pairs.", "labels": [], "entities": []}, {"text": "The target-side was used to train a trigram language model.", "labels": [], "entities": []}, {"text": "The test sets were WMT06 and WMT07.", "labels": [], "entities": [{"text": "WMT06", "start_pos": 19, "end_pos": 24, "type": "DATASET", "confidence": 0.9782356023788452}, {"text": "WMT07", "start_pos": 29, "end_pos": 34, "type": "DATASET", "confidence": 0.9809313416481018}]}, {"text": "1 LDC2002E18 (4,000 sentences), LDC2002T01, LDC2003E07, LDC2003E14, LDC2004T07, LDC2005T10, LDC2004T08 HK Hansards (500,000 sentences) 2 http://www.statmt.org/wmt07/shared-task.html For both the tasks, the word alignment were trained by GIZA++ in two translation directions and refined by \"grow-diag-final\" method ().", "labels": [], "entities": [{"text": "HK Hansards", "start_pos": 103, "end_pos": 114, "type": "DATASET", "confidence": 0.6926206648349762}, {"text": "word alignment", "start_pos": 206, "end_pos": 220, "type": "TASK", "confidence": 0.7502183318138123}]}, {"text": "The source-side of the parallel corpus is used to extract key phrases.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: C-value threshold effect on the rule table size and BLEU scores.", "labels": [], "entities": [{"text": "C-value threshold effect", "start_pos": 10, "end_pos": 34, "type": "METRIC", "confidence": 0.9531277219454447}, {"text": "BLEU", "start_pos": 62, "end_pos": 66, "type": "METRIC", "confidence": 0.9995447993278503}]}, {"text": " Table 2: BLEU scores on the test sets of the Chinese-English task.  *  means significantly better than  baseline at p < 0.01. + means significantly better than C-value at p < 0.05.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9994791150093079}]}]}