{"title": [{"text": "Setting Up User Action Probabilities in User Simulations for Dialog System Development", "labels": [], "entities": [{"text": "User Simulations", "start_pos": 40, "end_pos": 56, "type": "TASK", "confidence": 0.704900473356247}, {"text": "Dialog System Development", "start_pos": 61, "end_pos": 86, "type": "TASK", "confidence": 0.7963769833246866}]}], "abstractContent": [{"text": "User simulations are shown to be useful in spoken dialog system development.", "labels": [], "entities": [{"text": "spoken dialog system development", "start_pos": 43, "end_pos": 75, "type": "TASK", "confidence": 0.8221992701292038}]}, {"text": "Since most current user simulations deploy probability models to mimic human user behaviors , how to setup user action probabilities in these models is a key problem to solve.", "labels": [], "entities": []}, {"text": "One generally used approach is to estimate these probabilities from human user data.", "labels": [], "entities": []}, {"text": "However, when building anew dialog system, usually no data or only a small amount of data is available.", "labels": [], "entities": []}, {"text": "In this study, we compare estimating user probabilities from a small user data set versus handcrafting the probabilities.", "labels": [], "entities": []}, {"text": "We discuss the pros and cons of both solutions for different dialog system development tasks.", "labels": [], "entities": [{"text": "dialog system development", "start_pos": 61, "end_pos": 86, "type": "TASK", "confidence": 0.7319598396619161}]}], "introductionContent": [{"text": "User simulations are widely used in spoken dialog system development.", "labels": [], "entities": [{"text": "spoken dialog system development", "start_pos": 36, "end_pos": 68, "type": "TASK", "confidence": 0.7831399887800217}]}, {"text": "Recent studies use user simulations to generate training corpora to learn dialog strategies automatically,), or to evaluate dialog system performance ().", "labels": [], "entities": []}, {"text": "Most studies show that using user simulations significantly improves dialog system performance as well as speeds up system development.", "labels": [], "entities": []}, {"text": "Since user simulation is such a useful tool, dialog system researchers have studied how to build user simulations from a variety of perspectives.", "labels": [], "entities": []}, {"text": "Some studies look into the impact of training data on user simulations.", "labels": [], "entities": []}, {"text": "For example, ( observe differences between simulated users trained from human users of different age groups.", "labels": [], "entities": []}, {"text": "Other studies explore different simulation models, i.e. the mechanism of deciding the next user actions given the current dialog context.", "labels": [], "entities": []}, {"text": "() give a thorough review of different types of simulation models.", "labels": [], "entities": []}, {"text": "Since most of these current user simulation techniques use probabilistic models to generate user actions, how to setup the probabilities in the simulations is another important problem to solve.", "labels": [], "entities": []}, {"text": "One general approach to setup user action probabilities is to learn the probabilities from a collected human user dialog corpus ((,).", "labels": [], "entities": []}, {"text": "While this approach takes advantage of observed user behaviors in predicting future user behaviors, it suffers from the problem of learning probabilities from one group of users while potentially using them with another group of users.", "labels": [], "entities": []}, {"text": "The accuracy of the learned probabilities becomes more questionable when the collected human corpus is small.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9990239143371582}]}, {"text": "However, this is a common problem in building new dialog systems, when often no data or only a small amount of data is available.", "labels": [], "entities": []}, {"text": "An alternative approach is to handcraft user action probabilities ((,).", "labels": [], "entities": []}, {"text": "This approach is less dataintensive, but requires nontrivial work by domain experts.", "labels": [], "entities": []}, {"text": "What is more, as the number of probabilities increases, it is hard even for the experts to set the probabilities.", "labels": [], "entities": []}, {"text": "Since both handcrafting and training user action probabilities have their own pros and cons, it is an interesting research question to investigate which approach is better fora certain task given the amount of data that is available.", "labels": [], "entities": []}, {"text": "In this study, we investigate a manual and a trained approach insetting up user action probabilities, applied to building the same probabilistic simulation model.", "labels": [], "entities": []}, {"text": "For the manual user simulations, we look into two sets of handcrafted probabilities which use the same expert knowledge but differ in individual probability values.", "labels": [], "entities": []}, {"text": "This aims to take into account small variations that can possi-bly be introduced by different domain experts.", "labels": [], "entities": []}, {"text": "For the trained user simulations, we examine two sets of probabilities trained from user corpora of different sizes, since the amount of training data will impact the quality of the trained probability models.", "labels": [], "entities": []}, {"text": "We compare the trained and the handcrafted simulations on three tasks.", "labels": [], "entities": []}, {"text": "We observe that in our task settings, the two manual simulations do not differ significantly on any tasks.", "labels": [], "entities": []}, {"text": "In addition, there is no significant difference among the trained and the manual simulations in generating corpus level dialog behaviors as well as in generating training corpora for learning dialog strategies.", "labels": [], "entities": []}, {"text": "When comparing on a dialog system evaluation task, the simulation trained from more data significantly outperforms the two manual simulations, which again outperforms the simulation trained from less data.", "labels": [], "entities": []}, {"text": "Based on our observations, we answer the original question of how to design user action probabilities for simulations that are similar to ours in terms of the complexity of the simulations 2 . We suggest that handcrafted user simulations can perform reasonably well in building anew dialog system, especially when we are not sure that there is enough data for training simulation models.", "labels": [], "entities": []}, {"text": "However, once we have a dialog system, it is useful to collect human user data in order to train anew user simulation model since the trained simulations perform better than the handcrafted user simulations on more tasks.", "labels": [], "entities": []}, {"text": "Since how to decide whether enough data is available for simulation training is another research question to answer, we will further discuss the impact of our results later in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we introduce the evaluation measures for comparing the simulated corpora generated by different simulation models to the human user corpus.", "labels": [], "entities": []}, {"text": "In Section 4.1, we use a set of widely used domain independent features to compare the simulated and the human user corpora on corpus-level dialog behaviors.", "labels": [], "entities": []}, {"text": "These comparisons give us a direct impression of how similar the simulated dialogs are to human user dialogs.", "labels": [], "entities": []}, {"text": "Then, we compare the simulations in task-oriented contexts.", "labels": [], "entities": []}, {"text": "Since simulated user corpora are often used as training corpora for using MDPs to learn new dialog strategies, in Section 4.2 we estimate how different the learned dialog strategies would be when trained from different simulated corpora.", "labels": [], "entities": []}, {"text": "Another way to use user simulation is to test dialog systems.", "labels": [], "entities": []}, {"text": "Therefore, in Section 4.3, we compare the user actions predicted by the various simulation models with actual human user actions.", "labels": [], "entities": []}, {"text": "In this section, we introduce two ways to compare human user actions with the actions predicted by the simulations.", "labels": [], "entities": []}, {"text": "The aim of this comparison is to assess how accurately the simulations can replicate human user behaviors when encountering the same dialog situation.", "labels": [], "entities": []}, {"text": "A simulated user that can accurately predict human user behaviors is needed to replace human users when evaluating dialog systems.", "labels": [], "entities": []}, {"text": "We randomly divide the human user dialog corpus into four parts: each part contains a balanced amount of high/low learner data.", "labels": [], "entities": []}, {"text": "Then we perform fourfold cross validation by always using 3 parts of the data as our training corpus for user simulations, and the remaining one part of the data as testing data to compare with simulated user actions.", "labels": [], "entities": []}, {"text": "We always compare high human learners only with simulation models that represent high learners and low human learners only with simulation models that represent low learners.", "labels": [], "entities": []}, {"text": "Comparisons are done on a turn by turn basis.", "labels": [], "entities": []}, {"text": "Every time the human user takes an action in the dialogs in the testing data, the user simulations are used to predict an action based on related dialog information from the human user dialog.", "labels": [], "entities": []}, {"text": "For a KC Model, the related dialog information includes qCluster and prevCorrectness . We first compare the simulation predicted user actions directly with human user actions.", "labels": [], "entities": []}, {"text": "We define simulation accuracy as: Correctly predicted human user actions Total number of human user actions However, since our simulation model is a probabilistic model, the model will take an action stochastically after the same tutor turn.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9848662614822388}]}, {"text": "In other words, we need to take into account the probability for the simulation to predict the right human user action.", "labels": [], "entities": []}, {"text": "If the simulation outputs the right action with a small probability, it is less likely that this simulation can correctly predict human user behaviors when generating a large dialog corpus.", "labels": [], "entities": []}, {"text": "We consider a simulated action associated with a higher probability to be ranked higher than an action with a lower probability.", "labels": [], "entities": []}, {"text": "Then, we use the reciprocal ranking from information retrieval tasks () to assess the simulation performance . Mean Reciprocal Ranking is defined as: In Equation 6, A stands for the total number of human user actions, rank i stands for the ranking of the simulated action which matches the i-th human user action.", "labels": [], "entities": [{"text": "Mean Reciprocal Ranking", "start_pos": 111, "end_pos": 134, "type": "METRIC", "confidence": 0.936299463113149}]}, {"text": "shows an example of comparing simulated user actions with human user actions in the sample dialog in.", "labels": [], "entities": []}, {"text": "In the first turn Student1, a simulation model has a 60% chance to output an incorrect answer and a 40% chance to output a correct answer while it actually outputs an incorrect answer.", "labels": [], "entities": [{"text": "Student1", "start_pos": 18, "end_pos": 26, "type": "DATASET", "confidence": 0.9561211466789246}]}, {"text": "In this case, we consider the simulation ranks the actions in the order of: ic, c.", "labels": [], "entities": []}, {"text": "Since the human user gives an incorrect answer at this time, the simulated action matches with this human user action and the reciprocal ranking is 1.", "labels": [], "entities": []}, {"text": "However, in the turn Student2, the simulation's output does not match the human user action.", "labels": [], "entities": [{"text": "Student2", "start_pos": 21, "end_pos": 29, "type": "DATASET", "confidence": 0.9649028778076172}]}, {"text": "This time, the correct simulated user action is ranked second.", "labels": [], "entities": []}, {"text": "Therefore, the reciprocal ranking of this simulation action is 1/2.", "labels": [], "entities": []}, {"text": "We hypothesize that the measures introduced in this section have larger power in differentiating different simulated user behaviors since every simulated user action contributes to the comparison between different simulations.", "labels": [], "entities": []}, {"text": "In contrast, the measures introduced in Section 4.1 and Section 4.2 have less differentiating power since they compare at the corpus level.", "labels": [], "entities": []}, {"text": "Finally, we compare how accurately the user simulations can predict human user actions given the same dialog context.", "labels": [], "entities": []}, {"text": "shows the averages and CIs (in parenthesis) from the fourfold cross validations.", "labels": [], "entities": [{"text": "CIs", "start_pos": 23, "end_pos": 26, "type": "METRIC", "confidence": 0.8877124786376953}]}, {"text": "The second row shows the results based on direct comparisons with human user actions, and the third row shows the mean reciprocal ranking of simulated actions.", "labels": [], "entities": []}, {"text": "We observe that in terms of both the accuracy and the reciprocal ranking, the performance ranking from the highest to the lowest (with significant difference between adjacent ranks) is: the Tmore Model, both of the manual models (no significant differences between these two models), the Tless Model, and the Ran Model.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.9994279742240906}]}, {"text": "Therefore, we suggest that the handcrafted user simulation is not sufficient to be used in evaluating dialog systems because it does not generate user actions that are as similar to human user actions.", "labels": [], "entities": []}, {"text": "However, the handcrafted user simulation is still better than a user simulation trained with not enough training data.", "labels": [], "entities": []}, {"text": "This result also indicates that this evaluation measure has more differentiating power than the previous measures since it captures significant differences that are not shown by the previous measures.", "labels": [], "entities": []}, {"text": "In sum, the Tmore simulation performs the best in predicting human user actions.", "labels": [], "entities": [{"text": "predicting human user actions", "start_pos": 50, "end_pos": 79, "type": "TASK", "confidence": 0.8862855732440948}]}], "tableCaptions": [{"text": " Table 2: An Example of Comparing Simulated Actions with Human User Actions.", "labels": [], "entities": []}, {"text": " Table 3: Comparisons of MDP transition proba- bilities at state (c, lc) (Numbers in this table are  percentages).", "labels": [], "entities": []}, {"text": " Table 4: Comparisons of ECR of learned dialog  strategies.", "labels": [], "entities": []}, {"text": " Table 5: Comparisons of correctly predicted hu- man user actions.", "labels": [], "entities": []}]}