{"title": [{"text": "Bayesian Learning of a Tree Substitution Grammar", "labels": [], "entities": []}], "abstractContent": [{"text": "Tree substitution grammars (TSGs) offer many advantages over context-free grammars (CFGs), but are hard to learn.", "labels": [], "entities": [{"text": "Tree substitution grammars (TSGs)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.8163013607263565}]}, {"text": "Past approaches have resorted to heuris-tics.", "labels": [], "entities": []}, {"text": "In this paper, we learn a TSG using Gibbs sampling with a nonparamet-ric prior to control subtree size.", "labels": [], "entities": []}, {"text": "The learned grammars perform significantly better than heuristically extracted ones on parsing accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 95, "end_pos": 103, "type": "METRIC", "confidence": 0.9409223794937134}]}], "introductionContent": [{"text": "Tree substition grammars (TSGs) have potential advantages over regular context-free grammars (CFGs), but there is no obvious way to learn these grammars.", "labels": [], "entities": [{"text": "Tree substition grammars (TSGs)", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.7906011343002319}]}, {"text": "In particular, learning procedures are notable to take direct advantage of manually annotated corpora like the Penn Treebank, which are not marked for derivations and thus assume a standard CFG.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 111, "end_pos": 124, "type": "DATASET", "confidence": 0.9951967895030975}, {"text": "CFG", "start_pos": 190, "end_pos": 193, "type": "DATASET", "confidence": 0.9442865252494812}]}, {"text": "Since different TSG derivations can produce the same parse tree, learning procedures must guess the derivations, the number of which is exponential in the tree size.", "labels": [], "entities": []}, {"text": "This compels heuristic methods of subtree extraction, or maximum likelihood estimators which tend to extract large subtrees that overfit the training data.", "labels": [], "entities": [{"text": "subtree extraction", "start_pos": 34, "end_pos": 52, "type": "TASK", "confidence": 0.7554275095462799}]}, {"text": "These problems are common in natural language processing tasks that search fora hidden segmentation.", "labels": [], "entities": []}, {"text": "Recently, many groups have had success using Gibbs sampling to address the complexity issue and nonparametric priors to address the overfitting problem (.", "labels": [], "entities": []}, {"text": "In this paper we apply these techniques to learn a tree substitution grammar, evaluate it on the Wall Street Journal parsing task, and compare it to previous work.", "labels": [], "entities": [{"text": "tree substitution grammar", "start_pos": 51, "end_pos": 76, "type": "TASK", "confidence": 0.7884739438692728}, {"text": "Wall Street Journal parsing task", "start_pos": 97, "end_pos": 129, "type": "DATASET", "confidence": 0.8390092253684998}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Labeled precision, recall, and F 1 on  WSJ \u00a723.", "labels": [], "entities": [{"text": "Labeled", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.8739684820175171}, {"text": "precision", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.9389091730117798}, {"text": "recall", "start_pos": 29, "end_pos": 35, "type": "METRIC", "confidence": 0.9996294975280762}, {"text": "F 1", "start_pos": 41, "end_pos": 44, "type": "METRIC", "confidence": 0.9925603866577148}, {"text": "WSJ \u00a723", "start_pos": 49, "end_pos": 56, "type": "DATASET", "confidence": 0.9555462797482809}]}]}