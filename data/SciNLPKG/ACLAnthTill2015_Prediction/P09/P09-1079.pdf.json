{"title": [{"text": "Mine the Easy, Classify the Hard: A Semi-Supervised Approach to Automatic Sentiment Classification", "labels": [], "entities": [{"text": "Automatic Sentiment Classification", "start_pos": 64, "end_pos": 98, "type": "TASK", "confidence": 0.715870181719462}]}], "abstractContent": [{"text": "Supervised polarity classification systems are typically domain-specific.", "labels": [], "entities": [{"text": "polarity classification", "start_pos": 11, "end_pos": 34, "type": "TASK", "confidence": 0.7387557029724121}]}, {"text": "Building these systems involves the expensive process of annotating a large amount of data for each domain.", "labels": [], "entities": []}, {"text": "A potential solution to this corpus annotation bottleneck is to build unsupervised polarity classification systems.", "labels": [], "entities": []}, {"text": "However, unsupervised learning of polarity is difficult, owing in part to the prevalence of sentimentally ambiguous reviews , where reviewers discuss both the positive and negative aspects of a product.", "labels": [], "entities": []}, {"text": "To address this problem, we propose a semi-supervised approach to sentiment classification where we first mine the unambiguous reviews using spectral techniques and then exploit them to classify the ambiguous reviews via a novel combination of active learning, transductive learning, and ensemble learning.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 66, "end_pos": 90, "type": "TASK", "confidence": 0.9617952406406403}]}], "introductionContent": [{"text": "Sentiment analysis has recently received a lot of attention in the Natural Language Processing (NLP) community.", "labels": [], "entities": [{"text": "Sentiment analysis", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9706583321094513}]}, {"text": "Polarity classification, whose goal is to determine whether the sentiment expressed in a document is \"thumbs up\" or \"thumbs down\", is arguably one of the most popular tasks in document-level sentiment analysis.", "labels": [], "entities": [{"text": "Polarity classification", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.6774671226739883}, {"text": "document-level sentiment analysis", "start_pos": 176, "end_pos": 209, "type": "TASK", "confidence": 0.7266353070735931}]}, {"text": "Unlike topic-based text classification, where a high accuracy can be achieved even for datasets with a large number of classes (e.g., 20 Newsgroups), polarity classification appears to be a more difficult task.", "labels": [], "entities": [{"text": "topic-based text classification", "start_pos": 7, "end_pos": 38, "type": "TASK", "confidence": 0.6308835446834564}, {"text": "accuracy", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.9973589777946472}, {"text": "polarity classification", "start_pos": 150, "end_pos": 173, "type": "TASK", "confidence": 0.7322468012571335}]}, {"text": "One reason topic-based text classification is easier than polarity classification is that topic clusters are typically well-separated from each other, resulting from the fact that word usage differs considerably between two topically-different documents.", "labels": [], "entities": [{"text": "topic-based text classification", "start_pos": 11, "end_pos": 42, "type": "TASK", "confidence": 0.624811053276062}, {"text": "polarity classification", "start_pos": 58, "end_pos": 81, "type": "TASK", "confidence": 0.7601994574069977}]}, {"text": "On the other hand, many reviews are sentimentally ambiguous fora variety of reasons.", "labels": [], "entities": []}, {"text": "For instance, an author of a movie review may have negative opinions of the actors but at the same time talk enthusiastically about how much she enjoyed the plot.", "labels": [], "entities": []}, {"text": "Here, the review is ambiguous because she discussed both the positive and negative aspects of the movie, which is not uncommon in reviews.", "labels": [], "entities": []}, {"text": "As another example, a large portion of a movie review maybe devoted exclusively to the plot, with the author only briefly expressing her sentiment at the end of the review.", "labels": [], "entities": []}, {"text": "In this case, the review is ambiguous because the objective material in the review, which bears no sentiment orientation, significantly outnumbers its subjective counterpart.", "labels": [], "entities": []}, {"text": "Realizing the challenges posed by ambiguous reviews, researchers have explored a number of techniques to improve supervised polarity classifiers.", "labels": [], "entities": []}, {"text": "For instance, train an independent subjectivity classifier to identify and remove objective sentences from a review prior to polarity classification.", "labels": [], "entities": [{"text": "polarity classification", "start_pos": 125, "end_pos": 148, "type": "TASK", "confidence": 0.6855672299861908}]}, {"text": "use neutral reviews to help improve the classification of positive and negative reviews.", "labels": [], "entities": []}, {"text": "More recently, have investigated a model for jointly performing sentence-and document-level sentiment analysis, allowing the relationship between the two tasks to be captured and exploited.", "labels": [], "entities": [{"text": "sentence-and document-level sentiment analysis", "start_pos": 64, "end_pos": 110, "type": "TASK", "confidence": 0.6614239364862442}]}, {"text": "However, the increased sophistication of supervised polarity classifiers has also resulted in their increased dependence on annotated data.", "labels": [], "entities": []}, {"text": "For instance, Koppel and Schler needed to manually identify neutral reviews to train their polarity classifier, and McDonald et al.'s joint model requires that each sentence in a review be labeled with polarity information.", "labels": [], "entities": []}, {"text": "Given the difficulties of supervised polarity classification, it is conceivable that unsupervised polarity classification is a very challenging task.", "labels": [], "entities": [{"text": "supervised polarity classification", "start_pos": 26, "end_pos": 60, "type": "TASK", "confidence": 0.6836993594964346}, {"text": "unsupervised polarity classification", "start_pos": 85, "end_pos": 121, "type": "TASK", "confidence": 0.7609915733337402}]}, {"text": "Nevertheless, a solution to unsupervised polarity classification is of practical significance.", "labels": [], "entities": [{"text": "polarity classification", "start_pos": 41, "end_pos": 64, "type": "TASK", "confidence": 0.8391527533531189}]}, {"text": "One reason is that the vast majority of supervised polarity classification systems are domain-specific.", "labels": [], "entities": [{"text": "supervised polarity classification", "start_pos": 40, "end_pos": 74, "type": "TASK", "confidence": 0.6554780503114065}]}, {"text": "Hence, when given anew domain, a large amount of annotated data from the domain typically needs to be collected in order to train a high-performance polarity classification system.", "labels": [], "entities": []}, {"text": "As point out, this data collection process can be \"prohibitively expensive, especially since product features can changeover time\".", "labels": [], "entities": [{"text": "data collection", "start_pos": 19, "end_pos": 34, "type": "TASK", "confidence": 0.7589830458164215}]}, {"text": "Unfortunately, to our knowledge, unsupervised polarity classification is largely an under-investigated task in NLP.", "labels": [], "entities": [{"text": "polarity classification", "start_pos": 46, "end_pos": 69, "type": "TASK", "confidence": 0.7154206931591034}]}, {"text": "work is perhaps one of the most notable examples of unsupervised polarity classification.", "labels": [], "entities": [{"text": "unsupervised polarity classification", "start_pos": 52, "end_pos": 88, "type": "TASK", "confidence": 0.6760526597499847}]}, {"text": "However, while his system learns the semantic orientation of phrases in a review in an unsupervised manner, such information is used to heuristically predict the polarity of a review.", "labels": [], "entities": []}, {"text": "At first glance, it may seem plausible to apply an unsupervised clustering algorithm such as kmeans to cluster the reviews according to their polarity.", "labels": [], "entities": []}, {"text": "However, there is reason to believe that such a clustering approach is doomed to fail: in the absence of annotated data, an unsupervised learner is unable to identify which features are relevant for polarity classification.", "labels": [], "entities": [{"text": "polarity classification", "start_pos": 199, "end_pos": 222, "type": "TASK", "confidence": 0.7612513899803162}]}, {"text": "The situation is further complicated by the prevalence of ambiguous reviews, which may contain a large amount of irrelevant and/or contradictory information.", "labels": [], "entities": []}, {"text": "In light of the difficulties posed by ambiguous reviews, we differentiate between ambiguous and unambiguous reviews in our classification process by addressing the task of semi-supervised polarity classification via a \"mine the easy, classify the hard\" approach.", "labels": [], "entities": [{"text": "semi-supervised polarity classification", "start_pos": 172, "end_pos": 211, "type": "TASK", "confidence": 0.7186960577964783}]}, {"text": "Specifically, we propose a novel system architecture where we first automatically identify and label the unambiguous (i.e., \"easy\") reviews, then handle the ambiguous (i.e., \"hard\") reviews using a discriminative learner to bootstrap from the automatically labeled unambiguous reviews and a small number of manually labeled reviews that are identified by an active learner.", "labels": [], "entities": []}, {"text": "It is worth noting that our system differs from existing work on unsupervised/active learning in two aspects.", "labels": [], "entities": []}, {"text": "First, while existing unsupervised approaches typically rely on clustering or learning via a generative model, our approach distinguishes between easy and hard instances and exploits the strengths of discriminative models to classify the hard instances.", "labels": [], "entities": []}, {"text": "Second, while existing active learners typically start with manually labeled seeds, our active learner relies only on seeds that are automatically extracted from the data.", "labels": [], "entities": []}, {"text": "Experimental results on five sentiment classification datasets demonstrate that our system can generate high-quality labeled data from unambiguous reviews, which, together with a small number of manually labeled reviews selected by the active learner, can be used to effectively classify ambiguous reviews in a discriminative fashion.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 29, "end_pos": 53, "type": "TASK", "confidence": 0.8458984196186066}]}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 gives an overview of spectral clustering, which will facilitate the presentation of our approach to unsupervised sentiment classification in Section 3.", "labels": [], "entities": [{"text": "spectral clustering", "start_pos": 31, "end_pos": 50, "type": "TASK", "confidence": 0.7219381779432297}, {"text": "sentiment classification", "start_pos": 123, "end_pos": 147, "type": "TASK", "confidence": 0.7846607565879822}]}, {"text": "We evaluate our approach in Section 4 and present our conclusions in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "Iterative: Seed accuracies on five datasets.", "labels": [], "entities": []}, {"text": "they are not necessarily relevant for creating polarity clusters.", "labels": [], "entities": []}, {"text": "In fact, owing to the absence of labeled data, unsupervised clustering algorithms are unable to distinguish between useful and irrelevant features for polarity classification.", "labels": [], "entities": [{"text": "polarity classification", "start_pos": 151, "end_pos": 174, "type": "TASK", "confidence": 0.7703293561935425}]}, {"text": "Nevertheless, being able to distinguish between relevant and irrelevant information is important for polarity classification, as discussed before.", "labels": [], "entities": [{"text": "polarity classification", "start_pos": 101, "end_pos": 124, "type": "TASK", "confidence": 0.9274842143058777}]}, {"text": "Now that we have a small, high-quality seed set, we can potentially make better use of the available features by training a discriminative classifier on the seed set and having it identify the relevant and irrelevant features for polarity classification.", "labels": [], "entities": [{"text": "polarity classification", "start_pos": 230, "end_pos": 253, "type": "TASK", "confidence": 0.7931184470653534}]}, {"text": "Despite the high quality of the seed set, the resulting classifier may not perform well when applied to the remaining (unlabeled) points, as there is no reason to believe that a classifier trained solely on unambiguous reviews can achieve a high accuracy when classifying ambiguous reviews.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 246, "end_pos": 254, "type": "METRIC", "confidence": 0.9969017505645752}]}, {"text": "We hypothesize that a high accuracy can be achieved only if the classifier is trained on both ambiguous and unambiguous reviews.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9989138841629028}]}, {"text": "As a result, we apply active learning to identify the ambiguous reviews.", "labels": [], "entities": []}, {"text": "Specifically, we train a discriminative classifier using the support vector machine (SVM) learning algorithm) on the set of unambiguous reviews, and then apply the resulting classifier to all the reviews in the training folds 4 that are not seeds.", "labels": [], "entities": []}, {"text": "Since this classifier is trained solely on the unambiguous reviews, it is reasonable to assume that the reviews whose labels the classifier is most uncertain about (and therefore are most informative to the classifier) are those that are ambiguous.", "labels": [], "entities": []}, {"text": "Following previous work on active learning for SVMs (e.g.,,,), we define the uncertainty of a data point as its distance from the separating hyperplane.", "labels": [], "entities": [{"text": "SVMs", "start_pos": 47, "end_pos": 51, "type": "TASK", "confidence": 0.9266251921653748}]}, {"text": "In other words, points that are closer to the hyperplane are more uncertain than those that are farther away.", "labels": [], "entities": []}, {"text": "We perform active learning for five iterations.", "labels": [], "entities": []}, {"text": "In each iteration, we select the 10 most uncertain points from each side of the hyperplane for human annotation, and then re-train a classifier on all of the points annotated so far.", "labels": [], "entities": []}, {"text": "This yields a total of 100 manually labeled reviews.", "labels": [], "entities": []}, {"text": "For evaluation, we use five sentiment classification datasets, including the widely-used movie review dataset () as well as four datasets that contain reviews of four different types of product from Amazon [books (BOO), DVDs (DVD), electronics (ELE), and kitchen appliances (KIT)] (.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 28, "end_pos": 52, "type": "TASK", "confidence": 0.8731961846351624}, {"text": "movie review dataset", "start_pos": 89, "end_pos": 109, "type": "DATASET", "confidence": 0.5925701260566711}]}, {"text": "Each dataset has 2000 labeled reviews (1000 positives and 1000 negatives).", "labels": [], "entities": []}, {"text": "We divide the 2000 reviews into 10 equal-sized folds for cross-validation purposes, maintaining balanced class distributions in each fold.", "labels": [], "entities": []}, {"text": "It is important to note that while the test fold is accessible to the transductive learner (Step 3), only the reviews in training folds (but not their labels) are used for the acquisition of seeds (Step 1) and the selection of active learning points (Step 2).", "labels": [], "entities": []}, {"text": "We report averaged 10-fold cross-validation results in terms of accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.9994437098503113}]}, {"text": "Following, we also evaluate the clusters produced by our approach against the gold-standard clusters using Adjusted Rand Index (ARI).", "labels": [], "entities": [{"text": "Adjusted Rand Index (ARI)", "start_pos": 107, "end_pos": 132, "type": "METRIC", "confidence": 0.8871967295805613}]}, {"text": "ARI ranges from \u22121 to 1; better clusterings have higher ARI values.", "labels": [], "entities": [{"text": "ARI", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.9207404851913452}, {"text": "ARI", "start_pos": 56, "end_pos": 59, "type": "METRIC", "confidence": 0.9859273433685303}]}, {"text": "To gain insight into how the design decisions we made in our approach impact performance, we conducted the following additional experiments.", "labels": [], "entities": []}, {"text": "showed that for all but one dataset, the seeds obtained through multiple iterations are more accurate than those obtained in a single iteration.", "labels": [], "entities": []}, {"text": "To envisage the importance of seeds, we conducted an experiment where we repeated our approach using the seeds learned in a single iteration.", "labels": [], "entities": []}, {"text": "Results are shown in the first row of, we can see that results are indeed better when we bootstrap from higher-quality seeds.", "labels": [], "entities": []}, {"text": "To further understand the role of seeds, we experimented with aversion of our approach that bootstraps from no seeds.", "labels": [], "entities": []}, {"text": "Specifically, we used the 500 seeds to guide the selection of active learning points, but trained a transductive SVM using only the active learning points as labeled data (and the rest as unlabeled data).", "labels": [], "entities": []}, {"text": "As can be seen in row 2 of, the results are poor, suggesting that our approach yields better performance than the baselines not only because of the way the active learning points were chosen, but also because of contributions from the imperfectly labeled seeds.", "labels": [], "entities": []}, {"text": "We also experimented with training a transductive SVM using only the 100 least ambiguous seeds (i.e., the points with the largest unsigned  second eigenvector values) in combination with the active learning points as labeled data (and the rest as unlabeled data).", "labels": [], "entities": []}, {"text": "Note that the accuracy of these 100 least ambiguous seeds is 4-5% higher than that of the 500 least ambiguous seeds shown in.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9996398687362671}]}, {"text": "Results are shown in row 3 of.", "labels": [], "entities": []}, {"text": "As we can see, using only 100 seeds turns out to be less beneficial than using all of them via an ensemble.", "labels": [], "entities": []}, {"text": "One reason is that since these 100 seeds are the most unambiguous, they may also be the least informative as far as learning is concerned.", "labels": [], "entities": []}, {"text": "Remember that SVM uses only the support vectors to acquire the hyperplane, and since an unambiguous seed is likely to be faraway from the hyperplane, it is less likely to be a support vector.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Seed accuracies on five datasets.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 15, "end_pos": 25, "type": "METRIC", "confidence": 0.8470302224159241}]}, {"text": " Table 2: Results in terms of accuracy and Adjusted Rand Index for the five datasets.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9996064305305481}, {"text": "Adjusted Rand Index", "start_pos": 43, "end_pos": 62, "type": "METRIC", "confidence": 0.8636988600095113}]}, {"text": " Table 3: Additional results in terms of accuracy and Adjusted Rand Index for the five datasets.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.9996176958084106}, {"text": "Adjusted Rand Index", "start_pos": 54, "end_pos": 73, "type": "METRIC", "confidence": 0.8719541231791178}]}]}