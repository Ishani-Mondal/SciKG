{"title": [{"text": "Paraphrase Recognition Using Machine Learning to Combine Similarity Measures", "labels": [], "entities": [{"text": "Paraphrase Recognition", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9179772436618805}, {"text": "Similarity Measures", "start_pos": 57, "end_pos": 76, "type": "TASK", "confidence": 0.7944279909133911}]}], "abstractContent": [{"text": "This paper presents three methods that can be used to recognize paraphrases.", "labels": [], "entities": []}, {"text": "They all employ string similarity measures applied to shallow abstractions of the input sentences, and a Maximum Entropy clas-sifier to learn how to combine the resulting features.", "labels": [], "entities": []}, {"text": "Two of the methods also exploit WordNet to detect synonyms and one of them also exploits a dependency parser.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 32, "end_pos": 39, "type": "DATASET", "confidence": 0.9556629657745361}]}, {"text": "We experiment on two datasets, the MSR paraphrasing corpus and a dataset that we automatically created from the MTC corpus.", "labels": [], "entities": [{"text": "MSR paraphrasing corpus", "start_pos": 35, "end_pos": 58, "type": "DATASET", "confidence": 0.6840639313062032}, {"text": "MTC corpus", "start_pos": 112, "end_pos": 122, "type": "DATASET", "confidence": 0.9015139043331146}]}, {"text": "Our system achieves state of the art or better results.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recognizing or generating semantically equivalent phrases is of significant importance in many natural language applications.", "labels": [], "entities": [{"text": "Recognizing or generating semantically equivalent phrases", "start_pos": 0, "end_pos": 57, "type": "TASK", "confidence": 0.8018267055352529}]}, {"text": "In question answering, for example, a question maybe phrased differently than in a document collection (e.g., \"Who is the author of War and Peace?\" vs. \"Leo Tolstoy is the writer of War and Peace.\"), and taking such variations into account can improve system performance significantly ().", "labels": [], "entities": [{"text": "question answering", "start_pos": 3, "end_pos": 21, "type": "TASK", "confidence": 0.8039378225803375}]}, {"text": "A paraphrase generator, meaning a module that produces new phrases or patterns that are semantically equivalent (or almost equivalant) to a given input phrase or pattern (e.g., \"X is the writer of Y \" \u21d4 \"X wrote Y \" \u21d4 \"Y was written by X\" \u21d4 \"X is the author of Y \", or \"X produces Y \" \u21d4 \"X manufactures Y \" \u21d4 \"X is the manufacturer of Y \") can be used to produce alternative phrasings of the question, before matching it against a document collection.", "labels": [], "entities": []}, {"text": "Unlike paraphrase generators, paraphrase recognizers decide whether or not two given phrases (or patterns) are paraphrases, possibly by generalizing over many different training pairs of phrases.", "labels": [], "entities": [{"text": "paraphrase recognizers", "start_pos": 30, "end_pos": 52, "type": "TASK", "confidence": 0.8029078543186188}]}, {"text": "Paraphrase recognizers can be embedded in paraphrase generators to filter out erroneous generated paraphrases; but they are also useful on their own.", "labels": [], "entities": [{"text": "Paraphrase recognizers", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8999345302581787}]}, {"text": "In question answering, for example, they can be used to check if a pattern extracted from the question (possibly by replacing named entities by their semantic categories and turning the question into a statement) matches any patterns extracted from candidate answers.", "labels": [], "entities": [{"text": "question answering", "start_pos": 3, "end_pos": 21, "type": "TASK", "confidence": 0.8759151697158813}]}, {"text": "As a further example, in text summarization, especially multi-document summarization, a paraphrase recognizer can be used to check if a sentence is a paraphrase of any other sentence already present in a partially constructed summary.", "labels": [], "entities": [{"text": "text summarization", "start_pos": 25, "end_pos": 43, "type": "TASK", "confidence": 0.6958794295787811}, {"text": "multi-document summarization", "start_pos": 56, "end_pos": 84, "type": "TASK", "confidence": 0.7369458675384521}]}, {"text": "Note that, although \"paraphrasing\" and \"textual entailment\" are sometimes used as synonyms, we use the former to refer to methods that generate or recognize semantically equivalent (or almost equivalent) phrases or patterns, whereas in textual entailment ( ;;) the expressions or patterns are not necessarily semantically equivalent; it suffices if one entails the other, even if the reverse direction does not hold.", "labels": [], "entities": []}, {"text": "For example, \"Y was written by X\" textually entails \"Y is the work of X\", but the reverse direction does not necessarily hold (e.g., if Y is a statue); hence, the two sentences are not paraphrases.", "labels": [], "entities": []}, {"text": "In this paper, we focus on paraphrase recognition.", "labels": [], "entities": [{"text": "paraphrase recognition", "start_pos": 27, "end_pos": 49, "type": "TASK", "confidence": 0.9702147841453552}]}, {"text": "We propose three methods that employ string similarity measures, which are applied to several abstractions of a pair of input phrases (e.g., the phrases themselves, their stems, POS tags).", "labels": [], "entities": []}, {"text": "The scores returned by the similarity measures are used as features in a Maximum Entropy (ME) classifier, which learns to separate true paraphrase pairs from false ones.", "labels": [], "entities": []}, {"text": "Two of our methods also exploit WordNet to detect synonyms, and one of them uses additional features to measure similarities of grammatical relations obtained by a dependency parser.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 32, "end_pos": 39, "type": "DATASET", "confidence": 0.9558287262916565}]}, {"text": "1 Our experiments were conducted on two datasets: the publicly available Microsoft Research Paraphrasing corpus () and a dataset that we constructed from the MTC corpus.", "labels": [], "entities": [{"text": "Microsoft Research Paraphrasing corpus", "start_pos": 73, "end_pos": 111, "type": "DATASET", "confidence": 0.9355409890413284}, {"text": "MTC corpus", "start_pos": 158, "end_pos": 168, "type": "DATASET", "confidence": 0.947940319776535}]}, {"text": "The experimental results show that our methods perform very well.", "labels": [], "entities": []}, {"text": "Even the simplest one manages to achieve state of the art results, even though it uses fewer linguistic resources than other reported systems.", "labels": [], "entities": []}, {"text": "The other two, more elaborate methods perform even better.", "labels": [], "entities": []}, {"text": "Section 2 presents the three methods, and section 3 our experiments.", "labels": [], "entities": []}, {"text": "Section 4 covers related work.", "labels": [], "entities": []}, {"text": "Section 5 concludes and proposes further work.", "labels": [], "entities": []}], "datasetContent": [{"text": "We now present our experiments, starting from a description of the datasets used.", "labels": [], "entities": []}, {"text": "We mainly used the Microsoft Research (MSR) Paraphrasing Corpus (), which consists of 5,801 pairs of sentences.", "labels": [], "entities": [{"text": "Microsoft Research (MSR) Paraphrasing Corpus", "start_pos": 19, "end_pos": 63, "type": "DATASET", "confidence": 0.7959234288760594}]}, {"text": "Each pair is manually annotated by two human judges as a true or false paraphrase; a third judge resolved disagreements.", "labels": [], "entities": []}, {"text": "The data are split into 4,076 training pairs and 1,725 testing pairs.", "labels": [], "entities": []}, {"text": "We have experimented with a dataset we created from the MTC corpus.", "labels": [], "entities": [{"text": "MTC corpus", "start_pos": 56, "end_pos": 66, "type": "DATASET", "confidence": 0.8913886547088623}]}, {"text": "MTC is a corpus containing news articles in Mandarin Chinese; for each article 11 English translations (by different translators) are also provided.", "labels": [], "entities": [{"text": "MTC", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9173299074172974}]}, {"text": "We considered the translations of the same Chinese sentence as paraphrases.", "labels": [], "entities": []}, {"text": "We obtained all the possible paraphrase pairs and we added an equal number of randomly selected non paraphrase pairs, which contained sentences that were not translations of the same sentence.", "labels": [], "entities": []}, {"text": "In this way, we constructed a dataset containing 82,260 pairs of sentences.", "labels": [], "entities": []}, {"text": "The dataset was then split in training (70%) and test (30%) parts, with an equal number of positive and negative pairs in each part.", "labels": [], "entities": []}, {"text": "We used four evaluation measures, namely accuracy (correctly classified pairs overall pairs), precision (P , pairs correctly classified in the positive class overall pairs classified in the positive class), recall (R, pairs correctly classified in the positive class overall true positive pairs), and F -measure (with equal weight on precision and recall, defined as 2\u00b7P \u00b7R P +R ).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.9993138313293457}, {"text": "precision", "start_pos": 94, "end_pos": 103, "type": "METRIC", "confidence": 0.9995456337928772}, {"text": "recall", "start_pos": 207, "end_pos": 213, "type": "METRIC", "confidence": 0.9991900324821472}, {"text": "F -measure", "start_pos": 301, "end_pos": 311, "type": "METRIC", "confidence": 0.9939718246459961}, {"text": "precision", "start_pos": 334, "end_pos": 343, "type": "METRIC", "confidence": 0.9977505803108215}, {"text": "recall", "start_pos": 348, "end_pos": 354, "type": "METRIC", "confidence": 0.9969821572303772}]}, {"text": "These measures are not to be confused with the R 1 , R 2 , and F R 1 ,R 2 of section 2.3 which are used as features.", "labels": [], "entities": [{"text": "F R 1", "start_pos": 63, "end_pos": 68, "type": "METRIC", "confidence": 0.8891750971476237}]}, {"text": "A reasonable baseline method (BASE) is to use just the edit distance similarity measure and a threshold in order to decide whether two phrases are paraphrases or not.", "labels": [], "entities": [{"text": "BASE", "start_pos": 30, "end_pos": 34, "type": "METRIC", "confidence": 0.9941165447235107}, {"text": "edit distance similarity measure", "start_pos": 55, "end_pos": 87, "type": "METRIC", "confidence": 0.6855705827474594}]}, {"text": "The threshold is chosen using a grid search utility and 10-fold cross validation on the training data.", "labels": [], "entities": []}, {"text": "More precisely, in a first step we search the range [-1, 1] with a step of 0.1.", "labels": [], "entities": []}, {"text": "In each step, we perform 10-fold cross validation and the value that achieves the best Fmeasure is our initial threshold, th, for the second step.", "labels": [], "entities": [{"text": "Fmeasure", "start_pos": 87, "end_pos": 95, "type": "METRIC", "confidence": 0.9585344791412354}]}, {"text": "In the second step, we perform the same procedure in the range [th -0.1, th + 0.1] and with a step of 0.001.", "labels": [], "entities": []}, {"text": "With both datasets, we experimented with a Maximum Entropy (ME) classifier.", "labels": [], "entities": [{"text": "Maximum Entropy (ME)", "start_pos": 43, "end_pos": 63, "type": "METRIC", "confidence": 0.7868802428245545}]}, {"text": "However, preliminary results (see (upper part) lists the results of our experiments on the MSR corpus.", "labels": [], "entities": [{"text": "MSR corpus", "start_pos": 91, "end_pos": 101, "type": "DATASET", "confidence": 0.7764904499053955}]}, {"text": "We optionally performed feature selection with both forward hillclimbing (FHC) and forward beam search (FBS).", "labels": [], "entities": [{"text": "feature selection", "start_pos": 24, "end_pos": 41, "type": "TASK", "confidence": 0.6512934416532516}, {"text": "forward beam search (FBS)", "start_pos": 83, "end_pos": 108, "type": "TASK", "confidence": 0.55232106645902}]}, {"text": "All of our methods clearly perform better than BASE.", "labels": [], "entities": [{"text": "BASE", "start_pos": 47, "end_pos": 51, "type": "METRIC", "confidence": 0.9562052488327026}]}, {"text": "As one might expect, there is a lot of redundancy in the complete feature set.", "labels": [], "entities": []}, {"text": "Hence, the two feature selection methods (FHC and FBS) lead to competitive results with much fewer features (7 and 10, respectively, instead of 136).", "labels": [], "entities": [{"text": "FHC", "start_pos": 42, "end_pos": 45, "type": "DATASET", "confidence": 0.8597148060798645}, {"text": "FBS", "start_pos": 50, "end_pos": 53, "type": "DATASET", "confidence": 0.8299669623374939}]}, {"text": "However, feature selection deteriorates performance, especially accuracy, i.e., the full feature set is better, despite its redundancy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.9995754361152649}]}, {"text": "also includes all other reported results for the MSR corpus that we are aware of; we are not aware of the exact number of features used by the other researchers.", "labels": [], "entities": [{"text": "MSR corpus", "start_pos": 49, "end_pos": 59, "type": "DATASET", "confidence": 0.7922464311122894}]}, {"text": "It is noteworthy that INIT achieves state of the art performance, even though the other approaches use many more linguistic resources.", "labels": [], "entities": [{"text": "INIT", "start_pos": 22, "end_pos": 26, "type": "TASK", "confidence": 0.7091194987297058}]}, {"text": "For example, Wan et al.'s approach (), which achieved the best previously reported results, is similar to ours, in that it also trains a classifier with similarity measures; but some of Wan et al.'s measures require a dependency grammar parser, unlike INIT.", "labels": [], "entities": []}, {"text": "More precisely, for each pair of sentences, Wan et al. construct a feature vector with values that measure lexical and dependency similarities.", "labels": [], "entities": []}, {"text": "The measures are: word overlap, length difference (in words),), dependency relation overlap (i.e., R 1 and R 2 but not F R 1 ,R 2 ), and dependency tree edit distance.", "labels": [], "entities": [{"text": "length difference", "start_pos": 32, "end_pos": 49, "type": "METRIC", "confidence": 0.9158194959163666}]}, {"text": "The measures are also applied on sequences containing the lemmatized words of the original sentences, similarly to one of our levels of abstraction.", "labels": [], "entities": []}, {"text": "Interestingly, INIT achieves the same (and slightly better) accuracy as Wan et al.'s system, without employing any parsing.", "labels": [], "entities": [{"text": "INIT", "start_pos": 15, "end_pos": 19, "type": "METRIC", "confidence": 0.7170491814613342}, {"text": "accuracy", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.99906986951828}]}, {"text": "Our more enhanced methods, INIT+WN and INIT+WN+DEP, achieve even better results.", "labels": [], "entities": [{"text": "INIT+WN+DEP", "start_pos": 39, "end_pos": 50, "type": "METRIC", "confidence": 0.6023292005062103}]}, {"text": "use a dependency grammar parser to convert passive voice phrases to active voice ones.", "labels": [], "entities": []}, {"text": "They also use a preprocessing stage to generalize the pairs of sentences.", "labels": [], "entities": []}, {"text": "The preprocessing replaces dates, times, percentages, etc. with generic tags, something that we have also done in the MSR corpus, but it also replaces words and phrases indicating future actions (e.g., \"plans to\", \"be expected to\") with the word \"will\"; the latter is an example of further preprocessing that could be added to our system.", "labels": [], "entities": [{"text": "MSR corpus", "start_pos": 118, "end_pos": 128, "type": "DATASET", "confidence": 0.8142109215259552}]}, {"text": "After the preprocessing, Zhang and Patrick create for each sentence pair a feature vector whose values measure the lexical similarity between the two sentences; they appear to be using the maximum number of consecutive common words, the number of common words, edit distance (in words), and modified n-gram precision, a measure similar to BLEU.", "labels": [], "entities": [{"text": "precision", "start_pos": 307, "end_pos": 316, "type": "METRIC", "confidence": 0.961932897567749}, {"text": "BLEU", "start_pos": 339, "end_pos": 343, "type": "METRIC", "confidence": 0.9957838654518127}]}, {"text": "The produced vectors are then used to train a decision tree classifier.", "labels": [], "entities": [{"text": "decision tree classifier", "start_pos": 46, "end_pos": 70, "type": "TASK", "confidence": 0.6722554167111715}]}, {"text": "Hence, Zhang and Patrick's approach is similar to ours, but we use more and different similarity measures and several levels of abstraction of the two sentences.", "labels": [], "entities": []}, {"text": "We also use ME, along with a wrapper approach to feature selection, rather than decision tree induction and its embedded information gain-based feature selection.", "labels": [], "entities": [{"text": "decision tree induction", "start_pos": 80, "end_pos": 103, "type": "TASK", "confidence": 0.7145580450693766}]}, {"text": "Furthermore, all of our methods, even INIT which employs no parsing at all, achieve better results compared to Zhang and Patrick's. first convert the sentences into tuples using parsing and semantic role labeling.", "labels": [], "entities": [{"text": "INIT", "start_pos": 38, "end_pos": 42, "type": "METRIC", "confidence": 0.7078010439872742}, {"text": "semantic role labeling", "start_pos": 190, "end_pos": 212, "type": "TASK", "confidence": 0.6319448053836823}]}, {"text": "They then match similar tuples across the two sentences, and use an SVM) classifier to decide whether or not the tuples that have not been matched are important or not.", "labels": [], "entities": []}, {"text": "If not, the sentences are paraphrases.", "labels": [], "entities": []}, {"text": "Despite using a parser and a semantic role identifier, Qiu et al.'s system performs worse than our methods.", "labels": [], "entities": []}, {"text": "Finally, Finch et al.'s system (2005) achieved the second best overall results by employing POS tagging, synonymy resolution, and an SVM.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 92, "end_pos": 103, "type": "TASK", "confidence": 0.7483790218830109}, {"text": "synonymy resolution", "start_pos": 105, "end_pos": 124, "type": "TASK", "confidence": 0.7423253059387207}]}, {"text": "Interestingly, the features of the SVM correspond to machine translation evaluation metrics, rather than string similarity measures, unlike our system.", "labels": [], "entities": [{"text": "machine translation evaluation", "start_pos": 53, "end_pos": 83, "type": "TASK", "confidence": 0.733433206876119}]}, {"text": "We plan to examine further how the features of Finch et al. and other ideas from machine translation can be embedded in our system, although", "labels": [], "entities": [{"text": "machine translation", "start_pos": 81, "end_pos": 100, "type": "TASK", "confidence": 0.7167894393205643}]}], "tableCaptions": [{"text": " Table 1: Results (%) of our methods on our MTC dataset.", "labels": [], "entities": [{"text": "MTC dataset", "start_pos": 44, "end_pos": 55, "type": "DATASET", "confidence": 0.8965622782707214}]}, {"text": " Table 2: Results (%) of our methods (upper part) and other methods (lower part) on the MSR corpus.", "labels": [], "entities": [{"text": "MSR corpus", "start_pos": 88, "end_pos": 98, "type": "DATASET", "confidence": 0.7939626276493073}]}]}