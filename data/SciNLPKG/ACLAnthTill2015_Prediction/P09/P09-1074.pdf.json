{"title": [{"text": "Conundrums in Noun Phrase Coreference Resolution: Making Sense of the State-of-the-Art", "labels": [], "entities": [{"text": "Noun Phrase Coreference Resolution", "start_pos": 14, "end_pos": 48, "type": "TASK", "confidence": 0.7444390282034874}]}], "abstractContent": [{"text": "We aim to shed light on the state-of-the-art in NP coreference resolution by teasing apart the differences in the MUC and ACE task definitions, the assumptions made in evaluation methodologies, and inherent differences in text corpora.", "labels": [], "entities": [{"text": "NP coreference resolution", "start_pos": 48, "end_pos": 73, "type": "TASK", "confidence": 0.9577372471491495}, {"text": "MUC and ACE task definitions", "start_pos": 114, "end_pos": 142, "type": "DATASET", "confidence": 0.7364084482192993}]}, {"text": "First, we examine three subproblems that play a role in coreference resolution: named entity recognition, anaphoric-ity determination, and coreference element detection.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 56, "end_pos": 78, "type": "TASK", "confidence": 0.9685817956924438}, {"text": "named entity recognition", "start_pos": 80, "end_pos": 104, "type": "TASK", "confidence": 0.6392251948515574}, {"text": "anaphoric-ity determination", "start_pos": 106, "end_pos": 133, "type": "TASK", "confidence": 0.6573628038167953}, {"text": "coreference element detection", "start_pos": 139, "end_pos": 168, "type": "TASK", "confidence": 0.8771010239919027}]}, {"text": "We measure the impact of each subproblem on coreference resolution and confirm that certain assumptions regarding these subproblems in the evaluation methodology can dramatically simplify the overall task.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 44, "end_pos": 66, "type": "TASK", "confidence": 0.9775382280349731}]}, {"text": "Second, we measure the performance of a state-of-the-art coreference resolver on several classes of anaphora and use these results to develop a quantitative measure for estimating coreference resolution performance on new data sets.", "labels": [], "entities": [{"text": "coreference resolver", "start_pos": 57, "end_pos": 77, "type": "TASK", "confidence": 0.8890863060951233}, {"text": "coreference resolution", "start_pos": 180, "end_pos": 202, "type": "TASK", "confidence": 0.8017729818820953}]}], "introductionContent": [{"text": "As is common for many natural language processing problems, the state-of-the-art in noun phrase (NP) coreference resolution is typically quantified based on system performance on manually annotated text corpora.", "labels": [], "entities": [{"text": "noun phrase (NP) coreference resolution", "start_pos": 84, "end_pos": 123, "type": "TASK", "confidence": 0.6704325675964355}]}, {"text": "In spite of the availability of several benchmark data sets (e.g., ACE NIST) and their use in many formal evaluations, as afield we can make surprisingly few conclusive statements about the state-of-theart in NP coreference resolution.", "labels": [], "entities": [{"text": "ACE NIST", "start_pos": 67, "end_pos": 75, "type": "DATASET", "confidence": 0.8776255249977112}, {"text": "NP coreference resolution", "start_pos": 209, "end_pos": 234, "type": "TASK", "confidence": 0.8548006415367126}]}, {"text": "In particular, it remains difficult to assess the effectiveness of different coreference resolution approaches, even in relative terms.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 77, "end_pos": 99, "type": "TASK", "confidence": 0.9434463679790497}]}, {"text": "For example, the 91.5 F-measure reported by was produced by a system using perfect information for several linguistic subproblems.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 22, "end_pos": 31, "type": "METRIC", "confidence": 0.824202835559845}]}, {"text": "In contrast, the 71.3 F-measure reported by represents a fully automatic end-to-end resolver.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 22, "end_pos": 31, "type": "METRIC", "confidence": 0.918277382850647}]}, {"text": "It is impossible to assess which approach truly performs best because of the dramatically different assumptions of each evaluation.", "labels": [], "entities": []}, {"text": "Results vary widely across data sets.", "labels": [], "entities": []}, {"text": "Coreference resolution scores range from 85-90% on the ACE 2004 and 2005 data sets to a much lower 60-70% on the MUC 6 and 7 data sets (e.g. and).", "labels": [], "entities": [{"text": "Coreference resolution", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.5771477222442627}, {"text": "ACE 2004 and 2005 data sets", "start_pos": 55, "end_pos": 82, "type": "DATASET", "confidence": 0.9727364083131155}, {"text": "MUC 6 and 7 data sets", "start_pos": 113, "end_pos": 134, "type": "DATASET", "confidence": 0.971806933482488}]}, {"text": "What accounts for these differences?", "labels": [], "entities": []}, {"text": "Are they due to properties of the documents or domains?", "labels": [], "entities": []}, {"text": "Or do differences in the coreference task definitions account for the differences in performance?", "labels": [], "entities": []}, {"text": "Given anew text collection and domain, what level of performance should we expect?", "labels": [], "entities": [{"text": "text collection", "start_pos": 11, "end_pos": 26, "type": "TASK", "confidence": 0.7000469416379929}]}, {"text": "We have little understanding of which aspects of the coreference resolution problem are handled well or poorly by state-of-the-art systems.", "labels": [], "entities": [{"text": "coreference resolution problem", "start_pos": 53, "end_pos": 83, "type": "TASK", "confidence": 0.9577313860257467}]}, {"text": "Except for some fairly general statements, for example that proper names are easier to resolve than pronouns, which are easier than common nouns, there has been little analysis of which aspects of the problem have achieved success and which remain elusive.", "labels": [], "entities": []}, {"text": "The goal of this paper is to take initial steps toward making sense of the disparate performance results reported for NP coreference resolution.", "labels": [], "entities": [{"text": "NP coreference resolution", "start_pos": 118, "end_pos": 143, "type": "TASK", "confidence": 0.9157780806223551}]}, {"text": "For our investigations, we employ a state-of-the-art classification-based NP coreference resolver and focus on the widely used MUC and ACE coreference resolution data sets.", "labels": [], "entities": [{"text": "NP coreference resolver", "start_pos": 74, "end_pos": 97, "type": "TASK", "confidence": 0.6490787963072459}, {"text": "MUC", "start_pos": 127, "end_pos": 130, "type": "DATASET", "confidence": 0.7923574447631836}, {"text": "ACE coreference resolution data sets", "start_pos": 135, "end_pos": 171, "type": "DATASET", "confidence": 0.7607251703739166}]}, {"text": "We hypothesize that performance variation within and across coreference resolvers is, at least in part, a function of (1) the (sometimes unstated) assumptions in evaluation methodologies, and (2) the relative difficulty of the benchmark text corpora.", "labels": [], "entities": [{"text": "coreference resolvers", "start_pos": 60, "end_pos": 81, "type": "TASK", "confidence": 0.8738666474819183}]}, {"text": "With these in mind, Section 3 first examines three subproblems that play an important role in coreference resolution: named entity recognition, anaphoricity determination, and coreference element detection.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 94, "end_pos": 116, "type": "TASK", "confidence": 0.9566183388233185}, {"text": "named entity recognition", "start_pos": 118, "end_pos": 142, "type": "TASK", "confidence": 0.6395923594633738}, {"text": "anaphoricity determination", "start_pos": 144, "end_pos": 170, "type": "TASK", "confidence": 0.6682086884975433}, {"text": "coreference element detection", "start_pos": 176, "end_pos": 205, "type": "TASK", "confidence": 0.867114802201589}]}, {"text": "We quantitatively measure the impact of each of these subproblems on coreference resolution performance as a whole.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 69, "end_pos": 91, "type": "TASK", "confidence": 0.9563388824462891}]}, {"text": "Our results suggest that the availability of accurate detectors for anaphoricity or coreference elements could substantially improve the performance of state-ofthe-art resolvers, while improvements to named entity recognition likely offer little gains.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 201, "end_pos": 225, "type": "TASK", "confidence": 0.6515297194321951}]}, {"text": "Our results also confirm that the assumptions adopted in some evaluations dramatically simplify the resolution task, rendering it an unrealistic surrogate for the original problem.", "labels": [], "entities": [{"text": "resolution task", "start_pos": 100, "end_pos": 115, "type": "TASK", "confidence": 0.8905161917209625}]}, {"text": "In Section 4, we quantify the difficulty of a text corpus with respect to coreference resolution by analyzing performance on different resolution classes.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 74, "end_pos": 96, "type": "TASK", "confidence": 0.9541286528110504}]}, {"text": "Our goals are twofold: to measure the level of performance of state-of-the-art coreference resolvers on different types of anaphora, and to develop a quantitative measure for estimating coreference resolution performance on new data sets.", "labels": [], "entities": [{"text": "coreference resolvers", "start_pos": 79, "end_pos": 100, "type": "TASK", "confidence": 0.8572618961334229}, {"text": "coreference resolution", "start_pos": 186, "end_pos": 208, "type": "TASK", "confidence": 0.8151110708713531}]}, {"text": "We introduce a coreference performance prediction (CPP) measure and show that it accurately predicts the performance of our coreference resolver.", "labels": [], "entities": [{"text": "coreference performance prediction (CPP)", "start_pos": 15, "end_pos": 55, "type": "TASK", "confidence": 0.7839143673578898}, {"text": "coreference resolver", "start_pos": 124, "end_pos": 144, "type": "TASK", "confidence": 0.8602179288864136}]}, {"text": "As aside effect of our research, we provide anew set of much-needed benchmark results for coreference resolution under common sets of fully-specified evaluation assumptions.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 90, "end_pos": 112, "type": "TASK", "confidence": 0.9740852415561676}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 3: Impact of Three Subtasks on Coreference Resolution Performance. A score marked with a * indicates that a 0.5", "labels": [], "entities": [{"text": "Coreference Resolution", "start_pos": 38, "end_pos": 60, "type": "TASK", "confidence": 0.95098015666008}]}, {"text": " Table 4: Frequencies and scores for each resolution class.", "labels": [], "entities": [{"text": "Frequencies", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9307553172111511}]}, {"text": " Table 6: Predicted (P) vs Observed (O) scores.", "labels": [], "entities": [{"text": "Predicted (P) vs Observed (O) scores", "start_pos": 10, "end_pos": 46, "type": "METRIC", "confidence": 0.8952813148498535}]}]}