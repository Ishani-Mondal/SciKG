{"title": [{"text": "A Syntactic and Lexical-Based Discourse Segmenter", "labels": [], "entities": [{"text": "Syntactic and Lexical-Based Discourse Segmenter", "start_pos": 2, "end_pos": 49, "type": "TASK", "confidence": 0.6450308084487915}]}], "abstractContent": [{"text": "We present a syntactic and lexically based discourse segmenter (SLSeg) that is designed to avoid the common problem of over-segmenting text.", "labels": [], "entities": [{"text": "lexically based discourse segmenter (SLSeg)", "start_pos": 27, "end_pos": 70, "type": "TASK", "confidence": 0.7486989327839443}]}, {"text": "Segmentation is the first step in a discourse parser, a system that constructs discourse trees from elementary discourse units.", "labels": [], "entities": []}, {"text": "We compare SLSeg to a probabilistic segmenter, showing that a conservative approach increases precision at the expense of recall, while retaining a high F-score across both formal and informal texts.", "labels": [], "entities": [{"text": "precision", "start_pos": 94, "end_pos": 103, "type": "METRIC", "confidence": 0.9989690780639648}, {"text": "recall", "start_pos": 122, "end_pos": 128, "type": "METRIC", "confidence": 0.9990052580833435}, {"text": "F-score", "start_pos": 153, "end_pos": 160, "type": "METRIC", "confidence": 0.9986951947212219}]}], "introductionContent": [], "datasetContent": [{"text": "The evaluation uses standard precision, recall and F-score to compute correctly inserted segment boundaries (we do not consider sentence boundaries since that would inflate the scores).", "labels": [], "entities": [{"text": "precision", "start_pos": 29, "end_pos": 38, "type": "METRIC", "confidence": 0.9991017580032349}, {"text": "recall", "start_pos": 40, "end_pos": 46, "type": "METRIC", "confidence": 0.9994052648544312}, {"text": "F-score", "start_pos": 51, "end_pos": 58, "type": "METRIC", "confidence": 0.9983071088790894}]}, {"text": "Precision is the number of boundaries in agreement with the gold standard.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9899162650108337}]}, {"text": "Recall is the total number of boundaries correct in the system's output divided by the number of total boundaries in the gold standard.", "labels": [], "entities": [{"text": "Recall", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9922498464584351}]}, {"text": "We compare the output of SLSeg to SPADE.", "labels": [], "entities": []}, {"text": "Since SPADE is trained on RST-DT, it inserts segment boundaries that are different from what our annotation guidelines prescribe.", "labels": [], "entities": [{"text": "SPADE", "start_pos": 6, "end_pos": 11, "type": "TASK", "confidence": 0.9159786701202393}, {"text": "RST-DT", "start_pos": 26, "end_pos": 32, "type": "TASK", "confidence": 0.848030149936676}]}, {"text": "To provide a fair comparison, we implement a coarse version of SPADE where segment boundaries prescribed by the RST-DT guidelines, but not part of our segmentation guidelines, are manually removed.", "labels": [], "entities": [{"text": "SPADE", "start_pos": 63, "end_pos": 68, "type": "TASK", "confidence": 0.8780046105384827}]}, {"text": "This version leads to increased precision while maintaining identical recall, thus improving F-score.", "labels": [], "entities": [{"text": "precision", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.9994080066680908}, {"text": "recall", "start_pos": 70, "end_pos": 76, "type": "METRIC", "confidence": 0.984654426574707}, {"text": "F-score", "start_pos": 93, "end_pos": 100, "type": "METRIC", "confidence": 0.9968256950378418}]}, {"text": "In addition to SPADE, we also used the Sundance parser () in our evaluation.", "labels": [], "entities": [{"text": "SPADE", "start_pos": 15, "end_pos": 20, "type": "TASK", "confidence": 0.5176457166671753}]}, {"text": "Sundance is a shallow parser which provides clause segmentation on top of a basic word-tagging and phrase-chunking system.", "labels": [], "entities": [{"text": "clause segmentation", "start_pos": 44, "end_pos": 63, "type": "TASK", "confidence": 0.7668930888175964}]}, {"text": "Since Sundance clauses are also too fine-grained for our purposes, we use a few simple rules to collapse clauses that are unlikely to meet our definition of EDU.", "labels": [], "entities": []}, {"text": "The baseline segmenter in inserts segment boundaries before and after all instances of S, SBAR, SQ, SINV, SBARQ from the syntactic parse (text spans that represent full clauses able to standalone as sentential units).", "labels": [], "entities": []}, {"text": "Finally, two parsers are compared for their effect on segmentation quality: Charniak (Charniak, 2000) and Stanford (.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 54, "end_pos": 66, "type": "TASK", "confidence": 0.96773362159729}]}], "tableCaptions": [{"text": " Table 1: Comparison of segmenters", "labels": [], "entities": [{"text": "Comparison of segmenters", "start_pos": 10, "end_pos": 34, "type": "TASK", "confidence": 0.6902459263801575}]}]}