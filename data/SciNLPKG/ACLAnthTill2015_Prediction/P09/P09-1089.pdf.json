{"title": [{"text": "Source-Language Entailment Modeling for Translating Unknown Terms", "labels": [], "entities": [{"text": "Source-Language Entailment Modeling", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.7747791508833567}, {"text": "Translating Unknown Terms", "start_pos": 40, "end_pos": 65, "type": "TASK", "confidence": 0.8853374322255453}]}], "abstractContent": [{"text": "This paper addresses the task of handling unknown terms in SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 59, "end_pos": 62, "type": "TASK", "confidence": 0.7076425552368164}]}, {"text": "We propose using source-language monolingual models and resources to paraphrase the source text prior to translation.", "labels": [], "entities": []}, {"text": "We further present a conceptual extension to prior work by allowing translations of entailed texts rather than paraphrases only.", "labels": [], "entities": [{"text": "translations of entailed texts", "start_pos": 68, "end_pos": 98, "type": "TASK", "confidence": 0.8507965505123138}]}, {"text": "A method for performing this process efficiently is presented and applied to some 2500 sentences with unknown terms.", "labels": [], "entities": []}, {"text": "Our experiments show that the proposed approach substantially increases the number of properly translated texts.", "labels": [], "entities": []}], "introductionContent": [{"text": "Machine Translation systems frequently encounter terms they are notable to translate due to some missing knowledge.", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8101666569709778}]}, {"text": "For instance, a Statistical Machine Translation (SMT) system translating the sentence \"Cisco filed a lawsuit against Apple for patent violation\" may lack words like filed and lawsuit in its phrase table.", "labels": [], "entities": [{"text": "Statistical Machine Translation (SMT) system translating the sentence \"Cisco filed a lawsuit against Apple for patent violation", "start_pos": 16, "end_pos": 143, "type": "TASK", "confidence": 0.8229883164167404}]}, {"text": "The problem is especially severe for languages for which parallel corpora are scarce, or in the common scenario when the SMT system is used to translate texts of a domain different from the one it was trained on.", "labels": [], "entities": [{"text": "SMT", "start_pos": 121, "end_pos": 124, "type": "TASK", "confidence": 0.973118245601654}]}, {"text": "A previously suggested solution) is to learn paraphrases of source terms from multilingual (parallel) corpora, and expand the phrase table with these paraphrases . Such solutions could potentially yield a paraphrased sentence like \"Cisco sued Apple for patent violation\", although their dependence on bilingual resources limits their utility.", "labels": [], "entities": []}, {"text": "In this paper we propose an approach that consists in directly replacing unknown source terms, using source-language resources and models in order to achieve two goals.", "labels": [], "entities": []}, {"text": "The first goal is coverage increase.", "labels": [], "entities": [{"text": "coverage", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9903895258903503}]}, {"text": "The availability of bilingual corpora, from which paraphrases can be learnt, is in many cases limited.", "labels": [], "entities": []}, {"text": "On the other hand, monolingual resources and methods for extracting paraphrases from monolingual corpora are more readily available.", "labels": [], "entities": []}, {"text": "These include manually constructed resources, such as WordNet, and automatic methods for paraphrases acquisition, such as DIRT ().", "labels": [], "entities": [{"text": "WordNet", "start_pos": 54, "end_pos": 61, "type": "DATASET", "confidence": 0.959770679473877}, {"text": "paraphrases acquisition", "start_pos": 89, "end_pos": 112, "type": "TASK", "confidence": 0.8369682133197784}]}, {"text": "However, such resources have not been applied yet to the problem of substituting unknown terms in SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 98, "end_pos": 101, "type": "TASK", "confidence": 0.9715315103530884}]}, {"text": "We suggest that by using such monolingual resources we could provide paraphrases fora larger number of texts with unknown terms, thus increasing the overall coverage of the SMT system, i.e. the number of texts it properly translates.", "labels": [], "entities": [{"text": "SMT", "start_pos": 173, "end_pos": 176, "type": "TASK", "confidence": 0.988261878490448}]}, {"text": "Even with larger paraphrase resources, we may encounter texts in which not all unknown terms are successfully handled through paraphrasing, which often results in poor translations (see Section 2.1).", "labels": [], "entities": []}, {"text": "To further increase coverage, we therefore propose to generate and translate texts that convey a somewhat more general meaning than the original source text.", "labels": [], "entities": []}, {"text": "For example, using such approach, the following text could be generated: \"Cisco accused Apple of patent violation\".", "labels": [], "entities": []}, {"text": "Although less informative than the original, a translation for such texts maybe useful.", "labels": [], "entities": []}, {"text": "Such non-symmetric relationships (as between filed a lawsuit and accused) are difficult to learn from parallel corpora and therefore monolingual resources are more appropriate for this purpose.", "labels": [], "entities": []}, {"text": "The second goal we wish to accomplish by employing source-language resources is to rank the alternative generated texts.", "labels": [], "entities": []}, {"text": "This goal can be achieved by using context-models on the source language prior to translation.", "labels": [], "entities": []}, {"text": "First, the ranking allows us to prune some candidates before supplying them to the translation engine, thus improving translation efficiency.", "labels": [], "entities": [{"text": "translation engine", "start_pos": 83, "end_pos": 101, "type": "TASK", "confidence": 0.900199294090271}, {"text": "translation", "start_pos": 118, "end_pos": 129, "type": "TASK", "confidence": 0.9635500311851501}]}, {"text": "Second, the ranking maybe combined with target language information in order to choose the best translation, thus improving translation quality.", "labels": [], "entities": []}, {"text": "We position the problem of generating alternative texts for translation within the Textual Entailment (TE) framework (.", "labels": [], "entities": []}, {"text": "TE provides a generic way for handling language variability, identifying when the meaning of one text is entailed by the other (i.e. the meaning of the entailed text can be inferred from the meaning of the entailing one).", "labels": [], "entities": []}, {"text": "When the meanings of two texts are equivalent (paraphrase), entailment is mutual.", "labels": [], "entities": []}, {"text": "Typically, a more general version of a certain text is entailed by it.", "labels": [], "entities": []}, {"text": "Hence, through TE we can formalize the generation of both equivalent and more general texts for the source text.", "labels": [], "entities": [{"text": "TE", "start_pos": 15, "end_pos": 17, "type": "METRIC", "confidence": 0.7736799120903015}]}, {"text": "When possible, a paraphrase is used.", "labels": [], "entities": []}, {"text": "Otherwise, an alternative text whose meaning is entailed by the original source is generated and translated.", "labels": [], "entities": []}, {"text": "We assess our approach by applying an SMT system to a text domain that is different from the one used to train the system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 38, "end_pos": 41, "type": "TASK", "confidence": 0.9879601001739502}]}, {"text": "We use WordNet as a source language resource for entailment relationships and several common statistical contextmodels for selecting the best generated texts to be sent to translation.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 7, "end_pos": 14, "type": "DATASET", "confidence": 0.9506266117095947}]}, {"text": "We show that the use of source language resources, and in particular the extension to non-symmetric textual entailment relationships, is useful for substantially increasing the amount of texts that are properly translated.", "labels": [], "entities": []}, {"text": "This increase is observed relative to both using paraphrases produced by the same resource (WordNet) and using paraphrases produced from multilingual parallel corpora.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 92, "end_pos": 99, "type": "DATASET", "confidence": 0.9388072490692139}]}, {"text": "We demonstrate that by using simple context-models on the source, efficiency can be improved, while translation quality is maintained.", "labels": [], "entities": []}, {"text": "We believe that with the use of more sophisticated context-models further quality improvement can be achieved.", "labels": [], "entities": []}], "datasetContent": [{"text": "To assess our approach, we conducted a series of experiments; in each experiment we applied the scheme described in 3, changing only the models being used for scoring the generated and translated texts.", "labels": [], "entities": []}, {"text": "The setting of these experiments is described in what follows.", "labels": [], "entities": []}, {"text": "SMT data To produce sentences for our experiments, we use Matrax (), a standard phrase-based SMT system, with the exception that it allows gaps in phrases.", "labels": [], "entities": [{"text": "SMT", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.9549695253372192}, {"text": "SMT", "start_pos": 93, "end_pos": 96, "type": "TASK", "confidence": 0.872904896736145}]}, {"text": "We use approximately 1M sentence pairs from the English-French Europarl corpus for training, and then translate a test set of 5,859 English sentences from the News corpus into French.", "labels": [], "entities": [{"text": "Europarl corpus", "start_pos": 63, "end_pos": 78, "type": "DATASET", "confidence": 0.9757032990455627}, {"text": "News corpus", "start_pos": 159, "end_pos": 170, "type": "DATASET", "confidence": 0.9669048488140106}]}, {"text": "Both resources are taken from the shared translation task in).", "labels": [], "entities": []}, {"text": "Hence, we compare our method in a setting where the training and test data are from different domains, a common scenario in the practical use of MT systems.", "labels": [], "entities": [{"text": "MT", "start_pos": 145, "end_pos": 147, "type": "TASK", "confidence": 0.9885170459747314}]}, {"text": "Of the 5,859 translated sentences, 2,494 contain unknown terms (considering only sequences with alphabetic symbols), summing up to 4,255 occurrences of unknown terms.", "labels": [], "entities": []}, {"text": "39% of the 2,494 sentences contain more than a single unknown term.", "labels": [], "entities": []}, {"text": "Entailment resource We use WordNet 3.0 as a resource for entailment rules.", "labels": [], "entities": []}, {"text": "Paraphrases are generated using synonyms.", "labels": [], "entities": []}, {"text": "Directionally entailed texts are created using hypernyms, which typically conform with entailment.", "labels": [], "entities": []}, {"text": "We do not rely on sense information in WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 39, "end_pos": 46, "type": "DATASET", "confidence": 0.9706254601478577}]}, {"text": "Hence, any other semantic resource for entailment rules can be utilized.", "labels": [], "entities": []}, {"text": "Each sentence is tagged using the OpenNLP POS tagger 2 . Entailment rules are applied for unknown terms tagged as nouns, verbs, adjectives and adverbs.", "labels": [], "entities": [{"text": "OpenNLP POS tagger 2", "start_pos": 34, "end_pos": 54, "type": "DATASET", "confidence": 0.9118047654628754}]}, {"text": "The use of relations from WordNet results in 1,071 sentences with applicable rules (with phrase table entries) for the unknown terms when using synonyms, and 1,643 when using both synonyms and hypernyms, accounting for 43% and 66% of the test sentences, respectively.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 26, "end_pos": 33, "type": "DATASET", "confidence": 0.9465559720993042}]}, {"text": "The number of alternative sentences generated for each source text varies from 1 to 960 when paraphrasing rules were applied, and reaches very large numbers, up to 89,700 at the \"worst case\", when all TE rules are employed, an average of 456 alternatives per sentence.", "labels": [], "entities": []}, {"text": "Scoring source texts We test our proposed method using several context-models shown to perform reasonably well in previous work: \u2022 FREQ: The first model we use is a contextindependent baseline.", "labels": [], "entities": [{"text": "FREQ", "start_pos": 131, "end_pos": 135, "type": "METRIC", "confidence": 0.9955251812934875}]}, {"text": "A common useful heuristic to pick an entailment rule is to select the candidate with the highest frequency in the corpus ().", "labels": [], "entities": []}, {"text": "In this model, a rule's score is the normalized number of occurrences of its RHS in the training corpus, ignoring the context of the LHS.", "labels": [], "entities": []}, {"text": "\u2022 LSA: Latent Semantic Analysis) is a well-known method for rep-resenting the contextual usage of words based on corpus statistics.", "labels": [], "entities": []}, {"text": "We represented each term by a normalized vector of the top 100 SVD dimensions, as described in (.", "labels": [], "entities": []}, {"text": "This model measures the similarity between the sentence words and the RHS in the LSA space.", "labels": [], "entities": [{"text": "LSA space", "start_pos": 81, "end_pos": 90, "type": "DATASET", "confidence": 0.895706444978714}]}, {"text": "\u2022 NB: We implemented the unsupervised Na\u00a8\u0131veNa\u00a8\u0131ve Bayes model described in ) to estimate the probability that the unknown term entails the RHS in the given context.", "labels": [], "entities": []}, {"text": "The estimation is based on corpus co-occurrence statistics of the context words with the RHS.", "labels": [], "entities": [{"text": "RHS", "start_pos": 89, "end_pos": 92, "type": "DATASET", "confidence": 0.9503588080406189}]}, {"text": "\u2022 LMS: This model generates the Language Model probability of the RHS in the source.", "labels": [], "entities": []}, {"text": "We use 3-grams probabilities as produced by the SRILM toolkit).", "labels": [], "entities": [{"text": "SRILM toolkit", "start_pos": 48, "end_pos": 61, "type": "DATASET", "confidence": 0.9021621346473694}]}, {"text": "Finally, as a simple baseline, we generated a random score for each rule application, RAND.", "labels": [], "entities": [{"text": "RAND", "start_pos": 86, "end_pos": 90, "type": "METRIC", "confidence": 0.6916224956512451}]}, {"text": "The score of each rule application by any of the above models is normalized to the range.", "labels": [], "entities": []}, {"text": "To combine individual rule applications in a given sentence, we use the product of their scores.", "labels": [], "entities": []}, {"text": "The monolingual data used for the models above is the source side of the training parallel corpus.", "labels": [], "entities": []}, {"text": "Target-language scores On the target side we used either a standard 3-gram language-model, denoted LMT, or the score assigned by the complete SMT log-linear model, which includes the language model as one of its components (SMT).", "labels": [], "entities": [{"text": "SMT", "start_pos": 142, "end_pos": 145, "type": "TASK", "confidence": 0.9627642035484314}]}, {"text": "A pair of a source:target models comprises a complete model for selecting the best translated sentence, where the overall score is the product of the scores of the two models.", "labels": [], "entities": []}, {"text": "We also applied several combinations of source models, such as LSA combined with LMS, to take advantage of their complementary strengths.", "labels": [], "entities": []}, {"text": "Additionally, we assessed our method with sourceonly models, by setting the number of sentences to be selected by the source model to one (k = 1).", "labels": [], "entities": []}, {"text": "To evaluate the translations produced using the various source and target models and the different rule-sets, we rely mostly on manual assessment, since automatic MT evaluation metrics like BLEU do not capture well the type of semantic variations: Translation acceptance when using only paraphrases and when using all entailment rules.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 163, "end_pos": 176, "type": "TASK", "confidence": 0.9123862385749817}, {"text": "BLEU", "start_pos": 190, "end_pos": 194, "type": "METRIC", "confidence": 0.9960229396820068}, {"text": "Translation acceptance", "start_pos": 248, "end_pos": 270, "type": "TASK", "confidence": 0.9417440593242645}]}, {"text": "\":\" indicates which model is applied to the source (left side) and which to the target language (right side).", "labels": [], "entities": []}, {"text": "generated in our experiments, particularly at the sentence level.", "labels": [], "entities": []}, {"text": "In the manual evaluation, two native speakers of the target language judged whether each translation preserves the meaning of its reference sentence, marking it as acceptable or unacceptable.", "labels": [], "entities": []}, {"text": "From the sentences for which rules were applicable, we randomly selected a sample of sentences for each annotator, allowing for some overlapping for agreement analysis.", "labels": [], "entities": [{"text": "agreement analysis", "start_pos": 149, "end_pos": 167, "type": "TASK", "confidence": 0.8348821699619293}]}, {"text": "In total, the translations of 1,014 unique source sentences were manually annotated, of which 453 were produced using only hypernyms (no paraphrases were applicable).", "labels": [], "entities": [{"text": "translations of 1,014 unique source sentences", "start_pos": 14, "end_pos": 59, "type": "TASK", "confidence": 0.7896320025126139}]}, {"text": "When a sentence was annotated by both annotators, one annotation was picked randomly.", "labels": [], "entities": []}, {"text": "Inter-annotator agreement was measured by the percentage of sentences the annotators agreed on, as well as via the Kappa measure.", "labels": [], "entities": []}, {"text": "For different models, the agreement rate varied from 67% to 78% (72% overall), and the Kappa value ranged from 0.34 to 0.55, which is comparable to figures reported for other standard SMT evaluation metrics . Translation with TE For each model m, we measured P recision m , the percentage of acceptable translations out of all sampled translations.", "labels": [], "entities": [{"text": "agreement rate", "start_pos": 26, "end_pos": 40, "type": "METRIC", "confidence": 0.9847386181354523}, {"text": "Kappa", "start_pos": 87, "end_pos": 92, "type": "METRIC", "confidence": 0.9874699711799622}, {"text": "SMT evaluation", "start_pos": 184, "end_pos": 198, "type": "TASK", "confidence": 0.9027653634548187}, {"text": "Translation", "start_pos": 209, "end_pos": 220, "type": "TASK", "confidence": 0.9697129726409912}, {"text": "TE", "start_pos": 226, "end_pos": 228, "type": "METRIC", "confidence": 0.9951011538505554}, {"text": "P recision m", "start_pos": 259, "end_pos": 271, "type": "METRIC", "confidence": 0.9513089060783386}]}, {"text": "P recision m was measured both when using only paraphrases (PARAPH.) and when using all entailment rules (TE).", "labels": [], "entities": [{"text": "P recision m", "start_pos": 0, "end_pos": 12, "type": "METRIC", "confidence": 0.9132070541381836}, {"text": "PARAPH.", "start_pos": 60, "end_pos": 67, "type": "METRIC", "confidence": 0.9894658327102661}, {"text": "all entailment rules (TE)", "start_pos": 84, "end_pos": 109, "type": "METRIC", "confidence": 0.6175740162531534}]}, {"text": "We also measured Coverage m , the percentage of sentences with acceptable translations, Am , out of all sentences (2,494).", "labels": [], "entities": [{"text": "Coverage m", "start_pos": 17, "end_pos": 27, "type": "METRIC", "confidence": 0.8880302011966705}]}, {"text": "As our annotators evaluated only a sample of sentences, Am is estimated as the model's total number of sentences with applicable rules, Sm , multiplied by the model's Precision (S m was 1,071 for paraphrases and 1,643 for entailment rules): . presents the results of several sourcetarget combinations when using only paraphrases and when also using directional entailment rules.", "labels": [], "entities": [{"text": "Precision", "start_pos": 167, "end_pos": 176, "type": "METRIC", "confidence": 0.9993977546691895}]}, {"text": "When all rules are used, a substantial improvement in coverage is consistently obtained across all models, reaching a relative increase of 50% over paraphrases only, while just a slight decrease in precision is observed (see Section 5.3 for some error analysis).", "labels": [], "entities": [{"text": "coverage", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.9972907900810242}, {"text": "precision", "start_pos": 198, "end_pos": 207, "type": "METRIC", "confidence": 0.9992250204086304}]}, {"text": "This confirms our hypothesis that directional entailment rules can be very useful for replacing unknown terms.", "labels": [], "entities": []}, {"text": "For the combination of source-target models, the value of k is set depending on which rule-set is used.", "labels": [], "entities": []}, {"text": "Preliminary analysis showed that k = 5 is sufficient when only paraphrases are used and k = 20 when directional entailment rules are also considered.", "labels": [], "entities": []}, {"text": "We measured statistical significance between different models for precision of the TE results according to the Wilcoxon signed ranks test.", "labels": [], "entities": [{"text": "precision", "start_pos": 66, "end_pos": 75, "type": "METRIC", "confidence": 0.9988958835601807}, {"text": "TE", "start_pos": 83, "end_pos": 85, "type": "TASK", "confidence": 0.504797101020813}]}, {"text": "Models 1-6 in are significantly better than the RAND baseline (p < 0.03), and models 1-3 are significantly better than model 6 (p < 0.05).", "labels": [], "entities": [{"text": "RAND baseline", "start_pos": 48, "end_pos": 61, "type": "DATASET", "confidence": 0.7124716937541962}]}, {"text": "The difference between -:SMT and NB:SMT or LSA:SMT is not statistically significant.", "labels": [], "entities": [{"text": "SMT", "start_pos": 36, "end_pos": 39, "type": "TASK", "confidence": 0.7305160760879517}]}, {"text": "The results in therefore suggest that taking a source model into account preserves the quality of translation.", "labels": [], "entities": []}, {"text": "Furthermore, the quality is maintained even when source models' selections are restricted to a rather small top-k ranks, at a lower computational cost (for the models combining source and target, like NB:SMT or LSA:SMT).", "labels": [], "entities": [{"text": "NB:SMT or LSA:SMT", "start_pos": 201, "end_pos": 218, "type": "TASK", "confidence": 0.5260065793991089}]}, {"text": "This is particularly relevant for on-demand MT systems, where time is an issue.", "labels": [], "entities": [{"text": "MT", "start_pos": 44, "end_pos": 46, "type": "TASK", "confidence": 0.9844004511833191}]}, {"text": "For such systems, using this source-language based pruning methodology will yield significant performance gains as compared to target-only models.", "labels": [], "entities": []}, {"text": "We also evaluated the baseline strategy where unknown terms are omitted from the translation, resulting in 25% precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 111, "end_pos": 120, "type": "METRIC", "confidence": 0.9984135627746582}]}, {"text": "Leaving unknown words untranslated also yielded very poor translation quality in an analysis performed on a similar dataset.", "labels": [], "entities": []}, {"text": "Comparison to related work We compared our algorithm with an implementation of the algorithm proposed by) (see Section 2.2), henceforth CB, using the Spanish side of Europarl as the pivot language.", "labels": [], "entities": [{"text": "Europarl", "start_pos": 166, "end_pos": 174, "type": "DATASET", "confidence": 0.6412221789360046}]}, {"text": "Out of the tested 2,494 sentences with unknown terms, CB found paraphrases for 706 sentences (28.3%), while with any of our models, including  NB:SMT, our algorithm found applicable entailment rules for 1,643 sentences (66%).", "labels": [], "entities": [{"text": "NB:SMT", "start_pos": 143, "end_pos": 149, "type": "TASK", "confidence": 0.45021866758664447}]}, {"text": "The quality of the CB translations was manually assessed fora sample of 150 sentences.", "labels": [], "entities": [{"text": "CB translations", "start_pos": 19, "end_pos": 34, "type": "TASK", "confidence": 0.6740490198135376}]}, {"text": "presents the precision and coverage on this sample for both CB and NB:SMT, as well as the number of times each model's translation was preferred by the annotators.", "labels": [], "entities": [{"text": "precision", "start_pos": 13, "end_pos": 22, "type": "METRIC", "confidence": 0.9996336698532104}, {"text": "coverage", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9906014204025269}, {"text": "SMT", "start_pos": 70, "end_pos": 73, "type": "TASK", "confidence": 0.5226281881332397}]}, {"text": "While both models achieve equally high precision scores on this sample, the NB:SMT model's translations were undoubtedly preferred by the annotators, with a considerably higher coverage.", "labels": [], "entities": [{"text": "precision", "start_pos": 39, "end_pos": 48, "type": "METRIC", "confidence": 0.9932182431221008}, {"text": "NB:SMT", "start_pos": 76, "end_pos": 82, "type": "TASK", "confidence": 0.5190501113732656}]}, {"text": "With the CB method, given that many of the phrases added to the phrase table are noisy, the global quality of the sentences seem to have been affected, explaining why the judges preferred the NB:SMT translations.", "labels": [], "entities": [{"text": "NB:SMT translations", "start_pos": 192, "end_pos": 211, "type": "TASK", "confidence": 0.5852078199386597}]}, {"text": "One reason for the lower coverage of CB is the fact that paraphrases were acquired from a corpus whose domain is different from that of the test sentences.", "labels": [], "entities": [{"text": "coverage", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.956961989402771}]}, {"text": "The entailment rules in our models are not limited to paraphrases and are derived from WordNet, which has broader applicability.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 87, "end_pos": 94, "type": "DATASET", "confidence": 0.9482241272926331}]}, {"text": "Hence, utilizing monolingual resources has proven beneficial for the task.", "labels": [], "entities": []}, {"text": "Although automatic MT evaluation metrics are less appropriate for capturing the variations generated by our method, to ensure that there was no degradation in the system-level scores according to such metrics we also measured the models' performance using BLEU and METEOR.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 19, "end_pos": 32, "type": "TASK", "confidence": 0.9125798642635345}, {"text": "BLEU", "start_pos": 256, "end_pos": 260, "type": "METRIC", "confidence": 0.9983571171760559}, {"text": "METEOR", "start_pos": 265, "end_pos": 271, "type": "METRIC", "confidence": 0.8654147386550903}]}, {"text": "The version of METEOR we used on the target language (French) considers the stems of the words, instead of surface forms only, but does not make use of WordNet synonyms.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 15, "end_pos": 21, "type": "METRIC", "confidence": 0.5813814401626587}]}, {"text": "We evaluated the performance of the top models of, as well as of a baseline SMT system that left unknown terms untranslated, on the sample of 1,014 manually annotated sentences.", "labels": [], "entities": [{"text": "SMT", "start_pos": 76, "end_pos": 79, "type": "TASK", "confidence": 0.9909453392028809}]}, {"text": "As shown in, all models resulted in improvement with respect to the original sentences (base-  line).", "labels": [], "entities": []}, {"text": "The difference in METEOR scores is statistically significant (p < 0.05) for the three top models against the baseline.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 18, "end_pos": 24, "type": "METRIC", "confidence": 0.9720895886421204}]}, {"text": "The generally low scores maybe attributed to the fact that training and test sentences are from different domains.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Translation acceptance when using only para-", "labels": [], "entities": [{"text": "Translation acceptance", "start_pos": 10, "end_pos": 32, "type": "TASK", "confidence": 0.969127357006073}]}, {"text": " Table 2: Comparison between our top model and the", "labels": [], "entities": []}, {"text": " Table 3: Performance of the best models according to auto-", "labels": [], "entities": []}]}