{"title": [{"text": "Robust Approach to Abbreviating Terms: A Discriminative Latent Variable Model with Global Information", "labels": [], "entities": []}], "abstractContent": [{"text": "The present paper describes a robust approach for abbreviating terms.", "labels": [], "entities": []}, {"text": "First, in order to incorporate non-local information into abbreviation generation tasks, we present both implicit and explicit solutions: the latent variable model, or alternatively , the label encoding approach with global information.", "labels": [], "entities": [{"text": "abbreviation generation tasks", "start_pos": 58, "end_pos": 87, "type": "TASK", "confidence": 0.8859192132949829}, {"text": "label encoding", "start_pos": 188, "end_pos": 202, "type": "TASK", "confidence": 0.7126316726207733}]}, {"text": "Although the two approaches compete with one another, we demonstrate that these approaches are also complementary.", "labels": [], "entities": []}, {"text": "By combining these two approaches, experiments revealed that the proposed abbreviation generator achieved the best results for both the Chinese and English languages.", "labels": [], "entities": []}, {"text": "Moreover, we directly apply our generator to perform a very different task from tradition, the abbreviation recognition.", "labels": [], "entities": [{"text": "abbreviation recognition", "start_pos": 95, "end_pos": 119, "type": "TASK", "confidence": 0.8549677133560181}]}, {"text": "Experiments revealed that the proposed model worked robustly, and out-performed five out of six state-of-the-art abbreviation recognizers.", "labels": [], "entities": [{"text": "abbreviation recognizers", "start_pos": 113, "end_pos": 137, "type": "TASK", "confidence": 0.8191613554954529}]}], "introductionContent": [{"text": "Abbreviations represent fully expanded forms (e.g., hidden markov model) through the use of shortened forms (e.g., HMM).", "labels": [], "entities": []}, {"text": "At the same time, abbreviations increase the ambiguity in a text.", "labels": [], "entities": []}, {"text": "For example, in computational linguistics, the acronym HMM stands for hidden markov model, whereas, in the field of biochemistry, HMM is generally an abbreviation for heavy meromyosin.", "labels": [], "entities": []}, {"text": "Associating abbreviations with their fully expanded forms is of great importance in various NLP applications.", "labels": [], "entities": []}, {"text": "The core technology for abbreviation disambiguation is to recognize the abbreviation definitions in the actual text.", "labels": [], "entities": [{"text": "abbreviation disambiguation", "start_pos": 24, "end_pos": 51, "type": "TASK", "confidence": 0.9479949474334717}]}, {"text": "reported that 64,242 new abbreviations were introduced into the biomedical literatures in 2004.", "labels": [], "entities": []}, {"text": "As such, it is important to maintain sense inventories (lists of abbreviation definitions) that are updated with the neologisms.", "labels": [], "entities": []}, {"text": "In addition, based on the one-sense-per-discourse assumption, the recognition of abbreviation definitions assumes senses of abbreviations that are locally defined in a document.", "labels": [], "entities": [{"text": "recognition of abbreviation definitions", "start_pos": 66, "end_pos": 105, "type": "TASK", "confidence": 0.8693827837705612}]}, {"text": "Therefore, a number of studies have attempted to model the generation processes of abbreviations: e.g., inferring the abbreviating mechanism of the hidden markov model into HMM.", "labels": [], "entities": []}, {"text": "An obvious approach is to manually design rules for abbreviations.", "labels": [], "entities": []}, {"text": "Early studies attempted to determine the generic rules that humans use to intuitively abbreviate given words (.", "labels": [], "entities": []}, {"text": "Since the late 1990s, researchers have presented various methods by which to extract abbreviation definitions that appear in actual texts;).", "labels": [], "entities": []}, {"text": "For example, implemented a simple algorithm that mapped all alpha-numerical letters in an abbreviation to its expanded form, starting from the end of both the abbreviation and its expanded forms, and moving from right to left.", "labels": [], "entities": []}, {"text": "These studies performed highly, especially for English abbreviations.", "labels": [], "entities": []}, {"text": "However, a more extensive investigation of abbreviations is needed in order to further improve definition extraction.", "labels": [], "entities": [{"text": "definition extraction", "start_pos": 95, "end_pos": 116, "type": "TASK", "confidence": 0.9362213909626007}]}, {"text": "In addition, we cannot simply transfer the knowledge of the hand-crafted rules from one language to another.", "labels": [], "entities": []}, {"text": "For instance, in English, abbreviation characters are preferably chosen from the initial and/or capital characters in their full forms, whereas some p o l y g l y co l i ca c id PS SS PS SS SS SS S PS SS [PGA]", "labels": [], "entities": []}], "datasetContent": [{"text": "For Chinese abbreviation generation, we used the corpus of, which contains 2,914 abbreviation definitions for training, and 729 pairs for testing.", "labels": [], "entities": [{"text": "Chinese abbreviation generation", "start_pos": 4, "end_pos": 35, "type": "TASK", "confidence": 0.6452764769395193}]}, {"text": "This corpus consists primarily of noun phrases (38%), organization names (32%), and verb phrases (21%).", "labels": [], "entities": []}, {"text": "For English abbreviation generation, we evaluated the corpus of.", "labels": [], "entities": [{"text": "English abbreviation generation", "start_pos": 4, "end_pos": 35, "type": "TASK", "confidence": 0.6451979577541351}]}, {"text": "This corpus contains 1,200 aligned pairs extracted from MEDLINE biomedical abstracts (published in 2001).", "labels": [], "entities": [{"text": "MEDLINE biomedical abstracts", "start_pos": 56, "end_pos": 84, "type": "DATASET", "confidence": 0.78290327390035}]}, {"text": "For both tasks, we converted the aligned pairs of the corpora into labeled full forms and used the labeled full forms as the training/evaluation data.", "labels": [], "entities": []}, {"text": "The evaluation metrics used in the abbreviation generation are exact-match accuracy (hereinafter accuracy), including top-1 accuracy, top-2 accuracy, and top-3 accuracy.", "labels": [], "entities": [{"text": "exact-match accuracy", "start_pos": 63, "end_pos": 83, "type": "METRIC", "confidence": 0.7572905719280243}, {"text": "accuracy", "start_pos": 97, "end_pos": 105, "type": "METRIC", "confidence": 0.7299394011497498}, {"text": "accuracy", "start_pos": 124, "end_pos": 132, "type": "METRIC", "confidence": 0.8461154103279114}, {"text": "accuracy", "start_pos": 140, "end_pos": 148, "type": "METRIC", "confidence": 0.9259685277938843}, {"text": "accuracy", "start_pos": 160, "end_pos": 168, "type": "METRIC", "confidence": 0.9561910033226013}]}, {"text": "The top-N accuracy represents the percentage of correct abbreviations that are covered, if we take the top N candidates from the ranked labelings of an abbreviation generator.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9123691916465759}]}, {"text": "We implemented the DPLVM in C++ and optimized the system to cope with large-scale problems.", "labels": [], "entities": []}, {"text": "We employ the feature templates defined in Section 2.3, taking into account these 81,827 features for the Chinese abbreviation generation task, and the 50,149 features for the English abbreviation generation task.", "labels": [], "entities": [{"text": "Chinese abbreviation generation task", "start_pos": 106, "end_pos": 142, "type": "TASK", "confidence": 0.6541951969265938}, {"text": "English abbreviation generation task", "start_pos": 176, "end_pos": 212, "type": "TASK", "confidence": 0.6367943659424782}]}, {"text": "For numerical optimization, we performed a gradient descent with the Limited-Memory BFGS (L-BFGS) optimization technique ().", "labels": [], "entities": [{"text": "numerical optimization", "start_pos": 4, "end_pos": 26, "type": "TASK", "confidence": 0.8580171465873718}]}, {"text": "L-BFGS is a second-order Quasi-Newton method that numerically estimates the curvature from previous gradients and updates.", "labels": [], "entities": []}, {"text": "With no requirement on specialized Hessian approximation, L-BFGS can handle largescale problems efficiently.", "labels": [], "entities": [{"text": "Hessian approximation", "start_pos": 35, "end_pos": 56, "type": "TASK", "confidence": 0.8080895245075226}]}, {"text": "Since the objective function of the DPLVM model is non-convex, different parameter initializations normally bring different optimization results.", "labels": [], "entities": []}, {"text": "Therefore, to approach closer to the global optimal point, it is recommended to perform multiple experiments on DPLVMs with random initialization and then select a good start point.", "labels": [], "entities": []}, {"text": "To reduce overfitting, we employed a L 2 Gaussian weight prior, with the objective function: During training and validation, we set \u03c3 = 1 for the DPLVM generators.", "labels": [], "entities": [{"text": "validation", "start_pos": 113, "end_pos": 123, "type": "TASK", "confidence": 0.9603394269943237}]}, {"text": "We also set four latent variables for each label, in order to make a compromise between accuracy and efficiency.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 88, "end_pos": 96, "type": "METRIC", "confidence": 0.9987919926643372}]}, {"text": "Note that, for the label encoding with global information, many label transitions (e.g., P 2 S 3 ) are actually impossible: the label transitions are strictly constrained, i.e., y i y i+1 \u2208 {P j S j , P j P j+1 , S j P j+1 , S j S j }.", "labels": [], "entities": []}, {"text": "These constraints on the model topology (forward-backward lattice) are enforced by giving appropriate features a weight of \u2212\u221e, thereby forcing all forbidden labelings to have zero probability.", "labels": [], "entities": []}, {"text": "originally proposed this concept of implementing transition restrictions.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Results of Chinese abbreviation gener- ation. T1A, T2A, and T3A represent top-1, top- 2, and top-3 accuracy, respectively. The system  marked with the * symbol is the recommended  system.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 109, "end_pos": 117, "type": "METRIC", "confidence": 0.9901385307312012}]}, {"text": " Table 3: Results of English abbreviation genera- tion.", "labels": [], "entities": []}, {"text": " Table 4: Re-evaluating Chinese abbreviation gen- eration with GIB.", "labels": [], "entities": [{"text": "Re-evaluating Chinese abbreviation gen- eration", "start_pos": 10, "end_pos": 57, "type": "TASK", "confidence": 0.6379670451084772}, {"text": "GIB", "start_pos": 63, "end_pos": 66, "type": "METRIC", "confidence": 0.6427007913589478}]}, {"text": " Table 6: Results of English abbreviation recogni- tion.", "labels": [], "entities": []}, {"text": " Table 7: English abbreviation recognition with  back-off.", "labels": [], "entities": [{"text": "English abbreviation recognition", "start_pos": 10, "end_pos": 42, "type": "TASK", "confidence": 0.6476166546344757}]}]}