{"title": [{"text": "Cross-Domain Dependency Parsing Using a Deep Linguistic Grammar", "labels": [], "entities": [{"text": "Cross-Domain Dependency Parsing", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.6625191768010458}]}], "abstractContent": [{"text": "Pure statistical parsing systems achieves high in-domain accuracy but performs poorly out-domain.", "labels": [], "entities": [{"text": "statistical parsing", "start_pos": 5, "end_pos": 24, "type": "TASK", "confidence": 0.6292199194431305}, {"text": "accuracy", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.9890192151069641}]}, {"text": "In this paper, we propose two different approaches to produce syntactic dependency structures using a large-scale hand-crafted HPSG grammar.", "labels": [], "entities": []}, {"text": "The dependency backbone of an HPSG analysis is used to provide general linguistic insights which, when combined with state-of-the-art statistical dependency parsing models, achieves performance improvements on out-domain tests.", "labels": [], "entities": [{"text": "statistical dependency parsing", "start_pos": 134, "end_pos": 164, "type": "TASK", "confidence": 0.7055711150169373}]}], "introductionContent": [{"text": "Syntactic dependency parsing is attracting more and more research focus in recent years, partially due to its theory-neutral representation, but also thanks to its wide deployment in various NLP tasks (machine translation, textual entailment recognition, question answering, information extraction, etc.).", "labels": [], "entities": [{"text": "Syntactic dependency parsing", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.9386829535166422}, {"text": "machine translation", "start_pos": 202, "end_pos": 221, "type": "TASK", "confidence": 0.7756502330303192}, {"text": "textual entailment recognition", "start_pos": 223, "end_pos": 253, "type": "TASK", "confidence": 0.7397975424925486}, {"text": "question answering", "start_pos": 255, "end_pos": 273, "type": "TASK", "confidence": 0.8419646918773651}, {"text": "information extraction", "start_pos": 275, "end_pos": 297, "type": "TASK", "confidence": 0.7858960926532745}]}, {"text": "In combination with machine learning methods, several statistical dependency parsing models have reached comparable high parsing accuracy).", "labels": [], "entities": [{"text": "statistical dependency parsing", "start_pos": 54, "end_pos": 84, "type": "TASK", "confidence": 0.633958637714386}, {"text": "accuracy", "start_pos": 129, "end_pos": 137, "type": "METRIC", "confidence": 0.8829383254051208}]}, {"text": "In the meantime, successful continuation of CoNLL Shared Tasks since 2006) have witnessed how easy it has become to train a statistical syntactic dependency parser provided that there is annotated treebank.", "labels": [], "entities": [{"text": "statistical syntactic dependency parser", "start_pos": 124, "end_pos": 163, "type": "TASK", "confidence": 0.5858270972967148}]}, {"text": "While the dissemination continues towards various languages, several issues arise with such purely data-driven approaches.", "labels": [], "entities": []}, {"text": "One common observation is that statistical parser performance drops significantly when tested on a dataset different from the training set.", "labels": [], "entities": []}, {"text": "For instance, when using \u2020 The first author thanks the German Excellence Cluster of Multimodal Computing and Interaction for the support of the work.", "labels": [], "entities": [{"text": "German Excellence Cluster", "start_pos": 55, "end_pos": 80, "type": "DATASET", "confidence": 0.8935235738754272}]}, {"text": "The second author is funded by the PIRE PhD scholarship program.", "labels": [], "entities": [{"text": "PIRE PhD scholarship", "start_pos": 35, "end_pos": 55, "type": "DATASET", "confidence": 0.7542768120765686}]}, {"text": "the Wall Street Journal (WSJ) sections of the Penn Treebank ( as training set, tests on BROWN Sections typically result in a 6-8% drop in labeled attachment scores, although the average sentence length is much shorter in BROWN than that in WSJ.", "labels": [], "entities": [{"text": "Wall Street Journal (WSJ) sections", "start_pos": 4, "end_pos": 38, "type": "DATASET", "confidence": 0.9433750935963222}, {"text": "Penn Treebank", "start_pos": 46, "end_pos": 59, "type": "DATASET", "confidence": 0.9411899745464325}, {"text": "WSJ", "start_pos": 240, "end_pos": 243, "type": "DATASET", "confidence": 0.9495671391487122}]}, {"text": "The common interpretation is that the test set is heterogeneous to the training set, hence in a different \"domain\" (in a loose sense).", "labels": [], "entities": []}, {"text": "The typical cause of this is that the model overfits the training domain.", "labels": [], "entities": []}, {"text": "The concerns over random choice of training corpus leading to linguistically inadequate parsing systems increase overtime.", "labels": [], "entities": [{"text": "overtime", "start_pos": 113, "end_pos": 121, "type": "METRIC", "confidence": 0.995246946811676}]}, {"text": "While the statistical revolution in the field of computational linguistics gaining high publicity, the conventional symbolic grammar-based parsing approaches have undergone a quiet period of development during the past decade, and reemerged very recently with several large scale grammar-driven parsing systems, benefiting from the combination of well-established linguistic theories and data-driven stochastic models.", "labels": [], "entities": [{"text": "symbolic grammar-based parsing", "start_pos": 116, "end_pos": 146, "type": "TASK", "confidence": 0.6683364311854044}]}, {"text": "The obvious advantage of such systems over pure statistical parsers is their usage of hand-coded linguistic knowledge irrespective of the training data.", "labels": [], "entities": []}, {"text": "A common problem with grammar-based parser is the lack of robustness.", "labels": [], "entities": []}, {"text": "Also it is difficult to derive grammar compatible annotations to train the statistical components.", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate the performance of our different dependency parsing models, we tested our approaches on several dependency treebanks for English in a similar spirit to the In this section, we will first describe the datasets, then present the results.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 45, "end_pos": 63, "type": "TASK", "confidence": 0.7305659055709839}]}, {"text": "An error analysis is also carried out to show both pros and cons of different models.", "labels": [], "entities": []}, {"text": "In previous years of CoNLL Shared Tasks, several datasets have been created for the purpose of dependency parser evaluation.", "labels": [], "entities": [{"text": "dependency parser evaluation", "start_pos": 95, "end_pos": 123, "type": "TASK", "confidence": 0.8867836197217306}]}, {"text": "Most of them are converted automatically from existing treebanks in various forms.", "labels": [], "entities": []}, {"text": "Our experiments adhere to the CoNLL 2008 dependency syntax () which was used to convert Penn-Treebank constituent trees into single-head, single-root, traceless and nonprojective dependencies.", "labels": [], "entities": [{"text": "CoNLL 2008 dependency syntax", "start_pos": 30, "end_pos": 58, "type": "DATASET", "confidence": 0.9184031635522842}, {"text": "Penn-Treebank constituent trees", "start_pos": 88, "end_pos": 119, "type": "DATASET", "confidence": 0.9142638842264811}]}, {"text": "WSJ This dataset comprises of three portions.", "labels": [], "entities": [{"text": "WSJ This dataset", "start_pos": 0, "end_pos": 16, "type": "DATASET", "confidence": 0.861807386080424}]}, {"text": "The larger part is converted from the Penn Treebank Wall Street Journal Sections #2-#21, and is used for training statistical dependency parsing models; the smaller part, which covers sentences from Section #23, is used for testing.", "labels": [], "entities": [{"text": "Penn Treebank Wall Street Journal Sections #2", "start_pos": 38, "end_pos": 83, "type": "DATASET", "confidence": 0.9667545780539513}, {"text": "statistical dependency parsing", "start_pos": 114, "end_pos": 144, "type": "TASK", "confidence": 0.6450726588567098}]}, {"text": "Childes This is another out-domain test set from the children language component of the TalkBank, containing dialogs between parents and children.", "labels": [], "entities": [{"text": "TalkBank", "start_pos": 88, "end_pos": 96, "type": "DATASET", "confidence": 0.9775577783584595}]}, {"text": "This is the other datasets used in the domain adaptation track of the CoNLL 2007 Shared Task.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 39, "end_pos": 56, "type": "TASK", "confidence": 0.7290105223655701}, {"text": "CoNLL 2007 Shared Task", "start_pos": 70, "end_pos": 92, "type": "DATASET", "confidence": 0.9178184419870377}]}, {"text": "The dataset is annotated with unlabeled dependencies.", "labels": [], "entities": []}, {"text": "As have been reported by others, several systematic differences in the original CHILDES annotation scheme has led to the poor system performances on this track of the Shared Task in 2007.", "labels": [], "entities": [{"text": "CHILDES annotation scheme", "start_pos": 80, "end_pos": 105, "type": "DATASET", "confidence": 0.7615175247192383}, {"text": "Shared Task in 2007", "start_pos": 167, "end_pos": 186, "type": "DATASET", "confidence": 0.7728887647390366}]}, {"text": "Two main differences concern a) root attachments, and b) coordinations.", "labels": [], "entities": []}, {"text": "With several simple heuristics, we change the annotation scheme of the original dataset to match the Penn Treebankbased datasets.", "labels": [], "entities": [{"text": "Penn Treebankbased datasets", "start_pos": 101, "end_pos": 128, "type": "DATASET", "confidence": 0.9957377910614014}]}, {"text": "The new dataset is referred to as CHILDES*.", "labels": [], "entities": [{"text": "CHILDES", "start_pos": 34, "end_pos": 41, "type": "METRIC", "confidence": 0.7928434014320374}]}], "tableCaptions": [{"text": " Table 3: Agreement between HPSG dependency  backbone and CoNLL 2008 dependency in unla- beled attachment score. DB(F): full parsing mode;  DB(P): partial parsing mode; Punctuations are ex- cluded from the evaluation.", "labels": [], "entities": [{"text": "HPSG dependency  backbone", "start_pos": 28, "end_pos": 53, "type": "DATASET", "confidence": 0.8601208329200745}, {"text": "CoNLL 2008", "start_pos": 58, "end_pos": 68, "type": "DATASET", "confidence": 0.8697933256626129}]}, {"text": " Table 4: Performance of the MSTParser with different feature models. Numbers in parentheses are  performance drops in out-domain tests, comparing to in-domain results. The upper part represents the  results on the complete data sets, and the lower part is on the fully parsed subsets, indicated by \"-P\".", "labels": [], "entities": []}, {"text": " Table 5: Performance of the MaltParser with different feature models.", "labels": [], "entities": [{"text": "MaltParser", "start_pos": 29, "end_pos": 39, "type": "DATASET", "confidence": 0.9045578241348267}]}]}