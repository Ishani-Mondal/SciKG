{"title": [{"text": "Comparing Objective and Subjective Measures of Usability in a Human-Robot Dialogue System", "labels": [], "entities": []}], "abstractContent": [{"text": "We present a human-robot dialogue system that enables a robot to work together with a human user to build wooden construction toys.", "labels": [], "entities": []}, {"text": "We then describe a study in which na\u00a8\u0131vena\u00a8\u0131ve subjects interacted with this system under a range of conditions and then completed a user-satisfaction questionnaire.", "labels": [], "entities": []}, {"text": "The results of this study provide a wide range of subjective and objective measures of the quality of the interactions.", "labels": [], "entities": []}, {"text": "To assess which aspects of the interaction had the greatest impact on the users' opinions of the system, we used a method based on the PARADISE evaluation framework (Walker et al., 1997) to derive a performance function from our data.", "labels": [], "entities": [{"text": "PARADISE", "start_pos": 135, "end_pos": 143, "type": "METRIC", "confidence": 0.8747732043266296}]}, {"text": "The major contributors to user satisfaction were the number of repetition requests (which had a negative effect on satisfaction), the dialogue length, and the users' recall of the system instructions (both of which contributed positively).", "labels": [], "entities": [{"text": "satisfaction", "start_pos": 115, "end_pos": 127, "type": "METRIC", "confidence": 0.9732835292816162}, {"text": "recall", "start_pos": 166, "end_pos": 172, "type": "METRIC", "confidence": 0.9979673027992249}]}], "introductionContent": [{"text": "Evaluating the usability of a spoken language dialogue system generally requires a large-scale user study, which can be a time-consuming process both for the experimenters and for the experimental subjects.", "labels": [], "entities": []}, {"text": "In fact, it can be difficult even to define what the criteria are for evaluating such a system (cf.).", "labels": [], "entities": []}, {"text": "In recent years, techniques have been introduced that are designed to predict user satisfaction based on more easily measured properties of an interaction such as dialogue length and speech-recognition error rate.", "labels": [], "entities": [{"text": "speech-recognition error rate", "start_pos": 183, "end_pos": 212, "type": "METRIC", "confidence": 0.6394405961036682}]}, {"text": "The design of such performance methods for evaluating dialogue systems is still an area of open research.", "labels": [], "entities": []}, {"text": "The PARADISE framework (PARAdigm for DIalogue System Evaluation;) describes a method for using data to derive a performance function that predicts user-satisfaction scores from the results on other, more easily computed measures.", "labels": [], "entities": []}, {"text": "PARADISE uses stepwise multiple linear regression to model user satisfaction based on measures representing the performance dimensions of task success, dialogue quality, and dialogue efficiency, and has been applied to a wide range of systems (e.g.,.", "labels": [], "entities": []}, {"text": "If the resulting performance function can be shown to predict user satisfaction as a function of other, more easily measured system properties, it will be widely applicable: in addition to making it possible to evaluate systems based on automatically available data from log files without the need for extensive experiments with users, for example, such a performance function can be used in an online, incremental manner to adapt system behaviour to avoid entering a state that is likely to reduce user satisfaction, or can be used as a reward function in a reinforcement-learning scenario).", "labels": [], "entities": []}, {"text": "Automated evaluation metrics that rate system behaviour based on automatically computable properties have been developed in a number of other fields: widely used measures include BLEU () for machine translation and ROUGE) for summarisation, for example.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 179, "end_pos": 183, "type": "METRIC", "confidence": 0.9980512857437134}, {"text": "machine translation", "start_pos": 191, "end_pos": 210, "type": "TASK", "confidence": 0.8062548935413361}, {"text": "ROUGE", "start_pos": 215, "end_pos": 220, "type": "METRIC", "confidence": 0.9533125758171082}, {"text": "summarisation", "start_pos": 226, "end_pos": 239, "type": "TASK", "confidence": 0.983074963092804}]}, {"text": "When employing any such metric, it is crucial to verify that the predictions of the automated evaluation process agree with human judgements of the important aspects of the system output.", "labels": [], "entities": []}, {"text": "If not, the risk arises that the automated measures do not capture the behaviour that is actually relevant for the human users of a system.", "labels": [], "entities": []}, {"text": "For example, presented a number of counter-examples to the claim that BLEU agrees with human judgements.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 70, "end_pos": 74, "type": "METRIC", "confidence": 0.986308753490448}]}, {"text": "Also, Foster (2008) examined a range of automated metrics for evaluation generated multimodal output and found that few agreed with the preferences expressed by human judges.", "labels": [], "entities": []}, {"text": "In this paper, we apply a PARADISE-style process to the results of a user study of a human-robot dialogue system.", "labels": [], "entities": [{"text": "PARADISE-style", "start_pos": 26, "end_pos": 40, "type": "METRIC", "confidence": 0.9387274384498596}]}, {"text": "We build models to predict the results on a set of subjective user-satisfaction measures, based on objective measures that were either gathered automatically from the system logs or derived from the video recordings of the interactions.", "labels": [], "entities": []}, {"text": "The results indicate that the most significant contributors to user satisfaction were the number of system turns in the dialogues, the users' ability to recall the instructions given by the robot, and the number of times that the user had to ask for instructions to be repeated.", "labels": [], "entities": []}, {"text": "The former two measures were positively correlated with user satisfaction, while the latter had a negative impact on user satisfaction; however the correlation in all cases was relatively low.", "labels": [], "entities": []}, {"text": "At the end of the paper, we discuss possible reasons for these results and propose other measures that might have a larger effect on users' judgements.", "labels": [], "entities": []}], "datasetContent": [{"text": "The human-robot system was evaluated via a user study in which subjects interacted with the complete system; all interactions were in German.", "labels": [], "entities": []}, {"text": "As a between-subjects factor, we manipulated two aspects of the generated output: the strategy used by the dialogue manager to explain a plan to the user, and the type of referring expressions produced by the system.", "labels": [], "entities": []}, {"text": "give the details of these factors and describes the effects of each individual manipulation.", "labels": [], "entities": []}, {"text": "In this paper, we concentrate on the relationships among the different factors that were measured during the study: the efficiency and quality of the dialogues, the users' success at building the required objects and at learning the construction plans for new objects, and the users' subjective reactions to the system.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Dialogue efficiency results", "labels": [], "entities": []}, {"text": " Table 2: Dialogue quality results", "labels": [], "entities": []}, {"text": " Table 3: Task success results", "labels": [], "entities": []}, {"text": " Table 4: User-satisfaction questionnaire results", "labels": [], "entities": []}, {"text": " Table 5: Mean emotional assessments", "labels": [], "entities": [{"text": "Mean emotional assessments", "start_pos": 10, "end_pos": 36, "type": "TASK", "confidence": 0.598468154668808}]}]}