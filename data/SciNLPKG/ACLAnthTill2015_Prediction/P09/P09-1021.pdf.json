{"title": [{"text": "Active Learning for Multilingual Statistical Machine Translation *", "labels": [], "entities": [{"text": "Multilingual Statistical Machine Translation", "start_pos": 20, "end_pos": 64, "type": "TASK", "confidence": 0.5912851840257645}]}], "abstractContent": [{"text": "Statistical machine translation (SMT) models require bilingual corpora for training , and these corpora are often multilingual with parallel text in multiple languages simultaneously.", "labels": [], "entities": [{"text": "Statistical machine translation (SMT)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.798346589008967}]}, {"text": "We introduce an active learning task of adding anew language to an existing multilingual set of parallel text and constructing high quality MT systems, from each language in the collection into this new target language.", "labels": [], "entities": [{"text": "MT", "start_pos": 140, "end_pos": 142, "type": "TASK", "confidence": 0.9815003871917725}]}, {"text": "We show that adding anew language using active learning to the EuroParl corpus provides a significant improvement compared to a random sentence selection baseline.", "labels": [], "entities": [{"text": "EuroParl corpus", "start_pos": 63, "end_pos": 78, "type": "DATASET", "confidence": 0.9929812550544739}]}, {"text": "We also provide new highly effective sentence selection methods that improve AL for phrase-based SMT in the multilingual and single language pair setting.", "labels": [], "entities": [{"text": "sentence selection", "start_pos": 37, "end_pos": 55, "type": "TASK", "confidence": 0.6894156187772751}, {"text": "AL", "start_pos": 77, "end_pos": 79, "type": "METRIC", "confidence": 0.7526981830596924}, {"text": "SMT", "start_pos": 97, "end_pos": 100, "type": "TASK", "confidence": 0.7368096709251404}]}], "introductionContent": [{"text": "The main source of training data for statistical machine translation (SMT) models is a parallel corpus.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 37, "end_pos": 74, "type": "TASK", "confidence": 0.8012292087078094}]}, {"text": "In many cases, the same information is available in multiple languages simultaneously as a multilingual parallel corpus, e.g., European Parliament (EuroParl) and U.N. proceedings.", "labels": [], "entities": [{"text": "European Parliament (EuroParl) and U.N. proceedings", "start_pos": 127, "end_pos": 178, "type": "DATASET", "confidence": 0.6976952478289604}]}, {"text": "In this paper, we consider how to use active learning (AL) in order to add anew language to such a multilingual parallel corpus and at the same time we construct an MT system from each language in the original corpus into this new target language.", "labels": [], "entities": []}, {"text": "We introduce a novel combined measure of translation quality for multiple target language outputs (the same content from multiple source languages).", "labels": [], "entities": []}, {"text": "The multilingual setting provides new opportunities for AL over and above a single language pair.", "labels": [], "entities": []}, {"text": "This setting is similar to the multi-task AL scenario ().", "labels": [], "entities": []}, {"text": "In our case, the multiple tasks are individual machine translation tasks for several language pairs.", "labels": [], "entities": []}, {"text": "The nature of the translation processes vary from any of the source * Thanks to James Peltier for systems support for our experiments.", "labels": [], "entities": [{"text": "translation", "start_pos": 18, "end_pos": 29, "type": "TASK", "confidence": 0.9574601650238037}]}, {"text": "This research was partially supported by NSERC, and an languages to the new language depending on the characteristics of each source-target language pair, hence these tasks are competing for annotating the same resource.", "labels": [], "entities": [{"text": "NSERC", "start_pos": 41, "end_pos": 46, "type": "DATASET", "confidence": 0.92709881067276}]}, {"text": "However it maybe that in a single language pair, AL would pick a particular sentence for annotation, but in a multilingual setting, a different source language might be able to provide a good translation, thus saving annotation effort.", "labels": [], "entities": []}, {"text": "In this paper, we explore how multiple MT systems can be used to effectively pick instances that are more likely to improve training quality.", "labels": [], "entities": [{"text": "MT", "start_pos": 39, "end_pos": 41, "type": "TASK", "confidence": 0.9833311438560486}]}, {"text": "Active learning is framed as an iterative learning process.", "labels": [], "entities": []}, {"text": "In each iteration new human labeled instances (manual translations) are added to the training data based on their expected training quality.", "labels": [], "entities": []}, {"text": "However, if we start with only a small amount of initial parallel data for the new target language, then translation quality is very poor and requires a very large injection of human labeled data to be effective.", "labels": [], "entities": [{"text": "translation", "start_pos": 105, "end_pos": 116, "type": "TASK", "confidence": 0.9716802835464478}]}, {"text": "To deal with this, we use a novel framework for active learning: we assume we are given a small amount of parallel text and a large amount of monolingual source language text; using these resources, we create a large noisy parallel text which we then iteratively improve using small injections of human translations.", "labels": [], "entities": []}, {"text": "When we build multiple MT systems from multiple source languages to the new target language, each MT system can be seen as a different 'view' on the desired output translation.", "labels": [], "entities": [{"text": "MT", "start_pos": 23, "end_pos": 25, "type": "TASK", "confidence": 0.9676417112350464}]}, {"text": "Thus, we can train our multiple MT systems using either self-training or co-training).", "labels": [], "entities": [{"text": "MT", "start_pos": 32, "end_pos": 34, "type": "TASK", "confidence": 0.9854228496551514}]}, {"text": "In selftraining each MT system is re-trained using human labeled data plus its own noisy translation output on the unlabeled data.", "labels": [], "entities": [{"text": "MT", "start_pos": 21, "end_pos": 23, "type": "TASK", "confidence": 0.9834559559822083}]}, {"text": "In co-training each MT system is re-trained using human labeled data plus noisy translation output from the other MT systems in the ensemble.", "labels": [], "entities": [{"text": "MT", "start_pos": 20, "end_pos": 22, "type": "TASK", "confidence": 0.9703779816627502}]}, {"text": "We use consensus translations () as an effective method for co-training between multiple MT systems.", "labels": [], "entities": [{"text": "consensus translations", "start_pos": 7, "end_pos": 29, "type": "TASK", "confidence": 0.8238394260406494}, {"text": "MT", "start_pos": 89, "end_pos": 91, "type": "TASK", "confidence": 0.9732078909873962}]}, {"text": "This paper makes the following contributions: \u2022 We provide anew framework for multilingual MT, in which we build multiple MT systems and add anew language to an existing multilingual parallel corpus.", "labels": [], "entities": [{"text": "MT", "start_pos": 91, "end_pos": 93, "type": "TASK", "confidence": 0.6952582597732544}]}, {"text": "The multilingual set-ting allows new features for active learning which we exploit to improve translation quality while reducing annotation effort.", "labels": [], "entities": []}, {"text": "\u2022 We introduce new highly effective sentence selection methods that improve phrase-based SMT in the multilingual and single language pair setting.", "labels": [], "entities": [{"text": "sentence selection", "start_pos": 36, "end_pos": 54, "type": "TASK", "confidence": 0.6979599744081497}, {"text": "SMT", "start_pos": 89, "end_pos": 92, "type": "TASK", "confidence": 0.7342720031738281}]}, {"text": "\u2022 We describe a novel co-training based active learning framework that exploits consensus translations to effectively select only those sentences that are difficult to translate for all MT systems, thus sharing annotation cost.", "labels": [], "entities": [{"text": "MT", "start_pos": 186, "end_pos": 188, "type": "TASK", "confidence": 0.9475821256637573}]}, {"text": "\u2022 We show that using active learning to add anew language to the EuroParl corpus provides a significant improvement compared to the strong random sentence selection baseline.", "labels": [], "entities": [{"text": "EuroParl corpus", "start_pos": 65, "end_pos": 80, "type": "DATASET", "confidence": 0.9931521117687225}]}], "datasetContent": [{"text": "We pre-processed the EuroParl corpus (http://www.statmt.org/europarl) ( and built a multilingual parallel corpus with 653,513 sentences, excluding the Q4/2000 portion of the data which is reserved as the test set.", "labels": [], "entities": [{"text": "EuroParl corpus", "start_pos": 21, "end_pos": 36, "type": "DATASET", "confidence": 0.990119218826294}, {"text": "Q4/2000 portion of the data", "start_pos": 151, "end_pos": 178, "type": "DATASET", "confidence": 0.8364193609782627}]}, {"text": "We subsampled 5,000 sentences as the labeled data Land 20,000 sentences as U for the pool of untranslated sentences (while hiding the English part).", "labels": [], "entities": []}, {"text": "The test set consists of 2,000 multi-language sentences and comes from the multilingual parallel corpus built from Q4/2000 portion of the data.", "labels": [], "entities": [{"text": "Q4/2000 portion of the data", "start_pos": 115, "end_pos": 142, "type": "DATASET", "confidence": 0.8175731982503619}]}, {"text": "Let T be the union of the nbest lists of translations fora particular sentence.", "labels": [], "entities": []}, {"text": "The consensus translation t c is arg max where LM(t) is the score from a 3-gram language model, Q d (t) is the translation score generated by the decoder for M F d \u2192E if t is produced by the dth SMT model, Rd (t) is the rank of the translation in the n-best list produced by the dth model, w 4,d is a bias term for each translation model to make their scores comparable, and |t| is the length  of the translation sentence.", "labels": [], "entities": [{"text": "SMT", "start_pos": 195, "end_pos": 198, "type": "TASK", "confidence": 0.9253565669059753}]}, {"text": "The number of weights w i is 3 plus the number of source languages, and they are trained using minimum error-rate training (MERT) to maximize the BLEU score on a development set.", "labels": [], "entities": [{"text": "minimum error-rate training (MERT)", "start_pos": 95, "end_pos": 129, "type": "METRIC", "confidence": 0.840838630994161}, {"text": "BLEU score", "start_pos": 146, "end_pos": 156, "type": "METRIC", "confidence": 0.9795838594436646}]}, {"text": "We use add-smoothing where = .5 to smooth the probabilities in Sec.", "labels": [], "entities": []}, {"text": "4; moreover \u03bb = .4 for ELPR and LEPR sentence scoring and maximum phrase length k is set to 4.", "labels": [], "entities": [{"text": "LEPR sentence scoring", "start_pos": 32, "end_pos": 53, "type": "TASK", "confidence": 0.5666590631008148}]}, {"text": "For the multilingual experiments (which involve four source languages) we set \u03b1 d = .25 to make the importance of individual translation tasks equal.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Comparison of multilingual selection methods with  WER (word error rate), PER (position independent WER).  95% confidence interval for WER numbers is 0.7 and for PER  numbers is 0.5. Bold: best result, italic: significantly better.", "labels": [], "entities": [{"text": "WER (word error rate", "start_pos": 61, "end_pos": 81, "type": "METRIC", "confidence": 0.6780960023403168}, {"text": "PER", "start_pos": 84, "end_pos": 87, "type": "METRIC", "confidence": 0.9830657243728638}, {"text": "95% confidence interval", "start_pos": 117, "end_pos": 140, "type": "METRIC", "confidence": 0.6955548748373985}]}, {"text": " Table 2: For regular/OOV phrases, the KL-divergence be- tween the true distribution (P  *  ) and the estimated (P ) or uni- form (unif ) distributions are shown, where:", "labels": [], "entities": []}]}