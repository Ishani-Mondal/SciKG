{"title": [{"text": "A non-contiguous Tree Sequence Alignment-based Model for Statistical Machine Translation", "labels": [], "entities": [{"text": "Sequence Alignment-based", "start_pos": 22, "end_pos": 46, "type": "TASK", "confidence": 0.6829250156879425}, {"text": "Statistical Machine Translation", "start_pos": 57, "end_pos": 88, "type": "TASK", "confidence": 0.8170109192530314}]}], "abstractContent": [{"text": "The tree sequence based translation model allows the violation of syntactic boundaries in a rule to capture non-syntactic phrases, where a tree sequence is a contiguous sequence of sub-trees.", "labels": [], "entities": []}, {"text": "This paper goes further to present a translation model based on non-contiguous tree sequence alignment, where a non-contiguous tree sequence is a sequence of sub-trees and gaps.", "labels": [], "entities": [{"text": "translation", "start_pos": 37, "end_pos": 48, "type": "TASK", "confidence": 0.9771509170532227}]}, {"text": "Compared with the contiguous tree sequence-based model, the proposed model can well handle non-contiguous phrases with any large gaps by means of non-contiguous tree sequence alignment.", "labels": [], "entities": []}, {"text": "An algorithm targeting the non-contiguous constituent decoding is also proposed.", "labels": [], "entities": []}, {"text": "Experimental results on the NIST MT-05 Chi-nese-English translation task show that the proposed model statistically significantly outper-forms the baseline systems.", "labels": [], "entities": [{"text": "NIST MT-05 Chi-nese-English translation task", "start_pos": 28, "end_pos": 72, "type": "TASK", "confidence": 0.7936775326728821}]}], "introductionContent": [{"text": "Current research in statistical machine translation (SMT) mostly settles itself in the domain of either phrase-based or syntax-based.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 20, "end_pos": 57, "type": "TASK", "confidence": 0.8267499059438705}]}, {"text": "Between them, the phrase-based approach () allows local reordering and contiguous phrase translation.", "labels": [], "entities": [{"text": "phrase translation", "start_pos": 82, "end_pos": 100, "type": "TASK", "confidence": 0.7366749495267868}]}, {"text": "However, it is hard for phrase-based models to learn global reorderings and to deal with noncontiguous phrases.", "labels": [], "entities": []}, {"text": "To address this issue, many syntax-based approaches tend to integrate more syntactic information to enhance the non-contiguous phrase modeling.", "labels": [], "entities": [{"text": "phrase modeling", "start_pos": 127, "end_pos": 142, "type": "TASK", "confidence": 0.7270135283470154}]}, {"text": "In general, most of them achieve this goal by introducing syntactic non-terminals as translational equivalent placeholders in both source and target sides.", "labels": [], "entities": []}, {"text": "Nevertheless, the generated rules are strictly required to be derived from the contiguous translational equivalences (;;).", "labels": [], "entities": []}, {"text": "Among them, acquire the non-contiguous phrasal rules from the contiguous tree sequence pairs , and find them useless via real syntax-based translation systems.", "labels": [], "entities": []}, {"text": "However, statistically report that discontinuities are very useful for translational equivalence analysis using binary branching structures under word alignment and parse tree constraints.", "labels": [], "entities": [{"text": "translational equivalence analysis", "start_pos": 71, "end_pos": 105, "type": "TASK", "confidence": 0.927255392074585}, {"text": "word alignment", "start_pos": 146, "end_pos": 160, "type": "TASK", "confidence": 0.7010791301727295}]}, {"text": "also finds that discontinues phrasal rules make significant improvement in linguistically motivated STSG-based translation model.", "labels": [], "entities": [{"text": "STSG-based translation", "start_pos": 100, "end_pos": 122, "type": "TASK", "confidence": 0.9176669120788574}]}, {"text": "The above observations are conflicting to each other.", "labels": [], "entities": []}, {"text": "In our opinion, the non-contiguous phrasal rules themselves may not play a trivial role, as reported in.", "labels": [], "entities": []}, {"text": "We believe that the effectiveness of non-contiguous phrasal rules highly depends on how to extract and utilize them.", "labels": [], "entities": []}, {"text": "To verify the above assumption, suppose there is only one tree pair in the training data with its alignment information illustrated as . A test sentence is given in: the source sentence with its syntactic tree structure as the upper tree and the expected target output with its syntactic structure as the lower tree.", "labels": [], "entities": []}, {"text": "In the tree sequence alignment based model, in addition to the entire tree pair, it is capable to acquire the contiguous tree sequence pairs: TSP (1~4) in.", "labels": [], "entities": [{"text": "TSP", "start_pos": 142, "end_pos": 145, "type": "METRIC", "confidence": 0.8831405639648438}]}, {"text": "By means of the rules derived from these contiguous tree sequence pairs, it is easy to translate the contiguous phrase \" /time\", the only related rule is r 1 derived from TSP4 and the entire tree pair.", "labels": [], "entities": []}, {"text": "However, the source side of r 1 does not match the source tree structure of the test sentence.", "labels": [], "entities": []}, {"text": "Therefore, we can only partially translate the illustrated test sentence with this training sample.", "labels": [], "entities": []}, {"text": "A tree sequence pair in this context is a kind of translational equivalence comprised of a pair of tree sequences.", "labels": [], "entities": []}, {"text": "We illustrate the rule extraction with an example from the tree-to-tree translation model based on tree sequence alignment () without losing of generality to most syntactic tree based models.", "labels": [], "entities": [{"text": "rule extraction", "start_pos": 18, "end_pos": 33, "type": "TASK", "confidence": 0.7993656992912292}]}, {"text": "We only list the contiguous tree sequence pairs with one single sub-tree in both sides without losing of generality.", "labels": [], "entities": []}, {"text": "As discussed above, the problem lies in that the non-contiguous phrases derived from the contiguous tree sequence pairs demand greater reliance on the context.", "labels": [], "entities": []}, {"text": "Consequently, when applying those rules to unseen data, it may suffer from the data sparseness problem.", "labels": [], "entities": []}, {"text": "The expressiveness of the model also slacks due to their weak ability of generalization.", "labels": [], "entities": []}, {"text": "To address this issue, we propose a syntactic translation model based on non-contiguous tree sequence alignment.", "labels": [], "entities": [{"text": "syntactic translation", "start_pos": 36, "end_pos": 57, "type": "TASK", "confidence": 0.7095615565776825}]}, {"text": "This model extracts the translation rules not only from the contiguous tree sequence pairs but also from the non-contiguous tree sequence pairs where a non-contiguous tree sequence is a sequence of sub-trees and gaps.", "labels": [], "entities": []}, {"text": "With the help of the non-contiguous tree sequence, the proposed model can well capture the noncontiguous phrases in avoidance of the constraints of large applicability of context and enhance the non-contiguous constituent modeling.", "labels": [], "entities": []}, {"text": "As for the above example, the proposed model enables the non-contiguous tree sequence pair indexed as TSP5 in and is allowed to further derive r 2 from TSP5.", "labels": [], "entities": []}, {"text": "By means of r 2 and the same processing to the contiguous phrase \" /he \u00a1 \u00a2 /show up \u00a4 /'s\" as the contiguous tree sequence based model, we can successfully translate the entire source sentence in.", "labels": [], "entities": []}, {"text": "We define asynchronous grammar, named Synchronous non-contiguous Tree Sequence Substitution Grammar (SncTSSG), extended from synchronous tree substitution grammar (STSG:) to illustrate our model.", "labels": [], "entities": []}, {"text": "The proposed synchronous grammar is able to cover the previous proposed grammar based on tree) and tree sequence) alignment.", "labels": [], "entities": []}, {"text": "Besides, we modify the traditional parsing based decoding algorithm for syntax-based SMT to facilitate the non-contiguous constituent decoding for our model.", "labels": [], "entities": [{"text": "SMT", "start_pos": 85, "end_pos": 88, "type": "TASK", "confidence": 0.9202967286109924}]}, {"text": "To the best of our knowledge, this is the first attempt to acquire the translation rules with rich syntactic structures from the non-contiguous Translational Equivalences (non-contiguous tree sequence pairs in this context).", "labels": [], "entities": []}, {"text": "The rest of this paper is organized as follows: Section 2 presents a formal definition of our model with detailed parameterization.", "labels": [], "entities": []}, {"text": "Sections 3 and 4 elaborate the extraction of the non-contiguous tree sequence pairs and the decoding algorithm respectively.", "labels": [], "entities": []}, {"text": "The experiments we conduct to assess the effectiveness of the proposed method are reported in Section 5.", "labels": [], "entities": []}, {"text": "We finally conclude this work in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "In the experiments, we train the translation model on FBIS corpus (7.2M (Chinese) + 9.2M (English) words) and train a 4-gram language model on the Xinhua portion of the English Gigaword corpus (181M words) using the SRILM Toolkits).", "labels": [], "entities": [{"text": "FBIS corpus", "start_pos": 54, "end_pos": 65, "type": "DATASET", "confidence": 0.9164180159568787}, {"text": "English Gigaword corpus", "start_pos": 169, "end_pos": 192, "type": "DATASET", "confidence": 0.7503684163093567}, {"text": "SRILM Toolkits", "start_pos": 216, "end_pos": 230, "type": "DATASET", "confidence": 0.9476546943187714}]}, {"text": "We use these sentences with less than 50 characters from the NIST MT-2002 test set as the development set and the NIST MT-2005 test set as our test set.", "labels": [], "entities": [{"text": "NIST MT-2002 test set", "start_pos": 61, "end_pos": 82, "type": "DATASET", "confidence": 0.9230497926473618}, {"text": "NIST MT-2005 test set", "start_pos": 114, "end_pos": 135, "type": "DATASET", "confidence": 0.9351779520511627}]}, {"text": "We use the Stanford parser ( to parse bilingual sentences on the training set and Chinese sentences on the development and test set.", "labels": [], "entities": []}, {"text": "The evaluation metric is casesensitive BLEU-4 ().", "labels": [], "entities": [{"text": "BLEU-4", "start_pos": 39, "end_pos": 45, "type": "METRIC", "confidence": 0.9978777170181274}]}, {"text": "We base on the m-to-n word alignments dumped by GI-ZA++ to extract the tree sequence pairs.", "labels": [], "entities": []}, {"text": "For the MER training, we modify Koehn's version).", "labels": [], "entities": [{"text": "MER", "start_pos": 8, "end_pos": 11, "type": "TASK", "confidence": 0.8971220850944519}]}, {"text": "We use Zhang et al's implementation () for 95% confidence intervals significant test.", "labels": [], "entities": []}, {"text": "We compare the SncTSSG based model against two baseline models: the phrase-based and the STSSG-based models.", "labels": [], "entities": []}, {"text": "For the phrase-based model, we use Moses () with its default settings; for the STSSG and SncTSSG based models we use our decoder Pisces by setting the following parameters: , , , , , . Additionally, for STSSG we set , and for SncTSSG, we set . measures the contribution of different combination of rules.", "labels": [], "entities": []}, {"text": "cR refers to the rules derived from contiguous tree sequence pairs (i.e., all STSSG rules); ncPR refers to non-contiguous phrasal rules derived from contiguous tree sequence pairs with at least one non-terminal leaf node between two lexicalized leaf nodes (i.e., all non-contiguous rules in STSSG defined as in); srcncR refers to source side non-contiguous rules (SncTSSG rules only, not STSSG rules); tgtncR refers to target side noncontiguous rules (SncTSSG rules only, not STSSG rules) and src&tgtncR refers non-contiguous rules with gaps in either side (srcncR+ tgtncR).", "labels": [], "entities": []}, {"text": "The last three kinds of rules are all derived from noncontiguous tree sequence pairs.", "labels": [], "entities": []}, {"text": "1) From Exp 1 and 2 in, we find that non-contiguous phrasal rules (ncPR) derived from contiguous tree sequence pairs make little impact on the translation performance which is consistent with the discovery of.", "labels": [], "entities": []}, {"text": "However, if we append the non-contiguous phrasal rules derived from non-contiguous tree sequence pairs, no matter whether non-contiguous in source or in target, the performance statistically significantly (p < 0.05) improves (as presented in Exp 2~5), which validates our prediction that the noncontiguous rules derived from non-contiguous tree sequence pairs contribute more to the performance than those acquired from contiguous tree sequence pairs.", "labels": [], "entities": []}, {"text": "2) Not only that, after comparing Exp 6,7,8 against Exp 3,4,5 respectively, we find that the ability of rules derived from non-contiguous tree sequence pairs generally covers that of the rules derived from the contiguous tree sequence pairs, due to the slight change in BLEU score.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 270, "end_pos": 280, "type": "METRIC", "confidence": 0.9782280921936035}]}, {"text": "3) The further comparison of the noncontiguous rules from non-contiguous spans in Exp.", "labels": [], "entities": [{"text": "Exp", "start_pos": 82, "end_pos": 85, "type": "DATASET", "confidence": 0.941275417804718}]}, {"text": "6&7 as well as Exp 3&4, shows that noncontiguity in the target side in Chinese-English translation task is not so useful as that in the source side when constructing the non-contiguous phrasal rules.", "labels": [], "entities": []}, {"text": "This also validates the findings in that varying the gaps on the English side (the target side in this context) seldom reduce the hierarchical alignment failures.", "labels": [], "entities": []}, {"text": "explores the contribution of the noncontiguous translational equivalence to phrasebased models (all the rules in has no grammar tags, but a gap <***> is allowed in the last three rows).", "labels": [], "entities": []}, {"text": "tgtncBP refers to the bilingual phrases with gaps in the target side; srcncBP refers to the bilingual phrases with gaps in the source side; src&tgtncBP refers to the bilingual phrases with gaps in either side.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1 compares the performance of different  models across the two systems. The proposed  SncTSSG based model significantly outperforms  (p < 0.05) the two baseline models. Since the  SncTSSG based model covers the STSSG based  model in its modeling ability and obtains a superset  in rules, the improvement empirically verifies the  effectiveness of the additional non-contiguous  rules.", "labels": [], "entities": []}, {"text": " Table 3: Performance of bilingual phrasal rules", "labels": [], "entities": []}, {"text": " Table 2: Performance of different rule combination", "labels": [], "entities": []}]}