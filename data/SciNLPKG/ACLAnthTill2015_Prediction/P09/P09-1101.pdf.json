{"title": [{"text": "Dialogue Segmentation with Large Numbers of Volunteer Internet Annotators", "labels": [], "entities": [{"text": "Dialogue Segmentation", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.779826819896698}]}], "abstractContent": [{"text": "This paper shows the results of an experiment in dialogue segmentation.", "labels": [], "entities": [{"text": "dialogue segmentation", "start_pos": 49, "end_pos": 70, "type": "TASK", "confidence": 0.8499117195606232}]}, {"text": "In this experiment, segmentation was done on a level of analysis similar to adjacency pairs.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 20, "end_pos": 32, "type": "TASK", "confidence": 0.9811722636222839}]}, {"text": "The method of annotation was somewhat novel: volunteers were invited to participate over the Web, and their responses were aggregated using a simple voting method.", "labels": [], "entities": []}, {"text": "Though volunteers received a minimum of training, the aggregated responses of the group showed very high agreement with expert opinion.", "labels": [], "entities": []}, {"text": "The group, as a unit, performed at the top of the list of annotators, and in many cases performed as well as or better than the best annotator.", "labels": [], "entities": []}], "introductionContent": [{"text": "Aggregated human behaviour is a valuable source of information.", "labels": [], "entities": []}, {"text": "The Internet shows us many examples of collaboration as a means of resource creation.", "labels": [], "entities": []}, {"text": "Wikipedia, Amazon.com reviews, and Yahoo!", "labels": [], "entities": [{"text": "Wikipedia", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.9372195601463318}]}, {"text": "Answers are just some examples of large repositories of information powered by individuals who voluntarily contribute their time and talents.", "labels": [], "entities": []}, {"text": "Some NLP projects are now using this idea, notably the \u00d4ESP), a data collection effort presented as a game in which players label images from the Web.", "labels": [], "entities": []}, {"text": "This paper presents an extension of this collaborative volunteer ethic in the area of dialogue annotation.", "labels": [], "entities": [{"text": "dialogue annotation", "start_pos": 86, "end_pos": 105, "type": "TASK", "confidence": 0.7997360825538635}]}, {"text": "For dialogue researchers, the prospect of using volunteer annotators from the Web can bean attractive option.", "labels": [], "entities": []}, {"text": "The task of training annotators can be time-consuming, expensive, and (if inter-annotator agreement turns out to be poor) risky.", "labels": [], "entities": []}, {"text": "Getting Internet volunteers for annotation has its own pitfalls.", "labels": [], "entities": []}, {"text": "Dialogue annotation is often not very interesting, so it can be difficult to attract willing participants.", "labels": [], "entities": [{"text": "Dialogue annotation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7953846454620361}]}, {"text": "Experimenters will have little control over the conditions of the annotation and the skill of the annotators.", "labels": [], "entities": []}, {"text": "Training will be minimal, limited to whatever an average Web surfer is willing to read.", "labels": [], "entities": []}, {"text": "There may also be perverse or uncomprehending users whose answers may skew the data.", "labels": [], "entities": []}, {"text": "This project began as an exploratory study about the intuitions of language users with regard to dialogue segmentation.", "labels": [], "entities": [{"text": "dialogue segmentation", "start_pos": 97, "end_pos": 118, "type": "TASK", "confidence": 0.7546236515045166}]}, {"text": "We wanted information about how language users perceive dialogue segments, and we wanted to be able to use this information as a kind of gold standard against which we could compare the performance of an automatic dialogue segmenter.", "labels": [], "entities": []}, {"text": "For our experiment, the advantages of Internet annotation were compelling.", "labels": [], "entities": []}, {"text": "We could get free data from as many language users as we could attract, instead of just two or three well-trained experts.", "labels": [], "entities": []}, {"text": "Having more respondents meant that our results could be more readily generalised to language users as a whole.", "labels": [], "entities": []}, {"text": "We expected that multiple users would converge upon some kind of uniform result.", "labels": [], "entities": []}, {"text": "What we found (for this task at least) was that large numbers of volunteers show very strong tendencies that correspond well to expert opinion, and that these patterns of agreement are surprisingly resilient in the face of noisy input from some users.", "labels": [], "entities": []}, {"text": "We also gained some insights into the way that people perceived dialogue segments.", "labels": [], "entities": []}], "datasetContent": [{"text": "We used the WindowDiff (WD) metric to evaluate the responses of our volunteers against expert opinion (our responses).", "labels": [], "entities": []}, {"text": "The WD algorithm calculates agreement between a reference copy of the corpus and a volunteer\u00d5s hypothesis by moving a window over the utterances in the two corpora.", "labels": [], "entities": []}, {"text": "The window has a size equal to half the average segment length.", "labels": [], "entities": []}, {"text": "Within the window, the algorithm examines the number of segment boundaries in the reference and in the hypothesis, and a counter is augmented by one if they disagree.", "labels": [], "entities": []}, {"text": "The WD score between the reference and the hypothesis is equal to the number of discrepancies divided by the number of measurements taken.", "labels": [], "entities": [{"text": "WD score", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9864367544651031}]}, {"text": "A score of 0 would be given to two annotators who agree perfectly, and 1 would signify perfect disagreement.", "labels": [], "entities": []}, {"text": "shows the WD scores for the volunteers.", "labels": [], "entities": [{"text": "WD", "start_pos": 10, "end_pos": 12, "type": "TASK", "confidence": 0.6964107751846313}]}, {"text": "Most volunteers achieved a WD score between .15 and .2, with an average of \u00e1 245.", "labels": [], "entities": [{"text": "WD score", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9798192083835602}, {"text": "\u00e1 245", "start_pos": 75, "end_pos": 80, "type": "METRIC", "confidence": 0.9798715114593506}]}, {"text": "Cohen\u00d5s Kappa (!)) is another method of comparing inter-annotator agreement in segmentation that is widely used in computational language tasks.", "labels": [], "entities": []}, {"text": "It measures the observed agreement (AO) against the agreement we should expect by chance (AE), as follows: For segmentation tasks, ! is a more stringent method than WindowDiff, as it does not consider near-misses.", "labels": [], "entities": [{"text": "observed agreement (AO)", "start_pos": 16, "end_pos": 39, "type": "METRIC", "confidence": 0.7777523398399353}, {"text": "AE)", "start_pos": 90, "end_pos": 93, "type": "METRIC", "confidence": 0.9319063127040863}, {"text": "segmentation tasks", "start_pos": 111, "end_pos": 129, "type": "TASK", "confidence": 0.9237379729747772}]}, {"text": "Even so, ! scores are reported in Section 4.", "labels": [], "entities": [{"text": "! scores", "start_pos": 9, "end_pos": 17, "type": "METRIC", "confidence": 0.9555908143520355}]}, {"text": "About a third of the data came from volunteers who chose to complete all eleven of the dialogues.", "labels": [], "entities": []}, {"text": "Since they contributed so much of the data, we wanted to find out whether they were performing better than the other volunteers.", "labels": [], "entities": []}, {"text": "This group had an average WD score of .199, better than the rest of the group at .268.", "labels": [], "entities": [{"text": "WD score", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9882855117321014}]}, {"text": "However, skill does not appear to increase smoothly as more dialogues are completed.", "labels": [], "entities": [{"text": "skill", "start_pos": 9, "end_pos": 14, "type": "METRIC", "confidence": 0.9990249872207642}]}, {"text": "The highest performance came from the group that completed 5 dialogues (average WD = .187), the", "labels": [], "entities": [{"text": "WD", "start_pos": 80, "end_pos": 82, "type": "METRIC", "confidence": 0.9865556955337524}]}], "tableCaptions": [{"text": " Table 2. Summary of WD results for dialogues. Data has not been aggregated for two dialogues because they  are being held out for future work.", "labels": [], "entities": [{"text": "WD", "start_pos": 21, "end_pos": 23, "type": "TASK", "confidence": 0.6931530237197876}]}]}