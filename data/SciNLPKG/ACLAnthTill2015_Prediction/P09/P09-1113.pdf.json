{"title": [{"text": "Distant supervision for relation extraction without labeled data", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 24, "end_pos": 43, "type": "TASK", "confidence": 0.8852228820323944}]}], "abstractContent": [{"text": "Modern models of relation extraction for tasks like ACE are based on supervised learning of relations from small hand-labeled corpora.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 17, "end_pos": 36, "type": "TASK", "confidence": 0.8544475436210632}]}, {"text": "We investigate an alternative paradigm that does not require labeled corpora, avoiding the domain dependence of ACE-style algorithms, and allowing the use of corpora of any size.", "labels": [], "entities": []}, {"text": "Our experiments use Freebase, a large semantic database of several thousand relations, to provide distant supervision.", "labels": [], "entities": []}, {"text": "For each pair of entities that appears in some Freebase relation, we find all sentences containing those entities in a large un-labeled corpus and extract textual features to train a relation classifier.", "labels": [], "entities": []}, {"text": "Our algorithm combines the advantages of supervised IE (combining 400,000 noisy pattern features in a probabilistic classifier) and unsupervised IE (extracting large numbers of relations from large corpora of any domain).", "labels": [], "entities": []}, {"text": "Our model is able to extract 10,000 instances of 102 relations at a precision of 67.6%.", "labels": [], "entities": [{"text": "precision", "start_pos": 68, "end_pos": 77, "type": "METRIC", "confidence": 0.9962297081947327}]}, {"text": "We also analyze feature performance, showing that syntactic parse features are particularly helpful for relations that are ambiguous or lexically distant in their expression.", "labels": [], "entities": []}], "introductionContent": [{"text": "At least three learning paradigms have been applied to the task of extracting relational facts from text (for example, learning that a person is employed by a particular organization, or that a geographic entity is located in a particular region).", "labels": [], "entities": []}, {"text": "In supervised approaches, sentences in a corpus are first hand-labeled for the presence of entities and the relations between them.", "labels": [], "entities": []}, {"text": "The NIST Automatic Content Extraction (ACE) RDC 2003 and 2004 corpora, for example, include over 1,000 documents in which pairs of entities have been labeled with 5 to 7 major relation types and 23 to 24 subrelations, totaling 16,771 relation instances.", "labels": [], "entities": [{"text": "NIST Automatic Content Extraction (ACE) RDC 2003", "start_pos": 4, "end_pos": 52, "type": "TASK", "confidence": 0.7826441725095113}]}, {"text": "ACE systems then extract a wide variety of lexical, syntactic, and semantic features, and use supervised classifiers to label the relation mention holding between a given pair of entities in a test set sentence, optionally combining relation mentions (.", "labels": [], "entities": []}, {"text": "Supervised relation extraction suffers from a number of problems, however.", "labels": [], "entities": [{"text": "Supervised relation extraction", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.6760248839855194}]}, {"text": "Labeled training data is expensive to produce and thus limited in quantity.", "labels": [], "entities": []}, {"text": "Also, because the relations are labeled on a particular corpus, the resulting classifiers tend to be biased toward that text domain.", "labels": [], "entities": []}, {"text": "An alternative approach, purely unsupervised information extraction, extracts strings of words between entities in large amounts of text, and clusters and simplifies these word strings to produce relation-strings (;.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 45, "end_pos": 67, "type": "TASK", "confidence": 0.7778565585613251}]}, {"text": "Unsupervised approaches can use very large amounts of data and extract very large numbers of relations, but the resulting relations may not be easy to map to relations needed fora particular knowledge base.", "labels": [], "entities": []}, {"text": "A third approach has been to use a very small number of seed instances or patterns to do bootstrap learning;.", "labels": [], "entities": []}, {"text": "These seeds are used with a large corpus to extract anew set of patterns, which are used to extract more instances, which are used to extract more patterns, in an iterative fashion.", "labels": [], "entities": []}, {"text": "The resulting patterns often suffer from low precision and semantic drift.", "labels": [], "entities": [{"text": "precision", "start_pos": 45, "end_pos": 54, "type": "METRIC", "confidence": 0.9987917542457581}]}, {"text": "We propose an alternative paradigm, distant supervision, that combines some of the advantages of each of these approaches.", "labels": [], "entities": []}, {"text": "Distant supervision is an extension of the paradigm used by for exploiting WordNet to extract hypernym (is-a) relations between entities, and is similar to the use of weakly labeled data in bioinformatics: Ten relation instances extracted by our system that did not appear in Freebase.", "labels": [], "entities": []}, {"text": "Our algorithm uses Freebase (, a large semantic database, to provide distant supervision for relation extraction.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 93, "end_pos": 112, "type": "TASK", "confidence": 0.8893157541751862}]}, {"text": "Freebase contains 116 million instances of 7,300 relations between 9 million entities.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9581375122070312}]}, {"text": "The intuition of distant supervision is that any sentence that contains a pair of entities that participate in a known Freebase relation is likely to express that relation in someway.", "labels": [], "entities": []}, {"text": "Since there maybe many sentences containing a given entity pair, we can extract very large numbers of (potentially noisy) features that are combined in a logistic regression classifier.", "labels": [], "entities": []}, {"text": "Thus whereas the supervised training paradigm uses a small labeled corpus of only 17,000 relation instances as training data, our algorithm can use much larger amounts of data: more text, more relations, and more instances.", "labels": [], "entities": []}, {"text": "We use 1.2 million Wikipedia articles and 1.8 million instances of 102 relations connecting 940,000 entities.", "labels": [], "entities": []}, {"text": "In addition, combining vast numbers of features in a large classifier helps obviate problems with bad features.", "labels": [], "entities": []}, {"text": "Because our algorithm is supervised by a database, rather than by labeled text, it does not suffer from the problems of overfitting and domain-dependence that plague supervised systems.", "labels": [], "entities": []}, {"text": "Supervision by a database also means that, unlike in unsupervised approaches, the output of our classifier uses canonical names for relations.", "labels": [], "entities": []}, {"text": "Our paradigm offers a natural way of integrating data from multiple sentences to decide if a relation holds between two entities.", "labels": [], "entities": []}, {"text": "Because our algorithm can use large amounts of unlabeled data, a pair of entities may occur multiple times in the test set.", "labels": [], "entities": []}, {"text": "For each pair of entities, we aggregate the features from the many different sentences in which that pair appeared into a single feature vector, allowing us to provide our classifier with more information, resulting in more accurate labels.", "labels": [], "entities": []}, {"text": "shows examples of relation instances extracted by our system.", "labels": [], "entities": []}, {"text": "We also use this system to investigate the value of syntactic versus lexical (word sequence) features in relation extraction.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 105, "end_pos": 124, "type": "TASK", "confidence": 0.8614157140254974}]}, {"text": "While syntactic features are known to improve the performance of supervised IE, at least using clean hand-labeled ACE data (), we do not know whether syntactic features can improve the performance of unsupervised or distantly supervised IE.", "labels": [], "entities": [{"text": "IE", "start_pos": 76, "end_pos": 78, "type": "TASK", "confidence": 0.9028195142745972}]}, {"text": "Most previous research in bootstrapping or unsupervised IE has used only simple lexical features, thereby avoiding the computational expense of parsing, and the few systems that have used unsupervised IE have not compared the performance of these two types of feature.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate labels in two ways: automatically, by holding out part of the Freebase relation data during training, and comparing newly discovered relation instances against this held-out data, and manually, having humans who look at each positively labeled entity pair and mark whether the relation indeed holds between the participants.", "labels": [], "entities": [{"text": "Freebase relation data", "start_pos": 74, "end_pos": 96, "type": "DATASET", "confidence": 0.9373780886332194}]}, {"text": "Both evaluations allow us to calculate the precision of the system for the best N instances.", "labels": [], "entities": [{"text": "precision", "start_pos": 43, "end_pos": 52, "type": "METRIC", "confidence": 0.9991468191146851}]}, {"text": "shows the performance of our classifier on held-out Freebase relation data.", "labels": [], "entities": [{"text": "Freebase relation data", "start_pos": 52, "end_pos": 74, "type": "DATASET", "confidence": 0.9045394460360209}]}, {"text": "While held-out evaluation suffers from false negatives, it gives a rough measure of precision without requiring expensive human evaluation, making it useful for parameter setting.", "labels": [], "entities": [{"text": "precision", "start_pos": 84, "end_pos": 93, "type": "METRIC", "confidence": 0.9991154074668884}, {"text": "parameter setting", "start_pos": 161, "end_pos": 178, "type": "TASK", "confidence": 0.7212643623352051}]}, {"text": "At most recall levels, the combination of syntactic and lexical features offers a substantial improvement in precision over either of these feature sets on its own.", "labels": [], "entities": [{"text": "recall", "start_pos": 8, "end_pos": 14, "type": "METRIC", "confidence": 0.9964686632156372}, {"text": "precision", "start_pos": 109, "end_pos": 118, "type": "METRIC", "confidence": 0.999147891998291}]}, {"text": "Human evaluation was performed by evaluators on Amazon's Mechanical Turk service, shown to be effective for natural language annotation in.", "labels": [], "entities": [{"text": "Amazon's Mechanical Turk service", "start_pos": 48, "end_pos": 80, "type": "DATASET", "confidence": 0.8977658748626709}, {"text": "natural language annotation", "start_pos": 108, "end_pos": 135, "type": "TASK", "confidence": 0.670942356189092}]}, {"text": "We ran three experiments: one using only syntactic features; one using only lexical features; and one using both syntactic and lexical features.", "labels": [], "entities": []}, {"text": "For each of the 10 relations that appeared most frequently in our test data (according to our classifier), we took samples from the first 100 and 1000 instances of this relation generated in each experiment, and sent these to Mechanical Turk for human evaluation.", "labels": [], "entities": [{"text": "Mechanical Turk", "start_pos": 226, "end_pos": 241, "type": "DATASET", "confidence": 0.8730296492576599}]}, {"text": "Our sample size was 100.", "labels": [], "entities": []}, {"text": "Each predicted relation instance was labeled as true or false by between 1 and 3 labelers on Mechanical Turk.", "labels": [], "entities": [{"text": "Mechanical Turk", "start_pos": 93, "end_pos": 108, "type": "DATASET", "confidence": 0.9101467132568359}]}, {"text": "We assigned the truth or falsehood of each relation according to the majority vote of the labels; in the case of a tie (one vote each way) we assigned the relation as true or false with equal probability.", "labels": [], "entities": []}, {"text": "The evaluation of the syntactic, lexical, and combination of features at a recall of 100 and 1000 instances is presented in.", "labels": [], "entities": [{"text": "recall", "start_pos": 75, "end_pos": 81, "type": "METRIC", "confidence": 0.9918244481086731}]}, {"text": "At a recall of 100 instances, the combination of lexical and syntactic features has the best performance fora majority of the relations, while at a recall level of 1000 instances the results are mixed.", "labels": [], "entities": [{"text": "recall", "start_pos": 5, "end_pos": 11, "type": "METRIC", "confidence": 0.9763960242271423}, {"text": "recall", "start_pos": 148, "end_pos": 154, "type": "METRIC", "confidence": 0.9872776865959167}]}, {"text": "No feature set strongly outperforms any of the others across all relations.", "labels": [], "entities": []}], "tableCaptions": []}