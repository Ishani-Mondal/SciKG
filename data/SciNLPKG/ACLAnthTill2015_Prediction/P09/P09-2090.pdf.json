{"title": [], "abstractContent": [{"text": "We describe a set of techniques for Ara-bic cross-document coreference resolution.", "labels": [], "entities": [{"text": "Ara-bic cross-document coreference resolution", "start_pos": 36, "end_pos": 81, "type": "TASK", "confidence": 0.6316367164254189}]}, {"text": "We compare a baseline system of exact mention string-matching to ones that include local mention context information as well as information from an existing machine translation system.", "labels": [], "entities": []}, {"text": "It turns out that the machine translation-based technique outperforms the baseline, but local entity context similarity does not.", "labels": [], "entities": [{"text": "machine translation-based", "start_pos": 22, "end_pos": 47, "type": "TASK", "confidence": 0.6762691587209702}, {"text": "local entity context similarity", "start_pos": 88, "end_pos": 119, "type": "TASK", "confidence": 0.6153038591146469}]}, {"text": "This helps to point the way for future cross-document coreference work in languages with few existing resources for the task.", "labels": [], "entities": []}], "introductionContent": [{"text": "Our world contains at least two noteworthy George Bushes: President George H.", "labels": [], "entities": []}, {"text": "W. Bush and President George W. Bush.", "labels": [], "entities": []}, {"text": "They are both frequently referred to as \"George Bush.\"", "labels": [], "entities": []}, {"text": "If we wish to use a search engine to find documents about one of them, we are likely also to find documents about the other.", "labels": [], "entities": []}, {"text": "Improving our ability to find all documents referring to one and none referring to the other in a targeted search is a goal of crossdocument entity coreference detection.", "labels": [], "entities": [{"text": "crossdocument entity coreference detection", "start_pos": 127, "end_pos": 169, "type": "TASK", "confidence": 0.7941684424877167}]}, {"text": "Here we describe some results from a system we built to perform this task on Arabic documents.", "labels": [], "entities": []}, {"text": "We base our work partly on previous work done by, which has also been used in later work).", "labels": [], "entities": []}, {"text": "Other work such as) focus on techniques specific to English.", "labels": [], "entities": []}, {"text": "The main contribution of this work to crossdocument coreference lies in the conditions under which it was done.", "labels": [], "entities": [{"text": "crossdocument coreference", "start_pos": 38, "end_pos": 63, "type": "TASK", "confidence": 0.8885594606399536}]}, {"text": "Even now, there is no largescale resource-in terms of annotated data-for cross-document coreference in Arabic as there is in English (e.g. WebPeople).", "labels": [], "entities": [{"text": "cross-document coreference", "start_pos": 73, "end_pos": 99, "type": "TASK", "confidence": 0.7552440464496613}]}, {"text": "Thus, we employed techniques for high-performance processing in a resource-poor environment.", "labels": [], "entities": []}, {"text": "We provide early steps in cross-document coreference detection for resource-poor languages.", "labels": [], "entities": [{"text": "cross-document coreference detection", "start_pos": 26, "end_pos": 62, "type": "TASK", "confidence": 0.8178704778353373}]}], "datasetContent": [{"text": "We performed our experiments in the context of the Automatic Content Extraction (ACE) evaluation of 2008, run by the National Institute of Standards and Technology (NIST).", "labels": [], "entities": [{"text": "Automatic Content Extraction (ACE) evaluation of 2008", "start_pos": 51, "end_pos": 104, "type": "TASK", "confidence": 0.7289698057704501}]}, {"text": "The evaluation corpus contained approximately 10,000 documents from the following domains: broadcast conversation transcripts, broadcast news transcripts, conversational telephone speech transcripts, newswire, Usenet Newsgroup/Discussion Groups, and weblogs.", "labels": [], "entities": []}, {"text": "Systems were required to process the large source sets completely.", "labels": [], "entities": []}, {"text": "For performance measurement after the evaluation, NIST selected 412 of the Arabic source documents out of the larger set.", "labels": [], "entities": [{"text": "NIST", "start_pos": 50, "end_pos": 54, "type": "DATASET", "confidence": 0.9095329642295837}]}, {"text": "For development purposes we used the NIST ACE 2005 Arabic data with within-document ground truth.", "labels": [], "entities": [{"text": "NIST ACE 2005 Arabic data", "start_pos": 37, "end_pos": 62, "type": "DATASET", "confidence": 0.9784257292747498}]}, {"text": "This consisted of 1,245 documents.", "labels": [], "entities": []}, {"text": "We also used exactly 12,000 randomly selected documents from the LDC Arabic Gigaword Third Edition corpus, processed through Serif.", "labels": [], "entities": [{"text": "LDC Arabic Gigaword Third Edition corpus", "start_pos": 65, "end_pos": 105, "type": "DATASET", "confidence": 0.867513527472814}]}, {"text": "The Arabic Gigaword corpus was used to select a threshold of 0.4956 for the context similarity technique via inspection of (A, B) link scores by a native speaker of Arabic.", "labels": [], "entities": [{"text": "Arabic Gigaword corpus", "start_pos": 4, "end_pos": 26, "type": "DATASET", "confidence": 0.7041099071502686}]}, {"text": "It must be emphasized that there was no ground truth available for this task in Arabic.", "labels": [], "entities": []}, {"text": "Performing this task in the absence of significant training or evaluation data is one emphasis of this work.", "labels": [], "entities": []}, {"text": "We used NIST's scoring techniques to evaluate the performance of our systems.", "labels": [], "entities": [{"text": "NIST", "start_pos": 8, "end_pos": 12, "type": "DATASET", "confidence": 0.9283179640769958}]}, {"text": "Scoring for the ACE evaluation is done using an scoring script provided by NIST which produces many kinds of statistics.", "labels": [], "entities": [{"text": "ACE evaluation", "start_pos": 16, "end_pos": 30, "type": "TASK", "confidence": 0.7721855640411377}, {"text": "NIST", "start_pos": 75, "end_pos": 79, "type": "DATASET", "confidence": 0.9662039875984192}]}, {"text": "NIST mainly uses a measure called the ACE value, but it also computes B-cubed.", "labels": [], "entities": [{"text": "NIST", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9753706455230713}, {"text": "ACE value", "start_pos": 38, "end_pos": 47, "type": "METRIC", "confidence": 0.9541033804416656}, {"text": "B-cubed", "start_pos": 70, "end_pos": 77, "type": "METRIC", "confidence": 0.9768532514572144}]}, {"text": "B-Cubed represents the task of finding crossdocument entities in the following way: if a user of the system is searching fora particular Bush and finds document D, he or she should be able to find all of the other documents with the same Bush in them as links from D-that is, cross-document entities represent graphs connecting documents.", "labels": [], "entities": []}, {"text": "Bagga and Baldwin are able to define precision, recall, and F-measure over a collection of documents in this way.", "labels": [], "entities": [{"text": "precision", "start_pos": 37, "end_pos": 46, "type": "METRIC", "confidence": 0.9993033409118652}, {"text": "recall", "start_pos": 48, "end_pos": 54, "type": "METRIC", "confidence": 0.9991518259048462}, {"text": "F-measure", "start_pos": 60, "end_pos": 69, "type": "METRIC", "confidence": 0.9989675283432007}]}, {"text": "The ACE Value represents a score similar to B-Cubed, except that every mention and withindocument entity is weighted in NIST's specification by a number of factors.", "labels": [], "entities": [{"text": "ACE Value", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.8209888339042664}]}, {"text": "Every entity is worth 1 point, a missing entity worth 0, and attribute errors are discounted by multiplying by a factor (0.75 for CLASS, 0.5 for TYPE, and 0.9 for SUBTYPE).", "labels": [], "entities": [{"text": "TYPE", "start_pos": 145, "end_pos": 149, "type": "METRIC", "confidence": 0.7194297313690186}]}, {"text": "Before scoring can be accomplished, the entities found by the system must be mapped onto those found in the reference provided by NIST.", "labels": [], "entities": [{"text": "NIST", "start_pos": 130, "end_pos": 134, "type": "DATASET", "confidence": 0.967492938041687}]}, {"text": "The ACE scorer does this document-by-document, selecting the mapping that produces the highest score.", "labels": [], "entities": []}, {"text": "A description of the evaluation method and entity categorization is available at (NIST, 2008).", "labels": [], "entities": [{"text": "NIST, 2008)", "start_pos": 82, "end_pos": 93, "type": "DATASET", "confidence": 0.9547343105077744}]}], "tableCaptions": [{"text": " Table 1: Scores from ACE evaluation script.", "labels": [], "entities": [{"text": "ACE evaluation script", "start_pos": 22, "end_pos": 43, "type": "DATASET", "confidence": 0.7212392886479696}]}]}