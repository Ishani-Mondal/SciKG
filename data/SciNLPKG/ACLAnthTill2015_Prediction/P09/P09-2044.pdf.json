{"title": [{"text": "Finding Hedges by Chasing Weasels: Hedge Detection Using Wikipedia Tags and Shallow Linguistic Features", "labels": [], "entities": [{"text": "Finding Hedges", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.8869994282722473}, {"text": "Hedge Detection", "start_pos": 35, "end_pos": 50, "type": "TASK", "confidence": 0.8273174166679382}]}], "abstractContent": [{"text": "We investigate the automatic detection of sentences containing linguistic hedges using corpus statistics and syntactic patterns.", "labels": [], "entities": [{"text": "automatic detection of sentences containing linguistic hedges", "start_pos": 19, "end_pos": 80, "type": "TASK", "confidence": 0.8161442492689405}]}, {"text": "We take Wikipedia as an already annotated corpus using its tagged weasel words which mark sentences and phrases as non-factual.", "labels": [], "entities": []}, {"text": "We evaluate the quality of Wikipedia as training data for hedge detection , as well as shallow linguistic features.", "labels": [], "entities": [{"text": "hedge detection", "start_pos": 58, "end_pos": 73, "type": "TASK", "confidence": 0.8265074193477631}]}], "introductionContent": [{"text": "While most research in natural language processing is dealing with identifying, extracting and classifying facts, recent years have seen a surge in research on sentiment and subjectivity (see for an overview).", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 23, "end_pos": 50, "type": "TASK", "confidence": 0.6671104033788046}, {"text": "identifying, extracting and classifying facts", "start_pos": 67, "end_pos": 112, "type": "TASK", "confidence": 0.746160497268041}]}, {"text": "However, even opinions have to be backed up by facts to be effective as arguments.", "labels": [], "entities": []}, {"text": "Distinguishing facts from fiction requires to detect subtle variations in the use of linguistic devices such as linguistic hedges which indicate that speakers do not backup their opinions with facts.", "labels": [], "entities": [{"text": "Distinguishing facts from fiction", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.8895662128925323}]}, {"text": "Many NLP applications could benefit from identifying linguistic hedges, e.g. question answering systems (), information extraction from biomedical documents, and deception detection (.", "labels": [], "entities": [{"text": "question answering", "start_pos": 77, "end_pos": 95, "type": "TASK", "confidence": 0.8523184061050415}, {"text": "information extraction from biomedical documents", "start_pos": 108, "end_pos": 156, "type": "TASK", "confidence": 0.8424715161323547}, {"text": "deception detection", "start_pos": 162, "end_pos": 181, "type": "TASK", "confidence": 0.9437491595745087}]}, {"text": "While NLP research on classifying linguistic hedges has been restricted to analysing biomedical documents, the above (incomplete) list of applications suggests that domain-and languageindependent approaches for hedge detection need to be developed.", "labels": [], "entities": [{"text": "classifying linguistic hedges", "start_pos": 22, "end_pos": 51, "type": "TASK", "confidence": 0.8353126247723898}, {"text": "hedge detection", "start_pos": 211, "end_pos": 226, "type": "TASK", "confidence": 0.7497311532497406}]}, {"text": "We investigate Wikipedia as a source of training data for hedge classification.", "labels": [], "entities": [{"text": "hedge classification", "start_pos": 58, "end_pos": 78, "type": "TASK", "confidence": 0.860528439283371}]}, {"text": "We adopt Wikipedia's notion of weasel words which we argue to be closely related to hedges and private states.", "labels": [], "entities": []}, {"text": "Many Wikipedia articles contain a specific weasel tag, so that Wikipedia can be viewed as a readily annotated corpus.", "labels": [], "entities": []}, {"text": "Based on this data, we have built a system to detect sentences that contain linguistic hedges.", "labels": [], "entities": []}, {"text": "We compare a baseline relying on word frequency measures with one combining word frequency with shallow linguistic features.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: F-scores at different thresholds (bold at  the precision/recall break-even-points determined  on the development data)", "labels": [], "entities": [{"text": "F-scores", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9980484247207642}, {"text": "precision", "start_pos": 57, "end_pos": 66, "type": "METRIC", "confidence": 0.9993083477020264}, {"text": "recall", "start_pos": 67, "end_pos": 73, "type": "METRIC", "confidence": 0.9274649620056152}]}]}