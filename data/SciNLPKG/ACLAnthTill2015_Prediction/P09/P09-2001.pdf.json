{"title": [{"text": "Variational Inference for Grammar Induction with Prior Knowledge", "labels": [], "entities": [{"text": "Variational Inference", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.8406862318515778}, {"text": "Grammar Induction", "start_pos": 26, "end_pos": 43, "type": "TASK", "confidence": 0.7659257352352142}]}], "abstractContent": [{"text": "Variational EM has become a popular technique in probabilistic NLP with hidden variables.", "labels": [], "entities": [{"text": "Variational EM", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.6186307370662689}]}, {"text": "Commonly, for computational tractability, we make strong independence assumptions, such as the mean-field assumption, in approximating posterior distributions over hidden variables.", "labels": [], "entities": []}, {"text": "We show how a looser restriction on the approximate posterior, requiring it to be a mixture, can help inject prior knowledge to exploit soft constraints during the varia-tional E-step.", "labels": [], "entities": []}], "introductionContent": [{"text": "Learning natural language in an unsupervised way commonly involves the expectation-maximization (EM) algorithm to optimize the parameters of a generative model, often a probabilistic grammar.", "labels": [], "entities": []}, {"text": "Later approaches include variational EM in a Bayesian setting, which has been shown to obtain even better results for various natural language tasks over EM (e.g.,.", "labels": [], "entities": []}, {"text": "Variational EM usually makes the mean-field assumption, factoring the posterior over hidden variables into independent distributions.", "labels": [], "entities": []}, {"text": "showed how to use a less strict assumption: a mixture of factorized distributions.", "labels": [], "entities": []}, {"text": "In other work, soft or hard constraints on the posterior during the E-step have been explored in order to improve performance.", "labels": [], "entities": []}, {"text": "For example, have penalized the approximate posterior over dependency structures in a natural language grammar induction task to avoid long range dependencies between words.", "labels": [], "entities": [{"text": "natural language grammar induction task", "start_pos": 86, "end_pos": 125, "type": "TASK", "confidence": 0.7453471660614014}]}, {"text": "added linear constraints on expected values of features of the hidden variables in an alignment task.", "labels": [], "entities": []}, {"text": "In this paper, we use posterior mixtures to inject bias or prior knowledge into a Bayesian model.", "labels": [], "entities": []}, {"text": "We show that empirically, injecting prior knowledge improves performance on an unsupervised Chinese grammar induction task.", "labels": [], "entities": [{"text": "Chinese grammar induction task", "start_pos": 92, "end_pos": 122, "type": "TASK", "confidence": 0.7227711752057076}]}], "datasetContent": [{"text": "We tested our method on the unsupervised learning problem of dependency grammar induction.", "labels": [], "entities": [{"text": "dependency grammar induction", "start_pos": 61, "end_pos": 89, "type": "TASK", "confidence": 0.750006894270579}]}, {"text": "For the generative model, we used the dependency model with valence as it appears in.", "labels": [], "entities": []}, {"text": "We used the data from the Chinese treebank).", "labels": [], "entities": [{"text": "Chinese treebank", "start_pos": 26, "end_pos": 42, "type": "DATASET", "confidence": 0.9250603318214417}]}, {"text": "Following standard practice, sentences were stripped of words and punctuation, leaving part-of-speech tags for the unsupervised induction of dependency structure, and sentences of length more than 10 were removed from the set.", "labels": [], "entities": []}, {"text": "We experimented with a Dirichlet prior over the parameters and logistic normal priors over the parameters, and found the latter to still be favorable with our method, as in.", "labels": [], "entities": []}, {"text": "We therefore report results with our method only for the logistic normal prior.", "labels": [], "entities": []}, {"text": "We do inference on sections 1-270 and 301-1151 of CTB10 (4,909 sentences) by running the EM algorithm for 20 iterations, for which all algorithms have their variational bound converge.", "labels": [], "entities": [{"text": "CTB10", "start_pos": 50, "end_pos": 55, "type": "DATASET", "confidence": 0.9569807648658752}]}, {"text": "To evaluate performance, we report the fraction of words whose predicted parent matches the gold standard (attachment accuracy).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 118, "end_pos": 126, "type": "METRIC", "confidence": 0.7864288687705994}]}, {"text": "For parsing, we use the minimum Bayes risk parse.", "labels": [], "entities": [{"text": "parsing", "start_pos": 4, "end_pos": 11, "type": "TASK", "confidence": 0.9931119680404663}]}, {"text": "Our mixture components Q i are based on simple linguistic tendencies of Chinese syntax.", "labels": [], "entities": []}, {"text": "These observations include the tendency of dependencies to (a) emanate from the right of the current position and (b) connect words which are nearby (in string distance).", "labels": [], "entities": []}, {"text": "We experiment with six mixture components: (1) RIGHTATTACH: Each word's parent is to the word's right.", "labels": [], "entities": [{"text": "RIGHTATTACH", "start_pos": 47, "end_pos": 58, "type": "METRIC", "confidence": 0.9971351623535156}]}, {"text": "The root, therefore, is always the rightmost word; (2) ALLRIGHT: The rightmost word is the parent of all positions in the sentence (there is only one such tree);   word is governed by the word to its right; (4) VER-BASROOT: Only verbs can attach to the wall node $; (5) NOUNSEQUENCE: Every sequence of n NN (nouns) is assumed to be a noun phrase, hence the first n \u2212 1 NNs are attached to the last NN; and (6) SHORTDEP: Allow only dependencies of length four or less.", "labels": [], "entities": [{"text": "ALLRIGHT", "start_pos": 55, "end_pos": 63, "type": "METRIC", "confidence": 0.9894759654998779}, {"text": "VER-BASROOT", "start_pos": 211, "end_pos": 222, "type": "METRIC", "confidence": 0.9888299107551575}, {"text": "SHORTDEP", "start_pos": 410, "end_pos": 418, "type": "METRIC", "confidence": 0.9699425101280212}]}, {"text": "This is a strict model reminiscent of the successful application of structural bias to grammar induction ().", "labels": [], "entities": [{"text": "grammar induction", "start_pos": 87, "end_pos": 104, "type": "TASK", "confidence": 0.7185037434101105}]}, {"text": "These components are added to a variational DMV model without the sum-to-1 constraint on \u03b8.", "labels": [], "entities": []}, {"text": "This complements variational techniques which state that the optimal solution during the E-step for the mean-field variational EM algorithm is a weighted grammar of the same form of p(x, y | \u03b8) (DMV in our case).", "labels": [], "entities": []}, {"text": "Using the mixture components this way has the effect of smoothing the estimated grammar event counts during the E-step, in the direction of some prior expectations.", "labels": [], "entities": []}, {"text": "Let \u03bb 1 correspond to the component of the original DMV model, and let \u03bb 2 correspond to one of the components from the above list.", "labels": [], "entities": []}, {"text": "Variational techniques show that if we let \u03bb 1 obtain the value 1, then the optimal solution will be \u03bb 1 = 1 and \u03bb 2 = 0.", "labels": [], "entities": []}, {"text": "We therefore restrict \u03bb 1 to be smaller than 1.", "labels": [], "entities": []}, {"text": "More specifically, we use an annealing process which starts by limiting \u03bb 1 to be \u2264 s = 0.85 (and hence limits \u03bb 2 to be \u2265 0.15) and increases sat each step by 1% until s reaches 0.95.", "labels": [], "entities": []}, {"text": "In addition, we also ran the algorithm with \u03bb 1 fixed at 0.85 and \u03bb 1 fixed at 0.95 to check the effectiveness of annealing on the simplex.", "labels": [], "entities": []}, {"text": "describes the results of our experiments.", "labels": [], "entities": []}, {"text": "In general, using additional mixture components has a clear advantage over the mean-field assumption.", "labels": [], "entities": []}, {"text": "The best result with a single mixture is achieved with annealing, and the VERBAS-ROOT component.", "labels": [], "entities": [{"text": "VERBAS-ROOT", "start_pos": 74, "end_pos": 85, "type": "METRIC", "confidence": 0.9848985075950623}]}, {"text": "A combination of the mixtures (RIGHTATTACH) together with VERBAS-ROOT and SHORTDEP led to an additional improvement, implying that proper selection of several mixture components together can achieve a performance gain.", "labels": [], "entities": [{"text": "RIGHTATTACH", "start_pos": 31, "end_pos": 42, "type": "METRIC", "confidence": 0.9844928979873657}, {"text": "VERBAS-ROOT", "start_pos": 58, "end_pos": 69, "type": "METRIC", "confidence": 0.9921568632125854}, {"text": "SHORTDEP", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.9703012108802795}]}], "tableCaptions": [{"text": " Table 1: Results (attachment accuracy). The baselines are  LEFTCHAIN as a parsing model (attaches each word to the  word on its right), non-Bayesian EM, and mean-field vari- ational EM without any constraints. These are compared  against the six mixture components mentioned in the text. (I)  corresponds to simplex annealing experiments (\u03bb", "labels": [], "entities": [{"text": "accuracy", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9299414753913879}, {"text": "LEFTCHAIN", "start_pos": 60, "end_pos": 69, "type": "METRIC", "confidence": 0.9972081780433655}]}]}