{"title": [{"text": "Phrase-Based Statistical Machine Translation as a Traveling Salesman Problem", "labels": [], "entities": [{"text": "Phrase-Based Statistical Machine Translation", "start_pos": 0, "end_pos": 44, "type": "TASK", "confidence": 0.8073819577693939}]}], "abstractContent": [{"text": "An efficient decoding algorithm is a crucial element of any statistical machine translation system.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 60, "end_pos": 91, "type": "TASK", "confidence": 0.6330972711245219}]}, {"text": "Some researchers have noted certain similarities between SMT decoding and the famous Traveling Salesman Problem; in particular (Knight, 1999) has shown that any TSP instance can be mapped to a sub-case of a word-based SMT model, demonstrating NP-hardness of the decoding task.", "labels": [], "entities": [{"text": "SMT decoding", "start_pos": 57, "end_pos": 69, "type": "TASK", "confidence": 0.9090594053268433}]}, {"text": "In this paper, we focus on the reverse mapping, showing that any phrase-based SMT decoding problem can be directly reformulated as a TSP.", "labels": [], "entities": [{"text": "SMT decoding problem", "start_pos": 78, "end_pos": 98, "type": "TASK", "confidence": 0.8970136841138204}]}, {"text": "The transformation is very natural, deepens our understanding of the decoding problem, and allows direct use of any of the powerful existing TSP solvers for SMT decoding.", "labels": [], "entities": [{"text": "TSP solvers", "start_pos": 141, "end_pos": 152, "type": "TASK", "confidence": 0.8956665694713593}, {"text": "SMT decoding", "start_pos": 157, "end_pos": 169, "type": "TASK", "confidence": 0.92369145154953}]}, {"text": "We test our approach on three datasets, and compare a TSP-based de-coder to the popular beam-search algorithm.", "labels": [], "entities": []}, {"text": "In all cases, our method provides competitive or better performance.", "labels": [], "entities": []}], "introductionContent": [{"text": "Phrase-based systems ( are probably the most widespread class of Statistical Machine Translation systems, and arguably one of the most successful.", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 65, "end_pos": 96, "type": "TASK", "confidence": 0.810317854086558}]}, {"text": "They use aligned sequences of words, called biphrases, as building blocks for translations, and score alternative candidate translations for the same source sentence based on a log-linear model of the conditional probability of target sentences given the source sentence: where the h k are features, that is, functions of the source string S, of the target string T , and of the * This work was conducted during an internship at XRCE.", "labels": [], "entities": [{"text": "XRCE", "start_pos": 429, "end_pos": 433, "type": "DATASET", "confidence": 0.9343927502632141}]}, {"text": "alignment a, where the alignment is a representation of the sequence of biphrases that where used in order to build T from S; The \u03bb k 's are weights and Z S is a normalization factor that guarantees that p is a proper conditional probability distribution over the pairs (T, A).", "labels": [], "entities": []}, {"text": "Some features are local, i.e. decompose over biphrases and can be precomputed and stored in advance.", "labels": [], "entities": []}, {"text": "These typically include forward and reverse phrase conditional probability features log p( \u02dc t|\u02dcst|\u02dcs) as well as log p(\u02dc s|\u02dcts|\u02dc s|\u02dct), where\u02dcswhere\u02dc where\u02dcs is the source side of the biphrase and\u02dctand\u02dc and\u02dct the target side, and the so-called \"phrase penalty\" and \"word penalty\" features, which count the number of phrases and words in the alignment.", "labels": [], "entities": []}, {"text": "Other features are non-local, i.e. depend on the order in which biphrases appear in the alignment.", "labels": [], "entities": []}, {"text": "Typical non-local features include one or more n-gram language models as well as a distortion feature, measuring by how much the order of biphrases in the candidate translation deviates from their order in the source sentence.", "labels": [], "entities": [{"text": "distortion", "start_pos": 83, "end_pos": 93, "type": "METRIC", "confidence": 0.9635478258132935}]}, {"text": "Given such a model, where the \u03bb i 's have been tuned on a development set in order to minimize some error rate (see e.g. (), together with a library of biphrases extracted from some large training corpus, a decoder implements the actual search among alternative translations: (a * , T * ) = arg max P (T, a|S).", "labels": [], "entities": []}, {"text": "The decoding problem (2) is a discrete optimization problem.", "labels": [], "entities": []}, {"text": "Usually, it is very hard to find the exact optimum and, therefore, an approximate solution is used.", "labels": [], "entities": []}, {"text": "Currently, most decoders are based on some variant of a heuristic left-to-right search, that is, they attempt to build a candidate translation (a, T ) incrementally, from left to right, extending the current partial translation at each step with anew biphrase, and computing a score composed of two contributions: one for the known elements of the partial translation so far, and one a heuristic estimate of the remaining cost for completing the translation.", "labels": [], "entities": []}, {"text": "The variant which is mostly used is a form of beam-search, where several partial candidates are maintained in parallel, and candidates for which the current score is too low are pruned in favor of candidates that are more promising.", "labels": [], "entities": []}, {"text": "We will see in the next section that some characteristics of beam-search make it a suboptimal choice for phrase-based decoding, and we will propose an alternative.", "labels": [], "entities": []}, {"text": "This alternative is based on the observation that phrase-based decoding can be very naturally cast as a Traveling Salesman Problem (TSP), one of the best studied problems in combinatorial optimization.", "labels": [], "entities": []}, {"text": "We will show that this formulation is not only a powerful conceptual device for reasoning on decoding, but is also practically convenient: in the same amount of time, off-the-shelf TSP solvers can find higher scoring solutions than the state-of-the art beam-search decoder implemented in Moses ().", "labels": [], "entities": [{"text": "TSP solvers", "start_pos": 181, "end_pos": 192, "type": "TASK", "confidence": 0.9318669140338898}]}], "datasetContent": [{"text": "In this section we consider two real translation tasks, namely, translation from English to French, trained on Europarl () and translation from German to Spanish training on the NewsCommentary corpus.", "labels": [], "entities": [{"text": "translation from English to French", "start_pos": 64, "end_pos": 98, "type": "TASK", "confidence": 0.882997465133667}, {"text": "Europarl", "start_pos": 111, "end_pos": 119, "type": "DATASET", "confidence": 0.9793477654457092}, {"text": "translation from German to Spanish", "start_pos": 127, "end_pos": 161, "type": "TASK", "confidence": 0.8656302213668823}, {"text": "NewsCommentary corpus", "start_pos": 178, "end_pos": 199, "type": "DATASET", "confidence": 0.9686228930950165}]}, {"text": "For Europarl, the training set includes 2.81 million sentences, and the test set 500.", "labels": [], "entities": [{"text": "Europarl", "start_pos": 4, "end_pos": 12, "type": "DATASET", "confidence": 0.9508729577064514}]}, {"text": "For NewsCommentary the training set is smaller: around 63k sentences, with a test set of 500 sentences.", "labels": [], "entities": [{"text": "NewsCommentary", "start_pos": 4, "end_pos": 18, "type": "DATASET", "confidence": 0.944082498550415}]}, {"text": "presents Decoder and Bleu scores as functions of time for the two corpuses.", "labels": [], "entities": []}, {"text": "Since in the real translation task, the size of the TSP graph is much larger than in the artificial reordering task (in our experiments the median size of the TSP graph was around 400 nodes, sometimes growing up to 2000 nodes), directly applying the exact TSP solver would take too long; instead we use the approximate LK algorithm and compare it to Beam-Search.", "labels": [], "entities": [{"text": "TSP solver", "start_pos": 256, "end_pos": 266, "type": "TASK", "confidence": 0.7857175171375275}]}, {"text": "The efficiency of the LK algorithm can be significantly increased by using a good initialization.", "labels": [], "entities": []}, {"text": "To compare the quality of the LK and Beam-Search methods we take a rough initial solution produced by the Beam-Search algorithm using a small value for the stack size and then use it as initial point, both for the LK algorithm and for further Beam-Search optimization (where as before we vary the Beam-Search thresholds in order to trade quality for time).", "labels": [], "entities": []}, {"text": "In the case of the Europarl corpus, we observe that LK outperforms Beam-Search in terms of the Decoder score as well as in terms of the BLEU score.", "labels": [], "entities": [{"text": "Europarl corpus", "start_pos": 19, "end_pos": 34, "type": "DATASET", "confidence": 0.99315345287323}, {"text": "BLEU score", "start_pos": 136, "end_pos": 146, "type": "METRIC", "confidence": 0.9708439111709595}]}, {"text": "Note that the difference between the two algorithms increases steeply at the beginning, which means that we can significantly increase the quality of the Beam-Search solution by using the LK algorithm at a very small price.", "labels": [], "entities": []}, {"text": "In addition, it is important to note that the BLEU scores obtained in these experiments correspond to feature weights, in the log-linear model (1), that have been optimized for the Moses decoder, but not for the TSP decoder: optimizing these parameters relatively to the TSP decoder could improve its BLEU scores still further.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 46, "end_pos": 50, "type": "METRIC", "confidence": 0.9985727071762085}, {"text": "BLEU", "start_pos": 301, "end_pos": 305, "type": "METRIC", "confidence": 0.9983568787574768}]}, {"text": "On the News corpus, again, LK outperforms Beam-Search in terms of the Decoder score.", "labels": [], "entities": [{"text": "News corpus", "start_pos": 7, "end_pos": 18, "type": "DATASET", "confidence": 0.9521164298057556}]}, {"text": "The situation with the BLEU score is more confuse.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 23, "end_pos": 33, "type": "METRIC", "confidence": 0.9580687880516052}]}, {"text": "Both algorithms do not show any clear score improvement with increasing running time which suggests that the decoder's objective function is not very well correlated with the BLEU score on this corpus.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 175, "end_pos": 179, "type": "METRIC", "confidence": 0.9989319443702698}]}], "tableCaptions": []}