{"title": [{"text": "Combining Lexical Semantic Resources with Question & Answer Archives for Translation-Based Answer Finding", "labels": [], "entities": [{"text": "Translation-Based Answer Finding", "start_pos": 73, "end_pos": 105, "type": "TASK", "confidence": 0.915213922659556}]}], "abstractContent": [{"text": "Monolingual translation probabilities have recently been introduced in retrieval models to solve the lexical gap problem.", "labels": [], "entities": []}, {"text": "They can be obtained by training statistical translation models on parallel mono-lingual corpora, such as question-answer pairs, where answers act as the \"source\" language and questions as the \"target\" language.", "labels": [], "entities": []}, {"text": "In this paper, we propose to use as a parallel training dataset the definitions and glosses provided for the same term by different lexical semantic resources.", "labels": [], "entities": []}, {"text": "We compare monolingual translation models built from lexical semantic resources with two other kinds of datasets: manually-tagged question reformulations and question-answer pairs.", "labels": [], "entities": []}, {"text": "We also show that the monolingual translation probabilities obtained (i) are comparable to traditional semantic relatedness measures and (ii) significantly improve the results over the query likelihood and the vector-space model for answer finding.", "labels": [], "entities": [{"text": "answer finding", "start_pos": 233, "end_pos": 247, "type": "TASK", "confidence": 0.9201049208641052}]}], "introductionContent": [{"text": "The lexical gap (or lexical chasm) often observed between queries and documents or questions and answers is a pervasive problem both in Information Retrieval (IR) and Question Answering (QA).", "labels": [], "entities": [{"text": "Information Retrieval (IR)", "start_pos": 136, "end_pos": 162, "type": "TASK", "confidence": 0.8210570931434631}, {"text": "Question Answering (QA)", "start_pos": 167, "end_pos": 190, "type": "TASK", "confidence": 0.8556710004806518}]}, {"text": "This problem arises from alternative ways of conveying the same information, due to synonymy or paraphrasing, and is especially severe for retrieval over shorter documents, such as sentence retrieval or question retrieval in Question & Answer archives.", "labels": [], "entities": [{"text": "sentence retrieval", "start_pos": 181, "end_pos": 199, "type": "TASK", "confidence": 0.7676632702350616}, {"text": "question retrieval in Question & Answer archives", "start_pos": 203, "end_pos": 251, "type": "TASK", "confidence": 0.7474302564348493}]}, {"text": "Several solutions to this problem have been proposed including query expansion (, query reformulation or paraphrasing ( and semantic information retrieval.", "labels": [], "entities": [{"text": "query expansion", "start_pos": 63, "end_pos": 78, "type": "TASK", "confidence": 0.8397305607795715}, {"text": "query reformulation", "start_pos": 82, "end_pos": 101, "type": "TASK", "confidence": 0.7031847238540649}, {"text": "semantic information retrieval", "start_pos": 124, "end_pos": 154, "type": "TASK", "confidence": 0.6549876630306244}]}, {"text": "Berger and Lafferty (1999) have formulated a further solution to the lexical gap problem consisting in integrating monolingual statistical translation models in the retrieval process.", "labels": [], "entities": []}, {"text": "Monolingual translation models encode statistical word associations which are trained on parallel monolingual corpora.", "labels": [], "entities": [{"text": "Monolingual translation", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.7660883069038391}]}, {"text": "The major drawback of this approach lies in the limited availability of truly parallel monolingual corpora.", "labels": [], "entities": []}, {"text": "In practice, training data for translation-based retrieval often consist in question-answer pairs, usually extracted from the evaluation corpus itself (.", "labels": [], "entities": [{"text": "translation-based retrieval", "start_pos": 31, "end_pos": 58, "type": "TASK", "confidence": 0.9300791025161743}]}, {"text": "While collectionspecific translation models effectively encode statistical word associations for the target document collection, it also introduces a bias in the evaluation and makes it difficult to assess the quality of the translation model per se, independently from a specific task and document collection.", "labels": [], "entities": []}, {"text": "In this paper, we propose new kinds of datasets for training domain-independent monolingual translation models.", "labels": [], "entities": []}, {"text": "We use the definitions and glosses provided for the same term by different lexical semantic resources to automatically train the translation models.", "labels": [], "entities": []}, {"text": "This approach has been very recently made possible by the emergence of new kinds of lexical semantic and encyclopedic resources such as Wikipedia and Wiktionary.", "labels": [], "entities": [{"text": "Wikipedia", "start_pos": 136, "end_pos": 145, "type": "DATASET", "confidence": 0.9499195218086243}]}, {"text": "These resources are freely available, up-to-date and have abroad coverage and good quality.", "labels": [], "entities": []}, {"text": "Thanks to the combination of several resources, it is possible to obtain monolingual parallel corpora which are large enough to train domain-independent translation models.", "labels": [], "entities": []}, {"text": "In addition, we collected question-answer pairs and manually-tagged question reformulations from asocial Q&A site.", "labels": [], "entities": []}, {"text": "We use these datasets to build further translation models.", "labels": [], "entities": []}, {"text": "Translation-based retrieval models have been widely used in practice by the IR and QA community.", "labels": [], "entities": [{"text": "Translation-based retrieval", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.9135340452194214}]}, {"text": "However, the quality of the semantic information encoded in the translation tables has never been assessed intrinsically.", "labels": [], "entities": []}, {"text": "To do so, we compare translation probabilities with concept vector based semantic relatedness measures with respect to human relatedness rankings for reference word pairs.", "labels": [], "entities": []}, {"text": "This study provides empirical evidence for the high quality of the semantic information encoded in statistical word translation tables.", "labels": [], "entities": [{"text": "statistical word translation", "start_pos": 99, "end_pos": 127, "type": "TASK", "confidence": 0.5989934305349985}]}, {"text": "We then use the translation models in an answer finding task based on anew question-answer dataset which is totally independent from the resources used for training the translation models.", "labels": [], "entities": [{"text": "answer finding task", "start_pos": 41, "end_pos": 60, "type": "TASK", "confidence": 0.8739757140477499}]}, {"text": "This extrinsic evaluation shows that our translation models significantly improve the results over the query likelihood and the vector-space model.", "labels": [], "entities": []}, {"text": "The remainder of the paper is organised as follows.", "labels": [], "entities": []}, {"text": "Section 2 discusses related work on semantic relatedness and statistical translation models for retrieval.", "labels": [], "entities": []}, {"text": "Section 3 presents the monolingual parallel datasets we used for obtaining monolingual translation probabilities.", "labels": [], "entities": []}, {"text": "Semantic relatedness experiments are detailed in Section 4.", "labels": [], "entities": []}, {"text": "Section 5 presents answer finding experiments.", "labels": [], "entities": [{"text": "answer finding", "start_pos": 19, "end_pos": 33, "type": "TASK", "confidence": 0.966590404510498}]}, {"text": "Finally, we conclude in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to obtain parallel training data for the translation models, we collected three different datasets: manually-tagged question reformulations and question-answer pairs from the WikiAnswers social Q&A site (Section 3.1), and glosses from WordNet, Wiktionary, Wikipedia and Simple Wikipedia (Section 3.2).", "labels": [], "entities": [{"text": "Wikipedia", "start_pos": 265, "end_pos": 274, "type": "DATASET", "confidence": 0.9432453513145447}, {"text": "Simple Wikipedia", "start_pos": 279, "end_pos": 295, "type": "DATASET", "confidence": 0.7459144294261932}]}, {"text": "In order to investigate the role played by different kinds of training data, we combined the several translation models, using the two methods described by.", "labels": [], "entities": []}, {"text": "The first method consists in a linear combination of the word-to-word translation probabilities after training: where \u03b1 + \u03b3 + \u03b4 = 1.", "labels": [], "entities": [{"text": "word-to-word translation", "start_pos": 57, "end_pos": 81, "type": "TASK", "confidence": 0.6501648873090744}]}, {"text": "This approach will be labelled with the Lin subscript.", "labels": [], "entities": []}, {"text": "The second method consists in pooling the training datasets, i.e. concatenating the parallel corpora, before training.", "labels": [], "entities": []}, {"text": "This approach will be labelled with the Pool subscript.", "labels": [], "entities": []}, {"text": "Examples for word-to-word translations obtained with this type of combination can be found in the last column for each word in.", "labels": [], "entities": []}, {"text": "The ALL Pool setting corresponds to the pooling of all three parallel datasets: WAQ+WAQA+LSR.", "labels": [], "entities": [{"text": "LSR", "start_pos": 89, "end_pos": 92, "type": "METRIC", "confidence": 0.9272565245628357}]}, {"text": "The aim of this first experiment is to perform an intrinsic evaluation of the word translation probabilities obtained by comparing them to traditional semantic relatedness measures on the task of ranking word pairs.", "labels": [], "entities": [{"text": "word translation", "start_pos": 78, "end_pos": 94, "type": "TASK", "confidence": 0.69816854596138}]}, {"text": "Human judgements of semantic relatedness can be used to evaluate how well semantic relatedness measures reflect human rankings by correlating their ranking results with Spearman's rank correlation coefficient.", "labels": [], "entities": []}, {"text": "Several evaluation datasets are available for English, but we restrict our study to the larger dataset created by due to the low coverage of many pairs in the word-to-word translation tables.", "labels": [], "entities": []}, {"text": "This dataset comprises two subsets, which have been annotated by different annotators: Fin1-153, containing 153 word pairs, and Fin2-200, containing 200 word pairs.", "labels": [], "entities": []}, {"text": "Word-to-word translation probabilities are compared with a concept vector based measure relying on Explicit Semantic Analysis (, since this approach has been shown to yield very good results (.", "labels": [], "entities": [{"text": "Word-to-word translation probabilities", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.7302705546220144}]}, {"text": "The method consists in representing words as a concept vector, where concepts correspond to WordNet synsets, Wikipedia article titles or Wiktionary entry names.", "labels": [], "entities": []}, {"text": "Concept vectors for each word are derived from the textual representation available for each concept, i.e. glosses in WordNet, the full article or the first paragraph of the article in Wikipedia or the full contents of a Wiktionary entry.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 118, "end_pos": 125, "type": "DATASET", "confidence": 0.9395775198936462}]}, {"text": "We refer the reader to () for technical details on how the concept vectors are built and used to obtain semantic relatedness values.", "labels": [], "entities": []}, {"text": "lists Spearman's rank correlation coefficients obtained for concept vector based measures and translation probabilities.", "labels": [], "entities": []}, {"text": "In order to ensure a fair evaluation, we limit the comparison to the word pairs which are contained in all resources and translation tables.", "labels": [], "entities": []}, {"text": "Fin1  The first observation is that the coverage over the two evaluation datasets is rather small: only 46 pairs have been evaluated for the Fin1-153 dataset and 42 for the Fin2-200 dataset.", "labels": [], "entities": [{"text": "Fin1", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8591545224189758}, {"text": "coverage", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9743897318840027}, {"text": "Fin1-153 dataset", "start_pos": 141, "end_pos": 157, "type": "DATASET", "confidence": 0.9322165250778198}, {"text": "Fin2-200 dataset", "start_pos": 173, "end_pos": 189, "type": "DATASET", "confidence": 0.9367603957653046}]}, {"text": "This is mainly due to the natural absence of many word pairs in the translation tables.", "labels": [], "entities": []}, {"text": "Indeed, translation probabilities can only be obtained from observed parallel pairs in the training data.", "labels": [], "entities": [{"text": "translation", "start_pos": 8, "end_pos": 19, "type": "TASK", "confidence": 0.9360166192054749}]}, {"text": "Concept vector based measures are more flexible in that respect since the relatedness value is based on a common representation in a concept vector space.", "labels": [], "entities": []}, {"text": "It is therefore possible to measure relatedness fora far greater number of word pairs, as long as they share some concept vector dimensions.", "labels": [], "entities": []}, {"text": "The second observation is that, on the restricted subset of word pairs considered, the results obtained by word-to-word translation probabilities are most of the time better than those of concept vector measures.", "labels": [], "entities": []}, {"text": "However, the differences are not statistically significant.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Spearman's rank correlation coefficients  on the Fin1-153 and Fin2-200 datasets. Best val- ues for each dataset are in bold format. For  Wikipedia First , the concept vectors are based on  the first paragraph of each article.", "labels": [], "entities": [{"text": "Spearman's rank correlation", "start_pos": 10, "end_pos": 37, "type": "METRIC", "confidence": 0.5093749538064003}, {"text": "Fin1-153 and Fin2-200 datasets", "start_pos": 59, "end_pos": 89, "type": "DATASET", "confidence": 0.7319907695055008}]}, {"text": " Table 3: Example relevance judgements in the Microsoft QA corpus.", "labels": [], "entities": [{"text": "Microsoft QA corpus", "start_pos": 46, "end_pos": 65, "type": "DATASET", "confidence": 0.8862061897913615}]}, {"text": " Table 4: Answer retrieval results. The WAQ+WAQA+LSR Lin results have been obtained with \u03b1=0.2  \u03b3=0.2 and \u03b4=0.6 (the parameter values have been determined empirically based on MAP and R-Prec).", "labels": [], "entities": [{"text": "Answer retrieval", "start_pos": 10, "end_pos": 26, "type": "TASK", "confidence": 0.9113217294216156}, {"text": "WAQ+WAQA+LSR Lin", "start_pos": 40, "end_pos": 56, "type": "METRIC", "confidence": 0.49559860428174335}]}]}