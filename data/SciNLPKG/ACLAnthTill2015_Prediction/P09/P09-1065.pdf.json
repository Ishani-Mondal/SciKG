{"title": [{"text": "Joint Decoding with Multiple Translation Models", "labels": [], "entities": [{"text": "Joint Decoding", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7481728792190552}, {"text": "Multiple Translation", "start_pos": 20, "end_pos": 40, "type": "TASK", "confidence": 0.7107984721660614}]}], "abstractContent": [{"text": "Current SMT systems usually decode with single translation models and cannot benefit from the strengths of other models in decoding phase.", "labels": [], "entities": [{"text": "SMT", "start_pos": 8, "end_pos": 11, "type": "TASK", "confidence": 0.9894911050796509}]}, {"text": "We instead propose joint decoding, a method that combines multiple translation models in one decoder.", "labels": [], "entities": []}, {"text": "Our joint decoder draws connections among multiple models by integrating the translation hypergraphs they produce individually.", "labels": [], "entities": []}, {"text": "Therefore, one model can share translations and even derivations with other models.", "labels": [], "entities": []}, {"text": "Comparable to the state-of-the-art system combination technique, joint decoding achieves an absolute improvement of 1.5 BLEU points over individual decoding .", "labels": [], "entities": [{"text": "BLEU", "start_pos": 120, "end_pos": 124, "type": "METRIC", "confidence": 0.9992994070053101}]}], "introductionContent": [{"text": "System combination aims to find consensus translations among different machine translation systems.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 71, "end_pos": 90, "type": "TASK", "confidence": 0.7255019545555115}]}, {"text": "It proves that such consensus translations are usually better than the output of individual systems.", "labels": [], "entities": []}, {"text": "Recent several years have witnessed the rapid development of system combination methods based on confusion networks (e.g.,), which show state-of-theart performance in MT benchmarks.", "labels": [], "entities": [{"text": "MT", "start_pos": 167, "end_pos": 169, "type": "TASK", "confidence": 0.9860447645187378}]}, {"text": "A confusion network consists of a sequence of sets of candidate words.", "labels": [], "entities": []}, {"text": "Each candidate word is associated with a score.", "labels": [], "entities": []}, {"text": "The optimal consensus translation can be obtained by selecting one word from each set of candidates to maximizing the overall score.", "labels": [], "entities": [{"text": "consensus translation", "start_pos": 12, "end_pos": 33, "type": "TASK", "confidence": 0.8123420476913452}]}, {"text": "While it is easy and efficient to manipulate strings, current methods usually have no access to most information available in decoding phase, which might be useful for obtaining further improvements.", "labels": [], "entities": []}, {"text": "In this paper, we propose a framework for combining multiple translation models directly in decoding phase.", "labels": [], "entities": []}, {"text": "1 Based on max-translation decoding and max-derivation decoding used in conventional individual decoders (Section 2), we go further to develop a joint decoder that integrates multiple models on a firm basis: \u2022 Structuring the search space of each model as a translation hypergraph (Section 3.1), our joint decoder packs individual translation hypergraphs together by merging nodes that have identical partial translations (Section 3.2).", "labels": [], "entities": []}, {"text": "Although such translation-level combination will not produce new translations, it does change the way of selecting promising candidates.", "labels": [], "entities": []}, {"text": "\u2022 Two models could even share derivations with each other if they produce the same structures on the target side (Section 3.3), which we refer to as derivation-level combination.", "labels": [], "entities": []}, {"text": "This method enlarges the search space by allowing for mixing different types of translation rules within one derivation.", "labels": [], "entities": []}, {"text": "\u2022 As multiple derivations are used for finding optimal translations, we extend the minimum error rate training (MERT) algorithm to tune feature weights with respect to BLEU score for max-translation decoding (Section 4).", "labels": [], "entities": [{"text": "minimum error rate training (MERT)", "start_pos": 83, "end_pos": 117, "type": "METRIC", "confidence": 0.884892361504691}, {"text": "BLEU score", "start_pos": 168, "end_pos": 178, "type": "METRIC", "confidence": 0.9763683080673218}]}, {"text": "We evaluated our joint decoder that integrated a hierarchical phrase-based model and a tree-to-string model ( ) on the NIST 2005 Chinese-English testset.", "labels": [], "entities": [{"text": "NIST 2005 Chinese-English testset", "start_pos": 119, "end_pos": 152, "type": "DATASET", "confidence": 0.9738071709871292}]}, {"text": "Experimental results show that joint decod-S \u2192 X 1 , X 1 X \u2192 fabiao X 1 , give a X 1 X \u2192 yanjiang, talk: A derivation composed of SCFG rules that translates a Chinese sentence \"fabiao yanjiang\" into an English sentence \"give a talk\".", "labels": [], "entities": []}, {"text": "ing with multiple models achieves an absolute improvement of 1.5 BLEU points over individual decoding with single models (Section 5).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 65, "end_pos": 69, "type": "METRIC", "confidence": 0.9992660880088806}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Comparison of individual decoding and joint decoding on average decoding time (sec- onds/sentence) and BLEU score (case-insensitive).", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 113, "end_pos": 123, "type": "METRIC", "confidence": 0.9837327599525452}]}, {"text": " Table 4: Comparison of individual training and  joint training.", "labels": [], "entities": []}]}