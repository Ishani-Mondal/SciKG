{"title": [{"text": "A Gibbs Sampler for Phrasal Synchronous Grammar Induction", "labels": [], "entities": [{"text": "Phrasal Synchronous Grammar Induction", "start_pos": 20, "end_pos": 57, "type": "TASK", "confidence": 0.7687879055738449}]}], "abstractContent": [{"text": "We present a phrasal synchronous grammar model of translational equivalence.", "labels": [], "entities": [{"text": "translational equivalence", "start_pos": 50, "end_pos": 75, "type": "TASK", "confidence": 0.916146993637085}]}, {"text": "Unlike previous approaches, we do not resort to heuristics or constraints from a word-alignment model, but instead directly induce asynchronous grammar from parallel sentence-aligned corpora.", "labels": [], "entities": []}, {"text": "We use a hierarchical Bayesian prior to bias towards compact grammars with small translation units.", "labels": [], "entities": []}, {"text": "Inference is performed using a novel Gibbs sampler over synchronous derivations.", "labels": [], "entities": []}, {"text": "This sam-pler side-steps the intractability issues of previous models which required inference over derivation forests.", "labels": [], "entities": []}, {"text": "Instead each sampling iteration is highly efficient, allowing the model to be applied to larger translation corpora than previous approaches.", "labels": [], "entities": []}], "introductionContent": [{"text": "The field of machine translation has seen many advances in recent years, most notably the shift from word-based () to phrasebased models which use token n-grams as translation units (.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 13, "end_pos": 32, "type": "TASK", "confidence": 0.7363222390413284}]}, {"text": "Although very few researchers use word-based models for translation per se, such models are still widely used in the training of phrase-based models.", "labels": [], "entities": []}, {"text": "These wordbased models are used to find the latent wordalignments between bilingual sentence pairs, from which a weighted string transducer can be induced (either finite state ( or synchronous context free grammar).", "labels": [], "entities": []}, {"text": "Although wide-spread, the disconnect between the translation model and the alignment model is artificial and clearly undesirable.", "labels": [], "entities": []}, {"text": "Word-based models are incapable of learning translational equivalences between non-compositional phrasal units, while the algorithms used for inducing weighted transducers from word-alignments are based on heuristics with little theoretical justification.", "labels": [], "entities": []}, {"text": "A model which can fulfil both roles would address both the practical and theoretical short-comings of the machine translation pipeline.", "labels": [], "entities": [{"text": "machine translation pipeline", "start_pos": 106, "end_pos": 134, "type": "TASK", "confidence": 0.8573281764984131}]}, {"text": "The machine translation literature is littered with various attempts to learn a phrase-based string transducer directly from aligned sentence pairs, doing away with the separate word alignment step (.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.756709635257721}]}, {"text": "Unfortunately none of these approaches resulted in an unqualified success, due largely to intractable estimation.", "labels": [], "entities": []}, {"text": "Large training sets with hundreds of thousands of sentence pairs are common in machine translation, leading to a parameter space of billions or even trillions of possible bilingual phrase-pairs.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 79, "end_pos": 98, "type": "TASK", "confidence": 0.7297228276729584}]}, {"text": "Moreover, the inference procedure for each sentence pair is non-trivial, proving NP-complete for learning phrase based models ) or a high order polynomial (O(|f | 3 |e| 3 )) fora sub-class of weighted synchronous context free grammars.", "labels": [], "entities": [{"text": "O", "start_pos": 156, "end_pos": 157, "type": "METRIC", "confidence": 0.9522609710693359}]}, {"text": "Consequently, for such models both the parameterisation and approximate inference techniques are fundamental to their success.", "labels": [], "entities": []}, {"text": "In this paper we present a novel SCFG translation model using a non-parametric Bayesian formulation.", "labels": [], "entities": [{"text": "SCFG translation", "start_pos": 33, "end_pos": 49, "type": "TASK", "confidence": 0.9159640669822693}]}, {"text": "The model includes priors to impose a bias towards small grammars with few rules, each of which is as simple as possible (e.g., terminal productions consisting of short phrase pairs).", "labels": [], "entities": []}, {"text": "This explicitly avoids the degenerate solutions of maximum likelihood estimation (), without resort to the heuristic estimator of.", "labels": [], "entities": []}, {"text": "We develop a novel Gibbs sampler to perform inference over the latent synchronous derivation trees for our training instances.", "labels": [], "entities": []}, {"text": "The sampler reasons over the infinite space of possible translation units without recourse to arbitrary restrictions (e.g., constraints drawn from a wordalignment) or a grammar fixed a priori ().", "labels": [], "entities": []}, {"text": "The sampler performs local edit operations to nodes in the synchronous trees, each of which is very fast, leading to a highly efficient inference technique.", "labels": [], "entities": []}, {"text": "This allows us to train the model on large corpora without resort to punitive length limits, unlike previous approaches which were only applied to small data sets with short sentences.", "labels": [], "entities": []}, {"text": "This paper is structured as follows: In Section 3 we argue for the use of efficient sampling techniques over SCFGs as an effective solution to the modelling and scaling problems of previous approaches.", "labels": [], "entities": []}, {"text": "We describe our Bayesian SCFG model in Section 4 and a Gibbs sampler to explore its posterior.", "labels": [], "entities": []}, {"text": "We apply this sampler to build phrase-based and hierarchical translation models and evaluate their performance on small and large corpora.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our evaluation aims to determine whether the phrase/SCFG rule distributions created by sampling from the model described in Section 4 impact upon the performance of state-of-theart translation systems.", "labels": [], "entities": []}, {"text": "We conduct experiments translating both Chinese (high reordering) and Arabic (low reordering) into English.", "labels": [], "entities": []}, {"text": "We use the GIZA++ implementation of IBM Model 4 () coupled with the phrase extraction heuristics of and the SCFG rule extraction heuristics of Chiang as our benchmark.", "labels": [], "entities": [{"text": "IBM Model 4", "start_pos": 36, "end_pos": 47, "type": "DATASET", "confidence": 0.8503432472546896}, {"text": "phrase extraction heuristics", "start_pos": 68, "end_pos": 96, "type": "TASK", "confidence": 0.7507356206576029}, {"text": "SCFG rule extraction", "start_pos": 108, "end_pos": 128, "type": "TASK", "confidence": 0.673312226931254}]}, {"text": "All the SCFG models employ a single X non-terminal, we leave experiments with multiple non-terminals to future work.", "labels": [], "entities": []}, {"text": "Our hypothesis is that our grammar based induction of translation units should benefit language pairs with significant reordering more than those with less.", "labels": [], "entities": []}, {"text": "While for mostly monotone translation pairs, such as Arabic-English, the benchmark GIZA++-based system is well suited due to its strong monotone bias (the sequential Markov model and diagonal growing heuristic).", "labels": [], "entities": []}, {"text": "We conduct experiments on both small and large corpora to allow a range of alignment qualities and also to verify the effectiveness of our distributed approximation of the Bayesian inference.", "labels": [], "entities": []}, {"text": "The samplers are initialised with trees created from GIZA++ Model 4 alignments, altered such that they are consistent with our ternary grammar.", "labels": [], "entities": [{"text": "GIZA++ Model 4 alignments", "start_pos": 53, "end_pos": 78, "type": "DATASET", "confidence": 0.8918493628501892}]}, {"text": "This is achieved by using the factorisation algorithm of to first create initial trees.", "labels": [], "entities": []}, {"text": "Where these factored trees contain nodes with mixed terminals and non-terminals, or more than three non-terminals, we discard alignment points until the node factorises correctly.", "labels": [], "entities": []}, {"text": "As the alignments contain many such non-factorisable nodes, these trees are of poor quality.", "labels": [], "entities": []}, {"text": "However, all samplers used in these experiments are first 'burnt-in' for 1000 full passes through the data.", "labels": [], "entities": []}, {"text": "This allows the sampler to diverge from its initialisation condition, and thus gives us confidence that subsequent samples will be drawn from the posterior.", "labels": [], "entities": []}, {"text": "An expectation over phrase tables and Hiero grammars is built from every 50th sample after the burn-in, up until the 1500th sample.", "labels": [], "entities": []}, {"text": "We evaluate the translation models using IBM BLEU ().: IWSLT Chinese to English translation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 45, "end_pos": 49, "type": "METRIC", "confidence": 0.901369035243988}, {"text": "IWSLT Chinese to English translation", "start_pos": 55, "end_pos": 91, "type": "TASK", "confidence": 0.7190579295158386}]}], "tableCaptions": [{"text": " Table 2: IWSLT Chinese to English translation.", "labels": [], "entities": [{"text": "IWSLT Chinese to English translation", "start_pos": 10, "end_pos": 46, "type": "TASK", "confidence": 0.6929516315460205}]}, {"text": " Table 3: NIST Chinese to English translation.", "labels": [], "entities": [{"text": "NIST Chinese to English translation", "start_pos": 10, "end_pos": 45, "type": "TASK", "confidence": 0.7720534801483154}]}, {"text": " Table 4: NIST Arabic to English translation.", "labels": [], "entities": [{"text": "NIST Arabic to English translation", "start_pos": 10, "end_pos": 44, "type": "TASK", "confidence": 0.7456619143486023}]}]}