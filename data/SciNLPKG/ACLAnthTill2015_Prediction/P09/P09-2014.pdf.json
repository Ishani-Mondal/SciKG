{"title": [{"text": "Comparing the Accuracy of CCG and Penn Treebank Parsers", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9983267188072205}, {"text": "CCG", "start_pos": 26, "end_pos": 29, "type": "DATASET", "confidence": 0.9248505234718323}, {"text": "Penn Treebank Parsers", "start_pos": 34, "end_pos": 55, "type": "DATASET", "confidence": 0.9759675860404968}]}], "abstractContent": [{"text": "We compare the CCG parser of Clark and Curran (2007) with a state-of-the-art Penn Treebank (PTB) parser.", "labels": [], "entities": [{"text": "Penn Treebank (PTB) parser", "start_pos": 77, "end_pos": 103, "type": "DATASET", "confidence": 0.9717989861965179}]}, {"text": "An accuracy comparison is performed by converting the CCG derivations into PTB trees.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 3, "end_pos": 11, "type": "METRIC", "confidence": 0.9989043474197388}]}, {"text": "We show that the conversion is extremely difficult to perform, but are able to fairly compare the parsers on a representative subset of the PTB test section, obtaining results for the CCG parser that are statistically no different to those for the Berkeley parser.", "labels": [], "entities": [{"text": "PTB test section", "start_pos": 140, "end_pos": 156, "type": "DATASET", "confidence": 0.9628951549530029}]}], "introductionContent": [{"text": "There area number of approaches emerging in statistical parsing.", "labels": [], "entities": [{"text": "statistical parsing", "start_pos": 44, "end_pos": 63, "type": "TASK", "confidence": 0.9264220595359802}]}, {"text": "The first approach, which began in the mid-90s and now has an extensive literature, is based on the Penn Treebank (PTB) parsing task: inferring skeletal phrase-structure trees for unseen sentences of the WSJ, and evaluating accuracy according to the Parseval metrics.", "labels": [], "entities": [{"text": "Penn Treebank (PTB) parsing", "start_pos": 100, "end_pos": 127, "type": "DATASET", "confidence": 0.8778561651706696}, {"text": "WSJ", "start_pos": 204, "end_pos": 207, "type": "DATASET", "confidence": 0.9245362877845764}, {"text": "accuracy", "start_pos": 224, "end_pos": 232, "type": "METRIC", "confidence": 0.9984484910964966}]}, {"text": "Collins (1999) is a seminal example.", "labels": [], "entities": [{"text": "Collins (1999)", "start_pos": 0, "end_pos": 14, "type": "DATASET", "confidence": 0.9507420212030411}]}, {"text": "The second approach is to apply statistical methods to parsers based on linguistic formalisms, such as HPSG, LFG, TAG, and CCG, with the grammar being defined manually or extracted from a formalism-specific treebank.", "labels": [], "entities": []}, {"text": "Evaluation is typically performed by comparing against predicate-argument structures extracted from the treebank, or against a test set of manually annotated grammatical relations (GRs).", "labels": [], "entities": []}, {"text": "Examples of this approach include,,, and.", "labels": [], "entities": []}, {"text": "Despite the many examples from both approaches, there has been little comparison across the two groups, which we refer to as PTB parsing and formalism-based parsing, respectively.", "labels": [], "entities": [{"text": "PTB parsing", "start_pos": 125, "end_pos": 136, "type": "TASK", "confidence": 0.7288852632045746}]}, {"text": "The PTB parser we use for comparison is the publicly available Berkeley parser.", "labels": [], "entities": []}, {"text": "The formalism-based parser we use is the CCG parser of, which is based on CCGbank), a CCG version of the Penn Treebank.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 105, "end_pos": 118, "type": "DATASET", "confidence": 0.9942513704299927}]}, {"text": "We compare this parser with a PTB parser because both are derived from the same original source, and both produce phrase-structure in some form or another; the interesting question is whether anything is gained by converting the PTB into CCG.", "labels": [], "entities": []}, {"text": "The comparison focuses on accuracy and is performed by converting CCG derivations into PTB phrase-structure trees.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9989597797393799}]}, {"text": "A contribution of this paper is to demonstrate the difficulty of mapping from a grammatical resource based on the PTB back to the PTB, and we also comment on the (non-)suitability of the PTB as a general formalism-independent evaluation resource.", "labels": [], "entities": [{"text": "PTB", "start_pos": 130, "end_pos": 133, "type": "DATASET", "confidence": 0.9222914576530457}]}, {"text": "A second contribution is to provide the first accuracy comparison of the CCG parser with a PTB parser, obtaining competitive scores for the CCG parser on a representative subset of the PTB test sections.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.9989809393882751}, {"text": "PTB test sections", "start_pos": 185, "end_pos": 202, "type": "DATASET", "confidence": 0.9016234278678894}]}, {"text": "It is important to note that the purpose of this evaluation is comparison with a PTB parser, rather than evaluation of the CCG parser per se.", "labels": [], "entities": []}, {"text": "The CCG parser has been extensively evaluated elsewhere, and arguably GRs or predicate-argument structures provide a more suitable test set for the CCG parser than PTB phrase-structure trees.", "labels": [], "entities": []}], "datasetContent": [{"text": "The Berkeley parser) provides performance close to the state-of-the-art for the PTB parsing task, with reported F-scores of around 90%.", "labels": [], "entities": [{"text": "PTB parsing task", "start_pos": 80, "end_pos": 96, "type": "TASK", "confidence": 0.7986145416895548}, {"text": "F-scores", "start_pos": 112, "end_pos": 120, "type": "METRIC", "confidence": 0.9932940006256104}]}, {"text": "Since the oracle score for CCGbank is less than 95%, it would not be a fair comparison to use the complete test set.", "labels": [], "entities": [{"text": "CCGbank", "start_pos": 27, "end_pos": 34, "type": "DATASET", "confidence": 0.879607081413269}]}, {"text": "However, there area number of sentences which are correct, or almost correct, according to EVALB after the conversion, and we are able to use those fora fair comparison.", "labels": [], "entities": [{"text": "EVALB", "start_pos": 91, "end_pos": 96, "type": "DATASET", "confidence": 0.8121456503868103}]}, {"text": "gives the EVALB results for the CCG parser on various subsets of section 00 of the PTB.", "labels": [], "entities": [{"text": "EVALB", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9606044292449951}, {"text": "PTB", "start_pos": 83, "end_pos": 86, "type": "DATASET", "confidence": 0.8998383283615112}]}, {"text": "The first row shows the results on only those sentences which the conversion process can convert sucessfully (as measured by converting gold-standard CCGbank derivations and comparing with PTB trees; although, to be clear, the scores are for the CCG parser on those sentences).", "labels": [], "entities": []}, {"text": "As can be seen from the scores, these sentences form a slightly easier subset than the full section 00, but this is a subset which can be used fora fair comparison against the Berkeley parser, since the conversion process is not lossy for this subset.", "labels": [], "entities": []}, {"text": "The second row shows the scores on those sentences for which the conversion process was somewhat lossy, but when the gold-standard CCGbank derivations are converted, the oracle F-measure is greater than 95%.", "labels": [], "entities": [{"text": "CCGbank", "start_pos": 131, "end_pos": 138, "type": "DATASET", "confidence": 0.8150680065155029}, {"text": "F-measure", "start_pos": 177, "end_pos": 186, "type": "METRIC", "confidence": 0.9579786658287048}]}, {"text": "The third row is similar, but for sentences for which the oracle F-score is geater than 92%.", "labels": [], "entities": [{"text": "F-score", "start_pos": 65, "end_pos": 72, "type": "METRIC", "confidence": 0.9753177165985107}]}, {"text": "The final row is for the whole of section 00.", "labels": [], "entities": []}, {"text": "The UB column gives the upper bound on the accuracy of the CCG parser.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.999618411064148}]}, {"text": "Results are calculated using both gold standard and automatically assigned POS tags; # is the number of sentences in the sample, and the % column gives the sample size as a percentage of the whole section.", "labels": [], "entities": []}, {"text": "We compare the CCG parser to the Berkeley parser using the accurate mode of the Berkeley parser, together with the model supplied with the publicly available version.", "labels": [], "entities": []}, {"text": "gives the results for Section 23, comparing the CCG and Berkeley parsers.", "labels": [], "entities": [{"text": "CCG", "start_pos": 48, "end_pos": 51, "type": "DATASET", "confidence": 0.9569953680038452}]}, {"text": "The projected columns give the projected scores for the CCG parser, if it performed at the same accuracy level for those sentences which could not be converted successfully.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 96, "end_pos": 104, "type": "METRIC", "confidence": 0.9981020092964172}]}, {"text": "The purpose of this column is to obtain an approximation of the CCG parser score fora perfect conversion process.", "labels": [], "entities": []}, {"text": "The results in bold are those which we consider to be a fair comparison against the Berkeley parser.", "labels": [], "entities": []}, {"text": "The difference in scores is not statistically significant at p=0.05 (using Dan Bikel's stratified shuffling test).", "labels": [], "entities": []}, {"text": "One possible objection to this comparison is that the subset for which we have a fair compar-   ison is likely to bean easy subset consisting of shorter sentences, and so the most that can be said is that the CCG parser performs as well as the Berkeley parser on short sentences.", "labels": [], "entities": []}, {"text": "In fact, the subset for which we perform a perfect conversion contains sentences with an average length of 18.1 words, compared to 21.4 for sentences with 40 words or less (a standard test set for reporting Parseval figures).", "labels": [], "entities": []}, {"text": "Hence we do consider the comparison to be highly informative.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Oracle conversion evaluation", "labels": [], "entities": [{"text": "Oracle conversion", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.8914243578910828}]}, {"text": " Table 3: Results on the development set (CCG parser only)", "labels": [], "entities": []}, {"text": " Table 4: Results on the test set (CCG parser and Berkeley)", "labels": [], "entities": []}]}