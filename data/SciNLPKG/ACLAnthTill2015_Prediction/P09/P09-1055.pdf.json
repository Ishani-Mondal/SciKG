{"title": [{"text": "A global model for joint lemmatization and part-of-speech prediction", "labels": [], "entities": [{"text": "part-of-speech prediction", "start_pos": 43, "end_pos": 68, "type": "TASK", "confidence": 0.8024203777313232}]}], "abstractContent": [{"text": "We present a global joint model for lemmatization and part-of-speech prediction.", "labels": [], "entities": [{"text": "part-of-speech prediction", "start_pos": 54, "end_pos": 79, "type": "TASK", "confidence": 0.7901207506656647}]}, {"text": "Using only morphological lexicons and unlabeled data, we learn a partially-supervised part-of-speech tagger and a lemmatizer which are combined using features on a dynamically linked dependency structure of words.", "labels": [], "entities": []}, {"text": "We evaluate our model on English, Bulgarian, Czech, and Slovene, and demonstrate substantial improvements over both a direct transduction approach to lemmatization and a pipelined approach, which predicts part-of-speech tags before lemmatization.", "labels": [], "entities": []}], "introductionContent": [{"text": "The traditional problem of morphological analysis is, given a word form, to predict the set of all of its possible morphological analyses.", "labels": [], "entities": [{"text": "morphological analysis", "start_pos": 27, "end_pos": 49, "type": "TASK", "confidence": 0.8278003334999084}]}, {"text": "A morphological analysis consists of a part-of-speech tag (POS), possibly other morphological features, and a lemma (basic form) corresponding to this tag and features combination (see for examples).", "labels": [], "entities": []}, {"text": "We address this problem in the setting where we are given a morphological dictionary for training, and can additionally make use of un-annotated text in the language.", "labels": [], "entities": []}, {"text": "We present anew machine learning model for this task setting.", "labels": [], "entities": []}, {"text": "In addition to the morphological analysis task we are interested in performance on two subtasks: tag-set prediction (predicting the set of possible tags of words) and lemmatization (predicting the set of possible lemmas).", "labels": [], "entities": [{"text": "morphological analysis", "start_pos": 19, "end_pos": 41, "type": "TASK", "confidence": 0.759734570980072}, {"text": "tag-set prediction", "start_pos": 97, "end_pos": 115, "type": "TASK", "confidence": 0.7215156853199005}, {"text": "predicting the set of possible tags of words", "start_pos": 117, "end_pos": 161, "type": "TASK", "confidence": 0.7414556071162224}]}, {"text": "The result of these subtasks is directly useful for some applications.", "labels": [], "entities": []}, {"text": "If we are interested in the results of each of these two subtasks in isolation, we might build independent solutions which ignore the other subtask.", "labels": [], "entities": []}, {"text": "In this paper, we show that there are strong dependencies between the two subtasks and we can improve performance on both by sharing information between them.", "labels": [], "entities": []}, {"text": "We present a joint model for these two subtasks: it is joint not only in that it performs both tasks simultaneously, sharing information, but also in that it reasons about multiple words jointly.", "labels": [], "entities": []}, {"text": "It uses component tag-set and lemmatization models and combines their predictions while incorporating joint features in a loglinear model, defined on a dynamically linked dependency structure of words.", "labels": [], "entities": []}, {"text": "The model is formalized in Section 5 and evaluated in Section 6.", "labels": [], "entities": []}, {"text": "We report results on English, Bulgarian, Slovene, and Czech and show that joint modeling reduces the lemmatization error by up to 19%, the tag-prediction error by up to 26% and the error on the complete morphological analysis task by up to 22.6%.", "labels": [], "entities": [{"text": "lemmatization error", "start_pos": 101, "end_pos": 120, "type": "METRIC", "confidence": 0.8054628670215607}]}], "datasetContent": [{"text": "As a first experiment which motivates our joint modeling approach, we present a comparison on lemmatization performance in two settings: (i) when no tags are used in training or testing by the transducer, and (ii) when correct tags are used in training and tags predicted by the tagging model are used in testing.", "labels": [], "entities": []}, {"text": "In this section, we report performance on English and Bulgarian only.", "labels": [], "entities": []}, {"text": "Comparable performance on the other Multext-East languages is shown in Section 6.", "labels": [], "entities": [{"text": "Multext-East languages", "start_pos": 36, "end_pos": 58, "type": "DATASET", "confidence": 0.9450719058513641}]}, {"text": "The experiments are performed using LTrain for training and LDev for testing.", "labels": [], "entities": [{"text": "LTrain", "start_pos": 36, "end_pos": 42, "type": "DATASET", "confidence": 0.9753943681716919}, {"text": "LDev", "start_pos": 60, "end_pos": 64, "type": "DATASET", "confidence": 0.9368205666542053}]}, {"text": "We evaluate the models on tagset F-measure (Tag), lemma-set F-measure(Lem) and complete analysis F-measure (T+L).", "labels": [], "entities": []}, {"text": "We show the performance on lemmatization when tags are not predicted (Tag Model is none), and when tags are predicted by the tag-set model.", "labels": [], "entities": []}, {"text": "We can see that on both languages lemmatization is significantly improved when a latent tag-set variable is used as a basis for prediction: the relative error reduction in Lem F-measure is 21.7% for English and 25% for Bulgarian.", "labels": [], "entities": [{"text": "prediction", "start_pos": 128, "end_pos": 138, "type": "TASK", "confidence": 0.9583947658538818}, {"text": "F-measure", "start_pos": 176, "end_pos": 185, "type": "METRIC", "confidence": 0.5510218143463135}]}, {"text": "For Bulgarian and the other Slavic languages we predicted only main POS tags, because this resulted in better lemmatization performance.", "labels": [], "entities": []}, {"text": "It is also interesting to evaluate the contribution of the unlabeled data T to the performance of the tag-set model.", "labels": [], "entities": []}, {"text": "This can be achieved by removing the word-context sub-model of the tagger and also removing related word features.", "labels": [], "entities": []}, {"text": "The results achieved in this setting for English and Bulgarian are shown in the rows labeled \"no unlab data\".", "labels": [], "entities": []}, {"text": "We can see that the tag-set F-measure of such models is reduced by 8 to 9 points and the lemmatization F-measure is similarly reduced.", "labels": [], "entities": []}, {"text": "Thus a large portion of the positive impact tagging has on lemmatization is due to the ability of tagging models to exploit unlabeled data.", "labels": [], "entities": []}, {"text": "The results of this experiment show there are strong dependencies between the tagging and lemmatization subtasks, which a joint model could exploit.", "labels": [], "entities": []}, {"text": "Since our joint model re-ranks candidates produced by the component tagger and lemmatizer, there is an upper bound on the achievable performance.", "labels": [], "entities": []}, {"text": "We report these upper bounds for the four languages in, at the rows which list m-best oracle under Model.", "labels": [], "entities": []}, {"text": "The oracle is computed using five-best tag-set predictions and three-best lemma predictions per tag.", "labels": [], "entities": []}, {"text": "We can see that the oracle performance on tag F-measure is quite high for all languages, but the performance on lemmatization and the complete task is close to only 90 percent for the Slavic languages.", "labels": [], "entities": []}, {"text": "As a second oracle we also report the perfect tag oracle, which selects the lemmas determined by the transducer using the correct part-of-speech tags.", "labels": [], "entities": []}, {"text": "This shows how well we could do if we made the tagging model perfect without changing the lemmatizer.", "labels": [], "entities": []}, {"text": "For the Slavic languages this is quite a bit lower than the m-best oracles, showing that the majority of errors of the pipelined approach cannot be fixed by simply improving the tagging model.", "labels": [], "entities": []}, {"text": "Our global model has the potential to improve lemma assignments even given correct tags, by sharing information among multiple words.", "labels": [], "entities": [{"text": "lemma assignments", "start_pos": 46, "end_pos": 63, "type": "TASK", "confidence": 0.7247783541679382}]}, {"text": "The actual achieved performance for three different models is also shown.", "labels": [], "entities": []}, {"text": "For comparison, the lemmatization performance of the direct transduction approach which makes no use of tags is also shown.", "labels": [], "entities": []}, {"text": "The pipelined models select onebest tag-set predictions from the tagging model, and the 1-best lemmas for each tag, like the models used in Section 6.2.", "labels": [], "entities": []}, {"text": "The model name local FS denotes a joint log-linear model which has only word-internal features.", "labels": [], "entities": [{"text": "FS", "start_pos": 21, "end_pos": 23, "type": "TASK", "confidence": 0.6897125840187073}]}, {"text": "Even with only word-internal features, performance is improved for most languages.", "labels": [], "entities": []}, {"text": "The the highest improvement is for Slovene and represents a 7.8% relative reduction in F-measure error on the complete task.", "labels": [], "entities": [{"text": "F-measure error", "start_pos": 87, "end_pos": 102, "type": "METRIC", "confidence": 0.9511841535568237}]}, {"text": "When features looking at the joint assignments of multiple words are added, the model achieves much larger improvements (models joint FS in the across all languages.", "labels": [], "entities": [{"text": "FS", "start_pos": 134, "end_pos": 136, "type": "METRIC", "confidence": 0.6881596446037292}]}, {"text": "The highest overall improvement compared to the pipelined approach is again for Slovene and represents 22.6% reduction in error for the full task; the reduction is 40% Since the optimization is stochastic, the results are averaged over four runs.", "labels": [], "entities": [{"text": "error", "start_pos": 122, "end_pos": 127, "type": "METRIC", "confidence": 0.7497028112411499}]}, {"text": "The standard deviations are between 0.02 and 0.11.", "labels": [], "entities": []}, {"text": "relative to the upper bound achieved by the m-best oracle.", "labels": [], "entities": []}, {"text": "The smallest overall improvement is for English, representing a 10% error reduction overall, which is still respectable.", "labels": [], "entities": [{"text": "error reduction", "start_pos": 68, "end_pos": 83, "type": "METRIC", "confidence": 0.9643295109272003}]}, {"text": "The larger improvement for Slavic languages might be due to the fact that there are many more forms of a single lemma and joint reasoning allows us to pool information across the forms.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Data sets used in experiments. The number of", "labels": [], "entities": []}, {"text": " Table 3: Development set results using different tag-set", "labels": [], "entities": []}, {"text": " Table 4: Results on the test set achieved by joint and", "labels": [], "entities": []}]}