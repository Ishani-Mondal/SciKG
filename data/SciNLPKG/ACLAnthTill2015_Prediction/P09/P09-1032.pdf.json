{"title": [], "abstractContent": [{"text": "It is usually assumed that the kind of noise existing in annotated data is random classification noise.", "labels": [], "entities": []}, {"text": "Yet there is evidence that differences between annotators are not always random attention slips but could result from different biases towards the classification categories, at least for the harder-to-decide cases.", "labels": [], "entities": []}, {"text": "Under an annotation generation model that takes this into account, there is a hazard that some of the training instances are actually hard cases with unreliable annotations.", "labels": [], "entities": []}, {"text": "We show that these are relatively unproblematic for an algorithm operating under the 0-1 loss model, whereas for the commonly used voted perceptron algorithm, hard training cases could result in incorrect prediction on the uncontroversial cases attest time.", "labels": [], "entities": []}], "introductionContent": [{"text": "It is assumed, often tacitly, that the kind of noise existing in human-annotated datasets used in computational linguistics is random classification noise, resulting from annotator attention slips randomly distributed across instances.", "labels": [], "entities": []}, {"text": "For example, evaluates noise tolerance of shallow parsers, with random classification noise taken to be \"crudely approximating annotation errors.\"", "labels": [], "entities": []}, {"text": "It has been shown, both theoretically and empirically, that this type of noise is tolerated well by the commonly used machine learning algorithms).", "labels": [], "entities": []}, {"text": "Yet this might be overly optimistic.", "labels": [], "entities": []}, {"text": "Reidsma and op den show that apparent differences between annotators are not random slips of attention but rather result from different biases annotators might have towards the classification categories.", "labels": [], "entities": []}, {"text": "When training data comes from one annotator and test data from another, the first annotator's biases are sometimes systematic enough fora machine learner to pick them up, with detrimental results for the algorithm's performance on the test data.", "labels": [], "entities": []}, {"text": "A small subset of doubly annotated data (for inter-annotator agreement check) and large chunks of singly annotated data (for training algorithms) is not uncommon in computational linguistics datasets; such a setup is prone to problems if annotators are differently biased.", "labels": [], "entities": []}, {"text": "Annotator bias is consistent with a number of noise models.", "labels": [], "entities": []}, {"text": "For example, it could be that an annotator's bias is exercised on each and every instance, making his preferred category likelier for any instance than in another person's annotations.", "labels": [], "entities": []}, {"text": "Another possibility, recently explored by Beigman, is that some items are really quite clear-cut for an annotator with any bias, belonging squarely within one particular category.", "labels": [], "entities": []}, {"text": "However, some instances -termed hard cases therein -are harder to decide upon, and this is where various preferences and biases come into play.", "labels": [], "entities": []}, {"text": "Ina metaphor annotation study reported by Beigman, certain markups received overwhelming annotator support when people were asked to validate annotations after a certain time delay.", "labels": [], "entities": []}, {"text": "Other instances saw opinions split; moreover, Beigman observed cases where people retracted their own earlier annotations.", "labels": [], "entities": []}, {"text": "To start accounting for such annotator behavior, Beigman proposed a model where instances are either easy, and then all annotators agree on them, or hard, and then each annotator flips his or her own coin to de-cide on a label (each annotator can have a different \"coin\" reflecting his or her biases).", "labels": [], "entities": []}, {"text": "For annotations generated under such a model, there is a danger of hard instances posing as easy -an observed agreement between annotators being a result of all coins coming up heads by chance.", "labels": [], "entities": []}, {"text": "They therefore define the expected proportion of hard instances in agreed items as annotation noise.", "labels": [], "entities": []}, {"text": "They provide an example from the literature where an annotation noise rate of about 15% is likely.", "labels": [], "entities": [{"text": "annotation noise rate", "start_pos": 53, "end_pos": 74, "type": "METRIC", "confidence": 0.9086290001869202}]}, {"text": "The question addressed in this article is: How problematic is learning from training data with annotation noise?", "labels": [], "entities": []}, {"text": "Specifically, we are interested in estimating the degree to which performance on easy instances attest time can be hurt by the presence of hard instances in training data.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}