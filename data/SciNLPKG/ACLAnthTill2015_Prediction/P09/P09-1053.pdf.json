{"title": [{"text": "Paraphrase Identification as Probabilistic Quasi-Synchronous Recognition", "labels": [], "entities": [{"text": "Paraphrase Identification", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.8636577129364014}, {"text": "Quasi-Synchronous Recognition", "start_pos": 43, "end_pos": 72, "type": "TASK", "confidence": 0.6193725019693375}]}], "abstractContent": [{"text": "We present a novel approach to deciding whether two sentences hold a paraphrase relationship.", "labels": [], "entities": []}, {"text": "We employ a gen-erative model that generates a paraphrase of a given sentence, and we use proba-bilistic inference to reason about whether two sentences share the paraphrase relationship.", "labels": [], "entities": []}, {"text": "The model cleanly incorporates both syntax and lexical semantics using quasi-synchronous dependency grammars (Smith and Eisner, 2006).", "labels": [], "entities": []}, {"text": "Furthermore, using a product of experts (Hinton, 2002), we combine the model with a complementary logistic regression model based on state-of-the-art lexical overlap features.", "labels": [], "entities": []}, {"text": "We evaluate our models on the task of distinguishing true paraphrase pairs from false ones on a standard corpus, giving competitive state-of-the-art performance.", "labels": [], "entities": []}], "introductionContent": [{"text": "The problem of modeling paraphrase relationships between natural language utterances has recently attracted interest.", "labels": [], "entities": [{"text": "paraphrase relationships between natural language utterances", "start_pos": 24, "end_pos": 84, "type": "TASK", "confidence": 0.6710685094197592}]}, {"text": "For computational linguists, solving this problem may shed light on how best to model the semantics of sentences.", "labels": [], "entities": []}, {"text": "For natural language engineers, the problem bears on information management systems like abstractive summarizers that must measure semantic overlap between sentences (Barzilay and, question answering modules) and machine translation).", "labels": [], "entities": [{"text": "machine translation", "start_pos": 213, "end_pos": 232, "type": "TASK", "confidence": 0.7295131981372833}]}, {"text": "The paraphrase identification problem asks whether two sentences have essentially the same meaning.", "labels": [], "entities": [{"text": "paraphrase identification", "start_pos": 4, "end_pos": 29, "type": "TASK", "confidence": 0.9627645313739777}]}, {"text": "Although paraphrase identification is defined in semantic terms, it is usually solved using statistical classifiers based on shallow lexical, n-gram, and syntactic \"overlap\" features.", "labels": [], "entities": [{"text": "paraphrase identification", "start_pos": 9, "end_pos": 34, "type": "TASK", "confidence": 0.9263750910758972}]}, {"text": "Such overlap features give the best-published classification accuracy for the paraphrase identification task (, inter alia), but do not explicitly model correspondence structure (or \"alignment\") between the parts of two sentences.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.9722564220428467}, {"text": "paraphrase identification task", "start_pos": 78, "end_pos": 108, "type": "TASK", "confidence": 0.9208998481432596}]}, {"text": "In this paper, we adopt a model that posits correspondence between the words in the two sentences, defining it in loose syntactic terms: if two sentences are paraphrases, we expect their dependency trees to align closely, though some divergences are also expected, with some more likely than others.", "labels": [], "entities": []}, {"text": "Following, we adopt the view that the syntactic structure of sentences paraphrasing some sentence s should be \"inspired\" by the structure of s.", "labels": [], "entities": []}, {"text": "Because dependency syntax is still only a crude approximation to semantic structure, we augment the model with a lexical semantics component, based on WordNet, that models how words are probabilistically altered in generating a paraphrase.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 151, "end_pos": 158, "type": "DATASET", "confidence": 0.9541842937469482}]}, {"text": "This combination of loose syntax and lexical semantics is similar to the \"Jeopardy\" model of.", "labels": [], "entities": []}, {"text": "This syntactic framework represents a major departure from useful and popular surface similarity features, and the latter are difficult to incorporate into our probabilistic model.", "labels": [], "entities": []}, {"text": "We use a product of experts) to bring together a logistic regression classifier built from n-gram overlap features and our syntactic model.", "labels": [], "entities": []}, {"text": "This combined model leverages complementary strengths of the two approaches, outperforming a strong state-ofthe-art baseline ().", "labels": [], "entities": []}, {"text": "This paper is organized as follows.", "labels": [], "entities": []}, {"text": "We introduce our probabilistic model in \u00a72.", "labels": [], "entities": []}, {"text": "The model makes use of three quasi-synchronous grammar models (, QG, hereafter) as components (one modeling paraphrase, one modeling not-paraphrase, and one abase grammar); these are detailed, along with latent-variable inference and discriminative training algorithms, in \u00a73.", "labels": [], "entities": []}, {"text": "We discuss the Microsoft Research Paraphrase Corpus, upon which we conduct experiments, in \u00a74.", "labels": [], "entities": [{"text": "Microsoft Research Paraphrase Corpus", "start_pos": 15, "end_pos": 51, "type": "DATASET", "confidence": 0.9431905746459961}]}, {"text": "In \u00a75, we present experiments on paraphrase identification with our model and make comparisons with the existing state-of-the-art.", "labels": [], "entities": [{"text": "paraphrase identification", "start_pos": 33, "end_pos": 58, "type": "TASK", "confidence": 0.8677828013896942}]}, {"text": "We describe the product of experts and our lexical overlap model, and discuss the results achieved in \u00a76.", "labels": [], "entities": []}, {"text": "We relate our approach to prior work ( \u00a77) and conclude ( \u00a78).", "labels": [], "entities": []}], "datasetContent": [{"text": "Here we present our experimental evaluation using p Q . We trained on the training set (3,001 pairs) and tuned model metaparameters (C in Eq.", "labels": [], "entities": []}, {"text": "17) and the effect of different feature sets on the development set (1,075 pairs).", "labels": [], "entities": []}, {"text": "We report accuracy on the official MSRPC test dataset.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9997860789299011}, {"text": "MSRPC test dataset", "start_pos": 35, "end_pos": 53, "type": "DATASET", "confidence": 0.9224968353907267}]}, {"text": "If the posterior probability p Q (p | s 1 , s 2 ) is greater than 0.5, the pair is labeled \"paraphrase\" (as in Eq. 1).", "labels": [], "entities": []}], "tableCaptions": []}