{"title": [{"text": "A Beam-Search Extraction Algorithm for Comparable Data", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper extends previous work on extracting parallel sentence pairs from comparable data (Munteanu and Marcu, 2005).", "labels": [], "entities": [{"text": "extracting parallel sentence pairs", "start_pos": 36, "end_pos": 70, "type": "TASK", "confidence": 0.8285996168851852}]}, {"text": "For a given source sentence S, a maximum entropy (ME) classifier is applied to a large set of candidate target translations . A beam-search algorithm is used to abandon target sentences as non-parallel early on during classification if they fall outside the beam.", "labels": [], "entities": [{"text": "maximum entropy (ME)", "start_pos": 33, "end_pos": 53, "type": "METRIC", "confidence": 0.6995158970355988}]}, {"text": "This way, our novel algorithm avoids any document-level pre-filtering step.", "labels": [], "entities": []}, {"text": "The algorithm increases the number of extracted parallel sentence pairs significantly, which leads to a BLEU improvement of about 1 % on our Spanish-English data.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 104, "end_pos": 108, "type": "METRIC", "confidence": 0.9997594952583313}]}], "introductionContent": [{"text": "The paper presents a novel algorithm for extracting parallel sentence pairs from comparable monolingual news data.", "labels": [], "entities": [{"text": "extracting parallel sentence pairs from comparable monolingual news", "start_pos": 41, "end_pos": 108, "type": "TASK", "confidence": 0.8254278227686882}]}, {"text": "We select source-target sentence pairs (S, T ) based on a ME classifier (.", "labels": [], "entities": []}, {"text": "Because the set of target sentences T considered can be huge, previous work () pre-selects target sentences T at the document level . We have re-implemented a particular filtering scheme based on BM25.", "labels": [], "entities": [{"text": "BM25", "start_pos": 196, "end_pos": 200, "type": "DATASET", "confidence": 0.9747485518455505}]}, {"text": "In this paper, we demonstrate a different strategy . We compute the ME score incrementally at the word level and apply a beamsearch algorithm to a large number of sentences.", "labels": [], "entities": [{"text": "ME score", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.9853459000587463}]}, {"text": "We abandon target sentences early on during classification if they fall outside the beam.", "labels": [], "entities": [{"text": "classification", "start_pos": 44, "end_pos": 58, "type": "TASK", "confidence": 0.9574326872825623}]}, {"text": "For comparison purposes, we run our novel extraction algorithm with and without the document-level prefiltering step.", "labels": [], "entities": []}, {"text": "The results in Section 4 show that the number of extracted sentence pairs is more than doubled which also leads to an increase in BLEU by about 1 % on the Spanish-English data.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 130, "end_pos": 134, "type": "METRIC", "confidence": 0.9993815422058105}, {"text": "Spanish-English data", "start_pos": 155, "end_pos": 175, "type": "DATASET", "confidence": 0.6865416318178177}]}, {"text": "The classification probability is defined as follows: where S = s J 1 is a source sentence of length J and T = t I 1 is a target sentence of length I. c \u2208 {0, 1} is a binary variable . p(c|S, T ) \u2208 [0, 1] is a probability where a value p(c = 1|S, T ) close to 1.0 indicates that Sand T are translations of each other.", "labels": [], "entities": []}, {"text": "w \u2208 Rn is a weight vector obtained during training.", "labels": [], "entities": []}, {"text": "f (c, S, T ) is a feature vector where the features are co-indexed with respect to the alignment variable c.", "labels": [], "entities": []}, {"text": "Finally, Z(S, T ) is an appropriately chosen normalization constant.", "labels": [], "entities": []}, {"text": "Section 2 summarizes the use of the binary classifier.", "labels": [], "entities": []}, {"text": "Section 3 presents the beam-search algorithm.", "labels": [], "entities": []}, {"text": "In Section 4, we show experimental results.", "labels": [], "entities": []}, {"text": "Finally, Section 5 discusses the novel algorithm.", "labels": [], "entities": []}], "datasetContent": [{"text": "The parallel sentence extraction algorithm presented in this paper is tested in detail on all of the large-scale Spanish-English Gigaword data as well as on some smaller Portuguese-English news data . For the SpanishEnglish data , matching sentence pairs come from the same news feed.", "labels": [], "entities": [{"text": "parallel sentence extraction", "start_pos": 4, "end_pos": 32, "type": "TASK", "confidence": 0.6620536049207052}, {"text": "SpanishEnglish data", "start_pos": 209, "end_pos": 228, "type": "DATASET", "confidence": 0.9319763779640198}]}, {"text": "shows the size of the comparable data, and shows the effect of including the additional sentence pairs into the training of a phrase-based SMT system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 139, "end_pos": 142, "type": "TASK", "confidence": 0.872548520565033}]}, {"text": "Here, both languages use a test set with a single reference.", "labels": [], "entities": []}, {"text": "The test data comes from Spanish and Portuguese news web pages that have been translated into English.", "labels": [], "entities": []}, {"text": "Including about 1.35 million sentence pairs extracted from the Gigaword data, we obtain a statistically significant improvement from 42.3 to 45.7 in BLEU.", "labels": [], "entities": [{"text": "Gigaword data", "start_pos": 63, "end_pos": 76, "type": "DATASET", "confidence": 0.9607593417167664}, {"text": "BLEU", "start_pos": 149, "end_pos": 153, "type": "METRIC", "confidence": 0.9765991568565369}]}, {"text": "The baseline system has been trained on about 1.8 million sentence For this data, no document-level information was available.", "labels": [], "entities": []}, {"text": "To gauge the effect of the documentlevel pre-filtering step, we have re-implemented an IR technique based on BM25 (.", "labels": [], "entities": [{"text": "BM25", "start_pos": 109, "end_pos": 113, "type": "DATASET", "confidence": 0.9124730825424194}]}, {"text": "This type of pre-filtering has also been used in.", "labels": [], "entities": []}, {"text": "We split the Spanish data into documents.", "labels": [], "entities": []}, {"text": "Each Spanish document is translated into a bag of English words using Model-1 lexicon probabilities trained on the baseline data.", "labels": [], "entities": []}, {"text": "Each of these English bag-of-words is then issued as a query against all the English documents that have been published within a 7 day window of the source document.", "labels": [], "entities": []}, {"text": "We select the 20 highest scoring English documents for each source document . These 20 documents provide a restricted set of target sentence candidates.", "labels": [], "entities": []}, {"text": "The sentence-level beam-search algorithm without the document-level filtering step searches through close to 1 trillion sentence pairs.", "labels": [], "entities": []}, {"text": "For the data obtained by the BM25-based filtering step, we still use the same beam-search algorithm but on a much smaller candidate set of only 25.4 billion sentence pairs.", "labels": [], "entities": [{"text": "BM25-based filtering", "start_pos": 29, "end_pos": 49, "type": "TASK", "confidence": 0.6923250257968903}]}, {"text": "The probability selection threshold \u03b8 is determined on some development set in terms of precision and recall (based on the definitions in (Munteanu and).", "labels": [], "entities": [{"text": "precision", "start_pos": 88, "end_pos": 97, "type": "METRIC", "confidence": 0.999574601650238}, {"text": "recall", "start_pos": 102, "end_pos": 108, "type": "METRIC", "confidence": 0.9995716214179993}, {"text": "Munteanu", "start_pos": 139, "end_pos": 147, "type": "DATASET", "confidence": 0.9008071422576904}]}, {"text": "The classifier obtains an F-measure classifications performance of about 85 %.", "labels": [], "entities": [{"text": "F-measure classifications performance", "start_pos": 26, "end_pos": 63, "type": "METRIC", "confidence": 0.9012718200683594}]}, {"text": "The BM25 filtering step leads to a significantly more complex processing pipeline since sentences have to be indexed with respect to document boundaries and publication date.", "labels": [], "entities": [{"text": "BM25 filtering", "start_pos": 4, "end_pos": 18, "type": "TASK", "confidence": 0.7554029226303101}]}, {"text": "The document-level pre-filtering reduces the overall processing time by about 40 % (from 4 to 2.5 days on a 100-CPU cluster).", "labels": [], "entities": []}, {"text": "However, the exhaustive sentence-level search improves the BLEU score by about 1 % on the Spanish-English data.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 59, "end_pos": 63, "type": "METRIC", "confidence": 0.9991759657859802}]}], "tableCaptions": [{"text": " Table 2: Spanish-English and Portuguese-English  extraction results. Extraction threshold is \u03b8 =  0.75 for both language pairs. # cands reports the", "labels": [], "entities": [{"text": "Extraction threshold", "start_pos": 70, "end_pos": 90, "type": "METRIC", "confidence": 0.9878246486186981}]}]}