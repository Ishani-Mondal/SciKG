{"title": [{"text": "A Note on the Implementation of Hierarchical Dirichlet Processes", "labels": [], "entities": [{"text": "Implementation of Hierarchical Dirichlet Processes", "start_pos": 14, "end_pos": 64, "type": "TASK", "confidence": 0.7182927966117859}]}], "abstractContent": [{"text": "The implementation of collapsed Gibbs samplers for non-parametric Bayesian models is non-trivial, requiring considerable book-keeping.", "labels": [], "entities": []}, {"text": "(2006a) presented an approximation which significantly reduces the storage and computation overhead, but we show here that their formulation was incorrect and, even after correction, is grossly inaccurate.", "labels": [], "entities": []}, {"text": "We present an alternative formulation which is exact and can be computed easily.", "labels": [], "entities": []}, {"text": "However this approach does notwork for hierarchical models, for which case we present an efficient data structure which has a better space complexity than the naive approach.", "labels": [], "entities": []}], "introductionContent": [{"text": "Unsupervised learning of natural language is one of the most challenging areas in NLP.", "labels": [], "entities": []}, {"text": "Recently, methods from nonparametric Bayesian statistics have been gaining popularity as away to approach unsupervised learning fora variety of tasks, including language modeling, word and morpheme segmentation, parsing, and machine translation ().", "labels": [], "entities": [{"text": "language modeling", "start_pos": 161, "end_pos": 178, "type": "TASK", "confidence": 0.7380082607269287}, {"text": "word and morpheme segmentation", "start_pos": 180, "end_pos": 210, "type": "TASK", "confidence": 0.6135811060667038}, {"text": "parsing", "start_pos": 212, "end_pos": 219, "type": "TASK", "confidence": 0.9362587928771973}, {"text": "machine translation", "start_pos": 225, "end_pos": 244, "type": "TASK", "confidence": 0.797933965921402}]}, {"text": "These models are often based on the Dirichlet process (DP) or hierarchical Dirichlet process (HDP) (), with Gibbs sampling as a method of inference.", "labels": [], "entities": []}, {"text": "Exact implementation of such sampling methods requires considerable bookkeeping of various counts, which motivated to develop an approximation using expected counts.", "labels": [], "entities": []}, {"text": "However, we show here that their approximation is flawed in two respects: 1) It omits an important factor in the expectation, and 2) Even after correction, the approximation is poor for hierarchical models, which are commonly used for NLP applications.", "labels": [], "entities": []}, {"text": "We derive an improved O(1) formula that gives exact values for the expected counts in non-hierarchical models.", "labels": [], "entities": [{"text": "O", "start_pos": 22, "end_pos": 23, "type": "METRIC", "confidence": 0.995530903339386}]}, {"text": "For hierarchical models, where our formula is not exact, we present an efficient method for sampling from the HDP (and related models, such as the hierarchical PitmanYor process) that considerably decreases the memory footprint of such models as compared to the naive implementation.", "labels": [], "entities": []}, {"text": "As we have noted, the issues described in this paper apply to models for various kinds of NLP tasks; for concreteness, we will focus on n-gram language modeling for the remainder of the paper, closely following the presentation in GGJ06.", "labels": [], "entities": [{"text": "GGJ06", "start_pos": 231, "end_pos": 236, "type": "DATASET", "confidence": 0.9720643162727356}]}], "datasetContent": [], "tableCaptions": []}