{"title": [{"text": "Optimizing Word Alignment Combination For Phrase Table Training", "labels": [], "entities": [{"text": "Optimizing Word Alignment Combination", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.7665939778089523}, {"text": "Phrase Table Training", "start_pos": 42, "end_pos": 63, "type": "TASK", "confidence": 0.8905068039894104}]}], "abstractContent": [{"text": "Combining word alignments trained in two translation directions has mostly relied on heuristics that are not directly motivated by intended applications.", "labels": [], "entities": [{"text": "Combining word alignments", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.7554752131303152}]}, {"text": "We propose a novel method that performs combination as an optimization process.", "labels": [], "entities": []}, {"text": "Our algorithm explicitly maximizes the effectiveness function with greedy search for phrase table training or synchronized grammar extraction.", "labels": [], "entities": [{"text": "phrase table training", "start_pos": 85, "end_pos": 106, "type": "TASK", "confidence": 0.711804191271464}, {"text": "synchronized grammar extraction", "start_pos": 110, "end_pos": 141, "type": "TASK", "confidence": 0.6826703449090322}]}, {"text": "Experimental results show that the proposed method leads to significantly better translation quality than existing methods.", "labels": [], "entities": [{"text": "translation", "start_pos": 81, "end_pos": 92, "type": "TASK", "confidence": 0.960773766040802}]}, {"text": "Analysis suggests that this simple approach is able to maintain accuracy while maximizing coverage.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.9992403984069824}, {"text": "coverage", "start_pos": 90, "end_pos": 98, "type": "METRIC", "confidence": 0.969490110874176}]}], "introductionContent": [{"text": "Word alignment is the process of identifying word-to-word links between parallel sentences.", "labels": [], "entities": [{"text": "Word alignment", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7180073112249374}]}, {"text": "It is a fundamental and often a necessary step before linguistic knowledge acquisitions, such as training a phrase translation table in phrasal machine translation (MT) system (, or extracting hierarchial phrase rules or synchronized grammars in syntax-based translation framework.", "labels": [], "entities": [{"text": "phrase translation table", "start_pos": 108, "end_pos": 132, "type": "TASK", "confidence": 0.7660821477572123}, {"text": "phrasal machine translation (MT)", "start_pos": 136, "end_pos": 168, "type": "TASK", "confidence": 0.8063195546468099}]}, {"text": "Most word alignment models distinguish translation direction in deriving word alignment matrix.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 5, "end_pos": 19, "type": "TASK", "confidence": 0.783298522233963}, {"text": "word alignment", "start_pos": 73, "end_pos": 87, "type": "TASK", "confidence": 0.6700817048549652}]}, {"text": "Given a parallel sentence, word alignments in two directions are established first, and then they are combined as knowledge source for phrase training or rule extraction.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 27, "end_pos": 42, "type": "TASK", "confidence": 0.7099515944719315}, {"text": "phrase training", "start_pos": 135, "end_pos": 150, "type": "TASK", "confidence": 0.8182545900344849}, {"text": "rule extraction", "start_pos": 154, "end_pos": 169, "type": "TASK", "confidence": 0.7707377672195435}]}, {"text": "This process is also called symmetrization.", "labels": [], "entities": []}, {"text": "It is a common practice inmost state of the art MT systems.", "labels": [], "entities": [{"text": "MT", "start_pos": 48, "end_pos": 50, "type": "TASK", "confidence": 0.9895796179771423}]}, {"text": "Widely used alignment models, such as IBM Model serial and HMM , all assume one-to-many alignments.", "labels": [], "entities": [{"text": "IBM Model serial", "start_pos": 38, "end_pos": 54, "type": "DATASET", "confidence": 0.8491403063138326}]}, {"text": "Since many-to-many links are commonly observed in natural language, symmetrization is able to makeup for this modeling limitation.", "labels": [], "entities": []}, {"text": "On the other hand, combining two directional alignments practically can lead to improved performance.", "labels": [], "entities": []}, {"text": "Symmetrization can also be realized during alignment model training ().", "labels": [], "entities": []}, {"text": "Given two sets of word alignments trained in two translation directions, two extreme combination are intersection and union.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 18, "end_pos": 33, "type": "TASK", "confidence": 0.7036512345075607}]}, {"text": "While intersection achieves high precision with low recall, union is the opposite.", "labels": [], "entities": [{"text": "precision", "start_pos": 33, "end_pos": 42, "type": "METRIC", "confidence": 0.996967613697052}, {"text": "recall", "start_pos": 52, "end_pos": 58, "type": "METRIC", "confidence": 0.9991889595985413}]}, {"text": "A right balance of these two extreme cases would offer a good coverage with reasonable accuracy.", "labels": [], "entities": [{"text": "coverage", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.8172271847724915}, {"text": "accuracy", "start_pos": 87, "end_pos": 95, "type": "METRIC", "confidence": 0.9978575110435486}]}, {"text": "So starting from intersection, gradually adding elements in the union by heuristics is typically used.", "labels": [], "entities": []}, {"text": "grow the set of word links by appending neighboring points, while try to avoid both horizontal and vertical neighbors.", "labels": [], "entities": []}, {"text": "These heuristicbased combination methods are not driven explicitly by the intended application of the resulting output.", "labels": [], "entities": []}, {"text": "Ayan (2005) exploits many advanced machine learning techniques for general word alignment combination problem.", "labels": [], "entities": [{"text": "general word alignment combination", "start_pos": 67, "end_pos": 101, "type": "TASK", "confidence": 0.7386143505573273}]}, {"text": "However, human annotation is required for supervised training in those techniques.", "labels": [], "entities": []}, {"text": "We propose anew combination method.", "labels": [], "entities": []}, {"text": "Like heuristics, we aim to find a balance between intersection and union.", "labels": [], "entities": []}, {"text": "But unlike heuristics, combination is carried out as an optimization process driven by an effectiveness function.", "labels": [], "entities": []}, {"text": "We evaluate the impact of each alignment pair w.r.t. the target application, say phrase table training, and gradually add or remove the word link that currently can maximize the predicted benefit measured by the effectiveness function.", "labels": [], "entities": []}, {"text": "More specifically, we consider the goal of word alignment combination is for phrase table training, and we directly motivate word alignment combination as a process of maximizing the number of phrase translations that can be extracted within a sentence pair.", "labels": [], "entities": [{"text": "word alignment combination", "start_pos": 43, "end_pos": 69, "type": "TASK", "confidence": 0.8449954787890116}, {"text": "phrase table training", "start_pos": 77, "end_pos": 98, "type": "TASK", "confidence": 0.8228748639424642}, {"text": "word alignment combination", "start_pos": 125, "end_pos": 151, "type": "TASK", "confidence": 0.7996370196342468}]}], "datasetContent": [{"text": "We test the proposed new idea on Persian Farsi to English translation.", "labels": [], "entities": [{"text": "Persian Farsi to English translation", "start_pos": 33, "end_pos": 69, "type": "TASK", "confidence": 0.5732474386692047}]}, {"text": "The task is to translate spoken Farsi into English.", "labels": [], "entities": [{"text": "translate spoken Farsi", "start_pos": 15, "end_pos": 37, "type": "TASK", "confidence": 0.9017416834831238}]}, {"text": "We decode reference transcription so recognition is not an issue.", "labels": [], "entities": [{"text": "recognition", "start_pos": 37, "end_pos": 48, "type": "TASK", "confidence": 0.9542152881622314}]}, {"text": "The training data was provided by the DARPA TransTac program.", "labels": [], "entities": [{"text": "DARPA TransTac", "start_pos": 38, "end_pos": 52, "type": "DATASET", "confidence": 0.7629823088645935}]}, {"text": "It consists of around 110K sentence pairs with 850K English words in the military force protection domain.", "labels": [], "entities": [{"text": "military force protection domain", "start_pos": 73, "end_pos": 105, "type": "TASK", "confidence": 0.6853856667876244}]}, {"text": "We train IBM Model-4 using GIZA++ toolkit) in two translation directions and perform different word alignment combination.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 95, "end_pos": 109, "type": "TASK", "confidence": 0.7386663258075714}]}, {"text": "The resulting alignment set is used to train a phrase translation table, where Farsi phrases are limited to up to 6 words.", "labels": [], "entities": [{"text": "phrase translation", "start_pos": 47, "end_pos": 65, "type": "TASK", "confidence": 0.7662056386470795}]}, {"text": "The quality of resulting phrase translation table is measured by translation results.", "labels": [], "entities": [{"text": "phrase translation", "start_pos": 25, "end_pos": 43, "type": "TASK", "confidence": 0.6881412267684937}]}, {"text": "Our decoder is a phrase-based multi-stack implementation of the log-linear model similar to Pharaoh (.", "labels": [], "entities": []}, {"text": "Like other log-linear model based decoders, active features in our translation engine include translation models in two directions, lexicon weights in two directions, language model, lexicalized reordering models, sentence length penalty and other heuristics.", "labels": [], "entities": []}, {"text": "These feature weights are tuned on the dev set to achieve optimal translation performance evaluated by automatic metric.", "labels": [], "entities": []}, {"text": "The language model is a statistical 4-gram model estimated with Modified Kneser-Ney smoothing) using only English sentences in the parallel training data.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of word alignment set and the  resulting phrase table size (number of entries in  thousand (K)) with different combination methods", "labels": [], "entities": [{"text": "word alignment", "start_pos": 24, "end_pos": 38, "type": "TASK", "confidence": 0.7514068782329559}]}, {"text": " Table 2: Translation results (BLEU score) with  phrase tables trained with different word align- ment combination methods", "labels": [], "entities": [{"text": "Translation", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.9676457643508911}, {"text": "BLEU score", "start_pos": 31, "end_pos": 41, "type": "METRIC", "confidence": 0.981227844953537}]}]}