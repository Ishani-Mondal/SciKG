{"title": [], "abstractContent": [{"text": "In this paper, we propose a novel ranking framework-Co-Feedback Ranking (Co-FRank), which allows two base rankers to supervise each other during the ranking process by providing their own ranking results as feedback to the other parties so as to boost the ranking performance.", "labels": [], "entities": []}, {"text": "The mutual ranking refinement process continues until the two base rankers cannot learn from each other anymore.", "labels": [], "entities": [{"text": "mutual ranking refinement", "start_pos": 4, "end_pos": 29, "type": "TASK", "confidence": 0.611718108256658}]}, {"text": "The overall performance is improved by the enhancement of the base rankers through the mutual learning mechanism.", "labels": [], "entities": []}, {"text": "We apply this framework to the sentence ranking problem in query-focused summarization and evaluate its effectiveness on the DUC 2005 data set.", "labels": [], "entities": [{"text": "sentence ranking", "start_pos": 31, "end_pos": 47, "type": "TASK", "confidence": 0.7441208958625793}, {"text": "DUC 2005 data set", "start_pos": 125, "end_pos": 142, "type": "DATASET", "confidence": 0.9842706620693207}]}], "introductionContent": [], "datasetContent": [{"text": "We take the DUC 2005 data set as the evaluation corpus in this preliminary study.", "labels": [], "entities": [{"text": "DUC 2005 data set", "start_pos": 12, "end_pos": 29, "type": "DATASET", "confidence": 0.9897236078977585}]}, {"text": "ROUGE, which has been officially adopted in the DUC for years is used as the evaluation criterion.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9795007705688477}, {"text": "DUC", "start_pos": 48, "end_pos": 51, "type": "DATASET", "confidence": 0.9352075457572937}]}, {"text": "For the purpose of comparison, we implement the following two basic ranking functions and the linear combination of them for reference, i.e. the query relevance based ranker (denoted by QRR, same as f 1 ) and the word centroid based ranker (denoted by WCR, same as f 2 ), and the linear combined ranker, LCR= QRR+(1-)WCR, where is a combination parameter.", "labels": [], "entities": [{"text": "LCR", "start_pos": 304, "end_pos": 307, "type": "METRIC", "confidence": 0.9409024119377136}]}, {"text": "QRR and WCR are normalized by \ud97b\udf59 \ud97b\udf59 min max minx , where x, max and min denote the original ranking score, the maximum ranking score and minimum ranking score produced by a ranker, respectively.", "labels": [], "entities": []}, {"text": "shows the results of the average recall scores of ROUGE-1, ROUGE-2 and ROUGE-SU4 along with their 95% confidence intervals included within square brackets.", "labels": [], "entities": [{"text": "recall", "start_pos": 33, "end_pos": 39, "type": "METRIC", "confidence": 0.9988476037979126}, {"text": "ROUGE-1", "start_pos": 50, "end_pos": 57, "type": "METRIC", "confidence": 0.9562420845031738}, {"text": "ROUGE-2", "start_pos": 59, "end_pos": 66, "type": "METRIC", "confidence": 0.8966857194900513}, {"text": "ROUGE-SU4", "start_pos": 71, "end_pos": 80, "type": "METRIC", "confidence": 0.8167933225631714}]}, {"text": "Among them, ROUGE-2 is the primary DUC evaluation criterion.", "labels": [], "entities": [{"text": "ROUGE-2", "start_pos": 12, "end_pos": 19, "type": "METRIC", "confidence": 0.9949144124984741}, {"text": "DUC evaluation", "start_pos": 35, "end_pos": 49, "type": "TASK", "confidence": 0.715874969959259}]}], "tableCaptions": [{"text": " Table 4 Compare with DUC participating systems", "labels": [], "entities": [{"text": "DUC participating", "start_pos": 22, "end_pos": 39, "type": "DATASET", "confidence": 0.630068302154541}]}]}