{"title": [{"text": "Learning foci for Question Answering over Topic Maps", "labels": [], "entities": [{"text": "Question Answering over Topic Maps", "start_pos": 18, "end_pos": 52, "type": "TASK", "confidence": 0.8596340894699097}]}], "abstractContent": [{"text": "This paper introduces the concepts of asking point and expected answer type as variations of the question focus.", "labels": [], "entities": []}, {"text": "They are of particular importance for QA over semi-structured data, as represented by Topic Maps, OWL or custom XML formats.", "labels": [], "entities": [{"text": "QA", "start_pos": 38, "end_pos": 40, "type": "TASK", "confidence": 0.9505236744880676}, {"text": "OWL", "start_pos": 98, "end_pos": 101, "type": "DATASET", "confidence": 0.8708637356758118}]}, {"text": "We describe an approach to the identification of the question focus from questions asked to a Question Answering system over Topic Maps by extracting the asking point and falling back to the expected answer type when necessary.", "labels": [], "entities": [{"text": "identification of the question focus from questions asked", "start_pos": 31, "end_pos": 88, "type": "TASK", "confidence": 0.7274296879768372}, {"text": "Question Answering", "start_pos": 94, "end_pos": 112, "type": "TASK", "confidence": 0.6048352122306824}]}, {"text": "We use known machine learning techniques for expected answer type extraction and we implement a novel approach to the asking point extraction.", "labels": [], "entities": [{"text": "expected answer type extraction", "start_pos": 45, "end_pos": 76, "type": "TASK", "confidence": 0.6431226059794426}, {"text": "asking point extraction", "start_pos": 118, "end_pos": 141, "type": "TASK", "confidence": 0.6692440410455068}]}, {"text": "We also provide a mathematical model to predict the performance of the system.", "labels": [], "entities": []}], "introductionContent": [{"text": "Topic Maps is an ISO standard 1 for knowledge representation and information integration.", "labels": [], "entities": [{"text": "Topic Maps", "start_pos": 0, "end_pos": 10, "type": "TASK", "confidence": 0.8089009821414948}, {"text": "knowledge representation", "start_pos": 36, "end_pos": 60, "type": "TASK", "confidence": 0.7158624529838562}, {"text": "information integration", "start_pos": 65, "end_pos": 88, "type": "TASK", "confidence": 0.7833777368068695}]}, {"text": "It provides the ability to store complex meta-data together with the data itself.", "labels": [], "entities": []}, {"text": "This work addresses domain portable Question Answering (QA) over Topic Maps.", "labels": [], "entities": [{"text": "domain portable Question Answering (QA)", "start_pos": 20, "end_pos": 59, "type": "TASK", "confidence": 0.7376889969621386}]}, {"text": "That is, a QA system capable of retrieving answers to a question asked against one particular topic map or topic maps collection at a time.", "labels": [], "entities": []}, {"text": "We concentrate on an empirical approach to extract the question focus.", "labels": [], "entities": []}, {"text": "The extracted focus is then anchored to a topic map construct.", "labels": [], "entities": []}, {"text": "This way, we map the type of the answer as provided in the question to the type of the answer as available in the source data.", "labels": [], "entities": []}, {"text": "Our system runs over semi-structured data that encodes ontological information.", "labels": [], "entities": []}, {"text": "The classification scheme we propose is based on one dynamic and one static layer, contrasting with previous work that uses static taxonomies ().", "labels": [], "entities": []}, {"text": "We use the term asking point or AP when the type of the answer is explicit, e.g. the word operas in the question What operas did Puccini write?", "labels": [], "entities": [{"text": "AP", "start_pos": 32, "end_pos": 34, "type": "METRIC", "confidence": 0.9178987741470337}]}, {"text": "We use the term expected answer type or EAT when the type of the answer is implicit but can be deduced from the question using formal methods.", "labels": [], "entities": [{"text": "EAT", "start_pos": 40, "end_pos": 43, "type": "METRIC", "confidence": 0.7372119426727295}]}, {"text": "The question Who composed Tosca?", "labels": [], "entities": [{"text": "Who composed Tosca?", "start_pos": 13, "end_pos": 32, "type": "TASK", "confidence": 0.6141633912920952}]}, {"text": "implies that the answer is a person.", "labels": [], "entities": []}, {"text": "That is, person is the expected answer type.", "labels": [], "entities": []}, {"text": "We consider that AP takes precedence over the EAT.", "labels": [], "entities": [{"text": "AP", "start_pos": 17, "end_pos": 19, "type": "METRIC", "confidence": 0.9894149303436279}, {"text": "EAT", "start_pos": 46, "end_pos": 49, "type": "TASK", "confidence": 0.5353409051895142}]}, {"text": "That is, if the AP (the explicit focus) has been successfully identified in the question, it is considered to be the type of the question, and the EAT (the implicit focus) is left aside.", "labels": [], "entities": [{"text": "AP", "start_pos": 16, "end_pos": 18, "type": "METRIC", "confidence": 0.955146312713623}, {"text": "EAT", "start_pos": 147, "end_pos": 150, "type": "METRIC", "confidence": 0.8808124661445618}]}, {"text": "The claim that the exploitation of AP yields better results in QA over Topic Maps has been tested with 100 questions over the Italian Opera topic map 2 . AP, EAT and the answers of the questions were manually annotated.", "labels": [], "entities": [{"text": "AP", "start_pos": 35, "end_pos": 37, "type": "METRIC", "confidence": 0.9641430974006653}, {"text": "Italian Opera topic map 2", "start_pos": 126, "end_pos": 151, "type": "DATASET", "confidence": 0.9761315107345581}, {"text": "AP", "start_pos": 154, "end_pos": 156, "type": "DATASET", "confidence": 0.6773262023925781}, {"text": "EAT", "start_pos": 158, "end_pos": 161, "type": "DATASET", "confidence": 0.4867013990879059}]}, {"text": "The answers to the questions were annotated as topic map constructs (i.e. as topics or as occurrences).", "labels": [], "entities": []}, {"text": "An evaluation for QA over Topic Maps has been devised that has shown that choosing APs as foci leads to a much better recall and precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 118, "end_pos": 124, "type": "METRIC", "confidence": 0.9993202686309814}, {"text": "precision", "start_pos": 129, "end_pos": 138, "type": "METRIC", "confidence": 0.998242974281311}]}, {"text": "A detailed description of this testis beyond the scope of this paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "The performance of the classifiers was evaluated on our corpus of 2100 questions annotated for AP and EAT.", "labels": [], "entities": [{"text": "AP", "start_pos": 95, "end_pos": 97, "type": "DATASET", "confidence": 0.9012527465820312}, {"text": "EAT", "start_pos": 102, "end_pos": 105, "type": "DATASET", "confidence": 0.6679524779319763}]}, {"text": "The corpus was split into 80% of training and 20% test data, and data re-sampled 10 times in order to account for variance.", "labels": [], "entities": []}, {"text": "lists the figures for the accuracy of the classifiers, that is, the ratio between the correct instances and the overall number of instances.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9992218017578125}]}, {"text": "As the AP classifier operates on words while the EAT classifier operates on questions, we had to estimate the accuracy of the AP classifier per question, to allow for comparison.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 110, "end_pos": 118, "type": "METRIC", "confidence": 0.9992247819900513}]}, {"text": "Two simple metrics are possible.", "labels": [], "entities": []}, {"text": "A lenient metric assumes that the AP extractor performed correctly in the question if there is an overlap between the system output and the annotation on the question level.", "labels": [], "entities": [{"text": "AP extractor", "start_pos": 34, "end_pos": 46, "type": "TASK", "confidence": 0.6282322406768799}]}, {"text": "An exact metric assumes that the AP extractor performed correctly if there is an exact match between the system output and the annotation.", "labels": [], "entities": [{"text": "AP extractor", "start_pos": 33, "end_pos": 45, "type": "TASK", "confidence": 0.832460880279541}]}, {"text": "In the example What are Italian Operas?", "labels": [], "entities": [{"text": "What are Italian Operas?", "start_pos": 15, "end_pos": 39, "type": "TASK", "confidence": 0.6206258356571197}]}, {"text": "(Table 1), assuming the system only tagged operas as AP, lenient accuracy will be 1, exact accuracy will be 0, precision for the AskingPoint class will be 1 and its recall will be 0.5.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.9584694504737854}, {"text": "exact", "start_pos": 85, "end_pos": 90, "type": "METRIC", "confidence": 0.9958723187446594}, {"text": "accuracy", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.7487269639968872}, {"text": "precision", "start_pos": 111, "end_pos": 120, "type": "METRIC", "confidence": 0.9989939332008362}, {"text": "recall", "start_pos": 165, "end_pos": 171, "type": "METRIC", "confidence": 0.9994877576828003}]}, {"text": "shows EAT results by class.", "labels": [], "entities": [{"text": "EAT", "start_pos": 6, "end_pos": 9, "type": "METRIC", "confidence": 0.6054441928863525}]}, {"text": "show AP results by class for the machine learning and the rule-based classifier.", "labels": [], "entities": [{"text": "AP", "start_pos": 5, "end_pos": 7, "type": "METRIC", "confidence": 0.9948306679725647}]}, {"text": "As shown in, when AP classification is available it is used.", "labels": [], "entities": [{"text": "AP classification", "start_pos": 18, "end_pos": 35, "type": "TASK", "confidence": 0.8634407818317413}]}, {"text": "During the evaluation, AP was found in 49.4% of questions.", "labels": [], "entities": [{"text": "AP was", "start_pos": 23, "end_pos": 29, "type": "METRIC", "confidence": 0.970208466053009}]}, {"text": "A mathematical model has been devised to predict the accuracy of the focus extractor on an annotated corpus.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.9990633130073547}]}, {"text": "It is expected that the focus accuracy, that is, the accuracy of the focus extraction system, is dependent on the performance of the AP and the EAT classifiers.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.6704348921775818}, {"text": "accuracy", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.9992308616638184}, {"text": "focus extraction", "start_pos": 69, "end_pos": 85, "type": "TASK", "confidence": 0.7693299651145935}]}, {"text": "Given N the total number of questions,    we define the branching factor, that is, the percentage of questions for which AP is provided by the system, as follows: shows that the sum AP true positives and EAT correct classifications represents the overall number of questions that were classified correctly.", "labels": [], "entities": [{"text": "AP", "start_pos": 121, "end_pos": 123, "type": "METRIC", "confidence": 0.9800338745117188}, {"text": "EAT correct classifications", "start_pos": 204, "end_pos": 231, "type": "METRIC", "confidence": 0.8249494036038717}]}, {"text": "This accuracy can be further developed to present the dependencies as follows: That is, the overall accuracy is dependent on the precision of the AskingPoint class of the AP classifier, the accuracy of EAT and the branching factor.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 5, "end_pos": 13, "type": "METRIC", "confidence": 0.9977723956108093}, {"text": "accuracy", "start_pos": 100, "end_pos": 108, "type": "METRIC", "confidence": 0.9988031387329102}, {"text": "precision", "start_pos": 129, "end_pos": 138, "type": "METRIC", "confidence": 0.9970895648002625}, {"text": "accuracy", "start_pos": 190, "end_pos": 198, "type": "METRIC", "confidence": 0.9993057250976562}]}, {"text": "The branching factor itself can be predicted using the performance of the AP classifier and the ratio between the number of questions annotated with AP and the total number of questions.", "labels": [], "entities": []}, {"text": "4 Related work) describe MOSES, a multilingual QA system delivering answers from Topic Maps.", "labels": [], "entities": []}, {"text": "MOSES extracts a focus constraint (defined after) as part of the question analysis, which is evaluated to an accuracy of 76% for the 85 Danish questions and 70% for the 83 Italian questions.", "labels": [], "entities": [{"text": "MOSES", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.699160099029541}, {"text": "accuracy", "start_pos": 109, "end_pos": 117, "type": "METRIC", "confidence": 0.9992716908454895}]}, {"text": "The focus is an ontological type dependent from the topic map, and its extraction is based on hand-crafted rules.", "labels": [], "entities": []}, {"text": "In our case, focus extraction -though defined with topic map retrieval in mind -stays clear of ontological dependencies so that the same question analysis module can be applied to any topic map.", "labels": [], "entities": [{"text": "focus extraction", "start_pos": 13, "end_pos": 29, "type": "TASK", "confidence": 0.8291621208190918}]}, {"text": "In open domain QA, machine learning approaches have proved successful since).", "labels": [], "entities": []}, {"text": "Despite using similar features, the F-Score (0.824) for our EAT classes is slightly lower than reported by) for coarse classes.", "labels": [], "entities": [{"text": "F-Score", "start_pos": 36, "end_pos": 43, "type": "METRIC", "confidence": 0.9991962313652039}]}, {"text": "We may speculate that the difference is primarily due to our limited training set size (1,680 questions versus 21,500 questions for.", "labels": [], "entities": []}, {"text": "On the other hand, we are not aware of any work attempting to extract AP on word level using machine learning in order to provide dynamic classes to a question classification module.", "labels": [], "entities": [{"text": "question classification module", "start_pos": 151, "end_pos": 181, "type": "TASK", "confidence": 0.7770968576272329}]}], "tableCaptions": [{"text": " Table 1: Gold standard AP annotation", "labels": [], "entities": [{"text": "AP annotation", "start_pos": 24, "end_pos": 37, "type": "TASK", "confidence": 0.6721348762512207}]}, {"text": " Table 2: Distribution of AP classes (word level)", "labels": [], "entities": [{"text": "Distribution of AP classes", "start_pos": 10, "end_pos": 36, "type": "TASK", "confidence": 0.7927891761064529}]}, {"text": " Table 3: Distribution of EAT classes (question  level)", "labels": [], "entities": [{"text": "Distribution of EAT classes", "start_pos": 10, "end_pos": 37, "type": "TASK", "confidence": 0.7851363718509674}]}, {"text": " Table 4: Accuracy of the classifiers (question  level)", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9974639415740967}]}, {"text": " Table 5: EAT performance by class (question  level)", "labels": [], "entities": [{"text": "EAT", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.8946667313575745}]}, {"text": " Table 6: AP performance by class (word level)", "labels": [], "entities": []}, {"text": " Table 7: Rule-based AP performance by class  (word level)", "labels": [], "entities": []}]}