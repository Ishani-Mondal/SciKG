{"title": [{"text": "Collaborative Decoding: Partial Hypothesis Re-ranking Using Translation Consensus between Decoders", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper presents collaborative decoding (co-decoding), anew method to improve machine translation accuracy by leveraging translation consensus between multiple machine translation decoders.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 81, "end_pos": 100, "type": "TASK", "confidence": 0.7761881947517395}, {"text": "accuracy", "start_pos": 101, "end_pos": 109, "type": "METRIC", "confidence": 0.7022788524627686}]}, {"text": "Different from system combination and MBR decoding, which post-process the n-best lists or word lattice of machine translation decoders, in our method multiple machine translation decoders collaborate by exchanging partial translation results.", "labels": [], "entities": [{"text": "MBR decoding", "start_pos": 38, "end_pos": 50, "type": "TASK", "confidence": 0.5797725021839142}]}, {"text": "Using an iterative decoding approach, n-gram agreement statistics between translations of multiple decoders are employed to re-rank both full and partial hypothesis explored in decoding.", "labels": [], "entities": []}, {"text": "Experimental results on data sets for NIST Chinese-to-English machine translation task show that the co-decoding method can bring significant improvements to all baseline decoders, and the outputs from co-decoding can be used to further improve the result of system combination.", "labels": [], "entities": [{"text": "NIST Chinese-to-English machine translation task", "start_pos": 38, "end_pos": 86, "type": "TASK", "confidence": 0.7331924557685852}]}], "introductionContent": [{"text": "Recent research has shown substantial improvements can be achieved by utilizing consensus statistics obtained from outputs of multiple machine translation systems.", "labels": [], "entities": []}, {"text": "Translation consensus can be measured either at sentence level or at word level.", "labels": [], "entities": [{"text": "Translation consensus", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.9015690982341766}]}, {"text": "For example, Minimum Bayes Risk (MBR) () decoding over n-best list tries to find a hypothesis with lowest expected loss with respect to all the other translations, which can be viewed as sentence-level consensus-based decoding.", "labels": [], "entities": [{"text": "Minimum Bayes Risk (MBR)", "start_pos": 13, "end_pos": 37, "type": "METRIC", "confidence": 0.7836218823989233}]}, {"text": "Word based methods proposed range from straightforward consensus voting () to more complicated word-based system combination model (.", "labels": [], "entities": [{"text": "consensus voting", "start_pos": 55, "end_pos": 71, "type": "TASK", "confidence": 0.71182581782341}]}, {"text": "Typically, the resulting systems take outputs of individual machine translation systems as input, and build anew confusion network for second-pass decoding.", "labels": [], "entities": []}, {"text": "There have been many efforts dedicated to advance the state-of-the-art performance by combining multiple systems' outputs.", "labels": [], "entities": []}, {"text": "Most of the work focused on seeking better word alignment for consensus-based confusion network decoding () or word-level system combination (.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 43, "end_pos": 57, "type": "TASK", "confidence": 0.7595119476318359}, {"text": "consensus-based confusion network decoding", "start_pos": 62, "end_pos": 104, "type": "TASK", "confidence": 0.5855103135108948}, {"text": "word-level system combination", "start_pos": 111, "end_pos": 140, "type": "TASK", "confidence": 0.640608568986257}]}, {"text": "In addition to better alignment, introduced an incremental strategy for confusion network construction proposed a hypotheses reranking model for multiple systems' outputs with more features including word translation probability and n-gram agreement statistics.", "labels": [], "entities": [{"text": "confusion network construction", "start_pos": 72, "end_pos": 102, "type": "TASK", "confidence": 0.7453567981719971}, {"text": "word translation", "start_pos": 200, "end_pos": 216, "type": "TASK", "confidence": 0.7186813950538635}]}, {"text": "A common property of all the work mentioned above is that the combination models work on the basis of n-best translation lists (full hypotheses) of existing machine translation systems.", "labels": [], "entities": []}, {"text": "However, the n-best list only presents a very small portion of the entire search space of a Statistical Machine Translation (SMT) model while a majority of the space, within which there are many potentially good translations, is pruned away in decoding.", "labels": [], "entities": [{"text": "Statistical Machine Translation (SMT)", "start_pos": 92, "end_pos": 129, "type": "TASK", "confidence": 0.8057771225770315}]}, {"text": "In fact, due to the limitations of present-day computational resources, a considerable number of promising possibilities have to be abandoned at the early stage of the decoding process.", "labels": [], "entities": []}, {"text": "It is therefore expected that exploring additional possibilities beyond n-best hypotheses lists for full sentences could bring improvements to consensus-based decoding.", "labels": [], "entities": []}, {"text": "In this paper, we present collaborative decoding (or co-decoding), anew SMT decoding scheme to leverage consensus information between multiple machine translation systems.", "labels": [], "entities": [{"text": "SMT decoding", "start_pos": 72, "end_pos": 84, "type": "TASK", "confidence": 0.9284802079200745}]}, {"text": "In this scheme, instead of using a post-processing step, multiple machine translation decoders collaborate during the decoding process, and translation consensus statistics are taken into account to improve ranking not only for full translations, but also for partial hypotheses.", "labels": [], "entities": []}, {"text": "In this way, we expect to reduce search errors caused by partial hypotheses pruning, maximize the contribution of translation consensus, and result in better final translations.", "labels": [], "entities": []}, {"text": "We will discuss the general co-decoding model, requirements for decoders that enable collaborative decoding and describe the updated model structures.", "labels": [], "entities": []}, {"text": "We will present experimental results on the data sets of NIST Chinese-to-English machine translation task, and demonstrate that co-decoding can bring significant improvements to baseline systems.", "labels": [], "entities": [{"text": "NIST Chinese-to-English machine translation task", "start_pos": 57, "end_pos": 105, "type": "TASK", "confidence": 0.8350406885147095}]}, {"text": "We also conduct extensive investigations when different settings of codecoding are applied, and make comparisons with related methods such as word-level system combination of hypothesis selection from multiple n-best lists.", "labels": [], "entities": []}, {"text": "The rest of the paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 gives a formal description of the codecoding model, the strategy to apply consensus information and hypotheses ranking in decoding.", "labels": [], "entities": []}, {"text": "In Section 3, we make detailed comparison between co-decoding and related work such as system combination and hypotheses selection out of multiple systems.", "labels": [], "entities": [{"text": "system combination", "start_pos": 87, "end_pos": 105, "type": "TASK", "confidence": 0.7466482818126678}]}, {"text": "Experimental results and discussions are presented in Section 4.", "labels": [], "entities": []}, {"text": "Section 5 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we present experiments to evaluate the co-decoding method.", "labels": [], "entities": []}, {"text": "We first describe the data sets and baseline systems.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Data set statistics", "labels": [], "entities": []}, {"text": " Table 2: Co-decoding results on test data", "labels": [], "entities": []}, {"text": " Table 3: TER scores between co-decoding  translation outputs", "labels": [], "entities": [{"text": "TER", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9971365928649902}]}, {"text": " Table 4: Co-decoding with/without n-gram  agreement and disagreement features", "labels": [], "entities": []}, {"text": " Table 5: Co-decoding with varied n-gram agree- ment and disagreement features", "labels": [], "entities": []}]}