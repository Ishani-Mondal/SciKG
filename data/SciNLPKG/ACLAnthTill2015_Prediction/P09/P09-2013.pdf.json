{"title": [{"text": "A Unified Single Scan Algorithm for Japanese Base Phrase Chunking and Dependency Parsing", "labels": [], "entities": [{"text": "Japanese Base Phrase Chunking and Dependency Parsing", "start_pos": 36, "end_pos": 88, "type": "TASK", "confidence": 0.5610426579202924}]}], "abstractContent": [{"text": "We describe an algorithm for Japanese analysis that does both base phrase chunk-ing and dependency parsing simultaneously in linear-time with a single scan of a sentence.", "labels": [], "entities": [{"text": "Japanese analysis", "start_pos": 29, "end_pos": 46, "type": "TASK", "confidence": 0.8222792446613312}, {"text": "dependency parsing", "start_pos": 88, "end_pos": 106, "type": "TASK", "confidence": 0.7422991096973419}]}, {"text": "In this paper, we show a pseudo code of the algorithm and evaluate its performance empirically on the Kyoto University Corpus.", "labels": [], "entities": [{"text": "Kyoto University Corpus", "start_pos": 102, "end_pos": 125, "type": "DATASET", "confidence": 0.9866196910540262}]}, {"text": "Experimental results show that the proposed algorithm with the voted perceptron yields reasonably good accuracy .", "labels": [], "entities": [{"text": "accuracy", "start_pos": 103, "end_pos": 111, "type": "METRIC", "confidence": 0.9984398484230042}]}], "introductionContent": [{"text": "Single scan algorithms of parsing are important for interactive applications of NLP.", "labels": [], "entities": [{"text": "parsing", "start_pos": 26, "end_pos": 33, "type": "TASK", "confidence": 0.977229118347168}]}, {"text": "For instance, such algorithms would be more suitable for robots accepting speech inputs or chatbots handling natural language inputs which should respond quickly in some situations even when human inputs are not clearly ended.", "labels": [], "entities": []}, {"text": "Japanese sentence analysis typically consists of three major steps, namely morphological analysis, bunsetsu (base phrase) chunking, and dependency parsing.", "labels": [], "entities": [{"text": "Japanese sentence analysis", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.5858783423900604}, {"text": "morphological analysis", "start_pos": 75, "end_pos": 97, "type": "TASK", "confidence": 0.7803169190883636}, {"text": "bunsetsu (base phrase) chunking", "start_pos": 99, "end_pos": 130, "type": "TASK", "confidence": 0.5949406822522482}, {"text": "dependency parsing", "start_pos": 136, "end_pos": 154, "type": "TASK", "confidence": 0.8300356566905975}]}, {"text": "In this paper, we describe a novel algorithm that combines the last two steps into a single scan process.", "labels": [], "entities": []}, {"text": "The algorithm, which is an extension of, allows us to chunk morphemes into base phrases and decide dependency relations of the phrases in a strict left-toright manner.", "labels": [], "entities": []}, {"text": "We show a pseudo code of the algorithm and evaluate its performance empirically with the voted perceptron on the Kyoto University Corpus ().", "labels": [], "entities": [{"text": "Kyoto University Corpus", "start_pos": 113, "end_pos": 136, "type": "DATASET", "confidence": 0.9833837151527405}]}], "datasetContent": [{"text": "Corpus For evaluation, we used the Kyoto University Corpus Version 2 ().", "labels": [], "entities": [{"text": "Kyoto University Corpus Version 2", "start_pos": 35, "end_pos": 68, "type": "DATASET", "confidence": 0.9755564093589782}]}, {"text": "The split for training/test/development is the same as in other papers, e.g.,).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Performance on the test set. This result is  achieved by the following parameters: The size of  context window is 2 and epoch T is 4.", "labels": [], "entities": []}, {"text": " Table 2: Dependency accuracy. The system with  the previous method employs the algorithm (", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9694167971611023}]}, {"text": " Table 3: Performance change depending on the  context window size", "labels": [], "entities": []}]}