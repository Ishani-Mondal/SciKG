{"title": [{"text": "A Graph-based Semi-Supervised Learning for Question-Answering", "labels": [], "entities": []}], "abstractContent": [{"text": "We present a graph-based semi-supervised learning for the question-answering (QA) task for ranking candidate sentences.", "labels": [], "entities": []}, {"text": "Using textual entailment analysis, we obtain entailment scores between a natural language question posed by the user and the candidate sentences returned from search engine.", "labels": [], "entities": []}, {"text": "The textual entailment between two sentences is assessed via features representing high-level attributes of the en-tailment problem such as sentence structure matching, question-type named-entity matching based on a question-classifier, etc.", "labels": [], "entities": [{"text": "sentence structure matching", "start_pos": 140, "end_pos": 167, "type": "TASK", "confidence": 0.6594956616560618}, {"text": "question-type named-entity matching", "start_pos": 169, "end_pos": 204, "type": "TASK", "confidence": 0.6115927497545878}]}, {"text": "We implement a semi-supervised learning (SSL) approach to demonstrate that utilization of more unlabeled data points can improve the answer-ranking task of QA.", "labels": [], "entities": [{"text": "semi-supervised learning (SSL)", "start_pos": 15, "end_pos": 45, "type": "TASK", "confidence": 0.7004449367523193}]}, {"text": "We create a graph for labeled and unlabeled data using match-scores of textual entailment features as similarity weights between data points.", "labels": [], "entities": []}, {"text": "We apply a summarization method on the graph to make the computations feasible on large datasets.", "labels": [], "entities": []}, {"text": "With anew representation of graph-based SSL on QA datasets using only a handful of features, and under limited amounts of labeled data, we show improvement in generalization performance over state-of-the-art QA models.", "labels": [], "entities": []}], "introductionContent": [{"text": "Open domain natural language question answering (QA) is a process of automatically finding answers to questions searching collections of text files.", "labels": [], "entities": [{"text": "Open domain natural language question answering (QA)", "start_pos": 0, "end_pos": 52, "type": "TASK", "confidence": 0.7590288983450996}, {"text": "automatically finding answers to questions searching collections of text files", "start_pos": 69, "end_pos": 147, "type": "TASK", "confidence": 0.6081806391477584}]}, {"text": "There are intensive research in this area fostered by evaluation-based conferences, such as the Text REtrieval Conference (TREC), etc.", "labels": [], "entities": [{"text": "Text REtrieval Conference (TREC)", "start_pos": 96, "end_pos": 128, "type": "TASK", "confidence": 0.7635286450386047}]}, {"text": "One of the focus of these research, as well as our work, is on factoid questions in English, whereby the answer is a short string that indicates a fact, usually a named entity.", "labels": [], "entities": []}, {"text": "A typical QA system has a pipeline structure starting from extraction of candidate sentences to ranking true answers.", "labels": [], "entities": []}, {"text": "In order to improve QA systems' performance many research focus on different structures such as question processing (, information retrieval (), information extraction), textual entailment (TE) () for ranking, answer extraction, etc.", "labels": [], "entities": [{"text": "QA", "start_pos": 20, "end_pos": 22, "type": "TASK", "confidence": 0.9611223936080933}, {"text": "question processing", "start_pos": 96, "end_pos": 115, "type": "TASK", "confidence": 0.7876124382019043}, {"text": "information retrieval", "start_pos": 119, "end_pos": 140, "type": "TASK", "confidence": 0.6831965148448944}, {"text": "information extraction", "start_pos": 145, "end_pos": 167, "type": "TASK", "confidence": 0.7227138429880142}, {"text": "answer extraction", "start_pos": 210, "end_pos": 227, "type": "TASK", "confidence": 0.8649937808513641}]}, {"text": "Our QA system has a similar pipeline structure and implements anew TE module for information extraction phase of the QA task.", "labels": [], "entities": [{"text": "TE", "start_pos": 67, "end_pos": 69, "type": "METRIC", "confidence": 0.955638587474823}, {"text": "information extraction", "start_pos": 81, "end_pos": 103, "type": "TASK", "confidence": 0.8079468905925751}]}, {"text": "TE is a task of determining if the truth of a text entails the truth of another text (hypothesis). has shown that using TE for filtering or ranking answers can enhance the accuracy of current QA systems, where the answer of a question must be entailed by the text that supports the correctness of this answer.", "labels": [], "entities": [{"text": "TE", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.8127269744873047}, {"text": "accuracy", "start_pos": 172, "end_pos": 180, "type": "METRIC", "confidence": 0.9981061220169067}]}, {"text": "We derive information from pair of texts, i.e., question as hypothesis and candidate sentence as the text, potentially indicating containment of true answer, and cast the inference recognition as classification problem to determine if a question text follows candidate text.", "labels": [], "entities": []}, {"text": "One of the challenges we face with is that we have very limited amount of labeled data, i.e., correctly labeled (true/false entailment) sentences.", "labels": [], "entities": []}, {"text": "Recent research indicates that using labeled and unlabeled data in semi-supervised learning (SSL) environment, with an emphasis on graph-based methods, can improve the performance of information extraction from data for tasks such as question classification (), web classification (), relation extraction (), passage-retrieval (, various natural language processing tasks such as partof-speech tagging, and named-entity recognition, word-sense disam-biguation (), etc.", "labels": [], "entities": [{"text": "information extraction from data", "start_pos": 183, "end_pos": 215, "type": "TASK", "confidence": 0.8206804990768433}, {"text": "question classification", "start_pos": 234, "end_pos": 257, "type": "TASK", "confidence": 0.7605356276035309}, {"text": "web classification", "start_pos": 262, "end_pos": 280, "type": "TASK", "confidence": 0.7269678711891174}, {"text": "relation extraction", "start_pos": 285, "end_pos": 304, "type": "TASK", "confidence": 0.8066416084766388}, {"text": "partof-speech tagging", "start_pos": 380, "end_pos": 401, "type": "TASK", "confidence": 0.8226591944694519}, {"text": "named-entity recognition", "start_pos": 407, "end_pos": 431, "type": "TASK", "confidence": 0.7431810200214386}]}, {"text": "We consider situations where there are much more unlabeled data, X U , than labeled data, XL , i.e., n Ln U . We construct a textual entailment (TE) module by extracting features from each paired question and answer sentence and designing a classifier with a novel yet feasible graphbased SSL method.", "labels": [], "entities": []}, {"text": "The main contributions are: \u2212 construction of a TE module to extract matching structures between question and answer sentences, i.e., q/a pairs.", "labels": [], "entities": []}, {"text": "Our focus is on identifying good matching features from q/a pairs, concerning different sentence structures in section 2, \u2212 representation of our linguistic system by a form of a special graph that uses TE scores in designing a novel affinity matrix in section 3, \u2212 application of a graph-summarization method to enable learning from a very large unlabeled and rather small labeled data, which would not have been feasible for most sophisticated learning tools in section 4.", "labels": [], "entities": [{"text": "TE", "start_pos": 203, "end_pos": 205, "type": "METRIC", "confidence": 0.9962843656539917}]}, {"text": "Finally we demonstrate the results of experiments with real datasets in section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "1: Given X = {x1, ..., xn} , X = XL \u222a XU 2: Set q \u2190 max number of subsets 3: for s \u2190 1, ..., q do 4: Choose a random subset with repetitions 5: Summarize X s to obtain X sin 7: end for 8: Obtain summary dataset and local density constrains, \u03b4 = {\u03b4i} p i=1 . After all data points are evaluated, the sample dataset X scan now be represented with the summary representative vertices as and corresponding local density constraints as, The summarization algorithm is repeated for each random subset X s , s = 1, ..., q of very large dataset X = XL \u222a X U , see Algorithm 1.", "labels": [], "entities": []}, {"text": "As a result q number of summary datasets X s each of which with nb labeled data points are combined to form a representative sample of X, X = X sq s=1 reducing the number of data from n to a much smaller number of data, p = q * nb n.", "labels": [], "entities": []}, {"text": "So the new summary of the X can be represented with X = X i p i=1 . For example, an original dataset with 1M data points can be divided up to q = 50 random samples of m = 5000 data points each.", "labels": [], "entities": []}, {"text": "Then using graph summarization each summarized dataset maybe represented with nb \u223c = 500 data points.", "labels": [], "entities": []}, {"text": "After merging summarized data, final summarized samples compile to 500 * 50 \u223c = 25K 1M data points, reduced to 1/40 of its original size.", "labels": [], "entities": []}, {"text": "Each representative data point in the summarized dataset X is associated with a local density constraints, a p = q * nb dimensional row vector as \u03b4 = {\u03b4 i } p i=1 . We can summarize a graph separately for different sentence structures, i.e., copula and noncopula sentences.", "labels": [], "entities": []}, {"text": "Then representative data points from each summary dataset are merged to form final summary dataset.", "labels": [], "entities": []}, {"text": "The Hybrid graph summary models in the experiments follow such approach.", "labels": [], "entities": []}, {"text": "Instead of using large dataset, we now use summary dataset with predicted labels, and local density constraints to learn the class labels of nte number of unseen data points, i.e., testing data points, X Te = {x 1 , ..., x nte }.", "labels": [], "entities": []}, {"text": "Using graph-based SSL method on the new representative dataset, X = X \u222a X Te , which is comprised of sum- , as labeled data points, and the testing dataset, X Te as unlabeled data points.", "labels": [], "entities": []}, {"text": "Since we do not know estimated local density constraints of unlabeled data points, we use constants to construct local density constraint column vector for X dataset as follows: 0 < \u03b4 i \u2264 1.", "labels": [], "entities": []}, {"text": "To embed the local density constraints, the second term in (3) is replaced with the constrained normalized Laplacian, L c = \u03b4 T L\u03b4, i,j\u2208L\u222aT If any testing vector has an edge between a labeled vector, then with the usage of the local density constraints, the edge weights will not not only be affected by that labeled node, but also how dense that node is within that part of the graph.", "labels": [], "entities": []}, {"text": "We demonstrate the results from three sets of experiments to explore how our graph representation, which encodes textual entailment information, can be used to improve the performance of the QA systems.", "labels": [], "entities": []}, {"text": "We show that as we increase the number of unlabeled data, with our graphsummarization, it is feasible to extract information that can improve the performance of QA models.", "labels": [], "entities": []}, {"text": "We performed experiments on a set of 1449 questions from TREC-99-03.", "labels": [], "entities": [{"text": "TREC-99-03", "start_pos": 57, "end_pos": 67, "type": "DATASET", "confidence": 0.816146731376648}]}, {"text": "Using the search engine 2 , we retrieved around 5 top-ranked candidate sentences from a large newswire corpus for each question to compile around 7200 q/a pairs.", "labels": [], "entities": []}, {"text": "We manually labeled each candidate sentence as true or false entailment depending on the containment of the true answer string and soundness of the entailment to compile quality training set.", "labels": [], "entities": []}, {"text": "We also used a set of 340 QA-type sentence pairs from RTE02-03 and 195 pairs from RTE04 by converting the hypothesis sentences into question form to create additional set of q/a pairs.", "labels": [], "entities": [{"text": "RTE02-03", "start_pos": 54, "end_pos": 62, "type": "DATASET", "confidence": 0.9783696532249451}, {"text": "RTE04", "start_pos": 82, "end_pos": 87, "type": "DATASET", "confidence": 0.9755208492279053}]}, {"text": "In total, we created labeled training dataset XL of around 7600 q/a pairs . We evaluated the performance of graphbased QA system using a set of 202 questions from the TREC04 as testing dataset, ().", "labels": [], "entities": [{"text": "TREC04", "start_pos": 167, "end_pos": 173, "type": "DATASET", "confidence": 0.9332630634307861}]}, {"text": "We retrieved around 20 candidate sentences for each of the 202 test questions and manually labeled each q/a pair as true/false entailment to compile 4037 test data.", "labels": [], "entities": []}, {"text": "To obtain more unlabeled training data X U , we extracted around 100,000 document headlines from a large newswire corpus.", "labels": [], "entities": []}, {"text": "Instead of matching headline and first sentence of the document as in (), we followed a different approach.", "labels": [], "entities": []}, {"text": "Using each headline as a query, we retrieved around 20 top-ranked sentences from search engine.", "labels": [], "entities": []}, {"text": "For each headline, we picked the 1st and the 20th retrieved sentences.", "labels": [], "entities": []}, {"text": "Our assumption is that the first retrieved sentence may have higher probability to entail the headline, whereas the last one may have lower probability.", "labels": [], "entities": []}, {"text": "Each of these headline-candidate sentence pairs is used as additional unlabeled q/a pair.", "labels": [], "entities": []}, {"text": "Since each head-: MRR for different features and methods.", "labels": [], "entities": [{"text": "MRR", "start_pos": 18, "end_pos": 21, "type": "METRIC", "confidence": 0.9432433247566223}]}, {"text": "line represents a converted question, in order to extract the question-type feature, we use a matching NER-type between the headline and candidate sentence to set question-type NER match feature.", "labels": [], "entities": []}, {"text": "We applied pre-processing and feature extraction steps of section 2 to compile labeled and unlabeled training and labeled testing datasets.", "labels": [], "entities": []}, {"text": "We use the rank scores obtained from the search engine as baseline of our system.", "labels": [], "entities": []}, {"text": "We present the performance of the models using Mean Reciprocal Rank (MRR), top 1 (Top1) and top 5 prediction accuracies (Top5) as they are the most commonly used performance measures of QA systems).", "labels": [], "entities": [{"text": "Mean Reciprocal Rank (MRR)", "start_pos": 47, "end_pos": 73, "type": "METRIC", "confidence": 0.9582545558611552}, {"text": "prediction accuracies (Top5)", "start_pos": 98, "end_pos": 126, "type": "METRIC", "confidence": 0.8460704922676087}]}, {"text": "We performed manual iterative parameter optimization during training based on prediction accuracy to find the best k-nearest parameter for SSL, i.e., k = {3, 5, 10, 20, 50} , and best C = 10 \u22122 , .., 10 2 and \u03b3 = 2 \u22122 , .., 2 3 for RBF kernel SVM.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 89, "end_pos": 97, "type": "METRIC", "confidence": 0.9597451090812683}, {"text": "SSL", "start_pos": 139, "end_pos": 142, "type": "TASK", "confidence": 0.8735740184783936}, {"text": "RBF kernel SVM", "start_pos": 232, "end_pos": 246, "type": "DATASET", "confidence": 0.7599409421284994}]}, {"text": "Next we describe three different experiments and present individual results.", "labels": [], "entities": []}, {"text": "Graph summarization makes it feasible to execute SSL on very large unlabeled datasets, which was otherwise impossible.", "labels": [], "entities": [{"text": "Graph summarization", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.772668182849884}]}, {"text": "This paper has no assumptions on the performance of the method in comparison to other SSL methods.", "labels": [], "entities": []}, {"text": "Here we test individual contribution of each set of features on our QA system.", "labels": [], "entities": [{"text": "QA system", "start_pos": 68, "end_pos": 77, "type": "DATASET", "confidence": 0.7590686678886414}]}, {"text": "We applied SVM and our graph based SSL method with no summarization to learn models using labeled training and testing datasets.", "labels": [], "entities": []}, {"text": "For SSL we used the training as labeled and testing as unlabeled dataset in transductive way to predict the entailment scores.", "labels": [], "entities": [{"text": "SSL", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.9638411998748779}]}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "From section 2.2, QTCF represents question-type NER match feature, LexSem is the bundle of lexico-semantic features and QComp is the matching features of subject, head, object, and three complements.", "labels": [], "entities": []}, {"text": "In comparison to the baseline, QComp have a significant effect on the accuracy of the QA system.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 70, "end_pos": 78, "type": "METRIC", "confidence": 0.9994353652000427}]}, {"text": "In addition, QTCF has shown to improve the MRR performance by about 22%.", "labels": [], "entities": [{"text": "QTCF", "start_pos": 13, "end_pos": 17, "type": "DATASET", "confidence": 0.4917350113391876}, {"text": "MRR", "start_pos": 43, "end_pos": 46, "type": "TASK", "confidence": 0.891299307346344}]}, {"text": "Although the LexSem features have minimal semantic properties, they can improve MRR performance by 14%.", "labels": [], "entities": [{"text": "LexSem", "start_pos": 13, "end_pos": 19, "type": "DATASET", "confidence": 0.9386454820632935}, {"text": "MRR", "start_pos": 80, "end_pos": 83, "type": "TASK", "confidence": 0.9528961777687073}]}, {"text": "To evaluate the performance of graph summarization we performed two separate experiments.", "labels": [], "entities": [{"text": "graph summarization", "start_pos": 31, "end_pos": 50, "type": "TASK", "confidence": 0.6219854652881622}]}, {"text": "In the first part, we randomly selected subsets of labeled training dataset where n L represents the sample size of XL . At each random selection, the rest of the labeled dataset is hypothetically used as unlabeled data to verify the performance of our SSL using different sizes of labeled data.", "labels": [], "entities": []}, {"text": "reports the MRR performance of QA system on testing dataset using SVM and our graph-summary SSL (gSum SSL) method using the similarity function in (1).", "labels": [], "entities": [{"text": "MRR", "start_pos": 12, "end_pos": 15, "type": "TASK", "confidence": 0.685920774936676}]}, {"text": "In the second part of the experiment, we applied graph summarization on copula and noncopula questions separately and merged obtained representative points to create labeled summary dataset.", "labels": [], "entities": []}, {"text": "Then using similarity function in (2) we applied SSL on labeled summary and unlabeled testing via transduction.", "labels": [], "entities": []}, {"text": "We call these models as Hybrid gSum SSL.", "labels": [], "entities": []}, {"text": "To build SVM models in the same way, we separated the training dataset into two based on copula and non-copula questions, X cp , X ncp and re-run the SVM method separately.", "labels": [], "entities": []}, {"text": "The testing dataset is divided into two accordingly.", "labels": [], "entities": []}, {"text": "Predicted models from copula sentence datasets are applied on copula sentences of testing dataset and vice versa for non-copula sentences.", "labels": [], "entities": []}, {"text": "The predicted scores are combined to measure overall performance of Hybrid SVM models.", "labels": [], "entities": []}, {"text": "We repeated the experiments five times with different random samples and averaged the results.", "labels": [], "entities": []}, {"text": "Note from that, when the number of labeled data is small (n i L < 10% * n L ), graph based SSL, gSum SSL, has a better performance compared to SVM.", "labels": [], "entities": []}, {"text": "As the percentage of labeled points in training data increase, the SVM performance increases, however graph summary SSL is still comparable with SVM.", "labels": [], "entities": []}, {"text": "On the other hand, when we build separate models for copula and non-copula questions with different features, the performance of the overall model significantly increases in both methods.", "labels": [], "entities": []}, {"text": "Especially in Hybrid graph-Summary SSL, Hybrid gSum SSL, when the number of labeled data is small (n i L < 25% * n L ) performance improvement is better than rest) i.e., MRR=67.0%, Top1=62.0%, Top5=74.0%.", "labels": [], "entities": [{"text": "Hybrid graph-Summary SSL", "start_pos": 14, "end_pos": 38, "type": "TASK", "confidence": 0.5410539706548055}, {"text": "MRR", "start_pos": 170, "end_pos": 173, "type": "METRIC", "confidence": 0.9990511536598206}]}, {"text": "This is due to the fact that we establish two seperate entailment models for copula and non-copula q/a sentence pairs that enables extracting useful information and better representation of the specific data.", "labels": [], "entities": []}, {"text": "Although SSL methods are capable of exploiting information from unlabeled data, learning becomes infeasible as the number of data points gets very large.", "labels": [], "entities": []}, {"text": "There are various research on SLL to overcome the usage of large number of unlabeled dataset challenge).", "labels": [], "entities": [{"text": "SLL", "start_pos": 30, "end_pos": 33, "type": "TASK", "confidence": 0.9750705361366272}]}, {"text": "Our graph summarization method, Hybrid gsum SSL, has a different approach.", "labels": [], "entities": [{"text": "graph summarization", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.6046399772167206}]}, {"text": "which can summarize very large datasets into representative data points and embed the original spatial information of data points, namely local density constraints, within the SSL summarization schema.", "labels": [], "entities": []}, {"text": "We demonstrate that as more labeled data is used, we would have a richer summary dataset with additional spatial information that would help to improve the the performance of the graph summary models.", "labels": [], "entities": []}, {"text": "We gradually increase the number of unlabeled data samples as shown in to demonstrate the effects on the performance of testing dataset.", "labels": [], "entities": []}, {"text": "The results show that the number of unlabeled data has positive effect on performance of graph summarization SSL.", "labels": [], "entities": [{"text": "graph summarization SSL", "start_pos": 89, "end_pos": 112, "type": "TASK", "confidence": 0.6519982119401296}]}], "tableCaptions": [{"text": " Table 1: MRR for different features and methods.", "labels": [], "entities": [{"text": "MRR", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.5005759596824646}]}, {"text": " Table 1. From section 2.2, QTCF represents  question-type NER match feature, LexSem is the  bundle of lexico-semantic features and QComp is  the matching features of subject, head, object, and  three complements. In comparison to the baseline,  QComp have a significant effect on the accuracy  of the QA system. In addition, QTCF has shown", "labels": [], "entities": [{"text": "accuracy", "start_pos": 285, "end_pos": 293, "type": "METRIC", "confidence": 0.9990633130073547}]}, {"text": " Table 2: The MRR (%) results of graph-summary SSL (gSum SSL) and SVM as well as Hybrid gSum  SSL and Hybrid SVM with different sizes of labeled data.", "labels": [], "entities": [{"text": "MRR", "start_pos": 14, "end_pos": 17, "type": "METRIC", "confidence": 0.9783565402030945}]}, {"text": " Table 3: The effect of number of unlabeled data  on MRR from Hybrid graph Summarization SSL.", "labels": [], "entities": [{"text": "MRR", "start_pos": 53, "end_pos": 56, "type": "TASK", "confidence": 0.706767201423645}, {"text": "Summarization SSL", "start_pos": 75, "end_pos": 92, "type": "TASK", "confidence": 0.7168299853801727}]}]}