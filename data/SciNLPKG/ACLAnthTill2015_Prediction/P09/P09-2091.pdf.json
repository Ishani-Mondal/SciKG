{"title": [{"text": "The Impact of Query Refinement in the Web People Search Task", "labels": [], "entities": [{"text": "Query Refinement", "start_pos": 14, "end_pos": 30, "type": "TASK", "confidence": 0.7090398371219635}]}], "abstractContent": [{"text": "Searching fora person name in a Web Search Engine usually leads to a number of web pages that refer to several people sharing the same name.", "labels": [], "entities": []}, {"text": "In this paper we study whether it is reasonable to assume that pages about the desired person can be filtered by the user by adding query terms.", "labels": [], "entities": []}, {"text": "Our results indicate that, although inmost occasions there is a query refinement that gives all and only those pages related to an individual, it is unlikely that the user is able to find this expression a priori.", "labels": [], "entities": []}], "introductionContent": [{"text": "The Web has now become an essential resource to obtain information about individuals but, at the same time, its growth has made web people search (WePS) a challenging task, because every single name is usually shared by many different people.", "labels": [], "entities": [{"text": "web people search (WePS)", "start_pos": 128, "end_pos": 152, "type": "TASK", "confidence": 0.619713361064593}]}, {"text": "One of the mainstream approaches to solve this problem is designing meta-search engines that cluster search results, producing one cluster per person which contains all documents referring to this person.", "labels": [], "entities": []}, {"text": "Up to now, two evaluation campaigns -WePS 1 in 2007 ( and WePS 2 in 2009 ( ) -have produced datasets for this clustering task, with over 15 research groups submitting results in each campaign.", "labels": [], "entities": [{"text": "clustering task", "start_pos": 110, "end_pos": 125, "type": "TASK", "confidence": 0.8958865702152252}]}, {"text": "Since the release of the first datasets, this task is becoming an increasingly popular research topic among Information Retrieval and Natural Language Processing researchers.", "labels": [], "entities": [{"text": "Information Retrieval and Natural Language Processing", "start_pos": 108, "end_pos": 161, "type": "TASK", "confidence": 0.6608288784821829}]}, {"text": "For precision oriented queries (for instance, finding the homepage, the email or the phone number of a given person), clustered results might help locating the desired data faster while avoiding confusion with other people sharing the same name.", "labels": [], "entities": []}, {"text": "But the utility of clustering is more obvious for recall oriented queries, where the goal is to mine the web for information about a person.", "labels": [], "entities": []}, {"text": "Ina typical hiring process, for instance, candidates are evaluated not only according to their cv, but also according to their web profile, i.e. information about them available in the Web.", "labels": [], "entities": []}, {"text": "One question that naturally arises is whether search results clustering can effectively help users for this task.", "labels": [], "entities": [{"text": "search results clustering", "start_pos": 46, "end_pos": 71, "type": "TASK", "confidence": 0.62545578678449}]}, {"text": "Eventually, a query refinement made by the user -for instance, adding an affiliation or a location -might have the desired disambiguation effect without compromising recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 166, "end_pos": 172, "type": "METRIC", "confidence": 0.9952399730682373}]}, {"text": "The hypothesis underlying most research on Web People Search is that query refinement is risky, because it can enhance precision but it will usually harm recall.", "labels": [], "entities": [{"text": "query refinement", "start_pos": 69, "end_pos": 85, "type": "TASK", "confidence": 0.7961792647838593}, {"text": "precision", "start_pos": 119, "end_pos": 128, "type": "METRIC", "confidence": 0.9978356957435608}, {"text": "recall", "start_pos": 154, "end_pos": 160, "type": "METRIC", "confidence": 0.9969545602798462}]}, {"text": "Adding the current affiliation of a person, for instance, might make information about previous jobs disappear from search results.", "labels": [], "entities": []}, {"text": "This hypothesis has not, up to now, been empirically confirmed, and it is the goal of this paper.", "labels": [], "entities": []}, {"text": "We want to evaluate the actual impact of using query refinements in the Web People Search (WePS) clustering task (as defined in the framework of the WePS evaluation).", "labels": [], "entities": [{"text": "Web People Search (WePS) clustering task", "start_pos": 72, "end_pos": 112, "type": "TASK", "confidence": 0.5739849135279655}]}, {"text": "For this, we have studied to what extent a query refinement can successfully filter relevant results and which type of refinements are the most successful.", "labels": [], "entities": []}, {"text": "In our experiments we have considered the search results associated to one individual as a set of relevant documents, and we have tested the ability of different query refinement strategies to retrieve those documents.", "labels": [], "entities": []}, {"text": "Our results are conclusive: inmost occasions there is a \"near-perfect\" refinement that filters out most relevant information about a given person, but this refinement is very hard to predict from a user's perspective.", "labels": [], "entities": []}, {"text": "In Section 2 we describe the datasets that where used for our experiments.", "labels": [], "entities": []}, {"text": "The experimental methodology and results are presented in Section 3.", "labels": [], "entities": []}, {"text": "Finally we present our conclusions in 4.", "labels": [], "entities": []}], "datasetContent": [{"text": "In our experiments we consider each set of documents (cluster) related to one individual in the WePS corpus as a set of relevant documents fora person search.", "labels": [], "entities": [{"text": "WePS corpus", "start_pos": 96, "end_pos": 107, "type": "DATASET", "confidence": 0.9601804912090302}]}, {"text": "For instance the James Patter-362 field F prec.", "labels": [], "entities": [{"text": "James Patter-362 field F prec", "start_pos": 17, "end_pos": 46, "type": "DATASET", "confidence": 0.817149305343628}]}, {"text": "best-ae 1.00 0.99 1.00 0.74 best-all 1.00 1.00 1.00 1.00 best-ner 1.00 1.00 1.00 0.99 best-nl 1.00 1.00 1.00 1.00: Results for clusters of size >=3 son dataset in the WePS corpus contains a total of 100 documents, and 10 of them belong to a British politician named James Patterson.", "labels": [], "entities": [{"text": "son dataset", "start_pos": 148, "end_pos": 159, "type": "DATASET", "confidence": 0.8749454915523529}, {"text": "WePS corpus", "start_pos": 167, "end_pos": 178, "type": "DATASET", "confidence": 0.9773713648319244}]}, {"text": "The WePS-2 corpus contains a total of 552 clusters that were used to evaluate the different types of QRs.", "labels": [], "entities": [{"text": "WePS-2 corpus", "start_pos": 4, "end_pos": 17, "type": "DATASET", "confidence": 0.9762043058872223}]}, {"text": "For each person cluster, our goal is to find the best query refinements; in an ideal case, an expression that is present in all documents in the cluster, and not present in documents outside the cluster.", "labels": [], "entities": []}, {"text": "For each QR type (affiliation, e-mail, n-grams of various sizes, etc.) we consider all candidates found in at least one document from the cluster, and pickup the one that leads to the best harmonic mean (F \u03b1=.5 ) of precision and recall on the cluster documents (there might be more than one).", "labels": [], "entities": [{"text": "F \u03b1", "start_pos": 204, "end_pos": 207, "type": "METRIC", "confidence": 0.9713401794433594}, {"text": "precision", "start_pos": 216, "end_pos": 225, "type": "METRIC", "confidence": 0.9982157945632935}, {"text": "recall", "start_pos": 230, "end_pos": 236, "type": "METRIC", "confidence": 0.9986956715583801}]}, {"text": "For instance, when we evaluate a set of token QR candidates for the politician in the James Patterson dataset we find that among all the tokens that appear in the documents of its cluster, \"republican\" gives us a perfect score, while \"politician\" obtains a low precision (we retrieve documents of other politicians named James Patterson).", "labels": [], "entities": [{"text": "James Patterson dataset", "start_pos": 86, "end_pos": 109, "type": "DATASET", "confidence": 0.9357315301895142}, {"text": "precision", "start_pos": 261, "end_pos": 270, "type": "METRIC", "confidence": 0.9955065250396729}]}, {"text": "In some cases a cluster might not have any candidate fora particular type of QR.", "labels": [], "entities": []}, {"text": "For instance, manual person attributes like phone number are sparse and won't be available for every individual, whereas tokens and ngrams are always present.", "labels": [], "entities": []}, {"text": "We exclude those cases when computing F, and instead we report a coverage measure which represents the number of clusters which have at least one candidate of this type of QR.", "labels": [], "entities": []}, {"text": "This way we know how often we can use an attribute (coverage): Distribution of the person attributes used for the \"best-ae\" strategy and how useful it is when available (F measure).", "labels": [], "entities": [{"text": "F measure)", "start_pos": 170, "end_pos": 180, "type": "METRIC", "confidence": 0.9846514860788981}]}, {"text": "These figures represent a ceiling for each type of query refinement: they represent the efficiency of the query when the user selects the best possible refinement fora given QR type.", "labels": [], "entities": []}, {"text": "We have split the results in three groups depending on the size of the target cluster: (i) rare people, mentioned in only one document (335 clusters of size 1); (ii)people that appear in two documents (92 clusters of size 2), often these documents belong to the same domain, or are very similar; and (iii) all other cases (125 clusters of size >=3).", "labels": [], "entities": []}, {"text": "We also report on the aggregated results for certain subsets of QR types.", "labels": [], "entities": []}, {"text": "For instance, if we want to know what results will get a user that picks the best person attribute, we consider all types of attributes (e-mail, affiliation, etc.) for every cluster, and pickup the ones that lead to the best results.", "labels": [], "entities": []}, {"text": "We consider four groups: (i) best-all selects the best QR among all the available QR types (ii) bestae considers all manually annotated attributes (iii) best-ner considers automatically annotated NEs; and (iv) best-ng uses only tokens and ngrams.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results for clusters of size 1", "labels": [], "entities": []}, {"text": " Table 2: Results for clusters of size 2", "labels": [], "entities": []}, {"text": " Table 3: Results for clusters of size >=3", "labels": [], "entities": []}, {"text": " Table 4: Results for clusters of size 1", "labels": [], "entities": []}, {"text": " Table 5: Results for clusters of size 2", "labels": [], "entities": []}, {"text": " Table 6: Results for clusters of size >=3", "labels": [], "entities": []}, {"text": " Table 7: Distribution of the person attributes used  for the \"best-ae\" strategy", "labels": [], "entities": []}]}