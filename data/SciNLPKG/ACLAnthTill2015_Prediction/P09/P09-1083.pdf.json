{"title": [{"text": "Answering Opinion Questions with Random Walks on Graphs", "labels": [], "entities": [{"text": "Answering Opinion Questions", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.8965377410252889}]}], "abstractContent": [{"text": "Opinion Question Answering (Opinion QA), which aims to find the authors' sentimental opinions on a specific target, is more challenging than traditional fact-based question answering problems.", "labels": [], "entities": [{"text": "Opinion Question Answering (Opinion QA)", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.7774912927831922}, {"text": "fact-based question answering", "start_pos": 153, "end_pos": 182, "type": "TASK", "confidence": 0.7464165886243185}]}, {"text": "To extract the opinion oriented answers, we need to consider both topic relevance and opinion sentiment issues.", "labels": [], "entities": []}, {"text": "Current solutions to this problem are mostly ad-hoc combinations of question topic information and opinion information.", "labels": [], "entities": []}, {"text": "In this paper , we propose an Opinion PageRank model and an Opinion HITS model to fully explore the information from different relations among questions and answers, answers and answers, and topics and opinions.", "labels": [], "entities": []}, {"text": "By fully exploiting these relations, the experiment results show that our proposed algorithms outperform several state of the art baselines on benchmark data set.", "labels": [], "entities": []}, {"text": "A gain of over 10% in F scores is achieved as compared to many other systems.", "labels": [], "entities": [{"text": "F scores", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9829015731811523}]}], "introductionContent": [{"text": "Question Answering (QA), which aims to provide answers to human-generated questions automatically, is an important research area in natural language processing (NLP) and much progress has been made on this topic in previous years.", "labels": [], "entities": [{"text": "Question Answering (QA)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.8804803133010864}, {"text": "natural language processing (NLP)", "start_pos": 132, "end_pos": 165, "type": "TASK", "confidence": 0.7912160356839498}]}, {"text": "However, the objective of most state-of-the-art QA systems is to find answers to factual questions, such as \"What is the longest river in the United States?\" and \"Who is Andrew Carnegie?\"", "labels": [], "entities": []}, {"text": "In fact, rather than factual information, people would also like to know about others' opinions, thoughts and feelings toward some specific objects, people and events.", "labels": [], "entities": []}, {"text": "Some examples of these questions are: \"How is Bush's decision not to ratify the Kyoto Protocol looked upon by Japan and other US allies?\"() and \"Why do people like Subway Sandwiches?\" from TAC 2008.", "labels": [], "entities": [{"text": "TAC 2008", "start_pos": 189, "end_pos": 197, "type": "DATASET", "confidence": 0.9192048013210297}]}, {"text": "Systems designed to deal with such questions are called opinion QA systems.", "labels": [], "entities": []}, {"text": "Researchers () have found that opinion questions have very different characteristics when compared with fact-based questions: opinion questions are often much longer, more likely to represent partial answers rather than complete answers and vary much more widely.", "labels": [], "entities": []}, {"text": "These features make opinion QA a harder problem to tackle than fact-based QA.", "labels": [], "entities": [{"text": "opinion QA", "start_pos": 20, "end_pos": 30, "type": "TASK", "confidence": 0.6867248713970184}]}, {"text": "Also as shown in), directly applying previous systems designed for fact-based QA onto opinion QA tasks would not achieve good performances.", "labels": [], "entities": []}, {"text": "Similar to other complex QA tasks, the problem of opinion QA can be viewed as a sentence ranking problem.", "labels": [], "entities": [{"text": "sentence ranking", "start_pos": 80, "end_pos": 96, "type": "TASK", "confidence": 0.6998841613531113}]}, {"text": "The Opinion QA task needs to consider not only the topic relevance of a sentence (to identify whether this sentence matches the topic of the question) but also the sentiment of a sentence (to identify the opinion polarity of a sentence).", "labels": [], "entities": [{"text": "Opinion QA task", "start_pos": 4, "end_pos": 19, "type": "TASK", "confidence": 0.7542687157789866}]}, {"text": "Current solutions to opinion QA tasks are generally in ad hoc styles: the topic score and the opinion score are usually separately calculated and then combined via a linear combination () or just filter out the candidate without matching the question sentiment ().", "labels": [], "entities": [{"text": "opinion QA tasks", "start_pos": 21, "end_pos": 37, "type": "TASK", "confidence": 0.7622032562891642}]}, {"text": "However, topic and opinion are not independent in reality.", "labels": [], "entities": []}, {"text": "The opinion words are closely associated with their contexts.", "labels": [], "entities": []}, {"text": "Another problem is that existing algorithms compute the score for each answer candidate individually, in other words, they do not consider the relations between answer candidates.", "labels": [], "entities": []}, {"text": "The quality of a answer candidate is not only determined by the relevance to the question, but also by other candidates.", "labels": [], "entities": []}, {"text": "For example, the good answer maybe mentioned by many candidates.", "labels": [], "entities": []}, {"text": "In this paper, we propose two models to address the above limitations of previous sentence ranking models.", "labels": [], "entities": []}, {"text": "We incorporate both the topic relevance information and the opinion sentiment information into our sentence ranking procedure.", "labels": [], "entities": []}, {"text": "Meanwhile, our sentence ranking models could naturally consider the relationships between different answer candidates.", "labels": [], "entities": []}, {"text": "More specifically, our first model, called Opinion PageRank, incorporates opinion sentiment information into the graph model as a condition.", "labels": [], "entities": []}, {"text": "The second model, called Opinion HITS model, considers the sentences as authorities and both question topic information and opinion sentiment information as hubs.", "labels": [], "entities": []}, {"text": "The experiment results on the TAC QA data set demonstrate the effectiveness of the proposed Random Walk based methods.", "labels": [], "entities": [{"text": "TAC QA data set", "start_pos": 30, "end_pos": 45, "type": "DATASET", "confidence": 0.9259300976991653}]}, {"text": "Our proposed method performs better than the best method in the TAC 2008 competition.", "labels": [], "entities": [{"text": "TAC 2008 competition", "start_pos": 64, "end_pos": 84, "type": "DATASET", "confidence": 0.8519015113512675}]}, {"text": "The rest of this paper is organized as follows: Section 2 introduces some related works.", "labels": [], "entities": []}, {"text": "We will discuss our proposed models in Section 3.", "labels": [], "entities": []}, {"text": "In Section 4, we present an overview of our opinion QA system.", "labels": [], "entities": []}, {"text": "The experiment results are shown in Section 5.", "labels": [], "entities": []}, {"text": "Finally, Section 6 concludes this paper and provides possible directions for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "We employ the dataset from the TAC 2008 QA track.", "labels": [], "entities": [{"text": "TAC 2008 QA track", "start_pos": 31, "end_pos": 48, "type": "DATASET", "confidence": 0.9597820788621902}]}, {"text": "The task contains a total of 87 squishy opinion questions.", "labels": [], "entities": []}, {"text": "1 These questions have simple forms, and can be easily divided into positive type or negative type, for example \"Why do people like Mythbusters?\" and \"What were the specific actions or reasons given fora negative attitude towards Mahmoud Ahmadinejad?\".", "labels": [], "entities": []}, {"text": "The initial topic word for each question (called target in TAC) is also provided.", "labels": [], "entities": []}, {"text": "Since our work in this paper focuses on sentence ranking for opinion QA, these characteristics of TAC data make it easy to process question analysis.", "labels": [], "entities": [{"text": "process question analysis", "start_pos": 123, "end_pos": 148, "type": "TASK", "confidence": 0.617603212594986}]}, {"text": "Answers for all questions must be retrieved from the TREC Blog06 collection).", "labels": [], "entities": [{"text": "TREC Blog06 collection", "start_pos": 53, "end_pos": 75, "type": "DATASET", "confidence": 0.9144662022590637}]}, {"text": "The collection is a large sample of the blog sphere, crawled over an eleven-week period from December.", "labels": [], "entities": []}, {"text": "We retrieve the top 50 documents for each question.", "labels": [], "entities": []}, {"text": "We adopt the evaluation metrics used in the TAC squishy opinion QA task.", "labels": [], "entities": [{"text": "TAC squishy opinion QA task", "start_pos": 44, "end_pos": 71, "type": "TASK", "confidence": 0.6980183362960816}]}, {"text": "The TAC assessors create a list of acceptable information nuggets for each question.", "labels": [], "entities": []}, {"text": "Each nugget will be assigned a normalized weight based on the number of assessors who judged it to be vital.", "labels": [], "entities": []}, {"text": "We use these nuggets and corresponding weights to assess our approach.", "labels": [], "entities": []}, {"text": "Three human assessors complete the evaluation process.", "labels": [], "entities": []}, {"text": "Every question is scored using nugget recall (NR) and an approximation to nugget precision (NP) based on length.", "labels": [], "entities": [{"text": "recall", "start_pos": 38, "end_pos": 44, "type": "METRIC", "confidence": 0.8398066759109497}, {"text": "nugget precision (NP)", "start_pos": 74, "end_pos": 95, "type": "METRIC", "confidence": 0.7576929211616517}]}, {"text": "The final score will be calculated using F measure with TAC official value \u03b2 = 3.", "labels": [], "entities": [{"text": "F measure", "start_pos": 41, "end_pos": 50, "type": "METRIC", "confidence": 0.9800411760807037}, {"text": "TAC official value \u03b2", "start_pos": 56, "end_pos": 76, "type": "METRIC", "confidence": 0.9692372381687164}]}, {"text": "This means recall is 3 times as important as precision: where NP is the sum of weights of nuggets returned in response over the total sum of weights of all nuggets in nugget list, and NP = 1 \u2212 (length \u2212 allowance)/(length) if length is no less than allowance and 0 otherwise.", "labels": [], "entities": [{"text": "recall", "start_pos": 11, "end_pos": 17, "type": "METRIC", "confidence": 0.9992519021034241}, {"text": "precision", "start_pos": 45, "end_pos": 54, "type": "METRIC", "confidence": 0.9995642304420471}, {"text": "NP", "start_pos": 184, "end_pos": 186, "type": "METRIC", "confidence": 0.9655386209487915}, {"text": "length \u2212 allowance)/(length)", "start_pos": 194, "end_pos": 222, "type": "METRIC", "confidence": 0.6528557489315668}]}, {"text": "Here allowance = 100 \u00d7 (\u266fnuggets returned) and length equals to the number of non-white characters in strings.", "labels": [], "entities": [{"text": "allowance", "start_pos": 5, "end_pos": 14, "type": "METRIC", "confidence": 0.9836193919181824}, {"text": "length", "start_pos": 47, "end_pos": 53, "type": "METRIC", "confidence": 0.9984627962112427}]}, {"text": "We will use average F Score to evaluate the performance for each system.", "labels": [], "entities": [{"text": "F Score", "start_pos": 20, "end_pos": 27, "type": "METRIC", "confidence": 0.988042801618576}]}], "tableCaptions": [{"text": " Table 1: Sentiment lexicon description", "labels": [], "entities": [{"text": "Sentiment lexicon", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.9369227588176727}]}, {"text": " Table 3: Comparison results with TAC 2008 Three  Top Ranked Systems (system 1-3 demonstrate top  3 systems in TAC)", "labels": [], "entities": [{"text": "TAC 2008", "start_pos": 34, "end_pos": 42, "type": "DATASET", "confidence": 0.8841560781002045}, {"text": "TAC", "start_pos": 111, "end_pos": 114, "type": "DATASET", "confidence": 0.9228300452232361}]}]}