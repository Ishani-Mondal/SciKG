{"title": [{"text": "Discriminative Lexicon Adaptation for Improved Character Accuracy - A New Direction in Chinese Language Modeling", "labels": [], "entities": [{"text": "Discriminative Lexicon Adaptation", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.6450050671895345}, {"text": "Chinese Language Modeling", "start_pos": 87, "end_pos": 112, "type": "TASK", "confidence": 0.5880947212378184}]}], "abstractContent": [{"text": "While OOV is always a problem for most languages in ASR, in the Chinese case the problem can be avoided by utilizing character n-grams and moderate performances can be obtained.", "labels": [], "entities": [{"text": "OOV", "start_pos": 6, "end_pos": 9, "type": "METRIC", "confidence": 0.841716468334198}, {"text": "ASR", "start_pos": 52, "end_pos": 55, "type": "TASK", "confidence": 0.9329543709754944}]}, {"text": "However, character n-gram has its own limitation and proper addition of new words can increase the ASR performance.", "labels": [], "entities": [{"text": "ASR", "start_pos": 99, "end_pos": 102, "type": "TASK", "confidence": 0.9303727149963379}]}, {"text": "Here we propose a dis-criminative lexicon adaptation approach for improved character accuracy, which not only adds new words but also deletes some words from the current lexicon.", "labels": [], "entities": [{"text": "dis-criminative lexicon adaptation", "start_pos": 18, "end_pos": 52, "type": "TASK", "confidence": 0.6785773436228434}, {"text": "accuracy", "start_pos": 85, "end_pos": 93, "type": "METRIC", "confidence": 0.9689018726348877}]}, {"text": "Different from other lexicon adaptation approaches, we consider the acoustic features and make our lexicon adaptation criterion consistent with that in the decoding process.", "labels": [], "entities": []}, {"text": "The proposed approach not only improves the ASR character accuracy but also significantly enhances the performance of a character-based spoken document retrieval system.", "labels": [], "entities": [{"text": "ASR character", "start_pos": 44, "end_pos": 57, "type": "TASK", "confidence": 0.8608672618865967}, {"text": "accuracy", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.7418500185012817}]}], "introductionContent": [{"text": "Generally, an automatic speech recognition (ASR) system requires a lexicon.", "labels": [], "entities": [{"text": "automatic speech recognition (ASR)", "start_pos": 14, "end_pos": 48, "type": "TASK", "confidence": 0.8111786047617594}]}, {"text": "The lexicon defines the possible set of output words and also the building units in the language model (LM).", "labels": [], "entities": []}, {"text": "Lexical words offer local constraints to combine phonemes into short chunks while the language model combines phonemes into longer chunks by more global constraints.", "labels": [], "entities": []}, {"text": "However, it's almost impossible to include all words into a lexicon both due to the technical difficulty and also the fact that new words are created continuously.", "labels": [], "entities": []}, {"text": "The missed out words will never be recognized, which is the well-known OOV problem.", "labels": [], "entities": [{"text": "OOV", "start_pos": 71, "end_pos": 74, "type": "METRIC", "confidence": 0.6224476099014282}]}, {"text": "Using graphemes for OOV handling is proposed in English (.", "labels": [], "entities": [{"text": "OOV handling", "start_pos": 20, "end_pos": 32, "type": "TASK", "confidence": 0.8462114930152893}]}, {"text": "Although this sacrifices some of the lexical constraints and introduces a further difficulty to combine graphemes back into words, it is compensated by its ability for open vocabulary ASR.", "labels": [], "entities": [{"text": "open vocabulary ASR", "start_pos": 168, "end_pos": 187, "type": "TASK", "confidence": 0.5385583142439524}]}, {"text": "Morphs are another possibility, which are longer than graphemes but shorter than words, in other western languages).", "labels": [], "entities": []}, {"text": "Chinese language, on the other hand, is quite different from western languages.", "labels": [], "entities": []}, {"text": "There are no blanks between words and the definition for words is vague.", "labels": [], "entities": []}, {"text": "Since almost all characters in Chinese have their own meanings and words are composed of the characters, there is an obvious solution for the OOV problem: simply using all characters as the lexicon.", "labels": [], "entities": [{"text": "OOV", "start_pos": 142, "end_pos": 145, "type": "TASK", "confidence": 0.5674631595611572}]}, {"text": "In we seethe differences in character recognition accuracy by using only 5.8K characters and a full set of 61.5K lexicon.", "labels": [], "entities": [{"text": "character recognition", "start_pos": 28, "end_pos": 49, "type": "TASK", "confidence": 0.8973550200462341}, {"text": "accuracy", "start_pos": 50, "end_pos": 58, "type": "METRIC", "confidence": 0.951970100402832}]}, {"text": "The training set and testing set are the same as those that will be introduced in Section 4.1.", "labels": [], "entities": []}, {"text": "It is clear that characters alone can provide moderate recognition accuracies while augmenting new words significantly improves the performance.", "labels": [], "entities": []}, {"text": "If the words' semantic functionality can be abandoned, which definitely cannot be replaced by characters, we can treat words as a means to enhance character recognition accuracy.", "labels": [], "entities": [{"text": "character recognition", "start_pos": 147, "end_pos": 168, "type": "TASK", "confidence": 0.7299730479717255}, {"text": "accuracy", "start_pos": 169, "end_pos": 177, "type": "METRIC", "confidence": 0.7595398426055908}]}, {"text": "Such arguments stand at least for Chinese ASR since they evaluate on character error rate and do not add explicit blanks between words.", "labels": [], "entities": [{"text": "Chinese ASR", "start_pos": 34, "end_pos": 45, "type": "TASK", "confidence": 0.5883085131645203}]}, {"text": "Here we formulate a lexicon adaptation problem and try to discriminatively find out not only OOV words beneficial for ASR but also those existing words that can be deleted.", "labels": [], "entities": [{"text": "ASR", "start_pos": 118, "end_pos": 121, "type": "TASK", "confidence": 0.9882412552833557}]}, {"text": "Unlike previous lexicon adaptation or construction approaches), we consider the acoustic signals and also the whole speech decoding structure.", "labels": [], "entities": []}, {"text": "We propose to use a simple approximation for the character posterior probabilities (PPs), which combines acoustic model and language model scores after decoding.", "labels": [], "entities": [{"text": "character posterior probabilities (PPs)", "start_pos": 49, "end_pos": 88, "type": "METRIC", "confidence": 0.6611820658047994}]}, {"text": "Based on the character PPs, we adapt the current lexicon.", "labels": [], "entities": []}, {"text": "The language model is then re-trained according the new lexicon.", "labels": [], "entities": []}, {"text": "Such procedure can be iterated until convergence.", "labels": [], "entities": []}, {"text": "Characters, are not only the output units in Chinese ASR but also have their roles in spoken document retrieval (SDR).", "labels": [], "entities": [{"text": "ASR", "start_pos": 53, "end_pos": 56, "type": "TASK", "confidence": 0.5801711082458496}, {"text": "spoken document retrieval (SDR)", "start_pos": 86, "end_pos": 117, "type": "TASK", "confidence": 0.7996082206567129}]}, {"text": "It has been shown that characters are good indexing units.", "labels": [], "entities": []}, {"text": "Generally, characters can at least help OOV query handling; in the subword-based confusion network (S-CN) proposed by, characters are even better than words for in-vocabulary (IV) queries.", "labels": [], "entities": [{"text": "OOV query handling", "start_pos": 40, "end_pos": 58, "type": "TASK", "confidence": 0.9147236943244934}]}, {"text": "In addition to evaluating the proposed approach on ASR performance, we investigate its helpfulness when integrated with an S-CN framework.", "labels": [], "entities": [{"text": "ASR", "start_pos": 51, "end_pos": 54, "type": "TASK", "confidence": 0.9949992895126343}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Character recognition accuracy under dif- ferent lexicons and the order of language model.", "labels": [], "entities": [{"text": "Character recognition", "start_pos": 10, "end_pos": 31, "type": "TASK", "confidence": 0.9131090641021729}, {"text": "accuracy", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.969096839427948}]}, {"text": " Table 3: ASR character accuracies for the baseline  and the proposed LAICA approach. Two iterations  are performed, each with three versions. A: only  add new words, D: only delete words and A+D: si- multaneously add and delete words. + and -means  the number of words added and deleted, respec- tively.", "labels": [], "entities": [{"text": "ASR character accuracies", "start_pos": 10, "end_pos": 34, "type": "TASK", "confidence": 0.8391043742497762}]}, {"text": " Table 4: ASR character accuracies on the lexicon  adapted by different approaches.", "labels": [], "entities": [{"text": "ASR character accuracies", "start_pos": 10, "end_pos": 34, "type": "TASK", "confidence": 0.8155268629391988}]}, {"text": " Table 5: ASR character accuracies and SDR MAP  performances under S-CN structure.", "labels": [], "entities": [{"text": "ASR character accuracies", "start_pos": 10, "end_pos": 34, "type": "TASK", "confidence": 0.8007956941922506}, {"text": "SDR MAP", "start_pos": 39, "end_pos": 46, "type": "TASK", "confidence": 0.6225641667842865}]}, {"text": " Table 6: Average ranks of reference characters in  the confusion networks constructed by different  lexicons and corresponding language models", "labels": [], "entities": []}]}