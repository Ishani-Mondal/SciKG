{"title": [{"text": "Handling phrase reorderings for machine translation", "labels": [], "entities": [{"text": "Handling phrase reorderings", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.7323007583618164}, {"text": "machine translation", "start_pos": 32, "end_pos": 51, "type": "TASK", "confidence": 0.8042165637016296}]}], "abstractContent": [{"text": "We propose a distance phrase reordering model (DPR) for statistical machine translation (SMT), where the aim is to capture phrase reorderings using a structure learning framework.", "labels": [], "entities": [{"text": "distance phrase reordering", "start_pos": 13, "end_pos": 39, "type": "TASK", "confidence": 0.6800729235013326}, {"text": "statistical machine translation (SMT)", "start_pos": 56, "end_pos": 93, "type": "TASK", "confidence": 0.8088653832674026}]}, {"text": "On both the reordering classification and a Chinese-to-English translation task, we show improved performance over a baseline SMT system.", "labels": [], "entities": [{"text": "reordering classification", "start_pos": 12, "end_pos": 37, "type": "TASK", "confidence": 0.9017693102359772}, {"text": "Chinese-to-English translation task", "start_pos": 44, "end_pos": 79, "type": "TASK", "confidence": 0.7315931816895803}, {"text": "SMT", "start_pos": 126, "end_pos": 129, "type": "TASK", "confidence": 0.987652063369751}]}], "introductionContent": [{"text": "Word or phrase reordering is a common problem in bilingual translations arising from different grammatical structures.", "labels": [], "entities": [{"text": "Word or phrase reordering", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.5574369803071022}]}, {"text": "For example, in Chinese the expression of the date follows \"Year/Month/Date\", while when translated into English, \"Month/Date/Year\" is often the correct grammar.", "labels": [], "entities": []}, {"text": "In general, the fluency of machine translations can be greatly improved by obtaining the correct word order in the target language.", "labels": [], "entities": []}, {"text": "As the reordering problem is computationally expensive, a word distance-based reordering model is commonly used among SMT decoders, in which the costs of phrase movements are linearly proportional to the reordering distance.", "labels": [], "entities": [{"text": "SMT decoders", "start_pos": 118, "end_pos": 130, "type": "TASK", "confidence": 0.9306091368198395}]}, {"text": "Although this model is simple and efficient, the content independence makes it difficult to capture many distant phrase reordering caused by the grammar.", "labels": [], "entities": []}, {"text": "To tackle the problem, () developed a lexicalized reordering model that attempted to learn the phrase reordering based on content.", "labels": [], "entities": []}, {"text": "The model learns the local orientation (e.g. \"monotone\" order or \"switching\" order) probabilities for each bilingual phrase pair using Maximum Likelihood Estimation (MLE).", "labels": [], "entities": []}, {"text": "These orientation probabilities are then integrated into an SMT decoder to help finding a Viterbi-best local orientation sequence.", "labels": [], "entities": [{"text": "SMT", "start_pos": 60, "end_pos": 63, "type": "TASK", "confidence": 0.9632198214530945}]}, {"text": "Improvements by this * the author's new address: Xerox Research Centre Europe 6, Chemin model have been reported in ().", "labels": [], "entities": [{"text": "Xerox Research Centre Europe 6", "start_pos": 49, "end_pos": 79, "type": "DATASET", "confidence": 0.9611259698867798}]}, {"text": "However, the amount of the training data for each bilingual phrase is so small that the model usually suffers from the data sparseness problem.", "labels": [], "entities": []}, {"text": "Adopting the idea of predicting the orientation, () started exploiting the context and grammar which may relate to phrase reorderings.", "labels": [], "entities": [{"text": "predicting the orientation", "start_pos": 21, "end_pos": 47, "type": "TASK", "confidence": 0.8727826078732809}]}, {"text": "In general, a Maximum Entropy (ME) framework is utilized and the feature parameters are tuned by a discriminative model.", "labels": [], "entities": []}, {"text": "However, the training times for ME models are usually relatively high, especially when the output classes (i.e. phrase reordering orientations) increase.", "labels": [], "entities": []}, {"text": "Alternative to the ME framework, we propose using a classification scheme here for phrase reorderings and employs a structure learning framework.", "labels": [], "entities": [{"text": "phrase reorderings", "start_pos": 83, "end_pos": 101, "type": "TASK", "confidence": 0.7886048853397369}]}, {"text": "Our results confirm that this distance phrase reordering model (DPR) can lead to improved performance with a reasonable time efficiency.", "labels": [], "entities": [{"text": "distance phrase reordering", "start_pos": 30, "end_pos": 56, "type": "TASK", "confidence": 0.6301221052805582}]}], "datasetContent": [{"text": "Experiments used the Hong Kong Laws corpus 1 (Chinese-to-English), where sentences of lengths between 1 and 100 words were extracted and the ratio of source/target lengths was no more than 2 : 1.", "labels": [], "entities": [{"text": "Hong Kong Laws corpus 1", "start_pos": 21, "end_pos": 44, "type": "DATASET", "confidence": 0.9238334774971009}]}, {"text": "The training and test sizes are 50, 290 and 1, 000 respectively.", "labels": [], "entities": []}, {"text": "Target word class tag n-grams of the phrase [e i l , . .", "labels": [], "entities": []}, {"text": ", e ir ]: The environment for the feature extraction.", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 34, "end_pos": 52, "type": "TASK", "confidence": 0.7393724769353867}]}, {"text": "The word class tags are provided by MOSES.", "labels": [], "entities": [{"text": "MOSES", "start_pos": 36, "end_pos": 41, "type": "DATASET", "confidence": 0.9108157157897949}]}, {"text": "Figure 2: Classification results with respect to d.", "labels": [], "entities": [{"text": "Classification", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.9503848552703857}]}, {"text": "We used GIZA++ to produce alignments, enabling us to compare using a DPR model against a baseline lexicalized reordering model () that uses MLE orientation prediction and a discriminative model () that utilizes an ME framework.", "labels": [], "entities": [{"text": "MLE orientation prediction", "start_pos": 140, "end_pos": 166, "type": "TASK", "confidence": 0.8739340305328369}]}, {"text": "Two orientation classification tasks are carried out: one with threeclass setup and one with five-class setup.", "labels": [], "entities": [{"text": "orientation classification", "start_pos": 4, "end_pos": 30, "type": "TASK", "confidence": 0.910820335149765}]}, {"text": "We discarded points that had long distance reordering (|d| > 15) to avoid some alignment errors cause by GIZA++ (representing less than 5% of the data).", "labels": [], "entities": [{"text": "alignment errors", "start_pos": 79, "end_pos": 95, "type": "METRIC", "confidence": 0.9517922699451447}]}, {"text": "This resulted in data sizes shown in Table 3.", "labels": [], "entities": []}, {"text": "The classification performance is measured by an overall precision across all classes and the class-specific F1 measures and the experiments are are repeated three times to asses variance.", "labels": [], "entities": [{"text": "precision", "start_pos": 57, "end_pos": 66, "type": "METRIC", "confidence": 0.9903856515884399}, {"text": "F1", "start_pos": 109, "end_pos": 111, "type": "METRIC", "confidence": 0.9835981726646423}]}, {"text": "depicts the classification results obtained, where we observed consistent improvements for the DPR model over the baseline and the ME models.", "labels": [], "entities": []}, {"text": "When the number of classes (orientations) increases, the average relative improvements of DPR for the switching classes (i.e. d = 0) increase from 41.6% to 83.2% over the baseline and from 7.8% to 14.2% over the ME model, which implies a potential benefit of structure learning.", "labels": [], "entities": [{"text": "DPR", "start_pos": 90, "end_pos": 93, "type": "METRIC", "confidence": 0.9565327763557434}]}, {"text": "further demonstrate the average accuracy for each reordering distance d.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.9992701411247253}]}, {"text": "It shows that even for long distance reordering, the DPR model still performs well, while the MLE baseline usually performs badly (more than half examples are classified incorrectly).", "labels": [], "entities": []}, {"text": "With so many classification errors, the effect of this baseline in an SMT system is in doubt, even with a powerful language model.", "labels": [], "entities": [{"text": "SMT", "start_pos": 70, "end_pos": 73, "type": "TASK", "confidence": 0.9922325611114502}]}, {"text": "At training time, training a DPR model is much faster than training an ME model (both algorithms are coded in Python), especially when the number of classes increase.", "labels": [], "entities": []}, {"text": "This is because the generative iterative scaling algorithm of an ME model requires going through all examples twice at each round: one is for updating the conditional distributions p(o| \u00af f j , \u00af e i ) and the other is for updating {w o } o\u2208\u2126 . Alternatively, the PSL algorithm only goes through all examples once at each round, making it faster and more applicable for larger data sets.", "labels": [], "entities": [{"text": "generative iterative scaling", "start_pos": 20, "end_pos": 48, "type": "TASK", "confidence": 0.8671249945958456}]}, {"text": "We now test the effect of the DPR model in an MT system, using MOSES () as a baseline system.", "labels": [], "entities": [{"text": "MT", "start_pos": 46, "end_pos": 48, "type": "TASK", "confidence": 0.9657049775123596}]}, {"text": "To keep the comparison fair, our MT system just replaces MOSES's reordering models with DPR while sharing all other models (i.e. phrase translation probability model, 4-gram language model) and beam search decoder).", "labels": [], "entities": [{"text": "MT", "start_pos": 33, "end_pos": 35, "type": "TASK", "confidence": 0.9251548051834106}, {"text": "phrase translation", "start_pos": 129, "end_pos": 147, "type": "TASK", "confidence": 0.7516910135746002}]}, {"text": "As in classification experiments the three-class setup shows better results in switching classes, we use this setup in DPR.", "labels": [], "entities": []}, {"text": "In detail, all consistent phrases are extracted from the training sentence pairs and form the sample pool.", "labels": [], "entities": []}, {"text": "The three-class DPR model is then trained by the PSL algorithm and the function h(z) = exp(z) is applied to equation (1) to transform the prediction scores.", "labels": [], "entities": []}, {"text": "Contrasting the direct use of the reordering probabilities used in (), we utilize the probabilities to adjust the word distance-based reordering cost, where the reordering cost of a sentence is computed as Po (f , e) = Settings three-class setup five-class setup Classes  System three-class setup task Precision Training time (hours) Lexicalized 77.1 \u00b1 0.1 55.7 \u00b1 0.1 86.5 \u00b1 0.1 49.2 \u00b1 0.3 1.0 ME 83.7 \u00b1 0.3 67.9 \u00b1 0.3 90.8 \u00b1 0.3 69.2 \u00b1 0.1 58.6 DPR 86.7 \u00b1 0.1 73.3 \u00b1 0.1 92.5 \u00b1 0.2 74.6 \u00b1 0.5 27.0 System five-class setup task  } with tuning parameter \u03b2.", "labels": [], "entities": [{"text": "ME", "start_pos": 394, "end_pos": 396, "type": "METRIC", "confidence": 0.9687679409980774}]}, {"text": "This distance-sensitive expression is able to fill the deficiency of the three-class setup of DPR and is verified to produce better results.", "labels": [], "entities": []}, {"text": "For parameter tuning, minimum-error-rating training (F. J.) is used in both systems.", "labels": [], "entities": [{"text": "parameter tuning", "start_pos": 4, "end_pos": 20, "type": "TASK", "confidence": 0.7096902132034302}, {"text": "minimum-error-rating training (F. J.)", "start_pos": 22, "end_pos": 59, "type": "METRIC", "confidence": 0.8853366871674856}]}, {"text": "Note that there are 7 parameters needed tuning in MOSES's reordering models, while only 1 requires tuning in DPR.", "labels": [], "entities": [{"text": "MOSES", "start_pos": 50, "end_pos": 55, "type": "DATASET", "confidence": 0.774709165096283}]}, {"text": "The translation performance is evaluated by four MT measurements used in ().", "labels": [], "entities": [{"text": "translation", "start_pos": 4, "end_pos": 15, "type": "TASK", "confidence": 0.9585623145103455}, {"text": "MT", "start_pos": 49, "end_pos": 51, "type": "TASK", "confidence": 0.852708637714386}]}, {"text": "shows the translation results, where we observe consistent improvements on most evaluations.", "labels": [], "entities": [{"text": "translation", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.9558496475219727}]}, {"text": "Indeed both systems produced similar word accuracy, but our MT system does better in phrase reordering and produces more fluent translations.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.958145260810852}, {"text": "MT", "start_pos": 60, "end_pos": 62, "type": "TASK", "confidence": 0.783527672290802}, {"text": "phrase reordering", "start_pos": 85, "end_pos": 102, "type": "TASK", "confidence": 0.8524539768695831}]}], "tableCaptions": [{"text": " Table 3: Data statistics for the classification experiments.", "labels": [], "entities": []}, {"text": " Table 4: Overall precision and class-specific F1 scores [%] using different number of orientation classes.  Bold numbers refer to the best results.", "labels": [], "entities": [{"text": "precision", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.9983142614364624}, {"text": "F1 scores", "start_pos": 47, "end_pos": 56, "type": "METRIC", "confidence": 0.9740068018436432}]}]}