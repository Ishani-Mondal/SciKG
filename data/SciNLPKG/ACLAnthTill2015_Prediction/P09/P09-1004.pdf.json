{"title": [{"text": "Unsupervised Argument Identification for Semantic Role Labeling", "labels": [], "entities": [{"text": "Unsupervised Argument Identification", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.56367626786232}, {"text": "Semantic Role Labeling", "start_pos": 41, "end_pos": 63, "type": "TASK", "confidence": 0.6956969102223715}]}], "abstractContent": [{"text": "The task of Semantic Role Labeling (SRL) is often divided into two sub-tasks: verb argument identification, and argument classification.", "labels": [], "entities": [{"text": "Semantic Role Labeling (SRL)", "start_pos": 12, "end_pos": 40, "type": "TASK", "confidence": 0.8452413082122803}, {"text": "verb argument identification", "start_pos": 78, "end_pos": 106, "type": "TASK", "confidence": 0.6838792562484741}, {"text": "argument classification", "start_pos": 112, "end_pos": 135, "type": "TASK", "confidence": 0.7316345125436783}]}, {"text": "Current SRL algorithms show lower results on the identification sub-task.", "labels": [], "entities": [{"text": "SRL", "start_pos": 8, "end_pos": 11, "type": "TASK", "confidence": 0.9753069281578064}]}, {"text": "Moreover, most SRL algorithms are supervised, relying on large amounts of manually created data.", "labels": [], "entities": [{"text": "SRL", "start_pos": 15, "end_pos": 18, "type": "TASK", "confidence": 0.9922612309455872}]}, {"text": "In this paper we present an unsupervised algorithm for identifying verb arguments, where the only type of annotation required is POS tagging.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 129, "end_pos": 140, "type": "TASK", "confidence": 0.7080872356891632}]}, {"text": "The algorithm makes use of a fully unsupervised syntactic parser, using its output in order to detect clauses and gather candidate argument colloca-tion statistics.", "labels": [], "entities": []}, {"text": "We evaluate our algorithm on PropBank10, achieving a precision of 56%, as opposed to 47% of a strong base-line.", "labels": [], "entities": [{"text": "PropBank10", "start_pos": 29, "end_pos": 39, "type": "DATASET", "confidence": 0.964545726776123}, {"text": "precision", "start_pos": 53, "end_pos": 62, "type": "METRIC", "confidence": 0.9993866682052612}]}, {"text": "We also obtain an 8% increase in precision fora Spanish corpus.", "labels": [], "entities": [{"text": "precision", "start_pos": 33, "end_pos": 42, "type": "METRIC", "confidence": 0.9992700219154358}, {"text": "Spanish corpus", "start_pos": 48, "end_pos": 62, "type": "DATASET", "confidence": 0.7021951228380203}]}, {"text": "This is the first paper that tackles unsupervised verb argument identification without using manually encoded rules or extensive lexical or syntactic resources.", "labels": [], "entities": [{"text": "verb argument identification", "start_pos": 50, "end_pos": 78, "type": "TASK", "confidence": 0.6860548655192057}]}], "introductionContent": [{"text": "Semantic Role Labeling (SRL) is a major NLP task, providing a shallow sentence-level semantic analysis.", "labels": [], "entities": [{"text": "Semantic Role Labeling (SRL)", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.813274492820104}, {"text": "sentence-level semantic analysis", "start_pos": 70, "end_pos": 102, "type": "TASK", "confidence": 0.6799292465051016}]}, {"text": "SRL aims at identifying the relations between the predicates (usually, verbs) in the sentence and their associated arguments.", "labels": [], "entities": [{"text": "SRL", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.9239224791526794}]}, {"text": "The SRL task is often viewed as consisting of two parts: argument identification (ARGID) and argument classification.", "labels": [], "entities": [{"text": "SRL task", "start_pos": 4, "end_pos": 12, "type": "TASK", "confidence": 0.9321076571941376}, {"text": "argument identification (ARGID)", "start_pos": 57, "end_pos": 88, "type": "TASK", "confidence": 0.580657422542572}, {"text": "argument classification", "start_pos": 93, "end_pos": 116, "type": "TASK", "confidence": 0.7129973620176315}]}, {"text": "The former aims at identifying the arguments of a given predicate present in the sentence, while the latter determines the type of relation that holds between the identified arguments and their corresponding predicates.", "labels": [], "entities": []}, {"text": "The division into two sub-tasks is justified by the fact that they are best addressed using different feature sets ().", "labels": [], "entities": []}, {"text": "Performance in the ARGID stage is a serious bottleneck for general SRL performance, since only about 81% of the arguments are identified, while about 95% of the identified arguments are labeled correctly.", "labels": [], "entities": [{"text": "SRL", "start_pos": 67, "end_pos": 70, "type": "TASK", "confidence": 0.916538655757904}]}, {"text": "SRL is a complex task, which is reflected by the algorithms used to address it.", "labels": [], "entities": [{"text": "SRL", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.9642996788024902}]}, {"text": "A standard SRL algorithm requires thousands to dozens of thousands sentences annotated with POS tags, syntactic annotation and SRL annotation.", "labels": [], "entities": [{"text": "SRL", "start_pos": 11, "end_pos": 14, "type": "TASK", "confidence": 0.9789871573448181}]}, {"text": "Current algorithms show impressive results but only for languages and domains where plenty of annotated data is available, e.g., English newspaper texts (see Section 2).", "labels": [], "entities": []}, {"text": "Results are markedly lower when testing is on a domain wider than the training one, even in English (see the WSJ-Brown results in).", "labels": [], "entities": [{"text": "WSJ-Brown", "start_pos": 109, "end_pos": 118, "type": "DATASET", "confidence": 0.9085078239440918}]}, {"text": "Only a small number of works that do not require manually labeled SRL training data have been done).", "labels": [], "entities": [{"text": "SRL training", "start_pos": 66, "end_pos": 78, "type": "TASK", "confidence": 0.8391176164150238}]}, {"text": "These papers have replaced this data with the VerbNet () lexical resource or a set of manually written rules and supervised parsers.", "labels": [], "entities": []}, {"text": "A potential answer to the SRL training data bottleneck are unsupervised SRL models that require little to no manual effort for their training.", "labels": [], "entities": [{"text": "SRL training", "start_pos": 26, "end_pos": 38, "type": "TASK", "confidence": 0.8815710842609406}]}, {"text": "Their output can be used either by itself, or as training material for modern supervised SRL algorithms.", "labels": [], "entities": [{"text": "SRL algorithms", "start_pos": 89, "end_pos": 103, "type": "TASK", "confidence": 0.923037588596344}]}, {"text": "In this paper we present an algorithm for unsupervised argument identification.", "labels": [], "entities": [{"text": "unsupervised argument identification", "start_pos": 42, "end_pos": 78, "type": "TASK", "confidence": 0.6900633970896403}]}, {"text": "The only type of annotation required by our algorithm is POS tag-ging, which needs relatively little manual effort.", "labels": [], "entities": []}, {"text": "The algorithm consists of two stages.", "labels": [], "entities": []}, {"text": "As preprocessing, we use a fully unsupervised parser to parse each sentence.", "labels": [], "entities": []}, {"text": "Initially, the set of possible arguments fora given verb consists of all the constituents in the parse tree that do not contain that predicate.", "labels": [], "entities": []}, {"text": "The first stage of the algorithm attempts to detect the minimal clause in the sentence that contains the predicate in question.", "labels": [], "entities": []}, {"text": "Using this information, it further reduces the possible arguments only to those contained in the minimal clause, and further prunes them according to their position in the parse tree.", "labels": [], "entities": []}, {"text": "In the second stage we use pointwise mutual information to estimate the collocation strength between the arguments and the predicate, and use it to filter out instances of weakly collocating predicate argument pairs.", "labels": [], "entities": []}, {"text": "We use two measures to evaluate the performance of our algorithm, precision and F-score.", "labels": [], "entities": [{"text": "precision", "start_pos": 66, "end_pos": 75, "type": "METRIC", "confidence": 0.9997909665107727}, {"text": "F-score", "start_pos": 80, "end_pos": 87, "type": "METRIC", "confidence": 0.9978197813034058}]}, {"text": "Precision reflects the algorithm's applicability for creating training data to be used by supervised SRL models, while the standard SRL F-score measures the model's performance when used by itself.", "labels": [], "entities": [{"text": "SRL", "start_pos": 101, "end_pos": 104, "type": "TASK", "confidence": 0.9440909624099731}, {"text": "SRL", "start_pos": 132, "end_pos": 135, "type": "DATASET", "confidence": 0.5756911635398865}, {"text": "F-score", "start_pos": 136, "end_pos": 143, "type": "METRIC", "confidence": 0.8090147376060486}]}, {"text": "The first stage of our algorithm is shown to outperform a strong baseline both in terms of Fscore and of precision.", "labels": [], "entities": [{"text": "Fscore", "start_pos": 91, "end_pos": 97, "type": "METRIC", "confidence": 0.9992918968200684}, {"text": "precision", "start_pos": 105, "end_pos": 114, "type": "METRIC", "confidence": 0.9986217021942139}]}, {"text": "The second stage is shown to increase precision while maintaining a reasonable recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 38, "end_pos": 47, "type": "METRIC", "confidence": 0.9996352195739746}, {"text": "recall", "start_pos": 79, "end_pos": 85, "type": "METRIC", "confidence": 0.9985792636871338}]}, {"text": "We evaluated our model on sections 2-21 of Propbank.", "labels": [], "entities": [{"text": "Propbank", "start_pos": 43, "end_pos": 51, "type": "DATASET", "confidence": 0.9775041341781616}]}, {"text": "As is customary in unsupervised parsing work (e.g.), we bounded sentence length by 10 (excluding punctuation).", "labels": [], "entities": []}, {"text": "Our first stage obtained a precision of 52.8%, which is more than 6% improvement over the baseline.", "labels": [], "entities": [{"text": "precision", "start_pos": 27, "end_pos": 36, "type": "METRIC", "confidence": 0.9996196031570435}]}, {"text": "Our second stage improved precision to nearly 56%, a 9.3% improvement over the baseline.", "labels": [], "entities": [{"text": "precision", "start_pos": 26, "end_pos": 35, "type": "METRIC", "confidence": 0.9997090697288513}]}, {"text": "In addition, we carried out experiments on Spanish (on sentences of length bounded by 15, excluding punctuation), achieving an increase of over 7.5% in precision over the baseline.", "labels": [], "entities": [{"text": "precision", "start_pos": 152, "end_pos": 161, "type": "METRIC", "confidence": 0.9992682337760925}]}, {"text": "Our algorithm increases F-score as well, showing an 1.8% improvement over the baseline in English and a 2.2% improvement in Spanish.", "labels": [], "entities": [{"text": "F-score", "start_pos": 24, "end_pos": 31, "type": "METRIC", "confidence": 0.9978390336036682}]}, {"text": "Section 2 reviews related work.", "labels": [], "entities": []}, {"text": "In Section 3 we detail our algorithm.", "labels": [], "entities": []}, {"text": "Sections 4 and 5 describe the experimental setup and results.", "labels": [], "entities": []}], "datasetContent": [{"text": "We used the PropBank corpus for development and for evaluation on English.", "labels": [], "entities": [{"text": "PropBank corpus", "start_pos": 12, "end_pos": 27, "type": "DATASET", "confidence": 0.9712654948234558}]}, {"text": "Section 24 was used for the development of our model, and sections 2 to 21 were used as our test data.", "labels": [], "entities": []}, {"text": "The free parameters of the collocation extraction phase were tuned on the development data.", "labels": [], "entities": []}, {"text": "Following the unsupervised parsing literature, multiple brackets and brackets covering a single word are omitted.", "labels": [], "entities": []}, {"text": "We exclude punctuation according to the scheme of (.", "labels": [], "entities": []}, {"text": "As is customary in unsupervised parsing (e.g.), we bounded the lengths of the sentences in the corpus to beat most 10 (excluding punctuation).", "labels": [], "entities": []}, {"text": "This results in 207 sentences in the development data, containing a total of 132 different verbs and 173 verb instances (of the non-auxiliary verbs in the SRL task, see 'evaluation' below) having 403 arguments.", "labels": [], "entities": [{"text": "SRL task", "start_pos": 155, "end_pos": 163, "type": "TASK", "confidence": 0.8793908953666687}]}, {"text": "The test data has 6007 sentences containing 1008 different verbs and 5130 verb instances (as above) having 12436 arguments.", "labels": [], "entities": []}, {"text": "Our algorithm requires large amounts of data to gather argument structure and collocation patterns.", "labels": [], "entities": []}, {"text": "For the statistics gathering phase of the clause detection algorithm, we used 4.5M sentences of the NANC (Graff, 1995) corpus, bounding their length in the same manner.", "labels": [], "entities": [{"text": "statistics gathering", "start_pos": 8, "end_pos": 28, "type": "TASK", "confidence": 0.7072691172361374}, {"text": "clause detection", "start_pos": 42, "end_pos": 58, "type": "TASK", "confidence": 0.8574193120002747}, {"text": "NANC (Graff, 1995) corpus", "start_pos": 100, "end_pos": 125, "type": "DATASET", "confidence": 0.9374336430004665}]}, {"text": "In order to extract collocations, we used 2M sentences from the British National Corpus and about 29M sentences from the Dmoz corpus ().", "labels": [], "entities": [{"text": "British National Corpus", "start_pos": 64, "end_pos": 87, "type": "DATASET", "confidence": 0.9185274839401245}, {"text": "Dmoz corpus", "start_pos": 121, "end_pos": 132, "type": "DATASET", "confidence": 0.9362859725952148}]}, {"text": "Dmoz is a web corpus obtained by crawling and cleaning the URLs in the Open Directory Project (dmoz.org).", "labels": [], "entities": [{"text": "Dmoz", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8852724432945251}]}, {"text": "All of the above corpora were parsed using Seginer's parser and POS-tagged by MX-POST.", "labels": [], "entities": [{"text": "MX-POST", "start_pos": 78, "end_pos": 85, "type": "DATASET", "confidence": 0.901205837726593}]}, {"text": "For our experiments on Spanish, we used 3.3M sentences of length at most 15 (excluding punctuation) extracted from the Spanish Wikipedia.", "labels": [], "entities": []}, {"text": "Here we chose to bound the length by 15 due to the smaller size of the available test corpus.", "labels": [], "entities": [{"text": "length", "start_pos": 27, "end_pos": 33, "type": "METRIC", "confidence": 0.9613062143325806}]}, {"text": "The same data was used both for the first and the second stages.", "labels": [], "entities": []}, {"text": "Our development and test data were taken from the training data released for the SemEval 2007 task on semantic annotation of Spanish.", "labels": [], "entities": [{"text": "SemEval 2007 task", "start_pos": 81, "end_pos": 98, "type": "TASK", "confidence": 0.8361284136772156}, {"text": "semantic annotation of Spanish", "start_pos": 102, "end_pos": 132, "type": "TASK", "confidence": 0.7664267495274544}]}, {"text": "This data consisted of 1048 sentences of length up to 15, from which 200 were randomly selected as our development data and 848 as our test data.", "labels": [], "entities": []}, {"text": "The development data included 313 verb instances while the test data included 1279.", "labels": [], "entities": []}, {"text": "All corpora were parsed using the Seginer parser and tagged by the \"TreeTagger\".", "labels": [], "entities": []}, {"text": "Since this is the first paper, to our knowledge, which addresses the problem of unsupervised argument identification, we do not have any previous results to compare to.", "labels": [], "entities": [{"text": "argument identification", "start_pos": 93, "end_pos": 116, "type": "TASK", "confidence": 0.7118183970451355}]}, {"text": "We instead compare to a baseline which marks all k-th degree cousins of the predicate (for every k) as arguments (this is the second pruning we use in the clause detection stage).", "labels": [], "entities": [{"text": "clause detection", "start_pos": 155, "end_pos": 171, "type": "TASK", "confidence": 0.7985447645187378}]}, {"text": "We name this baseline the ALL COUSINS baseline.", "labels": [], "entities": [{"text": "ALL COUSINS baseline", "start_pos": 26, "end_pos": 46, "type": "DATASET", "confidence": 0.5971048921346664}]}, {"text": "We note that a random baseline would score very poorly since any sequence of terminals which does not contain the predicate is a possible candidate.", "labels": [], "entities": []}, {"text": "Therefore, beating this random baseline is trivial.", "labels": [], "entities": []}, {"text": "Evaluation is carried out using standard SRL evaluation software . The algorithm is provided with a list of predicates, whose arguments it needs to annotate.", "labels": [], "entities": [{"text": "SRL evaluation", "start_pos": 41, "end_pos": 55, "type": "TASK", "confidence": 0.8790412247180939}]}, {"text": "For the task addressed in this paper, non-consecutive parts of arguments are treated as full arguments.", "labels": [], "entities": []}, {"text": "A match is considered each time an argument in the gold standard data matches a marked argument in our model's output.", "labels": [], "entities": []}, {"text": "An unmatched argument is an argument which appears in the gold standard data, and fails to appear in our model's output, and an excessive argument is an argument which appears in our model's output but does not appear in the gold standard.", "labels": [], "entities": [{"text": "gold standard data", "start_pos": 58, "end_pos": 76, "type": "DATASET", "confidence": 0.7826148271560669}]}, {"text": "Precision and recall are defined accordingly.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9884421229362488}, {"text": "recall", "start_pos": 14, "end_pos": 20, "type": "METRIC", "confidence": 0.998891294002533}]}, {"text": "We report an F-score as well (the harmonic mean of precision and recall).", "labels": [], "entities": [{"text": "F-score", "start_pos": 13, "end_pos": 20, "type": "METRIC", "confidence": 0.9982390403747559}, {"text": "precision", "start_pos": 51, "end_pos": 60, "type": "METRIC", "confidence": 0.9985926747322083}, {"text": "recall", "start_pos": 65, "end_pos": 71, "type": "METRIC", "confidence": 0.9984486103057861}]}, {"text": "We do not attempt to identify multi-word verbs, and therefore do not report the model's performance in identifying verb boundaries.", "labels": [], "entities": []}, {"text": "Since our model detects clauses as an intermediate product, we provide a separate evaluation of this task for the English corpus.", "labels": [], "entities": []}, {"text": "We show results on our development data.", "labels": [], "entities": []}, {"text": "We use the standard parsing F-score evaluation measure.", "labels": [], "entities": [{"text": "parsing", "start_pos": 20, "end_pos": 27, "type": "TASK", "confidence": 0.9394752383232117}, {"text": "F-score", "start_pos": 28, "end_pos": 35, "type": "METRIC", "confidence": 0.6964414715766907}]}, {"text": "As a gold standard in this evaluation, we mark for each of the verbs in our development data the minimal clause containing it.", "labels": [], "entities": []}, {"text": "A minimal clause is the lowest ancestor of the verb in the parse tree that has a syntactic label of a clause according to the gold standard parse of the PTB.", "labels": [], "entities": [{"text": "PTB", "start_pos": 153, "end_pos": 156, "type": "DATASET", "confidence": 0.9308561086654663}]}, {"text": "A verb is any terminal marked by one of the POS tags of type verb according to the gold standard POS tags of the PTB.", "labels": [], "entities": [{"text": "PTB", "start_pos": 113, "end_pos": 116, "type": "DATASET", "confidence": 0.9738914370536804}]}], "tableCaptions": [{"text": " Table 1: Precision, Recall and F1 score for the different stages of our algorithm. Results are given for English (PTB, sentences", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.999464213848114}, {"text": "Recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9972251653671265}, {"text": "F1 score", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.9728297591209412}]}]}