{"title": [{"text": "Iterative Scaling and Coordinate Descent Methods for Maximum Entropy", "labels": [], "entities": []}], "abstractContent": [{"text": "Maximum entropy (Maxent) is useful in many areas.", "labels": [], "entities": [{"text": "Maxent)", "start_pos": 17, "end_pos": 24, "type": "METRIC", "confidence": 0.9435199499130249}]}, {"text": "Iterative scaling (IS) methods are one of the most popular approaches to solve Maxent.", "labels": [], "entities": [{"text": "Iterative scaling (IS)", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.6836211681365967}, {"text": "solve Maxent", "start_pos": 73, "end_pos": 85, "type": "TASK", "confidence": 0.6642914712429047}]}, {"text": "With many variants of IS methods, it is difficult to understand them and seethe differences.", "labels": [], "entities": []}, {"text": "In this paper, we create a general and unified framework for IS methods.", "labels": [], "entities": []}, {"text": "This framework also connects IS and coordinate descent (CD) methods.", "labels": [], "entities": []}, {"text": "Besides, we develop a CD method for Maxent.", "labels": [], "entities": [{"text": "Maxent", "start_pos": 36, "end_pos": 42, "type": "DATASET", "confidence": 0.8284593820571899}]}, {"text": "Results show that it is faster than existing iterative scaling methods 1 .", "labels": [], "entities": []}], "introductionContent": [{"text": "Maximum entropy (Maxent) is widely used in many areas such as natural language processing (NLP) and document classification.", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 62, "end_pos": 95, "type": "TASK", "confidence": 0.7654601534207662}, {"text": "document classification", "start_pos": 100, "end_pos": 123, "type": "TASK", "confidence": 0.806031733751297}]}, {"text": "Maxent models the conditional probability as: P w (y|x) \u2261 S w (x, y)/T w (x), (1) S w (x, y) \u2261 e Pt wtft , T w (x) \u2261 y S w (x, y), where x indicates a context, y is the label of the context, and w \u2208 Rn is the weight vector.", "labels": [], "entities": []}, {"text": "A function ft (x, y) denotes the t-th feature extracted from the context x and the label y.", "labels": [], "entities": []}, {"text": "Given an empirical probability distributio\u00f1distributio\u00f1 P (x, y) obtained from training samples, Maxent minimizes the following negative log-likelihood: min w \u2212 x,y \u02dc P (x, y) log P w (y|x) where\u02dcPwhere\u02dc where\u02dcP (x) = y \u02dc P (x, y) is the marginal probability of x, and\u02dcPand\u02dc and\u02dcP (f t ) = where \u03c3 is a regularization parameter.", "labels": [], "entities": []}, {"text": "We focus on (3) instead of because is strictly convex.", "labels": [], "entities": []}, {"text": "Iterative scaling (IS) methods are popular in training Maxent models.", "labels": [], "entities": [{"text": "Iterative scaling (IS)", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.680702543258667}]}, {"text": "They all share the same property of solving a one-variable sub-problem at a time.", "labels": [], "entities": []}, {"text": "Existing IS methods include generalized iterative scaling (GIS) by, improved iterative scaling (IIS) by Della, and sequential conditional generalized iterative scaling (SCGIS) by.", "labels": [], "entities": []}, {"text": "In optimization, coordinate descent (CD) is a popular method which also solves a one-variable sub-problem at a time.", "labels": [], "entities": [{"text": "coordinate descent (CD)", "start_pos": 17, "end_pos": 40, "type": "TASK", "confidence": 0.8004242360591889}]}, {"text": "With these many IS and CD methods, it is uneasy to see their differences.", "labels": [], "entities": []}, {"text": "In Section 2, we propose a unified framework to describe IS and CD methods from an optimization viewpoint.", "labels": [], "entities": []}, {"text": "Using this framework, we design a fast CD approach for Maxent in Section 3.", "labels": [], "entities": []}, {"text": "In Section 4, we compare the proposed CD method with IS and LBFGS methods.", "labels": [], "entities": []}, {"text": "Results show that the CD method is more efficient.", "labels": [], "entities": []}, {"text": "Notation n is the number of features.", "labels": [], "entities": []}, {"text": "The total number of nonzeros in samples and the average number of nonzeros per feature are respectively #nz \u2261 x,y t:ft(x,y)\ud97b\udf59 =0 1 and \u00af l \u2261 #nz/n.", "labels": [], "entities": []}], "datasetContent": [{"text": "We apply Maxent models to part of speech (POS) tagging for BROWN corpus (http://www.nltk.org) and chunking tasks for CoNLL2000 (http://www. cnts.ua.ac.be/conll2000/chunking).", "labels": [], "entities": [{"text": "part of speech (POS) tagging", "start_pos": 26, "end_pos": 54, "type": "TASK", "confidence": 0.6047685742378235}, {"text": "BROWN corpus", "start_pos": 59, "end_pos": 71, "type": "DATASET", "confidence": 0.8780175149440765}, {"text": "CoNLL2000", "start_pos": 117, "end_pos": 126, "type": "DATASET", "confidence": 0.9232931137084961}]}, {"text": "We randomly split the BROWN corpus to 4/5 training and 1/5 testing.", "labels": [], "entities": [{"text": "BROWN corpus", "start_pos": 22, "end_pos": 34, "type": "DATASET", "confidence": 0.8441178500652313}]}, {"text": "Our implementation is built upon OpenNLP (http://maxent.sourceforge.net).", "labels": [], "entities": [{"text": "OpenNLP", "start_pos": 33, "end_pos": 40, "type": "DATASET", "confidence": 0.9497934579849243}]}, {"text": "We implement CD (the new one in Section 3.2), GIS, SCGIS, and LBFGS for comparisons.", "labels": [], "entities": [{"text": "LBFGS", "start_pos": 62, "end_pos": 67, "type": "METRIC", "confidence": 0.9178045392036438}]}, {"text": "We include LBFGS as reported that it is better than other approaches including GIS  We begin at checking time versus the relative difference of the function value to the optimum: Results are in the first row of.", "labels": [], "entities": [{"text": "LBFGS", "start_pos": 11, "end_pos": 16, "type": "METRIC", "confidence": 0.8948550820350647}, {"text": "checking time", "start_pos": 96, "end_pos": 109, "type": "METRIC", "confidence": 0.9473850727081299}]}, {"text": "We check in the second row of about testing accuracy/F1 versus training time.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 44, "end_pos": 52, "type": "METRIC", "confidence": 0.9861407279968262}, {"text": "F1", "start_pos": 53, "end_pos": 55, "type": "METRIC", "confidence": 0.9982317090034485}]}, {"text": "Among the three IS/CD methods compared, the new CD approach is the fastest.", "labels": [], "entities": []}, {"text": "SCGIS comes the second, while GIS is the last.", "labels": [], "entities": [{"text": "SCGIS", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8107488751411438}, {"text": "GIS", "start_pos": 30, "end_pos": 33, "type": "DATASET", "confidence": 0.7267186045646667}]}, {"text": "This result is consistent with the tightness of their approximation functions; see.", "labels": [], "entities": []}, {"text": "LBFGS has fast final convergence, but it does not perform well in the beginning.", "labels": [], "entities": [{"text": "LBFGS", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8371686935424805}]}], "tableCaptions": []}