{"title": [{"text": "Sub-Sentence Division for Tree-Based Machine Translation", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 37, "end_pos": 56, "type": "TASK", "confidence": 0.7124277055263519}]}], "abstractContent": [{"text": "Tree-based statistical machine translation models have made significant progress in recent years, especially when replacing 1-best trees with packed forests.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 11, "end_pos": 42, "type": "TASK", "confidence": 0.6137117743492126}]}, {"text": "However, as the parsing accuracy usually goes down dramatically with the increase of sentence length, translating long sentences often takes longtime and only produces degenerate translations.", "labels": [], "entities": [{"text": "parsing", "start_pos": 16, "end_pos": 23, "type": "TASK", "confidence": 0.9565598368644714}, {"text": "accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.8676416277885437}]}, {"text": "We propose anew method named sub-sentence division that reduces the decoding time and improves the translation quality for tree-based translation.", "labels": [], "entities": [{"text": "sub-sentence division", "start_pos": 29, "end_pos": 50, "type": "TASK", "confidence": 0.7316928505897522}]}, {"text": "Our approach divides long sentences into several sub-sentences by exploiting tree structures.", "labels": [], "entities": []}, {"text": "Large-scale experiments on the NIST 2008 Chinese-to-English test set show that our approach achieves an absolute improvement of 1.1 BLEU points over the baseline system in 50% less time.", "labels": [], "entities": [{"text": "NIST 2008 Chinese-to-English test set", "start_pos": 31, "end_pos": 68, "type": "DATASET", "confidence": 0.9745999217033386}, {"text": "BLEU", "start_pos": 132, "end_pos": 136, "type": "METRIC", "confidence": 0.9941533207893372}]}], "introductionContent": [{"text": "Tree-based statistical machine translation models in days have witness promising progress in recent years, such as tree-to-string models (), tree-to-tree models.", "labels": [], "entities": [{"text": "Tree-based statistical machine translation", "start_pos": 0, "end_pos": 42, "type": "TASK", "confidence": 0.5374091044068336}]}, {"text": "Especially, when incorporated with forest, the correspondent forest-based tree-to-string models (, tree-to-tree models () have achieved a promising improvements over correspondent treebased systems.", "labels": [], "entities": []}, {"text": "However, when we translate long sentences, we argue that two major issues will be raised.", "labels": [], "entities": []}, {"text": "On one hand, parsing accuracy will be lower as the length of sentence grows.", "labels": [], "entities": [{"text": "parsing", "start_pos": 13, "end_pos": 20, "type": "TASK", "confidence": 0.9603333473205566}, {"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9797264337539673}]}, {"text": "It will inevitably hurt the translation quality; . On the other hand, decoding on long sentences will be time consuming, especially for forest approaches.", "labels": [], "entities": [{"text": "translation", "start_pos": 28, "end_pos": 39, "type": "TASK", "confidence": 0.9604843258857727}]}, {"text": "So splitting long sentences into sub- A simple way is to split long sentences by punctuations.", "labels": [], "entities": []}, {"text": "However, without concerning about the original whole tree structures, this approach will result in ill-formed sub-trees which don't respect to original structures.", "labels": [], "entities": []}, {"text": "In this paper, we present anew approach, which pays more attention to parse trees on the long sentences.", "labels": [], "entities": []}, {"text": "We firstly parse the long sentences into trees, and then divide them accordingly into sub-sentences, which will be translated independently (Section 3).", "labels": [], "entities": []}, {"text": "Finally, we combine sub translations into a full translation (Section 4).", "labels": [], "entities": []}, {"text": "Large-scale experiments (Section 5) show that the BLEU score achieved by our approach is 1.1 higher than direct decoding and 0.3 higher than always splitting on commas on the 2008 NIST MT ChineseEnglish test set.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 50, "end_pos": 60, "type": "METRIC", "confidence": 0.9796452522277832}, {"text": "NIST MT ChineseEnglish test set", "start_pos": 180, "end_pos": 211, "type": "DATASET", "confidence": 0.8874714374542236}]}, {"text": "Moreover, our approach has reduced decoding time significantly.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2. BLEU results (case sensitive)", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9988006353378296}]}, {"text": " Table 3. Decoding time of our experiments", "labels": [], "entities": []}]}