{"title": [{"text": "Better Word Alignments with Supervised ITG Models", "labels": [], "entities": [{"text": "Word Alignments", "start_pos": 7, "end_pos": 22, "type": "TASK", "confidence": 0.6910448372364044}]}], "abstractContent": [{"text": "This work investigates supervised word alignment methods that exploit inversion transduc-tion grammar (ITG) constraints.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 34, "end_pos": 48, "type": "TASK", "confidence": 0.7133148610591888}]}, {"text": "We consider maximum margin and conditional likelihood objectives, including the presentation of anew normal form grammar for canoni-calizing derivations.", "labels": [], "entities": []}, {"text": "Even for non-ITG sentence pairs, we show that it is possible learn ITG alignment models by simple relaxations of structured discriminative learning objectives.", "labels": [], "entities": [{"text": "ITG alignment", "start_pos": 67, "end_pos": 80, "type": "TASK", "confidence": 0.723444253206253}]}, {"text": "For efficiency, we describe a set of pruning techniques that together allow us to align sentences two orders of magnitude faster than naive bitext CKY parsing.", "labels": [], "entities": [{"text": "CKY parsing", "start_pos": 147, "end_pos": 158, "type": "TASK", "confidence": 0.6060642749071121}]}, {"text": "Finally, we introduce many-to-one block alignment features, which significantly improve our ITG models.", "labels": [], "entities": [{"text": "block alignment", "start_pos": 34, "end_pos": 49, "type": "TASK", "confidence": 0.7188348174095154}]}, {"text": "Altogether, our method results in the best reported AER numbers for Chinese-English and a performance improvement of 1.1 BLEU over GIZA++ alignments.", "labels": [], "entities": [{"text": "AER", "start_pos": 52, "end_pos": 55, "type": "METRIC", "confidence": 0.9991652965545654}, {"text": "BLEU", "start_pos": 121, "end_pos": 125, "type": "METRIC", "confidence": 0.9969184398651123}]}], "introductionContent": [{"text": "Inversion transduction grammar (ITG) constraints (Wu, 1997) provide coherent structural constraints on the relationship between a sentence and its translation.", "labels": [], "entities": [{"text": "Inversion transduction grammar (ITG)", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.8120873073736826}]}, {"text": "ITG has been extensively explored in unsupervised statistical word alignment () and machine translation decoding.", "labels": [], "entities": [{"text": "statistical word alignment", "start_pos": 50, "end_pos": 76, "type": "TASK", "confidence": 0.6084284087022146}, {"text": "machine translation decoding", "start_pos": 84, "end_pos": 112, "type": "TASK", "confidence": 0.8460303942362467}]}, {"text": "In this work, we investigate large-scale, discriminative ITG word alignment.", "labels": [], "entities": [{"text": "ITG word alignment", "start_pos": 57, "end_pos": 75, "type": "TASK", "confidence": 0.6680342753728231}]}, {"text": "Past work on discriminative word alignment has focused on the family of at-most-one-to-one matchings).", "labels": [], "entities": [{"text": "discriminative word alignment", "start_pos": 13, "end_pos": 42, "type": "TASK", "confidence": 0.6142930686473846}]}, {"text": "An exception to this is the work of, who discriminatively trained one-to-one ITG models, albeit with limited feature sets.", "labels": [], "entities": []}, {"text": "As they found, ITG approaches offer several advantages over general matchings.", "labels": [], "entities": []}, {"text": "First, the additional structural constraint can result in superior alignments.", "labels": [], "entities": []}, {"text": "We confirm and extend this result, showing that one-toone ITG models can perform as well as, or better than, general one-to-one matching models, either using heuristic weights or using rich, learned features.", "labels": [], "entities": []}, {"text": "A second advantage of ITG approaches is that they admit a range of training options.", "labels": [], "entities": []}, {"text": "As with general one-to-one matchings, we can optimize margin-based objectives.", "labels": [], "entities": []}, {"text": "However, unlike with general matchings, we can also efficiently compute expectations over the set of ITG derivations, enabling the training of conditional likelihood models.", "labels": [], "entities": []}, {"text": "A major challenge in both cases is that our training alignments are often not one-to-one ITG alignments.", "labels": [], "entities": []}, {"text": "Under such conditions, directly training to maximize margin is unstable, and training to maximize likelihood is ill-defined, since the target alignment derivations don't exist in our hypothesis class.", "labels": [], "entities": [{"text": "margin", "start_pos": 53, "end_pos": 59, "type": "METRIC", "confidence": 0.8341052532196045}, {"text": "likelihood", "start_pos": 98, "end_pos": 108, "type": "METRIC", "confidence": 0.8072342276573181}]}, {"text": "We show how to adapt both margin and likelihood objectives to learn good ITG aligners.", "labels": [], "entities": [{"text": "likelihood", "start_pos": 37, "end_pos": 47, "type": "METRIC", "confidence": 0.931978702545166}]}, {"text": "In the case of likelihood training, two innovations are presented.", "labels": [], "entities": [{"text": "likelihood training", "start_pos": 15, "end_pos": 34, "type": "TASK", "confidence": 0.9458511769771576}]}, {"text": "The simple, two-rule ITG grammar exponentially over-counts certain alignment structures relative to others.", "labels": [], "entities": []}, {"text": "Because of this, and introduced a normal form ITG which avoids this over-counting.", "labels": [], "entities": []}, {"text": "We extend this normal form to null productions and give the first extensive empirical comparison of simple and normal form ITGs, for posterior decoding under our likelihood models.", "labels": [], "entities": []}, {"text": "Additionally, we show how to deal with training instances where the gold alignments are outside of the hypothesis class by instead optimizing the likelihood of a set of minimum-loss alignments.", "labels": [], "entities": []}, {"text": "Perhaps the greatest advantage of ITG models is that they straightforwardly permit block-structured alignments (i.e. phrases), which general matchings cannot efficiently do.", "labels": [], "entities": []}, {"text": "The need for block alignments is especially acute in ChineseEnglish data, where oracle AERs drop from 10.2 without blocks to around 1.2 with them.", "labels": [], "entities": [{"text": "ChineseEnglish data", "start_pos": 53, "end_pos": 72, "type": "DATASET", "confidence": 0.9633737206459045}, {"text": "AERs", "start_pos": 87, "end_pos": 91, "type": "METRIC", "confidence": 0.9717815518379211}]}, {"text": "Indeed, blocks are the primary reason for gold alignments being outside the space of one-to-one ITG alignments.", "labels": [], "entities": []}, {"text": "We show that placing linear potential functions on many-to-one blocks can substantially improve performance.", "labels": [], "entities": []}, {"text": "Finally, to scale up our system, we give a combination of pruning techniques that allows us to sum ITG alignments two orders of magnitude faster than naive inside-outside parsing.", "labels": [], "entities": [{"text": "ITG alignments", "start_pos": 99, "end_pos": 113, "type": "TASK", "confidence": 0.7172972857952118}]}, {"text": "All in all, our discriminatively trained, block ITG models produce alignments which exhibit the best AER on the NIST 2002 Chinese-English alignment data set.", "labels": [], "entities": [{"text": "AER", "start_pos": 101, "end_pos": 104, "type": "METRIC", "confidence": 0.999444305896759}, {"text": "NIST 2002 Chinese-English alignment data set", "start_pos": 112, "end_pos": 156, "type": "DATASET", "confidence": 0.9765032033125559}]}, {"text": "Furthermore, they result in a 1.1 BLEU-point improvement over GIZA++ alignments in an end-to-end Hiero (Chiang, 2007) machine translation system.", "labels": [], "entities": [{"text": "BLEU-point", "start_pos": 34, "end_pos": 44, "type": "METRIC", "confidence": 0.9994398951530457}, {"text": "Hiero (Chiang, 2007) machine translation", "start_pos": 97, "end_pos": 137, "type": "TASK", "confidence": 0.8217321559786797}]}], "datasetContent": [{"text": "We present results which measure the quality of our models on two hand-aligned data sets.", "labels": [], "entities": []}, {"text": "Our first is the English-French Hansards data set from the 2003 NAACL shared task: Word alignment results on Chinese-English.", "labels": [], "entities": [{"text": "Hansards data set from the 2003 NAACL shared task", "start_pos": 32, "end_pos": 81, "type": "DATASET", "confidence": 0.8779336081610786}, {"text": "Word alignment", "start_pos": 83, "end_pos": 97, "type": "TASK", "confidence": 0.7061846256256104}]}, {"text": "Each column is a learning objective paired with an alignment family.", "labels": [], "entities": []}, {"text": "The first row represents our best model without external alignment models and the second row includes features from the jointly trained HMM.", "labels": [], "entities": []}, {"text": "Under likelihood, BITG-S uses the simple grammar (Section 2.2).", "labels": [], "entities": [{"text": "BITG-S", "start_pos": 18, "end_pos": 24, "type": "METRIC", "confidence": 0.5134634971618652}]}, {"text": "BITG-N uses the normal form grammar (Section 4.1).", "labels": [], "entities": [{"text": "BITG-N", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.8928039073944092}]}, {"text": "(2005); we compute external features from the same unlabeled data, 1.1 million sentence pairs.", "labels": [], "entities": []}, {"text": "Our second is the Chinese-English hand-aligned portion of the 2002 NIST MT evaluation set.", "labels": [], "entities": [{"text": "2002 NIST MT evaluation set", "start_pos": 62, "end_pos": 89, "type": "DATASET", "confidence": 0.7722576856613159}]}, {"text": "This dataset has 491 sentences, which we split into a training set of 150 and a test set of 191.", "labels": [], "entities": []}, {"text": "When we trained external Chinese models, we used the same unlabeled data set as, including the bilingual dictionary.", "labels": [], "entities": []}, {"text": "For likelihood based models, we set the L2 regularization parameter, \u03c3 2 , to 100 and the threshold for posterior decoding to 0.33.", "labels": [], "entities": []}, {"text": "We report results using the simple ITG grammar (ITG-S, Section 2.2) where summing over derivations double counts alignments, as well as the normal form ITG grammar (ITG-N,Section 4.1) which does not double count.", "labels": [], "entities": []}, {"text": "We ran our annealed lossaugmented MIRA for 15 iterations, beginning with \u03bb at 0 and increasing it linearly to 0.5.", "labels": [], "entities": [{"text": "MIRA", "start_pos": 34, "end_pos": 38, "type": "METRIC", "confidence": 0.619463324546814}]}, {"text": "We compute Viterbi alignments using the averaged weight vector from this procedure.", "labels": [], "entities": []}, {"text": "We further evaluated our alignments in an end-toend Chinese to English translation task using the publicly available hierarchical pipeline JosHUa (.", "labels": [], "entities": [{"text": "end-toend Chinese to English translation task", "start_pos": 42, "end_pos": 87, "type": "TASK", "confidence": 0.6805579562981924}]}, {"text": "The pipeline extracts a Hiero-style synchronous context-free grammar, employs suffix-array based rule extraction, and tunes model parameters with minimum error rate training.", "labels": [], "entities": [{"text": "rule extraction", "start_pos": 97, "end_pos": 112, "type": "TASK", "confidence": 0.7582961320877075}]}, {"text": "We trained on the FBIS corpus using sentences up to length 40, which includes 2.7 million English words.", "labels": [], "entities": [{"text": "FBIS corpus", "start_pos": 18, "end_pos": 29, "type": "DATASET", "confidence": 0.9145784080028534}]}, {"text": "We used a 5-gram language model trained on 126 million words of the Xinhua section of the English Gigaword corpus, estimated with SRILM).", "labels": [], "entities": [{"text": "Xinhua section of the English Gigaword corpus", "start_pos": 68, "end_pos": 113, "type": "DATASET", "confidence": 0.7604525515011379}, {"text": "SRILM", "start_pos": 130, "end_pos": 135, "type": "METRIC", "confidence": 0.6828206181526184}]}, {"text": "We tuned on 300 sentences of the NIST MT04 test set.", "labels": [], "entities": [{"text": "NIST MT04 test set", "start_pos": 33, "end_pos": 51, "type": "DATASET", "confidence": 0.9429446011781693}]}, {"text": "Results on the NIST MT05 test set appear in.", "labels": [], "entities": [{"text": "NIST MT05 test set", "start_pos": 15, "end_pos": 33, "type": "DATASET", "confidence": 0.9128769785165787}]}, {"text": "We compared four sets of alignments.", "labels": [], "entities": []}, {"text": "The GIZA++ alignments 7 are combined across directions with the grow-diag-final heuristic, which outperformed the union.", "labels": [], "entities": []}, {"text": "The joint HMM alignments are generated from competitive posterior", "labels": [], "entities": [{"text": "HMM alignments", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.8413173258304596}]}], "tableCaptions": [{"text": " Table 1: Results on the French Hansards dataset. Columns indicate models and training methods. The rows  indicate the feature sets used. ITG-S uses the simple grammar (Section 2.2). ITG-N uses the normal form grammar  (Section 4.1). For MIRA (Viterbi inference), the highest-scoring alignment is the same, regardless of grammar.", "labels": [], "entities": [{"text": "French Hansards dataset", "start_pos": 25, "end_pos": 48, "type": "DATASET", "confidence": 0.9136910239855448}, {"text": "ITG-S", "start_pos": 138, "end_pos": 143, "type": "DATASET", "confidence": 0.9086580872535706}, {"text": "MIRA", "start_pos": 238, "end_pos": 242, "type": "METRIC", "confidence": 0.7006762027740479}]}, {"text": " Table 2: Word alignment results on Chinese-English. Each column is a learning objective paired with an alignment  family. The first row represents our best model without external alignment models and the second row includes  features from the jointly trained HMM. Under likelihood, BITG-S uses the simple grammar (Section 2.2). BITG-N  uses the normal form grammar (Section 4.1).", "labels": [], "entities": [{"text": "Word alignment", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.7655056118965149}, {"text": "BITG-S", "start_pos": 283, "end_pos": 289, "type": "METRIC", "confidence": 0.8125725388526917}, {"text": "BITG-N", "start_pos": 329, "end_pos": 335, "type": "DATASET", "confidence": 0.5702733993530273}]}]}