{"title": [{"text": "Hidden Markov Tree Model in Dependency-based Machine Translation *", "labels": [], "entities": [{"text": "Dependency-based Machine Translation", "start_pos": 28, "end_pos": 64, "type": "TASK", "confidence": 0.5877345601717631}]}], "abstractContent": [{"text": "We would like to draw attention to Hidden Markov Tree Models (HMTM), which are to our knowledge still unexploited in the field of Computational Linguistics, in spite of highly successful Hidden Markov (Chain) Models.", "labels": [], "entities": []}, {"text": "In dependency trees, the independence assumptions made by HMTM correspond to the intuition of linguistic dependency.", "labels": [], "entities": []}, {"text": "Therefore we suggest to use HMTM and tree-modified Viterbi algorithm for tasks interpretable as labeling nodes of dependency trees.", "labels": [], "entities": []}, {"text": "In particular , we show that the transfer phase in a Machine Translation system based on tectogrammatical dependency trees can be seen as a task suitable for HMTM.", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 53, "end_pos": 72, "type": "TASK", "confidence": 0.7688556909561157}]}, {"text": "When using the HMTM approach for the English-Czech translation, we reach a moderate improvement over the baseline.", "labels": [], "entities": []}], "introductionContent": [{"text": "Hidden Markov Tree Models (HMTM) were introduced in) and used in applications such as image segmentation, signal classification, denoising, and image document categorization, see) for references.", "labels": [], "entities": [{"text": "image segmentation", "start_pos": 86, "end_pos": 104, "type": "TASK", "confidence": 0.7478451430797577}, {"text": "signal classification", "start_pos": 106, "end_pos": 127, "type": "TASK", "confidence": 0.7423176169395447}]}, {"text": "Although Hidden Markov Models belong to the most successful techniques in Computational Linguistics (CL), the HMTM modification remains to the best of our knowledge unknown in the field.", "labels": [], "entities": []}, {"text": "The first novel claim made in this paper is that the independence assumptions made by Markov Tree Models can be useful for modeling syntactic trees.", "labels": [], "entities": []}, {"text": "Especially, they fit dependency trees well, because these models assume conditional dependence (in the probabilistic sense) only along tree edges, which corresponds to intuition behind dependency relations (in the linguistic sense) in dependency trees.", "labels": [], "entities": []}, {"text": "Moreover, analogously to applications of HMM on sequence labeling, HMTM can be used for labeling nodes of a dependency tree, interpreted as revealing the hidden states 1 in the tree nodes, given another (observable) labeling of the nodes of the same tree.", "labels": [], "entities": []}, {"text": "The second novel claim is that HMTMs are suitable for modeling the transfer phase in Machine Translation systems based on deep-syntactic dependency trees.", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 85, "end_pos": 104, "type": "TASK", "confidence": 0.8365998268127441}]}, {"text": "Emission probabilities represent the translation model, whereas transition (edge) probabilities represent the target-language tree model.", "labels": [], "entities": []}, {"text": "This decomposition can be seen as a tree-shaped analogy to the popular n-gram approaches to Statistical Machine Translation (e.g. (), in which translation and language models are trainable separately too.", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 92, "end_pos": 123, "type": "TASK", "confidence": 0.7988702058792114}]}, {"text": "Moreover, given the input dependency tree and HMTM parameters, there is a computationally efficient HMTM-modified Viterbi algorithm for finding the globally optimal target dependency tree.", "labels": [], "entities": []}, {"text": "It should be noted that when using HMTM, the source-language and target-language trees are required to be isomorphic.", "labels": [], "entities": []}, {"text": "Obviously, this is an unrealistic assumption in real translation.", "labels": [], "entities": []}, {"text": "However, we argue that tectogrammatical deep-syntactic dependency trees (as introduced in the Functional Generative Description framework,) are relatively close to this requirement, which makes the HMTM approach practically testable.", "labels": [], "entities": []}, {"text": "As for the related work, one can found a number of experiments with dependency-based MT in the literature, e.g.,),,.", "labels": [], "entities": [{"text": "MT", "start_pos": 85, "end_pos": 87, "type": "TASK", "confidence": 0.7734245657920837}]}, {"text": "However, to our knowledge none of the published systems searches for the optimal target representa-", "labels": [], "entities": []}], "datasetContent": [{"text": "To check the real applicability of HMTM transfer, we performed the following preliminary MT experiment.", "labels": [], "entities": [{"text": "HMTM transfer", "start_pos": 35, "end_pos": 48, "type": "TASK", "confidence": 0.8462027609348297}, {"text": "MT", "start_pos": 89, "end_pos": 91, "type": "TASK", "confidence": 0.9584758877754211}]}, {"text": "First, we used the tectogrammar-based MT system described in ( \u02c7 Zabokrtsk\u00b4yZabokrtsk\u00b4y et al., 2008) as a baseline.", "labels": [], "entities": [{"text": "MT", "start_pos": 38, "end_pos": 40, "type": "TASK", "confidence": 0.9058108329772949}]}, {"text": "Then we substituted its transfer phase by the HMTM variant, with parameters estimated from 800 million word Czech corpus and 60 million word parallel corpus.", "labels": [], "entities": []}, {"text": "As shown in, the HMTM approach outperforms the baseline solution both in terms of BLEU and NIST metrics.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 82, "end_pos": 86, "type": "METRIC", "confidence": 0.9982195496559143}]}], "tableCaptions": [{"text": " Table 1: Evaluation of English-Czech translation.", "labels": [], "entities": [{"text": "English-Czech translation", "start_pos": 24, "end_pos": 49, "type": "TASK", "confidence": 0.6119160950183868}]}]}