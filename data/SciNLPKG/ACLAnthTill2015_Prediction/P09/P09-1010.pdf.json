{"title": [{"text": "Reinforcement Learning for Mapping Instructions to Actions", "labels": [], "entities": [{"text": "Reinforcement Learning", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8851518034934998}, {"text": "Mapping Instructions to Actions", "start_pos": 27, "end_pos": 58, "type": "TASK", "confidence": 0.9089989364147186}]}], "abstractContent": [{"text": "In this paper, we present a reinforcement learning approach for mapping natural language instructions to sequences of executable actions.", "labels": [], "entities": [{"text": "mapping natural language instructions to sequences of executable actions", "start_pos": 64, "end_pos": 136, "type": "TASK", "confidence": 0.6939099000559913}]}, {"text": "We assume access to a reward function that defines the quality of the executed actions.", "labels": [], "entities": []}, {"text": "During training , the learner repeatedly constructs action sequences fora set of documents, executes those actions, and observes the resulting reward.", "labels": [], "entities": []}, {"text": "We use a policy gradient algorithm to estimate the parameters of a log-linear model for action selection.", "labels": [], "entities": [{"text": "action selection", "start_pos": 88, "end_pos": 104, "type": "TASK", "confidence": 0.6942863464355469}]}, {"text": "We apply our method to interpret instructions in two domains-Windows troubleshoot-ing guides and game tutorials.", "labels": [], "entities": []}, {"text": "Our results demonstrate that this method can rival supervised learning techniques while requiring few or no annotated training examples.", "labels": [], "entities": []}], "introductionContent": [{"text": "The problem of interpreting instructions written in natural language has been widely studied since the early days of artificial intelligence.", "labels": [], "entities": [{"text": "interpreting instructions written in natural language", "start_pos": 15, "end_pos": 68, "type": "TASK", "confidence": 0.8945893943309784}]}, {"text": "Mapping instructions to a sequence of executable actions would enable the automation of tasks that currently require human participation.", "labels": [], "entities": []}, {"text": "Examples include configuring software based on how-to guides and operating simulators using instruction manuals.", "labels": [], "entities": []}, {"text": "In this paper, we present a reinforcement learning framework for inducing mappings from text to actions without the need for annotated training examples.", "labels": [], "entities": []}, {"text": "For concreteness, consider instructions from a Windows troubleshooting guide on deleting temporary folders, shown in.", "labels": [], "entities": []}, {"text": "We aim to map this text to the corresponding low-level commands and parameters.", "labels": [], "entities": []}, {"text": "For example, properly interpreting the third instruction requires clicking on a tab, finding the appropriate option in a tree control, and clearing its associated checkbox.", "labels": [], "entities": [{"text": "interpreting the third instruction", "start_pos": 22, "end_pos": 56, "type": "TASK", "confidence": 0.8459409922361374}]}, {"text": "In this and many other applications, the validity of a mapping can be verified by executing the induced actions in the corresponding environment and observing their effects.", "labels": [], "entities": []}, {"text": "For instance, in the example above we can assess whether the goal described in the instructions is achieved, i.e., the folder is deleted.", "labels": [], "entities": []}, {"text": "The key idea of our approach is to leverage the validation process as the main source of supervision to guide learning.", "labels": [], "entities": []}, {"text": "This form of supervision allows us to learn interpretations of natural language instructions when standard supervised techniques are not applicable, due to the lack of human-created annotations.", "labels": [], "entities": []}, {"text": "Reinforcement learning is a natural framework for building models using validation from an environment ().", "labels": [], "entities": [{"text": "Reinforcement learning", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.898505300283432}]}, {"text": "We assume that supervision is provided in the form of a reward function that defines the quality of executed actions.", "labels": [], "entities": []}, {"text": "During training, the learner repeatedly constructs action sequences fora set of given documents, executes those actions, and observes the resulting reward.", "labels": [], "entities": []}, {"text": "The learner's goal is to estimate a policy -a distribution over actions given instruction text and environment state -that maximizes future expected reward.", "labels": [], "entities": []}, {"text": "Our policy is modeled in a log-linear fashion, allowing us to incorporate features of both the instruction text and the environment.", "labels": [], "entities": []}, {"text": "We employ a policy gradient algorithm to estimate the parameters of this model.", "labels": [], "entities": []}, {"text": "We evaluate our method on two distinct applications: Windows troubleshooting guides and puzzle game tutorials.", "labels": [], "entities": []}, {"text": "The key findings of our experiments are twofold.", "labels": [], "entities": []}, {"text": "First, models trained only with simple reward signals achieve surprisingly high results, coming within 11% of a fully supervised method in the Windows domain.", "labels": [], "entities": []}, {"text": "Second, augmenting unlabeled documents with even a small fraction of annotated examples greatly reduces this performance gap, to within 4% in that domain.", "labels": [], "entities": []}, {"text": "These results indicate the power of learning from this new form of automated supervision.", "labels": [], "entities": []}], "datasetContent": [{"text": "Datasets For the Windows domain, our dataset consists of 128 documents, divided into 70 for training, 18 for development, and 40 for test.", "labels": [], "entities": []}, {"text": "In the puzzle game domain, we use 50 tutorials, divided into 40 for training and 10 for test.", "labels": [], "entities": []}, {"text": "Statistics for the datasets are shown below.", "labels": [], "entities": []}, {"text": "The data exhibits certain qualities that make fora challenging learning problem.", "labels": [], "entities": []}, {"text": "For instance, there area surprising variety of linguistic constructs -as shows, in the Windows domain even a simple command is expressed in at least six different ways.", "labels": [], "entities": []}, {"text": "Experimental Framework To apply our algorithm to the Windows domain, we use the Win32 application programming interface to simulate human interactions with the user interface, and to gather environment state information.", "labels": [], "entities": []}, {"text": "The operating system environment is hosted within a virtual machine, 10 allowing us to rapidly save and reset system state snapshots.", "labels": [], "entities": []}, {"text": "For the puzzle game domain, we replicated the game with an implementation that facilitates automatic play.", "labels": [], "entities": []}, {"text": "As is commonly done in reinforcement learning, we use a softmax temperature parameter to smooth the policy distribution, set to 0.1 in our experiments.", "labels": [], "entities": [{"text": "reinforcement learning", "start_pos": 23, "end_pos": 45, "type": "TASK", "confidence": 0.9031306207180023}]}, {"text": "For Windows, the development set is used to select the best parameters.", "labels": [], "entities": []}, {"text": "For Crossblock, we choose the parameters that produce the highest reward during training.", "labels": [], "entities": []}, {"text": "During evaluation, we use these parameters to predict mappings for the test documents.", "labels": [], "entities": []}, {"text": "Evaluation Metrics For evaluation, we compare the results to manually constructed sequences of actions.", "labels": [], "entities": []}, {"text": "We measure the number of correct actions, sentences, and documents.", "labels": [], "entities": []}, {"text": "An action is correct if it matches the annotations in terms of command and parameters.", "labels": [], "entities": []}, {"text": "A sentence is correct if all of its actions are correctly identified, and analogously for documents.", "labels": [], "entities": []}, {"text": "11 Statistical significance is measured with the sign test.", "labels": [], "entities": [{"text": "Statistical significance", "start_pos": 3, "end_pos": 27, "type": "METRIC", "confidence": 0.799092561006546}]}, {"text": "Additionally, we compute a word alignment score to investigate the extent to which the input text is used to construct correct analyses.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 27, "end_pos": 41, "type": "TASK", "confidence": 0.6871027052402496}]}, {"text": "This score measures the percentage of words that are aligned to the corresponding annotated actions in correctly analyzed documents.", "labels": [], "entities": []}, {"text": "Baselines We consider the following baselines to characterize the performance of our approach.", "labels": [], "entities": []}, {"text": "VMware Workstation, available at www.vmware.com In these tasks, each action depends on the correct execution of all previous actions, so a single error can render the remainder of that document's mapping incorrect.", "labels": [], "entities": [{"text": "VMware Workstation", "start_pos": 0, "end_pos": 18, "type": "DATASET", "confidence": 0.9018310904502869}]}, {"text": "In addition, due to variability in document lengths, overall action accuracy is not guaranteed to be higher than document accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.9748234152793884}, {"text": "accuracy", "start_pos": 122, "end_pos": 130, "type": "METRIC", "confidence": 0.8752669095993042}]}, {"text": "\u2022 Full Supervision Sequence prediction problems like ours are typically addressed using supervised techniques.", "labels": [], "entities": [{"text": "Full Supervision Sequence prediction", "start_pos": 2, "end_pos": 38, "type": "TASK", "confidence": 0.8250901252031326}]}, {"text": "We measure how a standard supervised approach would perform on this task by using a reward signal based on manual annotations of output action sequences, as defined in Section 5.2.", "labels": [], "entities": []}, {"text": "As shown there, policy gradient with this reward is equivalent to stochastic gradient ascent with a maximum likelihood objective.", "labels": [], "entities": []}, {"text": "\u2022 Partial Supervision We consider the case when only a subset of training documents is annotated, and environment reward is used for the remainder.", "labels": [], "entities": [{"text": "Partial Supervision", "start_pos": 2, "end_pos": 21, "type": "TASK", "confidence": 0.8545216023921967}]}, {"text": "Our method seamlessly combines these two kinds of rewards.", "labels": [], "entities": []}, {"text": "\u2022 Random and Majority (Windows) We consider two na\u00a8\u0131vena\u00a8\u0131ve baselines.", "labels": [], "entities": [{"text": "Random", "start_pos": 2, "end_pos": 8, "type": "METRIC", "confidence": 0.9527730345726013}]}, {"text": "Both scan through each sentence from left to right.", "labels": [], "entities": []}, {"text": "A command c is executed on the object whose name is encountered first in the sentence.", "labels": [], "entities": []}, {"text": "This command c is either selected randomly, or set to the majority command, which is leftclick.", "labels": [], "entities": []}, {"text": "This procedure is repeated until no more words match environment objects.", "labels": [], "entities": []}, {"text": "\u2022 Random (Puzzle) We consider a baseline that randomly selects among the actions that are valid in the current game state.", "labels": [], "entities": []}, {"text": "There are several indicators of the difficulty of this task.", "labels": [], "entities": []}, {"text": "The random and majority baselines' poor performance in both domains indicates that na\u00a8\u0131vena\u00a8\u0131ve approaches are inadequate for these tasks.", "labels": [], "entities": []}, {"text": "The performance of the fully supervised approach provides further evidence that the task is challenging.", "labels": [], "entities": []}, {"text": "This difficulty can be attributed in part to the large branching factor of possible actions at each stepon average, there are 27.14 choices per action in the Windows domain, and 9.78 in the Crossblock domain.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Performance on the test set with different reward signals and baselines. Our evaluation measures  the proportion of correct actions, sentences, and documents. We also report the percentage of correct  word alignments for the successfully completed documents. Note the puzzle domain has only single- sentence documents, so its sentence and document scores are identical. The partial supervision line  refers to 20 out of 70 annotated training documents for Windows, and 10 out of 40 for the puzzle. Each  result marked with  *  or is a statistically significant improvement over the result immediately above it;   *  indicates p < 0.01 and indicates p < 0.05.", "labels": [], "entities": []}]}