{"title": [{"text": "Optimizing Language Model Information Retrieval System with Expectation Maximization Algorithm", "labels": [], "entities": [{"text": "Optimizing Language Model Information Retrieval", "start_pos": 0, "end_pos": 47, "type": "TASK", "confidence": 0.6765745460987092}, {"text": "Expectation Maximization Algorithm", "start_pos": 60, "end_pos": 94, "type": "TASK", "confidence": 0.7560553550720215}]}], "abstractContent": [{"text": "Statistical language modeling (SLM) has been used in many different domains for decades and has also been applied to information retrieval (IR) recently.", "labels": [], "entities": [{"text": "Statistical language modeling (SLM)", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.8742944995562235}, {"text": "information retrieval (IR)", "start_pos": 117, "end_pos": 143, "type": "TASK", "confidence": 0.8572821199893952}]}, {"text": "Documents retrieved using this approach are ranked according their probability of generating the given query.", "labels": [], "entities": []}, {"text": "In this paper, we present a novel approach that employs the generalized Expectation Maximization (EM) algorithm to improve language models by representing their parameters as observation probabilities of Hidden Markov Models (HMM).", "labels": [], "entities": [{"text": "generalized Expectation Maximization (EM)", "start_pos": 60, "end_pos": 101, "type": "TASK", "confidence": 0.7134973307450613}]}, {"text": "In the experiments , we demonstrate that our method out-performs standard SLM-based and tf.idf-based methods on TREC 2005 HARD Track data.", "labels": [], "entities": [{"text": "TREC 2005 HARD Track data", "start_pos": 112, "end_pos": 137, "type": "DATASET", "confidence": 0.9437235713005065}]}], "introductionContent": [{"text": "In 1945, soon after the computer was invented, Vannevar Bush wrote a famous article---\"As we may think\" (V., which formed the basis of research into Information Retrieval (IR).", "labels": [], "entities": [{"text": "Information Retrieval (IR)", "start_pos": 149, "end_pos": 175, "type": "TASK", "confidence": 0.8771175980567932}]}, {"text": "The pioneers in IR developed two models for ranking: the vector space model (G. Salton and M. J.) and the probabilistic model (S. E. Robertson and S..", "labels": [], "entities": [{"text": "IR", "start_pos": 16, "end_pos": 18, "type": "TASK", "confidence": 0.9926175475120544}]}, {"text": "Since then, the research of classical probabilistic models of relevance has been widely studied.", "labels": [], "entities": []}, {"text": "For example, Robertson (S. E. Robertson and S. modeled word occurrences into relevant or non-relevant classes, and ranked documents according to the probabilities they belong to the relevant one.", "labels": [], "entities": []}, {"text": "In 1998, proposed a language modeling framework which opens anew point of view in IR.", "labels": [], "entities": [{"text": "IR", "start_pos": 82, "end_pos": 84, "type": "TASK", "confidence": 0.9782310128211975}]}, {"text": "In this approach, they gave up the model of relevance; instead, they treated query generation as random sampling from every document model.", "labels": [], "entities": [{"text": "query generation", "start_pos": 77, "end_pos": 93, "type": "TASK", "confidence": 0.7425595819950104}]}, {"text": "The retrieval results were based on the probabilities that a document can generate the query string.", "labels": [], "entities": []}, {"text": "Several improvements were proposed after their work., for example, was the first to bring up a model with bi-grams and Good Turing re-estimation to smooth the document models.", "labels": [], "entities": []}, {"text": "Latter, used Hidden Markov Model (HMM) for ranking, which also included the use of bigrams.", "labels": [], "entities": []}, {"text": "HMM, firstly introduced by in 1986, has been successfully applied into many domains, such as named entity recognition (D. M.), topic classification (R. ), or speech recognition (J. Makhoul and R..", "labels": [], "entities": [{"text": "named entity recognition (D. M.)", "start_pos": 93, "end_pos": 125, "type": "TASK", "confidence": 0.7975507974624634}, {"text": "topic classification (R. )", "start_pos": 127, "end_pos": 153, "type": "TASK", "confidence": 0.8532246470451355}, {"text": "speech recognition", "start_pos": 158, "end_pos": 176, "type": "TASK", "confidence": 0.8067941069602966}]}, {"text": "In practice, the model requires solving three basic problems.", "labels": [], "entities": []}, {"text": "Given the parameters of the model, computing the probability of a particular output sequence is the first problem.", "labels": [], "entities": []}, {"text": "This process is often referred to as decoding.", "labels": [], "entities": []}, {"text": "Both Forward and Backward procedure are solutions for this problem.", "labels": [], "entities": [{"text": "Forward", "start_pos": 5, "end_pos": 12, "type": "METRIC", "confidence": 0.987846851348877}, {"text": "Backward", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.9871658682823181}]}, {"text": "The second problem is finding the most possible state sequence with the parameters of the model and a particular output sequence.", "labels": [], "entities": []}, {"text": "This is usually completed with Viterbi algorithm.", "labels": [], "entities": []}, {"text": "The third problem is the learning problem of HMM models.", "labels": [], "entities": []}, {"text": "It is often solved by Baum-Welch algorithm (L. E..", "labels": [], "entities": []}, {"text": "Given training data, the algorithm computes the maximum likelihood estimates and posterior mode estimate.", "labels": [], "entities": [{"text": "posterior mode estimate", "start_pos": 81, "end_pos": 104, "type": "METRIC", "confidence": 0.7953341404596964}]}, {"text": "It is in essence a generalized Expectation Maximization (EM) algorithm which was first explained and given name by Dempster, EM can estimate the maximum likelihood of parameters in probabilistic models which has unseen variables.", "labels": [], "entities": [{"text": "generalized Expectation Maximization (EM)", "start_pos": 19, "end_pos": 60, "type": "TASK", "confidence": 0.7544673532247543}]}, {"text": "Nonetheless, in our knowledge, the EM procedure in HMM has never been used in IR domain.", "labels": [], "entities": []}, {"text": "In this paper, we proposed anew language model approach which models the user query and documents as HMM models.", "labels": [], "entities": []}, {"text": "We then used EM algorithm to maximize the probability of query words in our model.", "labels": [], "entities": []}, {"text": "Our assumption is that if the word's probability in a document is maximized, we can estimate the probability of generating the query word from documents more confidently.", "labels": [], "entities": []}, {"text": "Because they not only been calculated by language modeling view features, but also been maximized with statistical methods.", "labels": [], "entities": []}, {"text": "Therefore the imprecise cases caused by special distribution in language modeling approach can be further prevented in this way.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 64, "end_pos": 81, "type": "TASK", "confidence": 0.7047259211540222}]}, {"text": "The remainders of this paper are organized as follows.", "labels": [], "entities": []}, {"text": "We review two related works in Section 2.", "labels": [], "entities": []}, {"text": "In Section 3, we introduce our EM IR approach.", "labels": [], "entities": [{"text": "EM IR", "start_pos": 31, "end_pos": 36, "type": "TASK", "confidence": 0.7718304991722107}]}, {"text": "Section 4 compares our results to two other approaches proposed by and based on the data from TREC HARD track (J.).", "labels": [], "entities": [{"text": "TREC HARD track (J.)", "start_pos": 94, "end_pos": 114, "type": "DATASET", "confidence": 0.8281883001327515}]}, {"text": "Section 5 discusses the effectiveness of our EM training and the EM-based document weighting we proposed.", "labels": [], "entities": [{"text": "EM-based document weighting", "start_pos": 65, "end_pos": 92, "type": "TASK", "confidence": 0.6820941766103109}]}, {"text": "Finally, we conclude our paper in Section 6 and provide some future directions at Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "By using the AQUAINT corpus, two different traditional IR methods are implemented for comparing.", "labels": [], "entities": [{"text": "AQUAINT corpus", "start_pos": 13, "end_pos": 27, "type": "DATASET", "confidence": 0.923633337020874}]}, {"text": "The two IR methods which we use as baselines are the General Language Modeling (GLM) proposed by and the tf.idf measure proposed by For the proposed EM IR approach, two configurations are listed to compare.", "labels": [], "entities": [{"text": "General Language Modeling (GLM)", "start_pos": 53, "end_pos": 84, "type": "TASK", "confidence": 0.702780211965243}, {"text": "EM IR", "start_pos": 149, "end_pos": 154, "type": "TASK", "confidence": 0.86960369348526}]}, {"text": "The first (Con-) is the proposed HMM model without making use of the EM-based document weighting that is don't multiply the transition probability, trans(d), in equation.", "labels": [], "entities": []}, {"text": "The second (Config.2) is the HMM model with EM-based document weighting.", "labels": [], "entities": [{"text": "EM-based document weighting", "start_pos": 44, "end_pos": 71, "type": "TASK", "confidence": 0.5637734333674113}]}, {"text": "The comparison is based on precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 27, "end_pos": 36, "type": "METRIC", "confidence": 0.9996604919433594}]}, {"text": "For each problem, we retrieved the documents with the highest 20 scores, and divided the number of correct answer with the number of retrieved document to obtain precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 162, "end_pos": 171, "type": "METRIC", "confidence": 0.9991893172264099}]}, {"text": "If there are documents with same score at the rank of 20, all of them will be retrieved.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. Term distribution in AQUAINT corpus", "labels": [], "entities": [{"text": "Term", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.7961175441741943}, {"text": "AQUAINT", "start_pos": 31, "end_pos": 38, "type": "DATASET", "confidence": 0.7728090286254883}]}]}