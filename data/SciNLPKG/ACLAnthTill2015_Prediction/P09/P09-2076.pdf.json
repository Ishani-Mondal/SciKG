{"title": [{"text": "Validating the web-based evaluation of NLG systems", "labels": [], "entities": []}], "abstractContent": [{"text": "The GIVE Challenge is a recent shared task in which NLG systems are evaluated over the Internet.", "labels": [], "entities": []}, {"text": "In this paper, we validate this novel NLG evaluation methodology by comparing the Internet-based results with results we collected in a lab experiment.", "labels": [], "entities": [{"text": "NLG evaluation", "start_pos": 38, "end_pos": 52, "type": "TASK", "confidence": 0.9155600666999817}]}, {"text": "We find that the results delivered by both methods are consistent, but the Internet-based approach offers the statistical power necessary for more fine-grained evaluations and is cheaper to carryout.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recently, there has been an increased interest in evaluating and comparing natural language generation (NLG) systems on shared tasks.", "labels": [], "entities": [{"text": "comparing natural language generation (NLG)", "start_pos": 65, "end_pos": 108, "type": "TASK", "confidence": 0.7541880437305996}]}, {"text": "However, this is a notoriously hard problem: Task-based evaluations with human experimental subjects are time-consuming and expensive, and corpus-based evaluations of NLG systems are problematic because a mismatch between humangenerated output and system-generated output does not necessarily mean that the system's output is inferior . This lack of evaluation methods which are both effective and efficient is a serious obstacle to progress in NLG research.", "labels": [], "entities": []}, {"text": "The GIVE Challenge () is a recent shared task which takes a third approach to NLG evaluation: By connecting NLG systems to experimental subjects over the Internet, it achieves a true task-based evaluation at a much lower cost.", "labels": [], "entities": [{"text": "NLG evaluation", "start_pos": 78, "end_pos": 92, "type": "TASK", "confidence": 0.8951592147350311}]}, {"text": "Indeed, the first GIVE Challenge acquired data from over 1100 experimental subjects online.", "labels": [], "entities": []}, {"text": "However, it still remains to be shown that the results that can be obtained in this way are in fact comparable to more established task-based evaluation efforts, which are based on a carefully selected subject pool and carried out in a controlled laboratory environment.", "labels": [], "entities": []}, {"text": "By accepting connections from arbitrary subjects over the Internet, the evaluator gives up control over the subjects' behavior, level of language proficiency, cooperativeness, etc.; there is also an issue of whether demographic factors such as gender might skew the results.", "labels": [], "entities": []}, {"text": "In this paper, we provide the missing link by repeating the GIVE evaluation in a laboratory environment and comparing the results.", "labels": [], "entities": [{"text": "GIVE", "start_pos": 60, "end_pos": 64, "type": "METRIC", "confidence": 0.7087217569351196}]}, {"text": "It turns out that where the two experiments both find a significant difference between two NLG systems with respect to a given evaluation measure, they always agree.", "labels": [], "entities": []}, {"text": "However, the Internet-based experiment finds considerably more such differences, perhaps because of the higher number of experimental subjects (n = 374 vs. n = 91), and offers other opportunities for more fine-grained analysis as well.", "labels": [], "entities": []}, {"text": "We take this as an empirical validation of the Internetbased evaluation of GIVE, and propose that it can be applied to NLG more generally.", "labels": [], "entities": [{"text": "GIVE", "start_pos": 75, "end_pos": 79, "type": "DATASET", "confidence": 0.7039201855659485}]}, {"text": "Our findings are inline with studies from psychology that indicate that the results of web-based experiments are typically consistent with the results of traditional experiments ().", "labels": [], "entities": []}, {"text": "Nevertheless, we do find and discuss some effects of the uncontrolled subject pool that should be addressed in future Internet-based NLG challenges.", "labels": [], "entities": []}], "datasetContent": [{"text": "For the GIVE-1 challenge, 1143 valid games were collected over the Internet over the course of three months.", "labels": [], "entities": [{"text": "GIVE-1 challenge", "start_pos": 8, "end_pos": 24, "type": "TASK", "confidence": 0.47472843527793884}]}, {"text": "These were distributed over three evaluation worlds (World 1: 374, World 2: 369, World 3: 400).", "labels": [], "entities": []}, {"text": "A game was considered valid if the game client didn't crash, the game wasn't marked as a test run by the developers, and the player completed the tutorial.", "labels": [], "entities": []}, {"text": "Of these games, 80% were played by males and 10% by females (the remaining 10% of the participants did not specify their gender).", "labels": [], "entities": []}, {"text": "The players were widely distributed over countries: 37% connected from IP addresses in the US, 33% from Germany, and 17% from China; the rest connected from 45 further countries.", "labels": [], "entities": []}, {"text": "About 34% of the participants self-reported as native English speakers, and 62% specified a language proficiency level of at least \"expert\" (3 on a 5-point scale).", "labels": [], "entities": []}, {"text": "We repeated the GIVE-1 evaluation in a traditional laboratory setting with 91 participants recruited from a college campus.", "labels": [], "entities": [{"text": "GIVE-1", "start_pos": 16, "end_pos": 22, "type": "METRIC", "confidence": 0.43190476298332214}]}, {"text": "In the lab, each participant played the GIVE game once with each of the five NLG systems.", "labels": [], "entities": []}, {"text": "To avoid learning effects, we only used the first game run from each subject in the comparison with the web experiment; as a consequence, subjects were distributed evenly over the NLG systems.", "labels": [], "entities": []}, {"text": "To accommodate for the much lower number of participants, the laboratory experiment only used a single game world -World 1, which was known from the online version to be the easiest world.", "labels": [], "entities": []}, {"text": "Among this group of subjects, 93% self-rated their English proficiency as \"expert\" or better; 81% were native speakers.", "labels": [], "entities": []}, {"text": "In contrast to the online experiment, 31% of participants were male and 65% were female (4% did not specify their gender).", "labels": [], "entities": []}, {"text": "The GIVE software automatically recorded data for five objective measures: the percentage of successfully completed games and, for the successfully completed games, the number of instructions generated by the NLG system, of actions performed by the user (such as pushing buttons), of steps taken by the user (i.e., actions plus movements), and the task completion time (in seconds).", "labels": [], "entities": []}, {"text": "shows the results for the objective measures collected in both experiments.", "labels": [], "entities": []}, {"text": "To make the results comparable, the table for the Internet experiment only includes data for World 1.", "labels": [], "entities": [{"text": "World 1", "start_pos": 93, "end_pos": 100, "type": "DATASET", "confidence": 0.9664870202541351}]}, {"text": "The task success rate is only evaluated on games that were completed successfully or lost, not cancelled, as laboratory subjects were asked not to cancel.", "labels": [], "entities": []}, {"text": "This brings the number of Internet subjects to 322 for the success rate, and to 227 (only successful games) for the other measures.", "labels": [], "entities": []}, {"text": "Task success is the percentage of successfully completed games; the other measures are reported as means.", "labels": [], "entities": []}, {"text": "The chart assigns systems to groups A through C or D for each evaluation measure.", "labels": [], "entities": []}, {"text": "Systems in group A are better than systems in group B, and soon; if two systems have no letter in common, the difference between them is significant with p < 0.05.", "labels": [], "entities": []}, {"text": "Significance was tested using a \u03c7 2 -test for task success and ANOVAs for instructions, steps, actions, and seconds.", "labels": [], "entities": [{"text": "Significance", "start_pos": 0, "end_pos": 12, "type": "METRIC", "confidence": 0.9422996640205383}, {"text": "ANOVAs", "start_pos": 63, "end_pos": 69, "type": "METRIC", "confidence": 0.9974070191383362}]}, {"text": "These were followed by post hoc tests (pairwise \u03c7 2 and Tukey) to compare the NLG systems pairwise.", "labels": [], "entities": [{"text": "Tukey", "start_pos": 56, "end_pos": 61, "type": "DATASET", "confidence": 0.8863174319267273}]}, {"text": "Users were asked to fill in a questionnaire collecting subjective ratings of various aspects of the instructions.", "labels": [], "entities": []}, {"text": "For example, users were asked to rate the overall quality of the direction giving system (on a 7-point scale), the choice of words and the referring expressions (on 5-point scales), and they were asked whether they thought the instructions came at the right time.", "labels": [], "entities": [{"text": "direction giving", "start_pos": 65, "end_pos": 81, "type": "TASK", "confidence": 0.8834835588932037}]}, {"text": "Overall, there were twelve subjective measures (see), of which we only present four typical ones for space reasons.", "labels": [], "entities": []}, {"text": "For each question, the user could choose not to answer.", "labels": [], "entities": []}, {"text": "On the Internet, subjects made considerable use of this option: for instance, 32% of users: Objective and selected subjective measures on the web (top) and in the lab (bottom).", "labels": [], "entities": []}, {"text": "didn't fill in the \"overall evaluation\" field of the questionnaire.", "labels": [], "entities": []}, {"text": "In the laboratory experiment, the subjects were asked to fill in the complete questionnaire and the response rate is close to 100%.", "labels": [], "entities": [{"text": "response rate", "start_pos": 100, "end_pos": 113, "type": "METRIC", "confidence": 0.9355831146240234}]}, {"text": "The results for the four selected subjective measures are summarized in in the same way as the objective measures.", "labels": [], "entities": []}, {"text": "Also as above, the table is based only on successfully completed games in World 1.", "labels": [], "entities": [{"text": "World 1", "start_pos": 74, "end_pos": 81, "type": "DATASET", "confidence": 0.9718304574489594}]}, {"text": "We will justify this latter choice below.", "labels": [], "entities": []}], "tableCaptions": []}