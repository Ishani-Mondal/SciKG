{"title": [{"text": "Capturing Salience with a Trainable Cache Model for Zero-anaphora Resolution", "labels": [], "entities": [{"text": "Capturing Salience", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9099231660366058}]}], "abstractContent": [{"text": "This paper explores how to apply the notion of caching introduced by Walker (1996) to the task of zero-anaphora resolution.", "labels": [], "entities": [{"text": "zero-anaphora resolution", "start_pos": 98, "end_pos": 122, "type": "TASK", "confidence": 0.7152383774518967}]}, {"text": "We propose a machine learning-based implementation of a cache model to reduce the computational cost of identifying an antecedent.", "labels": [], "entities": []}, {"text": "Our empirical evaluation with Japanese newspaper articles shows that the number of candidate antecedents for each zero-pronoun can be dramatically reduced while preserving the accuracy of resolving it.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 176, "end_pos": 184, "type": "METRIC", "confidence": 0.9986563920974731}]}], "introductionContent": [{"text": "There have been recently increasing concerns with the need for anaphora resolution to make NLP applications such as IE and MT more reliable.", "labels": [], "entities": [{"text": "anaphora resolution", "start_pos": 63, "end_pos": 82, "type": "TASK", "confidence": 0.7406575977802277}, {"text": "IE", "start_pos": 116, "end_pos": 118, "type": "TASK", "confidence": 0.9075431227684021}, {"text": "MT", "start_pos": 123, "end_pos": 125, "type": "TASK", "confidence": 0.8454686403274536}]}, {"text": "In particular, for languages such as Japanese, anaphora resolution is crucial for resolving a phrase in a text to its referent since phrases, especially nominative arguments of predicates, are frequently omitted by anaphoric functions in discourse ().", "labels": [], "entities": [{"text": "anaphora resolution", "start_pos": 47, "end_pos": 66, "type": "TASK", "confidence": 0.7937863767147064}]}, {"text": "Many researchers have recently explored machine learning-based methods using considerable amounts of annotated data provided by, for example, the Message Understanding Conference and Automatic Context Extraction programs ().", "labels": [], "entities": [{"text": "Message Understanding Conference", "start_pos": 146, "end_pos": 178, "type": "TASK", "confidence": 0.8317852218945821}, {"text": "Automatic Context Extraction", "start_pos": 183, "end_pos": 211, "type": "TASK", "confidence": 0.6223822931448618}]}, {"text": "These methods reach a level comparable to or better than the state-of-the-art rule-based systems (e.g.) by recasting the task of anaphora resolution into classification or clustering problems.", "labels": [], "entities": [{"text": "anaphora resolution", "start_pos": 129, "end_pos": 148, "type": "TASK", "confidence": 0.7231955230236053}]}, {"text": "However, such approaches tend to disregard theoretical findings from discourse theories, such as Centering Theory (.", "labels": [], "entities": []}, {"text": "Therefore, one of the challenging issues in this area is to incorporate such findings from linguistic theories into machine learning-based approaches.", "labels": [], "entities": []}, {"text": "A typical machine learning-based approach to zero-anaphora resolution searches for an antecedent in the set of candidates appearing in all the preceding contexts.", "labels": [], "entities": [{"text": "zero-anaphora resolution", "start_pos": 45, "end_pos": 69, "type": "TASK", "confidence": 0.7323976457118988}]}, {"text": "However, computational time makes this approach largely infeasible for long texts.", "labels": [], "entities": []}, {"text": "An alternative approach is to heuristically limit the search space (e.g. the system deals with candidates only occurring in the N previous sentences).", "labels": [], "entities": []}, {"text": "Various research such as has adopted this approach, but it also leads to problems when an antecedent is located far from its anaphor, causing it to be excluded from target candidate antecedents.", "labels": [], "entities": []}, {"text": "On the other hand, rule-based methods derived from theoretical background such as Centering Theory ( only deal with the salient discourse entities at each point of the discourse status.", "labels": [], "entities": []}, {"text": "By incrementally updating the discourse status, the set of candidates in question is automatically limited.", "labels": [], "entities": []}, {"text": "Although these methods have a theoretical advantage, they have a serious drawback in that Centering Theory only retains information about the previous sentence.", "labels": [], "entities": []}, {"text": "A few methods have attempted to overcome this fault (), but they are overly dependent upon the restrictions fundamental to the notion of centering.", "labels": [], "entities": []}, {"text": "We hope that by relaxing such restrictions it will be possible for an anaphora resolution system to achieve a good balance between accuracy and computational cost.", "labels": [], "entities": [{"text": "anaphora resolution", "start_pos": 70, "end_pos": 89, "type": "TASK", "confidence": 0.731781467795372}, {"text": "accuracy", "start_pos": 131, "end_pos": 139, "type": "METRIC", "confidence": 0.9988479614257812}]}, {"text": "From this background, we focus on the issue of reducing candidate antecedents (discourse entities) fora given anaphor.", "labels": [], "entities": []}, {"text": "Inspired by Walker's argument, we propose a machine learning-based caching mechanism that captures the most salient candidates at each point of the discourse for efficient anaphora resolution.", "labels": [], "entities": [{"text": "anaphora resolution", "start_pos": 172, "end_pos": 191, "type": "TASK", "confidence": 0.7201073318719864}]}, {"text": "More specifically, we choose salient candidates for each sentence from the set of candidates appearing in that sentence and the candidates which are already in the cache.", "labels": [], "entities": []}, {"text": "Searching only through the set of salient candidates, the computational cost of zeroanaphora resolution is effectively reduced.", "labels": [], "entities": [{"text": "zeroanaphora resolution", "start_pos": 80, "end_pos": 103, "type": "TASK", "confidence": 0.639692485332489}]}, {"text": "In the empirical evaluation, we investigate how efficiently this caching mechanism contributes to reducing the search space while preserving accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 141, "end_pos": 149, "type": "METRIC", "confidence": 0.9962789416313171}]}, {"text": "This paper focuses on Japanese though the proposed cache mechanism maybe applicable to any language.", "labels": [], "entities": []}, {"text": "This paper is organized as follows.", "labels": [], "entities": []}, {"text": "First, Section 2 presents the task of zero-anaphora resolution and then Section 3 gives an overview of previous work.", "labels": [], "entities": [{"text": "zero-anaphora resolution", "start_pos": 38, "end_pos": 62, "type": "TASK", "confidence": 0.6817466467618942}]}, {"text": "Next, in Section 4 we propose a machine learning-based cache model.", "labels": [], "entities": []}, {"text": "Section 5 presents the antecedent identification and anaphoricity determination models used in the experiments.", "labels": [], "entities": []}, {"text": "To evaluate the model, we conduct several empirical evaluations and report their results in Section 6.", "labels": [], "entities": []}, {"text": "Finally, we conclude and discuss the future direction of this research in Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "We investigate how the cache model contributes to candidate reduction.", "labels": [], "entities": [{"text": "candidate reduction", "start_pos": 50, "end_pos": 69, "type": "TASK", "confidence": 0.7920019030570984}]}, {"text": "More specifically, we ex-  In this experiment, we directly compare the proposed static and dynamic cache models with the heuristic methods presented in Section 2.", "labels": [], "entities": []}, {"text": "Note that in the dynamic cache model is disregarded in this experiment because its performance crucially depends on the performance of the zeroanaphora resolution model.", "labels": [], "entities": []}, {"text": "The performance of the cache model is evaluated by coverage, which is a percentage of retained antecedents when appearing zero-pronouns refer to an antecedent in a preceding sentence, i.e. we evaluate the cases of inter-sentential anaphora resolution.", "labels": [], "entities": [{"text": "coverage", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.9902575016021729}, {"text": "inter-sentential anaphora resolution", "start_pos": 214, "end_pos": 250, "type": "TASK", "confidence": 0.6766297618548075}]}, {"text": "As a baseline, we adopt the following two cache models.", "labels": [], "entities": []}, {"text": "One is the Centering-derived model which only stores the preceding 'wa' (topic)-marked or 'ga' (subject)-marked candidate antecedents in the cache.", "labels": [], "entities": []}, {"text": "It is an approximation of the model proposed by for extending the local focus transition defined by Centering Theory.", "labels": [], "entities": []}, {"text": "We henceforth call this model the centering-based cache model.", "labels": [], "entities": []}, {"text": "The other baseline model stores candidates appearing in the N previous sentences of a zero-pronoun to simulate a heuristic approach used in works like.", "labels": [], "entities": []}, {"text": "We call this model the sentence-based cache model.", "labels": [], "entities": []}, {"text": "By comparing these baselines with our cache models, we can see whether our models contribute to more efficiently storing salient candidates or not.", "labels": [], "entities": []}, {"text": "The above dynamic cache model retains the salient candidates independently of the results of antecedent identification conducted in the preceding contexts.", "labels": [], "entities": []}, {"text": "However, if the zero-anaphora resolution in the current utterance is performed correctly, it will be available for use as information about the recency of candidates and the anaphoric chain of each candidate.", "labels": [], "entities": []}, {"text": "Therefore, we also investigate whether correct zero-anaphora resolution contributes to the dynamic cache model or not.", "labels": [], "entities": []}, {"text": "To integrate zero-anaphora resolution information, we create training instances of the dynamic cache model by updating the recency using the function 'updateSalienceInfo' shown in and also using an additional feature, CHAIN NUM, defined in.", "labels": [], "entities": []}, {"text": "The results are shown in 7 . We can seethe effect of the machine learning-based cache models in comparison to the other two heuristic models.", "labels": [], "entities": []}, {"text": "The results demonstrate that the former achieves good coverage at each point compared to the latter.", "labels": [], "entities": [{"text": "coverage", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.9921859502792358}]}, {"text": "In addition, the difference between the static and dynamic cache models demonstrates that the dynamic one is always better then the static.", "labels": [], "entities": []}, {"text": "It maybe this way because the dynamic cache model simultaneously retains global focus of a given text and the locally salient entities in the current discourse.", "labels": [], "entities": []}, {"text": "By comparing the dynamic cache model using correct zero-anaphora resolution (denoted by DCM (with ZAR) in) and the one without it (DCM (w/o ZAR)), we can see that correct zeroanaphora resolution contributes to improving the caching for every cache size.", "labels": [], "entities": []}, {"text": "However, in the practical setting the current zero-anaphora resolu-7 Expressions such as verbs were rarely annotated as antecedents, so these are not extracted as candidate antecedents in our current setting.", "labels": [], "entities": []}, {"text": "This is the reason why the coverage of using all the candidates is less than 1.0.", "labels": [], "entities": [{"text": "coverage", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9974011182785034}]}, {"text": "tion system sometimes chooses the wrong candidate as an antecedent or does not choose any candidate due to wrong anaphoricity determination, negatively impacting the performance of the cache model.", "labels": [], "entities": []}, {"text": "For this reason, in the following two experiments we decided not to use zero-anaphora resolution in the dynamic cache model.", "labels": [], "entities": []}, {"text": "We next investigate the impact of the dynamic cache model shown in Section 4.1 on the antecedent identification task of inter-sentential zeroanaphora resolution altering the cache size from 5 to the number of all candidates.", "labels": [], "entities": [{"text": "inter-sentential zeroanaphora resolution", "start_pos": 120, "end_pos": 160, "type": "TASK", "confidence": 0.6145519018173218}]}, {"text": "We compare the following three cache model within the task of inter-sentential antecedent identification: the centering-based cache model, the sentence-based cache model and the dynamic cache model disregarding updateSalienceInfo (i.e. DCM (w/o ZAR) in).", "labels": [], "entities": [{"text": "inter-sentential antecedent identification", "start_pos": 62, "end_pos": 104, "type": "TASK", "confidence": 0.6670690576235453}]}, {"text": "We also investigate the computational time of the process of inter-sentential antecedent identification with each cache model altering its parameter . The results are shown in.", "labels": [], "entities": [{"text": "inter-sentential antecedent identification", "start_pos": 61, "end_pos": 103, "type": "TASK", "confidence": 0.707097331682841}]}, {"text": "From these results, we can seethe antecedent identification model using the dynamic cache model obtains almost the same accuracy for every cache size.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 120, "end_pos": 128, "type": "METRIC", "confidence": 0.9990492463111877}]}, {"text": "It indicates that if the model can acquire a small number of the most salient discourse entities in the current discourse, the model achieves accuracy comparable to the model which searches all the preceding discourse entities, while drastically reducing the computational time.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 142, "end_pos": 150, "type": "METRIC", "confidence": 0.999277651309967}]}, {"text": "The results also show that the current antecedent identification model with the dynamic cache model does not necessarily outperform the model with the baseline cache models.", "labels": [], "entities": []}, {"text": "For example, the sentence-based cache model using the preceding two sentences (SM (s=2)) achieved an accuracy comparable to the dynamic cache model with the cache size 15 (DCM (n=15)), both spending almost the same computational time.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 101, "end_pos": 109, "type": "METRIC", "confidence": 0.9993003606796265}]}, {"text": "This is supposed to be due to the limited accuracy of the current antecedent identification model.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.9993958473205566}]}, {"text": "Since the dynamic cache models provide much better search spaces than the baseline models as shown in, there is presumably more room for improvement with the dynamic cache models.", "labels": [], "entities": []}, {"text": "More investigations are to be concluded in our future work.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Results on antecedent identification", "labels": [], "entities": []}]}