{"title": [{"text": "The Contribution of Linguistic Features to Automatic Machine Translation Evaluation", "labels": [], "entities": [{"text": "Automatic Machine Translation Evaluation", "start_pos": 43, "end_pos": 83, "type": "TASK", "confidence": 0.656464472413063}]}], "abstractContent": [{"text": "A number of approaches to Automatic MT Evaluation based on deep linguistic knowledge have been suggested.", "labels": [], "entities": [{"text": "MT Evaluation", "start_pos": 36, "end_pos": 49, "type": "TASK", "confidence": 0.8788720667362213}]}, {"text": "However , n-gram based metrics are still today the dominant approach.", "labels": [], "entities": []}, {"text": "The main reason is that the advantages of employing deeper linguistic information have not been clarified yet.", "labels": [], "entities": []}, {"text": "In this work, we propose a novel approach for meta-evaluation of MT evaluation metrics, since correlation cofficient against human judges do not reveal details about the advantages and disadvantages of particular metrics.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 65, "end_pos": 78, "type": "TASK", "confidence": 0.9203714728355408}]}, {"text": "We then use this approach to investigate the benefits of introducing linguistic features into evaluation metrics.", "labels": [], "entities": []}, {"text": "Overall, our experiments show that (i) both lexical and linguistic metrics present complementary advantages and (ii) combining both kinds of metrics yields the most robust meta-evaluation performance.", "labels": [], "entities": []}], "introductionContent": [{"text": "Automatic evaluation methods based on similarity to human references have substantially accelerated the development cycle of many NLP tasks, such as Machine Translation, Automatic Summarization, Sentence Compression and Language Generation.", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 149, "end_pos": 168, "type": "TASK", "confidence": 0.8572421669960022}, {"text": "Automatic Summarization", "start_pos": 170, "end_pos": 193, "type": "TASK", "confidence": 0.7015171349048615}, {"text": "Language Generation", "start_pos": 220, "end_pos": 239, "type": "TASK", "confidence": 0.7554202973842621}]}, {"text": "These automatic evaluation metrics allow developers to optimize their systems without the need for expensive human assessments for each of their possible system configurations.", "labels": [], "entities": []}, {"text": "However, estimating the system output quality according to its similarity to human references is not a trivial task.", "labels": [], "entities": []}, {"text": "The main problem is that many NLP tasks are open/subjective; therefore, different humans may generate different outputs, all of them equally valid.", "labels": [], "entities": []}, {"text": "Thus, language variability is an issue.", "labels": [], "entities": [{"text": "language variability", "start_pos": 6, "end_pos": 26, "type": "TASK", "confidence": 0.7199888527393341}]}, {"text": "In order to tackle language variability in the context of Machine Translation, a considerable effort has also been made to include deeper linguistic information in automatic evaluation metrics, both syntactic and semantic (see Section 2 for details).", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 58, "end_pos": 77, "type": "TASK", "confidence": 0.7931436598300934}]}, {"text": "However, the most commonly used metrics are still based on n-gram matching.", "labels": [], "entities": []}, {"text": "The reason is that the advantages of employing higher linguistic processing levels have not been clarified yet.", "labels": [], "entities": []}, {"text": "The main goal of our work is to analyze to what extent deep linguistic features can contribute to the automatic evaluation of translation quality.", "labels": [], "entities": []}, {"text": "For that purpose, we compare -using four different test beds -the performance of 16 n-gram based metrics, 48 linguistic metrics and one combined metric from the state of the art.", "labels": [], "entities": []}, {"text": "Analyzing the reliability of evaluation metrics requires meta-evaluation criteria.", "labels": [], "entities": []}, {"text": "In this respect, we identify important drawbacks of the standard meta-evaluation methods based on correlation with human judgements.", "labels": [], "entities": []}, {"text": "In order to overcome these drawbacks, we then introduce six novel meta-evaluation criteria which represent different metric reliability dimensions.", "labels": [], "entities": []}, {"text": "Our analysis indicates that: (i) both lexical and linguistic metrics have complementary advantages and different drawbacks; (ii) combining both kinds of metrics is a more effective and robust evaluation method across all meta-evaluation criteria.", "labels": [], "entities": []}, {"text": "In addition, we also perform a qualitative analysis of one hundred sentences that were incorrectly evaluated by state-of-the-art metrics.", "labels": [], "entities": []}, {"text": "The analysis confirms that deep linguistic techniques are necessary to avoid the most common types of error.", "labels": [], "entities": []}, {"text": "Section 2 examines the state of the art Section 3 describes the test beds and metrics considered in our experiments.", "labels": [], "entities": []}, {"text": "In Section 4 the correlation between human assessors and metrics is computed, with a discussion of its drawbacks.", "labels": [], "entities": []}, {"text": "In Section 5 different quality aspects of metrics are analysed.", "labels": [], "entities": []}, {"text": "Conclusions are drawn in the last section.", "labels": [], "entities": []}], "datasetContent": [{"text": "Insofar as automatic evaluation metrics for machine translation have been proposed, different meta-evaluation frameworks have been gradually introduced.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 44, "end_pos": 63, "type": "TASK", "confidence": 0.7734848260879517}]}, {"text": "For instance, introduced the BLEU metric and evaluated its reliability in terms of Pearson correlation with human assessments for adequacy and fluency judgements.", "labels": [], "entities": [{"text": "BLEU metric", "start_pos": 29, "end_pos": 40, "type": "METRIC", "confidence": 0.967345118522644}, {"text": "reliability", "start_pos": 59, "end_pos": 70, "type": "METRIC", "confidence": 0.9704363346099854}, {"text": "Pearson correlation", "start_pos": 83, "end_pos": 102, "type": "METRIC", "confidence": 0.9577063620090485}]}, {"text": "With the aim of overcoming some of the deficiencies of BLEU, Doddington (2002) introduced the NIST metric.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 55, "end_pos": 59, "type": "METRIC", "confidence": 0.9461789727210999}, {"text": "NIST metric", "start_pos": 94, "end_pos": 105, "type": "DATASET", "confidence": 0.7358316779136658}]}, {"text": "Metric reliability was also estimated in terms of correlation with human assessments, but over different document sources and fora varying number of references and segment sizes.", "labels": [], "entities": []}, {"text": "argued, at the time of introducing the GTM metric, that Pearson correlation coefficients can be affected by scale properties, and suggested, in order to avoid this effect, to use the non-parametric Spearman correlation coefficients instead.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 56, "end_pos": 75, "type": "METRIC", "confidence": 0.6596391797065735}]}, {"text": "experimented, unlike previous works, with a wide set of metrics, including NIST, WER (), PER (, and variants of ROUGE, BLEU and GTM.", "labels": [], "entities": [{"text": "NIST", "start_pos": 75, "end_pos": 79, "type": "METRIC", "confidence": 0.6451804637908936}, {"text": "WER", "start_pos": 81, "end_pos": 84, "type": "METRIC", "confidence": 0.9884041547775269}, {"text": "PER", "start_pos": 89, "end_pos": 92, "type": "METRIC", "confidence": 0.9960119724273682}, {"text": "ROUGE", "start_pos": 112, "end_pos": 117, "type": "METRIC", "confidence": 0.9808804392814636}, {"text": "BLEU", "start_pos": 119, "end_pos": 123, "type": "METRIC", "confidence": 0.9938129186630249}]}, {"text": "They computed both Pearson and Spearman correlation, obtaining similar results in both cases.", "labels": [], "entities": [{"text": "Pearson and Spearman correlation", "start_pos": 19, "end_pos": 51, "type": "METRIC", "confidence": 0.576668456196785}]}, {"text": "Ina different work, argued that the measured reliability of metrics can be due to averaging effects but might not be robust across translations.", "labels": [], "entities": []}, {"text": "In order to address this issue, they computed the translation-by-translation correlation with human judgements (i.e., correlation at the segment level).", "labels": [], "entities": []}, {"text": "All that metrics were based on n-gram overlap.", "labels": [], "entities": []}, {"text": "But there is also extensive research focused on including linguistic knowledge in metrics;) among others.", "labels": [], "entities": []}, {"text": "In all these cases, metrics were also evaluated by means of correlation with human judgements.", "labels": [], "entities": []}, {"text": "Ina different research line, several authors have suggested approaching automatic evaluation through the combination of individual metric scores.", "labels": [], "entities": []}, {"text": "Among the most relevant let us cite research by,.", "labels": [], "entities": []}, {"text": "But finding optimal metric combinations requires a meta-evaluation criterion.", "labels": [], "entities": []}, {"text": "Most approaches again rely on correlation with human judgements.", "labels": [], "entities": []}, {"text": "However, some of them measured the reliability of metric combinations in terms of their ability to discriminate between human translations and automatic ones (human likeness)).", "labels": [], "entities": [{"text": "reliability", "start_pos": 35, "end_pos": 46, "type": "METRIC", "confidence": 0.9725741744041443}]}, {"text": "In this work, we present a novel approach to meta-evaluation which is distinguished by the use of additional easily interpretable meta-evaluation criteria oriented to measure different aspects of metric reliability.", "labels": [], "entities": []}, {"text": "We then apply this approach to find out about the advantages and challenges of including linguistic features in meta-evaluation criteria.", "labels": [], "entities": []}, {"text": "We have seen that correlation with human judgements has serious limitations for metric evaluation.", "labels": [], "entities": [{"text": "metric evaluation", "start_pos": 80, "end_pos": 97, "type": "TASK", "confidence": 0.9586311280727386}]}, {"text": "Therefore, we have focused on other aspects of metric reliability that have revealed differences between n-gram and linguistic based metrics: 1.", "labels": [], "entities": []}, {"text": "Is the metric able to accurately reveal improvements between two systems?", "labels": [], "entities": []}, {"text": "2. Can we trust the metric when it says that a translation is very good or very bad?", "labels": [], "entities": []}, {"text": "We now discuss each of these aspects separately.", "labels": [], "entities": []}, {"text": "In order to shed some light on the reasons for the automatic evaluation failures when assigning low scores, we have manually analyzed cases in which a metric score is low but the quality according to humans is high (Q Nm \u2264 3 and Q h \u2265 7).", "labels": [], "entities": []}, {"text": "We have studied 100 sentence evaluation cases from representatives of each metric family including: 1-PER, BLEU, DP-O r -, GTM (e = 2), METEOR and ROUGE L . The evaluation cases have been extracted from the four test beds.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 107, "end_pos": 111, "type": "METRIC", "confidence": 0.9990584254264832}, {"text": "DP-O r -", "start_pos": 113, "end_pos": 121, "type": "METRIC", "confidence": 0.9273646076520284}, {"text": "METEOR", "start_pos": 136, "end_pos": 142, "type": "METRIC", "confidence": 0.9848907589912415}, {"text": "ROUGE L", "start_pos": 147, "end_pos": 154, "type": "METRIC", "confidence": 0.9696674048900604}]}, {"text": "We have identified four main (non exclusive) failure causes: Format issues, e.g. \"US \" vs \"United States\").", "labels": [], "entities": []}, {"text": "Elements such as abbreviations, acronyms or numbers which do not match the manual translation.", "labels": [], "entities": []}, {"text": "Pseudo-synonym terms, e.g. \"US Scheduled the Release\" vs. \"US set to Release\").", "labels": [], "entities": []}, {"text": ") In most of these cases, synonymy can only be identified from the discourse context.", "labels": [], "entities": []}, {"text": "Therefore, terminological resources (e.g., WordNet) are not enough to tackle this problem.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 43, "end_pos": 50, "type": "DATASET", "confidence": 0.952589750289917}]}, {"text": "Non relevant information omissions, e.g. \"Thank you\" vs. \"Thank you very much\" or \"dollar\" vs. \"US dollar\")).", "labels": [], "entities": []}, {"text": "The translation system obviates some information which, in context, is not considered crucial by the human assessors.", "labels": [], "entities": []}, {"text": "This effect is specially important in short sentences.", "labels": [], "entities": []}, {"text": "Incorrect structures that change the meaning while maintaining the same idea (e.g., \"Bush Praises NASA 's Mars Mission\" vs \" Bush praises nasa of Mars mission\" ).", "labels": [], "entities": []}, {"text": "Note that all of these kinds of failure -except formatting issues -require deep linguistic processing while n-gram overlap or even synonyms extracted from a standard ontology are not enough to deal with them.", "labels": [], "entities": [{"text": "formatting", "start_pos": 48, "end_pos": 58, "type": "TASK", "confidence": 0.9586061239242554}]}, {"text": "This conclusion motivates the incorporation of linguistic processing into automatic evaluation metrics.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: NIST 2004/2005 MT Evaluation Cam- paigns. Test bed description", "labels": [], "entities": [{"text": "NIST 2004/2005 MT Evaluation Cam- paigns", "start_pos": 10, "end_pos": 50, "type": "DATASET", "confidence": 0.8925131228235033}]}, {"text": " Table 3: System pairs with a significant difference  according to human judgements (Wilcoxon test)", "labels": [], "entities": [{"text": "Wilcoxon test", "start_pos": 85, "end_pos": 98, "type": "DATASET", "confidence": 0.840447336435318}]}, {"text": " Table 4: Metrics ranked according to the Oracle  System Test", "labels": [], "entities": [{"text": "Oracle  System Test", "start_pos": 42, "end_pos": 61, "type": "DATASET", "confidence": 0.9191405971844991}]}]}