{"title": [{"text": "Reducing the Annotation Effort for Letter-to-Phoneme Conversion", "labels": [], "entities": [{"text": "Letter-to-Phoneme Conversion", "start_pos": 35, "end_pos": 63, "type": "TASK", "confidence": 0.7026466876268387}]}], "abstractContent": [{"text": "Letter-to-phoneme (L2P) conversion is the process of producing a correct phoneme sequence fora word, given its letters.", "labels": [], "entities": [{"text": "Letter-to-phoneme (L2P) conversion", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.6489937901496887}]}, {"text": "It is often desirable to reduce the quantity of training data-and hence human annotation that is needed to train an L2P classifier fora new language.", "labels": [], "entities": []}, {"text": "In this paper , we confront the challenge of building an accurate L2P classifier with a minimal amount of training data by combining several diverse techniques: context ordering, letter clustering, active learning, and pho-netic L2P alignment.", "labels": [], "entities": [{"text": "context ordering", "start_pos": 161, "end_pos": 177, "type": "TASK", "confidence": 0.7383025586605072}, {"text": "letter clustering", "start_pos": 179, "end_pos": 196, "type": "TASK", "confidence": 0.7689051926136017}, {"text": "L2P alignment", "start_pos": 229, "end_pos": 242, "type": "TASK", "confidence": 0.6775805950164795}]}, {"text": "Experiments on six languages show up to 75% reduction in annotation effort.", "labels": [], "entities": []}], "introductionContent": [{"text": "The task of letter-to-phoneme (L2P) conversion is to produce a correct sequence of phonemes, given the letters that comprise a word.", "labels": [], "entities": [{"text": "letter-to-phoneme (L2P) conversion", "start_pos": 12, "end_pos": 46, "type": "TASK", "confidence": 0.6741343379020691}]}, {"text": "An accurate L2P converter is an important component of a text-to-speech system.", "labels": [], "entities": []}, {"text": "In general, a lookup table does not suffice for L2P conversion, since out-of-vocabulary words (e.g., proper names) are inevitably encountered.", "labels": [], "entities": [{"text": "L2P conversion", "start_pos": 48, "end_pos": 62, "type": "TASK", "confidence": 0.733727365732193}]}, {"text": "This motivates the need for classification techniques that can predict the phonemes for an unseen word.", "labels": [], "entities": []}, {"text": "Numerous studies have contributed to the development of increasingly accurate L2P systems (.", "labels": [], "entities": []}, {"text": "A common assumption made in these works is that ample amounts of labelled data are available for training a classifier.", "labels": [], "entities": []}, {"text": "Yet, in practice, this is the case for only a small number of languages.", "labels": [], "entities": []}, {"text": "In order to train an L2P classifier fora new language, we must first annotate words in that language with their correct phoneme sequences.", "labels": [], "entities": []}, {"text": "As annotation is expensive, we would like to minimize the amount of effort that is required to build an adequate training set.", "labels": [], "entities": []}, {"text": "The objective of this work is not necessarily to achieve state-of-the-art performance when presented with large amounts of training data, but to outperform other approaches when training data is limited.", "labels": [], "entities": []}, {"text": "This paper proposes a system for training an accurate L2P classifier while requiring as few annotated words as possible.", "labels": [], "entities": []}, {"text": "We employ decision trees as our supervised learning method because of their transparency and flexibility.", "labels": [], "entities": []}, {"text": "We incorporate context ordering into a decision tree learner that guides its tree-growing procedure towards generating more intuitive rules.", "labels": [], "entities": []}, {"text": "A clustering over letters serves as a back-off model in cases where individual letter counts are unreliable.", "labels": [], "entities": []}, {"text": "An active learning technique is employed to request the phonemes (labels) for the words that are expected to be the most informative.", "labels": [], "entities": []}, {"text": "Finally, we apply a novel L2P alignment technique based on phonetic similarity, which results in impressive gains inaccuracy without relying on any training data.", "labels": [], "entities": [{"text": "L2P alignment", "start_pos": 26, "end_pos": 39, "type": "TASK", "confidence": 0.6018885523080826}]}, {"text": "Our empirical evaluation on several L2P datasets demonstrates that significant reductions in annotation effort are indeed possible in this domain.", "labels": [], "entities": []}, {"text": "Individually, all four enhancements improve the accuracy of our decision tree learner.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.9991357922554016}]}, {"text": "The combined system yields savings of up to 75% in the number of words that have to be labelled, and reductions of at least 52% are observed on all the datasets.", "labels": [], "entities": []}, {"text": "This is achieved without any additional tuning for the various languages.", "labels": [], "entities": []}, {"text": "The paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 explains how supervised learning for L2P conversion is carried outwith decision trees, our classifier of choice.", "labels": [], "entities": [{"text": "L2P conversion", "start_pos": 47, "end_pos": 61, "type": "TASK", "confidence": 0.6588665843009949}]}, {"text": "Sections 3 through 6 describe our four main contributions towards reducing the annotation effort for L2P: context ordering (Section 3), clustering letters (Section 4), active learning (Section 5), and phonetic alignment (Section 6).", "labels": [], "entities": [{"text": "context ordering", "start_pos": 106, "end_pos": 122, "type": "TASK", "confidence": 0.7638149559497833}, {"text": "phonetic alignment", "start_pos": 201, "end_pos": 219, "type": "TASK", "confidence": 0.7755919992923737}]}, {"text": "Our experimental setup and results are discussed in Sections 7 and 8, respectively.", "labels": [], "entities": []}, {"text": "Finally, Section 9 offers some concluding remarks.", "labels": [], "entities": []}], "datasetContent": [{"text": "We performed experiments on six datasets, which were obtained from the PRONALSYL letterto-phoneme conversion challenge.", "labels": [], "entities": [{"text": "PRONALSYL letterto-phoneme conversion challenge", "start_pos": 71, "end_pos": 118, "type": "TASK", "confidence": 0.760327160358429}]}, {"text": "They are: English CMUDict (Carnegie Mellon University, 1998); French BRULEX (), Dutch and German CELEX (, the Italian Festival dictionary (), and the Spanish lexicon.", "labels": [], "entities": [{"text": "English CMUDict (Carnegie Mellon University, 1998)", "start_pos": 10, "end_pos": 60, "type": "DATASET", "confidence": 0.9268949429194132}, {"text": "BRULEX", "start_pos": 69, "end_pos": 75, "type": "METRIC", "confidence": 0.8615438938140869}, {"text": "CELEX", "start_pos": 97, "end_pos": 102, "type": "METRIC", "confidence": 0.8972039818763733}, {"text": "Italian Festival dictionary", "start_pos": 110, "end_pos": 137, "type": "DATASET", "confidence": 0.7494346598784128}]}, {"text": "Duplicate words and words containing punctuation or numerals were removed, as were abbreviations and acronyms.", "labels": [], "entities": []}, {"text": "The resulting datasets range in size from 31,491 to 111,897 words.", "labels": [], "entities": []}, {"text": "The PRONALSYL datasets are already divided into 10 folds; we used the first fold as our test set, and the other folds were merged together to form the learning set.", "labels": [], "entities": [{"text": "PRONALSYL datasets", "start_pos": 4, "end_pos": 22, "type": "DATASET", "confidence": 0.8119778037071228}]}, {"text": "In our preliminary experiments, we randomly set aside 10 percent of this learning set to serve as our development set.", "labels": [], "entities": []}, {"text": "Since the focus of our work is on algorithmic enhancements, we simulate the annotator with an oracle and do not address the potential human interface factors.", "labels": [], "entities": []}, {"text": "During an experiment, 100 words were drawn at random from the learning set; these constituted the data on which an initial classifier was trained.", "labels": [], "entities": []}, {"text": "The rest of the words in the learning set formed the unlabelled pool for active learning; their phonemes were hidden, and a given word's phonemes were revealed if the word was selected for labelling.", "labels": [], "entities": []}, {"text": "After training a classifier on the 100 annotated words, we performed 190 iterations of active learning.", "labels": [], "entities": []}, {"text": "On each iteration, 10 words were selected according to Equation 1, labelled by an oracle, and added to the training set.", "labels": [], "entities": []}, {"text": "In order to speedup the experiments, a random sample of 2000 words was drawn from the pool and presented to the active learner each time.", "labels": [], "entities": []}, {"text": "Hence, QBB selected 10 words from the 2000 candidates.", "labels": [], "entities": [{"text": "QBB", "start_pos": 7, "end_pos": 10, "type": "DATASET", "confidence": 0.7821886539459229}]}, {"text": "We set the QBB committee size |C| to 10.", "labels": [], "entities": []}, {"text": "At each step, we measured word accuracy with respect to the holdout set as the percentage of test words that yielded no erroneous phoneme predictions.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.967896044254303}]}, {"text": "Henceforth, we use accuracy to refer to word accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.99853515625}, {"text": "accuracy", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.9450552463531494}]}, {"text": "Note that although we query examples using a committee, we train a singletree on these examples in order to produce an intelligible model.", "labels": [], "entities": []}, {"text": "Prior work has demonstrated that this configuration performs well in practice).", "labels": [], "entities": []}, {"text": "Our results report the accuracy of the singletree grown on each iteration, averaged over 10 random draws of the initial training set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9995421171188354}]}, {"text": "For our decision tree learner, we utilized the J48 algorithm provided by).", "labels": [], "entities": []}, {"text": "We also experimented with, an implementation of CART, but J48 performed better during preliminary trials.", "labels": [], "entities": [{"text": "CART", "start_pos": 48, "end_pos": 52, "type": "TASK", "confidence": 0.6632209420204163}, {"text": "J48", "start_pos": 58, "end_pos": 61, "type": "DATASET", "confidence": 0.909208357334137}]}, {"text": "We ran J48 with default parameter settings, except that binary trees were grown (see Section 2), and subtree raising was disabled.", "labels": [], "entities": [{"text": "subtree raising", "start_pos": 101, "end_pos": 116, "type": "TASK", "confidence": 0.7210193425416946}]}, {"text": "Our feature template was established during development set experiments with the English CMU data; the data from the other five languages did not influence these choices.", "labels": [], "entities": [{"text": "English CMU data", "start_pos": 81, "end_pos": 97, "type": "DATASET", "confidence": 0.787193218866984}]}, {"text": "The letter context consisted of the focus letter and the 3 letters appearing before and after the focus (or beginning/end of word markers, where applicable).", "labels": [], "entities": []}, {"text": "For letter class features, bit strings of length 1 through 6 were used for the focus letter and its immediate neighbors.", "labels": [], "entities": []}, {"text": "Bit strings of length at most 3 were used at positions +2 and \u22122, and no such features were added at \u00b13.", "labels": [], "entities": []}, {"text": "We experimented with other configurations, including using bit strings of up to length 6 at all positions, but they did not produce consistent improvements over the selected scheme.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Hierarchical clustering of English letters", "labels": [], "entities": [{"text": "Hierarchical clustering of English letters", "start_pos": 10, "end_pos": 52, "type": "TASK", "confidence": 0.8982533812522888}]}]}