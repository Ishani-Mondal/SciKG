{"title": [{"text": "Efficient Minimum Error Rate Training and Minimum Bayes-Risk Decoding for Translation Hypergraphs and Lattices", "labels": [], "entities": [{"text": "Efficient Minimum Error Rate Training", "start_pos": 0, "end_pos": 37, "type": "METRIC", "confidence": 0.6099903464317322}, {"text": "Translation Hypergraphs and Lattices", "start_pos": 74, "end_pos": 110, "type": "TASK", "confidence": 0.8375065177679062}]}], "abstractContent": [{"text": "Minimum Error Rate Training (MERT) and Minimum Bayes-Risk (MBR) decoding are used inmost current state-of-the-art Statistical Machine Translation (SMT) systems.", "labels": [], "entities": [{"text": "Error Rate Training (MERT", "start_pos": 8, "end_pos": 33, "type": "METRIC", "confidence": 0.8550307393074036}, {"text": "Minimum Bayes-Risk (MBR)", "start_pos": 39, "end_pos": 63, "type": "METRIC", "confidence": 0.667098993062973}, {"text": "Statistical Machine Translation (SMT)", "start_pos": 114, "end_pos": 151, "type": "TASK", "confidence": 0.8019401778777441}]}, {"text": "The algorithms were originally developed to work with N-best lists of translations, and recently extended to lattices that encode many more hypotheses than typical N-best lists.", "labels": [], "entities": []}, {"text": "We here extend lattice-based MERT and MBR algorithms to work with hypergraphs that encode avast number of translations produced by MT systems based on Synchronous Context Free Grammars.", "labels": [], "entities": [{"text": "MERT", "start_pos": 29, "end_pos": 33, "type": "METRIC", "confidence": 0.7503461241722107}, {"text": "MT", "start_pos": 131, "end_pos": 133, "type": "TASK", "confidence": 0.8985611200332642}]}, {"text": "These algorithms are more efficient than the lattice-based versions presented earlier.", "labels": [], "entities": []}, {"text": "We show how MERT can be employed to optimize parameters for MBR decoding.", "labels": [], "entities": [{"text": "MERT", "start_pos": 12, "end_pos": 16, "type": "METRIC", "confidence": 0.772094190120697}, {"text": "MBR decoding", "start_pos": 60, "end_pos": 72, "type": "TASK", "confidence": 0.6306516528129578}]}, {"text": "Our experiments show speedups from MERT and MBR as well as performance improvements from MBR decoding on several language pairs.", "labels": [], "entities": [{"text": "MERT", "start_pos": 35, "end_pos": 39, "type": "DATASET", "confidence": 0.5165908336639404}, {"text": "MBR", "start_pos": 44, "end_pos": 47, "type": "DATASET", "confidence": 0.8359231352806091}]}], "introductionContent": [{"text": "Statistical Machine Translation (SMT) systems have improved considerably by directly using the error criterion in both training and decoding.", "labels": [], "entities": [{"text": "Statistical Machine Translation (SMT)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.8210636178652445}]}, {"text": "By doing so, the system can be optimized for the translation task instead of a criterion such as likelihood that is unrelated to the evaluation metric.", "labels": [], "entities": [{"text": "translation task", "start_pos": 49, "end_pos": 65, "type": "TASK", "confidence": 0.901768296957016}]}, {"text": "Two popular techniques that incorporate the error criterion are Minimum Error Rate Training (MERT) and Minimum BayesRisk (MBR) decoding ().", "labels": [], "entities": [{"text": "Minimum Error Rate Training (MERT", "start_pos": 64, "end_pos": 97, "type": "METRIC", "confidence": 0.7577827225128809}]}, {"text": "These two techniques were originally developed for N -best lists of translation hypotheses and recently extended to translation lattices () generated by a phrase-based SMT system ().", "labels": [], "entities": [{"text": "SMT", "start_pos": 168, "end_pos": 171, "type": "TASK", "confidence": 0.8846550583839417}]}, {"text": "Translation lattices contain a significantly higher number of translation alternatives relative to Nbest lists.", "labels": [], "entities": []}, {"text": "The extension to lattices reduces the runtimes for both MERT and MBR, and gives performance improvements from MBR decoding.", "labels": [], "entities": [{"text": "MERT", "start_pos": 56, "end_pos": 60, "type": "DATASET", "confidence": 0.6916627287864685}, {"text": "MBR", "start_pos": 65, "end_pos": 68, "type": "DATASET", "confidence": 0.8580819368362427}]}, {"text": "SMT systems based on synchronous context free grammars (SCFG)) have recently been shown to give competitive performance relative to phrase-based SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.986066997051239}, {"text": "phrase-based SMT", "start_pos": 132, "end_pos": 148, "type": "TASK", "confidence": 0.5668157935142517}]}, {"text": "For these systems, a hypergraph or packed forest provides a compact representation for encoding a huge number of translation hypotheses.", "labels": [], "entities": []}, {"text": "In this paper, we extend MERT and MBR decoding to work on hypergraphs produced by SCFG-based MT systems.", "labels": [], "entities": [{"text": "MERT", "start_pos": 25, "end_pos": 29, "type": "METRIC", "confidence": 0.8274111747741699}, {"text": "MT", "start_pos": 93, "end_pos": 95, "type": "TASK", "confidence": 0.6624496579170227}]}, {"text": "We present algorithms that are more efficient relative to the lattice algorithms presented in.", "labels": [], "entities": []}, {"text": "Lattice MBR decoding uses a linear approximation to the BLEU score); the weights in this linear loss are set heuristically by assuming that n-gram precisions decay exponentially with n.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 56, "end_pos": 66, "type": "METRIC", "confidence": 0.9661081433296204}]}, {"text": "However, this may not be optimal in practice.", "labels": [], "entities": []}, {"text": "We employ MERT to select these weights by optimizing BLEU score on a development set.", "labels": [], "entities": [{"text": "MERT", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9779601693153381}, {"text": "BLEU score", "start_pos": 53, "end_pos": 63, "type": "METRIC", "confidence": 0.9800625145435333}]}, {"text": "A related MBR-inspired approach for hypergraphs was developed by.", "labels": [], "entities": []}, {"text": "In this work, hypergraphs were rescored to maximize the expected count of synchronous constituents in the translation.", "labels": [], "entities": []}, {"text": "In contrast, our MBR algorithm directly selects the hypothesis in the hypergraph with the maximum expected approximate corpus BLEU score (.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 126, "end_pos": 136, "type": "METRIC", "confidence": 0.9495133757591248}]}], "datasetContent": [{"text": "We now describe our experiments to evaluate MERT and MBR on lattices and hypergraphs, and show how MERT can be used to tune MBR parameters.", "labels": [], "entities": [{"text": "MERT", "start_pos": 44, "end_pos": 48, "type": "METRIC", "confidence": 0.690828263759613}]}], "tableCaptions": [{"text": " Table 1: Statistics over the NIST dev/test sets.", "labels": [], "entities": [{"text": "NIST dev/test sets", "start_pos": 30, "end_pos": 48, "type": "DATASET", "confidence": 0.9577934741973877}]}, {"text": " Table 2: Average time for computing envelopes.", "labels": [], "entities": [{"text": "Average time", "start_pos": 10, "end_pos": 22, "type": "METRIC", "confidence": 0.946841299533844}]}, {"text": " Table 3: Lattice MBR for a phrase-based system.", "labels": [], "entities": [{"text": "MBR", "start_pos": 18, "end_pos": 21, "type": "METRIC", "confidence": 0.3627307415008545}]}, {"text": " Table 4: Hypergraph MBR for Hiero/SAMT systems.", "labels": [], "entities": [{"text": "Hypergraph MBR", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.5774300843477249}, {"text": "Hiero/SAMT", "start_pos": 29, "end_pos": 39, "type": "DATASET", "confidence": 0.6661669611930847}]}, {"text": " Table 5: MBR Parameter Tuning on NIST systems", "labels": [], "entities": [{"text": "MBR Parameter Tuning", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.5086832741896311}, {"text": "NIST", "start_pos": 34, "end_pos": 38, "type": "DATASET", "confidence": 0.9464582204818726}]}, {"text": " Table 6: MBR on Multi-language systems.", "labels": [], "entities": []}]}