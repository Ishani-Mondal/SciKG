{"title": [{"text": "Transfer Learning, Feature Selection and Word Sense Disambguation", "labels": [], "entities": [{"text": "Transfer Learning", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9237622320652008}, {"text": "Feature Selection", "start_pos": 19, "end_pos": 36, "type": "TASK", "confidence": 0.7287781089544296}, {"text": "Word Sense Disambguation", "start_pos": 41, "end_pos": 65, "type": "TASK", "confidence": 0.6647263566652933}]}], "abstractContent": [{"text": "We propose a novel approach for improving Feature Selection for Word Sense Dis-ambiguation by incorporating a feature relevance prior for each word indicating which features are more likely to be selected.", "labels": [], "entities": [{"text": "Feature Selection", "start_pos": 42, "end_pos": 59, "type": "TASK", "confidence": 0.7127495110034943}, {"text": "Word Sense Dis-ambiguation", "start_pos": 64, "end_pos": 90, "type": "TASK", "confidence": 0.6596767405668894}]}, {"text": "We use transfer of knowledge from similar words to learn this prior over the features, which permits us to learn higher accuracy models, particularly for the rarer word senses.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 120, "end_pos": 128, "type": "METRIC", "confidence": 0.9835440516471863}]}, {"text": "Results on the ONTONOTES verb data show significant improvement over the baseline feature selection algorithm and results that are comparable to or better than other state-of-the-art methods.", "labels": [], "entities": [{"text": "ONTONOTES verb data", "start_pos": 15, "end_pos": 34, "type": "DATASET", "confidence": 0.7856029073397318}]}], "introductionContent": [{"text": "The task of WSD has been mostly studied in a supervised learning setting e.g. () and feature selection has always been an important component of high accuracy word sense disambiguation, as one often has thousands of features but only hundreds of observations of the words).", "labels": [], "entities": [{"text": "WSD", "start_pos": 12, "end_pos": 15, "type": "TASK", "confidence": 0.9831916689872742}, {"text": "word sense disambiguation", "start_pos": 159, "end_pos": 184, "type": "TASK", "confidence": 0.5580165187517802}]}, {"text": "The main problem that arises with supervised WSD techniques, including ones that do feature selection, is the paucity of labeled data.", "labels": [], "entities": [{"text": "WSD", "start_pos": 45, "end_pos": 48, "type": "TASK", "confidence": 0.973762571811676}]}, {"text": "For example, the training set of SENSEVAL-2 English lexical sample task has only 10 labeled examples per sense, which makes it difficult to build high accuracy models using only supervised learning techniques.", "labels": [], "entities": [{"text": "SENSEVAL-2 English lexical sample task", "start_pos": 33, "end_pos": 71, "type": "TASK", "confidence": 0.5216562807559967}]}, {"text": "It is thus an attractive alternative to use transfer learning (), which improves performance by generalizing from solutions to \"similar\" learning problems.", "labels": [], "entities": []}, {"text": "() (abbreviated as Ando) have successfully applied the ASO (Alternating Structure Optimization) technique proposed by, in its transfer learning configuration, to the problem of WSD by doing joint empirical risk minimization of a set of related problems (words in this case).", "labels": [], "entities": [{"text": "ASO (Alternating Structure Optimization)", "start_pos": 55, "end_pos": 95, "type": "TASK", "confidence": 0.6255634973446528}, {"text": "WSD", "start_pos": 177, "end_pos": 180, "type": "TASK", "confidence": 0.9617963433265686}]}, {"text": "In this paper, we show how a novel form of transfer learning that learns a feature relevance prior from similar word senses, aids in the process of feature selection and hence benefits the task of WSD.", "labels": [], "entities": [{"text": "feature selection", "start_pos": 148, "end_pos": 165, "type": "TASK", "confidence": 0.6936206072568893}, {"text": "WSD", "start_pos": 197, "end_pos": 200, "type": "TASK", "confidence": 0.9745957851409912}]}, {"text": "Feature selection algorithms usually put a uniform prior over the features.", "labels": [], "entities": [{"text": "Feature selection", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7983501851558685}]}, {"text": "I.e., they consider each feature to have the same probability of being selected.", "labels": [], "entities": []}, {"text": "In this paper we relax this overly simplistic assumption by transferring a prior for feature relevance of a given word sense from \"similar\" word senses.", "labels": [], "entities": []}, {"text": "Learning this prior for feature relevance of a test word sense makes those features that have been selected in the models of other \"similar\" word senses become more likely to be selected.", "labels": [], "entities": []}, {"text": "We learn the feature relevance prior only from distributionally similar word senses, rather than \"all\" senses of each word, as it is difficult to find words which are similar in \"all\" the senses.", "labels": [], "entities": []}, {"text": "We can, however, often find words which have one or a few similar senses.", "labels": [], "entities": []}, {"text": "For example, one sense of \"fire\" (as in \"fire someone\") should share features with one sense of \"dismiss\" (as in \"dismiss someone\"), but other senses of \"fire\" (as in \"fire the gun\") do not.", "labels": [], "entities": []}, {"text": "Similarly, other meanings of \"dismiss\" (as in \"dismiss an idea\") should not share features with \"fire\".", "labels": [], "entities": []}, {"text": "As just mentioned, knowledge can only be fruitfully transfered between the shared senses of different words, even though the models being learned are for disambiguating different senses of a single word.", "labels": [], "entities": []}, {"text": "To address this problem, we cluster similar word senses of different words, and then use the models learned for all but one of the word senses in the cluster (called the \"training word senses\") to put a feature relevance prior on which features will be more predictive for the held out test word sense.", "labels": [], "entities": []}, {"text": "We holdout each word sense in the cluster once and learn a prior from the remaining word senses in that cluster.", "labels": [], "entities": []}, {"text": "For example, we can use the models for discriminating the senses of the words \"kill\" and the senses of \"capture\", to put a prior on what features should be included in a model to disambiguate corresponding senses of the distributionally similar word \"arrest\".", "labels": [], "entities": []}, {"text": "The remainder of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2 we describe our \"baseline\" information theoretic feature selection method, and extend it to our \"TRANSFEAT\" method.", "labels": [], "entities": [{"text": "information theoretic feature selection", "start_pos": 40, "end_pos": 79, "type": "TASK", "confidence": 0.6121217533946037}]}, {"text": "Section 3 contains experimental results comparing TRANS-FEAT with the baseline and Ando on ONTONOTES data.", "labels": [], "entities": [{"text": "TRANS-FEAT", "start_pos": 50, "end_pos": 60, "type": "METRIC", "confidence": 0.7623400688171387}, {"text": "ONTONOTES data", "start_pos": 91, "end_pos": 105, "type": "DATASET", "confidence": 0.8060311675071716}]}, {"text": "We conclude with a brief summary in Section 4.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we present the experimental results of TRANSFEAT on ONTONOTES data.", "labels": [], "entities": [{"text": "TRANSFEAT", "start_pos": 55, "end_pos": 64, "type": "DATASET", "confidence": 0.48181045055389404}, {"text": "ONTONOTES data", "start_pos": 68, "end_pos": 82, "type": "DATASET", "confidence": 0.8430674970149994}]}], "tableCaptions": [{"text": " Table 1: 10-fold CV (microaveraged) accuracies  of various methods for various Transfer Learning  settings. Note: These are true cross-validation ac- curacies; No parameters have been tuned on them.", "labels": [], "entities": [{"text": "Transfer Learning", "start_pos": 80, "end_pos": 97, "type": "TASK", "confidence": 0.9194446802139282}]}]}