{"title": [], "abstractContent": [{"text": "We investigate the task of unsupervised constituency parsing from bilingual parallel corpora.", "labels": [], "entities": [{"text": "constituency parsing", "start_pos": 40, "end_pos": 60, "type": "TASK", "confidence": 0.6753357201814651}]}, {"text": "Our goal is to use bilingual cues to learn improved parsing models for each language and to evaluate these models on held-out monolingual test data.", "labels": [], "entities": []}, {"text": "We formulate a generative Bayesian model which seeks to explain the observed parallel data through a combination of bilingual and monolingual parameters.", "labels": [], "entities": []}, {"text": "To this end, we adapt a formalism known as un-ordered tree alignment to our probabilistic setting.", "labels": [], "entities": [{"text": "un-ordered tree alignment", "start_pos": 43, "end_pos": 68, "type": "TASK", "confidence": 0.7393710215886434}]}, {"text": "Using this formalism, our model loosely binds parallel trees while allowing language-specific syntactic structure.", "labels": [], "entities": []}, {"text": "We perform inference under this model using Markov Chain Monte Carlo and dynamic programming.", "labels": [], "entities": []}, {"text": "Applying this model to three parallel corpora (Korean-English, Urdu-English, and Chinese-English) we find substantial performance gains over the CCM model, a strong monolingual baseline.", "labels": [], "entities": []}, {"text": "On average, across a variety of testing scenarios, our model achieves an 8.8 absolute gain in F-measure.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 94, "end_pos": 103, "type": "METRIC", "confidence": 0.9930336475372314}]}], "introductionContent": [{"text": "In this paper we investigate the task of unsupervised constituency parsing when bilingual parallel text is available.", "labels": [], "entities": [{"text": "constituency parsing", "start_pos": 54, "end_pos": 74, "type": "TASK", "confidence": 0.7759794890880585}]}, {"text": "Our goal is to improve parsing performance on monolingual test data for each language by using unsupervised bilingual cues at training time.", "labels": [], "entities": []}, {"text": "Multilingual learning has been successful for other linguistic induction tasks such as lexicon acquisition, morphological segmentation, and part-of-speech tagging).", "labels": [], "entities": [{"text": "morphological segmentation", "start_pos": 108, "end_pos": 134, "type": "TASK", "confidence": 0.7552184164524078}, {"text": "part-of-speech tagging", "start_pos": 140, "end_pos": 162, "type": "TASK", "confidence": 0.705579936504364}]}, {"text": "We focus hereon the unsupervised induction of unlabeled constituency brackets.", "labels": [], "entities": []}, {"text": "This task has been extensively studied in a monolingual setting and has proven to be difficult).", "labels": [], "entities": []}, {"text": "The key premise of our approach is that ambiguous syntactic structures in one language may correspond to less uncertain structures in the other language.", "labels": [], "entities": []}, {"text": "For instance, the English sentence I saw [the student] exhibits the classic problem of PP-attachment ambiguity.", "labels": [], "entities": []}, {"text": "However, its Urdu translation, literally glossed as I [[MIT of ] student] saw, uses a genitive phrase that may only be attached to the adjacent noun phrase.", "labels": [], "entities": [{"text": "MIT of ] student] saw", "start_pos": 56, "end_pos": 77, "type": "DATASET", "confidence": 0.915379802385966}]}, {"text": "Knowing the correspondence between these sentences should help us resolve the English ambiguity.", "labels": [], "entities": []}, {"text": "One of the main challenges of unsupervised multilingual learning is to exploit cross-lingual patterns discovered in data, while still allowing a wide range of language-specific idiosyncrasies.", "labels": [], "entities": []}, {"text": "To this end, we adapt a formalism known as unordered tree alignment () to a probabilistic setting.", "labels": [], "entities": []}, {"text": "Under this formalism, any two trees can be embedded in an alignment tree.", "labels": [], "entities": []}, {"text": "This alignment tree allows arbitrary parts of the two trees to diverge in structure, permitting language-specific grammatical structure to be preserved.", "labels": [], "entities": []}, {"text": "Additionally, a computational advantage of this formalism is that the marginalized probability overall possible alignments for any two trees can be efficiently computed with a dynamic program in linear time.", "labels": [], "entities": []}, {"text": "We formulate a generative Bayesian model which seeks to explain the observed parallel data through a combination of bilingual and monolingual parameters.", "labels": [], "entities": []}, {"text": "Our model views each pair of sentences as having been generated as follows: First an alignment tree is drawn.", "labels": [], "entities": []}, {"text": "Each node in this alignment tree contains either a solitary monolingual constituent or a pair of coupled bilingual constituents.", "labels": [], "entities": []}, {"text": "For each solitary mono-lingual constituent, a sequence of part-of-speech tags is drawn from a language-specific distribution.", "labels": [], "entities": []}, {"text": "For each pair of coupled bilingual constituents, a pair of part-of-speech sequences are drawn jointly from a cross-lingual distribution.", "labels": [], "entities": []}, {"text": "Word-level alignments are then drawn based on the tree alignment.", "labels": [], "entities": []}, {"text": "Finally, parallel sentences are assembled from these generated part-of-speech sequences and word-level alignments.", "labels": [], "entities": []}, {"text": "To perform inference under this model, we use a Metropolis-Hastings within-Gibbs sampler.", "labels": [], "entities": []}, {"text": "We sample pairs of trees and then compute marginalized probabilities overall possible alignments using dynamic programming.", "labels": [], "entities": []}, {"text": "We test the effectiveness of our bilingual grammar induction model on three corpora of parallel text: English-Korean, English-Urdu and EnglishChinese.", "labels": [], "entities": []}, {"text": "The model is trained using bilingual data with automatically induced word-level alignments, but is tested on purely monolingual data for each language.", "labels": [], "entities": []}, {"text": "In all cases, our model outperforms a state-of-the-art baseline: the Constituent Context Model (CCM)), sometimes by substantial margins.", "labels": [], "entities": []}, {"text": "On average, overall the testing scenarios that we studied, our model achieves an absolute increase in F-measure of 8.8 points, and a 19% reduction in error relative to a theoretical upper bound.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 102, "end_pos": 111, "type": "METRIC", "confidence": 0.9995111227035522}, {"text": "error", "start_pos": 150, "end_pos": 155, "type": "METRIC", "confidence": 0.990105926990509}]}], "datasetContent": [{"text": "We test our model on three corpora of bilingual parallel sentences: English-Korean, EnglishUrdu, and English-Chinese.", "labels": [], "entities": []}, {"text": "Though the model is trained using parallel data, during testing it has access only to monolingual data.", "labels": [], "entities": []}, {"text": "This set-up ensures that we are testing our model's ability to learn better parameters at training time, rather than its ability to exploit parallel data attest time., we restrict our model to binary trees, though we note that the alignment trees do not follow this restriction.", "labels": [], "entities": []}, {"text": "Data The Penn Korean Treebank () consists of 5,083 Korean sentences translated into English for the purposes of language training in a military setting.", "labels": [], "entities": [{"text": "Penn Korean Treebank", "start_pos": 9, "end_pos": 29, "type": "DATASET", "confidence": 0.9860321879386902}]}, {"text": "Both the Korean and English sentences are annotated with syntactic trees.", "labels": [], "entities": []}, {"text": "We use the first 4,000 sentences for training and the last 1,083 sentences for testing.", "labels": [], "entities": []}, {"text": "We note that in the Korean data, a separate tag is given for each morpheme.", "labels": [], "entities": [{"text": "Korean data", "start_pos": 20, "end_pos": 31, "type": "DATASET", "confidence": 0.9021715819835663}]}, {"text": "We simply concatenate all the morpheme tags given for each word and treat the concatenation as a single tag.", "labels": [], "entities": []}, {"text": "This procedure results in 199 different tags.", "labels": [], "entities": []}, {"text": "The English-Urdu parallel corpus consists of 4,325 sentences from the first three sections of the Penn Treebank and their Urdu translations annotated at the part-of-speech level.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 98, "end_pos": 111, "type": "DATASET", "confidence": 0.9864590167999268}]}, {"text": "The Urdu side of this corpus does not provide tree annotations so here we can test parse accuracy only on English.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 89, "end_pos": 97, "type": "METRIC", "confidence": 0.9694775342941284}]}, {"text": "We use the remaining sections of the Penn Treebank for English testing.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 37, "end_pos": 50, "type": "DATASET", "confidence": 0.9931308925151825}, {"text": "English testing", "start_pos": 55, "end_pos": 70, "type": "TASK", "confidence": 0.5883755087852478}]}, {"text": "The English-Chinese treebank ( consists of 3,850 Chinese newswire sentences translated into English.", "labels": [], "entities": [{"text": "English-Chinese treebank", "start_pos": 4, "end_pos": 28, "type": "DATASET", "confidence": 0.7488189339637756}]}, {"text": "Both the English and Chinese sentences are annotated with parse trees.", "labels": [], "entities": []}, {"text": "We use the first 4/5 for training and the final 1/5 for testing.", "labels": [], "entities": []}, {"text": "During preprocessing of the corpora we remove all punctuation marks and special symbols, following the setup in previous grammar induction work ().", "labels": [], "entities": []}, {"text": "To obtain lexical alignments between the parallel sentences we employ GIZA++.", "labels": [], "entities": []}, {"text": "We use intersection alignments, which are one-to-one alignments produced by taking the intersection of oneto-many alignments in each direction.", "labels": [], "entities": []}, {"text": "These oneto-one intersection alignments tend to have higher precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 60, "end_pos": 69, "type": "METRIC", "confidence": 0.9986178874969482}]}, {"text": "We initialize the trees by making uniform split decisions recursively from the top down for sentences in both languages.", "labels": [], "entities": []}, {"text": "Then for each pair of parallel sentences we randomly sample an initial alignment tree for the two sampled trees.", "labels": [], "entities": []}, {"text": "Baseline We implement a Bayesian version of the CCM as a baseline.", "labels": [], "entities": []}, {"text": "This model uses the same inference procedure as our bilingual model (Gibbs sampling).", "labels": [], "entities": []}, {"text": "In fact, our model reduces to this Bayesian CCM when it is assumed that no nodes between the two parallel trees are ever aligned and when word-level alignments are ignored.", "labels": [], "entities": []}, {"text": "We also reimplemented the original EM version of CCM and found virtually no difference in performance when using EM or Gibbs sampling.", "labels": [], "entities": []}, {"text": "In both cases our implementation achieves F-measure in the range of 69-70% on WSJ10, broadly inline with the performance reported by.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 42, "end_pos": 51, "type": "METRIC", "confidence": 0.9989054203033447}, {"text": "WSJ10", "start_pos": 78, "end_pos": 83, "type": "DATASET", "confidence": 0.9773678779602051}]}, {"text": "Hyperparameters Klein (2005) reports using smoothing pseudo-counts of 2 for constituent: The F-measure of the CCM baseline (dotted line) and bilingual model (solid line) plotted on the y-axis, as the maximum sentence length in the test set is increased (x-axis).", "labels": [], "entities": [{"text": "F-measure", "start_pos": 93, "end_pos": 102, "type": "METRIC", "confidence": 0.9971940517425537}]}, {"text": "Results are averaged overall training scenarios given in yields and contexts and 8 for distituent yields and contexts.", "labels": [], "entities": []}, {"text": "In our Bayesian model, these similar smoothing counts occur as the parameters of the Dirichlet priors.", "labels": [], "entities": []}, {"text": "For Korean we found that the baseline performed well using these values.", "labels": [], "entities": []}, {"text": "However, on our English and Chinese data, we found that somewhat higher smoothing values worked best, so we utilized values of 20 and 80 for constituent and distituent smoothing counts, respectively.", "labels": [], "entities": []}, {"text": "Our model additionally requires hyperparameter values for \u03c9 (the coupling distribution for aligned yields), Gz pair and Gz node (the distributions over Giza-scores for aligned nodes and unaligned nodes, respectively).", "labels": [], "entities": []}, {"text": "For \u03c9 we used asymmetric Dirichlet prior with parameter 1.", "labels": [], "entities": []}, {"text": "For Gz pair and Gz node , in order to create a strong bias towards high Giza-scores, we used non-symmetric Dirichlet priors.", "labels": [], "entities": []}, {"text": "In both cases, we capped the absolute value of the scores at 3, to prevent count sparsity.", "labels": [], "entities": []}, {"text": "In the case of Gz pair we gave pseudocounts of 1,000 for negative values and zero, and pseudo-counts of 1,000,000 for positive scores.", "labels": [], "entities": []}, {"text": "For Gz node we gave a pseudo-count of 1,000,000 fora score of zero, and 1,000 for all negative scores.", "labels": [], "entities": []}, {"text": "This very strong prior bias encodes our intuition that syntactic alignments which respect lexical alignments should be preferred.", "labels": [], "entities": []}, {"text": "Our method is not sensitive to these exact values and any reasonably strong bias gave similar results.", "labels": [], "entities": []}, {"text": "In all our experiments, we consider the hyperparameters fixed and observed values.", "labels": [], "entities": []}, {"text": "Testing and evaluation As mentioned above, we test our model only on monolingual data, where the parallel sentences are not provided to the model.", "labels": [], "entities": []}, {"text": "To predict the bracketings of these monolingual test sentences, we take the smoothed counts accumulated in the final round of sampling over the training data and perform a maximum likelihood estimate of the monolingual CCM parameters.", "labels": [], "entities": []}, {"text": "These parameters are then used to produce the highest probability bracketing of the test set.", "labels": [], "entities": []}, {"text": "To evaluate both our model as well as the baseline, we use (unlabeled) bracket precision, recall, and F-measure (.", "labels": [], "entities": [{"text": "precision", "start_pos": 79, "end_pos": 88, "type": "METRIC", "confidence": 0.6243162155151367}, {"text": "recall", "start_pos": 90, "end_pos": 96, "type": "METRIC", "confidence": 0.9993488192558289}, {"text": "F-measure", "start_pos": 102, "end_pos": 111, "type": "METRIC", "confidence": 0.9971985816955566}]}, {"text": "Following previous work, we include the wholesentence brackets but ignore single-word brackets.", "labels": [], "entities": []}, {"text": "We perform experiments on different subsets of training and testing data based on the sentencelength.", "labels": [], "entities": []}, {"text": "In particular we experimented with sentence length limits of 10, 20, and 30 for both the training and testing sets.", "labels": [], "entities": []}, {"text": "We also report the upper bound on F-measure for binary trees.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 34, "end_pos": 43, "type": "METRIC", "confidence": 0.981300950050354}]}, {"text": "We average the results over 10 separate sampling runs.", "labels": [], "entities": []}, {"text": "reports the full results of our experiments.", "labels": [], "entities": []}, {"text": "In all testing scenarios the bilingual model outperforms its monolingual counterpart in terms of both precision and recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 102, "end_pos": 111, "type": "METRIC", "confidence": 0.9994996786117554}, {"text": "recall", "start_pos": 116, "end_pos": 122, "type": "METRIC", "confidence": 0.9979761242866516}]}, {"text": "On average, the bilingual model gains 10.2 percentage points in precision, 7.7 in recall, and 8.8 in F-measure.", "labels": [], "entities": [{"text": "precision", "start_pos": 64, "end_pos": 73, "type": "METRIC", "confidence": 0.999710738658905}, {"text": "recall", "start_pos": 82, "end_pos": 88, "type": "METRIC", "confidence": 0.9996533393859863}, {"text": "F-measure", "start_pos": 101, "end_pos": 110, "type": "METRIC", "confidence": 0.9954663515090942}]}, {"text": "The gap between monolingual performance and the binary tree upper bound is reduced by over 19%.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Unlabeled precision, recall and F-measure for the monolingual baseline and the bilingual model  on several test sets. We report results for different combinations of maximum sentence length in both the  training and test sets. The right most column, in all cases, contains the maximum F-measure achievable  using binary trees. The best performance for each test-length is highlighted in bold.", "labels": [], "entities": [{"text": "precision", "start_pos": 20, "end_pos": 29, "type": "METRIC", "confidence": 0.982136070728302}, {"text": "recall", "start_pos": 31, "end_pos": 37, "type": "METRIC", "confidence": 0.9993973970413208}, {"text": "F-measure", "start_pos": 42, "end_pos": 51, "type": "METRIC", "confidence": 0.9927335977554321}]}]}