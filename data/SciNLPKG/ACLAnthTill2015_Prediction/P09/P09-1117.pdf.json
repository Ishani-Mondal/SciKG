{"title": [{"text": "Semi-Supervised Active Learning for Sequence Labeling", "labels": [], "entities": [{"text": "Sequence Labeling", "start_pos": 36, "end_pos": 53, "type": "TASK", "confidence": 0.9452232718467712}]}], "abstractContent": [{"text": "While Active Learning (AL) has already been shown to markedly reduce the annotation efforts for many sequence labeling tasks compared to random selection, AL remains unconcerned about the internal structure of the selected sequences (typ-ically, sentences).", "labels": [], "entities": []}, {"text": "We propose a semi-supervised AL approach for sequence labeling where only highly uncertain sub-sequences are presented to human anno-tators, while all others in the selected sequences are automatically labeled.", "labels": [], "entities": [{"text": "sequence labeling", "start_pos": 45, "end_pos": 62, "type": "TASK", "confidence": 0.6892500817775726}]}, {"text": "For the task of entity recognition, our experiments reveal that this approach reduces annotation efforts in terms of manually labeled tokens by up to 60 % compared to the standard , fully supervised AL scheme.", "labels": [], "entities": [{"text": "entity recognition", "start_pos": 16, "end_pos": 34, "type": "TASK", "confidence": 0.8192378282546997}]}], "introductionContent": [{"text": "Supervised machine learning (ML) approaches are currently the methodological backbone for lots of NLP activities.", "labels": [], "entities": [{"text": "Supervised machine learning (ML)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7256011813879013}]}, {"text": "Despite their success they create a costly follow-up problem, viz.", "labels": [], "entities": []}, {"text": "the need for human annotators to supply large amounts of \"golden\" annotation data on which ML systems can be trained.", "labels": [], "entities": [{"text": "ML", "start_pos": 91, "end_pos": 93, "type": "TASK", "confidence": 0.9730990529060364}]}, {"text": "In most annotation campaigns, the language material chosen for manual annotation is selected randomly from some reference corpus.", "labels": [], "entities": []}, {"text": "Active Learning (AL) has recently shaped as a much more efficient alternative for the creation of precious training material.", "labels": [], "entities": [{"text": "Active Learning (AL)", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.6781404078006744}]}, {"text": "In the AL paradigm, only examples of high training utility are selected for manual annotation in an iterative manner.", "labels": [], "entities": []}, {"text": "Different approaches to AL have been successfully applied to a wide range of NLP tasks.", "labels": [], "entities": [{"text": "AL", "start_pos": 24, "end_pos": 26, "type": "TASK", "confidence": 0.975513219833374}]}, {"text": "When used for sequence labeling tasks such as POS tagging, chunking, or named entity recognition (NER), the examples selected by AL are sequences of text, typically sentences.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 46, "end_pos": 57, "type": "TASK", "confidence": 0.8311179280281067}, {"text": "named entity recognition (NER)", "start_pos": 72, "end_pos": 102, "type": "TASK", "confidence": 0.8082587718963623}]}, {"text": "Approaches to AL for sequence labeling are usually unconcerned about the internal structure of the selected sequences.", "labels": [], "entities": [{"text": "sequence labeling", "start_pos": 21, "end_pos": 38, "type": "TASK", "confidence": 0.6142829656600952}]}, {"text": "Although a high overall training utility might be attributed to a sequence as a whole, the subsequences it is composed of tend to exhibit different degrees of training utility.", "labels": [], "entities": []}, {"text": "In the NER scenario, e.g., large portions of the text do not contain any target entity mention at all.", "labels": [], "entities": []}, {"text": "To further exploit this observation for annotation purposes, we here propose an approach to AL where human annotators are required to label only uncertain subsequences within the selected sentences, while the remaining subsequences are labeled automatically based on the model available from the previous AL iteration round.", "labels": [], "entities": [{"text": "AL", "start_pos": 92, "end_pos": 94, "type": "TASK", "confidence": 0.9698787331581116}]}, {"text": "The hardness of subsequences is characterized by the classifier's confidence in the predicted labels.", "labels": [], "entities": []}, {"text": "Accordingly, our approach is a combination of AL and self-training to which we will refer as semi-supervised Active Learning (SeSAL) for sequence labeling.", "labels": [], "entities": [{"text": "AL", "start_pos": 46, "end_pos": 48, "type": "METRIC", "confidence": 0.9301520586013794}, {"text": "sequence labeling", "start_pos": 137, "end_pos": 154, "type": "TASK", "confidence": 0.5946860611438751}]}, {"text": "While self-training and other bootstrapping approaches often fail to produce good results on NLP tasks due to an inherent tendency of deteriorated data quality, SeSAL circumvents this problem and still yields large savings in terms annotation decisions, i.e., tokens to be manually labeled, compared to a standard, fully supervised AL approach.", "labels": [], "entities": []}, {"text": "After a brief overview of the formal underpinnings of Conditional Random Fields, our base classifier for sequence labeling tasks (Section 2), a fully supervised approach to AL for sequence labeling is introduced and complemented by our semi-supervised approach in Section 3.", "labels": [], "entities": []}, {"text": "In Section 4, we discuss SeSAL in relation to bootstrapping and existing AL techniques.", "labels": [], "entities": [{"text": "SeSAL", "start_pos": 25, "end_pos": 30, "type": "TASK", "confidence": 0.8216355443000793}]}, {"text": "Our experiments are laid out in Section 5 where we compare fully and semi-supervised AL for NER on two corpora, the newspaper selection of MUC7 and PENNBIOIE, a biological abstracts corpus.", "labels": [], "entities": [{"text": "AL", "start_pos": 85, "end_pos": 87, "type": "METRIC", "confidence": 0.9520998597145081}, {"text": "NER", "start_pos": 92, "end_pos": 95, "type": "TASK", "confidence": 0.5813385248184204}, {"text": "newspaper selection of MUC7", "start_pos": 116, "end_pos": 143, "type": "DATASET", "confidence": 0.8449166119098663}, {"text": "PENNBIOIE", "start_pos": 148, "end_pos": 157, "type": "METRIC", "confidence": 0.8741010427474976}]}], "datasetContent": [{"text": "In this section, we turn to the empirical assessment of semi-supervised AL (SeSAL) for sequence labeling on the NLP task of named entity recognition.", "labels": [], "entities": [{"text": "sequence labeling", "start_pos": 87, "end_pos": 104, "type": "TASK", "confidence": 0.6000964790582657}, {"text": "named entity recognition", "start_pos": 124, "end_pos": 148, "type": "TASK", "confidence": 0.57006107767423}]}, {"text": "By the nature of this task, the sequencesin this case, sentences -are only sparsely populated with entity mentions and most of the tokens belong to the OUTSIDE class 3 so that SeSAL can be expected to be very beneficial.", "labels": [], "entities": []}, {"text": "In all experiments, we employ the linear-chain CRF model described in Section 2 as the base learner.", "labels": [], "entities": []}, {"text": "A set of common feature functions was employed, including orthographical (regular expression patterns), lexical and morphological (suffixes/prefixes, lemmatized tokens), and contextual (features of neighboring tokens) ones.", "labels": [], "entities": []}, {"text": "All experiments start from a seed set of 20 randomly selected examples and, in each iteration, 50 new examples are selected using AL.", "labels": [], "entities": [{"text": "AL", "start_pos": 130, "end_pos": 132, "type": "METRIC", "confidence": 0.9221147894859314}]}, {"text": "The efficiency of the different selection mechanisms is determined by learning curves which relate the annotation costs to the performance achieved by the respective model in terms of F 1 -score.", "labels": [], "entities": [{"text": "F 1 -score", "start_pos": 184, "end_pos": 194, "type": "METRIC", "confidence": 0.9751461744308472}]}, {"text": "The unit of annotation costs are manually labeled tokens.", "labels": [], "entities": []}, {"text": "Although the assumption of uniform costs per token has already been subject of legitimate criticism ( , we believe that the number of annotated tokens is still a reasonable approximation in the absence of an empirically more adequate task-specific annotation cost model.", "labels": [], "entities": []}, {"text": "We ran the experiments on two entity-annotated corpora.", "labels": [], "entities": []}, {"text": "From the general-language newspaper domain, we took the training part of the MUC7 corpus) which incorporates seven different entity types, viz.", "labels": [], "entities": [{"text": "MUC7 corpus", "start_pos": 77, "end_pos": 88, "type": "DATASET", "confidence": 0.9785041511058807}]}, {"text": "per- The OUTSIDE class is assigned to each token that does not denote an entity in the underlying domain of discourse.) and removed all but three gene entity subtypes (generic, protein, and rna).", "labels": [], "entities": []}, {"text": "summarizes the quantitative characteristics of both corpora.", "labels": [], "entities": []}, {"text": "The results reported below are averages of 20 independent runs.", "labels": [], "entities": []}, {"text": "For each run, we randomly split each corpus into a pool of unlabeled examples to select from (90 % of the corpus), and a complementary evaluation set (10 % of the corpus).", "labels": [], "entities": []}, {"text": "We compare semi-supervised AL (SeSAL) with its fully supervised counterpart (FuSAL), using a passive learning scheme where examples are randomly selected (RAND) as baseline.", "labels": [], "entities": [{"text": "FuSAL", "start_pos": 77, "end_pos": 82, "type": "METRIC", "confidence": 0.5023650527000427}]}, {"text": "SeSAL is first applied in a default configuration with a very high confidence threshold (t = 0.99) without any delay (d = 0).", "labels": [], "entities": []}, {"text": "In further experiments, these parameters are varied to study their impact on SeSAL's performance.", "labels": [], "entities": [{"text": "SeSAL", "start_pos": 77, "end_pos": 82, "type": "TASK", "confidence": 0.9357824325561523}]}, {"text": "All experiments were run on both the newspaper (MUC7) and biological (PENNBIOIE) corpus.", "labels": [], "entities": [{"text": "newspaper (MUC7) and biological (PENNBIOIE) corpus", "start_pos": 37, "end_pos": 87, "type": "DATASET", "confidence": 0.6865981936454773}]}, {"text": "When results are similar to each other, only one data set will be discussed.", "labels": [], "entities": []}, {"text": "The leading assumption for SeSAL is that only a small portion of tokens within the selected sentences constitute really hard decision problems, while the majority of tokens are easy to account for by the current model.", "labels": [], "entities": [{"text": "SeSAL", "start_pos": 27, "end_pos": 32, "type": "TASK", "confidence": 0.9375191330909729}]}, {"text": "To test this stipulation we investigate the distribution of the model's confidence values C \u03bb (y * j ) overall tokens of the sentences (cf. Equation (9)) selected within one iteration of FuSAL., as an example, depicts the histogram for an early AL iteration round on the MUC7 corpus.", "labels": [], "entities": [{"text": "FuSAL.", "start_pos": 187, "end_pos": 193, "type": "DATASET", "confidence": 0.8161398768424988}, {"text": "MUC7 corpus", "start_pos": 271, "end_pos": 282, "type": "DATASET", "confidence": 0.9856576323509216}]}, {"text": "The vast majority of tokens has a confidence score close to 1, the median lies at 0.9966.", "labels": [], "entities": [{"text": "confidence score", "start_pos": 34, "end_pos": 50, "type": "METRIC", "confidence": 0.969203382730484}]}, {"text": "Histograms of subsequent AL iterations are very similar with an even higher median.", "labels": [], "entities": [{"text": "Histograms", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9581505656242371}]}, {"text": "This is so because the model gets continuously more confident when trained on additional data and fewer hard cases remain in the shrinking pool.", "labels": [], "entities": []}, {"text": "Fully Supervised vs. Semi-Supervised AL.", "labels": [], "entities": []}, {"text": "compares the performance of FuSAL and SeSAL on the two corpora.", "labels": [], "entities": [{"text": "FuSAL", "start_pos": 28, "end_pos": 33, "type": "METRIC", "confidence": 0.6772862672805786}]}, {"text": "SeSAL is run with a delay rate of d = 0 and a very high confidence threshold oft = 0.99 so that only those tokens are automatically labeled on which the current model is almost certain.", "labels": [], "entities": [{"text": "delay rate", "start_pos": 20, "end_pos": 30, "type": "METRIC", "confidence": 0.9538114368915558}]}, {"text": "clearly shows that SeSAL is much more efficient than its fully supervised counterpart.", "labels": [], "entities": [{"text": "SeSAL", "start_pos": 19, "end_pos": 24, "type": "TASK", "confidence": 0.8793533444404602}]}, {"text": "depicts the exact numbers of manually labeled tokens to reach the maximal (supervised) F-score on both corpora.", "labels": [], "entities": [{"text": "F-score", "start_pos": 87, "end_pos": 94, "type": "METRIC", "confidence": 0.9280620217323303}]}, {"text": "FuSAL saves about 50 % compared to RAND, while SeSAL saves about 60 % compared to FuSAL which constitutes an overall saving of over 80 % compared to RAND.", "labels": [], "entities": [{"text": "FuSAL", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8851239085197449}, {"text": "RAND", "start_pos": 35, "end_pos": 39, "type": "DATASET", "confidence": 0.660155177116394}, {"text": "FuSAL", "start_pos": 82, "end_pos": 87, "type": "DATASET", "confidence": 0.805566132068634}, {"text": "RAND", "start_pos": 149, "end_pos": 153, "type": "DATASET", "confidence": 0.8296869397163391}]}, {"text": "These savings are calculated relative to the number of tokens which have to be manually labeled.", "labels": [], "entities": []}, {"text": "Yet, consider the following gedanken experiment.", "labels": [], "entities": []}, {"text": "Assume that, using SeSAL, every second token in a sequence would have to be labeled.", "labels": [], "entities": []}, {"text": "Though this comes to a 'formal' saving of 50 %, the actual annotation effort in terms of the time needed would hardly go down.", "labels": [], "entities": []}, {"text": "It appears that only when SeSAL splits a sentence into larger   well-packaged, chunk-like subsequences annotation time can really be saved.", "labels": [], "entities": []}, {"text": "To demonstrate that SeSAL comes close to this, we counted the number of base noun phrases (NPs) containing one or more tokens to be manually labeled.", "labels": [], "entities": [{"text": "SeSAL", "start_pos": 20, "end_pos": 25, "type": "TASK", "confidence": 0.9146604537963867}]}, {"text": "On the MUC7 corpus, FuSAL requires 7,374 annotated NPs to yield an F-score of 87 %, while SeSAL hit the same F-score with only 4,017 NPs.", "labels": [], "entities": [{"text": "MUC7 corpus", "start_pos": 7, "end_pos": 18, "type": "DATASET", "confidence": 0.9646739065647125}, {"text": "FuSAL", "start_pos": 20, "end_pos": 25, "type": "DATASET", "confidence": 0.6759458780288696}, {"text": "F-score", "start_pos": 67, "end_pos": 74, "type": "METRIC", "confidence": 0.9989883303642273}, {"text": "F-score", "start_pos": 109, "end_pos": 116, "type": "METRIC", "confidence": 0.9959419369697571}]}, {"text": "Thus, also in terms of the number of NPs, SeSAL saves about 45 % of the material to be considered.", "labels": [], "entities": [{"text": "SeSAL", "start_pos": 42, "end_pos": 47, "type": "TASK", "confidence": 0.8484812378883362}]}, {"text": "5 Detailed Analysis of SeSAL.", "labels": [], "entities": []}, {"text": "As reveals, the learning curves of SeSAL stop early (on MUC7 after 12,800 tokens, on PENNBIOIE after 27,600 tokens) because at that point the whole corpus has been labeled exhaustively -either manually, or automatically.", "labels": [], "entities": [{"text": "MUC7", "start_pos": 56, "end_pos": 60, "type": "DATASET", "confidence": 0.8437432646751404}]}, {"text": "So, using SeSAL the complete corpus can be labeled with only a small fraction of it actually being manually annotated (MUC7: about 18 %, PENNBIOIE: about 13 %).: Analysis of SeSAL on MUC7: Manually and automatically labeled tokens, annotation rate (AR) as the portion of manually labeled tokens in the total amount of labeled tokens, errors and accuracy (ACC) of the created corpus.", "labels": [], "entities": [{"text": "PENNBIOIE", "start_pos": 137, "end_pos": 146, "type": "METRIC", "confidence": 0.9538673162460327}, {"text": "annotation rate (AR)", "start_pos": 232, "end_pos": 252, "type": "METRIC", "confidence": 0.9177422165870667}, {"text": "errors", "start_pos": 334, "end_pos": 340, "type": "METRIC", "confidence": 0.9784272909164429}, {"text": "accuracy (ACC)", "start_pos": 345, "end_pos": 359, "type": "METRIC", "confidence": 0.9651412218809128}]}, {"text": "The majority of the automatically labeled tokens (97-98 %) belong to the OUTSIDE class.", "labels": [], "entities": []}, {"text": "This coincides with the assumption that SeSAL works especially well for labeling tasks where some classes occur predominantly and can, inmost cases, easily be discriminated from the other classes, as is the casein the NER scenario.", "labels": [], "entities": [{"text": "labeling tasks", "start_pos": 72, "end_pos": 86, "type": "TASK", "confidence": 0.9043923318386078}]}, {"text": "An analysis of the errors induced by the self-tagging component reveals that most of the errors (90-100 %) are due to missed entity classes, i.e., while the correct class label fora token is one of the entity classes, the OUTSIDE class was assigned.", "labels": [], "entities": []}, {"text": "This effect is more severe in early than in later AL iterations (see for the exact numbers: Distribution of errors of the self-tagging component.", "labels": [], "entities": []}, {"text": "Error types: OUTSIDE class assigned though an entity class is correct (E2O), entity class assigned but OUTSIDE is correct (O2E), wrong entity class assigned (E2E).", "labels": [], "entities": []}, {"text": "Impact of the Confidence Threshold.", "labels": [], "entities": [{"text": "Confidence Threshold", "start_pos": 14, "end_pos": 34, "type": "TASK", "confidence": 0.859284520149231}]}, {"text": "We also ran SeSAL with different confidence thresholds t (0.99, 0.95, 0.90, and 0.70) and analyzed the results with respect to tagging errors and the model performance.", "labels": [], "entities": []}, {"text": "shows the learning and error curves for different thresholds on the MUC7 corpus.", "labels": [], "entities": [{"text": "MUC7 corpus", "start_pos": 68, "end_pos": 79, "type": "DATASET", "confidence": 0.9668086469173431}]}, {"text": "The supervised F-score of 87.7 % is only reached by the highest and most restrictive threshold oft = 0.99.", "labels": [], "entities": [{"text": "F-score", "start_pos": 15, "end_pos": 22, "type": "METRIC", "confidence": 0.9207955598831177}]}, {"text": "With all other thresholds, SeSAL    Impact of the Delay Rate.", "labels": [], "entities": [{"text": "SeSAL    Impact", "start_pos": 27, "end_pos": 42, "type": "METRIC", "confidence": 0.6848271489143372}]}, {"text": "We also measured the impact of delay rates on SeSAL's efficiency considering three delay rates (1,000, 5,000, and 10,000 tokens) in combination with three confidence thresholds (0.99, 0.9, and 0.7).", "labels": [], "entities": [{"text": "SeSAL", "start_pos": 46, "end_pos": 51, "type": "TASK", "confidence": 0.9205867052078247}]}, {"text": "depicts the respective learning curves on the MUC7 corpus.", "labels": [], "entities": [{"text": "MUC7 corpus", "start_pos": 46, "end_pos": 57, "type": "DATASET", "confidence": 0.9710349142551422}]}, {"text": "For SeSAL with t = 0.99, the delay  has no particularly beneficial effect.", "labels": [], "entities": [{"text": "SeSAL", "start_pos": 4, "end_pos": 9, "type": "TASK", "confidence": 0.8965129256248474}, {"text": "delay", "start_pos": 29, "end_pos": 34, "type": "METRIC", "confidence": 0.9948900938034058}]}, {"text": "However, in combination with lower thresholds, the delay rates show positive effects as SeSAL yields Fscores closer to the maximal F-score of 87.7 %, thus clearly outperforming undelayed SeSAL.", "labels": [], "entities": [{"text": "delay", "start_pos": 51, "end_pos": 56, "type": "METRIC", "confidence": 0.9876261353492737}, {"text": "Fscores", "start_pos": 101, "end_pos": 108, "type": "METRIC", "confidence": 0.9990518689155579}, {"text": "F-score", "start_pos": 131, "end_pos": 138, "type": "METRIC", "confidence": 0.9857556223869324}]}], "tableCaptions": [{"text": " Table 1: Quantitative characteristics of the chosen corpora", "labels": [], "entities": []}, {"text": " Table 2: Tokens manually labeled to reach the maximal (su- pervised) F-score", "labels": [], "entities": [{"text": "su- pervised) F-score", "start_pos": 56, "end_pos": 77, "type": "METRIC", "confidence": 0.568867415189743}]}, {"text": " Table 3: Analysis of SeSAL on MUC7: Manually and auto- matically labeled tokens, annotation rate (AR) as the portion  of manually labeled tokens in the total amount of labeled to- kens, errors and accuracy (ACC) of the created corpus.", "labels": [], "entities": [{"text": "annotation rate (AR)", "start_pos": 82, "end_pos": 102, "type": "METRIC", "confidence": 0.9472145438194275}, {"text": "accuracy (ACC)", "start_pos": 198, "end_pos": 212, "type": "METRIC", "confidence": 0.9697794616222382}]}, {"text": " Table 4: Distribution of errors of the self-tagging component.", "labels": [], "entities": []}, {"text": " Table 5: Maximum model performance on MUC7 in terms of  F-score (F), recall (R), precision (P) and accuracy (Acc) -the  labeled corpus obtained by SeSAL with different thresholds", "labels": [], "entities": [{"text": "F-score (F)", "start_pos": 57, "end_pos": 68, "type": "METRIC", "confidence": 0.9507258385419846}, {"text": "recall (R)", "start_pos": 70, "end_pos": 80, "type": "METRIC", "confidence": 0.9646809250116348}, {"text": "precision (P)", "start_pos": 82, "end_pos": 95, "type": "METRIC", "confidence": 0.9682779163122177}, {"text": "accuracy (Acc)", "start_pos": 100, "end_pos": 114, "type": "METRIC", "confidence": 0.8742483854293823}]}]}