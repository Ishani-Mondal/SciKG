{"title": [{"text": "Automatic Cost Estimation for Tree Edit Distance Using Particle Swarm Optimization", "labels": [], "entities": [{"text": "Automatic Cost Estimation", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.566023180882136}]}], "abstractContent": [{"text": "Recently, there is a growing interest in working with tree-structured data in different applications and domains such as computational biology and natural language processing.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 147, "end_pos": 174, "type": "TASK", "confidence": 0.6456704139709473}]}, {"text": "Moreover, many applications in computational linguistics require the computation of similarities over pair of syntactic or semantic trees.", "labels": [], "entities": []}, {"text": "In this context, Tree Edit Distance (TED) has been widely used for many years.", "labels": [], "entities": [{"text": "Tree Edit Distance (TED)", "start_pos": 17, "end_pos": 41, "type": "TASK", "confidence": 0.5414346555868784}]}, {"text": "However, one of the main constraints of this method is to tune the cost of edit operations, which makes it difficult or sometimes very challenging in dealing with complex problems.", "labels": [], "entities": []}, {"text": "In this paper, we propose an original method to estimate and optimize the operation costs in TED, applying the Particle Swarm Optimization algorithm.", "labels": [], "entities": [{"text": "Particle Swarm Optimization", "start_pos": 111, "end_pos": 138, "type": "TASK", "confidence": 0.713090717792511}]}, {"text": "Our experiments on Recognizing Textual Entailment show the success of this method in automatic estimation , rather than manual assignment of edit costs.", "labels": [], "entities": [{"text": "Recognizing Textual Entailment", "start_pos": 19, "end_pos": 49, "type": "TASK", "confidence": 0.883272111415863}]}], "introductionContent": [{"text": "Among many tree-based algorithms, Tree Edit Distance (TED) has offered many solutions for various NLP applications such as information retrieval, information extraction, similarity estimation and textual entailment.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 123, "end_pos": 144, "type": "TASK", "confidence": 0.8021811842918396}, {"text": "information extraction", "start_pos": 146, "end_pos": 168, "type": "TASK", "confidence": 0.8421804010868073}, {"text": "similarity estimation", "start_pos": 170, "end_pos": 191, "type": "TASK", "confidence": 0.713871493935585}, {"text": "textual entailment", "start_pos": 196, "end_pos": 214, "type": "TASK", "confidence": 0.7589406967163086}]}, {"text": "Tree edit distance is defined as the minimum costly set of basic operations transforming one tree to another.", "labels": [], "entities": []}, {"text": "In common, TED approaches use an initial fixed cost for each operation.", "labels": [], "entities": [{"text": "TED", "start_pos": 11, "end_pos": 14, "type": "TASK", "confidence": 0.950527548789978}]}, {"text": "Generally, the initial assigned cost to each edit operation depends on the nature of nodes, applications and dataset.", "labels": [], "entities": []}, {"text": "For example the probability of deleting a function word from a string is not the same as deleting a symbol in RNA structure.", "labels": [], "entities": []}, {"text": "According to this fact, tree comparison maybe affected by application and dataset.", "labels": [], "entities": [{"text": "tree comparison", "start_pos": 24, "end_pos": 39, "type": "TASK", "confidence": 0.6664258390665054}]}, {"text": "A solution to this problem is assigning the cost to each edit operation empirically or based on the expert knowledge and recommendation.", "labels": [], "entities": []}, {"text": "These methods emerge a critical problem when the domain, field or application is new and the level of expertise and empirical knowledge is very limited.", "labels": [], "entities": []}, {"text": "Other approaches towards this problem tried to learn a generative or discriminative probabilistic model () from the data.", "labels": [], "entities": [{"text": "generative or discriminative probabilistic", "start_pos": 55, "end_pos": 97, "type": "TASK", "confidence": 0.8263919353485107}]}, {"text": "One of the drawbacks of those approaches is that the cost values of edit operations are hidden behind the probabilistic model.", "labels": [], "entities": []}, {"text": "Additionally, the cost cannot be weighted or varied according to the tree context and node location.", "labels": [], "entities": []}, {"text": "In order to overcome these drawbacks, we are proposing a stochastic method based on Particle Swarm Optimization (PSO) to estimate the cost of each edit operation based on the user defined application and dataset.", "labels": [], "entities": [{"text": "Particle Swarm Optimization (PSO)", "start_pos": 84, "end_pos": 117, "type": "TASK", "confidence": 0.6952289342880249}]}, {"text": "A further advantage of the method, besides automatic learning of the operation costs, is to investigate the cost values in order to better understand how TED approaches the application and data in different domains.", "labels": [], "entities": [{"text": "TED", "start_pos": 154, "end_pos": 157, "type": "TASK", "confidence": 0.939911961555481}]}, {"text": "As for the experiments, we learn a model for recognizing textual entailment, based on TED, where the input is a pair of strings represented as syntactic dependency trees.", "labels": [], "entities": [{"text": "recognizing textual entailment", "start_pos": 45, "end_pos": 75, "type": "TASK", "confidence": 0.7997344334920248}]}, {"text": "Our results illustrate that optimizing the cost of each operation can dramatically affect the accuracy and achieve a better model for recognizing textual entailment.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 94, "end_pos": 102, "type": "METRIC", "confidence": 0.9993537068367004}, {"text": "recognizing textual entailment", "start_pos": 134, "end_pos": 164, "type": "TASK", "confidence": 0.8213775555292765}]}], "datasetContent": [{"text": "Our experiments were conducted on the basis of Recognizing Textual Entailment (RTE) datasets . Textual Entailment can be explained as an association between a coherent text(T) and a language expression, called hypothesis(H).", "labels": [], "entities": [{"text": "Recognizing Textual Entailment (RTE) datasets", "start_pos": 47, "end_pos": 92, "type": "DATASET", "confidence": 0.6384227914469582}]}, {"text": "The entailment function for the pair T-H returns the true value when the meaning of H can be inferred from the meaning of T and false otherwise.", "labels": [], "entities": []}, {"text": "In another word, Textual Entailment can be defined as human reading comprehension task.", "labels": [], "entities": [{"text": "Textual Entailment", "start_pos": 17, "end_pos": 35, "type": "TASK", "confidence": 0.8148191273212433}]}, {"text": "One of the approaches to textual entailment problem is based on the distance between T and H.", "labels": [], "entities": [{"text": "textual entailment problem", "start_pos": 25, "end_pos": 51, "type": "TASK", "confidence": 0.8285185496012369}]}, {"text": "In this approach, the entailment score fora pair is calculated on the minimal set of edit operations that transform T into H.", "labels": [], "entities": []}, {"text": "An entailment relation is assigned to a T-H pair in the case that overall cost of the transformations is below a certain threshold.", "labels": [], "entities": []}, {"text": "The threshold, which corresponds to tree edit 1 http://www.pascal-network.org/Challenges/RTE1-4 distace, is empirically estimated over the dataset.", "labels": [], "entities": []}, {"text": "This method was implemented by), based on TED algorithm.", "labels": [], "entities": [{"text": "TED", "start_pos": 42, "end_pos": 45, "type": "METRIC", "confidence": 0.6330525875091553}]}, {"text": "Each RTE dataset includes its own development and test set, however, RTE-4 was released only as a test set and the data from RTE-1 to RTE-3 were exploited as development set for evaluating RTE-4 data.", "labels": [], "entities": [{"text": "RTE dataset", "start_pos": 5, "end_pos": 16, "type": "DATASET", "confidence": 0.887977123260498}, {"text": "RTE-4", "start_pos": 69, "end_pos": 74, "type": "DATASET", "confidence": 0.8828296661376953}, {"text": "RTE-3", "start_pos": 134, "end_pos": 139, "type": "DATASET", "confidence": 0.48631590604782104}]}, {"text": "In order to deal with TED approach to textual entailment, we used EDITS 2 package (Edit Distance Textual Entailment Suite) ( ).", "labels": [], "entities": []}, {"text": "In addition, We partially exploit JSwarm-PSO 3 package with some adaptations as an implementation of PSO algorithm.", "labels": [], "entities": []}, {"text": "Each pair in the datasets converted to two syntactic dependency trees using Stanford statistical parser , developed in the Stanford university NLP group by.", "labels": [], "entities": []}, {"text": "We conducted six different experiments in two sets on each RTE dataset.", "labels": [], "entities": [{"text": "RTE dataset", "start_pos": 59, "end_pos": 70, "type": "DATASET", "confidence": 0.9272169172763824}]}, {"text": "The costs were estimated on the training set, then we evaluate the estimated costs on the test set.", "labels": [], "entities": []}, {"text": "In the first set of experiments, we set a simple cost scheme based on three operations.", "labels": [], "entities": []}, {"text": "Implementing this cost scheme, we expect to optimize the cost of each edit operation without considering that the operation costs may vary based on different characteristics of anode, such assize, location or content.", "labels": [], "entities": []}, {"text": "The results were obtained using: 1) The random cost assignment, 2) Assigning the cost based on the expertise knowledge and intuition (So called Intuitive), and 3) Automatic estimated and optimized cost for each operation.", "labels": [], "entities": []}, {"text": "In the second case, we applied the same cost values which was used in EDITS by its developers ( ).", "labels": [], "entities": [{"text": "EDITS", "start_pos": 70, "end_pos": 75, "type": "DATASET", "confidence": 0.8177919387817383}]}, {"text": "In the second set of experiments, we tried to take advantage of an advanced cost scheme with more fine-grained operations to assign a weight to the edit operations based on the characteristics of the nodes ( . For example if anode is in the list of stop-words, the deletion cost should be different from the cost of deleting a content word.", "labels": [], "entities": []}, {"text": "By this intuition, we tried to optimize 9 specialized costs for edit operations (A swarm of size 9).", "labels": [], "entities": []}, {"text": "At each experiment, both fitness functions were applied and the best results were chosen for presentation.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Comparison of accuracy on all RTE  datasets based on optimized and unoptimized cost  schemes.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9990947246551514}, {"text": "RTE  datasets", "start_pos": 40, "end_pos": 53, "type": "DATASET", "confidence": 0.7390851676464081}]}, {"text": " Table 1. We show  the accuracy gained by a distance-based base- line for textual entailment", "labels": [], "entities": [{"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9994906187057495}, {"text": "textual entailment", "start_pos": 74, "end_pos": 92, "type": "TASK", "confidence": 0.7714579105377197}]}]}