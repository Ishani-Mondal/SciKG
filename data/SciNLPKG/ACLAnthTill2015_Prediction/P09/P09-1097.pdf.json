{"title": [{"text": "Semantic Tagging of Web Search Queries", "labels": [], "entities": [{"text": "Semantic Tagging of Web Search Queries", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.8818551699320475}]}], "abstractContent": [{"text": "We present a novel approach to parse web search queries for the purpose of automatic tagging of the queries.", "labels": [], "entities": [{"text": "parse web search queries", "start_pos": 31, "end_pos": 55, "type": "TASK", "confidence": 0.8983562290668488}]}, {"text": "We will define a set of probabilistic context-free rules, which generates bags (i.e. multi-sets) of words.", "labels": [], "entities": []}, {"text": "Using this new type of rule in combination with the traditional probabilistic phrase structure rules, we define a hybrid grammar, which treats each search query as a bag of chunks (i.e. phrases).", "labels": [], "entities": []}, {"text": "A hybrid probabilistic parser is used to parse the queries.", "labels": [], "entities": []}, {"text": "In order to take contextual information into account, a discriminative model is used on top of the parser to re-rank the n-best parse trees generated by the parser.", "labels": [], "entities": []}, {"text": "Experiments show that our approach outperforms a basic model, which is based on Conditional Random Fields.", "labels": [], "entities": []}], "introductionContent": [{"text": "Understanding users' intent from web search queries is an important step in designing an intelligent search engine.", "labels": [], "entities": []}, {"text": "While it remains a challenge to have a scientific definition of ''intent'', many efforts have been devoted to automatically mapping queries into different domains i.e. topical classes such as product, job and travel ().", "labels": [], "entities": []}, {"text": "This work goes beyond query-level classification.", "labels": [], "entities": [{"text": "query-level classification", "start_pos": 22, "end_pos": 48, "type": "TASK", "confidence": 0.7772082388401031}]}, {"text": "We assume that the queries are already classified into the correct domain and investigate the problem of semantic tagging at the word level, which is to assign a label from a set of pre-defined semantic labels (specific to the domain) to every word in the query.", "labels": [], "entities": [{"text": "semantic tagging", "start_pos": 105, "end_pos": 121, "type": "TASK", "confidence": 0.7436912953853607}]}, {"text": "For example, a search query in the product domain can be tagged as: Many specialized search engines build their indexes directly from relational databases, which contain highly structured information.", "labels": [], "entities": []}, {"text": "Given a query tagged with the semantic labels, a search engine is able to compare the values of semantic labels in the query (e.g., Brand = \"garmin\") with its counterpart values in documents, thereby providing users with more relevant search results.", "labels": [], "entities": []}, {"text": "Despite this importance, there has been relatively little published work on semantic tagging of web search queries. and study the linguistic structure of queries by performing part-of-speech tagging.", "labels": [], "entities": [{"text": "semantic tagging of web search queries.", "start_pos": 76, "end_pos": 115, "type": "TASK", "confidence": 0.811822642882665}, {"text": "part-of-speech tagging", "start_pos": 176, "end_pos": 198, "type": "TASK", "confidence": 0.6944745481014252}]}, {"text": "use queries as a source of knowledge for extracting prominent attributes for semantic concepts.", "labels": [], "entities": []}, {"text": "On the other hand, there has been much work on extracting structured information from larger text segments, such as addresses, bibliographic citations (), and classified advertisements (), among many others.", "labels": [], "entities": [{"text": "extracting structured information from larger text segments", "start_pos": 47, "end_pos": 106, "type": "TASK", "confidence": 0.7472301551273891}]}, {"text": "The most widely used approaches to these problems have been sequential models including hidden Markov models (HMMs), maximum entropy Markov models (MEMMs), and conditional random fields (CRFs) ( These sequential models, however, are not optimal for processing web search queries for the following reasons..", "labels": [], "entities": []}, {"text": "The first problem is that the global constraints and long distance dependencies on state variables are difficult to capture using sequential models.", "labels": [], "entities": []}, {"text": "Because of this limitation, use a discriminative context-free (phrase structure) grammar for extracting information from semi-structured data and report higher performances over CRFs.", "labels": [], "entities": []}, {"text": "Secondly, sequential models treat the input text as an ordered sequence of words.", "labels": [], "entities": []}, {"text": "A web search query, however, is often formulated by a user as a bag of keywords.", "labels": [], "entities": []}, {"text": "For example, if a user is look-ing for cheap garmin gps, it is possible that the query comes in any ordering of these three words.", "labels": [], "entities": []}, {"text": "We are looking fora model that, once it observes this query, assumes that the other permutations of the words in this query are also likely.", "labels": [], "entities": []}, {"text": "This model should also be able to handle cases where some local orderings have to be fixed as in the query buses from New York City to Boston, where the words in the phrases from New York city and to Boston have to come in the exact order.", "labels": [], "entities": []}, {"text": "The third limitation is that the sequential models treat queries as unstructured (linear) sequences of words.", "labels": [], "entities": []}, {"text": "The study by on Yahoo!", "labels": [], "entities": [{"text": "Yahoo!", "start_pos": 16, "end_pos": 22, "type": "DATASET", "confidence": 0.8987747132778168}]}, {"text": "query logs suggests that web search queries, to some degree, carry an underlying linguistic structure.", "labels": [], "entities": []}, {"text": "As an example, consider a query about finding a local business near some location such as:", "labels": [], "entities": []}], "datasetContent": [{"text": "Our resources area set of 21000 manually labeled queries, a manually designed grammar, a lexicon for every tag (except Type and Attribute), and a set of regular expressions defined for Models and Attributes.", "labels": [], "entities": []}, {"text": "Note that with a grammar similar to the one in figure (6), generating a parse tree from a labeled query is straightforward.", "labels": [], "entities": []}, {"text": "Then the parser is trained on the trees to learn the parameters of the model (probabilities in this case).", "labels": [], "entities": []}, {"text": "We randomly extracted 3000, out of 21000, queries as the test set and used the remaining 18000 for training.", "labels": [], "entities": []}, {"text": "We created training sets with different sizes to evaluate the impact of training data size on tagging performance.", "labels": [], "entities": [{"text": "tagging", "start_pos": 94, "end_pos": 101, "type": "TASK", "confidence": 0.9683570861816406}]}, {"text": "Three modules were used in the evaluation: the CRF-based model 4 , the parser, and the parser plus the SVM-based re-ranking.", "labels": [], "entities": []}, {"text": "shows the learning curve of the word-level F-score for all the three modules.", "labels": [], "entities": [{"text": "F-score", "start_pos": 43, "end_pos": 50, "type": "METRIC", "confidence": 0.8554496169090271}]}, {"text": "As seen in this plot, when there is a small amount of training data, the parser performs better than the CRF module and parser+SVM module performs better than the other two.", "labels": [], "entities": []}, {"text": "With a large amount of training data, the CRF and parser almost have the same performance.", "labels": [], "entities": []}, {"text": "Once again the parser+SVM module outperforms the other two.", "labels": [], "entities": []}, {"text": "These results show that, as expected, the CRF-based model is more dependent on the training data than the parser.", "labels": [], "entities": []}, {"text": "Parser+SVM always performs at least as well as the parser-only module even with a very small set of training data.", "labels": [], "entities": []}, {"text": "This is because the rank given to every parse tree by the parser is used as a feature in the SVM module.", "labels": [], "entities": []}, {"text": "When there is a very small amount of training data, this feature is dominant and the output of the re-reranking module is basically the same as the parser's highest-rank output.", "labels": [], "entities": []}, {"text": "shows the performance of all three modules when the whole training set was used to train the system.", "labels": [], "entities": []}, {"text": "The first three columns in the table show the word-level precision, recall, and F-score; and the last column represents the query level accuracy (a query is considered correct if all the words in the query have been labeled correctly).", "labels": [], "entities": [{"text": "precision", "start_pos": 57, "end_pos": 66, "type": "METRIC", "confidence": 0.9109435081481934}, {"text": "recall", "start_pos": 68, "end_pos": 74, "type": "METRIC", "confidence": 0.9992591738700867}, {"text": "F-score", "start_pos": 80, "end_pos": 87, "type": "METRIC", "confidence": 0.9991723299026489}, {"text": "accuracy", "start_pos": 136, "end_pos": 144, "type": "METRIC", "confidence": 0.7640460133552551}]}, {"text": "There are two rows for the parser+SVM in the table: one for n=2 (i.e. re-ranking the 2-Best trees) and one for n=10.", "labels": [], "entities": []}, {"text": "It is interesting to see that even with the re-ranking of only the first two trees generated by the parser, the difference between the accuracy of the parser+SVM module and the parser-only module is quite significant.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 135, "end_pos": 143, "type": "METRIC", "confidence": 0.9991697072982788}]}, {"text": "Re-ranking with a larger number of trees (n>10) did not increase performance significantly.", "labels": [], "entities": []}], "tableCaptions": []}