{"title": [{"text": "From Extractive to Abstractive Meeting Summaries: Can It Be Done by Sentence Compression?", "labels": [], "entities": [{"text": "Extractive to Abstractive Meeting Summaries", "start_pos": 5, "end_pos": 48, "type": "TASK", "confidence": 0.6586843848228454}]}], "abstractContent": [{"text": "Most previous studies on meeting summariza-tion have focused on extractive summariza-tion.", "labels": [], "entities": []}, {"text": "In this paper, we investigate if we can apply sentence compression to extractive summaries to generate abstractive summaries.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 46, "end_pos": 66, "type": "TASK", "confidence": 0.7257907688617706}]}, {"text": "We use different compression algorithms, including integer linear programming with an additional step of filler phrase detection, a noisy-channel approach using Markovization formulation of grammar rules, as well as human compressed sentences.", "labels": [], "entities": [{"text": "filler phrase detection", "start_pos": 105, "end_pos": 128, "type": "TASK", "confidence": 0.663910448551178}]}, {"text": "Our experiments on the ICSI meeting corpus show that when compared to the abstractive summaries, using sentence compression on the extractive summaries improves their ROUGE scores; however , the best performance is still quite low, suggesting the need of language generation for abstractive summarization.", "labels": [], "entities": [{"text": "ICSI meeting corpus", "start_pos": 23, "end_pos": 42, "type": "DATASET", "confidence": 0.9291069706281027}, {"text": "ROUGE", "start_pos": 167, "end_pos": 172, "type": "METRIC", "confidence": 0.9975244402885437}, {"text": "abstractive summarization", "start_pos": 279, "end_pos": 304, "type": "TASK", "confidence": 0.5161315202713013}]}], "introductionContent": [{"text": "Meeting summaries provide an efficient way for people to browse through the lengthy recordings.", "labels": [], "entities": []}, {"text": "Most current research on meeting summarization has focused on extractive summarization, that is, it extracts important sentences (or dialogue acts) from speech transcripts, either manual transcripts or automatic speech recognition (ASR) output.", "labels": [], "entities": [{"text": "meeting summarization", "start_pos": 25, "end_pos": 46, "type": "TASK", "confidence": 0.9045192301273346}, {"text": "extractive summarization", "start_pos": 62, "end_pos": 86, "type": "TASK", "confidence": 0.6093115210533142}, {"text": "automatic speech recognition (ASR) output", "start_pos": 202, "end_pos": 243, "type": "TASK", "confidence": 0.7843638147626605}]}, {"text": "Various approaches to extractive summarization have been evaluated recently.", "labels": [], "entities": [{"text": "extractive summarization", "start_pos": 22, "end_pos": 46, "type": "TASK", "confidence": 0.9354117214679718}]}, {"text": "Popular unsupervised approaches are maximum marginal relevance (MMR), latent semantic analysis (LSA), and integer programming (.", "labels": [], "entities": [{"text": "latent semantic analysis (LSA)", "start_pos": 70, "end_pos": 100, "type": "TASK", "confidence": 0.7638434767723083}]}, {"text": "Supervised methods include hidden Markov model (HMM), maximum entropy, conditional random fields (CRF), and support vector machines (SVM)).", "labels": [], "entities": []}, {"text": "() used a word based speech summarization approach that utilized dynamic programming to obtain a set of words to maximize a summarization score.", "labels": [], "entities": [{"text": "word based speech summarization", "start_pos": 10, "end_pos": 41, "type": "TASK", "confidence": 0.5711196959018707}, {"text": "summarization", "start_pos": 124, "end_pos": 137, "type": "TASK", "confidence": 0.9609759449958801}]}, {"text": "Most of these summarization approaches aim for selecting the most informative sentences, while less attempt has been made to generate abstractive summaries, or compress the extracted sentences and merge them into a concise summary.", "labels": [], "entities": [{"text": "summarization", "start_pos": 14, "end_pos": 27, "type": "TASK", "confidence": 0.9828999638557434}]}, {"text": "Simply concatenating extracted sentences may not comprise a good summary, especially for spoken documents, since speech transcripts often contain many disfluencies and are redundant.", "labels": [], "entities": []}, {"text": "The following example shows two extractive summary sentences (they are from the same speaker), and part of the abstractive summary that is related to these two extractive summary sentences.", "labels": [], "entities": []}, {"text": "This is an example from the ICSI meeting corpus (see Section 2.1 for more information on the data).", "labels": [], "entities": [{"text": "ICSI meeting corpus", "start_pos": 28, "end_pos": 47, "type": "DATASET", "confidence": 0.9583183526992798}]}, {"text": "Extractive summary sentences: Sent1: um we have to refine the tasks more and more which of course we haven't done at all so far in order to avoid this rephrasing Sent2: and uh my suggestion is of course we we keep the wizard because i think she did a wonderful job Corresponding abstractive summary: the group decided to hire the wizard and continue with the refinement...", "labels": [], "entities": []}, {"text": "In this paper, our goal is to answer the question if we can perform sentence compression on an extractive summary to improve its readability and make it more like an abstractive summary.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 68, "end_pos": 88, "type": "TASK", "confidence": 0.7142855376005173}]}, {"text": "Compressing sentences could be a first step toward our ultimate goal of creating an abstract for spoken documents.", "labels": [], "entities": []}, {"text": "Sentence compression has been widely studied in language processing.", "labels": [], "entities": [{"text": "Sentence compression", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.9362195134162903}]}, {"text": "() learned rewriting rules that indicate which words should be dropped in a given context.", "labels": [], "entities": []}, {"text": "() applied the noisy-channel framework to predict the possibilities of translating a sentence to a shorter word sequence.", "labels": [], "entities": []}, {"text": "() extended the noisy-channel approach and proposed a head-driven Markovization formulation of synchronous contextfree grammar (SCFG) deletion rules.", "labels": [], "entities": []}, {"text": "Unlike these approaches that need a training corpus,) encoded the language model and a variety of linguistic constraints as linear inequalities, and employed the integer programming approach to find a subset of words that maximize an objective function.", "labels": [], "entities": []}, {"text": "Our focus in this paper is not on new compression algorithms, but rather on using compression to bridge the gap of extractive and abstractive summarization.", "labels": [], "entities": []}, {"text": "We use different automatic compression algorithms.", "labels": [], "entities": []}, {"text": "The first one is the integer programming (IP) framework, where we also introduce a filler phrase (FP) detection 261 module based on the Web resources.", "labels": [], "entities": [{"text": "filler phrase (FP) detection 261", "start_pos": 83, "end_pos": 115, "type": "TASK", "confidence": 0.6465362097535815}]}, {"text": "The second one uses the SCFG that considers the grammaticality of the compressed sentences.", "labels": [], "entities": []}, {"text": "Finally, as a comparison, we also use human compression.", "labels": [], "entities": []}, {"text": "All of these compressed sentences are compared to abstractive summaries.", "labels": [], "entities": []}, {"text": "Our experiments using the ICSI meeting corpus show that compressing extractive summaries can improve human readability and the ROUGE scores against the reference abstractive summaries.", "labels": [], "entities": [{"text": "ICSI meeting corpus", "start_pos": 26, "end_pos": 45, "type": "DATASET", "confidence": 0.9531661669413248}, {"text": "ROUGE", "start_pos": 127, "end_pos": 132, "type": "METRIC", "confidence": 0.9979444146156311}]}], "datasetContent": [{"text": "First we perform human evaluation for the compressed sentences.", "labels": [], "entities": []}, {"text": "Again we use the Amazon Mechanical Turk for the subjective evaluation process.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 17, "end_pos": 39, "type": "DATASET", "confidence": 0.9368844032287598}]}, {"text": "For each extractive summary sentence, we asked 10 human subjects to rate the compressed sentences from the three systems, as well as the human compression.", "labels": [], "entities": []}, {"text": "This evaluation was conducted on three meetings, containing 244 sentences in total.", "labels": [], "entities": []}, {"text": "Participants were asked to read the original sentence and assign scores to each of the compressed sentences for its informativeness and grammaticality respectively using a 1 to 5 scale.", "labels": [], "entities": []}, {"text": "An overall score is calculated as the average of the informativeness and grammaticality scores.", "labels": [], "entities": []}, {"text": "For a comparison, we also include the ROUGE-1 Fscores: Human evaluation results.", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 38, "end_pos": 45, "type": "METRIC", "confidence": 0.9830144643783569}, {"text": "Fscores", "start_pos": 46, "end_pos": 53, "type": "METRIC", "confidence": 0.5059515237808228}]}, {"text": "Also shown is the ROUGE-1 (unigram match) F-score of different systems compared to human compression.", "labels": [], "entities": [{"text": "ROUGE-1 (unigram match) F-score", "start_pos": 18, "end_pos": 49, "type": "METRIC", "confidence": 0.7058168947696686}]}, {"text": "We can see from the table that as expected, the human compression yields the best performance on both informativeness and grammaticality.", "labels": [], "entities": []}, {"text": "'FP + IP' and 'Markov (S1)' approaches also achieve satisfying performance under both evaluation metrics.", "labels": [], "entities": []}, {"text": "The relatively low scores for 'Markov (S2)' output are partly due to its low compression rate (see for the length information).", "labels": [], "entities": []}, {"text": "As an example, we show below the compressed sentences from human and systems for the first sentence in the example in Sec 1.", "labels": [], "entities": []}, {"text": "Human: we have to refine the tasks in order to avoid rephrasing Markov (S1): we have to refine the tasks more and more which we haven't done in order to avoid this rephrasing Markov (S2): we have to refine the tasks which we haven't done order to avoid this rephrasing FP + IP: we have to refine the tasks more and more which we haven't done to avoid this rephrasing Since our goal is to answer the question if we can use sentence compression to generate abstractive summaries, we compare the compressed summaries, as well as the original extractive summaries, against the reference abstractive summaries.", "labels": [], "entities": []}, {"text": "The ROUGE-1 results along with the word compression ratio for each compression approach are shown in.", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 4, "end_pos": 11, "type": "METRIC", "confidence": 0.9944403767585754}]}, {"text": "We can see that all of the compression algorithms yield better ROUGE score than the original extractive summaries.", "labels": [], "entities": [{"text": "ROUGE score", "start_pos": 63, "end_pos": 74, "type": "METRIC", "confidence": 0.9805232882499695}]}, {"text": "Take Markov (S2) as an example.", "labels": [], "entities": []}, {"text": "The recall rate dropped only 8% (from the original 66% to 58%) when only 53% words in the extractive summaries are preserved.", "labels": [], "entities": [{"text": "recall rate", "start_pos": 4, "end_pos": 15, "type": "METRIC", "confidence": 0.9868396818637848}]}, {"text": "This demonstrates that it is possible for the current sentence compression systems to greatly condense the extractive summaries while preserving the desirable information, and thus yield summaries that are more like abstractive summaries.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 54, "end_pos": 74, "type": "TASK", "confidence": 0.7296124845743179}]}, {"text": "However, since the abstractive summaries are much shorter than the extractive summaries (even after compression), it is not surprising to seethe low precision results as shown in  the extractive summaries more like abstractive summaries, we conduct an oracle experiment: we compute the ROUGE score for each of the extractive summary sentences (the original sentence or the compressed sentence) against the abstract, and select the sentences with the highest scores until the number of selected words is about the same as that in the abstract.", "labels": [], "entities": [{"text": "precision", "start_pos": 149, "end_pos": 158, "type": "METRIC", "confidence": 0.9512718319892883}, {"text": "ROUGE score", "start_pos": 286, "end_pos": 297, "type": "METRIC", "confidence": 0.9737421572208405}]}, {"text": "The ROUGE results using these selected top sentences are shown in the right part of.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 4, "end_pos": 9, "type": "METRIC", "confidence": 0.9920687675476074}]}, {"text": "There is some difference using all the sentences vs. the top sentences regarding the ranking of different compression algorithms (comparing the two blocks in).", "labels": [], "entities": []}, {"text": "From, we notice significant performance improvement when using the selected sentences to form a summary.", "labels": [], "entities": []}, {"text": "These results indicate that, it maybe possible to convert extractive summaries to abstractive summaries.", "labels": [], "entities": []}, {"text": "On the other hand, this is an oracle result since we compare the extractive summaries to the abstract for sentence selection.", "labels": [], "entities": [{"text": "sentence selection", "start_pos": 106, "end_pos": 124, "type": "TASK", "confidence": 0.7611269354820251}]}, {"text": "In the real scenario, we will need other methods to rank sentences.", "labels": [], "entities": []}, {"text": "Moreover, the current ROUGE score is not very high.", "labels": [], "entities": [{"text": "ROUGE score", "start_pos": 22, "end_pos": 33, "type": "METRIC", "confidence": 0.9546388685703278}]}, {"text": "This suggests that there is a limit using extractive summarization and sentence compression to form abstractive summaries, and that sophisticated language generation is still needed.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 71, "end_pos": 91, "type": "TASK", "confidence": 0.7049388289451599}]}], "tableCaptions": [{"text": " Table 1: Human evaluation results. Also shown is the  ROUGE-1 (unigram match) F-score of different sys- tems compared to human compression.", "labels": [], "entities": [{"text": "ROUGE-1 (unigram match) F-score", "start_pos": 55, "end_pos": 86, "type": "METRIC", "confidence": 0.7137239426374435}]}, {"text": " Table 2: Compression ratio of different systems and ROUGE-1 scores compared to human abstractive summaries.", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 53, "end_pos": 60, "type": "METRIC", "confidence": 0.9957804679870605}]}]}