{"title": [{"text": "Employing Topic Models for Pattern-based Semantic Class Discovery", "labels": [], "entities": [{"text": "Semantic Class Discovery", "start_pos": 41, "end_pos": 65, "type": "TASK", "confidence": 0.633698026339213}]}], "abstractContent": [{"text": "\uf02a A semantic class is a collection of items (words or phrases) which have semantically peer or sibling relationship.", "labels": [], "entities": []}, {"text": "This paper studies the employment of topic models to automatically construct semantic classes, taking as the source data a collection of raw semantic classes (RASCs), which were extracted by applying predefined patterns to web pages.", "labels": [], "entities": []}, {"text": "The primary requirement (and challenge) here is dealing with multi-membership: An item may belong to multiple semantic classes; and we need to discover as many as possible the different semantic classes the item belongs to.", "labels": [], "entities": []}, {"text": "To adopt topic models, we treat RASCs as \"doc-uments\", items as \"words\", and the final semantic classes as \"topics\".", "labels": [], "entities": []}, {"text": "Appropriate preprocessing and postprocessing are performed to improve results quality, to reduce computation cost, and to tackle the fixed-k constraint of atypical topic model.", "labels": [], "entities": []}, {"text": "Experiments conducted on 40 million web pages show that our approach could yield better results than alternative approaches.", "labels": [], "entities": []}], "introductionContent": [{"text": "Semantic class construction () tries to discover the peer or sibling relationship among terms or phrases by organizing them into semantic classes.", "labels": [], "entities": [{"text": "Semantic class construction", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.7549212574958801}]}, {"text": "For example, {red, white, black\u2026} is a semantic class consisting of color instances.", "labels": [], "entities": []}, {"text": "A popular way for semantic class discovery is pattern-based approach, where predefined patterns are applied to a \uf02a This work was performed when the authors were interns at Microsoft Research Asia collection of web pages or an online web search engine to produce some raw semantic classes (abbreviated as RASCs,).", "labels": [], "entities": [{"text": "semantic class discovery", "start_pos": 18, "end_pos": 42, "type": "TASK", "confidence": 0.7086753249168396}]}, {"text": "RASCs cannot be treated as the ultimate semantic classes, because they are typically noisy and incomplete, as shown in.", "labels": [], "entities": [{"text": "RASCs", "start_pos": 0, "end_pos": 5, "type": "TASK", "confidence": 0.8066883683204651}]}, {"text": "In addition, the information of one real semantic class maybe distributed in lots of RASCs (R 2 and R 3 in).", "labels": [], "entities": []}, {"text": "This paper aims to discover high-quality semantic classes from a large collection of noisy RASCs.", "labels": [], "entities": []}, {"text": "The primary requirement (and challenge) here is to deal with multi-membership, i.e., one item may belong to multiple different semantic classes.", "labels": [], "entities": []}, {"text": "For example, the term \"Lincoln\" can simultaneously represent a person, a place, or a car brand name.", "labels": [], "entities": []}, {"text": "Multi-membership is more popular than at a first glance, because quite a lot of English common words have also been borrowed as company names, places, or product names.", "labels": [], "entities": []}, {"text": "For a given item (as a query) which belongs to multiple semantic classes, we intend to return the semantic classes separately, rather than mixing all their items together.", "labels": [], "entities": []}, {"text": "Existing pattern-based approaches only provide very limited support to multi-membership.", "labels": [], "entities": []}, {"text": "For example, RASCs with the same labels (or hypernyms) are merged in) to gen-erate the ultimate semantic classes.", "labels": [], "entities": []}, {"text": "This is problematic, because RASCs may not have (accurate) hypernyms with them.", "labels": [], "entities": [{"text": "RASCs", "start_pos": 29, "end_pos": 34, "type": "TASK", "confidence": 0.9664929509162903}]}, {"text": "In this paper, we propose to use topic models to address the problem.", "labels": [], "entities": []}, {"text": "In some topic models, a document is modeled as a mixture of hidden topics.", "labels": [], "entities": []}, {"text": "The words of a document are generated according to the word distribution over the topics corresponding to the document (see Section 2 for details).", "labels": [], "entities": []}, {"text": "Given a corpus, the latent topics can be obtained by a parameter estimation procedure.", "labels": [], "entities": []}, {"text": "Topic modeling provides a formal and convenient way of dealing with multi-membership, which is our primary motivation of adopting topic models here.", "labels": [], "entities": [{"text": "Topic modeling", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.8083248734474182}]}, {"text": "To employ topic models, we treat RASCs as \"documents\", items as \"words\", and the final semantic classes as \"topics\".", "labels": [], "entities": [{"text": "RASCs", "start_pos": 33, "end_pos": 38, "type": "TASK", "confidence": 0.9500442743301392}]}, {"text": "There are, however, several challenges in applying topic models to our problem.", "labels": [], "entities": []}, {"text": "To begin with, the computation is intractable for processing a large collection of RASCs (our dataset for experiments contains 2.7 million unique RASCs extracted from 40 million web pages).", "labels": [], "entities": []}, {"text": "Second, typical topic models require the number of topics (k) to be given.", "labels": [], "entities": []}, {"text": "But it lacks an easy way of acquiring the ideal number of semantic classes from the source RASC collection.", "labels": [], "entities": [{"text": "RASC collection", "start_pos": 91, "end_pos": 106, "type": "DATASET", "confidence": 0.7575824856758118}]}, {"text": "For the first challenge, we choose to apply topic models to the RASCs containing an item q, rather than the whole RASC collection.", "labels": [], "entities": [{"text": "RASC collection", "start_pos": 114, "end_pos": 129, "type": "DATASET", "confidence": 0.808103621006012}]}, {"text": "In addition, we also perform some preprocessing operations in which some items are discarded to further improve efficiency.", "labels": [], "entities": []}, {"text": "For the second challenge, considering that most items only belong to a small number of semantic classes, we fix (for all items q) a topic number which is slightly larger than the number of classes an item could belong to.", "labels": [], "entities": []}, {"text": "And then a postprocessing operation is performed to merge the results of topic models to generate the ultimate semantic classes.", "labels": [], "entities": []}, {"text": "Experimental results show that, our topic model approach is able to generate higher-quality semantic classes than popular clustering algorithms (e.g., K-Medoids and DBSCAN).", "labels": [], "entities": []}, {"text": "We make two contributions in the paper: On one hand, we find an effective way of constructing high-quality semantic classes in the patternbased category which deals with multimembership.", "labels": [], "entities": []}, {"text": "On the other hand, we demonstrate, for the first time, that topic modeling can be utilized to help mining the peer relationship among words.", "labels": [], "entities": []}, {"text": "In contrast, the general related relationship between words is extracted in existing topic modeling applications.", "labels": [], "entities": []}, {"text": "Thus we expand the application scope of topic modeling.", "labels": [], "entities": [{"text": "topic modeling", "start_pos": 40, "end_pos": 54, "type": "TASK", "confidence": 0.7229111641645432}]}], "datasetContent": [{"text": "Datasets: By using the Open Directory Project (ODP 3 ) URLs as seeds, we crawled about 40 million English web pages in a breadth-first way.", "labels": [], "entities": []}, {"text": "RASCs are extracted via applying a list of sentence structure patterns and HTML tag patterns (see for some examples).", "labels": [], "entities": [{"text": "RASCs", "start_pos": 0, "end_pos": 5, "type": "TASK", "confidence": 0.9360694885253906}]}, {"text": "Our RASC collection C R contains about 2.7 million unique RASCs and 1 million distinct items.", "labels": [], "entities": [{"text": "RASC collection C R", "start_pos": 4, "end_pos": 23, "type": "DATASET", "confidence": 0.8218352347612381}, {"text": "RASCs", "start_pos": 58, "end_pos": 63, "type": "TASK", "confidence": 0.8651121258735657}]}, {"text": "Query set and labeling: We have volunteers to try Google Sets 4 , record their queries being used, and select overall 55 queries to form our query set.", "labels": [], "entities": []}, {"text": "For each query, the results of all approaches are mixed together and labeled by following two steps.", "labels": [], "entities": []}, {"text": "In the first step, the standard (or ideal) semantic classes (SSCs) for the query are manually determined.", "labels": [], "entities": []}, {"text": "For example, the ideal semantic classes for item \"Georgia\" may include Countries, and U.S. states.", "labels": [], "entities": []}, {"text": "In the second step, each item is assigned a label of \"Good\", \"Fair\", or \"Bad\" with respect to each SSC.", "labels": [], "entities": []}, {"text": "For example, \"silver\" is labeled \"Good\" with respect to \"colors\" and \"chemical elements\".", "labels": [], "entities": []}, {"text": "We adopt metric MnDCG (Section 4.2) as our evaluation metric.", "labels": [], "entities": [{"text": "MnDCG", "start_pos": 16, "end_pos": 21, "type": "DATASET", "confidence": 0.7124497890472412}]}, {"text": "K-Medoids clustering needs to predefine the cluster number k.", "labels": [], "entities": [{"text": "K-Medoids clustering", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.5520415604114532}]}, {"text": "We fix the k value for all different query item q, as has been done for the topic model approach.", "labels": [], "entities": []}, {"text": "For fair comparison, the same postprocessing is made for all the approaches.", "labels": [], "entities": []}, {"text": "And the same preprocessing is made for all the approaches except for the item clustering ones (to which the preprocessing is not applicable).", "labels": [], "entities": []}, {"text": "Each produced semantic class is an ordered list of items.", "labels": [], "entities": []}, {"text": "A couple of metrics in the information retrieval (IR) community like Precision@10, MAP (mean average precision), and nDCG (normalized discounted cumulative gain) are available for evaluating a single ranked list of items per query.", "labels": [], "entities": [{"text": "information retrieval (IR)", "start_pos": 27, "end_pos": 53, "type": "TASK", "confidence": 0.8303998351097107}, {"text": "Precision@10", "start_pos": 69, "end_pos": 81, "type": "METRIC", "confidence": 0.963963508605957}, {"text": "MAP (mean average precision)", "start_pos": 83, "end_pos": 111, "type": "METRIC", "confidence": 0.8973167737325033}, {"text": "nDCG (normalized discounted cumulative gain)", "start_pos": 117, "end_pos": 161, "type": "METRIC", "confidence": 0.8350506595202855}]}, {"text": "Among the metrics, nDCG) can handle our three-level judgments (\"Good\", \"Fair\", and \"Bad\", refer to Section 4.1), \u00ed \u00b5\u00ed\u00b1\u009b\u00ed \u00b5\u00ed\u00b0\u00b7\u00ed \u00b5\u00ed\u00b0 \u00b6\u00ed \u00b5\u00ed\u00b0\u00ba@\u00ed \u00b5\u00ed\u00b1\u0098 = \u00ed \u00b5\u00ed\u00b0\u00ba \u00ed \u00b5\u00ed\u00b1\u0096 /log(\u00ed \u00b5\u00ed\u00b1\u0096 + 1) \u00ed \u00b5\u00ed\u00b0\u00ba * \u00ed \u00b5\u00ed\u00b1\u0096 /log(\u00ed \u00b5\u00ed\u00b1\u0096 + 1) \u00ed \u00b5\u00ed\u00b1\u0098 \u00ed \u00b5\u00ed\u00b1\u0096=1 where G(i) is the gain value assigned to the i'th item, and G * (i) is the gain value assigned to the i'th item of an ideal (or perfect) ranking list.", "labels": [], "entities": [{"text": "nDCG", "start_pos": 19, "end_pos": 23, "type": "DATASET", "confidence": 0.8765543103218079}]}, {"text": "Here we extend the IR metrics to the evaluation of multiple ordered lists per query.", "labels": [], "entities": [{"text": "IR", "start_pos": 19, "end_pos": 21, "type": "TASK", "confidence": 0.8826106786727905}]}, {"text": "We use nDCG as the basic metric and extend it to MnDCG.", "labels": [], "entities": [{"text": "MnDCG", "start_pos": 49, "end_pos": 54, "type": "DATASET", "confidence": 0.8347361087799072}]}, {"text": "Assume labelers have determined m SSCs (SSC 1 ~SSC m , refer to Section 4.1) for query q and the weight (or importance) of SSC i is w i . Assume n semantic classes are generated by an approach and n 1 of them have corresponding SSCs (i.e., no appropriate SSC can be found for the remaining n-n 1 semantic classes).", "labels": [], "entities": []}, {"text": "We define the MnDCG score of an approach (with respect to query q) as, (\u00ed \u00b5\u00ed\u00b1\u009b\u00ed \u00b5\u00ed\u00b0\u00b7\u00ed \u00b5\u00ed\u00b0 \u00b6\u00ed \u00b5\u00ed\u00b0\u00ba \u00ed \u00b5\u00ed\u00b0\u00ba \u00ed \u00b5\u00ed\u00b1\u0096,\u00ed \u00b5\u00ed\u00b1\u0097 ) \u00ed \u00b5\u00ed\u00b1\u0096\u00ed \u00b5\u00ed\u00b1\u0093 \u00ed \u00b5\u00ed\u00b1\u0098 \u00ed \u00b5\u00ed\u00b1\u0096 \u2260 0 (4.3) In the above formula, nDCG(G i,j ) is the nDCG score of semantic class G i,j ; and k i denotes the number of semantic classes assigned to SSC i . For a list of queries, the MnDCG score of an algorithm is the average of all scores for the queries.", "labels": [], "entities": []}, {"text": "The metric is designed to properly deal with the following cases, i).", "labels": [], "entities": []}, {"text": "One semantic class is wrongly split into multiple ones: Punished by dividing \u00ed \u00b5\u00ed\u00b1\u0098 \u00ed \u00b5\u00ed\u00b1\u0096 in Formula 4.3; ii).", "labels": [], "entities": []}, {"text": "A semantic class is too noisy to be assigned to any SSC: Processed by the \"n 1 /n\" in Formula 4.2; iii).", "labels": [], "entities": []}, {"text": "Fewer semantic classes (than the number of SSCs) are produced: Punished in Formula 4.3 by assigning a zero value.", "labels": [], "entities": []}, {"text": "Wrongly merge multiple semantic classes into one: The nDCG score of the merged one will be small because it is computed with respect to only one single SSC.", "labels": [], "entities": []}, {"text": "The gain values of nDCG for the three relevance levels (\"Bad\", \"Fair\", and \"Good\") are respectively -1, 1, and 2 in experiments.", "labels": [], "entities": []}, {"text": "shows the performance comparison between the approaches listed in Section 4.1, using metrics MnDCG@n (n=1\u202610).", "labels": [], "entities": [{"text": "MnDCG", "start_pos": 93, "end_pos": 98, "type": "METRIC", "confidence": 0.7255670428276062}]}, {"text": "Postprocessing is performed for all the approaches, where Formula 3.2 is adopted to compute the similarity between semantic classes.", "labels": [], "entities": [{"text": "Postprocessing", "start_pos": 0, "end_pos": 14, "type": "METRIC", "confidence": 0.9462104439735413}]}, {"text": "The results show that that the topic modeling approaches produce higher-quality semantic classes than the other approaches.", "labels": [], "entities": []}, {"text": "It indicates that the topic mixture assumption of topic modeling can handle the multi-membership problem very well here.", "labels": [], "entities": [{"text": "topic modeling", "start_pos": 50, "end_pos": 64, "type": "TASK", "confidence": 0.73112952709198}]}, {"text": "Among the alternative approaches, RASC clustering behaves better than item clustering.", "labels": [], "entities": [{"text": "RASC clustering", "start_pos": 34, "end_pos": 49, "type": "TASK", "confidence": 0.9787023961544037}, {"text": "item clustering", "start_pos": 70, "end_pos": 85, "type": "TASK", "confidence": 0.6929674595594406}]}, {"text": "The reason might be that an item cannot belong to multiple clusters in the two item clustering approaches, while RASC clustering allows this.", "labels": [], "entities": [{"text": "RASC clustering", "start_pos": 113, "end_pos": 128, "type": "TASK", "confidence": 0.9155479967594147}]}, {"text": "For the RASC clustering approaches, although one item has the chance to belong to different semantic classes, one RASC can only belong to one semantic class.", "labels": [], "entities": [{"text": "RASC clustering", "start_pos": 8, "end_pos": 23, "type": "TASK", "confidence": 0.95790895819664}]}, {"text": "shows the average query processing time and results quality of the LDA approach, by varying frequency threshold h.", "labels": [], "entities": []}, {"text": "Similar results are observed for the pLSI approach.", "labels": [], "entities": []}, {"text": "In the table, h=1 means no preprocessing is performed.", "labels": [], "entities": [{"text": "preprocessing", "start_pos": 27, "end_pos": 40, "type": "METRIC", "confidence": 0.9614807367324829}]}, {"text": "The average query processing time is calculated overall items in our dataset.", "labels": [], "entities": []}, {"text": "As the threshold h increases, the processing time decreases as expected, because the input of topic modeling gets smaller.", "labels": [], "entities": []}, {"text": "The second column lists the results quality (measured by MnDCG@10).", "labels": [], "entities": [{"text": "MnDCG", "start_pos": 57, "end_pos": 62, "type": "METRIC", "confidence": 0.9194901585578918}]}, {"text": "Interestingly, we get the best results quality when h=4 (i.e., the items with frequency less than 4 are discarded).", "labels": [], "entities": []}, {"text": "The reason maybe that most low-frequency items are noisy ones.", "labels": [], "entities": []}, {"text": "As a result, preprocessing can improve both results quality and processing efficiency; and h=4 seems a good choice in preprocessing for our dataset.", "labels": [], "entities": []}, {"text": "The same preprocessing (h=4) is performed in generating the data.", "labels": [], "entities": [{"text": "preprocessing", "start_pos": 9, "end_pos": 22, "type": "METRIC", "confidence": 0.968848466873169}]}, {"text": "It can be seen that postprocessing improves results quality.", "labels": [], "entities": []}, {"text": "Sim2 achieves more performance improvement than Sim1, which demonstrates the effectiveness of the similarity measure in Formula 3.2.", "labels": [], "entities": [{"text": "similarity measure", "start_pos": 98, "end_pos": 116, "type": "METRIC", "confidence": 0.9565265476703644}]}, {"text": "shows the semantic classes generated by our LDA approach for some sample queries in which the bad classes or bad members are highlighted (to save space, 10 items are listed here, and the query itself is omitted in the resultant semantic classes).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4. Time complexity and quality comparison  among LDA approaches of different thresholds", "labels": [], "entities": []}]}