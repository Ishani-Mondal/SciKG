{"title": [{"text": "Query-Focused Summaries or Query-Biased Summaries ?", "labels": [], "entities": []}], "abstractContent": [{"text": "In the context of the Document Understanding Conferences, the task of Query-Focused Multi-Document Summarization is intended to improve agreement in content among human-generated model summaries.", "labels": [], "entities": [{"text": "Document Understanding Conferences", "start_pos": 22, "end_pos": 56, "type": "TASK", "confidence": 0.878015379110972}, {"text": "Query-Focused Multi-Document Summarization", "start_pos": 70, "end_pos": 112, "type": "TASK", "confidence": 0.6160213947296143}]}, {"text": "Query-focus also aids the automated summarizers in directing the summary at specific topics, which may result in better agreement with these model summaries.", "labels": [], "entities": [{"text": "summarizers", "start_pos": 36, "end_pos": 47, "type": "TASK", "confidence": 0.930101215839386}]}, {"text": "However, while query focus correlates with performance, we show that high-performing automatic systems produce summaries with disproportionally higher query term density than human summarizers do.", "labels": [], "entities": []}, {"text": "Experimental evidence suggests that automatic systems heavily rely on query term occurrence and repetition to achieve good performance.", "labels": [], "entities": []}], "introductionContent": [{"text": "The problem of automatically summarizing text documents has received a lot of attention since the early work by.", "labels": [], "entities": [{"text": "summarizing text documents", "start_pos": 29, "end_pos": 55, "type": "TASK", "confidence": 0.818984866142273}]}, {"text": "Most of the current automatic summarization systems rely on a sentence extractive paradigm, where key sentences in the original text are selected to form the summary based on the clues (or heuristics), or learning based approaches.", "labels": [], "entities": [{"text": "summarization", "start_pos": 30, "end_pos": 43, "type": "TASK", "confidence": 0.739814043045044}, {"text": "sentence extractive", "start_pos": 62, "end_pos": 81, "type": "TASK", "confidence": 0.7230810225009918}]}, {"text": "Common approaches for identifying key sentences include: training a binary classifier (, training a Markov model or CRF ( or directly assigning weights to sentences based on a variety of features and heuristically determined feature weights (.", "labels": [], "entities": []}, {"text": "But, the question of which components and features of automatic summarizers contribute most to their performance has largely remained unanswered (Marcu and Gerber, 2001), until) explored the contribution of frequency based measures.", "labels": [], "entities": []}, {"text": "In this paper, we examine the role a query plays in automated multi-document summarization of newswire.", "labels": [], "entities": [{"text": "multi-document summarization of newswire", "start_pos": 62, "end_pos": 102, "type": "TASK", "confidence": 0.6677947416901588}]}, {"text": "One of the issues studied since the inception of automatic summarization is that of human agreement: different people choose different content for their summaries (.", "labels": [], "entities": [{"text": "summarization", "start_pos": 59, "end_pos": 72, "type": "TASK", "confidence": 0.8824282884597778}]}, {"text": "Later, it was assumed) that having a question/query to provide focus would improve agreement between any two human-generated model summaries, as well as between a model summary and an automated summary.", "labels": [], "entities": []}, {"text": "Starting in 2005 until 2007, a query-focused multidocument summarization task was conducted as part of the annual Document Understanding Conference.", "labels": [], "entities": [{"text": "Document Understanding Conference", "start_pos": 114, "end_pos": 147, "type": "TASK", "confidence": 0.7966935435930887}]}, {"text": "This task models a real-world complex question answering scenario, where systems need to synthesize from a set of 25 documents, a brief (250 words), well organized fluent answer to an information need.", "labels": [], "entities": [{"text": "question answering", "start_pos": 38, "end_pos": 56, "type": "TASK", "confidence": 0.7581914067268372}]}, {"text": "Query-focused summarization is a topic of ongoing importance within the summarization and question answering communities.", "labels": [], "entities": [{"text": "summarization", "start_pos": 14, "end_pos": 27, "type": "TASK", "confidence": 0.6926481127738953}, {"text": "summarization and question answering", "start_pos": 72, "end_pos": 108, "type": "TASK", "confidence": 0.7622535526752472}]}, {"text": "Most of the work in this area has been conducted under the guise of \"query-focused multi-document summarization\", \"descriptive question answering\", or even \"complex question answering\".", "labels": [], "entities": [{"text": "descriptive question answering", "start_pos": 115, "end_pos": 145, "type": "TASK", "confidence": 0.641802579164505}, {"text": "complex question answering", "start_pos": 157, "end_pos": 183, "type": "TASK", "confidence": 0.669784148534139}]}, {"text": "In this paper, based on structured empirical evaluations, we show that most of the systems participating in DUC's Query-Focused Multi-Document Summarization (QF-MDS) task have been query-biased in building extractive summaries.", "labels": [], "entities": [{"text": "DUC's Query-Focused Multi-Document Summarization (QF-MDS) task", "start_pos": 108, "end_pos": 170, "type": "TASK", "confidence": 0.8092436790466309}]}, {"text": "Throughout our discussion, the term 'query-bias', with respect to a sentence, is precisely defined to mean that the sentence has at least one query term within it.", "labels": [], "entities": []}, {"text": "The term 'query-focus' is less precisely defined, but is related to the cognitive task of focusing a summary on the query, which we assume humans do naturally.", "labels": [], "entities": []}, {"text": "In other words, the human generated model summaries are assumed to be queryfocused.", "labels": [], "entities": []}, {"text": "Here we first discuss query-biased content in Summary Content Units (SCUs) in Section 2 and then in Section 3 by building formal models on query-bias we discuss why/how automated systems are query-biased rather than being query-focused.", "labels": [], "entities": []}, {"text": "2 Query-biased content in Summary Content Units (SCUs) Summary content units, referred as SCUs hereafter, are semantically motivated subsentential units that are variable in length but not bigger than a sentential clause.", "labels": [], "entities": []}, {"text": "SCUs are constructed from annotation of a collection of human summaries on a given document collection.", "labels": [], "entities": []}, {"text": "They are identified by noting information that is repeated across summaries.", "labels": [], "entities": []}, {"text": "The repetition is as small as a modifier of a noun phrase or as large as a clause.", "labels": [], "entities": [{"text": "repetition", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9802539944648743}]}, {"text": "The evaluation method that is based on overlapping SCUs inhuman and automatic summaries is called the pyramid method (.", "labels": [], "entities": []}, {"text": "The University of Ottawa has organized the pyramid annotation data such that for some of the sentences in the original document collection, a list of corresponding content units is known).", "labels": [], "entities": []}, {"text": "A sample of an SCU mapping from topic D0701A of the DUC 2007 QF-MDS corpus is shown in.", "labels": [], "entities": [{"text": "SCU mapping from topic D0701A of the DUC 2007 QF-MDS corpus", "start_pos": 15, "end_pos": 74, "type": "DATASET", "confidence": 0.7035764726725492}]}, {"text": "Three sentences are seen in the figure among which two have been annotated with system IDs and SCU weights wherever applicable.", "labels": [], "entities": []}, {"text": "The first sentence has not been picked by any of the summarizers participating in Pyramid Evaluations, hence it is unknown if the sentence would have contributed to any SCU.", "labels": [], "entities": []}, {"text": "The second sentence was picked by 8 summarizers and that sentence contributed to an SCU of weight 3.", "labels": [], "entities": [{"text": "SCU", "start_pos": 84, "end_pos": 87, "type": "METRIC", "confidence": 0.9622120261192322}]}, {"text": "The third sentence in the example was picked by one summarizer, however, it did not contribute to any SCU.", "labels": [], "entities": [{"text": "SCU", "start_pos": 102, "end_pos": 105, "type": "TASK", "confidence": 0.586651086807251}]}, {"text": "This example shows all the three types of sentences available in the corpus: unknown samples, positive samples and negative samples.", "labels": [], "entities": []}, {"text": "We extracted the positive and negative samples in the source documents from these annotations; types of second and third sentences shown in.", "labels": [], "entities": []}, {"text": "A total of 14.8% sentences were annotated to be either positive or negative.", "labels": [], "entities": []}, {"text": "When we analyzed the positive set, we found that 84.63% sentences in this set were querybiased.", "labels": [], "entities": []}, {"text": "Also, on the negative sample set, we found that 69.12% sentences were query-biased.", "labels": [], "entities": []}, {"text": "That is, on an average, 76.67% of the sentences picked by any automated summarizer are query-biased.", "labels": [], "entities": []}, {"text": "On the other hand, for human summaries only 58% sentences were query-biased.", "labels": [], "entities": [{"text": "summaries", "start_pos": 29, "end_pos": 38, "type": "TASK", "confidence": 0.8729016184806824}]}, {"text": "All the above numbers are based on the DUC 2007 dataset shown in boldface in 1 . There is one caveat: The annotated sentences come only from the summaries of systems that participated in the pyramid evaluations.", "labels": [], "entities": [{"text": "DUC 2007 dataset", "start_pos": 39, "end_pos": 55, "type": "DATASET", "confidence": 0.9768320719401041}]}, {"text": "Since only 13 among a total 32 participating systems were evaluated using pyramid evaluations, the dataset is limited.", "labels": [], "entities": []}, {"text": "However, despite this small issue, it is very clear that at least those systems that participated in pyramid evaluations have been biased towards query-terms, or at least, they have been better at correctly identifying important sentences from the query-biased sentences than from query-unbiased sentences.", "labels": [], "entities": []}, {"text": "We used DUC 2007 dataset for all experiments reported.", "labels": [], "entities": [{"text": "DUC 2007 dataset", "start_pos": 8, "end_pos": 24, "type": "DATASET", "confidence": 0.9770016074180603}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Statistical information on counts of query-biased sentences.", "labels": [], "entities": []}, {"text": " Table 2: Rank, Averaged log-likelihood score based on binomial model, true ROUGE-2 score for the summaries  of various systems in DUC'07 query-focused multi-document summarization task.", "labels": [], "entities": [{"text": "Rank", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9798737168312073}, {"text": "Averaged log-likelihood score", "start_pos": 16, "end_pos": 45, "type": "METRIC", "confidence": 0.8908947706222534}, {"text": "ROUGE-2 score", "start_pos": 76, "end_pos": 89, "type": "METRIC", "confidence": 0.9330381155014038}, {"text": "DUC'07 query-focused multi-document summarization task", "start_pos": 131, "end_pos": 185, "type": "TASK", "confidence": 0.7206391096115112}]}, {"text": " Table 3: Rank, Averaged log-likelihood score based on multinomial model, true ROUGE-2 score for the sum- maries of various systems in DUC'07 query-focused multi-document summarization task.", "labels": [], "entities": [{"text": "Rank", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.977367103099823}, {"text": "Averaged log-likelihood score", "start_pos": 16, "end_pos": 45, "type": "METRIC", "confidence": 0.8962199091911316}, {"text": "ROUGE-2 score", "start_pos": 79, "end_pos": 92, "type": "METRIC", "confidence": 0.9379193186759949}, {"text": "DUC'07 query-focused multi-document summarization task", "start_pos": 135, "end_pos": 189, "type": "TASK", "confidence": 0.7177577614784241}]}, {"text": " Table 4: Correlation of ROUGE measures with log- likelihood scores for automated systems", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 25, "end_pos": 30, "type": "METRIC", "confidence": 0.9701353311538696}, {"text": "log- likelihood scores", "start_pos": 45, "end_pos": 67, "type": "METRIC", "confidence": 0.6684616059064865}]}, {"text": " Table 5: Correlation of ROUGE measures with log- likelihood scores for humans", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 25, "end_pos": 30, "type": "METRIC", "confidence": 0.9762758016586304}, {"text": "log- likelihood scores", "start_pos": 45, "end_pos": 67, "type": "METRIC", "confidence": 0.6926379352807999}]}]}