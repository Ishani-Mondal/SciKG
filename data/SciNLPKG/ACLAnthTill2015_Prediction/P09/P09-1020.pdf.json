{"title": [{"text": "Forest-based Tree Sequence to String Translation Model", "labels": [], "entities": [{"text": "Tree Sequence to String Translation", "start_pos": 13, "end_pos": 48, "type": "TASK", "confidence": 0.5455871045589447}]}], "abstractContent": [{"text": "This paper proposes a forest-based tree sequence to string translation model for syntax-based statistical machine translation, which automatically learns tree sequence to string translation rules from word-aligned source-side-parsed bilingual texts.", "labels": [], "entities": [{"text": "string translation", "start_pos": 52, "end_pos": 70, "type": "TASK", "confidence": 0.7584587633609772}, {"text": "statistical machine translation", "start_pos": 94, "end_pos": 125, "type": "TASK", "confidence": 0.6300536592801412}]}, {"text": "The proposed model leverages on the strengths of both tree sequence-based and forest-based translation models.", "labels": [], "entities": []}, {"text": "Therefore, it cannot only utilize forest structure that compactly encodes exponential number of parse trees but also capture non-syntactic translation equivalences with linguistically structured information through tree sequence.", "labels": [], "entities": []}, {"text": "This makes our model potentially more robust to parse errors and structure divergence.", "labels": [], "entities": [{"text": "structure divergence", "start_pos": 65, "end_pos": 85, "type": "TASK", "confidence": 0.6497719585895538}]}, {"text": "Experimental results on the NIST MT-2003 Chinese-English translation task show that our method statistically significantly outperforms the four baseline systems.", "labels": [], "entities": [{"text": "NIST MT-2003 Chinese-English translation task", "start_pos": 28, "end_pos": 73, "type": "TASK", "confidence": 0.784206485748291}]}], "introductionContent": [{"text": "Recently syntax-based statistical machine translation (SMT) methods have achieved very promising results and attracted more and more interests in the SMT research community.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 22, "end_pos": 59, "type": "TASK", "confidence": 0.7898631542921066}, {"text": "SMT research", "start_pos": 150, "end_pos": 162, "type": "TASK", "confidence": 0.9211687445640564}]}, {"text": "Fundamentally, syntax-based SMT views translation as a structural transformation process.", "labels": [], "entities": [{"text": "SMT", "start_pos": 28, "end_pos": 31, "type": "TASK", "confidence": 0.8822423219680786}]}, {"text": "Therefore, structure divergence and parse errors are two of the major issues that may largely compromise the performance of syntax-based SMT ().", "labels": [], "entities": [{"text": "structure divergence", "start_pos": 11, "end_pos": 31, "type": "TASK", "confidence": 0.7145952731370926}, {"text": "SMT", "start_pos": 137, "end_pos": 140, "type": "TASK", "confidence": 0.8435829877853394}]}, {"text": "Many solutions have been proposed to address the above two issues.", "labels": [], "entities": []}, {"text": "Among these advances, forest-based modeling (  and tree sequence-based modeling () are two interesting modeling methods with promising results reported.", "labels": [], "entities": []}, {"text": "Forest-based modeling aims to improve translation accuracy through digging the potential better parses from n-bests (i.e. forest) while tree sequence-based modeling aims to model non-syntactic translations with structured syntactic knowledge.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 50, "end_pos": 58, "type": "METRIC", "confidence": 0.8224536180496216}]}, {"text": "In nature, the two methods would be complementary to each other since they manage to solve the negative impacts of monolingual parse errors and cross-lingual structure divergence on translation results from different viewpoints.", "labels": [], "entities": [{"text": "cross-lingual structure divergence", "start_pos": 144, "end_pos": 178, "type": "TASK", "confidence": 0.6669517755508423}]}, {"text": "Therefore, one natural way is to combine the strengths of the two modeling methods for better performance of syntax-based SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 122, "end_pos": 125, "type": "TASK", "confidence": 0.8369110226631165}]}, {"text": "However, there are many challenges in combining the two methods into a single model from both theoretical and implementation engineering viewpoints.", "labels": [], "entities": []}, {"text": "In theory, one may worry about whether the advantage of tree sequence has already been covered by forest because forest encodes implicitly a huge number of parse trees and these parse trees may generate many different phrases and structure segmentations given a source sentence.", "labels": [], "entities": []}, {"text": "In system implementation, the exponential combinations of tree sequences with forest structures make the rule extraction and decoding tasks much more complicated than that of the two individual methods.", "labels": [], "entities": [{"text": "rule extraction", "start_pos": 105, "end_pos": 120, "type": "TASK", "confidence": 0.783475935459137}]}, {"text": "In this paper, we propose a forest-based tree sequence to string model, which is designed to integrate the strengths of the forest-based and the tree sequence-based modeling methods.", "labels": [], "entities": []}, {"text": "We present our solutions that are able to extract translation rules and decode translation results for our model very efficiently.", "labels": [], "entities": []}, {"text": "A general, configurable platform was designed for our model.", "labels": [], "entities": []}, {"text": "With this platform, we can easily implement our method and many previous syntax-based methods by simple parameter setting.", "labels": [], "entities": []}, {"text": "We evaluate our method on the NIST MT-2003 Chinese-English translation tasks.", "labels": [], "entities": [{"text": "NIST MT-2003 Chinese-English translation tasks", "start_pos": 30, "end_pos": 76, "type": "TASK", "confidence": 0.7485171675682067}]}, {"text": "Experimental results show that our method significantly outperforms the two individual methods and other baseline methods.", "labels": [], "entities": []}, {"text": "Our study shows that the proposed method is able to effectively combine the strengths of the forest-based and tree sequence-based methods, and thus having great potential to address the issues of parse errors and non-syntactic transla-tions resulting from structure divergence.", "labels": [], "entities": []}, {"text": "It also indicates that tree sequence and forest play different roles and make contributions to our model in different ways.", "labels": [], "entities": []}, {"text": "The remainder of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes related work while section 3 defines our translation model.", "labels": [], "entities": [{"text": "translation", "start_pos": 61, "end_pos": 72, "type": "TASK", "confidence": 0.9724598526954651}]}, {"text": "In section 4 and section 5, the key rule extraction and decoding algorithms are elaborated.", "labels": [], "entities": [{"text": "rule extraction", "start_pos": 36, "end_pos": 51, "type": "TASK", "confidence": 0.7806612551212311}]}, {"text": "Experimental results are reported in section 6 and the paper is concluded in section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our method on Chinese-English translation task.", "labels": [], "entities": [{"text": "Chinese-English translation", "start_pos": 26, "end_pos": 53, "type": "TASK", "confidence": 0.662407636642456}]}, {"text": "We use the FBIS corpus as training set, the NIST MT-2002 test set as development (dev) set and the NIST MT-2003 test set as test set.", "labels": [], "entities": [{"text": "FBIS corpus", "start_pos": 11, "end_pos": 22, "type": "DATASET", "confidence": 0.9267696738243103}, {"text": "NIST MT-2002 test set", "start_pos": 44, "end_pos": 65, "type": "DATASET", "confidence": 0.8982296586036682}, {"text": "NIST MT-2003 test set", "start_pos": 99, "end_pos": 120, "type": "DATASET", "confidence": 0.931383803486824}]}, {"text": "We train Charniak's parser (Charniak 2000) on CTB5 to do Chinese parsing, and modify it to output packed forest.", "labels": [], "entities": [{"text": "CTB5", "start_pos": 46, "end_pos": 50, "type": "DATASET", "confidence": 0.9750101566314697}, {"text": "Chinese parsing", "start_pos": 57, "end_pos": 72, "type": "TASK", "confidence": 0.5448376089334488}]}, {"text": "We tune the parser on section 301-325 and test it on section 271-300.", "labels": [], "entities": []}, {"text": "The F-measure on all sentences is 80.85%.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9992018342018127}]}, {"text": "A 3-gram language model is trained on the Xin-hua portion of the English Gigaword3 corpus and the target side of the FBIS corpus using the SRILM Toolkits) with modified Kneser-Ney smoothing.", "labels": [], "entities": [{"text": "English Gigaword3 corpus", "start_pos": 65, "end_pos": 89, "type": "DATASET", "confidence": 0.8987714648246765}, {"text": "FBIS corpus", "start_pos": 117, "end_pos": 128, "type": "DATASET", "confidence": 0.9425796270370483}]}, {"text": "GIZA++ (Och and Ney, 2003) and the heuristics \"grow-diag-final-and\" are used to generate m-ton word alignments.", "labels": [], "entities": []}, {"text": "For the MER training), Koehn's MER trainer is modified for our system.", "labels": [], "entities": [{"text": "MER", "start_pos": 8, "end_pos": 11, "type": "TASK", "confidence": 0.9123178124427795}]}, {"text": "For significance test, we use Zhang et al.'s implementation ().", "labels": [], "entities": []}, {"text": "Our evaluation metrics is casesensitive BLEU-4 ().", "labels": [], "entities": [{"text": "BLEU-4", "start_pos": 40, "end_pos": 46, "type": "METRIC", "confidence": 0.9981088638305664}]}, {"text": "For parse forest pruning ( , we utilize the Margin-based pruning algorithm presented in.", "labels": [], "entities": [{"text": "parse forest pruning", "start_pos": 4, "end_pos": 24, "type": "TASK", "confidence": 0.8660059372584025}]}, {"text": "Different from  that use a static pruning threshold, our threshold is sentence-depended.", "labels": [], "entities": []}, {"text": "For each sentence, we compute the Margin between the n-th best and the top 1 parse tree, then use the Margin-based pruning algorithm presented in to do pruning.", "labels": [], "entities": [{"text": "Margin", "start_pos": 34, "end_pos": 40, "type": "METRIC", "confidence": 0.9198444485664368}]}, {"text": "By doing so, we can guarantee to use at least all the top n best parse trees in the forest.", "labels": [], "entities": []}, {"text": "However, please note that even after pruning there is still exponential number of additional trees embedded in the forest because of the sharing structure of forest.", "labels": [], "entities": []}, {"text": "Other parameters are set as follows: maximum number of roots in a tree sequence is 3, maximum height of a translation rule is 3, maximum number of leaf nodes is 7, maximum number of node sequences on each span is 10, and maximum number of rules extracted from one node is 10000.", "labels": [], "entities": []}, {"text": "We implement our proposed methods as a general, configurable platform for syntax-based SMT study.", "labels": [], "entities": [{"text": "SMT study", "start_pos": 87, "end_pos": 96, "type": "TASK", "confidence": 0.9538583159446716}]}, {"text": "Based on this platform, we are able to easily implement most of the state-of-the-art syntax-based x-to-string SMT methods via simple parameter setting.", "labels": [], "entities": [{"text": "SMT", "start_pos": 110, "end_pos": 113, "type": "TASK", "confidence": 0.9227903485298157}]}, {"text": "For training, we set forest pruning threshold to 1 best for tree-based methods and 100 best for forest-based methods.", "labels": [], "entities": []}, {"text": "For decoding, we set: 1) TT2S: tree-based tree-to-string model by setting the forest pruning threshold to 1 best and the number of sub-trees in a tree sequence to 1. 2) TTS2S: tree-based tree-sequence to string system by setting the forest pruning threshold to 1 best and the maximum number of sub-trees in a tree sequence to 3. 3) FT2S: forest-based tree-to-string system by setting the forest pruning threshold to 500 best, the number of sub-trees in a tree sequence to 1. 4) FTS2S: forest-based tree-sequence to string system by setting the forest pruning threshold to 500 best and the maximum number of sub-trees in a tree sequence to 3.", "labels": [], "entities": [{"text": "FT2S", "start_pos": 332, "end_pos": 336, "type": "DATASET", "confidence": 0.7453414797782898}, {"text": "FTS2S", "start_pos": 478, "end_pos": 483, "type": "DATASET", "confidence": 0.5625828504562378}]}, {"text": "We use the first three syntax-based systems (TT2S, TTS2S, FT2S) and Moses (, the state-of-the-art phrase-based system, as our baseline systems.", "labels": [], "entities": [{"text": "FT2S", "start_pos": 58, "end_pos": 62, "type": "DATASET", "confidence": 0.6381675601005554}]}, {"text": "compares the performance of the five methods, all of which are fine-tuned.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2. # of rules extracted from training cor- pus. L means fully lexicalized, P means partially  lexicalized, U means unlexicalized.", "labels": [], "entities": []}, {"text": " Table 3. # of rules used to generate one-best  translation result in testing", "labels": [], "entities": []}, {"text": " Table 4. Impact of the forest pruning", "labels": [], "entities": []}]}