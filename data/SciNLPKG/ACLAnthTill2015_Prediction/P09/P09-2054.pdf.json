{"title": [{"text": "Chinese Term Extraction Using Different Types of Relevance", "labels": [], "entities": [{"text": "Chinese Term Extraction", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.5853040516376495}]}], "abstractContent": [{"text": "This paper presents anew term extraction approach using relevance between term candidates calculated by a link analysis based method.", "labels": [], "entities": [{"text": "term extraction", "start_pos": 25, "end_pos": 40, "type": "TASK", "confidence": 0.7165961414575577}]}, {"text": "Different types of relevance are used separately or jointly for term verification.", "labels": [], "entities": []}, {"text": "The proposed approach requires no prior domain knowledge and no adaptation for new domains.", "labels": [], "entities": []}, {"text": "Consequently, the method can be used in any domain corpus and it is especially useful for resource-limited domains.", "labels": [], "entities": []}, {"text": "Evaluations conducted on two different domains for Chinese term extraction show significant improvements over existing techniques and also verify the efficiency and relative domain independent nature of the approach.", "labels": [], "entities": [{"text": "Chinese term extraction", "start_pos": 51, "end_pos": 74, "type": "TASK", "confidence": 0.6505886614322662}]}], "introductionContent": [{"text": "Terms are the lexical units to represent the most fundamental knowledge of a domain.", "labels": [], "entities": []}, {"text": "Term extraction is an essential task in domain knowledge acquisition which can be used for lexicon update, domain ontology construction, etc.", "labels": [], "entities": [{"text": "Term extraction", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.9181030094623566}, {"text": "domain knowledge acquisition", "start_pos": 40, "end_pos": 68, "type": "TASK", "confidence": 0.6531530817349752}, {"text": "lexicon update", "start_pos": 91, "end_pos": 105, "type": "TASK", "confidence": 0.7481976747512817}, {"text": "domain ontology construction", "start_pos": 107, "end_pos": 135, "type": "TASK", "confidence": 0.7382600506146749}]}, {"text": "Term extraction involves two steps.", "labels": [], "entities": [{"text": "Term extraction", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.9833207130432129}]}, {"text": "The first step extracts candidates by unithood calculation to qualify a string as a valid term.", "labels": [], "entities": []}, {"text": "The second step verifies them through termhood measures to validate their domain specificity.", "labels": [], "entities": []}, {"text": "Many previous studies are conducted on term candidate extraction.", "labels": [], "entities": [{"text": "term candidate extraction", "start_pos": 39, "end_pos": 64, "type": "TASK", "confidence": 0.6145190795262655}]}, {"text": "Other tasks such as named entity recognition, meaningful word extraction and unknown word detection, use techniques similar to that for term candidate extraction.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 20, "end_pos": 44, "type": "TASK", "confidence": 0.6236657202243805}, {"text": "meaningful word extraction", "start_pos": 46, "end_pos": 72, "type": "TASK", "confidence": 0.6112174689769745}, {"text": "unknown word detection", "start_pos": 77, "end_pos": 99, "type": "TASK", "confidence": 0.6313095887502035}, {"text": "term candidate extraction", "start_pos": 136, "end_pos": 161, "type": "TASK", "confidence": 0.6421836415926615}]}, {"text": "But, their focuses are not on domain specificity.", "labels": [], "entities": []}, {"text": "This study focuses on the verification of candidates by termhood calculation.", "labels": [], "entities": []}, {"text": "Relevance between term candidates and documents is the most popular feature used for term verification such as TF-IDF ( and Inter-Domain Entropy, which are all based on the hypothesis that \"if a candidate occurs frequently in a few documents of a domain, it is likely a term\".", "labels": [], "entities": []}, {"text": "Limited distribution information of term candidates in different documents often limits the ability of such algorithms to distinguish terms from non-terms.", "labels": [], "entities": []}, {"text": "There are also attempts to use prior domain specific knowledge and annotated corpora for term verification.", "labels": [], "entities": []}, {"text": "calculates the percentage of context words in a domain lexicon using both frequency information and semantic information.", "labels": [], "entities": []}, {"text": "However, this technique requires a domain lexicon whose size and quality have great impact on the performance of the algorithm.", "labels": [], "entities": []}, {"text": "Some supervised learning approaches have been applied to protein/gene name recognition ( and Chinese new word identification () using SVM classifiers which also require large domain corpora and annotations.", "labels": [], "entities": [{"text": "protein/gene name recognition", "start_pos": 57, "end_pos": 86, "type": "TASK", "confidence": 0.6472663342952728}, {"text": "Chinese new word identification", "start_pos": 93, "end_pos": 124, "type": "TASK", "confidence": 0.5980377569794655}]}, {"text": "The latest work by applied the relevance between term candidates and sentences by using the link analysis approach based on the HITS algorithm to achieve better performance.", "labels": [], "entities": [{"text": "HITS algorithm", "start_pos": 128, "end_pos": 142, "type": "DATASET", "confidence": 0.8225734829902649}]}, {"text": "In this work, anew feature on the relevance between different term candidates is integrated with other features to validate their domain specificity.", "labels": [], "entities": []}, {"text": "The relevance between candidate terms maybe useful to identify domain specific terms based on two assumptions.", "labels": [], "entities": []}, {"text": "First, terms are more likely to occur with other terms in order to express domain information.", "labels": [], "entities": []}, {"text": "Second, term candidates extracted from domain corpora are likely to be domain specific.", "labels": [], "entities": []}, {"text": "Previous work by (e.g. Ji and Lu, 2007) uses similar information by comparing the context to an existing large domain lexicon.", "labels": [], "entities": []}, {"text": "In this study, the relevance between term candidates are iteratively calculated by graphs using link analysis algorithm to avoid the dependency on prior domain knowledge.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes the proposed algorithms.", "labels": [], "entities": []}, {"text": "Section 3 explains the experiments and the performance evaluation.", "labels": [], "entities": []}, {"text": "Section 4 concludes and presents the future plans.", "labels": [], "entities": []}], "datasetContent": [{"text": "For comparison, three reference algorithms are used in the evaluation.", "labels": [], "entities": []}, {"text": "The first algorithm is TV_LinkA which takes CS and CD into consideration and performs well ().", "labels": [], "entities": [{"text": "TV_LinkA", "start_pos": 23, "end_pos": 31, "type": "DATASET", "confidence": 0.7324497302373251}]}, {"text": "The second one is a supervised learning approach based on a SVM classifier, SVM light (Joachims, 1999).", "labels": [], "entities": []}, {"text": "Internal and external features are used by SVM light . The third algorithm is the popular used TF-IDF algorithm.", "labels": [], "entities": []}, {"text": "All the reference algorithms require no training except SVM light . Two training sets containing thousands of positive and negative examples from IT domain and legal domain are constructed for the SVM classifier.", "labels": [], "entities": []}, {"text": "The training and testing sets are not overlapped.", "labels": [], "entities": []}, {"text": "show the performance of the proposed algorithms using different features for IT domain and legal domain, respectively.", "labels": [], "entities": []}, {"text": "The algorithm using CD alone is the same as the TF-IDF algorithm.", "labels": [], "entities": []}, {"text": "The algorithm using CS and CD is the TV_LinkA algorithm..", "labels": [], "entities": [{"text": "TV_LinkA", "start_pos": 37, "end_pos": 45, "type": "DATASET", "confidence": 0.75688503185908}]}, {"text": "Performance on Legal Domain and show that the proposed algorithms achieve similar performance on both domains.", "labels": [], "entities": []}, {"text": "The proposed algorithm using all three features (CC+CS+CD) performs the best.", "labels": [], "entities": []}, {"text": "The results confirm that the proposed approach are quite stable across domains and the relevance between candidates are efficient for improving performance of term extraction in different domains.", "labels": [], "entities": [{"text": "term extraction", "start_pos": 159, "end_pos": 174, "type": "TASK", "confidence": 0.7124543786048889}]}, {"text": "The algorithm using CC only does not achieve good performance.", "labels": [], "entities": []}, {"text": "Neither does CC+CS.", "labels": [], "entities": [{"text": "CC+CS", "start_pos": 13, "end_pos": 18, "type": "DATASET", "confidence": 0.8617343703905741}]}, {"text": "The main reason is that the term candidates used in the experiments are extracted using the TCE_DI algorithm which can extract candidates with low statistical significance.", "labels": [], "entities": [{"text": "TCE_DI", "start_pos": 92, "end_pos": 98, "type": "METRIC", "confidence": 0.8350257078806559}]}, {"text": "TCE_DI provides a better compromise between recall and precision.", "labels": [], "entities": [{"text": "TCE_DI", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.641228179136912}, {"text": "recall", "start_pos": 44, "end_pos": 50, "type": "METRIC", "confidence": 0.998675525188446}, {"text": "precision", "start_pos": 55, "end_pos": 64, "type": "METRIC", "confidence": 0.9961814880371094}]}, {"text": "CC alone is vulnerable to noisy candidates since it relies on the relevance between candidates themselves.", "labels": [], "entities": []}, {"text": "However, as an additional feature to the combined use of CS and CD (TV_LinkA), improvement of over 10% on Fvalue is obtained for the IT domain, and 5% for the legal domain.", "labels": [], "entities": [{"text": "Fvalue", "start_pos": 106, "end_pos": 112, "type": "DATASET", "confidence": 0.7970085740089417}]}, {"text": "This is because the noise data are eliminated by CS and CD, and CC help to identify additional terms that may not be statistically significant.", "labels": [], "entities": [{"text": "CC", "start_pos": 64, "end_pos": 66, "type": "METRIC", "confidence": 0.9437142610549927}]}], "tableCaptions": [{"text": " Table 1. Performance on IT Domain", "labels": [], "entities": []}, {"text": " Table 2. Performance on Legal Domain", "labels": [], "entities": []}]}