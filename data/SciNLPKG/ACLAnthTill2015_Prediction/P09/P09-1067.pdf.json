{"title": [{"text": "Variational Decoding for Statistical Machine Translation", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 25, "end_pos": 56, "type": "TASK", "confidence": 0.8633511861165365}]}], "abstractContent": [{"text": "Statistical models in machine translation exhibit spurious ambiguity.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 22, "end_pos": 41, "type": "TASK", "confidence": 0.7892659902572632}]}, {"text": "That is, the probability of an output string is split among many distinct derivations (e.g., trees or segmentations).", "labels": [], "entities": []}, {"text": "In principle, the goodness of a string is measured by the total probability of its many derivations.", "labels": [], "entities": []}, {"text": "However, finding the best string (e.g., during decoding) is then computationally intractable.", "labels": [], "entities": []}, {"text": "Therefore, most systems use a simple Viterbi approximation that measures the goodness of a string using only its most probable derivation.", "labels": [], "entities": []}, {"text": "Instead, we develop a variational approximation, which considers all the derivations but still allows tractable decoding.", "labels": [], "entities": []}, {"text": "Our particular variational distributions are parameterized as n-gram models.", "labels": [], "entities": []}, {"text": "We also analytically show that interpolating these n-gram models for different n is similar to minimum-risk decoding for BLEU (Tromble et al., 2008).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 121, "end_pos": 125, "type": "METRIC", "confidence": 0.9902883768081665}]}, {"text": "Experiments show that our approach improves the state of the art.", "labels": [], "entities": []}], "introductionContent": [{"text": "Ambiguity is a central issue in natural language processing.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 32, "end_pos": 59, "type": "TASK", "confidence": 0.6459130048751831}]}, {"text": "Many systems try to resolve ambiguities in the input, for example by tagging words with their senses or choosing a particular syntax tree fora sentence.", "labels": [], "entities": []}, {"text": "These systems are designed to recover the values of interesting latent variables, such as word senses, syntax trees, or translations, given the observed input.", "labels": [], "entities": []}, {"text": "However, some systems resolve too many ambiguities.", "labels": [], "entities": []}, {"text": "They recover additional latent variablesso-called nuisance variables-that are not of interest to the user.", "labels": [], "entities": []}, {"text": "For example, though machine translation (MT) seeks to output a string, typical MT systems ( will also recover a particular derivation of that output string, which specifies a tree or segmentation and its alignment to the input string.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 20, "end_pos": 44, "type": "TASK", "confidence": 0.8540269076824188}]}, {"text": "The competing derivations of a string are interchangeable fora user who is only interested in the string itself, so a system that unnecessarily tries to choose among them is said to be resolving spurious ambiguity.", "labels": [], "entities": []}, {"text": "Of course, the nuisance variables are important components of the system's model.", "labels": [], "entities": []}, {"text": "For example, the translation process from one language to another language may follow some hidden tree transformation process, in a recursive fashion.", "labels": [], "entities": []}, {"text": "Many features of the model will crucially make reference to such hidden structures or alignments.", "labels": [], "entities": []}, {"text": "However, collapsing the resulting spurious ambiguity-i.e., marginalizing out the nuisance variables-causes significant computational difficulties.", "labels": [], "entities": []}, {"text": "The goodness of a possible MT output string should be measured by summing up the probabilities of all its derivations.", "labels": [], "entities": [{"text": "MT output string", "start_pos": 27, "end_pos": 43, "type": "TASK", "confidence": 0.8794067104657491}]}, {"text": "Unfortunately, finding the best string is then computationally intractable).", "labels": [], "entities": []}, {"text": "Therefore, most systems merely identify the single most probable derivation and report the corresponding string.", "labels": [], "entities": []}, {"text": "This corresponds to a Viterbi approximation that measures the goodness of an output string using only its most probable derivation, ignoring all the others.", "labels": [], "entities": []}, {"text": "In this paper, we propose a variational method that considers all the derivations but still allows tractable decoding.", "labels": [], "entities": []}, {"text": "Given an input string, the original system produces a probability distribution p over possible output strings and their derivations (nuisance variables).", "labels": [], "entities": []}, {"text": "Our method constructs a second distribution q \u2208 Q that approximates p as well as possible, and then finds the best string according to q.", "labels": [], "entities": []}, {"text": "The last step is tractable because each q \u2208 Q is defined (unlike p) without reference to nuisance variables.", "labels": [], "entities": []}, {"text": "Notice that q here does not approximate the entire translation process, but only the distribution over output strings fora particular input.", "labels": [], "entities": []}, {"text": "This is why it can be a fairly good approximation even without using the nuisance variables.", "labels": [], "entities": []}, {"text": "In practice, we approximate with several different variational families Q, corresponding to ngram (Markov) models of different orders.", "labels": [], "entities": []}, {"text": "We geometrically interpolate the resulting approximations q with one another (and with the original distribution p), justifying this interpolation as similar to the minimum-risk decoding for BLEU proposed by.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 191, "end_pos": 195, "type": "METRIC", "confidence": 0.9346739649772644}]}, {"text": "Experiments show that our approach improves the state of the art.", "labels": [], "entities": []}, {"text": "The methods presented in this paper should be applicable to collapsing spurious ambiguity for other tasks as well.", "labels": [], "entities": []}, {"text": "Such tasks include dataoriented parsing (DOP), applications of Hidden Markov Models (HMMs) and mixture models, and other models with latent variables.", "labels": [], "entities": [{"text": "dataoriented parsing (DOP)", "start_pos": 19, "end_pos": 45, "type": "TASK", "confidence": 0.8433296203613281}]}, {"text": "Indeed, our methods were inspired by past work on variational decoding for DOP and for latent-variable parsing ().", "labels": [], "entities": []}], "datasetContent": [{"text": "We report results using an open source MT toolkit, called Joshua (, which implements Hiero (Chiang, 2007).", "labels": [], "entities": [{"text": "Hiero (Chiang, 2007)", "start_pos": 85, "end_pos": 105, "type": "DATASET", "confidence": 0.7691887517770132}]}, {"text": "We work on a Chinese to English translation task.", "labels": [], "entities": [{"text": "Chinese to English translation", "start_pos": 13, "end_pos": 43, "type": "TASK", "confidence": 0.5841951295733452}]}, {"text": "Our translation model was trained on about 1M parallel sentence pairs (about 28M words in each language), which are sub-sampled from corpora distributed by LDC for the NIST MT evaluation using a sampling method based on the ngram matches between training and test sets in the foreign side.", "labels": [], "entities": [{"text": "NIST MT evaluation", "start_pos": 168, "end_pos": 186, "type": "TASK", "confidence": 0.6012837390104929}]}, {"text": "We also used a 5-gram language model with modified Kneser-Ney smoothing, trained on a data set consisting of a 130M words in English Gigaword (LDC2007T07) and the English side of the parallel corpora.", "labels": [], "entities": []}, {"text": "We use GIZA++), a suffix-array, SRILM), and risk-based deterministic annealing () to obtain word alignments, translation models, language models, and the optimal weights for combining these models, respectively.", "labels": [], "entities": [{"text": "SRILM", "start_pos": 32, "end_pos": 37, "type": "METRIC", "confidence": 0.942168653011322}, {"text": "word alignments", "start_pos": 92, "end_pos": 107, "type": "TASK", "confidence": 0.7504108548164368}]}, {"text": "We use standard beam-pruning and cube-pruning parameter settings, following, when generating the hypergraphs.", "labels": [], "entities": []}, {"text": "The NIST MT'03 set is used to tune model weights (e.g. those of: BLEU scores for Viterbi, Crunching, MBR, and variational decoding.", "labels": [], "entities": [{"text": "NIST MT'03 set", "start_pos": 4, "end_pos": 18, "type": "DATASET", "confidence": 0.8372713128725687}, {"text": "BLEU", "start_pos": 65, "end_pos": 69, "type": "METRIC", "confidence": 0.9986125230789185}]}, {"text": "All the systems improve significantly over the Viterbi baseline (paired permutation test, p < 0.05).", "labels": [], "entities": [{"text": "Viterbi baseline", "start_pos": 47, "end_pos": 63, "type": "DATASET", "confidence": 0.8413155376911163}, {"text": "paired permutation test", "start_pos": 65, "end_pos": 88, "type": "METRIC", "confidence": 0.8296636343002319}]}, {"text": "In each column, we boldface the best result as well as all results that are statistically indistinguishable from it.", "labels": [], "entities": []}, {"text": "In MBR, K is the number of unique strings.", "labels": [], "entities": []}, {"text": "For Crunching and Crunching+MBR, N represents the number of derivations.", "labels": [], "entities": []}, {"text": "On average, each string has about 115 distinct derivations.", "labels": [], "entities": []}, {"text": "The variational method \"1to4gram+wp+vt\" is our full interpolation (16) of four variational n-gram models (\"1to4gram\"), the Viterbi baseline (\"vt\"), and a word penalty feature (\"wp\").", "labels": [], "entities": []}, {"text": "\u03b3 of, and MT'04 and MT'05 are blind testsets.", "labels": [], "entities": [{"text": "MT'04", "start_pos": 10, "end_pos": 15, "type": "DATASET", "confidence": 0.5323278903961182}, {"text": "MT'05", "start_pos": 20, "end_pos": 25, "type": "DATASET", "confidence": 0.7136461138725281}]}, {"text": "We will report results for lowercase BLEU-4, using the shortest reference translation in computing brevity penalty.", "labels": [], "entities": [{"text": "BLEU-4", "start_pos": 37, "end_pos": 43, "type": "METRIC", "confidence": 0.9919915795326233}]}, {"text": "The difference between MBR and Crunching+MBR lies in how we approximate the distribution p(y | x) in (17).", "labels": [], "entities": []}, {"text": "For MBR, we take p(y | x) to be proportional top Viterbi (y | x) if y is among the K best distinct strings on that measure, and 0 otherwise.", "labels": [], "entities": [{"text": "MBR", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.5977644324302673}]}, {"text": "For Crunching+MBR, we take p(y | x) to be proportional top crunch (y | x), which is based on the N best derivations.", "labels": [], "entities": [{"text": "Crunching+MBR", "start_pos": 4, "end_pos": 17, "type": "TASK", "confidence": 0.504086991151174}]}, {"text": "improves the Viterbi baseline (except the case with a unigram VM), though often not statistically significant.", "labels": [], "entities": [{"text": "Viterbi baseline", "start_pos": 13, "end_pos": 29, "type": "DATASET", "confidence": 0.6798616349697113}]}, {"text": "Moreover, a bigram (i.e., \"2gram\") achieves the best BLEU scores among the four different orders of VMs.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 53, "end_pos": 57, "type": "METRIC", "confidence": 0.9991845488548279}]}], "tableCaptions": [{"text": " Table 1: BLEU scores for Viterbi, Crunching, MBR, and vari- ational decoding. All the systems improve significantly over  the Viterbi baseline (paired permutation test, p < 0.05). In  each column, we boldface the best result as well as all results  that are statistically indistinguishable from it. In MBR, K is  the number of unique strings. For Crunching and Crunch- ing+MBR, N represents the number of derivations. On av- erage, each string has about 115 distinct derivations. The  variational method \"1to4gram+wp+vt\" is our full interpola- tion (16) of four variational n-gram models (\"1to4gram\"), the  Viterbi baseline (\"vt\"), and a word penalty feature (\"wp\").", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9993090629577637}]}, {"text": " Table 2: BLEU scores under different variational decoders  discussed in Section 3.2.3. A star  *  indicates a result that is  significantly better than Viterbi decoding (paired permutation  test, p < 0.05).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9982243180274963}]}, {"text": " Table 3: Cross-entropies H(p, q) achieved by various ap- proximations q. The notation H denotes the sum of cross- entropies of all test sentences, divided by the total number  of test words. A perfect approximation would achieve H(p),  which we estimate using the true H d (p) and a 10000-best list.", "labels": [], "entities": []}]}