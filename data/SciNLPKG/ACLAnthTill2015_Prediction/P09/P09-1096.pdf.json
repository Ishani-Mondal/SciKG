{"title": [{"text": "SMS based Interface for FAQ Retrieval", "labels": [], "entities": [{"text": "FAQ Retrieval", "start_pos": 24, "end_pos": 37, "type": "TASK", "confidence": 0.7747997641563416}]}], "abstractContent": [{"text": "Short Messaging Service (SMS) is popularly used to provide information access to people on the move.", "labels": [], "entities": [{"text": "Short Messaging Service (SMS)", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.7673641194899877}]}, {"text": "This has resulted in the growth of SMS based Question Answering (QA) services.", "labels": [], "entities": [{"text": "SMS based Question Answering (QA)", "start_pos": 35, "end_pos": 68, "type": "TASK", "confidence": 0.7027650305203029}]}, {"text": "However automatically handling SMS questions poses significant challenges due to the inherent noise in SMS questions.", "labels": [], "entities": []}, {"text": "In this work we present an automatic FAQ-based question answering system for SMS users.", "labels": [], "entities": [{"text": "FAQ-based question answering", "start_pos": 37, "end_pos": 65, "type": "TASK", "confidence": 0.6714999278386434}]}, {"text": "We handle the noise in a SMS query by formulating the query similarity over FAQ questions as a combinatorial search problem.", "labels": [], "entities": []}, {"text": "The search space consists of combinations of all possible dictionary variations of tokens in the noisy query.", "labels": [], "entities": []}, {"text": "We present an efficient search algorithm that does not require any training data or SMS normaliza-tion and can handle semantic variations in question formulation.", "labels": [], "entities": [{"text": "question formulation", "start_pos": 141, "end_pos": 161, "type": "TASK", "confidence": 0.7562902867794037}]}, {"text": "We demonstrate the effectiveness of our approach on two real-life datasets.", "labels": [], "entities": []}], "introductionContent": [{"text": "The number of mobile users is growing at an amazing rate.", "labels": [], "entities": []}, {"text": "In India alone a few million subscribers are added each month with the total subscriber base now crossing 370 million.", "labels": [], "entities": []}, {"text": "The anytime anywhere access provided by mobile networks and portability of handsets coupled with the strong human urge to quickly find answers has fueled the growth of information based services on mobile devices.", "labels": [], "entities": []}, {"text": "These services can be simple advertisements, polls, alerts or complex applications such as browsing, search and e-commerce.", "labels": [], "entities": []}, {"text": "The latest mobile devices come equipped with high resolution screen space, inbuilt web browsers and full message keypads, however a majority of the users still use cheaper models that have limited screen space and basic keypad.", "labels": [], "entities": []}, {"text": "On such devices, SMS is the only mode of text communication.", "labels": [], "entities": []}, {"text": "This has encouraged service providers to build information based services around SMS technology.", "labels": [], "entities": []}, {"text": "Today, a majority of SMS based information services require users to type specific codes to retrieve information.", "labels": [], "entities": []}, {"text": "For example to get a duplicate bill fora specific month, say June, the user has to type DUPBILLJUN.", "labels": [], "entities": [{"text": "DUPBILLJUN", "start_pos": 88, "end_pos": 98, "type": "METRIC", "confidence": 0.8884053826332092}]}, {"text": "This unnecessarily constraints users who generally find it easy and intuitive to type in a \"texting\" language.", "labels": [], "entities": []}, {"text": "Some businesses have recently allowed users to formulate queries in natural language using SMS.", "labels": [], "entities": []}, {"text": "For example, many contact centers now allow customers to \"text\" their complaints and requests for information over SMS.", "labels": [], "entities": []}, {"text": "This mode of communication not only makes economic sense but also saves the customer from the hassle of waiting in a call queue.", "labels": [], "entities": []}, {"text": "Most of these contact center based services and other regular services like \"AQA 63336\" 1 by Issuebits Ltd, GTIP 2 by AlienPant Ltd., \"Texperts\" by Number UK Ltd. and \"ChaCha\" 4 use human agents to understand the SMS text and respond to these SMS queries.", "labels": [], "entities": [{"text": "AQA 63336\" 1", "start_pos": 77, "end_pos": 89, "type": "DATASET", "confidence": 0.7797326892614365}, {"text": "Number UK Ltd.", "start_pos": 148, "end_pos": 162, "type": "DATASET", "confidence": 0.8229892253875732}]}, {"text": "The nature of texting language, which often as a rule rather than exception, has misspellings, non-standard abbreviations, transliterations, phonetic substitutions and omissions, makes it difficult to build automated question answering systems around SMS technology.", "labels": [], "entities": [{"text": "question answering", "start_pos": 217, "end_pos": 235, "type": "TASK", "confidence": 0.7666029930114746}]}, {"text": "This is true even for questions whose answers are well documented like a FAQ database.", "labels": [], "entities": [{"text": "FAQ database", "start_pos": 73, "end_pos": 85, "type": "DATASET", "confidence": 0.8650401830673218}]}, {"text": "Unlike other automatic question answering systems that focus on generating or searching answers, in a FAQ database the question and answers are already provided by an expert.", "labels": [], "entities": [{"text": "question answering", "start_pos": 23, "end_pos": 41, "type": "TASK", "confidence": 0.731233075261116}, {"text": "FAQ database", "start_pos": 102, "end_pos": 114, "type": "DATASET", "confidence": 0.7649687230587006}]}, {"text": "The task is then to identify the best matching question-answer pair fora given query.", "labels": [], "entities": []}, {"text": "In this paper we present a FAQ-based question answering system over a SMS interface.", "labels": [], "entities": [{"text": "FAQ-based question answering", "start_pos": 27, "end_pos": 55, "type": "TASK", "confidence": 0.7244056264559428}]}, {"text": "Our system allows the user to enter a question in the SMS texting language.", "labels": [], "entities": []}, {"text": "Such questions are noisy and contain spelling mistakes, abbreviations, deletions, phonetic spellings, transliterations etc.", "labels": [], "entities": []}, {"text": "Since mobile handsets have limited screen space, it necessitates that the system have high accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.9983001351356506}]}, {"text": "We handle the noise in a SMS query by formulating the query similarity over FAQ questions as a combinatorial search problem.", "labels": [], "entities": []}, {"text": "The search space consists of combinations of all possible dictionary variations of tokens in the noisy query.", "labels": [], "entities": []}, {"text": "The quality of the solution, i.e. the retrieved questions is formalized using a scoring function.", "labels": [], "entities": []}, {"text": "Unlike other SMS processing systems our model does not require training data or human intervention.", "labels": [], "entities": [{"text": "SMS processing", "start_pos": 13, "end_pos": 27, "type": "TASK", "confidence": 0.8879091441631317}]}, {"text": "Our system handles not only the noisy variations of SMS query tokens but also semantic variations.", "labels": [], "entities": []}, {"text": "We demonstrate the effectiveness of our system on real-world data sets.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes the relevant prior work in this area and talks about our specific contributions.", "labels": [], "entities": []}, {"text": "In Section 3 we give the problem formulation.", "labels": [], "entities": []}, {"text": "Section 4 describes the Pruning Algorithm which finds the best matching question fora given SMS query.", "labels": [], "entities": []}, {"text": "Section 5 provides system implementation details.", "labels": [], "entities": []}, {"text": "Section 6 provides details about our experiments.", "labels": [], "entities": []}, {"text": "Finally we conclude in Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "We validated the effectiveness and usability of our system by carrying out experiments on two FAQ data sets.", "labels": [], "entities": [{"text": "FAQ data sets", "start_pos": 94, "end_pos": 107, "type": "DATASET", "confidence": 0.9609232147534689}]}, {"text": "The first FAQ data set, referred to as the Telecom Data-Set, consists of 1500 frequently asked questions, collected from a Telecom service provider's website.", "labels": [], "entities": [{"text": "FAQ data set", "start_pos": 10, "end_pos": 22, "type": "DATASET", "confidence": 0.860485295454661}, {"text": "Telecom Data-Set", "start_pos": 43, "end_pos": 59, "type": "DATASET", "confidence": 0.9748244881629944}]}, {"text": "The questions in this data set are related to the Telecom providers products or services.", "labels": [], "entities": []}, {"text": "For example queries about call rates/charges, bill drop locations, how to install caller tunes, how to activate GPRS etc.", "labels": [], "entities": [{"text": "GPRS", "start_pos": 112, "end_pos": 116, "type": "DATASET", "confidence": 0.9170292019844055}]}, {"text": "The second FAQ corpus, referred to as the Yahoo DataSet, consists of 7500 questions from three Yahoo!", "labels": [], "entities": [{"text": "FAQ corpus", "start_pos": 11, "end_pos": 21, "type": "DATASET", "confidence": 0.7722632884979248}, {"text": "Yahoo DataSet", "start_pos": 42, "end_pos": 55, "type": "DATASET", "confidence": 0.9480257928371429}]}, {"text": "Answers 12 categories namely Sports.Swimming, Sports.Tennis, Sports.Running.", "labels": [], "entities": []}, {"text": "To measure the effectiveness of our system, a user evaluation study was performed.", "labels": [], "entities": []}, {"text": "Ten human evaluators were asked to choose 10 questions randomly from the FAQ data set.", "labels": [], "entities": [{"text": "FAQ data set", "start_pos": 73, "end_pos": 85, "type": "DATASET", "confidence": 0.9689249793688456}]}, {"text": "None of the evaluators were authors of the paper.", "labels": [], "entities": []}, {"text": "They were provided with a mobile keypad interface and asked to \"text\" the selected 10 questions as SMS queries.", "labels": [], "entities": []}, {"text": "Through that exercise 100 relevant SMS queries per FAQ data set were collected.", "labels": [], "entities": [{"text": "FAQ data set", "start_pos": 51, "end_pos": 63, "type": "DATASET", "confidence": 0.9527070124944051}]}, {"text": "In order to validate that the system was able to handle queries that were out of gives the number of relevant and irrelevant queries used in our experiments.", "labels": [], "entities": []}, {"text": "The average word length of the collected SMS messages for Telecom and Yahoo datasets was 4 and 7 respectively.", "labels": [], "entities": [{"text": "Telecom and Yahoo datasets", "start_pos": 58, "end_pos": 84, "type": "DATASET", "confidence": 0.7926247119903564}]}, {"text": "We manually cleaned the SMS query data word byword to create a clean SMS test-set.", "labels": [], "entities": [{"text": "SMS query data word byword", "start_pos": 24, "end_pos": 50, "type": "DATASET", "confidence": 0.7712672114372253}, {"text": "SMS test-set", "start_pos": 69, "end_pos": 81, "type": "DATASET", "confidence": 0.8109111189842224}]}, {"text": "For example, the SMS query \"h2 mke a pdl bke fstr\" was manually cleaned to get \"how to make pedal bike faster\".", "labels": [], "entities": []}, {"text": "In order to quantify the level of noise in the collected SMS data, we built a character-level language model(LM) 13 using the questions in the FAQ data-set (vocabulary size is 44 characters) and computed the perplexity 14 of the language model on the noisy and the cleaned SMS test-set.", "labels": [], "entities": [{"text": "FAQ data-set", "start_pos": 143, "end_pos": 155, "type": "DATASET", "confidence": 0.9754830896854401}]}, {"text": "The perplexity of the LM on a corpus gives an indication of the average number of bits needed per n-gram to encode the corpus.", "labels": [], "entities": []}, {"text": "Noise will result in the introduction of many previously unseen n-grams in the corpus.", "labels": [], "entities": []}, {"text": "Higher number of bits are needed to encode these improbable n-grams which results in increased perplexity.", "labels": [], "entities": []}, {"text": "From we can seethe difference in perplexity for noisy and clean SMS data for the Yahoo and Telecom data-set.", "labels": [], "entities": [{"text": "Telecom data-set", "start_pos": 91, "end_pos": 107, "type": "DATASET", "confidence": 0.8613762855529785}]}, {"text": "The high level of perplexity in the SMS data set indicates the extent of noise present in the SMS corpus.", "labels": [], "entities": [{"text": "SMS data set", "start_pos": 36, "end_pos": 48, "type": "DATASET", "confidence": 0.8941888014475504}, {"text": "SMS corpus", "start_pos": 94, "end_pos": 104, "type": "DATASET", "confidence": 0.8503866195678711}]}, {"text": "To handle irrelevant queries the algorithm described in Section 4 is modified.", "labels": [], "entities": []}, {"text": "Only if the Score(Q * ) is above a certain threshold, it's answer is returned, else we return \"null\".", "labels": [], "entities": [{"text": "Score(Q * )", "start_pos": 12, "end_pos": 23, "type": "METRIC", "confidence": 0.9348090410232544}]}, {"text": "The threshold  To retrieve the correct answer for the posed SMS query, the SMS query is matched against questions in the FAQ data set and the best matching question(Q * ) is identified using the Pruning algorithm.", "labels": [], "entities": [{"text": "FAQ data set", "start_pos": 121, "end_pos": 133, "type": "DATASET", "confidence": 0.9723443984985352}]}, {"text": "The system then returns the answer to this best matching question to the human evaluator.", "labels": [], "entities": []}, {"text": "The evaluator then scores the response on a binary scale.", "labels": [], "entities": []}, {"text": "A score of 1 is given if the returned answer is the correct response to the SMS query, else it is assigned 0.", "labels": [], "entities": []}, {"text": "The scoring procedure is reversed for irrelevant queries i.e. a score of 0 is assigned if the system returns an answer and 1 is assigned if it returns \"null\" for an \"irrelevant\" query.", "labels": [], "entities": []}, {"text": "The result of this evaluation on both data-sets is shown in and 8.", "labels": [], "entities": []}, {"text": "In order to compare the performance of our system, we benchmark our results against Lucene's 15 Fuzzy match feature.", "labels": [], "entities": [{"text": "Lucene's 15 Fuzzy match", "start_pos": 84, "end_pos": 107, "type": "DATASET", "confidence": 0.6507226467132569}]}, {"text": "Lucene supports fuzzy searches based on the Levenshtein Distance, or Edit Distance algorithm.", "labels": [], "entities": [{"text": "Levenshtein Distance", "start_pos": 44, "end_pos": 64, "type": "METRIC", "confidence": 0.6123561412096024}]}, {"text": "To do a fuzzy search http://lucene.apache.org we specify the \u223c symbol at the end of each token of the SMS query.", "labels": [], "entities": []}, {"text": "For example, the SMS query \"romg actvt\" on the FAQ corpus is reformulated as \"romg\u223c 0.3 actvt\u223c 0.3\".", "labels": [], "entities": [{"text": "FAQ corpus", "start_pos": 47, "end_pos": 57, "type": "DATASET", "confidence": 0.9647978246212006}]}, {"text": "The parameter after the \u223c specifies the required similarity.", "labels": [], "entities": []}, {"text": "The parameter value is between 0 and 1, with a value closer to 1 only terms with higher similarity will be matched.", "labels": [], "entities": []}, {"text": "These queries are run on the indexed FAQs.", "labels": [], "entities": [{"text": "FAQs", "start_pos": 37, "end_pos": 41, "type": "DATASET", "confidence": 0.9411295652389526}]}, {"text": "The results of this evaluation on both data-sets is shown in and 8.", "labels": [], "entities": []}, {"text": "The results clearly demonstrate that our method performs 2 to 2.5 times better than Lucene's Fuzzy match.", "labels": [], "entities": []}, {"text": "It was observed that with higher values of similarity parameter (\u223c 0.6, \u223c 0.8), the number of correctly answered queries was even lower.", "labels": [], "entities": [{"text": "similarity", "start_pos": 43, "end_pos": 53, "type": "METRIC", "confidence": 0.9530961513519287}]}, {"text": "In we show the runtime performance of the Naive vs Pruning algorithm on the Yahoo FAQ Dataset for 150 SMS queries.", "labels": [], "entities": [{"text": "Yahoo FAQ Dataset", "start_pos": 76, "end_pos": 93, "type": "DATASET", "confidence": 0.9168032010396322}]}, {"text": "It is evident from that not only does the Pruning Algorithm outperform the Naive one but also gives a nearconstant runtime performance overall the queries.", "labels": [], "entities": []}, {"text": "The substantially better performance of the Pruning algorithm is due to the fact that it queries much less number of terms and ends up with a smaller candidate set compared to the Naive algorithm.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: SMS Data Set.", "labels": [], "entities": [{"text": "SMS Data Set", "start_pos": 10, "end_pos": 22, "type": "DATASET", "confidence": 0.9321760733922323}]}, {"text": " Table 2: Perplexity for Cleaned and Noisy SMS", "labels": [], "entities": [{"text": "Perplexity", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.902999997138977}]}]}