{"title": [{"text": "A Framework of Feature Selection Methods for Text Categorization", "labels": [], "entities": [{"text": "Text Categorization", "start_pos": 45, "end_pos": 64, "type": "TASK", "confidence": 0.7672509253025055}]}], "abstractContent": [{"text": "In text categorization, feature selection (FS) is a strategy that aims at making text classifiers more efficient and accurate.", "labels": [], "entities": [{"text": "text categorization", "start_pos": 3, "end_pos": 22, "type": "TASK", "confidence": 0.765947163105011}, {"text": "feature selection (FS)", "start_pos": 24, "end_pos": 46, "type": "TASK", "confidence": 0.8104775547981262}]}, {"text": "However, when dealing with anew task, it is still difficult to quickly select a suitable one from various FS methods provided by many previous studies.", "labels": [], "entities": [{"text": "FS", "start_pos": 106, "end_pos": 108, "type": "TASK", "confidence": 0.8329285383224487}]}, {"text": "In this paper, we propose a theoretic framework of FS methods based on two basic measurements: frequency measurement and ratio measurement.", "labels": [], "entities": [{"text": "FS", "start_pos": 51, "end_pos": 53, "type": "TASK", "confidence": 0.9610483050346375}, {"text": "ratio", "start_pos": 121, "end_pos": 126, "type": "METRIC", "confidence": 0.9722898006439209}]}, {"text": "Then six popular FS methods are in detail discussed under this framework.", "labels": [], "entities": [{"text": "FS", "start_pos": 17, "end_pos": 19, "type": "TASK", "confidence": 0.9712075591087341}]}, {"text": "Moreover, with the guidance of our theoretical analysis, we propose a novel method called weighed frequency and odds (WFO) that combines the two measurements with trained weights.", "labels": [], "entities": [{"text": "weighed frequency and odds (WFO)", "start_pos": 90, "end_pos": 122, "type": "METRIC", "confidence": 0.8476294023650033}]}, {"text": "The experimental results on data sets from both topic-based and sentiment classification tasks show that this new method is robust across different tasks and numbers of selected features.", "labels": [], "entities": [{"text": "sentiment classification tasks", "start_pos": 64, "end_pos": 94, "type": "TASK", "confidence": 0.8857039014498392}]}], "introductionContent": [{"text": "With the rapid growth of online information, text classification, the task of assigning text documents to one or more predefined categories, has become one of the key tools for automatically handling and organizing text information.", "labels": [], "entities": [{"text": "text classification", "start_pos": 45, "end_pos": 64, "type": "TASK", "confidence": 0.7916806936264038}]}, {"text": "The problems of text classification normally involve the difficulty of extremely high dimensional feature space which sometimes makes learning algorithms intractable.", "labels": [], "entities": [{"text": "text classification", "start_pos": 16, "end_pos": 35, "type": "TASK", "confidence": 0.8008889853954315}]}, {"text": "A standard procedure to reduce the feature dimensionality is called feature selection (FS).", "labels": [], "entities": [{"text": "feature selection (FS)", "start_pos": 68, "end_pos": 90, "type": "TASK", "confidence": 0.6684884011745453}]}, {"text": "Various FS methods, such as document frequency (DF), information gain (IG), mutual information (MI), \u03c7 -test (CHI), Bi-Normal Separation (BNS), and weighted log-likelihood ratio (WLLR), have been proposed for the tasks and make text classification more efficient and accurate.", "labels": [], "entities": [{"text": "FS", "start_pos": 8, "end_pos": 10, "type": "TASK", "confidence": 0.9592679142951965}, {"text": "mutual information (MI)", "start_pos": 76, "end_pos": 99, "type": "METRIC", "confidence": 0.6545765697956085}, {"text": "\u03c7 -test (CHI)", "start_pos": 101, "end_pos": 114, "type": "METRIC", "confidence": 0.893777589003245}, {"text": "weighted log-likelihood ratio (WLLR)", "start_pos": 148, "end_pos": 184, "type": "METRIC", "confidence": 0.8160003026326498}, {"text": "text classification", "start_pos": 228, "end_pos": 247, "type": "TASK", "confidence": 0.8223239481449127}]}, {"text": "However, comparing these FS methods appears to be difficult because they are usually based on different theories or measurements.", "labels": [], "entities": [{"text": "FS", "start_pos": 25, "end_pos": 27, "type": "TASK", "confidence": 0.9475658535957336}]}, {"text": "For example, MI and IG are based on information theory, while CHI is mainly based on the measurements of statistic independence.", "labels": [], "entities": [{"text": "MI", "start_pos": 13, "end_pos": 15, "type": "TASK", "confidence": 0.9043363928794861}]}, {"text": "Previous comparisons of these methods have mainly depended on empirical studies that are heavily affected by the experimental sets.", "labels": [], "entities": []}, {"text": "As a result, conclusions from those studies are sometimes inconsistent.", "labels": [], "entities": []}, {"text": "In order to better understand the relationship between these methods, building a general theoretical framework provides a fascinating perspective.", "labels": [], "entities": []}, {"text": "Furthermore, in real applications, selecting an appropriate FS method remains hard fora new task because too many FS methods are available due to the long history of FS studies.", "labels": [], "entities": [{"text": "FS", "start_pos": 60, "end_pos": 62, "type": "TASK", "confidence": 0.9030571579933167}, {"text": "FS", "start_pos": 166, "end_pos": 168, "type": "TASK", "confidence": 0.8602900505065918}]}, {"text": "For example, merely in an early survey paper), eight methods are mentioned.", "labels": [], "entities": []}, {"text": "These methods are provided by previous work for dealing with different text classification tasks but none of them is shown to be robust across different classification applications.", "labels": [], "entities": [{"text": "text classification tasks", "start_pos": 71, "end_pos": 96, "type": "TASK", "confidence": 0.8083915909131368}]}, {"text": "In this paper, we propose a framework with two basic measurements for theoretical comparison of six FS methods which are widely used in text classification.", "labels": [], "entities": [{"text": "FS", "start_pos": 100, "end_pos": 102, "type": "TASK", "confidence": 0.8059602379798889}, {"text": "text classification", "start_pos": 136, "end_pos": 155, "type": "TASK", "confidence": 0.8388816714286804}]}, {"text": "Moreover, a novel method is set forth that combines the two measurements and tunes their influences considering different application domains and numbers of selected features.", "labels": [], "entities": []}, {"text": "The remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 introduces the related work on feature selection for text classification.", "labels": [], "entities": [{"text": "feature selection", "start_pos": 41, "end_pos": 58, "type": "TASK", "confidence": 0.7068467736244202}, {"text": "text classification", "start_pos": 63, "end_pos": 82, "type": "TASK", "confidence": 0.8252343535423279}]}, {"text": "Section 3 theoretically analyzes six FS methods and proposes anew FS approach.", "labels": [], "entities": [{"text": "FS", "start_pos": 37, "end_pos": 39, "type": "TASK", "confidence": 0.8860234022140503}, {"text": "FS", "start_pos": 66, "end_pos": 68, "type": "TASK", "confidence": 0.7544934749603271}]}, {"text": "Experimental results are presented and analyzed in Section 4.", "labels": [], "entities": []}, {"text": "Finally, Section 5 draws our conclusions and outlines the future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "Data Set: The experiments are carried out on both topic-based and sentiment text classification datasets.", "labels": [], "entities": [{"text": "sentiment text classification", "start_pos": 66, "end_pos": 95, "type": "TASK", "confidence": 0.5882152915000916}]}, {"text": "In topic-based text classification, we use two popular data sets: one subset of Reuters-21578 referred to as R2 and the 20 Newsgroup dataset referred to as 20NG.", "labels": [], "entities": [{"text": "topic-based text classification", "start_pos": 3, "end_pos": 34, "type": "TASK", "confidence": 0.6786044438680013}, {"text": "Reuters-21578", "start_pos": 80, "end_pos": 93, "type": "DATASET", "confidence": 0.9554483294487}, {"text": "20 Newsgroup dataset", "start_pos": 120, "end_pos": 140, "type": "DATASET", "confidence": 0.8717893560727438}]}, {"text": "In detail, R2 consist of about 2,000 2-category documents from standard corpus of Reuters-21578.", "labels": [], "entities": [{"text": "standard corpus of Reuters-21578", "start_pos": 63, "end_pos": 95, "type": "DATASET", "confidence": 0.7592225670814514}]}, {"text": "And 20NG is a collection of approximately 20,000 20-category documents . In sentiment text classification, we also use two data sets: one is the widely used Cornell movie-review dataset 2 () and one dataset from product reviews of domain DVD.", "labels": [], "entities": [{"text": "sentiment text classification", "start_pos": 76, "end_pos": 105, "type": "TASK", "confidence": 0.9076874454816183}, {"text": "Cornell movie-review dataset 2", "start_pos": 157, "end_pos": 187, "type": "DATASET", "confidence": 0.858869880437851}]}, {"text": "Both of them are 2-category tasks and each consists of 2,000 reviews.", "labels": [], "entities": []}, {"text": "In our experiments, the document numbers of all data sets are (nearly) equally distributed cross all categories.", "labels": [], "entities": []}, {"text": "Classification Algorithm: Many classification algorithms are available for text classification, such as Na\u00efve Bayes, Maximum Entropy, k-NN, and SVM.", "labels": [], "entities": [{"text": "text classification", "start_pos": 75, "end_pos": 94, "type": "TASK", "confidence": 0.7340834587812424}]}, {"text": "Among these methods, SVM is shown to perform better than other methods).", "labels": [], "entities": [{"text": "SVM", "start_pos": 21, "end_pos": 24, "type": "TASK", "confidence": 0.9162102937698364}]}, {"text": "Hence we apply SVM algorithm with the help of the LIBSVM 4 tool.", "labels": [], "entities": []}, {"text": "Almost all parameters are set to their default values except the kernel function which is changed from a polynomial kernel function to a linear one because the linear one usually performs better for text classification tasks.", "labels": [], "entities": [{"text": "text classification tasks", "start_pos": 199, "end_pos": 224, "type": "TASK", "confidence": 0.8349197308222452}]}, {"text": "Experiment Implementation: In the experiments, each dataset is randomly and evenly split into two subsets: 90% documents as the training data and the remaining 10% as testing data.", "labels": [], "entities": [{"text": "Experiment Implementation", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.6851913332939148}]}, {"text": "The training data are used for training SVM classifiers, learning parameters in WFO method and selecting \"good\" features for each FS method.", "labels": [], "entities": [{"text": "SVM classifiers", "start_pos": 40, "end_pos": 55, "type": "TASK", "confidence": 0.8298389911651611}]}, {"text": "The features are single words with a bool weight (0 or 1), representing the presence or absence of a feature.", "labels": [], "entities": []}, {"text": "In addition to the \"principled\" FS methods, terms occurring in less than three documents ( 3 DF \u2264 ) in the training set are removed.", "labels": [], "entities": [{"text": "FS", "start_pos": 32, "end_pos": 34, "type": "TASK", "confidence": 0.9186998605728149}, {"text": "DF", "start_pos": 93, "end_pos": 95, "type": "METRIC", "confidence": 0.9576947093009949}]}], "tableCaptions": [{"text": " Table 1. The mean values of all top-2% terms' MI and DF scores using six FS methods in each domain", "labels": [], "entities": [{"text": "MI and DF scores", "start_pos": 47, "end_pos": 63, "type": "METRIC", "confidence": 0.7609193846583366}, {"text": "FS", "start_pos": 74, "end_pos": 76, "type": "METRIC", "confidence": 0.6162199378013611}]}]}