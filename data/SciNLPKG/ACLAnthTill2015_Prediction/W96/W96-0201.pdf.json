{"title": [{"text": "A Geometric Approach to Mapping Bitext Correspondence", "labels": [], "entities": [{"text": "Mapping Bitext Correspondence", "start_pos": 24, "end_pos": 53, "type": "TASK", "confidence": 0.7134180267651876}]}], "abstractContent": [{"text": "The first step inmost corpus-based multilingual NLP work is to construct a detailed map of the correspondence between a text and its translation.", "labels": [], "entities": []}, {"text": "Several automatic methods for this task have been proposed in recent years.", "labels": [], "entities": []}, {"text": "\"Yet even the best of these methods can err by several typeset pages.", "labels": [], "entities": []}, {"text": "The Smooth Injective Map Recog-nizer (SIMR) is anew bitext mapping algorithm.", "labels": [], "entities": []}, {"text": "SIMR's errors are smaller than those of the previous front-runner by more than a factor of 4.", "labels": [], "entities": [{"text": "errors", "start_pos": 7, "end_pos": 13, "type": "METRIC", "confidence": 0.9783737659454346}]}, {"text": "Its robustness has enabled new commercial-quality applications.", "labels": [], "entities": []}, {"text": "The greedy nature of the algorithm makes it independent of memory resources.", "labels": [], "entities": []}, {"text": "Unlike other bitext mapping algorithms, SIMR allows crossing correspondences to account for word order differences.", "labels": [], "entities": []}, {"text": "Its output can be converted quickly and easily into a sentence alignment.", "labels": [], "entities": [{"text": "sentence alignment", "start_pos": 54, "end_pos": 72, "type": "TASK", "confidence": 0.7036676108837128}]}, {"text": "SIMR's output has been used to align more than 200 megabytes of the Canadian Hansards for publication by the Linguistic Data Consortium.", "labels": [], "entities": [{"text": "Canadian Hansards", "start_pos": 68, "end_pos": 85, "type": "DATASET", "confidence": 0.9250279664993286}]}], "introductionContent": [{"text": "The first step inmost corpus-based multilingual NLP work is to construct a detailed map of the correspondence between a text and its translation (a bitext map).", "labels": [], "entities": []}, {"text": "Several automatic methods have been proposed for this task in recent years.", "labels": [], "entities": []}, {"text": "However, most of these methods address only the sub-problem of alignment (.", "labels": [], "entities": [{"text": "alignment", "start_pos": 63, "end_pos": 72, "type": "TASK", "confidence": 0.9762392640113831}]}, {"text": "Alignment algorithms assume the availability of text unit boundary information and their output has less expressive power than a general bitext map.", "labels": [], "entities": []}, {"text": "The only published solution to the more difficult general bitext mapping problem can err by several typeset pages.", "labels": [], "entities": []}, {"text": "Such frailty can expose lexicographers and terminologists to spurious concordances, feed noisy training data into statistical translation models, and degrade the performance of corpus-based machine translation.", "labels": [], "entities": [{"text": "corpus-based machine translation", "start_pos": 177, "end_pos": 209, "type": "TASK", "confidence": 0.6855426828066508}]}, {"text": "Some multilingual NLP tasks, such as automatic validation of terminological consistency and automatic detection of omissions in translations (implemented for the first time in), have been technologically impossible until now, because they are highly sensitive to large errors in the bitext map.", "labels": [], "entities": [{"text": "automatic detection of omissions in translations", "start_pos": 92, "end_pos": 140, "type": "TASK", "confidence": 0.762108713388443}]}, {"text": "The Smooth Injective Map Recognizer (SIMR) is a greedy algorithm for mapping bitext correspondence.", "labels": [], "entities": [{"text": "Smooth Injective Map Recognizer (SIMR)", "start_pos": 4, "end_pos": 42, "type": "TASK", "confidence": 0.7059656807354519}, {"text": "mapping bitext correspondence", "start_pos": 69, "end_pos": 98, "type": "TASK", "confidence": 0.7604543964068095}]}, {"text": "SIMR borrows several insights from previous work.", "labels": [], "entities": [{"text": "SIMR", "start_pos": 0, "end_pos": 4, "type": "TASK", "confidence": 0.9456233978271484}]}, {"text": "Like and, SIMR relies on the high correlation between the lengths of mutual translations.", "labels": [], "entities": [{"text": "SIMR", "start_pos": 10, "end_pos": 14, "type": "TASK", "confidence": 0.9722430109977722}]}, {"text": "Like char_.align, SIMR infers bitext maps from likely points of correspondence between the two texts, points that are plotted in a two-dimensional space of possibilities.", "labels": [], "entities": []}, {"text": "Unlike previous methods, SIMR searches for only a handful of points of correspondence at a time.", "labels": [], "entities": [{"text": "SIMR", "start_pos": 25, "end_pos": 29, "type": "TASK", "confidence": 0.9700477123260498}]}, {"text": "Each set of correspondence points is found in two steps.", "labels": [], "entities": []}, {"text": "First, SIMR generates a number of possible points of correspondence between the two texts, as described in Section 3.1.", "labels": [], "entities": [{"text": "SIMR", "start_pos": 7, "end_pos": 11, "type": "TASK", "confidence": 0.9634749293327332}]}, {"text": "Second, SIMR selects those points whose geometric arrangement most resembles the typical arrangement of true points of correspondence.", "labels": [], "entities": [{"text": "SIMR", "start_pos": 8, "end_pos": 12, "type": "TASK", "confidence": 0.9215670228004456}]}, {"text": "This selection involves localized pattern recognition heuristics, which Section 3.2 refers to collectively as the chain recognition heuristic.", "labels": [], "entities": [{"text": "pattern recognition heuristics", "start_pos": 34, "end_pos": 64, "type": "TASK", "confidence": 0.7994698882102966}, {"text": "chain recognition heuristic", "start_pos": 114, "end_pos": 141, "type": "TASK", "confidence": 0.7803177237510681}]}, {"text": "SIMR then interpolates between successive selected points to produce a bitext map, as described in Section 3.3.", "labels": [], "entities": []}], "datasetContent": [{"text": "The standard method of evaluating bitext mapping algorithms is to compare their output to a hand-constructed reference set of TPCs.", "labels": [], "entities": [{"text": "bitext mapping", "start_pos": 34, "end_pos": 48, "type": "TASK", "confidence": 0.6167419105768204}]}, {"text": "Michel Simard of CITI graciously provided me with several such reference sets for French-English bitexts, including the same \"easy\" and \"hard\" Hansard bitexts that have been used to evaluate other bitext mapping and alignment algorithms in the literature (.", "labels": [], "entities": [{"text": "bitext mapping and alignment", "start_pos": 197, "end_pos": 225, "type": "TASK", "confidence": 0.6245015412569046}]}, {"text": "A non-Hansard reference set was used for SIMR's development.", "labels": [], "entities": [{"text": "SIMR", "start_pos": 41, "end_pos": 45, "type": "TASK", "confidence": 0.959909200668335}]}, {"text": "All of SIMR's parameters, namely the thresholds for maximum point dispersal, maximum angle deviation, maximum point ambiguity, and the LCSR used in the matching predicate, as well as the fixed chain size, were simultaneously optimized on this data set using simulated annealing.", "labels": [], "entities": [{"text": "LCSR", "start_pos": 135, "end_pos": 139, "type": "METRIC", "confidence": 0.9806944131851196}]}, {"text": "Different parameter settings considered by the optimization process resulted in different bitext maps for the development bitext.", "labels": [], "entities": []}, {"text": "Each set of parameter values was scored according to the root mean squared error between the resulting bitext map and the reference set of TPCs.", "labels": [], "entities": []}, {"text": "The best-scoring set of parameter values was used to evaluate SIMR.", "labels": [], "entities": [{"text": "SIMR", "start_pos": 62, "end_pos": 66, "type": "TASK", "confidence": 0.9373080730438232}]}, {"text": "SIMR was evaluated on the \"easy\" and \"hard\" Hansard bitexts.", "labels": [], "entities": [{"text": "SIMR", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.5601463317871094}, {"text": "Hansard bitexts", "start_pos": 44, "end_pos": 59, "type": "DATASET", "confidence": 0.8283154368400574}]}, {"text": "Note that these bitexts are so named because one was easier than the other for the alignment algorithm that was first evaluated on them.", "labels": [], "entities": []}, {"text": "There is no a priori reason to believe that one or the other will be easier for SIMR.", "labels": [], "entities": [{"text": "SIMR", "start_pos": 80, "end_pos": 84, "type": "TASK", "confidence": 0.9325347542762756}]}, {"text": "compares SIMR's error distribution on these bitexts with that of the previous front-runner, char._al:i.gn, as reported by Church 7 (1993).", "labels": [], "entities": [{"text": "SIMR's error distribution", "start_pos": 9, "end_pos": 34, "type": "METRIC", "confidence": 0.6149097084999084}, {"text": "Church 7 (1993)", "start_pos": 122, "end_pos": 137, "type": "DATASET", "confidence": 0.9460434198379517}]}, {"text": "SIMR's RMS error is lower by more than a factor of 4.", "labels": [], "entities": [{"text": "RMS error", "start_pos": 7, "end_pos": 16, "type": "METRIC", "confidence": 0.7014367878437042}]}, {"text": "SIMR is also much more robust: it rarely errs by more than half the length of an average sentence.", "labels": [], "entities": [{"text": "SIMR", "start_pos": 0, "end_pos": 4, "type": "TASK", "confidence": 0.906068742275238}]}, {"text": "Such robustness has enabled at least one new commercial-quality application --automatic detection of omissions in translations.", "labels": [], "entities": [{"text": "automatic detection of omissions in translations", "start_pos": 78, "end_pos": 126, "type": "TASK", "confidence": 0.7647062291701635}]}, {"text": "This task was impossible until now, because it cannot tolerate even a few wild errors, such as those produced by an independent implementation of char_al:i.gn.", "labels": [], "entities": []}, {"text": "Note that the error between a bitext map and each reference point can be defined as the horizontal distance, the vertical distance, or the distance perpendicular to the main diagonal.", "labels": [], "entities": []}, {"text": "The latter distance will always be shortest, on average.", "labels": [], "entities": [{"text": "shortest", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9584900140762329}]}, {"text": "Church (1993) did not specify which metric he used.", "labels": [], "entities": []}, {"text": "Of the three possibilities, conservatively reports the highest error estimates for SIMR.", "labels": [], "entities": [{"text": "SIMR", "start_pos": 83, "end_pos": 87, "type": "TASK", "confidence": 0.8095628619194031}]}, {"text": "The lowest estimates for SIMR without the translation lexicon are an RMS error of 6.1 for the \"easy\" bitext and 5.4 for the \"hard\" bitext.", "labels": [], "entities": [{"text": "SIMR", "start_pos": 25, "end_pos": 29, "type": "TASK", "confidence": 0.9326230883598328}, {"text": "RMS error", "start_pos": 69, "end_pos": 78, "type": "METRIC", "confidence": 0.8767708539962769}]}, {"text": "With the translation lexicon, the lowest error estimates drop to 6.0 for the \"easy\" bitext and 4.6 for the \"hard\" bitext.", "labels": [], "entities": [{"text": "error", "start_pos": 41, "end_pos": 46, "type": "METRIC", "confidence": 0.9618853330612183}]}], "tableCaptions": [{"text": " Table 1: Comparison of error distributions for SIMR and char_align, in characters.", "labels": [], "entities": []}]}