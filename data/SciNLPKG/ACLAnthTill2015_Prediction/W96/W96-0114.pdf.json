{"title": [{"text": "Towards Automatic Grammar Acquisition from a Bracketed Corpus", "labels": [], "entities": [{"text": "Automatic Grammar Acquisition", "start_pos": 8, "end_pos": 37, "type": "TASK", "confidence": 0.7097470362981161}]}], "abstractContent": [{"text": "In this paper, we propose a method to group brackets in a bracketed corpus (with lexical tags), according to their local contextual information, as a first step towards the automatic acquisition of a context-free grammar.", "labels": [], "entities": []}, {"text": "Using a bracketed corpus, the learning task is reduced to the problem of how to determine the nonterminal label of each bracket in the corpus.", "labels": [], "entities": []}, {"text": "Ina grouping process, a single nonterminai label is assigned to each group of brackets which are similar.", "labels": [], "entities": []}, {"text": "Two techniques, distributional analysis and hierarchical Bayesian clustering, are applied to exploit local contextual information for computing similarity between two brackets.", "labels": [], "entities": [{"text": "distributional analysis", "start_pos": 16, "end_pos": 39, "type": "TASK", "confidence": 0.8290091753005981}]}, {"text": "We also show a technique developed for determining the appropriate number of bracket groups based on the concept of entropy analysis.", "labels": [], "entities": [{"text": "entropy analysis", "start_pos": 116, "end_pos": 132, "type": "TASK", "confidence": 0.772496223449707}]}, {"text": "Finally, we present a set of experimental results and evaluate the obtained results with a model solution given by humans.", "labels": [], "entities": []}], "introductionContent": [{"text": "Designing and refining a natural language grammar is a diiBcult and time-consuming task and requires a large amount of skilled effort.", "labels": [], "entities": []}, {"text": "A hand-crafted grammar is usually not completely satisfactory and frequently fails to cover many unseen sentences.", "labels": [], "entities": []}, {"text": "Automatic acquisition of grammars is a solution to this problem.", "labels": [], "entities": [{"text": "Automatic acquisition of grammars", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.8125354051589966}]}, {"text": "Recently, with the increasing availability of large, machine-readable, parsed corpora, there have been numerous attempts to automatically acquire a CFG grammar through the application of enormous existing corporaILar90][Per92].", "labels": [], "entities": [{"text": "Per92", "start_pos": 220, "end_pos": 225, "type": "DATASET", "confidence": 0.9434213042259216}]}, {"text": "proposed so-called inside-outside algorithm, which constructs a grammar from an unbracketed corpus based on probability theory.", "labels": [], "entities": []}, {"text": "The grammar acquired by this method is assumed to be in Chomsky normal form and a large amount of computation is required.", "labels": [], "entities": []}, {"text": "Later, Pereira applied this algorithm to a partially bracketed corpus to improve the computation time.[Kiy94a] combined symbolic and statistical approaches to extract useful grammar rules from a partially bracketed corpus.", "labels": [], "entities": []}, {"text": "To avoid generating a large number of grammar rules, some basic grammatical constraints, local boundaries constraints and X bar-theory were applied.", "labels": [], "entities": []}, {"text": "Kiyono's approach performed a refinement of an original grammar by adding some additional rules while the inside-outside algorithm tries to construct a whole grammar from a corpus based on Maximum Likelihood.", "labels": [], "entities": []}, {"text": "However, it is costly to obtain a suitable grammar from an unbracketed corpus and hard to evaluate results of these approaches.", "labels": [], "entities": []}, {"text": "As the increase of the construction of bracketed corpora, an attempt to use a bracketed (tagged) corpus for grammar inference was made by Shiral.", "labels": [], "entities": []}, {"text": "Shirai constructed a Japanese grammar based on some simple rules to give a name (a label) to each bracket in the corpus.", "labels": [], "entities": []}, {"text": "To reduce the grammar size and ambiguity, some hand-encoded knowledge is applied in this approach.", "labels": [], "entities": []}, {"text": "In our work, like Shirai's approach, we make use of a bracketed corpus with lexical tags, but instead of using a set of human-encoded predefined rules to give a name (a label) to each bracket, we introduce some statistical techniques to acquire such label automatically.", "labels": [], "entities": []}, {"text": "Using a bracketed corpus, the grammar learning task is reduced to the problem of how to determine the nonterminal label of each bracket in the corpus.", "labels": [], "entities": []}, {"text": "More precisely, this task is concerned with the way to classify brackets to some certain groups and give each group a label.", "labels": [], "entities": []}, {"text": "We propose a method to group brackets in a bracketed corpus (with lexical tags), according to their local contextual information, as a first step towards the automatic acquisition of a context-free grammar.", "labels": [], "entities": []}, {"text": "In the grouping process, a single nontermina] label is assigned to each group of brackets which are similar.", "labels": [], "entities": []}, {"text": "To do this, we apply and compare two types of techniques called distributional analysis and hierarchical Bayesian clustering for setting a measure representing similarity among the bracket groups.", "labels": [], "entities": [{"text": "distributional analysis", "start_pos": 64, "end_pos": 87, "type": "TASK", "confidence": 0.680361419916153}]}, {"text": "We also propose a method to determine the appropriate number of bracket groups based on the concept of entropy analysis.", "labels": [], "entities": [{"text": "entropy analysis", "start_pos": 103, "end_pos": 119, "type": "TASK", "confidence": 0.753825843334198}]}, {"text": "Finally, we present a set of experimental results and evaluate our methods with a model solution given by humans.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we show some results of our preliminary experiments to confirm effectiveness of the proposed techniques.", "labels": [], "entities": []}, {"text": "The corpus we used is constructed by EDR and includes nearly 48,000 bracketed, tagged sentences.", "labels": [], "entities": [{"text": "EDR", "start_pos": 37, "end_pos": 40, "type": "DATASET", "confidence": 0.9480676651000977}]}, {"text": "As mentioned in the previous sections, we focus on only the rules with lexical categories as their right hand side 4.", "labels": [], "entities": []}, {"text": "For instance, cx --~ (ADJ)(NOUN), e2 --* (ART)(NOUN) and cs --* (PRON)(NOUN) in.", "labels": [], "entities": [{"text": "ART)(NOUN)", "start_pos": 42, "end_pos": 52, "type": "METRIC", "confidence": 0.8635923862457275}]}, {"text": "To evaluate our method, we use the rule tokens which appear more than 500 times in the corpus.", "labels": [], "entities": []}, {"text": "gives some characteristics of the corpus.", "labels": [], "entities": []}, {"text": "From the 35 initial rules, we calculate the similarity between any two rules (i.e., any rule pair) based on divergence and Bayesian posterior probability (BPP).", "labels": [], "entities": [{"text": "similarity", "start_pos": 44, "end_pos": 54, "type": "METRIC", "confidence": 0.977112889289856}, {"text": "Bayesian posterior probability (BPP)", "start_pos": 123, "end_pos": 159, "type": "METRIC", "confidence": 0.8315490285555521}]}, {"text": "For the divergence measure, the smaller the vaiue is, the more similar the rule pair is.", "labels": [], "entities": [{"text": "divergence", "start_pos": 8, "end_pos": 18, "type": "TASK", "confidence": 0.9121710658073425}]}, {"text": "Inversely, for BPP, the larger the value is, the more similar the pair looks.", "labels": [], "entities": [{"text": "BPP", "start_pos": 15, "end_pos": 18, "type": "DATASET", "confidence": 0.6079879403114319}]}, {"text": "After calculating all pairs' similarities, we merge the most similar pair (the minimum divergence or the maximum BPP) to anew label and recalculate the similarity of the new label with other remaining labels.", "labels": [], "entities": [{"text": "BPP", "start_pos": 113, "end_pos": 116, "type": "METRIC", "confidence": 0.9723630547523499}]}, {"text": "The merging process is carried out in an iterative way.", "labels": [], "entities": []}, {"text": "shows the minimum divergence (left) and the maximum Bayesian posterior probability (right) of each merge step.", "labels": [], "entities": [{"text": "Bayesian posterior probability", "start_pos": 52, "end_pos": 82, "type": "METRIC", "confidence": 0.8786935806274414}]}, {"text": "In each iterative step of the merging process, we calculate differential entropy for both cases.", "labels": [], "entities": []}, {"text": "The differential entropy of each step equals to the entropy difference between the entropy of two rules before merging and the entropy of anew rule after merging as described in the previous section.", "labels": [], "entities": []}, {"text": "4Other types of rules can be acquired in almost the same way and are left now as our further work.", "labels": [], "entities": []}, {"text": "Merge Step \" Merge Step: The minimum divergence (left) and the maximum Bayesian posterior probability (right) of each merge step Two graphs in indicate the results of differential entropy (LXE calculated by the formula in section 4) when the merging process advanced with divergence and BPP as its similarity measures.", "labels": [], "entities": [{"text": "Bayesian posterior probability", "start_pos": 71, "end_pos": 101, "type": "METRIC", "confidence": 0.7719935973485311}, {"text": "BPP", "start_pos": 287, "end_pos": 290, "type": "METRIC", "confidence": 0.9881864190101624}]}, {"text": "There are some sharp peaks indicating the rapid fluctuation of entropy in the graphs.", "labels": [], "entities": []}, {"text": "In this work, we use these peaks as a clue to find the timing we should to terminate the merging process.", "labels": [], "entities": []}, {"text": "As the result, we halt up the process at the 22nd step and the 27th step for the cases of divergence and BPP, respectively.", "labels": [], "entities": [{"text": "divergence", "start_pos": 90, "end_pos": 100, "type": "TASK", "confidence": 0.951473593711853}, {"text": "BPP", "start_pos": 105, "end_pos": 108, "type": "METRIC", "confidence": 0.9253333806991577}]}, {"text": "shows the obtained grouping results.", "labels": [], "entities": []}, {"text": "In these tables, there axe 13 groups for divergence and 8 groups for Bayesian posterior probability.", "labels": [], "entities": [{"text": "divergence", "start_pos": 41, "end_pos": 51, "type": "TASK", "confidence": 0.9754289388656616}]}, {"text": "To clarify the result in the tables, Some sample words of each label axe given in the appendix.", "labels": [], "entities": []}, {"text": "We also made an experiment to evaluate these results with the solution given by three human evaluators (later called A, B and C) who axe non-natlve but high-educated with more than 20 years of English education.", "labels": [], "entities": []}, {"text": "The evaluators were told to construct 7-15 groups from the 35 initial rules, based on the grammatical similarity as they thought.", "labels": [], "entities": []}, {"text": "As the result, the evaluators A, B and C classified the rules into 14, 13 and 14 groups, respectively.: The number of entry pairs for evaluating accuracy To evaluate the system with the model solutions, we applied a contingency table model as one shown in.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 145, "end_pos": 153, "type": "METRIC", "confidence": 0.9981824159622192}]}, {"text": "This table model was introduced in and widely used in Information Retrieval and Psychology.", "labels": [], "entities": [{"text": "Information Retrieval", "start_pos": 54, "end_pos": 75, "type": "TASK", "confidence": 0.8438916206359863}]}, {"text": "In the table, a is the number of the label pairs which an evaiuator assigned in the same group and so did the system, b is the number of the pairs which an evaluator did not assign in the same group but the system did, e is the number of the pairs which an evaluator assigned but the system did not, and dis the number of the pairs which both an evaluator and the system did not assign in the same group.", "labels": [], "entities": []}, {"text": "From this table, we define seven measures, as shown below, for evaluating performance of the proposed methods.", "labels": [], "entities": []}, {"text": "This evaluation technique was also applied partly for computing \"closeness\" between a system's answer and an evaluator's answer in \u2022 Positive Recall (PR) : \u2022 Positive Precision (PP) :: Eva~luation results using three human eva~uators' solutions From these results, we observe some features as follows.", "labels": [], "entities": [{"text": "Positive Recall (PR)", "start_pos": 133, "end_pos": 153, "type": "METRIC", "confidence": 0.7880947113037109}]}, {"text": "The divergence gives a better solution than Bayesian posterior probability does.", "labels": [], "entities": [{"text": "divergence", "start_pos": 4, "end_pos": 14, "type": "TASK", "confidence": 0.9623462557792664}]}, {"text": "Normally, the positive measures (PR and PP) have smaller values than the negative ones (NR and NP) do.", "labels": [], "entities": []}, {"text": "This means that it is difficult to judge two labels to be in a same group rather than to judge them to be in a separate group.", "labels": [], "entities": []}, {"text": "Using divergence as a similarity measure, we get, on average, 84 % positive recall and 67 ~ positive precision and up to 90 ~ and 82 % when considering both positive and negative measures.", "labels": [], "entities": [{"text": "divergence", "start_pos": 6, "end_pos": 16, "type": "TASK", "confidence": 0.9494861960411072}, {"text": "recall", "start_pos": 76, "end_pos": 82, "type": "METRIC", "confidence": 0.9812631607055664}, {"text": "precision", "start_pos": 101, "end_pos": 110, "type": "METRIC", "confidence": 0.9628180265426636}]}, {"text": "Even for the worst result(Evaluator B), we can getup to 84 % and 81% for averaged recall and precision.", "labels": [], "entities": [{"text": "Evaluator B)", "start_pos": 26, "end_pos": 38, "type": "METRIC", "confidence": 0.9454108277956644}, {"text": "recall", "start_pos": 82, "end_pos": 88, "type": "METRIC", "confidence": 0.9962544441223145}, {"text": "precision", "start_pos": 93, "end_pos": 102, "type": "METRIC", "confidence": 0.9971352815628052}]}, {"text": "In order to confirm the performance of the system, the evaluators' results axe compared with each other.", "labels": [], "entities": []}, {"text": "This comparison is useful for investigating the difficulty of the grouping problem.", "labels": [], "entities": [{"text": "grouping problem", "start_pos": 66, "end_pos": 82, "type": "TASK", "confidence": 0.9165288805961609}]}, {"text": "The comparison result is shown in.", "labels": [], "entities": []}, {"text": "At this point, we can observe that the label grouping process is a hard problem that may make an evaluator's solution inconsistent with the others' solutions.", "labels": [], "entities": [{"text": "label grouping", "start_pos": 39, "end_pos": 53, "type": "TASK", "confidence": 0.7254907488822937}]}, {"text": "However, our proposed method seem to give a reconciliation solution between those solutions.", "labels": [], "entities": []}, {"text": "Especially, the method which applies divergence as the similarity measure, has a good performance in grouping brackets in the bracketed corpus.", "labels": [], "entities": []}, {"text": "We also make an experiment to evaluate whether divergence is a better measure than BPP, and whether the application of differential entropy to cutoff the merging process is appropriate.", "labels": [], "entities": [{"text": "BPP", "start_pos": 83, "end_pos": 86, "type": "METRIC", "confidence": 0.8798695206642151}]}, {"text": "This examination can beheld by plotting values of recall, precision and F-measure during each step of merging process.", "labels": [], "entities": [{"text": "recall", "start_pos": 50, "end_pos": 56, "type": "METRIC", "confidence": 0.9996321201324463}, {"text": "precision", "start_pos": 58, "end_pos": 67, "type": "METRIC", "confidence": 0.9995102882385254}, {"text": "F-measure", "start_pos": 72, "end_pos": 81, "type": "METRIC", "confidence": 0.9988901019096375}]}, {"text": "shows the fluctuation of positive recall(PR), positive preclsion(AP), averaged recall(AR), averaged precision and F-measure (FM).", "labels": [], "entities": [{"text": "recall(PR)", "start_pos": 34, "end_pos": 44, "type": "METRIC", "confidence": 0.9377730190753937}, {"text": "positive preclsion(AP)", "start_pos": 46, "end_pos": 68, "type": "METRIC", "confidence": 0.7964073657989502}, {"text": "averaged recall(AR)", "start_pos": 70, "end_pos": 89, "type": "METRIC", "confidence": 0.8802783846855163}, {"text": "precision", "start_pos": 100, "end_pos": 109, "type": "METRIC", "confidence": 0.9285818934440613}, {"text": "F-measure (FM)", "start_pos": 114, "end_pos": 128, "type": "METRIC", "confidence": 0.9637643992900848}]}, {"text": "From the graphs, we found out that the maximum value of F-measure is 0.75 in the case of divergence while it is only 0.65 in the case of BPP.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 56, "end_pos": 65, "type": "METRIC", "confidence": 0.9652118682861328}, {"text": "divergence", "start_pos": 89, "end_pos": 99, "type": "TASK", "confidence": 0.9565923810005188}, {"text": "BPP", "start_pos": 137, "end_pos": 140, "type": "DATASET", "confidence": 0.8192784786224365}]}, {"text": "That is, divergence provides a better solution than BPP.", "labels": [], "entities": [{"text": "divergence", "start_pos": 9, "end_pos": 19, "type": "TASK", "confidence": 0.9878708720207214}]}, {"text": "Moreover, the 22nd an 25th merge steps were the most suitable points to terminate the merging process for divergence and BPP, respectively.", "labels": [], "entities": [{"text": "divergence", "start_pos": 106, "end_pos": 116, "type": "TASK", "confidence": 0.9814515709877014}, {"text": "BPP", "start_pos": 121, "end_pos": 124, "type": "METRIC", "confidence": 0.8269550204277039}]}, {"text": "This result is consistent with the grouping result of our system (13 groups) in the case of divergence.", "labels": [], "entities": [{"text": "divergence", "start_pos": 92, "end_pos": 102, "type": "TASK", "confidence": 0.9584741592407227}]}, {"text": "Although differential entropy leads us to terminate the merging process at the 27th merge step in the case of BPP, we observe that there is just a little difference between the F-measure value of the 25th merge step and that of the 27th merge step.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 177, "end_pos": 186, "type": "METRIC", "confidence": 0.9935826659202576}]}, {"text": "From this result, we conclude that differential entropy can be used a good measure to predict the cut-off timing of the merging process.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: Eva~luation results using three human eva~uators' solutions", "labels": [], "entities": []}, {"text": " Table 5. At this point, we can observe that the label grouping process is a hard  problem that may make an evaluator's solution inconsistent with the others' solutions. However,  our proposed method seem to give a reconciliation solution between those solutions. Especially, the  method which applies divergence as the similarity measure, has a good performance in grouping  brackets in the bracketed corpus.", "labels": [], "entities": [{"text": "label grouping", "start_pos": 49, "end_pos": 63, "type": "TASK", "confidence": 0.7130471616983414}]}, {"text": " Table 5: Comparing the grouping results obtained by the evaluators(A,B,C)", "labels": [], "entities": []}]}