{"title": [{"text": "Modeling Conversational Speech for Speech Recognition", "labels": [], "entities": [{"text": "Speech Recognition", "start_pos": 35, "end_pos": 53, "type": "TASK", "confidence": 0.6784791201353073}]}], "abstractContent": [{"text": "In language modeling for speech recognition the goal is to constrain the search of the speech recognizer by providing a model which can, given a context, indicate what the next most likely word will be.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 25, "end_pos": 43, "type": "TASK", "confidence": 0.7449377775192261}]}, {"text": "In this paper, we explore how the addition of information to the text, in particular part of speech and dysfluency annotations, can be used to,build more complex language models.", "labels": [], "entities": []}, {"text": "In particular, we ask two questions.", "labels": [], "entities": []}, {"text": "First, in conversational speech, where there is a less clear notion of \"sentence\" than in written text, does segmenting the text into linguistically or semantically based units contribute to a better language model than merely segmenting based on broad acoustic information, such as pauses.", "labels": [], "entities": []}, {"text": "Second, is the sentence itself a good unit to be modeling, or should we look at smaller units, for example, dividing a sentence into a \"given\" and \"new\" portion and segmenting out acknowledgments and replies.", "labels": [], "entities": []}, {"text": "To answer these questions, we present a variety of kinds of analysis, from vocabulary distributions to perplexities on language models.", "labels": [], "entities": []}, {"text": "The next step will be modeling conversations and incorporating those models into a speech recognizer.", "labels": [], "entities": [{"text": "speech recognizer", "start_pos": 83, "end_pos": 100, "type": "TASK", "confidence": 0.6906288117170334}]}], "introductionContent": [{"text": "In language modeling for speech recognition the goal is to constrain the search of the speech recognizer by providing a model which can, given a context, indicate what the next most likely word will be.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 25, "end_pos": 43, "type": "TASK", "confidence": 0.7449377775192261}]}, {"text": "Currently, the field is predominated by the use of n-grams, in particular bi-grams and tri-grams.", "labels": [], "entities": []}, {"text": "One advantage of this type of model is that only the text itself is needed to create the model.", "labels": [], "entities": []}, {"text": "In the case where what is being modeled is written text in a style for which millions of words of text are available, such as the WSJ corpus, this kind of modeling is effective.", "labels": [], "entities": [{"text": "WSJ corpus", "start_pos": 130, "end_pos": 140, "type": "DATASET", "confidence": 0.9739297926425934}]}, {"text": "However, this is rarely the case since the ultimate goal of speech recognition is to model extemporaneous spoken language.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 60, "end_pos": 78, "type": "TASK", "confidence": 0.7279184013605118}]}, {"text": "The interesting problem in language modeling is how to bring generalizations above the level of the words themselves to the text.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 27, "end_pos": 44, "type": "TASK", "confidence": 0.7161976546049118}]}, {"text": "One approach is to annotate text, either by hand or semi-automatically, to bring additional information to the text.", "labels": [], "entities": []}, {"text": "Another is to develop algorithms that use rules or heuristics that operate over the corpus, again bringing additional information.", "labels": [], "entities": []}, {"text": "It is this additional information that can help create generalizations over the text and contribute to a model which can go beyond the training corpus.", "labels": [], "entities": []}, {"text": "In this paper, we describe annotations to the Switchboard corpus which add part of speech and dysfluency markings and our latest work using those annotations to create a level of generalization on top of the annotation to capture the structure of conversational speech.", "labels": [], "entities": [{"text": "Switchboard corpus", "start_pos": 46, "end_pos": 64, "type": "DATASET", "confidence": 0.888578861951828}]}, {"text": "In particular, we ask two questions.", "labels": [], "entities": []}, {"text": "First, in conversational speech, where there is a less clear notion of \"sentence\" than in written text, does segmenting the text into linguistically or semantically based units contribute to a better language model than merely segmenting based on broad acoustic information, such as pauses.", "labels": [], "entities": []}, {"text": "Second, is the sentence itself a good unit to be modeling, or should we look at smaller units, for example, dividing a sentence into a \"given\" and \"new\" portion.", "labels": [], "entities": []}, {"text": "To answer these questions, we present a variety of kinds of analysis, from vocabulary distributions to perplexities on language models.", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to analyze the above issues, we first obtained our baseline training and testing data.", "labels": [], "entities": []}, {"text": "Since the linguistic segmentations are available for only two thirds of the Switchboard data, we decided to use the corresponding two thirds of the acoustically segmented training data for our comparative experiments.", "labels": [], "entities": [{"text": "Switchboard data", "start_pos": 76, "end_pos": 92, "type": "DATASET", "confidence": 0.9171037673950195}]}, {"text": "The test set was obtained from the Switchboard lattices which served as the baseline for the 1995 Language Modeling Workshop at Johns Hopkins.", "labels": [], "entities": [{"text": "1995 Language Modeling Workshop at Johns Hopkins", "start_pos": 93, "end_pos": 141, "type": "TASK", "confidence": 0.7287496200629643}]}, {"text": "The test set was acoustically segmented.", "labels": [], "entities": []}, {"text": "A corresponding linguistically segmented test set was also made available 2.", "labels": [], "entities": []}, {"text": "There were a couple of experimental constraints to analyze the aforementioned issues in terms of recognition word error rate.", "labels": [], "entities": []}, {"text": "We were constrained to use the lattices that had been provided to the workshop.", "labels": [], "entities": []}, {"text": "Since these lattices were built on acoustic segments, the models had to deal with implicit acoustic segment boundaries.", "labels": [], "entities": []}, {"text": "The context from the previous lattice was not provided for the current lattice.", "labels": [], "entities": []}, {"text": "We tried to alleviate this problem by trying to provide the context for the current lattice by selecting the most likely pair of words from the previous lattice using pair occurrence frequency.", "labels": [], "entities": []}, {"text": "One problem with this approch is that since the standard Switchboard WER is about 50%, about 73% of the time we were providing incorrect context using these lattices.", "labels": [], "entities": []}, {"text": "We used our segment hypothesizing scheme for scoring an N-best list corresponding to these lattices (N=2500).", "labels": [], "entities": []}, {"text": "While the initial context was provided for the N-best lists, we had to throwaway the final segment boundary.", "labels": [], "entities": []}, {"text": "This led to a degradation in performance., the mismatch between the training and test segmentations degrades performance by half a point, from 50.46% to 50.88%.", "labels": [], "entities": []}, {"text": "Throwing out the end segment boundary from the N-best lists degrades performance by slightly more than an absolute 1%.", "labels": [], "entities": []}, {"text": "Also, the ling-seg model does slightly better at hypothesizing segment boundaries than the acoustic-seg model.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Trigram perplexity measurements on  LM95 SWBD dev. test set", "labels": [], "entities": [{"text": "LM95 SWBD dev. test set", "start_pos": 46, "end_pos": 69, "type": "DATASET", "confidence": 0.9345164795716604}]}, {"text": " Table 3: N-best rescoring performance (N=2500)  measurements on LM95 SWBD dev. test set", "labels": [], "entities": [{"text": "N-best rescoring performance", "start_pos": 10, "end_pos": 38, "type": "METRIC", "confidence": 0.8435663382212321}, {"text": "LM95 SWBD dev. test set", "start_pos": 65, "end_pos": 88, "type": "DATASET", "confidence": 0.9352593123912811}]}, {"text": " Table 5: Switchboard Corpus Divided by Pivot Point", "labels": [], "entities": [{"text": "Switchboard Corpus Divided", "start_pos": 10, "end_pos": 36, "type": "TASK", "confidence": 0.701000988483429}]}, {"text": " Table 6: Totals for 15 Most Frequent words", "labels": [], "entities": [{"text": "Totals", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9777758717536926}]}, {"text": " Table 7: Before and After Totals for 50 Most Frequent words", "labels": [], "entities": [{"text": "After Totals", "start_pos": 21, "end_pos": 33, "type": "METRIC", "confidence": 0.8633127212524414}]}, {"text": " Table 8: Vocabulary Differences in Before Pivot  and After Pivot Sentence Parts", "labels": [], "entities": [{"text": "Vocabulary Differences", "start_pos": 10, "end_pos": 32, "type": "TASK", "confidence": 0.8993403017520905}]}, {"text": " Table 9: Words occurring more than 50 times in the After Pivot sentence parts and  not at all in the Before Pivot parts", "labels": [], "entities": []}, {"text": " Table 11: Perplexity experiments on full sentences, before and after subparts", "labels": [], "entities": []}, {"text": " Table 12: Bigram Transition Probabilities for  Conversational Model", "labels": [], "entities": []}]}