{"title": [{"text": "A Re-estimation Method for Stochastic Language Modeling from Ambiguous Observations", "labels": [], "entities": [{"text": "Stochastic Language Modeling from Ambiguous Observations", "start_pos": 27, "end_pos": 83, "type": "TASK", "confidence": 0.772102932135264}]}], "abstractContent": [{"text": "This paper describes a reestimation method for stochastic language models such as the N-gram model and the Hidden Maxkov Model(HMM) from ambiguous observations.", "labels": [], "entities": []}, {"text": "It is applied to model estimation fora tagger from a~ untagged corpus.", "labels": [], "entities": []}, {"text": "We make extensions to a previous algorithm that reestimates the N-gram model from an untagged segmented language (e.g., English) text as training data.", "labels": [], "entities": []}, {"text": "The new method can estimate not only the N-gram model, but also the HMM from untagged, unsegmented language (e.g., Japanese) text.", "labels": [], "entities": []}, {"text": "Credit factors for training data to improve the reliability of the estimated models axe also introduced.", "labels": [], "entities": []}, {"text": "In experiments, the extended algorithm could estimate the HMM as well as the N-gram model from an untagged, unsegmented Japanese corpus and the credit factor was effective in improving model accuracy.", "labels": [], "entities": [{"text": "HMM", "start_pos": 58, "end_pos": 61, "type": "DATASET", "confidence": 0.6630668640136719}, {"text": "accuracy", "start_pos": 191, "end_pos": 199, "type": "METRIC", "confidence": 0.9816983938217163}]}, {"text": "The use of credit factors is a useful approach to estimating a reliable stochastic language model from untagged corpora which axe noisy by nature.", "labels": [], "entities": []}], "introductionContent": [{"text": "Stochastic language models are useful for many language processing applications such as speech recognition, natural language processing and soon.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 88, "end_pos": 106, "type": "TASK", "confidence": 0.8011036217212677}, {"text": "natural language processing", "start_pos": 108, "end_pos": 135, "type": "TASK", "confidence": 0.6605042219161987}]}, {"text": "However, in order to build an accurate stochastic language model, large amounts of tagged text are needed and a tagged corpus may not always match a target application because of, for example, differences between the tag systems.", "labels": [], "entities": []}, {"text": "If the language model can be estimated from untagged corpora and the dictionary of a target application, then the above two problems would be resolved because large amounts of untagged corpora could be easily used and untagged corpora are neutral toward any applications. has proposed an estimation method for the N-gram language model using the Baum-Welch reestimation algorithm from an untagged corpus and have applied this method to an English tagging system.", "labels": [], "entities": [{"text": "English tagging", "start_pos": 439, "end_pos": 454, "type": "TASK", "confidence": 0.5385934710502625}]}, {"text": "also have developed an extended method for unsegmented languages (e.g., Japanese) and applied it to their Japanese tagger.", "labels": [], "entities": []}, {"text": "However, and have criticized methods of estimation from an untagged corpus based on the maximum likelihood principle.", "labels": [], "entities": [{"text": "estimation", "start_pos": 40, "end_pos": 50, "type": "TASK", "confidence": 0.9762561917304993}]}, {"text": "They pointed out limitation of such methods revealed by their experiments and said that the optimization of likelihood didn't necessarily improve tagging accuracy.", "labels": [], "entities": [{"text": "tagging", "start_pos": 146, "end_pos": 153, "type": "TASK", "confidence": 0.9521992802619934}, {"text": "accuracy", "start_pos": 154, "end_pos": 162, "type": "METRIC", "confidence": 0.8420084714889526}]}, {"text": "In other words, the training data extracted from an untagged corpus using only a dictionary are, by nature, too noisy to build a reliable model.", "labels": [], "entities": []}, {"text": "I would like to know whether or not the noise problem occurs in other language models such as the HMM.", "labels": [], "entities": []}, {"text": "have shown, in the experiments of word prediction from the previous word sequence, that the HMM is more powerful than the bigram model and is nearly equivalent to the trigram model, though the number of parameters of the HMM is less than that in the N-gram model.", "labels": [], "entities": [{"text": "word prediction", "start_pos": 34, "end_pos": 49, "type": "TASK", "confidence": 0.8100217580795288}]}, {"text": "In general, models with fewer parameters are more robust.", "labels": [], "entities": []}, {"text": "Here, I investigate a method that can estimate HMM parameters from an untagged corpus and also a general technique for supressing noise in untagged training data.", "labels": [], "entities": [{"text": "supressing noise in untagged training", "start_pos": 119, "end_pos": 156, "type": "TASK", "confidence": 0.7982341766357421}]}, {"text": "The goals of this paper are as follows.", "labels": [], "entities": []}, {"text": "\u2022 Extension of Baum-Welch algorithm: I formulate an algorithm that can be applied to untagged, unsegmented language corpora and estimate not only the N-gram model, but the HMM.", "labels": [], "entities": []}, {"text": "Also, a scaling procedure is defined in the algorithm.", "labels": [], "entities": []}, {"text": "\u2022 Credit factor: In order to overcome the noise of untagged corpora, I introduce credit factors that are assigned to training data.", "labels": [], "entities": [{"text": "Credit factor", "start_pos": 2, "end_pos": 15, "type": "METRIC", "confidence": 0.9446362555027008}]}, {"text": "The estimation algorithm can approximately maximize the modified likelihood that is weighted by the credit factors.", "labels": [], "entities": [{"text": "modified likelihood", "start_pos": 56, "end_pos": 75, "type": "METRIC", "confidence": 0.7665312588214874}]}, {"text": "The problem of stochastic tagging is formulated in the next section(2) and the extended reestimation method in section 3.", "labels": [], "entities": [{"text": "stochastic tagging", "start_pos": 15, "end_pos": 33, "type": "TASK", "confidence": 0.5006194561719894}]}, {"text": "A way of determining the credit factor based on a rulebased tagger is described in section 4.", "labels": [], "entities": [{"text": "credit factor", "start_pos": 25, "end_pos": 38, "type": "METRIC", "confidence": 0.8730353116989136}]}, {"text": "Experiments which evaluate the proposed method are reported in section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "I used 26108 Japanese untagged sentences as training data and 100 hand-tagged sentences as test data, both from the Nikkei newspaper 1994 corpus.", "labels": [], "entities": [{"text": "Nikkei newspaper 1994 corpus", "start_pos": 116, "end_pos": 144, "type": "DATASET", "confidence": 0.9827243983745575}]}, {"text": "The test sentences include about 2500 Japanese morphemes.", "labels": [], "entities": []}, {"text": "The tags were defined as the combination of part-of-speech, conjugation, and class of conjugation.", "labels": [], "entities": []}, {"text": "The number of kinds of tags was 104.", "labels": [], "entities": []}, {"text": "In the precision evaluation, the correct morpheme was defined as that matching the segmentation, tag, and spelling of the base form of the hand-tagged morpheme.", "labels": [], "entities": [{"text": "precision", "start_pos": 7, "end_pos": 16, "type": "METRIC", "confidence": 0.9977477192878723}]}, {"text": "The precision was defined as the proportion of correct morphemes relative to the total number of morphemes in the sequence which the tagger outputted as the best alignment of tags and words.", "labels": [], "entities": [{"text": "precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.999115526676178}]}], "tableCaptions": [{"text": " Table 2: The precision and recall of Juman on each cost-width.", "labels": [], "entities": [{"text": "precision", "start_pos": 14, "end_pos": 23, "type": "METRIC", "confidence": 0.9997239708900452}, {"text": "recall", "start_pos": 28, "end_pos": 34, "type": "METRIC", "confidence": 0.9995991587638855}]}]}