{"title": [{"text": "Summarization: an Application for NL Generation", "labels": [], "entities": [{"text": "Summarization", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.9821589589118958}, {"text": "NL Generation", "start_pos": 34, "end_pos": 47, "type": "TASK", "confidence": 0.9553813636302948}]}], "abstractContent": [], "introductionContent": [{"text": "In this paper, I will be exploring techniques for automatically summarising texts, concentrating on selecting the content of the summary from a parsed (semantic) representation of the original text.", "labels": [], "entities": [{"text": "summarising texts", "start_pos": 64, "end_pos": 81, "type": "TASK", "confidence": 0.8503992855548859}]}, {"text": "Summarization is a particularly nice application for natural language generation because the original text can serve as the knowledge base for generating the summary.", "labels": [], "entities": [{"text": "Summarization", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.9768289923667908}, {"text": "natural language generation", "start_pos": 53, "end_pos": 80, "type": "TASK", "confidence": 0.6985281109809875}]}, {"text": "In addition, we only need to develop a lexicon limited to the words and senses in the original text (as long as we use the same words in the same context as the original text).", "labels": [], "entities": []}, {"text": "This simplifies the generation task somewhat.", "labels": [], "entities": []}, {"text": "However, summarization is not a trivial task.", "labels": [], "entities": [{"text": "summarization", "start_pos": 9, "end_pos": 22, "type": "TASK", "confidence": 0.9923156499862671}]}, {"text": "We must first analyze the original text using a robust grammar that can produce a reliable semantic interpretation of the text.", "labels": [], "entities": []}, {"text": "To simplify this investigation, I will not tackle the many problems of NL analysis, but will use already parsed texts from the TAG Tree Bank.", "labels": [], "entities": [{"text": "NL analysis", "start_pos": 71, "end_pos": 82, "type": "TASK", "confidence": 0.9129396975040436}, {"text": "TAG Tree Bank", "start_pos": 127, "end_pos": 140, "type": "DATASET", "confidence": 0.9638823668162028}]}, {"text": "I use a perl script to convert the syntactic structures in this parsed corpus into a list of logical forms that roughly indicate the predicate-argument structure of each clause in the text.", "labels": [], "entities": []}, {"text": "1 We can generate a summary by choosing a subset of this list of LFs.", "labels": [], "entities": []}, {"text": "However, choosing the right subset is not easy.", "labels": [], "entities": []}, {"text": "The problem is how to judge which clauses are important: Sophisticated discourse analysis is needed in order to interpret the intentional and rhetorical structure of the original text and then prune it in the appropriate ways.", "labels": [], "entities": [{"text": "Sophisticated discourse analysis", "start_pos": 57, "end_pos": 89, "type": "TASK", "confidence": 0.6823381185531616}]}, {"text": "1A parser which directly produces the pred-arg structure is probably preferable to this method.", "labels": [], "entities": []}, {"text": "Note that the parser probably would not have to resolve all syntactic ambiguities in the the summarization task, because we can preserve the same ambiguities in the summary, or delete some of the problem phrases such as PPs in the summary anyway.", "labels": [], "entities": [{"text": "summarization task", "start_pos": 93, "end_pos": 111, "type": "TASK", "confidence": 0.9115830957889557}]}, {"text": "However, discourse analysis is a hard task that requires an immense amount of world knowledge.", "labels": [], "entities": [{"text": "discourse analysis", "start_pos": 9, "end_pos": 27, "type": "TASK", "confidence": 0.8434192538261414}]}, {"text": "I investigate ways to generate a summary without full interpretation of the original text.", "labels": [], "entities": []}, {"text": "I use Centering Theory to roughly segment the text, as described in the next section.", "labels": [], "entities": []}, {"text": "Then, as described in section 3, a set of pruning rules based on centers and discourse relations are used to select the content of the summary.", "labels": [], "entities": []}, {"text": "First, those segments that are about the most frequent centers of attention are selected, and then these segments are pruned by recognizing non-critical elaborations among the propositions.", "labels": [], "entities": []}, {"text": "Another heuristic used is to select restatements among the propositions for the summary, since restatement is a good indicator of important information.", "labels": [], "entities": []}, {"text": "The proposed summarization heuristics are tested out on a sample text in section 4; an implementation to test out these heuristics is in progress.", "labels": [], "entities": [{"text": "summarization heuristics", "start_pos": 13, "end_pos": 37, "type": "TASK", "confidence": 0.907208114862442}]}], "datasetContent": [], "tableCaptions": []}