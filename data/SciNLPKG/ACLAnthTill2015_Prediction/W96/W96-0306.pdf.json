{"title": [{"text": "Acquisition of Semantic Lexicons: Using Word Sense Disambiguation to Improve Precision", "labels": [], "entities": [{"text": "Precision", "start_pos": 77, "end_pos": 86, "type": "TASK", "confidence": 0.6306777596473694}]}], "abstractContent": [{"text": "This paper addresses the problem of large-scale acquisition of computational-semantic lexicons from machine-readable resources.", "labels": [], "entities": []}, {"text": "We describe semantic filters designed to reduce the number of incorrect assignments (i.e., improve precision) made by a purely syntactic technique.", "labels": [], "entities": [{"text": "precision", "start_pos": 99, "end_pos": 108, "type": "METRIC", "confidence": 0.9951874017715454}]}, {"text": "We demonstrate that it is possible to use these filters to build broad-coverage lexicons with minimal effort, at a depth of knowledge that lies at the syntax-semantics interface.", "labels": [], "entities": []}, {"text": "We report on our results of disambiguating the verbs in the semantic filters by adding WordNet 1 sense annotations.", "labels": [], "entities": [{"text": "WordNet 1 sense", "start_pos": 87, "end_pos": 102, "type": "DATASET", "confidence": 0.9149036407470703}]}, {"text": "We then show the results of our classification on unknown words and we evaluate these results.", "labels": [], "entities": []}], "introductionContent": [{"text": "This paper addresses the problem of large-scale acquisition of computational-semantic lexicons from machine-readable resources.", "labels": [], "entities": []}, {"text": "We describe semantic filters designed to reduce the number of incorrect assignments (i.e., improve precision) made by a purely syntactic technique.", "labels": [], "entities": [{"text": "precision", "start_pos": 99, "end_pos": 108, "type": "METRIC", "confidence": 0.9951874017715454}]}, {"text": "We demonstrate that it is possible to use these filters to build broad-coverage lexicons with minimal effort, at a depth of knowledge that lies at the syntax-semantics interface.", "labels": [], "entities": []}, {"text": "We report on our results of disambiguating the verbs in the semantic filters by adding WordNet sense annotations.", "labels": [], "entities": []}, {"text": "We then show the results of our classification on unknown words and we evaluate these results.", "labels": [], "entities": []}, {"text": "As machine-readable resources (i.e., online dictionaries, thesauri, and other knowledge sources) become readily available to NLP researchers, automated acquisition has become increasingly more attractive.", "labels": [], "entities": [{"text": "automated acquisition", "start_pos": 142, "end_pos": 163, "type": "TASK", "confidence": 0.6979571580886841}]}, {"text": "Several researchers have noted that the average time needed to construct a lexical entry by hand can be as much as 30 minutes (see, e.g.,).", "labels": [], "entities": []}, {"text": "Given that most large-scale NLP applications require lexicons of 20-60,000 words, automation of the acquisition process has become a necessity.", "labels": [], "entities": []}, {"text": "Previous research in automatic acquisition focuses primarily on the use of statistical techniques, such as bilingual alignment, or extraction of syntactic constructions from online dictionaries and corpora.", "labels": [], "entities": [{"text": "automatic acquisition", "start_pos": 21, "end_pos": 42, "type": "TASK", "confidence": 0.7911891639232635}, {"text": "bilingual alignment", "start_pos": 107, "end_pos": 126, "type": "TASK", "confidence": 0.6997258812189102}, {"text": "extraction of syntactic constructions from online dictionaries and corpora", "start_pos": 131, "end_pos": 205, "type": "TASK", "confidence": 0.7672337028715346}]}, {"text": "Others who have taken a more knowledge-based (interlingual) approach) do not provide a means for systematically deriving the relation between surface syntactic structures and their underlying semantic representations.", "labels": [], "entities": []}, {"text": "Those who have taken more argument structures into account, e.g.,, do not take full advantage of the systematic relation between syntax and semantics during lexical acquisition.", "labels": [], "entities": []}, {"text": "1We used Version 1.5 of WordNet, available at http://www.cogsci.princeton.edu/~wn.", "labels": [], "entities": [{"text": "1We", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.957819938659668}, {"text": "WordNet", "start_pos": 24, "end_pos": 31, "type": "DATASET", "confidence": 0.8884930610656738}]}, {"text": "We adopt the central thesis of, i.e., that the semantic class of a verb and its syntactic behavior are predictably related.", "labels": [], "entities": []}, {"text": "We base our work on a correlation between semantic classes and patterns of grammar codes in the Longman's Dictionary of Contemporary English (LDOCE).", "labels": [], "entities": [{"text": "Longman's Dictionary of Contemporary English (LDOCE)", "start_pos": 96, "end_pos": 148, "type": "DATASET", "confidence": 0.926676332950592}]}, {"text": "We extend this work by coupling the syntax-semantics relation with a pre-defined association between WordNet word senses and Levin's verbs in order to group the full Set of LDOCE verbs into semantic classes.", "labels": [], "entities": []}, {"text": "While the LDOCE has been used previously in automatic extraction tasks) these tasks are primarily concerned with the extraction of other types of information including syntactic phrase structure and broad argument restrictions or with the derivation of semantic structures from definition analyses.", "labels": [], "entities": [{"text": "automatic extraction tasks", "start_pos": 44, "end_pos": 70, "type": "TASK", "confidence": 0.8213381369908651}]}, {"text": "The work of is more closely related to our approach in that it attempts to recover a syntactic-semantic relation from machine-readable dictionaries.", "labels": [], "entities": []}, {"text": "However, they claim that the semantic classification of verbs based on standard machine-readable dictionaries (e.g., the LDOCE) is \"a hopeless pursuit standard dictionaries are simply not equipped to offer this kind of information with consistency and exhaustiveness.\"", "labels": [], "entities": [{"text": "semantic classification of verbs", "start_pos": 29, "end_pos": 61, "type": "TASK", "confidence": 0.7947754263877869}]}, {"text": "Others have also argued that the task of simplifying lexical entries on the basis of broad semantic class membership is complex and, perhaps, infeasible (see, e.g.,).", "labels": [], "entities": []}, {"text": "However, a number of researchers have demonstrated conclusively that there is a clear relationship between syntactic context and word senses; it is our aim to exploit this relationship for the acquisition of semantic lexicons.", "labels": [], "entities": []}, {"text": "We first describe the LDOCE verb classification resulting from a purely syntactic approach to deriving semantic classes.", "labels": [], "entities": [{"text": "LDOCE verb classification", "start_pos": 22, "end_pos": 47, "type": "TASK", "confidence": 0.5989541510740916}]}, {"text": "We then describe a semantic filter designed to reduce the number of incorrect assignments made by the syntactic technique; we show how this filter can be enhanced with a method that accounts for multiple word senses.", "labels": [], "entities": []}, {"text": "Finally we show the results of our classification of unknown verbs, and we evaluate these results.", "labels": [], "entities": []}, {"text": "Our results clearly indicate that the resolution of polysemy is a key component to developing an effective semantic filter.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Increasing Precision with the Semantic Filter", "labels": [], "entities": []}]}