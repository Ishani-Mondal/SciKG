{"title": [{"text": "Unsupervised Learning of Syntactic Knowledge: methods and measures", "labels": [], "entities": []}], "abstractContent": [{"text": "Supervised methods for ambiguity resolution learn in \"sterile\" environments, in absence of syntactic noise.", "labels": [], "entities": [{"text": "ambiguity resolution", "start_pos": 23, "end_pos": 43, "type": "TASK", "confidence": 0.7275876402854919}]}, {"text": "However, in many language engineering applications manually tagged corpora are not available nor easily implemented.", "labels": [], "entities": []}, {"text": "On the other side, the \"exportability\" of disambiguation cues acquired from a given, noise-free, domain (e.g. the Wall Street Journal) to other domains is not obvious.", "labels": [], "entities": [{"text": "Wall Street Journal)", "start_pos": 114, "end_pos": 134, "type": "DATASET", "confidence": 0.7846967875957489}]}, {"text": "Unsupervised methods of lexical learning have, just as well, many inherent limitations.", "labels": [], "entities": []}, {"text": "First, the type of syntactic ambiguity phenomena occurring in real domains are much more complex than the standard V N PP patterns analyzed in literature.", "labels": [], "entities": []}, {"text": "Second, especially in sublanguages, syntactic noise seems to be a systematic phenomenon, because many ambiguities occur within identical phrases.", "labels": [], "entities": []}, {"text": "In such cases there is little hope to acquire a higher statistical evidence of the correct attachment.", "labels": [], "entities": []}, {"text": "Class-based models may reduce this problem only to a certain degree, depending upon the richness of the sublanguage, and upon the size of the application corpus.", "labels": [], "entities": []}, {"text": "Because of these inherent difficulties, we believe that syntactic learning should be a gradual process, in which the most difficult decisions are made as late as possible, using increasingly refined levels of knowledge.", "labels": [], "entities": []}, {"text": "In this paper we present an incremental, class-based, unsupervised method to reduce syntactic ambiguity.", "labels": [], "entities": []}, {"text": "We show that our method achieves a considerable compression of noise, preserving only those ambiguous patterns for which shallow techniques do not allow reliable decisions.", "labels": [], "entities": []}, {"text": "Unsupervised vs. supervised models of syntactic learning Several corpus-based methods for syntactic ambiguity resolution have been recently presented in the literature.", "labels": [], "entities": [{"text": "syntactic ambiguity resolution", "start_pos": 90, "end_pos": 120, "type": "TASK", "confidence": 0.815950353940328}]}, {"text": "In (Hindle and Rooth, 1993) hereafter H&R, lexi-calized rules are derived according to the probability of noun-preposition or verb-preposition bigrams for ambiguous structures like verb-noun-preposition-noun sequences.", "labels": [], "entities": []}, {"text": "This method has been criticised because it does not consider the PP object in the attachment decision scheme.", "labels": [], "entities": []}, {"text": "However collecting bigrams rather than trigrams reduces the well known problem of data sparse-ness.", "labels": [], "entities": []}, {"text": "In subsequent studies, trigrams rather than bigrams were collected from corpora to derive disambiguation cues.", "labels": [], "entities": []}, {"text": "In (Collins and Brooks,1995) the problems of data sparseness is approached with a supervised back-off model, with interesting results.", "labels": [], "entities": []}, {"text": "In (Resnik and Hearst, 1993) class-based trigrams are obtained by generalizing the PP head, using WordNet synonymy sets.", "labels": [], "entities": [{"text": "WordNet synonymy sets", "start_pos": 98, "end_pos": 119, "type": "DATASET", "confidence": 0.8583965301513672}]}, {"text": "In (Rat-naparkhi et al, 1994) word classes are derived automatically with a clustering procedure.", "labels": [], "entities": []}, {"text": "(Franz, 1995) uses a loglinear model to estimate preferred attachments according to the linguistic features of co-occurring words (e.g. bigrams, the accompanying noun determiner, etc.).", "labels": [], "entities": []}, {"text": "(Brill and Resnik, 1994) use transformation-based error-driven learning (Brill, 1992) to derive dis-ambiguation rules based on simple context information (e.g. right and left adjacent words or POSs).", "labels": [], "entities": []}, {"text": "All these approaches need extensive collections of positive examples (i.e. hand corrected attachment instances) in order to trigger the acquisition process.", "labels": [], "entities": []}, {"text": "Probabilistic, backed-off or loglinear models rely entirely on noise-free data, that is, correct parse trees or bracketed structures.", "labels": [], "entities": []}, {"text": "In general the training set is the parsed Wall Street Journal (Marcus et al, 1993), with few exceptions, and the size of the training samples is around 10-20,000 test cases.", "labels": [], "entities": [{"text": "Wall Street Journal (Marcus et al, 1993)", "start_pos": 42, "end_pos": 82, "type": "DATASET", "confidence": 0.943644005060196}]}, {"text": "Some methods do not require manually validated PP attachments, but word 2S", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Mutual Information of co-occurring esl's", "labels": [], "entities": []}, {"text": " Table 3: Mutual Information of right-generalized esl's in", "labels": [], "entities": []}, {"text": " Table 2: Mutual Information of esl's occurring with fre-", "labels": [], "entities": [{"text": "Mutual", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9491724967956543}]}, {"text": " Table 7: Performance values of the MCP1 without learning", "labels": [], "entities": []}, {"text": " Table 8: Performance values of the LA without learning", "labels": [], "entities": []}]}