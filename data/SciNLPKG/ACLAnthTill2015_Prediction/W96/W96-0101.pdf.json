{"title": [{"text": "Using word class for Part-of-speech disambiguation", "labels": [], "entities": [{"text": "Part-of-speech disambiguation", "start_pos": 21, "end_pos": 50, "type": "TASK", "confidence": 0.7966255843639374}]}], "abstractContent": [{"text": "This paper presents a methodology for improving part-of-speech disambiguation using word classes.", "labels": [], "entities": [{"text": "part-of-speech disambiguation", "start_pos": 48, "end_pos": 77, "type": "TASK", "confidence": 0.723651111125946}]}, {"text": "We build on earlier work for tagging French where we showed that statistical estimates can be computed without lexical probabilities.", "labels": [], "entities": [{"text": "tagging French", "start_pos": 29, "end_pos": 43, "type": "TASK", "confidence": 0.9618733525276184}]}, {"text": "We investigate new directions for coming up with different kinds of probabilities based on paradigms of tags forgiven words.", "labels": [], "entities": []}, {"text": "We base estimates not on the words, but on the set of tags associated with a word.", "labels": [], "entities": []}, {"text": "We compute frequencies of unigrams, bigrams, and trigrams of word classes in order to further refine the disambiguation.", "labels": [], "entities": []}, {"text": "This new approach gives a more efficient representation of the data in order to disambiguate word part-of-speech.", "labels": [], "entities": []}, {"text": "We show empirical results to support our claim.", "labels": [], "entities": []}, {"text": "We demonstrate that, besides providing good estimates for disambiguation, word classes solve some of the problems caused by sparse training data.", "labels": [], "entities": []}, {"text": "We describe a part-of-speech tagger built on these principles and we suggest a methodology for developing an adequate training corpus.", "labels": [], "entities": [{"text": "part-of-speech tagger", "start_pos": 14, "end_pos": 35, "type": "TASK", "confidence": 0.7087604254484177}]}], "introductionContent": [{"text": "In the part-of-speech hterature, whether taggers are based on a rule-based approach,,, or on a statistical one (, (,,,,, there is a debate as to whether more attention should be paid to lexical probabilities rather than contextual ones.", "labels": [], "entities": []}, {"text": "claims that part-of-speech taggers depend almost exclusively on lexical probabilities, whereas other researchers, such as Voutilainen ( argue that word ambiguities vary widely in function of the specific text and genre.", "labels": [], "entities": [{"text": "part-of-speech taggers", "start_pos": 12, "end_pos": 34, "type": "TASK", "confidence": 0.7142863124608994}]}, {"text": "Indeed, part of Church's argument is relevant if a system is based on a large corpus such as the Brown corpus which represents one million surface forms of morpho-syntacticaJly disambiguated words from a range of balanced texts.", "labels": [], "entities": [{"text": "Brown corpus", "start_pos": 97, "end_pos": 109, "type": "DATASET", "confidence": 0.9085950255393982}]}, {"text": "Consider, for example, a word like \"cover\" as discussed by: in the Brown and the LOB Corpus, the word \"cover\" is a noun 40% of the occurrences and a verb 60% of the other, but in the context of a car maintenance manual, it is a noun 100~0 of the time.", "labels": [], "entities": [{"text": "Brown and the LOB Corpus", "start_pos": 67, "end_pos": 91, "type": "DATASET", "confidence": 0.9293104887008667}]}, {"text": "Since, for statistical taggers, 90% of texts can be disambiguated solely applying lexical probabilities, it is, in fact, tempting to think that with more data and more accurate lexical estimates, more text could be better disambiguated.", "labels": [], "entities": [{"text": "statistical taggers", "start_pos": 11, "end_pos": 30, "type": "TASK", "confidence": 0.6971012353897095}]}, {"text": "If this hypothesis is true for English, we show that it does not hold for languages for which publicly available tagged corpora do not exist.", "labels": [], "entities": []}, {"text": "We also argue against Church's position, supporting the claim that more attention needs to be paid to contextual information for part-of-speech disambiguation (.", "labels": [], "entities": [{"text": "part-of-speech disambiguation", "start_pos": 129, "end_pos": 158, "type": "TASK", "confidence": 0.7026655673980713}]}, {"text": "The problem tackled here is to develop an \"efficient\" training corpus.", "labels": [], "entities": []}, {"text": "Unless large effort, money, and time are devoted to this project, only small corpora can be disambiguated manually.", "labels": [], "entities": []}, {"text": "Consequently, the problem of extracting lexical probabilities from a small training corpus is twofold: first, the statistical model may not necessarily represent the use of a particular word in a particular context.", "labels": [], "entities": []}, {"text": "Ina morphologically inflected language, this argument is particularly serious since a word can be tagged with a large number of parts of speech, i.e. the ambiguity potential is high.", "labels": [], "entities": []}, {"text": "Second, word ambiguity may vary widely depending on the particular genre of the text, and this could differ from the training corpus.", "labels": [], "entities": []}, {"text": "When there is no equivalent for the Brown corpus in French, how should one build an adequate training corpus which reflects properly lexical probabilities?", "labels": [], "entities": [{"text": "Brown corpus", "start_pos": 36, "end_pos": 48, "type": "DATASET", "confidence": 0.8034008741378784}]}, {"text": "How can the numerous morphological variants that render this task even harder be handled?", "labels": [], "entities": []}, {"text": "The next section gives examples from French and describes how morphology affects part-ofspeech disambiguation and what types of ambiguities are found in the language.", "labels": [], "entities": []}, {"text": "Section 3 examines different techniques used to obtain lexical probabilities.", "labels": [], "entities": []}, {"text": "Given the problems created by estimating probabilities on a corpus of restricted size, we present in Section 4 a solution for coping with these difficulties.", "labels": [], "entities": []}, {"text": "We suggest anew paradigm called genotype, derived from the concept of ambiguity class, which gives a more efficient representation of the data in order to achieve more accuracy in part-of-speech disambiguation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 168, "end_pos": 176, "type": "METRIC", "confidence": 0.9948548078536987}]}, {"text": "Section 5 shows how our approach differs from the approach taken by Cutting and Kupiec.", "labels": [], "entities": []}, {"text": "The frequencies of unigram, bigram, and trigram genotypes are computed in order to further refine the disambiguation and results are provided to support our claims.", "labels": [], "entities": []}, {"text": "The final section offers a methodology for developing an adequate training corpus.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 5: Influence of context for n-gram genotype disambiguation.", "labels": [], "entities": []}, {"text": " Table 6: Coverage in the training corpus of n-gram genotypes that appear in the test corpus.", "labels": [], "entities": []}, {"text": " Table 7: An example of cost computation for the bigram FST [p r] [jmp nmp].", "labels": [], "entities": [{"text": "FST", "start_pos": 56, "end_pos": 59, "type": "DATASET", "confidence": 0.607572078704834}]}, {"text": " Table 8: An example of biased cost for the unigram sub-FST's [p r] and [jmp nmp].", "labels": [], "entities": []}, {"text": " Table 7 and Table 8. The top part shows how  the genotype bigram [p r] [jmp nmp] can be tagged as a sequence of two unigrams; the bottom  part uses one bigram to tag it. The notation on all arcs in the FST is the following:  input string : output string / cost  e.g.,", "labels": [], "entities": [{"text": "FST", "start_pos": 203, "end_pos": 206, "type": "DATASET", "confidence": 0.7901979684829712}]}]}