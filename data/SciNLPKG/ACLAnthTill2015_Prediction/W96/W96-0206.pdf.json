{"title": [{"text": "Better Language Models with Model Merging", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper investigates model merging, a technique for deriving Markov models from text or speech corpora.", "labels": [], "entities": [{"text": "model merging", "start_pos": 24, "end_pos": 37, "type": "TASK", "confidence": 0.7225402891635895}]}, {"text": "Models are derived by starting with a large and specific model and by successively combining states to build smaller and more general models.", "labels": [], "entities": []}, {"text": "We present methods to reduce the time complexity of the algorithm and report on experiments on deriving language models fora speech recognition task.", "labels": [], "entities": [{"text": "speech recognition task", "start_pos": 125, "end_pos": 148, "type": "TASK", "confidence": 0.8127629558245341}]}, {"text": "The experiments show the advantage of model merging over the standard bigram approach.", "labels": [], "entities": [{"text": "model merging", "start_pos": 38, "end_pos": 51, "type": "TASK", "confidence": 0.7578726708889008}]}, {"text": "The merged model assigns a lower perplexity to the test set and uses considerably fewer states.", "labels": [], "entities": []}], "introductionContent": [{"text": "Hidden Markov Models are commonly used for statistical language models, e.g. in part-of-speech tagging and speech recognition.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 80, "end_pos": 102, "type": "TASK", "confidence": 0.7387753129005432}, {"text": "speech recognition", "start_pos": 107, "end_pos": 125, "type": "TASK", "confidence": 0.8016228973865509}]}, {"text": "The models need a large set of parameters which are induced from a (text-) corpus.", "labels": [], "entities": []}, {"text": "The parameters should be optimal in the sense that the resulting models assign high probabilities to seen training data as well as new data that arises in an application.", "labels": [], "entities": []}, {"text": "There are several methods to estimate model parameters.", "labels": [], "entities": []}, {"text": "The first one is to use each word (type) as a state and estimate the transition probabilities between two or three words by using the relative frequencies of a corpus.", "labels": [], "entities": []}, {"text": "This method is commonly used in speech recognition and known as word-bigram or word-trigram model.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 32, "end_pos": 50, "type": "TASK", "confidence": 0.7538397312164307}]}, {"text": "The relative frequencies have to be smoothed to handle the sparse data problem and to avoid zero probabilities.", "labels": [], "entities": []}, {"text": "The second method is a variation of the first method.", "labels": [], "entities": []}, {"text": "Words are automatically grouped, e.g. by similarity of distribution in the corpus (.", "labels": [], "entities": []}, {"text": "The relative frequencies of pairs or triples of groups (categories, clusters) are used as model parameters, each group is represented by a state in the model.", "labels": [], "entities": []}, {"text": "The second method has the advantage of drastically reducing the number of model parameters and thereby reducing the sparse data problem; there is more data per group than per word, thus estimates are more precise.", "labels": [], "entities": []}, {"text": "The third method uses manually defined categories.", "labels": [], "entities": []}, {"text": "They are linguistically motivated and usually called parts-of-speech.", "labels": [], "entities": []}, {"text": "An important difference to the second method with automatically derived categories is that with the manual definition a word can belong to more than one category.", "labels": [], "entities": []}, {"text": "A corpus is (manually) tagged with the categories and transition probabilities between two or three categories are estimated from their relative frequencies.", "labels": [], "entities": []}, {"text": "This method is commonly used for part-of-speech tagging.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 33, "end_pos": 55, "type": "TASK", "confidence": 0.7771098017692566}]}, {"text": "The fourth method is a variation of the third method and is also used for part-of-speech tagging.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 74, "end_pos": 96, "type": "TASK", "confidence": 0.7801515161991119}]}, {"text": "This method does not need a pre-annotated corpus for parameter estimation.", "labels": [], "entities": []}, {"text": "Instead it uses a lexicon stating the possible parts-of-speech for each word, a raw text corpus, and an initial bias for the transition and output probabilities.", "labels": [], "entities": []}, {"text": "The parameters are estimated by using the Baum-Welch algorithm (.", "labels": [], "entities": []}, {"text": "The accuracy of the derived model depends heavily on the initial bias, but with a good choice results are comparable to those of method three.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9991406202316284}]}, {"text": "This paper investigates a fifth method for estimating natural language models, combining the advantages of the methods mentioned above.", "labels": [], "entities": [{"text": "estimating natural language models", "start_pos": 43, "end_pos": 77, "type": "TASK", "confidence": 0.8729567378759384}]}, {"text": "It is suitable for both speech recognition and partof-speech tagging, has the advantage of automatically deriving word categories from a corpus and is capable of recognizing the fact that a word belongs to more than one category.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 24, "end_pos": 42, "type": "TASK", "confidence": 0.7539965212345123}, {"text": "partof-speech tagging", "start_pos": 47, "end_pos": 68, "type": "TASK", "confidence": 0.8442058861255646}]}, {"text": "Unlike other techniques it not only induces transition and output probabilities, but also the model topology, i.e., the number of states, and for each state the outputs that have a non-zero probability.", "labels": [], "entities": []}, {"text": "The method is called model merging and was introduced by.", "labels": [], "entities": [{"text": "model merging", "start_pos": 21, "end_pos": 34, "type": "TASK", "confidence": 0.727468729019165}]}, {"text": "The rest of the paper is structured as follows.", "labels": [], "entities": []}, {"text": "We first give a short introduction to Markov mo-dels and present the model merging technique.", "labels": [], "entities": [{"text": "model merging", "start_pos": 69, "end_pos": 82, "type": "TASK", "confidence": 0.7313595712184906}]}, {"text": "Then, techniques for reducing the time complexity are presented and we report two experiments using these techniques.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Number of states and Log Perplexity for  the derived models and an additional, previously  test part, consisting of 9,784 words. (a) stan- dard bigram model, (b) constrained model mer- ging (first experiment), (c) model merging starting  with a bigram model(second experiment)", "labels": [], "entities": []}]}