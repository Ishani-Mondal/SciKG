{"title": [{"text": "A Maximum Entropy Model for Part-Of-Speech Tagging", "labels": [], "entities": [{"text": "Part-Of-Speech Tagging", "start_pos": 28, "end_pos": 50, "type": "TASK", "confidence": 0.6863901913166046}]}], "abstractContent": [{"text": "This paper presents a statistical model which trains from a corpus annotated with Part-Of-Speech tags and assigns them to previously unseen text with state-of-the-art accuracy(96.6%).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 167, "end_pos": 175, "type": "METRIC", "confidence": 0.9972482323646545}]}, {"text": "The model can be classified as a Maximum Entropy model and simultaneously uses many contextual \"features\" to predict the POS tag.", "labels": [], "entities": []}, {"text": "Furthermore, this paper demonstrates the use of specialized features to model difficult tagging decisions, discusses the corpus consistency problems discovered during the implementation of these features, and proposes a training strategy that mitigates these problems.", "labels": [], "entities": []}], "introductionContent": [{"text": "Many natural language tasks require the accurate assignment of Part-Of-Speech (POS) tags to previously unseen text.", "labels": [], "entities": []}, {"text": "Due to the availability of large corpora which have been manually annotated with POS information, many taggers use annotated text to \"learn\" either probability distributions or rules and use them to automatically assign POS tags to unseen text.", "labels": [], "entities": []}, {"text": "The experiments in this paper were conducted on the Wall Street Journal corpus from the Penn Treebank project), although the model can trai~n from any large corpus annotated with POS tags.", "labels": [], "entities": [{"text": "Wall Street Journal corpus from the Penn Treebank project", "start_pos": 52, "end_pos": 109, "type": "DATASET", "confidence": 0.9542956815825568}]}, {"text": "Since most realistic natural language applications must process words that were never seen before in training data, all experiments in this paper are conducted on test data that include unknown words.", "labels": [], "entities": []}, {"text": "Several recent papers have reported 96.5% tagging accuracy on the Wall St. Journal corpus.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 50, "end_pos": 58, "type": "METRIC", "confidence": 0.9850497841835022}, {"text": "Wall St. Journal corpus", "start_pos": 66, "end_pos": 89, "type": "DATASET", "confidence": 0.9961779415607452}]}, {"text": "The experiments in this paper test the hypothesis that better use of context will improve the accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 94, "end_pos": 102, "type": "METRIC", "confidence": 0.998653769493103}]}, {"text": "A Maximum Entropy model is well-suited for such experiments since it cornbines diverse forms of contextual information in a principled manner, and does not impose any distributional assumptions on the training data.", "labels": [], "entities": []}, {"text": "Previous uses of this model include language modeling(, machine translation(, prepositional phrase attachment( , and word morphology.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 36, "end_pos": 53, "type": "TASK", "confidence": 0.7535025477409363}, {"text": "machine translation", "start_pos": 56, "end_pos": 75, "type": "TASK", "confidence": 0.7738084495067596}, {"text": "prepositional phrase attachment", "start_pos": 78, "end_pos": 109, "type": "TASK", "confidence": 0.6680642863114675}, {"text": "word morphology", "start_pos": 117, "end_pos": 132, "type": "TASK", "confidence": 0.7599664032459259}]}, {"text": "This paper briefly describes the maximum entropy and maximum likelihood properties of the model, features used for POS tagging, and the experiments on the Penn Treebank Wall St. Journal corpus.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 115, "end_pos": 126, "type": "TASK", "confidence": 0.8067937195301056}, {"text": "Penn Treebank Wall St. Journal corpus", "start_pos": 155, "end_pos": 192, "type": "DATASET", "confidence": 0.9795519312222799}]}, {"text": "It then discusses the consistency problems discovered during an attempt to use specialized features on the word context.", "labels": [], "entities": [{"text": "consistency", "start_pos": 22, "end_pos": 33, "type": "METRIC", "confidence": 0.9730138182640076}]}, {"text": "Lastly, the results in this paper are compared to those from previous work on POS tagging.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 78, "end_pos": 89, "type": "TASK", "confidence": 0.8522612452507019}]}], "datasetContent": [{"text": "In order to conduct tagging experiments, the Wall St. Journal data has been split into three contiguous sections, as shown in.", "labels": [], "entities": [{"text": "tagging", "start_pos": 20, "end_pos": 27, "type": "TASK", "confidence": 0.9702610373497009}, {"text": "Wall St. Journal data", "start_pos": 45, "end_pos": 66, "type": "DATASET", "confidence": 0.9948859661817551}]}, {"text": "The feature set and search algorithm were tested and debugged only on the Training and Development sets, and the official test result on the unseen Test set is presented in the conclusion of the paper.", "labels": [], "entities": [{"text": "Training and Development sets", "start_pos": 74, "end_pos": 103, "type": "DATASET", "confidence": 0.6152333691716194}]}, {"text": "The performances of the \"baseline\" model on the Development Set, both with and without the Tag Dictionary, are shown in.", "labels": [], "entities": []}, {"text": "All experiments use abeam size of N = 5; further increasing the beam size does not significantly increase performance on the Development Set but adversely affects the speed of the tagger.", "labels": [], "entities": []}, {"text": "Even though use of the Tag Dictionary gave an apparently insignificant (.", "labels": [], "entities": [{"text": "Tag Dictionary", "start_pos": 23, "end_pos": 37, "type": "DATASET", "confidence": 0.8956860899925232}]}, {"text": "12%) improvement inaccuracy, it is used in further experiments since it significantly reduces the number of hypotheses and thus speeds up the tagger.", "labels": [], "entities": [{"text": "inaccuracy", "start_pos": 17, "end_pos": 27, "type": "METRIC", "confidence": 0.5240459442138672}]}, {"text": "Top Tagging Mistakes on Training Set for Baseline Model to correctly tag the \"residue\" that the baseline features cannot model.", "labels": [], "entities": []}, {"text": "Since such features typically occur infrequently, the training set consistency must be good enough to yield reliable statistics.", "labels": [], "entities": []}, {"text": "Otherwise the specialized features will model noise and perform poorly on test data.", "labels": [], "entities": []}, {"text": "Such features can be designed for those words which are especially problematic for the model.", "labels": [], "entities": []}, {"text": "The top errors of the model (over the training set) are shown in; clearly, the model has trouble with the words that and about, among others.", "labels": [], "entities": []}, {"text": "As hypothesized in the introduction, better features on the context surrounding that and about should correct the tagging mistakes for these two words, assuming that the tagging errors are due to an impoverished feature set, and not inconsistent data.", "labels": [], "entities": []}, {"text": "Specialized features fora given word are constructed by conjoining certain features in the baseline model with a question about the word itself.", "labels": [], "entities": []}, {"text": "The features which ask about previous tags and surrounding words now additionally ask about the identity of the current word, e.g., a specialized feature for the word about in could be: shows the results of an experiment in which specialized features are constructed for \"difficult\" words, and are added to the baseline feature set.", "labels": [], "entities": []}, {"text": "Here, \"difficult\" words are those that are mistagged a certain way at least 50 times when the training set is tagged with the baseline model.", "labels": [], "entities": []}, {"text": "Using the set of 29 difficult words, the model performs at 96.49% accuracy on the Development Set, an insignificant improvement from the baseline accuracy of 96.43%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 66, "end_pos": 74, "type": "METRIC", "confidence": 0.9991679191589355}, {"text": "Development Set", "start_pos": 82, "end_pos": 97, "type": "DATASET", "confidence": 0.8141405582427979}, {"text": "accuracy", "start_pos": 146, "end_pos": 154, "type": "METRIC", "confidence": 0.9832838773727417}]}, {"text": "shows the change in error rates on the Development Set for the frequently occurring \"difficult\" words.", "labels": [], "entities": [{"text": "error rates", "start_pos": 20, "end_pos": 31, "type": "METRIC", "confidence": 0.9570161700248718}]}, {"text": "For most words, the specialized model yields little or no improvement, and for some, i.e., more and about, the specialized model performs worse.", "labels": [], "entities": []}, {"text": "The lack of improvement implies that either the feature set is still impoverished, or that the training data is inconsistent.", "labels": [], "entities": []}, {"text": "A simple consistency testis to graph the POS tag assignments fora given word as a function of the article in which it occurs.", "labels": [], "entities": []}, {"text": "Consistently tagged words should have roughly the same tag distribution as the article numbers vary.", "labels": [], "entities": []}, {"text": "represents each POS tag with a unique integer and graphs the POS annotation of about in the training set as a function of the 138 articles (the points are \"scattered\" to show density).", "labels": [], "entities": []}, {"text": "As seen in, about is usually annotated with tag#l, which denotes IN (preposition), or tag#9, which denotes RB (adverb), and the observed probability of either choice depends heavily on the current article-~.", "labels": [], "entities": [{"text": "IN", "start_pos": 65, "end_pos": 67, "type": "METRIC", "confidence": 0.9845165610313416}]}, {"text": "Upon further examination 5, the tagging distribution for about changes precisely when the annotator changes., which again uses integers to denote POS tags, shows the tag distribution of about as a function of annotator, and implies that the tagging errors for this word are due mostly to inconsistent data.", "labels": [], "entities": []}, {"text": "The words ago, chief, down, executive, off, out, up and yen also exhibit similar bias.", "labels": [], "entities": []}, {"text": "Thus specialized features maybe less effective for those words affected by inter-annotator bias.", "labels": [], "entities": []}, {"text": "A simple solution to eliminate inter-annotator inconsistency is to train and test the model on data that has been created by the same annotator.", "labels": [], "entities": []}, {"text": "The results of such an experiment 6 are shown in Table 10.", "labels": [], "entities": []}, {"text": "The total accuracy is higher, implying that the singly-annotated training and test sets are more consistent, and the improvement due to the specialized features is higher than before (.1%) but still modest, implying that either the features need further improvement or that intra-annotator inconsistencies exist in the corpus.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9991851449012756}]}], "tableCaptions": [{"text": " Table 5: WSJ Data Sizes", "labels": [], "entities": [{"text": "WSJ Data", "start_pos": 10, "end_pos": 18, "type": "DATASET", "confidence": 0.7998019754886627}, {"text": "Sizes", "start_pos": 19, "end_pos": 24, "type": "TASK", "confidence": 0.4399489164352417}]}, {"text": " Table 6: Baseline Performance on Development Set", "labels": [], "entities": []}, {"text": " Table 8: Performance of Baseline Model with Specialized Features", "labels": [], "entities": []}, {"text": " Table 9: Errors on Development Set with Baseline and Specialized Models", "labels": [], "entities": []}]}