{"title": [{"text": "Two Questions about Data-Oriented Parsing*", "labels": [], "entities": [{"text": "Data-Oriented Parsing", "start_pos": 20, "end_pos": 41, "type": "TASK", "confidence": 0.6692033410072327}]}], "abstractContent": [{"text": "In this paper I present ongoing work on the data-oriented parsing (DOP) model.", "labels": [], "entities": [{"text": "data-oriented parsing (DOP)", "start_pos": 44, "end_pos": 71, "type": "TASK", "confidence": 0.8264086484909058}]}, {"text": "In previous work, DOP was tested on a cleaned-up set of analyzed part-of-speech strings from the Penn Treebank, achieving excellent test results.", "labels": [], "entities": [{"text": "DOP", "start_pos": 18, "end_pos": 21, "type": "METRIC", "confidence": 0.6410491466522217}, {"text": "Penn Treebank", "start_pos": 97, "end_pos": 110, "type": "DATASET", "confidence": 0.9853676557540894}]}, {"text": "This left, however, two important questions unanswered: (1) how does DOP perform if tested on unedited data, and (2) how can DOP be used for parsing word strings that contain unknown words?", "labels": [], "entities": [{"text": "parsing word strings that contain unknown words", "start_pos": 141, "end_pos": 188, "type": "TASK", "confidence": 0.8549394948141915}]}, {"text": "This paper addresses these questions.", "labels": [], "entities": []}, {"text": "We show that parse results on unedited data are worse than on cleaned-up data, although still very competitive if compared to other models.", "labels": [], "entities": [{"text": "parse", "start_pos": 13, "end_pos": 18, "type": "TASK", "confidence": 0.9647902250289917}]}, {"text": "As to the parsing of word strings, we show that the hardness of the problem does not so much depend on unknown words, but on previously unseen lexical categories of known words.", "labels": [], "entities": [{"text": "parsing of word strings", "start_pos": 10, "end_pos": 33, "type": "TASK", "confidence": 0.8743799477815628}]}, {"text": "We give a novel method for parsing these words by estimating the probabilities of unknown subtrees.", "labels": [], "entities": []}, {"text": "The method is of general interest since it shows that good performance can be obtained without the use of a part-of-speech tagger.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, our method outperforms other statistical parsers tested on Penn Treebank word strings.", "labels": [], "entities": [{"text": "Penn Treebank word strings", "start_pos": 89, "end_pos": 115, "type": "DATASET", "confidence": 0.9798102527856827}]}, {"text": "1 Introduction The Data-Oriented Parsing (DOP) method suggested in Scha (1990) and developed in Bod (1992-1995) is a probabilistic parsing strategy which does not single out a narrowly predefined set of structures as the statistically significant ones.", "labels": [], "entities": [{"text": "Bod", "start_pos": 96, "end_pos": 99, "type": "DATASET", "confidence": 0.9695083498954773}]}, {"text": "It accomplishes this by maintaining a large corpus of analyses of previously occurring utterances.", "labels": [], "entities": []}, {"text": "New input is parsed by combining tree-fragments from the corpus; the frequencies of these fragments are used to estimate which analysis is the most probable one.", "labels": [], "entities": []}, {"text": "In previous work, we tested the DOP method on a cleaned-up set of analyzed part-of-speech strings from the Penn Treebank (Marcus et al., 1993), achieving excellent test results (Bod, 1993a, b).", "labels": [], "entities": [{"text": "Penn Treebank (Marcus et al., 1993)", "start_pos": 107, "end_pos": 142, "type": "DATASET", "confidence": 0.9055408835411072}, {"text": "Bod, 1993a", "start_pos": 178, "end_pos": 188, "type": "DATASET", "confidence": 0.9230477809906006}]}, {"text": "This left, however, two important questions unanswered: (1) how does DOP perform if tested on unediteddata, and (2), how can DOP be used for parsing word strings that contain unknown words?", "labels": [], "entities": [{"text": "parsing word strings that contain unknown words", "start_pos": 141, "end_pos": 188, "type": "TASK", "confidence": 0.8451009222439357}]}, {"text": "This paper addresses these questions.", "labels": [], "entities": []}, {"text": "The rest of it is divided into three parts.", "labels": [], "entities": []}, {"text": "In section 2 we give a short resume of the DOP method.", "labels": [], "entities": [{"text": "DOP", "start_pos": 43, "end_pos": 46, "type": "TASK", "confidence": 0.5777077078819275}]}, {"text": "In section 3 we address the first question: how does DOP perform on unedited data?", "labels": [], "entities": []}, {"text": "In section 4 we deal with the question how DOP can be used for parsing word strings that contain unknown words.", "labels": [], "entities": [{"text": "parsing word strings that contain unknown words", "start_pos": 63, "end_pos": 110, "type": "TASK", "confidence": 0.8410035627228873}]}, {"text": "This second question turns out to be the actual focus of the article, while the answer to the first question serves as a baseline.", "labels": [], "entities": []}], "introductionContent": [{"text": "The Data-Oriented Parsing (DOP) method suggested in and developed in) is a probabilistic parsing strategy which does not single out a narrowly predefined set of structures as the statistically significant ones.", "labels": [], "entities": [{"text": "Data-Oriented Parsing (DOP)", "start_pos": 4, "end_pos": 31, "type": "TASK", "confidence": 0.7416679501533509}]}, {"text": "It accomplishes this by maintaining a large corpus of analyses of previously occurring utterances.", "labels": [], "entities": []}, {"text": "New input is parsed by combining tree-fragments from the corpus; the frequencies of these fragments are used to estimate which analysis is the most probable one.", "labels": [], "entities": []}, {"text": "In previous work, we tested the DOP method on a cleaned-up set of analyzed part-of-speech strings from the Penn Treebank (, achieving excellent test results.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 107, "end_pos": 120, "type": "DATASET", "confidence": 0.9881846010684967}]}, {"text": "This left, however, two important questions unanswered: (1) how does DOP perform if tested on unediteddata, and (2), how can DOP be used for parsing word strings that contain unknown words?", "labels": [], "entities": [{"text": "parsing word strings that contain unknown words", "start_pos": 141, "end_pos": 188, "type": "TASK", "confidence": 0.8451009222439357}]}, {"text": "This paper addresses these questions.", "labels": [], "entities": []}, {"text": "The rest of it is divided into three parts.", "labels": [], "entities": []}, {"text": "In section 2 we give a short resume of the DOP method.", "labels": [], "entities": [{"text": "DOP", "start_pos": 43, "end_pos": 46, "type": "TASK", "confidence": 0.5777077078819275}]}, {"text": "In section 3 we address the first question: how does DOP perform on unedited data?", "labels": [], "entities": []}, {"text": "In section 4 we deal with the question how DOP can be used for parsing word strings that contain unknown words.", "labels": [], "entities": [{"text": "parsing word strings that contain unknown words", "start_pos": 63, "end_pos": 110, "type": "TASK", "confidence": 0.8410035627228873}]}, {"text": "This second question turns out to be the actual focus of the article, while the answer to the first question serves as a baseline.", "labels": [], "entities": []}], "datasetContent": [{"text": "It is one of the most essential features of the DOP approach, that arbitrarily large subtrees are taken into consideration to estimate the probability of a parse.", "labels": [], "entities": []}, {"text": "In order to test the usefulness of this feature, we performed different experiments constraining the depth of the subtrees.", "labels": [], "entities": []}, {"text": "The depth of a tree is defined as the length of its longest path.", "labels": [], "entities": []}, {"text": "The following table shows the results of seven experiments for different maximum depths of the subtrees.", "labels": [], "entities": []}, {"text": "These were obtained by dividing the ATIS trees at random into 500 training set trees and 100 test set trees.", "labels": [], "entities": [{"text": "ATIS trees", "start_pos": 36, "end_pos": 46, "type": "DATASET", "confidence": 0.7574781775474548}]}, {"text": "The training set trees were converted into subtrees together with their substitution probabilities.", "labels": [], "entities": []}, {"text": "The part-of-speech sequences from the test set served as input sentences that were parsed and disambiguated using the subtrees from the training set.", "labels": [], "entities": []}, {"text": "The parse accuracy is defined as the percentage of test sentences for which the most probable parse exactly matches with the test set parse.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.8705003261566162}]}, {"text": "The table shows a considerable increase in parse accuracy when enlarging the maximum depth of the subtrees from 1 to 2.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.9800641536712646}]}, {"text": "The accuracy keeps increasing, at a slower rate, when the depth is enlarged further.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9996926784515381}]}, {"text": "The highest accuracy of 64% is achieved by using all subtrees from the training set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9995817542076111}]}, {"text": "If once-occurring subtrees were ignored, the maximum parse accuracy decreased to 60%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.9232245087623596}]}, {"text": "This shows the importance of taking all subtrees from the training set.", "labels": [], "entities": []}, {"text": "However, the accuracy of 64% is disappointingly bad when compared to exact match results reported on clean ATIS data: 96% in Bod (1995a), 90% to 95% in.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.9997463822364807}, {"text": "ATIS data", "start_pos": 107, "end_pos": 116, "type": "DATASET", "confidence": 0.9313095808029175}, {"text": "Bod (1995a)", "start_pos": 125, "end_pos": 136, "type": "DATASET", "confidence": 0.9738256335258484}]}, {"text": "An explanation maybe that the exact match metric is very sensitive to annotation errors.", "labels": [], "entities": []}, {"text": "Since the raw Penn Treebank data contains many inconsistencies in its annotations (cf., a single inconsistency in a test set tree will very likely yield a zero percent parse accuracy for the particular test set sentence.", "labels": [], "entities": [{"text": "Penn Treebank data", "start_pos": 14, "end_pos": 32, "type": "DATASET", "confidence": 0.9924739797910055}, {"text": "accuracy", "start_pos": 174, "end_pos": 182, "type": "METRIC", "confidence": 0.6290982365608215}]}, {"text": "Thus, we should raise the question as to whether the exact match is an interesting metric for parsing with inconsistent dam.", "labels": [], "entities": [{"text": "parsing", "start_pos": 94, "end_pos": 101, "type": "TASK", "confidence": 0.9775601625442505}]}, {"text": "3 Accuracy metrics that are less sensitive to annotations errors are the so-called bracketing accuracy (the percentage of the brackets of the most probable parses that do not cross the brackets in the test set parses), and the sentence accuracy (the percentage of the most probable parses in which no brackets cross the brackets in the test set parses).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 94, "end_pos": 102, "type": "METRIC", "confidence": 0.6865935325622559}, {"text": "accuracy", "start_pos": 236, "end_pos": 244, "type": "METRIC", "confidence": 0.513653576374054}]}, {"text": "We also calculated the accuracies according to these metrics for DOP1.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 23, "end_pos": 33, "type": "METRIC", "confidence": 0.9967508316040039}, {"text": "DOP1", "start_pos": 65, "end_pos": 69, "type": "DATASET", "confidence": 0.5901937484741211}]}, {"text": "To increase the reliability of our results, we performed experiments with 8 different random divisions of ATIS into training sets of 500 and test sets of 100 trees.", "labels": [], "entities": [{"text": "reliability", "start_pos": 16, "end_pos": 27, "type": "METRIC", "confidence": 0.9884204864501953}, {"text": "ATIS", "start_pos": 106, "end_pos": 110, "type": "DATASET", "confidence": 0.7972461581230164}]}, {"text": "The following table shows the means of the results for the three accuracy metrics with their standard deviations., it is shown that on semantically enriched ATIS trees, 88% of the test sentences obtain the correct semantic interpretation, while only 62% obtain a fully correct syntactic structure.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.9963536262512207}]}, {"text": "In our experiments with DOP2, we used the same initial division of the ATIS corpus as in section 3.1 into a training set of 500 trees and a test set of I00 trees, but now the trees were not stripped of their words.", "labels": [], "entities": [{"text": "ATIS corpus", "start_pos": 71, "end_pos": 82, "type": "DATASET", "confidence": 0.9311243891716003}]}, {"text": "For time-cost reasons, no experiments were performed with subtrees larger than depth 3.", "labels": [], "entities": []}, {"text": "The following table gives the results of these experiments (for subtree-depth _< 3).", "labels": [], "entities": []}, {"text": "We represented respectively the parse accuracy of the test sentences that contained at least one unknown word, the parse accuracy of the test sentences without unknown words, and finally, the parse accuracy of all test sentences together (we will comeback to the other accuracy metrics later).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.5662645101547241}, {"text": "parse accuracy", "start_pos": 115, "end_pos": 129, "type": "METRIC", "confidence": 0.600539356470108}, {"text": "accuracy", "start_pos": 198, "end_pos": 206, "type": "METRIC", "confidence": 0.5217519402503967}, {"text": "accuracy", "start_pos": 269, "end_pos": 277, "type": "METRIC", "confidence": 0.9847791790962219}]}, {"text": "The table shows that the results of the partial parse method are disappointingly bad.", "labels": [], "entities": []}, {"text": "For the sentences with unknown words, only 20% are parsed correctly.", "labels": [], "entities": []}, {"text": "However, if plain DOP1 were used, the accuracy would have been 0% for these sentences.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.9996217489242554}]}, {"text": "If we look at the sentences with only known words (where DOP2 is equivalent to DOP1), we see that the parse accuracy of 33% is higher than for sentences with unknown words.", "labels": [], "entities": [{"text": "DOP2", "start_pos": 57, "end_pos": 61, "type": "METRIC", "confidence": 0.8679354190826416}, {"text": "accuracy", "start_pos": 108, "end_pos": 116, "type": "METRIC", "confidence": 0.8066821694374084}]}, {"text": "However, it remains far behind the 56% parse accuracy of DOP1 for part-of-speech strings (for the same subtree depth; see table 3.1).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.7447522878646851}]}, {"text": "Word parsing is obviously a much more difficult task than part-of-speech parsing, even if all words are known.", "labels": [], "entities": [{"text": "Word parsing", "start_pos": 0, "end_pos": 12, "type": "TASK", "confidence": 0.7549916803836823}, {"text": "part-of-speech parsing", "start_pos": 58, "end_pos": 80, "type": "TASK", "confidence": 0.7254257798194885}]}, {"text": "Looking more carefully to the parse results of test sentences with only known words, we discover a striking result: for many of these sentences no parse could be generated at all, not because a word was unknown, but because an ambiguous word required a lexical category which it didn't have in the training set.", "labels": [], "entities": []}, {"text": "We will call these words unknown-category words.", "labels": [], "entities": []}, {"text": "One might argue that the problem of unknown-category words is due to the tiny size of the ATIS corpus.", "labels": [], "entities": [{"text": "ATIS corpus", "start_pos": 90, "end_pos": 101, "type": "DATASET", "confidence": 0.96080082654953}]}, {"text": "However, no corpus of any size will ever contain all possible uses of all possible words.", "labels": [], "entities": []}, {"text": "Even the extension with a dictionary does not solve the problem.", "labels": [], "entities": []}, {"text": "There will be domain-specific words and word senses, abbreviations, proper nouns etc. that are not found in a dictionary.", "labels": [], "entities": []}, {"text": "It remains therefore important to study how to deal with unknown words and unknown-category words in a statistically adequate way.", "labels": [], "entities": []}, {"text": "Treating all words as potential unknown-category words would certainly lead to an impractically large number of subtrees in the parse forest.", "labels": [], "entities": []}, {"text": "As we have seen, the set of possible NP-subtrees (of maximal depth three) consists of 10 9 types, which is a factor 12000 larger than the set of seen NP-subtrees.", "labels": [], "entities": []}, {"text": "It is therefore evident that we will get impractical processing times with DOP3.", "labels": [], "entities": []}, {"text": "If we still want to perform experiments with DOP3, we need to limit the mismatches as much as possible.", "labels": [], "entities": []}, {"text": "It seems reasonable to allow the mismatches only for unknown words, and fora restricted set of potential unknown-category words.", "labels": [], "entities": []}, {"text": "From the ATIS training set we derive that only nouns and verbs are actually lexically ambiguous.", "labels": [], "entities": [{"text": "ATIS training set", "start_pos": 9, "end_pos": 26, "type": "DATASET", "confidence": 0.8650159041086832}]}, {"text": "In our experiments, we will therefore limit the potential unknowncategory words of an input sentence to the nouns and verbs.", "labels": [], "entities": []}, {"text": "This means that only the words which are unknown in the training set and the words of the test sentence which are tagged as a noun or a verb in the training set are allowed to mismatch with subtree-terminals.", "labels": [], "entities": []}, {"text": "We used the initial random division of the ATIS corpus into a training set of 500 trees and a test set of 100 trees.", "labels": [], "entities": [{"text": "ATIS corpus", "start_pos": 43, "end_pos": 54, "type": "DATASET", "confidence": 0.9163039326667786}]}, {"text": "In order to carefully study the experimental merits of DOP3, we distinguished two classes of test sentences: 1.", "labels": [], "entities": []}, {"text": "test sentences containing both unknown and unknown-category words 2.", "labels": [], "entities": []}, {"text": "test sentences containing only unknown-category words Note that all 100 test sentences contained at least one potential unknown-category word (verb or noun  The table shows that DOP3 has better performance than DOP2 in all respects (cf. table 4.1).", "labels": [], "entities": []}, {"text": "The parse accuracy for the sentences with unknown and unknown-category words is with 34% much higher than the 20% of DOP2.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.915804922580719}]}, {"text": "As to the sentences with only unknown-category words, the improvement of DOP3 with respect to DOP2 is most noticeable: the accuracy increased from 33% to 57%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 123, "end_pos": 131, "type": "METRIC", "confidence": 0.9995212554931641}]}, {"text": "However, the comparison with DOP2 may not be fair, as DOP2 cannot deal with unknown-category words at all.", "labels": [], "entities": []}, {"text": "What the parse results of DOP3 do indicate is, that, for sentences without unknown words, the parse accuracy for word strings is of the same order as the parse accuracy for p-o-s strings (which was 56% at maximum depth 3; see section 4.2).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 100, "end_pos": 108, "type": "METRIC", "confidence": 0.7626714706420898}, {"text": "accuracy", "start_pos": 160, "end_pos": 168, "type": "METRIC", "confidence": 0.6747300028800964}]}, {"text": "Nevertheless, the total parse accuracy of 41% is still bad.", "labels": [], "entities": [{"text": "parse", "start_pos": 24, "end_pos": 29, "type": "METRIC", "confidence": 0.7669316530227661}, {"text": "accuracy", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.8179398775100708}]}], "tableCaptions": [{"text": " Table 3.1. Parse accuracies for DOP1 for different maximum subtree depths.", "labels": [], "entities": [{"text": "Parse accuracies", "start_pos": 12, "end_pos": 28, "type": "METRIC", "confidence": 0.8180220425128937}]}, {"text": " Table 3.2. Mean accuracies for DOP1 for 8 different training-test sets from ATIS", "labels": [], "entities": [{"text": "Mean accuracies", "start_pos": 12, "end_pos": 27, "type": "METRIC", "confidence": 0.7696561217308044}, {"text": "DOP1", "start_pos": 32, "end_pos": 36, "type": "METRIC", "confidence": 0.9773468375205994}, {"text": "ATIS", "start_pos": 77, "end_pos": 81, "type": "DATASET", "confidence": 0.8437063097953796}]}, {"text": " Table 4.2. Adjusted frequencies for NP-subtrees", "labels": [], "entities": []}, {"text": " Table 4.4. Parse accuracy for word strings from the ATIS corpus by DOP4 against DOP3.", "labels": [], "entities": [{"text": "Parse", "start_pos": 12, "end_pos": 17, "type": "METRIC", "confidence": 0.9589243531227112}, {"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.8508026599884033}, {"text": "ATIS corpus", "start_pos": 53, "end_pos": 64, "type": "DATASET", "confidence": 0.9333448708057404}]}]}