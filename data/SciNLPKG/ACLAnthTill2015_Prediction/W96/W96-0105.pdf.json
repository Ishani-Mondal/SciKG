{"title": [{"text": "Selective Sampling of Effective Example Sentence Sets for Word Sense Disambiguation", "labels": [], "entities": [{"text": "Word Sense Disambiguation", "start_pos": 58, "end_pos": 83, "type": "TASK", "confidence": 0.6900913516680399}]}], "abstractContent": [{"text": "This paper proposes an efficient example selection method for example-based word sense disambiguation systems.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 76, "end_pos": 101, "type": "TASK", "confidence": 0.6695021092891693}]}, {"text": "To construct a practical size database, a considerable overhead for manual sense disambiguation is required.", "labels": [], "entities": [{"text": "manual sense disambiguation", "start_pos": 68, "end_pos": 95, "type": "TASK", "confidence": 0.5937143663565317}]}, {"text": "Our method is characterized by the reliance on the notion of the training utility: the degree to which each example is informative for future example selection when used for the training of the system.", "labels": [], "entities": []}, {"text": "The system progressively collects examples by selecting those with greatest utility.", "labels": [], "entities": []}, {"text": "The paper reports the effectivity of our method through experiments on about one thousand sentences.", "labels": [], "entities": []}, {"text": "Compared to experiments with random example selection, our method reduced the overhead without the degeneration of the performance of the system.", "labels": [], "entities": []}], "introductionContent": [{"text": "Word sense disambiguation is a crucial task in many NLP applications, such as machine translation, parsing and text retrieval.", "labels": [], "entities": [{"text": "Word sense disambiguation", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.7005099455515543}, {"text": "machine translation", "start_pos": 78, "end_pos": 97, "type": "TASK", "confidence": 0.8102020919322968}, {"text": "parsing", "start_pos": 99, "end_pos": 106, "type": "TASK", "confidence": 0.9369625449180603}, {"text": "text retrieval", "start_pos": 111, "end_pos": 125, "type": "TASK", "confidence": 0.750597208738327}]}, {"text": "Given the growing utilization of machine readable texts, word sense disambiguation techniques have been variously used in corpus-based approaches.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 57, "end_pos": 82, "type": "TASK", "confidence": 0.7511589328447977}]}, {"text": "Unlike rule-based approaches, corpus-based approaches release us from the task of generalizing observed phenomena in order to disambiguate word senses.", "labels": [], "entities": []}, {"text": "Our system is based on such an approach, or more precisely it is based on an examplebased approach.", "labels": [], "entities": []}, {"text": "Since this approach requires a certain number of examples of disambiguated verbs, we have to carryout this task manually, that is, we disambiguate verbs appearing in a corpus prior to their use by the system.", "labels": [], "entities": []}, {"text": "A preliminary experiment on ten Japanese verbs showed that the system needed on average about one hundred examples for each verb in order to achieve 82% of accuracy in disambiguating verb senses.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 156, "end_pos": 164, "type": "METRIC", "confidence": 0.9992572665214539}]}, {"text": "In order to build an operational system, the following problems have to betaken into account: 1.", "labels": [], "entities": []}, {"text": "Since there are about one thousand basic verbs in Japanese, a considerable overhead is associated with manual word sense disambiguation.", "labels": [], "entities": [{"text": "manual word sense disambiguation", "start_pos": 103, "end_pos": 135, "type": "TASK", "confidence": 0.7058620750904083}]}, {"text": "2. Given human resource limitations, it is not reasonable to manually analyze large corpora as they can provide virtually infinite input.", "labels": [], "entities": []}, {"text": "3. Given the fact that example-based natural language systems, including our system, search the example-database (database, hereafter) for the most similar examples with regard to the input, the computational cost becomes prohibitive if one works with a very large database size.", "labels": [], "entities": []}, {"text": "All these problems suggest a different approach, namely to select a small number of optimally informative examples from a given corpora.", "labels": [], "entities": []}, {"text": "Hereafter we will call these examples \"samples.\"", "labels": [], "entities": []}, {"text": "Our method, based on the utility maximization principle, decides on which examples should be included in the database.", "labels": [], "entities": []}, {"text": "This decision procedure is usually called selective sampling.", "labels": [], "entities": []}, {"text": "Selective sampling directly addresses the first two problems mentioned above.", "labels": [], "entities": [{"text": "Selective sampling", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8393179774284363}]}, {"text": "The overall control flow of systems based on selective sampling can be depicted as in, where \"system\" refers to dedicated NLP applications.", "labels": [], "entities": []}, {"text": "The sampling process basically cycles between the execution and the training phases.", "labels": [], "entities": []}, {"text": "During the execution phase, the system generates an interpretation for each example, in terms of parts-of-speech, text categories or word senses.", "labels": [], "entities": []}, {"text": "During the training phase, the system selects samples for training from the previously produced outputs.", "labels": [], "entities": []}, {"text": "During this phase, a human expert provides the correct interpretation of the samples so that the system can then be trained for the execution of the remaining data.", "labels": [], "entities": []}, {"text": "Several researchers have proposed such an approach..", "labels": [], "entities": []}, {"text": "In this method, the system always selects samples which are not certain with respect to the correctness of the answer.", "labels": [], "entities": []}, {"text": "Dagan et al. proposed a committee-based sampling method, which is currently applied to HMM training for part-of-speech tagging.", "labels": [], "entities": [{"text": "HMM training", "start_pos": 87, "end_pos": 99, "type": "TASK", "confidence": 0.8781087696552277}, {"text": "part-of-speech tagging", "start_pos": 104, "end_pos": 126, "type": "TASK", "confidence": 0.7654288113117218}]}, {"text": "This method selects samples based on the training utility factor of the examples, i.e. the informativity of the data with respect to future training.", "labels": [], "entities": []}, {"text": "However, as all these methods are implemented for statistics-based models, there is a need to explore how to formalize and map these concepts into the examplebased approach.", "labels": [], "entities": []}, {"text": "With respect to problem 3, a possible solution would be the generalization of redundant examples.", "labels": [], "entities": []}, {"text": "However, such an approach implies a significant overhead for the manual training of each example prior to the generalization.", "labels": [], "entities": []}, {"text": "This shortcoming is precisely what our approach allows to avoid: reducing both the overhead as well as the size of the database.", "labels": [], "entities": []}, {"text": "Section 2 briefly describes our method fora verb sense disambiguation system.", "labels": [], "entities": [{"text": "verb sense disambiguation", "start_pos": 44, "end_pos": 69, "type": "TASK", "confidence": 0.7079041103521982}]}, {"text": "The next Section 3 elaborates on the example sampling method, while section 4 reports on the results of our experiment.", "labels": [], "entities": []}, {"text": "Before concluding in section 6, discussion is added in section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "We compared the performance of our example sampling method with random sampling, in which a certain proportion of a given corpus is randomly selected for training.", "labels": [], "entities": []}, {"text": "We compared the two sampling methods by evaluating the relation between various numbers of examples in training, and the performance of the system on another corpus.", "labels": [], "entities": []}, {"text": "We conducted a six foldcross validation as described in section 3.2, but in this experiment, each method selected some proportion of the training set as samples.", "labels": [], "entities": []}, {"text": "We used the same corpus as described in table 2 as training/test data.", "labels": [], "entities": []}, {"text": "Both sampling methods used examples from IPAL to initialize the system (as seeds) with the number of example case fillers for each case being on average of about 3.7.", "labels": [], "entities": []}, {"text": "The training/test data used in the experiment contained about one thousand simple Japanese sentences collected from news articles.", "labels": [], "entities": []}, {"text": "Each of the sentences in the training/test data used in our experiment contained one or several complement(s) followed by one of the ten verbs enumerated in, the column of \"English gloss\" describes typical English translations of the Japanese verbs.", "labels": [], "entities": []}, {"text": "The column of \"# of sentences\" denotes the number of sentences in the corpus, \"# of senses\" denotes the number of verb senses based on IPAL, and \"lower bound\" denotes the precision gained by using a naive method, where the system systematically chooses the most frequently appearing interpretation in the training data.", "labels": [], "entities": [{"text": "precision", "start_pos": 171, "end_pos": 180, "type": "METRIC", "confidence": 0.9988995790481567}]}, {"text": "We at first estimated the system's performance by its precision, that is the ratio of the number of correct outputs, compared to the number of inputs.", "labels": [], "entities": [{"text": "precision", "start_pos": 54, "end_pos": 63, "type": "METRIC", "confidence": 0.999523401260376}]}, {"text": "In this experiment, we set = 0.5 in equation, and k = 1 in equation (10).", "labels": [], "entities": []}, {"text": "The influence of CCD, i.e. o~ in equation (4), was extremely large so that the system virtually relied solely on the SIM of the case with the greatest CCD.", "labels": [], "entities": []}, {"text": "shows the relation between the size of the training data and the precision of the system.", "labels": [], "entities": [{"text": "precision", "start_pos": 65, "end_pos": 74, "type": "METRIC", "confidence": 0.999325156211853}]}, {"text": "In, when the x-axis is zero, the system has used only the seeds given by IPAL.", "labels": [], "entities": [{"text": "IPAL", "start_pos": 73, "end_pos": 77, "type": "DATASET", "confidence": 0.9494647979736328}]}, {"text": "It should be noted that with the final step, where all examples in the training set have been provided to the database, the precision of both methods is equal.", "labels": [], "entities": [{"text": "precision", "start_pos": 124, "end_pos": 133, "type": "METRIC", "confidence": 0.9995090961456299}]}, {"text": "Looking at figure 9 one can see that the precision of random sampling was surpassed by our training utility sampling method.", "labels": [], "entities": [{"text": "precision", "start_pos": 41, "end_pos": 50, "type": "METRIC", "confidence": 0.9995843768119812}]}, {"text": "It solves the first two problems mentioned in section 1.", "labels": [], "entities": []}, {"text": "One can also see that the size of the database can be reduced without degrading the system's precision, and as such it can solve the third problem mentioned in section 1.", "labels": [], "entities": [{"text": "precision", "start_pos": 93, "end_pos": 102, "type": "METRIC", "confidence": 0.9992251396179199}]}, {"text": "We further evaluated the system's performance in the following way.", "labels": [], "entities": []}, {"text": "Integrated with other NLP systems, the task of our verb sense disambiguation system is not only to output the most plausible verb sense, but also the interpretation certainty of its output, so that other systems can vary the degree of reliance on our system's output.", "labels": [], "entities": []}, {"text": "The following are properties which are required for our system: \u2022 the system should output as many correct answers as possible, \u2022 the system should output correct answers with great interpretation certainty, \u2022 the system should output incorrect answers with diminished interpretation certainty.", "labels": [], "entities": [{"text": "interpretation certainty", "start_pos": 182, "end_pos": 206, "type": "METRIC", "confidence": 0.8439358472824097}]}, {"text": "Motivated by these properties, we formulated anew performance estimation measure, PM, as shown in equation.", "labels": [], "entities": [{"text": "PM", "start_pos": 82, "end_pos": 84, "type": "METRIC", "confidence": 0.9895368814468384}]}, {"text": "A greater accuracy of performance of the system will lead to a greater PM value.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9992247819900513}, {"text": "PM", "start_pos": 71, "end_pos": 73, "type": "METRIC", "confidence": 0.9988663196563721}]}, {"text": "In equation, Cmax is the maximum value of the interpretation certainty, which can be derived by substituting the maximum and the mimimum interpretation score for Si(x) and S2(x), respectively, in equation.", "labels": [], "entities": [{"text": "certainty", "start_pos": 61, "end_pos": 70, "type": "METRIC", "confidence": 0.4847962558269501}, {"text": "mimimum interpretation score", "start_pos": 129, "end_pos": 157, "type": "METRIC", "confidence": 0.792351245880127}]}, {"text": "Following table 1, we assign 11 and 0 to be the maximum and the minimum of the interpretation score, and therefore Cma~ = 11, disregarding the value of ~ in equation.", "labels": [], "entities": [{"text": "interpretation", "start_pos": 79, "end_pos": 93, "type": "TASK", "confidence": 0.8909767866134644}]}, {"text": "N is the total number of the inputs and 5 is a coefficient defined as in equation.", "labels": [], "entities": []}, {"text": "1 if the interpratation of x is correct = (12) -p otherwise In equation, p is the parametric constant to control the degree of the penalty fora system error.", "labels": [], "entities": []}, {"text": "For our experiment, we set p = 1, meaning that PM was in the range -1 to 1.", "labels": [], "entities": [{"text": "PM", "start_pos": 47, "end_pos": 49, "type": "METRIC", "confidence": 0.96930992603302}]}, {"text": "shows the relation between the size of the training data and the value of PM.", "labels": [], "entities": [{"text": "PM", "start_pos": 74, "end_pos": 76, "type": "METRIC", "confidence": 0.9067514538764954}]}, {"text": "In this experiment, it can be seen that the performance of random sampling was again surpassed by our training utility sampling method, and the size of the database can be reduced without degrading the system's performance.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: The corpus used for the experiments", "labels": [], "entities": []}]}