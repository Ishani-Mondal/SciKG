{"title": [{"text": "Comparative Experiments on Disambiguating Word Senses: An Illustration of the Role of Bias in Machine Learning", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper describes an experimental comparison of seven different learning algorithms on the problem of learning to disambiguate the meaning of a word from context.", "labels": [], "entities": [{"text": "disambiguate the meaning of a word from context", "start_pos": 117, "end_pos": 164, "type": "TASK", "confidence": 0.7312211878597736}]}, {"text": "The algorithms tested include statistical, neural-network, decision-tree, rule-based, and case-based classification techniques.", "labels": [], "entities": [{"text": "case-based classification", "start_pos": 90, "end_pos": 115, "type": "TASK", "confidence": 0.6400077193975449}]}, {"text": "The specific problem tested involves dis-ambiguating six senses of the word \"line\" using the words in the current and proceeding sentence as context.", "labels": [], "entities": []}, {"text": "The statistical and neural-network methods perform the best on this particular problem and we discuss a potential reason for this observed difference.", "labels": [], "entities": []}, {"text": "We also discuss the role of bias in machine learning and its importance in explaining performance differences observed on specific problems.", "labels": [], "entities": [{"text": "machine learning", "start_pos": 36, "end_pos": 52, "type": "TASK", "confidence": 0.7269078195095062}]}], "introductionContent": [{"text": "Recent research in empirical (corpus-based) natural language processing has explored a number of different methods for learning from data.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 44, "end_pos": 71, "type": "TASK", "confidence": 0.6633104085922241}]}, {"text": "Three general approaches are statistical, neural-network, and symbolic machine learning and numerous specific methods have been developed under each of these paradigms).", "labels": [], "entities": [{"text": "symbolic machine learning", "start_pos": 62, "end_pos": 87, "type": "TASK", "confidence": 0.6998169422149658}]}, {"text": "An important question is whether some methods perform significantly better than others on particular types of problems.", "labels": [], "entities": []}, {"text": "Unfortunately, there have been very few direct comparisons of alternative methods on identical test data.", "labels": [], "entities": []}, {"text": "A somewhat indirect comparison of applying stochastic context-free grammars, a transformation-based method, and inductive logic programming to parsing the ATIS (Airline Travel Information Service) corpus from the Penn Treebank indicates fairly similar performance for these three very different methods.", "labels": [], "entities": [{"text": "ATIS (Airline Travel Information Service) corpus from the Penn Treebank", "start_pos": 155, "end_pos": 226, "type": "DATASET", "confidence": 0.777753139535586}]}, {"text": "Also, comparisons of Bayesian, informationretrieval, neural-network, and case-based methods on word-sense disambiguation have also demonstrated similar performance.", "labels": [], "entities": [{"text": "word-sense disambiguation", "start_pos": 95, "end_pos": 120, "type": "TASK", "confidence": 0.720506340265274}]}, {"text": "However, in a comparison of neural-network and decision-tree methods on learning to generate the past tense of an English verb, decision trees performed significantly better.", "labels": [], "entities": []}, {"text": "Subsequent experiments on this problem have demonstrated that an inductive logic programming method produces even better results than decision trees.", "labels": [], "entities": []}, {"text": "In this paper, we present direct comparisons of a fairly wide range of general learning algorithms on the problem of discriminating six senses of the word \"line\" from context, using data assembled by.", "labels": [], "entities": []}, {"text": "We compare a naive Bayesian classifier, a perceptron, a decision-tree learner, a k nearest-neighbor classifier, logic-based DNF (disjunctive normal form) and CNF (conjunctive normal form) learners and a decisionlist learner.", "labels": [], "entities": []}, {"text": "Tests on all methods used identical training and test sets, and ten separate random trials were run in order to measure average performance and allow statistical testing of the significance of any observed differences.", "labels": [], "entities": []}, {"text": "On this particular task, we found that the Bayesian and perceptron methods perform significantly better than the remaining methods and discuss a potential reason for this observed difference.", "labels": [], "entities": []}, {"text": "We also discuss the role of bias in machine learning and its importance in explaining the observed differences in the performance of alternative methods on specific problems.", "labels": [], "entities": [{"text": "machine learning", "start_pos": 36, "end_pos": 52, "type": "TASK", "confidence": 0.7261479496955872}]}], "datasetContent": [{"text": "In order to evaluate the performance of these seven algorithms, direct multi-trial comparisons on identical training and test sets were run on the \"line\" corpus.", "labels": [], "entities": []}, {"text": "Such head-to-head comparisons of methods are unfortunately relatively rare in the empirical natural-language literature, where papers generally report results of a single method on a single training set with, at best, indirect comparisons to other methods.", "labels": [], "entities": []}, {"text": "The resulting learning curves are shown in and results on training and testing time are shown in. presents the time required to classify the complete set of 894 test examples.", "labels": [], "entities": []}, {"text": "With respect to accuracy, naive Bayes and perceptron perform significantly better (p _< 0.05) than all other methods for all training-set sizes.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.9994280934333801}]}, {"text": "Naive Bayes and perceptron are not significantly different, except at 1,200 training examples where naive Bayes has a slight advantage.", "labels": [], "entities": []}, {"text": "Note that the results for 1,200 training examples are comparable to those obtained by for similar methods.", "labels": [], "entities": []}, {"text": "PFOIL-DLIsT is always significantly better than PFoIL-DNF and PFoIL-CNF and significantly better than 3 Nearest Neighbor and C4.5 at 600 and 1,200 training examples.", "labels": [], "entities": []}, {"text": "C4.5 and 3 Nearest Neighbor are always significantly better than PFoIL-DNF and PFoIL-CNF but", "labels": [], "entities": [{"text": "PFoIL-CNF", "start_pos": 79, "end_pos": 88, "type": "DATASET", "confidence": 0.8491213321685791}]}], "tableCaptions": []}