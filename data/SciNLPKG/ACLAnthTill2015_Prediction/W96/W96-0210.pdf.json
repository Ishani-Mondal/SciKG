{"title": [{"text": "The Measure of a Model *", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper describes measures for evaluating the three determinants of how well a probabilistic elas-sifter performs on a given test set.", "labels": [], "entities": []}, {"text": "These determinants are the appropriateness, for the test set, of the results of (1)feature selection, (2) formulation of the parametric form of the model, and (3) parameter estimation.", "labels": [], "entities": []}, {"text": "These are part of any model formulation procedure, even if not broken out as separate steps, so the tradeoffs explored in this paper are relevant to a wide variety of methods.", "labels": [], "entities": [{"text": "model formulation", "start_pos": 22, "end_pos": 39, "type": "TASK", "confidence": 0.7458518147468567}]}, {"text": "The measures are demonstrated in a large experiment , in which they are used to analyze the results of roughly 300 classifiers that perform word-sense disambiguation.", "labels": [], "entities": [{"text": "word-sense disambiguation", "start_pos": 140, "end_pos": 165, "type": "TASK", "confidence": 0.6897558718919754}]}], "introductionContent": [{"text": "This paper presents techniques that can be used to analyze the formulation of a probabilistic classifter.", "labels": [], "entities": []}, {"text": "As part of this presentation, we apply these techniques to the results of a large number of classifiers, developed using the methodology presented in (2), (3), (4), (5), (12) and (16), which tag words according to their meanings (i.e., that perform word-sense disambiguation).", "labels": [], "entities": []}, {"text": "Other NLP tasks that have been performed using probabilistic classifiers include part-of-speech tagging (11), assignment of semantic classes (8), cue phrase identification (9), prepositional phrase attachment (15), other grammatical disambiguation tasks (6), anaphora resolution (7) and even translation equivalence (1).", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 81, "end_pos": 103, "type": "TASK", "confidence": 0.7233655750751495}, {"text": "assignment of semantic classes", "start_pos": 110, "end_pos": 140, "type": "TASK", "confidence": 0.8750781714916229}, {"text": "cue phrase identification", "start_pos": 146, "end_pos": 171, "type": "TASK", "confidence": 0.6160441140333811}, {"text": "prepositional phrase attachment", "start_pos": 177, "end_pos": 208, "type": "TASK", "confidence": 0.674952228864034}, {"text": "anaphora resolution", "start_pos": 259, "end_pos": 278, "type": "TASK", "confidence": 0.7068468481302261}]}, {"text": "In fact, it could be argued that any problem with a known set of possible solutions can be cast as a classification problem.", "labels": [], "entities": []}, {"text": "A probabilistic classifier assigns, out of a set of possible classes, the one that is most probable according to a probabilistic model.", "labels": [], "entities": []}, {"text": "The model expresses the relationships among the classification variable (the variable representing the classification tag) and var]ables that correspond to prop-*This research Was supported by the Office of Naval Research under grant number N00014-95-1-0776.", "labels": [], "entities": []}, {"text": "erties of the ambiguous object and the context in which it occurs (the non-classification variables).", "labels": [], "entities": []}, {"text": "Each model uniquely defines a classifier.", "labels": [], "entities": []}, {"text": "The basic premise of a probabilistic approach to classification is that the process of assigning object classes is non-deterministic, i.e., there is no infallible indicator of the correct classification.", "labels": [], "entities": []}, {"text": "The purpose of a probabilistic model is to characterize the uncertainty in the classification process.", "labels": [], "entities": []}, {"text": "The probabilistie model defines, for each class and each ambiguous object, the probability that the object belongs to that class, given the values of the nonclassification variables.", "labels": [], "entities": []}, {"text": "The main steps in developing a probabilistic classifier and performing classification on the basis of a probability model are the following.", "labels": [], "entities": []}, {"text": "Feature Selection: selecting informative contextual features.", "labels": [], "entities": [{"text": "Feature Selection", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7635047733783722}]}, {"text": "These are the properties of the ambiguous object and the context in which it occurs that are indicative of its classification.", "labels": [], "entities": []}, {"text": "Typically, each feature is represented as a random variable (a non-classification variable) in the probabilistic model.", "labels": [], "entities": []}, {"text": "Here we will use Fi to designate a random variable that corresponds to the ith contextual feature, and fl to designate the value of Fi.", "labels": [], "entities": []}, {"text": "The contextual features play a very important role in the performance of a model.", "labels": [], "entities": []}, {"text": "They are the representation of context in the model, and it is on the basis of them that we must distinguish among the classes of objects.", "labels": [], "entities": []}, {"text": "2. Selection of the parametric form of the model.", "labels": [], "entities": []}, {"text": "The form of the model expresses the joint distribution of all variables as a function of the values of a set of unknown parameters.", "labels": [], "entities": []}, {"text": "Therefore, the parametric form of a model specifies a family of distributions.", "labels": [], "entities": []}, {"text": "Each member of that family corresponds to a different set of values for the unknown parameters.", "labels": [], "entities": []}, {"text": "The form of a model 1Although these are always involved in developing probabilistic classifiers, they may not be broken out into three separate steps in a particular method; an example is decision tree induction.", "labels": [], "entities": [{"text": "1Although", "start_pos": 20, "end_pos": 29, "type": "METRIC", "confidence": 0.9837256073951721}, {"text": "decision tree induction", "start_pos": 188, "end_pos": 211, "type": "TASK", "confidence": 0.8245556751887003}]}, {"text": "specifies the stochastic relationships, the interdependencies, that exist among the variables.", "labels": [], "entities": []}, {"text": "The parameters define the distributions of the sets of interdependent variables, i.e., the probabilities of the various combinations of the values of the interdependent variables.", "labels": [], "entities": []}, {"text": "As an illustration, consider the following three parametric forms, each specifying different sets of interdependencies among variables in describing the joint distribution of a classification variable, Tag, and a set of nonclassification variables, F1 through Fn.", "labels": [], "entities": [{"text": "F1", "start_pos": 249, "end_pos": 251, "type": "METRIC", "confidence": 0.990483283996582}]}, {"text": "In the equations below, tag represents the value of the classification variable and the fi's denote the values of the non-classification variables.", "labels": [], "entities": []}, {"text": "The model for interdependence among all variables: V tag, fl, f2,..., fn P(tag, fl, f2 ....", "labels": [], "entities": []}, {"text": ", fn) = P(tag, fl,f2,...,fn) (1) The model for conditional independence among all non-classification variables given the value of the classification variable: The model for independence among all variables: V tag, fl,f~,...,fn P(tag,fl,f2,...,fn) = P(tag) x P(fl) x P(f2) x ...", "labels": [], "entities": []}, {"text": "x P(fn)3) The objective in defining the parametric form of a model is to describe the relationships among all variables in terms of only the most important interdependencies.", "labels": [], "entities": []}, {"text": "While it is always true that all variables can be treated as interdependent (equation 1), if there are several features, such a model could have too many parameters to estimate in practice.", "labels": [], "entities": []}, {"text": "The greater the number of interdependeneies expressed in a model the more complex the model is said to be.", "labels": [], "entities": []}, {"text": "3. Estimation of the model parameters from the training data.", "labels": [], "entities": [{"text": "Estimation", "start_pos": 3, "end_pos": 13, "type": "METRIC", "confidence": 0.8066598773002625}]}, {"text": "While the form of a model identifies the relationships among the variables, the parameters express the uncertainty inherent in those relationships.", "labels": [], "entities": []}, {"text": "Recall that the parameters of a model describe the distributions of the sets of interdependent variables by defining the likelihood of seeing each combination of the values of those variables.", "labels": [], "entities": []}, {"text": "For example, the parameters of the model for independence are the following: V tag, fl,f~,...,fr, : P(tag), P(fl), P(f2) ...", "labels": [], "entities": []}], "datasetContent": [{"text": "In this paper, we analyze the performance of classifters developed for the disambiguation of twelve different words.", "labels": [], "entities": []}, {"text": "For each of these words, we develop a range of classifiers based on models of varying complexity.", "labels": [], "entities": []}, {"text": "Our purpose is to study the contribution that each of feature selection, selection of the form of a model, and parameter estimation makes to overall model performance.", "labels": [], "entities": []}, {"text": "In this section, we describe the basic experimental setup used in these evaluations, in particular, the protocol used in the disambiguation experiments and the procedure used to formulate each model.", "labels": [], "entities": []}, {"text": "There are three parameters that define a wordsense disambiguation experiment: (1)the choice of words and word meanings (their number and type), (2) the method used to identify the \"correct\" word meaning, and (3) the choice of text from which the data is taken.", "labels": [], "entities": [{"text": "wordsense disambiguation", "start_pos": 41, "end_pos": 65, "type": "TASK", "confidence": 0.704979807138443}]}, {"text": "In these experiments, the complete set of non-idiomatic senses defined in the Longman's Dictionary of Contemporary English (LDOCE) is used as the tag set for each word to be disambiguated.", "labels": [], "entities": [{"text": "Longman's Dictionary of Contemporary English (LDOCE)", "start_pos": 78, "end_pos": 130, "type": "DATASET", "confidence": 0.9245843357510037}]}, {"text": "For each use of a targeted word, the best tag, from among the set of LDOCE sense tags, is determined by a human judge.", "labels": [], "entities": []}, {"text": "The tag assigned by the classifier is accepted as correct only when it is identical to the tag pre-selected by the human judge.", "labels": [], "entities": []}, {"text": "All data used in these experiments are taken from the Penn Treebank Wall Street Journal corpus (10).", "labels": [], "entities": [{"text": "Penn Treebank Wall Street Journal corpus (10)", "start_pos": 54, "end_pos": 99, "type": "DATASET", "confidence": 0.9722447594006857}]}, {"text": "This corpus was selected because of its availability and size.", "labels": [], "entities": []}, {"text": "Further, the POS categories assigned in the Penn 'IYeebank corpus are used to resolve syntactic ambiguity so that word-meaning disambiguation occurs only after the syntactic category of a word has been identified.", "labels": [], "entities": [{"text": "Penn 'IYeebank corpus", "start_pos": 44, "end_pos": 65, "type": "DATASET", "confidence": 0.9610961675643921}, {"text": "word-meaning disambiguation", "start_pos": 114, "end_pos": 141, "type": "TASK", "confidence": 0.7050070315599442}]}, {"text": "The following words were selected for disambiguation based on their relatively high frequency of occurrence and the appropriateness of their sense distinctions for the textual domain.", "labels": [], "entities": []}, {"text": "* Nouns: interest, bill, concern, and drug.", "labels": [], "entities": []}, {"text": "\u2022 Verbs: close, help, agree, and include.", "labels": [], "entities": [{"text": "Verbs", "start_pos": 2, "end_pos": 7, "type": "METRIC", "confidence": 0.9944057464599609}]}, {"text": "\u2022 Adjectives: chief, public, last, and common.", "labels": [], "entities": []}, {"text": "Because word senses from a particular dictionary are used, the degree of ambiguity for each word is fixed, and the overall level of ambiguity addressed by the experiment is determined by this selection of words.", "labels": [], "entities": []}, {"text": "This paper describes measures that can be used to examine the appropriateness, for the test set, of the features used in a model, the parametric form of the model, and the parameter estimates.", "labels": [], "entities": []}, {"text": "Below, a completed model is a model in which the features have been specified; the parametric form has been specified; and the parameters have been estimated.", "labels": [], "entities": []}, {"text": "Given a completed model in which the parameters have been estimated from the training data: the overall model performance is the percentage of the test set tagged correctly by a classifier using that model to tag the test set.", "labels": [], "entities": []}, {"text": "Comments: Other widely-used loss functions are entropy, cross-entropy, and squared error.", "labels": [], "entities": []}, {"text": "Let FT be the most frequently-occurring (correct) tag fora word in the test set.", "labels": [], "entities": [{"text": "FT", "start_pos": 4, "end_pos": 6, "type": "METRIC", "confidence": 0.9720544219017029}]}, {"text": "The lower bound for that word is the percentage of the test set assigned tag FT.", "labels": [], "entities": [{"text": "FT", "start_pos": 77, "end_pos": 79, "type": "METRIC", "confidence": 0.5351811647415161}]}, {"text": "Comments: The classification performance of a probabilistie model should not be worse than that of the simplest model, the model for independence: V tag, fl,f2,...,fn P(tag, fl,f2,...,fn)= P(tag) x P(ffl) x P(f2) \u00d7...x P(fn) Because the probability of seeing each value of the classification variable (i.e., each tag) is independent of the context, this model assigns every object the most frequently occurring tag:", "labels": [], "entities": []}], "tableCaptions": []}