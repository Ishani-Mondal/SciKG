{"title": [{"text": "MBT: A Memory-Based Part of Speech Tagger-Generator", "labels": [], "entities": [{"text": "MBT", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7623676061630249}, {"text": "Speech Tagger-Generator", "start_pos": 28, "end_pos": 51, "type": "TASK", "confidence": 0.7238204479217529}]}], "abstractContent": [{"text": "We introduce a memory-based approach to part of speech tagging.", "labels": [], "entities": [{"text": "part of speech tagging", "start_pos": 40, "end_pos": 62, "type": "TASK", "confidence": 0.6405830979347229}]}, {"text": "Memory-based learning is a form of supervised learning based on similarity-based reasoning.", "labels": [], "entities": []}, {"text": "The part of speech tag of a word in a particular context is extrapolated from the most similar cases held in memory.", "labels": [], "entities": []}, {"text": "Supervised learning approaches are useful when a tagged corpus is available as an example of the desired output of the tagger.", "labels": [], "entities": []}, {"text": "Based on such a corpus, the tagger-generator automatically builds a tagger which is able to tag new text the same way, diminishing development time for the construction of a tagger considerably.", "labels": [], "entities": []}, {"text": "Memory-based tagging shares this advantage with other statistical or machine learning approaches.", "labels": [], "entities": [{"text": "Memory-based tagging", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.6969605684280396}]}, {"text": "Additional advantages specific to a memory-based approach include (i) the relatively small tagged corpus size sufficient for training, (ii) incremental learning, (iii) explanation capabilities, (iv) flexible integration of information in case representations, (v) its non-parametric nature, (vi) reasonably good results on unknown words without morphological analysis, and (vii) fast learning and tagging.", "labels": [], "entities": []}, {"text": "In this paper we show that a large-scale application of the memory-based approach is feasible: we obtain a tagging accuracy that is on a par with that of known statistical approaches, and with attractive space and time complexity properties when using IGTree, a tree-based formalism for indexing and searching huge case bases.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 115, "end_pos": 123, "type": "METRIC", "confidence": 0.8873090147972107}]}, {"text": "The use of IGTree has as additional advantage that optimal context size for disambiguation is dynamically computed.", "labels": [], "entities": []}], "introductionContent": [{"text": "Part of Speech (POS) tagging is a process in which syntactic categories are assigned to words.", "labels": [], "entities": [{"text": "Part of Speech (POS) tagging", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.7407037360327584}]}, {"text": "It can be seen as a mapping from sentences to strings of tags.", "labels": [], "entities": []}, {"text": "Automatic tagging is useful fora number of applications: as a preprocessing stage to parsing, in information retrieval, in text to speech systems, in corpus linguistics, etc.", "labels": [], "entities": [{"text": "Automatic tagging", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7712200880050659}, {"text": "parsing", "start_pos": 85, "end_pos": 92, "type": "TASK", "confidence": 0.9783601760864258}, {"text": "information retrieval", "start_pos": 97, "end_pos": 118, "type": "TASK", "confidence": 0.8161388635635376}]}, {"text": "The two factors determining the syntactic category of a word are its lexical probability (e.g. without context, man is more probably a noun than a verb), and its contextual probability (e.g. after a pronoun, man is more probably a verb than a noun, as in they man the boats).", "labels": [], "entities": []}, {"text": "Several approaches have been proposed to construct automatic taggers.", "labels": [], "entities": []}, {"text": "Most work on statistical methods has used n-gram models or Hidden Markov Model-based taggers (e.g.).", "labels": [], "entities": []}, {"text": "In these approaches, a tag sequence is chosen fora sentence that maximizes the product of lexical and contextual probabilities as estimated from a tagged corpus.", "labels": [], "entities": []}, {"text": "In rule-based approaches, words are assigned a tag based on a set of rules and a lexicon.", "labels": [], "entities": []}, {"text": "These rules can either be hand-crafted (, or learned, as in or the transformation-based error-driven approach of.", "labels": [], "entities": []}, {"text": "Ina memory-based approach, a set of cases is kept in memory.", "labels": [], "entities": []}, {"text": "Each case consists of a word (or a lexical representation for the word) with preceding and following context, and the corresponding category for that word in that context.", "labels": [], "entities": []}, {"text": "A new sentence is tagged by selecting for each word in the sentence and its context the most similar case(s) in memory, and extrapolating the category of the word from these 'nearest neighbors'.", "labels": [], "entities": []}, {"text": "A memorybased approach has features of both learning rule-based taggers (each case can be regarded as a very specific rule, the similarity based reasoning as a form of conflict resolution and rule selection mechanism) and of stochastic taggers: it is fundamentally a form of k-nearest neighbors (k-nn) modeling, a well-known non-parametric statistical pattern recognition technique.", "labels": [], "entities": [{"text": "conflict resolution and rule selection", "start_pos": 168, "end_pos": 206, "type": "TASK", "confidence": 0.6554602205753326}, {"text": "statistical pattern recognition", "start_pos": 340, "end_pos": 371, "type": "TASK", "confidence": 0.6828051606814066}]}, {"text": "The approach in its basic form is computationally expensive, however; each new word in context that has to be tagged, has to be compared to each pattern kept in memory.", "labels": [], "entities": []}, {"text": "In this paper we show that a heuristic case base compression formalism (, makes the memory-based approach computationally attractive.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we report first results on our memory-based tagging approach.", "labels": [], "entities": [{"text": "memory-based tagging", "start_pos": 48, "end_pos": 68, "type": "TASK", "confidence": 0.5883039236068726}]}, {"text": "Ina first set of experiments, we compared our IGTree implementation of memory-based learning to more traditional implementations of the approach.", "labels": [], "entities": []}, {"text": "In further experiments we studied the performance of our system on predicting the category of both known and unknown words.", "labels": [], "entities": []}, {"text": "Set-up The experimental methodology was taken from Machine Learning practice (e.g.: independent training and test sets were selected from the original corpus, the system was trained on the training set, and the generalization accuracy (percentage of correct category assignments) was computed on the independent test set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 226, "end_pos": 234, "type": "METRIC", "confidence": 0.8612992167472839}]}, {"text": "Storage and time requirements were computed as well.", "labels": [], "entities": []}, {"text": "Where possible, we used a 10-fold cross-validation approach.", "labels": [], "entities": []}, {"text": "In this experimental method, a data set is partitioned ten times into 90% training material, and 10% testing material.", "labels": [], "entities": []}, {"text": "Average accuracy provides a reliable estimate of the generalization accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 8, "end_pos": 16, "type": "METRIC", "confidence": 0.9518246054649353}, {"text": "generalization", "start_pos": 53, "end_pos": 67, "type": "TASK", "confidence": 0.9643262028694153}, {"text": "accuracy", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.6728110909461975}]}, {"text": "Our goal is to adhere to the concept of memory-based learning with full memory while at the same timekeeping memory and processing speed within attractive bounds.", "labels": [], "entities": []}, {"text": "To this end, we applied the IGTree formalism to the task.", "labels": [], "entities": []}, {"text": "In order to prove that IGTree is a suitable candidate for practical memory-based tagging, we compared three memory-based learning algorithms: (i) IB1, a slight extension (to cope with symbolic values and ambiguous training items) of the well-known k-nn algorithm in statistical pattern recognition (see, (ii) IBI-IG, an extension of IB1 which uses feature relevance weighting (described in Section 2), and (iii) IGTree, a memory-and processing time saving heuristic implementation of IBi-IG (see Section 3).", "labels": [], "entities": [{"text": "statistical pattern recognition", "start_pos": 266, "end_pos": 297, "type": "TASK", "confidence": 0.7648190061251322}]}, {"text": "lists the results in generalization accuracy, storage requirements and speed for the three algorithms using a ddfat pattern, a 100,000 word training set, and a 10,000 word test set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.994797945022583}]}, {"text": "In this experiment, accuracy was tested on known words only.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9994787573814392}]}, {"text": "The IGTree version turns out to be better or equally good in terms of generalization accuracy, but also is more than 100 times faster for tagging of new words 4, and compresses 4In training, i.e. building the case base, IB1 and IBi-IG (4 seconds) are faster than IGTree (26 seconds) because the latter has to build a tree instead of just storing the patterns.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 85, "end_pos": 93, "type": "METRIC", "confidence": 0.9599815607070923}, {"text": "tagging of new words 4", "start_pos": 138, "end_pos": 160, "type": "TASK", "confidence": 0.8596863627433777}]}, {"text": "the original case base to 4% of the size of the original case base.", "labels": [], "entities": []}, {"text": "This experiment shows that for this problem, we can use IGTree as a time and memory saving approximation of memory-based learning (IB-IG version), without loss in generalization accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 178, "end_pos": 186, "type": "METRIC", "confidence": 0.9318679571151733}]}, {"text": "The time and speed advantage of IGTree grows with larger training sets.", "labels": [], "entities": [{"text": "time", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.9862085580825806}, {"text": "speed", "start_pos": 13, "end_pos": 18, "type": "METRIC", "confidence": 0.9603176712989807}, {"text": "IGTree", "start_pos": 32, "end_pos": 38, "type": "DATASET", "confidence": 0.7169655561447144}]}, {"text": "A ten-fold cross-validation experiment on the first two million words of the WSJ corpus shows an average generalization performance of IGTree (on known words only) of 96.3%.", "labels": [], "entities": [{"text": "WSJ corpus", "start_pos": 77, "end_pos": 87, "type": "DATASET", "confidence": 0.9718614816665649}, {"text": "IGTree", "start_pos": 135, "end_pos": 141, "type": "METRIC", "confidence": 0.7578195333480835}]}, {"text": "We did 10-fold cross-validation experiments for several sizes of datasets (in steps of 100,000 memory items), revealing the learning curve in.", "labels": [], "entities": []}, {"text": "Training set size is on\" the X-axis, generalization performance as measured in a 10-fold cross-validation experiment is on the Y-axis.", "labels": [], "entities": []}, {"text": "the 'error' range indicate averages plus and minus one standard deviation on each 10-fold cross-validation experiment.", "labels": [], "entities": [{"text": "error' range", "start_pos": 5, "end_pos": 17, "type": "METRIC", "confidence": 0.9547015031178793}]}, {"text": "5 Already at small data set sizes, performance is relatively high.", "labels": [], "entities": []}, {"text": "With increasingly larger data sets, the performance becomes more stable (witness the error ranges).", "labels": [], "entities": []}, {"text": "It should be noted that in this experiment, we assumed correctly disambiguated tags in the left context.", "labels": [], "entities": []}, {"text": "In practice, when using our tagger, this is of course not the case because the disambiguated tags in the left context of the current word to be tagged are the result of a previous decision of the tagger, which maybe a mistake.", "labels": [], "entities": []}, {"text": "To test the influence of this effect we performed a third experiment.", "labels": [], "entities": []}, {"text": "We performed the complete tagger generation process on a 2 million words training set (lexicon construction and known and unknown words case-base construction), and tested on 200,000 test words.", "labels": [], "entities": [{"text": "tagger generation", "start_pos": 26, "end_pos": 43, "type": "TASK", "confidence": 0.9759340584278107}]}, {"text": "Performance on known words, unknown words, and total are given in.", "labels": [], "entities": []}, {"text": "In this experiment, numbers were not stored in the known words case base; they are looked up in the unknown words case base.", "labels": [], "entities": []}, {"text": "~We are not convinced that variation in the results of the experiments in a 10-fold-cv set-up is statistically meaningful (the 10 experiments are not independent), but follow common practice here.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Case representation and information gain pattern for unknown words.", "labels": [], "entities": [{"text": "Case representation", "start_pos": 10, "end_pos": 29, "type": "TASK", "confidence": 0.8620935380458832}]}, {"text": " Table 3: Comparison of three memory-based learning techniques.", "labels": [], "entities": []}, {"text": " Table 4: Accuracy of IGTree tagging on known and unknown words", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9963558912277222}, {"text": "IGTree tagging", "start_pos": 22, "end_pos": 36, "type": "TASK", "confidence": 0.8468317985534668}]}]}