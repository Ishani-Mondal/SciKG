{"title": [{"text": "Coping with Ambiguity and Unknown Words through Probabilistic Models Marie Meteer t Rensselaer Polytechnic Institute", "labels": [], "entities": []}], "abstractContent": [{"text": "From spring 1990 through fall 1991, we performed a battery of small experiments to test the effectiveness of supplementing knowledge-based techniques with probabilistic models.", "labels": [], "entities": []}, {"text": "This paper reports our experiments in predicting parts of speech of highly ambiguous words, predicting the intended interpretation of an utterance when more than one interpretation satisfies all known syntactic and semantic constraints, and learning case frame information for verbs from example uses.", "labels": [], "entities": [{"text": "predicting parts of speech of highly ambiguous words", "start_pos": 38, "end_pos": 90, "type": "TASK", "confidence": 0.8263194710016251}, {"text": "predicting the intended interpretation of an utterance", "start_pos": 92, "end_pos": 146, "type": "TASK", "confidence": 0.7233408434050423}]}, {"text": "From these experiments, we are convinced that probabilistic models based on annotated corpora can effectively reduce the ambiguity in processing text and can be used to acquire lexical information from a corpus, by supplementing knowledge-based techniques.", "labels": [], "entities": []}, {"text": "Based on the results of those experiments, we have constructed anew natural language system (PLUM)for extracting data from text, e.g., newswire text.", "labels": [], "entities": []}], "introductionContent": [{"text": "Natural language processing, and AI in general, have focused mainly on building rule-based systems with carefully handcrafted rules and domain knowledge.", "labels": [], "entities": [{"text": "Natural language processing", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.6524521907170614}]}, {"text": "Our own natural language database query systems, JANUS (),, have used these techniques quite successfully.", "labels": [], "entities": [{"text": "JANUS", "start_pos": 49, "end_pos": 54, "type": "DATASET", "confidence": 0.7728741765022278}]}, {"text": "However, as we move from the application of understanding database queries in limited domains to applications of processing open-ended text, we found challenges that questioned our previous assumptions and suggested probabilistic models instead.", "labels": [], "entities": []}, {"text": "2. Having semantics for all (or even most) of the words of the vocabulary would violate the limited domain assumption, since roughly 50% of the message stream mentions no terrorist incident, and even those that do maybe primarily about a different topic or topics.", "labels": [], "entities": []}, {"text": "Therefore, the power of semantic constraints in limited domains would be diluted.", "labels": [], "entities": []}, {"text": "Probability models could be employed where less knowledge was available.", "labels": [], "entities": []}, {"text": "3. Given the vocabulary size, we could not expect to give full syntactic or semantic features.", "labels": [], "entities": []}, {"text": "The labor for handcrafted definitions would not be warranted.", "labels": [], "entities": []}, {"text": "Statistical language models have a learning component that might supplement handcrafted knowledge.", "labels": [], "entities": []}, {"text": "4. Purely rule-based techniques seemed too brittle for dealing with the variety of constructions, the long sentences (averaging 29 words per sentence), and the degree of unexpected input.", "labels": [], "entities": []}, {"text": "Statistical models based on local information (e.g., might operate effectively in spite of sentence length and unexpected input.", "labels": [], "entities": []}, {"text": "To see whether our four hypotheses (in italics above) effectively addressed the four concerns above, we chose to test the hypotheses on two well-known problems: ambiguity (both at the structural level and at the part-of-speech level) and inferring syntactic and semantic information about unknown words.", "labels": [], "entities": [{"text": "inferring syntactic and semantic information about unknown words", "start_pos": 238, "end_pos": 302, "type": "TASK", "confidence": 0.7850352302193642}]}, {"text": "Guided by the past success of probabilistic models in speech processing, we have integrated probabilistic models into our language processing systems.", "labels": [], "entities": []}, {"text": "Early speech research used purely knowledge-based approaches, analogous to knowledge-based approaches in NLP systems today.", "labels": [], "entities": []}, {"text": "These required much detailed, handcrafted knowledge from several sources (e.g., acoustic and phonetic).", "labels": [], "entities": []}, {"text": "However, when it became clear that these techniques were too brittle and not scalable, speech researchers turned to probabilistic models.", "labels": [], "entities": []}, {"text": "These provided a flexible control structure for combining multiple sources of knowledge (providing improved accuracy and ability to deal with more complex domains) and algorithms for training the system on large bodies of data (providing reduced cost in moving the technology to anew application domain).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 108, "end_pos": 116, "type": "METRIC", "confidence": 0.9983713030815125}]}, {"text": "Since probability theory offers a general mathematical modeling tool for estimating how likely an event is, probability theory maybe applied at all levels in natural language processing, because some set of events can be associated with each algorithm.", "labels": [], "entities": []}, {"text": "For example, in morphological processing in English (Section 2), the events are the use of a word with a particular part of speech in a string of words.", "labels": [], "entities": []}, {"text": "At the level of syntax (Section 3), an event is the use of a particular structure; the model predicts what the most likely rule is given a particular situation.", "labels": [], "entities": []}, {"text": "One can similarly use probabilities for assigning semantic structure (Section 4).", "labels": [], "entities": [{"text": "assigning semantic structure", "start_pos": 40, "end_pos": 68, "type": "TASK", "confidence": 0.8293083111445109}]}, {"text": "We report in Section 2 on our experiments on the assignment of part of speech to words in text.", "labels": [], "entities": [{"text": "assignment of part of speech to words in text", "start_pos": 49, "end_pos": 94, "type": "TASK", "confidence": 0.8305274579260085}]}, {"text": "The effectiveness of such models is well known, and they are currently in use in parsers (e.g..", "labels": [], "entities": []}, {"text": "Our work is an incremental improvement on these models in three ways: (1) Much less training data than theoretically required proved adequate; (2) we integrated a probabilistic model of word features to handle unknown words uniformly within the probabilistic model and measured its contribution; and (3) we have applied the forward-backward algorithm to accurately compute the most likely tag set.", "labels": [], "entities": []}, {"text": "In Section 3, we demonstrate that probability models can improve the performance of knowledge-based syntactic and semantic processing in dealing with structural ambiguity and with unknown words.", "labels": [], "entities": [{"text": "knowledge-based syntactic and semantic processing", "start_pos": 84, "end_pos": 133, "type": "TASK", "confidence": 0.6451457858085632}]}, {"text": "Though the probability model employed is not new, our empirical findings are novel.", "labels": [], "entities": []}, {"text": "When a choice among alternative interpretations pro-duced by a unification-based parser and semantic interpreter must be made, a simple context-free probability model reduced the error rate by a factor of two compared with using no model.", "labels": [], "entities": []}, {"text": "It is well known that a unification parser can process an unknown word by collecting the assumptions it makes while trying to find an interpretation fora sentence.", "labels": [], "entities": [{"text": "unification parser", "start_pos": 24, "end_pos": 42, "type": "TASK", "confidence": 0.8816518485546112}]}, {"text": "As a second result, we found that adding a context-free probability model improved the unification predictions of syntactic and semantic properties of an unknown word, reducing the error rate by a factor of two compared with no model.", "labels": [], "entities": [{"text": "error rate", "start_pos": 181, "end_pos": 191, "type": "METRIC", "confidence": 0.967418760061264}]}, {"text": "In Section 4, we report an experiment in learning case frame information of unknown verbs from examples.", "labels": [], "entities": []}, {"text": "The probabilistic algorithm is critical to selecting the appropriate generalizations to make from a set of examples.", "labels": [], "entities": []}, {"text": "The effectiveness of the semantic case frames inferred is measured by testing how well those case frames predict the correct attachment point for prepositional phrases.", "labels": [], "entities": []}, {"text": "In this case, a significant new model synthesizing both semantic and syntactic knowledge is employed.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}