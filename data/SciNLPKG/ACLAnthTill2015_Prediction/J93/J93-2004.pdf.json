{"title": [{"text": "Building a Large Annotated Corpus of English: The Penn Treebank", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 50, "end_pos": 63, "type": "DATASET", "confidence": 0.9918027818202972}]}], "abstractContent": [], "introductionContent": [], "datasetContent": [{"text": "To determine how to maximize the speed, inter-annotator consistency, and accuracy of POS tagging, we performed an experiment at the very beginning of the project to compare two alternative modes of annotation.", "labels": [], "entities": [{"text": "speed", "start_pos": 33, "end_pos": 38, "type": "METRIC", "confidence": 0.9909806847572327}, {"text": "consistency", "start_pos": 56, "end_pos": 67, "type": "METRIC", "confidence": 0.8969042301177979}, {"text": "accuracy", "start_pos": 73, "end_pos": 81, "type": "METRIC", "confidence": 0.9995197057723999}, {"text": "POS tagging", "start_pos": 85, "end_pos": 96, "type": "TASK", "confidence": 0.8235034644603729}]}, {"text": "In the first annotation mode (\"tagging\"), annotators tagged unannotated text entirely by hand; in the second mode (\"correcting\"), they verified and corrected the output of PARTS, modified as described above.", "labels": [], "entities": []}, {"text": "This experiment showed that manual tagging took about twice as long as correcting, with about twice the inter-annotator disagreement rate and an error rate that was about 50% higher.", "labels": [], "entities": [{"text": "inter-annotator disagreement rate", "start_pos": 104, "end_pos": 137, "type": "METRIC", "confidence": 0.6860501368840536}, {"text": "error rate", "start_pos": 145, "end_pos": 155, "type": "METRIC", "confidence": 0.9849911630153656}]}, {"text": "Four annotators, all with graduate training in linguistics, participated in the experiment.", "labels": [], "entities": []}, {"text": "All completed a training sequence consisting of 15 hours of correcting followed by 6 hours of tagging.", "labels": [], "entities": [{"text": "correcting", "start_pos": 60, "end_pos": 70, "type": "TASK", "confidence": 0.9226130247116089}]}, {"text": "The training material was selected from a variety of nonfiction genres in the Brown Corpus.", "labels": [], "entities": [{"text": "Brown Corpus", "start_pos": 78, "end_pos": 90, "type": "DATASET", "confidence": 0.9911916255950928}]}, {"text": "All the annotators were familiar with GNU Emacs at the outset of the experiment.", "labels": [], "entities": []}, {"text": "Eight 2,000-word samples were selected from the Brown Corpus, two each from four different genres (two fiction, two nonfiction), none of which any of the annotators had encountered in training.", "labels": [], "entities": [{"text": "Brown Corpus", "start_pos": 48, "end_pos": 60, "type": "DATASET", "confidence": 0.983881413936615}]}, {"text": "The texts for the correction task were automatically tagged as described in Section 2.3.", "labels": [], "entities": [{"text": "correction task", "start_pos": 18, "end_pos": 33, "type": "TASK", "confidence": 0.9097287356853485}]}, {"text": "Each annotator first manually tagged four texts and then corrected four automatically tagged texts.", "labels": [], "entities": []}, {"text": "Each annotator completed the four genres in a different permutation.", "labels": [], "entities": []}, {"text": "A repeated measures analysis of annotation speed with annotator identity, genre, and annotation mode (tagging vs. correcting) as classification variables showed a significant annotation mode effect (p = .05).", "labels": [], "entities": []}, {"text": "No other effects or interactions were significant.", "labels": [], "entities": []}, {"text": "The average speed for correcting was more than twice as fast as the average speed for tagging: 20 minutes vs. 44 minutes per 1,000 words.", "labels": [], "entities": [{"text": "correcting", "start_pos": 22, "end_pos": 32, "type": "TASK", "confidence": 0.9693636298179626}]}, {"text": "(Median speeds per 1,000 words were 22 vs. 42 minutes.)", "labels": [], "entities": []}, {"text": "A simple measure of tagging consistency is inter-annotator disagreement rate, the rate at which annotators disagree with one another over the tagging of lexical tokens, expressed as a percentage of the raw number of such disagreements over the number of words in a given text sample.", "labels": [], "entities": [{"text": "tagging consistency", "start_pos": 20, "end_pos": 39, "type": "TASK", "confidence": 0.7173633277416229}, {"text": "inter-annotator disagreement rate", "start_pos": 43, "end_pos": 76, "type": "METRIC", "confidence": 0.556508352359136}]}, {"text": "For a given text and n annotators, there are disagreement ratios (one for each possible pair of annotators).", "labels": [], "entities": []}, {"text": "Mean inter-annotator disagreement was 7.2% for the tagging task and 4.1% for the correcting task (with medians 7.2% and 3.6%, respectively).", "labels": [], "entities": [{"text": "tagging task", "start_pos": 51, "end_pos": 63, "type": "TASK", "confidence": 0.893320620059967}, {"text": "correcting task", "start_pos": 81, "end_pos": 96, "type": "TASK", "confidence": 0.8835462927818298}]}, {"text": "Upon examination, a disproportionate amount of disagreement in the correcting case was found to be caused by one text that contained many instances of a cover symbol for chemical and other formulas.", "labels": [], "entities": []}, {"text": "In the absence of an explicit guideline for tagging this case, the annotators had made different decisions on what part of speech this cover symbol represented.", "labels": [], "entities": []}, {"text": "When this text is excluded from consideration, mean inter-annotator disagreement for the correcting task drops to 3.5%, with the median unchanged at 3.6%.", "labels": [], "entities": [{"text": "mean inter-annotator disagreement", "start_pos": 47, "end_pos": 80, "type": "METRIC", "confidence": 0.7429783940315247}, {"text": "correcting task", "start_pos": 89, "end_pos": 104, "type": "TASK", "confidence": 0.8950345516204834}]}, {"text": "Consistency, while desirable, tells us nothing about the validity of the annotators' corrections.", "labels": [], "entities": []}, {"text": "We therefore compared each annotator's output not only with the output of each of the others, but also with a benchmark version of the eight texts.", "labels": [], "entities": []}, {"text": "This benchmark version was derived from the tagged Brown Corpus by (1) mapping the original Brown Corpus tags onto the Penn Treebank tagset and (2) carefully handcorrecting the revised version in accordance with the tagging conventions in force at the time of the experiment.", "labels": [], "entities": [{"text": "Brown Corpus", "start_pos": 51, "end_pos": 63, "type": "DATASET", "confidence": 0.9630729556083679}, {"text": "Brown Corpus tags", "start_pos": 92, "end_pos": 109, "type": "DATASET", "confidence": 0.958467423915863}, {"text": "Penn Treebank tagset", "start_pos": 119, "end_pos": 139, "type": "DATASET", "confidence": 0.9882616996765137}]}, {"text": "Accuracy was then computed as the rate of disagreement between each annotator's results and the benchmark version.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9956466555595398}]}, {"text": "The mean accuracy was 5.4% for the tagging task (median 5.7%) and 4.0% for the correcting task (median 3.4%).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 9, "end_pos": 17, "type": "METRIC", "confidence": 0.9901220202445984}, {"text": "tagging task", "start_pos": 35, "end_pos": 47, "type": "TASK", "confidence": 0.8986786305904388}, {"text": "correcting task", "start_pos": 79, "end_pos": 94, "type": "TASK", "confidence": 0.8915387392044067}]}, {"text": "Excluding the same text as above gives a revised mean accuracy for the correcting task of 3.4%, with the median unchanged.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.9273545145988464}, {"text": "correcting task", "start_pos": 71, "end_pos": 86, "type": "TASK", "confidence": 0.8905259370803833}]}, {"text": "We obtained a further measure of the annotators' accuracy by comparing their error rates to the rates at which the raw output of Church's PARTS program--appropriately modified to conform to the Penn Treebank tagset--disagreed with the benchmark version.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.9982654452323914}, {"text": "Penn Treebank tagset", "start_pos": 194, "end_pos": 214, "type": "DATASET", "confidence": 0.9885138471921285}]}, {"text": "The mean disagreement rate between PARTS and the benchmark version was 9.6%, while the corrected version had a mean disagreement rate of 5.4%, as noted above.", "labels": [], "entities": [{"text": "disagreement rate", "start_pos": 9, "end_pos": 26, "type": "METRIC", "confidence": 0.8641146719455719}, {"text": "PARTS", "start_pos": 35, "end_pos": 40, "type": "DATASET", "confidence": 0.7640694975852966}, {"text": "disagreement rate", "start_pos": 116, "end_pos": 133, "type": "METRIC", "confidence": 0.8620831966400146}]}, {"text": "1\u00b0 The annotators were thus reducing the error rate by about 4.2%.", "labels": [], "entities": [{"text": "error rate", "start_pos": 41, "end_pos": 51, "type": "METRIC", "confidence": 0.9888791143894196}]}], "tableCaptions": []}