{"title": [{"text": "Machine Translation with Inferred Stochastic Finite-State Transducers", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7360615432262421}]}], "abstractContent": [{"text": "Finite-state transducers are models that are being used in different areas of pattern recognition and computational linguistics.", "labels": [], "entities": [{"text": "pattern recognition", "start_pos": 78, "end_pos": 97, "type": "TASK", "confidence": 0.8251839280128479}]}, {"text": "One of these areas is machine translation, in which the approaches that are based on building models automatically from training examples are becoming more and more attractive.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 22, "end_pos": 41, "type": "TASK", "confidence": 0.867546409368515}]}, {"text": "Finite-state transducers are very adequate for use in constrained tasks in which training samples of pairs of sentences are available.", "labels": [], "entities": []}, {"text": "A technique for inferring finite-state transducers is proposed in this article.", "labels": [], "entities": []}, {"text": "This technique is based on formal relations between finite-state transducers and rational grammars.", "labels": [], "entities": []}, {"text": "Given a training corpus of source-target pairs of sentences, the proposed approach uses statistical alignment methods to produce a set of conventional strings from which a stochastic rational grammar (e.g., an n-gram) is inferred.", "labels": [], "entities": []}, {"text": "This grammar is finally converted into a finite-state transducer.", "labels": [], "entities": []}, {"text": "The proposed methods are assessed through a series of machine translation experiments within the framework of the EuTrans project.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 54, "end_pos": 73, "type": "TASK", "confidence": 0.6943043172359467}]}], "introductionContent": [{"text": "Formal transducers give rise to an important framework in syntactic-pattern recognition and in language processing).", "labels": [], "entities": [{"text": "syntactic-pattern recognition", "start_pos": 58, "end_pos": 87, "type": "TASK", "confidence": 0.7770386636257172}]}, {"text": "Many tasks in automatic speech recognition can be viewed as simple translations from acoustic sequences to sublexical or lexical sequences (acoustic-phonetic decoding) or from acoustic or lexical sequences to query strings (for database access) or (robot control) commands (semantic decoding).", "labels": [], "entities": [{"text": "automatic speech recognition", "start_pos": 14, "end_pos": 42, "type": "TASK", "confidence": 0.6394835213820139}]}, {"text": "Another similar application is the recognition of continuous hand-written characters ().", "labels": [], "entities": [{"text": "recognition of continuous hand-written characters", "start_pos": 35, "end_pos": 84, "type": "TASK", "confidence": 0.8484405994415283}]}, {"text": "Yet a more complex application of formal transducers is language translation, in which input and output can be text, speech, (continuous) handwritten text, etc..", "labels": [], "entities": [{"text": "language translation", "start_pos": 56, "end_pos": 76, "type": "TASK", "confidence": 0.7255221605300903}]}, {"text": "Rational transductions constitute an important class within the field of formal translation.", "labels": [], "entities": [{"text": "Rational transductions", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9020981192588806}, {"text": "formal translation", "start_pos": 73, "end_pos": 91, "type": "TASK", "confidence": 0.747574508190155}]}, {"text": "These transductions are realized by the so-called finite-state transducers.", "labels": [], "entities": []}, {"text": "Even though other, more powerful transduction models exist, finite-state transducers generally entail much more affordable computational costs, thereby making these simpler models more interesting in practice.", "labels": [], "entities": []}, {"text": "One of the main reasons for the interest in finite-state machines for language translation comes from the fact that these machines can be learned automatically from examples.", "labels": [], "entities": [{"text": "language translation", "start_pos": 70, "end_pos": 90, "type": "TASK", "confidence": 0.7202147245407104}]}, {"text": "Nowadays, only a few techniques exist for inferring finite-state transducers where (q \u03c6 i\u22121 , s \u03c6 i , \u00af t \u03c6 i , q \u03c6 i ) \u2208 \u03b4, q \u03c6 0 = q 0 , and q \u03c6 I \u2208 F.", "labels": [], "entities": []}, {"text": "A pair (s, t) \u2208 \u03a3 \ud97b\udf59 \u00d7 \u2206 \ud97b\udf59 is a translation pair if there is a translation form \u03c6 of length I in T such that I =| s | and t = \u00af t \u03c6 1 \u00af t \u03c6 2 . .", "labels": [], "entities": []}, {"text": "\u00af t \u03c6 I . By d(s, t) we will denote the set of translation forms 2 in T associated with the pair (s, t).", "labels": [], "entities": []}, {"text": "A rational translation is the set of all translation pairs of some finite-state transducer T . This definition of a finite-state transducer is similar to the definition of a regular or finite-state grammar G.", "labels": [], "entities": []}, {"text": "The main difference is that in a finite-state grammar, the set of target symbols \u2206 does not exist, and the transitions are defined on Q \u00d7 \u03a3 \u00d7 Q.", "labels": [], "entities": []}, {"text": "A translation form is the transducer counterpart of a derivation in a finite-state grammar, and the concept of rational translation is reminiscent of the concept of (regular) language, defined as the set of strings associated with the derivations in the grammar G.", "labels": [], "entities": []}, {"text": "Rational translations exhibit many properties similar to those shown for regular languages.", "labels": [], "entities": [{"text": "Rational translations", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.834428071975708}]}, {"text": "One of these properties can be stated as follows: Theorem 1 T \u2286 \u03a3 \ud97b\udf59 \u00d7 \u2206 \ud97b\udf59 is a rational translation if and only if there exist an alphabet \u0393, a regular language L \u2282 \u0393 \ud97b\udf59 , and two morphisms h \u03a3 : \u0393 \ud97b\udf59 \u2192 \u03a3 \ud97b\udf59 and h \u2206 : \u0393 \ud97b\udf59 \u2192 \u2206 \ud97b\udf59 , such that T = {(h \u03a3 (w), h \u2206 (w)) | w \u2208 L}.", "labels": [], "entities": []}, {"text": "As will be discussed later, this theorem directly suggests the transducer inference methods proposed in this article.", "labels": [], "entities": []}], "datasetContent": [{"text": "Different translation tasks of different levels of difficulty were selected to assess the capabilities of the proposed inference method in the framework of the EuTrans project): two Spanish-English tasks (EuTrans-0 and EuTrans-I), an Italian-English task (EuTrans-II) and a Spanish-German task (EuTrans-Ia).", "labels": [], "entities": []}, {"text": "The EuTrans-0 task, with a large semi-automatically generated training corpus, was used for studying the convergence of transducer learning algorithms for increasingly large training sets.", "labels": [], "entities": []}, {"text": "In this article it is used to get an estimation of performance limits of the GIATI technique by assuming an unbounded amount of training data.", "labels": [], "entities": []}, {"text": "The EuTrans-I task was similar to EuTrans-0 but with a more realistically sized training corpus.", "labels": [], "entities": []}, {"text": "This corpus was defined as a first benchmark in the EuTrans project, and therefore results with other techniques are available.", "labels": [], "entities": [{"text": "EuTrans project", "start_pos": 52, "end_pos": 67, "type": "DATASET", "confidence": 0.9201914072036743}]}, {"text": "The EuTrans-II task, with a quite small and highly spontaneous natural training set, was a second benchmark of the project.", "labels": [], "entities": []}, {"text": "Finally, EuTrans-Ia was similar to EuTrans-I, but with a higher degree of nonmonotonicity between corresponding words in input/output sentence pairs., and 7 show some important features of these corpora.", "labels": [], "entities": []}, {"text": "As can be seen in these tables, the training sets of EuTrans-0, EuTrans-I and EuTrans-Ia contain nonnegligible amounts of repeated sentence pairs.", "labels": [], "entities": []}, {"text": "Most of these repetitions correspond to simple and/or usual sentences such as good morning, thank you, and do you have a single room for tonight.", "labels": [], "entities": []}, {"text": "The repetition rate is quite significant for EuTrans-0, but it was explicitly reduced in the more realistic benchmark tasks EuTrans-I and EuTrans-Ia.", "labels": [], "entities": [{"text": "repetition rate", "start_pos": 4, "end_pos": 19, "type": "METRIC", "confidence": 0.9909480512142181}, {"text": "EuTrans-I", "start_pos": 124, "end_pos": 133, "type": "DATASET", "confidence": 0.9033289551734924}]}, {"text": "It is worth noting, however, that no repetitions appear in any of the test sets of these tasks.", "labels": [], "entities": []}, {"text": "While repetitions can be helpful for probability estimation, they are completely useless for inducing the transducer structure.", "labels": [], "entities": [{"text": "probability estimation", "start_pos": 37, "end_pos": 59, "type": "TASK", "confidence": 0.7815217971801758}]}, {"text": "Moreover, since no repetitions appear in the test sets, the estimated probabilities will not be as useful as they could be if test data repetitions exhibited the same patterns as those in the corresponding training materials.", "labels": [], "entities": []}, {"text": "In all the experiments reported in this article, the approximate optimal translations (equation) of the source test strings were computed and the word error rate (WER), the sentence error rate (SER), and the bilingual evaluation understudy (BLEU) metric for the translations were used as assessment criteria.", "labels": [], "entities": [{"text": "word error rate (WER)", "start_pos": 146, "end_pos": 167, "type": "METRIC", "confidence": 0.9122052391370138}, {"text": "sentence error rate (SER)", "start_pos": 173, "end_pos": 198, "type": "METRIC", "confidence": 0.9049989382425944}, {"text": "bilingual evaluation understudy (BLEU) metric", "start_pos": 208, "end_pos": 253, "type": "METRIC", "confidence": 0.732353035892759}]}, {"text": "The WER is the minimum number of substitution, insertion, and deletion operations needed to convert the word string hypothesized by the translation system into a given single reference word string).", "labels": [], "entities": [{"text": "WER", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.99660325050354}]}, {"text": "The SER is the result of a direct comparison between the hypothesized and reference word strings as a whole.", "labels": [], "entities": [{"text": "SER", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.872677206993103}]}, {"text": "The BLEU metric is based on the n-grams of the hypothesized translation that occur in the reference translations.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.9978371262550354}]}, {"text": "The BLEU metric ranges from 0.0 (worst score) to 1.0 (best score).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.9986019730567932}]}], "tableCaptions": [{"text": " Table 1  The Spanish-English corpus. There was no overlap between  training and test sentences, and the test set did not contain  out-of-vocabulary words with respect to any of the training sets.", "labels": [], "entities": []}, {"text": " Table 2  Results with the standard corpus EuTrans-0. The  underlying regular models were smoothed n-grams for  different values of n.", "labels": [], "entities": [{"text": "EuTrans-0", "start_pos": 43, "end_pos": 52, "type": "DATASET", "confidence": 0.8387992978096008}]}, {"text": " Table 3  Results with the standard corpus EuTrans-I. The  underlying regular models were smoothed n-grams for  different values of n.", "labels": [], "entities": [{"text": "EuTrans-I", "start_pos": 43, "end_pos": 52, "type": "DATASET", "confidence": 0.8576675653457642}]}, {"text": " Table 4  The EuTrans-II corpus. There was a small  overlap of seven pairs between the training and  test sets, but 107 source words in the test set were  not in the (training-set-derived) vocabulary.", "labels": [], "entities": [{"text": "EuTrans-II corpus", "start_pos": 14, "end_pos": 31, "type": "DATASET", "confidence": 0.8713596761226654}]}, {"text": " Table 5  Results with the standard EuTrans-II corpus. The  underlying regular models were smoothed  n-grams (Rosenfeld 1995) for different values of n.", "labels": [], "entities": [{"text": "EuTrans-II corpus", "start_pos": 36, "end_pos": 53, "type": "DATASET", "confidence": 0.9355516731739044}]}, {"text": " Table 7  The Spanish-German corpus. There was no overlap  between training and test sets and no  out-of-vocabulary words in the test set.", "labels": [], "entities": [{"text": "Spanish-German corpus", "start_pos": 14, "end_pos": 35, "type": "DATASET", "confidence": 0.7123241126537323}]}, {"text": " Table 8  Results with the standard corpus EuTrans-Ia. The  underlying regular models were smoothed n-grams for  different values of n.", "labels": [], "entities": [{"text": "EuTrans-Ia", "start_pos": 43, "end_pos": 53, "type": "DATASET", "confidence": 0.8578034043312073}]}]}