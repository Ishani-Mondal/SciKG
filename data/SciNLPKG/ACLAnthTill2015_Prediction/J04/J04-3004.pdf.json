{"title": [], "abstractContent": [{"text": "University of Michigan Many problems in computational linguistics are well suited for bootstrapping (semisupervised learning) techniques.", "labels": [], "entities": []}, {"text": "The Yarowsky algorithm is a well-known bootstrapping algorithm, but it is not mathematically well understood.", "labels": [], "entities": []}, {"text": "This article analyzes it as optimizing an objective function.", "labels": [], "entities": []}, {"text": "More specifically, a number of variants of the Yarowsky algorithm (though not the original algorithm itself) are shown to optimize either likelihood or a closely related objective function K.", "labels": [], "entities": []}], "introductionContent": [{"text": "Bootstrapping, or semisupervised learning, has become an important topic in computational linguistics.", "labels": [], "entities": []}, {"text": "For many language-processing tasks, there are an abundance of unlabeled data, but labeled data are lacking and too expensive to create in large quantities, making bootstrapping techniques desirable.", "labels": [], "entities": []}, {"text": "The algorithm was one of the first bootstrapping algorithms to become widely known in computational linguistics.", "labels": [], "entities": []}, {"text": "In brief, it consists of two loops.", "labels": [], "entities": []}, {"text": "The \"inner loop\" or base learner is a supervised learning algorithm.", "labels": [], "entities": []}, {"text": "Specifically, Yarowsky uses a simple decision list learner that considers rules of the form \"If instance x contains feature f , then predict label j\" and selects those rules whose precision on the training data is highest.", "labels": [], "entities": [{"text": "precision", "start_pos": 180, "end_pos": 189, "type": "METRIC", "confidence": 0.9973861575126648}]}, {"text": "The \"outer loop\" is given a seed set of rules to start with.", "labels": [], "entities": []}, {"text": "In each iteration, it uses the current set of rules to assign labels to unlabeled data.", "labels": [], "entities": []}, {"text": "It selects those instances regarding which the base learner's predictions are most confident and constructs a labeled training set from them.", "labels": [], "entities": []}, {"text": "It then calls the inner loop to construct anew classifier (that is, anew set of rules), and the cycle repeats.", "labels": [], "entities": []}, {"text": "An alternative algorithm, co-training, has subsequently become more popular, perhaps in part because it has proven amenable to theoretical analysis, in contrast to the Yarowsky algorithm, which is as yet mathematically poorly understood.", "labels": [], "entities": []}, {"text": "The current article aims to rectify this lack of understanding, increasing the attractiveness of the Yarowsky algorithm as an alternative to co-training.", "labels": [], "entities": []}, {"text": "The Yarowsky algorithm does have the advantage of placing less of a restriction on the data sets it can be applied to.", "labels": [], "entities": []}, {"text": "Co-training requires data attributes to be separable into two views that are conditionally independent given the target label; the Yarowsky algorithm makes no such assumption about its data.", "labels": [], "entities": []}, {"text": "In previous work, I did propose an assumption about the data called precision independence, under which the Yarowsky algorithm could be shown effective.", "labels": [], "entities": [{"text": "precision independence", "start_pos": 68, "end_pos": 90, "type": "METRIC", "confidence": 0.9660970866680145}]}, {"text": "That assumption is ultimately unsatisfactory, however, not only because it The Yarowsky algorithm variants.", "labels": [], "entities": []}, {"text": "Y-1/DL-EM reduces H; the others reduce K.", "labels": [], "entities": []}, {"text": "Y-1/DL-EM-\u039b EM inner loop that uses labeled examples only Y-1/DL-EM-X EM inner loop that uses all examples Y-1/DL-1-R Near-original Yarowsky inner loop, no smoothing Y-1/DL-1-VS Near-original Yarowsky inner loop, \"variable smoothing\" YS-P Sequential update, \"antismoothing\" YS-R Sequential update, no smoothing YS-FS Sequential update, original Yarowsky smoothing restricts the data sets on which the algorithm can be shown effective, but also for additional internal reasons.", "labels": [], "entities": []}, {"text": "A detailed discussion would take us too far afield here, but suffice it to say that precision independence is a property that it would be preferable not to assume, but rather to derive from more basic properties of a data set, and that closer empirical study shows that precision independence fails to be satisfied in some data sets on which the Yarowsky algorithm is effective.", "labels": [], "entities": [{"text": "precision", "start_pos": 84, "end_pos": 93, "type": "METRIC", "confidence": 0.994111955165863}, {"text": "precision", "start_pos": 270, "end_pos": 279, "type": "METRIC", "confidence": 0.9778681993484497}]}, {"text": "This article proposes a different approach.", "labels": [], "entities": []}, {"text": "Instead of making assumptions about the data, it views the Yarowsky algorithm as optimizing an objective function.", "labels": [], "entities": []}, {"text": "We will show that several variants of the algorithm (though not the algorithm in precisely its original form) optimize either negative log likelihood H or an alternative objective function, K, that imposes an upper bound on H.", "labels": [], "entities": []}, {"text": "Ideally, we would like to show that the Yarowsky algorithm minimizes H.", "labels": [], "entities": []}, {"text": "Unfortunately, we are notable to do so.", "labels": [], "entities": []}, {"text": "But we are able to show that a variant of the Yarowsky algorithm, which we call Y-1/DL-EM, decreases H in each iteration.", "labels": [], "entities": []}, {"text": "It combines the outer loop of the Yarowsky algorithm with a different inner loop based on the expectation-maximization (EM) algorithm.", "labels": [], "entities": []}, {"text": "A second proposed variant of the Yarowsky algorithm, Y-1/DL-1, has the advantage that its inner loop is very similar to the original Yarowsky inner loop, unlike Y-1/DL-EM, whose inner loop bears little resemblance to the original.", "labels": [], "entities": []}, {"text": "Y-1/DL-1 has the disadvantage that it does not directly reduce H, but we show that it does reduce the alternative objective function K.", "labels": [], "entities": []}, {"text": "We also consider a third variant, YS.", "labels": [], "entities": []}, {"text": "It differs from Y-1/DL-EM and Y-1/DL-1 in that it updates sequentially (adding a single rule in each iteration), rather than in parallel (updating all rules in each iteration).", "labels": [], "entities": []}, {"text": "Besides having the intrinsic interest of sequential update, YS can be proven effective when using exactly the same smoothing method as used in the original Yarowsky algorithm, in contrast to Y-1/DL-1, which uses either no smoothing or a nonstandard \"variable smoothing.\"", "labels": [], "entities": []}, {"text": "YS is proven to decrease K.", "labels": [], "entities": [{"text": "YS", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.7592781782150269}, {"text": "K", "start_pos": 25, "end_pos": 26, "type": "METRIC", "confidence": 0.9926637411117554}]}, {"text": "The Yarowsky algorithm variants that we consider are summarized in.", "labels": [], "entities": []}, {"text": "To the extent that these variants capture the essence of the original algorithm, we have a better formal understanding of its effectiveness.", "labels": [], "entities": []}, {"text": "Even if the variants are deemed to depart substantially from the original algorithm, we have at least obtained a family of new bootstrapping algorithms that are mathematically understood.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}