{"title": [{"text": "Fast Approximate Search in Large Dictionaries", "labels": [], "entities": [{"text": "Approximate", "start_pos": 5, "end_pos": 16, "type": "METRIC", "confidence": 0.9603343605995178}]}], "abstractContent": [{"text": "The need to correct garbled strings arises in many areas of natural language processing.", "labels": [], "entities": []}, {"text": "If a dictionary is available that covers all possible input tokens, a natural set of candidates for correcting an erroneous input P is the set of all words in the dictionary for which the Levenshtein distance to P does not exceed a given (small) bound k.", "labels": [], "entities": []}, {"text": "In this article we describe methods for efficiently selecting such candidate sets.", "labels": [], "entities": []}, {"text": "After introducing as a starting point a basic correction method based on the concept of a \"universal Levenshtein automaton,\" we show how two filtering methods known from the field of approximate text search can be used to improve the basic procedure in a significant way.", "labels": [], "entities": []}, {"text": "The first method, which uses standard dictionaries plus dictionaries with reversed words, leads to very short correction times for most classes of input strings.", "labels": [], "entities": []}, {"text": "Our evaluation results demonstrate that correction times for fixed-distance bounds depend on the expected number of correction candidates, which decreases for longer input words.", "labels": [], "entities": []}, {"text": "Similarly the choice of an optimal filtering method depends on the length of the input words.", "labels": [], "entities": []}], "introductionContent": [{"text": "In this article, we face a situation in which we receive some input in the form of strings that maybe garbled.", "labels": [], "entities": []}, {"text": "A dictionary that is assumed to contain all possible correct input strings is at our disposal.", "labels": [], "entities": []}, {"text": "The dictionary is used to check whether a given input is correct.", "labels": [], "entities": []}, {"text": "If it is not, we would like to select the most plausible correction candidates from the dictionary.", "labels": [], "entities": []}, {"text": "We are primarily interested in applications in the area of natural language processing in which the background dictionary is very large and fast selection of an appropriate set of correction candidates is important.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 59, "end_pos": 86, "type": "TASK", "confidence": 0.6492290993531545}]}, {"text": "By a \"dictionary,\" we mean any regular (finite or infinite) set of strings.", "labels": [], "entities": []}, {"text": "Some possible concrete application scenarios are the following: \u2022 The dictionary describes the set of words of a highly inflectional or agglutinating language (e.g., Russian, German, Turkish, Finnish, Hungarian) or a language with compound nouns (German).", "labels": [], "entities": []}, {"text": "The dictionary is used by an automated or interactive spelling checker.", "labels": [], "entities": []}, {"text": "\u2022 The dictionary is multilingual and describes the set of all words of a family of languages.", "labels": [], "entities": []}, {"text": "It is used in a system for postcorrection of results of OCR in which scanned texts have a multilingual vocabulary.", "labels": [], "entities": []}, {"text": "\u2022 The dictionary describes the set of all indexed words and phrases of an Internet search engine.", "labels": [], "entities": []}, {"text": "It is used to determine the plausibility that anew query is correct and to suggest \"repaired\" queries when the answer set returned is empty.", "labels": [], "entities": []}, {"text": "\u2022 The input is a query to some bibliographic search engine.", "labels": [], "entities": []}, {"text": "The dictionary contains titles of articles, books, etc.", "labels": [], "entities": []}, {"text": "The selection of an appropriate set of correction candidates fora garbled input P is often based on two steps.", "labels": [], "entities": []}, {"text": "First, all entries W of the dictionary are selected for which the distance between P and W does not exceed a given bound k.", "labels": [], "entities": []}, {"text": "Popular distance measures are the Levenshtein distance or n-gram distances) Second, statistical data, such as frequency information, maybe used to compute a ranking of the correction candidates.", "labels": [], "entities": []}, {"text": "In this article, we ignore the ranking problem and concentrate on the first step.", "labels": [], "entities": []}, {"text": "For selection of correction candidates we use the standard Levenshtein distance.", "labels": [], "entities": [{"text": "Levenshtein distance", "start_pos": 59, "end_pos": 79, "type": "METRIC", "confidence": 0.6497963070869446}]}, {"text": "In most of the above-mentioned applications, the number of correction candidates becomes huge for large values of k.", "labels": [], "entities": []}, {"text": "Hence small bounds are more realistic.", "labels": [], "entities": []}, {"text": "In light of this background, the algorithmic problem discussed in the article can be described as follows: Given a pattern P, a dictionary D, and a small bound k, efficiently compute the set of all entries W in D such that the Levenshtein distance between P and W does not exceed k.", "labels": [], "entities": []}, {"text": "We describe a basic method and two refinements for solving this problem.", "labels": [], "entities": []}, {"text": "The basic method depends on the new concept of a universal deterministic Levenshtein automaton of fixed degree k.", "labels": [], "entities": []}, {"text": "The automaton of degree k maybe used to decide, for arbitrary words U and V, whether the Levenshtein distance between U and V does not exceed k.", "labels": [], "entities": []}, {"text": "The automaton is \"universal\" in the sense that it does not depend on U and V.", "labels": [], "entities": []}, {"text": "The input of the automaton is a sequence of bitvectors computed from U and V.", "labels": [], "entities": []}, {"text": "Though universal Levenshtein automata have not been discussed previously in the literature, determining Levenshtein neighborhood using universal Levenshtein automata is closely related to a more complex table-based method described by the authors.", "labels": [], "entities": []}, {"text": "Hence the main advantage of the new notion is its conceptual simplicity.", "labels": [], "entities": []}, {"text": "In order to use the automaton for solving the above problem, we assume that the dictionary is given as a determininistic finite-state automaton.", "labels": [], "entities": []}, {"text": "The basic method may then be described as a parallel backtracking traversal of the universal Levenshtein automaton and the dictionary automaton.", "labels": [], "entities": []}, {"text": "Backtracking procedures of this form are well-known and have been used previously: for example, by and the authors.", "labels": [], "entities": [{"text": "Backtracking", "start_pos": 0, "end_pos": 12, "type": "METRIC", "confidence": 0.8871315717697144}]}, {"text": "For the first refinement of the basic method, a filtering method used in the field of approximate text search is adapted to the problem of approximate search in a dictionary.", "labels": [], "entities": [{"text": "approximate text search", "start_pos": 86, "end_pos": 109, "type": "TASK", "confidence": 0.6729609370231628}]}, {"text": "In this approach, an additional \"backwards\" dictionary D \u2212R (representing the set of all reverses of the words of a given dictionary D) is used to reduce approximate search in D with a given bound k \u2265 1 to related search problems for smaller bounds k \ud97b\udf59 < kin D and D \u2212R . As for the basic method, universal Levenshtein automata are used to control the search.", "labels": [], "entities": []}, {"text": "Ignoring very short input words and correction bound k = 1, this approach leads to a drastic increase in speed.", "labels": [], "entities": [{"text": "correction bound k", "start_pos": 36, "end_pos": 54, "type": "METRIC", "confidence": 0.9588396747907003}, {"text": "speed", "start_pos": 105, "end_pos": 110, "type": "METRIC", "confidence": 0.9742351770401001}]}, {"text": "Hence the \"backwards dictionary method\" can be considered the central contribution of this article.", "labels": [], "entities": []}, {"text": "The second refinement, which is only interesting for bound k = 1 and short input words, also uses a filtering method from the field of approximate text search.", "labels": [], "entities": []}, {"text": "In this approach, \"dictionaries with single deletions\" are used to reduce approximate search in a dictionary D with bound k = 1 to a conventional lookup technique for finite-state transducers.", "labels": [], "entities": []}, {"text": "Dictionaries with single deletions are constructed by deleting the symbol at a fixed position n in all words of a given dictionary.", "labels": [], "entities": []}, {"text": "For the basic method and the two refinements, detailed evaluation results are given for three dictionaries that differ in terms of the number and average length of entries: a dictionary of the Bulgarian language with 965,339 entries (average length 10.23 symbols), a dictionary of German with 3,871,605 entries (dominated by compound nouns, average length 18.74 symbols), and a dictionary representing a collection of 1,200,073 book titles (average length 47.64 symbols).", "labels": [], "entities": []}, {"text": "Tests were restricted to distance bounds k = 1, 2, 3.", "labels": [], "entities": []}, {"text": "For the approach based on backwards dictionaries, the average correction time fora given input word-including the displaying of all correction suggestionsis between a few microseconds and a few milliseconds, depending on the dictionary, the length of the input word, and the bound k.", "labels": [], "entities": []}, {"text": "Correction times over one millisecond occur only in a few cases for bound k = 3 and short input words.", "labels": [], "entities": [{"text": "Correction", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9683871269226074}]}, {"text": "For bound k = 1, which is important for practical applications, average correction times did not exceed 40 microseconds.", "labels": [], "entities": [{"text": "correction", "start_pos": 72, "end_pos": 82, "type": "METRIC", "confidence": 0.9357255101203918}]}, {"text": "As a matter of fact, correction times area joint result of hardware improvements and algorithmic solutions.", "labels": [], "entities": []}, {"text": "In order to judge the quality of the correction procedure in absolute terms, we introduce an \"idealized\" correction algorithm in which any kind of blind search and superfluous backtracking is eliminated.", "labels": [], "entities": []}, {"text": "Based on an analysis of this algorithm, we believe that using purely algorithmic improvements, our correction times can be improved only by a factor of 50-250, depending on the kind of dictionary used.", "labels": [], "entities": [{"text": "correction", "start_pos": 99, "end_pos": 109, "type": "METRIC", "confidence": 0.9295026659965515}]}, {"text": "This factor represents a theoretical limit in the sense that the idealized algorithm probably cannot be realized in practice.", "labels": [], "entities": []}, {"text": "This article is structured as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we collect some formal preliminaries.", "labels": [], "entities": []}, {"text": "In Section 3, we briefly summarize some known techniques from approximate string search in a text.", "labels": [], "entities": [{"text": "approximate string search", "start_pos": 62, "end_pos": 87, "type": "TASK", "confidence": 0.6261857151985168}]}, {"text": "In Section 4, we introduce universal deterministic Levenshtein automata of degree k and describe how the problem of deciding whether the Levenshtein distance between two strings P and W does not exceed k can be efficiently solved using this automaton.", "labels": [], "entities": []}, {"text": "Since the method is closely related to a table-based approach introduced by the authors (, most of the formal details have been omitted.", "labels": [], "entities": []}, {"text": "Sections 5, 6, and 7 describe, respectively, the basic method, the refined approach based on backwards dictionaries, and the approach based on dictionaries with single deletions.", "labels": [], "entities": []}, {"text": "Evaluation results are given for the three dictionaries mentioned above.", "labels": [], "entities": []}, {"text": "In Section 8 we briefly comment on the difficulties that we encountered when trying to combine dictionary automata and similarity keys.", "labels": [], "entities": []}, {"text": "Theoretical bounds for correction times are discussed in Section 9.", "labels": [], "entities": [{"text": "Section 9", "start_pos": 57, "end_pos": 66, "type": "DATASET", "confidence": 0.8447193503379822}]}, {"text": "The problem considered in this article is well-studied.", "labels": [], "entities": []}, {"text": "Since the number of contributions is enormous, a complete review of related work cannot be given here.", "labels": [], "entities": []}, {"text": "Relevant references with an emphasis on spell-checking and OCR correction are,,,,,,,,, and.", "labels": [], "entities": [{"text": "OCR correction", "start_pos": 59, "end_pos": 73, "type": "TASK", "confidence": 0.8804910480976105}]}, {"text": "Exact or approximate search in a dictionary is discussed, for example, in,,,, and.", "labels": [], "entities": []}, {"text": "Some relevant work from approximate search in texts is described in Section 3.", "labels": [], "entities": []}], "datasetContent": [{"text": "Experimental results were obtained using a Bulgarian lexicon (BL) with 965, 339 word entries (average length 10.23 symbols), a German dictionary (GL) with 3, 871, 605 entries (dominated by compound nouns, average length 18.74 symbols), and a \"lexicon\" (TL) containing 1, 200, 073 bibliographic titles from the Bavarian National Library (average length 47.64 symbols).", "labels": [], "entities": []}, {"text": "The German dictionary and the title dictionary are nonpublic.", "labels": [], "entities": [{"text": "German dictionary", "start_pos": 4, "end_pos": 21, "type": "DATASET", "confidence": 0.977681577205658}, {"text": "title dictionary", "start_pos": 30, "end_pos": 46, "type": "DATASET", "confidence": 0.8494381606578827}]}, {"text": "The basic correction algorithm was implemented in C and tested on a 1.6 GHz Pentium IV machine under Linux.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1  Evaluation results for the basic correction algorithm, Bulgarian dictionary, standard  Levenshtein distance, and distance bounds k = 1, 2, 3. Times in milliseconds.", "labels": [], "entities": [{"text": "Bulgarian dictionary", "start_pos": 65, "end_pos": 85, "type": "DATASET", "confidence": 0.8318414390087128}, {"text": "Levenshtein distance", "start_pos": 97, "end_pos": 117, "type": "METRIC", "confidence": 0.8361014127731323}]}, {"text": " Table 2  Evaluation results for the basic correction algorithm, German dictionary, standard Levenshtein  distance, and distance bounds k = 1, 2, 3. Times in milliseconds.", "labels": [], "entities": [{"text": "German dictionary", "start_pos": 65, "end_pos": 82, "type": "DATASET", "confidence": 0.8872023522853851}, {"text": "Levenshtein  distance", "start_pos": 93, "end_pos": 114, "type": "METRIC", "confidence": 0.80084028840065}]}, {"text": " Table 3  Evaluation results for the basic correction algorithm, title \"lexicon,\" standard Levenshtein  distance, and distance bounds k = 1, 2, 3. Times in milliseconds.", "labels": [], "entities": []}, {"text": " Table 4  Evaluation results using the backwards-dictionary filtering method, Bulgarian dictionary, and  distance bounds k = 1, 2, 3. Times in milliseconds and speedup factors (ratio of times) with  respect to basic algorithm.", "labels": [], "entities": [{"text": "Bulgarian dictionary", "start_pos": 78, "end_pos": 98, "type": "DATASET", "confidence": 0.8243623375892639}]}, {"text": " Table 5  Evaluation results using the backwards-dictionary filtering method, German dictionary, and  distance bounds k = 1, 2, 3. Times in milliseconds and speedup factors (ratio of times) with  respect to basic algorithm.", "labels": [], "entities": [{"text": "German dictionary", "start_pos": 78, "end_pos": 95, "type": "DATASET", "confidence": 0.9175347089767456}]}, {"text": " Table 7  Evaluation results using the backwards-dictionary filtering method for the modified  Levenshtein distance d \ud97b\udf59  L with transpositions, for German dictionary and title \"lexicon,\"  distance bound k = 3. Times in milliseconds and speedup factors (ratio of times) with respect  to basic algorithm.", "labels": [], "entities": []}, {"text": " Table 8  Results for BL, using dictionaries with single deletions for filtering, distance bound k = 1.  Times in milliseconds and speedup factors (ratio of times) with respect to basic algorithm.  Length (CT1) Speedup 1 Length (CT1) Speedup 1  3  0 .011  9.73  12  0.026  3.38  4  0 .011  8.91  13  0.028  3.18  5  0 .010  8.50  14  0.033  2.70  6  0 .011  7.18  15  0.035  2.54  7  0 .013  6.08  16  0.039  2.23  8  0 .015  5.40  17  0.044  1.95  9  0 .017  4.88  18  0.052  1.67  10  0.020  4.40  19  0.055  1.58  11  0.022  4.00  20  0.063  1.44", "labels": [], "entities": []}]}