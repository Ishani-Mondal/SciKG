{"title": [{"text": "Accessor Variety Criteria for Chinese Word Extraction", "labels": [], "entities": [{"text": "Chinese Word Extraction", "start_pos": 30, "end_pos": 53, "type": "TASK", "confidence": 0.5575389464696249}]}], "abstractContent": [{"text": "We are interested in the problem of word extraction from Chinese text collections.", "labels": [], "entities": [{"text": "word extraction from Chinese text collections", "start_pos": 36, "end_pos": 81, "type": "TASK", "confidence": 0.8551826874415079}]}, {"text": "We define a word to be a meaningful string composed of several Chinese characters.", "labels": [], "entities": []}, {"text": "For example, , 'percent', and , 'more and more', are not recognized as traditional Chinese words from the viewpoint of some people.", "labels": [], "entities": []}, {"text": "However, in our work, they are words because they are very widely used and have specific meanings.", "labels": [], "entities": []}, {"text": "We start with the viewpoint that a word is a distinguished linguistic entity that can be used in many different language environments.", "labels": [], "entities": []}, {"text": "We consider the characters that are directly before a string (predecessors) and the characters that are directly after a string (successors) as important factors for determining the independence of the string.", "labels": [], "entities": []}, {"text": "We call such characters accessors of the string, consider the number of distinct predecessors and successors of a string in a large corpus (TREC 5 and TREC 6 documents), and use them as the measurement of the context independency of a string from the rest of the sentences in the document.", "labels": [], "entities": [{"text": "TREC 5", "start_pos": 140, "end_pos": 146, "type": "DATASET", "confidence": 0.8361739814281464}, {"text": "TREC 6 documents", "start_pos": 151, "end_pos": 167, "type": "DATASET", "confidence": 0.8207953770955404}]}, {"text": "Our experiments confirm our hypothesis and show that this simple rule gives quite good results for Chinese word extraction and is comparable to, and for long words outperforms, other iterative methods.", "labels": [], "entities": [{"text": "Chinese word extraction", "start_pos": 99, "end_pos": 122, "type": "TASK", "confidence": 0.6422206362088522}]}], "introductionContent": [{"text": "Words are the basic linguistic units of natural language processing.", "labels": [], "entities": []}, {"text": "The importance of word extraction is stressed in many papers.", "labels": [], "entities": [{"text": "word extraction", "start_pos": 18, "end_pos": 33, "type": "TASK", "confidence": 0.8400361239910126}]}, {"text": "According to, the word is the basic unit in natural language processing (NLP), as it is at the lexical level where all modules interface.", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 44, "end_pos": 77, "type": "TASK", "confidence": 0.7293235957622528}]}, {"text": "Possible modules involved are the lexicon, speech recognition, syntactic parsing, speech synthesis, semantic interpretation, and soon.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 43, "end_pos": 61, "type": "TASK", "confidence": 0.7986078560352325}, {"text": "syntactic parsing", "start_pos": 63, "end_pos": 80, "type": "TASK", "confidence": 0.7546980082988739}, {"text": "speech synthesis", "start_pos": 82, "end_pos": 98, "type": "TASK", "confidence": 0.7867467403411865}, {"text": "semantic interpretation", "start_pos": 100, "end_pos": 123, "type": "TASK", "confidence": 0.7444877028465271}]}, {"text": "Thus, the identification of lexical words and/or the delimitation of words in running texts is a prerequisite of NLP.", "labels": [], "entities": [{"text": "identification of lexical words", "start_pos": 10, "end_pos": 41, "type": "TASK", "confidence": 0.8758619725704193}]}, {"text": "state that interpreting a text as a sequence of words is beneficial for some information retrieval and storage tasks: for example, full-text searches, word-based compression, and key-phrase extraction.", "labels": [], "entities": [{"text": "interpreting a text as a sequence of words", "start_pos": 11, "end_pos": 53, "type": "TASK", "confidence": 0.8517657145857811}, {"text": "information retrieval and storage", "start_pos": 77, "end_pos": 110, "type": "TASK", "confidence": 0.7385517209768295}, {"text": "word-based compression", "start_pos": 151, "end_pos": 173, "type": "TASK", "confidence": 0.758926272392273}, {"text": "key-phrase extraction", "start_pos": 179, "end_pos": 200, "type": "TASK", "confidence": 0.7122136354446411}]}, {"text": "According to, words and tokens are the primary building blocks in almost all linguistic theories and language-processing systems, including Japanese,,, and English (, in various media, such , where a and bare the two biggest substrings of string c.", "labels": [], "entities": []}, {"text": "Zhang, propose the application of a statistical method that is based on context dependence and mutual information.", "labels": [], "entities": []}, {"text": "experiment with both mutual information and residual inverse document frequency (RIDF) as criteria for deciding Japanese words, and their main contribution is in affording a reduced method for computing term and document frequency.", "labels": [], "entities": [{"text": "residual inverse document frequency (RIDF)", "start_pos": 44, "end_pos": 86, "type": "METRIC", "confidence": 0.8236790214266095}]}, {"text": "In almost all of the work cited to this point, the dimension that is used to compute mutual information is term frequency.", "labels": [], "entities": [{"text": "term frequency", "start_pos": 107, "end_pos": 121, "type": "METRIC", "confidence": 0.9097113609313965}]}, {"text": "propose a corpus-based learning approach that learns grammatical rules and automatically evaluates them.", "labels": [], "entities": []}, {"text": "use an unsupervised Viterbi training process to select potential unknown words and iteratively truncate unlikely unknown words in the augmented dictionary.", "labels": [], "entities": []}, {"text": "propose a compression-based algorithm for Chinese text segmentation.", "labels": [], "entities": [{"text": "Chinese text segmentation", "start_pos": 42, "end_pos": 67, "type": "TASK", "confidence": 0.5912164549032847}]}, {"text": "demonstrate an effective combination of deeper linguistic knowledge with the robustness and scalability of a statistical technique to derive knowledge about thematic relations for verb classification.", "labels": [], "entities": [{"text": "verb classification", "start_pos": 180, "end_pos": 199, "type": "TASK", "confidence": 0.7449952065944672}]}, {"text": "deal with the identification of the determinative-measure compounds in parsing Mandarin Chinese by developing grammatical rules to combine determinators and measures.", "labels": [], "entities": [{"text": "parsing Mandarin Chinese", "start_pos": 71, "end_pos": 95, "type": "TASK", "confidence": 0.8684471249580383}]}, {"text": "We introduce another concept, accessor variety (AV) (for a detailed definition, refer to subsection 3.1), to describe the extent to which a string is likely to be a meaningful word.", "labels": [], "entities": []}, {"text": "Actually, uses similar criteria to determine English morpheme boundaries, and our work is partially motivated by his success.", "labels": [], "entities": [{"text": "determine English morpheme boundaries", "start_pos": 35, "end_pos": 72, "type": "TASK", "confidence": 0.569674476981163}]}, {"text": "We first discard those strings with accessor varieties that are smaller than a certain number (called the threshold; see subsequent discussion).", "labels": [], "entities": []}, {"text": "The remaining strings are considered to be potentially meaningful words.", "labels": [], "entities": []}, {"text": "In addition, we apply rules to remove strings that consist of a word and adhesive characters (clarified in subsection 3.2).", "labels": [], "entities": []}, {"text": "Our experiment shows that even for small thresholds, quite good results can be obtained.", "labels": [], "entities": []}, {"text": "In Section 2, we introduce examples of unknown words, the identification of which is the task of our work.", "labels": [], "entities": []}, {"text": "In Section 3, we discuss our method.", "labels": [], "entities": []}, {"text": "In Section 4, we present our experimental results.", "labels": [], "entities": []}, {"text": "We conclude our work with a discussion and a comparison to previous results in Section 5.", "labels": [], "entities": []}, {"text": "In Section 6, we list some future work that can be pursued following the concept of AV.", "labels": [], "entities": []}, {"text": "We note that although our method is quite simple, it is marginally better than previous comparable results.", "labels": [], "entities": []}, {"text": "This method distinguishes itself from statistically based approaches and grammatical rules.", "labels": [], "entities": []}, {"text": "Because of its simplicity, it can be used easily in computer-based applications.", "labels": [], "entities": []}, {"text": "Moreover, innovative variations of our method and its combination with statistical methods and grammatical methods are worthy of further exploration.", "labels": [], "entities": []}], "datasetContent": [{"text": "The corpus-based word extraction method described in the previous section was tested on a 153 MB corpus consisting of People's Daily news and Xinhua news from TREC5 and TREC6 (Linguistic Data Consortium, n.d.).", "labels": [], "entities": [{"text": "word extraction", "start_pos": 17, "end_pos": 32, "type": "TASK", "confidence": 0.7047821879386902}, {"text": "People's Daily news and Xinhua news from TREC5", "start_pos": 118, "end_pos": 164, "type": "DATASET", "confidence": 0.8477438754505582}, {"text": "TREC6 (Linguistic Data Consortium, n.d.", "start_pos": 169, "end_pos": 208, "type": "DATASET", "confidence": 0.78690248302051}]}, {"text": "We also conducted experiments on a small corpus that has approximately 1.7 MB of data and is apart of the former corpus.", "labels": [], "entities": []}, {"text": "The system dictionary that we used in each experiment was downloaded from http://www.mandarintools.com/segmenter.html and contains 119,538 terms from two to seven characters long.", "labels": [], "entities": []}, {"text": "In our method, a preprocessing step was performed on the corpus in which we eliminated all of the non-Chinese symbols.", "labels": [], "entities": []}, {"text": "Each uninterrupted Chinese character sequence was kept as one line in the transformed data.", "labels": [], "entities": []}, {"text": "For each line in the data file, all possible substrings were extracted, along with their predecessors and successors.", "labels": [], "entities": []}, {"text": "Those predecessors and successors were finally merged, and the AV, L av , and R av values were calculated.", "labels": [], "entities": [{"text": "AV", "start_pos": 63, "end_pos": 65, "type": "METRIC", "confidence": 0.9892879724502563}]}, {"text": "Different thresholds were used for discarding those strings with low AV values and checking how the threshold affects the results.", "labels": [], "entities": [{"text": "AV", "start_pos": 69, "end_pos": 71, "type": "METRIC", "confidence": 0.9776016473770142}]}, {"text": "Moreover, the ADHESIVE JUDGE rules were used for the further discarding of those strings that seemed unlikely to be words.", "labels": [], "entities": [{"text": "ADHESIVE", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.7066740989685059}, {"text": "JUDGE", "start_pos": 23, "end_pos": 28, "type": "METRIC", "confidence": 0.47345834970474243}]}, {"text": "A list of adhesive characters is needed when we apply the ADHESIVE JUDGE rules.", "labels": [], "entities": [{"text": "ADHESIVE", "start_pos": 58, "end_pos": 66, "type": "DATASET", "confidence": 0.5636650323867798}, {"text": "JUDGE", "start_pos": 67, "end_pos": 72, "type": "METRIC", "confidence": 0.3814317584037781}]}, {"text": "We constructed the adhesive character list based on the accessor variety information of single characters.", "labels": [], "entities": []}, {"text": "Characters with high L av values were considered to be tail-adhesive characters.", "labels": [], "entities": []}, {"text": "Characters with high R av values were considered to be headadhesive characters.", "labels": [], "entities": [{"text": "R av values", "start_pos": 21, "end_pos": 32, "type": "METRIC", "confidence": 0.9644314646720886}]}, {"text": "Characters with very high AV values were considered to be the delimiters that are used in rule (3) of the ADHESIVE JUDGE rules.", "labels": [], "entities": [{"text": "AV", "start_pos": 26, "end_pos": 28, "type": "METRIC", "confidence": 0.9956406354904175}, {"text": "ADHESIVE JUDGE rules", "start_pos": 106, "end_pos": 126, "type": "DATASET", "confidence": 0.8341751297314962}]}, {"text": "In the end, we placed 68 tail-adhesive characters, 66 head-adhesive characters, and 16 delimiters on our list.", "labels": [], "entities": []}, {"text": "In our experiments, we performed only one step of each of the ADHESIVE JUDGE rules (in either direction) for discarding meaningless multicharacter strings.", "labels": [], "entities": [{"text": "ADHESIVE JUDGE", "start_pos": 62, "end_pos": 76, "type": "METRIC", "confidence": 0.5575413256883621}]}, {"text": "That is, in any of the three styles (h+core, core+t, or h+core+t), only the leftmost or rightmost character was considered among all of the head-or tail-adhesive characters.", "labels": [], "entities": []}, {"text": "If the first character of a string was a head-adhesive character and the remaining substring (after stripping the first character) was found in the system dictionary or the preextracted shorter word lists (and thus a core was found), such a string was considered to be in the h+core form and thrown away.", "labels": [], "entities": []}, {"text": "The same judgment process was used in the core+t and h+core+t styles.", "labels": [], "entities": []}, {"text": "In other words, only the first or last character, or both, of a string were used in rule (2) of the ADHESIVE JUDGE rules.", "labels": [], "entities": [{"text": "ADHESIVE JUDGE rules", "start_pos": 100, "end_pos": 120, "type": "DATASET", "confidence": 0.7832117676734924}]}, {"text": "Such simplification does not hurt the results too much.", "labels": [], "entities": []}, {"text": "The AV value threshold is another important factor in this method.", "labels": [], "entities": [{"text": "AV value threshold", "start_pos": 4, "end_pos": 22, "type": "METRIC", "confidence": 0.9639535347620646}]}, {"text": "We tested different thresholds to evaluate how they influenced the performance.", "labels": [], "entities": []}, {"text": "One might imagine that a higher threshold will result in higher precision while causing the loss of some recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 64, "end_pos": 73, "type": "METRIC", "confidence": 0.9987850785255432}, {"text": "recall", "start_pos": 105, "end_pos": 111, "type": "METRIC", "confidence": 0.9983869791030884}]}, {"text": "This phenomenon was certainly observed in our experiments.", "labels": [], "entities": []}, {"text": "Word length has a relationship with the threshold: that is, longer words required a smaller threshold to reach the same precision, or higher precision could be obtained on longer words with the same threshold, because longer words have more specific usage and appear in fewer environments.", "labels": [], "entities": [{"text": "precision", "start_pos": 120, "end_pos": 129, "type": "METRIC", "confidence": 0.9950245022773743}, {"text": "precision", "start_pos": 141, "end_pos": 150, "type": "METRIC", "confidence": 0.9945178627967834}]}, {"text": "Our first experiment was carried out on the small corpus of Xinhua news.", "labels": [], "entities": [{"text": "small corpus of Xinhua news", "start_pos": 44, "end_pos": 71, "type": "DATASET", "confidence": 0.8695916175842285}]}, {"text": "Strings with lengths varying from two to ten characters were examined.", "labels": [], "entities": []}, {"text": "In the following, we tested our method on the large corpus and all strings with lengths from two to seven characters.", "labels": [], "entities": []}, {"text": "In the end, we extracted the numeric-type compounds from each corpus.", "labels": [], "entities": []}, {"text": "In addition, there is no commonly accepted standard for evaluating the performance of word extraction methods, and it is very hard to decide whether a word is meaningful or not ().", "labels": [], "entities": [{"text": "word extraction", "start_pos": 86, "end_pos": 101, "type": "TASK", "confidence": 0.7830264866352081}]}, {"text": "We define precision as the number of extracted words that would be meaningful in a Chinese native speaker's opinion, divided by the total number of extracted compounds.", "labels": [], "entities": [{"text": "precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9989930987358093}]}, {"text": "As it is very hard to find all of the words in the original corpus that would be found meaningful by a Chinese person, it is very hard to count recall in the traditional way, that is, the number of meaningful words extracted divided by the number of all meaningful words in the original data.", "labels": [], "entities": [{"text": "recall", "start_pos": 144, "end_pos": 150, "type": "METRIC", "confidence": 0.9839725494384766}]}, {"text": "On the other hand, it is also impossible to approach traditional precision and traditional recall by comparing the hand-segmented sample sentences and the automatically segmented sentences, as people usually do, because our method does not touch upon segmentation.", "labels": [], "entities": [{"text": "precision", "start_pos": 65, "end_pos": 74, "type": "METRIC", "confidence": 0.9979645013809204}, {"text": "recall", "start_pos": 91, "end_pos": 97, "type": "METRIC", "confidence": 0.9814873933792114}]}, {"text": "The reason that we do not consider segmentation is that we aim only to investigate the performance of AV itself, whereas the involvement of a segmentation module would inevitablly influence our judgment on the performance of AV.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 35, "end_pos": 47, "type": "TASK", "confidence": 0.9731195569038391}]}, {"text": "Therefore, we substitute partial recall for traditional recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 33, "end_pos": 39, "type": "METRIC", "confidence": 0.9463152885437012}, {"text": "recall", "start_pos": 56, "end_pos": 62, "type": "METRIC", "confidence": 0.9772130846977234}]}, {"text": "We define partial recall as the number of extracted meaningful words (from the whole corpus) that appear in a sample corpus divided by the total number of meaningful words in the sample corpus.", "labels": [], "entities": [{"text": "recall", "start_pos": 18, "end_pos": 24, "type": "METRIC", "confidence": 0.7932320237159729}]}, {"text": "Evidently, the partial recall value will be no smaller, and usually greater, than the recall value calculated in the traditional way.", "labels": [], "entities": [{"text": "recall value", "start_pos": 23, "end_pos": 35, "type": "METRIC", "confidence": 0.9593285322189331}, {"text": "recall value", "start_pos": 86, "end_pos": 98, "type": "METRIC", "confidence": 0.9789314866065979}]}, {"text": "This point will be clearly reflected by the following experimental results.", "labels": [], "entities": []}, {"text": "What should be pointed out here is that some people use the F-measure as an evaluation metric (.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 60, "end_pos": 69, "type": "METRIC", "confidence": 0.9377537965774536}]}, {"text": "However, this is difficult to interpret according to.", "labels": [], "entities": []}, {"text": "In our opinion, as the F-measure or precision-recall curves are based on two parameters, recall and precision, it is enough for us only to list the partial recall and precision.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 23, "end_pos": 32, "type": "METRIC", "confidence": 0.9935731291770935}, {"text": "precision-recall", "start_pos": 36, "end_pos": 52, "type": "METRIC", "confidence": 0.9747856855392456}, {"text": "recall", "start_pos": 89, "end_pos": 95, "type": "METRIC", "confidence": 0.9977284073829651}, {"text": "precision", "start_pos": 100, "end_pos": 109, "type": "METRIC", "confidence": 0.9762217402458191}, {"text": "recall", "start_pos": 156, "end_pos": 162, "type": "METRIC", "confidence": 0.9470936059951782}, {"text": "precision", "start_pos": 167, "end_pos": 176, "type": "METRIC", "confidence": 0.9933465719223022}]}, {"text": "As noted previously, the small corpus contained approximately 1.7 MB data of Xinhua news.", "labels": [], "entities": [{"text": "Xinhua news", "start_pos": 77, "end_pos": 88, "type": "DATASET", "confidence": 0.9340203702449799}]}, {"text": "We processed all of the strings in the corpus with lengths from one to ten shows some of the extracted correct words that were not contained in the system dictionary.", "labels": [], "entities": []}, {"text": "We can see that almost all of these words are compound words, proper names, or derived words.", "labels": [], "entities": []}, {"text": "It would be almost impossible to list all of them in a generalpurpose dictionary.", "labels": [], "entities": []}, {"text": "Furthermore, some of them occur only a few times.", "labels": [], "entities": []}, {"text": "For example, only occurs three times in this corpus.", "labels": [], "entities": []}, {"text": "The method we used has the ability to extract low-frequency words.", "labels": [], "entities": []}, {"text": "shows the overall precision performance when the word length is not specified.", "labels": [], "entities": [{"text": "precision", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.9993157386779785}]}, {"text": "We set the threshold from two to nine and observed that with a larger threshold we could obtain more precise results.", "labels": [], "entities": []}, {"text": "As the number of words extracted was very large (approximately 30,000 words), we randomly chose a portion (often approximately 1,000 words) of the total set of extracted words as the test set to calculate the precision; that is, we listed all of the extracted words, and then for each word, we generated a random number between zero and one.", "labels": [], "entities": [{"text": "precision", "start_pos": 209, "end_pos": 218, "type": "METRIC", "confidence": 0.9992033839225769}]}, {"text": "If the number was smaller than the number of test words divided by the number of all extracted words (here 1, 000/30, 000), then the corresponding word was chosen.", "labels": [], "entities": []}, {"text": "Human judgment was then used to check whether an extracted word was a corrector spurious word.", "labels": [], "entities": []}, {"text": "In the evaluation phase we found that the method performed differently on strings of different lengths.", "labels": [], "entities": []}, {"text": "Hence, we also checked the precision performance with specified word lengths.", "labels": [], "entities": [{"text": "precision", "start_pos": 27, "end_pos": 36, "type": "METRIC", "confidence": 0.9986802935600281}]}, {"text": "We set the threshold to three and obtained the data in.", "labels": [], "entities": []}, {"text": "Again we used the sampling method just described to test the overall precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 69, "end_pos": 78, "type": "METRIC", "confidence": 0.9986628293991089}]}, {"text": "From we can see that the method worked almost equally well on all word lengths except length three.", "labels": [], "entities": []}, {"text": "After checking the results, we found that three-character strings are often constructed from a two-character legitimiate word together with a single character.", "labels": [], "entities": []}, {"text": "It is difficult to judge with such a simple method whether such threecharacter strings are legitimate words.", "labels": [], "entities": []}, {"text": "Beyond precision, another concern is partial recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 7, "end_pos": 16, "type": "METRIC", "confidence": 0.9978799819946289}, {"text": "recall", "start_pos": 45, "end_pos": 51, "type": "METRIC", "confidence": 0.9873212575912476}]}, {"text": "In other words, how many words will be missed using such a method?", "labels": [], "entities": []}, {"text": "The corpus contained 55,788 sentences.", "labels": [], "entities": []}, {"text": "We checked only a small portion (a random sample of approximately 2,000 sentences) of the total corpus.", "labels": [], "entities": []}, {"text": "We used this sample to find meaningful words by hand.", "labels": [], "entities": []}, {"text": "The result of automatic extraction from the whole corpus was then compared with that of hand extraction of the sample sentences.", "labels": [], "entities": []}, {"text": "The partial recall was computed as the number of words in both sets divided by the number of words in the human extraction set.", "labels": [], "entities": [{"text": "recall", "start_pos": 12, "end_pos": 18, "type": "METRIC", "confidence": 0.9721687436103821}]}, {"text": "We list the experimental partial-recall values in.", "labels": [], "entities": []}, {"text": "We analyzed the instance with the threshold of two.", "labels": [], "entities": []}, {"text": "Some of the words were missed because they occurred only once, which was less than the threshold.", "labels": [], "entities": []}, {"text": "Some of the words were missed because they occurred only in very restricted environments.", "labels": [], "entities": []}, {"text": "This means that although they appeared more than once in the corpus, their accessor variety value was only one.", "labels": [], "entities": []}, {"text": "In the latter case, we could extract the strings that contained such strings as substrings.", "labels": [], "entities": []}, {"text": "The details are discussed in the section on error analysis.", "labels": [], "entities": [{"text": "error analysis", "start_pos": 44, "end_pos": 58, "type": "TASK", "confidence": 0.8150073289871216}]}, {"text": "The corpus that was used for these experiments was the TREC Chinese corpus (Linguistic Data Consortium, n.d.), which contains 160,000 articles, including articles that were published in the People's Daily from 1991 to 1993 and a portion of news released by the Xinhua News Agency in 1994 and 1995.", "labels": [], "entities": [{"text": "TREC Chinese corpus (Linguistic Data Consortium, n.d.)", "start_pos": 55, "end_pos": 109, "type": "DATASET", "confidence": 0.8931042194366455}, {"text": "People's Daily", "start_pos": 190, "end_pos": 204, "type": "DATASET", "confidence": 0.8514902393023173}, {"text": "Xinhua News Agency", "start_pos": 261, "end_pos": 279, "type": "DATASET", "confidence": 0.8007594347000122}]}, {"text": "In the experiment, we extracted words with lengths of two to seven characters.", "labels": [], "entities": []}, {"text": "The data contained approximately 7,000,000 sentences.", "labels": [], "entities": []}, {"text": "We first eliminated the non-Chinese characters.", "labels": [], "entities": []}, {"text": "All of the experiments that were carried out on the small corpus were also conducted on the large corpus.", "labels": [], "entities": []}, {"text": "In we first show some legitimate words that were extracted from the large corpus.", "labels": [], "entities": []}, {"text": "Notice that these words cannot be found in the word list that was extracted from the small corpus or in the system dictionary.", "labels": [], "entities": []}, {"text": "In, we show the overall precision performance.", "labels": [], "entities": [{"text": "precision", "start_pos": 24, "end_pos": 33, "type": "METRIC", "confidence": 0.9994860887527466}]}, {"text": "The performance trends that were observed in can be also observed here.", "labels": [], "entities": []}, {"text": "However, as this corpus is much larger than the previous one, many characters have the chance to occur together to form spurious words.", "labels": [], "entities": []}, {"text": "That is why the precision is much lower than that for the small corpus.", "labels": [], "entities": [{"text": "precision", "start_pos": 16, "end_pos": 25, "type": "METRIC", "confidence": 0.9996230602264404}]}, {"text": "Nevertheless, as the corpus is much larger now, a correct word can occur in many more environments than in the small corpus, which suggests that we can improve the precision by using a large threshold for the accessor variety value without overly degrading the partial recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 164, "end_pos": 173, "type": "METRIC", "confidence": 0.9991018772125244}, {"text": "recall", "start_pos": 269, "end_pos": 275, "type": "METRIC", "confidence": 0.8154770135879517}]}, {"text": "For example, when the threshold is set to nine, the precision is as high as 73.4% and the partial recall remains as high as 80.4%.", "labels": [], "entities": [{"text": "precision", "start_pos": 52, "end_pos": 61, "type": "METRIC", "confidence": 0.9996858835220337}, {"text": "recall", "start_pos": 98, "end_pos": 104, "type": "METRIC", "confidence": 0.9673864245414734}]}, {"text": "The precision and partial-recall performance in respect to the word length was also tested on the large corpus.", "labels": [], "entities": [{"text": "precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9994819760322571}]}, {"text": "The same sample method was used, and the results for thresholds three and nine are shown in, respectively.", "labels": [], "entities": []}, {"text": "Note that there is a great jump in the precision for word lengths two and three after we change the threshold from three to nine, but the partial recall does not change much.", "labels": [], "entities": [{"text": "precision", "start_pos": 39, "end_pos": 48, "type": "METRIC", "confidence": 0.9993854761123657}, {"text": "recall", "start_pos": 146, "end_pos": 152, "type": "METRIC", "confidence": 0.9662883281707764}]}, {"text": "For longer words, the method even performs well with threshold three.", "labels": [], "entities": []}, {"text": "The next experiment was intended to test the partial-recall performance for all of the words with lengths from two to seven.", "labels": [], "entities": []}, {"text": "The result is shown in, which indicates that the partial-recall value is satisfactory even with a large threshold.", "labels": [], "entities": []}, {"text": "This means that we can extract most of the words in the corpus.", "labels": [], "entities": []}, {"text": "In this section, we consider numeric-type compounds.", "labels": [], "entities": []}, {"text": "Some of the compounds of this type that were extracted from the large corpus are listed in.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2  Experiments on the threshold-precision relationship of the small corpus.", "labels": [], "entities": []}, {"text": " Table 3  Experiments on the word length-precision relationship of the small corpus.", "labels": [], "entities": []}, {"text": " Table 4  Experiments on the threshold-partial recall relationship of the small corpus.", "labels": [], "entities": [{"text": "recall", "start_pos": 47, "end_pos": 53, "type": "METRIC", "confidence": 0.868434488773346}]}, {"text": " Table 6  Experiments on the threshold-precision relationship of the large corpus.", "labels": [], "entities": []}, {"text": " Table 7  Experiments on the word length-precision relationship of the large corpus with threshold  three.", "labels": [], "entities": []}, {"text": " Table 8  Experiments on the word length-precision relationship of the large corpus with threshold nine.", "labels": [], "entities": []}, {"text": " Table 9  Experiments on the threshold-partial recall relationship of the large corpus.", "labels": [], "entities": [{"text": "recall", "start_pos": 47, "end_pos": 53, "type": "METRIC", "confidence": 0.8609898090362549}]}, {"text": " Table 11  Precision and partial recall of word lengths two to four of the first experiment on IT and AV.", "labels": [], "entities": [{"text": "recall", "start_pos": 33, "end_pos": 39, "type": "METRIC", "confidence": 0.8533681631088257}]}, {"text": " Table 12  Precision and partial recall of word lengths two to seven of the second experiment on IT and  AV.", "labels": [], "entities": [{"text": "recall", "start_pos": 33, "end_pos": 39, "type": "METRIC", "confidence": 0.8586500883102417}]}]}