{"title": [], "abstractContent": [{"text": "Corpus-based statistical parsing relies on using large quantities of annotated text as training examples.", "labels": [], "entities": [{"text": "Corpus-based statistical parsing", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.5981851021448771}]}, {"text": "Building this kind of resource is expensive and labor-intensive.", "labels": [], "entities": []}, {"text": "This work proposes to use sample selection to find helpful training examples and reduce human effort spent on annotating less informative ones.", "labels": [], "entities": []}, {"text": "We consider several criteria for predicting whether unlabeled data might be a helpful training example.", "labels": [], "entities": []}, {"text": "Experiments are performed across two syntactic learning tasks and within the single task of parsing across two learning models to compare the effect of different predictive criteria.", "labels": [], "entities": []}, {"text": "We find that sample selection can significantly reduce the size of annotated training corpora and that uncertainty is a robust predictive criterion that can be easily applied to different learning models.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "Based on the three classes of predictive criteria discussed in Section 2, we propose several evaluation functions for the Collins-Brooks model.", "labels": [], "entities": []}, {"text": "To determine the relative merits of the proposed evaluation functions, we compare the learning curve of training with sample selection according to each function against a baseline of random selection in an empirical study.", "labels": [], "entities": []}, {"text": "The corpus for this comparison is a collection of phrases extracted from the Wall Street Journal (WSJ) Treebank.", "labels": [], "entities": [{"text": "Wall Street Journal (WSJ) Treebank", "start_pos": 77, "end_pos": 111, "type": "DATASET", "confidence": 0.9205737369401115}]}, {"text": "We use Section 00 as the development set and Sections 2-23 as the training and test sets.", "labels": [], "entities": []}, {"text": "We perform 10-fold cross-validation to ensure the statistical significance of the results.", "labels": [], "entities": []}, {"text": "For each fold, the training candidate pool contains about 21,000 phrases, and the test set contained about 2,000 phrases.", "labels": [], "entities": []}, {"text": "As shown in, the learner generates an initial hypothesis based on a small set of training examples, L. These examples are randomly selected from the pool of unlabeled candidates and annotated by a human.", "labels": [], "entities": []}, {"text": "Random sampling ensures that the initial trained set reflects the distribution of the candidate pool and thus that the initial hypothesis is unbiased.", "labels": [], "entities": []}, {"text": "Starting with an unbiased hypothesis is important for those evaluation functions whose scoring metrics are affected by the accuracy of the hypothesis.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 123, "end_pos": 131, "type": "METRIC", "confidence": 0.9979972243309021}]}, {"text": "In these experiments, L initially contains 500 randomly selected examples.", "labels": [], "entities": []}, {"text": "In each selection iteration, all the candidates are scored by the evaluation function, and n examples with the highest TUVs are picked out from U to be labeled and added to L.", "labels": [], "entities": []}, {"text": "Ideally, we would like to haven = 1 for each iteration.", "labels": [], "entities": [{"text": "haven", "start_pos": 26, "end_pos": 31, "type": "METRIC", "confidence": 0.9760076403617859}]}, {"text": "In practice, however, it is often more convenient for the human annotator to label data in larger batches rather than one at a time.", "labels": [], "entities": []}, {"text": "In these experiments, we use a batch size of n = 500 examples.", "labels": [], "entities": []}, {"text": "We make note of one caveat to this kind of n-best batch selection.", "labels": [], "entities": []}, {"text": "Under a hypothesis-dependent evaluation function, identical examples will receive identical scores.", "labels": [], "entities": []}, {"text": "Because identical (or very similar) examples tend to address the same deficiency in the hypothesis, adding n very similar examples to the training set is unlikely to lead to big improvements in the hypothesis.", "labels": [], "entities": []}, {"text": "To diversify the examples in each batch, we simulate single-example selection (whenever possible) by reestimating the scores of the candidates after each selection.", "labels": [], "entities": []}, {"text": "Suppose we have just chosen to add candidate x to the batch.", "labels": [], "entities": []}, {"text": "Then, before selecting the next candidate, we estimate the potential decrease in scores of candidates similar to x once it belongs to the annotated training set.", "labels": [], "entities": []}, {"text": "The estimation is based entirely on the knowledge that x is chosen, but not on the classification of x.", "labels": [], "entities": []}, {"text": "Thus, only certain types of evaluation functions are amenable to the reestimation process.", "labels": [], "entities": []}, {"text": "For example, if scores have been assigned by f conf , then we know that the confidence intervals of the candidates similar to x must decrease slightly after learning x.", "labels": [], "entities": []}, {"text": "On the other hand, if scores have been assigned by f unc , then we cannot perceive any changes in the scores of similar candidates without knowing the true classification of x.", "labels": [], "entities": []}, {"text": "Similarly to scoring a PP candidate based on the novelty and frequencies of its characteristic tuples, we define an evaluation function, flex (w, G) that scores a sentence candidate, w, based on the novelty and frequencies of word pair co-occurrences: where w is the unlabeled sentence candidate, G is the current parsing model (which is ignored by problem-space-based evaluation functions), new(w i , w j ) is an indicator function that returns one if we have not yet selected any sentence in which w i and w j co-occurred, and coocc(w i , w j ) is a function that returns the number of times that w i cooccurs 8 with w j in the candidate pool.", "labels": [], "entities": []}, {"text": "We expect these evaluation functions to be less relevant for the parsing domain than for the PP-attachment domain for two reasons.", "labels": [], "entities": [{"text": "parsing domain", "start_pos": 65, "end_pos": 79, "type": "TASK", "confidence": 0.8917275071144104}]}, {"text": "First, because we do not have the actual parses, the extraction of lexical relationships is based on co-occurrence statistics, not syntactic relationships.", "labels": [], "entities": []}, {"text": "Second, because the distribution of words that form lexical relationships is wider and more uniform than that of words that form PP characteristic tuples, most word pairs will be novel and appear only once.", "labels": [], "entities": []}, {"text": "Another simple evaluation function based on the problem space is one that estimates the TUV of a candidate from its sentence length: The intuition behind this function is based on the general observation that longer sentences tend to have complex structures and introduce more opportunities for ambiguous parses.", "labels": [], "entities": [{"text": "TUV", "start_pos": 88, "end_pos": 91, "type": "METRIC", "confidence": 0.9916702508926392}]}, {"text": "Although these evaluation functions may seem simplistic, they have one major advantage: They are easy to compute and require little processing time.", "labels": [], "entities": []}, {"text": "Because inducing parsing models demands significantly more time than inducing PPattachment models, it becomes more important that the evaluation functions for parsing models be as efficient as possible.", "labels": [], "entities": [{"text": "parsing", "start_pos": 17, "end_pos": 24, "type": "TASK", "confidence": 0.9591839909553528}]}, {"text": "We compare the effectiveness of sample selection using the proposed evaluation functions against a baseline of random selection (f rand (w, G) = rand()).", "labels": [], "entities": [{"text": "sample selection", "start_pos": 32, "end_pos": 48, "type": "TASK", "confidence": 0.7031407207250595}]}, {"text": "Similarly to previous experimental designs, the learner is given a small set of annotated seed data from the WSJ Treebank and a large set of unlabeled data (also from the WSJ Treebank but with the labels removed) from which to select new training examples.", "labels": [], "entities": [{"text": "WSJ Treebank", "start_pos": 109, "end_pos": 121, "type": "DATASET", "confidence": 0.9921839535236359}, {"text": "WSJ Treebank", "start_pos": 171, "end_pos": 183, "type": "DATASET", "confidence": 0.9928390383720398}]}, {"text": "All training data are from Sections 2-21 of the treebank.", "labels": [], "entities": [{"text": "Sections 2-21 of the treebank", "start_pos": 27, "end_pos": 56, "type": "DATASET", "confidence": 0.7149693250656128}]}, {"text": "We monitor the learning progress of the parser by testing it on unseen test sentences.", "labels": [], "entities": []}, {"text": "We use Section 00 for development and Section 23 for testing.", "labels": [], "entities": []}, {"text": "This study is repeated for two different models, the PLTIG parser and Collins's Model 2 parser.", "labels": [], "entities": [{"text": "PLTIG", "start_pos": 53, "end_pos": 58, "type": "DATASET", "confidence": 0.7779006361961365}, {"text": "Collins's Model 2 parser", "start_pos": 70, "end_pos": 94, "type": "DATASET", "confidence": 0.9070660233497619}]}], "tableCaptions": []}