{"title": [{"text": "User Modeling by Using Bag-of-Behaviors for Building a Dialog System Sensitive to the Interlocutor's Internal State", "labels": [], "entities": []}], "abstractContent": [{"text": "When using spoken dialog systems in actual environments, users sometimes abandon the dialog without making any input utterance.", "labels": [], "entities": []}, {"text": "To help these users before they give up, the system should know why they could not make an utterance.", "labels": [], "entities": []}, {"text": "Thus, we have examined a method to estimate the state of a dialog user by capturing the user's non-verbal behavior even when the user's utterance is not observed.", "labels": [], "entities": []}, {"text": "The proposed method is based on vector quan-tization of multi-modal features such as non-verbal speech, feature points of the face, and gaze.", "labels": [], "entities": []}, {"text": "The histogram of the VQ code is used as a feature for determining the state.", "labels": [], "entities": [{"text": "VQ code", "start_pos": 21, "end_pos": 28, "type": "DATASET", "confidence": 0.9121899604797363}]}, {"text": "We call this feature \"the Bag-of-Behaviors.\"", "labels": [], "entities": []}, {"text": "According to the experimental results, we prove that the proposed method surpassed the results of conventional approaches and discriminated the target user's states with an accuracy of more than 70%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 173, "end_pos": 181, "type": "METRIC", "confidence": 0.9989660978317261}]}], "introductionContent": [{"text": "Spoken dialog systems have an advantage of being a natural interface since speech commands are less subject to the physical constraints imposed by devices.", "labels": [], "entities": []}, {"text": "On the other hand, if the system accepts only a limited expression, the user need to learn how to use the system.", "labels": [], "entities": []}, {"text": "If the user is not familiar with the system, he/she cannot even make an input utterance.", "labels": [], "entities": []}, {"text": "Not all users are motivated to converse with the system in actual environments, and sometimes a user will abandon the dialog without making any input utterance.", "labels": [], "entities": []}, {"text": "When the user has difficulty to make the utterance, conventional systems just repeat the prompt at fixed interval or taking the initiative in the dialog to complete the task.", "labels": [], "entities": []}, {"text": "However, we think that the system has to cope with the user's implicit requests to help the user more adequately.", "labels": [], "entities": []}, {"text": "To solve this problem,  proposed a method to estimate two \"user's states\" by capturing their non-verbal cues.", "labels": [], "entities": []}, {"text": "Here, the state A is when the user does not know what to input, and the state B is when the user is considering how to answer the system's prompt.", "labels": [], "entities": []}, {"text": "These states have not been distinguished by the conventional dialog systems so far, but should be handled differently.", "labels": [], "entities": []}, {"text": "The researchers of spoken dialog systems have focused on the various internal states of users such as emotion) and familiarity with the system () to build natural dialog system.", "labels": [], "entities": []}, {"text": "In particular, the user's \"uncertainty\" is assumed to be the nearest user's states that we wish to study.", "labels": [], "entities": []}, {"text": "Forbes-Riley and Litman (2011b) and Pon- introduced a framework for estimating the user's uncertainty to a tutor system.", "labels": [], "entities": []}, {"text": "The above-mentioned researches have a certain result by employing linguistic information for the estimation, but it remains difficult to assist a user who does not make any input utterance.", "labels": [], "entities": []}, {"text": "By contrast, the method by  estimated the target user's state by only using the user's non-verbal information.", "labels": [], "entities": []}, {"text": "In their work, the user's multi-modal behaviors were defined empirically, and the labels of the behaviors were annotated manually.", "labels": [], "entities": []}, {"text": "Based on this result, the present paper proposes the method that does not use manually-defined labels nor manual annotation.", "labels": [], "entities": []}, {"text": "The multi-modal behaviors are determined automatically using the vector quantization, and the frequency distribution of the VQ code is used for estimation of the user's state.", "labels": [], "entities": [{"text": "VQ code", "start_pos": 124, "end_pos": 131, "type": "DATASET", "confidence": 0.8521645069122314}]}, {"text": "Because this approach expects to construct clusters of the speech events or behaviors of the user, we called it as Bagof-Behaviors approach.", "labels": [], "entities": []}], "datasetContent": [{"text": "We employed the SVM with RBF-kernel as a classifier.", "labels": [], "entities": [{"text": "RBF-kernel", "start_pos": 25, "end_pos": 35, "type": "DATASET", "confidence": 0.8195798397064209}]}, {"text": "The experimental conditions are summarized in.", "labels": [], "entities": []}, {"text": "The hyperparameters of the classifier were decided by grid-searching.", "labels": [], "entities": []}, {"text": "Since the session of state C and the other states (state A and state B) were clearly distinguished by the duration of the session, we used only the session of state A and state B for the experiments.", "labels": [], "entities": []}, {"text": "Hence, each experiment was a two-class discrimination task.", "labels": [], "entities": []}, {"text": "As explained, the experimental data were unbalanced.", "labels": [], "entities": []}, {"text": "Since it is desirable that the system can discriminate the user's state without deviation, the harmonic mean H of the accuracy of the two states was used for measuring the performance.", "labels": [], "entities": [{"text": "harmonic mean H", "start_pos": 95, "end_pos": 110, "type": "METRIC", "confidence": 0.7582284013430277}, {"text": "accuracy", "start_pos": 118, "end_pos": 126, "type": "METRIC", "confidence": 0.9965506792068481}]}, {"text": "This is calculated by where CA and CB represent the discrimination accuracy of state A and state B, respectively.", "labels": [], "entities": [{"text": "CA", "start_pos": 28, "end_pos": 30, "type": "METRIC", "confidence": 0.9858125448226929}, {"text": "CB", "start_pos": 35, "end_pos": 37, "type": "METRIC", "confidence": 0.9326643347740173}, {"text": "accuracy", "start_pos": 67, "end_pos": 75, "type": "METRIC", "confidence": 0.9413486123085022}]}, {"text": "The experiments were conducted based on a 5-fold cross validation.", "labels": [], "entities": []}, {"text": "arranged in descending order  The results of condition (1) are shown in.", "labels": [], "entities": []}, {"text": "The figure shows the best H of each number of clusters.", "labels": [], "entities": []}, {"text": "In condition (1), the best result (H = 70.0%) was obtained when the number of clusters K was 64.", "labels": [], "entities": [{"text": "H", "start_pos": 35, "end_pos": 36, "type": "METRIC", "confidence": 0.9905458688735962}]}, {"text": "shows the results of condition (2).", "labels": [], "entities": []}, {"text": "In this figure, the results are shown in descending order of the harmonic mean for all combination of codebook size of the three codebooks (there were 5 3 = 125 conditions).", "labels": [], "entities": []}, {"text": "The best H = 70.7% was obtained when K a = 8, K f = 8 and K e = 64.", "labels": [], "entities": [{"text": "H", "start_pos": 9, "end_pos": 10, "type": "METRIC", "confidence": 0.9922089576721191}]}, {"text": "The best results of the tested methods are summarized in.", "labels": [], "entities": []}, {"text": "Here, \"Baseline + NN\" in the table denotes the result in, where the visual events and acoustic events were annotated manually, and the manual labels were used as input fora neural network for the classification.", "labels": [], "entities": []}, {"text": "The gaze feature was not used in \"Baseline + NN.\"", "labels": [], "entities": []}, {"text": "We added the result when including the gaze feature, shown as \"Baseline + Gaze + NN.\"", "labels": [], "entities": []}, {"text": "As shown in, the performance of the method proposed in this paper surpassed the baseline methods.", "labels": [], "entities": []}, {"text": "Therefore, the proposed method could not only automatically determine the inventory of the audio-visual events, but also achieved better discrimination accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 152, "end_pos": 160, "type": "METRIC", "confidence": 0.9865517616271973}]}, {"text": "One of the reasons of the improvement is VQ can construct the clusters in proper quantities.", "labels": [], "entities": []}, {"text": "Comparing the two conditions of feature combination, H of condition (2) (denoted as \"Condition (2) + RBF-SVM\") was slightly higher than that of condition (1) (denoted as \"Condition (1) + RBF-SVM\").", "labels": [], "entities": [{"text": "H", "start_pos": 53, "end_pos": 54, "type": "METRIC", "confidence": 0.9616286754608154}]}, {"text": "This result was similar to Split-VQ where a single feature vector split into subvectors and the input vector was quantized subvector by subvector.", "labels": [], "entities": []}, {"text": "We conducted additional experiments for condition (2) by using SVM with combined kernel trained by Multiple Kernel Learning (MKL)).", "labels": [], "entities": []}, {"text": "The combined kernel is represented as a linear combination of several subkernels.", "labels": [], "entities": []}, {"text": "The distinct kernel was employed for the speech, face feature and gaze feature, respectively.", "labels": [], "entities": []}, {"text": "This paper used the RBF-kernel having the same width as the sub-kernelsThe best result was shown as \"Condition (2) + MKL-SVM\" in.", "labels": [], "entities": [{"text": "RBF-kernel", "start_pos": 20, "end_pos": 30, "type": "DATASET", "confidence": 0.6460360288619995}]}, {"text": "As shown in the table, the MKL-SVM showed the highest performance of 72.0 %.", "labels": [], "entities": [{"text": "MKL-SVM", "start_pos": 27, "end_pos": 34, "type": "DATASET", "confidence": 0.797014594078064}]}, {"text": "The weights of the audio, face and gaze feature were 0.246, 0.005 and 0.749, respectively.", "labels": [], "entities": []}, {"text": "This result suggested that the contribution of the face feature was weaker than the other features.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Comparison of estimation methods", "labels": [], "entities": []}]}