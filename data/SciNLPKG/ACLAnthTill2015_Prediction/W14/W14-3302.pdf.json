{"title": [{"text": "Findings of the 2014 Workshop on Statistical Machine Translation", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 33, "end_pos": 64, "type": "TASK", "confidence": 0.7445093790690104}]}], "abstractContent": [{"text": "This paper presents the results of the WMT14 shared tasks, which included a standard news translation task, a separate medical translation task, a task for run-time estimation of machine translation quality, and a metrics task.", "labels": [], "entities": [{"text": "WMT14 shared tasks", "start_pos": 39, "end_pos": 57, "type": "TASK", "confidence": 0.5269104739030203}, {"text": "news translation task", "start_pos": 85, "end_pos": 106, "type": "TASK", "confidence": 0.7425005833307902}, {"text": "medical translation task", "start_pos": 119, "end_pos": 143, "type": "TASK", "confidence": 0.7989195585250854}, {"text": "run-time estimation of machine translation", "start_pos": 156, "end_pos": 198, "type": "TASK", "confidence": 0.6176942646503448}]}, {"text": "This year, 143 machine translation systems from 23 institutions were submitted to the ten translation directions in the standard translation task.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 15, "end_pos": 34, "type": "TASK", "confidence": 0.7249639928340912}, {"text": "standard translation task", "start_pos": 120, "end_pos": 145, "type": "TASK", "confidence": 0.7021696666876475}]}, {"text": "An additional 6 anonymized systems were included, and were then evaluated both automatically and manually.", "labels": [], "entities": []}, {"text": "The quality estimation task had four subtasks, with a total of 10 teams, submitting 57 entries .", "labels": [], "entities": [{"text": "quality estimation task", "start_pos": 4, "end_pos": 27, "type": "TASK", "confidence": 0.7175286710262299}]}], "introductionContent": [{"text": "We present the results of the shared tasks of the Workshop on Statistical Machine Translation (WMT) held at ACL 2014.", "labels": [], "entities": [{"text": "Statistical Machine Translation (WMT) held at ACL 2014", "start_pos": 62, "end_pos": 116, "type": "TASK", "confidence": 0.8015249669551849}]}, {"text": "This workshop builds on eight previous WMT workshops (.", "labels": [], "entities": [{"text": "WMT", "start_pos": 39, "end_pos": 42, "type": "TASK", "confidence": 0.6438277959823608}]}, {"text": "This year we conducted four official tasks: a translation task, a quality estimation task, a metrics task 1 and a medical translation task.", "labels": [], "entities": [{"text": "translation task", "start_pos": 46, "end_pos": 62, "type": "TASK", "confidence": 0.9272766411304474}, {"text": "medical translation task", "start_pos": 114, "end_pos": 138, "type": "TASK", "confidence": 0.7698718309402466}]}, {"text": "In the translation task ( \u00a72), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data.", "labels": [], "entities": [{"text": "translation task", "start_pos": 7, "end_pos": 23, "type": "TASK", "confidence": 0.9291570782661438}]}, {"text": "We held ten translation tasks this year, between English and each of Czech, French, German, Hindi, and Russian.", "labels": [], "entities": [{"text": "translation tasks", "start_pos": 12, "end_pos": 29, "type": "TASK", "confidence": 0.8909687697887421}]}, {"text": "The Hindi translation tasks were new this year, providing a lesser resourced data condition on a challenging language pair.", "labels": [], "entities": [{"text": "Hindi translation", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.8123091459274292}]}, {"text": "The system outputs for each task were evaluated both automatically and manually.", "labels": [], "entities": []}, {"text": "The human evaluation ( \u00a73) involves asking human judges to rank sentences output by anonymized systems.", "labels": [], "entities": []}, {"text": "We obtained large numbers of rankings from researchers who contributed evaluations proportional to the number of tasks they entered.", "labels": [], "entities": []}, {"text": "Last year, we dramatically increased the number of judgments, achieving much more meaningful rankings.", "labels": [], "entities": []}, {"text": "This year, we developed anew ranking method that allows us to achieve the same with fewer judgments.", "labels": [], "entities": []}, {"text": "The quality estimation task ( \u00a74) this year included sentence-and word-level subtasks: sentence-level prediction of 1-3 likert scores, sentence-level prediction of percentage of word edits necessary to fix a sentence, sentence-level prediction of post-editing time, and word-level prediction of scores at different levels of granularity (correct/incorrect, accuracy/fluency errors, and specific types of errors).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 357, "end_pos": 365, "type": "METRIC", "confidence": 0.9954213500022888}]}, {"text": "Datasets were released with English-Spanish, English-German, SpanishEnglish and German-English news translations produced by 2-3 machine translation systems and, for some subtasks, a human translation.", "labels": [], "entities": []}, {"text": "The medical translation task ( \u00a75) was introduced this year.", "labels": [], "entities": [{"text": "medical translation task", "start_pos": 4, "end_pos": 28, "type": "TASK", "confidence": 0.8893763025601705}]}, {"text": "Unlike the \"standard\" translation task, the test sets come from the very specialized domain of medical texts.", "labels": [], "entities": []}, {"text": "The aim of this task was not only domain adaptation but also the utilization of translation systems in a larger scenario, namely cross-lingual information retrieval (IR).", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 34, "end_pos": 51, "type": "TASK", "confidence": 0.7572484016418457}, {"text": "cross-lingual information retrieval (IR)", "start_pos": 129, "end_pos": 169, "type": "TASK", "confidence": 0.7798633476098379}]}, {"text": "Extrinsic evaluation in an IR setting was apart of this task (on the other hand, manual evaluation of translation quality was not carried out).", "labels": [], "entities": [{"text": "IR setting", "start_pos": 27, "end_pos": 37, "type": "TASK", "confidence": 0.9146565496921539}]}, {"text": "The primary objectives of WMT are to evaluate the state of the art in machine translation, to disseminate common test sets and public training data with published performance numbers, and to refine evaluation and estimation methodologies for machine translation.", "labels": [], "entities": [{"text": "WMT", "start_pos": 26, "end_pos": 29, "type": "TASK", "confidence": 0.8719257116317749}, {"text": "machine translation", "start_pos": 70, "end_pos": 89, "type": "TASK", "confidence": 0.8308209776878357}, {"text": "machine translation", "start_pos": 242, "end_pos": 261, "type": "TASK", "confidence": 0.8069436848163605}]}, {"text": "As before, all of the data, translations, and collected human judgments are publicly available.", "labels": [], "entities": []}, {"text": "We hope these datasets serve as a valuable resource for research into statistical machine translation and automatic evaluation or prediction of translation quality.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 70, "end_pos": 101, "type": "TASK", "confidence": 0.6579025685787201}]}], "datasetContent": [{"text": "As with past workshops, we contend that automatic measures of machine translation quality are an imperfect substitute for human assessments.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 62, "end_pos": 81, "type": "TASK", "confidence": 0.7519227862358093}]}, {"text": "We therefore conduct a manual evaluation of the system outputs and define its results to be the principal ranking of the workshop.", "labels": [], "entities": []}, {"text": "In this section, we describe how we collected this data and compute the results, and then present the official results of the ranking.", "labels": [], "entities": []}, {"text": "This year's evaluation was conducted a bit differently.", "labels": [], "entities": []}, {"text": "The main differences are: \u2022 In contrast to the past two years, we collected judgments entirely from researchers participating in the shared tasks and trusted friends of the community.", "labels": [], "entities": []}, {"text": "Last year, about two thirds of the data were solicited from random volunteers on the Amazon Mechanical Turk.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 85, "end_pos": 107, "type": "DATASET", "confidence": 0.9719003438949585}]}, {"text": "For some language pairs, the Turkers data had much lower inter-annotator agreement compared to the researchers.", "labels": [], "entities": [{"text": "Turkers data", "start_pos": 29, "end_pos": 41, "type": "DATASET", "confidence": 0.9321736097335815}]}, {"text": "\u2022 As a result, we collected about seventy-five percent less data, but were able to obtain good confidence intervals on the clusters with the use of new approaches to ranking.", "labels": [], "entities": []}, {"text": "\u2022 We compared three different ranking methodologies, selecting the one with the highest accuracy on held-out data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 88, "end_pos": 96, "type": "METRIC", "confidence": 0.998845100402832}]}, {"text": "We also maintain many of our customs from prior years, including the presentation of the results in terms of a partial ordering (clustering) of the systems.", "labels": [], "entities": []}, {"text": "Systems in the same cluster could not be meaningfully distinguished and should be considered ties.", "labels": [], "entities": []}, {"text": "This year's shared task made available datasets for more than one language pair with the same or different types of annotation, 2-3 multiple MT systems (plus a human translation) per language pair, and out-of-domain test data (Tasks 1.1 and 2).", "labels": [], "entities": []}, {"text": "Instances for each language pair were kept in separate datasets and thus the \"language pair\" variable can be analysed independently.", "labels": [], "entities": []}, {"text": "However, fora given language pair, datasets mix translation systems (and humans) in Task 1.1, and also text domains in Task 2.", "labels": [], "entities": []}, {"text": "Directly comparing the performance across language pairs is not possible, given that their datasets have different numbers of instances (produced by 3 or 4 systems) and/or different true score distributions (see).", "labels": [], "entities": []}, {"text": "For a relative comparison (although not all systems submitted results for all language pairs, which is especially true in Task 2), we observe in Task 1.1 that for all language pairs generally at least half of the systems did better than the baseline.", "labels": [], "entities": []}, {"text": "To our surprise, only one submission combined data for multiple languages together for Task 1.1: SHEF-lite, treating each language pair data as a different task in a multi-task learning setting.", "labels": [], "entities": [{"text": "SHEF-lite", "start_pos": 97, "end_pos": 106, "type": "DATASET", "confidence": 0.6166053414344788}]}, {"text": "However, only for the 'sparse' variant of the submission significant gains were reported over modelling each task independently (with the tasks still sharing the same data kernel and the same hyperparameters).", "labels": [], "entities": []}, {"text": "The interpretation of the results for Task 2 is very dependent on the evaluation metric used, but generally speaking a large variation in performance was found between different languages, with English-Spanish performing the best, possibly given the much larger number of training instances.", "labels": [], "entities": []}, {"text": "Data for Task 2 also presented varied true score distributions (as shown by the performance of the baseline (e.g. always \"OK\") in One of the main goals with Task 1.1 (and Task 2 to some extent) was to test the robustness of models in a blind setting where multiple MT systems (and human translations) are put together and their identifiers are now known.", "labels": [], "entities": []}, {"text": "All submissions for these tasks were therefore translation system agnostic, with no submission attempting to perform meta-identification of the origins of the translations.", "labels": [], "entities": []}, {"text": "For Task 1.1, data from multiple MT systems was explicitly used by USHEFF though the idea of consensus translations.", "labels": [], "entities": [{"text": "MT", "start_pos": 33, "end_pos": 35, "type": "TASK", "confidence": 0.9531904458999634}, {"text": "USHEFF", "start_pos": 67, "end_pos": 73, "type": "DATASET", "confidence": 0.9504480361938477}, {"text": "consensus translations", "start_pos": 93, "end_pos": 115, "type": "TASK", "confidence": 0.7146736085414886}]}, {"text": "Translations from all but the system of interest for the same source segment were used as pseudo-references.", "labels": [], "entities": []}, {"text": "The submission significantly outperformed the baseline for all language pairs and did particularly well for Spanish-English and English-Spanish.", "labels": [], "entities": []}, {"text": "For Tasks 1.2 and 1.3, two submissions included English-Spanish data which had been produced by yet different MT systems (SHEF-lite and DFKI).", "labels": [], "entities": []}, {"text": "While using these additional instances seemed attractive given the small number of instances available for these tasks, it is not clear what their contribution was.", "labels": [], "entities": []}, {"text": "For example, with a reduced set of instances (only 400) from the combined sets, SHEFlite/sparse performed significantly better than its variant SHEF-lite.", "labels": [], "entities": []}, {"text": "Finally, with respect to out-of-domain (different text domain and MT system) test data, for Task 1.1, none of the papers submitted included experiments.", "labels": [], "entities": [{"text": "MT system) test data", "start_pos": 66, "end_pos": 86, "type": "DATASET", "confidence": 0.7056924700737}]}, {"text": ") applied the models trained on pooled datasets (as explained above) for each language pair to the out-of-domain test sets.", "labels": [], "entities": []}, {"text": "The results were surprisingly positive, with average MAE score of 0.5, compared to the 0.5-0.65 range for in-domain data (see above).", "labels": [], "entities": [{"text": "MAE score", "start_pos": 53, "end_pos": 62, "type": "METRIC", "confidence": 0.9866235256195068}]}, {"text": "Further analysis is necessary to understand the reasons for that.", "labels": [], "entities": []}, {"text": "In Task 2, the official training and test sets already include out-of-domain data because of the very small amount of in-domain data available, and thus is is hard to isolate the effect of this data on the results.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Amount of data collected in the WMT14 manual evaluation. The final three rows report summary information from  the previous two workshops.", "labels": [], "entities": [{"text": "Amount", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9571298956871033}, {"text": "WMT14 manual evaluation", "start_pos": 42, "end_pos": 65, "type": "DATASET", "confidence": 0.7902825276056925}]}, {"text": " Table 4: \u03ba scores measuring inter-annotator agreement. See Table 5 for corresponding intra-annotator agreement scores.", "labels": [], "entities": []}, {"text": " Table 5: \u03ba scores measuring intra-annotator agreement, i.e., self-consistency of judges, across for the past few years of the  human evaluation.", "labels": [], "entities": []}, {"text": " Table 6: The partial ordering computed with the provided  scores when r = 0.15.", "labels": [], "entities": []}, {"text": " Table 7: Accuracies for each method across 100 folds, for  each translation task. The oracle uses the most frequent out- come between each pair of systems, and therefore might not  constitute a feasible ranking.", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9918045997619629}]}, {"text": " Table 8: Official results for the WMT14 translation task. Systems are ordered by their inferred system means. Lines between  systems indicate clusters according to bootstrap resampling at p-level p \u2264 .05, except for English-German, where p \u2264 0.1.  This method is also used to determine the range of ranks into which system falls. Systems with grey background indicate use  of resources that fall outside the constraints provided for the shared task.", "labels": [], "entities": [{"text": "WMT14 translation task", "start_pos": 35, "end_pos": 57, "type": "TASK", "confidence": 0.8010711272557577}]}, {"text": " Table 11: Official results for the ranking variant of the WMT14 Quality Evaluation Task 1.1. The winning submissions  are indicated by a \u2022. These are the top-scoring submission and those that are not significantly worse according to bootstrap  resampling (1M times) with 95% confidence intervals. The systems in the gray area are not different from the baseline system  at a statistically significant level according to the same test.", "labels": [], "entities": [{"text": "WMT14 Quality Evaluation Task 1.1", "start_pos": 59, "end_pos": 92, "type": "DATASET", "confidence": 0.6121833682060241}]}, {"text": " Table 12: Official results for the scoring variant of the WMT14 Quality Evaluation Task 1.1. The winning submissions  are indicated by a \u2022. These are the top-scoring submission and those that are not significantly worse according to bootstrap  resampling (1M times) with 95% confidence intervals. The systems in the gray area are not different from the baseline system  at a statistically significant level according to the same test.", "labels": [], "entities": [{"text": "WMT14 Quality Evaluation Task 1.1", "start_pos": 59, "end_pos": 92, "type": "DATASET", "confidence": 0.6106175661087037}]}, {"text": " Table 13: Official results for the ranking variant of the WMT14 Quality Evaluation Task 1.2. The winning submissions  are indicated by a \u2022. These are the top-scoring submission and those that are not significantly worse according to bootstrap  resampling (100k times) with 95% confidence intervals. The systems in the gray area are not different from the baseline system  at a statistically significant level according to the same test.", "labels": [], "entities": [{"text": "WMT14 Quality Evaluation Task 1.2", "start_pos": 59, "end_pos": 92, "type": "DATASET", "confidence": 0.6114652872085571}]}, {"text": " Table 14: Official results for the scoring variant of the WMT14 Quality Evaluation Task 1.2. The winning submissions  are indicated by a \u2022. They are statistically indistinguishable from the top submission according to bootstrap resampling (1M  times) with 95% confidence intervals. The systems in the gray area are not different from the baseline system at a statistically  significant level according to the same test.", "labels": [], "entities": [{"text": "WMT14 Quality Evaluation Task 1.2", "start_pos": 59, "end_pos": 92, "type": "DATASET", "confidence": 0.6036283791065216}]}, {"text": " Table 15: Official results for the ranking variant of the WMT14 Quality Evaluation Task 1.3. The winning submissions  are indicated by a \u2022. They are statistically indistinguishable from the top submission according to bootstrap resampling (1M  times) with a 95% confidence interval. The systems in the gray area are not different from the baseline system at a statistically  significant level according to the same test.", "labels": [], "entities": [{"text": "WMT14 Quality Evaluation Task 1.3", "start_pos": 59, "end_pos": 92, "type": "DATASET", "confidence": 0.6449422359466552}]}, {"text": " Table 16: Official results for the scoring variant of the WMT14 Quality Evaluation Task 1.3. The winning submissions  are indicated by a \u2022. They are statistically indistinguishable from the top submission according to bootstrap resampling (1M  times) with a 95% confidence interval. The systems in the gray area are not different from the baseline system at a statistically  significant level according to the same test.", "labels": [], "entities": [{"text": "WMT14 Quality Evaluation Task 1.3", "start_pos": 59, "end_pos": 92, "type": "DATASET", "confidence": 0.6262480676174164}]}, {"text": " Table 17: Official results for the binary part of the WMT14 Quality Evaluation Task 2. The winning submissions are indicated  by a \u2022. All values are given as percentages.", "labels": [], "entities": [{"text": "WMT14 Quality Evaluation Task 2", "start_pos": 55, "end_pos": 86, "type": "DATASET", "confidence": 0.6197110176086426}]}, {"text": " Table 18: Official results for the Level 1 classification part of the WMT14 Quality Evaluation Task 2. The winning submissions  are indicated by a \u2022. All values are given as percentages.", "labels": [], "entities": [{"text": "WMT14 Quality Evaluation Task 2", "start_pos": 71, "end_pos": 102, "type": "DATASET", "confidence": 0.7743568897247315}]}, {"text": " Table 19: Official results for the Multi-class classification part of the WMT14 Quality Evaluation Task 2. The winning  submissions are indicated by a \u2022. All values are given as percentages.", "labels": [], "entities": [{"text": "WMT14 Quality Evaluation Task 2", "start_pos": 75, "end_pos": 106, "type": "DATASET", "confidence": 0.7673795938491821}]}, {"text": " Table 20: Statistics of summary test data.", "labels": [], "entities": []}, {"text": " Table 21: Statistics of query test data.", "labels": [], "entities": []}, {"text": " Table 22: Statistics of the in-domain parallel training data allowed for the constrained task (in thousands).", "labels": [], "entities": []}, {"text": " Table 23: Sizes of monolingual training data allowed for the  constrained tasks (in thousands of tokens).", "labels": [], "entities": []}, {"text": " Table 25: Official results of translation quality evaluation in the medical summary translation subtask.", "labels": [], "entities": [{"text": "translation", "start_pos": 31, "end_pos": 42, "type": "TASK", "confidence": 0.9551793932914734}, {"text": "medical summary translation subtask", "start_pos": 69, "end_pos": 104, "type": "TASK", "confidence": 0.6357876434922218}]}, {"text": " Table 26: Official results of translation quality evaluation in the medical query translation subtask.", "labels": [], "entities": [{"text": "medical query translation subtask", "start_pos": 69, "end_pos": 102, "type": "TASK", "confidence": 0.6273886039853096}]}, {"text": " Table 27: Official results of retrieval evaluation in the query translation subtask.", "labels": [], "entities": []}, {"text": " Table 8. Gray lines separate clusters based on non-overlapping rank ranges.", "labels": [], "entities": []}, {"text": " Table 29: Head to head comparison, ignoring ties, for English-Czech systems", "labels": [], "entities": []}, {"text": " Table 30: Head to head comparison, ignoring ties, for German-English systems", "labels": [], "entities": []}, {"text": " Table 31: Head to head comparison, ignoring ties, for English-German systems", "labels": [], "entities": []}]}