{"title": [{"text": "Vector Space Models for Phrase-based Machine Translation", "labels": [], "entities": [{"text": "Phrase-based Machine Translation", "start_pos": 24, "end_pos": 56, "type": "TASK", "confidence": 0.880362848440806}]}], "abstractContent": [{"text": "This paper investigates the application of vector space models (VSMs) to the standard phrase-based machine translation pipeline.", "labels": [], "entities": [{"text": "phrase-based machine translation pipeline", "start_pos": 86, "end_pos": 127, "type": "TASK", "confidence": 0.6991352885961533}]}, {"text": "VSMs are models based on continuous word representations embedded in a vector space.", "labels": [], "entities": []}, {"text": "We exploit word vectors to augment the phrase table with new inferred phrase pairs.", "labels": [], "entities": []}, {"text": "This helps reduce out-of-vocabulary (OOV) words.", "labels": [], "entities": []}, {"text": "In addition, we present a simple way to learn bilingually-constrained phrase vectors.", "labels": [], "entities": []}, {"text": "The phrase vectors are then used to provide additional scoring of phrase pairs, which fits into the standard log-linear framework of phrase-based statistical machine translation.", "labels": [], "entities": [{"text": "phrase-based statistical machine translation", "start_pos": 133, "end_pos": 177, "type": "TASK", "confidence": 0.5971337333321571}]}, {"text": "Both methods result in significant improvements over a competitive in-domain baseline applied to the Arabic-to-English task of IWSLT 2013.", "labels": [], "entities": [{"text": "IWSLT 2013", "start_pos": 127, "end_pos": 137, "type": "DATASET", "confidence": 0.7930189669132233}]}], "introductionContent": [{"text": "Categorical word representation has been widely used in many natural language processing (NLP) applications including statistical machine translation (SMT), where words are treated as discrete random variables.", "labels": [], "entities": [{"text": "Categorical word representation", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.7499136527379354}, {"text": "statistical machine translation (SMT)", "start_pos": 118, "end_pos": 155, "type": "TASK", "confidence": 0.8045667012532552}]}, {"text": "Continuous word representations, on the other hand, have been applied successfully in many NLP areas).", "labels": [], "entities": []}, {"text": "However, their application to machine translation is still an open research question.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 30, "end_pos": 49, "type": "TASK", "confidence": 0.8153812289237976}]}, {"text": "Several works tried to address the question recently (, and this work is but another step in that direction.", "labels": [], "entities": []}, {"text": "While categorical representations do not encode any information about word identities, continuous representations embed words in a vector space, resulting in geometric arrangements that reflect information about the represented words.", "labels": [], "entities": []}, {"text": "Such embeddings open the potential for applying information retrieval approaches where it becomes possible to define and compute similarity between different words.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 48, "end_pos": 69, "type": "TASK", "confidence": 0.7647722363471985}]}, {"text": "We focus on continuous representations whose training is influenced by the surrounding context of the token being represented.", "labels": [], "entities": []}, {"text": "One motivation for such representations is to capture word semantics (.", "labels": [], "entities": []}, {"text": "This is based on the distributional hypothesis which says that words that occur in similar contexts tend to have similar meanings.", "labels": [], "entities": []}, {"text": "We make use of continuous vectors learned using simple neural networks.", "labels": [], "entities": []}, {"text": "Neural networks have been gaining increasing attention recently, where they have been able to enhance strong SMT baselines).", "labels": [], "entities": [{"text": "SMT", "start_pos": 109, "end_pos": 112, "type": "TASK", "confidence": 0.9881207942962646}]}, {"text": "While neural language and translation modeling make intermediate use of continuous representations, there have been also attempts at explicit learning of continuous representations to improve translation (.", "labels": [], "entities": [{"text": "translation modeling", "start_pos": 26, "end_pos": 46, "type": "TASK", "confidence": 0.9671871066093445}]}, {"text": "This work explores the potential of word semantics based on continuous vector representations to enhance the performance of phrase-based machine translation.", "labels": [], "entities": [{"text": "phrase-based machine translation", "start_pos": 124, "end_pos": 156, "type": "TASK", "confidence": 0.6359260479609171}]}, {"text": "We present a greedy algorithm that employs the phrase table to identify phrases in a training corpus.", "labels": [], "entities": []}, {"text": "The phrase table serves to bilingually restrict the phrases spotted in the monolingual corpus.", "labels": [], "entities": []}, {"text": "The algorithm is applied separately to the source and target sides of the training data, resulting in source and target corpora of phrases (instead of words).", "labels": [], "entities": []}, {"text": "The phrase corpus is used to learn phrase vectors using the same methods that produce word vectors.", "labels": [], "entities": []}, {"text": "The vectors are then used to provide semantic scoring of phrase pairs.", "labels": [], "entities": [{"text": "semantic scoring of phrase pairs", "start_pos": 37, "end_pos": 69, "type": "TASK", "confidence": 0.7673742055892945}]}, {"text": "We also learn word vectors and employ them to augment the phrase table with paraphrased entries.", "labels": [], "entities": []}, {"text": "This leads to a reduction in 1 the OOV rate which translates to improved BLEU and and TER scores.", "labels": [], "entities": [{"text": "OOV rate", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9888814985752106}, {"text": "BLEU", "start_pos": 73, "end_pos": 77, "type": "METRIC", "confidence": 0.9996210336685181}, {"text": "TER scores", "start_pos": 86, "end_pos": 96, "type": "METRIC", "confidence": 0.9785991907119751}]}, {"text": "We apply the two methods on the IWSLT 2013 Arabic-to-English task and show significant improvements over a strong in-domain baseline.", "labels": [], "entities": [{"text": "IWSLT 2013 Arabic-to-English task", "start_pos": 32, "end_pos": 65, "type": "DATASET", "confidence": 0.8850933015346527}]}, {"text": "The rest of the paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 presents a background on word and phrase vectors.", "labels": [], "entities": [{"text": "word and phrase vectors", "start_pos": 35, "end_pos": 58, "type": "TASK", "confidence": 0.5980323851108551}]}, {"text": "The construction of the phrase corpus is discussed in Section 3, while Section 4 demonstrates how to use word and phrase vectors in the standard phrase-based SMT pipeline.", "labels": [], "entities": [{"text": "SMT pipeline", "start_pos": 158, "end_pos": 170, "type": "TASK", "confidence": 0.9023275375366211}]}, {"text": "Experiments are presented in Section 5, followed by an overview of the related word in Section 6, and finally Section 7 concludes the work.", "labels": [], "entities": []}], "datasetContent": [{"text": "In the following we first provide an analysis of the word vectors that are later used for translation experiments.", "labels": [], "entities": [{"text": "translation", "start_pos": 90, "end_pos": 101, "type": "TASK", "confidence": 0.9685672521591187}]}, {"text": "We use word vectors (as opposed to phrase vectors) for phrase table paraphrasing to reduce the OOV rate.", "labels": [], "entities": [{"text": "phrase table paraphrasing", "start_pos": 55, "end_pos": 80, "type": "TASK", "confidence": 0.7275541226069132}, {"text": "OOV rate", "start_pos": 95, "end_pos": 103, "type": "METRIC", "confidence": 0.9813362658023834}]}, {"text": "Next, we present end-toend translation results using the proposed semantic feature and our OOV reduction method.", "labels": [], "entities": [{"text": "OOV reduction", "start_pos": 91, "end_pos": 104, "type": "TASK", "confidence": 0.6034961044788361}]}, {"text": "The experiments are based on vectors trained using the word2vec 1 toolkit, setting vector dimensionality to 800 for Arabic and 200 for English vectors.", "labels": [], "entities": []}, {"text": "We used the skip-gram model with a maximum skip length of 10.", "labels": [], "entities": []}, {"text": "The phrase corpus was constructed using 5 passes, with scores computed according to Eq.", "labels": [], "entities": [{"text": "Eq", "start_pos": 84, "end_pos": 86, "type": "DATASET", "confidence": 0.6903196573257446}]}, {"text": "1 using 2 phrasal and 2 lexical features.", "labels": [], "entities": []}, {"text": "The phrasal and lexical weights were set to 1 and 0.5 respectively, with all features being negative log-probabilities, and the scoring threshold \u03b8 was set to 10.", "labels": [], "entities": [{"text": "scoring threshold \u03b8", "start_pos": 128, "end_pos": 147, "type": "METRIC", "confidence": 0.8246588508288065}]}, {"text": "All translation experiments are performed with the Jane toolkit ().", "labels": [], "entities": [{"text": "translation", "start_pos": 4, "end_pos": 15, "type": "TASK", "confidence": 0.9774283766746521}, {"text": "Jane toolkit", "start_pos": 51, "end_pos": 63, "type": "DATASET", "confidence": 0.9808251559734344}]}], "tableCaptions": [{"text": " Table 1: Arabic and English corpora statistics.", "labels": [], "entities": []}, {"text": " Table 4: Semantic feature and paraphrasing re- sults. The symbol  \u2021 indicates statistical signifi- cance with p < 0.01.", "labels": [], "entities": []}, {"text": " Table 5: OOV change due to paraphrasing. Vocab- ulary refers to the number of unique tokens in the  Arabic dev and test sets.", "labels": [], "entities": [{"text": "OOV", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9787014722824097}]}]}