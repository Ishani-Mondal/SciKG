{"title": [], "abstractContent": [{"text": "NLP definitions of Terminology are usually application-dependent.", "labels": [], "entities": [{"text": "Terminology", "start_pos": 19, "end_pos": 30, "type": "TASK", "confidence": 0.9226928949356079}]}, {"text": "IR terms are noun sequences that characterize topics.", "labels": [], "entities": [{"text": "IR terms", "start_pos": 0, "end_pos": 8, "type": "TASK", "confidence": 0.8546444177627563}]}, {"text": "Terms can also be arguments for relations like abbreviation, definition or IS-A.", "labels": [], "entities": []}, {"text": "In contrast, this paper explores techniques for extracting terms fitting a broader definition: noun sequences specific to topics and not well-known to naive adults.", "labels": [], "entities": []}, {"text": "We describe a chunking-based approach, an evaluation, and applications to non-topic-specific relation extraction.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 93, "end_pos": 112, "type": "TASK", "confidence": 0.7213660925626755}]}], "introductionContent": [{"text": "Webster's II New College Dictionary, p.1138) defines terminology as: The vocabulary of technical terms and usages appropriate to a particular field, subject, science, or art.", "labels": [], "entities": [{"text": "Webster's II New College Dictionary", "start_pos": 0, "end_pos": 35, "type": "DATASET", "confidence": 0.8540456295013428}]}, {"text": "Systems for automatically extracting instances of terminology (terms) usually assume narrow operational definitions that are compatible with particular tasks.", "labels": [], "entities": [{"text": "automatically extracting instances of terminology (terms)", "start_pos": 12, "end_pos": 69, "type": "TASK", "confidence": 0.797679428011179}]}, {"text": "Terminology, in the context of Information Retrieval (IR)) refers to keyword search terms (microarray, potato, genetic algorithm), single or multi-word (mostly nominal) expressions collectively representing topics of documents that contain them.", "labels": [], "entities": [{"text": "Information Retrieval (IR)) refers to keyword search terms (microarray, potato, genetic algorithm), single or multi-word (mostly nominal) expressions collectively representing topics of documents that contain them", "start_pos": 31, "end_pos": 244, "type": "Description", "confidence": 0.7637020596436092}]}, {"text": "These same terms are also used for creating domain-specific thesauri and ontologies ().", "labels": [], "entities": []}, {"text": "We will refer to these types of terms as topic-terms and this type of terminology topic-terminology.", "labels": [], "entities": []}, {"text": "In other work, types of terminology (genes, chemical names, biological processes, etc.) are defined relative to a specific field like Chemistry or Biology ().", "labels": [], "entities": []}, {"text": "These classes are used for narrow tasks, e.g., Information Extraction (IE) slot filling tasks within a particular genre of interest ().", "labels": [], "entities": [{"text": "Information Extraction (IE) slot filling tasks", "start_pos": 47, "end_pos": 93, "type": "TASK", "confidence": 0.8302161693572998}]}, {"text": "Other projects are limited to Information Extraction tasks that may not be terminology-specific, but have terms as arguments, e.g.,) detect abbreviation and definition relations respectively and the arguments are terms.", "labels": [], "entities": [{"text": "Information Extraction tasks", "start_pos": 30, "end_pos": 58, "type": "TASK", "confidence": 0.8361102739969889}]}, {"text": "In contrast to this previous work, we have built a system that extracts a larger set of terminology, which we call jargon-terminology.", "labels": [], "entities": []}, {"text": "Jargon-terms may include ultracentrifuge, which is unlikely to be a topic-term of a current biology article, but will not include potato, a non-technical word that could be a valid topic-term.", "labels": [], "entities": []}, {"text": "We aim to find all the jargon-terms found in a text, not just the ones that fill slots for specific relations.", "labels": [], "entities": []}, {"text": "As we show, jargon-terminology closely matches the notional (e.g., Webster's) definition of terminology.", "labels": [], "entities": []}, {"text": "Furthermore, the important nominals in technical documents tend to be jargon-terms, making them likely arguments of a wide variety of possible IE relations (concepts or objects that are invented, two nominals that are in contrast, one object that is \"better than\" another, etc.).", "labels": [], "entities": []}, {"text": "Specifically, the identification of jargon-terms lays the ground for IE tasks that are not genre or task dependent.", "labels": [], "entities": [{"text": "IE tasks", "start_pos": 69, "end_pos": 77, "type": "TASK", "confidence": 0.9289717972278595}]}, {"text": "Our approach which finds all instances of terms (tokens) in text is conducive to these tasks.", "labels": [], "entities": []}, {"text": "In contrast, topic-term detection techniques find smaller sets of terms (types), each term occurring multiple times and the set of terms collectively represents a topic, in a similar way that a set of documents can represent a topic.", "labels": [], "entities": [{"text": "topic-term detection", "start_pos": 13, "end_pos": 33, "type": "TASK", "confidence": 0.8468660116195679}]}, {"text": "This paper describes a system for extracting jargon-terms in technical documents (patents and journal articles); the evaluation of this system using manually annotated documents; and a set of information extraction (IE) relations which take jargon-terms as arguments.", "labels": [], "entities": [{"text": "extracting jargon-terms in technical documents (patents and journal articles)", "start_pos": 34, "end_pos": 111, "type": "TASK", "confidence": 0.6715186211195859}, {"text": "information extraction (IE) relations", "start_pos": 192, "end_pos": 229, "type": "TASK", "confidence": 0.8566894630591074}]}, {"text": "We incorporate previous work in terminology extraction, assuming that terminology is restricted to noun groups (minus some left modifiers)); 1 and we use both topic-term extraction techniques () and relation-based extraction techniques () in components of our system.", "labels": [], "entities": [{"text": "terminology extraction", "start_pos": 32, "end_pos": 54, "type": "TASK", "confidence": 0.8943958282470703}, {"text": "topic-term extraction", "start_pos": 159, "end_pos": 180, "type": "TASK", "confidence": 0.7612077593803406}, {"text": "relation-based extraction", "start_pos": 199, "end_pos": 224, "type": "TASK", "confidence": 0.7227690517902374}]}, {"text": "Rather than looking at the distribution of noun groups as a whole for determining term-hood, we refine the classes used by the noun group chunker itself, placing limitations on the candidate noun groups proposed and then filtering the output by setting thresholds on the number and quality of the \"jargon-like\" components of the phrase.", "labels": [], "entities": []}, {"text": "The resulting system admits not only topic-terms, but also other non-topic instances of terminology.", "labels": [], "entities": []}, {"text": "Using the more inclusive set of jargon-terms (rather than just topic-terms) as arguments of the IE relations in section 6, we are able to detect a larger and more informative set of relation.", "labels": [], "entities": []}, {"text": "Furthermore, these relations are salient fora wide variety of genres (unlike those in)) -a genre-neutral definition of terminology makes this possible.", "labels": [], "entities": []}, {"text": "For example, the CONTRAST relation between the two boldface terms in necrotrophic effector system A1 that is an exciting contrast to the biotrophic effector models A2 . would be applicable inmost academic genres.", "labels": [], "entities": [{"text": "CONTRAST", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.9536306262016296}]}, {"text": "Our jargon-terms also contrast with the tactic of filling terminology slots in relations with any noun-group, as such a strategy overgenerates, lowering precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 153, "end_pos": 162, "type": "METRIC", "confidence": 0.9967294931411743}]}], "datasetContent": [{"text": "For evaluation purposes, we annotated all the instances of jargon-terms in a speech recognition patent (SRP), a sunscreen patent (SUP) and an article about a virus vaccine (VVA).", "labels": [], "entities": [{"text": "speech recognition patent (SRP)", "start_pos": 77, "end_pos": 108, "type": "TASK", "confidence": 0.7668540477752686}]}, {"text": "Each document was annotated by 2 people and then adjudicated by Annotator 2 after discussing controversial cases.", "labels": [], "entities": []}, {"text": "scores the system, annotator 1 and annotator 2, by comparing each against the answer key providing: number of terms in the answer key, number of matches, precision, recall and F-measure.", "labels": [], "entities": [{"text": "precision", "start_pos": 154, "end_pos": 163, "type": "METRIC", "confidence": 0.9996904134750366}, {"text": "recall", "start_pos": 165, "end_pos": 171, "type": "METRIC", "confidence": 0.9997374415397644}, {"text": "F-measure", "start_pos": 176, "end_pos": 185, "type": "METRIC", "confidence": 0.9993172883987427}]}, {"text": "The \"strict\" scores are based on exact matches between system terms and answer key terms, whereas the \"sloppy\" scores count as correct instances where part of a system term matches part of an answer key term (span errors).", "labels": [], "entities": []}, {"text": "As the SRP document was annotated first, some of specification agreement process took place after annotation and the scores for annotators are somewhat lower than for the other documents.", "labels": [], "entities": [{"text": "SRP document", "start_pos": 7, "end_pos": 19, "type": "DATASET", "confidence": 0.7332026958465576}]}, {"text": "However, Annotator 1's scores for SUP and VVA are good approximations of how well a human being should be expected to perform and the system's scores should be compared to Annotator 1 (i.e., accounting for the adjudicator's bias).", "labels": [], "entities": [{"text": "SUP", "start_pos": 34, "end_pos": 37, "type": "TASK", "confidence": 0.8126181364059448}, {"text": "VVA", "start_pos": 42, "end_pos": 45, "type": "DATASET", "confidence": 0.6617782115936279}]}, {"text": "There are 4 system results: two baseline systems and two stages of the system described in section 3.", "labels": [], "entities": []}, {"text": "Baseline 1 assumes terms derived by removing determiners from noun groups -we used an MEMM chunker using features from the GENIA corpus (.", "labels": [], "entities": [{"text": "GENIA corpus", "start_pos": 123, "end_pos": 135, "type": "DATASET", "confidence": 0.9661199450492859}]}, {"text": "That system has relatively high recall, but overgenerates, yielding a lower precision and F-measure than our full system -it is also inaccurate at determining the extent of terms.", "labels": [], "entities": [{"text": "recall", "start_pos": 32, "end_pos": 38, "type": "METRIC", "confidence": 0.9994505047798157}, {"text": "precision", "start_pos": 76, "end_pos": 85, "type": "METRIC", "confidence": 0.9990228414535522}, {"text": "F-measure", "start_pos": 90, "end_pos": 99, "type": "METRIC", "confidence": 0.9991075396537781}]}, {"text": "Baseline 2 restricts the noun groups from this same chunker to those with O-NOUN heads.", "labels": [], "entities": []}, {"text": "This improves the precision at a high cost to recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.999624490737915}, {"text": "recall", "start_pos": 46, "end_pos": 52, "type": "METRIC", "confidence": 0.9838759303092957}]}, {"text": "Similarly, we first ran our system without filtering the potential jargon-terms, and then we ran the full system.", "labels": [], "entities": []}, {"text": "Clearly our more complex strategy performs better than these baselines and the linguistic filters increase precision more than they reduce recall, resulting in higher F-measures (though low-precision high-recall output maybe better for some applications).", "labels": [], "entities": [{"text": "precision", "start_pos": 107, "end_pos": 116, "type": "METRIC", "confidence": 0.9989516735076904}, {"text": "recall", "start_pos": 139, "end_pos": 145, "type": "METRIC", "confidence": 0.9988043308258057}, {"text": "F-measures", "start_pos": 167, "end_pos": 177, "type": "METRIC", "confidence": 0.9982326030731201}]}, {"text": "6 Relations with Jargon-Terms) describes the annotation of 200 PubMed articles from and 26 patents with several relations, as well as a system for automatically extracting relations.", "labels": [], "entities": []}, {"text": "It turned out that the automatic system depended on the creation of a jargon-term extraction system and thus that work was the major motivating factor for the research described here.", "labels": [], "entities": [{"text": "jargon-term extraction", "start_pos": 70, "end_pos": 92, "type": "TASK", "confidence": 0.6820444613695145}]}, {"text": "Choosing topic-terms as potential arguments would have resulted in low recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 71, "end_pos": 77, "type": "METRIC", "confidence": 0.9989981055259705}]}, {"text": "In contrast, allowing any noun-group to bean argument would have lowered precision, e.g., diagram, large number, accordance and first step are unlikely to be valid arguments of relations.", "labels": [], "entities": [{"text": "precision", "start_pos": 73, "end_pos": 82, "type": "METRIC", "confidence": 0.9991795420646667}, {"text": "accordance", "start_pos": 113, "end_pos": 123, "type": "METRIC", "confidence": 0.9631497859954834}]}, {"text": "In the example: The resequencing pathogen microarray A2 in the diagram is a promising new technology., we can detect that the authors of the articles view pathogen microarray as significant, and not the NG diagram.", "labels": [], "entities": [{"text": "NG diagram", "start_pos": 203, "end_pos": 213, "type": "DATASET", "confidence": 0.8646995723247528}]}, {"text": "By selecting jargon-terms as potential arguments we are selecting the most probable noun group arguments for our relations.", "labels": [], "entities": []}, {"text": "For the current system (which does not use a parser), the system performs best if non-jargon-terms are not considered as potential relation arguments at all.", "labels": [], "entities": []}, {"text": "However, one could imagine a wider coverage (and slower) system incorporating a preference for jargon-terms (like a selection restriction) with dependency-based constraints.", "labels": [], "entities": []}, {"text": "We will only describe a few of these relations due to space considerations.", "labels": [], "entities": []}, {"text": "Our relations include: (1) ABBREVIATE, a relation between two terms that are equivalent.", "labels": [], "entities": [{"text": "ABBREVIATE", "start_pos": 27, "end_pos": 37, "type": "METRIC", "confidence": 0.9985597729682922}]}, {"text": "In the normal case, one term is clearly a shorthand version of the other, e.g., \"The D. melanogaster gene Muscle LIM protein at 84B A1 (abbreviated as Mlp84B A2 )\".", "labels": [], "entities": [{"text": "Muscle LIM protein at 84B A1 (abbreviated as Mlp84B A2 )\"", "start_pos": 106, "end_pos": 163, "type": "METRIC", "confidence": 0.6976606448491415}]}, {"text": "However, in the special case (ABBREVIATE:ALIAS) neither term is a shorthand for the other.", "labels": [], "entities": [{"text": "ABBREVIATE:ALIAS", "start_pos": 30, "end_pos": 46, "type": "METRIC", "confidence": 0.6786786715189616}]}, {"text": "For example in \"Silver behenate A1 , also known as CH3-(CH2)20-COOAg A2 \", the chemical name establishes that this substance is a salt, whereas the formula provides the proportions of all its constituent elements; (2) ORIGINATE, the relation between an ARG1 (person, organization or document) and an ARG2 (a term), such that the ARG1 is an inventor, discoverer, manufacturer, or distributor of the ARG2 and some of these roles are differentiated as subtypes of the relation.", "labels": [], "entities": [{"text": "ORIGINATE", "start_pos": 218, "end_pos": 227, "type": "METRIC", "confidence": 0.9943301677703857}]}, {"text": "Examples include the following: \"Eagle A1 's minimum essential media A2 and DOPG A2 was obtained from Avanti Polar Lipids A1 \".", "labels": [], "entities": [{"text": "Eagle A1", "start_pos": 33, "end_pos": 41, "type": "DATASET", "confidence": 0.8838654458522797}, {"text": "DOPG A2", "start_pos": 76, "end_pos": 83, "type": "METRIC", "confidence": 0.9444788694381714}, {"text": "Avanti Polar Lipids A1", "start_pos": 102, "end_pos": 124, "type": "DATASET", "confidence": 0.9039886891841888}]}, {"text": "These relations are applicable to most technical genres.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Evaluation of Annotation, Baseline and Complete System Against Adjudicated Data", "labels": [], "entities": [{"text": "Complete", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.9938136339187622}]}]}