{"title": [{"text": "FBK-UPV-UEdin participation in the WMT14 Quality Estimation shared-task", "labels": [], "entities": [{"text": "FBK-UPV-UEdin", "start_pos": 0, "end_pos": 13, "type": "DATASET", "confidence": 0.7862163186073303}, {"text": "WMT14 Quality Estimation shared-task", "start_pos": 35, "end_pos": 71, "type": "TASK", "confidence": 0.5375685766339302}]}], "abstractContent": [{"text": "This paper describes the joint submission of Fondazione Bruno Kessler, Universitat Polit\u00e8cnica deVa\u00ec encia and University of Edinburgh to the Quality Estimation tasks of the Workshop on Statistical Machine Translation 2014.", "labels": [], "entities": [{"text": "Quality Estimation tasks of the Workshop on Statistical Machine Translation 2014", "start_pos": 142, "end_pos": 222, "type": "TASK", "confidence": 0.8181004822254181}]}, {"text": "We present our submissions for Task 1.2, 1.3 and 2.", "labels": [], "entities": []}, {"text": "Our systems ranked first for Task 1.2 and for the Binary and Level1 settings in Task 2.", "labels": [], "entities": []}], "introductionContent": [{"text": "Quality Estimation (QE) for Machine Translation (MT) is the task of evaluating the quality of the output of an MT system without reference translations.", "labels": [], "entities": [{"text": "Quality Estimation (QE)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.5607596039772034}, {"text": "Machine Translation (MT)", "start_pos": 28, "end_pos": 52, "type": "TASK", "confidence": 0.8645212054252625}]}, {"text": "Within the WMT 2014 QE Shared Task four evaluation tasks were proposed, covering both word and sentence level QE.", "labels": [], "entities": [{"text": "WMT 2014 QE Shared Task", "start_pos": 11, "end_pos": 34, "type": "DATASET", "confidence": 0.6748192071914673}]}, {"text": "In this work we describe the Fondazione Bruno Kessler (FBK), Universitat Polit\u00e8cnica deVa\u00ec encia (UPV) and University of Edinburgh (UEdin) approach and system setup for the shared task.", "labels": [], "entities": []}, {"text": "We developed models for two sentence-level tasks: Task 1.2, scoring for post-editing effort, and Task 1.3, predicting post-editing time, and for all word-level variants of Task 2, binary and multiclass classification.", "labels": [], "entities": [{"text": "multiclass classification", "start_pos": 191, "end_pos": 216, "type": "TASK", "confidence": 0.7347478270530701}]}, {"text": "As opposed to previous editions of the shared task, this year the participants were not supplied with the MT system that was used to produce the translation.", "labels": [], "entities": []}, {"text": "Furthermore no system-internal features were provided.", "labels": [], "entities": []}, {"text": "Thus, while the trained models are tuned to detect the errors of a specific system the features have to be generated independently (black-box).", "labels": [], "entities": []}], "datasetContent": [{"text": "The free parameters of the BLSTM-RNNs are optimized by 10-fold cross-validation on the training set.", "labels": [], "entities": [{"text": "BLSTM-RNNs", "start_pos": 27, "end_pos": 37, "type": "DATASET", "confidence": 0.7460283041000366}]}, {"text": "Each cross-validation experiment consider eight folds for training, one held-out fold for development, and a final held-out fold for testing.", "labels": [], "entities": []}, {"text": "We estimate the neural network with the eight training folds using the prediction performance in the validation fold as stopping criterion.", "labels": [], "entities": []}, {"text": "The result of each complete cross-validation experiment is the average of the results for the predictions of the ten held-out test folds.", "labels": [], "entities": []}, {"text": "Additionally, to avoid noise due to the random initialization of the network, we repeat each cross-validation experiment ten times and average the results.", "labels": [], "entities": []}, {"text": "Once the optimal values of the free parameters are established, we estimate anew BLSTM-RNN using the full training corpus and we use it as the final model to predict the class labels of the test words.", "labels": [], "entities": [{"text": "BLSTM-RNN", "start_pos": 81, "end_pos": 90, "type": "METRIC", "confidence": 0.7323840260505676}]}, {"text": "Since our objective is to detect words that need to be edited, we use the weighted averaged F 1 score over the different class labels that denote an error as our main performance metric (wF1 err ).", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 92, "end_pos": 101, "type": "METRIC", "confidence": 0.9280306895573934}, {"text": "wF1 err )", "start_pos": 187, "end_pos": 196, "type": "METRIC", "confidence": 0.7882437507311503}]}, {"text": "We also report the weighted averaged F 1 scores: Cross-validation results for the different setups tested for Task 2.", "labels": [], "entities": [{"text": "F 1 scores", "start_pos": 37, "end_pos": 47, "type": "METRIC", "confidence": 0.9684229691823324}]}, {"text": "Our two submissions are marked as and respectively.", "labels": [], "entities": []}, {"text": "overall the classes (wF1 all ).", "labels": [], "entities": []}, {"text": "Analyzing the results we observe that prediction accuracy is quite low.", "labels": [], "entities": [{"text": "prediction", "start_pos": 38, "end_pos": 48, "type": "TASK", "confidence": 0.9166947603225708}, {"text": "accuracy", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.9778247475624084}]}, {"text": "Our hypothesis is that this is due to the skewed class distribution.", "labels": [], "entities": []}, {"text": "Even for the binary classification scenario (the most balanced of the three conditions), OK labels account for two thirds of the samples.", "labels": [], "entities": [{"text": "binary classification", "start_pos": 13, "end_pos": 34, "type": "TASK", "confidence": 0.7170164585113525}]}, {"text": "This effect worsens with increasing number of error classes and the resulting sparsity of observations.", "labels": [], "entities": []}, {"text": "As a result, the system tends to classify all samples as OK which leads to the low F 1 scores presented in.", "labels": [], "entities": [{"text": "F 1 scores", "start_pos": 83, "end_pos": 93, "type": "METRIC", "confidence": 0.9884608785311381}]}], "tableCaptions": [{"text": " Table 1: Training and test results for Task 1.2 and 1.3. Scores are the MAE on a development set  randomly sampled from the training data (20%). Baseline features were provided by the shared task  organizers. We used Support Vector Machines (SVM) regression to train the baseline models (first row).  Submissions are marked with 1 and 2 for primary and secondary, respectively.", "labels": [], "entities": [{"text": "MAE", "start_pos": 73, "end_pos": 76, "type": "METRIC", "confidence": 0.9979590177536011}]}, {"text": " Table 2: Cross-validation results for the different setups tested for Task 2. Our two submissions are  marked as", "labels": [], "entities": []}, {"text": " Table 3: Test results for Task 2. Numbers are  weighted averaged F 1 scores (%) for all but the  OK class.", "labels": [], "entities": [{"text": "F 1 scores", "start_pos": 66, "end_pos": 76, "type": "METRIC", "confidence": 0.9723482131958008}]}]}