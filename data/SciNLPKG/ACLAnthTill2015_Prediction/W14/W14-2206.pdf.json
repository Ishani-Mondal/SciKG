{"title": [{"text": "Learning Grammar Specifications from IGT: A Case Study of Chintang", "labels": [], "entities": [{"text": "IGT", "start_pos": 37, "end_pos": 40, "type": "DATASET", "confidence": 0.7847709059715271}, {"text": "Chintang", "start_pos": 58, "end_pos": 66, "type": "DATASET", "confidence": 0.6601879596710205}]}], "abstractContent": [{"text": "We present a case study of the methodology of using information extracted from interlinear glossed text (IGT) to create of actual working HPSG grammar fragments using the Grammar Matrix focusing on one language: Chintang.", "labels": [], "entities": []}, {"text": "Though the results are barely measurable in terms of coverage overrunning text, they nonetheless provide a proof of concept.", "labels": [], "entities": []}, {"text": "Our experience report reflects on the ways in which this task is non-trivial and on mismatches between the assumptions of the methodology and the realities of IGT as produced in a large-scale field project.", "labels": [], "entities": []}], "introductionContent": [{"text": "We explore the possibility of learning precision grammar fragments from existing products of documentary linguistic work.", "labels": [], "entities": []}, {"text": "A precision grammar is a grammar which encodes a sharp notion of grammaticality and furthermore relates strings to elaborate semantic representations.", "labels": [], "entities": [{"text": "precision", "start_pos": 2, "end_pos": 11, "type": "METRIC", "confidence": 0.9633514285087585}]}, {"text": "Such objects are of interest in the context of documentary linguistics because: (1) they are valuable tools in the exploration of linguistic hypotheses (especially regarding the interaction of various phenomena); (2) they facilitate the search for examples in corpora which are not yet understood; and (3) they can support the development of treebanks (see).", "labels": [], "entities": []}, {"text": "However, they are expensive to build.", "labels": [], "entities": []}, {"text": "The present work is carried out in the context of the AGGREGATION project, 1 which is exploring whether such grammars can be learned on the basis of data already collected and enriched through the work of descriptive linguists, specifically, collections of IGT (interlinear glossed text).", "labels": [], "entities": []}, {"text": "The grammars themselves are not likely targets for machine learning, especially in the absence of treebanks, which are not generally available for languages that are the focus of descriptive and documentary linguistics.", "labels": [], "entities": []}, {"text": "Instead, we take advantage of the LinGO Grammar Matrix customization system) which maps from collections of statements of linguistic properties (encoded in choices files) to HPSG grammar fragments which in turn can be used to parse strings into semantic representations in the format of Minimal Recursion Semantics (MRS;) and conversely, to generate strings from MRS representations.", "labels": [], "entities": [{"text": "LinGO Grammar Matrix", "start_pos": 34, "end_pos": 54, "type": "DATASET", "confidence": 0.8595351378122965}]}, {"text": "The choices files area much simpler representation than the grammars derived from them and therefore a more approachable learning target.", "labels": [], "entities": []}, {"text": "Furthermore, using the Grammar Matrix customization system to produce the grammars results in much less noise in the automatically derived grammar code than would arise in a system learning grammars directly.", "labels": [], "entities": []}, {"text": "Here, we focus on a case study of Chintang, a Kiranti language of Nepal, described by the Chintang Language Research Project (CLRP) (.", "labels": [], "entities": [{"text": "Chintang Language Research Project (CLRP)", "start_pos": 90, "end_pos": 131, "type": "DATASET", "confidence": 0.7521675910268512}]}, {"text": "Where and apply similar methodologies to extract large scale properties for many languages, we focus on a case study of a single language, looking at both the large scale properties and the lexical details.", "labels": [], "entities": []}, {"text": "This is important for two reasons: First, it gives us a chance to look indepth at the possible sources of difficulty in extracting the large scale properties.", "labels": [], "entities": []}, {"text": "Second, while large-scale properties are undoubtedly important, the bulk of the information specified in a precision grammar is far more fine-grained.", "labels": [], "entities": []}, {"text": "In this case study we apply the methodology of to extract general word order and case properties and examine the sources of error affecting those results.", "labels": [], "entities": []}, {"text": "We also explore extensions of those methodologies and that of to extract lexical entries and specifications for morpho-logical rules.", "labels": [], "entities": []}, {"text": "Together with a few default specifications, this information is enough to allow us to define grammars through the Grammar Matrix customization system and thus evaluate the results in terms of parsing coverage, accuracy and ambiguity overrunning text.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 210, "end_pos": 218, "type": "METRIC", "confidence": 0.9979146122932434}]}, {"text": "Chintang is particularly well-suited for this case study because it is an actual endangered language subject to active descriptive research, making the evaluation of our techniques realistic.", "labels": [], "entities": []}, {"text": "Furthermore, the descriptive research on Chintang is fairly advanced, having produced both large corpora of high-quality IGT and sophisticated linguistic descriptions, making the evaluation and error analysis possible.", "labels": [], "entities": []}], "datasetContent": [{"text": "Chintang (ISO639-3: ctn) is a language spoken by about 5000 people in Nepal and believed to belong to the Eastern subgroup of the Kiranti languages, which in turn are argued to belong to the larger Tibeto-Burman family ().", "labels": [], "entities": []}, {"text": "Here we briefly summarize properties of the language that relate to the information we are attempting to automatically detect in the IGT, and in many cases make the problem interestingly difficult.) describe Chintang as exhibiting information-structurally constrained word order: All permutations of the major sentential constituents are expected to be valid, with the different orders subject to different felicity conditions.", "labels": [], "entities": [{"text": "IGT", "start_pos": 133, "end_pos": 136, "type": "DATASET", "confidence": 0.7930195331573486}]}, {"text": "They state, however, that no detailed analysis of word order has yet been carried out, and so this description should betaken as preliminary.", "labels": [], "entities": []}, {"text": "In contrast, much detailed work has been done on the marking of arguments, both via agreement on the verb and via case marking of dependents ().", "labels": [], "entities": []}, {"text": "The case marking system can be understood as following an ergativeabsolutive pattern, but with several variations from that theme.", "labels": [], "entities": [{"text": "case marking", "start_pos": 4, "end_pos": 16, "type": "TASK", "confidence": 0.8419978022575378}]}, {"text": "In an ergative-absolutive pattern, the sole argument of an intransitive verb (here called S) is marked the same as the most patient-like argument of a transitive verb (here called O) and differentiated from the most agent-like argument of a transitive verb (here called A).", "labels": [], "entities": []}, {"text": "Most A arguments are marked with an overt case marker called ergative, while Sand O arguments appear without a case marker.", "labels": [], "entities": [{"text": "ergative", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.9374738335609436}]}, {"text": "In most writing about the language, this unmarked case is called nominative; here we will use the term absolutive.", "labels": [], "entities": []}, {"text": "Similarly, verbs agree with up to two arguments, and the agreement markers for Sand O are generally shared and distinguished from those for A.", "labels": [], "entities": []}, {"text": "Divergences from the ergative-absolutive pattern include variable marking of ergative case on first and second person pronouns as well as valence alternations such as one that licenses occurrences of transitive verbs with two absolutive arguments (and S-style agreement with the A argument) when the O argument is of an indefinite quantity ().", "labels": [], "entities": []}, {"text": "Furthermore, the language allows dropping of arguments (A, S, and O).", "labels": [], "entities": []}, {"text": "Finally, there are of course valences beyond simple intransitive and transitive, as well as case frames even for two-argument verbs other than { ERG, ABS }.", "labels": [], "entities": []}, {"text": "As a result of the combination of these facts, the actual occurrence of ergative-casemarked arguments in speech is relatively low: Examining a corpus of speech spoken to and around children, find that only 11% of (semantically) transitive verb tokens have an overt, ergative-marked NP A argument.", "labels": [], "entities": []}, {"text": "As discussed below, these properties make it difficult for automated methods to detect both the overall case system of the language and accurate information regarding the case frames of individual verbs.", "labels": [], "entities": []}, {"text": "The dataset we are using contains 9793 (8863 train, 930 test) IGT instances which come from the corpus of narratives and other speech collected, transcribed, translated and glossed by the CLRP.", "labels": [], "entities": [{"text": "CLRP", "start_pos": 188, "end_pos": 192, "type": "DATASET", "confidence": 0.9784947633743286}]}, {"text": "An example is shown in.", "labels": [], "entities": []}, {"text": "As can be seen in, the glossing in this dataset is extremely thorough.", "labels": [], "entities": []}, {"text": "It is also supported by a detailed Toolbox lexicon that encodes not only alternative forms for each lemma as well as glosses in English and Nepali, but also valence frames for most verb entries which list the expected case marking on the arguments.", "labels": [], "entities": []}, {"text": "Finally, note that morphosyntactic properties without a morphological reflex are systematically unglossed in the data, so that ABS never appears (nor does SG for singular nouns, etc.).", "labels": [], "entities": []}, {"text": "In our experiments, we abstract away from the problem of morphophonological analysis in order to focus on morphosyntax and lexical acquisition.", "labels": [], "entities": [{"text": "morphophonological analysis", "start_pos": 57, "end_pos": 84, "type": "TASK", "confidence": 0.7280499935150146}, {"text": "lexical acquisition", "start_pos": 123, "end_pos": 142, "type": "TASK", "confidence": 0.7253620326519012}]}, {"text": "Accordingly, our grammars target the second line of the IGT, which represents each form as a sequence of phonologically regularized morphemes.", "labels": [], "entities": [{"text": "IGT", "start_pos": 56, "end_pos": 59, "type": "DATASET", "confidence": 0.8593891263008118}]}, {"text": "We evaluate the grammars generated by the choices files over both the data used to develop them ('training'; 8863 items) as well as data not included in the development process (held-out 'test' data; 930 items).", "labels": [], "entities": []}, {"text": "We run both of these evaluations because we are actually testing two separate questions.", "labels": [], "entities": []}, {"text": "The first is whether the grammars generated in this way can provide useful analytical tools to linguists.", "labels": [], "entities": []}, {"text": "In this primary use-case, we expect a linguist to provide the system with all of their IGT and then use the generated grammars in order to gain insights into that same data.", "labels": [], "entities": []}, {"text": "This does not amount to a case of testing on the training data because the annotations provided to the system (IGT) are not the same as those produced by the system (full parses, including semantic representations).", "labels": [], "entities": []}, {"text": "However, we are still interested in also testing on held-out data in order to answer the second question: whether grammars generated in this way can also generalize to further texts.", "labels": [], "entities": []}, {"text": "We evaluate the grammars generated by the choices files we create in terms of lexical coverage, parse coverage, parse accuracy and ambiguity.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 118, "end_pos": 126, "type": "METRIC", "confidence": 0.8740226626396179}]}, {"text": "Lexical coverage measures how many items consist only of word forms recognized by the grammar.", "labels": [], "entities": [{"text": "Lexical coverage", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.822211742401123}]}, {"text": "Any item with unknown lexical items won't parse.", "labels": [], "entities": []}, {"text": "Parse coverage is the number of items that receive any analysis at all, where ambiguity is the number of different analyses each item receives.", "labels": [], "entities": [{"text": "Parse coverage", "start_pos": 0, "end_pos": 14, "type": "METRIC", "confidence": 0.9449786841869354}]}, {"text": "To measure parse accuracy, we examined the items that parse and determined which parses had semantic representations whose predicate-argument structures plausibly matched what was indicated in the gloss.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.9706186056137085}]}, {"text": "compares the lexical information encoded in each of the choices files in a quantitative fashion.", "labels": [], "entities": []}, {"text": "The first thing to note is that the grammars vary widely in the size of their lexicons.", "labels": [], "entities": []}, {"text": "The BASE-LINE/FF lexicons are expected to be larger than the others because they take each fully inflected form encountered as a separate lexical entry.", "labels": [], "entities": [{"text": "BASE-LINE", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9537658095359802}]}, {"text": "On the other hand, the ORACLE choices file was built on the basis of the Toolbox lexicon (dictionary) from the CLRP and thus is effectively created on the basis of a much larger dataset.", "labels": [], "entities": [{"text": "ORACLE", "start_pos": 23, "end_pos": 29, "type": "METRIC", "confidence": 0.9392966032028198}, {"text": "CLRP", "start_pos": 111, "end_pos": 115, "type": "DATASET", "confidence": 0.5033645629882812}]}, {"text": "The GRAM choices files only contain verbs for which a case frame could be identified.", "labels": [], "entities": [{"text": "GRAM choices files", "start_pos": 4, "end_pos": 22, "type": "DATASET", "confidence": 0.8891551891962687}]}, {"text": "If the projected tree was not interpretable by our extraction heuristics or if the example had no overt arguments, then the verb will not be extracted.", "labels": [], "entities": []}, {"text": "The MOM choices files, on the other hand, only need to identify verbs in the string to be able to extract them, and should be able to generalize across different inflected forms of the same verb.", "labels": [], "entities": []}, {"text": "This gives a number of verb entries intermediate between that for BASELINE/FF and the GRAM files.", "labels": [], "entities": [{"text": "BASELINE/FF", "start_pos": 66, "end_pos": 77, "type": "TASK", "confidence": 0.4811300237973531}, {"text": "GRAM files", "start_pos": 86, "end_pos": 96, "type": "DATASET", "confidence": 0.8366040885448456}]}, {"text": "For nouns, there is less variation: the MOM files use the same data as the BASELINE, while the GRAM method faces as simpler problem than for verbs: it only needs to identify the case gram (if any) in a noun's gloss.", "labels": [], "entities": [{"text": "BASELINE", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.9480140209197998}, {"text": "GRAM", "start_pos": 95, "end_pos": 99, "type": "METRIC", "confidence": 0.9606992602348328}]}, {"text": "The slightly larger numbers of nouns in the GRAM files v. the others can be explained by the same form being glossed in two different ways in the training data.", "labels": [], "entities": [{"text": "GRAM files", "start_pos": 44, "end_pos": 54, "type": "DATASET", "confidence": 0.9224073588848114}]}], "tableCaptions": [{"text": " Table 2: Amount of lexical information in each choices file", "labels": [], "entities": [{"text": "Amount", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9181685447692871}]}]}