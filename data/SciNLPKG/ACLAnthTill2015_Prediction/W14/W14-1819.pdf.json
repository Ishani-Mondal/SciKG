{"title": [{"text": "Exploiting Morphological, Grammatical, and Semantic Correlates for Improved Text Difficulty Assessment", "labels": [], "entities": [{"text": "Improved Text Difficulty Assessment", "start_pos": 67, "end_pos": 102, "type": "TASK", "confidence": 0.7491110190749168}]}], "abstractContent": [{"text": "We present a low-resource, language-independent system for text difficulty assessment.", "labels": [], "entities": [{"text": "text difficulty assessment", "start_pos": 59, "end_pos": 85, "type": "TASK", "confidence": 0.830920954545339}]}, {"text": "We replicate and improve upon a baseline by Shen et al.", "labels": [], "entities": []}, {"text": "(2013) on the Interagency Language Roundtable (ILR) scale.", "labels": [], "entities": [{"text": "Interagency Language Roundtable (ILR)", "start_pos": 14, "end_pos": 51, "type": "TASK", "confidence": 0.6404806027809778}]}, {"text": "Our work demonstrates that the addition of morphological, information the-oretic, and language modeling features to a traditional readability baseline greatly benefits our performance.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 86, "end_pos": 103, "type": "TASK", "confidence": 0.6924354732036591}]}, {"text": "We use the Margin-Infused Relaxed Algorithm and Support Vector Machines for experiments on Arabic, Dari, English, and Pashto, and provide a detailed analysis of our results.", "labels": [], "entities": []}], "introductionContent": [{"text": "While there is a growing breadth of reading materials available in various languages, finding pertinent documents at suitable reading levels remains difficult.", "labels": [], "entities": []}, {"text": "Information retrieval methods can find resources with desired vocabulary, but educators still need to filter these to find appropriate difficulty levels.", "labels": [], "entities": [{"text": "Information retrieval", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.7672226130962372}]}, {"text": "This task is often more challenging than manually adapting the documents themselves.", "labels": [], "entities": []}, {"text": "Reading level assessment systems can be used to automatically find documents at specific Interagency Language Roundtable (ILR) levels, aiding both instructors and learners by providing proficiency-tailored materials.", "labels": [], "entities": []}, {"text": "While interest in readability assessment has been gaining momentum in many languages, the majority of previous work is language-specific.", "labels": [], "entities": [{"text": "readability assessment", "start_pos": 18, "end_pos": 40, "type": "TASK", "confidence": 0.8008752763271332}]}, {"text": "introduced a baseline for language-independent text difficulty assessment, based on the ILR proficiency scale.", "labels": [], "entities": [{"text": "language-independent text difficulty assessment", "start_pos": 26, "end_pos": 73, "type": "TASK", "confidence": 0.6220136284828186}, {"text": "ILR proficiency scale", "start_pos": 88, "end_pos": 109, "type": "DATASET", "confidence": 0.8595358928044637}]}, {"text": "In this work, we replicate and extend their results.", "labels": [], "entities": []}, {"text": "The ILR scale is the standard language proficiency measure for the U.S. federal government.", "labels": [], "entities": []}, {"text": "It ranges from no proficiency to native proficiency on a scale of 0-5, with half-level denotations where proficiency meets some but not all of the criteria for the next level).", "labels": [], "entities": []}, {"text": "For second language learners, it is sufficient to use up to ILR level 4.", "labels": [], "entities": [{"text": "ILR level 4", "start_pos": 60, "end_pos": 71, "type": "METRIC", "confidence": 0.6848816871643066}]}, {"text": "Since proficiency is a continuous spectrum, text difficulty assessment is often treated as a regression problem, as we do here.", "labels": [], "entities": [{"text": "text difficulty assessment", "start_pos": 44, "end_pos": 70, "type": "TASK", "confidence": 0.775281548500061}]}, {"text": "Though the ILR levels may appear to be discrete categories, documents can fall between levels.", "labels": [], "entities": []}, {"text": "The degree to which they do is important for us to measure.", "labels": [], "entities": []}], "datasetContent": [{"text": "We compare our results to the best performing Suport Vector Machine (SVM) and Margin-Infused Relaxed Algorithm (MIRA) baselines from.", "labels": [], "entities": [{"text": "Margin-Infused Relaxed Algorithm (MIRA)", "start_pos": 78, "end_pos": 117, "type": "METRIC", "confidence": 0.8409668803215027}]}, {"text": "Both of these baselines have the same features: TFLOG weighted word vectors, average sentence length by document, average word length by document, and document word count.", "labels": [], "entities": [{"text": "TFLOG weighted word vectors", "start_pos": 48, "end_pos": 75, "type": "METRIC", "confidence": 0.866648867726326}]}, {"text": "We used an implementation of the MIRA algorithm for regression.", "labels": [], "entities": [{"text": "MIRA", "start_pos": 33, "end_pos": 37, "type": "METRIC", "confidence": 0.6118412017822266}, {"text": "regression", "start_pos": 52, "end_pos": 62, "type": "TASK", "confidence": 0.9723129868507385}]}, {"text": "We embedded Morfessor for unsupervised morphological segmentation and preprocessed our data as required by this algorithm.", "labels": [], "entities": []}, {"text": "To verify our results across classifiers, we compare with SVM).", "labels": [], "entities": []}, {"text": "We also compare Morfessor to ParaMor), an unsupervised system with a different level of segmentation aggression, as well as to language-specific analyzers.", "labels": [], "entities": []}, {"text": "Our experiments apply word-usage features, shallow length features, and language models.", "labels": [], "entities": []}, {"text": "For the first, we compare TFLOG vectors based on word types, all morphemes, and stems only.", "labels": [], "entities": []}, {"text": "For the second, we tested the three baseline shallow length features (average word length in characters per document, average sentence length per document, and document word count) as well as measures of relative entropy, average stem fertility, average morphemes per word, and the ratio of types to tokens.", "labels": [], "entities": []}, {"text": "Of these, only relative entropy positively impacted performance, and only its results are reported in this paper.", "labels": [], "entities": []}, {"text": "All length features were z-normalized.", "labels": [], "entities": []}, {"text": "We compare both word-and classbased language models.", "labels": [], "entities": []}, {"text": "We trained LMs for each ILR level and used the document perplexity measured against each as features.", "labels": [], "entities": []}, {"text": "Optimal settings were determined by sweeping algorithm parameters, and Morfessor's perplexity threshold for each language.", "labels": [], "entities": []}, {"text": "We conducted a feature analysis for all combinations of word, length, and LM features across all four languages.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Baseline results in MSE, SVM vs. MIRA", "labels": [], "entities": [{"text": "Baseline", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9112588763237}, {"text": "MIRA", "start_pos": 43, "end_pos": 47, "type": "METRIC", "confidence": 0.926793098449707}]}, {"text": " Table 4: Average MSE results comparing the use  of types, all morphs, and stems for TFLOG vec- tors. Morfessor algorithm used for segmentation.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 131, "end_pos": 143, "type": "TASK", "confidence": 0.9752609133720398}]}, {"text": " Table 5. All further  results use Morfessor for stemming.", "labels": [], "entities": [{"text": "stemming", "start_pos": 49, "end_pos": 57, "type": "TASK", "confidence": 0.9720322489738464}]}, {"text": " Table 5: Comparison of unsupervised segmenters", "labels": [], "entities": [{"text": "Comparison of unsupervised segmenters", "start_pos": 10, "end_pos": 47, "type": "TASK", "confidence": 0.6294068843126297}]}, {"text": " Table 6: Comparison of English segmenters", "labels": [], "entities": [{"text": "Comparison of English segmenters", "start_pos": 10, "end_pos": 42, "type": "TASK", "confidence": 0.6369351297616959}]}, {"text": " Table 7: Average MSE results comparing features  from Sections 4.1 and 4.2. LMs are order 5.", "labels": [], "entities": [{"text": "MSE", "start_pos": 18, "end_pos": 21, "type": "TASK", "confidence": 0.5527084469795227}]}, {"text": " Table 8: Average MSE results comparing all fea- tures. LMs and cLMs are order 5.", "labels": [], "entities": []}, {"text": " Table 9: Final system results, comparing avg.  MSE with the MIRA and SVM algorithms", "labels": [], "entities": [{"text": "MIRA", "start_pos": 61, "end_pos": 65, "type": "DATASET", "confidence": 0.5467236638069153}]}]}