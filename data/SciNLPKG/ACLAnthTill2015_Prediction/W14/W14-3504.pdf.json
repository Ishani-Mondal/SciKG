{"title": [{"text": "Leveraging known semantics for spelling correction", "labels": [], "entities": []}], "abstractContent": [{"text": "Focusing on applications for analyzing learner language which evaluate semantic appropriate-ness and accuracy, we build from previous work which modeled some aspects of interaction, namely a picture description task (PDT), with the goal of integrating a spelling correction component in this context.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 101, "end_pos": 109, "type": "METRIC", "confidence": 0.997998058795929}, {"text": "picture description task (PDT)", "start_pos": 191, "end_pos": 221, "type": "TASK", "confidence": 0.83048015832901}, {"text": "spelling correction", "start_pos": 254, "end_pos": 273, "type": "TASK", "confidence": 0.872359037399292}]}, {"text": "After parsing a sentence and extracting semantic relations, a surprising number of analysis failures stem from misspellings, deviating from expected input in ways that can be modeled when the content of the interaction is known.", "labels": [], "entities": []}, {"text": "We thus explore the use of spelling correction tools and language modeling to correct misspellings that often lead to errors in obtaining semantic forms, and we show that such tools can significantly reduce the number of unanalyzable cases.", "labels": [], "entities": [{"text": "spelling correction", "start_pos": 27, "end_pos": 46, "type": "TASK", "confidence": 0.8858156204223633}]}, {"text": "The work is useful for any context where image descriptions or some expected content is available, but not necessarily expected linguistic forms.", "labels": [], "entities": []}, {"text": "Much current work on analyzing learner language focuses on grammatical error detection and correction (e.g., Dale et al., 2012) and lesson semantic analysis; many Intelligent Computer-Assisted Language Learning (ICALL) and Intelligent Language Tutoring (ILT) systems (e.g., Heift and Schulze, 2007; Meurers, 2012) also focus more on grammatical feedback.", "labels": [], "entities": [{"text": "grammatical error detection and correction", "start_pos": 59, "end_pos": 101, "type": "TASK", "confidence": 0.6671858668327332}, {"text": "lesson semantic analysis", "start_pos": 132, "end_pos": 156, "type": "TASK", "confidence": 0.811380018790563}]}, {"text": "An exception to this rule is Herr Komissar, an ILT for German learners that includes rather robust content analysis and sentence generation (DeSmedt, 1995), but this involves a great deal of hand-built tools and does not connect to modern NLP.", "labels": [], "entities": [{"text": "sentence generation", "start_pos": 120, "end_pos": 139, "type": "TASK", "confidence": 0.7153619080781937}]}, {"text": "Some work addresses content assessment for short answer tasks (Meurers et al., 2011), but there is still a need to move towards naturalistic, more conversational interactions (see Petersen, 2010).", "labels": [], "entities": []}, {"text": "Such interactions are both more and less difficult to process: to provide feedback requires keeping track of the content of the interaction, but such content can also be used to disambiguate new learner productions.", "labels": [], "entities": []}, {"text": "We exploit this tension in the context of spelling correction, as semantic information severely restricts the learner's expected content, and thus also their word forms.", "labels": [], "entities": [{"text": "spelling correction", "start_pos": 42, "end_pos": 61, "type": "TASK", "confidence": 0.9503461122512817}]}, {"text": "Since our overarching goal is to move towards the facilitation of ILTs and language assessment tools that maximize free interaction, we have to deal with removing impediments to interaction.", "labels": [], "entities": []}, {"text": "Given the preponderance of spelling errors in learner data, and specifically interactive data (King and Dickinson, 2013), our specific goal is to use basic NLP (pre)processing-namely, language modeling for spelling correction-to make the meaning of a learner's sentence clearer.", "labels": [], "entities": [{"text": "spelling correction-to", "start_pos": 206, "end_pos": 228, "type": "TASK", "confidence": 0.763945460319519}]}, {"text": "We examine methods for automatically correcting misspellings, showing that preprocessing with spelling correction tools, when information about the interactive context is known (i.e., the picture's description), can greatly reduce downstream errors.", "labels": [], "entities": [{"text": "automatically correcting misspellings", "start_pos": 23, "end_pos": 60, "type": "TASK", "confidence": 0.6966720422108968}, {"text": "spelling correction", "start_pos": 94, "end_pos": 113, "type": "TASK", "confidence": 0.7696586549282074}]}, {"text": "Unlike, for example, linguistic abstractions such as part-of-speech, both are intimately rooted in the particular lexical items used.", "labels": [], "entities": []}, {"text": "This then raises the question of whether we are modeling what the learner said (modulo some spelling variation), what the learner intended, or what the learner should have intended, an issue we take up in section 4, after covering the background in section 3.", "labels": [], "entities": []}, {"text": "The methods are covered in section 5 and the evaluation in section 6. 2 Related Work Research into the patterns of spelling errors particular to native speakers (NSs) and non-native speakers (NNSs) highlights the challenge of applying spelling correction techniques to non-native text.", "labels": [], "entities": [{"text": "spelling correction", "start_pos": 235, "end_pos": 254, "type": "TASK", "confidence": 0.7387970685958862}]}, {"text": "(2013) examined spelling errors found in the ETS Spelling Corpus (3000 GRE and TOEFL essays) and found that NNS spelling errors were more severe (i.e., had a greater edit distance from the intended word) than NS errors.", "labels": [], "entities": [{"text": "ETS Spelling Corpus (3000 GRE and TOEFL essays", "start_pos": 45, "end_pos": 91, "type": "DATASET", "confidence": 0.8006008466084799}]}, {"text": "Moreover, NNSs made more spelling errors than NSs for words of 3-7 letters, but this trend reversed for words of 8 letters or more.", "labels": [], "entities": []}, {"text": "These effects were shown to disappear among the most proficient NNSs in the sample, however.", "labels": [], "entities": []}, {"text": "Similarly, Hovermale (2010) compared the spelling errors in corpora of Japanese learners of English to previous studies of NS spelling errors and found that the learner errors have a greater average edit distance and are nearly twice as likely to involve the first letter of the word.", "labels": [], "entities": []}, {"text": "Given such variability inform, correcting spelling errors for NNSs strictly via edit 44 distance operations would thus seem to have its limits.", "labels": [], "entities": [{"text": "correcting spelling errors", "start_pos": 31, "end_pos": 57, "type": "TASK", "confidence": 0.7509698867797852}]}, {"text": "Using the ETS Spelling Corpus and the ConSpell spelling correction tool, Flor (2012) demonstrates significant gains in automatic spelling correction when modules using contextual information are added.", "labels": [], "entities": [{"text": "ETS Spelling Corpus", "start_pos": 10, "end_pos": 29, "type": "DATASET", "confidence": 0.9628918170928955}, {"text": "ConSpell spelling correction", "start_pos": 38, "end_pos": 66, "type": "TASK", "confidence": 0.662252684434255}, {"text": "automatic spelling correction", "start_pos": 119, "end_pos": 148, "type": "TASK", "confidence": 0.5838241179784139}]}, {"text": "Four types of context, each of which benefitted spelling correction, were explored: 1) word n-grams (length 1-5) and a web-scale language model (LM); 2) word n-grams and the positive normalized pointwise mutual information (PNPMI) of the words within them (based on a web-scale distributional model); 3) the entire essay (and the recurrence or lack of a given candidate spelling correction in the essay); and 4) the text of the essay prompt.", "labels": [], "entities": [{"text": "spelling correction", "start_pos": 48, "end_pos": 67, "type": "TASK", "confidence": 0.715300977230072}, {"text": "positive normalized pointwise mutual information (PNPMI)", "start_pos": 174, "end_pos": 230, "type": "METRIC", "confidence": 0.7244857549667358}]}, {"text": "Notably, a 3.8% improvement comes through the use of \"global mutual optimization\", i.e., at each given spelling correction decision, the module is biased not only toward other words in the text, but also the candidate spelling lists of these other words.", "labels": [], "entities": [{"text": "spelling correction decision", "start_pos": 103, "end_pos": 131, "type": "TASK", "confidence": 0.7489104767640432}]}, {"text": "The work presents a strong case for the use of n-grams with both LMs and PNPMI, as the best results come from this setting, boosting performance 11.48% above the non-contextual spelling correction baseline.", "labels": [], "entities": []}, {"text": "Flor and Futagi (2012) further examine the use of context for correcting learner misspellings and claim that three major issues contribute to the task's difficulty: \"local error density\" (a misspelled word near other misspellings) weakens n-gram approaches; poor grammar can lead to the selection of an incorrect spelling candidate based on its agreement with nearby incorrect words; and competition among closely related spelling candidates can lead to the selection of an incorrect inflectional variant.", "labels": [], "entities": [{"text": "correcting learner misspellings", "start_pos": 62, "end_pos": 93, "type": "TASK", "confidence": 0.7970079978307089}]}, {"text": "These challenges indicate that for potentially error-rich learner sentences, sentence or n-gram level contexts maybe more effective when combined with higher-level contextual information, such as task prompts and discourse-level information about verb inflections.", "labels": [], "entities": []}, {"text": "We explore including information about picture content.", "labels": [], "entities": []}, {"text": "3 Background 3.1 Data Response (L1) The man killing the beard.", "labels": [], "entities": []}, {"text": "(Arabic) A man is shutting a bird.", "labels": [], "entities": []}, {"text": "(Chinese) A man is shooting a bird.", "labels": [], "entities": []}, {"text": "(English) The man shouted the bird.", "labels": [], "entities": []}, {"text": "(Spanish) Figure 1: Example item and responses In previous work (King and Dickinson, 2013), we collected responses to a picture description (PDT) task to approximate interactive behavior.", "labels": [], "entities": [{"text": "picture description (PDT) task", "start_pos": 120, "end_pos": 150, "type": "TASK", "confidence": 0.7378367781639099}]}, {"text": "The current study relies on the same set of responses.", "labels": [], "entities": []}, {"text": "We use a PDT because it helps constrain both form and content , without providing textual prompts that may influence a learner.", "labels": [], "entities": []}, {"text": "Moreover, PDTs area well-established tool in areas of study ranging from SLA to Alzheimer's disease (Ellis, 2000; Forbes-McKay and Venneri, 2005).", "labels": [], "entities": [{"text": "PDTs", "start_pos": 10, "end_pos": 14, "type": "TASK", "confidence": 0.951138436794281}, {"text": "SLA", "start_pos": 73, "end_pos": 76, "type": "TASK", "confidence": 0.9628030061721802}]}, {"text": "The use of visual stimuli also helps model the visual nature of online games.", "labels": [], "entities": []}, {"text": "The stimuli are chosen to elicit relatively unambiguous transitive sentences.", "labels": [], "entities": []}, {"text": "The PDT consisted of 10 items (8 line drawings and 2 photographs) intended to elicit a single sentence each; an example is given in Figure 1.", "labels": [], "entities": []}, {"text": "Participants were asked to view the image and describe the action in a complete 45 sentence, with any tense or aspect appropriate.", "labels": [], "entities": []}, {"text": "25 of the 39 non-native speaker (NNS) participants performed the task in a setting where automatic spell checking was disabled; the remaining 14 performed the task online on their own computers, and although they were instructed to disable spell checking, we have noway of knowing if they did so.", "labels": [], "entities": []}, {"text": "The NNSs were intermediate and upper-level adult English learners in an intensive English as a Second Language program at Indiana University.", "labels": [], "entities": []}, {"text": "This data set contains responses from 53 informants, including native speakers (NSs) (14 NSs, 39 NNSs), fora total of 530 sentences.", "labels": [], "entities": []}, {"text": "The distribution of first languages (L1s) is: 16 Arabic, 7 Chinese, 14 English, 2 Japanese, 4 Korean, 1 Kurdish, 1 Polish, 2 Portuguese, and 6 Spanish.", "labels": [], "entities": []}, {"text": "3.2 Method As in King and Dickinson (2013), our method to obtain a semantic form from a NNS production takes two steps: 1) obtain a syntactic dependency representation from the off-the-shelf Stanford parser (de Marneffe et al., 2006; Klein and Manning, 2003), and 2) obtain a semantic form from the parse, via a small set of handwritten rules.", "labels": [], "entities": []}, {"text": "To illustrate this process, consider (1).", "labels": [], "entities": []}, {"text": "This sentence is passed through the parser to obtain the dependency parse shown in Figure 2.", "labels": [], "entities": []}, {"text": "Based on the presence of the nsubjpass (noun subject, passive) node, the extraction script takes the logical subject from under the agent label, the verb from root, and the logical object from nsubjpass.", "labels": [], "entities": []}, {"text": "This results in the semantic triple shot(man,bird), lemmatized to shoot(man,bird), using the Stanford CoreNLP lemmatizer (Manning et al., 2014).", "labels": [], "entities": [{"text": "Stanford CoreNLP lemmatizer", "start_pos": 93, "end_pos": 120, "type": "DATASET", "confidence": 0.8850928346316019}]}, {"text": "Very little effort is needed: the parser is pre-built; the decision tree is small; and the extraction rules are minimal.", "labels": [], "entities": []}, {"text": "Note, too, that certain relations (e.g., det) are completely ignored in the extraction.", "labels": [], "entities": []}, {"text": "(1) A bird is shot by a man.", "labels": [], "entities": []}, {"text": "vroot A bird is shot by a man root det nsubjpass auxpass agent det Figure 2: The dependency parse of (1) One is able to use little effort in part due to the constraints in the pictures.", "labels": [], "entities": []}, {"text": "For figure 1, for example, the artist, the man in the beret, and the man are all acceptable subjects, whereas if there were multiple men in the picture, the man would not be specific enough.", "labels": [], "entities": []}, {"text": "Evaluation in King and Dickinson (2013) addresses two major questions.", "labels": [], "entities": [{"text": "King and Dickinson (2013)", "start_pos": 14, "end_pos": 39, "type": "DATASET", "confidence": 0.5120906184116999}]}, {"text": "First, how accurate is the extraction of semantic information from potentially innovative sentences?", "labels": [], "entities": [{"text": "extraction of semantic information from potentially innovative sentences", "start_pos": 27, "end_pos": 99, "type": "TASK", "confidence": 0.76839779317379}]}, {"text": "Secondly, how much coverage does one have in a gold standard of semantic forms (triples), to capture the variability in meaning in learner sentences?", "labels": [], "entities": []}, {"text": "We focus more on the first question and again use native speaker semantic forms as a proxy fora gold standard-albeit, limited by mismatches between native and (correct) non-native ways of saying the same thing.", "labels": [], "entities": []}, {"text": "To mitigate this and better seethe effect of spelling correction, much of our evaluation relies on hand-analysis which determines whether a \"reasonable gold standard\" could contain the information (see section 4).", "labels": [], "entities": [{"text": "spelling correction", "start_pos": 45, "end_pos": 64, "type": "TASK", "confidence": 0.9389167726039886}]}, {"text": "Semantic extraction For the purpose of evaluating an extraction system, King and Dickinson (2013) define two major classes of errors.", "labels": [], "entities": [{"text": "Semantic extraction", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7242666929960251}]}, {"text": "The first are triple errors, responses for which the 46 system fails to extract one or more of the desired subject, verb, or object, based on the sentence at hand and without regard to the target content.", "labels": [], "entities": []}, {"text": "Second are content errors, responses for which the system extracts the desired subject, verb and object, but the resulting triple does not accurately describe the image (i.e., is an error of the participant's).", "labels": [], "entities": []}, {"text": "In this paper, we focus on reducing the triple errors, i.e., system errors.", "labels": [], "entities": [{"text": "triple errors", "start_pos": 40, "end_pos": 53, "type": "METRIC", "confidence": 0.8756023347377777}]}, {"text": "For example, the spelling error in (2) leads to a completely incorrect triple.", "labels": [], "entities": [{"text": "spelling error", "start_pos": 17, "end_pos": 31, "type": "METRIC", "confidence": 0.971385657787323}]}, {"text": "We will unpack our error types in section 4.", "labels": [], "entities": []}, {"text": "(2) A man swipped leaves.", "labels": [], "entities": []}, {"text": "\u21d2 leave(swipped,man) Focusing on triple (system) errors, we have obtained 92.3% accuracy on extraction for NNS data and roughly the same for NS data, 92.9% (King and Dickinson, 2013).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 80, "end_pos": 88, "type": "METRIC", "confidence": 0.9992954730987549}]}, {"text": "Furthermore, more than half of the errors for NNSs involve misspellings (4.1% of the total 7.7% of errors).", "labels": [], "entities": []}, {"text": "For a system interacting with learners, spelling errors are thus a high priority (cf. Hovermale, 2008).", "labels": [], "entities": [{"text": "spelling errors", "start_pos": 40, "end_pos": 55, "type": "METRIC", "confidence": 0.8263968825340271}]}, {"text": "Content errors are subcategorized as spelling or meaning errors, depending on whether the resulting triple has spelling errors that do not result in real words-as in (3)-or that do result in real but unintended words and thus convey an inappropriate meaning (e.g., shout(man,bird) instead of shoot(man,bird)).", "labels": [], "entities": []}, {"text": "We will see this distinction play out in the spelling correction techniques in section 5.", "labels": [], "entities": [{"text": "spelling correction", "start_pos": 45, "end_pos": 64, "type": "TASK", "confidence": 0.8874861299991608}]}], "introductionContent": [], "datasetContent": [{"text": "Here we present the results of the modifications detailed above.", "labels": [], "entities": []}, {"text": "At this stage, we are primarily interested in our system's ability to robustly extract evaluable triples, potentially in the face of minor errors.", "labels": [], "entities": []}, {"text": "While we present coverage scores in the following sections-calculating coverage with respect to the particular (and limited) gold standard set of triples-we focus mainly on the effect the modifications have on (Form) error counts.; the default source in parentheses was chosen in cases where neither triple was found (see Section 6.2).", "labels": [], "entities": [{"text": "coverage", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.980924129486084}, {"text": "coverage", "start_pos": 71, "end_pos": 79, "type": "METRIC", "confidence": 0.991370677947998}]}, {"text": "The number of changes between error types, moving from NNS to LM, from LM to J(NNS), and from J(NNS) to J(Oracle).", "labels": [], "entities": []}, {"text": "So far, each sentence form and triple output by the LM pipeline is evaluated alone, without regard to the NNS form or the output of the the original process.", "labels": [], "entities": []}, {"text": "In this section we present a joint analysis, wherein we take both the LM triple and its NNS counterpart; in cases where one of the two triples is found in the set of NS responses, we keep that triple and ignore the other; in cases where neither triple is found, we default to the NNS triple; we refer to this as Joint(NNS).", "labels": [], "entities": []}, {"text": "(A joint analysis defaulting to the LM (Joint(LM)) was also performed, but this resulted in weaker performance, as shown in, and is omitted from the discussion).", "labels": [], "entities": []}, {"text": "The idea behind this joint analysis is simply to give the system the choice between two triples fora single response, using information about the picture's contents (NS responses) to pick one, effectively allowing us to undo any errors introduced by Aspell or the LM.", "labels": [], "entities": []}, {"text": "We again focus primarily on the changes in error counts.", "labels": [], "entities": [{"text": "error counts", "start_pos": 43, "end_pos": 55, "type": "METRIC", "confidence": 0.8820474743843079}]}, {"text": "Unlike the analyses above, however, under this joint evaluation there is an unavoidable possibility for the set of NS responses to affect error counts.", "labels": [], "entities": []}, {"text": "This is because a triple's presence or absence in this set determines which of the two triple versions is considered.", "labels": [], "entities": []}, {"text": "Consider the following constructed example to illustrate this concern.", "labels": [], "entities": []}, {"text": "Under our joint analysis, given an original triple of shout(hunter,bird), which is an error (and of course absent from the list) and an LM triple of shoot(hunter,bird), which is correct but absent from the NS list, we default to the original triple, thus including an error that would have been avoided if the NS list had covered the LM triple.", "labels": [], "entities": [{"text": "NS list", "start_pos": 206, "end_pos": 213, "type": "DATASET", "confidence": 0.8938514888286591}, {"text": "NS list", "start_pos": 310, "end_pos": 317, "type": "DATASET", "confidence": 0.9012016654014587}]}, {"text": "Such cases illustrate the fact that the error types are not equally (un)desirable.", "labels": [], "entities": []}, {"text": "A Gold miss is better than a Form or Triple error for us, because the Gold miss is not a system error at all and could be covered by an improved gold standard.", "labels": [], "entities": [{"text": "Gold miss", "start_pos": 2, "end_pos": 11, "type": "METRIC", "confidence": 0.9619533717632294}, {"text": "Gold miss", "start_pos": 70, "end_pos": 79, "type": "METRIC", "confidence": 0.9133236706256866}]}, {"text": "Likewise, a (NNS) Form error changed to a (LM) Triple error is a partial success, because this means the spelling correction module was successful, while the parser or semantic extractor needs improvement.", "labels": [], "entities": [{"text": "spelling correction", "start_pos": 105, "end_pos": 124, "type": "TASK", "confidence": 0.8670182824134827}]}, {"text": "To address these issues, we perform a Joint(Oracle) experiment (section 6.2.2), in which errors were ranked by preference, from non-error, to Gold miss, to Triple error, to Form error.", "labels": [], "entities": [{"text": "Gold miss", "start_pos": 142, "end_pos": 151, "type": "METRIC", "confidence": 0.9452986717224121}, {"text": "Triple error", "start_pos": 156, "end_pos": 168, "type": "METRIC", "confidence": 0.978960245847702}, {"text": "Form error", "start_pos": 173, "end_pos": 183, "type": "METRIC", "confidence": 0.9239577651023865}]}, {"text": "In cases where neither the NNS nor the LM triple was found and the error types were different, the oracle chooses the preferred error type, minimizing Form errors and maximizing Gold misses.", "labels": [], "entities": [{"text": "Gold misses", "start_pos": 178, "end_pos": 189, "type": "METRIC", "confidence": 0.9627916812896729}]}, {"text": "The results of this experiment give a better approximation of the potential of the current system given an ideal set of triples covering the content of the picture (which the NS responses serve as a proxy for).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Errors types and coverage for the full set of responses (390 sentences). Joint indicates  a joint analysis of both sources (NNS & LM", "labels": [], "entities": [{"text": "coverage", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9852525591850281}, {"text": "NNS & LM", "start_pos": 134, "end_pos": 142, "type": "DATASET", "confidence": 0.863957405090332}]}]}