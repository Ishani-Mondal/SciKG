{"title": [{"text": "Exploring Mental Lexicon in an Efficient and Economic Way: Crowdsourcing Method for Linguistic Experiments", "labels": [], "entities": [{"text": "Exploring Mental Lexicon", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.7426192164421082}]}], "abstractContent": [{"text": "Mental lexicon plays a central role inhuman language competence and inspires the creation of new lexical resources.", "labels": [], "entities": []}, {"text": "The traditional linguistic experiment method which is used to explore mental lexicon has some disadvantages.", "labels": [], "entities": []}, {"text": "Crowdsourcing has become a promising method to conduct linguistic experiments which enables us to explore mental lexicon in an efficient and economic way.", "labels": [], "entities": []}, {"text": "We focus on the feasibility and quality control issues of conducting Chinese linguistic experiments to collect Chinese word segmentation and semantic transparency data on the international crowdsourcing platforms Amazon Mechanical Turk and Crowdflower.", "labels": [], "entities": [{"text": "collect Chinese word segmentation and semantic transparency", "start_pos": 103, "end_pos": 162, "type": "TASK", "confidence": 0.6761022550719125}]}, {"text": "Through this work, a framework for crowdsourcing linguistic experiments is proposed.", "labels": [], "entities": []}], "introductionContent": [{"text": "Mental lexicon as a theoretical construct has two important implications.", "labels": [], "entities": []}, {"text": "For an individual, it is where all the grammatical and world information is stored and organized to enable speech.", "labels": [], "entities": []}, {"text": "For a group of speakers of the same language, however, the mental lexicon is a shared knowledge structure allowing speakers to process and understand what each other said.", "labels": [], "entities": []}, {"text": "WordNets, for example the English WordNet and the Chinese WordNet (CWN) (, and ontologies, for example the Suggested Upper Merged Ontology (SUMO)) and the Sinica BOW (, have been proposed as a representational framework for this shared mental lexicon; and psycho-and neuro-linguistic experiments have been designed to explore how individuals access their mental lexicon.", "labels": [], "entities": [{"text": "English WordNet", "start_pos": 26, "end_pos": 41, "type": "DATASET", "confidence": 0.830432802438736}, {"text": "Chinese WordNet (CWN)", "start_pos": 50, "end_pos": 71, "type": "DATASET", "confidence": 0.7654489636421203}]}, {"text": "However, the question of whether there is a shared principle or strategy of mental lexicon by all speakers of the same language was never seriously studied as the cognitive experimental paradigm does not allow manipulation of a large number of subjects simultaneously.", "labels": [], "entities": []}, {"text": "In this paper, we explore the possibility of conducting lexical access related experiments through crowdsourcing.", "labels": [], "entities": []}, {"text": "With the crowdsourcing experiments, we intend to ask specific question about the share strategy of determination of lexical units, as well as determination of semantic transparencies, two issues that would have direct implications of how individuals access their mental lexicon.", "labels": [], "entities": []}, {"text": "Many scholars discuss applying crowdsourcing method to language resource construction recent years (.", "labels": [], "entities": [{"text": "language resource construction", "start_pos": 55, "end_pos": 85, "type": "TASK", "confidence": 0.6376447180906931}]}, {"text": "Crowdsourcing has been proved to bean efficient tool to build lexical resources, for example, Wiktionary, whose goal is to become the free online dictionary for all the words in all languages; presents another example which creates the Turk Bootstrap Word Sense Inventory for 397 frequent nouns from scratch using Amazon Mechanical Turk.", "labels": [], "entities": []}, {"text": "And there is more and more literature focusing on conducting experiments on crowdsourcing platforms (.", "labels": [], "entities": []}, {"text": "Using crowdsourcing method, it is easier to access highly diverse and huge amount of participants, so it is possible to obtain more representative language behavioral data.", "labels": [], "entities": []}, {"text": "The anonymous nature of crowdsourcing makes the participants more open to contribute sensitive data.", "labels": [], "entities": []}, {"text": "And Crowdsourcing experiments are usually much faster and cheaper than laboratory experiments which enables \" faster iteration between developing theory and executing experiments\".", "labels": [], "entities": []}, {"text": "It can be a promising tool to explore mental lexicon in an efficient and economic way.", "labels": [], "entities": []}, {"text": "MTurk and Crowdflower are perhaps the two most important MTurk-like crowdsourcing platforms.", "labels": [], "entities": [{"text": "MTurk", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9775246381759644}]}, {"text": "MTurk is the platform appears most frequently in the literature, so popular that it represents a major genre of crowdsourcing and its name has become the name of that genre.", "labels": [], "entities": [{"text": "MTurk", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9067726135253906}]}, {"text": "Crowdflower is a rapid developing platform and is drawing more and more attention.", "labels": [], "entities": [{"text": "Crowdflower", "start_pos": 0, "end_pos": 11, "type": "DATASET", "confidence": 0.9174116253852844}]}, {"text": "Although they are both MTurk-like platforms, they differ from each other.", "labels": [], "entities": []}, {"text": "On the MTurk platform, invalid responses submitted can be manually rejected which is a very convenient quality control method; however Crowdflower has a much larger worker pool than MTurk.", "labels": [], "entities": [{"text": "MTurk platform", "start_pos": 7, "end_pos": 21, "type": "DATASET", "confidence": 0.8876362144947052}, {"text": "MTurk", "start_pos": 182, "end_pos": 187, "type": "DATASET", "confidence": 0.9196911454200745}]}, {"text": "Since MTurk is one channel of Crowdflower and Crowdflower can access the worker pool of MTurk 1 , besides MTurk, Crowdflower has several dozens of other channels to which it can distribute tasks.", "labels": [], "entities": [{"text": "MTurk", "start_pos": 6, "end_pos": 11, "type": "DATASET", "confidence": 0.9101076722145081}, {"text": "MTurk", "start_pos": 106, "end_pos": 111, "type": "DATASET", "confidence": 0.8822230100631714}]}, {"text": "More importantly, Crowdflower is more accessible to requesters outside the U.S. (MTurk does not support requesters outside the U.S. by now).", "labels": [], "entities": [{"text": "MTurk", "start_pos": 81, "end_pos": 86, "type": "DATASET", "confidence": 0.9240302443504333}]}, {"text": "Crowdflower basically doesn't support manual rejection of invalid responses but it integrates an effective quality control method named Test Questions which uses predefined gold standard questions to measure the quality of contributions of workers and screens low quality workers automatically in order to produce high quality data.", "labels": [], "entities": [{"text": "Crowdflower", "start_pos": 0, "end_pos": 11, "type": "DATASET", "confidence": 0.9015200734138489}]}, {"text": "Unfortunately, it is not suitable to our task, for it requires multiple submissions form a worker.", "labels": [], "entities": []}, {"text": "Neither MTurk nor Crowdflower is a native Chinese crowdsourcing platform, so we can suppose that native Chinese speakers can only occupy a small proposition in their worker pools, in this case, a larger worker pool means higher possibility of successful data collection.", "labels": [], "entities": [{"text": "MTurk", "start_pos": 8, "end_pos": 13, "type": "DATASET", "confidence": 0.8894690275192261}]}, {"text": "We have two objectives in this study: (1) to check if it is feasible to conduct Chinese language experiments to collect Chinese word segmentation and semantic transparency) data which can be used to explore the mental lexicon of Chinese speakers on international crowdsourcing platforms, such as Amazon Mechanical Turk (MTurk) and Crowdflower; (2) to identify and solve some quality control and experimental design issues in order to obtain high quality data and to establish a preliminary framework for crowdsourcing linguistic experiments.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 120, "end_pos": 145, "type": "TASK", "confidence": 0.7088518341382345}]}], "datasetContent": [{"text": "The experiment was divided into two stages, and there was a time interval of about two months between them.", "labels": [], "entities": []}, {"text": "Based on the initial calibration tests, the experiment was conducted to test the feasibility of collecting Chinese language data on international crowdsourcing platforms and to identify and solve some quality control and experimental design issues.", "labels": [], "entities": []}, {"text": "Our original plan was to conduct one experiment to collect a sample of 200 responses.", "labels": [], "entities": []}, {"text": "But after we had collected 135 responses, we found a serious spammer problem which must be properly solved otherwise the data quality would be greatly threatened and the feasibility of our task would be questionable.", "labels": [], "entities": []}, {"text": "Meanwhile, we found the amounts of responses from the region of mainland China and the channel \"bitcoinget\" were unexpectedly large, we doubted that it might result from the frequent media reports on bitcoin at that time in mainland China.", "labels": [], "entities": []}, {"text": "When the media reports ebbed, would the experiment be replicable?", "labels": [], "entities": []}, {"text": "Thus we thought it's necessary to pause the experiment to seek a solution for the spammer problem and to evade the strong external factor of media report.", "labels": [], "entities": [{"text": "spammer problem", "start_pos": 82, "end_pos": 97, "type": "TASK", "confidence": 0.9271074831485748}]}, {"text": "Thus the experiment was divided into two stages.", "labels": [], "entities": []}, {"text": "We chose to pause the experiment instead of stopping it so that the participants who had already taken part in the experiment (Stage 1) could not take part again when the experiment was resumed (Stage 2).", "labels": [], "entities": []}, {"text": "The experiment was resumed after two months, with a spammer monitor program based on the API of Crowdflower which could detect and combat spammers automatically.", "labels": [], "entities": []}, {"text": "Other aspects of the experiment remained unchanged.", "labels": [], "entities": []}, {"text": "The Stage 2 experiment could be used to check the experimental repeatability and to solve the spammer problem found in Stage 1.", "labels": [], "entities": [{"text": "spammer", "start_pos": 94, "end_pos": 101, "type": "TASK", "confidence": 0.9514105916023254}]}, {"text": "Experiment control measures are used to ensure the validity of participants and their participations.", "labels": [], "entities": [{"text": "validity", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.9651987552642822}]}, {"text": "Because we cannot access the real identities of the participants, we can only use some indirect methods which are not completely reliable but can satisfy our demands at large.", "labels": [], "entities": []}, {"text": "Firstly, all the participants must be native Chinese speakers.", "labels": [], "entities": []}, {"text": "The questionnaire was displayed in Chinese characters which can be a natural barrier to non-native Chinese speakers.", "labels": [], "entities": []}, {"text": "Ten screening questions are designed in the questionnaire to test the language backgrounds of the participants.", "labels": [], "entities": []}, {"text": "By the above measures, we can effectively discriminate native Chinese speakers from non-native ones.", "labels": [], "entities": []}, {"text": "Chinese learners are not a major threat due to their small amount and low overall Chinese fluency.", "labels": [], "entities": []}, {"text": "We invited two Chinese learners to test our questionnaire; neither of them could finish it.", "labels": [], "entities": []}, {"text": "Secondly, one participant can only submit one response.", "labels": [], "entities": []}, {"text": "We used the methods which are already explained in 2.1: one account can only submit one response; one IP address can only submit one response.", "labels": [], "entities": []}, {"text": "Stage 1 of the experiment lasted for about two days, with multiple manual pauses in between to resist spamming attempts.", "labels": [], "entities": [{"text": "spamming", "start_pos": 102, "end_pos": 110, "type": "TASK", "confidence": 0.9749166369438171}]}, {"text": "A total of 135 responses were received, out of which 88 (65.19%) were valid and 47 (34.81%) were invalid according to the criteria stated above.", "labels": [], "entities": []}, {"text": "Among the valid responses, 81 (92.05%) were contributed by participants who claimed to be from Mainland China and only 7 (7.95%) by participants from Hong Kong.", "labels": [], "entities": []}, {"text": "38 out of the 47 invalid responses (80.85%) were probably produced by spammers because their completion times were very short and/or the validation measures were bypassed.", "labels": [], "entities": []}, {"text": "The 3 largest source channels of valid responses were bitcoinget (n=52, 59.09%), prodege (n=11, 12.50%) and getpaid (n=7, 7.95%), while the 3 largest source regions (based on the IP addresses) were Mainland China (n=54, 61.36%), USA (n=14, 15.91%) and Canada (n=6, 6.82%).", "labels": [], "entities": [{"text": "USA", "start_pos": 229, "end_pos": 232, "type": "DATASET", "confidence": 0.923467218875885}]}, {"text": "Stage 2 of the experiment lasted for about 4 days also with several breaks.", "labels": [], "entities": []}, {"text": "65 responses were received in Stage 2, among which 54 (83.08%) were valid and 11 (11.92%) were invalid.", "labels": [], "entities": []}, {"text": "46 (85.19%) of the valid responses were contributed by participants from Mainland China and 8 (14.81%) by participants from Hong Kong.", "labels": [], "entities": []}, {"text": "6 (54.55%) of the invalid responses were probably produced by spammers.", "labels": [], "entities": []}, {"text": "The main contributing source channels and regions of valid data in Stage 2 were slightly different from Stage 1.", "labels": [], "entities": []}, {"text": "Top 3 source channels were prodege (n=25, 46.30%), bitcoinget (n=7, 12.96%) and instagc (n=5, 9.26 %); top 3 source regions were Canada (n=22, 40.74%), USA (n=15, 27.78%) and Mainland China (n=11, 20.37%).", "labels": [], "entities": []}, {"text": "Despite the different distributions of source channels and regions, the data obtained from Stage 1 and Stage 2 were highly similar, suggesting that the experiment was highly replicable.", "labels": [], "entities": []}, {"text": "In total, we obtained 200 responses in this experiment, among which 142 (71%) were valid.", "labels": [], "entities": []}, {"text": "The valid responses showed high consistency in their answers to the language tasks in Part 2 and Part 3.", "labels": [], "entities": [{"text": "consistency", "start_pos": 32, "end_pos": 43, "type": "METRIC", "confidence": 0.9966275095939636}]}, {"text": "For example, among the 127 valid responses from Mainland China, the answers to the word segmentation questions in Part 2 had an average consistency 2 of 74.30% (SD=12.94%), while the semantic similarity ratings in Part 3 had an average consistency of 58.46% (SD=21.97%).", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 83, "end_pos": 100, "type": "TASK", "confidence": 0.7173469215631485}, {"text": "consistency 2", "start_pos": 136, "end_pos": 149, "type": "METRIC", "confidence": 0.9793479740619659}, {"text": "consistency", "start_pos": 236, "end_pos": 247, "type": "METRIC", "confidence": 0.951786458492279}]}, {"text": "Majority-voted answers and ratings were verified by a team of trained linguists as the most likely segmentations/ratings of the given linguistic materials, while the less popular answers were also verified as possible or reasonable alternatives.", "labels": [], "entities": []}, {"text": "These results suggest that the language behavioral data acquired in this experiment, when pruned of invalid responses, were largely consistent with expectations for native language users' judgment.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. We can see that the consistency  is high, however the majority-voted result \"\u53ea\u6709/\u4f9d\u9760/\u7fa4\u4f17/\u624d\u80fd/\u505a\u597d/\u5de5\u4f5c\" is different from the the- oretical segmentation result. Most participants treat the slice \"\u624d\u80fd\" as one word instead of two words  and the same thing happened to the slice \"\u505a\u597d\". Speakers' intuition can be different from theoretical  analysis: this is an important clue to investigate the representation of Chinese words in the mental lexicon  of Chinese speakers.", "labels": [], "entities": [{"text": "consistency", "start_pos": 30, "end_pos": 41, "type": "METRIC", "confidence": 0.9991415739059448}]}, {"text": " Table 1: Chinese Word Segmentation Data Example", "labels": [], "entities": [{"text": "Chinese Word Segmentation Data Example", "start_pos": 10, "end_pos": 48, "type": "TASK", "confidence": 0.6639041423797607}]}, {"text": " Table 2: Semantic Similarity Rating Data Example", "labels": [], "entities": []}]}