{"title": [{"text": "BEER: BEtter Evaluation as Ranking", "labels": [], "entities": [{"text": "BEER", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9792400598526001}, {"text": "BEtter", "start_pos": 6, "end_pos": 12, "type": "METRIC", "confidence": 0.9782187342643738}, {"text": "Evaluation", "start_pos": 13, "end_pos": 23, "type": "METRIC", "confidence": 0.5529097318649292}]}], "abstractContent": [{"text": "We present the UvA-ILLC submission of the BEER metric to WMT 14 metrics task.", "labels": [], "entities": [{"text": "BEER", "start_pos": 42, "end_pos": 46, "type": "METRIC", "confidence": 0.978631854057312}, {"text": "WMT 14 metrics task", "start_pos": 57, "end_pos": 76, "type": "DATASET", "confidence": 0.6665680408477783}]}, {"text": "BEER is a sentence level metric that can incorporate a large number of features combined in a linear model.", "labels": [], "entities": [{"text": "BEER", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9716453552246094}]}, {"text": "Novel contributions are (1) efficient tuning of a large number of features for maximizing correlation with human system ranking, and (2) novel features that give smoother sentence level scores.", "labels": [], "entities": []}], "introductionContent": [{"text": "The quality of sentence level (also called segment level) evaluation metrics in machine translation is often considered inferior to the quality of corpus (or system) level metrics.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 80, "end_pos": 99, "type": "TASK", "confidence": 0.7166772782802582}]}, {"text": "Yet, a sentence level metrics has important advantages as it: 1.", "labels": [], "entities": []}, {"text": "provides an informative score to individual translations 2. is assumed by MT tuning algorithms).", "labels": [], "entities": [{"text": "MT tuning", "start_pos": 74, "end_pos": 83, "type": "TASK", "confidence": 0.5914744138717651}]}, {"text": "3. facilitates easier statistical testing using sign test or t-test We think that the root cause for most of the difficulty in creating a good sentence level metric is the sparseness of the features often used.", "labels": [], "entities": []}, {"text": "Consider the n-gram counting metrics (BLEU ()): counts of higher order n-grams are usually rather small, if not zero, when counted at the individual sentence level.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 38, "end_pos": 42, "type": "METRIC", "confidence": 0.9763884544372559}]}, {"text": "Metrics based on such counts are brittle at the sentence level even when they might be good at the corpus level.", "labels": [], "entities": []}, {"text": "Ideally we should have features of varying granularity that we can optimize on the actual evaluation task: relative ranking of system outputs.", "labels": [], "entities": []}, {"text": "Therefore, in this paper we explore two kinds of less sparse features: Character n-grams are features at the sub-word level that provide evidence for translation adequacy -for example whether the stem is correctly translated, Abstract ordering patterns found in tree factorizations of permutations into Permutation Trees (PETs) (, including non-lexical alignment patterns.", "labels": [], "entities": []}, {"text": "The BEER metric combines features of both kinds (presented in Section 2).", "labels": [], "entities": [{"text": "BEER", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.9819481372833252}]}, {"text": "With the growing number of adequacy and ordering features we need a model that facilitates efficient training.", "labels": [], "entities": []}, {"text": "We would like to train for optimal Kendall \u03c4 correlation with rankings by human evaluators.", "labels": [], "entities": [{"text": "Kendall \u03c4 correlation", "start_pos": 35, "end_pos": 56, "type": "METRIC", "confidence": 0.7845615347226461}]}, {"text": "The models in the literature tackle this problem by 1.", "labels": [], "entities": []}, {"text": "training for another similar objective -e.g., tuning for absolute adequacy and fluency scores instead on rankings, or 2.", "labels": [], "entities": []}, {"text": "training for rankings directly but with metaheuristic approaches like hill-climbing, or 3.", "labels": [], "entities": []}, {"text": "training for pairwise rankings using learningto-rank techniques Approach (1) has two disadvantages.", "labels": [], "entities": [{"text": "Approach", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.9837788939476013}]}, {"text": "One is the inconsistency between the training and the testing objectives.", "labels": [], "entities": []}, {"text": "The other, is that absolute rankings are not reliable enough because humans are better at giving relative than absolute judgments (see WMT manual evaluations).", "labels": [], "entities": [{"text": "WMT manual", "start_pos": 135, "end_pos": 145, "type": "TASK", "confidence": 0.5765058398246765}]}, {"text": "Approach (2) does not allow integrating a large number of features which makes it less attractive.", "labels": [], "entities": [{"text": "Approach", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9472563862800598}]}, {"text": "Approach (3) allows integration of a large number of features whose weights could be determined in an elegant machine learning framework.", "labels": [], "entities": [{"text": "Approach", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9077506065368652}]}, {"text": "The output of learning in this approach can be either a function that ranks all hypotheses directly (global ranking model) or a function that assigns a score to each hypothesis individually which can be used for ranking (local ranking model).", "labels": [], "entities": []}, {"text": "Local ranking models are preferable because they provide absolute distance between hypotheses like most existing evaluation metrics.", "labels": [], "entities": []}, {"text": "In this paper we follow the learning-to-rank approach which produces a local ranking model in a similar way to PRO MT systems tuning (Hopkins and May, 2011).", "labels": [], "entities": [{"text": "PRO MT systems tuning", "start_pos": 111, "end_pos": 132, "type": "TASK", "confidence": 0.8055796176195145}]}], "datasetContent": [{"text": "We conducted experiments for the metric which in total has 33 features (27 for adequacy and 6 for word order).", "labels": [], "entities": []}, {"text": "Some of the features in the metric depend on external sources of information.", "labels": [], "entities": []}, {"text": "For function words we use listings that are created for many languages and are distributed with METEOR toolkit).", "labels": [], "entities": []}, {"text": "The permutations are extracted using ME-TEOR aligner which does fuzzy matching using resources such as WordNet, paraphrase tables and stemmers.", "labels": [], "entities": [{"text": "fuzzy matching", "start_pos": 64, "end_pos": 78, "type": "TASK", "confidence": 0.7138805389404297}, {"text": "WordNet", "start_pos": 103, "end_pos": 110, "type": "DATASET", "confidence": 0.9574381709098816}]}, {"text": "METEOR is not used for any scoring, but only for aligning hypothesis and reference.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.7245676517486572}]}, {"text": "For training we used the data from WMT13 human evaluation of the systems.", "labels": [], "entities": [{"text": "WMT13", "start_pos": 35, "end_pos": 40, "type": "DATASET", "confidence": 0.8544498085975647}]}, {"text": "Before evaluation, all data was lowercased and tokenized.", "labels": [], "entities": []}, {"text": "After preprocessing, we extract training examples for our binary classifier.", "labels": [], "entities": []}, {"text": "The number of non-tied human judgments per language pair are shown in.", "labels": [], "entities": []}, {"text": "Each human judgment produces two training instances : one positive and one negative.", "labels": [], "entities": []}, {"text": "For learning we use regression implementation in the Vowpal Wabbit toolkit . Tuned metric is tested on the human evaluated data from WMT12 (Callison-Burch et al., 2012) for correlation with the human judgment.", "labels": [], "entities": [{"text": "Vowpal Wabbit toolkit", "start_pos": 53, "end_pos": 74, "type": "DATASET", "confidence": 0.9001977642377218}, {"text": "human evaluated data from WMT12 (Callison-Burch et al., 2012)", "start_pos": 107, "end_pos": 168, "type": "DATASET", "confidence": 0.8164052565892538}]}, {"text": "As baseline we used one of the best ranked metrics on the sentence level evaluations from previous WMT tasks -METEOR (  Kendall \u03c4 from the WMT12 (Callison-Burch et al., 2012) so the scores could be compared with other metrics on the same dataset that were reported in the proceedings of that year).", "labels": [], "entities": [{"text": "WMT tasks", "start_pos": 99, "end_pos": 108, "type": "TASK", "confidence": 0.7772390246391296}, {"text": "METEOR", "start_pos": 110, "end_pos": 116, "type": "METRIC", "confidence": 0.9890289306640625}, {"text": "WMT12 (Callison-Burch et al., 2012)", "start_pos": 139, "end_pos": 174, "type": "DATASET", "confidence": 0.861232690513134}]}, {"text": "The results show that BEER with and without paraphrase support outperforms METEOR (and almost all other metrics on WMT12 metrics task) on the majority of language pairs.", "labels": [], "entities": [{"text": "BEER", "start_pos": 22, "end_pos": 26, "type": "METRIC", "confidence": 0.9989390969276428}, {"text": "METEOR", "start_pos": 75, "end_pos": 81, "type": "METRIC", "confidence": 0.9745320081710815}, {"text": "WMT12 metrics task", "start_pos": 115, "end_pos": 133, "type": "DATASET", "confidence": 0.7378017703692118}]}, {"text": "Paraphrase support matters mostly when the target language is English, but even in language pairs where it does not help significantly it can be useful.", "labels": [], "entities": [{"text": "Paraphrase support", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8623269200325012}]}, {"text": "In you can seethe results of top 5 ranked metrics on the segment level evaluation task of WMT14.", "labels": [], "entities": [{"text": "WMT14", "start_pos": 90, "end_pos": 95, "type": "DATASET", "confidence": 0.820190966129303}]}, {"text": "In 5 out of 10 language pairs BEER was ranked the first, on 4 the second best and on one third best metric.", "labels": [], "entities": [{"text": "BEER", "start_pos": 30, "end_pos": 34, "type": "METRIC", "confidence": 0.959791898727417}]}, {"text": "The cases where it failed to win the first place are: \u2022 against DISCOTK-PARTY-TUNED on * -English except Hindi-English.", "labels": [], "entities": []}, {"text": "DISCOTK-PARTY-TUNED participated only in evaluation of English which suggests that it uses some language specific components which is not the case with the current version of BEER \u2022 against METEOR and AMBER on EnglishHindi.", "labels": [], "entities": [{"text": "DISCOTK-PARTY-TUNED", "start_pos": 0, "end_pos": 19, "type": "DATASET", "confidence": 0.8107585310935974}, {"text": "BEER", "start_pos": 175, "end_pos": 179, "type": "METRIC", "confidence": 0.9951053857803345}, {"text": "METEOR", "start_pos": 190, "end_pos": 196, "type": "METRIC", "confidence": 0.616260290145874}]}, {"text": "The reason for this is simply that we  From metrics that participated in all language pairs on the sentence level on average BEER has the best correlation with the human judgment.", "labels": [], "entities": [{"text": "BEER", "start_pos": 125, "end_pos": 129, "type": "METRIC", "confidence": 0.9993667006492615}]}], "tableCaptions": [{"text": " Table 1: Number of human judgments in WMT13", "labels": [], "entities": [{"text": "WMT13", "start_pos": 39, "end_pos": 44, "type": "TASK", "confidence": 0.5682377219200134}]}, {"text": " Table 2: Kendall \u03c4 correleation on WMT12 data", "labels": [], "entities": [{"text": "WMT12 data", "start_pos": 36, "end_pos": 46, "type": "DATASET", "confidence": 0.9389521181583405}]}, {"text": " Table 3: Kendall \u03c4 correlations on the WMT14 hu- man judgements when translating out of English.", "labels": [], "entities": [{"text": "Kendall \u03c4 correlations", "start_pos": 10, "end_pos": 32, "type": "METRIC", "confidence": 0.7950095931688944}, {"text": "WMT14 hu- man judgements", "start_pos": 40, "end_pos": 64, "type": "DATASET", "confidence": 0.8273451328277588}]}, {"text": " Table 4: Kendall \u03c4 correlations on the WMT14  human judgements when translating into English.", "labels": [], "entities": [{"text": "Kendall \u03c4 correlations", "start_pos": 10, "end_pos": 32, "type": "METRIC", "confidence": 0.7595658302307129}, {"text": "WMT14  human judgements", "start_pos": 40, "end_pos": 63, "type": "DATASET", "confidence": 0.8055700659751892}]}]}