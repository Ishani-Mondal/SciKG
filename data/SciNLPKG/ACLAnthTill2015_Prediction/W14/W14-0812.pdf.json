{"title": [{"text": "Extraction of Nominal Multiword Expressions in French", "labels": [], "entities": [{"text": "Extraction of Nominal Multiword Expressions", "start_pos": 0, "end_pos": 43, "type": "TASK", "confidence": 0.8675848960876464}]}], "abstractContent": [{"text": "Multiword expressions (MWEs) can be extracted automatically from large corpora using association measures, and tools like mwetoolkit allow researchers to generate training data for MWE extraction given a tagged corpus and a lexicon.", "labels": [], "entities": [{"text": "MWE extraction", "start_pos": 181, "end_pos": 195, "type": "TASK", "confidence": 0.9757194221019745}]}, {"text": "We use mwe-toolkit on a sample of the French Europarl corpus together with the French lexicon Dela, and use Weka to train classifiers for MWE extraction on the generated training data.", "labels": [], "entities": [{"text": "French Europarl corpus", "start_pos": 38, "end_pos": 60, "type": "DATASET", "confidence": 0.8778262535730997}, {"text": "MWE extraction", "start_pos": 138, "end_pos": 152, "type": "TASK", "confidence": 0.9761020243167877}]}, {"text": "A manual evaluation shows that the classifiers achieve 60-75% precision and that about half of the MWEs found are novel and not listed in the lexicon.", "labels": [], "entities": [{"text": "precision", "start_pos": 62, "end_pos": 71, "type": "METRIC", "confidence": 0.9992565512657166}]}, {"text": "We also investigate the impact of the patterns used to generate the training data and find that this can affect the trade-off between precision and novelty.", "labels": [], "entities": [{"text": "precision", "start_pos": 134, "end_pos": 143, "type": "METRIC", "confidence": 0.9989781379699707}]}], "introductionContent": [{"text": "In alphabetic languages, words are delimited by spaces.", "labels": [], "entities": []}, {"text": "Some words can combine to create anew unit of meaning that we calla multiword expression (MWE).", "labels": [], "entities": []}, {"text": "However, MWEs such as kick the bucket must be distinguished from free combinations of words such as kick the ball.", "labels": [], "entities": [{"text": "MWEs", "start_pos": 9, "end_pos": 13, "type": "TASK", "confidence": 0.9130178689956665}]}, {"text": "A sequence of several words is an MWE if \"at least one of its syntactic, distributional or semantic properties cannot be deduced from the properties of its component\" (.", "labels": [], "entities": []}, {"text": "So how can we extract them?", "labels": [], "entities": []}, {"text": "Statistical association measures have long been used for MWE extraction, and by training supervised classifiers that use association measures as features we can further improve the quality of the extraction process.", "labels": [], "entities": [{"text": "MWE extraction", "start_pos": 57, "end_pos": 71, "type": "TASK", "confidence": 0.9948395490646362}]}, {"text": "However, supervised machine learning requires annotated data, which creates a bottleneck in the absence of large corpora annotated for MWEs.", "labels": [], "entities": []}, {"text": "In order to circumvent this bottleneck, mwetoolkit generates training instances by first extracting candidates that fit a certain part-of-speech pattern, such as Noun-Noun or Noun-Adjective, and then marking the candidates as positive or negative instances depending on whether they can be found in a given lexicon or not.", "labels": [], "entities": []}, {"text": "Such a training set will presumably not contain any false positives (that is, candidates marked as positive instances that are not real MWEs), but depending on the coverage of the lexicon there will be a smaller or larger proportion of false negatives.", "labels": [], "entities": []}, {"text": "The question is what quality can be obtained using such a noisy training set.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, we cannot find the answer for French in literature.", "labels": [], "entities": []}, {"text": "Indeed,  compares the performance of mwetoolkit with another toolkit on English and French corpora, but they never use the data generated by mwetoolkit to train a model.", "labels": [], "entities": []}, {"text": "In contrast, Zilio et al.", "labels": [], "entities": []}, {"text": "(2011) make a study involving training a model but use it only on English and use extra lexical resources to complement the machine learning method, so their study does not focus just on classifier evaluation.", "labels": [], "entities": []}, {"text": "This paper presents the first evaluation of mwetoolkit on French together with two resources very commonly used by the French NLP community: the tagger TreeTagger ( and the dictionary Dela.", "labels": [], "entities": []}, {"text": "1 Training and test data are taken from the French Europarl corpus ( and classifiers are trained using the Weka machine learning toolkit ().", "labels": [], "entities": [{"text": "French Europarl corpus", "start_pos": 44, "end_pos": 66, "type": "DATASET", "confidence": 0.8718229929606119}, {"text": "Weka machine learning toolkit", "start_pos": 107, "end_pos": 136, "type": "DATASET", "confidence": 0.871019572019577}]}, {"text": "The primary goal is to evaluate what level of precision can be achieved for nominal MWEs, using a manual evaluation of MWEs extracted, and to what extent the MWEs extracted are novel and can be used to enrich the lexicon.", "labels": [], "entities": [{"text": "precision", "start_pos": 46, "end_pos": 55, "type": "METRIC", "confidence": 0.995384156703949}]}, {"text": "In addition, we will investigate what effect the choice of part-of-speech patterns used to generate the training data has on precision and novelty.", "labels": [], "entities": [{"text": "precision", "start_pos": 125, "end_pos": 134, "type": "METRIC", "confidence": 0.9993482232093811}, {"text": "novelty", "start_pos": 139, "end_pos": 146, "type": "METRIC", "confidence": 0.9528146982192993}]}, {"text": "Our results indicate that classifiers achieve precision in the 60-75% range and that about half of the MWEs found are novel ones.", "labels": [], "entities": [{"text": "precision", "start_pos": 46, "end_pos": 55, "type": "METRIC", "confidence": 0.9994114637374878}]}, {"text": "In addition, it seems that the choice of patterns used to generate the training data can affect the tradeoff between precision and novelty.", "labels": [], "entities": [{"text": "precision", "start_pos": 117, "end_pos": 126, "type": "METRIC", "confidence": 0.9990801811218262}]}], "datasetContent": [{"text": "The data automatically annotated by mwetoolkit could be used for training, but to properly evaluate the precision of MWE extraction on new data and not penalize the system for 'false positives' that are due to lack of coverage of the lexicon, we needed to perform a manual annotation.", "labels": [], "entities": [{"text": "precision", "start_pos": 104, "end_pos": 113, "type": "METRIC", "confidence": 0.9987687468528748}, {"text": "MWE extraction", "start_pos": 117, "end_pos": 131, "type": "TASK", "confidence": 0.9702242016792297}]}, {"text": "To do so, we randomly picked 100 candidates annotated as True by each model (regardless if they were in the Delac or not).", "labels": [], "entities": []}, {"text": "We then annotated all such candidates as True if they were found in Delac (without further inspection) and otherwise classified them manually following the definition of and the intuition of a native French speaker.", "labels": [], "entities": []}, {"text": "The results are in  As we see in, the experiment reveals a precision ranging from almost 60% up to 74%.", "labels": [], "entities": [{"text": "precision", "start_pos": 59, "end_pos": 68, "type": "METRIC", "confidence": 0.9995703101158142}]}, {"text": "The results of our comparative manual annotation indicate that the model trained on NN candidates has the capacity to find more MWEs not listed in our lexicon (41 out of 59) even if it is the least precise model.", "labels": [], "entities": []}, {"text": "On the other hand, we notice that the model based on Noun-Adjective patterns is more precise but at the same time extracts fewer MWEs that are not already in the lexicon (34 out of 74).", "labels": [], "entities": []}, {"text": "Our mixed model confirms these two tendencies with a performance in between (38 new MWEs out of 66).", "labels": [], "entities": []}, {"text": "Thus, the method appears to be sensitive to the patterns used for training.", "labels": [], "entities": []}, {"text": "We notice during evaluation different kinds of MWEs that are successfully extracted by models but that are not listed in the Delac.", "labels": [], "entities": [{"text": "Delac", "start_pos": 125, "end_pos": 130, "type": "DATASET", "confidence": 0.9659676551818848}]}, {"text": "Most of them are the MWEs specific to Europarl (e.g., 'dimension communautaire', 'l\u00e9gislation europ\u00e9enne' 2 ).", "labels": [], "entities": [{"text": "Europarl", "start_pos": 38, "end_pos": 46, "type": "DATASET", "confidence": 0.9838470816612244}]}, {"text": "Another category are those MWEs that became 'community scale', 'European legislation' popular in the French language after the years 2000's and therefore could not be included in the Delac, released in 1997.", "labels": [], "entities": [{"text": "Delac, released in 1997", "start_pos": 183, "end_pos": 206, "type": "DATASET", "confidence": 0.9645951747894287}]}, {"text": "Indeed by reading the first paragraph of the French version of Europarl we notice that the texts have been written after 1999.", "labels": [], "entities": [{"text": "French version of Europarl", "start_pos": 45, "end_pos": 71, "type": "DATASET", "confidence": 0.8705554902553558}]}, {"text": "Of course, they are not the majority of the successfully extracted MWEs but we still manage to find up to 3 of them in a sample of 100 that we checked ('d\u00e9veloppement durable', 'radiophonie num\u00e9rique', 'site internet' 3 ).", "labels": [], "entities": []}, {"text": "Furthermore the corpus in itself is already more than ten years old, so in a text of 2014 we can expect to find even more of them.", "labels": [], "entities": []}, {"text": "Finally, there are MWEs that are not in French (e.g., 'Partido popular'), these, however, did not appear systematically in our samples.", "labels": [], "entities": []}, {"text": "It is tricky to learn statistical properties of MWEs when, actually, we do not have all the information necessary for extracting the MWEs in the corpus.", "labels": [], "entities": []}, {"text": "Indeed, for this purpose the corpus should ideally be read and annotated by humans.", "labels": [], "entities": []}, {"text": "However, we still managed to train models with decent performance, even if it is likely that a lot of candidates pre-annotated as False in the training data were probably perfect MWEs.", "labels": [], "entities": [{"text": "False in the training data", "start_pos": 130, "end_pos": 156, "type": "DATASET", "confidence": 0.6340252339839936}]}, {"text": "This means that the Delac has covered enough MWEs for the features to not appear as completely meaningless and arbitrary.", "labels": [], "entities": [{"text": "Delac", "start_pos": 20, "end_pos": 25, "type": "DATASET", "confidence": 0.9411727786064148}]}, {"text": "The final precision would never be as good as it is, if the coverage had been not sufficient enough.", "labels": [], "entities": [{"text": "precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9878292083740234}, {"text": "coverage", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.9745573997497559}]}, {"text": "This shows that the method of automatic annotation offered by mwetoolkit is reliable given a lexicon as large as Delac.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Performance of three different models  on the same corpus of Noun-Adjective and Noun- Noun candidates. Percentages with 95% confi- dence intervals, sample size = 100.", "labels": [], "entities": []}]}