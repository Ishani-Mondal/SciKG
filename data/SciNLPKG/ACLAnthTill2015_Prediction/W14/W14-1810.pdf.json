{"title": [{"text": "An Explicit Feedback System for Preposition Errors based on Wikipedia Revisions", "labels": [], "entities": [{"text": "Preposition Errors", "start_pos": 32, "end_pos": 50, "type": "TASK", "confidence": 0.7987326681613922}, {"text": "Wikipedia Revisions", "start_pos": 60, "end_pos": 79, "type": "DATASET", "confidence": 0.9184756875038147}]}], "abstractContent": [{"text": "This paper presents a proof-of-concept tool for providing automated explicit feedback to language learners based on data mined from Wikipedia revisions.", "labels": [], "entities": []}, {"text": "The tool takes a sentence with a grammatical error as input and displays a ranked list of corrections for that error along with evidence to support each correction choice.", "labels": [], "entities": []}, {"text": "We use lexical and part-of-speech contexts , as well as query expansion with a thesaurus to automatically match the error with evidence from the Wikipedia revisions.", "labels": [], "entities": []}, {"text": "We demonstrate that the tool works well for the task of preposition selection errors, evaluating against a publicly available corpus.", "labels": [], "entities": [{"text": "preposition selection errors", "start_pos": 56, "end_pos": 84, "type": "TASK", "confidence": 0.8246314525604248}]}], "introductionContent": [{"text": "A core feature of learning to write is receiving feedback and making revisions based on that feedback.", "labels": [], "entities": []}, {"text": "In the field of second language acquisition, the main focus has been on explicit or direct feedback vs. implicit or indirect feedback.", "labels": [], "entities": [{"text": "second language acquisition", "start_pos": 16, "end_pos": 43, "type": "TASK", "confidence": 0.6672166685263315}]}, {"text": "In writing, explicit or direct feedback involves a clear indication of the location of an error as well as the correction itself, or, more recently, a meta-linguistic explanation (of the underlying grammatical rule).", "labels": [], "entities": []}, {"text": "Implicit or indirect written feedback indicates that an error has been made at a location, but it does not provide a correction.", "labels": [], "entities": []}, {"text": "The work in this paper describes a novel tool for presenting language learners with explicit feedback based on human-authored revisions in Wikipedia.", "labels": [], "entities": []}, {"text": "Here we describe the proof-of-concept tool that provides explicit feedback on one specific category of grammatical errors, preposition selection.", "labels": [], "entities": [{"text": "preposition selection", "start_pos": 123, "end_pos": 144, "type": "TASK", "confidence": 0.7168490141630173}]}, {"text": "We restrict the scope of the tool in order to be able to carryout a focused study, but expect that our findings presented here will also generalize to other error types.", "labels": [], "entities": []}, {"text": "The task of preposition selection errors has been well studied, and the availability of public, annotated corpora containing such errors provides easy access to evaluation data.", "labels": [], "entities": [{"text": "preposition selection errors", "start_pos": 12, "end_pos": 40, "type": "TASK", "confidence": 0.7921477953592936}]}, {"text": "Our tool takes a sentence with a grammatical error as input, and returns a ranked list of possible corrections.", "labels": [], "entities": []}, {"text": "The tool makes use of frequency of correction in edits to Wikipedia articles (as recorded in the Wikipedia revision history) to calculate the rank order.", "labels": [], "entities": [{"text": "frequency of correction", "start_pos": 22, "end_pos": 45, "type": "METRIC", "confidence": 0.8326716224352518}, {"text": "Wikipedia revision history", "start_pos": 97, "end_pos": 123, "type": "DATASET", "confidence": 0.8732667366663615}]}, {"text": "In addition to the ranked list of suggestions, the tool also provides evidence for each correction based on the actual changes made between different versions of Wikipedia articles.", "labels": [], "entities": []}, {"text": "The tool uses the notion of \"context similarity\" to determine whether a particular edit to a Wikipedia article can provide evidence of a correction in a given context.", "labels": [], "entities": []}, {"text": "Specifically, this paper makes the following contributions: 1.", "labels": [], "entities": []}, {"text": "We build a tool to provide explicit feedback for preposition selection errors in the form of ranked lists of suggested corrections.", "labels": [], "entities": [{"text": "preposition selection errors", "start_pos": 49, "end_pos": 77, "type": "TASK", "confidence": 0.8369104663530985}]}, {"text": "2. We use evidence from human-authored corrections for each suggested correction on a list.", "labels": [], "entities": []}, {"text": "3. We conduct a detailed examination of how the performance of the tool is affected by varying the type and size of contextual information and by the use of query expansion.", "labels": [], "entities": []}, {"text": "The remainder of this paper is organized as follows: \u00a72 describes related work and \u00a73 outlines potential approaches for using Wikipedia revision data in a feedback tool.", "labels": [], "entities": []}, {"text": "\u00a74 outlines the core system 79 for generating feedback and \u00a75 presents an empirical evaluation of this system.", "labels": [], "entities": []}, {"text": "In \u00a76 we describe a method for enhancing the system using query expansions.", "labels": [], "entities": []}, {"text": "We discuss our findings and some future work in \u00a77 and, finally, conclude in \u00a78.", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to determine how well the tool performs at suggesting corrections, we used sentences containing preposition errors from the CLC FCE dataset.", "labels": [], "entities": [{"text": "suggesting corrections", "start_pos": 52, "end_pos": 74, "type": "TASK", "confidence": 0.7184318900108337}, {"text": "CLC FCE dataset", "start_pos": 133, "end_pos": 148, "type": "DATASET", "confidence": 0.9414018193880717}]}, {"text": "The CLC FCE Dataset is a collection of 1,244 exam scripts written by learners of English as part of the Cambridge ESOL First Certificate in English ().", "labels": [], "entities": [{"text": "CLC FCE Dataset", "start_pos": 4, "end_pos": 19, "type": "DATASET", "confidence": 0.9461193084716797}, {"text": "Cambridge ESOL First Certificate in English", "start_pos": 104, "end_pos": 147, "type": "DATASET", "confidence": 0.8973116377989451}]}, {"text": "Our evaluation set consists of 3,134 sentences, each containing a single preposition error.", "labels": [], "entities": []}, {"text": "We evaluate the tool on two criteria: \u2022 Coverage.", "labels": [], "entities": []}, {"text": "We define coverage as the proportion of errors for which the tool is able to suggest any corrections.", "labels": [], "entities": [{"text": "coverage", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9938573241233826}]}, {"text": "The obvious definition of accu-: A screenshot of the tool suggesting the top 5 corrections fora sentence using two parts-of-speech on either side of the marked error as context.", "labels": [], "entities": []}, {"text": "The corrections are displayed in ranked fashion as a histogram and clicking on one displays the \"corrected\" sentence above and the corresponding evidence from Wikipedia revisions on the left.", "labels": [], "entities": []}, {"text": "racy would be the proportion of errors for which the tool's best suggestion is the correct one.", "labels": [], "entities": []}, {"text": "However, since the tool returns a ranked list of suggestions, it is important to award partial credit for errors where the tool made a correct suggestion but it was not ranked at the top.", "labels": [], "entities": []}, {"text": "Therefore, we use the Mean Reciprocal Rank (MRR), a standard metric used for evaluating ranked retrieval systems).", "labels": [], "entities": [{"text": "Mean Reciprocal Rank (MRR)", "start_pos": 22, "end_pos": 48, "type": "METRIC", "confidence": 0.9578919410705566}]}, {"text": "MRR is computed as follows:", "labels": [], "entities": [{"text": "MRR", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.6927880644798279}]}], "tableCaptions": [{"text": " Table 1: A detailed breakdown of the Found, Missing and Blank classes along with the Mean Reciprocal  Rank (MRR) values, for different types (words, tags) and sizes (1, 2, or 3 around the error) of  contextual information used in the search.", "labels": [], "entities": [{"text": "Mean Reciprocal  Rank (MRR)", "start_pos": 86, "end_pos": 113, "type": "METRIC", "confidence": 0.9630910952885946}]}, {"text": " Table 2: A detailed breakdown of the Found, Missing and Blank classes along with the Mean Reciprocal  Rank (MRR) values, for different number of query expansions (K).", "labels": [], "entities": [{"text": "Mean Reciprocal  Rank (MRR)", "start_pos": 86, "end_pos": 113, "type": "METRIC", "confidence": 0.9703134000301361}]}]}