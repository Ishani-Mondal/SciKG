{"title": [{"text": "Comparing CRF and template-matching in phrasing tasks within a Hybrid MT system", "labels": [], "entities": []}], "abstractContent": [{"text": "The present article focuses on improving the performance of a hybrid Machine Translation (MT) system, namely PRE-SEMT.", "labels": [], "entities": [{"text": "Machine Translation (MT)", "start_pos": 69, "end_pos": 93, "type": "TASK", "confidence": 0.8623923420906067}]}, {"text": "The PRESEMT methodology is readily portable to new language pairs, and allows the creation of MT systems with minimal reliance on expensive resources.", "labels": [], "entities": [{"text": "MT", "start_pos": 94, "end_pos": 96, "type": "TASK", "confidence": 0.9804100394248962}]}, {"text": "PRESEMT is phrase-based and uses a small parallel corpus from which to extract structural transformations from the source language (SL) to the target language (TL).", "labels": [], "entities": []}, {"text": "On the other hand, the TL language model is extracted from large monolingual corpora.", "labels": [], "entities": []}, {"text": "This article examines the task of maximising the amount of information extracted from a very limited parallel corpus.", "labels": [], "entities": []}, {"text": "Hence, emphasis is placed on the module that learns to segment into phrases arbitrary input text in SL, by extrapolating information from a limited-size parsed TL text, alleviating the need for an SL parser.", "labels": [], "entities": []}, {"text": "An established method based on Conditional Random Fields (CRF) is compared hereto a much simpler template-matching algorithm to determine the most suitable approach for extracting an accurate model.", "labels": [], "entities": []}, {"text": "Experimental results indicate that fora limited-size training set, template-matching generates a superior model leading to higher quality translations.", "labels": [], "entities": []}], "introductionContent": [{"text": "Most current MT systems translate sentences by operating at a sub-sentential level on parallel corpora.", "labels": [], "entities": [{"text": "MT", "start_pos": 13, "end_pos": 15, "type": "TASK", "confidence": 0.9817277193069458}]}, {"text": "However, this frequently necessitates parsers for both SL and TL, which either (i) develop matched segmentations that give similar outputs in terms of phrasing over the SL and TL or (ii) for which a mapping is externally defined between the two given segmentations.", "labels": [], "entities": []}, {"text": "Both alternatives limit portability to new languages, due to the need for matching the appropriate tools.", "labels": [], "entities": []}, {"text": "Another limitation involves the amount of parallel texts needed.", "labels": [], "entities": []}, {"text": "Statistical MT (SMT) generates high quality translations provided that large parallel corpora (of millions of words) are available.", "labels": [], "entities": [{"text": "Statistical MT (SMT", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.6873991042375565}]}, {"text": "However, this places a strict constraint on the volume of data required to create a functioning MT system.", "labels": [], "entities": [{"text": "MT", "start_pos": 96, "end_pos": 98, "type": "TASK", "confidence": 0.9856488704681396}]}, {"text": "For this reason, a number of researchers involved in SMT have recently investigated the extraction of information from monolingual corpora, including lexical translation probabilities () and topic-specific information (.", "labels": [], "entities": [{"text": "SMT", "start_pos": 53, "end_pos": 56, "type": "TASK", "confidence": 0.9929138422012329}]}, {"text": "A related direction in MT research concerns hybrid MT (HMT), where principles from multiple MT paradigms are combined, such as for instance SMT and RBMT.", "labels": [], "entities": [{"text": "MT", "start_pos": 23, "end_pos": 25, "type": "TASK", "confidence": 0.9924019575119019}, {"text": "MT (HMT)", "start_pos": 51, "end_pos": 59, "type": "TASK", "confidence": 0.8445264101028442}, {"text": "SMT", "start_pos": 140, "end_pos": 143, "type": "TASK", "confidence": 0.9387986660003662}]}, {"text": "HMT aims to combine the paradigms' positive aspects to achieve higher translation accuracy. has studied the trend of convergence of MT research towards hybrid systems.", "labels": [], "entities": [{"text": "HMT", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.867575466632843}, {"text": "accuracy.", "start_pos": 82, "end_pos": 91, "type": "METRIC", "confidence": 0.7758034467697144}, {"text": "MT", "start_pos": 132, "end_pos": 134, "type": "TASK", "confidence": 0.9919658899307251}]}, {"text": "have proposed an HMT system where statistical principles are combined with ExampleBased MT (EBMT) to improve the performance of SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 128, "end_pos": 131, "type": "TASK", "confidence": 0.984874427318573}]}, {"text": "The PRESEMT (www.presemt.eu) methodology) supports rapid development of hybrid MT systems for new language pairs.", "labels": [], "entities": [{"text": "MT", "start_pos": 79, "end_pos": 81, "type": "TASK", "confidence": 0.9727510213851929}]}, {"text": "The hybrid nature of PRESEMT arises from the use of data-driven pattern recognition algorithms that combine EBMT techniques with statistical principles when modelling the target language.", "labels": [], "entities": [{"text": "pattern recognition", "start_pos": 64, "end_pos": 83, "type": "TASK", "confidence": 0.7375729084014893}]}, {"text": "PRESEMT utilises a very small parallel corpus of a few hundred sentences, together with a large TL monolingual one to determine the translation.", "labels": [], "entities": [{"text": "PRESEMT", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.8210676312446594}, {"text": "translation", "start_pos": 132, "end_pos": 143, "type": "TASK", "confidence": 0.9535327553749084}]}, {"text": "The MT process encompasses three stages: Stage 1: this pre-processes the input sentence, by tagging and lemmatising tokens and grouping these tokens into phrases, preparing the actual translation.", "labels": [], "entities": [{"text": "MT process", "start_pos": 4, "end_pos": 14, "type": "TASK", "confidence": 0.9108963012695312}]}, {"text": "Stage 2: this comprises the main translation engine, which in turn is divided into two phases: Phase A: the establishment of the translation structure in terms of phrase order; Phase B: the definition of word order and the resolution of lexical ambiguities at an intra-phrase level.", "labels": [], "entities": [{"text": "resolution of lexical ambiguities", "start_pos": 223, "end_pos": 256, "type": "TASK", "confidence": 0.8159617781639099}]}, {"text": "Stage 3: post-processing, where the appropriate tokens are generated from lemmas.", "labels": [], "entities": []}, {"text": "In terms of resources, PRESEMT requires: (i) a bilingual lemma dictionary providing SL to TL lexical correspondences, (ii) an extensive TL monolingual corpus, compiled via web crawling to generate a language model, (iii) a very small bilingual corpus.", "labels": [], "entities": []}, {"text": "The bilingual corpus provides examples of the structural transformation from SL to TL.", "labels": [], "entities": []}, {"text": "In comparison to SMT, the use of a small corpus reduces substantially the need for locating parallel corpora, whose procurement or development can be extremely expensive.", "labels": [], "entities": [{"text": "SMT", "start_pos": 17, "end_pos": 20, "type": "TASK", "confidence": 0.9869498610496521}]}, {"text": "Instead, a small parallel corpus can be assembled with limited recourse to costly human resources.", "labels": [], "entities": []}, {"text": "The small size of the parallel corpus unavoidably places additional requirements on the processing accuracy in order to extract the necessary information.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 99, "end_pos": 107, "type": "METRIC", "confidence": 0.9862725734710693}]}, {"text": "The main task studied here is to extract from a parallel corpus of 200 sentences appropriate structural information to describe the transformation from SL to TL.", "labels": [], "entities": []}, {"text": "More specifically, a module needs to be trained to transfer a given TL phrasing scheme to SL, so that during translation the module segments arbitrary input text into phrases in a manner compatible to the TL phrasing scheme.", "labels": [], "entities": []}, {"text": "The question then is which method succeeds in extracting from the parallel corpus the most accurate structural knowledge, to support an effective MT system.", "labels": [], "entities": [{"text": "MT", "start_pos": 146, "end_pos": 148, "type": "TASK", "confidence": 0.9955450296401978}]}, {"text": "For transferring a TL phrasing scheme into SL, PRESEMT relies on word and phrase alignment of the parallel corpus.", "labels": [], "entities": [{"text": "word and phrase alignment", "start_pos": 65, "end_pos": 90, "type": "TASK", "confidence": 0.6338478997349739}]}, {"text": "This alignment allows the extrapolation of a model that segments the SL text.", "labels": [], "entities": []}, {"text": "The SL-side segmentation is limited to phrase identification, rather than a detailed syntactic analysis.", "labels": [], "entities": [{"text": "SL-side segmentation", "start_pos": 4, "end_pos": 24, "type": "TASK", "confidence": 0.8107639253139496}, {"text": "phrase identification", "start_pos": 39, "end_pos": 60, "type": "TASK", "confidence": 0.8917136788368225}]}, {"text": "The processing of a bilingual corpus and the elicitation of the corresponding SL-to-TL phrasing information involves two PRESEMT modules: (i) The Phrase aligner module (PAM), which performs text alignment at word and phrase level within the parallel corpus.", "labels": [], "entities": [{"text": "text alignment", "start_pos": 190, "end_pos": 204, "type": "TASK", "confidence": 0.7132722586393356}]}, {"text": "This languageindependent method identifies corresponding terms within the SL and TL sides of each sentence, and aligns the words between the two languages, while at the same time creating phrases for the non-parsed side of the corpus.", "labels": [], "entities": []}, {"text": "(ii) The Phrasing model generator (PMG), which elicits a phrasing model from this aligned parallel corpus.", "labels": [], "entities": []}, {"text": "PMG is trained on the aligned parallel SL -TL sentences incorporating the PAM output to generate a phrasing model.", "labels": [], "entities": []}, {"text": "This model is then employed to segment userspecified text during translation.", "labels": [], "entities": []}, {"text": "A number of studies relevant to this article involve the transfer of phrasing schemes from one language to another.", "labels": [], "entities": []}, {"text": "These studies have focussed on extrapolating information from a resourcerich to a resource-poor language.", "labels": [], "entities": []}, {"text": "have used automatically word-aligned raw bilingual corpora to project annotations.", "labels": [], "entities": []}, {"text": "use a two-stage process via a dynamic programming-type algorithm for aligning SL and TL tokens.", "labels": [], "entities": []}, {"text": "propose a more advanced approach allowing noncontiguous phrases, to cover additional linguistic phenomena.", "labels": [], "entities": []}, {"text": "have created a parser fora new language based on a set of parallel sentences together with a parser in a frequently-used language, by transferring deeper syntactic structure and introducing fix-up rules.", "labels": [], "entities": []}, {"text": "create a TL dependency parser by using bilingual text, a parser, and automatically-derived word alignments.", "labels": [], "entities": [{"text": "TL dependency parser", "start_pos": 9, "end_pos": 29, "type": "TASK", "confidence": 0.8597865303357443}]}], "datasetContent": [{"text": "To evaluate the proposed phrasing generator, the output of the entire translation chain up to the final translation result is studied.", "labels": [], "entities": []}, {"text": "This allows the contribution of different PMG models to be quantified using objective metrics.", "labels": [], "entities": []}, {"text": "For the purposes of the present article, the language pair Greek-to-English (denoted as EL\u2192EN) is employed.", "labels": [], "entities": []}, {"text": "Since the SL phrasing generated by PMG is based on the TL phrasing scheme, the phrase labels of the resulting SL phrases are inherited from the TL ones.", "labels": [], "entities": []}, {"text": "In the experiments reported here (with English as TL), the TreeTagger parser is used.", "labels": [], "entities": []}, {"text": "Thus the SL-side phrase types include PC, VC, ADVC and ADJC.", "labels": [], "entities": []}, {"text": "As TreeTagger also allows for certain words (such as conjunctions) to remain outside phrases, it is possible that isolated words occur in SL too.", "labels": [], "entities": []}, {"text": "For the purposes of modelling such occurrences, these words form single-token phrases, denoted as ISC (i.e. ISolated word Chunk).", "labels": [], "entities": []}, {"text": "Both the parallel corpus and the evaluation dataset employed here have been established in the PRESEMT project, and are available over the web (cf. www.presemt.eu/data).", "labels": [], "entities": [{"text": "PRESEMT project", "start_pos": 95, "end_pos": 110, "type": "DATASET", "confidence": 0.8359033465385437}]}, {"text": "The parallel corpus has been retrieved from the web (from an EU website discussing the history of the Union), with an average size of 18 words per sentence, while the smallest sentence comprises 4 words and the largest 38 words.", "labels": [], "entities": []}, {"text": "Only minimal editing was performed in the parallel corpus, to ensure parallelism between SL and TL.", "labels": [], "entities": []}, {"text": "The evaluation set comprises 200 isolated sentences, each with a single reference translation ().", "labels": [], "entities": []}, {"text": "These sentences have been drawn from the internet via web crawling, being required to have a length of between 7 and 40 tokens each.", "labels": [], "entities": []}, {"text": "contains the translation accuracy results obtained with PMG-simple using the criteria of.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.8959500193595886}, {"text": "PMG-simple", "start_pos": 56, "end_pos": 66, "type": "DATASET", "confidence": 0.7571253180503845}]}, {"text": "In all experiments, the results concern the objective evaluation of the final translation, using four of the most widely used objective evaluation metrics, namely BLEU, NIST, TER and METEOR).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 163, "end_pos": 167, "type": "METRIC", "confidence": 0.9981861710548401}, {"text": "NIST", "start_pos": 169, "end_pos": 173, "type": "DATASET", "confidence": 0.7194846868515015}, {"text": "TER", "start_pos": 175, "end_pos": 178, "type": "METRIC", "confidence": 0.9941231608390808}, {"text": "METEOR", "start_pos": 183, "end_pos": 189, "type": "METRIC", "confidence": 0.9683095812797546}]}, {"text": "For TER a lower value indicates a more successful translation while for other metrics, a higher value corresponds to a better translation.", "labels": [], "entities": [{"text": "TER", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.9627392292022705}]}, {"text": "Since other components of the MT implementation do not change, this set of metrics provides an accurate end-to-end measurement of the effect of the phrasing model on the translation process.", "labels": [], "entities": [{"text": "MT", "start_pos": 30, "end_pos": 32, "type": "TASK", "confidence": 0.9790054559707642}]}, {"text": "As can be seen from   A potential for optimisation concerns the cutoff frequency (freq_thres) below which a phrase is considered exceptionally infrequent and is handled differently.", "labels": [], "entities": [{"text": "freq_thres)", "start_pos": 82, "end_pos": 93, "type": "METRIC", "confidence": 0.914661779999733}]}, {"text": "Indicative results are shown for the four metrics studied in.", "labels": [], "entities": []}, {"text": "As can be seen, the best results are obtained with a cut-off frequency of 2, for the given parallel corpus.", "labels": [], "entities": []}, {"text": "Of course, this value is to an extent dependent on the training set.", "labels": [], "entities": []}, {"text": "However, based on detailed analyses of the experimental results, it has been found that phrases that represent hapax legomena (i.e. phrases which occur only once) are not reliable for chunking purposes.", "labels": [], "entities": []}, {"text": "Here, there are two possible explanations: (i) either such phrases represent spurious chunkings resulting from errors in the automatic alignment or (ii) they represent very infrequent phrases which again should not bias the phrasing process disproportionately.", "labels": [], "entities": []}, {"text": "In both cases, the activation of the cut-off frequency improves the translation accuracy.", "labels": [], "entities": [{"text": "translation", "start_pos": 68, "end_pos": 79, "type": "TASK", "confidence": 0.936295211315155}, {"text": "accuracy", "start_pos": 80, "end_pos": 88, "type": "METRIC", "confidence": 0.9265155792236328}]}], "tableCaptions": [{"text": " Table 2: Translation accuracy for EL\u2192EN, using  PMG-simple with various criteria.", "labels": [], "entities": [{"text": "Translation", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.9400090575218201}, {"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9743785262107849}]}, {"text": " Table 3: Translation scores for EL\u2192EN, using  PMG-simple with criterion 4 and various cut-off  frequencies.", "labels": [], "entities": [{"text": "Translation", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.9271945357322693}]}, {"text": " Table 3. As can be  seen, the best results are obtained with a cut-off  frequency of 2, for the given parallel corpus. Of  course, this value is to an extent dependent on  the training set. However, based on detailed  analyses of the experimental results, it has been  found that phrases that represent hapax legomena  (i.e. phrases which occur only once) are not  reliable for chunking purposes. Here, there are  two possible explanations: (i) either such phrases", "labels": [], "entities": []}, {"text": " Table 4: Translation accuracy for EL\u2192EN, using  PMG-simple with crit.4 and using CRF.", "labels": [], "entities": [{"text": "Translation", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.9348886609077454}, {"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9686112999916077}]}]}