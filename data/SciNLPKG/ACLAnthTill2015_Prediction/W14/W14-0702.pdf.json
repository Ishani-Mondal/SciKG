{"title": [{"text": "Annotating causality in the TempEval-3 corpus", "labels": [], "entities": []}], "abstractContent": [{"text": "While there is a wide consensus in the NLP community over the modeling of temporal relations between events, mainly based on Allen's temporal logic, the question on how to annotate other types of event relations, in particular causal ones, is still open.", "labels": [], "entities": []}, {"text": "In this work, we present some annotation guidelines to capture causality between event pairs, partly inspired by TimeML.", "labels": [], "entities": [{"text": "TimeML", "start_pos": 113, "end_pos": 119, "type": "DATASET", "confidence": 0.9448878765106201}]}, {"text": "We then implement a rule-based algorithm to automatically identify explicit causal relations in the TempEval-3 corpus.", "labels": [], "entities": [{"text": "TempEval-3 corpus", "start_pos": 100, "end_pos": 117, "type": "DATASET", "confidence": 0.8916049897670746}]}, {"text": "Based on this annotation, we report some statistics on the behavior of causal cues in text and perform a preliminary investigation on the interaction between causal and temporal relations.", "labels": [], "entities": []}], "introductionContent": [{"text": "The annotation of events and event relations in natural language texts has gained in recent years increasing attention, especially thanks to the development of TimeML annotation scheme (, the release of TimeBank () and the organization of several evaluation campaigns devoted to automatic temporal processing (.", "labels": [], "entities": [{"text": "annotation of events and event relations in natural language texts", "start_pos": 4, "end_pos": 70, "type": "TASK", "confidence": 0.7918902814388276}]}, {"text": "However, while there is a wide consensus in the NLP community over the modeling of temporal relations between events, mainly based on Allen's interval algebra, the question on how to model other types of event relations is still open.", "labels": [], "entities": []}, {"text": "In particular, linguistic annotation of causal relations, which have been widely investigated from a philosophical and logical point of view, are still under debate.", "labels": [], "entities": [{"text": "linguistic annotation of causal relations", "start_pos": 15, "end_pos": 56, "type": "TASK", "confidence": 0.8020424604415893}]}, {"text": "This leads, in turn, to the lack of a standard benchmark to evaluate causal relation extraction systems, making it difficult to compare systems performances, and to identify the state-ofthe-art approach for this particular task.", "labels": [], "entities": [{"text": "causal relation extraction", "start_pos": 69, "end_pos": 95, "type": "TASK", "confidence": 0.6200995544592539}]}, {"text": "Although several resources exist in which causality has been annotated, they cover only few aspects of causality and do not model it in a global way, comparable to what as been proposed for temporal relations in TimeML.", "labels": [], "entities": [{"text": "TimeML", "start_pos": 212, "end_pos": 218, "type": "DATASET", "confidence": 0.9227720499038696}]}, {"text": "See for instance the annotation of causal arguments in PropBank ( and of causal discourse relations in the Penn Discourse Treebank (The PDTB Research.", "labels": [], "entities": [{"text": "PropBank", "start_pos": 55, "end_pos": 63, "type": "DATASET", "confidence": 0.9664254784584045}, {"text": "Penn Discourse Treebank (The PDTB Research", "start_pos": 107, "end_pos": 149, "type": "DATASET", "confidence": 0.82977813908032}]}, {"text": "In this work, we propose annotation guidelines for causality inspired by TimeML, trying to take advantage of the clear definition of events, signals and relations proposed by.", "labels": [], "entities": [{"text": "TimeML", "start_pos": 73, "end_pos": 79, "type": "DATASET", "confidence": 0.9341453313827515}]}, {"text": "Besides, as a preliminary investigation of causality in the TempEval-3 corpus, we perform an automatic analysis of causal signals and relations observed in the corpus.", "labels": [], "entities": [{"text": "TempEval-3 corpus", "start_pos": 60, "end_pos": 77, "type": "DATASET", "confidence": 0.9033090174198151}]}, {"text": "This work is a first step towards the annotation of the TempEval-3 corpus with causality, with the final goal of investigating the strict connection between temporal and causal relations.", "labels": [], "entities": [{"text": "TempEval-3 corpus", "start_pos": 56, "end_pos": 73, "type": "DATASET", "confidence": 0.8070534467697144}]}, {"text": "In fact, there is a temporal constraint in causality, i.e. the cause must occur BEFORE the effect.", "labels": [], "entities": [{"text": "BEFORE", "start_pos": 80, "end_pos": 86, "type": "METRIC", "confidence": 0.9880581498146057}]}, {"text": "We believe that investigating this precondition on a corpus basis can contribute to improving the performance of temporal and causal relation extraction systems.", "labels": [], "entities": [{"text": "temporal and causal relation extraction", "start_pos": 113, "end_pos": 152, "type": "TASK", "confidence": 0.7339760184288024}]}], "datasetContent": [{"text": "We perform two types of evaluation.", "labels": [], "entities": []}, {"text": "The first is a qualitative one, and is carried out by manually inspecting the 173 CLINKs that have been automatically annotated.", "labels": [], "entities": []}, {"text": "The second is a quantitative evaluation, and is performed by comparing the automatic annotated data with a gold standard corpus of 100 documents taken from TimeBank.", "labels": [], "entities": [{"text": "TimeBank", "start_pos": 156, "end_pos": 164, "type": "DATASET", "confidence": 0.8871244788169861}]}, {"text": "The automatically annotated CLINKs have been manually checked in order to measure the precision of the adopted procedure.", "labels": [], "entities": [{"text": "precision", "start_pos": 86, "end_pos": 95, "type": "METRIC", "confidence": 0.9994332194328308}]}, {"text": "Out of 173 annotated CLINKs, 105 were correctly identified obtaining a precision of 0.61.", "labels": [], "entities": [{"text": "precision", "start_pos": 71, "end_pos": 80, "type": "METRIC", "confidence": 0.9990891218185425}]}, {"text": "Details on precision calculated on the different types of categories and linguistic cues defined in Section 3.2 are provided in.", "labels": [], "entities": [{"text": "precision", "start_pos": 11, "end_pos": 20, "type": "METRIC", "confidence": 0.9992270469665527}]}, {"text": "Statistics show that performances vary widely depending on the category and linguistic cue taken into consideration.", "labels": [], "entities": []}, {"text": "In particular, relations expressing causation of PRE-VENT type prove to be extremely difficult to be correctly detected with a rule-based approach: the algorithm precision is 0.25 for basic constructions and 0.17 for periphrastic constructions.", "labels": [], "entities": [{"text": "precision", "start_pos": 162, "end_pos": 171, "type": "METRIC", "confidence": 0.7009214758872986}]}, {"text": "During the manual evaluation, two main types 15 C-SIGNALs    The assignment of a wrong source or a wrong target to a CLINK is primarily caused by the dependency parser output that tends to establish a connection between a causal verb or signal and the closest previous verb.", "labels": [], "entities": []}, {"text": "For example, in the sentence \"StatesWest Airlines said it withdrew T its offer to acquire Mesa Airlines because the Farmington carrier did not respond S to its offer\", the CLINK is annotated between respond and acquire instead of between respond and withdrew.", "labels": [], "entities": [{"text": "StatesWest Airlines", "start_pos": 30, "end_pos": 49, "type": "DATASET", "confidence": 0.9569195210933685}, {"text": "Farmington carrier", "start_pos": 116, "end_pos": 134, "type": "DATASET", "confidence": 0.953924834728241}]}, {"text": "On the other hand, dependency structure is very effective in identifying cases where one event is the consequence or the cause of multiple events, as in \"The president offered to offset T Jordan's costs because 40% of its exports go S to Iraq and 90% of its oil comes S from there.\"", "labels": [], "entities": []}, {"text": "In this case, the algorithm annotates a causal link between go and offset, and also between comes and offset.", "labels": [], "entities": []}, {"text": "The annotation of CLINKs in sentences not containing causal relations is strongly related to the ambiguous nature of many verbs, prepositions and conjunctions, which encode a causal meaning or express a causal relation only in some specific contexts.", "labels": [], "entities": []}, {"text": "For instance, many mistakes are due to the erroneous disambiguation of the conjunction since.", "labels": [], "entities": []}, {"text": "According to the addDiscourse tool, since is a causal connector in around one third of the cases, as in \"For now, though, that would be a theoretical advantage since the authorities have admitted they have no idea where Kopp is.\"", "labels": [], "entities": []}, {"text": "However, there are many cases where the outcome of the tool is not perfect, as in \"Since then, 427 fugitives have been taken into custody or located, 133 of them as a result of citizen assistance, the FBI said\", where since acts as a temporal conjunction.", "labels": [], "entities": []}, {"text": "In order to perform also a quantitative evaluation of our automatic annotation, we manually annotated 100 documents taken from the TimeBank corpus according to the annotation guidelines discussed before.", "labels": [], "entities": [{"text": "TimeBank corpus", "start_pos": 131, "end_pos": 146, "type": "DATASET", "confidence": 0.9785618782043457}]}, {"text": "We then used this data set as a gold standard.", "labels": [], "entities": []}, {"text": "The agreement reached by two annotators on a subset of 5 documents is 0.844 Dice's coefficient on C-SIGNALS (micro-average over markables) and of 0.73 on CLINKS.", "labels": [], "entities": [{"text": "Dice's coefficient", "start_pos": 76, "end_pos": 94, "type": "METRIC", "confidence": 0.859436015288035}, {"text": "CLINKS", "start_pos": 154, "end_pos": 160, "type": "DATASET", "confidence": 0.9213454723358154}]}, {"text": "We found that there are several cases where the algorithm failed to recognize causal links due to events that were originally not annotated in TimeBank.", "labels": [], "entities": [{"text": "TimeBank", "start_pos": 143, "end_pos": 151, "type": "DATASET", "confidence": 0.9610852003097534}]}, {"text": "Therefore, as we proceed with the manual annotation, we also annotated missing events that are involved in causal relations.", "labels": [], "entities": []}, {"text": "shows that, in creating the gold standard, we annotated 61 new events.", "labels": [], "entities": []}, {"text": "As a result, we have around 52% increase in the number of CLINKs.", "labels": [], "entities": []}, {"text": "Nevertheless, explicit causal relations between events are by far less frequent than temporal ones, with an average of 1.4 relations per document.", "labels": [], "entities": []}, {"text": "If we compare the coverage of automatic annotation with the gold standard data (without newly added events, to be fair), we observe that automatic annotation covers around 76% of C-SIGNALs and only around 55% of CLINKs.", "labels": [], "entities": []}, {"text": "This is due to the limitation of the algorithm that only considers a: Automatic annotation performance small list of causal connectors.", "labels": [], "entities": []}, {"text": "Some examples of manually annotated causal signals that are not in the list used by the algorithm include due mostly to, thanks in part to and in punishment for.", "labels": [], "entities": []}, {"text": "Finally, we evaluate the performance of the algorithm for automatic annotation (shown in) by computing precision, recall and F1 on gold standard data without newly added events.", "labels": [], "entities": [{"text": "precision", "start_pos": 103, "end_pos": 112, "type": "METRIC", "confidence": 0.9995539784431458}, {"text": "recall", "start_pos": 114, "end_pos": 120, "type": "METRIC", "confidence": 0.9993053674697876}, {"text": "F1", "start_pos": 125, "end_pos": 127, "type": "METRIC", "confidence": 0.9990485310554504}]}, {"text": "We observe that our rule-based approach is too rigid to capture the causal information present in the data.", "labels": [], "entities": []}, {"text": "In particular, it suffers from low recall as regards CLINKs.", "labels": [], "entities": [{"text": "recall", "start_pos": 35, "end_pos": 41, "type": "METRIC", "confidence": 0.9996033310890198}]}, {"text": "We believe that this issue maybe alleviated by adopting a supervised approach, where the list of verbs and causal signals would be included in a larger feature set, considering among others the events' position, their PoS tags, the dependency path between the two events, etc.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of CLINKs with basic construc- tion", "labels": [], "entities": []}, {"text": " Table 2: Statistics of periphrastic causative verbs", "labels": [], "entities": []}, {"text": " Table 3: Statistics of causal signals in CLINKs", "labels": [], "entities": [{"text": "CLINKs", "start_pos": 42, "end_pos": 48, "type": "TASK", "confidence": 0.4788324236869812}]}, {"text": " Table 4: Statistics of CLINKs' polarity", "labels": [], "entities": []}, {"text": " Table 5: Statistics of CLINKs' overlapping with  TLINKs", "labels": [], "entities": [{"text": "TLINKs", "start_pos": 50, "end_pos": 56, "type": "TASK", "confidence": 0.3776502013206482}]}, {"text": " Table 6: Statistics of CLINKs triggered by C-SIGNALs overlapping with TLINKs", "labels": [], "entities": []}, {"text": " Table 8: Statistics of causality annotation in manual  versus automatic annotation", "labels": [], "entities": []}, {"text": " Table 9: Automatic annotation performance", "labels": [], "entities": []}]}