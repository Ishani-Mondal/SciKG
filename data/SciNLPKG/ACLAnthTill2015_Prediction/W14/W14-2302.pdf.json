{"title": [{"text": "Different Texts, Same Metaphors: Unigrams and Beyond", "labels": [], "entities": []}], "abstractContent": [{"text": "Current approaches to supervised learning of metaphor tend to use sophisticated features and restrict their attention to constructions and contexts where these features apply.", "labels": [], "entities": [{"text": "supervised learning of metaphor", "start_pos": 22, "end_pos": 53, "type": "TASK", "confidence": 0.6820550784468651}]}, {"text": "In this paper, we describe the development of a supervised learning system to classify all content words in a running text as either being used metaphori-cally or not.", "labels": [], "entities": []}, {"text": "We start by examining the performance of a simple unigram baseline that achieves surprisingly good results for some of the datasets.", "labels": [], "entities": []}, {"text": "We then show how the recall of the system can be improved over this strong baseline.", "labels": [], "entities": [{"text": "recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9992313385009766}]}], "introductionContent": [{"text": "Current approaches to supervised learning of metaphor tend to (a) use sophisticated features based on theories of metaphor, (b) apply to certain selected constructions, like adj-noun or verbobject pairs, and (c) concentrate on metaphors of certain kind, such as metaphors about governance or about the mind.", "labels": [], "entities": [{"text": "supervised learning of metaphor", "start_pos": 22, "end_pos": 53, "type": "TASK", "confidence": 0.643979974091053}]}, {"text": "In this paper, we describe the development of a supervised machine learning system to classify all content words in a running text as either being used metaphorically or not -a task not yet addressed in the literature, to our knowledge.", "labels": [], "entities": []}, {"text": "This approach would enable, for example, quantification of the extent to which a given text uses metaphor, or the extent to which two different texts use similar metaphors.", "labels": [], "entities": []}, {"text": "Both of these questions are important in our target application -scoring texts (in our case, essays written fora test) for various aspects of effective use of language, one of them being the use of metaphor.", "labels": [], "entities": []}, {"text": "We start by examining the performance of a simple unigram baseline that achieves surprisingly good results for some of the datasets.", "labels": [], "entities": []}, {"text": "We then show how the recall of the system can be improved over this strong baseline.", "labels": [], "entities": [{"text": "recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9992313385009766}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Datasets used in this study. NVAR =  Nouns, Verbs, Adjectives, Adverbs, as tagged by  the Stanford POS tagger (Toutanova et al., 2003).", "labels": [], "entities": []}, {"text": " Table 2: Summary of performance, in terms of  precision, recall, and F 1 . Set A, B, and VUAm- sterdam: cross-validation. B-A and A-B: Training  on B and testing on A, and vice versa, respectively.", "labels": [], "entities": [{"text": "precision", "start_pos": 47, "end_pos": 56, "type": "METRIC", "confidence": 0.9995830655097961}, {"text": "recall", "start_pos": 58, "end_pos": 64, "type": "METRIC", "confidence": 0.9995253086090088}, {"text": "F", "start_pos": 70, "end_pos": 71, "type": "METRIC", "confidence": 0.9995405673980713}, {"text": "VUAm- sterdam", "start_pos": 90, "end_pos": 103, "type": "METRIC", "confidence": 0.8942163387934366}]}, {"text": " Table 3: Ablation evaluations. Model M is a pseudo-system that classifies all instances as metaphors.", "labels": [], "entities": []}]}