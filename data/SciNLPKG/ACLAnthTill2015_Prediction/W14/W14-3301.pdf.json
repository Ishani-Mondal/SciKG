{"title": [{"text": "Efficient Elicitation of Annotations for Human Evaluation of Machine Translation Center for Language and Speech Processing", "labels": [], "entities": [{"text": "Efficient Elicitation of Annotations", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.763648048043251}, {"text": "Machine Translation", "start_pos": 61, "end_pos": 80, "type": "TASK", "confidence": 0.7076123803853989}, {"text": "Language and Speech Processing", "start_pos": 92, "end_pos": 122, "type": "TASK", "confidence": 0.6639700829982758}]}], "abstractContent": [{"text": "A main output of the annual Workshop on Statistical Machine Translation (WMT) is a ranking of the systems that participated in its shared translation tasks, produced by aggregating pairwise sentence-level comparisons collected from human judges.", "labels": [], "entities": [{"text": "Statistical Machine Translation (WMT)", "start_pos": 40, "end_pos": 77, "type": "TASK", "confidence": 0.7941999832789103}]}, {"text": "Over the past few years, there have been a number of tweaks to the ag-gregation formula in attempts to address issues arising from the inherent ambiguity and subjectivity of the task, as well as weaknesses in the proposed models and the manner of model selection.", "labels": [], "entities": []}, {"text": "We continue this line of work by adapting the TrueSkill TM algorithm-an online approach for modeling the relative skills of players in ongoing competitions, such as Microsoft's Xbox Live-to the human evaluation of machine translation output.", "labels": [], "entities": [{"text": "machine translation output", "start_pos": 214, "end_pos": 240, "type": "TASK", "confidence": 0.7018425365289053}]}, {"text": "Our experimental results show that TrueSkill outperforms other recently proposed models on accuracy, and also can significantly reduce the number of pair-wise annotations that need to be collected by sampling non-uniformly from the space of system competitions.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.9995108842849731}]}], "introductionContent": [{"text": "The Workshop on Statistical Machine Translation (WMT) has long been a central event in the machine translation (MT) community for the evaluation of MT output.", "labels": [], "entities": [{"text": "Statistical Machine Translation (WMT)", "start_pos": 16, "end_pos": 53, "type": "TASK", "confidence": 0.7835133721431097}, {"text": "machine translation (MT) community", "start_pos": 91, "end_pos": 125, "type": "TASK", "confidence": 0.8553978105386099}, {"text": "MT output", "start_pos": 148, "end_pos": 157, "type": "TASK", "confidence": 0.8539328575134277}]}, {"text": "It hosts an annual set of shared translation tasks focused mostly on the translation of western European languages.", "labels": [], "entities": [{"text": "translation of western European languages", "start_pos": 73, "end_pos": 114, "type": "TASK", "confidence": 0.8536376237869263}]}, {"text": "One of its main functions is to publish a ranking of the systems for each task, which are produced by aggregating a large number of human judgments of sentencelevel pairwise rankings of system outputs.", "labels": [], "entities": []}, {"text": "While the performance on many automatic metrics is also: System rankings presented as clusters (WMT13 French-English competition).", "labels": [], "entities": [{"text": "WMT13 French-English competition", "start_pos": 96, "end_pos": 128, "type": "DATASET", "confidence": 0.8733335534731547}]}, {"text": "The score column is the percentage of time each system was judged better across its comparisons ( \u00a72.1).", "labels": [], "entities": []}, {"text": "reported (e.g., BLEU ()), the human evaluation is considered primary, and is in fact used as the gold standard for its metrics task, where evaluation metrics are evaluated.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 16, "end_pos": 20, "type": "METRIC", "confidence": 0.9953951239585876}]}, {"text": "In machine translation, the longstanding disagreements about evaluation measures do not go away when moving from automatic metrics to human judges.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 3, "end_pos": 22, "type": "TASK", "confidence": 0.7645508944988251}]}, {"text": "This is due in no small part to the inherent ambiguity and subjectivity of the task, but also arises from the particular way that the WMT organizers produce the rankings.", "labels": [], "entities": []}, {"text": "The systemlevel rankings are produced by collecting pairwise sentence-level comparisons between system outputs.", "labels": [], "entities": []}, {"text": "These are then aggregated to produce a complete ordering of all systems, or, more recently, a partial ordering, with systems clustered where they cannot be distinguished in a statistically significant way, taken from).", "labels": [], "entities": []}, {"text": "A number of problems have been noted with this approach.", "labels": [], "entities": []}, {"text": "The first has to do with the nature of ranking itself.", "labels": [], "entities": []}, {"text": "Over the past few years, the WMT organizers have introduced a number of minor tweaks to the ranking algorithm ( \u00a72) in reaction to largely intuitive arguments that have been 1 raised about how the evaluation is conducted.", "labels": [], "entities": [{"text": "WMT", "start_pos": 29, "end_pos": 32, "type": "TASK", "confidence": 0.818156361579895}]}, {"text": "While these tweaks have been sensible (and later corroborated), point out that this is essentially a model selection task, and should properly be driven by empirical performance on heldout data according to some metric.", "labels": [], "entities": []}, {"text": "Instead of intuition, they suggest perplexity, and show that a novel graphical model outperforms existing approaches on that metric, with less amount of data.", "labels": [], "entities": []}, {"text": "A second problem is the deficiency of the models used to produce the ranking, which work by computing simple ratios of wins (and, optionally, ties) to losses.", "labels": [], "entities": []}, {"text": "Such approaches do not consider the relative difficulty of system matchups, and thus leave open the possibility that a system is ranked highly from the luck of comparisons against poorer opponents.", "labels": [], "entities": []}, {"text": "Third, a large number of judgments need to be collected in order to separate the systems into clusters to produce a partial ranking.", "labels": [], "entities": []}, {"text": "The sheer size of the space of possible comparisons (all pairs of systems times the number of segments in the test set) requires sampling from this space and distributing the annotations across a number of judges.", "labels": [], "entities": []}, {"text": "Even still, the number of judgments needed to produce statistically significant rankings like those in Table 1 grows quadratically in the number of participating systems, often forcing the use of paid, lower-quality annotators hired on Amazon's Mechanical Turk.", "labels": [], "entities": [{"text": "Amazon's Mechanical Turk", "start_pos": 236, "end_pos": 260, "type": "DATASET", "confidence": 0.7994732558727264}]}, {"text": "Part of the problem is that the sampling strategy collects data uniformly across system pairings.", "labels": [], "entities": []}, {"text": "Intuitively, we should need many fewer annotations between systems with divergent base performance levels, instead focusing the collection effort on system pairs whose performance is more matched, in order to tease out the gaps between similarly-performing systems.", "labels": [], "entities": []}, {"text": "Why spend precious human time on redundantly affirming predictable outcomes?", "labels": [], "entities": []}, {"text": "To address these issues, we developed a variation of the TrueSkill model (), an adaptative model of competitions originally developed for the Xbox Live online gaming community.", "labels": [], "entities": [{"text": "Xbox Live online gaming community", "start_pos": 142, "end_pos": 175, "type": "DATASET", "confidence": 0.7423072457313538}]}, {"text": "It assumes that each player's skill level follows a Gaussian distribution N (\u00b5, \u03c3 2 ), in which \u00b5 represents a player's mean performance, and \u03c3 2 the system's uncertainty about its current estimate of this mean.", "labels": [], "entities": []}, {"text": "These values are updated after each \"game\" (in our case, the value of a ternary judgment) in proportion to how surprising the outcome is.", "labels": [], "entities": []}, {"text": "TrueSkill has been adapted to a number of areas, including chess, advertising, and academic conference management.", "labels": [], "entities": [{"text": "TrueSkill", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.8530665636062622}, {"text": "academic conference management", "start_pos": 83, "end_pos": 113, "type": "TASK", "confidence": 0.6006933947404226}]}, {"text": "The rest of this paper provides an empirical comparison of a number of models of human evaluation ( \u00a72).", "labels": [], "entities": []}, {"text": "We evaluate on perplexity and also on accuracy, showing that the two are not always correlated, and arguing for the primacy of the latter ( \u00a73).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.9995718598365784}]}, {"text": "We find that TrueSkill outperforms other models ( \u00a74).", "labels": [], "entities": []}, {"text": "Moreover, TrueSkill also allows us to drastically reduce the amount of data that needs to be collected by sampling non-uniformly from the space of all competitions ( \u00a75), which also allows for greater separation of the systems into ranked clusters ( \u00a76).", "labels": [], "entities": []}], "datasetContent": [{"text": "We used the evaluation data released by WMT13.", "labels": [], "entities": [{"text": "WMT13", "start_pos": 40, "end_pos": 45, "type": "DATASET", "confidence": 0.9531329870223999}]}, {"text": "The data contains (1) five-way system rankings made by either researchers or Turkers and (2) translation data consisting of source sentences, human reference translations, and submitted translations.", "labels": [], "entities": []}, {"text": "Data exists for 10 language pairs.", "labels": [], "entities": []}, {"text": "More details about the dataset can be found in the WMT 2013 overview paper (.", "labels": [], "entities": [{"text": "WMT 2013 overview paper", "start_pos": 51, "end_pos": 74, "type": "DATASET", "confidence": 0.8600421398878098}]}, {"text": "Each five-way system ranking was converted into ten pairwise judgments ( \u00a72).", "labels": [], "entities": []}, {"text": "We trained the models using randomly selected sets of 400, 800, 1,600, 3,200, and 6,400 pairwise comparisons, each produced in two ways: selecting from all researchers, or split between researchers and Turkers.", "labels": [], "entities": []}, {"text": "An important note is that the training data differs according to the model.", "labels": [], "entities": []}, {"text": "For the Expected Wins and Hopkins and May model, we simply sample uniformly at random.", "labels": [], "entities": [{"text": "Expected Wins", "start_pos": 8, "end_pos": 21, "type": "METRIC", "confidence": 0.6511166989803314}]}, {"text": "The TrueSkill model, however, selects its own training data (with replacement) according to the description in Section 2.4.", "labels": [], "entities": []}, {"text": "For tuning hyperparameters and reporting test results, we used development and test sets of 2,000 comparisons drawn entirely from the researcher judgments, and fixed across all experiments.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: System rankings presented as clusters  (WMT13 French-English competition). The score  column is the percentage of time each system was  judged better across its comparisons ( \u00a72.1).", "labels": [], "entities": [{"text": "WMT13 French-English competition", "start_pos": 50, "end_pos": 82, "type": "DATASET", "confidence": 0.8917612632115682}]}, {"text": " Table 2: Model accuracies: models are tuned by  accuracy instead of perplexity. Upper bound is  computed by selecting the most frequent choice  (<, >, =) for each system pair.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.9990276098251343}]}, {"text": " Table 4: Accuracies when training with N-way  free-for-all models, fixing the number of pairwise  comparisons.", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9972967505455017}]}]}