{"title": [], "abstractContent": [{"text": "This paper presents the design and implementation details of an email synthesizer using two-stage stochastic natural language generation, where the first stage structures the emails according to sender style and topic structure, and the second stage synthesizes text content based on the particulars of an email structure element and the goals of a given communication for surface realization.", "labels": [], "entities": []}, {"text": "The synthesized emails reflect sender style and the intent of communication, which can be further used as synthetic evidence for developing other applications.", "labels": [], "entities": []}], "introductionContent": [{"text": "This paper focuses on synthesizing emails that reflect sender style and the intent of the communication.", "labels": [], "entities": []}, {"text": "Such a process might be used for the generation of common messages (for example a request fora meeting without direct intervention from the sender).", "labels": [], "entities": []}, {"text": "It can also be used in situations where naturalistic emails are needed for other applications.", "labels": [], "entities": []}, {"text": "For instance, our email synthesizer was developed to provide emails to be used as part of synthetic evidence of insider threats for purposes of training, prototyping, and evaluating anomaly detectors).", "labels": [], "entities": []}, {"text": "Oh and Rudnicky showed that stochastic generation benefits from two factors: 1) it takes advantage of the practical language of a domain expert instead of the developer and 2) it restates the problem in terms of classification and labeling, where expertise is not required for developing a rule-based generation system.", "labels": [], "entities": [{"text": "stochastic generation", "start_pos": 28, "end_pos": 49, "type": "TASK", "confidence": 0.8058099150657654}]}, {"text": "In the present work we investigate the use of stochastic techniques for generation of a different class of communications and whether global structures can be convincingly created.", "labels": [], "entities": []}, {"text": "Specifically we investigate whether stochastic techniques can be used to acceptably model longer texts and individual sender characteristics in the email domain, both of which may require higher cohesion to be acceptable).", "labels": [], "entities": []}, {"text": "Our proposed system involves two-stage stochastic generation, shown in, in which the first stage models email structures according to sender style and topic structure (high-level generation), and the second stage synthesizes text content based on the particulars of a given communication (surface-level generation).", "labels": [], "entities": []}], "datasetContent": [{"text": "We conduct a preliminary experiment to evaluate the proposed system.", "labels": [], "entities": []}, {"text": "The corpus used for our experiments is the Enron Email Dataset 1 , which contains a total of about 0.5M messages.", "labels": [], "entities": [{"text": "Enron Email Dataset 1", "start_pos": 43, "end_pos": 64, "type": "DATASET", "confidence": 0.9077626019716263}]}, {"text": "We selected the data related to daily business for our use.", "labels": [], "entities": []}, {"text": "This includes data from about 150 users, and we randomly picked 3 senders, ones who wrote many emails, and define additional 3 topic classes (meeting, discussion, issue) as topic-specific entities for the task.", "labels": [], "entities": []}, {"text": "Each sender-specific model (across topics) or topic-specific model (across senders) is trained on 30 emails.", "labels": [], "entities": []}, {"text": "To evaluate the performance of sender style, 7 subjects were given 5 real emails from each sender and then 9 synthesized emails.", "labels": [], "entities": []}, {"text": "They were asked to rate each synthesized email for each sender on a scale between 1 to 5.", "labels": [], "entities": []}, {"text": "With higher weight for sender-specific model when predicting mixture models, average normalized scores the corresponding senders receives account for 45%, which is above chance (33%).", "labels": [], "entities": [{"text": "chance", "start_pos": 170, "end_pos": 176, "type": "METRIC", "confidence": 0.9564456343650818}]}, {"text": "This suggests that sender style can be noticed by subjects.", "labels": [], "entities": []}, {"text": "Ina follow-up questionnaire, subjects indicated that their ratings were based on greeting usage, politeness, the length of email and other characteristics.", "labels": [], "entities": []}, {"text": "1 https://www.cs.cmu.edu/ \u02dc enron/  We conduct a comparative evaluation of two different generation algorithms, template-based generation and stochastic generation, on the same email structures.", "labels": [], "entities": []}, {"text": "Given a structural label, template-based generation consisted of randomly selecting an intact whole sentence with the target structural label.", "labels": [], "entities": []}, {"text": "This could be termed sentencelevel NLG, while stochastic generation is wordlevel NLG.", "labels": [], "entities": []}, {"text": "We presented 30 pairs of (sentence-, word-) synthesized emails, and 7 subjects were asked to compare the overall coherence of an email, its sentence fluency and naturalness; then select their preference.", "labels": [], "entities": []}, {"text": "The experiments showed that wordbased stochastic generation outperforms or performs as well as the template-based algorithm for all criteria (coherence, fluency, naturalness, and preference).", "labels": [], "entities": []}, {"text": "Some subjects noted that neither email seemed human-written, perhaps an artifact of our experimental design.", "labels": [], "entities": []}, {"text": "Nevertheless, we believe that this stochastic approach would require less effort compared to most rule-based or template-based systems in terms of knowledge engineering.", "labels": [], "entities": []}, {"text": "In the future, we plan to develop an automatic email structural label annotator in order to build better language models (structure language models and content language models) by increasing training data, and then improve the naturalness of synthesized emails.", "labels": [], "entities": []}], "tableCaptions": []}