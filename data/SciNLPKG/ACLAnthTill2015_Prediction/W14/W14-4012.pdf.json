{"title": [{"text": "On the Properties of Neural Machine Translation: Encoder-Decoder Approaches", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 21, "end_pos": 47, "type": "TASK", "confidence": 0.6518547932306925}]}], "abstractContent": [{"text": "Neural machine translation is a relatively new approach to statistical machine translation based purely on neural networks.", "labels": [], "entities": [{"text": "Neural machine translation", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8019165198008219}, {"text": "statistical machine translation", "start_pos": 59, "end_pos": 90, "type": "TASK", "confidence": 0.6548572182655334}]}, {"text": "The neural machine translation models often consist of an encoder and a decoder.", "labels": [], "entities": []}, {"text": "The encoder extracts a fixed-length representation from a variable-length input sentence , and the decoder generates a correct translation from this representation.", "labels": [], "entities": []}, {"text": "In this paper, we focus on analyzing the properties of the neural machine translation using two models; RNN Encoder-Decoder and a newly proposed gated recursive con-volutional neural network.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 59, "end_pos": 85, "type": "TASK", "confidence": 0.7725946108500162}]}, {"text": "We show that the neural machine translation performs relatively well on short sentences without unknown words, but its performance degrades rapidly as the length of the sentence and the number of unknown words increase.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 17, "end_pos": 43, "type": "TASK", "confidence": 0.7252717614173889}]}, {"text": "Furthermore, we find that the proposed gated recursive convolutional network learns a grammatical structure of a sentence automatically.", "labels": [], "entities": []}], "introductionContent": [{"text": "A new approach for statistical machine translation based purely on neural networks has recently been proposed).", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 19, "end_pos": 50, "type": "TASK", "confidence": 0.7528500954310099}]}, {"text": "This new approach, which we refer to as neural machine translation, is inspired by the recent trend of deep representational learning.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 40, "end_pos": 66, "type": "TASK", "confidence": 0.7298944791158041}]}, {"text": "All the neural network models used in) consist of an encoder and a decoder.", "labels": [], "entities": []}, {"text": "The encoder extracts a fixed-length vector representation from a variablelength input sentence, and from this representation the decoder generates a correct, variable-length target translation.", "labels": [], "entities": []}, {"text": "* Research done while visiting Universit\u00e9 de Montr\u00e9al The emergence of the neural machine translation is highly significant, both practically and theoretically.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 75, "end_pos": 101, "type": "TASK", "confidence": 0.7587485313415527}]}, {"text": "Neural machine translation models require only a fraction of the memory needed by traditional statistical machine translation (SMT) models.", "labels": [], "entities": [{"text": "Neural machine translation", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.6959730088710785}, {"text": "statistical machine translation (SMT)", "start_pos": 94, "end_pos": 131, "type": "TASK", "confidence": 0.7960806488990784}]}, {"text": "The models we trained for this paper require only 500MB of memory in total.", "labels": [], "entities": []}, {"text": "This stands in stark contrast with existing SMT systems, which often require tens of gigabytes of memory.", "labels": [], "entities": [{"text": "SMT", "start_pos": 44, "end_pos": 47, "type": "TASK", "confidence": 0.9907684922218323}]}, {"text": "This makes the neural machine translation appealing in practice.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 15, "end_pos": 41, "type": "TASK", "confidence": 0.6985660394032797}]}, {"text": "Furthermore, unlike conventional translation systems, each and every component of the neural translation model is trained jointly to maximize the translation performance.", "labels": [], "entities": []}, {"text": "As this approach is relatively new, there has not been much work on analyzing the properties and behavior of these models.", "labels": [], "entities": []}, {"text": "For instance: What are the properties of sentences on which this approach performs better?", "labels": [], "entities": []}, {"text": "How does the choice of source/target vocabulary affect the performance?", "labels": [], "entities": []}, {"text": "In which cases does the neural machine translation fail?", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 24, "end_pos": 50, "type": "TASK", "confidence": 0.6938977241516113}]}, {"text": "It is crucial to understand the properties and behavior of this new neural machine translation approach in order to determine future research directions.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 68, "end_pos": 94, "type": "TASK", "confidence": 0.7433343132336935}]}, {"text": "Also, understanding the weaknesses and strengths of neural machine translation might lead to better ways of integrating SMT and neural machine translation systems.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 52, "end_pos": 78, "type": "TASK", "confidence": 0.6919287244478861}, {"text": "SMT", "start_pos": 120, "end_pos": 123, "type": "TASK", "confidence": 0.9947209358215332}]}, {"text": "In this paper, we analyze two neural machine translation models.", "labels": [], "entities": []}, {"text": "One of them is the RNN Encoder-Decoder that was proposed recently in ().", "labels": [], "entities": [{"text": "RNN Encoder-Decoder", "start_pos": 19, "end_pos": 38, "type": "DATASET", "confidence": 0.851582795381546}]}, {"text": "The other model replaces the encoder in the RNN Encoder-Decoder model with a novel neural network, which we calla gated recursive convolutional neural network (grConv).", "labels": [], "entities": []}, {"text": "We evaluate these two models on the task of translation from French to English.", "labels": [], "entities": [{"text": "translation from French to English", "start_pos": 44, "end_pos": 78, "type": "TASK", "confidence": 0.8909288048744202}]}, {"text": "Our analysis shows that the performance of the neural machine translation model degrades quickly as the length of a source sentence increases.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 47, "end_pos": 73, "type": "TASK", "confidence": 0.7352123856544495}]}, {"text": "Furthermore, we find that the vocabulary size has a high impact on the translation performance.", "labels": [], "entities": [{"text": "translation", "start_pos": 71, "end_pos": 82, "type": "TASK", "confidence": 0.9541491270065308}]}, {"text": "Nonetheless, qualitatively we find that the both models are able to generate correct translations most of the time.", "labels": [], "entities": []}, {"text": "Furthermore, the newly proposed grConv model is able to learn, without supervision, a kind of syntactic structure over the source language.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate the encoder-decoder models on the task of English-to-French translation.", "labels": [], "entities": [{"text": "English-to-French translation", "start_pos": 54, "end_pos": 83, "type": "TASK", "confidence": 0.6834137588739395}]}, {"text": "We use the bilingual, parallel corpus which is a set of 348M selected by the method in) from a combination of Europarl (61M words), news commentary (5.5M), UN (421M) and two crawled corpora of 90M and 780M words respectively.", "labels": [], "entities": [{"text": "Europarl", "start_pos": 110, "end_pos": 118, "type": "DATASET", "confidence": 0.9850039482116699}]}, {"text": "We did not use separate monolingual data.", "labels": [], "entities": []}, {"text": "The performance of the neural machien translation models was measured on the news-test2012, news-test2013 and news-test2014 sets ( 3000 lines each).", "labels": [], "entities": [{"text": "news-test2013", "start_pos": 92, "end_pos": 105, "type": "DATASET", "confidence": 0.8828040361404419}, {"text": "news-test2014 sets", "start_pos": 110, "end_pos": 128, "type": "DATASET", "confidence": 0.8453677296638489}]}, {"text": "When comparing to the SMT system, we use news-test2012 and news-test2013 as our development set for tuning the SMT system, and news-test2014 as our test set.", "labels": [], "entities": [{"text": "SMT", "start_pos": 22, "end_pos": 25, "type": "TASK", "confidence": 0.9817065596580505}, {"text": "SMT", "start_pos": 111, "end_pos": 114, "type": "TASK", "confidence": 0.9647586345672607}]}, {"text": "Among all the sentence pairs in the prepared parallel corpus, for reasons of computational efficiency we only use the pairs where both English and French sentences are at most 30 words long to train neural networks.", "labels": [], "entities": []}, {"text": "Furthermore, we use only the 30,000 most frequent words for both English and French.", "labels": [], "entities": []}, {"text": "All the other rare words are consid-ered unknown and are mapped to a special token ().", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: BLEU scores computed on the development and test sets. The top three rows show the scores on  all the sentences, and the bottom three rows on the sentences having no unknown words. () The result  reported in (Cho et al., 2014) where the RNNenc was used to score phrase pairs in the phrase table. (\u2022)", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9982067346572876}]}]}