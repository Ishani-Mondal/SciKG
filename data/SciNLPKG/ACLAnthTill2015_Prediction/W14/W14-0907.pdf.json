{"title": [{"text": "Parsing Screenplays for Extracting Social Networks from Movies", "labels": [], "entities": [{"text": "Parsing Screenplays for Extracting Social Networks from Movies", "start_pos": 0, "end_pos": 62, "type": "TASK", "confidence": 0.8127287104725838}]}], "abstractContent": [{"text": "In this paper, we present a formalization of the task of parsing movie screenplays.", "labels": [], "entities": [{"text": "parsing movie screenplays", "start_pos": 57, "end_pos": 82, "type": "TASK", "confidence": 0.9427956938743591}]}, {"text": "While researchers have previously motivated the need for parsing movie screen-plays, to the best of our knowledge, there is no work that has presented an evaluation for the task.", "labels": [], "entities": [{"text": "parsing movie screen-plays", "start_pos": 57, "end_pos": 83, "type": "TASK", "confidence": 0.9133963783582052}]}, {"text": "Moreover, all the approaches in the literature thus far have been regular expression based.", "labels": [], "entities": []}, {"text": "In this paper , we present an NLP and ML based approach to the task, and show that this approach outperforms the regular expression based approach by a large and statistically significant margin.", "labels": [], "entities": []}, {"text": "One of the main challenges we faced early on was the absence of training and test data.", "labels": [], "entities": []}, {"text": "We propose a methodology for using well struc-tured screenplays to create training data for anticipated anomalies in the structure of screenplays.", "labels": [], "entities": []}], "introductionContent": [{"text": "Social network extraction from unstructured text has recently gained much attention.", "labels": [], "entities": [{"text": "Social network extraction from unstructured text", "start_pos": 0, "end_pos": 48, "type": "TASK", "confidence": 0.7810606608788172}]}, {"text": "Using Natural Language Processing (NLP) and Machine Learning (ML) techniques, researchers are now able to gain access to networks that are not associated with any meta-data (such as email links and self-declared friendship links).", "labels": [], "entities": []}, {"text": "Movies, which can be seen as visual approximations of unstructured literary works, contain rich social networks formed by interactions between characters.", "labels": [], "entities": []}, {"text": "There has been some effort in the past to extract social networks from movies ().", "labels": [], "entities": []}, {"text": "However, these approaches are primarily regular expression based with no evaluation of how well they work.", "labels": [], "entities": []}, {"text": "In this paper we introduce a formalization of the task of parsing screenplays and present an NLP and ML based approach to the task.", "labels": [], "entities": [{"text": "parsing screenplays", "start_pos": 58, "end_pos": 77, "type": "TASK", "confidence": 0.9378093481063843}]}, {"text": "By parsing a screenplay, we mean assigning each line of the screenplay one of the following five tags: \"S\" for scene boundary, \"N\" for scene description, \"C\" for character name, \"D\" for dialogue, and \"M\" for meta-data.", "labels": [], "entities": []}, {"text": "We expect screenplays to conform to a strict grammar but they often do not).", "labels": [], "entities": []}, {"text": "This disconnect gives rise to the need for developing a methodology that is able to handle anomalies in the structure of screenplays.", "labels": [], "entities": []}, {"text": "Though the methodology proposed in this paper is in the context of movie screenplays, we believe, it is general and applicable to parse other kinds of noisy documents.", "labels": [], "entities": []}, {"text": "One of the earliest challenges we faced was the absence of training and test data.", "labels": [], "entities": []}, {"text": "Screenplays, on average, have 7,000 lines of text, which limits the amount of annotated data we can obtain from humans.", "labels": [], "entities": []}, {"text": "We propose a methodology for using well structured screenplays to create training data for anticipated anomalies in the structure of screenplays.", "labels": [], "entities": []}, {"text": "For different types of anomalies, we train separate classifiers, and combine them using ensemble learning.", "labels": [], "entities": []}, {"text": "We show that our ensemble outperforms a regular-expression baseline by a large and statistically significant margin on an unseen test set (0.69 versus 0.96 macro-F1 measure for the five classes).", "labels": [], "entities": []}, {"text": "Apart from performing an intrinsic evaluation, we also present an extrinsic evaluation.", "labels": [], "entities": []}, {"text": "We show that the social network extracted from the screenplay tagged by our ensemble is closer to the network extracted from a screenplay tagged by a human, as compared to the network extracted from a screenplay tagged by the baseline.", "labels": [], "entities": []}, {"text": "The rest of the paper is structured as follows: in section 2, we present common terminology used to describe screenplays.", "labels": [], "entities": []}, {"text": "We survey existing literature in section 3.", "labels": [], "entities": []}, {"text": "Section 4 presents details of our data collection methodology, along with the data distribution.", "labels": [], "entities": []}, {"text": "Section 5 gives details of our regular-expression based system, which we use as a baseline for evaluation purposes.", "labels": [], "entities": []}, {"text": "In section 6, we present our machine learning approach.", "labels": [], "entities": []}, {"text": "In section 7, we give details of the features we use for machine learning.", "labels": [], "entities": [{"text": "machine learning", "start_pos": 57, "end_pos": 73, "type": "TASK", "confidence": 0.7482390701770782}]}, {"text": "In section 8, we present our experiments and results.", "labels": [], "entities": []}, {"text": "We conclude and give future directions of research in section 9.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we present experiments and results for the task of tagging the lines of a screenplay with one of five tags: {S, N, C, D, M}.", "labels": [], "entities": []}, {"text": "For parameter tuning, we use DEV1 (section 8.1).", "labels": [], "entities": [{"text": "parameter tuning", "start_pos": 4, "end_pos": 20, "type": "TASK", "confidence": 0.70345139503479}, {"text": "DEV1", "start_pos": 29, "end_pos": 33, "type": "DATASET", "confidence": 0.8717911243438721}]}, {"text": "We train separate models on different types of known and anticipated anomalies (as discussed in section 6).", "labels": [], "entities": []}, {"text": "In section 8.2, we present strategies for combining these models.", "labels": [], "entities": []}, {"text": "We select the right combination of models and features by tuning on DEV2.", "labels": [], "entities": [{"text": "DEV2", "start_pos": 68, "end_pos": 72, "type": "DATASET", "confidence": 0.9812910556793213}]}, {"text": "Finally, we show results on the test set, TEST.", "labels": [], "entities": [{"text": "TEST", "start_pos": 42, "end_pos": 46, "type": "METRIC", "confidence": 0.9932742118835449}]}, {"text": "For all our experiments, we use the default parameters of SVM as implemented by the SMO algorithm of Weka ().", "labels": [], "entities": []}, {"text": "We use a linear kernel.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Comparison of performance (macro-F1 measure) of our rule based baseline with our machine  learning based models on development sets DEV1_000, DEV1_001, ..., DEV1_111. All models are  trained on 50% of the training set, with the feature space including CONTEXT equal to 1.", "labels": [], "entities": [{"text": "DEV1_000", "start_pos": 142, "end_pos": 150, "type": "DATASET", "confidence": 0.8684836626052856}, {"text": "DEV1_111", "start_pos": 167, "end_pos": 175, "type": "DATASET", "confidence": 0.8498533964157104}, {"text": "CONTEXT", "start_pos": 262, "end_pos": 269, "type": "METRIC", "confidence": 0.9986940026283264}]}, {"text": " Table 3: Macro-F1 measure for the five classes for testing on DEV2 set. 000 refers to the model trained  on data TRAIN_000, 001 refers to the model trained on data TRAIN_001, and so on. MAJ, MAX, and  MAJ-MAX are the three ensembles. The first column is the movie name. LTC refers to the movie \"The  Last Temptation of Christ.\"", "labels": [], "entities": [{"text": "DEV2 set", "start_pos": 63, "end_pos": 71, "type": "DATASET", "confidence": 0.9747787714004517}]}, {"text": " Table 4: Performance of MAJ-MAX classifier with  feature removal. Statistically significant differ- ences are in bold.", "labels": [], "entities": []}, {"text": " Table 5: Performance comparison of our rule  based baseline with our best machine learning  model on the five classes.", "labels": [], "entities": []}, {"text": " Table 6: A comparison of network statistics for  the three networks extracted from the movie Silver  Linings Playbook.", "labels": [], "entities": []}, {"text": " Table 7: A comparison of Pearson's correlation coefficients of various centrality measures for N B and  N MAJ-MAX with N G .", "labels": [], "entities": []}]}