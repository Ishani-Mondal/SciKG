{"title": [{"text": "Unsupervised Word Segmentation Improves Dialectal Arabic to English Machine Translation", "labels": [], "entities": [{"text": "Unsupervised Word Segmentation Improves Dialectal Arabic to English Machine Translation", "start_pos": 0, "end_pos": 87, "type": "TASK", "confidence": 0.8047217547893524}]}], "abstractContent": [{"text": "We demonstrate the feasibility of using unsupervised morphological segmentation for dialects of Arabic, which are poor in linguistics resources.", "labels": [], "entities": []}, {"text": "Our experiments using a Qatari Arabic to English machine translation system show that unsupervised segmentation helps to improve the translation quality as compared to using no seg-mentation or to using ATB segmentation, which was especially designed for Modern Standard Arabic (MSA).", "labels": [], "entities": [{"text": "Qatari Arabic to English machine translation", "start_pos": 24, "end_pos": 68, "type": "TASK", "confidence": 0.5268343488375345}, {"text": "ATB segmentation", "start_pos": 203, "end_pos": 219, "type": "TASK", "confidence": 0.6638282835483551}, {"text": "Modern Standard Arabic (MSA)", "start_pos": 255, "end_pos": 283, "type": "DATASET", "confidence": 0.8410068154335022}]}, {"text": "We use MSA and other dialects to improve Qatari Ara-bic to English machine translation, and we show that a uniform segmentation scheme across them yields an improvement of 1.5 BLEU points over using no segmentation.", "labels": [], "entities": [{"text": "English machine translation", "start_pos": 59, "end_pos": 86, "type": "TASK", "confidence": 0.6210537652174631}, {"text": "BLEU", "start_pos": 176, "end_pos": 180, "type": "METRIC", "confidence": 0.9993057250976562}]}], "introductionContent": [{"text": "The Arabic language has many varieties, where the Modern Standard Arabic (MSA) coexists with various dialects.", "labels": [], "entities": [{"text": "Modern Standard Arabic (MSA)", "start_pos": 50, "end_pos": 78, "type": "DATASET", "confidence": 0.735821599761645}]}, {"text": "Dialects differ from MSA and from each other lexically, phonologically, morphologically and syntactically.", "labels": [], "entities": []}, {"text": "MSA has standard orthography and is used informal contexts (e.g., publications, newspaper articles, etc.), while the dialects are usually limited to daily verbal interactions.", "labels": [], "entities": [{"text": "MSA", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.904065728187561}]}, {"text": "However, with the recent rise of social media, it has become increasingly common to use dialects in written communication as well, which has constituted the research in dialectal Arabic (DA) as a separate field within the broader field of natural language processing (NLP).", "labels": [], "entities": []}, {"text": "As DA NLP is still in its infancy, there is lack of basic computational resources and tools, which are needed in order to apply standard NLP approaches to the dialects of Arabic.", "labels": [], "entities": [{"text": "DA NLP", "start_pos": 3, "end_pos": 9, "type": "TASK", "confidence": 0.6125818639993668}]}, {"text": "For instance, statistical approaches need a lot of training data, which makes it very hard, if not impossible, to apply them to resource-poor languages; this is especially true for statistical machine translation (SMT) of Arabic dialects.", "labels": [], "entities": [{"text": "statistical machine translation (SMT) of Arabic dialects", "start_pos": 181, "end_pos": 237, "type": "TASK", "confidence": 0.8245047562652164}]}, {"text": "The Arabic language and its dialects are highly inflectional, and a word can appear in many more inflected forms compared to English.", "labels": [], "entities": []}, {"text": "Consider the Arabic words , , , and : they all belong to one root word 'playing' /lEb/.", "labels": [], "entities": []}, {"text": "Each morphological variation is derived from a root word with different affixes addressing different functions.", "labels": [], "entities": []}, {"text": "This causes data sparseness, and covering all possible word forms of a root word may not be always possible.", "labels": [], "entities": []}, {"text": "Considering the different variants of Arabic, the problem is exacerabated as dialects could use different choices of affixes for the same function.", "labels": [], "entities": []}, {"text": "For example, the MSA word /yalEabuwn/, meaning 'they are playing', could be found as /ylEbuwn/ in Gulf, as /Eam yilEabuA/ in Levantine, and as /biylEabwA/ in Egyptian Arabic.", "labels": [], "entities": []}, {"text": "One possible solution is to use a morphological segmenter that segments words into simpler units such as stems and affixes, which might be covered in the training set ().", "labels": [], "entities": []}, {"text": "When applied to dialects, this may reduce the lexical gap between dialects and MSA by matching the common stems.", "labels": [], "entities": []}, {"text": "Unfortunately, there are no standard morphological segmentation tools for dialects.", "labels": [], "entities": []}, {"text": "Due to the difference in morphology, tools designed for MSA do notwork well for dialects.", "labels": [], "entities": []}, {"text": "Developing rule-based segmenters for each dialect might appear to be the ideal solution, but, as the orthography of dialects is not standardized, crafting linguistic rules for them is very hard.", "labels": [], "entities": []}, {"text": "In this paper, we focus on training an unsupervised model for word segmentation, which we apply to SMT fora given Arabic dialect.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 62, "end_pos": 79, "type": "TASK", "confidence": 0.7566844820976257}, {"text": "SMT", "start_pos": 99, "end_pos": 102, "type": "TASK", "confidence": 0.9861065149307251}]}, {"text": "We train a pre-existing unsupervised segmentation model on the Arabic side of the training bi-text (and on some other monolingual data), and then we optimize its parameters based on the resulting SMT quality.", "labels": [], "entities": [{"text": "SMT", "start_pos": 196, "end_pos": 199, "type": "TASK", "confidence": 0.9860868453979492}]}, {"text": "Similarly, a multi-dialectal word segmenter could be developed by training on multi-dialectal data.", "labels": [], "entities": [{"text": "multi-dialectal word segmenter", "start_pos": 13, "end_pos": 43, "type": "TASK", "confidence": 0.6503017048041025}]}, {"text": "In particular, we develop a Qatari Arabic to English (QA-EN) SMT system, which we train on a small pre-existing bi-text.", "labels": [], "entities": [{"text": "Qatari Arabic to English (QA-EN) SMT", "start_pos": 28, "end_pos": 64, "type": "TASK", "confidence": 0.43499892950057983}]}, {"text": "As part of the development of the unsupervised segmentation model, we also collected some additional monolingual data for Qatari Arabic.", "labels": [], "entities": []}, {"text": "Qatari Arabic is a subdialect of the more general Gulf dialect, among with Saudi, Kuwaiti, Emirati, Bahraini, and Omani; we collected additional monologual data for each of these subdialects, and we release this data to the research community.", "labels": [], "entities": []}, {"text": "We train an unsupervised segmentation tool, Morphessor, and its MAP model, using different variations of the collected Qatari data.", "labels": [], "entities": [{"text": "Qatari data", "start_pos": 119, "end_pos": 130, "type": "DATASET", "confidence": 0.7966760098934174}]}, {"text": "We optimize the single hyperparameter of the MAP model by maximizing the translation quality of the QA-EN SMT system in terms of BLEU.", "labels": [], "entities": [{"text": "SMT", "start_pos": 106, "end_pos": 109, "type": "TASK", "confidence": 0.6429768800735474}, {"text": "BLEU", "start_pos": 129, "end_pos": 133, "type": "METRIC", "confidence": 0.9974512457847595}]}, {"text": "Our experimental results demonstrate that the resulting unsupervised segmenter yields improvements in translation quality when compared to (i) using no segmentation and (ii) using an MSA-based ATB segmenter.", "labels": [], "entities": []}, {"text": "We further develop a multi-dialectal word segmentation model, which we train on the Arabic side of the multi-dialectal training data, which consists of Qatari Arabic, Egyptian Arabic (EGY), Levantine Arabic (LEV) and MSA to English, i.e., a scaled combination of all the available parallel data.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 37, "end_pos": 54, "type": "TASK", "confidence": 0.7360977530479431}]}, {"text": "We train a QA-EN SMT system using the segmented multi-dialectal data, and we show an absolute gain of 1.5 BLEU points compared to a baseline that uses no segmentation.", "labels": [], "entities": [{"text": "QA-EN SMT", "start_pos": 11, "end_pos": 20, "type": "TASK", "confidence": 0.5468669086694717}, {"text": "BLEU", "start_pos": 106, "end_pos": 110, "type": "METRIC", "confidence": 0.9986714124679565}]}, {"text": "The rest of the paper is organized as follows: First, we provide an overview of related work on Dialectal Arabic NLP (Section 2).", "labels": [], "entities": [{"text": "Dialectal Arabic NLP", "start_pos": 96, "end_pos": 116, "type": "TASK", "confidence": 0.5314451456069946}]}, {"text": "Next, we discuss and we illustrate the linguistic differences between different Arabic dialects in comparison with and with a focus on Qatari Arabic (Section 3).", "labels": [], "entities": []}, {"text": "Then, we provide statistics about the corpora we collected and used in our experiments, followed by an illustration of the orthographic normalization schemes we applied (Section 4).", "labels": [], "entities": []}, {"text": "We next provide a high-level description of our approach, which uses morphological segmentation to combine resources for other Arabic dialects in a QA-EN SMT system effectively (Section 4.3).", "labels": [], "entities": [{"text": "QA-EN SMT", "start_pos": 148, "end_pos": 157, "type": "TASK", "confidence": 0.4684279263019562}]}, {"text": "We also explain our experimental setup and we present the results (Section 5).", "labels": [], "entities": []}, {"text": "We then discuss translating in the reverse direction, i.e., into Qatari Arabic (Section 6).", "labels": [], "entities": [{"text": "translating", "start_pos": 16, "end_pos": 27, "type": "TASK", "confidence": 0.9779313206672668}]}, {"text": "Finally, we point to possible directions for future work and we conclude the paper (Section 7).", "labels": [], "entities": []}], "datasetContent": [{"text": "We performed an extrinsic evaluation of the variations in segmentation by building a Qatari Arabic to English machine translation system on each of them.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 58, "end_pos": 70, "type": "TASK", "confidence": 0.9624157547950745}, {"text": "Qatari Arabic to English machine translation", "start_pos": 85, "end_pos": 129, "type": "TASK", "confidence": 0.47799401978651684}]}, {"text": "We also tested Morfessor on other available dialects and on MSA, and we will show below how a uniform segmentation can help to better adapt resources for dialects and MSA for SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 175, "end_pos": 178, "type": "TASK", "confidence": 0.9904430508613586}]}, {"text": "This section describes our experimental setup.", "labels": [], "entities": []}, {"text": "Datasets: We divided the QCA corpus into 1k sentences each for development and testing, and we used the remaining 12k for training.", "labels": [], "entities": [{"text": "QCA corpus", "start_pos": 25, "end_pos": 35, "type": "DATASET", "confidence": 0.8698312342166901}]}, {"text": "We adapted parallel corpora for Egyptian, Levantine and MSA to English to be used for Qatari Arabic to English SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 111, "end_pos": 114, "type": "TASK", "confidence": 0.8546484708786011}]}, {"text": "For MSA, we used parallel corpora of TED talks ( and the AMARA corpus (, which consists of educational videos.", "labels": [], "entities": [{"text": "AMARA corpus", "start_pos": 57, "end_pos": 69, "type": "DATASET", "confidence": 0.7444514632225037}]}, {"text": "Since the QCA corpus is in the speech domain, we believe that an MSA corpus of spoken domain would be more helpful than a text domain such as News.", "labels": [], "entities": []}, {"text": "For Egyptian and Levantine, we used the parallel corpus provided by.", "labels": [], "entities": []}, {"text": "There is no Gulf-English parallel data available in the literature.", "labels": [], "entities": []}, {"text": "The data that we found was a very small collection of subdialects of Gulf Arabic; we did not use it for MT experiments.", "labels": [], "entities": [{"text": "MT", "start_pos": 104, "end_pos": 106, "type": "TASK", "confidence": 0.9962424039840698}]}, {"text": "However, we used the Qatari part of the AVIA corpus to train Morfessor.", "labels": [], "entities": [{"text": "Qatari part of the AVIA corpus", "start_pos": 21, "end_pos": 51, "type": "DATASET", "confidence": 0.7968252946933111}]}, {"text": "Machine translation system settings: We used a phrase-based statistical machine translation model as implemented in the Moses toolkit () for machine translation.", "labels": [], "entities": [{"text": "Machine translation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8091451823711395}, {"text": "phrase-based statistical machine translation", "start_pos": 47, "end_pos": 91, "type": "TASK", "confidence": 0.5836544558405876}, {"text": "machine translation", "start_pos": 141, "end_pos": 160, "type": "TASK", "confidence": 0.8033716678619385}]}, {"text": "We built separate directed word alignments for source-to-target and target-to-source using IBM model 4 (, and we symmetrized them using the grow-diag-final-and heuristics (.", "labels": [], "entities": []}, {"text": "We then extracted phrase pairs with a maximum length of seven, and we scored them using maximum likelihood estimation with Kneser-Ney smoothing.", "labels": [], "entities": []}, {"text": "We also built a lexicalized reordering model, msd-bidirectional-fe. We built a 5-gram language model on the English side of QCA-train using KenLM).", "labels": [], "entities": [{"text": "QCA-train", "start_pos": 124, "end_pos": 133, "type": "DATASET", "confidence": 0.8887971639633179}]}, {"text": "Finally, we built a log-linear model using the above features.", "labels": [], "entities": []}, {"text": "We tuned the model weights by optimizing BLEU () on the tuning set, using PRO () with sentencelevel BLEU+1 optimization ().", "labels": [], "entities": [{"text": "BLEU", "start_pos": 41, "end_pos": 45, "type": "METRIC", "confidence": 0.9981170892715454}, {"text": "PRO", "start_pos": 74, "end_pos": 77, "type": "METRIC", "confidence": 0.9432470202445984}, {"text": "BLEU+1 optimization", "start_pos": 100, "end_pos": 119, "type": "METRIC", "confidence": 0.9438949823379517}]}, {"text": "In testing, we used minimum Bayes risk decoding (), cube pruning, and the operation sequence model).", "labels": [], "entities": []}, {"text": "Baseline: Our baseline Qatari Arabic to English MT system is trained on the QCA bitext without any segmentation of Qatari Arabic.", "labels": [], "entities": [{"text": "MT", "start_pos": 48, "end_pos": 50, "type": "TASK", "confidence": 0.681465744972229}, {"text": "QCA bitext", "start_pos": 76, "end_pos": 86, "type": "DATASET", "confidence": 0.902050644159317}]}, {"text": "For the experiments described in this paper, we used the English side of the QCA corpus for language modeling.", "labels": [], "entities": [{"text": "QCA corpus", "start_pos": 77, "end_pos": 87, "type": "DATASET", "confidence": 0.9088691771030426}, {"text": "language modeling", "start_pos": 92, "end_pos": 109, "type": "TASK", "confidence": 0.6862129420042038}]}, {"text": "In this section, we first present our work on using Morfessor for segmenting Qatari Arabic.", "labels": [], "entities": [{"text": "segmenting Qatari Arabic", "start_pos": 66, "end_pos": 90, "type": "TASK", "confidence": 0.8224264780680338}]}, {"text": "We tried different values of its parameter, and we trained it using corpora of different sizes to find balanced settings that improve SMT quality as compared with no segmentation and with segmentation using the Stanford ATB segmenter.", "labels": [], "entities": [{"text": "SMT", "start_pos": 134, "end_pos": 137, "type": "TASK", "confidence": 0.9927823543548584}, {"text": "segmentation", "start_pos": 188, "end_pos": 200, "type": "TASK", "confidence": 0.9702438712120056}, {"text": "Stanford ATB segmenter", "start_pos": 211, "end_pos": 233, "type": "DATASET", "confidence": 0.9148313403129578}]}, {"text": "We further applied our selected settings to segment MSA, EGY and LEV and used them for Qatari Arabic to English machine translation.", "labels": [], "entities": [{"text": "EGY", "start_pos": 57, "end_pos": 60, "type": "METRIC", "confidence": 0.9026650190353394}, {"text": "LEV", "start_pos": 65, "end_pos": 68, "type": "METRIC", "confidence": 0.9273259043693542}, {"text": "Qatari Arabic to English machine translation", "start_pos": 87, "end_pos": 131, "type": "TASK", "confidence": 0.4641240984201431}]}, {"text": "Our results show that a uniform segmentation scheme across different dialects improves machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 87, "end_pos": 106, "type": "TASK", "confidence": 0.7959151864051819}]}, {"text": "Morfessor training variations: We trained Morfessor using three corpora: (i) QCA, (ii) AVIA QA plus Qatari Novels, and (iii) a combination thereof.", "labels": [], "entities": []}, {"text": "shows the results for our SMT system when trained on the QCA parallel corpus, which was segmented using different training models of Morfessor with B = 40.", "labels": [], "entities": [{"text": "SMT", "start_pos": 26, "end_pos": 29, "type": "TASK", "confidence": 0.9947547912597656}, {"text": "QCA parallel corpus", "start_pos": 57, "end_pos": 76, "type": "DATASET", "confidence": 0.8546482523282369}, {"text": "B", "start_pos": 148, "end_pos": 149, "type": "METRIC", "confidence": 0.9718128442764282}]}, {"text": "The result for segmented Qatari Arabic is always better than the baseline, irrespective of the training model used for segmentation.", "labels": [], "entities": [{"text": "segmented Qatari Arabic", "start_pos": 15, "end_pos": 38, "type": "TASK", "confidence": 0.7873898347218832}]}, {"text": "We can see that the Morfessor model trained on a large monolingual corpus, i.e., on (ii) or (iii), yields better results.: The effect of varying the perplexity threshold parameter B of Morfessor on SMT quality.", "labels": [], "entities": [{"text": "SMT", "start_pos": 198, "end_pos": 201, "type": "TASK", "confidence": 0.9951689839363098}]}, {"text": "\"After merging\" are the results using the postprocessed Qatari segmented data.", "labels": [], "entities": [{"text": "Qatari segmented data", "start_pos": 56, "end_pos": 77, "type": "DATASET", "confidence": 0.7873585025469462}]}, {"text": "The high reduction in OOV in is because of the fine-grained segmentation.", "labels": [], "entities": [{"text": "OOV", "start_pos": 22, "end_pos": 25, "type": "METRIC", "confidence": 0.9959405660629272}]}, {"text": "We tried different values for the perplexity parameter B in order to find a good balance between better BLEU scores and linguistically correct segmentations.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 104, "end_pos": 108, "type": "METRIC", "confidence": 0.9983354210853577}]}, {"text": "The first part of shows the effect of different values of B on the quality of the machine translation system trained on AVIA QA , Qatari Novels.", "labels": [], "entities": [{"text": "B", "start_pos": 58, "end_pos": 59, "type": "METRIC", "confidence": 0.9945869445800781}, {"text": "machine translation", "start_pos": 82, "end_pos": 101, "type": "TASK", "confidence": 0.7158567607402802}, {"text": "AVIA QA , Qatari Novels", "start_pos": 120, "end_pos": 143, "type": "DATASET", "confidence": 0.7871115803718567}]}, {"text": "We achieved the best SMT score at B = 70.", "labels": [], "entities": [{"text": "SMT", "start_pos": 21, "end_pos": 24, "type": "TASK", "confidence": 0.9314447045326233}, {"text": "B", "start_pos": 34, "end_pos": 35, "type": "METRIC", "confidence": 0.9972614049911499}]}, {"text": "We further analyzed the output of Morfessor at B = 70 and we noticed that it tends to generate very small segments of length two and three characters long.", "labels": [], "entities": [{"text": "B", "start_pos": 47, "end_pos": 48, "type": "METRIC", "confidence": 0.9952364563941956}]}, {"text": "The segmentation produces more than one stem in a word and does not generate legal word units.", "labels": [], "entities": []}, {"text": "For example, the word . We apply a post-processing step that merges all stems in a word and affixes between them to one stem.", "labels": [], "entities": []}, {"text": "So, a word can have only one stem.", "labels": [], "entities": []}, {"text": "For example, the word would be segmented as PRE/ + STM/ + SUF/ . This yielded linguistically correct segmentations in many cases.", "labels": [], "entities": []}, {"text": "The second part of TableFor rest of the experiments in this paper, we used a value of 70 for the perplexity threshold parameter plus the post-processing on segmentation.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 156, "end_pos": 168, "type": "TASK", "confidence": 0.9643611311912537}]}, {"text": "We trained Morfessor on the concatenation of QCA, AVIA QA and Novels.", "labels": [], "entities": [{"text": "QCA", "start_pos": 45, "end_pos": 48, "type": "DATASET", "confidence": 0.7632920145988464}, {"text": "AVIA QA", "start_pos": 50, "end_pos": 57, "type": "DATASET", "confidence": 0.8964806497097015}]}, {"text": "7 Using other Arabic variations: In this section, we present experiments using MSA, EGY and LEV to English bitexts combined with the QCA bitext for Qatari Arabic to English machine translation.", "labels": [], "entities": [{"text": "EGY", "start_pos": 84, "end_pos": 87, "type": "METRIC", "confidence": 0.9432636499404907}, {"text": "LEV", "start_pos": 92, "end_pos": 95, "type": "METRIC", "confidence": 0.9030413031578064}, {"text": "Qatari Arabic to English machine translation", "start_pos": 148, "end_pos": 192, "type": "TASK", "confidence": 0.5953808426856995}]}, {"text": "We explored three segmentation options for the Arabic side of the data: (i) no segmentation, (ii) ATB segmentation, and (iii) unsupervised segmentation using Morfessor.", "labels": [], "entities": [{"text": "ATB segmentation", "start_pos": 98, "end_pos": 114, "type": "TASK", "confidence": 0.7992232739925385}, {"text": "Morfessor", "start_pos": 158, "end_pos": 167, "type": "DATASET", "confidence": 0.9236674308776855}]}, {"text": "The QCA corpus is of much smaller size compared to other Arabic variants, say MSA.", "labels": [], "entities": [{"text": "QCA corpus", "start_pos": 4, "end_pos": 14, "type": "DATASET", "confidence": 0.8106934130191803}]}, {"text": "It is possible that in the training of the machine translation models, the large corpus dominates the QCA corpus.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 43, "end_pos": 62, "type": "TASK", "confidence": 0.7185239493846893}, {"text": "QCA corpus", "start_pos": 102, "end_pos": 112, "type": "DATASET", "confidence": 0.804263710975647}]}, {"text": "In order to avoid that, we balanced the two corpora by replicating the smaller corpus X number of times in order to make it approximately equal to the large corpus.", "labels": [], "entities": []}, {"text": "The complete procedure is described below.", "labels": [], "entities": []}, {"text": "Ina nutshell, for building a machine translation system using the MSA plus Qatari corpus, we first balanced the Qatari corpus to make it approximately equal to MSA and concatenated them.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 29, "end_pos": 48, "type": "TASK", "confidence": 0.7158390283584595}, {"text": "Qatari corpus", "start_pos": 75, "end_pos": 88, "type": "DATASET", "confidence": 0.7340825498104095}, {"text": "Qatari corpus", "start_pos": 112, "end_pos": 125, "type": "DATASET", "confidence": 0.754698246717453}, {"text": "MSA", "start_pos": 160, "end_pos": 163, "type": "DATASET", "confidence": 0.9083058834075928}]}, {"text": "For training Morfessor, the Qatari Arabic data consisted of QCA, Novels and AVIA QA , while for SMT, it consisted of QCA only.", "labels": [], "entities": [{"text": "Qatari Arabic data", "start_pos": 28, "end_pos": 46, "type": "DATASET", "confidence": 0.6973664959271749}, {"text": "AVIA QA", "start_pos": 76, "end_pos": 83, "type": "METRIC", "confidence": 0.8473506271839142}, {"text": "SMT", "start_pos": 96, "end_pos": 99, "type": "TASK", "confidence": 0.9871149063110352}]}, {"text": "In both cases, we balanced it to be approximately equal to MSA.", "labels": [], "entities": [{"text": "MSA", "start_pos": 59, "end_pos": 62, "type": "DATASET", "confidence": 0.5048770904541016}]}, {"text": "We then trained Morfessor on the balanced (QCA, Novels, AVIA QA ) plus MSA data and we segmented the Arabic side of the balanced QCA plus MSA training data for machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 160, "end_pos": 179, "type": "TASK", "confidence": 0.8378840982913971}]}, {"text": "We built a machine translation system on the segmented data.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 11, "end_pos": 30, "type": "TASK", "confidence": 0.6914901882410049}]}, {"text": "We segmented the testing and tuning data sets similarly.", "labels": [], "entities": []}, {"text": "We used the same balancing when we combined EGY-EN and LEV-EN with the Qatari Arabic -English data.", "labels": [], "entities": [{"text": "LEV-EN", "start_pos": 55, "end_pos": 61, "type": "METRIC", "confidence": 0.8614738583564758}, {"text": "Qatari Arabic -English data", "start_pos": 71, "end_pos": 98, "type": "DATASET", "confidence": 0.6700072169303894}]}, {"text": "We also tried training multiple unsupervised models, but this yielded lower SMT quality compared to using a single model trained on multidialects.", "labels": [], "entities": [{"text": "SMT", "start_pos": 76, "end_pos": 79, "type": "TASK", "confidence": 0.9854559302330017}]}, {"text": "Using different models could result in having different segmentation schemes, which might not help in reducing the vocabulary mismatch between different variants of Arabic.", "labels": [], "entities": []}, {"text": "We did not see a big difference in training Morfessor with and without the QCA corpus, and we decided to use the complete data for training.", "labels": [], "entities": [{"text": "QCA corpus", "start_pos": 75, "end_pos": 85, "type": "DATASET", "confidence": 0.9291382431983948}]}, {"text": "8 Due to the spoken nature of the QCA corpus, it contains shorter sentences.", "labels": [], "entities": [{"text": "QCA corpus", "start_pos": 34, "end_pos": 44, "type": "DATASET", "confidence": 0.8678975701332092}]}, {"text": "Thus, we balanced the corpora based on the number of tokens rather than on the number of sentences.", "labels": [], "entities": []}, {"text": "There are two things to point here.", "labels": [], "entities": []}, {"text": "First, the SMT systems that used the unsupervised morphological segmenter, Morfessor, outperformed the systems that used no segmentation and those using the ATB segmentation.", "labels": [], "entities": [{"text": "SMT", "start_pos": 11, "end_pos": 14, "type": "TASK", "confidence": 0.9904009699821472}]}, {"text": "The Morfessor-based systems showed consistent improvements compared to the ATB-based systems over the no-segmentation systems.", "labels": [], "entities": []}, {"text": "This validates our point that unsupervised morphological segmentation generalizes well fora variety of dialects and these SMT results complement that.", "labels": [], "entities": [{"text": "SMT", "start_pos": 122, "end_pos": 125, "type": "TASK", "confidence": 0.9865946769714355}]}, {"text": "The second observation is that adding a bitext for other dialects and MSA improves machine translation quality for Qatari-English SMT.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 83, "end_pos": 102, "type": "TASK", "confidence": 0.680402934551239}, {"text": "SMT", "start_pos": 130, "end_pos": 133, "type": "TASK", "confidence": 0.6886106133460999}]}], "tableCaptions": [{"text": " Table 1: Statistics about the collected parallel cor- pora (in thousands). AVIA O shows the statistics  about the AVIA corpus excluding Qatari data.", "labels": [], "entities": [{"text": "AVIA O", "start_pos": 76, "end_pos": 82, "type": "DATASET", "confidence": 0.575514018535614}, {"text": "AVIA corpus excluding Qatari data", "start_pos": 115, "end_pos": 148, "type": "DATASET", "confidence": 0.7760906159877777}]}, {"text": " Table 2: Statistics about the collected monolingual  corpora (in thousands of words).", "labels": [], "entities": []}, {"text": " Table 3: Study of the effect of varying the train- ing datasets for Morfessor on the Qatari to English  SMT. \"Baseline\" shows the output of the MT sys- tem with no segmentation.", "labels": [], "entities": [{"text": "SMT", "start_pos": 105, "end_pos": 108, "type": "TASK", "confidence": 0.5074027180671692}, {"text": "MT sys- tem", "start_pos": 145, "end_pos": 156, "type": "TASK", "confidence": 0.5835343599319458}]}, {"text": " Table 4: The effect of varying the perplexity  threshold parameter B of Morfessor on SMT qual- ity. \"After merging\" are the results using the post- processed Qatari segmented data.", "labels": [], "entities": [{"text": "perplexity  threshold parameter B", "start_pos": 36, "end_pos": 69, "type": "METRIC", "confidence": 0.7271944582462311}, {"text": "SMT", "start_pos": 86, "end_pos": 89, "type": "TASK", "confidence": 0.9740439653396606}, {"text": "Qatari segmented data", "start_pos": 159, "end_pos": 180, "type": "DATASET", "confidence": 0.7188847661018372}]}, {"text": " Table 5: BLEU scores for Qatari Arabic to English  SMT using three different segmentation settings.  'QCA means the modified QCA corpus with num- ber of tokens approximately equal to MSA, EGY  and LEV in the respective experiments.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9991869330406189}, {"text": "SMT", "start_pos": 52, "end_pos": 55, "type": "TASK", "confidence": 0.6377533674240112}, {"text": "EGY", "start_pos": 189, "end_pos": 192, "type": "METRIC", "confidence": 0.9504162669181824}, {"text": "LEV", "start_pos": 198, "end_pos": 201, "type": "METRIC", "confidence": 0.9353204965591431}]}, {"text": " Table 6: Results for English to Qatari SMT for  varying language models. In all cases, the transla- tion model is trained on the QCA bitext only.", "labels": [], "entities": [{"text": "English to Qatari SMT", "start_pos": 22, "end_pos": 43, "type": "TASK", "confidence": 0.507391557097435}, {"text": "QCA bitext", "start_pos": 130, "end_pos": 140, "type": "DATASET", "confidence": 0.8857479095458984}]}]}