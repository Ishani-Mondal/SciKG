{"title": [{"text": "Formulating Queries for Collecting Training Examples in Visual Concept Classification", "labels": [], "entities": [{"text": "Formulating Queries", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.9143560528755188}, {"text": "Collecting Training", "start_pos": 24, "end_pos": 43, "type": "TASK", "confidence": 0.7803562581539154}, {"text": "Visual Concept Classification", "start_pos": 56, "end_pos": 85, "type": "TASK", "confidence": 0.6865019798278809}]}], "abstractContent": [{"text": "Video content can be automatically analysed and indexed using trained classifiers which map low-level features to semantic concepts.", "labels": [], "entities": []}, {"text": "Such classifiers need training data consisting of sets of images which contain such concepts and recently it has been discovered that such training data can be located using text-based search to image databases on the internet.", "labels": [], "entities": []}, {"text": "Formulating the text queries which locate these training images is the challenge we address here.", "labels": [], "entities": []}, {"text": "In this paper we present preliminary results on TRECVid data of concept classification using automatically crawled images as training data and we compare the results with those obtained from manually annotated training sets.", "labels": [], "entities": [{"text": "TRECVid data", "start_pos": 48, "end_pos": 60, "type": "DATASET", "confidence": 0.7168832272291183}, {"text": "concept classification", "start_pos": 64, "end_pos": 86, "type": "TASK", "confidence": 0.6983603090047836}]}], "introductionContent": [{"text": "Content-based access to video archives is based on learning the presence of semantic concepts in video content by mapping low-level features like colour and texture, to high-level concepts.", "labels": [], "entities": []}, {"text": "Concept classification is typically based on training classifiers on a set of annotated ground truth images (called a training set) containing positive and negative example images of a given semantic concept.", "labels": [], "entities": [{"text": "Concept classification", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8093783855438232}]}, {"text": "The manual creation of training sets for each concept is a time-consuming and costly task.", "labels": [], "entities": []}, {"text": "An alternative is to automatically gather training examples using available resources on the Internet.", "labels": [], "entities": []}, {"text": "Several recent papers have demonstrated the effectiveness of such an approach.", "labels": [], "entities": []}, {"text": "() used search engine results to gather material for learning the appearance of categories, shows that effective classifiers can be trained on-the-fly at query time using examples collected from Google Image search.", "labels": [], "entities": []}, {"text": "The AXES research search engine () uses a combination of pre-trained classifiers and on-the-fly classifiers trained using examples from Google Image search.", "labels": [], "entities": [{"text": "AXES research search engine", "start_pos": 4, "end_pos": 31, "type": "DATASET", "confidence": 0.865377277135849}]}, {"text": "() investigate four practices for collecting training negative and positive examples from socially tagged videos and images.", "labels": [], "entities": [{"text": "collecting training negative and positive examples from socially tagged videos and images", "start_pos": 34, "end_pos": 123, "type": "TASK", "confidence": 0.7113569031159083}]}, {"text": "The above work exploits the visual content of the collected example images while the question of how to formulate a textual query for collecting the data is not yet considered.", "labels": [], "entities": []}, {"text": "It is important to note here that current search engines are not use content-based image classifiers, they are based the text from the embedding pages, and that is not always accurate or scalable.", "labels": [], "entities": []}, {"text": "This represents a unique relationship between vision (the images used to train a concept classifier) and language (the text used to find those training images).", "labels": [], "entities": []}, {"text": "In this work, we initiate a first step to addressing the problem of formulating text queries that collect positive example images for classifier training.", "labels": [], "entities": [{"text": "formulating text queries that collect positive example images", "start_pos": 68, "end_pos": 129, "type": "TASK", "confidence": 0.7749987989664078}]}, {"text": "This first step is based on querying web resources with single-term queries and comparing the classification results with those from manually annotated training sets.", "labels": [], "entities": []}, {"text": "The results show the potential of automatic crawling and open the way for enhancing query formulation by adding external lexical or semantic resources.", "labels": [], "entities": [{"text": "query formulation", "start_pos": 84, "end_pos": 101, "type": "TASK", "confidence": 0.7874366343021393}]}], "datasetContent": [{"text": "Experiments were conducted on the TRECVid () 2014 semantic indexing development data set.", "labels": [], "entities": [{"text": "TRECVid () 2014 semantic indexing development data set", "start_pos": 34, "end_pos": 88, "type": "DATASET", "confidence": 0.816383384168148}]}, {"text": "Single-term queries were posted to two data sources: Google Images, and ImageNet (an image database organized according to the nouns of the WordNet hierarchy where each node is depicted by an average of +500 images).", "labels": [], "entities": []}, {"text": "Unlike results from Google Images, examples gathered from ImageNet are classified by human annotators, and are therefore a \"purer\" source of training images.", "labels": [], "entities": []}, {"text": "To ensure a high-quality training set, we first search for the concept in ImageNet; if the concept does not exist in as an ImageNet visual category, we use images retrieved using a search for the term on Google Images.", "labels": [], "entities": []}, {"text": "We carried out two experiments to evaluate the performance of classifiers trained on manually annotated (internal) data provided by TRECVid versus data gathered from external sources.", "labels": [], "entities": [{"text": "TRECVid", "start_pos": 132, "end_pos": 139, "type": "DATASET", "confidence": 0.7998149394989014}]}, {"text": "These external sources are search engines that retrieve images using textual queries, as explained in section2.", "labels": [], "entities": []}, {"text": "The first experiment used data from the 2013x subset of the TRECVid 2014 development data and the second used external training data gathered as discussed above in the first paragraph of this section.", "labels": [], "entities": [{"text": "2013x subset of the TRECVid 2014 development data", "start_pos": 40, "end_pos": 89, "type": "DATASET", "confidence": 0.8370475731790066}]}, {"text": "Accuracy in both cases was evaluated using inferred average precision (infAP) on the 2013y subset of the development data.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9890615940093994}, {"text": "inferred average precision (infAP)", "start_pos": 43, "end_pos": 77, "type": "METRIC", "confidence": 0.9174242118994395}, {"text": "2013y subset of the development data", "start_pos": 85, "end_pos": 121, "type": "DATASET", "confidence": 0.6843816041946411}]}, {"text": "One-vs-all linear SVM classifiers were used for both experiments, trained on visual features extracted using pre-trained deep convolutional neural networks (CNN) using the Caffe software.", "labels": [], "entities": []}, {"text": "Classifiers for 34 of the 60 concepts were trained using data from ImageNet and the remaining using examples from Google Images.", "labels": [], "entities": [{"text": "ImageNet", "start_pos": 67, "end_pos": 75, "type": "DATASET", "confidence": 0.9270511865615845}]}, {"text": "All classifiers trained using images from Google Images demonstrated poorer infAP than those trained on internal data.", "labels": [], "entities": [{"text": "infAP", "start_pos": 76, "end_pos": 81, "type": "METRIC", "confidence": 0.99774569272995}]}, {"text": "Of the 34 classifiers trained on ImageNet, 7 demonstrated improved infAP (airplane, beach, bicycling, classroom, computers, dancing, flowers, highway).", "labels": [], "entities": [{"text": "infAP", "start_pos": 67, "end_pos": 72, "type": "METRIC", "confidence": 0.9977924823760986}]}, {"text": "In all cases it was possible to find more positive examples on ImageNet than in the internal set.", "labels": [], "entities": [{"text": "ImageNet", "start_pos": 63, "end_pos": 71, "type": "DATASET", "confidence": 0.965204656124115}]}, {"text": "Internal out-performed ImageNet in the remaining 27 cases.", "labels": [], "entities": [{"text": "ImageNet", "start_pos": 23, "end_pos": 31, "type": "DATASET", "confidence": 0.8585388660430908}]}, {"text": "There were several possible reasons for this.", "labels": [], "entities": []}, {"text": "In many cases there were fewer examples from ImageNet than in the internal set (12/27 cases) and in some cases the ImageNet examples were incorrect.", "labels": [], "entities": [{"text": "ImageNet", "start_pos": 45, "end_pos": 53, "type": "DATASET", "confidence": 0.9600505828857422}, {"text": "ImageNet", "start_pos": 115, "end_pos": 123, "type": "DATASET", "confidence": 0.9282827377319336}]}, {"text": "For example, in the case of the concept \"hand\", several synsets matching the term consisted entirely of wristwatches.", "labels": [], "entities": []}, {"text": "Finally, in other cases, the concept text (the query) was either ambiguous or insufficiently semantically rich, for example \"greeting\" (greeting cards were retrieved) and \"government leader\" (smaller subset of such leaders in internal training data).", "labels": [], "entities": []}], "tableCaptions": []}