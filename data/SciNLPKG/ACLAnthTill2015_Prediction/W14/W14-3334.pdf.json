{"title": [{"text": "Estimating Word Alignment Quality for SMT Reordering Tasks", "labels": [], "entities": [{"text": "Estimating Word Alignment", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.7999824086825053}, {"text": "SMT Reordering Tasks", "start_pos": 38, "end_pos": 58, "type": "TASK", "confidence": 0.9726253350575765}]}], "abstractContent": [{"text": "Previous studies of the effect of word alignment on translation quality in SMT generally explore link level metrics only and mostly do not show any clear connections between alignment and SMT quality.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 34, "end_pos": 48, "type": "TASK", "confidence": 0.7082400918006897}, {"text": "SMT", "start_pos": 75, "end_pos": 78, "type": "TASK", "confidence": 0.8679149746894836}, {"text": "SMT", "start_pos": 188, "end_pos": 191, "type": "TASK", "confidence": 0.9858289957046509}]}, {"text": "In this paper, we specifically investigate the impact of word alignment on two pre-reordering tasks in translation, using a wider range of quality indicators than previously done.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 57, "end_pos": 71, "type": "TASK", "confidence": 0.7449537515640259}]}, {"text": "Experiments on German-English translation show that reordering may require alignment models different from those used by the core translation system.", "labels": [], "entities": []}, {"text": "Sparse alignments with high precision on the link level, for translation units, and on the subset of crossing links, like intersected HMM models, are preferred.", "labels": [], "entities": [{"text": "precision", "start_pos": 28, "end_pos": 37, "type": "METRIC", "confidence": 0.9970777034759521}]}, {"text": "Unlike SMT performance the desired alignment characteristics are similar for small and large training data for the pre-reordering tasks.", "labels": [], "entities": [{"text": "SMT", "start_pos": 7, "end_pos": 10, "type": "TASK", "confidence": 0.9947130084037781}]}, {"text": "Moreover, we confirm previous research showing that the fuzzy reordering score is a useful and cheap proxy for performance on SMT reordering tasks.", "labels": [], "entities": [{"text": "SMT reordering tasks", "start_pos": 126, "end_pos": 146, "type": "TASK", "confidence": 0.9276739358901978}]}], "introductionContent": [{"text": "Word alignment is a key component in all state-ofthe-art statistical machine translation (SMT) systems, and there has been some work exploring the connection between word alignment quality and translation quality).", "labels": [], "entities": [{"text": "Word alignment", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7367720603942871}, {"text": "statistical machine translation (SMT)", "start_pos": 57, "end_pos": 94, "type": "TASK", "confidence": 0.8244677782058716}, {"text": "word alignment", "start_pos": 166, "end_pos": 180, "type": "TASK", "confidence": 0.692945584654808}]}, {"text": "The standard way to evaluate word alignments in this context is by using metrics like alignment error rate (AER) and F-measure on the link level, and the general conclusion appears to be that translation quality benefits from alignments with high recall (rather than precision), at least for large training data.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 29, "end_pos": 44, "type": "TASK", "confidence": 0.697952538728714}, {"text": "alignment error rate (AER)", "start_pos": 86, "end_pos": 112, "type": "METRIC", "confidence": 0.9336705406506857}, {"text": "F-measure", "start_pos": 117, "end_pos": 126, "type": "METRIC", "confidence": 0.9928551316261292}, {"text": "recall", "start_pos": 247, "end_pos": 253, "type": "METRIC", "confidence": 0.99592125415802}, {"text": "precision", "start_pos": 267, "end_pos": 276, "type": "METRIC", "confidence": 0.9970998764038086}]}, {"text": "Although many other ways of measuring alignment quality have been proposed, such as working on translation units () or using link degree and related measures, these methods have not been used to study the relation between alignment and translation quality, with the exception of.", "labels": [], "entities": [{"text": "link degree", "start_pos": 125, "end_pos": 136, "type": "METRIC", "confidence": 0.8164974451065063}]}, {"text": "Word alignment is also used for many other tasks besides translation, including term bank creation, cross-lingual annotation projection for part-of-speech tagging (), semantic roles (), pronoun anaphora (), and cross-lingual clustering).", "labels": [], "entities": [{"text": "Word alignment", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.6976796835660934}, {"text": "translation", "start_pos": 57, "end_pos": 68, "type": "TASK", "confidence": 0.9610286951065063}, {"text": "term bank creation", "start_pos": 80, "end_pos": 98, "type": "TASK", "confidence": 0.676931897799174}, {"text": "cross-lingual annotation projection", "start_pos": 100, "end_pos": 135, "type": "TASK", "confidence": 0.6584167083104452}, {"text": "part-of-speech tagging", "start_pos": 140, "end_pos": 162, "type": "TASK", "confidence": 0.6675563007593155}, {"text": "cross-lingual clustering", "start_pos": 211, "end_pos": 235, "type": "TASK", "confidence": 0.7169161885976791}]}, {"text": "Even within SMT itself, there are tasks such as reordering that often make crucial use of word alignments.", "labels": [], "entities": [{"text": "SMT", "start_pos": 12, "end_pos": 15, "type": "TASK", "confidence": 0.9837467074394226}, {"text": "word alignments", "start_pos": 90, "end_pos": 105, "type": "TASK", "confidence": 0.6868435144424438}]}, {"text": "For instance, source language reordering commonly relies on rules learnt automatically from word-aligned data (e.g.,).", "labels": [], "entities": [{"text": "source language reordering", "start_pos": 14, "end_pos": 40, "type": "TASK", "confidence": 0.6378367145856222}]}, {"text": "As far as we know, no one has studied the impact of alignment quality on these additional tasks, and it seems to be tacitly assumed that alignments that are good for translation are also good for other tasks.", "labels": [], "entities": []}, {"text": "In this paper we set out to explore the impact of alignment quality on two pre-reordering tasks for SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 100, "end_pos": 103, "type": "TASK", "confidence": 0.9961110949516296}]}, {"text": "In doing so, we employ a wider range of quality indicators than is customary, and for reference these indicators are used also to assess overall translation quality.", "labels": [], "entities": []}, {"text": "To allow an in-depth exploration of the connections between several aspects of word alignment and reordering, we limit our study to one language pair, German-English.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 79, "end_pos": 93, "type": "TASK", "confidence": 0.7711836695671082}]}, {"text": "We think this is a suitable language pair for studying reordering since it has both short range and long range reorderings.", "labels": [], "entities": []}, {"text": "Our main focus is on using relatively large training data, 2M sentences, but we also report results with small training data, 170K sentences.", "labels": [], "entities": []}, {"text": "The main conclusion of our study is that alignments that are optimal for translation are not necessarily optimal for reordering, where pre-275 cision is of greater importance than recall.", "labels": [], "entities": [{"text": "translation", "start_pos": 73, "end_pos": 84, "type": "TASK", "confidence": 0.9717417359352112}, {"text": "recall", "start_pos": 180, "end_pos": 186, "type": "METRIC", "confidence": 0.9946064352989197}]}, {"text": "For SMT the best alignments are different depending on corpus size, but for the reordering tasks results are stable across training data size.", "labels": [], "entities": [{"text": "SMT", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.9931729435920715}]}, {"text": "In section 2 we discuss previous work related to word alignment and SMT.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 49, "end_pos": 63, "type": "TASK", "confidence": 0.8242637813091278}, {"text": "SMT", "start_pos": 68, "end_pos": 71, "type": "TASK", "confidence": 0.9921355247497559}]}, {"text": "In section 3, we introduce the word alignment quality indicators we use, and show experimental results fora number of alignment systems on an SMT task.", "labels": [], "entities": [{"text": "SMT task", "start_pos": 142, "end_pos": 150, "type": "TASK", "confidence": 0.94216188788414}]}, {"text": "In section 4, we turn to reordering for SMT and use the same quality indicators to study the impact of alignment quality on reordering quality.", "labels": [], "entities": [{"text": "SMT", "start_pos": 40, "end_pos": 43, "type": "TASK", "confidence": 0.9902178645133972}]}, {"text": "In section 5 we briefly describe results using small training data.", "labels": [], "entities": []}, {"text": "In section 6, we conclude and suggest directions for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "We perform all our experiments for GermanEnglish.", "labels": [], "entities": [{"text": "GermanEnglish", "start_pos": 35, "end_pos": 48, "type": "DATASET", "confidence": 0.9846077561378479}]}, {"text": "The alignment indicators are calculated on a corpus of 987 hand aligned sentences (Pado and).", "labels": [], "entities": []}, {"text": "The gold standard contains explicit null links, which the symmetrized automatic alignments do not.", "labels": [], "entities": []}, {"text": "To allow a straightforward comparison we consistently remove all null links when comparing system alignments to the gold standard.", "labels": [], "entities": []}, {"text": "For creating the automatic alignments we used GIZA++ ( to compute directional alignments for model 2-4 and the HMM model, and fast align (fa) ( as newer alternatives to model 2.", "labels": [], "entities": [{"text": "GIZA", "start_pos": 46, "end_pos": 50, "type": "METRIC", "confidence": 0.7749148011207581}]}, {"text": "These models require large amounts of data to be estimated reliably.", "labels": [], "entities": []}, {"text": "To achieve this we concatenated the gold standard with the large SMT training data (see   Section 3.2) of 2M sentences during alignment.", "labels": [], "entities": [{"text": "SMT", "start_pos": 65, "end_pos": 68, "type": "TASK", "confidence": 0.9890972375869751}]}, {"text": "For symmetrization we used all methods in, as implemented in the Moses toolkit ( and in fast align ().", "labels": [], "entities": []}, {"text": "Based on the automatically aligned gold standard, we calculated all alignment indicators for all settings.", "labels": [], "entities": [{"text": "alignment", "start_pos": 68, "end_pos": 77, "type": "METRIC", "confidence": 0.9777381420135498}]}, {"text": "The complete results can be found in, where we have ordered the symmetrization methods with the most sparse, intersection, on top.", "labels": [], "entities": []}, {"text": "Overall we can see that while several of the alignment methods create a much higher number of alignment links than the gold standard, they do not produce many more translation units.", "labels": [], "entities": []}, {"text": "This is very interesting and indicates why link level statistics may not be accurate enough to predict the performance of certain downstream applications.", "labels": [], "entities": []}, {"text": "As expected, the metric scores for translation units are lower than for link level metrics.", "labels": [], "entities": [{"text": "translation", "start_pos": 35, "end_pos": 46, "type": "TASK", "confidence": 0.9556986689567566}]}, {"text": "This is partly due to the fact that these measures do not count any partially correct links; the MWU metrics which considers partial matches often have higher scores than link level metrics.", "labels": [], "entities": [{"text": "MWU metrics", "start_pos": 97, "end_pos": 108, "type": "DATASET", "confidence": 0.8533943593502045}]}, {"text": "Another finding is that the number of crossings vary a lot with more than twice as many as the reference for model2+union, and less than three times as many for HMM+intersection.", "labels": [], "entities": [{"text": "HMM+intersection", "start_pos": 161, "end_pos": 177, "type": "TASK", "confidence": 0.5180381238460541}]}, {"text": "The HMM and fa models have fewer reorderings than the IBM models.", "labels": [], "entities": []}, {"text": "We are now interested in the relation between alignment evaluation on the link level and on the translation unit level, which has not been thoroughly investigated before.", "labels": [], "entities": [{"text": "alignment evaluation", "start_pos": 46, "end_pos": 66, "type": "TASK", "confidence": 0.9843035340309143}]}, {"text": "shows the correlations between the various metrics.", "labels": [], "entities": []}, {"text": "Both precision and F-measure at the link level have significant correlations to all TU metrics.", "labels": [], "entities": [{"text": "precision", "start_pos": 5, "end_pos": 14, "type": "METRIC", "confidence": 0.9996069073677063}, {"text": "F-measure", "start_pos": 19, "end_pos": 28, "type": "METRIC", "confidence": 0.9978461265563965}]}, {"text": "Link level recall, on the other hand, is significantly negatively correlated with TU precision, but not significantly correlated to any other TU metric, not even TU recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 11, "end_pos": 17, "type": "METRIC", "confidence": 0.7392082810401917}, {"text": "TU", "start_pos": 82, "end_pos": 84, "type": "METRIC", "confidence": 0.841847836971283}, {"text": "precision", "start_pos": 85, "end_pos": 94, "type": "METRIC", "confidence": 0.5784832239151001}, {"text": "recall", "start_pos": 165, "end_pos": 171, "type": "METRIC", "confidence": 0.816525936126709}]}, {"text": "Link level precision is thus highly important for matching translation units.", "labels": [], "entities": [{"text": "precision", "start_pos": 11, "end_pos": 20, "type": "METRIC", "confidence": 0.751296877861023}, {"text": "matching translation", "start_pos": 50, "end_pos": 70, "type": "TASK", "confidence": 0.6425833255052567}]}, {"text": "We can also note here that while there is a trade-off between precision and recall on link level, this is not the case for translation units, which can have both high precision and high recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 62, "end_pos": 71, "type": "METRIC", "confidence": 0.9991992115974426}, {"text": "recall", "start_pos": 76, "end_pos": 82, "type": "METRIC", "confidence": 0.9971254467964172}, {"text": "precision", "start_pos": 167, "end_pos": 176, "type": "METRIC", "confidence": 0.9945242404937744}, {"text": "recall", "start_pos": 186, "end_pos": 192, "type": "METRIC", "confidence": 0.9925618767738342}]}, {"text": "The same is not true for MWU, that allows partial matching, where we also see at least some precision/recall trade-off.", "labels": [], "entities": [{"text": "MWU", "start_pos": 25, "end_pos": 28, "type": "DATASET", "confidence": 0.8515334725379944}, {"text": "precision", "start_pos": 92, "end_pos": 101, "type": "METRIC", "confidence": 0.997815728187561}, {"text": "recall", "start_pos": 102, "end_pos": 108, "type": "METRIC", "confidence": 0.893637478351593}]}, {"text": "For reference, we first study the impact of alignment on SMT performance.", "labels": [], "entities": [{"text": "SMT", "start_pos": 57, "end_pos": 60, "type": "TASK", "confidence": 0.9966012239456177}]}, {"text": "Our SMT system is a standard PBSMT system trained on WMT13: Pearson correlations between gold standard word alignment evaluation on the link level and on translation unit level.", "labels": [], "entities": [{"text": "SMT", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.9778043627738953}, {"text": "WMT13", "start_pos": 53, "end_pos": 58, "type": "DATASET", "confidence": 0.9124481081962585}, {"text": "Pearson correlations", "start_pos": 60, "end_pos": 80, "type": "METRIC", "confidence": 0.887677401304245}, {"text": "word alignment evaluation", "start_pos": 103, "end_pos": 128, "type": "TASK", "confidence": 0.7659915486971537}]}, {"text": "Significant correlations are marked with bold (< 0.01). data.", "labels": [], "entities": []}, {"text": "We trained a German-English system on 2M sentences from Europarl and News Commentary.", "labels": [], "entities": [{"text": "Europarl", "start_pos": 56, "end_pos": 64, "type": "DATASET", "confidence": 0.9760910868644714}]}, {"text": "We used the target side of the parallel corpus and the SRILM toolkit) to train a 5-gram language model.", "labels": [], "entities": [{"text": "SRILM toolkit", "start_pos": 55, "end_pos": 68, "type": "DATASET", "confidence": 0.8578677773475647}]}, {"text": "For training the translation model and for decoding we used the Moses toolkit ().", "labels": [], "entities": [{"text": "translation", "start_pos": 17, "end_pos": 28, "type": "TASK", "confidence": 0.9762426614761353}]}, {"text": "We applied a standard feature set consisting of a language model feature, four translation model features, word penalty, phrase penalty, and distortion cost.", "labels": [], "entities": []}, {"text": "For tuning we used minimum error-rate training.", "labels": [], "entities": []}, {"text": "In order to minimize the risk of tuning influencing the results, we used a fixed set of weights for each experiment, tuned on a model 4+gdfa alignment.", "labels": [], "entities": []}, {"text": "For tuning we used newstest2009 with 2525 sentences, and for testing we used newstest2013 with 3000 sentences.", "labels": [], "entities": [{"text": "newstest2009", "start_pos": 19, "end_pos": 31, "type": "DATASET", "confidence": 0.9096128344535828}]}, {"text": "Evaluation was performed using the Bleu metric ().", "labels": [], "entities": [{"text": "Bleu metric", "start_pos": 35, "end_pos": 46, "type": "METRIC", "confidence": 0.8156390488147736}]}, {"text": "The same system setup was used for the SMT systems with reordering.", "labels": [], "entities": [{"text": "SMT", "start_pos": 39, "end_pos": 42, "type": "TASK", "confidence": 0.9871748089790344}]}, {"text": "shows the results on the SMT task.", "labels": [], "entities": [{"text": "SMT task", "start_pos": 25, "end_pos": 33, "type": "TASK", "confidence": 0.9245540499687195}]}, {"text": "Model 3 and 4 with gd/gdfa symmetrization yield the highest scores.", "labels": [], "entities": []}, {"text": "There is a larger difference between systems with different symmetrization than between systems with different alignment models.", "labels": [], "entities": []}, {"text": "The sparse intersection symmetrization gives the poorest results.", "labels": [], "entities": []}, {"text": "The top row in shows correlations between Bleu and all word alignment quality indicators.", "labels": [], "entities": [{"text": "Bleu", "start_pos": 42, "end_pos": 46, "type": "DATASET", "confidence": 0.494973748922348}, {"text": "word alignment quality", "start_pos": 55, "end_pos": 77, "type": "TASK", "confidence": 0.7516560852527618}]}, {"text": "There are significant correlations with link level recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 51, "end_pos": 57, "type": "METRIC", "confidence": 0.932758092880249}]}, {"text": "A weighted link level F-measure with \u03b1 = 0.3 gives a significant correlation of .72, which confirms the results of Fraser and Marcu.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 22, "end_pos": 31, "type": "METRIC", "confidence": 0.7227044105529785}]}, {"text": "There are no significant correlations with the TU metrics but a positive correlation with the number of TUs.", "labels": [], "entities": []}, {"text": "For the MWU metrics the correlations are similar to the link level,  suggesting that they measure similar things.", "labels": [], "entities": [{"text": "MWU metrics", "start_pos": 8, "end_pos": 19, "type": "DATASET", "confidence": 0.8689029812812805}]}, {"text": "Intuitively it seems important for SMT to match full translation units, but it might be the case that the phrase extraction strategy is robust as long as there are partial matches.", "labels": [], "entities": [{"text": "SMT", "start_pos": 35, "end_pos": 38, "type": "TASK", "confidence": 0.9953806400299072}, {"text": "phrase extraction", "start_pos": 106, "end_pos": 123, "type": "TASK", "confidence": 0.7890382707118988}]}, {"text": "There are no significant correlations with link degree or link crossings, except a negative correlation with Crossdiff, which means that it is good to have a similar number of crossings as the baseline.", "labels": [], "entities": [{"text": "link degree or link crossings", "start_pos": 43, "end_pos": 72, "type": "METRIC", "confidence": 0.7863353729248047}, {"text": "Crossdiff", "start_pos": 109, "end_pos": 118, "type": "DATASET", "confidence": 0.8093555569648743}]}, {"text": "These results confirm results from previous studies that link level measures, especially recall and weighted F-measure show some correlation with SMT quality whereas precision does not.", "labels": [], "entities": [{"text": "recall", "start_pos": 89, "end_pos": 95, "type": "METRIC", "confidence": 0.998818576335907}, {"text": "F-measure", "start_pos": 109, "end_pos": 118, "type": "METRIC", "confidence": 0.7916685938835144}, {"text": "SMT", "start_pos": 146, "end_pos": 149, "type": "TASK", "confidence": 0.9909432530403137}, {"text": "precision", "start_pos": 166, "end_pos": 175, "type": "METRIC", "confidence": 0.9989519119262695}]}], "tableCaptions": [{"text": " Table 3: Pearson correlations between gold stan- dard word alignment evaluation on the link level  and on translation unit level. Significant correla- tions are marked with bold (< 0.01).", "labels": [], "entities": [{"text": "Pearson", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9797403216362}, {"text": "gold stan- dard word alignment evaluation", "start_pos": 39, "end_pos": 80, "type": "TASK", "confidence": 0.6066274259771619}]}, {"text": " Table 4: Baseline Bleu scores for different sym- metrization heuristics", "labels": [], "entities": []}, {"text": " Table 5: Pearson correlations between different alignment characteristics and scores for the translation  and reordering tasks. Significant correlations are marked with bold (< 0.01).", "labels": [], "entities": [{"text": "Pearson correlations", "start_pos": 10, "end_pos": 30, "type": "METRIC", "confidence": 0.9423810839653015}, {"text": "translation  and reordering", "start_pos": 94, "end_pos": 121, "type": "TASK", "confidence": 0.8021052281061808}]}, {"text": " Table 6: Fuzzy reordering scores for part-of- speech-based reordering for different alignments", "labels": [], "entities": [{"text": "Fuzzy reordering", "start_pos": 10, "end_pos": 26, "type": "TASK", "confidence": 0.6609600782394409}]}, {"text": " Table 7: Bleu scores for part-of-speech-based re- ordering for different alignments", "labels": [], "entities": [{"text": "Bleu", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9925791621208191}, {"text": "part-of-speech-based re- ordering", "start_pos": 26, "end_pos": 59, "type": "TASK", "confidence": 0.5915680229663849}]}, {"text": " Table 8: Fuzzy reordering scores for alignment- based reordering for different alignments", "labels": [], "entities": [{"text": "Fuzzy reordering", "start_pos": 10, "end_pos": 26, "type": "TASK", "confidence": 0.7882257401943207}]}, {"text": " Table 9: Bleu scores for alignment-based reorder- ing for different alignments", "labels": [], "entities": [{"text": "Bleu", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9894233345985413}, {"text": "alignment-based reorder- ing", "start_pos": 26, "end_pos": 54, "type": "TASK", "confidence": 0.789374053478241}]}]}