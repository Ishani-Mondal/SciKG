{"title": [{"text": "Joint Ensemble Model for POS Tagging and Dependency Parsing", "labels": [], "entities": [{"text": "POS Tagging", "start_pos": 25, "end_pos": 36, "type": "TASK", "confidence": 0.848565936088562}, {"text": "Dependency Parsing", "start_pos": 41, "end_pos": 59, "type": "TASK", "confidence": 0.638962522149086}]}], "abstractContent": [{"text": "In this paper we present several approaches towards constructing joint ensemble models for mor-phosyntactic tagging and dependency parsing fora morphologically rich language-Bulgarian.", "labels": [], "entities": [{"text": "mor-phosyntactic tagging", "start_pos": 91, "end_pos": 115, "type": "TASK", "confidence": 0.6362094283103943}, {"text": "dependency parsing", "start_pos": 120, "end_pos": 138, "type": "TASK", "confidence": 0.7449964284896851}]}, {"text": "In our experiments we use state-of-the-art taggers and dependency parsers to obtain an extended version of the treebank for Bulgarian, BulTreeBank, which, in addition to the standard CoNLL fields, contains predicted morphosyntactic tags and dependency arcs for each word.", "labels": [], "entities": [{"text": "BulTreeBank", "start_pos": 135, "end_pos": 146, "type": "METRIC", "confidence": 0.8583164215087891}]}, {"text": "In order to select the most suitable tag and arc from the proposed ones, we use several ensemble techniques, the result of which is a valid dependency tree.", "labels": [], "entities": []}, {"text": "Most of these approaches show improvement over the results achieved individually by the tools for tagging and parsing.", "labels": [], "entities": [{"text": "tagging and parsing", "start_pos": 98, "end_pos": 117, "type": "TASK", "confidence": 0.610353022813797}]}], "introductionContent": [{"text": "Language processing pipelines are the standard means for preprocessing natural language text for various natural language processing (NLP) tasks.", "labels": [], "entities": []}, {"text": "A typical pipeline applies the following modules sequentially: a tokenizer, a part-of-speech (POS) tagger, a lemmatizer, and a parser.", "labels": [], "entities": []}, {"text": "The main drawback of such an architecture is that the erroneous output of one module in the pipeline propagates through to its final step.", "labels": [], "entities": []}, {"text": "This usually has a more significant impact on the processing of languages with segmentation issues, like Chinese, or languages with rich morphological systems, like the Slavic and Romance ones, which exhibit greater morphological and syntactic ambiguity due to the high number of word forms and freer word order.", "labels": [], "entities": []}, {"text": "In this paper we present several experiments in which we simultaneously solve two of the aforementioned tasks -tagging and parsing.", "labels": [], "entities": [{"text": "parsing", "start_pos": 123, "end_pos": 130, "type": "TASK", "confidence": 0.9134825468063354}]}, {"text": "The motivation behind this idea is that the two tasks are highly dependent on each other when working with a morphologically rich language, and thus a better solution could be found for each if they are solved jointly.", "labels": [], "entities": []}, {"text": "We assemble the outputs of three morphosyntactic taggers (POS taggers) and five dependency parsers in a single step.", "labels": [], "entities": []}, {"text": "The ensemble approach uses weights in order to select the best solution from a number of alternatives.", "labels": [], "entities": []}, {"text": "We follow ( and use two classes of approaches for selecting weights for the alternatives: voting, where the weights are assigned by simple calculations over the number of used models and their performance measures; machine learning weighting 1 , where machine learning is exploited in order to rank the alternatives on the basis of a joint feature model.", "labels": [], "entities": []}, {"text": "We refer to both types of approaches as ranking.", "labels": [], "entities": []}, {"text": "The language of choice in our experiments is Bulgarian, but the techniques presented here are easily applicable to other languages, given the availability of training data.", "labels": [], "entities": []}, {"text": "The interaction between the two levels -morphology and syntax -is carried out via a joint model of features for machine learning.", "labels": [], "entities": []}, {"text": "Its aim is to determine the best possible combination out of the predictions of the different taggers and dependency parsers.", "labels": [], "entities": []}, {"text": "Working only with the outputs of the taggers and parsers, instead of considering all possibilities for tag, head and syntactic relation for each word in the sentence, reduces the search space and allows us to experiment with more complex features.", "labels": [], "entities": []}, {"text": "One limitation of this approach is that the correct combination of an POS tag and a dependency arc might not have been predicted by any of the tools in the first place.", "labels": [], "entities": []}, {"text": "Therefore the ensemble approach can be beneficial only to a certain extent.", "labels": [], "entities": []}, {"text": "The data used throughout our experiments consists of the dependency conversion 2 of the HPSG-based Treebank of Bulgarian -the BulTreeBank.", "labels": [], "entities": [{"text": "HPSG-based Treebank of Bulgarian", "start_pos": 88, "end_pos": 120, "type": "DATASET", "confidence": 0.9608480334281921}, {"text": "BulTreeBank", "start_pos": 126, "end_pos": 137, "type": "DATASET", "confidence": 0.6220521926879883}]}, {"text": "This data set contains non-projective dependency trees, which are more suitable for describing the relatively free word order of Bulgarian sentences.", "labels": [], "entities": []}, {"text": "The structure of the paper is as follows: in Section 2 we introduce related work on joint models and ensemble models; in Section 3 we introduce related work on Bulgarian parsing and POS tagging; in Section 4 we present our ensemble model; in Section 5 we report on our current experimental setup, including the construction of a parsebank of parses and tagging results; Section 6 presents the results from our ensemble experiments; the last section concludes the paper.", "labels": [], "entities": [{"text": "Bulgarian parsing", "start_pos": 160, "end_pos": 177, "type": "TASK", "confidence": 0.4815782457590103}, {"text": "POS tagging", "start_pos": 182, "end_pos": 193, "type": "TASK", "confidence": 0.6843920648097992}]}], "datasetContent": [{"text": "In this section we present in detail the way in which our ensemble experiment was setup, including the data format and choice of ranking and features for machine learning.", "labels": [], "entities": []}, {"text": "We ran both algorithms (LocTr and GloTr) for construction of dependency trees using various combinations of the outputs of our dependency parsing and tagging models.", "labels": [], "entities": [{"text": "dependency parsing and tagging", "start_pos": 127, "end_pos": 157, "type": "TASK", "confidence": 0.7265671491622925}]}, {"text": "shows the parsing accuracy results when combining all models (1), only the models of the two best performing parsers,, and the best combination we have found by trying all possible combinations (around 32K) (3).", "labels": [], "entities": [{"text": "parsing", "start_pos": 10, "end_pos": 17, "type": "TASK", "confidence": 0.9705691933631897}, {"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9730375409126282}]}, {"text": "We included the results for (1) and to demonstrate that the best combination cannot be predicted in advance by simply selecting the candidate with the largest number of models, or the one with the best performing individual parsers.", "labels": [], "entities": []}, {"text": "The best combination in this experiment in terms of UAS score is: MLT09+BLL, Mate01+BLL, MST05+BLL, Turbo02+BLL, MLT07+MateTagger, Mate01+MateTagger, Turbo02+MateTagger.", "labels": [], "entities": [{"text": "UAS score", "start_pos": 52, "end_pos": 61, "type": "METRIC", "confidence": 0.8896196484565735}, {"text": "MLT09", "start_pos": 66, "end_pos": 71, "type": "DATASET", "confidence": 0.48510515689849854}, {"text": "BLL", "start_pos": 72, "end_pos": 75, "type": "METRIC", "confidence": 0.7296261191368103}]}, {"text": "This combination achieves better UAS score (92.47%) than any of the individual parsers (see).", "labels": [], "entities": [{"text": "UAS score", "start_pos": 33, "end_pos": 42, "type": "METRIC", "confidence": 0.889658659696579}]}, {"text": "There was an improvement of 1.37% over the best performing individual parser Turbo01+BLL, which achieves 91.10% UAS.", "labels": [], "entities": [{"text": "BLL", "start_pos": 85, "end_pos": 88, "type": "METRIC", "confidence": 0.8028895258903503}, {"text": "UAS", "start_pos": 112, "end_pos": 115, "type": "METRIC", "confidence": 0.9974404573440552}]}, {"text": "The unlabelled accuracy after voting is, however, still 0.43% lower than the best result on the gold data achieved by an individual model Mate01.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9890729784965515}]}, {"text": "We suspect that this is due to having only three tagger models in the current experiment, and that adding a few more tagger models for voting can help improve the result.", "labels": [], "entities": []}, {"text": "presents the accuracy achieved for all possible combinations of the three taggers by voting per rank.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.9995889067649841}]}, {"text": "We have to stress the fact that the selection of the morphosyntactic tag in the extended dependency tree is independent from the selection of dependency arcs, because each new tag node is connected to the word node by equal weight.", "labels": [], "entities": []}, {"text": "The interaction between dependency arcs and morphosyntactic arcs is ensured by the features used in machine learning weighting.", "labels": [], "entities": []}, {"text": "The results for the combination improve the individual accuracy for all taggers.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 55, "end_pos": 63, "type": "METRIC", "confidence": 0.9813029170036316}]}, {"text": "The results in show that it is hard to predict the best combinations in advance without enumerating all possibilities.", "labels": [], "entities": []}, {"text": "Note that for voting    the combinations involving only two taggers, because in this case the output of voting will always be the same as the output of the better tagger.", "labels": [], "entities": []}, {"text": "presents the UAS and LAS measures achieved using machine learning weighting.", "labels": [], "entities": [{"text": "UAS", "start_pos": 13, "end_pos": 16, "type": "DATASET", "confidence": 0.5872684717178345}, {"text": "LAS", "start_pos": 21, "end_pos": 24, "type": "METRIC", "confidence": 0.9640616774559021}]}, {"text": "In this case the best combination is Mate01+BLL, Turbo02+BLL, Mate01+MateTagger, Turbo02+MateTagger.", "labels": [], "entities": [{"text": "BLL", "start_pos": 44, "end_pos": 47, "type": "METRIC", "confidence": 0.7713267207145691}, {"text": "BLL", "start_pos": 57, "end_pos": 60, "type": "METRIC", "confidence": 0.7033191919326782}]}, {"text": "Again, the results are better than the ones obtained by the individual parsing models.", "labels": [], "entities": []}, {"text": "They also demonstrate some small improvement over the voting ranking.", "labels": [], "entities": []}, {"text": "These experiments show the following: (1) the combination of taggers and parsers is a feasible task; (2) the combination improves the accuracy of both the taggers and the parsers; (3) the combination of both tasks is better than the pipeline approach; (4) there is room for improvement in order to reach the upper bounds presented in Section 5.1.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 134, "end_pos": 142, "type": "METRIC", "confidence": 0.9985941052436829}]}], "tableCaptions": [{"text": " Table 1: Average UAS scores from the 10-fold cross validation of the parsing models trained on gold  data and on data containing automatically generated fields obtained using the outputs of three taggers.", "labels": [], "entities": [{"text": "UAS", "start_pos": 18, "end_pos": 21, "type": "TASK", "confidence": 0.47791919112205505}]}, {"text": " Table 3: UAS and LAS obtained after voting using the algorithms LocTr and GloTr for tree construction.  (1) All 18 models; (2) A combination of the best individual models: Mate01 and Turbo02 + each tag- ger; (3) best combination: MLT09+BLL, Mate01+BLL, MST05+BLL, Turbo02+BLL, MLT07+MateTagger,  Mate01+MateTagger, Turbo02+MateTagger;", "labels": [], "entities": [{"text": "BLL", "start_pos": 237, "end_pos": 240, "type": "METRIC", "confidence": 0.7449156045913696}]}, {"text": " Table 4: Tagger accuracy after voting and machine learning weighting.", "labels": [], "entities": [{"text": "Tagger", "start_pos": 10, "end_pos": 16, "type": "TASK", "confidence": 0.9835187196731567}, {"text": "accuracy", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.980164110660553}]}, {"text": " Table 5: Results from the experiments with RandomForest. The best combination is Mate01+BLL,  Turbo02+BLL, Mate01+MateTagger, Turbo02+MateTagger.", "labels": [], "entities": [{"text": "BLL", "start_pos": 89, "end_pos": 92, "type": "METRIC", "confidence": 0.8835654258728027}, {"text": "BLL", "start_pos": 103, "end_pos": 106, "type": "METRIC", "confidence": 0.7466394901275635}]}]}