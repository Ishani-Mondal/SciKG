{"title": [{"text": "Measuring Language Closeness by Modeling Regularity", "labels": [], "entities": [{"text": "Modeling Regularity", "start_pos": 32, "end_pos": 51, "type": "TASK", "confidence": 0.8061578571796417}]}], "abstractContent": [{"text": "This paper addresses the problems of measuring similarity between languages-where the term language covers any of the senses denoted by language, dialect or linguistic variety, as defined by any theory.", "labels": [], "entities": []}, {"text": "We argue that to devise an effective way to measure the similarity between languages one should build a probabilistic model that tries to capture as much regular correspondence between the languages as possible.", "labels": [], "entities": []}, {"text": "This approach yields two benefits.", "labels": [], "entities": []}, {"text": "First, given a set of language data, for any two models, this gives away of objectively determining which model is better, i.e., which model is more likely to be accurate and informative.", "labels": [], "entities": []}, {"text": "Second, given a model, for any two languages we can determine, in a principled way, how close they are.", "labels": [], "entities": []}, {"text": "The better models will be better at judging similarity.", "labels": [], "entities": [{"text": "similarity", "start_pos": 44, "end_pos": 54, "type": "METRIC", "confidence": 0.8764662146568298}]}, {"text": "We present experiments on data from three language families to support these ideas.", "labels": [], "entities": []}, {"text": "In particular, our results demonstrate the arbitrary nature of terms such as language vs. dialect, when applied to related languages.", "labels": [], "entities": []}], "introductionContent": [{"text": "In the context of building and applying NLP tools to similar languages, language varieties, or dialects, we are interested in principled ways of capturing the notion of language closeness.", "labels": [], "entities": []}, {"text": "Starting from scratch to develop resources and tools for languages that are close to each other is expensive; the hope is that the cost can be reduced by making use of pre-existing resources and tools for related languages, which are richer in resources.", "labels": [], "entities": []}, {"text": "In the context of this workshop, we assume that we deal with some method, \"Method X,\" that is applied to two (or more) related languages.", "labels": [], "entities": []}, {"text": "For example, Method X may involve adapting/porting a linguistic resource from one language to another; or maybe trying to translate between the languages; etc.", "labels": [], "entities": []}, {"text": "We also assume that the success of Method X directly depends in someway on how similar-or close-the languages are: that is, the similarity between the languages is expected to be a good predictor of how successful the application of the method will be.", "labels": [], "entities": []}, {"text": "Thus, in such a setting, it is worthwhile to devote some effort to devising good ways of measuring similarity between languages.", "labels": [], "entities": []}, {"text": "This is the main position of this paper.", "labels": [], "entities": []}, {"text": "We survey some of the approaches to measuring inter-language similarity in Section 2.", "labels": [], "entities": []}, {"text": "We assume that we are dealing with languages that are related genetically (i.e., etymologically).", "labels": [], "entities": []}, {"text": "Related languages maybe (dis)similar on many levels; in this paper, we focus on similarity on the lexical level.", "labels": [], "entities": []}, {"text": "This is admittedly a potential limitation, since, e.g., for Method X, similarity on the level of syntactic structure maybe more relevant than similarity on the lexical level.", "labels": [], "entities": []}, {"text": "However, as is done in other work, we use lexical similarity as a \"general\" indicator of relatedness between the languages.", "labels": [], "entities": []}, {"text": "Most of the surveyed methods begin with alignment at the level of individual phonetic segments (phones), which is seen as an essential phase in the process of evaluating similarity.", "labels": [], "entities": []}, {"text": "Alignment procedures are applied to the input data, which are sets of words which are judged to be similar (cognate)-drawn from the related languages.", "labels": [], "entities": []}, {"text": "Once an alignment is obtained using some method, the natural question arises: how effective is the particular output alignment?", "labels": [], "entities": []}, {"text": "Once the data is aligned (and, hopefully, aligned well), it becomes possible to devise measures for computing distances between the aligned words.", "labels": [], "entities": []}, {"text": "One of the simplest of such measures is the Levenshtein edit distance (LED), which is a crude count of edit operations needed to transform one word into one another.", "labels": [], "entities": [{"text": "Levenshtein edit distance (LED)", "start_pos": 44, "end_pos": 75, "type": "METRIC", "confidence": 0.8513396779696146}]}, {"text": "Averaging across LEDs between individual word pairs gives an estimate of the distance between the languages.", "labels": [], "entities": []}, {"text": "The question then arises: how accurate is the obtained distance?", "labels": [], "entities": [{"text": "accurate", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9643316268920898}]}, {"text": "LED charges an edit operation for substituting similar as well as dissimilar phones-regardless of how regular (and hence, probable) a given substitution is.", "labels": [], "entities": []}, {"text": "Conversely, LED charges nothing for substituting a phone x in language A for the same phone in language B, even if x in A regularly (e.g., always!) corresponds toy in B.", "labels": [], "entities": [{"text": "LED", "start_pos": 12, "end_pos": 15, "type": "METRIC", "confidence": 0.8775069713592529}]}, {"text": "More sophisticated variants of LED are then proposed, which try to take into account some aspects of the natural alignment setting (such as assigning different weights to different edit operations, e.g., by saying that it is cheaper to transform t into d than t into w).", "labels": [], "entities": []}, {"text": "Thus, in pursuit of effective similarity measures, we are faced with a sequence of steps: procedures for aligning data produce alignments; from the individual word-level alignments we derive distance measures; averaging distances across all words we obtain similarity measures between languages; we then require methods for comparing and validating the resulting language distance measures.", "labels": [], "entities": []}, {"text": "At various phases, these steps involve subjectivity-typically in the form of gold standards.", "labels": [], "entities": []}, {"text": "We discuss the kinds of subjectivity encountered with this approach in detail in Section 2.1.", "labels": [], "entities": []}, {"text": "As an alternative approach, we advocate viewing closeness between languages in terms of regularity in the data: if two languages are very close, it means that either the differences between them are very few, or-if they are many-then they are very regular.", "labels": [], "entities": []}, {"text": "As the number of differences grows and their nature becomes less regular, the languages grow more distant.", "labels": [], "entities": []}, {"text": "The goal then is to build probabilistic models that capture regularity in the data; to do this, we need to devise algorithms to discover as much regularity as possible.", "labels": [], "entities": []}, {"text": "This approach yields several advantages.", "labels": [], "entities": []}, {"text": "First, a model assigns a probability to observed data.", "labels": [], "entities": []}, {"text": "This has deep implications for this task, since it allows us to quantify uncertainty in a principled fashion, rather than commit to ad-hoc decisions and prior assumptions.", "labels": [], "entities": []}, {"text": "We will show that probabilistic modeling requires us to make fewer subjective judgements.", "labels": [], "entities": []}, {"text": "Second, the probabilities that the models assign to data allow us to build natural distance measures.", "labels": [], "entities": []}, {"text": "A pair of languages whose data have a higher probability under a given model are closer than a pair with a lower probability, in a well-defined sense.", "labels": [], "entities": []}, {"text": "This also allows us to define distance between individual word pairs.", "labels": [], "entities": []}, {"text": "The smarter the model-i.e., the more regularity it captures in the data-the more we will be able to trust in the distance measures based on the model.", "labels": [], "entities": []}, {"text": "Third-and equally important for this problem setting-this offers a principled way of comparing methods: if model X assigns higher probability to real data than model Y, then model X is better, and can be trusted more.", "labels": [], "entities": []}, {"text": "The key point here is that we can then compare models without any \"ground truth\" or gold-standard, pre-annotated data.", "labels": [], "entities": []}, {"text": "One way to see this is by using the model to predict unobserved data.", "labels": [], "entities": []}, {"text": "We can withhold one word pair (w A , w B ) from languages A and B before building the model (so the model does not seethe true correspondence); once the model is built, show it w A , and ask what is the corresponding word in B.", "labels": [], "entities": []}, {"text": "Theoretically, this is simple: the best guess for\u02c6wfor\u02c6 for\u02c6w B is simply the one that maximizes the probability of the pair p M (w A , \u02c6 w B ) under the model, overall possible strings\u02c6wstrings\u02c6 strings\u02c6w B in B.", "labels": [], "entities": []}, {"text": "Measuring the distance between w B and\u02c6wand\u02c6 and\u02c6w B tells how good M is at predicting unseen data.", "labels": [], "entities": [{"text": "predicting unseen data", "start_pos": 76, "end_pos": 98, "type": "TASK", "confidence": 0.903205931186676}]}, {"text": "Now, if model M 1 consistently predicts better than M 2 , it is very difficult to argue that M 1 is in any sense the worse model; and it is able to predict better only because it has succeeded in learning more about the data and the regularities in it.", "labels": [], "entities": []}, {"text": "Thus we can compare different models for measuring linguistic similarity.", "labels": [], "entities": []}, {"text": "And this can be done in a principled fashion-if the distances are based on probabilistic models.", "labels": [], "entities": []}, {"text": "The paper is organized as follows.", "labels": [], "entities": []}, {"text": "We continue with a discussion of related work.", "labels": [], "entities": []}, {"text": "In Section 3 we present one particular approach to modeling, based on information-theoretic principles.", "labels": [], "entities": []}, {"text": "In Section 4 we show some applications of these models to several linguistic data sets, from three different language families.", "labels": [], "entities": []}, {"text": "We conclude with plans for fu-ture work, in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "To illustrate the principles discussed above, we experiment with the two principal model types described above-the baseline 1-1 model and the context-sensitive model, using data from three different language families.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Comparison of Uralic dialect/language  pairs, sorted by NCD: context model.", "labels": [], "entities": []}]}