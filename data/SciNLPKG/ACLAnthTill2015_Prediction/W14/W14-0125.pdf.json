{"title": [{"text": "Parse Ranking with Semantic Dependencies and WordNet", "labels": [], "entities": [{"text": "WordNet", "start_pos": 45, "end_pos": 52, "type": "DATASET", "confidence": 0.8998762965202332}]}], "abstractContent": [{"text": "In this paper, we investigate which features are useful for ranking semantic representations of text.", "labels": [], "entities": []}, {"text": "We show that two methods of generalization improved results: extended grand-parenting and super-types.", "labels": [], "entities": []}, {"text": "The models are tested on a subset of SemCor that has been annotated with both Dependency Minimal Recursion Semantic representations and WordNet senses.", "labels": [], "entities": []}, {"text": "Using both types of features gives a significant improvement in whole sentence parse selection accuracy over the baseline model.", "labels": [], "entities": [{"text": "whole sentence parse selection", "start_pos": 64, "end_pos": 94, "type": "TASK", "confidence": 0.6951430141925812}, {"text": "accuracy", "start_pos": 95, "end_pos": 103, "type": "METRIC", "confidence": 0.9519201517105103}]}], "introductionContent": [{"text": "In this paper we investigate various features to improve the accuracy of semantic parse ranking.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.9990691542625427}, {"text": "semantic parse ranking", "start_pos": 73, "end_pos": 95, "type": "TASK", "confidence": 0.8462683161099752}]}, {"text": "There has been considerable successful work on syntactic parse ranking and reranking), but very little that uses pure semantic representations.", "labels": [], "entities": [{"text": "syntactic parse ranking", "start_pos": 47, "end_pos": 70, "type": "TASK", "confidence": 0.8292820254961649}]}, {"text": "With recent work on building semantic representations (from deep grammars such as LFG () and HPSG (), directly through lambda calculus, or as in intermediate step in machine translation) the question of ranking them has become more important.", "labels": [], "entities": [{"text": "HPSG", "start_pos": 93, "end_pos": 97, "type": "DATASET", "confidence": 0.8757483959197998}, {"text": "machine translation", "start_pos": 166, "end_pos": 185, "type": "TASK", "confidence": 0.7168044447898865}]}, {"text": "The closest related work is who ranked parses using semantic features from Minimal Recursion Semantics (MRS) and syntactic trees, using a Maximum Entropy Ranker.", "labels": [], "entities": [{"text": "Minimal Recursion Semantics (MRS)", "start_pos": 75, "end_pos": 108, "type": "TASK", "confidence": 0.6696829597155253}]}, {"text": "They experimented with Japanese data, using the Hinoki Treebank (, using primarily elementary dependencies: single arcs between pred-\u2663 Currently at PointInside, Inc.", "labels": [], "entities": [{"text": "Hinoki Treebank", "start_pos": 48, "end_pos": 63, "type": "DATASET", "confidence": 0.935159832239151}]}, {"text": "These can miss some important connections between predicates.", "labels": [], "entities": []}, {"text": "An example parse tree for I treat dogs and cats with worms is shown in.", "labels": [], "entities": []}, {"text": ", for the interpretation \"I treat both dogs and cats that have worms\" (not \"I treat, using worms, dogs and cats\" or any of the other possibilities) The semantic representation we use is Dependency Minimal Recursion Semantics (DRMS:).", "labels": [], "entities": [{"text": "Dependency Minimal Recursion Semantics", "start_pos": 186, "end_pos": 224, "type": "TASK", "confidence": 0.7148843482136726}]}, {"text": "The Minimal Recursion Semantics (MRS:) is a computationally tractable flat semantics that underspecifies quantifier scope.", "labels": [], "entities": [{"text": "Minimal Recursion Semantics (MRS", "start_pos": 4, "end_pos": 36, "type": "TASK", "confidence": 0.7738288223743439}]}, {"text": "The Dependency MRS is an MRS representation format that keeps all the information from the MRS but is simpler to manipulate.", "labels": [], "entities": [{"text": "Dependency MRS", "start_pos": 4, "end_pos": 18, "type": "TASK", "confidence": 0.5313316881656647}, {"text": "MRS representation format", "start_pos": 25, "end_pos": 50, "type": "TASK", "confidence": 0.841285506884257}]}, {"text": "DMRSs differ from syntactic dependency graphs in that the relations are defined between slightly abstract predicates, not between surface forms.", "labels": [], "entities": []}, {"text": "Some semantically empty surface tokens (such as infinitive to) are not included, while some predicates are inserted that are not in the original text (such as the null article).", "labels": [], "entities": []}, {"text": "A simplified MRS representation of our example sentence and its DMRS equivalent are shown in.", "labels": [], "entities": [{"text": "MRS", "start_pos": 13, "end_pos": 16, "type": "TASK", "confidence": 0.9354891777038574}]}, {"text": "In the DMRS, the basic links between the nodes are present.", "labels": [], "entities": []}, {"text": "However, potentially interesting relations such as that between the verb treat and its conjoined arguments dogs and cats are not linked directly.", "labels": [], "entities": []}, {"text": "Similarly, the relation between dogs and cats and worms is conveyed by the preposition with, which links them through its external argument (ARG1: and) and internal argument (ARG2: worms).", "labels": [], "entities": [{"text": "ARG1", "start_pos": 141, "end_pos": 145, "type": "DATASET", "confidence": 0.8763173818588257}, {"text": "ARG2", "start_pos": 175, "end_pos": 179, "type": "DATASET", "confidence": 0.9176706075668335}]}, {"text": "There is no direct link.", "labels": [], "entities": []}, {"text": "We investigate new features that make these links more direct (Section 3.2).", "labels": [], "entities": []}, {"text": "We also explore the significance of the effectiveness of links between words that are connected arbitrarily faraway in the semantic graph (Section 3.2.3).", "labels": [], "entities": []}, {"text": "Finally, we experimented with generalizing over semantic classes.", "labels": [], "entities": []}, {"text": "We used WordNet semantic files as supertypes to reduce data sparseness (Section 3.2.4).", "labels": [], "entities": []}, {"text": "This will generalize the lexical semantics of the predicates, resulting in a reduction of feature size and ambiguity.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 6: Parse selection results with SD.", "labels": [], "entities": [{"text": "Parse selection", "start_pos": 10, "end_pos": 25, "type": "TASK", "confidence": 0.8478221595287323}]}, {"text": " Table 7: Parse selection results with SF.", "labels": [], "entities": [{"text": "Parse selection", "start_pos": 10, "end_pos": 25, "type": "TASK", "confidence": 0.8331689238548279}, {"text": "SF", "start_pos": 39, "end_pos": 41, "type": "METRIC", "confidence": 0.8946231007575989}]}]}