{"title": [{"text": "A Comparison of MT Methods for Closely Related Languages: a Case Study on Czech -Slovak Language Pair *", "labels": [], "entities": [{"text": "MT Methods for Closely Related Languages", "start_pos": 16, "end_pos": 56, "type": "TASK", "confidence": 0.7104376355806986}]}], "abstractContent": [{"text": "This paper describes an experiment comparing results of machine translation between two closely related languages, Czech and Slovak.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 56, "end_pos": 75, "type": "TASK", "confidence": 0.7018463164567947}]}, {"text": "The comparison is performed by means of two MT systems, one representing rule-based approach , the other one representing statistical approach to the task.", "labels": [], "entities": [{"text": "MT", "start_pos": 44, "end_pos": 46, "type": "TASK", "confidence": 0.9648120999336243}]}, {"text": "Both sets of results are manually evaluated by native speakers of the target language.", "labels": [], "entities": []}, {"text": "The results are discussed both from the linguistic and quantitative points of view.", "labels": [], "entities": []}], "introductionContent": [{"text": "Machine translation (MT) of related languages is a specific field in the domain of MT which attracted the attention of several research teams in the past by promising relatively good results through the application of classic rule-based methods.", "labels": [], "entities": [{"text": "Machine translation (MT) of related languages", "start_pos": 0, "end_pos": 45, "type": "TASK", "confidence": 0.9101520404219627}, {"text": "MT", "start_pos": 83, "end_pos": 85, "type": "TASK", "confidence": 0.9856547713279724}]}, {"text": "The exploitation of lexical, morphological and syntactic similarity of related languages seemed to balance the advantages of datadriven approaches, especially for the language pairs with smaller volumes of available parallel data.", "labels": [], "entities": []}, {"text": "This simple and straightforward assumption have led to the construction of numerous rule-based translation systems for related (or similar) natural languages.", "labels": [], "entities": []}, {"text": "The following list (ordered alphabetically) includes several examples of those systems: \u2022 (Altintas and) for Turkic languages.", "labels": [], "entities": []}, {"text": "\u2022 Apertium () for Romance languages.", "labels": [], "entities": [{"text": "Apertium", "start_pos": 2, "end_pos": 10, "type": "METRIC", "confidence": 0.9018521904945374}]}, {"text": "\u2022 \u02c7 Ces\u00edlko (), for Slavic languages with rich inflectional morphology, mostly language pairs with Czech language as a source.", "labels": [], "entities": []}, {"text": "\u2022 Ruslan ( full-fledged transfer based RBMT system from Czech to Russian.", "labels": [], "entities": []}, {"text": "* This work has been using language resources developed and/or stored and/or distributed by the LINDAT-Clarin project of the Ministry of Education of the Czech Republic (project LM2010013).", "labels": [], "entities": []}, {"text": "\u2022 () for Gaelic languages; Irish (Gaeilge) and Scottish Gaelic (G'aidhlig).", "labels": [], "entities": []}, {"text": "\u2022 () for the North Sami to Lule Sami language pair.", "labels": [], "entities": [{"text": "North Sami to Lule Sami language pair", "start_pos": 13, "end_pos": 50, "type": "DATASET", "confidence": 0.4814573696681431}]}, {"text": "\u2022 Guat for Slavic languages with rich inflectional morphology, mostly language pairs with Slovenian language.", "labels": [], "entities": [{"text": "Guat", "start_pos": 2, "end_pos": 6, "type": "METRIC", "confidence": 0.9497096538543701}]}, {"text": "Many of the systems listed above had been created in the period when it was hard to obtain a good quality data-driven system which would enable comparison against these systems.", "labels": [], "entities": []}, {"text": "The existence of Google Translate 1 which nowadays enables the automatic translation even between relatively small languages made it possible to investigate advantages and disadvantages of both approaches.", "labels": [], "entities": []}, {"text": "This paper introduces the first step in this direction -the comparison of results of two different systems for two really very closely related languagesCzech and Slovak.", "labels": [], "entities": []}], "datasetContent": [{"text": "The aim of the experiment was double: to show the quality of the simple RBMT methods (shallow-parse and shallow transfer RBMT) in comparison to the stateof-the-art SMT system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 164, "end_pos": 167, "type": "TASK", "confidence": 0.9879058599472046}]}, {"text": "The second part of the experiment was to outline the most obvious and most challenging errors produced by each translation paradigm.", "labels": [], "entities": []}, {"text": "This part of the experiment relied on the methodology similar to that used in the 2013 Workshop on Statistical Machine Translation ().", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 99, "end_pos": 130, "type": "TASK", "confidence": 0.7344127694765726}]}, {"text": "We conducted manual evaluation of both systems' outputs consisting of ranking individual translated sentences according to the translation quality (the evaluators had access to the original sentence).", "labels": [], "entities": []}, {"text": "Unlike the ranking of the SMT Workshop which worked always with 5 translations, our task was much simpler and the ranking naturally consisted of ranking translated sentences of both systems.", "labels": [], "entities": [{"text": "SMT Workshop", "start_pos": 26, "end_pos": 38, "type": "TASK", "confidence": 0.8677401840686798}]}, {"text": "The evaluator indicated which of the two systems is better, having also the chance to indicate that both translations are identical, because the systems produced relatively large number of identical results -see section 5).", "labels": [], "entities": []}, {"text": "The reason why we didn't automatic measures of translation quality was quite natural.", "labels": [], "entities": [{"text": "translation quality", "start_pos": 47, "end_pos": 66, "type": "TASK", "confidence": 0.7893106639385223}]}, {"text": "After a period of wide acceptance of automatic measures like BLEU () or NIST, recent MT evaluation experiments seem to prefer manual methods.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 61, "end_pos": 65, "type": "METRIC", "confidence": 0.9932110905647278}, {"text": "NIST", "start_pos": 72, "end_pos": 76, "type": "DATASET", "confidence": 0.9311161637306213}, {"text": "MT evaluation", "start_pos": 85, "end_pos": 98, "type": "TASK", "confidence": 0.9333513379096985}]}, {"text": "Many papers such as and authors of workshops such as WMT 2013 contend that automatic measures of machine translation quality are an imperfect substitute for human assessments, especially when it is necessary to compare different systems (or, even worse, the systems based on different paradigms).", "labels": [], "entities": [{"text": "WMT 2013", "start_pos": 53, "end_pos": 61, "type": "DATASET", "confidence": 0.8040266633033752}, {"text": "machine translation", "start_pos": 97, "end_pos": 116, "type": "TASK", "confidence": 0.7476009428501129}]}], "tableCaptions": [{"text": " Table 1: Evaluation of results", "labels": [], "entities": []}]}