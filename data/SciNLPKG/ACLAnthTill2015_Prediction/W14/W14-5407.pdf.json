{"title": [{"text": "Key Event Detection in Video using ASR and Visual Data", "labels": [], "entities": [{"text": "Key Event Detection", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.6017752389113108}]}], "abstractContent": [{"text": "Multimedia data grow day by day which makes it necessary to index them automatically and efficiently for fast retrieval, and more precisely to automatically index them with key events.", "labels": [], "entities": []}, {"text": "In this paper, we present preliminary work on key event detection in British royal wedding videos using automatic speech recognition (ASR) and visual data.", "labels": [], "entities": [{"text": "key event detection", "start_pos": 46, "end_pos": 65, "type": "TASK", "confidence": 0.664285937945048}, {"text": "British royal wedding videos", "start_pos": 69, "end_pos": 97, "type": "DATASET", "confidence": 0.7805128395557404}, {"text": "automatic speech recognition (ASR)", "start_pos": 104, "end_pos": 138, "type": "TASK", "confidence": 0.7475382586320242}]}, {"text": "The system first automatically acquires key events of royal weddings from an external corpus such as Wikipedia, and then identifies those events in the ASR data.", "labels": [], "entities": [{"text": "ASR data", "start_pos": 152, "end_pos": 160, "type": "DATASET", "confidence": 0.8708089590072632}]}, {"text": "The system also models name and face alignment to identify the persons involved in the wedding events.", "labels": [], "entities": [{"text": "name and face alignment", "start_pos": 23, "end_pos": 46, "type": "TASK", "confidence": 0.5918325483798981}]}, {"text": "We compare the results obtained with the ASR output with results obtained with subtitles.", "labels": [], "entities": [{"text": "ASR", "start_pos": 41, "end_pos": 44, "type": "TASK", "confidence": 0.6894022226333618}]}, {"text": "The error is only slightly higher when using ASR output in the detection of key events and their participants in the wedding videos compared to the results obtained with subtitles.", "labels": [], "entities": [{"text": "error", "start_pos": 4, "end_pos": 9, "type": "METRIC", "confidence": 0.967680037021637}, {"text": "ASR", "start_pos": 45, "end_pos": 48, "type": "TASK", "confidence": 0.4945380687713623}]}], "introductionContent": [{"text": "With the increase of multimedia data widely available on the Web and in social media, it becomes necessary to automatically index the multimedia resources with key events for information search and mining.", "labels": [], "entities": [{"text": "information search and mining", "start_pos": 175, "end_pos": 204, "type": "TASK", "confidence": 0.7488204091787338}]}, {"text": "For instance, it is not possible to manually index all the frames of a video.", "labels": [], "entities": []}, {"text": "Automatically indexing multimedia data with key events makes the retrieval and mining effective and efficient.", "labels": [], "entities": []}, {"text": "Event detection is an important and current research problem in the field of multimedia information retrieval.", "labels": [], "entities": [{"text": "Event detection", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.8553681373596191}, {"text": "multimedia information retrieval", "start_pos": 77, "end_pos": 109, "type": "TASK", "confidence": 0.6092816988627116}]}, {"text": "Most of the event detection in video is done by analyzing the visual features using manually transcribed data.", "labels": [], "entities": [{"text": "event detection", "start_pos": 12, "end_pos": 27, "type": "TASK", "confidence": 0.7219610661268234}]}, {"text": "In this paper, we propose key event detection in British royal wedding videos using automatic speech recognition (ASR) data and where possible also to recognize the actors involved in the recognized events using visual and textual data.", "labels": [], "entities": [{"text": "key event detection", "start_pos": 26, "end_pos": 45, "type": "TASK", "confidence": 0.6435768604278564}, {"text": "automatic speech recognition (ASR)", "start_pos": 84, "end_pos": 118, "type": "TASK", "confidence": 0.7502237260341644}]}, {"text": "An event is something that happens at a certain moment in time and at a certain location possibly involving different actors.", "labels": [], "entities": []}, {"text": "Events can be quite specific as in this case the key events are the typical events that makeup a royal wedding scenario.", "labels": [], "entities": []}, {"text": "For example, events like 'design of cake/dress/bouquet', 'couple heading to Buckingham palace', 'appearing on balcony' etc. are key events in British royal wedding video.", "labels": [], "entities": [{"text": "British royal wedding video", "start_pos": 142, "end_pos": 169, "type": "DATASET", "confidence": 0.840640589594841}]}, {"text": "shows an example of a frame containing an event with its actors, together with the associated subtitle and ASR output.", "labels": [], "entities": []}, {"text": "While most works in this domain have focussed on clean textual content such as manual transcripts or subtitles, which are difficult to acquire, we use the output of an ASR system.", "labels": [], "entities": []}, {"text": "While the event detection and name-face alignment problem by itself is already quite difficult, the nature of the ASR text adds an additional complexity.", "labels": [], "entities": [{"text": "event detection", "start_pos": 10, "end_pos": 25, "type": "TASK", "confidence": 0.8376778364181519}, {"text": "name-face alignment", "start_pos": 30, "end_pos": 49, "type": "TASK", "confidence": 0.7911665439605713}, {"text": "ASR", "start_pos": 114, "end_pos": 117, "type": "TASK", "confidence": 0.9775781631469727}]}, {"text": "ASR data is noisy and inaccurate, it does not contain some parts of the actual spoken text, and does not contain sentence boundaries.", "labels": [], "entities": [{"text": "ASR", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.9625867009162903}]}, {"text": "For the key events, the system first acquires the necessary knowledge from external corpora -in our case Wikipedia articles associated with royal weddings.", "labels": [], "entities": []}, {"text": "Then the system identifies the key events in the ASR data.", "labels": [], "entities": [{"text": "ASR data", "start_pos": 49, "end_pos": 57, "type": "DATASET", "confidence": 0.8884267807006836}]}, {"text": "The system also models name and face alignment to identify the persons involved in the wedding events.", "labels": [], "entities": [{"text": "name and face alignment", "start_pos": 23, "end_pos": 46, "type": "TASK", "confidence": 0.5918325483798981}]}, {"text": "We perform named entity recognition in the text associated with a window of frames to first generate a noisy label for the faces occurring in the frames and this rough alignment is refined using an Expectation-Maximization (EM) algorithm.", "labels": [], "entities": [{"text": "named entity recognition in the text associated with a window of frames", "start_pos": 11, "end_pos": 82, "type": "TASK", "confidence": 0.7848777323961258}]}, {"text": "We compare the results obtained with the ASR output with results obtained with subtitles.", "labels": [], "entities": [{"text": "ASR", "start_pos": 41, "end_pos": 44, "type": "TASK", "confidence": 0.6894022226333618}]}, {"text": "The error is only slightly Sub-title: \"Outside, fully 150,000 people with unbounded enthusiasm acclaimed Princess Margaret and her husband when they appeared on the balcony..", "labels": [], "entities": [{"text": "error", "start_pos": 4, "end_pos": 9, "type": "METRIC", "confidence": 0.9805371165275574}]}, {"text": "\" ASR: \"outside only a hundred and 50 people on TV and using it as a . .", "labels": [], "entities": [{"text": "ASR", "start_pos": 2, "end_pos": 5, "type": "METRIC", "confidence": 0.5914558172225952}]}, {"text": "\": An example of a frame containing an event with associated subtitle and ASR output higher when using ASR output in the detection of key events and their participants in the wedding videos compared to the results obtained with subtitles.", "labels": [], "entities": [{"text": "ASR output", "start_pos": 74, "end_pos": 84, "type": "METRIC", "confidence": 0.944693386554718}]}, {"text": "The methodology that we propose can be applied for the detection of many different types of video events.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we describe the dataset, experimental setup and the metrics used for evaluation.", "labels": [], "entities": []}, {"text": "The data used in our experiments is the DVD on Britain's Royal Weddings published by the BBC.", "labels": [], "entities": [{"text": "Britain's Royal Weddings published by the BBC", "start_pos": 47, "end_pos": 92, "type": "DATASET", "confidence": 0.9090543687343597}]}, {"text": "The duration of this video is 116 minutes at a frame rate of 25 frames per second, and the frame resolution is 720x576 pixels.", "labels": [], "entities": []}, {"text": "Frames are extracted at the rate of one frame per second using the ffmpeg tool (ffmpeg, 2012).", "labels": [], "entities": [{"text": "ffmpeg, 2012)", "start_pos": 80, "end_pos": 93, "type": "DATASET", "confidence": 0.9100534915924072}]}, {"text": "Faces in the frames are annotated manually using the Picasa tool for building the ground truth for evaluation.", "labels": [], "entities": []}, {"text": "This tool is very handy and user-friendly to tag the faces.", "labels": [], "entities": []}, {"text": "We have found that there are 69 persons including British wedding couples in the video.", "labels": [], "entities": []}, {"text": "The subtitles came along with the DVD which are already split into segments of around 3 seconds.", "labels": [], "entities": []}, {"text": "We use the (FBK, 2013) system to obtain the ASR data of the videos.", "labels": [], "entities": [{"text": "FBK, 2013)", "start_pos": 12, "end_pos": 22, "type": "DATASET", "confidence": 0.9355673044919968}, {"text": "ASR", "start_pos": 44, "end_pos": 47, "type": "METRIC", "confidence": 0.49764543771743774}]}, {"text": "Since the (FBK, 2013) system takes only sound (.mp3 file) as input, we have converted the video into a mp3 file using (ffmpeg, 2012).", "labels": [], "entities": [{"text": "FBK, 2013", "start_pos": 11, "end_pos": 20, "type": "DATASET", "confidence": 0.8982627987861633}]}, {"text": "The obtained ASR data is then in XML format without any sentence boundaries so we have converted the ASR data into segments in the range of three seconds, which is standard when presenting subtitles in video.", "labels": [], "entities": []}, {"text": "It is clear that the ASR transcription contains many words that are incorrectly transcribed.", "labels": [], "entities": [{"text": "ASR", "start_pos": 21, "end_pos": 24, "type": "TASK", "confidence": 0.9455490708351135}]}, {"text": "It is also visible that the ASR system does not recognize or misspells many words from the actual speech.", "labels": [], "entities": [{"text": "ASR", "start_pos": 28, "end_pos": 31, "type": "TASK", "confidence": 0.9847827553749084}]}, {"text": "As mentioned above, we have built a gazetteer of the couples' names.", "labels": [], "entities": []}, {"text": "A set of events are recognized by our system as being important in the context of weddings.", "labels": [], "entities": []}, {"text": "To evaluate the quality of these events, the events in the video were annotated by two annotators independently.", "labels": [], "entities": []}, {"text": "This annotation includes the actual event, and the start and end times of the event.", "labels": [], "entities": []}, {"text": "These two sets with annotations form the groundtruth.", "labels": [], "entities": []}, {"text": "To be able to compare the system generated events with the ground truth events, we adopt a two-step approach.", "labels": [], "entities": []}, {"text": "First, we combine the corresponding ground truth entries from different annotators into one sequence of frames.", "labels": [], "entities": []}, {"text": "Let FL be the final list of name and face alignment retrieved by our system for all the faces detected in all frames, and GL the complete ground truth list.", "labels": [], "entities": [{"text": "FL", "start_pos": 4, "end_pos": 6, "type": "METRIC", "confidence": 0.9844701290130615}, {"text": "GL", "start_pos": 122, "end_pos": 124, "type": "METRIC", "confidence": 0.9735935926437378}]}, {"text": "To evaluate the name and face alignment task, we use standard precision (P ), recall (R) and F 1 scores for evaluation: To evaluate correctness of event segment boundaries, precision and recall are too strict since they penalize boundaries placed very close to the ground truth boundaries.", "labels": [], "entities": [{"text": "name and face alignment task", "start_pos": 16, "end_pos": 44, "type": "TASK", "confidence": 0.6756220638751984}, {"text": "precision (P )", "start_pos": 62, "end_pos": 76, "type": "METRIC", "confidence": 0.9434773623943329}, {"text": "recall (R)", "start_pos": 78, "end_pos": 88, "type": "METRIC", "confidence": 0.9518950134515762}, {"text": "F 1", "start_pos": 93, "end_pos": 96, "type": "METRIC", "confidence": 0.9715066254138947}, {"text": "precision", "start_pos": 173, "end_pos": 182, "type": "METRIC", "confidence": 0.9979735016822815}, {"text": "recall", "start_pos": 187, "end_pos": 193, "type": "METRIC", "confidence": 0.9957885146141052}]}, {"text": "We use the) metric that measures the difference between the ground truth segment GT and the segment SE found by the machine originally designed for text segmentation.", "labels": [], "entities": [{"text": "SE", "start_pos": 100, "end_pos": 102, "type": "METRIC", "confidence": 0.9659152626991272}, {"text": "text segmentation", "start_pos": 148, "end_pos": 165, "type": "TASK", "confidence": 0.7668526768684387}]}, {"text": "For our scenario, this metric is defined as follows: where M = 7102, is the number of frames extracted, k = 1, is the window size and b(i, j) represents the number of boundaries between frame indices i and j.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Names and faces alignment results on subtitle vs. ASR data on events", "labels": [], "entities": [{"text": "Names and faces alignment", "start_pos": 10, "end_pos": 35, "type": "TASK", "confidence": 0.6819750219583511}]}, {"text": " Table 2: WinDiff score on event identification on subtitle vs. ASR data on the union setting", "labels": [], "entities": [{"text": "WinDiff score", "start_pos": 10, "end_pos": 23, "type": "METRIC", "confidence": 0.7694632411003113}, {"text": "event identification", "start_pos": 27, "end_pos": 47, "type": "TASK", "confidence": 0.6608627438545227}]}]}