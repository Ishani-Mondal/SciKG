{"title": [{"text": "Chinese Spell Checking Based on Noisy Channel Model", "labels": [], "entities": [{"text": "Chinese Spell Checking", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.5975654721260071}]}], "abstractContent": [{"text": "Chinese spell checking is an important component of many NLP applications, including word processors, search engines, and automatic essay rating.", "labels": [], "entities": [{"text": "Chinese spell checking", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.5681346952915192}, {"text": "automatic essay rating", "start_pos": 122, "end_pos": 144, "type": "TASK", "confidence": 0.5978055993715922}]}, {"text": "Compared to English, Chinese has no word boundaries and there are various Chinese input methods that cause different kinds of typos, so it is more difficult to develop spell checkers for Chinese.", "labels": [], "entities": []}, {"text": "In this paper, we introduce a novel method for correcting Chinese typographical errors based on sound or shape similarity.", "labels": [], "entities": [{"text": "correcting Chinese typographical errors", "start_pos": 47, "end_pos": 86, "type": "TASK", "confidence": 0.8360182046890259}]}, {"text": "In our approach, similar characters are automatically generated using Web corpora, and potential ty-pos in a given sentence are then corrected using a channel model and a character-based language model in the noisy channel model.", "labels": [], "entities": []}, {"text": "In the training phase, we estimate the channel probabilities for each character based on ngrams in Web corpus.", "labels": [], "entities": [{"text": "Web corpus", "start_pos": 99, "end_pos": 109, "type": "DATASET", "confidence": 0.7927797734737396}]}, {"text": "At run-time, the system generates correction candidates for each character in the given sentence and selects the appropriate correction using the channel model and the language model.", "labels": [], "entities": []}], "introductionContent": [{"text": "Spell checking is a necessary task for text processing of every written language, which involves automatically detecting and correcting typographical errors.", "labels": [], "entities": [{"text": "Spell checking", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.9558889865875244}, {"text": "text processing", "start_pos": 39, "end_pos": 54, "type": "TASK", "confidence": 0.7292227745056152}]}, {"text": "However, compared to spell checkers for alphabetical languages (e.g., English or French), Chinese spell checkers are more difficult to develop because there are no word boundaries in Chinese writing system and errors maybe caused by various Chinese input methods.", "labels": [], "entities": [{"text": "Chinese spell checkers", "start_pos": 90, "end_pos": 112, "type": "TASK", "confidence": 0.5980880955855051}]}, {"text": "In this thesis, we define typos as Chinese characters that are misused due to sound or shape similarity.", "labels": [], "entities": []}, {"text": "show that people tend to unintentionally generate typos due to sound similarity (e.g., * (suo ding) instead of (suo ding)) or shape similarity (e.g., * (xiao ding) instead of (suo ding)).", "labels": [], "entities": []}, {"text": "On the other hand, some typos found on the Web (e.g., forums or blogs) are used deliberately for the purpose of speed typing or just for fun.", "labels": [], "entities": []}, {"text": "Therefore, spell checking is an important component for many applications, including computer-aided writing, search engines, and social media text normalization.", "labels": [], "entities": [{"text": "spell checking", "start_pos": 11, "end_pos": 25, "type": "TASK", "confidence": 0.8848218321800232}, {"text": "social media text normalization", "start_pos": 129, "end_pos": 160, "type": "TASK", "confidence": 0.7430762201547623}]}, {"text": "Very little work has been done on the task of Chinese spell checking.", "labels": [], "entities": [{"text": "Chinese spell checking", "start_pos": 46, "end_pos": 68, "type": "TASK", "confidence": 0.7584245403607687}]}, {"text": "The methods proposed in the literature can be classified into two types: rule-based methods and statistical methods.", "labels": [], "entities": []}, {"text": "Rulebased methods use knowledge resources, for example, dictionaries, confusion sets, and segmentation systems.", "labels": [], "entities": []}, {"text": "Simple rule-based methods, however, have their limitations.", "labels": [], "entities": []}, {"text": "The following sentence is a snippet collected from students' written essays which is correct . (wei she me ni yao ru ci di yong gong ne ru guo wo bu yong gong na yi hou wo jiang gan bu shang zi ji suo ding de mu biao ) Unfortunately, based on simple rules the two characters (suo) and (ding) are likely to be regarded as typos of the dictionary word (suo ding) with identical pronunciation.", "labels": [], "entities": []}, {"text": "The data-driven, statistical spell checking approach appears to be more robust and perform better.", "labels": [], "entities": [{"text": "statistical spell checking", "start_pos": 17, "end_pos": 43, "type": "TASK", "confidence": 0.6672302186489105}]}, {"text": "Statistical methods typically use a large corpus to create a language model to validate the correction hypotheses.", "labels": [], "entities": []}, {"text": "Intuitively, by using (zi ji suo ding de mu biao), the three characters (suo ding de) area trigram with high probability in a monolingual corpus, we may determine the (suo ding) is not a typo after all.", "labels": [], "entities": []}, {"text": "shows the frequency and probability of (suo ding de) and (suo ding de).", "labels": [], "entities": [{"text": "frequency", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9882239103317261}, {"text": "probability", "start_pos": 24, "end_pos": 35, "type": "METRIC", "confidence": 0.9225634932518005}]}], "datasetContent": [{"text": "Our systems were designed to provide wide coverage spell checking for Chinese texts.", "labels": [], "entities": [{"text": "spell checking", "start_pos": 51, "end_pos": 65, "type": "TASK", "confidence": 0.7654248774051666}]}, {"text": "As such, we trained our systems using the confusion set, a compiled corpus, Web-scale ngrams, and an existing Chinese spellchecker.", "labels": [], "entities": []}, {"text": "These resources are used for different purposes: the confusion sets provide the correction candidates; the compiled corpus provide the training data for the language model; Web-scale ngrams and the existing Chinese spellchecker are used for training the channel model.", "labels": [], "entities": []}, {"text": "We evaluate our systems on the sentence level.", "labels": [], "entities": []}, {"text": "In this section, we present the details of data sources used in training(Section 3.1 to Section 3.4).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: Two sample collocates of and .", "labels": [], "entities": []}, {"text": " Table 5: Sample texts of typo and of from  the corpus.", "labels": [], "entities": []}, {"text": " Table 8: Sample of the typo pairs with frequency.", "labels": [], "entities": [{"text": "Sample", "start_pos": 10, "end_pos": 16, "type": "TASK", "confidence": 0.5474618077278137}]}, {"text": " Table 9: The result of the local error probability  with smoothing.", "labels": [], "entities": []}, {"text": " Table 10: A sample of the channel model for  (suo).", "labels": [], "entities": []}, {"text": " Table 15: The information of the word, character,  article, and percentage in the area of sinica corpus.", "labels": [], "entities": []}]}