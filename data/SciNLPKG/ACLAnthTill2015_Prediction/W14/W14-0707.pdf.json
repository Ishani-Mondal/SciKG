{"title": [{"text": "Recognizing Causality in Verb-Noun Pairs via Noun and Verb Semantics", "labels": [], "entities": []}], "abstractContent": [{"text": "Several supervised approaches have been proposed for causality identification by relying on shallow linguistic features.", "labels": [], "entities": [{"text": "causality identification", "start_pos": 53, "end_pos": 77, "type": "TASK", "confidence": 0.879031240940094}]}, {"text": "However , such features do not lead to improved performance.", "labels": [], "entities": []}, {"text": "Therefore, novel sources of knowledge are required to achieve progress on this problem.", "labels": [], "entities": []}, {"text": "In this paper, we propose a model for the recognition of causality in verb-noun pairs by employing additional types of knowledge along with linguistic features.", "labels": [], "entities": [{"text": "recognition of causality in verb-noun pairs", "start_pos": 42, "end_pos": 85, "type": "TASK", "confidence": 0.8176590104897817}]}, {"text": "In particular, we focus on identifying and employing semantic classes of nouns and verbs with high tendency to encode cause or non-cause relations.", "labels": [], "entities": []}, {"text": "Our model incorporates the information about these classes to minimize errors in predictions made by a basic supervised classifier relying merely on shallow linguistic features.", "labels": [], "entities": []}, {"text": "As compared with this basic classifier our model achieves 14.74% (29.57%) improvement in F-score (accuracy), respectively.", "labels": [], "entities": [{"text": "F-score", "start_pos": 89, "end_pos": 96, "type": "METRIC", "confidence": 0.9993525147438049}, {"text": "accuracy", "start_pos": 98, "end_pos": 106, "type": "METRIC", "confidence": 0.7793232798576355}]}], "introductionContent": [{"text": "The automatic detection of causal relations is important for various natural language processing applications such as question answering, text summarization, text understanding and event prediction.", "labels": [], "entities": [{"text": "automatic detection of causal relations", "start_pos": 4, "end_pos": 43, "type": "TASK", "confidence": 0.7719631910324096}, {"text": "question answering", "start_pos": 118, "end_pos": 136, "type": "TASK", "confidence": 0.890090823173523}, {"text": "text summarization", "start_pos": 138, "end_pos": 156, "type": "TASK", "confidence": 0.752390056848526}, {"text": "text understanding", "start_pos": 158, "end_pos": 176, "type": "TASK", "confidence": 0.8019737899303436}, {"text": "event prediction", "start_pos": 181, "end_pos": 197, "type": "TASK", "confidence": 0.7541391253471375}]}, {"text": "Causality can be expressed using various natural language constructions ().", "labels": [], "entities": []}, {"text": "Consider the following examples where causal relations are encoded using (1) a verb-verb pair, (2) a noun-noun pair and (3) a verb-noun pair.", "labels": [], "entities": []}, {"text": "1. Five shoppers were killed when a car blew up at an outdoor market.", "labels": [], "entities": []}, {"text": "2. The attack on Kirkuk's police intelligence complex sees further deaths after violence spilled over a nearby shopping mall.", "labels": [], "entities": [{"text": "Kirkuk's police intelligence complex", "start_pos": 17, "end_pos": 53, "type": "DATASET", "confidence": 0.6716874837875366}]}, {"text": "3. At least 1,833 people died in hurricane.", "labels": [], "entities": []}, {"text": "Since, the task of automatic recognition of causality is quite challenging, researchers have addressed this problem by considering specific constructions.", "labels": [], "entities": []}, {"text": "For example, various models have been proposed to identify causation between verbs ( and between nouns (.", "labels": [], "entities": []}, {"text": "have worked with verb-noun pairs for causality detection but they focused only on a small list of predefined nouns representing events.", "labels": [], "entities": [{"text": "causality detection", "start_pos": 37, "end_pos": 56, "type": "TASK", "confidence": 0.8828900158405304}]}, {"text": "In this paper, we focus on the task of identifying causality encoded by verb-noun pairs (example 3).", "labels": [], "entities": []}, {"text": "We propose a novel model which first predicts cause or non-cause relations using a supervised classifier and then incorporates additional types of knowledge to reduce errors in predictions.", "labels": [], "entities": []}, {"text": "Using a supervised classifier, our model identifies causation by employing shallow linguistic features (e.g., lemmas of verb and noun, words between verb and noun).", "labels": [], "entities": []}, {"text": "Such features have been used successfully for various NLP tasks (e.g., partof-speech tagging, named entity recognition, etc.) but confinement to such features does not help much to achieve performance for identifying causation (.", "labels": [], "entities": [{"text": "partof-speech tagging", "start_pos": 71, "end_pos": 92, "type": "TASK", "confidence": 0.8232133090496063}, {"text": "named entity recognition", "start_pos": 94, "end_pos": 118, "type": "TASK", "confidence": 0.6048693458239237}]}, {"text": "Therefore, in our model we plugin additional types of knowledge to obtain better predictions for the current task.", "labels": [], "entities": []}, {"text": "For example, we identify the semantic classes of nouns and verbs with high tendency to encode cause or non-causal relations and use this knowledge to achieve better performance.", "labels": [], "entities": []}, {"text": "Specifically, the contributions of this paper are as follows: \u2022 In order to build a supervised classifier, we use the annotations of FrameNet to generate a training corpus of verb-noun instances encoding cause and non-cause relations.", "labels": [], "entities": []}, {"text": "We propose a set of linguistic features to learn and identify causal relations.", "labels": [], "entities": []}, {"text": "\u2022 In order to make intelligent predictions, it is important for our model to have knowledge about the semantic classes of nouns with high tendency to encode causal or non-causal relations.", "labels": [], "entities": []}, {"text": "For example, a named entity such as person, organization or location may have high tendency to encode non-causality unless a metonymic reading is associated with it.", "labels": [], "entities": []}, {"text": "In our approach, we identify such semantic classes of nouns by exploiting a named entity recognizer, the annotations of frame elements provided in FrameNet and WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 160, "end_pos": 167, "type": "DATASET", "confidence": 0.9318493008613586}]}, {"text": "\u2022 Verbs are the important components of language for expressing events of various types.", "labels": [], "entities": []}, {"text": "For example, have classified events into eight semantic classes: OCCURRENCE, PERCEPTION, RE-PORTING, ASPECTUAL, STATE, I STATE, I ACTION and MODAL.", "labels": [], "entities": [{"text": "OCCURRENCE", "start_pos": 65, "end_pos": 75, "type": "METRIC", "confidence": 0.9225859642028809}, {"text": "PERCEPTION", "start_pos": 77, "end_pos": 87, "type": "METRIC", "confidence": 0.974179744720459}, {"text": "RE-PORTING", "start_pos": 89, "end_pos": 99, "type": "METRIC", "confidence": 0.9916898012161255}, {"text": "ASPECTUAL", "start_pos": 101, "end_pos": 110, "type": "METRIC", "confidence": 0.9538087844848633}, {"text": "MODAL", "start_pos": 141, "end_pos": 146, "type": "METRIC", "confidence": 0.8817280530929565}]}, {"text": "We argue that there are some semantic classes in this list with high tendency to encode cause or non-cause relations.", "labels": [], "entities": []}, {"text": "For example, reporting events represented by verbs say, tell, etc., have high tendency to just report other events instead of encoding causality with them.", "labels": [], "entities": []}, {"text": "In our model, we use such information to reduce errors in predictions.", "labels": [], "entities": []}, {"text": "\u2022 Each causal relation is characterized by two roles i.e., cause and its effect.", "labels": [], "entities": []}, {"text": "In example 3 above, the noun \"hurricane\" is cause and the verb \"died\" is its effect.", "labels": [], "entities": []}, {"text": "However, a verb-noun pair may not encode causality when a verb and a noun represent same event.", "labels": [], "entities": []}, {"text": "For example, in instance \"Colin Powell presented further evidence in his presentation.\", the verb \"presented\" and the noun \"presentation\" represent same event of \"presenting\" and thus encoding non-cause relation with each other.", "labels": [], "entities": []}, {"text": "In our model, we determine the verb-noun pairs representing same or distinct events to make predictions accordingly.", "labels": [], "entities": []}, {"text": "\u2022 We adopt the framework of Integer Linear Programming (ILP) () to combine all the above types of knowledge for the current task.", "labels": [], "entities": []}, {"text": "This paper is organized as follows.", "labels": [], "entities": []}, {"text": "In next section, we briefly review the previous research done for identifying causality.", "labels": [], "entities": []}, {"text": "We introduce our model and evaluation with discussion on results in section 3 and 4, respectively.", "labels": [], "entities": []}, {"text": "The section 5 of the paper concludes our current research.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we present the experiments, evaluation procedures, and a discussion on the results achieved through our model for the current task.", "labels": [], "entities": []}, {"text": "In order to evaluate our model, we generated a test set with instances of form verb-noun phrase where the verb is grammatically connected to the noun phrase in an instance.", "labels": [], "entities": []}, {"text": "For this purpose, we collected three wiki articles on the topics of Hurricane Katrina, Iraq War and Egyptian Revolution of 2011.", "labels": [], "entities": []}, {"text": "We selected first 100 sentences from these articles and applied part-of-speech tagger () and dependency parser) on these sentences.", "labels": [], "entities": []}, {"text": "Using each sentence, we extracted all verb-noun phrase pairs where the verb has a dependency relation with any word of noun phrase.", "labels": [], "entities": []}, {"text": "We manually inspected all of the extracted instances and removed those instances in which a word had been wrongly classified as a verb by the part-of-speech tagger.", "labels": [], "entities": []}, {"text": "There are total 1106 instances in our test set.", "labels": [], "entities": []}, {"text": "We assigned the task of annotation of these instances with cause and non-cause relations to a human annotator.", "labels": [], "entities": []}, {"text": "Using manipulation theory of causality, we adopted the annotation guidelines from which is as follows: \"Assign cause label to a pair (a, b), if the following two conditions are satisfied: (1), a temporally precedes/overlap bin time, (2) while keeping as many state of affairs constant as possible, modifying a must entail predictably modifying b.", "labels": [], "entities": []}, {"text": "\" We have 149 (957) cause (non-cause) instances in our test set 3 , respectively.", "labels": [], "entities": []}, {"text": "We evaluate the performance of our model using F-score and accuracy evaluation measures for results).", "labels": [], "entities": [{"text": "F-score", "start_pos": 47, "end_pos": 54, "type": "METRIC", "confidence": 0.9944256544113159}, {"text": "accuracy", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.9982026815414429}]}, {"text": "The results in table 2 reveal that the basic supervised classifier is a naive model and achieves only 27.27% F-score and 46.47% accuracy.", "labels": [], "entities": [{"text": "F-score", "start_pos": 109, "end_pos": 116, "type": "METRIC", "confidence": 0.9992213249206543}, {"text": "accuracy", "start_pos": 128, "end_pos": 136, "type": "METRIC", "confidence": 0.9986876845359802}]}, {"text": "The addition of novel types of knowledge introduced in section 3 (i.e., the model Basic+SCN M +SCV+IVN) brings 14.74% (29.57%) improvements in F-score (accuracy), respectively.", "labels": [], "entities": [{"text": "F-score", "start_pos": 143, "end_pos": 150, "type": "METRIC", "confidence": 0.9986705780029297}, {"text": "accuracy", "start_pos": 152, "end_pos": 160, "type": "METRIC", "confidence": 0.7750736474990845}]}, {"text": "These results show that the knowledge of semantics of nouns and verbs and the knowledge of indistinguishable verb and noun are critical to achieve performance.", "labels": [], "entities": []}, {"text": "The maximum improvement in results is achieved with the addition of semantic classes of nouns (i.e., Basic+SCN M ).", "labels": [], "entities": []}, {"text": "The consideration of association of metonymic readings using model Basic+SCN M helps us to maintain recall as compared with SCN !M and therefore brings better F-score.", "labels": [], "entities": [{"text": "recall", "start_pos": 100, "end_pos": 106, "type": "METRIC", "confidence": 0.9995241165161133}, {"text": "F-score", "start_pos": 159, "end_pos": 166, "type": "METRIC", "confidence": 0.9982668161392212}]}, {"text": "One can notice that almost all models suffer from low precision which leads to lower F-scores.", "labels": [], "entities": [{"text": "precision", "start_pos": 54, "end_pos": 63, "type": "METRIC", "confidence": 0.9985552430152893}, {"text": "F-scores", "start_pos": 85, "end_pos": 93, "type": "METRIC", "confidence": 0.9959743618965149}]}, {"text": "Although, our model achieves 14.58% increase in precision over basic supervised classifier, the lack of high precision is still responsible for lower F- We will make the test set available  In example 6, \"lawsuit\" is the direct object of the verb \"prompted\" and is part of the event represented by the verb \"prompt\".", "labels": [], "entities": [{"text": "precision", "start_pos": 48, "end_pos": 57, "type": "METRIC", "confidence": 0.9991232752799988}, {"text": "precision", "start_pos": 109, "end_pos": 118, "type": "METRIC", "confidence": 0.9745026230812073}, {"text": "F-", "start_pos": 150, "end_pos": 152, "type": "METRIC", "confidence": 0.9921258091926575}]}, {"text": "However there is a cause relation between \"protection failures\" and \"prompted\".", "labels": [], "entities": []}, {"text": "Similarly in example 7, the direct object \"forecasts\" is part of the \"providing\" event and thus the noun phrase \"weather forecasts\" encode non-cause relation with the verb \"provide\".", "labels": [], "entities": []}, {"text": "Therefore, following this observation we employed the training corpus of cause and noncause relations (see section 3.1) and learned the structure of verb-noun phrase pairs encoding noncause relations most of the time.", "labels": [], "entities": []}, {"text": "We considered only those training instances where the subject and/or object of the verb was available.", "labels": [], "entities": []}, {"text": "For the current purpose, we picked up following four features (1) sub in np, (2) !sub in np, (3) obj in np and (4) !obj in np.", "labels": [], "entities": []}, {"text": "Just to remind the reader, the feature sub in np (!sub in np) is set to 1 if the subject of the verb is (not) contained in the noun phrase np, respectively.", "labels": [], "entities": []}, {"text": "For each of the above four features, the percentage of cause and entropy of relations with that feature are as follows: There are two important observations from above scores: (1) verbs mostly encode non-cause relations with their objects and subjects (i.e., high %\u00acc with obj in np and sub in np), (2) among obj in np and sub in np features, obj in np yields least entropy i.e., there are least chances of encoding causality of a verb with its object.", "labels": [], "entities": []}, {"text": "Considering the above statistics, we enforce the constraint on each verb-noun phrase pair that if the object of the verb is contained in the noun phrase of the above pair then assigns non-cause relation to that pair.", "labels": [], "entities": []}, {"text": "Using this constraint, we obtain 46.61% (80.74%) F-score (accuracy), respectively.", "labels": [], "entities": [{"text": "F-score", "start_pos": 49, "end_pos": 56, "type": "METRIC", "confidence": 0.9984649419784546}, {"text": "accuracy", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.9923972487449646}]}, {"text": "This confirms our observation that the object of a verb is normally part of an event represented by the verb and thus it encodes non-cause relation with the verb.", "labels": [], "entities": []}, {"text": "In this research, we have utilized novel types of knowledge to improve the performance of our model.", "labels": [], "entities": []}, {"text": "In future, we need to consider more additional information (e.g., predictions from metonymy resolver) to achieve further progress.", "labels": [], "entities": [{"text": "metonymy resolver", "start_pos": 83, "end_pos": 100, "type": "TASK", "confidence": 0.707111269235611}]}], "tableCaptions": []}