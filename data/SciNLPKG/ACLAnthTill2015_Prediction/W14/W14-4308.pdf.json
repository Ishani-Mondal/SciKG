{"title": [], "abstractContent": [{"text": "Non-cooperative dialogue behaviour has been identified as important in a variety of application areas, including education , military operations, video games and healthcare.", "labels": [], "entities": []}, {"text": "However, it has not been addressed using statistical approaches to dialogue management, which have always been trained for cooperative dialogue.", "labels": [], "entities": [{"text": "dialogue management", "start_pos": 67, "end_pos": 86, "type": "TASK", "confidence": 0.8234235942363739}]}, {"text": "We develop and evaluate a statistical dialogue agent which learns to perform non-cooperative dialogue moves in order to complete its own objectives in a stochas-tic trading game.", "labels": [], "entities": []}, {"text": "We show that, when given the ability to perform both cooperative and non-cooperative dialogue moves, such an agent can learn to bluff and to lie so as to win games more often-against a variety of adversaries, and under various conditions such as risking penalties for being caught in deception.", "labels": [], "entities": []}, {"text": "For example , we show that a non-cooperative dialogue agent can learn to win an additional 15.47% of games against a strong rule-based adversary, when compared to an op-timised agent which cannot perform non-cooperative moves.", "labels": [], "entities": []}, {"text": "This work is the first to show how an agent can learn to use non-cooperative dialogue to effectively meet its own goals.", "labels": [], "entities": []}], "introductionContent": [{"text": "Research in automated conversational systems has almost exclusively focused on the case of cooperative dialogue, where a dialogue system's core goal is to assist humans in particular tasks, such as buying airline tickets () or finding a place to eat.", "labels": [], "entities": []}, {"text": "Gricean cooperative principles have been shown to emerge from multi-agent decision theory, in a language task modelled using Decentralised Partially Observable Markov Decision Processes (, and in related work conversational implicature was argued to be a by-product of agents who maximise joint utility ().", "labels": [], "entities": [{"text": "multi-agent decision theory", "start_pos": 62, "end_pos": 89, "type": "TASK", "confidence": 0.7030374805132548}]}, {"text": "However, non-cooperative dialogues, where an agent may act to satisfy its own goals rather than those of other participants, are also of practical and theoretical interest (, and the game-theoretic underpinnings of non-Gricean behaviour are actively being investigated.", "labels": [], "entities": []}, {"text": "For example, it maybe advantageous for an automated agent not to be fully cooperative when trying to gather information from a human, and when trying to persuade, argue, or debate, when trying to sell them something, when trying to detect illegal activity (for example on internet chat sites), or in the area of believable characters in video games and educational simulations (.", "labels": [], "entities": []}, {"text": "Another arena in which non-cooperative dialogue behaviour is desirable is in negotiation, where hiding information (and even outright lying) can be advantageous.", "labels": [], "entities": []}, {"text": "Furthermore, deception is considered to bean essential part of successful military operations.", "labels": [], "entities": []}, {"text": "According to Sun Tzu \"All warfare is based on deception\" and Machiavelli clearly states in The Discourses that \"Although deceit is detestable in all other things, yet in the conduct of war it is laudable and honorable\".", "labels": [], "entities": [{"text": "The Discourses", "start_pos": 91, "end_pos": 105, "type": "DATASET", "confidence": 0.8645963966846466}]}, {"text": "Indeed, Dennett argues that deception capability is required for higher-order intentionality in AI.", "labels": [], "entities": []}, {"text": "A complementary research direction in recent years has been the use of machine learning methods to automatically optimise cooperative dialogue management -i.e. the decision of what dialogue move to make next in a conversation, in order to maximise an agent's overall long-term expected utility, which is usually defined in terms of meeting a user's goals (.", "labels": [], "entities": [{"text": "cooperative dialogue management", "start_pos": 122, "end_pos": 153, "type": "TASK", "confidence": 0.6819640795389811}]}, {"text": "This research has shown how robust and efficient dialogue management strategies can be learned from data, but has only addressed the case of cooperative dialogue.", "labels": [], "entities": []}, {"text": "These approaches use Reinforcement Learning with a reward function that gives positive feedback to the agent only when it meets the user's goals.", "labels": [], "entities": []}, {"text": "An example of the type of non-cooperative dialogue behaviour which we are generating in this work is given by agent B in the following dialogue: A: \"I will give you a sheep if you give me a wheat\" B: \"No\" B: \"I really need rock\" [B actually needs wheat] A: \"OK...", "labels": [], "entities": []}, {"text": "I'll give you a wheat if you give me rock\" B: \"OK\" Here, A is deceived into providing the wheat that B actually needs, because A believes that B needs rock rather than wheat.", "labels": [], "entities": []}, {"text": "Similar behaviour can be observed in trading games such as Settlers of Catan ().", "labels": [], "entities": [{"text": "Settlers of Catan", "start_pos": 59, "end_pos": 76, "type": "DATASET", "confidence": 0.7072064677874247}]}], "datasetContent": [{"text": "This strategy was designed to form a challenging rational adversary for measuring baseline performance.", "labels": [], "entities": []}, {"text": "It cannot be manipulated at all, and noncooperative dialogue moves will have no effect on it -it simply ignores statements like \"I really need wheat\".", "labels": [], "entities": []}, {"text": "The strict rule-based strategy of the adversary will never ask fora resource that it does not need (in this case rocks).", "labels": [], "entities": []}, {"text": "Furthermore, if it has an available non-goal resource to give then it will offer it.", "labels": [], "entities": []}, {"text": "It only asks for resources that it needs (goal resources: wheat and sheep).", "labels": [], "entities": []}, {"text": "In the case where it does not have a non-goal resource (rocks) to offer then it offers a goal resource only if its quantity is more than it needs, and it asks for another goal resource if it is needed.", "labels": [], "entities": []}, {"text": "Following the same reasoning, when replying to the LA's trading proposals, the adversary will never agree to receive a non-goal resource (rock).", "labels": [], "entities": []}, {"text": "It only gives a non-goal resource (rock) for another one that it needs (wheat or sheep).", "labels": [], "entities": []}, {"text": "It also agrees to make a trade in the special case where it will give a goal resource of which it has more than it needs for another one that it still needs.", "labels": [], "entities": []}, {"text": "This is a strong strategy that wins a significant number of games.", "labels": [], "entities": []}, {"text": "In fact, it takes about 100,000 training games before the LA is able to start winning more games than this adversary, and a random LA policy loses 66% of games against this adversary (See, LA policy 'Random').", "labels": [], "entities": []}, {"text": "The adversary in this case retains the above strict base-line policy but it is also susceptible to the non-cooperative moves of the LA, as explained above.", "labels": [], "entities": []}, {"text": "For example, if the LA utters \"I really need rock\", weights of actions which transfer rock from the adversary will decrease, and the adversary will then be less likely to give rock to the LA.", "labels": [], "entities": []}, {"text": "Conversely, the adversary is then more likely to give the other two resources to the LA.", "labels": [], "entities": []}, {"text": "In this way the LA has the potential to mislead the adversary into trading resources that it really needs.", "labels": [], "entities": []}, {"text": "Here we investigate performance against adversaries who cannot be manipulated, but their strategy is to always restrict the LA from gaining a specific type of resource.", "labels": [], "entities": []}, {"text": "We need to explore how well a manipulated adversary (for example one who will no longer give rocks that only its opponent needs) performs.", "labels": [], "entities": []}, {"text": "This will show us the potential advantage to be gained by manipulation and most important, it will generalise our problem by showing that the restriction (boycott) of a resource that only the opponent needs, or of a resource that both of the players need, are actually reasonably good strategies compared to the baseline case (Experiment 1).", "labels": [], "entities": []}, {"text": "Hence, the manipulated adversary has indeed a reason for choosing to restrict resources (Experiment 2) rather than staying with its rule-based strategy.", "labels": [], "entities": []}, {"text": "In other words it has a rational reason to become gullible and fall in the learning agent's trap.", "labels": [], "entities": []}, {"text": "Here we extend the problem to include possible negative consequences of manipulative LA actions.", "labels": [], "entities": []}, {"text": "The adversary begins each game with a probability of detecting manipulation, that exponentially increases after everyone of the LA's manipulative actions.", "labels": [], "entities": [{"text": "detecting manipulation", "start_pos": 53, "end_pos": 75, "type": "TASK", "confidence": 0.7149482667446136}]}, {"text": "In more detail, every time the LA performs a manipulation, there is an additional chance that the adversary notices this (starts at 1-in-10 and increases after every manipulative move, up to 100% in the case of the 10th manipulative attempt).", "labels": [], "entities": []}, {"text": "The consequence of being detected is that the adversary will refuse to trade with the LA any further in that game (experiment 4.1), or that the adversary automatically wins the game (experiment 4.2).", "labels": [], "entities": []}, {"text": "In these two cases there is always a risk associated with attempting to manipulate, and the LA has to learn how to balance the potential rewards with this risk.", "labels": [], "entities": []}, {"text": "The LA scored a winning performance of 49.5% against 45.555% for the adversary, with 4.945% draws), in the 20 thousand test games, see.", "labels": [], "entities": []}, {"text": "This represents the baseline performance that the LA is able to achieve against an adversary who cannot be manipulated at all.", "labels": [], "entities": []}, {"text": "This shows that the game is 'solvable' as an MDP problem, and that a reinforcement learning agent can outperform a strict hand-coded adversary.", "labels": [], "entities": []}, {"text": "Here, the learning agent's strategy mainly focuses on offering the sheep resource that it does not need for the rocks that does need (for example action7 > action2 > action6 > action3 Table 2).", "labels": [], "entities": []}, {"text": "It is also interesting to notice that the LA learnt not to use action 3 at all (gives 1 wheat that they both need for 1 sheep that only the adversary needs).", "labels": [], "entities": []}, {"text": "Hence its frequency is 0.", "labels": [], "entities": []}, {"text": "The actions 4 and 5 are never accepted by the adversary so their role in both of the experiments is similar to that of the action 1 (do nothing).", "labels": [], "entities": []}, {"text": "The rejections of the adversary's trades dominate the acceptances with a ratio of 94 to 1 as our learning agent learns to become negative towards the adversarial trading proposals and therefore to prohibit its strategy.", "labels": [], "entities": []}, {"text": "In Experiment 2 the learning agent scored a winning performance of 59.17% against only 39.755% of its adversary, having 1.075% draws, in the 20 thousand test games, see.", "labels": [], "entities": []}, {"text": "Similarly to the previous experiment, the LA's strategy focuses again mainly on action 7, by offering the sheep resource that it does not need for rocks that it needs.", "labels": [], "entities": []}, {"text": "However in this case we also notice that the LA has learnt to use action 2 very often, exploiting cases where it will win by giving the wheat resource that they both need fora rock that only it needs.", "labels": [], "entities": []}, {"text": "This is a result of its current manipulation capabilities.", "labels": [], "entities": []}, {"text": "The high frequency manipulative actions 8 (\"I really need wheat\") and 9 (\"I really need rock\") assist in deceiving its adversary by hiding information, therefore significantly reinforcing its strategy as they both indirectly result in gaining sheep that only the adversary needs (experiment 3.2).", "labels": [], "entities": []}, {"text": "Rejections to adversarial trading offers over the acceptances were again the majority in this experiment.", "labels": [], "entities": []}, {"text": "However in this case they are significantly fewer than before, with a ratio of only 2.5 to 1, as our learning agent is now more eager to accept some trades because it has triggered them itself by appropriately manipulating its adversary.", "labels": [], "entities": []}, {"text": "In Experiment 1 the LA's dominating strategy (mainly based on requiring the rocks resource from its adversary) provides it with a difference in winning performance of +3.945%.", "labels": [], "entities": []}, {"text": "In Experiment 2 the adversary, further being deceived by the learning agent's hiding information actions, loses 19.415% more often than the learning agent.", "labels": [], "entities": []}, {"text": "In experiment 3 the LA uses no manipulative actions.", "labels": [], "entities": []}, {"text": "It is the same LA as that of Experiment 1.", "labels": [], "entities": [{"text": "LA", "start_pos": 15, "end_pos": 17, "type": "METRIC", "confidence": 0.9947452545166016}]}, {"text": "It is trained and then tested against 3 different types of restrictive adversaries.", "labels": [], "entities": []}, {"text": "The first one (Experiment 3.1) never gives wheat, the second one (Experiment 3.2) never gives rocks, and the third one never gives sheep (Experiment 3.3).", "labels": [], "entities": []}, {"text": "They all act randomly regarding the other 2 resources which are not restricted.", "labels": [], "entities": []}, {"text": "In the first case (adversary restricts wheat that they both need), the LA scored a winning performance of 50.015% against 47.9% of its adversary, having 2.085% draws in the 20 thousand test games.", "labels": [], "entities": []}, {"text": "In the second case (adversary restricts rocks that the LA only needs), the LA scored a winning performance of 53.375% against 44.525% of its adversary, having 2.1% draws in the 20 thousand test games.", "labels": [], "entities": []}, {"text": "In the third case (adversary restricts sheep that only itself needs), the LA scored a winning performance of 62.21% against 35.13% of its adversary, having 2.66% draws in the 20 thousand test games.", "labels": [], "entities": [{"text": "LA", "start_pos": 74, "end_pos": 76, "type": "METRIC", "confidence": 0.8694671988487244}]}, {"text": "These results show that restricting the resource that only the opponent needs (i.e. LA only needs rocks) and especially the resource that they both need (i.e. wheat) can be as effective as the strategy followed by the rule-based adversary (see).", "labels": [], "entities": []}, {"text": "The difference in the performances for the former case (rock) is +8.85% and for the latter (wheat) only +2.115%.", "labels": [], "entities": []}, {"text": "That means the adversary has indeed a reason to believe that boycotting its opponent's resources could be a winning opposing strategy, motivating its gullibility in experiment 2 (section 5.2).", "labels": [], "entities": []}, {"text": "In this case if the LA becomes exposed by the adversary then the latter wins the game.", "labels": [], "entities": []}, {"text": "The LA scored a winning performance of 36.125% against 61.15% of its adversary, having 2.725% draws in 20 thousand test games, see.", "labels": [], "entities": []}, {"text": "It is the only case where the LA so far has not yet found a strategy that wins more often than its adversary, and therefore in future work a larger set of training games will be used.", "labels": [], "entities": []}, {"text": "Note that this was only trained for 350 thousand games -we expect better performance with more training.", "labels": [], "entities": []}, {"text": "In fact, here we would expect a good policy to perform at least as well as experiment 1, which would be the case of learning never to use manipulative actions, since they are so dangerous.", "labels": [], "entities": []}, {"text": "Indeed, a good policy could be to lie (action 10) only once, at the start of a dialogue, and then to follow the policy of experiment 2.", "labels": [], "entities": []}, {"text": "This would lead to a winning percentage of about 49% (the 59% of experiment 2 minus a 10% loss for the chance of being detected after 1 manipulation).", "labels": [], "entities": []}, {"text": "The LA has so far managed to locate a strategy that again balances the use of the manipulative actions and that of the normal ones with the risk of losing the game as a result of exposure.", "labels": [], "entities": []}, {"text": "According to we notice that the LA gradually learns how to do that.", "labels": [], "entities": []}, {"text": "However its performance is not yet desirable, as it is still only slightly better than that of the Random case against the Baseline.", "labels": [], "entities": []}, {"text": "It is interesting though to see that the strategy that the LA uses here makes frequent use of the action 10 (\"I really need sheep\") that lies.", "labels": [], "entities": []}, {"text": "On the other hand, the actions 8 and 9 are almost non-existent.", "labels": [], "entities": []}, {"text": "That results in accepting wheat that they both need and rocks that it only needs, showing that the main focus of the manipulation is on the personal goal.", "labels": [], "entities": []}, {"text": "The LA has learned so far in this case that by lying it can get closer to its personal goal.", "labels": [], "entities": []}, {"text": "Rejections to adversary's proposals over the acceptances resulted in a ratio of approximately 1.7 to 1, meaning that the LA is again quite eager to accept the adversarial trading proposals that it has triggered already by itself through lying.", "labels": [], "entities": []}, {"text": "We report further results on this scenario in an updated version of this paper ().", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Frequencies of LA actions.", "labels": [], "entities": []}, {"text": " Table 1: Performance (% wins) in testing games, after training. (*= significant improvement over base- line, p < 0.05)", "labels": [], "entities": []}, {"text": " Table 3: Frequencies of LA actions.", "labels": [], "entities": []}]}