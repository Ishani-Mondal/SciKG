{"title": [{"text": "DALES: Automated Tool for Detection, Annotation, Labelling and Segmentation of Multiple Objects in Multi-Camera Video Streams", "labels": [], "entities": [{"text": "DALES", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.5508013963699341}, {"text": "Labelling and Segmentation of Multiple Objects in Multi-Camera Video Streams", "start_pos": 49, "end_pos": 125, "type": "TASK", "confidence": 0.6520034909248352}]}], "abstractContent": [{"text": "In this paper, we propose anew software tool called DALES to extract semantic information from multi-view videos based on the analysis of their visual content.", "labels": [], "entities": []}, {"text": "Our system is fully automatic and is well suited for multi-camera environment.", "labels": [], "entities": []}, {"text": "Once the multi-view video sequences are loaded into DALES, our software performs the detection, counting, and segmentation of the visual objects evolving in the provided video streams.", "labels": [], "entities": []}, {"text": "Then, these objects of interest are processed in order to be labelled, and the related frames are thus annotated with the corresponding semantic content.", "labels": [], "entities": []}, {"text": "Moreover, a textual script is automatically generated with the video annotations.", "labels": [], "entities": []}, {"text": "DALES system shows excellent performance in terms of accuracy and computational speed and is robustly designed to ensure view synchronization.", "labels": [], "entities": [{"text": "DALES", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8253144025802612}, {"text": "accuracy", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.9991808533668518}]}], "introductionContent": [{"text": "With the increasing use of electronic equipments, storage devices and computational systems for applications such as video surveillance () and sport event monitoring, the development of automated tools to process the resulting big amount of visual data in order to extract meaningful information becomes a necessity.", "labels": [], "entities": [{"text": "sport event monitoring", "start_pos": 143, "end_pos": 165, "type": "TASK", "confidence": 0.6196812192598978}]}, {"text": "In particular, the design of multi-view video annotation systems is a challenging, new task.", "labels": [], "entities": []}, {"text": "It aims to process multi-view video streams which consist of video sequences of a dynamic scene captured simultaneously by multiple cameras.", "labels": [], "entities": []}, {"text": "Such multi-input system is dedicated to automatically analyse the visual content of the multi-camera records and to generate semantic and visual annotations, in the way to assist users in the understanding and reasoning about large amount of acquired data.", "labels": [], "entities": [{"text": "understanding and reasoning about large amount of acquired data", "start_pos": 192, "end_pos": 255, "type": "TASK", "confidence": 0.7038388219144609}]}, {"text": "For this purpose, data should be processed through different, major stages such as object-of-interest detection and segmentation, frame labelling, and video annotation.", "labels": [], "entities": [{"text": "object-of-interest detection and segmentation", "start_pos": 83, "end_pos": 128, "type": "TASK", "confidence": 0.7406533658504486}, {"text": "frame labelling", "start_pos": 130, "end_pos": 145, "type": "TASK", "confidence": 0.7662753760814667}]}, {"text": "In the literature, most of the works dealing with the analysis of multi-camera video streams are focused on the sole task of tracking multiple, moving objects and use different approaches such as background subtraction), Bayesian framework (, particle filter (, or Cardinalized Probability Hypothesis Density (CPHD) based filter ().", "labels": [], "entities": []}, {"text": "On the other hand, research on video annotation has lead to the development of several efficient systems), but all designed fora single camera video stream input.", "labels": [], "entities": [{"text": "video annotation", "start_pos": 31, "end_pos": 47, "type": "TASK", "confidence": 0.7427052855491638}]}, {"text": "In this paper, we describe a full system which takes multi-camera video stream inputs and performs visual data processing to generate multi-view video annotations.", "labels": [], "entities": []}, {"text": "Our system has been developed in context of outdoor video-surveillance and is an automatic Detection, Annotation, LabElling and Segmentation (DALES) software tool.", "labels": [], "entities": []}, {"text": "It presents also the advantage to have an entire chain of data processing from visual to textual one, reducing thus the semantic gap.", "labels": [], "entities": []}, {"text": "As camera calibration is in general an expensive process) and in real life, surveillance application measurements of camera parameters are not readily available, DALES system does not involve any camera calibration parameters.", "labels": [], "entities": []}, {"text": "DALES provides not only the automatic annotation of multi-view video streams, but also performs, in multi-camera environment, tasks such as multiple object-of-interest detection and segmentation, target counting and labelling, and image annotations.", "labels": [], "entities": [{"text": "multiple object-of-interest detection and segmentation", "start_pos": 140, "end_pos": 194, "type": "TASK", "confidence": 0.7547438144683838}, {"text": "target counting and labelling", "start_pos": 196, "end_pos": 225, "type": "TASK", "confidence": 0.8102005273103714}, {"text": "image annotations", "start_pos": 231, "end_pos": 248, "type": "TASK", "confidence": 0.7186325490474701}]}, {"text": "Our approach could cope with any dynamic scene recorded by pan-tilt-zoom (PTZ) static cameras with overlapping color views, unlike systems such as) based on non-overlapping cameras.", "labels": [], "entities": []}, {"text": "The acquired multi-view sequences could contain complex backgrounds, moving objects or noisy foregrounds, and present illumination variations or poor resolution.", "labels": [], "entities": [{"text": "resolution", "start_pos": 150, "end_pos": 160, "type": "METRIC", "confidence": 0.951697587966919}]}, {"text": "Hence, the contribution of this paper is twofold: \u2022 the automated, textual annotation of multi-camera video streams based on visual features; \u2022 the development of a full, automatic system covering all the phases from multiple, visual multimotion target detection to multi-view video annotation and text-script generation.", "labels": [], "entities": [{"text": "multimotion target detection", "start_pos": 234, "end_pos": 262, "type": "TASK", "confidence": 0.7649057706197103}, {"text": "text-script generation", "start_pos": 298, "end_pos": 320, "type": "TASK", "confidence": 0.7209590822458267}]}, {"text": "The paper is structured as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we describe our DALES system for fast, multiple video-object detection, segmentation, labeling and effective multi-view video annotation.", "labels": [], "entities": [{"text": "multiple video-object detection", "start_pos": 53, "end_pos": 84, "type": "TASK", "confidence": 0.7034604350725809}]}, {"text": "Our tool has been successfully tested on standard, real-world video-surveillance dataset as reported and discussed in Section 3.", "labels": [], "entities": []}, {"text": "Conclusions are presented in Section 4.", "labels": [], "entities": []}], "datasetContent": [{"text": "To validate the DALES tool, we have applied our system on the standard dataset) consisting of video-surveillance dynamic scene recorded by two PTZ cameras.", "labels": [], "entities": [{"text": "PTZ", "start_pos": 143, "end_pos": 146, "type": "DATASET", "confidence": 0.9378386735916138}]}, {"text": "Each contains 2688 frames, whose average resolution is of 576x768 pixels and which were captured in outdoor environment.", "labels": [], "entities": []}, {"text": "This database owns challenges of multi-view video stream, as well as quantity, pose, motion, size, appearance and scale variations of the objects of interest, i.e. people and cars.", "labels": [], "entities": []}, {"text": "All the experiments have been run on a computer with Intel Core 2 Duo Pentium T9300, 2.5 GHz, 2Gb RAM, and using our DALES software implemented with C++.", "labels": [], "entities": [{"text": "RAM", "start_pos": 98, "end_pos": 101, "type": "METRIC", "confidence": 0.8192263841629028}]}, {"text": "Some examples of the results of our DALES system are presented in.", "labels": [], "entities": []}, {"text": "These frames present difficult situations such as poor foreground/background contrast or light reflection.", "labels": [], "entities": []}, {"text": "To assess the detection accuracy of DALES system, we adopt the standard criteria as follows: with T P, true positive, FP, false positive, and FN, false negative.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9417937994003296}, {"text": "FP", "start_pos": 118, "end_pos": 120, "type": "METRIC", "confidence": 0.9623717665672302}, {"text": "FN", "start_pos": 142, "end_pos": 144, "type": "METRIC", "confidence": 0.9754489064216614}]}, {"text": "The labelling accuracy of DALES system could be assessed using the following standard criterion: with TN, true negative.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9806952476501465}, {"text": "TN", "start_pos": 102, "end_pos": 104, "type": "METRIC", "confidence": 0.9864256381988525}]}, {"text": "In, we have reported the average detection and false alarm rates of our DALES method against the rates achieved by, while in, we have displayed the average accuracy of object-of-interest labelling of our DALES method against the rate obtained by.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 156, "end_pos": 164, "type": "METRIC", "confidence": 0.9988560676574707}]}, {"text": "From, we can conclude that our DALES system provides reliable detection and counting of objects of interest in multi-camera environment, and that the multiple-object labelling is very accurate as well, outperforming state-of-the art techniques.", "labels": [], "entities": []}, {"text": "DALES total precision to annotate multi-view videos is therefore very high.", "labels": [], "entities": [{"text": "DALES", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.6533057689666748}, {"text": "precision", "start_pos": 12, "end_pos": 21, "type": "METRIC", "confidence": 0.9820988178253174}]}, {"text": "For all the dataset, the average computational speed of our DALES software is in the range of few seconds, whereas the viewer function of DALES software takes only few milliseconds to process.", "labels": [], "entities": []}, {"text": "Hence, our developed system could be used in context of online scene analysis.", "labels": [], "entities": [{"text": "online scene analysis", "start_pos": 56, "end_pos": 77, "type": "TASK", "confidence": 0.5899475117524465}]}], "tableCaptions": [{"text": " Table 1: Average detection rates of object-of-interests in video frames.", "labels": [], "entities": []}, {"text": " Table 2: Average accuracy of object-of-interest labelling in video frames.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9673944115638733}]}]}