{"title": [{"text": "Probabilistic Modeling of Joint-context in Distributional Similarity", "labels": [], "entities": [{"text": "Similarity", "start_pos": 58, "end_pos": 68, "type": "TASK", "confidence": 0.6892374753952026}]}], "abstractContent": [{"text": "Most traditional distributional similarity models fail to capture syntagmatic patterns that group together multiple word features within the same joint context.", "labels": [], "entities": []}, {"text": "In this work we introduce a novel generic distributional similarity scheme under which the power of probabilistic models can be leveraged to effectively model joint contexts.", "labels": [], "entities": []}, {"text": "Based on this scheme, we implement a concrete model which utilizes probabilistic n-gram language models.", "labels": [], "entities": []}, {"text": "Our evaluations suggest that this model is particularly well-suited for measuring similarity for verbs, which are known to exhibit richer syntag-matic patterns, while maintaining comparable or better performance with respect to competitive baselines for nouns.", "labels": [], "entities": []}, {"text": "Following this, we propose our scheme as a framework for future semantic similarity models leveraging the substantial body of work that exists in probabilistic language modeling.", "labels": [], "entities": []}], "introductionContent": [{"text": "The Distributional Hypothesis is commonly phrased as \"words which are similar in meaning occur in similar contexts\".", "labels": [], "entities": [{"text": "Distributional Hypothesis", "start_pos": 4, "end_pos": 29, "type": "TASK", "confidence": 0.8469249606132507}]}, {"text": "Distributional similarity models following this hypothesis vary in two major aspects, namely the representation of the context and the respective computational model.", "labels": [], "entities": []}, {"text": "Probably the most prominent class of distributional similarity models represents context as a vector of word features and computes similarity using feature vector arithmetics.", "labels": [], "entities": []}, {"text": "To construct the feature vectors, the context of each target word token 1 , which is commonly a word window around it, is first broken into a set of individual independent words.", "labels": [], "entities": []}, {"text": "Then the weights of the entries in the word feature vector capture the degree of association between the target word type and each of the individual word features, independently of one another.", "labels": [], "entities": []}, {"text": "Despite its popularity, it was suggested that the word feature vector approach misses valuable information, which is embedded in the colocation and inter-relations of words (e.g. word order) within the same context.", "labels": [], "entities": []}, {"text": "Following this motivation, proposed an alternative compositefeature model, later adopted in (.", "labels": [], "entities": []}, {"text": "This model adopts a richer context representation by considering entire word window contexts as features, while keeping the same computational vector-based model.", "labels": [], "entities": []}, {"text": "Although showing interesting potential, this approach suffers from a very high-dimensional feature space resulting in data sparseness problems.", "labels": [], "entities": []}, {"text": "Therefore, it requires exceptionally large learning corpora to consider large windows effectively.", "labels": [], "entities": []}, {"text": "A parallel line of work adopted richer context representations as well, with a different computational model.", "labels": [], "entities": []}, {"text": "These works utilized neural networks to learn low dimensional continuous vector representations for word types, which were found useful for measuring semantic similarity).", "labels": [], "entities": []}, {"text": "These vectors are trained by optimizing the prediction of target words given their observed contexts (or variants of this objective).", "labels": [], "entities": []}, {"text": "Most of these models consider each observed context as a joint set of context words within a word window.", "labels": [], "entities": []}, {"text": "In this work we follow the motivation in the previous works above to exploit richer joint-context representations for modeling distributional similarity.", "labels": [], "entities": []}, {"text": "Under this approach the set of features in the context of each target word token is considered to jointly reflect on the meaning of the target word type.", "labels": [], "entities": []}, {"text": "To further facilitate this type of mod-eling we propose a novel probabilistic computational scheme for distributional similarity, which leverages the power of probabilistic models and addresses the data sparseness challenge associated with large joint-contexts.", "labels": [], "entities": [{"text": "distributional similarity", "start_pos": 103, "end_pos": 128, "type": "TASK", "confidence": 0.7611573040485382}]}, {"text": "Our scheme is based on the following probabilistic corollary to the distributional hypothesis: (1) \"words are similar in meaning if they are likely to occur in the same contexts\" To realize this corollary, our distributional similarity scheme assigns high similarity scores to word pairs a and b, for which a is likely in the contexts that are observed for band vice versa.", "labels": [], "entities": []}, {"text": "The scheme is generic in the sense that various underlying probabilistic models can be used to provide the estimates for the likelihood of a target word given a context.", "labels": [], "entities": []}, {"text": "This allows concrete semantic similarity models based on this scheme to leverage the capabilities of probabilistic models, such as established language models, which typically address the modeling of joint-contexts.", "labels": [], "entities": []}, {"text": "We hypothesize that an underlying model that could capture syntagmatic patterns in large word contexts, yet is flexible enough to deal with data sparseness, is desired.", "labels": [], "entities": []}, {"text": "It is generally accepted that the semantics of verbs in particular are correlated with their syntagmatic properties.", "labels": [], "entities": []}, {"text": "This provides grounds to expect that such model has the potential to excel for verbs.", "labels": [], "entities": []}, {"text": "To capture syntagmatic patterns, we choose in this work standard n-gram language models as the basis fora concrete model implementing our scheme.", "labels": [], "entities": []}, {"text": "This choice is inspired by recent work on learning syntactic categories (, which successfully utilized such language models to represent word window contexts of target words.", "labels": [], "entities": []}, {"text": "However, we note that other richer types of language models, such as class-based () or hybrid (, can be seamlessly integrated into our scheme.", "labels": [], "entities": []}, {"text": "Our evaluations suggest that our model is indeed particularly advantageous for measuring semantic similarity for verbs, while maintaining comparable or better performance with respect to competitive baselines for nouns.", "labels": [], "entities": []}], "datasetContent": [{"text": "Although sometimes used interchangeably, it is common to distinguish between semantic similarity and semantic relatedness).", "labels": [], "entities": []}, {"text": "Semantic similarity is used to describe 'likeness' relations, such as the relations between synonyms, hypernymhyponyms, and co-hyponyms.", "labels": [], "entities": []}, {"text": "Semantic relatedness refers to a broader range of relations including also meronymy and various other associative relations as in 'pencil-paper' or 'penguinAntarctica'.", "labels": [], "entities": []}, {"text": "In this work we focus on semantic similarity and evaluate all compared methods on several semantic similarity tasks.", "labels": [], "entities": [{"text": "semantic similarity", "start_pos": 25, "end_pos": 44, "type": "TASK", "confidence": 0.825321614742279}]}, {"text": "Following previous works we use Wordnet to construct large scale gold standards for semantic similarity evaluations.", "labels": [], "entities": [{"text": "Wordnet", "start_pos": 32, "end_pos": 39, "type": "DATASET", "confidence": 0.9452385306358337}, {"text": "semantic similarity evaluations", "start_pos": 84, "end_pos": 115, "type": "TASK", "confidence": 0.8041254281997681}]}, {"text": "We perform the evaluations separately for nouns and verbs to test our hypothesis that our model is particularly well-suited for verbs.", "labels": [], "entities": []}, {"text": "To further evaluate our results on verbs we use the verb similarity test-set released by), which contains pairs of verbs associated with semantic similarity scores based on human judgements.", "labels": [], "entities": []}, {"text": "There is a shortage of large scale test-sets for semantic similarity.", "labels": [], "entities": [{"text": "semantic similarity", "start_pos": 49, "end_pos": 68, "type": "TASK", "confidence": 0.7894589900970459}]}, {"text": "Popular test-sets such as WordSim353 and the TOEFL synonyms test contain only 353 and 80 test items respectively, and therefore make it difficult to obtain statistically significant results.", "labels": [], "entities": [{"text": "WordSim353", "start_pos": 26, "end_pos": 36, "type": "DATASET", "confidence": 0.9709790945053101}, {"text": "TOEFL synonyms test", "start_pos": 45, "end_pos": 64, "type": "DATASET", "confidence": 0.6800864934921265}]}, {"text": "To automatically construct largerscale test-sets for semantic similarity, we sampled large target word subsets from our corpus and used Wordnet as a gold standard for their semantically similar words, following related previous evaluations.", "labels": [], "entities": [{"text": "Wordnet", "start_pos": 136, "end_pos": 143, "type": "DATASET", "confidence": 0.9748077988624573}]}, {"text": "We constructed two test-sets for our primary evaluation, one for verb similarity and another for noun similarity.", "labels": [], "entities": []}, {"text": "To perform the verb similarity evaluation, we randomly sampled 1,000 verbs from the target verb set, where the probability of each verb to be sampled is set to be proportional to its frequency in the learning corpus.", "labels": [], "entities": [{"text": "verb similarity evaluation", "start_pos": 15, "end_pos": 41, "type": "TASK", "confidence": 0.765802135070165}]}, {"text": "Next, for each sampled verb awe constructed a Wordnet-based gold standard set of semantically similar words.", "labels": [], "entities": [{"text": "Wordnet-based gold standard set", "start_pos": 46, "end_pos": 77, "type": "DATASET", "confidence": 0.9367246031761169}]}, {"text": "In this set each verb a is annotated as a 'synonym' of a if at least one of the senses of a is a synonym of any of the senses of a.", "labels": [], "entities": []}, {"text": "In addition, each verb a is annotated as a 'semantic neighbor' of a if at least one of the senses of a is a synonym, co-hyponym, or a direct hypernym/hyponym of any of the senses of a.", "labels": [], "entities": []}, {"text": "We note that by definition all verbs annotated as synonyms of a are annotated as semantic neighbors as well.", "labels": [], "entities": []}, {"text": "Next, per each verb a and an evaluated method, we generated a ranked list of all other verbs, which was induced according to the similarity scores of this method.", "labels": [], "entities": []}, {"text": "Finally, we evaluated the compared methods on two tasks, 'synonym detection' and 'semantic neighbor detection'.", "labels": [], "entities": [{"text": "synonym detection", "start_pos": 58, "end_pos": 75, "type": "TASK", "confidence": 0.9067490100860596}, {"text": "semantic neighbor detection", "start_pos": 82, "end_pos": 109, "type": "TASK", "confidence": 0.6877481639385223}]}, {"text": "In the synonym detection task we evaluated the methods' ability to retrieve as much verbs annotated in our gold standard as 'synonyms', in the top-n entries of their ranked lists.", "labels": [], "entities": [{"text": "synonym detection", "start_pos": 7, "end_pos": 24, "type": "TASK", "confidence": 0.9718253016471863}]}, {"text": "Similarly, we evaluated all methods on the 'semantic neighbors' task.", "labels": [], "entities": []}, {"text": "The synonym detection task is designed to evaluate the ability of the compared methods to identify a more restrictive interpretation of semantic similarity, while the semantic neighbor detection task does the same fora somewhat broader interpretation.", "labels": [], "entities": [{"text": "synonym detection", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.9311619102954865}, {"text": "semantic neighbor detection", "start_pos": 167, "end_pos": 194, "type": "TASK", "confidence": 0.6936530669530233}]}, {"text": "We repeated the above procedure for sampling 1,000 target nouns, constructing the noun Wordnet-based gold standards and evaluating on the two semantic similarity tasks.", "labels": [], "entities": [{"text": "Wordnet-based gold standards", "start_pos": 87, "end_pos": 115, "type": "DATASET", "confidence": 0.8104161222775778}]}, {"text": "The publicly available VerbSim test-set contains 130 verb pairs, each annotated with an average of 6 human judgements of semantic similarity).", "labels": [], "entities": [{"text": "VerbSim test-set", "start_pos": 23, "end_pos": 39, "type": "DATASET", "confidence": 0.9097322225570679}]}, {"text": "We extracted a 107 pairs subset of this dataset for which all verbs are in our learning corpus.", "labels": [], "entities": []}, {"text": "We followed works such as) and compared the Spearman correlations between the verbpair similarity scores assigned by the compared methods and the manually annotated scores in this dataset.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Spearman correlation values obtained for  the VerbSim evaluation. Each method was evalu- ated with the optimal window order found in the  Wordnet verbs evaluation.", "labels": [], "entities": [{"text": "Wordnet verbs evaluation", "start_pos": 148, "end_pos": 172, "type": "DATASET", "confidence": 0.9098806579907736}]}]}