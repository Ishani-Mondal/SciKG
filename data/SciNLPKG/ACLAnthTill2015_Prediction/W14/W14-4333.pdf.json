{"title": [{"text": "SAWDUST: a Semi-Automated Wizard Dialogue Utterance Selection Tool for domain-independent large-domain dialogue", "labels": [], "entities": [{"text": "SAWDUST", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.7179538607597351}, {"text": "Semi-Automated Wizard Dialogue Utterance Selection", "start_pos": 11, "end_pos": 61, "type": "TASK", "confidence": 0.6201940715312958}, {"text": "domain-independent large-domain dialogue", "start_pos": 71, "end_pos": 111, "type": "TASK", "confidence": 0.6824588179588318}]}], "abstractContent": [{"text": "We present a tool that allows human wizards to select appropriate response utterances fora given dialogue context from a set of utterances observed in a dialogue corpus.", "labels": [], "entities": []}, {"text": "Such a tool can be used in Wizard-of-Oz studies and for collecting data which can be used for training and/or evaluating automatic dialogue models.", "labels": [], "entities": [{"text": "evaluating automatic dialogue models", "start_pos": 110, "end_pos": 146, "type": "TASK", "confidence": 0.6447098553180695}]}, {"text": "We also propose to incorporate such automatic dialogue models back into the tool as an aid in selecting utterances from a large dialogue corpus.", "labels": [], "entities": []}, {"text": "The tool allows a user to rank candidate utterances for selection according to these automatic models.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "We evaluated the tool by having four human volunteers (wizards) use it in order to establish an upper baseline for human-level performance in the static context evaluation task described in.", "labels": [], "entities": []}, {"text": "Wizards were instructed in how to use the search and relevance feedback features.", "labels": [], "entities": []}, {"text": "In order to not bias the wizards, they were not told exactly what score1 and score2 indicate, but just that the scores can be useful in search.", "labels": [], "entities": []}, {"text": "Each wizard is presented with a set of utterances (U train ) (|U train | \u2248 500) and is asked to select a subset from these that will be appropriate as a response for the presented dialogue context.", "labels": [], "entities": []}, {"text": "Each wizard was requested to select somewhere between 5 to 10 (at-least one) appropriate responses for each dialogue context extracted from five different human-human dialogues.", "labels": [], "entities": []}, {"text": "There area total of 89 dialogue contexts for the role that the wizards were to play.", "labels": [], "entities": []}, {"text": "shows the histogram for the number of utterances selected as appropriate responses by the four wizards.", "labels": [], "entities": []}, {"text": "As expected, wizards frequently chose multiple utterances as appropriate responses (mean = 7.80, min = 1, max = 25).", "labels": [], "entities": [{"text": "mean", "start_pos": 84, "end_pos": 88, "type": "METRIC", "confidence": 0.9799814820289612}]}, {"text": "To get an idea about how much the wizards agree among themselves for this task, we calculated the overlap between the utterances selected by a specific wizard and the utterances selected by another wizard or a set of wizards.", "labels": [], "entities": []}, {"text": "Let UT c be a set of utterances selected by a wizard T fora dialogue context c.", "labels": [], "entities": []}, {"text": "Let R be a set of wizards (T / \u2208 R) and U R c be the union of sets of utterances selected by the set of wizards (R) for the same context c.", "labels": [], "entities": []}, {"text": "Then we define the following overlap measures, We compute the average values of these overlap measures for all contexts and for all possible settings of test wizards and reference wizards.: Inter-wizard agreement Precision can be interpreted as the probability that a response utterance selected by a wizard is also considered appropriate by at least one other wizard.", "labels": [], "entities": [{"text": "Precision", "start_pos": 213, "end_pos": 222, "type": "METRIC", "confidence": 0.8805783987045288}]}, {"text": "Precision rapidly increases along with the number of reference wizards used.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.991043210029602}]}, {"text": "This happens because the size of the set U R c steadily increases with more reference wizards.", "labels": [], "entities": []}, {"text": "shows this observed increase and the expected increase if there were no overlap between the wizards.", "labels": [], "entities": []}, {"text": "The near-linear increase in |U R c | suggests that selecting appropriate responses is a hard task and may require a lot more than four wizards to achieve convergence.", "labels": [], "entities": []}, {"text": "Subjectively, the wizards reported no major usability problems with the tool, and were able to use all four utterance ordering techniques to find appropriate utterances.", "labels": [], "entities": [{"text": "utterance ordering", "start_pos": 108, "end_pos": 126, "type": "TASK", "confidence": 0.7039103657007217}]}], "tableCaptions": []}