{"title": [{"text": "Text simplification using synchronous dependency grammars: Generalising automatically harvested rules", "labels": [], "entities": []}], "abstractContent": [{"text": "We present an approach to text simplification based on synchronous dependency grammars.", "labels": [], "entities": [{"text": "text simplification", "start_pos": 26, "end_pos": 45, "type": "TASK", "confidence": 0.7988127768039703}]}, {"text": "Our main contributions in this work are (a) a study of how automatically derived lexical simplification rules can be generalised to enable their application in new contexts without introducing errors, and (b) an evaluation of our hybrid system that combines a large set of automatically acquired rules with a small set of hand-crafted rules for common syntactic simplification.", "labels": [], "entities": []}, {"text": "Our evaluation shows significant improvements over the state of the art, with scores comparable to human simplifications .", "labels": [], "entities": []}], "introductionContent": [{"text": "Text simplification is the process of reducing the linguistic complexity of a text, while still retaining the original information content and meaning.", "labels": [], "entities": [{"text": "Text simplification", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8108116388320923}]}, {"text": "Text Simplification is often thought of as consisting of two components -syntactic simplification and lexical simplification.", "labels": [], "entities": [{"text": "Text Simplification", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.6918191760778427}]}, {"text": "While syntactic simplification aims at reducing the grammatical complexity of a sentence, lexical simplification focuses on replacing difficult words or short phrases by simpler variants.", "labels": [], "entities": [{"text": "syntactic simplification", "start_pos": 6, "end_pos": 30, "type": "TASK", "confidence": 0.7714750468730927}]}, {"text": "Traditionally, entirely different approaches have been used for lexical) and syntactic simplification.", "labels": [], "entities": [{"text": "syntactic simplification", "start_pos": 77, "end_pos": 101, "type": "TASK", "confidence": 0.7373320460319519}]}, {"text": "Recent years have seen the application of machine translation inspired approaches to text simplification.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 42, "end_pos": 61, "type": "TASK", "confidence": 0.7226996272802353}, {"text": "text simplification", "start_pos": 85, "end_pos": 104, "type": "TASK", "confidence": 0.8432304561138153}]}, {"text": "These approaches learn from aligned English and Simplified English sentences extracted from the Simple English Wikipedia (SEW) corpus (simple.wikipedia.org).", "labels": [], "entities": [{"text": "Simple English Wikipedia (SEW) corpus", "start_pos": 96, "end_pos": 133, "type": "DATASET", "confidence": 0.7774310324873243}]}, {"text": "However, even these approaches () struggle to elegantly model the range of lexical and syntactic simplification operations observed in the monolingual simplification task within one framework, often differentiating between operation at leaf nodes of parse trees (lexical) and internal tree nodes (syntactic).", "labels": [], "entities": []}, {"text": "The key issue is the modelling of context for application of lexical rules.", "labels": [], "entities": []}, {"text": "While syntactic rules (for splitting conjoined clauses, or disembedding relative clauses) are typically not context dependent, words are typically polysemous and can only be replaced by others inappropriate contexts.", "labels": [], "entities": []}, {"text": "Our main contribution in this paper is to present a unified framework for representing rules for syntactic and lexical simplification (including paraphrase involving multiple words), and study for the first time how the definition of context affects system performance.", "labels": [], "entities": [{"text": "syntactic and lexical simplification", "start_pos": 97, "end_pos": 133, "type": "TASK", "confidence": 0.700235053896904}]}, {"text": "A second contribution is to provide a substantial human evaluation (63 sentences and 70 participants) to evaluate contemporary text simplification systems against manually simplified output.", "labels": [], "entities": []}], "datasetContent": [{"text": "We performed a manual evaluation of how fluent and simple the text produced by our simplification system is, and the extent to which it preserves meaning.", "labels": [], "entities": []}, {"text": "Our system (henceforth, HYBRID) is compared to QTSG, the system by Woodsend and Lapata (2011) that learns a quasi-synchronous tree substitution grammar.", "labels": [], "entities": []}, {"text": "This is the best performing system in the literature with a similar scope to ours in terms of the syntactic and lexical operations performed 2 . Further the two systems are trained on the same data.", "labels": [], "entities": []}, {"text": "QTSG relies entirely on an automatically acquired grammar of 1431 rules.", "labels": [], "entities": [{"text": "QTSG", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.939883828163147}]}, {"text": "Our automatically extracted grammar has 5466 lexicalised rules to augment the existing manually written syntactic rules in RegenT.", "labels": [], "entities": [{"text": "RegenT", "start_pos": 123, "end_pos": 129, "type": "DATASET", "confidence": 0.9889230728149414}]}, {"text": "We also compare the two systems to the manual gold standard SEW, and against the original EW sentences.", "labels": [], "entities": [{"text": "SEW", "start_pos": 60, "end_pos": 63, "type": "METRIC", "confidence": 0.4734247326850891}, {"text": "EW sentences", "start_pos": 90, "end_pos": 102, "type": "DATASET", "confidence": 0.8538010120391846}]}, {"text": "Data: We use the evaluation set previously used by several others.", "labels": [], "entities": []}, {"text": "This consists of 100 sentences from English Wikipedia (EW), aligned with Simple English Wikipedia (SEW) sentences.", "labels": [], "entities": [{"text": "Simple English Wikipedia (SEW) sentences", "start_pos": 73, "end_pos": 113, "type": "DATASET", "confidence": 0.8149357395512717}]}, {"text": "These 100 sentences have been excluded from our training data for rule acquisition, as is standard.", "labels": [], "entities": [{"text": "rule acquisition", "start_pos": 66, "end_pos": 82, "type": "TASK", "confidence": 0.9604286551475525}]}, {"text": "Following the protocol of, we used all the sentences from the evaluation set for which both QTSG and  HYBRID had performed at least one simplification (as selecting sentences where no simplification is performed by one system is likely to boost its fluency and meaning preservation ratings).", "labels": [], "entities": [{"text": "QTSG", "start_pos": 92, "end_pos": 96, "type": "DATASET", "confidence": 0.9122703671455383}, {"text": "HYBRID", "start_pos": 102, "end_pos": 108, "type": "DATASET", "confidence": 0.8908985257148743}]}, {"text": "This gave us a test set of 62 sentences from the original 100.", "labels": [], "entities": []}, {"text": "Method: We recruited participants on Amazon Mechanical Turk, filtered to live in the US and have an approval rating of 80%.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 37, "end_pos": 59, "type": "DATASET", "confidence": 0.9258876045544943}]}, {"text": "These participants were shown examples containing the original Wikipedia sentence, followed by QTSG, HY-BRID and SEW in a randomised manner.", "labels": [], "entities": [{"text": "QTSG", "start_pos": 95, "end_pos": 99, "type": "METRIC", "confidence": 0.559565544128418}, {"text": "HY-BRID", "start_pos": 101, "end_pos": 108, "type": "METRIC", "confidence": 0.7220669984817505}, {"text": "SEW", "start_pos": 113, "end_pos": 116, "type": "METRIC", "confidence": 0.8953580856323242}]}, {"text": "For each such set, they were asked to rate each simplified version for fluency, simplicity and the extent to which it preserved the meaning of the original.", "labels": [], "entities": [{"text": "simplicity", "start_pos": 80, "end_pos": 90, "type": "METRIC", "confidence": 0.9947863817214966}]}, {"text": "Additionally, participants were also asked to rate the fluency and simplicity of the original EW sentence.", "labels": [], "entities": [{"text": "simplicity", "start_pos": 67, "end_pos": 77, "type": "METRIC", "confidence": 0.9986163377761841}]}, {"text": "We used a Likert scale of 1-5, where 1 is totally unusable output, and 5 is output that is perfectly usable.", "labels": [], "entities": []}, {"text": "The experiment resulted in obtaining a total of 3669 ratings for 62 sentences involving 76 raters.", "labels": [], "entities": []}, {"text": "To test the correctness of the rule applications with  different rule sets, we performed a human evaluation to gauge how fluent and simple the simplified sentences were, and to what extent they preserved the meaning of the original.", "labels": [], "entities": []}, {"text": "We compared three versions in this experiment: the original ruleset, the context expanded using Sim Lin >= 0.1 (40% increase in rule applications) and with no lexicalised context (315% increase in rule applications).", "labels": [], "entities": []}, {"text": "The goal is to identify a level of generalisation that increases rule application in new contexts without introducing more errors.", "labels": [], "entities": []}, {"text": "We used the first 11,000 sentences from the dataset of, the same dataset used for rule acquisition.", "labels": [], "entities": [{"text": "rule acquisition", "start_pos": 82, "end_pos": 98, "type": "TASK", "confidence": 0.8906749784946442}]}, {"text": "We extracted at random 30 sentences where a simplification had been performed using the original ruleset.", "labels": [], "entities": []}, {"text": "This gives an upper bound on the performance of the original Wikipedia-context ruleset, as these are all sentences from which the rules have been derived.", "labels": [], "entities": []}, {"text": "We then selected a further 30 sentences where a simplification had been performed using the WordNet-context (Lin=0.1), but not with the original ruleset.", "labels": [], "entities": [{"text": "WordNet-context", "start_pos": 92, "end_pos": 107, "type": "DATASET", "confidence": 0.9608151316642761}]}, {"text": "These are new applications of the generalised ruleset on sentences that it hasn't directly learnt rules from.", "labels": [], "entities": []}, {"text": "Similarly, we selected a further 30 sentences where a simplification had been performed using the no-context ruleset, but not the Wikipedia-context or WordNet-context rulesets.", "labels": [], "entities": [{"text": "WordNet-context rulesets", "start_pos": 151, "end_pos": 175, "type": "DATASET", "confidence": 0.7960931062698364}]}, {"text": "Thus each set of 30 sentences contains new applications of the ruleset, as the lexical context is expanded, or abandoned completely.", "labels": [], "entities": []}, {"text": "This process gave us a total of 90 sentences to evaluate.", "labels": [], "entities": []}, {"text": "We recruited participants through Amazon Mechanical Turk.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 34, "end_pos": 56, "type": "DATASET", "confidence": 0.9519337217013041}]}, {"text": "Participants were filtered to be in the US and have an approval rating of 80%.", "labels": [], "entities": [{"text": "approval rating", "start_pos": 55, "end_pos": 70, "type": "METRIC", "confidence": 0.9538348913192749}]}, {"text": "These raters were shown 30 examples, each containing an original Wikipedia sentence followed by one of the simplified versions (WI, WN or NC).", "labels": [], "entities": []}, {"text": "Order of presentation was random.", "labels": [], "entities": []}, {"text": "For each such pair, raters were asked to rate each simplified version for fluency, simplicity and the extent to which it preserved the meaning of the original.", "labels": [], "entities": [{"text": "simplicity", "start_pos": 83, "end_pos": 93, "type": "METRIC", "confidence": 0.9957289099693298}]}, {"text": "The experiment provided 917 ratings for 90 sentences involving 28 raters.", "labels": [], "entities": []}, {"text": "We used a Likert scale of 1-5, where 1 is totally unusable output, and 5 is the output that is perfectly usable.", "labels": [], "entities": []}, {"text": "The mean values and the standard deviation for fluency, simplicity and meaning preservation for sentences simplified using WordNet (Lin=0.1), Wiki and no context is shown in Tab.", "labels": [], "entities": [{"text": "simplicity", "start_pos": 56, "end_pos": 66, "type": "METRIC", "confidence": 0.9972260594367981}, {"text": "meaning preservation", "start_pos": 71, "end_pos": 91, "type": "TASK", "confidence": 0.6877577304840088}, {"text": "WordNet", "start_pos": 123, "end_pos": 130, "type": "DATASET", "confidence": 0.9109987020492554}, {"text": "Lin", "start_pos": 132, "end_pos": 135, "type": "METRIC", "confidence": 0.9338375926017761}, {"text": "Tab", "start_pos": 174, "end_pos": 177, "type": "DATASET", "confidence": 0.9259001612663269}]}, {"text": "4. As seen, the difference between the mean values for all three criteria of fluency, simplicity and meaning preservation between WordNet and Wiki version is very small as compared to simplified sentences with no-context rules.", "labels": [], "entities": [{"text": "simplicity", "start_pos": 86, "end_pos": 96, "type": "METRIC", "confidence": 0.9972317814826965}, {"text": "meaning preservation", "start_pos": 101, "end_pos": 121, "type": "TASK", "confidence": 0.6704948991537094}, {"text": "WordNet", "start_pos": 130, "end_pos": 137, "type": "DATASET", "confidence": 0.9310936331748962}]}, {"text": "An analysis of variance (ANOVA) test was conducted to measure the effect of fluency, simplicity and meaning preservation for versions of simplified text.", "labels": [], "entities": [{"text": "analysis of variance (ANOVA)", "start_pos": 3, "end_pos": 31, "type": "METRIC", "confidence": 0.7867820064226786}, {"text": "simplicity", "start_pos": 85, "end_pos": 95, "type": "METRIC", "confidence": 0.99631667137146}]}, {"text": "Fluency: A one-way ANOVA conducted to evaluate fluency for versions of simplified text showed a highly significant effect of version (WN, WC, and NC) on the fluency score (F=51.54, p=2x10 -16 ).", "labels": [], "entities": [{"text": "Fluency", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.9563764929771423}, {"text": "F", "start_pos": 172, "end_pos": 173, "type": "METRIC", "confidence": 0.9912946820259094}]}, {"text": "A Tukey's pairwise comparison test (Tukey's HSD, overall alpha level = 0.05) indicated significant difference between WI and NC and between WN and NC at p = 0.01.", "labels": [], "entities": [{"text": "HSD", "start_pos": 44, "end_pos": 47, "type": "METRIC", "confidence": 0.516801655292511}]}, {"text": "However, the difference between WN and WI was not significant at p = 0.01.", "labels": [], "entities": [{"text": "WN", "start_pos": 32, "end_pos": 34, "type": "DATASET", "confidence": 0.8037377595901489}, {"text": "WI", "start_pos": 39, "end_pos": 41, "type": "DATASET", "confidence": 0.7766316533088684}]}, {"text": "Simplicity: The ANOVA conducted to evaluate simplicity for different versions also showed a significant effect of version on the simplicity score (F=76.7, p=2x10 -16 ).", "labels": [], "entities": [{"text": "Simplicity", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9065066576004028}, {"text": "ANOVA", "start_pos": 16, "end_pos": 21, "type": "METRIC", "confidence": 0.5613822340965271}, {"text": "simplicity", "start_pos": 44, "end_pos": 54, "type": "METRIC", "confidence": 0.9729644060134888}, {"text": "simplicity score", "start_pos": 129, "end_pos": 145, "type": "METRIC", "confidence": 0.9827573895454407}, {"text": "F", "start_pos": 147, "end_pos": 148, "type": "METRIC", "confidence": 0.9811773300170898}]}, {"text": "A Tukey's pairwise comparison test (Tukey's HSD, overall alpha level = 0.05) indicated significant difference between WN and NC and WI and NC (p < 0.01).", "labels": [], "entities": []}, {"text": "However, the difference between WN and WI was not significant at p = 0.01.", "labels": [], "entities": [{"text": "WN", "start_pos": 32, "end_pos": 34, "type": "DATASET", "confidence": 0.8037377595901489}, {"text": "WI", "start_pos": 39, "end_pos": 41, "type": "DATASET", "confidence": 0.7766316533088684}]}, {"text": "Meaning Preservation: The ANOVA conducted to evaluate meaning preservation for versions of simplified text also showed a highly significant effect of version on the meaning preservation score (F=17.22, p=4.55x10 -08 ).", "labels": [], "entities": [{"text": "Meaning Preservation", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.842059850692749}, {"text": "meaning preservation", "start_pos": 54, "end_pos": 74, "type": "TASK", "confidence": 0.813923180103302}, {"text": "meaning preservation", "start_pos": 165, "end_pos": 185, "type": "TASK", "confidence": 0.6801173388957977}, {"text": "F", "start_pos": 193, "end_pos": 194, "type": "METRIC", "confidence": 0.9964687824249268}]}, {"text": "A Tukey's pairwise comparison test (Tukey's HSD, overall alpha level = 0.05) indicated significant difference between WN and NC and WI and NC (p < 0.01).", "labels": [], "entities": []}, {"text": "However, the difference between WN and WI was not significant at p = 0.01.", "labels": [], "entities": [{"text": "WN", "start_pos": 32, "end_pos": 34, "type": "DATASET", "confidence": 0.8037377595901489}, {"text": "WI", "start_pos": 39, "end_pos": 41, "type": "DATASET", "confidence": 0.7766316533088684}]}, {"text": "This study suggests that there is no significant effect on accuracy of expanding the lexical context using WordNet (Lin=0.1), even though this results in an increase in rule application of 40%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.9993090629577637}, {"text": "WordNet", "start_pos": 107, "end_pos": 114, "type": "DATASET", "confidence": 0.9367587566375732}, {"text": "Lin", "start_pos": 116, "end_pos": 119, "type": "METRIC", "confidence": 0.969241738319397}, {"text": "application", "start_pos": 174, "end_pos": 185, "type": "METRIC", "confidence": 0.947043240070343}]}, {"text": "The study also confirms that there is a sharp and", "labels": [], "entities": [{"text": "sharp", "start_pos": 40, "end_pos": 45, "type": "METRIC", "confidence": 0.995063841342926}]}], "tableCaptions": [{"text": " Table 2: Details of rules derived with different  length in DELETE and INSERT relations", "labels": [], "entities": [{"text": "length", "start_pos": 51, "end_pos": 57, "type": "METRIC", "confidence": 0.9601706862449646}, {"text": "DELETE", "start_pos": 61, "end_pos": 67, "type": "METRIC", "confidence": 0.7537869811058044}, {"text": "INSERT", "start_pos": 72, "end_pos": 78, "type": "METRIC", "confidence": 0.5849127173423767}]}, {"text": " Table 3: Application of different versions of rules  on test dataset (% change is the increase in the  application of rules between Wiki-context and the  corresponding version)", "labels": [], "entities": []}, {"text": " Table 4: Results of human evaluation of different versions of simplified text (WN: WordNet-context  (Lin=0.1); WI: Wikipedia-context; NC: No-context)", "labels": [], "entities": [{"text": "WordNet-context", "start_pos": 84, "end_pos": 99, "type": "DATASET", "confidence": 0.4869963228702545}, {"text": "Lin", "start_pos": 102, "end_pos": 105, "type": "METRIC", "confidence": 0.8931390047073364}]}, {"text": " Table 5: Results of human evaluation of different simplified texts (EW: English Wikipedia; SEW: Simple  English Wikipedia; QTSG: Woodsend and Lapata (2011) system; HYB: Our hybrid system)", "labels": [], "entities": [{"text": "EW: English Wikipedia; SEW: Simple  English Wikipedia; QTSG: Woodsend and Lapata (2011) system", "start_pos": 69, "end_pos": 163, "type": "DATASET", "confidence": 0.7383394047617913}]}]}