{"title": [{"text": "Corpus annotation methodology for citation classification in scientific literature", "labels": [], "entities": [{"text": "citation classification", "start_pos": 34, "end_pos": 57, "type": "TASK", "confidence": 0.8475195169448853}]}], "abstractContent": [{"text": "Since, at the moment, there is not a gold-standard annotated corpus to allow generation and testing of automatic systems for classifying the purpose or function of a citation referenced in an article; it is necessary to build one, for this objective.", "labels": [], "entities": [{"text": "classifying the purpose or function of a citation referenced in an article", "start_pos": 125, "end_pos": 199, "type": "TASK", "confidence": 0.6519100964069366}]}, {"text": "The development of this kind of corpus is subject to two conditions: the first one is to present a clear and unambiguous classification scheme.", "labels": [], "entities": []}, {"text": "The second one is to secure an initial manual process of labeling to reach a sufficient inter-coder agreement among annotators to validate the annotation scheme and to be able to reproduce it even with coders who do not know in depth the topic of the analyzed articles.", "labels": [], "entities": []}, {"text": "This paper proposes and validates a methodology for corpus annotation for citation classification in scientific literature that facilitate annotation and produces substantial inter-annotator agreement.", "labels": [], "entities": [{"text": "citation classification", "start_pos": 74, "end_pos": 97, "type": "TASK", "confidence": 0.8211230337619781}]}], "introductionContent": [{"text": "Not all citations have the same effect in a citing article.", "labels": [], "entities": []}, {"text": "The impact of a cited paper may vary considerably.", "labels": [], "entities": []}, {"text": "It could go from being a criticism, or a starting point fora job or simply an acknowledge of the work of other authors.", "labels": [], "entities": []}, {"text": "However, accepted methods available today are variations of citation counting where all citations are considered equal and are evaluated with the same weight.", "labels": [], "entities": [{"text": "citation counting", "start_pos": 60, "end_pos": 77, "type": "TASK", "confidence": 0.9312973916530609}]}, {"text": "Current methods of measuring impact fall into one of three techniques: simple count of citations (more citations, more impact); co citation which adds as a measure of similarity between two works the number of common documents that cited them; and the Google's PageRank that measure citation relevance using the relevance and frequency of the citing document.", "labels": [], "entities": []}, {"text": "Not all citations are equal, so they should not weigh equally in the impact calculation.", "labels": [], "entities": []}, {"text": "None of the above mentioned counting methods takes into account whether the citation context is positive or negative, the purpose of the citing article, or if the citation has or not have influence on it.", "labels": [], "entities": []}, {"text": "It becomes important to identify more complete metrics that take into account the content about cited work to assess its impact and relevance.", "labels": [], "entities": []}, {"text": "It is necessary the construction of anew impact index enriched with qualitative criteria regarding the citation.", "labels": [], "entities": []}, {"text": "This process requires a content analysis of the context containing citations to obtain certain important features such as intent or purpose of the citing author when made the reference.", "labels": [], "entities": []}, {"text": "Content analysis is a group of procedures to recollect and organized information in standard format to make inferences about its characteristics and meaning using manual or automatic methods.", "labels": [], "entities": [{"text": "Content analysis", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.771352618932724}]}, {"text": "This analysis could be automatic starting from a tagged corpus to build a model.", "labels": [], "entities": []}, {"text": "Since, there is not a gold-standard annotated corpus for citation analysis data, it is necessary to work in the generation of one in order to facilitate collaborative work and results comparison among researches.", "labels": [], "entities": []}, {"text": "Development of a corpus starts from the definition of a citation classification scheme that considers function (purpose), polarity (disposition) and influence of cited paper to produce a reliable and reproducible data set that could be the basis for future work in this area.", "labels": [], "entities": []}, {"text": "This tagged corpus will allow overcoming problems currently present that make very difficult to strengthen collaborative efforts in this field (Hern\u00e1ndez y).", "labels": [], "entities": []}, {"text": "Present problems are, for instance, the lack of a standard classification scheme and of sufficient public data available such that researchers could test their systems and compare results.", "labels": [], "entities": []}, {"text": "According to, a corpus is reliable if annotators agree in the assigned categories because it displays a similar understanding of the classification scheme.", "labels": [], "entities": []}, {"text": "This criterion is a prerequisite to demonstrate validity of a scheme.", "labels": [], "entities": []}, {"text": "If there is no consistency among the obtained results, the representation maybe inappropriate for the data.", "labels": [], "entities": []}, {"text": "In our experiment, we pose a scheme to classify citation functions and we defined an annotation methodology to allow a greater accuracy in the process to facilitate decisionmaking and generate a greater inter-coder agreement.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 127, "end_pos": 135, "type": "METRIC", "confidence": 0.9941118359565735}]}, {"text": "The subject of this article focus in the proposed annotation methodology which could be applied to any scheme with the only condition that the scheme is not ambiguous i.e. its categories are clearly differentiated.", "labels": [], "entities": []}], "datasetContent": [{"text": "The annotation process comply three requirements in order to achieve reliability and reproducibility.", "labels": [], "entities": [{"text": "reliability", "start_pos": 69, "end_pos": 80, "type": "METRIC", "confidence": 0.9827883839607239}]}, {"text": "The annotators had a profile that allow them a good understanding of the scientific texts in computational linguistics; they worked in an independent way and they had a clear function classification scheme with detail instructions.", "labels": [], "entities": []}, {"text": "To test annotation reliability, we measured inter-annotator agreement in a small section of the corpus; the same people must review this sample.", "labels": [], "entities": []}, {"text": "It is necessary to achieve a good rate in this agreement because it certifies that the process is reliable and reproducible and that results maybe generalized to the complete process in which probably are going to work new annotators and not only the ones that coded the trial (Artstein y.", "labels": [], "entities": []}, {"text": "We analyzed 101 citations to classify them according to their function without preannotation and 101 different citations with pre-annotation.", "labels": [], "entities": []}, {"text": "We measured inter-annotator agreement in each case.", "labels": [], "entities": []}], "tableCaptions": []}