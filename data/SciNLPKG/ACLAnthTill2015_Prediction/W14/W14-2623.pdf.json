{"title": [{"text": "A cognitive study of subjectivity extraction in sentiment annotation", "labels": [], "entities": [{"text": "subjectivity extraction", "start_pos": 21, "end_pos": 44, "type": "TASK", "confidence": 0.7207998186349869}, {"text": "sentiment annotation", "start_pos": 48, "end_pos": 68, "type": "TASK", "confidence": 0.8178500533103943}]}], "abstractContent": [{"text": "Existing sentiment analysers are weak AI systems: they try to capture the function-ality of human sentiment detection faculty, without worrying about how such faculty is realized in the hardware of the human.", "labels": [], "entities": [{"text": "human sentiment detection faculty", "start_pos": 92, "end_pos": 125, "type": "TASK", "confidence": 0.7791061252355576}]}, {"text": "These analysers are agnostic of the actual cognitive processes involved.", "labels": [], "entities": []}, {"text": "This, however , does not deliver when applications demand order of magnitude facelift inaccuracy , as well as insight into characteristics of sentiment detection process.", "labels": [], "entities": [{"text": "sentiment detection", "start_pos": 142, "end_pos": 161, "type": "TASK", "confidence": 0.9477699398994446}]}, {"text": "In this paper, we present a cognitive study of sentiment detection from the perspective of strong AI.", "labels": [], "entities": [{"text": "sentiment detection", "start_pos": 47, "end_pos": 66, "type": "TASK", "confidence": 0.9536196887493134}]}, {"text": "We study the sentiment detection process of a set of human \"sen-timent readers\".", "labels": [], "entities": [{"text": "sentiment detection", "start_pos": 13, "end_pos": 32, "type": "TASK", "confidence": 0.9492127895355225}]}, {"text": "Using eye-tracking, we show that on the way to sentiment detection , humans first extract subjectivity.", "labels": [], "entities": [{"text": "sentiment detection", "start_pos": 47, "end_pos": 66, "type": "TASK", "confidence": 0.9614592492580414}]}, {"text": "They focus attention on a subset of sentences before arriving at the overall sentiment.", "labels": [], "entities": []}, {"text": "This they do either through \"antici-pation\" where sentences are skipped during the first pass of reading, or through \"homing\" where a subset of the sentences are read over multiple passes, or through both.", "labels": [], "entities": []}, {"text": "\"Homing\" behaviour is also observed at the sub-sentence level in complex sentiment phenomena like sarcasm.", "labels": [], "entities": [{"text": "Homing\"", "start_pos": 1, "end_pos": 8, "type": "TASK", "confidence": 0.9587059915065765}]}], "introductionContent": [{"text": "Over the years, supervised approaches using polarity-annotated datasets have shown promise for SA.", "labels": [], "entities": [{"text": "SA", "start_pos": 95, "end_pos": 97, "type": "TASK", "confidence": 0.9920879006385803}]}, {"text": "However, an alternate line of thought has co-existed.", "labels": [], "entities": []}, {"text": "showed that for SA, instead of a document in its entirety, an extract of the subjective sentences alone can be used.", "labels": [], "entities": [{"text": "SA", "start_pos": 16, "end_pos": 18, "type": "TASK", "confidence": 0.9729201197624207}]}, {"text": "This process of generating a subjective extract is referred to as subjectivity extraction.", "labels": [], "entities": [{"text": "subjectivity extraction", "start_pos": 66, "end_pos": 89, "type": "TASK", "confidence": 0.7298601567745209}]}, {"text": "show that for sentiment prediction of movie reviews, subjectivity extraction maybe used to discard the sentences describing movie plots since they do not contribute towards the speaker's view of the movie.", "labels": [], "entities": [{"text": "sentiment prediction of movie reviews", "start_pos": 14, "end_pos": 51, "type": "TASK", "confidence": 0.9278168678283691}, {"text": "subjectivity extraction", "start_pos": 53, "end_pos": 76, "type": "TASK", "confidence": 0.7123676687479019}]}, {"text": "While subjectivity extraction helps sentiment classification, the reason has not been sufficiently examined from the perspective of strong AI.", "labels": [], "entities": [{"text": "subjectivity extraction", "start_pos": 6, "end_pos": 29, "type": "TASK", "confidence": 0.7268245667219162}, {"text": "sentiment classification", "start_pos": 36, "end_pos": 60, "type": "TASK", "confidence": 0.9720605909824371}]}, {"text": "The classical definition of strong AI suggests that a machine must be perform sentiment analysis in a manner and accuracy similar to human beings.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 78, "end_pos": 96, "type": "TASK", "confidence": 0.9076767265796661}, {"text": "accuracy", "start_pos": 113, "end_pos": 121, "type": "METRIC", "confidence": 0.997473418712616}]}, {"text": "Our paper takes a step in this direction.", "labels": [], "entities": []}, {"text": "We study the cognitive processes underlying sentiment annotation using eye-fixation data of the participants.", "labels": [], "entities": [{"text": "sentiment annotation", "start_pos": 44, "end_pos": 64, "type": "TASK", "confidence": 0.9692670404911041}]}, {"text": "Our work is novel in two ways: \u2022 We view documents as a set of sentences through which sentiment changes.", "labels": [], "entities": []}, {"text": "We show that the nature of these polarity oscillations leads to changes in the reading behavior.", "labels": [], "entities": []}, {"text": "\u2022 To the best of our knowledge, the idea of using eye-tracking to validate assumptions is novel in case of sentiment analysis and many NLP applications.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 107, "end_pos": 125, "type": "TASK", "confidence": 0.970416396856308}]}], "datasetContent": [{"text": "This section describes the framework used for our eye-tracking experiment.", "labels": [], "entities": []}, {"text": "A participant is given the task of annotating documents with one out of the following labels: positive, negative and objective.", "labels": [], "entities": []}, {"text": "While she reads the document, her eyefixations are recorded.", "labels": [], "entities": []}, {"text": "To log eye-fixation data, we use Tobii T120 remote eye-tracker with Translog.", "labels": [], "entities": [{"text": "Tobii T120", "start_pos": 33, "end_pos": 43, "type": "DATASET", "confidence": 0.9118786156177521}, {"text": "Translog", "start_pos": 68, "end_pos": 76, "type": "DATASET", "confidence": 0.9098561406135559}]}, {"text": "Translog is a freeware for recording eye movements and keystrokes during translation.", "labels": [], "entities": []}, {"text": "We configure Translog for reading with the goal of sentiment.", "labels": [], "entities": []}, {"text": "We obtain two kinds of annotation from our annotators: (a) sentiment (positive, negative and objective), (b) eye-movement as recorded by an eyetracker.", "labels": [], "entities": []}, {"text": "They are given a set of instructions beforehand and can seek clarifications.", "labels": [], "entities": []}, {"text": "This experiment is conducted as follows: 1.", "labels": [], "entities": []}, {"text": "A complete document is displayed on the screen.", "labels": [], "entities": []}, {"text": "The font size and line separation are set to 17pt and 1.5 cm respectively to ensure clear visibility and minimize recording error.", "labels": [], "entities": []}, {"text": "2. The annotator verbally states the sentiment of this sentence, before (s)he can proceed to the next.", "labels": [], "entities": []}, {"text": "3. While the annotator is reading the sentence, a remote eye-tracker (Model: Tobii TX 300, Sampling rate: 300Hz) records the eyemovement data of the annotator.", "labels": [], "entities": [{"text": "Tobii TX 300", "start_pos": 77, "end_pos": 89, "type": "DATASET", "confidence": 0.9128050804138184}, {"text": "Sampling rate", "start_pos": 91, "end_pos": 104, "type": "METRIC", "confidence": 0.9666766226291656}]}, {"text": "The eyetracker is linked to Translog II software) in order to record the data.", "labels": [], "entities": []}, {"text": "A snapshot of the software is shown in.", "labels": [], "entities": []}, {"text": "The dots and circles represent position of eyes and fixations of the annotator respectively.", "labels": [], "entities": []}, {"text": "Each eye-fixation that is recorded consists of: coordinates, timestamp and duration.", "labels": [], "entities": [{"text": "duration", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.9986442923545837}]}, {"text": "These three parameters have been used to generate sentence progression graphs.", "labels": [], "entities": [{"text": "sentence progression graphs", "start_pos": 50, "end_pos": 77, "type": "TASK", "confidence": 0.7798225482304891}]}], "tableCaptions": [{"text": " Table 2: Number of unique and non-unique sentences read by each participant", "labels": [], "entities": []}]}