{"title": [{"text": "Mining human interactions to construct a virtual guide fora virtual fair", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper we describe how we mine interactions between a human guide and a human visitor to build a virtual guide.", "labels": [], "entities": []}, {"text": "A virtual guide is an agent capable of fulfilling the role of a human guide.", "labels": [], "entities": []}, {"text": "Its goal is to guide visitors to each booth of a virtual fair and to provide information about the company or organization through interactive objects located at the fair.", "labels": [], "entities": []}, {"text": "The guide decides what to say, using a graph search algorithm, and decides how to say using generation by selection based on contextual features.", "labels": [], "entities": []}, {"text": "The guide decides whereto speak at the virtual fair by creating clusters using a data classification algorithm to learn in what positions the human guide decided to talk.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "In the evaluation process 11 evaluators participated, completing the proposed visit to the virtual fair, each manipulating 9 interactive objects.", "labels": [], "entities": []}, {"text": "Evaluators were also asked to complete a questionnaire after the tour, in which we wanted to obtain several subjective metrics.", "labels": [], "entities": []}, {"text": "We were particularly interested in the questions \u2022 S1: I had difficulties identifying the objects that the system described for me \u2022 S2: The Utterances sounded robotic \u2022 S3: The system was repetitive where we previously supposed the virtual guide would have better results than other virtual instructors, if we consider the results showed in.", "labels": [], "entities": []}, {"text": "We compared our virtual guide results with the two best symbolic systems built for another virtual environment, the GIVE-2 Challenge.", "labels": [], "entities": []}, {"text": "Those systems were NA from INRIA and SAAR from University of Saarland (see ().", "labels": [], "entities": [{"text": "INRIA", "start_pos": 27, "end_pos": 32, "type": "DATASET", "confidence": 0.6196601986885071}, {"text": "SAAR", "start_pos": 37, "end_pos": 41, "type": "METRIC", "confidence": 0.9928723573684692}]}, {"text": "Furthermore, we checked if the virtual guide results were similar to another virtual instructor, also built for GIVE-2, called OUR, in which generation by selection was applied to make natural language generation possible.", "labels": [], "entities": [{"text": "OUR", "start_pos": 127, "end_pos": 130, "type": "METRIC", "confidence": 0.59880530834198}, {"text": "natural language generation", "start_pos": 185, "end_pos": 212, "type": "TASK", "confidence": 0.7572195927302042}]}, {"text": "In we show the results for each virtual instructor in the three categories we are interested.", "labels": [], "entities": []}, {"text": "We can see that the virtual guide obtained significantly better results than the SAAR and NA and in questions S1, S2 and S3, as we had supposed.", "labels": [], "entities": [{"text": "SAAR", "start_pos": 81, "end_pos": 85, "type": "DATASET", "confidence": 0.7946717143058777}]}, {"text": "All three questions range from 1 (one) to 9 (nine), the lower the number the better the system (since questions are negative).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results comparison between virtual guide  and three GIVE-2 systems  Question NA SAAR OUR VP  S1  4.1  4  3  1.81  S2  5.2  4.75  3.6  1.82  S3  6.55 6.3  5.4  2", "labels": [], "entities": [{"text": "NA SAAR OUR VP  S1  4.1", "start_pos": 87, "end_pos": 110, "type": "DATASET", "confidence": 0.6585305432478586}]}]}