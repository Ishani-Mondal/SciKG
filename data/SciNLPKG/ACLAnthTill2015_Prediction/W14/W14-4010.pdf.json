{"title": [{"text": "Ternary Segmentation for Improving Search in Top-down Induction of Segmental ITGs", "labels": [], "entities": [{"text": "Ternary Segmentation", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8109104335308075}, {"text": "Top-down Induction of Segmental ITGs", "start_pos": 45, "end_pos": 81, "type": "TASK", "confidence": 0.6940954804420472}]}], "abstractContent": [{"text": "We show that there are situations where iteratively segmenting sentence pairs top-down will fail to reach valid segments and propose a method for alleviating the problem.", "labels": [], "entities": []}, {"text": "Due to the enormity of the search space, error analysis has indicated that it is often impossible to get to a desired embedded segment purely through binary seg-mentation that divides existing segmental rules in half-the strategy typically employed by existing search strategies-as it requires two steps.", "labels": [], "entities": []}, {"text": "We propose anew method to hypothesize ternary segmenta-tions in a single step, making the embedded segments immediately discoverable.", "labels": [], "entities": []}], "introductionContent": [{"text": "One of the most important improvements to statistical machine translation to date was the move from token-based model to segmental models (also called phrasal).", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 42, "end_pos": 73, "type": "TASK", "confidence": 0.7080038289229075}]}, {"text": "This move accomplishes two things: it allows a flat surface-based model to memorize some relationships between word realizations, but more importantly, it allows the model to capture multi-word concepts or chunks.", "labels": [], "entities": []}, {"text": "These chunks are necessary in order to translate fixed expressions, or other multi-word units that do not have a compositional meaning.", "labels": [], "entities": []}, {"text": "If a sequence in one language can be broken down into smaller pieces which are then translated individually and reassembled in another language, the meaning of the sequence is compositional; if not, the only way to translate it accurately is to treat it as a single unit -a chunk.", "labels": [], "entities": []}, {"text": "Existing surface-based models) have high recall in capturing the chunks, but tend to over-generate, which leads to big models and low precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 41, "end_pos": 47, "type": "METRIC", "confidence": 0.9990962743759155}, {"text": "precision", "start_pos": 134, "end_pos": 143, "type": "METRIC", "confidence": 0.9966468214988708}]}, {"text": "Surface-based models have no concept of hierarchical composition, instead they make the assumption that a sentence consists of a sequence of segments that can be individually translated and reordered to form the translation.", "labels": [], "entities": []}, {"text": "This is counter-intuitive, as the who-did-what-towhoms of a sentence tends to be translated and reordered as units, rather than have their components mixed together.", "labels": [], "entities": []}, {"text": "Transduction grammars, also called hierarchical translation models or synchronous grammars, address this through a mechanism similar to context-free grammars.", "labels": [], "entities": []}, {"text": "Inducing a segmental transduction grammar is hard, so the standard practice is to use a similar method as the surfacebased models use to learn the chunks, which is problematic, since that method mostly relies on memorizing the relationships that the mechanics of a compositional model is designed to generalize.", "labels": [], "entities": []}, {"text": "A compositional translation model would be able to translate lexical chunks, as well as generalize different kinds of compositions; a segmental transduction grammar captures this by having segmental lexical rules and different nonterminal symbols for different categories of compositions.", "labels": [], "entities": [{"text": "compositional translation", "start_pos": 2, "end_pos": 27, "type": "TASK", "confidence": 0.7054404020309448}]}, {"text": "In this paper, we focus on inducing the former: segmental lexical rules in inversion transduction grammars (ITGs).", "labels": [], "entities": [{"text": "inversion transduction grammars (ITGs)", "start_pos": 75, "end_pos": 113, "type": "TASK", "confidence": 0.8014852702617645}]}, {"text": "One natural way would be to start with a tokenbased grammar and chunk adjacent tokens to form segments.", "labels": [], "entities": []}, {"text": "The main problem with chunking is that the data becomes more and more likely as the segments get larger, with the degenerate endpoint of all sentence pairs being memorized lexical items.", "labels": [], "entities": []}, {"text": "combat this tendency by introducing a sparsity prior over the rule probabilities, and variational Bayes to maximize the posterior probability of the data subject to this symmetric Dirichlet prior.", "labels": [], "entities": [{"text": "variational Bayes", "start_pos": 86, "end_pos": 103, "type": "METRIC", "confidence": 0.9432322680950165}]}, {"text": "To hypothesize possible chunks, they examine the Viterbi biparse of the existing model.", "labels": [], "entities": []}, {"text": "use the entire parse forest to generate the hypotheses.", "labels": [], "entities": []}, {"text": "They also bootstrap the ITG from linear and finite-state transduction grammars, and FSTGs), rather than initialize the lexical probabilities from IBM models.", "labels": [], "entities": []}, {"text": "Another way to arrive at a segmental ITG is to start with the degenerate chunking case: each sentence pair as a lexical item, and segment the existing lexical rules into shorter rules.", "labels": [], "entities": []}, {"text": "Since the start point is the degenerate case when optimizing for data likelihood, this approach requires a different objective function to optimize against.", "labels": [], "entities": []}, {"text": "proposes to use description length of the model and the data given the model, which is subsequently expressed in a Bayesian form with the addition of a prior over the rule probabilities ).", "labels": [], "entities": []}, {"text": "The way they generate hypotheses is restricted to segmenting an existing lexical item into two parts, which is problematic, because embedded lexical items are potentially overlooked.", "labels": [], "entities": []}, {"text": "There is also the option of implicitly defining all possible grammars, and sample from that distribution.", "labels": [], "entities": []}, {"text": "do exactly that; they induce with collapsed Gibbs sampling which keeps one derivation for each training sentence that is altered and then resampled.", "labels": [], "entities": []}, {"text": "The operations to change the derivations are split, join, delete and insert.", "labels": [], "entities": []}, {"text": "The split-operator corresponds to binary segmentation, the join-operator corresponds to chunking; the delete-operator removes an internal node, resulting in its parent having three children, and the insert-operator allows a parent with three children to be normalized to have only two.", "labels": [], "entities": []}, {"text": "The existence of ternary nodes in the derivation means that the learned grammar contains ternary rules.", "labels": [], "entities": []}, {"text": "Note that it still takes three operations: two split-operations and one delete-operation for their model to do what we propose to do in a single ternary segmentation.", "labels": [], "entities": []}, {"text": "Also, although we allow for single-step ternary segmentations, our grammar does not contain ternary rules; instead the results of a ternary segmentation is immediately normalized to the 2-normal form.", "labels": [], "entities": []}, {"text": "Although their model can theoretically sample from the entire model space, the split-operation alone is enough to do so; the other operations were added to get the model to do so in practice.", "labels": [], "entities": []}, {"text": "Similarly, we propose ternary segmentation to be able to reach areas of the model space that we failed to reach with binary segmentation.", "labels": [], "entities": [{"text": "ternary segmentation", "start_pos": 22, "end_pos": 42, "type": "TASK", "confidence": 0.8150067031383514}]}, {"text": "To illustrate the problem with embedded lexical items, we will introduce a small example corpus.", "labels": [], "entities": []}, {"text": "Although Swedish and English are relatively similar, with the structure of basic sentences being identical, they already illustrate the common problem of rare embedded correspondences.", "labels": [], "entities": []}, {"text": "Imagine a really simple corpus of three sentence pairs with identical structure: he has a red book / han har en r\u00f6d bok she has a biology book / hon har en biologibok it has begun / det har b\u00f6rjat The main difference is that Swedish concatenates rather than juxtaposes compounds such as biologibok instead of biology book.", "labels": [], "entities": []}, {"text": "A bilingual person looking at this corpus would produce bilingual parse trees like those in.", "labels": [], "entities": []}, {"text": "Inducing this relatively simple segmental ITG from the data is, however, quite a challenge.", "labels": [], "entities": []}, {"text": "The example above illustrates a problem with the chunking approach, as one of the most common chunks is has a/har en, whereas the linguistically motivated chunk biology book/biologibok occurs only once.", "labels": [], "entities": []}, {"text": "There is very little in this data that would lead the chunking approach towards the desired ITG.", "labels": [], "entities": []}, {"text": "It also illustrates a problem with the binary segmentation approach, as all the bilingual prefixes and suffixes, the biaffixes, are unique; there is noway of discovering that all the above sentences have the exact same verb.", "labels": [], "entities": [{"text": "binary segmentation", "start_pos": 39, "end_pos": 58, "type": "TASK", "confidence": 0.8017294108867645}]}, {"text": "In this paper, we propose a method to allow bilingual infixes to be hypothesized and used to drive the minimization of description length, which would be able to induce the desired ITG from the above corpus.", "labels": [], "entities": []}, {"text": "The paper is structured so that we start by giving a definition of the grammar formalism we use: ITGs (Section 2).", "labels": [], "entities": []}, {"text": "We then describe the notion of description length that we use (Section 3), and how ternary segmentation differs from and complements binary segmentation (Section 4).", "labels": [], "entities": []}, {"text": "We then present our induction algorithm (Section 5) and give an example of a run through (Section 6).", "labels": [], "entities": []}, {"text": "Finally we offer some concluding remarks (Section 7).", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}