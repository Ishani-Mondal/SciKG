{"title": [{"text": "Identifying Appropriate Support for Propositions in Online User Comments", "labels": [], "entities": [{"text": "Identifying Appropriate", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.7657063603401184}]}], "abstractContent": [{"text": "The ability to analyze the adequacy of supporting information is necessary for determining the strength of an argument.", "labels": [], "entities": []}, {"text": "1 This is especially the case for online user comments , which often consist of arguments lacking proper substantiation and reasoning.", "labels": [], "entities": []}, {"text": "Thus, we develop a framework for automatically classifying each proposition as UNVERIFIABLE, VERIFIABLE NON-EXPERIENTIAL, or VERIFIABLE EXPE-RIENTIAL 2 , where the appropriate type of support is reason, evidence, and optional evidence, respectively 3.", "labels": [], "entities": [{"text": "VERIFIABLE NON-EXPERIENTIAL", "start_pos": 93, "end_pos": 120, "type": "METRIC", "confidence": 0.7741950750350952}]}, {"text": "Once the existing support for propositions are identified , this classification can provide an estimate of how adequately the arguments have been supported.", "labels": [], "entities": []}, {"text": "We build a gold-standard dataset of 9,476 sentences and clauses from 1,047 comments submitted to an eRulemaking platform and find that Support Vector Machine (SVM) classifiers trained with n-grams and additional features capturing the verifiability and expe-rientiality exhibit statistically significant improvement over the unigram baseline, achieving a macro-averaged F 1 of 68.99%.", "labels": [], "entities": [{"text": "F 1", "start_pos": 370, "end_pos": 373, "type": "METRIC", "confidence": 0.9551315009593964}]}], "introductionContent": [{"text": "Argumentation mining is a relatively new field focusing on identifying and extracting argumentative structures in documents.", "labels": [], "entities": [{"text": "Argumentation mining", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8849760890007019}, {"text": "identifying and extracting argumentative structures in documents", "start_pos": 59, "end_pos": 123, "type": "TASK", "confidence": 0.6934547509465899}]}, {"text": "An argument is typically defined as a conclusion with supporting In this work, even unsupported propositions are consider part of an argument.", "labels": [], "entities": []}, {"text": "Not disregarding such implicit arguments allows us to discuss the types of support that can further be provided to strengthen the argument, as a form of assessment.", "labels": [], "entities": []}, {"text": "Verifiable Experiential propositions are verifiable propositions about personal state or experience.", "labels": [], "entities": []}, {"text": "We are assuming that there is no background knowledge that eliminates the need of support.", "labels": [], "entities": []}, {"text": "premises, which can be conclusions of other arguments themselves.", "labels": [], "entities": []}, {"text": "To date, much of the argumentation mining research has been conducted on domains like news articles, parliamentary records and legal documents, where the documents contain well-formed explicit arguments, i.e. propositions with supporting reasons and evidence present in the text (.", "labels": [], "entities": [{"text": "argumentation mining", "start_pos": 21, "end_pos": 41, "type": "TASK", "confidence": 0.9316365420818329}]}, {"text": "Unlike documents written by professionals, online user comments often contain arguments with inappropriate or missing justification.", "labels": [], "entities": []}, {"text": "One way to deal with such implicit arguments is to simply disregard them and focus on extracting arguments containing proper support.", "labels": [], "entities": []}, {"text": "However, recognizing such propositions as part of an argument, and determining the appropriate types of support can be useful for assessing the adequacy of the supporting information, and in turn, the strength of the whole argument.", "labels": [], "entities": []}, {"text": "Consider the following examples: How much does a small carton of milk cost?", "labels": [], "entities": []}, {"text": "1 More children should drink milk 2 , because children who drink milk everyday are taller than those who don't 3 . Children would want to drink milk, anyway 4 . Firstly, Sentence 1 does not need any support, nor is it part of an argument.", "labels": [], "entities": [{"text": "Sentence", "start_pos": 170, "end_pos": 178, "type": "TASK", "confidence": 0.9127808809280396}]}, {"text": "Next, Proposition 2 is an unverifiable proposition because it cannot be proved with objective evidence, due to the value judgement.", "labels": [], "entities": []}, {"text": "Instead, it can be supported by a reason explaining why it maybe true.", "labels": [], "entities": []}, {"text": "If the reason, Proposition 3, were not true, the whole ar-gument would fall apart, giving little weight to Proposition 2.", "labels": [], "entities": []}, {"text": "Thus, an objective evidence supporting Proposition 3, which is a verifiable proposition, could be provided to strengthen the argument.", "labels": [], "entities": []}, {"text": "Lastly, as Proposition 4 is unverifiable, we cannot expect an objective evidence that proves it, but a reason as its support.", "labels": [], "entities": []}, {"text": "Note that providing a reason why Proposition 3 might be true is not as effective as substantiating it with a proof, but is still better than having no support.", "labels": [], "entities": []}, {"text": "This shows that not only the presence, but also the type of supporting information affects the strength of the argument.", "labels": [], "entities": []}, {"text": "Examining each proposition in this way, i.e. with respect to its verifiability, provides a means to determine the desirable types of support, if any, and enables the analysis of the arguments in terms of the adequacy of their support.", "labels": [], "entities": []}, {"text": "Thus, we propose the task of classifying each proposition (the elementary unit of argumentation in this work) in an argument as UNVERIFIABLE, VERI-FIABLE PUBLIC, or VERIFIABLE PRIVATE, where the appropriate type of support is reason, evidence, and optional evidence, respectively.", "labels": [], "entities": [{"text": "UNVERIFIABLE", "start_pos": 128, "end_pos": 140, "type": "METRIC", "confidence": 0.6817296743392944}, {"text": "VERI-FIABLE PUBLIC", "start_pos": 142, "end_pos": 160, "type": "METRIC", "confidence": 0.8905649185180664}]}, {"text": "To perform the experiments, we annotate 9,476 sentences and clauses from 1,047 comments extracted from an eRulemaking platform.", "labels": [], "entities": []}, {"text": "In the remainder of the paper, we describe the annotation scheme and a newly created dataset (Section 2), propose a supervised learning approach to the task (Section 3), evaluate the approach (Section 4), and survey related work (Section 5).", "labels": [], "entities": []}, {"text": "We find that Support Vector Machines (SVM) classifiers trained with n-grams and other features to capture the verifiability and experientiality exhibit statistically significant improvement over the unigram baseline, achieving a macroaveraged F 1 score of 68.99%.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 243, "end_pos": 252, "type": "METRIC", "confidence": 0.9760371049245199}]}], "datasetContent": [{"text": "4.1 Methodology A Note on Argument Detection A natural first step in argumentation mining is to determine which portions of the given document comprise an argument.", "labels": [], "entities": [{"text": "Argument Detection", "start_pos": 26, "end_pos": 44, "type": "TASK", "confidence": 0.6897183805704117}, {"text": "argumentation mining", "start_pos": 69, "end_pos": 89, "type": "TASK", "confidence": 0.8562858998775482}]}, {"text": "It can also be framed as a binary classification task in which each proposition in the document needs to be classified as either argumentative or not.", "labels": [], "entities": []}, {"text": "Some authors choose to skip this step (Feng and Hirst, 2011), while others make use of various classifiers to achieve high level of accuracy, as Palau and Moens achieved over 70% accuracy on Araucaria and ECHR corpus.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 132, "end_pos": 140, "type": "METRIC", "confidence": 0.9987022876739502}, {"text": "accuracy", "start_pos": 179, "end_pos": 187, "type": "METRIC", "confidence": 0.9979606866836548}, {"text": "ECHR corpus", "start_pos": 205, "end_pos": 216, "type": "DATASET", "confidence": 0.8675269782543182}]}, {"text": "As we have discussed in Section 1, our setup is a bit unique in that we also consider implicit arguments, where propositions are not supported with explicit reason or evidence, as argumentative.", "labels": [], "entities": []}, {"text": "As a result, only about 7%( NONARG TOTAL in) of our entire dataset is marked as non-argumentative, most of which consists of questions and greetings.", "labels": [], "entities": [{"text": "NONARG TOTAL", "start_pos": 28, "end_pos": 40, "type": "METRIC", "confidence": 0.7094144523143768}]}, {"text": "By simply searching for specific unigrams, such as \"?\" and \"thank\", we achieve over 99% F 1 score in determining which propositions are part of an argument.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 88, "end_pos": 97, "type": "METRIC", "confidence": 0.9868084192276001}]}, {"text": "The remaining experiments were done without non-argumentative propositions, i.e. NONARG in.", "labels": [], "entities": [{"text": "NONARG", "start_pos": 81, "end_pos": 87, "type": "METRIC", "confidence": 0.8595210909843445}]}, {"text": "Experimental Setup We first randomly selected 292 comments as held-out test set, resulting in the distribution shown in.", "labels": [], "entities": []}, {"text": "Then, VERIF N ON and VERIF EXP in the training set were oversampled so that the classes are equally distributed.", "labels": [], "entities": [{"text": "VERIF N ON", "start_pos": 6, "end_pos": 16, "type": "METRIC", "confidence": 0.9620248476664225}, {"text": "VERIF EXP", "start_pos": 21, "end_pos": 30, "type": "METRIC", "confidence": 0.7827592492103577}]}, {"text": "During training, five fold cross-validation was done on the training set to tune the C parameter to 32.", "labels": [], "entities": []}, {"text": "Because the micro-averaged F 1 score can be easily boosted on datasets with highly skewed class distribution, we optimize for the macroaveraged F 1 score.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 27, "end_pos": 36, "type": "METRIC", "confidence": 0.9270339210828146}, {"text": "F 1 score", "start_pos": 144, "end_pos": 153, "type": "METRIC", "confidence": 0.7912515004475912}]}, {"text": "Preprocessing was kept at a minimal level: capital letters were lowercased after counting fully capitalized words, and numbers were converted to a NUM token.: # of propositions in Train and Test Set shows a summary of the classification results.", "labels": [], "entities": [{"text": "Preprocessing", "start_pos": 0, "end_pos": 13, "type": "METRIC", "confidence": 0.984440803527832}, {"text": "Train and Test Set", "start_pos": 180, "end_pos": 198, "type": "DATASET", "confidence": 0.8847799450159073}]}, {"text": "The best overall performance is achieved by combining all features (UNI+BI+VER+EXP), yielding 68.99% macro-averaged F 1 , where the gain over the baseline is statistically significant according to the bootstrap method with 10,000 samples (.", "labels": [], "entities": [{"text": "BI+VER+EXP", "start_pos": 72, "end_pos": 82, "type": "METRIC", "confidence": 0.701092791557312}, {"text": "F 1", "start_pos": 116, "end_pos": 119, "type": "METRIC", "confidence": 0.9430504441261292}]}, {"text": "Core Clause Tag (CCT) We do not report the performance of employing feature sets with Core Clause Tag (CCT) in, because the effect of CCT on each of the six sets of features is statistically insignificant.", "labels": [], "entities": []}, {"text": "The star (*) indicates that the given result is statistically significantly better than the unigram baseline.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Class Distribution Over Sentences and Clauses", "labels": [], "entities": []}, {"text": " Table 4: # of propositions in Train and Test Set", "labels": [], "entities": []}, {"text": " Table 3: Three class classification results in % (Crammer & Singer's Multiclass SVMs)", "labels": [], "entities": [{"text": "Three class classification", "start_pos": 10, "end_pos": 36, "type": "TASK", "confidence": 0.5894628365834554}]}]}