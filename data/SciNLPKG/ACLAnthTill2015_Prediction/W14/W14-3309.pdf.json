{"title": [{"text": "Edinburgh's Phrase-based Machine Translation Systems for WMT-14", "labels": [], "entities": [{"text": "Edinburgh", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.9706206321716309}, {"text": "Phrase-based Machine Translation", "start_pos": 12, "end_pos": 44, "type": "TASK", "confidence": 0.7159514427185059}, {"text": "WMT-14", "start_pos": 57, "end_pos": 63, "type": "TASK", "confidence": 0.6824031472206116}]}], "abstractContent": [{"text": "This paper describes the University of Ed-inburgh's (UEDIN) phrase-based submissions to the translation and medical translation shared tasks of the 2014 Workshop on Statistical Machine Translation (WMT).", "labels": [], "entities": [{"text": "translation and medical translation shared tasks of the 2014 Workshop on Statistical Machine Translation (WMT)", "start_pos": 92, "end_pos": 202, "type": "TASK", "confidence": 0.893593265729792}]}, {"text": "We participated in all language pairs.", "labels": [], "entities": []}, {"text": "We have improved upon our 2013 system by i) using generalized representations , specifically automatic word clusters for translations out of English, ii) using unsupervised character-based models to translate unknown words in Russian-English and Hindi-English pairs, iii) synthesizing Hindi data from closely-related Urdu data, and iv) building huge language on the common crawl corpus.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Using Word Clusters in Phrase-based and  OSM models -B 0 = System without Clusters,  +Cid = with Cluster", "labels": [], "entities": []}, {"text": " Table 2: Using POS and Morph Tags in  OSM models -B 0 = Baseline, +OSM p,m =  POS/Morph-based OSM", "labels": [], "entities": [{"text": "OSM", "start_pos": 95, "end_pos": 98, "type": "TASK", "confidence": 0.43695053458213806}]}, {"text": " Table 3: Using Unsupervised Transliteration  Model -Training = Extracted Transliteration Cor- pus (types), OOV = Out-of-vocabulary words (to- kens) B 0 = System without Transliteration, +T r  = Transliterating OOVs", "labels": [], "entities": []}, {"text": " Table 4: Evaluating Synthesized (Syn) Hindi- English Parallel Data, B 0 = System without Syn- thesized Data", "labels": [], "entities": []}, {"text": " Table 5. A full description  of the pipeline, including a public data release, ap- pears in Buck et al. (2014).", "labels": [], "entities": []}, {"text": " Table 5: Size of huge language model training data", "labels": [], "entities": [{"text": "Size", "start_pos": 10, "end_pos": 14, "type": "TASK", "confidence": 0.9545500874519348}]}, {"text": " Table 6: Gains obtained by using huge language  models -B 0 = Baseline, +L = Adding Huge LM", "labels": [], "entities": []}, {"text": " Table 7: Comparison of fast word alignment  method (Dyer et al., 2013) against GIZA++  (WMT 2013 data condition, test on new- stest2012). The method was not used in the official  submission.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 29, "end_pos": 43, "type": "TASK", "confidence": 0.7262952327728271}, {"text": "WMT 2013 data condition", "start_pos": 89, "end_pos": 112, "type": "DATASET", "confidence": 0.9096269309520721}]}, {"text": " Table 8: Hierarchical lexicalized reordering model  (", "labels": [], "entities": []}, {"text": " Table 9: Cumulative gains obtained for each lan- guage -B 0 = Baseline, B 1 = Best System", "labels": [], "entities": []}]}