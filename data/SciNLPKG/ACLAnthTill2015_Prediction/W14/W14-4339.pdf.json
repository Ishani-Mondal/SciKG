{"title": [{"text": "Web-style ranking and SLU combination for dialog state tracking", "labels": [], "entities": [{"text": "dialog state tracking", "start_pos": 42, "end_pos": 63, "type": "TASK", "confidence": 0.7594352960586548}]}], "abstractContent": [{"text": "In spoken dialog systems, statistical state tracking aims to improve robustness to speech recognition errors by tracking a posterior distribution over hidden dialog states.", "labels": [], "entities": [{"text": "statistical state tracking", "start_pos": 26, "end_pos": 52, "type": "TASK", "confidence": 0.656087319056193}]}, {"text": "This paper introduces two novel methods for this task.", "labels": [], "entities": []}, {"text": "First, we explain how state tracking is structurally similar to web-style ranking, enabling mature , powerful ranking algorithms to be applied.", "labels": [], "entities": [{"text": "state tracking", "start_pos": 22, "end_pos": 36, "type": "TASK", "confidence": 0.7483746707439423}]}, {"text": "Second, we show how to use multiple spoken language understanding engines (SLUs) instate tracking-multiple SLUs can expand the set of dialog states being tracked, and give more information about each, thereby increasing both recall and precision of state tracking.", "labels": [], "entities": [{"text": "recall", "start_pos": 225, "end_pos": 231, "type": "METRIC", "confidence": 0.9988694787025452}, {"text": "precision", "start_pos": 236, "end_pos": 245, "type": "METRIC", "confidence": 0.9984725117683411}]}, {"text": "We evaluate on the second Dialog State Tracking Challenge; together these two techniques yield highest accuracy in 2 of 3 tasks, including the most difficult and general task.", "labels": [], "entities": [{"text": "Dialog State Tracking Challenge", "start_pos": 26, "end_pos": 57, "type": "TASK", "confidence": 0.7907682061195374}, {"text": "accuracy", "start_pos": 103, "end_pos": 111, "type": "METRIC", "confidence": 0.9992696642875671}]}], "introductionContent": [{"text": "Spoken dialog systems interact with users via natural language to help them achieve a goal.", "labels": [], "entities": []}, {"text": "As the interaction progresses, the dialog manager maintains a representation of the state of the dialog in a process called dialog state tracking).", "labels": [], "entities": [{"text": "dialog state tracking", "start_pos": 124, "end_pos": 145, "type": "TASK", "confidence": 0.6327106753985087}]}, {"text": "For example, in a restaurant search application, the dialog state might indicate that the user is looking for an inexpensive restaurant in the center of town.", "labels": [], "entities": []}, {"text": "Dialog state tracking is difficult because errors in automatic speech recognition (ASR) and spoken language understanding (SLU) are common, and can cause the system to misunderstand the user's needs.", "labels": [], "entities": [{"text": "Dialog state tracking", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.9150991837183634}, {"text": "automatic speech recognition (ASR)", "start_pos": 53, "end_pos": 87, "type": "TASK", "confidence": 0.7895547846953074}, {"text": "spoken language understanding (SLU)", "start_pos": 92, "end_pos": 127, "type": "TASK", "confidence": 0.7677617371082306}]}, {"text": "At the same time, state tracking is crucial because the system relies on the estimated dialog state to choose actions -for example, which restaurants to present to the user.", "labels": [], "entities": [{"text": "state tracking", "start_pos": 18, "end_pos": 32, "type": "TASK", "confidence": 0.754609078168869}]}, {"text": "Historically, commercial systems have used hand-crafted rules for state tracking, selecting the SLU result with the highest confidence score observed so far, and discarding alternatives.", "labels": [], "entities": [{"text": "state tracking", "start_pos": 66, "end_pos": 80, "type": "TASK", "confidence": 0.7253178060054779}]}, {"text": "In contrast, statistical approaches compute a posterior distribution over many hypotheses for the dialog state, and in general these have been shown to be superior (.", "labels": [], "entities": []}, {"text": "This paper makes two contributions to the task of statistical dialog state tracking.", "labels": [], "entities": [{"text": "statistical dialog state tracking", "start_pos": 50, "end_pos": 83, "type": "TASK", "confidence": 0.7706758677959442}]}, {"text": "First, we show how to cast dialog state tracking as web-style ranking.", "labels": [], "entities": [{"text": "dialog state tracking", "start_pos": 27, "end_pos": 48, "type": "TASK", "confidence": 0.6253590186436971}]}, {"text": "Each dialog state can be viewed as a document, and each dialog turn can be viewed as a search instance.", "labels": [], "entities": []}, {"text": "The benefit of this construction is that it enables a rich literature of powerful ranking algorithms to be applied.", "labels": [], "entities": []}, {"text": "For example, the ranker we apply constructs a forest of decision trees, which -unlike existing work -automatically encodes conjunctions of low-level features.", "labels": [], "entities": []}, {"text": "Conjunctions are attractive in dialog state tracking where relationships exist between low-level concepts like grounding and confidence score.", "labels": [], "entities": [{"text": "dialog state tracking", "start_pos": 31, "end_pos": 52, "type": "TASK", "confidence": 0.7747490008672079}]}, {"text": "The second contribution is to incorporate the output of multiple spoken language understanding engines (SLUs) into dialog state tracking.", "labels": [], "entities": [{"text": "dialog state tracking", "start_pos": 115, "end_pos": 136, "type": "TASK", "confidence": 0.7983935077985128}]}, {"text": "Using more than one SLU can increase the number of dialog states being tracked, improving the chances of discovering the correct one.", "labels": [], "entities": []}, {"text": "Moreover, additional SLUs supply more features, such as semantic confidence scores, improving accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 94, "end_pos": 102, "type": "METRIC", "confidence": 0.9985648989677429}]}, {"text": "This paper is organized as follows.", "labels": [], "entities": []}, {"text": "First, section 2 states the problem formally and covers related work.", "labels": [], "entities": []}, {"text": "Section 3 then lays out the data, features, and experimental design.", "labels": [], "entities": []}, {"text": "Section 4 applies web-style ranking, and section 5 covers the usage of multiple SLUs.", "labels": [], "entities": []}, {"text": "Section 6 extends the types of tracking tasks, section 7 compares performance to other entries in DSTC2, and section 8 briefly con-282 cludes.", "labels": [], "entities": [{"text": "DSTC2", "start_pos": 98, "end_pos": 103, "type": "DATASET", "confidence": 0.9204641580581665}]}], "datasetContent": [{"text": "In DSTC2, there are 3 primary metrics for evaluation -accuracy of the top-scored hypothesis, the L2 probability quality, and an ROC measurement.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.9986048340797424}, {"text": "ROC measurement", "start_pos": 128, "end_pos": 143, "type": "METRIC", "confidence": 0.9774028360843658}]}, {"text": "The ROC measurement is only meaningful when compared across systems with similar accuracy; since our variants differ inaccuracy, we omit ROC.", "labels": [], "entities": [{"text": "ROC measurement", "start_pos": 4, "end_pos": 19, "type": "METRIC", "confidence": 0.8720934391021729}, {"text": "accuracy", "start_pos": 81, "end_pos": 89, "type": "METRIC", "confidence": 0.9950926303863525}]}, {"text": "However, note that all of the metrics, including ROC, for our final entries on the development set and test set are available for public download from the DSTC2 website.", "labels": [], "entities": [{"text": "ROC", "start_pos": 49, "end_pos": 52, "type": "METRIC", "confidence": 0.9987982511520386}, {"text": "DSTC2 website", "start_pos": 155, "end_pos": 168, "type": "DATASET", "confidence": 0.9356508851051331}]}, {"text": "The DSTC2 corpus consists of three partitions: train, development, and test.", "labels": [], "entities": [{"text": "DSTC2 corpus", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.9149809181690216}]}, {"text": "Throughout sections 4-6, we report accuracy by training on the training set, and report accuracy on the development set and test set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9975806474685669}, {"text": "accuracy", "start_pos": 88, "end_pos": 96, "type": "METRIC", "confidence": 0.9989217519760132}]}, {"text": "The development set was available during development of the models, whereas the test set was not.", "labels": [], "entities": []}, {"text": "When preparing final entries for the DSTC2 blind evaluation, we not longer needed a separate development set, so our final models are trained on the combined training and development sets.", "labels": [], "entities": [{"text": "DSTC2 blind evaluation", "start_pos": 37, "end_pos": 59, "type": "TASK", "confidence": 0.5868417918682098}]}, {"text": "In the DSTC2 results, we are team2.", "labels": [], "entities": [{"text": "DSTC2", "start_pos": 7, "end_pos": 12, "type": "DATASET", "confidence": 0.9305176138877869}]}, {"text": "Our entry 0 and 1 use the process described above, including score averaging across multiple models.", "labels": [], "entities": []}, {"text": "Entry0 used SLU0+1, and entry1 used SLU0+2.", "labels": [], "entities": [{"text": "Entry0", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.7508809566497803}]}, {"text": "Entry3 used a maxent model on SLU0+2, but without model averaging since its parameters are set with crossvalidation.", "labels": [], "entities": [{"text": "Entry3", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9337407946586609}]}, {"text": "For accuracy for the joint goal and method tasks, our entries had highest accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9983874559402466}, {"text": "accuracy", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.9984461665153503}]}, {"text": "After the evaluation, we learned that we were the only team to use features from the word confusion network (WCN).", "labels": [], "entities": [{"text": "word confusion network (WCN)", "start_pos": 85, "end_pos": 113, "type": "TASK", "confidence": 0.6171489159266154}]}, {"text": "Comparing our entry0, which does not use WCN features, to the other teams shows that, given the same input data, our entries were still best for the joint goal and method tasks.", "labels": [], "entities": []}, {"text": "The blind evaluation results give a final opportunity to compare the maxent model with the ranking model: entry1 and entry3 both use SLU0+2, and score an identical set of dialog states using identical features.", "labels": [], "entities": []}, {"text": "Joint goal accuracy is better for the ranking model.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.9689167737960815}]}, {"text": "However, as noted above, L2 performance for the ranking model was substantially worse than for the maxent model.", "labels": [], "entities": []}, {"text": "After the blind evaluation, we realized that we had inadvertently omitted a key feature from the \"requested\" binary classifiers -whether the \"request\" dialog act appeared in the SLU results.: Final DSTC2 evaluation results, training on the combined \"train\" and \"development\" sets.", "labels": [], "entities": []}, {"text": "In the results, we are team2.", "labels": [], "entities": []}, {"text": "\"Model comb.\" indicates score averaging over several model instances.", "labels": [], "entities": []}, {"text": "For the \"requested\" task, our entry in DSTC2 inadvertently omitted a key feature, which decreased performance significantly.", "labels": [], "entities": [{"text": "DSTC2", "start_pos": 39, "end_pos": 44, "type": "DATASET", "confidence": 0.9282703995704651}]}, {"text": "\"Requested*\" columns indicate results with this feature included.", "labels": [], "entities": [{"text": "Requested", "start_pos": 1, "end_pos": 10, "type": "METRIC", "confidence": 0.9665530920028687}]}, {"text": "They were computed after the blind evaluation and are not part of the official DSTC2 results.", "labels": [], "entities": [{"text": "DSTC2", "start_pos": 79, "end_pos": 84, "type": "DATASET", "confidence": 0.9592422246932983}]}], "tableCaptions": [{"text": " Table 1: Performance of three SLU engines. SLU0 is the DSTC2 corpus; SLU1 is our engine with  uni-grams and bi-grams of ASR results in the corpus; and SLU2 is SLU1 with the addition of unigram  features from the word confusion network. Precision, Recall, F-measure, and ICE evaluate the quality  of the SLU output, not state tracking. \"ICE\" is item-wise cross entropy -smaller numbers are better  (", "labels": [], "entities": [{"text": "DSTC2 corpus", "start_pos": 56, "end_pos": 68, "type": "DATASET", "confidence": 0.9660970866680145}, {"text": "Precision", "start_pos": 237, "end_pos": 246, "type": "METRIC", "confidence": 0.9881342053413391}, {"text": "Recall", "start_pos": 248, "end_pos": 254, "type": "METRIC", "confidence": 0.9889091849327087}, {"text": "F-measure", "start_pos": 256, "end_pos": 265, "type": "METRIC", "confidence": 0.9766684174537659}, {"text": "ICE", "start_pos": 271, "end_pos": 274, "type": "METRIC", "confidence": 0.9797714352607727}]}, {"text": " Table 2: Summary of accuracy and L2 for the three tracking tasks, trained on the \"train\" set. In rows  marked (*), joint goal accuracy used ranking, and the other two tasks used maxent. In rows marked (**),  several model classes/parameter settings were used and combined with score averaging.", "labels": [], "entities": [{"text": "Summary", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9216763973236084}, {"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9237858057022095}, {"text": "joint goal accuracy", "start_pos": 116, "end_pos": 135, "type": "METRIC", "confidence": 0.5401687820752462}]}, {"text": " Table 3: Final DSTC2 evaluation results, training on the combined \"train\" and \"development\" sets. In  the results, we are team2. \"Model comb.\" indicates score averaging over several model instances. For the  \"requested\" task, our entry in DSTC2 inadvertently omitted a key feature, which decreased performance  significantly. \"Requested*\" columns indicate results with this feature included. They were computed  after the blind evaluation and are not part of the official DSTC2 results.", "labels": [], "entities": [{"text": "DSTC2 results", "start_pos": 473, "end_pos": 486, "type": "DATASET", "confidence": 0.9576202034950256}]}]}