{"title": [{"text": "Unsupervised Techniques for Extracting and Clustering Complex Events in News", "labels": [], "entities": [{"text": "Extracting and Clustering Complex Events", "start_pos": 28, "end_pos": 68, "type": "TASK", "confidence": 0.7314060807228089}]}], "abstractContent": [{"text": "Structured machine-readable representations of news articles can radically change the way we interact with information.", "labels": [], "entities": []}, {"text": "One step towards obtaining these representations is event extraction-the identification of event triggers and arguments in text.", "labels": [], "entities": [{"text": "event extraction-the identification of event triggers and arguments in text", "start_pos": 52, "end_pos": 127, "type": "TASK", "confidence": 0.8729169130325317}]}, {"text": "With previous approaches mainly focus-ing on classifying events into a small set of predefined types, we analyze unsupervised techniques for complex event extraction.", "labels": [], "entities": [{"text": "event extraction", "start_pos": 149, "end_pos": 165, "type": "TASK", "confidence": 0.7618323266506195}]}, {"text": "In addition to extracting event mentions in news articles, we aim at obtaining a more general representation by disambiguating to concepts defined in knowledge bases.", "labels": [], "entities": [{"text": "extracting event mentions in news articles", "start_pos": 15, "end_pos": 57, "type": "TASK", "confidence": 0.80634073416392}]}, {"text": "These concepts are further used as features in a clustering application.", "labels": [], "entities": []}, {"text": "Two evaluation settings highlight the advantages and shortcomings of the proposed approach.", "labels": [], "entities": []}], "introductionContent": [{"text": "Event extraction is a key prerequisite for generating structured, machine-readable representations of natural language.", "labels": [], "entities": [{"text": "Event extraction", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.7846053242683411}]}, {"text": "Such representations can aid various tasks like a) question answering, by enabling systems to provide results for more complex queries, b) machine translation, by enhancing different translation models or c) novelty detection, as a basis for computing geometric distances or distributional similarities.", "labels": [], "entities": [{"text": "question answering", "start_pos": 51, "end_pos": 69, "type": "TASK", "confidence": 0.9033238887786865}, {"text": "machine translation", "start_pos": 139, "end_pos": 158, "type": "TASK", "confidence": 0.797881692647934}, {"text": "novelty detection", "start_pos": 208, "end_pos": 225, "type": "TASK", "confidence": 0.6891801953315735}]}, {"text": "Event extraction primarily requires identifying what has occurred and who or what was involved, as well as the time interval of the occurrence.", "labels": [], "entities": [{"text": "Event extraction", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.7804653346538544}]}, {"text": "Additional information related to the event mention may include its location.", "labels": [], "entities": []}, {"text": "Moreover, the event mention can also be labeled as belonging to a certain event type.", "labels": [], "entities": []}, {"text": "Generally speaking, the goal of event extraction is to identify the event trigger, i.e. the * The work was carried out while the first author was an intern words that most clearly define the event, and the event arguments.", "labels": [], "entities": [{"text": "event extraction", "start_pos": 32, "end_pos": 48, "type": "TASK", "confidence": 0.7500117421150208}]}, {"text": "For example, the event mention {Hurricane Katrina struck the coast of New Orleans in August 2005} belonging to the occurrence of natural disasters type of events includes the location of the disaster -New Orleans and the time of occurrence -August 2005.", "labels": [], "entities": []}, {"text": "The event trigger is the verb struck while the other words represent the arguments of this event.", "labels": [], "entities": []}, {"text": "The generalized form of the event mention is {natural disaster occurred at location on date}.", "labels": [], "entities": []}, {"text": "Another similar event mention is {Hurricane Katrina hit New Orleans}, having the generalized form {natural disaster occurred at location}.", "labels": [], "entities": []}, {"text": "Both event mentions can be generalized to {natural disaster occurred at location}, with the first event mention providing additional details regarding the date of the occurrence.", "labels": [], "entities": []}, {"text": "Supervised approaches imply classifying extracted event mentions according to predefined event types).", "labels": [], "entities": []}, {"text": "Lexical databases such as FrameNet (), VerbNet ( or PropBank () can serve as training data.", "labels": [], "entities": [{"text": "VerbNet", "start_pos": 39, "end_pos": 46, "type": "DATASET", "confidence": 0.8470752239227295}]}, {"text": "However, the coverage of this data is still limited, especially for domain-specific applications, and acquiring more labeled data can be expensive.", "labels": [], "entities": []}, {"text": "Unsupervised approaches, on the other hand, are usually used to extract large numbers of untyped events.", "labels": [], "entities": []}, {"text": "Despite the coverage of these techniques, some of the extracted events can suffer from reduced quality in terms of both precision and recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 120, "end_pos": 129, "type": "METRIC", "confidence": 0.9994524121284485}, {"text": "recall", "start_pos": 134, "end_pos": 140, "type": "METRIC", "confidence": 0.9969215989112854}]}, {"text": "Distant supervision aims at mitigating the disadvantages of both supervised and unsupervised techniques by leveraging events defined in knowledge bases (.", "labels": [], "entities": []}, {"text": "In this work we investigate unsupervised techniques for extracting and clustering complex events from news articles.", "labels": [], "entities": [{"text": "extracting and clustering complex events from news articles", "start_pos": 56, "end_pos": 115, "type": "TASK", "confidence": 0.8530946299433708}]}, {"text": "For clustering events we are using their generalized representation ob-", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated the extracted events, as well as the clusters obtained for the disambiguated events.", "labels": [], "entities": []}, {"text": "For each set of experiments we prepared a dataset by sampling Bloomberg news articles.", "labels": [], "entities": []}, {"text": "As there is no benchmark dataset for the news articles that we are analyzing, we propose to evaluate event extraction in terms of completeness.", "labels": [], "entities": [{"text": "event extraction", "start_pos": 101, "end_pos": 117, "type": "TASK", "confidence": 0.7731840014457703}]}, {"text": "Clustering evaluation is done based on the model itself, and for different feature combinations.", "labels": [], "entities": []}, {"text": "In what follows we describe the evaluation setting in more detail.", "labels": [], "entities": []}, {"text": "The evaluation dataset consists of a sample of 23 stories belonging to the MEDICARE topic, containing a total of 1088 sentences.", "labels": [], "entities": [{"text": "MEDICARE topic", "start_pos": 75, "end_pos": 89, "type": "DATASET", "confidence": 0.6973077654838562}]}, {"text": "The event extraction algorithms yields 229 entity paths and 515 verb and argument events.", "labels": [], "entities": [{"text": "event extraction", "start_pos": 4, "end_pos": 20, "type": "TASK", "confidence": 0.742181658744812}]}, {"text": "Each event is assessed in terms of completeness; an event is deemed to be complete if all event elements (the event trigger and the arguments) are correctly identified.", "labels": [], "entities": []}, {"text": "We only analyze two event patterns: {sub, verb, obj} and {sub, verb, obj, entities}, as events belonging to other patterns are rather noisy.", "labels": [], "entities": []}, {"text": "Two annotators independently rate each event with 1 if all event elements are correctly identified, and 0 otherwise.", "labels": [], "entities": []}, {"text": "Note that incomplete events receive a 0 score.", "labels": [], "entities": []}, {"text": "Cohen's kappa coefficient of inter-annotator agreement for this experiment was 0.70.", "labels": [], "entities": []}, {"text": "The entity path approach correctly identified 78.6% of the entities while the verb arguments approach identified 69.1% of the events.", "labels": [], "entities": []}, {"text": "Events obtained using entity paths tend to have a higher number of arguments compared to the verb arguments approach; this explains the higher score obtained by this technique.", "labels": [], "entities": []}, {"text": "As we do not know the cluster labels a priori, we opt for evaluating the clusters using the model itself.", "labels": [], "entities": []}, {"text": "To this end, we use the Silhouette Coefficient; we plan to investigate other clustering evaluation metrics in future work.", "labels": [], "entities": []}, {"text": "The Silhouette Coefficient is defined for each sample, and it incorporates two scores: where a is the mean distance between a sample and all other points within the same class whereas b is the mean distance to all other points in the next nearest class.", "labels": [], "entities": []}, {"text": "To determine the coefficient fora set of samples one needs to find the mean of the coefficient for each sample.", "labels": [], "entities": []}, {"text": "A higher coefficient score is associated with a model having better defined clusters.", "labels": [], "entities": []}, {"text": "The best clustering model will obtain a Silhouette coefficient of 1, while the worst one will obtain a -1 score.", "labels": [], "entities": [{"text": "Silhouette coefficient", "start_pos": 40, "end_pos": 62, "type": "METRIC", "confidence": 0.9886585772037506}]}, {"text": "Values close to 0 imply overlapping clusters.", "labels": [], "entities": []}, {"text": "Negative values signify that the model assigned samples to the wrong cluster, as a different cluster is more similar.", "labels": [], "entities": []}, {"text": "The evaluation dataset comprises 325 MEDI-CARE news articles and 16,450 sentences.", "labels": [], "entities": []}, {"text": "In this dataset we identify 7,491 verb and argument events and 2,046 shortest path events.", "labels": [], "entities": []}, {"text": "shows example events belonging to two event clusters.", "labels": [], "entities": []}, {"text": "The first cluster is obtained by extracting verb argument events while the second cluster is composed of shortest entity path events.", "labels": [], "entities": []}, {"text": "In we show clustering evaluation results for the (a) verbs and arguments and (b) shortest paths between entities, using different feature combinations.", "labels": [], "entities": []}, {"text": "As expected, the best results are obtained in the case of the WordNet super-senses, which are the most generic senses assigned to the events.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 62, "end_pos": 69, "type": "DATASET", "confidence": 0.9430199861526489}]}, {"text": "There is less overlap among the BabelNet senses and hypernyms, although results improve as more data is available.", "labels": [], "entities": []}, {"text": "The results also mark the difference between the two types of events: verbs and arguments versus shortest paths between entities.", "labels": [], "entities": []}, {"text": "Events extracted using the entity path approach tend to have a higher number of arguments, which in turn implies a richer set of features.", "labels": [], "entities": []}, {"text": "This explains the higher scores obtained in the case of shortest path events compared to verb argument events.", "labels": [], "entities": []}], "tableCaptions": []}