{"title": [], "abstractContent": [{"text": "We present a data-driven framework for image caption generation which incorporates visual and textual features with varying degrees of spatial structure.", "labels": [], "entities": [{"text": "image caption generation", "start_pos": 39, "end_pos": 63, "type": "TASK", "confidence": 0.831758995850881}]}, {"text": "We propose the task of domain-specific image captioning, where many relevant visual details cannot be captured by off-the-shelf general-domain entity detectors.", "labels": [], "entities": [{"text": "domain-specific image captioning", "start_pos": 23, "end_pos": 55, "type": "TASK", "confidence": 0.629405160744985}]}, {"text": "We extract previously-written descriptions from a database and adapt them to new query images, using a joint visual and textual bag-of-words model to determine the cor-rectness of individual words.", "labels": [], "entities": []}, {"text": "We implement our model using a large, unlabeled dataset of women's shoes images and natural language descriptions (Berg et al., 2010).", "labels": [], "entities": []}, {"text": "Using both automatic and human evaluations, we show that our caption-ing method effectively deletes inaccurate words from extracted captions while maintaining a high level of detail in the generated output.", "labels": [], "entities": []}], "introductionContent": [{"text": "Broadly, the task of image captioning is: given a query image, generate a natural language description of the image's visual content.", "labels": [], "entities": [{"text": "image captioning", "start_pos": 21, "end_pos": 37, "type": "TASK", "confidence": 0.7314182966947556}]}, {"text": "Both the image understanding and language generation components of this task are challenging open problems in their respective fields.", "labels": [], "entities": [{"text": "image understanding", "start_pos": 9, "end_pos": 28, "type": "TASK", "confidence": 0.8398753702640533}, {"text": "language generation", "start_pos": 33, "end_pos": 52, "type": "TASK", "confidence": 0.7519128024578094}]}, {"text": "A wide variety of approaches have been proposed in the literature, for both the specific task of caption generation as well as related problems in understanding images and text.", "labels": [], "entities": [{"text": "caption generation", "start_pos": 97, "end_pos": 115, "type": "TASK", "confidence": 0.946796715259552}]}, {"text": "Typically, image understanding systems use supervised algorithms to detect visual entities and concepts in images.", "labels": [], "entities": [{"text": "image understanding", "start_pos": 11, "end_pos": 30, "type": "TASK", "confidence": 0.769472748041153}]}, {"text": "However, these typically require accurate hand-labeled training data, which is not available inmost specific domains.", "labels": [], "entities": []}, {"text": "Ideally, a domain-specific image captioning system would learn in a less supervised fashion, using captioned images found on the web.", "labels": [], "entities": [{"text": "domain-specific image captioning", "start_pos": 11, "end_pos": 43, "type": "TASK", "confidence": 0.7112616697947184}]}, {"text": "This paper focuses on image caption generation fora specific domain -images of women's shoes, collected from online shopping websites.", "labels": [], "entities": [{"text": "image caption generation", "start_pos": 22, "end_pos": 46, "type": "TASK", "confidence": 0.8089904685815176}]}, {"text": "Our framework has three main components.", "labels": [], "entities": []}, {"text": "We extract an existing description from a database of human-captions, by projecting query images into a multi-dimensional space where structurally similar images are near each other.", "labels": [], "entities": []}, {"text": "We also train a joint topic model to discover the latent topics which generate both captions and images.", "labels": [], "entities": []}, {"text": "We combine these two approaches using sentence compression to delete modifying details in the extracted caption which are not relevant to the query image.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 38, "end_pos": 58, "type": "TASK", "confidence": 0.7115508019924164}]}, {"text": "Our captioning framework is inspired by several recent approaches at the intersection of Natural Language Processing and Computer Vision.", "labels": [], "entities": []}, {"text": "Previous work such as and explore extractive methods for image captioning, but these rely on generaldomain visual detection systems, and only gener-ate extractive captions.", "labels": [], "entities": [{"text": "image captioning", "start_pos": 57, "end_pos": 73, "type": "TASK", "confidence": 0.7110039591789246}, {"text": "generaldomain visual detection", "start_pos": 93, "end_pos": 123, "type": "TASK", "confidence": 0.5335911413033804}]}, {"text": "Other models learn correspondences between domain-specific images and natural language captions () but cannot generate descriptions for new images without the use of auxiliary text.", "labels": [], "entities": []}, {"text": "propose a sentence compression model for editing image captions, but their compression objective is not conditioned on a query image, and their system also requires general-domain visual detections.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.7239635735750198}, {"text": "editing image captions", "start_pos": 41, "end_pos": 63, "type": "TASK", "confidence": 0.6487147311369578}]}, {"text": "This paper proposes an image captioning framework which extends these ideas and culminates in the first domain-specific image caption generation system.", "labels": [], "entities": [{"text": "image captioning", "start_pos": 23, "end_pos": 39, "type": "TASK", "confidence": 0.7789775133132935}, {"text": "domain-specific image caption generation", "start_pos": 104, "end_pos": 144, "type": "TASK", "confidence": 0.739779531955719}]}, {"text": "More broadly, our goal for image caption generation is to work toward less supervised captioning methods which could be used to generate detailed and accurate descriptions fora variety of long-tail domains of captioned image data, such as in nature and medicine.", "labels": [], "entities": [{"text": "image caption generation", "start_pos": 27, "end_pos": 51, "type": "TASK", "confidence": 0.867944598197937}]}], "datasetContent": [{"text": "The dataset we use is the women's shoes section of the publicly available Attribute Discovery Dataset 3 from Berg et al., which consists of product images and captions scraped from the shopping website Like.com.", "labels": [], "entities": [{"text": "Attribute Discovery Dataset 3 from Berg et al.", "start_pos": 74, "end_pos": 120, "type": "DATASET", "confidence": 0.8221500813961029}]}, {"text": "We use the women's shoes section of the dataset which has 14764 captioned images.", "labels": [], "entities": []}, {"text": "Product descriptions describe many different attributes such as styles, colors, fabrics, patterns, decorations, and affordances (activities that can be performed while wearing the shoe).", "labels": [], "entities": []}, {"text": "Some examples are shown in.", "labels": [], "entities": []}, {"text": "For preprocessing in our framework, we first determine an 80/20% train test split.", "labels": [], "entities": []}, {"text": "We define a textual vocabulary of \"descriptive words\", which are non-function words -adjectives, adverbs, nouns (except proper nouns), and verbs.", "labels": [], "entities": []}, {"text": "This gives us a total of 9578 descriptive words in the training set, with an average of 16.33 descriptive words per caption.", "labels": [], "entities": []}, {"text": "We perform automatic evaluation using similarity measures between automatically generated and human-authored captions.", "labels": [], "entities": []}, {"text": "Note that currently our system and baselines only generate singlesentence captions, but we compare against entire BLEU@1 KL .2098 GIST .4259 LM-ONLY .4780 SYSTEM (COMPRESSION) .4841 held-out captions in order to increase the amount of text we have to compare against.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 114, "end_pos": 118, "type": "METRIC", "confidence": 0.9963886737823486}, {"text": "GIST", "start_pos": 130, "end_pos": 134, "type": "METRIC", "confidence": 0.9689061045646667}]}, {"text": "ROUGE) is a summarization evaluation metric which has also been used to evaluate image captions.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9529778957366943}, {"text": "summarization evaluation", "start_pos": 12, "end_pos": 36, "type": "TASK", "confidence": 0.903930127620697}]}, {"text": "It is usually a recall-oriented measure, but we also report precision and f-measure because our sentence compressions do not improve recall.", "labels": [], "entities": [{"text": "recall-oriented measure", "start_pos": 16, "end_pos": 39, "type": "METRIC", "confidence": 0.9747493267059326}, {"text": "precision", "start_pos": 60, "end_pos": 69, "type": "METRIC", "confidence": 0.9974446296691895}, {"text": "f-measure", "start_pos": 74, "end_pos": 83, "type": "METRIC", "confidence": 0.9668881893157959}, {"text": "recall", "start_pos": 133, "end_pos": 139, "type": "METRIC", "confidence": 0.9945448637008667}]}, {"text": "shows ROUGE-2 (bigram) scores computed without stopwords.", "labels": [], "entities": [{"text": "ROUGE-2", "start_pos": 6, "end_pos": 13, "type": "METRIC", "confidence": 0.9852074384689331}]}, {"text": "We observe that our system very significantly improves ROUGE-2 precision of the GIST extracted caption, without significantly reducing recall.", "labels": [], "entities": [{"text": "ROUGE-2", "start_pos": 55, "end_pos": 62, "type": "METRIC", "confidence": 0.9961477518081665}, {"text": "precision", "start_pos": 63, "end_pos": 72, "type": "METRIC", "confidence": 0.7950342297554016}, {"text": "GIST extracted caption", "start_pos": 80, "end_pos": 102, "type": "DATASET", "confidence": 0.7255529562632242}, {"text": "recall", "start_pos": 135, "end_pos": 141, "type": "METRIC", "confidence": 0.9982140064239502}]}, {"text": "While LM-Only also improves precision against GIST extraction, it indiscriminately removes some words which are relevant to the query image.", "labels": [], "entities": [{"text": "precision", "start_pos": 28, "end_pos": 37, "type": "METRIC", "confidence": 0.9983100891113281}, {"text": "GIST extraction", "start_pos": 46, "end_pos": 61, "type": "TASK", "confidence": 0.81501504778862}]}, {"text": "We also observe that GIST extraction strongly outperforms the KL model, which demonstrates the importance of visual structure.", "labels": [], "entities": [{"text": "GIST extraction", "start_pos": 21, "end_pos": 36, "type": "TASK", "confidence": 0.7969381809234619}]}, {"text": "We also report BLEU () scores, which are the most popularly accepted automatic metric for captioning evaluation.", "labels": [], "entities": [{"text": "BLEU () scores", "start_pos": 15, "end_pos": 29, "type": "METRIC", "confidence": 0.9680170019467672}]}, {"text": "Results are very similar to the ROUGE-2 precision scores, except the difference between our system and LM-Only is less pronounced because BLEU counts function words, while ROUGE does not.", "labels": [], "entities": [{"text": "ROUGE-2 precision scores", "start_pos": 32, "end_pos": 56, "type": "METRIC", "confidence": 0.7558338840802511}, {"text": "BLEU", "start_pos": 138, "end_pos": 142, "type": "METRIC", "confidence": 0.9972429275512695}]}, {"text": "We perform human evaluation of compressions generated by our system and LM-Only.", "labels": [], "entities": []}, {"text": "Users are shown the query image, the original uncompressed caption, and a compressed caption, and are asked two questions: does the compression improve the accuracy of the caption, and is the compression grammatical.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 156, "end_pos": 164, "type": "METRIC", "confidence": 0.9982397556304932}]}, {"text": "We collect 553 judgments from six women who are native English-speakers and knowledgeable", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Summary of ILP constraints.", "labels": [], "entities": [{"text": "Summary", "start_pos": 10, "end_pos": 17, "type": "TASK", "confidence": 0.7476796507835388}]}, {"text": " Table 7: Human evaluation results.", "labels": [], "entities": []}]}