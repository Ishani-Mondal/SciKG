{"title": [{"text": "Association for Computational Linguistics {bs,hr,sr}WaC -Web corpora of Bosnian", "labels": [], "entities": [{"text": "WaC -Web corpora", "start_pos": 52, "end_pos": 68, "type": "DATASET", "confidence": 0.894522950053215}]}], "abstractContent": [{"text": "In this paper we present the construction process of top-level-domain web corpora of Bosnian, Croatian and Serbian.", "labels": [], "entities": []}, {"text": "For constructing the corpora we use the Spi-derLing crawler with its associated tools adapted for simultaneous crawling and processing of text written in two scripts, Latin and Cyrillic.", "labels": [], "entities": []}, {"text": "In addition to the modified collection process we focus on two sources of noise in the resulting corpora: 1.", "labels": [], "entities": []}, {"text": "they contain documents written in the other, closely related languages that cannot be identified with standard language identification methods and 2.", "labels": [], "entities": []}, {"text": "as most web corpora, they partially contain low-quality data not suitable for the specific research and application objectives.", "labels": [], "entities": []}, {"text": "We approach both problems by using language mod-eling on the crawled data only, omitting the need for manually validated language samples for training.", "labels": [], "entities": []}, {"text": "On the task of discriminating between closely related languages we outperform the state-of-the-art Blacklist classifier reducing its error to a fourth.", "labels": [], "entities": [{"text": "error", "start_pos": 133, "end_pos": 138, "type": "METRIC", "confidence": 0.9682175517082214}]}], "introductionContent": [{"text": "Building web corpora for various NLP tasks has become quite a standard approach, especially if funding is limited and / or there is need for large amounts of textual data.", "labels": [], "entities": []}, {"text": "Although off-the-shelf solutions for compiling web corpora have emerged recently, there are still specific challenges that have to be addressed inmost corpus construction processes.", "labels": [], "entities": []}, {"text": "One such challenge that we face while constructing the corpora described in this paper is simultaneous usage of two scripts on two out of three top-level domains (TLDs) crawled.", "labels": [], "entities": []}, {"text": "Additionally, there are still many open questions and possibilities for improvement in the process of collecting data as well as data postprocessing.", "labels": [], "entities": []}, {"text": "We address two of the latter kinddiscrimination between similar, neighboring languages that are used on all selected TLDs, and the question of text quality in corpora collected in such a fully automated fashion.", "labels": [], "entities": []}, {"text": "In the paper we present the process of building web corpora of Bosnian, Croatian and Serbian by crawling the .ba, .hr and .rs TLDs.", "labels": [], "entities": []}, {"text": "The three languages belong to the South Slavic language branch and are very similar to each other.", "labels": [], "entities": []}, {"text": "The biggest differences between Croatian and Serbian are the proto-Slavic vowel jat (Croatia\u0148 covjek vs. Serbia\u0148 covek), way of handling proper nouns (Croatian New York vs. Serbian Nju Jork), specific syntactic constructions (Croatian ho\u00b4cuho\u00b4cu raditi vs. Serbian ho\u00b4cuho\u00b4cu da radim) and a series of lexical differences (Croatian mrkva vs. Serbia\u0148 sargarepa).", "labels": [], "entities": []}, {"text": "Bosnian is mostly seen as a mixture of those two and allows, beside its own lexical specificities, solutions from one or both languages.", "labels": [], "entities": []}, {"text": "This paper is structured as follows: in Section 2 we give an overview of related work regarding existing (web) corpora of the languages in question, language identification and web text quality estimation.", "labels": [], "entities": [{"text": "language identification", "start_pos": 149, "end_pos": 172, "type": "TASK", "confidence": 0.728441521525383}, {"text": "web text quality estimation", "start_pos": 177, "end_pos": 204, "type": "TASK", "confidence": 0.5382204353809357}]}, {"text": "Section 3 shows the process of collecting the three TLD corpora with emphasis on the problem of collecting data written in various scripts, while in Section 4 we describe the linguistic annotation layers added to the corpora.", "labels": [], "entities": []}, {"text": "Section 5 depicts our approach to discriminating between very similar languages while in Section 6 we describe our approach to identifying documents of low text quality, and both approaches use recently crawled data only.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1. Separate numbers  are shown for the new crawl of the Croatian TLD  and the final corpus consisting of both crawls.", "labels": [], "entities": [{"text": "Croatian TLD", "start_pos": 63, "end_pos": 75, "type": "DATASET", "confidence": 0.9292226433753967}]}, {"text": " Table 1: Size of the corpora in Mtokens after phys- ical duplicate (PHY), document near-duplicate  (DOCN) and paragraph near-duplicate removal  (PARN)", "labels": [], "entities": [{"text": "paragraph near-duplicate removal  (PARN)", "start_pos": 111, "end_pos": 151, "type": "TASK", "confidence": 0.5599504758914312}]}]}