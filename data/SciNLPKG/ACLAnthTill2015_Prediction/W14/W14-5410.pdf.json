{"title": [{"text": "Weakly supervised construction of a repository of iconic images", "labels": [], "entities": []}], "abstractContent": [{"text": "We present a first attempt at semi-automatically harvesting a dataset of iconic images, namely images that depict objects or scenes, which arouse associations to abstract topics.", "labels": [], "entities": []}, {"text": "Our method starts with representative topic-evoking images from Wikipedia, which are labeled with relevant concepts and entities found in their associated captions.", "labels": [], "entities": []}, {"text": "These are used to query an online image repository (i.e., Flickr), in order to further acquire additional examples of topic-specific iconic relations.", "labels": [], "entities": []}, {"text": "To this end, we leverage a combination of visual similarity measures, image clustering and matching algorithms to acquire clusters of iconic images that are topically connected to the original seed images, while also allowing for various degrees of diversity.", "labels": [], "entities": [{"text": "image clustering", "start_pos": 70, "end_pos": 86, "type": "TASK", "confidence": 0.6902101933956146}]}, {"text": "Our first results are promising in that they indicate the feasibility of the task and that we are able to build a first version of our resource with minimal supervision.", "labels": [], "entities": []}], "introductionContent": [{"text": "Figurative language and images area pervasive phenomenon associated with human communication.", "labels": [], "entities": [{"text": "Figurative language and images", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.8528337925672531}]}, {"text": "For instance, images used in news articles (especially on hot and sensitive topics) often make use of non-literal visual representations like iconic images, which are aimed at capturing the reader's attention.", "labels": [], "entities": []}, {"text": "For environmental topics, for instance, a windmill in an untouched and bright landscape surrounded by a clear sky is typically associated by humans with environmental friendliness, and accordingly causes positive emotions.", "labels": [], "entities": []}, {"text": "Ina similar way, images of a polar bear on a drifting ice floe are typically associated with the topic of global warming.", "labels": [], "entities": []}, {"text": "But while icons represent a pervasive device for visual communication, to date, there exists to the best of our knowledge no approach aimed at their computational modeling.", "labels": [], "entities": []}, {"text": "In order to enable the overarching goal of producing such kind of models from real-world data, we focus, in this work, on the preliminary task of semi-automatically compiling an electronic database of iconic images.", "labels": [], "entities": []}, {"text": "These consist, in our definition, of images produced to create privileged associations between a particular visual representation and a referent.", "labels": [], "entities": []}, {"text": "Iconic images are highly recognizable for media users and typically induce negative or positive emotions that have an impact on viewers' attitudes and actions.", "labels": [], "entities": []}, {"text": "In order to model them from a computational perspective, we initially formulate iconic image acquisition as a clustering task in which, given a set of initial, manually-selected 'seed' images -e.g., a photo of a polar bear on a drifting ice floe for the topic of global warming, a smokestack for the topic of pollution, etc.", "labels": [], "entities": [{"text": "iconic image acquisition", "start_pos": 80, "end_pos": 104, "type": "TASK", "confidence": 0.6868922710418701}]}, {"text": "-we use their associated textual descriptions in order to collect related images from the Web.", "labels": [], "entities": []}, {"text": "We then process these images using state-of-the-art image understanding techniques to produce clusters of semantically similar, yet different images depicting the same topic in an iconic way.", "labels": [], "entities": []}, {"text": "The acquisition of a database of iconic images represents the first step towards a full-fledged model to computationally capture the phenomenon of iconic images in context.", "labels": [], "entities": []}, {"text": "Our long-term vision is to coverall three aspects of content (what makes an image iconic?), usage (in which context are iconic images used?), and effects (which negative/positive emotions do iconic images evoke on viewers?) of iconic images.", "labels": [], "entities": []}, {"text": "To make this challenging problem feasible, we opt in this preliminary step for an approach that views the task of understanding iconic images as the ability to build a dataset for further research.", "labels": [], "entities": []}], "datasetContent": [{"text": "We first provide statistics on the size of the datasets created with our approach.", "labels": [], "entities": []}, {"text": "Using HSV correlation we initially generate 1232 clusters with an average size of 27.37 elements per cluster.", "labels": [], "entities": []}, {"text": "Additional filtering based on at least two of our image matching methods produces 870 clusters (19.33 elements on average), whereas the more restrictive clustering based on all three methods gives 261 small-sized clusters of only 5.8 instances on average.", "labels": [], "entities": []}, {"text": "This is because, naturally, applying matchingbased filtering tends to produce a smaller number of clusters with fewer elements.", "labels": [], "entities": []}, {"text": "Gold standard and filtering evaluation.", "labels": [], "entities": [{"text": "filtering evaluation", "start_pos": 18, "end_pos": 38, "type": "TASK", "confidence": 0.8121311068534851}]}, {"text": "To produce a gold standard for our task, we annotated all of the 4,000 images we retrieved from Flickr.", "labels": [], "entities": [{"text": "Flickr", "start_pos": 96, "end_pos": 102, "type": "DATASET", "confidence": 0.966379702091217}]}, {"text": "Each image is associated with a keyword query (Section 2): accordingly, we annotated each instance as being iconic or not with respect to the topic expressed by the keywords -e.g., given a picture of Hopetoun Falls, whether it captures the concept of waterfall or not.", "labels": [], "entities": [{"text": "Hopetoun Falls", "start_pos": 200, "end_pos": 214, "type": "DATASET", "confidence": 0.9770015478134155}]}, {"text": "This is because, in our work, we take keywords as proxies of the underlying topics (e.g., biodiversity is depicted using waterfalls): in this setting, negative instances consist of mismatches between the query text and the picture -e.g., a photography taken near Hopetoun Falls, showing beech trees and thus capturing a query search for \"forest\" rather than \"waterfalls\".", "labels": [], "entities": [{"text": "Hopetoun Falls", "start_pos": 263, "end_pos": 277, "type": "DATASET", "confidence": 0.9747038781642914}]}, {"text": "We next evaluate our system on the binary classification task of detecting whether an image is iconic or not.", "labels": [], "entities": [{"text": "binary classification", "start_pos": 35, "end_pos": 56, "type": "TASK", "confidence": 0.7105485796928406}]}, {"text": "In our case, we can quantify performance by taking all images not filtered out in the last step of image matching (and thus deemed as iconic in the final system output), and comparing them against our gold-standard annotations.", "labels": [], "entities": [{"text": "image matching", "start_pos": 99, "end_pos": 113, "type": "TASK", "confidence": 0.7213696837425232}]}, {"text": "This way we can compute standard metrics of precision, recall and balanced F-measure.", "labels": [], "entities": [{"text": "precision", "start_pos": 44, "end_pos": 53, "type": "METRIC", "confidence": 0.9995935559272766}, {"text": "recall", "start_pos": 55, "end_pos": 61, "type": "METRIC", "confidence": 0.999561607837677}, {"text": "F-measure", "start_pos": 75, "end_pos": 84, "type": "METRIC", "confidence": 0.9506502151489258}]}, {"text": "Our results indicate that combining the output of two image matching techniques allows us to reach 59.5% recall and 68.5% precision, whereas requiring all three methods to match reduces precision (46.9%) while drastically decreasing recall (14.3%).", "labels": [], "entities": [{"text": "image matching", "start_pos": 54, "end_pos": 68, "type": "TASK", "confidence": 0.7034833282232285}, {"text": "recall", "start_pos": 105, "end_pos": 111, "type": "METRIC", "confidence": 0.9989996552467346}, {"text": "precision", "start_pos": 122, "end_pos": 131, "type": "METRIC", "confidence": 0.9989953637123108}, {"text": "precision", "start_pos": 186, "end_pos": 195, "type": "METRIC", "confidence": 0.9994078874588013}, {"text": "recall", "start_pos": 233, "end_pos": 239, "type": "METRIC", "confidence": 0.9990591406822205}]}, {"text": "The results show that our system is precisionoriented, and that filtering based on the combination of all methods leads to an overall performance degradation.", "labels": [], "entities": []}, {"text": "This is because requiring all methods to match gives an over-constrained filtering: our methods, in fact, tend to match all together only with those images which are highly similar to the seeds, thus not being able to produce heterogeneous clusters.", "labels": [], "entities": []}, {"text": "We finally compute performance metrics for each single topic in turn, in order to experimentally investigate the different degrees of performance of our system, and determine whether some topics are more difficult than others).", "labels": [], "entities": []}, {"text": "Our results indicate that some topics are indeed more difficult than others -e.g., our system exhibits perfect precision on \"adaptation\" and \"greenhouse effect\" vs. much poorer one on \"biodiversity\" or \"climate refugee\".", "labels": [], "entities": [{"text": "precision", "start_pos": 111, "end_pos": 120, "type": "METRIC", "confidence": 0.998953104019165}, {"text": "climate refugee", "start_pos": 203, "end_pos": 218, "type": "TASK", "confidence": 0.7951684892177582}]}, {"text": "This is because some topics are bootstrapped from less heterogeneous, and hence 'easier', sets of seed images (e.g., all smokestacks, as in \"greenhouse effect\", are very similar to each other).", "labels": [], "entities": []}, {"text": "In general, this seems to point out that one of the key challenges in our scenario is to produce highly precise clusters, while allowing for image diversity as a trade-off.", "labels": [], "entities": []}, {"text": "We finally looked at the output of our system, in order to better understand its performance, as well as problems and future challenges.", "labels": [], "entities": []}, {"text": "Examples of a few sample clusters are shown in.", "labels": [], "entities": []}, {"text": "These clusters show that, thanks to our method, we are able to collect quite diverse, yet iconic images retaining a topical affinity with the original seeds -e.g., the poster on fighting deforestation or the drawing used to depict air pollution.", "labels": [], "entities": []}, {"text": "Due to the noise of our base image processing components, however, we also suffer from wrong matches such as the picture of a mobile phone for the topic of wildfire, where the meaning of a rapidly spreading conflagration is related to air pollution, whereas the mobile phone is not.", "labels": [], "entities": []}, {"text": "Based on a random sample of 10% of the output clusters, we manually identified the main sources of errors as related to: i) false image matching due to problems with contour detection; ii) SIFT performing best for detecting different images of the same objects, but exhibiting lower performance on the more complex task of detecting similar objects; iii) we applied our image matching methods using default parameters and thresholds: further improvements could be obtained by in-domain tuning.", "labels": [], "entities": [{"text": "image matching", "start_pos": 130, "end_pos": 144, "type": "TASK", "confidence": 0.6781995445489883}, {"text": "contour detection", "start_pos": 166, "end_pos": 183, "type": "TASK", "confidence": 0.774336963891983}, {"text": "image matching", "start_pos": 370, "end_pos": 384, "type": "TASK", "confidence": 0.6998009234666824}]}], "tableCaptions": [{"text": " Table 2: Performance results per topic on iconic image detection (percentages).", "labels": [], "entities": [{"text": "iconic image detection", "start_pos": 43, "end_pos": 65, "type": "TASK", "confidence": 0.6680966913700104}]}]}