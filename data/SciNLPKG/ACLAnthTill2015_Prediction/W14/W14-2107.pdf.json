{"title": [{"text": "Back up your Stance: Recognizing Arguments in Online Discussions", "labels": [], "entities": [{"text": "Recognizing Arguments in Online Discussions", "start_pos": 21, "end_pos": 64, "type": "TASK", "confidence": 0.7544612288475037}]}], "abstractContent": [{"text": "In online discussions, users often backup their stance with arguments.", "labels": [], "entities": []}, {"text": "Their arguments are often vague, implicit, and poorly worded, yet they provide valuable insights into reasons underpinning users' opinions.", "labels": [], "entities": []}, {"text": "In this paper, we make a first step towards argument-based opinion mining from on-line discussions and introduce anew task of argument recognition.", "labels": [], "entities": [{"text": "argument-based opinion mining", "start_pos": 44, "end_pos": 73, "type": "TASK", "confidence": 0.6145063440004984}, {"text": "argument recognition", "start_pos": 126, "end_pos": 146, "type": "TASK", "confidence": 0.7878221571445465}]}, {"text": "We match user-created comments to a set of predefined topic-based arguments, which can be either attacked or supported in the comment.", "labels": [], "entities": []}, {"text": "We present a manually-annotated corpus for argument recognition in online discussions.", "labels": [], "entities": [{"text": "argument recognition", "start_pos": 43, "end_pos": 63, "type": "TASK", "confidence": 0.9001087248325348}]}, {"text": "We describe a supervised model based on comment-argument similarity and entail-ment features.", "labels": [], "entities": []}, {"text": "Depending on problem formulation , model performance ranges from 70.5% to 81.8% F1-score, and decreases only marginally when applied to an unseen topic.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 80, "end_pos": 88, "type": "METRIC", "confidence": 0.9996988773345947}]}], "introductionContent": [{"text": "Whether about coffee preparation, music taste, or legal cases in courtrooms, arguing has always been the dominant way of rationalizing opinions.", "labels": [], "entities": [{"text": "coffee preparation", "start_pos": 14, "end_pos": 32, "type": "TASK", "confidence": 0.7237664014101028}]}, {"text": "An argument consists of one or more premises leading to exactly one conclusion, while argumentation connects together several arguments.", "labels": [], "entities": []}, {"text": "Across domains, argumentation differs in vocabulary, style, and purpose, ranging from legal () and scientific argumentation) to media and social argumentation.", "labels": [], "entities": []}, {"text": "When argumentation involves interactive argument exchange with elements of persuasion, we talk about debating.", "labels": [], "entities": []}, {"text": "In the increasingly popular online debates, such as VBATES, 1 users can en-gage in debates over controversial topics, introduce new arguments or use existing ones.", "labels": [], "entities": [{"text": "VBATES", "start_pos": 52, "end_pos": 58, "type": "DATASET", "confidence": 0.9079333543777466}]}, {"text": "Early computational approaches to argumentation have developed in two branches: logic-based approaches) and argumentative zoning).", "labels": [], "entities": []}, {"text": "The latter aims to recognize argumentative sections of specific purpose in scientific papers, such as goals, related work, or conclusion.", "labels": [], "entities": []}, {"text": "introduced argumentation mining as a research area involved with the automatic extraction of argumentation structure from free text, residing between NLP, argumentation theory, and information retrieval.", "labels": [], "entities": [{"text": "argumentation mining", "start_pos": 11, "end_pos": 31, "type": "TASK", "confidence": 0.9344850778579712}, {"text": "automatic extraction of argumentation structure from free text", "start_pos": 69, "end_pos": 131, "type": "TASK", "confidence": 0.824365995824337}]}, {"text": "Prior work in argumentation mining has focused on official documents, such as legal cases (, or moderated sources, such as debates.", "labels": [], "entities": [{"text": "argumentation mining", "start_pos": 14, "end_pos": 34, "type": "TASK", "confidence": 0.909021407365799}]}, {"text": "However, by far the largest source of opinions are online user discussions: comments on newspaper articles, social networks, blogs, and discussion forums -all argumentation arenas without strict rules.", "labels": [], "entities": []}, {"text": "Despite the fact that the user-generated content is not moderated nor structured, one can often find an abundance of opinions, most of them backed up with arguments.", "labels": [], "entities": []}, {"text": "By analyzing such arguments, we can gain valuable insight into the reasons underpinning users' opinions.", "labels": [], "entities": []}, {"text": "Understanding the reasons has obvious benefits in social opinion mining, with applications ranging from brand analysis to political opinion mining.", "labels": [], "entities": [{"text": "social opinion mining", "start_pos": 50, "end_pos": 71, "type": "TASK", "confidence": 0.7002651989459991}, {"text": "brand analysis", "start_pos": 104, "end_pos": 118, "type": "TASK", "confidence": 0.8095073997974396}, {"text": "political opinion mining", "start_pos": 122, "end_pos": 146, "type": "TASK", "confidence": 0.6767323811848959}]}, {"text": "Inspired by this idea, in this paper we take on the task of argument-based opinion mining.", "labels": [], "entities": [{"text": "argument-based opinion mining", "start_pos": 60, "end_pos": 89, "type": "TASK", "confidence": 0.6219207147757212}]}, {"text": "Instead of merely determining the general opinion or stance of users towards a given topic, in argumentbased opinion mining we wish to determine the arguments on which the users base their stance.", "labels": [], "entities": []}, {"text": "Unlike in argumentation mining, we are not ultimately interested in recovering the argumentation structure.", "labels": [], "entities": [{"text": "argumentation mining", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.9374799132347107}]}, {"text": "Instead, we wish to recognize what arguments the user has used to backup her opinion.", "labels": [], "entities": []}, {"text": "As an example, consider a discussion on the topic \"Should gay marriage be legal?\" and the following comment: Gay marriages must be legal in all 50 states.", "labels": [], "entities": []}, {"text": "A marriage is covenant between 2 people regardless of their genders.", "labels": [], "entities": []}, {"text": "Discrimination against gay marriage is unconstitutional and biased.", "labels": [], "entities": []}, {"text": "Tolerance, education and social justice make our world a better place.", "labels": [], "entities": []}, {"text": "This comment supports the argument \"It is discriminatory to refuse gay couples the right to marry\" and denies the argument \"Marriage should be between a man and a woman\".", "labels": [], "entities": []}, {"text": "The technical challenge here lies in the fact that, unlike in debates or other more formal argumentation sources, the arguments provided by the users, if any, are less formal, ambiguous, vague, implicit, or often simply poorly worded.", "labels": [], "entities": []}, {"text": "In this paper, we make a first step towards argument-based opinion mining from online discussions and introduce the task of argument recognition.", "labels": [], "entities": [{"text": "argument-based opinion mining", "start_pos": 44, "end_pos": 73, "type": "TASK", "confidence": 0.5848120351632436}, {"text": "argument recognition", "start_pos": 124, "end_pos": 144, "type": "TASK", "confidence": 0.7942148745059967}]}, {"text": "We define this task as identifying what arguments, from a predefined set of arguments, have been used in users' comments, and how.", "labels": [], "entities": []}, {"text": "We assume that a topic-dependent set of arguments has been prepared in advance.", "labels": [], "entities": []}, {"text": "Each argument is described with a single phrase or a sentence.", "labels": [], "entities": []}, {"text": "To backup her stance, the user will typically use one or more of the predefined arguments, but in their own wording and with varying degree of explicitness.", "labels": [], "entities": []}, {"text": "The task of argument recognition amounts to matching these arguments to the predefined arguments, which can be either attacked or supported by the comment.", "labels": [], "entities": [{"text": "argument recognition", "start_pos": 12, "end_pos": 32, "type": "TASK", "confidence": 0.7545768916606903}]}, {"text": "Note that the user's comment may by itself be a single argument.", "labels": [], "entities": []}, {"text": "However, we refer to it as comment to emphasize the fact that in general it may contain several arguments as well as non-argumentative text.", "labels": [], "entities": []}, {"text": "The contribution of our work is twofold.", "labels": [], "entities": []}, {"text": "First, we present COMARG, a manually-annotated corpus for argument recognition from online discussions, which we make freely available.", "labels": [], "entities": [{"text": "argument recognition from online discussions", "start_pos": 58, "end_pos": 102, "type": "TASK", "confidence": 0.8579450607299804}]}, {"text": "Secondly, we describe a supervised model for argument recognition based on comment-argument comparison.", "labels": [], "entities": [{"text": "argument recognition", "start_pos": 45, "end_pos": 65, "type": "TASK", "confidence": 0.7684605419635773}]}, {"text": "To address the fact that the arguments expressed in user comments are mostly vague and implicit, we use a series of semantic comment-argument comparison features based on semantic textual similarity (STS) and textual entailment (TE).", "labels": [], "entities": [{"text": "semantic textual similarity (STS)", "start_pos": 171, "end_pos": 204, "type": "METRIC", "confidence": 0.6915393024682999}, {"text": "textual entailment (TE)", "start_pos": 209, "end_pos": 232, "type": "METRIC", "confidence": 0.6155245065689087}]}, {"text": "To this end, we rely on state-of-the-art off-the-shelf STS and TE systems.", "labels": [], "entities": []}, {"text": "We consider different feature subsets and argument recognition tasks of varying difficulty.", "labels": [], "entities": [{"text": "argument recognition", "start_pos": 42, "end_pos": 62, "type": "TASK", "confidence": 0.7384516000747681}]}, {"text": "Depending on task formulation, their performance ranges from 70.5% to 81.8% micro-averaged F1-score.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.8494460582733154}]}, {"text": "Taking into account the difficulty of the task, we believe these results are promising.", "labels": [], "entities": []}, {"text": "In particular, we show that TE features work best when also taking into account the stance of the argument, and that a classifier trained to recognize arguments in one topic can be applied to another one with a decrease in performance of less than 3% F1-score.", "labels": [], "entities": [{"text": "TE", "start_pos": 28, "end_pos": 30, "type": "METRIC", "confidence": 0.7807603478431702}, {"text": "F1-score", "start_pos": 251, "end_pos": 259, "type": "METRIC", "confidence": 0.9987776875495911}]}, {"text": "The rest of the paper is structured as follows.", "labels": [], "entities": []}, {"text": "In the next section we review the related work.", "labels": [], "entities": []}, {"text": "In Section 3 we describe the construction and annotation of the COMARG corpus.", "labels": [], "entities": [{"text": "COMARG corpus", "start_pos": 64, "end_pos": 77, "type": "DATASET", "confidence": 0.9628036916255951}]}, {"text": "Section 4 describes the argument recognition model.", "labels": [], "entities": [{"text": "argument recognition", "start_pos": 24, "end_pos": 44, "type": "TASK", "confidence": 0.8550051748752594}]}, {"text": "In Section 5 we discuss the experimental results.", "labels": [], "entities": []}, {"text": "Section 6 concludes the paper and outlines future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "We consider three formulations of the argument detection task.", "labels": [], "entities": [{"text": "argument detection task", "start_pos": 38, "end_pos": 61, "type": "TASK", "confidence": 0.8676248987515768}]}, {"text": "In the first setting (A-a-N-s-S), we consider the classification of a comment-argument into one of the five labels, i.e., we wish to determine whether an argument has been used, its polarity, as well as the degree of explicitness.", "labels": [], "entities": []}, {"text": "In the second setting (As-N-sS), we conflate the two labels of equal polarity, thus we only consider whether an argument has been used and with what polarity.", "labels": [], "entities": []}, {"text": "In the third setting (A-N-S), we only consider the comment-argument pairs in which arguments are either not used or used explicitly.", "labels": [], "entities": []}, {"text": "This setting is not practically relevant, but we include it for purposes of comparison.", "labels": [], "entities": []}, {"text": "We compare to two baselines: (1) a majority class classifier (MCC), which assigns label N to every instance, and (2) a bag-of-words overlap classifier (BoWO), which uses the word overlap between the comment and the argument as the only feature.", "labels": [], "entities": []}, {"text": "For classification, we use the Support Vector Machine (SVM) algorithm with a Radial Basis Function kernel.", "labels": [], "entities": [{"text": "classification", "start_pos": 4, "end_pos": 18, "type": "TASK", "confidence": 0.9720525145530701}]}, {"text": "In each setting, we train and evaluate the model using nested 5\u00d73 cross-validation.", "labels": [], "entities": []}, {"text": "The hyperparameters C and \u03b3 of the SVM are optimized using grid search.", "labels": [], "entities": []}, {"text": "We rely on the well-   known LibSVM implementation (Chang and Lin, 2011).", "labels": [], "entities": []}, {"text": "shows the micro-averaged F1-score for the three problem formulations, for models trained separately on UGIP and GM topics.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9475415349006653}, {"text": "UGIP", "start_pos": 103, "end_pos": 107, "type": "DATASET", "confidence": 0.8549332618713379}]}, {"text": "The two baselines perform similarly.", "labels": [], "entities": []}, {"text": "The models that use only the STS or the SA features perform similar to the baseline.", "labels": [], "entities": [{"text": "STS", "start_pos": 29, "end_pos": 32, "type": "METRIC", "confidence": 0.9197264909744263}]}, {"text": "The TE model outperforms the baselines in all but one setting and on both topics: the difference ranges from 0.6 to 11.7 percentage points, depending on problem formulation, while the variation between the two topics is negligible.", "labels": [], "entities": [{"text": "TE", "start_pos": 4, "end_pos": 6, "type": "METRIC", "confidence": 0.9082921147346497}]}, {"text": "The STS model does not benefit from adding the SA feature, while the TE model does so in simpler settings (Aa-N-sS and A-N-S), where the average F1-scores increases by about 3 percentage points.", "labels": [], "entities": [{"text": "SA", "start_pos": 47, "end_pos": 49, "type": "METRIC", "confidence": 0.9862624406814575}, {"text": "TE", "start_pos": 69, "end_pos": 71, "type": "METRIC", "confidence": 0.8735619783401489}, {"text": "Aa-N-sS", "start_pos": 107, "end_pos": 114, "type": "METRIC", "confidence": 0.8769770264625549}, {"text": "F1-scores", "start_pos": 145, "end_pos": 154, "type": "METRIC", "confidence": 0.9920267462730408}]}, {"text": "This can be explained by referring to, which shows that even for the attacked arguments (labels A and a) entailment decisions are sometimes positive.", "labels": [], "entities": []}, {"text": "In such cases, the stance alignment feature helps to distinguish between entailment (supported argument) and contradiction (attacked argument).", "labels": [], "entities": [{"text": "stance alignment", "start_pos": 19, "end_pos": 35, "type": "TASK", "confidence": 0.7421776950359344}]}, {"text": "Combining all three feature types gives the best results for the A-a-N-s-S setting and the UGIP topic.", "labels": [], "entities": [{"text": "UGIP topic", "start_pos": 91, "end_pos": 101, "type": "DATASET", "confidence": 0.7631332576274872}]}, {"text": "The above evaluation was carried out in a withintopic setting.", "labels": [], "entities": []}, {"text": "To test how the models perform when applied to comments and arguments from unseen topics, we trained each model on one topic and (we show results only for the two problem formulations of practical interest).", "labels": [], "entities": []}, {"text": "The difference in performance is small (0.7 on average).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: Interannotator agreement on the  COMARG corpus", "labels": [], "entities": [{"text": "COMARG", "start_pos": 43, "end_pos": 49, "type": "DATASET", "confidence": 0.7697505950927734}]}, {"text": " Table 5: Distribution of labels in the COMARG  corpus", "labels": [], "entities": [{"text": "COMARG", "start_pos": 40, "end_pos": 46, "type": "DATASET", "confidence": 0.940760612487793}]}, {"text": " Table 7: Argument recognition F1-score (separate  models for UGIP and GM topics)", "labels": [], "entities": [{"text": "Argument recognition", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.7444603443145752}, {"text": "F1-score", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.6917688846588135}]}, {"text": " Table 8: Argument recognition F1-score on UGIP  and GM topics (cross-topic setting)", "labels": [], "entities": [{"text": "Argument", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9692664742469788}, {"text": "F1-score", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.6090918779373169}, {"text": "UGIP", "start_pos": 43, "end_pos": 47, "type": "DATASET", "confidence": 0.8000384569168091}]}, {"text": " Table 9: Argument recognition F1-score for TE+SA and STS+TE+SA models on UGIP+GM topics", "labels": [], "entities": [{"text": "Argument recognition", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.7968631982803345}, {"text": "F1-score", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.783784031867981}, {"text": "UGIP+GM", "start_pos": 74, "end_pos": 81, "type": "DATASET", "confidence": 0.8765920201937357}]}]}