{"title": [{"text": "Domain Adaptation for Medical Text Translation Using Web Re- sources", "labels": [], "entities": [{"text": "Medical Text Translation", "start_pos": 22, "end_pos": 46, "type": "TASK", "confidence": 0.6258219877878824}]}], "abstractContent": [{"text": "This paper describes adapting statistical machine translation (SMT) systems to medical domain using in-domain and general-domain data as well as web-crawled in-domain resources.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 30, "end_pos": 67, "type": "TASK", "confidence": 0.8004129578669866}]}, {"text": "In order to complement the limited in-domain corpora , we apply domain focused web-crawling approaches to acquire in-domain monolingual data and bilingual lexicon from the Internet.", "labels": [], "entities": []}, {"text": "The collected data is used for adapting the language model and translation model to boost the overall translation quality.", "labels": [], "entities": [{"text": "translation", "start_pos": 63, "end_pos": 74, "type": "TASK", "confidence": 0.9620161056518555}]}, {"text": "Besides, we propose an alternative filtering approach to clean the crawled data and to further optimize the domain-specific SMT system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 124, "end_pos": 127, "type": "TASK", "confidence": 0.9281935095787048}]}, {"text": "We attend the medical summary sentence unconstrained translation task of the Ninth Workshop on Statistical Machine Translation (WMT2014).", "labels": [], "entities": [{"text": "medical summary sentence unconstrained translation task", "start_pos": 14, "end_pos": 69, "type": "TASK", "confidence": 0.6262494027614594}, {"text": "Statistical Machine Translation (WMT2014)", "start_pos": 95, "end_pos": 136, "type": "TASK", "confidence": 0.722385361790657}]}, {"text": "Our systems achieve the second best BLEU scores for Czech-English, fourth for French-English, English-French language pairs and the third best results for reminding pairs.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 36, "end_pos": 40, "type": "METRIC", "confidence": 0.9990492463111877}]}], "introductionContent": [{"text": "In this paper, we report the experiments carried out by the NLP 2 CT Laboratory at University of Macau for WMT2014 medical sentence translation task on six language pairs: Czech-English (cs-en), French-English (fr-en), German-English (de-en) and the reverse direction pairs (i.e., en-cs, en-fr and en-de).", "labels": [], "entities": [{"text": "WMT2014 medical sentence translation task", "start_pos": 107, "end_pos": 148, "type": "TASK", "confidence": 0.8277526497840881}]}, {"text": "As data in specific domain are usually relatively scarce, the use of web resources to complement the training resources provides an effective way to enhance the SMT systems).", "labels": [], "entities": [{"text": "SMT", "start_pos": 161, "end_pos": 164, "type": "TASK", "confidence": 0.9936437606811523}]}, {"text": "In our experiments, we not only use all available training data provided by the WMT2014 standard translation task 1 (generaldomain data) and medical translation task 2 (indomain data), but also acquire addition indomain bilingual translations (i.e. dictionary) and monolingual data from online sources.", "labels": [], "entities": [{"text": "WMT2014", "start_pos": 80, "end_pos": 87, "type": "DATASET", "confidence": 0.8937765955924988}, {"text": "medical translation task", "start_pos": 141, "end_pos": 165, "type": "TASK", "confidence": 0.7558511594931284}]}, {"text": "First of all, we collect the medical terminologies from the web.", "labels": [], "entities": []}, {"text": "This tiny but significant parallel data are helpful to reduce the out-ofvocabulary words (OOVs) in translation models.", "labels": [], "entities": []}, {"text": "In addition, the use of larger language models during decoding is aided by more efficient storage and inference).", "labels": [], "entities": []}, {"text": "Thus, we crawl more in-domain monolingual data from the Internet based on domain focused web-crawling approach.", "labels": [], "entities": []}, {"text": "In order to detect and remove outdomain data from the crawled data, we not only explore text-to-topic classifier, but also propose an alternative filtering approach combined the existing one (text-to-topic classifier) with perplexity.", "labels": [], "entities": []}, {"text": "After carefully pre-processing all the available training data, we apply language model adaptation and translation model adaptation using various kinds of training corpora.", "labels": [], "entities": [{"text": "language model adaptation", "start_pos": 73, "end_pos": 98, "type": "TASK", "confidence": 0.659947524468104}, {"text": "translation model adaptation", "start_pos": 103, "end_pos": 131, "type": "TASK", "confidence": 0.8570874532063802}]}, {"text": "Experimental results show that the presented approaches are helpful to further boost the baseline system.", "labels": [], "entities": []}, {"text": "The reminder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we detail the workflow of web resources acquisition.", "labels": [], "entities": [{"text": "web resources acquisition", "start_pos": 40, "end_pos": 65, "type": "TASK", "confidence": 0.6613201002279917}]}, {"text": "Section 3 describes the pre-processing steps for the corpora.", "labels": [], "entities": []}, {"text": "Section 5 presents the baseline system.", "labels": [], "entities": []}, {"text": "Section 6 reports the experimental results and discussions.", "labels": [], "entities": []}, {"text": "Finally, the submitted systems and the official results are reported in Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "The official medical summary development sets (dev) are used for tuning and evaluating the comparative systems.", "labels": [], "entities": []}, {"text": "The official medical summary test sets (test) are only used in our final submitted systems.", "labels": [], "entities": []}, {"text": "The experiments were carried outwith the Moses 1.0 10 ( . The translation and the re-ordering model utilizes the \"growdiag-final\" symmetrized word-to-word alignments created with MGIZA++ 11 and the training scripts from Moses.", "labels": [], "entities": [{"text": "Moses 1.0 10", "start_pos": 41, "end_pos": 53, "type": "DATASET", "confidence": 0.795206606388092}, {"text": "translation", "start_pos": 62, "end_pos": 73, "type": "TASK", "confidence": 0.9669328927993774}, {"text": "MGIZA++ 11", "start_pos": 179, "end_pos": 189, "type": "DATASET", "confidence": 0.87307075659434}]}, {"text": "A 5-gram LM was trained using the SRILM toolkit), exploiting improved modified Kneser-Ney smoothing, and quantizing both probabilities and back-off weights.", "labels": [], "entities": [{"text": "SRILM toolkit", "start_pos": 34, "end_pos": 47, "type": "DATASET", "confidence": 0.9049681425094604}]}, {"text": "For the log-linear model training, we take the minimum-error-rate training (MERT) method as described in.", "labels": [], "entities": [{"text": "minimum-error-rate training (MERT)", "start_pos": 47, "end_pos": 81, "type": "METRIC", "confidence": 0.8492428064346313}]}, {"text": "In the following sub-sections, we describe the results of baseline systems, which are trained on the official corpora.", "labels": [], "entities": []}, {"text": "We also present the enhanced systems that make use of the webcrawled bilingual dictionary and monolingual data as the additional training resources.", "labels": [], "entities": []}, {"text": "Two variants of enhanced system are constructed based on different filtering criteria.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Size of terms used for topic definition.", "labels": [], "entities": [{"text": "topic definition", "start_pos": 33, "end_pos": 49, "type": "TASK", "confidence": 0.8084936439990997}]}, {"text": " Table 3: BLEU scores of baseline systems for  different language pairs.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9985047578811646}]}, {"text": " Table 2: Statistics summary of corpora after pre-processing.", "labels": [], "entities": []}, {"text": " Table 4: Evaluation results for systems that  trained on relevance-score-filtered documents.", "labels": [], "entities": []}, {"text": " Table 5: Evaluation results for systems that  trained on combination filtering approach.", "labels": [], "entities": []}]}