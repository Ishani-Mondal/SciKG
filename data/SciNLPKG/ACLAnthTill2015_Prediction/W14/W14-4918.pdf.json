{"title": [{"text": "Annotating Multiparty Discourse: Challenges for Agreement Metrics", "labels": [], "entities": []}], "abstractContent": [{"text": "To computationally model discourse phenomena such as argumentation we need corpora with reliable annotation of the phenomena understudy.", "labels": [], "entities": []}, {"text": "Annotating complex discourse phenomena poses two challenges: fuzziness of unit boundaries and the need for multiple annotators.", "labels": [], "entities": [{"text": "Annotating complex discourse phenomena", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.8846620470285416}]}, {"text": "We show that current metrics for inter-annotator agreement (IAA) such as P/R/F1 and Krippendorff's \u03b1 provide inconsistent results for the same text.", "labels": [], "entities": [{"text": "P/R/F1", "start_pos": 73, "end_pos": 79, "type": "METRIC", "confidence": 0.7087017059326172}]}, {"text": "In addition, IAA metrics do not tell us what parts of a text are easier or harder for human judges to annotate and so do not provide sufficiently specific information for evaluating systems that automatically identify discourse units.", "labels": [], "entities": []}, {"text": "We propose a hierarchical clustering approach that aggregates overlapping text segments of text identified by multiple annotators; the more annotators who identify a text segment, the easier we assume that the text segment is to annotate.", "labels": [], "entities": []}, {"text": "The clusters make it possible to quantify the extent of agreement judges show about text segments; this information can be used to assess the output of systems that automatically identify discourse units.", "labels": [], "entities": []}], "introductionContent": [{"text": "Annotation of discourse typically involves three subtasks: segmentation (identification of discourse units, including their boundaries), segment classification (labeling the role of discourse units) and relation identification (indicating the link between the discourse units)).", "labels": [], "entities": [{"text": "Annotation of discourse", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.8992202281951904}, {"text": "segment classification", "start_pos": 137, "end_pos": 159, "type": "TASK", "confidence": 0.7506148815155029}, {"text": "relation identification", "start_pos": 203, "end_pos": 226, "type": "TASK", "confidence": 0.7506978511810303}]}, {"text": "The difficulty of achieving an Inter-Annotator Agreement (IAA) of .80, which is generally accepted as good agreement, is compounded in studies of discourse annotations since annotators must unitize, i.e. identify the boundaries of discourse units.", "labels": [], "entities": [{"text": "Inter-Annotator Agreement (IAA)", "start_pos": 31, "end_pos": 62, "type": "METRIC", "confidence": 0.8736263632774353}]}, {"text": "The inconsistent assignment of boundaries in annotation of discourse has been noted at least since who observed that although annotators tended to identify essentially the same units, the boundaries differed slightly.", "labels": [], "entities": []}, {"text": "The need for annotators to identify the boundaries of text segments makes measurement of IAA more difficult because standard coefficients such as \u03ba assume that the units to be coded have been identified before the coding begins.", "labels": [], "entities": [{"text": "IAA", "start_pos": 89, "end_pos": 92, "type": "TASK", "confidence": 0.4831673800945282}]}, {"text": "A second challenge for measuring IAA for discourse annotation is associated with larger numbers of annotators.", "labels": [], "entities": [{"text": "IAA", "start_pos": 33, "end_pos": 36, "type": "METRIC", "confidence": 0.5509012937545776}, {"text": "discourse annotation", "start_pos": 41, "end_pos": 61, "type": "TASK", "confidence": 0.7510966956615448}]}, {"text": "Because of the many ways that ideas are expressed inhuman language, using multiple annotators to study discourse phenomena is important.", "labels": [], "entities": []}, {"text": "Such an approach capitalizes on the aggregated intuitions of multiple coders to overcome the potential biases of anyone coder and helps identify limitations in the coding scheme, thus adding to the reliability and validity of the annotation study.", "labels": [], "entities": [{"text": "reliability", "start_pos": 198, "end_pos": 209, "type": "METRIC", "confidence": 0.9614407420158386}, {"text": "validity", "start_pos": 214, "end_pos": 222, "type": "METRIC", "confidence": 0.9400370717048645}]}, {"text": "The more annotators, however, the harder it is to achieve an IAA of .80 ().", "labels": [], "entities": [{"text": "IAA", "start_pos": 61, "end_pos": 64, "type": "METRIC", "confidence": 0.984107255935669}]}, {"text": "What to annotate also depends, among other characteristics, on the phenomenon of interest, the text being annotated, the quality of the annotation scheme and the effectiveness of training.", "labels": [], "entities": []}, {"text": "But even if these are excellent, there is natural variability inhuman judgment fora task that involves subtle distinctions about which competent coders disagree.", "labels": [], "entities": []}, {"text": "An accurate computational model should reflect this variability ().", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 3: Callouts: EM P/R/F1 over 5 threads", "labels": [], "entities": [{"text": "EM P/R", "start_pos": 20, "end_pos": 26, "type": "METRIC", "confidence": 0.8540495485067368}, {"text": "F1", "start_pos": 27, "end_pos": 29, "type": "METRIC", "confidence": 0.5881773829460144}]}, {"text": " Table 4: Callouts: OM P/R/F1 over 5 threads", "labels": [], "entities": [{"text": "OM P/R", "start_pos": 20, "end_pos": 26, "type": "METRIC", "confidence": 0.7297928184270859}, {"text": "F1", "start_pos": 27, "end_pos": 29, "type": "METRIC", "confidence": 0.5696324706077576}]}, {"text": " Table 5: Targets: EM P/R/F1 over 5 threads", "labels": [], "entities": [{"text": "F1", "start_pos": 26, "end_pos": 28, "type": "METRIC", "confidence": 0.7796197533607483}]}, {"text": " Table 6: Targets: OM P/R/F1 over 5 threads", "labels": [], "entities": [{"text": "F1", "start_pos": 26, "end_pos": 28, "type": "METRIC", "confidence": 0.7604519128799438}]}, {"text": " Table 7: F1 and \u03b1 for all 5 threads", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9994895458221436}]}, {"text": " Table 11: A cluster with 2 cores, each selected by 2 judges", "labels": [], "entities": []}, {"text": " Table 12: Callouts: Clusters before splitting process", "labels": [], "entities": []}]}