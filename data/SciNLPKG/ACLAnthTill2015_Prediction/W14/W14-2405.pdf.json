{"title": [{"text": "A Deep Architecture for Semantic Parsing", "labels": [], "entities": [{"text": "Semantic Parsing", "start_pos": 24, "end_pos": 40, "type": "TASK", "confidence": 0.7263392210006714}]}], "abstractContent": [{"text": "Many successful approaches to semantic parsing build on top of the syntactic analysis of text, and make use of distribu-tional representations or statistical models to match parses to ontology-specific queries.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 30, "end_pos": 46, "type": "TASK", "confidence": 0.7262238562107086}]}, {"text": "This paper presents a novel deep learning architecture which provides a semantic parsing system through the union of two neural models of language semantics.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 72, "end_pos": 88, "type": "TASK", "confidence": 0.7477658689022064}]}, {"text": "It allows for the generation of ontology-specific queries from natural language statements and questions without the need for parsing, which makes it especially suitable to grammatically mal-formed or syntactically atypical text, such as tweets, as well as permitting the development of semantic parsers for resource-poor languages.", "labels": [], "entities": []}], "introductionContent": [{"text": "The ubiquity of always-online computers in the form of smartphones, tablets, and notebooks has boosted the demand for effective question answering systems.", "labels": [], "entities": [{"text": "question answering", "start_pos": 128, "end_pos": 146, "type": "TASK", "confidence": 0.8948206305503845}]}, {"text": "This is exemplified by the growing popularity of products like Apple's Siri or Google's Google Now services.", "labels": [], "entities": []}, {"text": "In turn, this creates the need for increasingly sophisticated methods for semantic parsing.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 74, "end_pos": 90, "type": "TASK", "confidence": 0.8297015130519867}]}, {"text": "Recent work, inter alia) has answered this call by progressively moving away from strictly rule-based semantic parsing, towards the use of distributed representations in conjunction with traditional grammatically-motivated re-write rules.", "labels": [], "entities": [{"text": "rule-based semantic parsing", "start_pos": 91, "end_pos": 118, "type": "TASK", "confidence": 0.6726716160774231}]}, {"text": "This paper seeks to extend this line of thinking to its logical conclusion, by providing the first (to our knowledge) entirely distributed neural semantic generative parsing model.", "labels": [], "entities": [{"text": "distributed neural semantic generative parsing", "start_pos": 127, "end_pos": 173, "type": "TASK", "confidence": 0.7472358286380768}]}, {"text": "It does so by adapting deep learning methods from related work in sentiment analysis, document classification, frame-semantic parsing (, and machine translation (, inter alia, combining two empirically successful deep learning models to form anew architecture for semantic parsing.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 66, "end_pos": 84, "type": "TASK", "confidence": 0.9389565885066986}, {"text": "document classification", "start_pos": 86, "end_pos": 109, "type": "TASK", "confidence": 0.7800311744213104}, {"text": "frame-semantic parsing", "start_pos": 111, "end_pos": 133, "type": "TASK", "confidence": 0.7229806035757065}, {"text": "machine translation", "start_pos": 141, "end_pos": 160, "type": "TASK", "confidence": 0.7665789723396301}, {"text": "semantic parsing", "start_pos": 264, "end_pos": 280, "type": "TASK", "confidence": 0.7412490248680115}]}, {"text": "The structure of this short paper is as follows.", "labels": [], "entities": []}, {"text": "We first provide a brief overview of the background literature this model builds on in \u00a72.", "labels": [], "entities": []}, {"text": "In \u00a73, we begin by introducing two deep learning models with different aims, namely the joint learning of embeddings in parallel corpora, and the generation of strings of a language conditioned on a latent variable, respectively.", "labels": [], "entities": []}, {"text": "We then discuss how both models can be combined and jointly trained to form a deep learning model supporting the generation of knowledgebase queries from natural language questions.", "labels": [], "entities": [{"text": "generation of knowledgebase queries from natural language questions", "start_pos": 113, "end_pos": 180, "type": "TASK", "confidence": 0.6540163569152355}]}, {"text": "Finally, in \u00a74 we conclude by discussing planned experiments and the data requirements to effectively train this model.", "labels": [], "entities": []}], "datasetContent": [{"text": "The particular training procedure for the model described in this paper requires aligned question/knowledgebase query pairs.", "labels": [], "entities": []}, {"text": "There exist some small corpora that could be used for this task.", "labels": [], "entities": []}, {"text": "In order to scale training beyond these small corpora, we hypothesise that larger amounts of (potentially noisy) training data could be obtained using a boot-strapping technique similar to.", "labels": [], "entities": []}, {"text": "To evaluate this model, we will follow the experimental setup of.", "labels": [], "entities": []}, {"text": "With the provisio that the model can generate freebase queries correctly, further work will seek to determine whether this architecture can generate other structured formal language expressions, such as lambda expressions for use in textual entailement tasks.", "labels": [], "entities": []}], "tableCaptions": []}