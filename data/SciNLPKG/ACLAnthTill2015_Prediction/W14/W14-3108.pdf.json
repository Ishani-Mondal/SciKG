{"title": [{"text": "MUCK: A toolkit for extracting and visualizing semantic dimensions of large text collections", "labels": [], "entities": [{"text": "MUCK", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.7704278826713562}]}], "abstractContent": [{"text": "Users with large text collections are often faced with one of two problems; either they wish to retrieve a semantically-relevant subset of data from the collection for further scrutiny (needle-in-a-haystack) or they wish to glean a high-level understanding of how a subset compares to the parent corpus in the context of afore-mentioned semantic dimensions (forest-for-the-trees).", "labels": [], "entities": []}, {"text": "In this paper, I describe MUCK 1 , an open-source toolkit that addresses both of these problems through a distributed text processing engine with an interactive visualization interface.", "labels": [], "entities": []}], "introductionContent": [{"text": "As gathering large text collections grows increasingly feasible for non-technical users, individuals such as journalists, marketing/communications analysts, and social scientists are accumulating vast quantities of documents in order to address key strategy or research questions.", "labels": [], "entities": []}, {"text": "But these groups often lack the technical skills to work with large text collections, in that the conventional approaches they employ (content analysis and individual document scrutiny) are not suitable for the scale of the data they have gathered.", "labels": [], "entities": [{"text": "content analysis", "start_pos": 135, "end_pos": 151, "type": "TASK", "confidence": 0.7105926275253296}]}, {"text": "Thus, users require tools with the capability to filter out irrelevant documents while drilling-down to the documents that they are most interested in investigating with closer scrutiny.", "labels": [], "entities": []}, {"text": "Furthermore, they require the capability to then evaluate their subset in context, as the contrast in attributes between their subset and the full corpora can often address many relevant questions.", "labels": [], "entities": []}, {"text": "This paper introduces a work-in-progress: the development of a toolkit that aids non-technical Mechanical Understanding of Contextual Knowledge users of large text collections by combining semantic search and semantic visualization methods.", "labels": [], "entities": [{"text": "Mechanical Understanding of Contextual Knowledge users of large text collections", "start_pos": 95, "end_pos": 175, "type": "TASK", "confidence": 0.7702702522277832}]}, {"text": "The purpose of this toolkit is two-fold: first, to ease the technical burden of working with largescale text collections by leveraging semantic information for the purposes of filtering a large collection of text down to the select sample documents that matter most to the user; second, to allow the user to visually explore semantic attributes of their subset in comparison to the rest of the text collection.", "labels": [], "entities": []}, {"text": "Thus, this toolkit comprises two components: 1.", "labels": [], "entities": []}, {"text": "a distributed text processing engine that decreases the cost of annotating massive quantities of text data for natural language information 2.", "labels": [], "entities": []}, {"text": "an interactive visualization interface that enables exploration of the collection along semantic dimensions, which then affords subsequent document selection and subset-tocorpora comparison The text processing engine is extensible, enabling the future development of plug-ins to allow for tasks beyond the included natural language processing tasks, such that future users can embed any sentence-or document-level task to their processing pipeline.", "labels": [], "entities": []}, {"text": "The visualization interface is built upon search engine technologies to decrease search result latency to user requests, enabling a high level of interactivity.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}