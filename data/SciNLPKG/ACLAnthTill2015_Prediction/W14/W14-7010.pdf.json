{"title": [{"text": "Consistent Improvement in Translation Quality of Chinese-Japanese Technical Texts by Adding Additional Quasi-parallel Training Data", "labels": [], "entities": [{"text": "Translation", "start_pos": 26, "end_pos": 37, "type": "TASK", "confidence": 0.9674738645553589}]}], "abstractContent": [{"text": "Bilingual parallel corpora are an extremely important resource as they are typically used in data-driven machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 105, "end_pos": 124, "type": "TASK", "confidence": 0.6941934674978256}]}, {"text": "There already exist many freely available corpora for European languages, but almost none between Chinese and Japanese.", "labels": [], "entities": []}, {"text": "The constitution of large bilingual corpora is a problem for less documented language pairs.", "labels": [], "entities": []}, {"text": "We construct a quasi-parallel corpus automatically by using analogical associations based on certain number of parallel corpus and a small number of mono-lingual data.", "labels": [], "entities": []}, {"text": "Furthermore, in SMT experiments performed on Chinese-Japanese, by adding this kind of data into the baseline training corpus, on the same test set, the evaluation scores of the translation results we obtained were significantly or slightly improved over the baseline systems.", "labels": [], "entities": [{"text": "SMT", "start_pos": 16, "end_pos": 19, "type": "TASK", "confidence": 0.9940478801727295}]}], "introductionContent": [{"text": "Bilingual corpora are an essential resource for current SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 56, "end_pos": 59, "type": "TASK", "confidence": 0.9940529465675354}]}, {"text": "So as to enlarge such corpora, technology research has been done in extracting parallel sentences from existing non-parallel corpora.", "labels": [], "entities": []}, {"text": "The approaches and difficulties depend on the parallelness of the given bilingual parallel corpus.", "labels": [], "entities": []}, {"text": "give a detailed description of the types of non-parallel corpora.", "labels": [], "entities": []}, {"text": "They proposed a completely unsupervised method for mining parallel sentences from quasi-comparable bilingual texts which include both in-topic and off-topic documents.", "labels": [], "entities": []}, {"text": "proposed a novel method of classifier training and testing that simulates the real parallel sentence extraction process.", "labels": [], "entities": [{"text": "classifier training", "start_pos": 27, "end_pos": 46, "type": "TASK", "confidence": 0.927909642457962}, {"text": "sentence extraction", "start_pos": 92, "end_pos": 111, "type": "TASK", "confidence": 0.7549427449703217}]}, {"text": "They used linguistic knowledge of Chinese character features.", "labels": [], "entities": []}, {"text": "Their approach improved in several aspects and worked well for extracting parallel sentences from quasi-comparable corpora.", "labels": [], "entities": [{"text": "extracting parallel sentences from quasi-comparable corpora", "start_pos": 63, "end_pos": 122, "type": "TASK", "confidence": 0.8300999701023102}]}, {"text": "Their experimental results on parallel sentence extraction from quasi-comparable corpora indicated that their proposed system performs significantly better than previous studies.", "labels": [], "entities": [{"text": "parallel sentence extraction from quasi-comparable corpora", "start_pos": 30, "end_pos": 88, "type": "TASK", "confidence": 0.7471272001663843}]}, {"text": "There also exist some works on extracting parallel parallel sentences from comparable corpora, such as Wikipedia.", "labels": [], "entities": [{"text": "Wikipedia", "start_pos": 103, "end_pos": 112, "type": "DATASET", "confidence": 0.9646478891372681}]}, {"text": "include features which make use of the additional annotation given by Wikipedia, and features using an automatically induced lexicon model.", "labels": [], "entities": []}, {"text": "In this paper, we propose to construct a bilingual corpus of quasi-parallel sentences automatically.", "labels": [], "entities": []}, {"text": "This is different from parallel or comparable or quasi-comparable corpora.", "labels": [], "entities": []}, {"text": "A quasi-parallel corpus contains aligned sentence pairs that are translations to each other to a certain extent.", "labels": [], "entities": []}, {"text": "The method relies on a certain number of existing parallel sentences and a small number of unaligned, unrelated, monolingual sentences.", "labels": [], "entities": []}, {"text": "To construct the quasi-parallel corpus, analogical associations captured by analogical clusters are used.", "labels": [], "entities": []}, {"text": "The motivation is that the construction of large bilingual corpora is a problem for less-resourced language pairs, but it is to be noticed that the monolingual data are easier to access in large amounts.", "labels": [], "entities": []}, {"text": "The languages that we tackle in this paper are: Chinese and Japanese.", "labels": [], "entities": []}, {"text": "Our approach leverages Chinese and Japanese monolingual data collected from the Web by clustering and grouping these sentences using analogical associations.", "labels": [], "entities": []}, {"text": "Our clusters can be considered as rewriting models for new sentence generation.", "labels": [], "entities": [{"text": "sentence generation", "start_pos": 59, "end_pos": 78, "type": "TASK", "confidence": 0.7443288564682007}]}, {"text": "We generate new sentences using these rewriting models starting from seed sentences from the monolingual part of the existing parallel corpus we used, and filter out dubious newly over-generated sentences.", "labels": [], "entities": []}, {"text": "Finally, we extract newly generated sentences and assess the strength of translation relations between them based on the similarity, across languages, between the clusters they were generated from.", "labels": [], "entities": []}, {"text": "2 Chinese and Japanese Linguistic Resources", "labels": [], "entities": []}], "datasetContent": [{"text": "In each language, independently, we also construct analogical clusters from the unrelated monolingual data.", "labels": [], "entities": []}, {"text": "The number of unique sentences used is 70,000 for both languages.", "labels": [], "entities": []}, {"text": "summarizes some statistics on the clusters produced.", "labels": [], "entities": []}, {"text": "Chinese Japanese # of different sentences 70,000 70,000 # of clusters 23,182 21,975: Statistics on the Chinese and Japanese clusters constructed from our unrelated monolingual data independently in each language.", "labels": [], "entities": []}, {"text": "Generation and Filtering by N-sequences For the generation of new sentences, we make use of the clusters we obtained from the experiments in Section 3.2 as rewriting models.", "labels": [], "entities": []}, {"text": "The seed sentences as input data for new sentences generation are the unique Chinese and Japanese short sentences from the 103,629 ASPEC-JC parallel sentences (less than 30 characters).", "labels": [], "entities": [{"text": "ASPEC-JC parallel sentences", "start_pos": 131, "end_pos": 158, "type": "TASK", "confidence": 0.6362324357032776}]}, {"text": "In this experiment, we generated new sentences with each pair of sentences in clusters for Chinese and Japanese respectively.", "labels": [], "entities": []}, {"text": "gives the statistics for new sentence generation.", "labels": [], "entities": [{"text": "sentence generation", "start_pos": 29, "end_pos": 48, "type": "TASK", "confidence": 0.7235645651817322}]}, {"text": "To filter out invalid and grammatically incorrect sentences and keep only well-formed sentences with high fluency of expression and adequacy of meaning, we eliminate any sentence that contains an N-sequence of a given length unseen in the reference corpus.", "labels": [], "entities": []}, {"text": "This technique to assess the quality of outputs of NLP systems has been used in previous works).", "labels": [], "entities": []}, {"text": "In our experiment, we introduced begin/end markers to make sure that the beginning and the end of a sentence are also correct.", "labels": [], "entities": []}, {"text": "The best quality was obtained for the values N=6 for Chinese and N=7 for Japanese with the size of reference corpus (about 1,700,000 monolingual data for both Chinese and Japanese).", "labels": [], "entities": []}, {"text": "Quality assessment was performed by extracting a sample of 1,000 sentences randomly and checking manually by native speakers.", "labels": [], "entities": []}, {"text": "The grammatical quality was at least 96%.", "labels": [], "entities": []}, {"text": "This means that 96% of the Chinese and Japanese sentences maybe considered as grammatically correct.", "labels": [], "entities": []}, {"text": "For new valid sentences, we remember their corresponding seed sentences and the cluster they were generated from.: Statistics on the quasi-parallel corpus deducing.", "labels": [], "entities": []}, {"text": "To assess the contribution of the generated quasiparallel corpus, we propose to compare two SMT systems.", "labels": [], "entities": [{"text": "SMT", "start_pos": 92, "end_pos": 95, "type": "TASK", "confidence": 0.9891266226768494}]}, {"text": "The first one is constructed using the initial given ASPEC-JC parallel corpus.", "labels": [], "entities": [{"text": "ASPEC-JC parallel corpus", "start_pos": 53, "end_pos": 77, "type": "DATASET", "confidence": 0.6857567032178243}]}, {"text": "The second one adds the additional quasi-parallel corpus obtained using analogical associations and analogical clusters.", "labels": [], "entities": []}, {"text": "Baseline: The statistics of the data used in the experiments are given in (left).", "labels": [], "entities": []}, {"text": "The training corpus consists of 672,315 sentences of initial Chinese-Japanese parallel corpus.", "labels": [], "entities": []}, {"text": "The tuning set is 2,090 sentences from the ASPEC-JC.dev corpus, and 2,107 sentences also from the ASPEC-JC.test corpus were used for testing.", "labels": [], "entities": [{"text": "ASPEC-JC.dev corpus", "start_pos": 43, "end_pos": 62, "type": "DATASET", "confidence": 0.9522538185119629}, {"text": "ASPEC-JC.test corpus", "start_pos": 98, "end_pos": 118, "type": "DATASET", "confidence": 0.9228581786155701}]}, {"text": "We perform all experiments using the standard GIZA++/MOSES pipeline.", "labels": [], "entities": [{"text": "GIZA++/MOSES pipeline", "start_pos": 46, "end_pos": 67, "type": "DATASET", "confidence": 0.8184083849191666}]}, {"text": "Adding Additional Quasi-parallel Corpus: The statistics of the data used in this second setting are given in (right).", "labels": [], "entities": []}, {"text": "The training corpus is made of sentences, i.e., the combination of the initial ChineseJapanese parallel corpus used in the baseline and the quasi-parallel corpus.", "labels": [], "entities": []}, {"text": "Experimental Results: give the evaluation results.", "labels": [], "entities": []}, {"text": "We use the standard metrics BLEU (), NIST), WER (), TER () and RIBES ().", "labels": [], "entities": [{"text": "BLEU", "start_pos": 28, "end_pos": 32, "type": "METRIC", "confidence": 0.9983395338058472}, {"text": "NIST", "start_pos": 37, "end_pos": 41, "type": "METRIC", "confidence": 0.5398215055465698}, {"text": "WER", "start_pos": 44, "end_pos": 47, "type": "METRIC", "confidence": 0.9935310482978821}, {"text": "TER", "start_pos": 52, "end_pos": 55, "type": "METRIC", "confidence": 0.9965829253196716}, {"text": "RIBES", "start_pos": 63, "end_pos": 68, "type": "METRIC", "confidence": 0.9953694939613342}]}, {"text": "As shows, significant improvement over the baseline is obtained by adding the quasi-parallel generated data based on the Moses version 1.0, and shows a slightly improvement over the baseline is obtained by adding the quasi-parallel generated data based on the Moses version 2.1.1.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics on the ASPEC Chinese-Japanese corpus used for training (672,315 sentences). Seg- mentation tools: urheen for Chinese and mecab for Japanese.", "labels": [], "entities": [{"text": "ASPEC Chinese-Japanese corpus", "start_pos": 28, "end_pos": 57, "type": "DATASET", "confidence": 0.6903574665387472}]}, {"text": " Table 2: Statistics on the cleaned Chinese and Japanese monolingual short sentences. Segmentation  tools: urheen for Chinese and mecab for Japanese.", "labels": [], "entities": []}, {"text": " Table 4: Statistics on new sentence generation in Chinese and Japanese. Q is the quality of the new  candidate sentences or new valid sentences after filtering.", "labels": [], "entities": [{"text": "Q", "start_pos": 73, "end_pos": 74, "type": "METRIC", "confidence": 0.9571534991264343}]}, {"text": " Table 5: Statistics on the quasi-parallel corpus deducing.", "labels": [], "entities": [{"text": "quasi-parallel corpus deducing", "start_pos": 28, "end_pos": 58, "type": "TASK", "confidence": 0.6295855442682902}]}, {"text": " Table 6: Statistics on the Chinese-Japanese corpus used for the training, tuning, and test sets in base- line (left) and baseline + quasi-parallel data (right). The tuning and testing sets are the same in both  experiments. Segmentation tools: urheen for Chinese and Mecab for Japanese.", "labels": [], "entities": []}, {"text": " Table 8: Evaluation results for Chinese-Japanese translation across two SMT systems (baseline and  baseline + additional quasi-parallel data), Moses version: 2.1.1, segmentation tools: Urheen and Mecab.", "labels": [], "entities": [{"text": "SMT", "start_pos": 73, "end_pos": 76, "type": "TASK", "confidence": 0.9770234227180481}]}, {"text": " Table 9: Evaluation results for Chinese-Japanese translation across two SMT systems (baseline and  baseline + additional quasi-parallel data), Moses version:1.0, segmentation tools: Kytea.", "labels": [], "entities": [{"text": "SMT", "start_pos": 73, "end_pos": 76, "type": "TASK", "confidence": 0.976598858833313}, {"text": "Kytea", "start_pos": 183, "end_pos": 188, "type": "DATASET", "confidence": 0.8946766257286072}]}, {"text": " Table 10: Evaluation results for Chinese-Japanese translation across two SMT systems (baseline and  baseline + additional quasi-parallel data), Moses version: 2.1.1, segmentation tools: Kytea.", "labels": [], "entities": [{"text": "SMT", "start_pos": 74, "end_pos": 77, "type": "TASK", "confidence": 0.9765802025794983}, {"text": "Kytea", "start_pos": 187, "end_pos": 192, "type": "DATASET", "confidence": 0.9046149253845215}]}]}