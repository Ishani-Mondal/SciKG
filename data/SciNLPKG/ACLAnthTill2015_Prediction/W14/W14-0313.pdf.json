{"title": [{"text": "Online Word Alignment for Online Adaptive Machine Translation", "labels": [], "entities": [{"text": "Online Word Alignment", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.5691823661327362}, {"text": "Online Adaptive Machine Translation", "start_pos": 26, "end_pos": 61, "type": "TASK", "confidence": 0.6383346170186996}]}], "abstractContent": [{"text": "A hot task in the Computer Assisted Translation scenario is the integration of Machine Translation (MT) systems that adapt sentence after sentence to the post-edits made by the translators.", "labels": [], "entities": [{"text": "Computer Assisted Translation", "start_pos": 18, "end_pos": 47, "type": "TASK", "confidence": 0.6748210589090983}, {"text": "Machine Translation (MT)", "start_pos": 79, "end_pos": 103, "type": "TASK", "confidence": 0.8527572989463806}]}, {"text": "A main role in the MT online adaptation process is played by the information extracted from source and post-edited sentences, which in turn depends on the quality of the word alignment between them.", "labels": [], "entities": [{"text": "MT online adaptation", "start_pos": 19, "end_pos": 39, "type": "TASK", "confidence": 0.9428931077321371}]}, {"text": "In fact, this step is particularly crucial when the user corrects the MT output with words for which the system has no prior information.", "labels": [], "entities": [{"text": "MT", "start_pos": 70, "end_pos": 72, "type": "TASK", "confidence": 0.9306800365447998}]}, {"text": "In this paper, we first discuss the application of popular state-of-the-art word aligners to this scenario and reveal their poor performance in aligning unknown words.", "labels": [], "entities": []}, {"text": "Then, we propose a fast procedure to refine their outputs and to get more reliable and accurate alignments for unknown words.", "labels": [], "entities": []}, {"text": "We evaluate our enhanced word-aligner on three language pairs, namely English-Italian, English-French, and English-Spanish, showing a consistent improvement in aligning unknown words up to 10% absolute F-measure.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 202, "end_pos": 211, "type": "METRIC", "confidence": 0.937926173210144}]}], "introductionContent": [{"text": "In the adaptive MT the goal is to let the MT system take as soon and as much as possible advantage of user feedback, in order to learn from corrections and to hence avoid repeating the same mistakes in future sentences.", "labels": [], "entities": [{"text": "MT", "start_pos": 16, "end_pos": 18, "type": "TASK", "confidence": 0.9264219403266907}, {"text": "MT", "start_pos": 42, "end_pos": 44, "type": "TASK", "confidence": 0.9456047415733337}]}, {"text": "A typical application scenario is the usage by a professional translator of a Computer Assisted Translation (CAT) tool enhanced with a SMT system.", "labels": [], "entities": [{"text": "Computer Assisted Translation (CAT)", "start_pos": 78, "end_pos": 113, "type": "TASK", "confidence": 0.8289418419202169}, {"text": "SMT", "start_pos": 135, "end_pos": 138, "type": "TASK", "confidence": 0.9617633819580078}]}, {"text": "For each input sentence, first the translator receives one or more translation suggestions from either a Translation Memory or a SMT system, then (s)he chooses which suggestion is more useful, and finally (s)he creates an approved translation by post-editing.", "labels": [], "entities": []}, {"text": "The pair of input sentence and post-edit is a valuable feedback to improve the quality of next suggestions.", "labels": [], "entities": []}, {"text": "While the sentence pair is trivially added to the Translation Memory, how to exploit it for improving the SMT system is far to be a solved problem, but rather is a hot and quite recent topic in the MT community.", "labels": [], "entities": [{"text": "Translation Memory", "start_pos": 50, "end_pos": 68, "type": "TASK", "confidence": 0.8878191709518433}, {"text": "SMT", "start_pos": 106, "end_pos": 109, "type": "TASK", "confidence": 0.9861897826194763}, {"text": "MT", "start_pos": 198, "end_pos": 200, "type": "TASK", "confidence": 0.9804081916809082}]}, {"text": "In online MT adaptation specific issues have to be addressed, which distinguish it from the more standard and investigated task of domain adaptation.", "labels": [], "entities": [{"text": "MT adaptation", "start_pos": 10, "end_pos": 23, "type": "TASK", "confidence": 0.9835790991783142}, {"text": "domain adaptation", "start_pos": 131, "end_pos": 148, "type": "TASK", "confidence": 0.7353589534759521}]}, {"text": "First of all, the SMT system should adapt very quickly, because the time between two consecutive requests are usually short, and very precisely, because the translator is annoyed by correcting the same error several time.", "labels": [], "entities": [{"text": "SMT", "start_pos": 18, "end_pos": 21, "type": "TASK", "confidence": 0.9931824803352356}]}, {"text": "Then, a crucial point is which and how information is extracted from the feedback, and how it is exploited to update the SMT system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 121, "end_pos": 124, "type": "TASK", "confidence": 0.9909572601318359}]}, {"text": "Finally, model updating relies on a little feedback consisting of just one sentence pair.", "labels": [], "entities": []}, {"text": "In this work we focus on the word alignment task which is the first and most important step in extracting information from the given source and its corresponding post-edit.", "labels": [], "entities": [{"text": "word alignment task", "start_pos": 29, "end_pos": 48, "type": "TASK", "confidence": 0.8398469885190328}]}, {"text": "In particular, we are interested in the cases where the given sentence pairs contain new words, for which no prior information is available.", "labels": [], "entities": []}, {"text": "This is an important and challenging problem in the online scenario, in which the user interacts with the system and expects that it learns from the previous corrections and does not repeat the same errors again and again.", "labels": [], "entities": []}, {"text": "Unfortunately, state-of-the-art word-aligners show poor generalization capability and are prone to errors when infrequent or new words occur in the sentence pair.", "labels": [], "entities": []}, {"text": "Word alignment errors at this stage could cause the extraction of wrong phrase pairs, i.e. wrong translation alternatives, which can lead in producing wrong translations for those words, if they appear in the following sentences.", "labels": [], "entities": [{"text": "Word alignment", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.6671235263347626}]}, {"text": "Our investigation focuses on how to quickly build a highly precise word alignment from a source sentence and its translation.", "labels": [], "entities": []}, {"text": "Moreover, we are interested in improving the word alignment of unknown terms, i.e. not present in the training data, because they are one of the most important source of errors in model updating.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 45, "end_pos": 59, "type": "TASK", "confidence": 0.6538019627332687}]}, {"text": "Although we are working in the online MT adaptation framework, our proposal is worthwhile per se; indeed, having an improved and fast word aligner can be useful for other interesting tasks, like for instance terminology extraction, translation error detection, and pivot translation.", "labels": [], "entities": [{"text": "MT adaptation", "start_pos": 38, "end_pos": 51, "type": "TASK", "confidence": 0.9763635098934174}, {"text": "terminology extraction", "start_pos": 208, "end_pos": 230, "type": "TASK", "confidence": 0.8611391484737396}, {"text": "translation error detection", "start_pos": 232, "end_pos": 259, "type": "TASK", "confidence": 0.9187542994817098}, {"text": "pivot translation", "start_pos": 265, "end_pos": 282, "type": "TASK", "confidence": 0.8093298375606537}]}, {"text": "In Section 2 we report on some recent approaches aiming at improving word alignment.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 69, "end_pos": 83, "type": "TASK", "confidence": 0.7923258244991302}]}, {"text": "In Section 3, we describe three widely used toolkits, highlight their pros and cons in the online MT adaptation scenario, and compare their performance in aligning unknown terms.", "labels": [], "entities": [{"text": "MT adaptation", "start_pos": 98, "end_pos": 111, "type": "TASK", "confidence": 0.9837160110473633}]}, {"text": "In Section 4 we propose a standalone module which refines the word alignment of unknown words; moreover, we present an enhanced faster implementation of the best performing word aligner, to make it usable in the online scenario.", "labels": [], "entities": [{"text": "word alignment of unknown words", "start_pos": 62, "end_pos": 93, "type": "TASK", "confidence": 0.7933363378047943}]}, {"text": "In Section 5 we show experimental results of this module on three different languages.", "labels": [], "entities": []}, {"text": "Finally, we draw some final comments in Section 6.", "labels": [], "entities": []}, {"text": "presented an incremental retraining method which simulates the procedure of learning from post-edited MT outputs (references), in areal time fashion.", "labels": [], "entities": [{"text": "MT outputs (references)", "start_pos": 102, "end_pos": 125, "type": "TASK", "confidence": 0.8042264580726624}]}, {"text": "By dividing the learning task into word alignment and phrase extraction tasks, and replacing the standard wordalignment module, which is a variation of EM algorithm, with a greedy search algorithm, they attempt to find a quick approximation of the word alignments of the newly translated sentence.", "labels": [], "entities": [{"text": "word alignment and phrase extraction", "start_pos": 35, "end_pos": 71, "type": "TASK", "confidence": 0.6991123378276825}]}, {"text": "They also use some heuristics to improve the obtained alignments, without supporting it with some proofs or even providing some experimental results.", "labels": [], "entities": []}, {"text": "Furthermore, the running time of this approach is not discussed, and it is not clear how effective this approach is in online scenarios.", "labels": [], "entities": []}, {"text": "have recently studied the problem of incremental learning from post-editing data, with minimum computational complexity and acceptable quality.", "labels": [], "entities": []}, {"text": "They use the MT output (hypothesis) as a pivot to find the word alignments between the source sentence and its corresponding reference.", "labels": [], "entities": [{"text": "MT", "start_pos": 13, "end_pos": 15, "type": "TASK", "confidence": 0.8788082599639893}]}, {"text": "Similarly to, once the word alignment between the source and post-edit sentence pair is generated, they use the standard phrase extraction method to extract the parallel phrase pairs.", "labels": [], "entities": [{"text": "phrase extraction", "start_pos": 121, "end_pos": 138, "type": "TASK", "confidence": 0.7315651476383209}]}, {"text": "This work is based on an implicit assumption that MT output is reliable enough to make abridge between source and reference.", "labels": [], "entities": [{"text": "MT", "start_pos": 50, "end_pos": 52, "type": "TASK", "confidence": 0.9712538719177246}]}, {"text": "However, in the real world this is not always true.", "labels": [], "entities": []}, {"text": "The post-editor sometimes makes a lot of changes in the MT output, or even translates the entire sentence from scratch, which makes the post-edit very different from the automatic translation.", "labels": [], "entities": [{"text": "MT output", "start_pos": 56, "end_pos": 65, "type": "TASK", "confidence": 0.8401792049407959}]}, {"text": "Moreover, in the presence of new words in the source sentence, the MT system either does not produce any translation for the new word, or directly copies it in the output.", "labels": [], "entities": [{"text": "MT", "start_pos": 67, "end_pos": 69, "type": "TASK", "confidence": 0.9384013414382935}]}, {"text": "Due to the above two reasons, there will be missing alignments between the automatic translation and postedit, which ultimately results in incomplete paths from source to post-edit.", "labels": [], "entities": []}, {"text": "But, the goal here is to accurately align the known words, as well as learning the alignments of the new words, which is not feasible by this approach.", "labels": [], "entities": []}], "datasetContent": [{"text": "A word aligner is usually evaluated in terms of Precision, Recall, and F-measure (or shortly F ), which are defined as follows: where A is the set of automatically computed alignments, and Sand P refer to the sure (unambiguous) and possible (ambiguous) manual alignments; note that S \u2286 P . In this paper, \u03b1 is set to 0.5 for all the experiments, in order to have a balance between Precision and Recall.", "labels": [], "entities": [{"text": "Recall", "start_pos": 59, "end_pos": 65, "type": "METRIC", "confidence": 0.9328190088272095}, {"text": "F-measure", "start_pos": 71, "end_pos": 80, "type": "METRIC", "confidence": 0.9829529523849487}]}, {"text": "In this paper we are mainly interested how the word-aligner performs on the unknown words; hence, we define aversion of Precision, Recall, and F metrics focused on the oov-alignment only, i.e. the alignments for which either the source or the target word is not included in the training corpus.", "labels": [], "entities": [{"text": "Precision", "start_pos": 120, "end_pos": 129, "type": "METRIC", "confidence": 0.9870822429656982}, {"text": "Recall", "start_pos": 131, "end_pos": 137, "type": "METRIC", "confidence": 0.8804057240486145}, {"text": "F", "start_pos": 143, "end_pos": 144, "type": "METRIC", "confidence": 0.8398167490959167}]}, {"text": "The subscript all identifies the standard metrics; the subscript oov identifies their oov-based versions.", "labels": [], "entities": []}, {"text": "In we show manual and automatic word alignments between an English-Italian sentence pair.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 32, "end_pos": 47, "type": "TASK", "confidence": 0.7072048187255859}]}, {"text": "A sure alignment, like are-sono, is represented by a solid line, and a possible alignment, like than-ai, by a dash line.", "labels": [], "entities": []}, {"text": "An oov-alignment, like that linking the unknown English word deployable to the Italian word attivabili, is identified by a dotted line.", "labels": [], "entities": []}, {"text": "According to this example, Precision and Recall will be about 0.85 (=11/13) and 0.91 (=10/11), respectively, and the corresponding F is hence about 0.88.", "labels": [], "entities": [{"text": "Precision", "start_pos": 27, "end_pos": 36, "type": "METRIC", "confidence": 0.9987303614616394}, {"text": "Recall", "start_pos": 41, "end_pos": 47, "type": "METRIC", "confidence": 0.9978392720222473}, {"text": "F", "start_pos": 131, "end_pos": 132, "type": "METRIC", "confidence": 0.9974282383918762}]}, {"text": "Focusing on the oov-alignment only, Precision oov is 1.00 (=1/1), Recall oov is 0.50 (=1/2), and F oov is 0.67.", "labels": [], "entities": [{"text": "Precision oov", "start_pos": 36, "end_pos": 49, "type": "METRIC", "confidence": 0.9271079301834106}, {"text": "Recall oov", "start_pos": 66, "end_pos": 76, "type": "METRIC", "confidence": 0.8780725002288818}, {"text": "F oov", "start_pos": 97, "end_pos": 102, "type": "METRIC", "confidence": 0.9339120388031006}]}, {"text": "In this paper, we compare word-alignment performance of three word-aligners introduced in Section 3.3 on three distinct tasks, namely EnglishItalian, English-French, and English-Spanish; the training corpora, common to all word-aligners, are subset of the JRC-legal corpus: Example of manual (above) and automatic (below) word alignments between an English-Italian sentence pair.", "labels": [], "entities": [{"text": "JRC-legal corpus", "start_pos": 256, "end_pos": 272, "type": "DATASET", "confidence": 0.7137358039617538}]}, {"text": "Sure and possible alignments are identified by solid and dash lines, respectively, and the oov-alignments by a dotted line.", "labels": [], "entities": []}, {"text": "The OOV words, like deployable (English) and finanziaria (Italian), are printed in italics.", "labels": [], "entities": []}, {"text": "Statistics of the three training corpora are reported in.", "labels": [], "entities": []}, {"text": "Three evaluation data sets are also available, which belong to the same domains of the corresponding training corpora.", "labels": [], "entities": []}, {"text": "The English-Italian test set was built by two professional translators by correcting an automatically produced wordalignment.", "labels": [], "entities": [{"text": "English-Italian test set", "start_pos": 4, "end_pos": 28, "type": "DATASET", "confidence": 0.7189703484376272}]}, {"text": "The English-French test set is the manually aligned parallel corpus introduced in) . The English-Spanish test set was provided by . Statistics of the three test sets are reported in.", "labels": [], "entities": [{"text": "English-French test set", "start_pos": 4, "end_pos": 27, "type": "DATASET", "confidence": 0.759351372718811}, {"text": "English-Spanish test set", "start_pos": 89, "end_pos": 113, "type": "DATASET", "confidence": 0.7627196709314982}]}, {"text": "In this section we evaluate the effectiveness of the proposed refinement module.", "labels": [], "entities": []}, {"text": "Each considered word aligner was equipped by our refinement module, and compared to its corresponding baseline.", "labels": [], "entities": []}, {"text": "shows the oov-based F-measure achieved by the baseline and enhanced word aligners on all test sets and all tasks.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 20, "end_pos": 29, "type": "METRIC", "confidence": 0.8445801138877869}]}, {"text": "We observe that the refinement module consistently improves the F-measure of all aligners on all language pairs; The improvement for mgiza++ are big (up to 10%) for very low oov-rates and decreases when the oov-rate increases; the same but smaller behavior is observed for fast-align.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 64, "end_pos": 73, "type": "METRIC", "confidence": 0.9946693778038025}]}, {"text": "This is due to the fact that by inserting more oov words into the test sets the systems are able to produce less accurate alignment points, which leads in lower contextual information (i.e. smaller number of overlapping phrase-pairs) for aligning the unknown words.", "labels": [], "entities": []}, {"text": "Interestingly, the refinement module applied to the berkeley output permits the correct detection of: Difference of performance in terms of standard F-measure of the enhanced word aligners from their corresponding baselines on test sets with increasing OOV rate, for all language pairs.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 149, "end_pos": 158, "type": "METRIC", "confidence": 0.9957581162452698}, {"text": "OOV rate", "start_pos": 253, "end_pos": 261, "type": "METRIC", "confidence": 0.9858214855194092}]}, {"text": "many oov-alignments, which the baseline system cannot find most of them.", "labels": [], "entities": []}, {"text": "Furthermore, reports the F-measure differences achieved by the enhanced wordaligners from their corresponding baselines on the full data sets.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 25, "end_pos": 34, "type": "METRIC", "confidence": 0.9915662407875061}]}, {"text": "The refinement module slightly but consistently improves the overall F-measure as well, especially for high oov-rates.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 69, "end_pos": 78, "type": "METRIC", "confidence": 0.9949116110801697}]}, {"text": "The highest improvement is achieved by the enhanced berkeley aligner, mainly because its baseline performs worse in this condition.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Staticts of the test corpora for English- Italian, English-French, and English-Spanish  tasks. oov-rate src and oov-rate trg are the ratio of  the new words in the source and target side of the  test corpus, respectively.", "labels": [], "entities": []}, {"text": " Table 3: Comparison of different widely-used  word aligners in terms of precision, recall, and F- measure on English-Italian, English-French, and  English-Spanish language pairs. Columns all re- port the evaluation performed on all alignments,  while columns oov the evaluation performed on  the oov-alignments.", "labels": [], "entities": [{"text": "precision", "start_pos": 73, "end_pos": 82, "type": "METRIC", "confidence": 0.999351441860199}, {"text": "recall", "start_pos": 84, "end_pos": 90, "type": "METRIC", "confidence": 0.9991136193275452}, {"text": "F- measure", "start_pos": 96, "end_pos": 106, "type": "METRIC", "confidence": 0.995596448580424}]}]}