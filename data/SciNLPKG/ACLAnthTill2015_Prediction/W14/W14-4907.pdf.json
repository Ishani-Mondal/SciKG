{"title": [{"text": "Optimizing annotation efforts to build reliable annotated corpora for training statistical models", "labels": [], "entities": []}], "abstractContent": [{"text": "Creating high-quality manual annotations on text corpus is time-consuming and often requires the work of experts.", "labels": [], "entities": []}, {"text": "In order to explore methods for optimizing annotation efforts, we study three key time burdens of the annotation process: (i) multiple annotations, (ii) consensus annotations, and (iii) careful annotations.", "labels": [], "entities": []}, {"text": "Through a series of experiments using a corpus of clinical documents annotated for personally identifiable information written in French, we address each of these aspects and draw conclusions on how to make the most of an annotation effort.", "labels": [], "entities": []}], "introductionContent": [{"text": "Statistical and Machine Learning methods have become prevalent in Natural Language Processing (NLP) over the past decades.", "labels": [], "entities": [{"text": "Natural Language Processing (NLP)", "start_pos": 66, "end_pos": 99, "type": "TASK", "confidence": 0.7616829176743826}]}, {"text": "These methods sucessfully address NLP tasks such as part-of-speech tagging or named entity recognition by relying on large annotated text corpora.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 52, "end_pos": 74, "type": "TASK", "confidence": 0.6946709454059601}, {"text": "named entity recognition", "start_pos": 78, "end_pos": 102, "type": "TASK", "confidence": 0.6083963612715403}]}, {"text": "As a result, developping highquality annotated corpora representing natural language phenomena that can be processed by statistical tools has become a major challenge for the scientific community.", "labels": [], "entities": []}, {"text": "Several aspects of the annotation task have been studied in order to ensure corpus quality and affordable cost.", "labels": [], "entities": []}, {"text": "Inter-annotator agreement (IAA) has been used as an indicator of annotation quality.", "labels": [], "entities": [{"text": "Inter-annotator agreement (IAA)", "start_pos": 0, "end_pos": 31, "type": "METRIC", "confidence": 0.6871096968650818}]}, {"text": "Early work showed that the use of automatic preannotation tools improved annotation consistency).", "labels": [], "entities": [{"text": "consistency", "start_pos": 84, "end_pos": 95, "type": "METRIC", "confidence": 0.7184341549873352}]}, {"text": "Careful and detailed annotation guideline definition was also shown to have positive impact on IAA ().", "labels": [], "entities": [{"text": "IAA", "start_pos": 95, "end_pos": 98, "type": "DATASET", "confidence": 0.36142444610595703}]}, {"text": "Efforts have investigated methods to reduce the human workload while annotating corpora.", "labels": [], "entities": []}, {"text": "In particular, active learning () sucessfully selects portions of corpora that yield the most benefit when annotated.", "labels": [], "entities": []}, {"text": "Alternatively, () investigated the need for double annotation and found that double annotation could be limited to carefully selected portions of a corpus.", "labels": [], "entities": []}, {"text": "They produced an algorithm that automatically selects portions of a corpus for double annotation.", "labels": [], "entities": []}, {"text": "Their approach allowed to reduce the amount of work by limiting the portion of doubly annotated data and maintained annotation quality to the standard of a fully doubly annotated corpus.", "labels": [], "entities": []}, {"text": "The use of automatic pre-annotations was shown to increase annotation consistency and result in producing quality annotation with a time gain over annotating raw data.", "labels": [], "entities": [{"text": "consistency", "start_pos": 70, "end_pos": 81, "type": "METRIC", "confidence": 0.8101144433021545}]}, {"text": "With the increasing use of crowdsourcing for obtaining annotated data, show that there are ethic aspects to consider in addition to technical and monetary cost when using a microworking platform for annotation.", "labels": [], "entities": []}, {"text": "While selecting the adequate methods for computing IAA is important) for interpreting the IAA fora particular task, annotator disagreement is inherent to all annotation tasks.", "labels": [], "entities": [{"text": "interpreting the IAA", "start_pos": 73, "end_pos": 93, "type": "TASK", "confidence": 0.8158377607663473}]}, {"text": "To address this situation () designed a method to estimate annotation confidence based on annotator modeling.", "labels": [], "entities": []}, {"text": "Overall, past work shows that creating high-quality manual annotations is time-consuming and often requires the work of experts.", "labels": [], "entities": []}, {"text": "The time burden is distributed between the sheer creation of the annotations, the act of producing multiple annotations for the same data and the subsequent analysis of multiple annotations to resolve conflicts, viz.", "labels": [], "entities": []}, {"text": "the creation of a consensus.", "labels": [], "entities": []}, {"text": "Research has addressed methods for reducing the time burden associated to these annotation activities (for example, adequate annotation tools such as automatic pre-annotations can reduce the time burden of annotation creation) with the final goal of producing the highest quality of annotations.", "labels": [], "entities": []}, {"text": "In contrast, our hypothesis in this work is that annotations are being developed for the purpose of training a machine learning model.", "labels": [], "entities": []}, {"text": "Therefore, our experiments consist in training a named entity recognizer on a training set comprising annotations of varying quality to study the impact of training annotation quality on model performance.", "labels": [], "entities": []}, {"text": "In order to explore methods for optimizing annotation efforts for the development of training corpora, we revisit the three key time burdens of the annotation process on textual corpora: (i) careful annotations, (ii) multiple annotations, and (iii) consensus annotations.", "labels": [], "entities": []}, {"text": "Through a series of experiments using a corpus of French clinical documents annotated for personally identifiable information (PHI), we address each of these aspects and draw conclusions on how to make the most of an annotation effort.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2 presents the detailed performance of each annotation run for individual PHI  categories in terms of F-measure.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 109, "end_pos": 118, "type": "METRIC", "confidence": 0.8928143978118896}]}, {"text": " Table 2: Performance per PHI category (F-measure)", "labels": [], "entities": [{"text": "F-measure", "start_pos": 40, "end_pos": 49, "type": "METRIC", "confidence": 0.8210595846176147}]}]}