{"title": [], "abstractContent": [{"text": "This paper describes a system for inter-annotator agreement analysis of ERE annotation , focusing on entity mentions and how the higher-order annotations such as EVENTS are dependent on those entity mentions.", "labels": [], "entities": [{"text": "inter-annotator agreement analysis", "start_pos": 34, "end_pos": 68, "type": "TASK", "confidence": 0.6407672564188639}, {"text": "EVENTS", "start_pos": 162, "end_pos": 168, "type": "METRIC", "confidence": 0.9601923227310181}]}, {"text": "The goal of this approach is to provide both (1) quantitative scores for the various levels of annotation, and (2) information about the types of annotation inconsistencies that might exist.", "labels": [], "entities": []}, {"text": "While primarily designed for inter-annotator agreement , it can also be considered a system for evaluation of ERE annotation.", "labels": [], "entities": []}], "introductionContent": [{"text": "In this paper we describe a system for analyzing dually human-annotated files of Entities, Relations, and Events (ERE) annotation for consistency between the two files.", "labels": [], "entities": []}, {"text": "This is an important aspect of training new annotators, to evaluate the consistency of their annotation with a \"gold\" file, or to evaluate the agreement between two annotators.", "labels": [], "entities": [{"text": "consistency", "start_pos": 72, "end_pos": 83, "type": "METRIC", "confidence": 0.9899780750274658}]}, {"text": "We refer to both cases here as the task of \"inter-annotator agreement\" (IAA).", "labels": [], "entities": [{"text": "inter-annotator agreement\" (IAA)", "start_pos": 44, "end_pos": 76, "type": "TASK", "confidence": 0.586889460682869}]}, {"text": "The light ERE annotation task was defined as part of the DARPA DEFT program) as a simpler version of tasks like ACE () to allow quick annotation of a simplified ontology of entities, relations, and events, along with identity coreference.", "labels": [], "entities": [{"text": "DARPA DEFT program", "start_pos": 57, "end_pos": 75, "type": "DATASET", "confidence": 0.9023637572924296}]}, {"text": "The ENTITIES consist of coreferenced entity mentions, which refer to a span of text in the source file.", "labels": [], "entities": [{"text": "ENTITIES", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.736907422542572}]}, {"text": "The entity mentions are also used as part of the annotation of RELATIONS and EVENTS, as a stand in for the whole ENTITY.", "labels": [], "entities": [{"text": "RELATIONS", "start_pos": 63, "end_pos": 72, "type": "METRIC", "confidence": 0.9923542737960815}, {"text": "EVENTS", "start_pos": 77, "end_pos": 83, "type": "METRIC", "confidence": 0.9423959851264954}]}, {"text": "The ACE program had a scoring metric described in ().", "labels": [], "entities": [{"text": "ACE program", "start_pos": 4, "end_pos": 15, "type": "DATASET", "confidence": 0.8794783353805542}]}, {"text": "However, our emphasis for IAA evaluation is somewhat different than that of scoring annotation files for accuracy with regard to a gold standard.", "labels": [], "entities": [{"text": "IAA evaluation", "start_pos": 26, "end_pos": 40, "type": "TASK", "confidence": 0.8320641219615936}, {"text": "accuracy", "start_pos": 105, "end_pos": 113, "type": "METRIC", "confidence": 0.9976845979690552}]}, {"text": "The IAA system aims to produce output to help an annotation manager understand the sorts of errors occurring, and the general range of possible problems.", "labels": [], "entities": []}, {"text": "Nevertheless, the approach to IAA evaluation described here can be used for scoring as well.", "labels": [], "entities": [{"text": "IAA evaluation", "start_pos": 30, "end_pos": 44, "type": "TASK", "confidence": 0.8356484174728394}]}, {"text": "This approach is inspired by the IAA work for treebanks in.", "labels": [], "entities": [{"text": "IAA", "start_pos": 33, "end_pos": 36, "type": "DATASET", "confidence": 0.6756942868232727}]}, {"text": "Because the entity mentions in ERE are the fundamental units used for the ENTITY, EVENT and RELATION annotations, they are also the fundamental units upon which the IAA evaluation is based.", "labels": [], "entities": [{"text": "ERE", "start_pos": 31, "end_pos": 34, "type": "METRIC", "confidence": 0.6964783072471619}, {"text": "ENTITY", "start_pos": 74, "end_pos": 80, "type": "METRIC", "confidence": 0.9953358769416809}, {"text": "EVENT", "start_pos": 82, "end_pos": 87, "type": "METRIC", "confidence": 0.9803179502487183}, {"text": "RELATION", "start_pos": 92, "end_pos": 100, "type": "METRIC", "confidence": 0.9724671840667725}, {"text": "IAA", "start_pos": 165, "end_pos": 168, "type": "TASK", "confidence": 0.6819677948951721}]}, {"text": "The description of the system therefore begins with a focus on the evaluation of the consistency of the entity mention annotations.", "labels": [], "entities": [{"text": "consistency", "start_pos": 85, "end_pos": 96, "type": "METRIC", "confidence": 0.9551483988761902}]}, {"text": "We derive a mapping between the entity mentions between the two files (henceforth called File A and File B).", "labels": [], "entities": []}, {"text": "We then move onto ENTITIES, RELATIONS, and EVENTS, pointing out the differences between them for purposes of evaluation, but also their similarities.", "labels": [], "entities": [{"text": "ENTITIES", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.985483705997467}, {"text": "RELATIONS", "start_pos": 28, "end_pos": 37, "type": "METRIC", "confidence": 0.9939835667610168}, {"text": "EVENTS", "start_pos": 43, "end_pos": 49, "type": "METRIC", "confidence": 0.9897878170013428}]}, {"text": "This is a first towards a more accurate use of the full ENTITIES in the comparison and scoring of ENTITIES and EVENTS annotations.", "labels": [], "entities": [{"text": "ENTITIES", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.9517503976821899}]}, {"text": "Work to expand in this direction is in progress.", "labels": [], "entities": []}, {"text": "When a more complete system is in place it will be more appropriate to report corpus-based results.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}