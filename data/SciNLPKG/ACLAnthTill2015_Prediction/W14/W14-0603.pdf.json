{"title": [{"text": "Bootstrapping a historical commodities lexicon with SKOS and DBpedia", "labels": [], "entities": [{"text": "DBpedia", "start_pos": 61, "end_pos": 68, "type": "DATASET", "confidence": 0.9222609400749207}]}], "abstractContent": [{"text": "Named entity recognition for novel domains can be challenging in the absence of suitable training materials for machine-learning or lexicons and gazetteers for term look-up.", "labels": [], "entities": [{"text": "Named entity recognition", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.7006634672482809}]}, {"text": "We describe an approach that starts from a small, manually created word list of commodities traded in the nineteenth century, and then uses semantic web techniques to augment the list by an order of magnitude, drawing on data stored in DBpedia.", "labels": [], "entities": []}, {"text": "This work was conducted during the Trading Consequences project on text mining and visualisation of historical documents for the study of global trading in the British empire.", "labels": [], "entities": [{"text": "text mining", "start_pos": 67, "end_pos": 78, "type": "TASK", "confidence": 0.7963041365146637}]}], "introductionContent": [{"text": "The Trading Consequences project 1 aims to assist environmental historians in understanding the economic and environmental consequences of commodity trading during the nineteenth century.", "labels": [], "entities": []}, {"text": "We are applying text mining to large quantities of historical text in order to convert unstructured textual information into structured data that can be queried and visualised.", "labels": [], "entities": [{"text": "text mining", "start_pos": 16, "end_pos": 27, "type": "TASK", "confidence": 0.8335789144039154}]}, {"text": "While prior historical research into commodity flows) has focused on a small number of widely traded natural resources, the large corpora of digitised documents processed by Trading Consequences is giving historians data about a much broader range of commodities.", "labels": [], "entities": []}, {"text": "A detailed appraisal of trade in these resources will yield a significantly more accurate picture of globalisation and its environmental consequences.", "labels": [], "entities": []}, {"text": "In this paper we focus on our approach to building a lexicon to support the recognition of commodity terms in text.", "labels": [], "entities": [{"text": "recognition of commodity terms in text", "start_pos": 76, "end_pos": 114, "type": "TASK", "confidence": 0.8316941559314728}]}, {"text": "We provide some background to this work in Section 2.", "labels": [], "entities": []}, {"text": "In Section 3, we describe the process of creating the lexicon; this starts from a manually collected seed set of commodity terms which is then expanded semi-automatically using DBpedia.", "labels": [], "entities": [{"text": "DBpedia", "start_pos": 177, "end_pos": 184, "type": "DATASET", "confidence": 0.9598479866981506}]}, {"text": "An evaluation of the quality of the commodity lexicon is provided in Section 4.", "labels": [], "entities": []}, {"text": "shows an overview of the architecture of the Trading Consequences system.", "labels": [], "entities": []}, {"text": "Input documents are processed by the text mining pipeline, which is based on the LT-XML2 and LT-TTT2 4 toolkits (.", "labels": [], "entities": [{"text": "text mining", "start_pos": 37, "end_pos": 48, "type": "TASK", "confidence": 0.714974582195282}, {"text": "LT-XML2", "start_pos": 81, "end_pos": 88, "type": "DATASET", "confidence": 0.9392996430397034}, {"text": "LT-TTT2 4 toolkits", "start_pos": 93, "end_pos": 111, "type": "DATASET", "confidence": 0.8456917802492777}]}, {"text": "After initial format conversion, the text under- goes language identification and OCR post-correction and normalisation.", "labels": [], "entities": [{"text": "language identification", "start_pos": 54, "end_pos": 77, "type": "TASK", "confidence": 0.7318618148565292}, {"text": "OCR", "start_pos": 82, "end_pos": 85, "type": "METRIC", "confidence": 0.7147796750068665}]}, {"text": "It is then processed further by shallow linguistic analysis, lexicon and gazetteer lookup, named entity recognition and grounding, and relation extraction (see.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 91, "end_pos": 115, "type": "TASK", "confidence": 0.6460461020469666}, {"text": "relation extraction", "start_pos": 135, "end_pos": 154, "type": "TASK", "confidence": 0.9066333174705505}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Number of documents and images per collec- tion. One image usually corresponds to one document  page, except in the case of CPRINT, where it mostly  corresponds to two document pages. The LETTERS col- lection does not contain OCRed text but summaries of  hand-written letters.", "labels": [], "entities": []}, {"text": " Table 2: Precision (P), recall (R) and F-score figures for evaluating the performance of the commodity recog- nition prototype, as well as numbers of true positive (TP), false positive (FP) and false negative (FN) mentions.  These figures are compared against equivalent inter-annotator agreement (IAA) scores in 25% of the gold standard  documents. We provide evaluation scores for strict and lax boundary matching of entity mentions.", "labels": [], "entities": [{"text": "Precision (P)", "start_pos": 10, "end_pos": 23, "type": "METRIC", "confidence": 0.9531945735216141}, {"text": "recall (R)", "start_pos": 25, "end_pos": 35, "type": "METRIC", "confidence": 0.9543841779232025}, {"text": "F-score", "start_pos": 40, "end_pos": 47, "type": "METRIC", "confidence": 0.9792942404747009}, {"text": "true positive (TP), false positive (FP) and false negative (FN) mentions", "start_pos": 151, "end_pos": 223, "type": "METRIC", "confidence": 0.7288825942410363}]}, {"text": " Table 3: Precision (P), recall (R) and F-score figures for evaluating the performance of the commodity recognition  prototype compared to the same scores for two optimisation steps. We provide evaluation scores for strict and lax  boundary matching of entity mentions.", "labels": [], "entities": [{"text": "Precision (P)", "start_pos": 10, "end_pos": 23, "type": "METRIC", "confidence": 0.9547003507614136}, {"text": "recall (R)", "start_pos": 25, "end_pos": 35, "type": "METRIC", "confidence": 0.9585420936346054}, {"text": "F-score", "start_pos": 40, "end_pos": 47, "type": "METRIC", "confidence": 0.9924778938293457}, {"text": "commodity recognition  prototype", "start_pos": 94, "end_pos": 126, "type": "TASK", "confidence": 0.7690504789352417}]}]}