{"title": [{"text": "A Systematic Study of Semantic Vector Space Model Parameters", "labels": [], "entities": []}], "abstractContent": [{"text": "We present a systematic study of parameters used in the construction of semantic vector space models.", "labels": [], "entities": []}, {"text": "Evaluation is carried out on a variety of similarity tasks, including a compositionality dataset, using several source corpora.", "labels": [], "entities": []}, {"text": "In addition to recommendations for optimal parameters, we present some novel findings, including a similarity metric that outperforms the alternatives on all tasks considered.", "labels": [], "entities": [{"text": "similarity metric", "start_pos": 99, "end_pos": 116, "type": "METRIC", "confidence": 0.965514600276947}]}], "introductionContent": [{"text": "Vector space models (VSMs) represent the meanings of lexical items as vectors in a \"semantic space\".", "labels": [], "entities": []}, {"text": "The benefit of VSMs is that they can easily be manipulated using linear algebra, allowing a degree of similarity between vectors to be computed.", "labels": [], "entities": []}, {"text": "They rely on the distributional hypothesis: the idea that \"words that occur in similar contexts tend to have similar meanings\" ().", "labels": [], "entities": []}, {"text": "The construction of a suitable VSM fora particular task is highly parameterised, and there appears to belittle consensus over which parameter settings to use.", "labels": [], "entities": []}, {"text": "This paper presents a systematic study of the following parameters: \u2022 vector size; \u2022 window size; \u2022 window-based or dependency-based context; \u2022 feature granularity; \u2022 similarity metric; \u2022 weighting scheme; \u2022 stopwords and high frequency cut-off.", "labels": [], "entities": []}, {"text": "A representative set of semantic similarity datasets has been selected from the literature, including a phrasal similarity dataset for evaluating compositionality.", "labels": [], "entities": []}, {"text": "The choice of source corpus is likely to influence the quality of the VSM, and so we use a selection of source corpora.", "labels": [], "entities": [{"text": "VSM", "start_pos": 70, "end_pos": 73, "type": "DATASET", "confidence": 0.6532553434371948}]}, {"text": "Hence there are two additional \"superparameters\": \u2022 dataset for evaluation; \u2022 source corpus.", "labels": [], "entities": []}, {"text": "Previous studies have been limited to investigating only a small number of parameters, and using a limited set of source corpora and tasks for evaluation.", "labels": [], "entities": []}, {"text": "considered several weighting schemes fora large variety of tasks, while did the same for similarity metrics.", "labels": [], "entities": []}, {"text": "investigated the effectiveness of sub-spacing corpora, where a larger corpus is queried in order to construct a smaller sub-spaced corpus ().", "labels": [], "entities": []}, {"text": "compare several types of vector representations for semantic composition tasks.", "labels": [], "entities": []}, {"text": "The most comprehensive existing studies of VSM parameters -encompassing window sizes, feature granularity, stopwords and dimensionality reduction -are by and.", "labels": [], "entities": [{"text": "VSM", "start_pos": 43, "end_pos": 46, "type": "TASK", "confidence": 0.8657850623130798}]}, {"text": "Section 2 introduces the various parameters of vector space model construction.", "labels": [], "entities": [{"text": "vector space model construction", "start_pos": 47, "end_pos": 78, "type": "TASK", "confidence": 0.6748038679361343}]}, {"text": "We then attempt, in Section 3, to answer some of the fundamental questions for building VSMs through a number of experiments that consider each of the selected parameters.", "labels": [], "entities": [{"text": "VSMs", "start_pos": 88, "end_pos": 92, "type": "TASK", "confidence": 0.8998157382011414}]}, {"text": "In Section 4 we examine how these findings relate to the recent development of distributional compositional semantics (, where vectors for words are combined into vectors for phrases.", "labels": [], "entities": [{"text": "distributional compositional semantics", "start_pos": 79, "end_pos": 117, "type": "TASK", "confidence": 0.6967849731445312}]}], "datasetContent": [{"text": "The parameter space is too large to analyse exhaustively, and so we adopted a strategy for how to navigate through it, selecting certain parameters to investigate first, which then get fixed or \"clamped\" in the remaining experiments.", "labels": [], "entities": []}, {"text": "Unless specified otherwise, vectors are generated with the following restrictions and transformations on features: stopwords are removed, numbers mapped to 'NUM', and only strings consisting of alphanumeric characters are allowed.", "labels": [], "entities": []}, {"text": "In all experiments, the features consist of the frequency-ranked first n words in the given source corpus.", "labels": [], "entities": []}, {"text": "Four of the five similarity datasets (RG, MC, W353, MEN) contain continuous scales of similarity ratings for word pairs; hence we follow standard practice in using a Spearman correlation coefficient \u03c1 s for evaluation.", "labels": [], "entities": [{"text": "Spearman correlation coefficient \u03c1 s", "start_pos": 166, "end_pos": 202, "type": "METRIC", "confidence": 0.7400839269161225}]}, {"text": "The fifth dataset (TOEFL) is a set of multiple-choice questions, for which an accuracy measure is appropriate.", "labels": [], "entities": [{"text": "TOEFL", "start_pos": 19, "end_pos": 24, "type": "METRIC", "confidence": 0.827134370803833}, {"text": "accuracy", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.9989988207817078}]}, {"text": "Calculating an aggregate score overall datasets is non-trivial, since taking the mean of correlation scores leads to an under-estimation of performance; hence for the aggregate score we use the Fisher-transformed z-variable of the correla-: Term weighting schemes.", "labels": [], "entities": []}, {"text": "f ij denotes the target word frequency in a particular context, f i the total target word frequency, f j the total context frequency, N the total of all frequencies, n j the number of non-zero contexts.", "labels": [], "entities": []}, {"text": "P (t ij |c j ) is defined as tion datasets, and take the weighted average of its inverse over the correlation datasets and the TOEFL accuracy score.", "labels": [], "entities": [{"text": "TOEFL", "start_pos": 127, "end_pos": 132, "type": "METRIC", "confidence": 0.9164873957633972}, {"text": "accuracy", "start_pos": 133, "end_pos": 141, "type": "METRIC", "confidence": 0.8343189358711243}]}, {"text": "The size of the window has a clear impact on performance: a large corpus with a small window size performs best, but high performance can be achieved on a small subspaced corpus, if the window size is large.", "labels": [], "entities": []}, {"text": "The size of the source corpus is more important than whether the model is window-or dependency-based.", "labels": [], "entities": []}, {"text": "Window-based methods with a window size of 3 yield better results than dependency-based methods with a window of 3 (i.e. having a single arc).", "labels": [], "entities": []}, {"text": "The Google Syntactic N-gram corpus yields very good performance, but it is unclear whether this is due to being dependency-based or being very large.", "labels": [], "entities": [{"text": "Google Syntactic N-gram corpus", "start_pos": 4, "end_pos": 34, "type": "DATASET", "confidence": 0.6878746449947357}]}, {"text": "Experiment 4 The granularity of the context words has a relatively low impact on performance, but stemming yields the best results.", "labels": [], "entities": []}, {"text": "Experiment 5 The optimal combination of weighting scheme and similarity metric is Positive Mutual Information with a mean-adjusted version of Cosine that we have called Correlation.", "labels": [], "entities": [{"text": "Cosine", "start_pos": 142, "end_pos": 148, "type": "METRIC", "confidence": 0.9438591003417969}]}, {"text": "Another high-performing weighting scheme is TTest, which works better for smaller vector sizes.", "labels": [], "entities": []}, {"text": "The Correlation similarity metric consistently outperforms Cosine, and we recommend its use.", "labels": [], "entities": [{"text": "Correlation similarity metric", "start_pos": 4, "end_pos": 33, "type": "METRIC", "confidence": 0.7220685382684072}]}, {"text": "Experiment 6 Use of a weighting scheme obviates the need for removing high-frequency features.", "labels": [], "entities": []}, {"text": "Without weighting, many of the highfrequency features should be removed.", "labels": [], "entities": []}, {"text": "However, if weighting is an option we recommend its use.", "labels": [], "entities": [{"text": "weighting", "start_pos": 12, "end_pos": 21, "type": "TASK", "confidence": 0.9644786715507507}]}, {"text": "Compositionality The best parameters for individual vectors generally carryover to a compositional similarity task where phrasal similarity is evaluated by combining vectors into phrasal vectors.", "labels": [], "entities": []}, {"text": "Furthermore, we observe that in general performance increases as source corpus size increases, so we recommend using a corpus such as ukWaC over smaller corpora like the BNC.", "labels": [], "entities": [{"text": "ukWaC", "start_pos": 134, "end_pos": 139, "type": "DATASET", "confidence": 0.9543284177780151}, {"text": "BNC", "start_pos": 170, "end_pos": 173, "type": "DATASET", "confidence": 0.937446653842926}]}, {"text": "Likewise, since the MEN dataset is the largest similarity dataset available and mirrors our aggregate score the best across the various experiments, we recommend evaluating on that similarity task if only a single dataset is used for evaluation.", "labels": [], "entities": [{"text": "MEN dataset", "start_pos": 20, "end_pos": 31, "type": "DATASET", "confidence": 0.956729382276535}]}, {"text": "Obvious extensions include an analysis of the performance of the various dimensionality reduction techniques, examining the importance of window size and feature granularity for dependencybased methods, and further exploring the relation between the size and frequency distribution of a corpus together with the optimal characteristics (such as the high-frequency cut-off point) of vectors generated from that source.", "labels": [], "entities": [{"text": "dimensionality reduction", "start_pos": 73, "end_pos": 97, "type": "TASK", "confidence": 0.7470316588878632}]}], "tableCaptions": [{"text": " Table 1: Datasets for evaluation", "labels": [], "entities": [{"text": "evaluation", "start_pos": 23, "end_pos": 33, "type": "TASK", "confidence": 0.5920900702476501}]}, {"text": " Table 6: Similarity scores on individual datasets  for positive mutual information (P) and T-test  (T) weighting, with cosine (COS) and correlation  (COR) similarity", "labels": [], "entities": [{"text": "correlation  (COR)", "start_pos": 137, "end_pos": 155, "type": "METRIC", "confidence": 0.9324467331171036}]}, {"text": " Table 5: Aggregated scores for combinations of weighting schemes and similarity metrics using ukWaC", "labels": [], "entities": [{"text": "Aggregated", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9491038918495178}, {"text": "ukWaC", "start_pos": 95, "end_pos": 100, "type": "DATASET", "confidence": 0.9690468311309814}]}, {"text": " Table 7: Selected Spearman \u03c1 scores on the  Mitchell & Lapata 2010 compositionality dataset", "labels": [], "entities": [{"text": "Mitchell & Lapata 2010 compositionality dataset", "start_pos": 45, "end_pos": 92, "type": "DATASET", "confidence": 0.7711290717124939}]}]}