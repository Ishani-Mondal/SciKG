{"title": [{"text": "Ontology-based Extraction of Structured Information from Publications on Preclinical Experiments for Spinal Cord Injury Treatments", "labels": [], "entities": [{"text": "Ontology-based Extraction of Structured Information from Publications on Preclinical Experiments for Spinal Cord Injury Treatments", "start_pos": 0, "end_pos": 130, "type": "TASK", "confidence": 0.7555061221122742}]}], "abstractContent": [{"text": "Preclinical research in the field of central nervous system trauma advances at a fast pace, currently yielding over 8,000 new publications per year, at an exponentially growing rate.", "labels": [], "entities": [{"text": "central nervous system trauma", "start_pos": 37, "end_pos": 66, "type": "TASK", "confidence": 0.7013361603021622}]}, {"text": "This amount of published information by far exceeds the capacity of individual scientists to read and understand the relevant literature.", "labels": [], "entities": []}, {"text": "So far, no clinical trial has led to therapeutic approaches which achieve functional recovery inhuman patients.", "labels": [], "entities": []}, {"text": "In this paper, we describe a first prototype of an ontology-based information extraction system that automatically extracts relevant preclinical knowledge about spinal cord injury treatments from natural language text by recognizing participating entity classes and linking them to each other.", "labels": [], "entities": [{"text": "ontology-based information extraction", "start_pos": 51, "end_pos": 88, "type": "TASK", "confidence": 0.6570611894130707}]}, {"text": "The evaluation on an independent test corpus of manually annotated full text articles shows a macro-average F 1 measure of 0.74 with precision 0.68 and recall 0.81 on the task of identifying entities participating in relations.", "labels": [], "entities": [{"text": "F 1 measure", "start_pos": 108, "end_pos": 119, "type": "METRIC", "confidence": 0.9614357153574625}, {"text": "precision", "start_pos": 133, "end_pos": 142, "type": "METRIC", "confidence": 0.9966346621513367}, {"text": "recall", "start_pos": 152, "end_pos": 158, "type": "METRIC", "confidence": 0.9992592930793762}]}], "introductionContent": [{"text": "Injury to the central nervous system of adult mammals typically results in lasting deficits, like permanent motor and sensor impairments, due to alack of profound neural regeneration.", "labels": [], "entities": []}, {"text": "Specifically, patients who have sustained spinal cord injuries (SCI) usually remain partially paralyzed for the rest of their lives.", "labels": [], "entities": []}, {"text": "Preclinical research in the field of central nervous system trauma advances at fast pace, currently yielding over 8,000 new publications per year, at an exponentially growing rate, with a total amount of approximately 160,000 PubMed-listed papers today.", "labels": [], "entities": [{"text": "central nervous system trauma", "start_pos": 37, "end_pos": 66, "type": "TASK", "confidence": 0.6728712618350983}]}, {"text": "However, translational neuroscience faces a strong disproportion between the immense preclinical research effort and the lack of successful clinical trials in SCI therapy: So far, no therapeutic approach has led to functional recovery inhuman patients.", "labels": [], "entities": [{"text": "translational neuroscience", "start_pos": 9, "end_pos": 35, "type": "TASK", "confidence": 0.921452522277832}]}, {"text": "As the vast amount of published information by far exceeds the capacity of individual scientists to read and understand the relevant knowledge, the selection of promising therapeutic interventions for clinical trials is notoriously based on incomplete information).", "labels": [], "entities": []}, {"text": "Thus, automatic information extraction methods are needed to gather structured, actionable knowledge from large amounts of unstructured text that describe outcomes of preclinical experiments in the SCI domain.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 16, "end_pos": 38, "type": "TASK", "confidence": 0.7323618978261948}]}, {"text": "Being stored in a database, such knowledge provides a highly valuable resource enabling curators and researchers to objectively assess the prospective success of experimental therapies in humans, and supports the cost-effective execution of meta studies based on all previously published data.", "labels": [], "entities": []}, {"text": "First steps towards such a database have already been undertaken by manually extracting the desired information from a limited number of papers (), which is not feasible on a large scale, though.", "labels": [], "entities": []}, {"text": "In this paper, we present a first prototype of an automated ontology-based information extraction system for the acquisition of structured knowledge about experimental SCI therapies.", "labels": [], "entities": [{"text": "ontology-based information extraction", "start_pos": 60, "end_pos": 97, "type": "TASK", "confidence": 0.6646200517813364}]}, {"text": "As main contributions, we point out the highly relational problem structure by describing the entity classes and relations relevant for The first four authors contributed equally.", "labels": [], "entities": []}, {"text": "As in this query to the database PubMed (link to http://www.ncbi.nlm.nih.gov/pubmed), as of April 2014.", "labels": [], "entities": [{"text": "PubMed", "start_pos": 33, "end_pos": 39, "type": "DATASET", "confidence": 0.8990573883056641}]}], "datasetContent": [{"text": "We evaluate the system with regard to two different tasks: extraction (\"Is the approachable to extract relevant information from the text, without regard to the exact location of the information?\") and annotation (\"Is the system able to annotate relevant information at the correct location as indicated by medical experts?\").", "labels": [], "entities": []}, {"text": "Furthermore, we distinguish between an all instances setting, where we consider all instances independently, and a fillers only setting, where only those annotations in the system output are considered, that are fillers in a relation (i.e. the fillers only-setting evaluates a subset of the all instances-setting).", "labels": [], "entities": []}, {"text": "The relation extraction procedure is not evaluated separately.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.8554122447967529}]}, {"text": "For each setting, we report precision, recall, and F 1 measure.", "labels": [], "entities": [{"text": "precision", "start_pos": 28, "end_pos": 37, "type": "METRIC", "confidence": 0.9997971653938293}, {"text": "recall", "start_pos": 39, "end_pos": 45, "type": "METRIC", "confidence": 0.9997534155845642}, {"text": "F 1 measure", "start_pos": 51, "end_pos": 62, "type": "METRIC", "confidence": 0.961748202641805}]}, {"text": "Taking the architecture into account, we have the following hypotheses: (i) For the all instances setting we expect high recall, but low precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 121, "end_pos": 127, "type": "METRIC", "confidence": 0.9994581341743469}, {"text": "precision", "start_pos": 137, "end_pos": 146, "type": "METRIC", "confidence": 0.9993390440940857}]}, {"text": "(ii) For the fillers only setting, precision should increase notably.", "labels": [], "entities": [{"text": "precision", "start_pos": 35, "end_pos": 44, "type": "METRIC", "confidence": 0.9998383522033691}]}, {"text": "(iii) Comparing the all entities and the fillers only setting, recall should remain at the same level.", "labels": [], "entities": [{"text": "recall", "start_pos": 63, "end_pos": 69, "type": "METRIC", "confidence": 0.9993552565574646}]}, {"text": "We therefore expect the extraction task to be simpler than the annotation task: For any information to be annotated at the correct position, it must have been extracted correctly.", "labels": [], "entities": []}, {"text": "On the other hand, information that has been extracted correctly, can still be found at a 'wrong' location in the text.", "labels": [], "entities": []}, {"text": "Thus, we expect a drop of precision and recall when moving from extraction to annotation.", "labels": [], "entities": [{"text": "precision", "start_pos": 26, "end_pos": 35, "type": "METRIC", "confidence": 0.9996752738952637}, {"text": "recall", "start_pos": 40, "end_pos": 46, "type": "METRIC", "confidence": 0.9995162487030029}]}], "tableCaptions": [{"text": " Table 3: The macro-averaged evaluation results for each class given in precision, recall and F 1 measure.", "labels": [], "entities": [{"text": "precision", "start_pos": 72, "end_pos": 81, "type": "METRIC", "confidence": 0.9996931552886963}, {"text": "recall", "start_pos": 83, "end_pos": 89, "type": "METRIC", "confidence": 0.9989972710609436}, {"text": "F 1 measure", "start_pos": 94, "end_pos": 105, "type": "METRIC", "confidence": 0.9897062579790751}]}]}