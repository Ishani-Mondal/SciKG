{"title": [{"text": "The DCU-ICTCAS MT system at WMT 2014 on German-English Translation Task", "labels": [], "entities": [{"text": "DCU-ICTCAS MT system at WMT 2014", "start_pos": 4, "end_pos": 36, "type": "DATASET", "confidence": 0.7729931672414144}, {"text": "German-English Translation", "start_pos": 40, "end_pos": 66, "type": "TASK", "confidence": 0.7130945026874542}]}], "abstractContent": [{"text": "This paper describes the DCU submission to WMT 2014 on German-English translation task.", "labels": [], "entities": [{"text": "DCU submission to WMT 2014", "start_pos": 25, "end_pos": 51, "type": "DATASET", "confidence": 0.8712293267250061}, {"text": "German-English translation task", "start_pos": 55, "end_pos": 86, "type": "TASK", "confidence": 0.6678275664647421}]}, {"text": "Our system uses phrase-based translation model with several popular techniques, including Lexicalized Reordering Model, Operation Sequence Model and Language Model interpolation.", "labels": [], "entities": [{"text": "phrase-based translation", "start_pos": 16, "end_pos": 40, "type": "TASK", "confidence": 0.7704398036003113}]}, {"text": "Our final submission is the result of system combination on several systems which have different pre-processing and alignments .", "labels": [], "entities": []}], "introductionContent": [{"text": "On the German-English translation task of WMT 2014, we submitted a system which is built with Moses phrase-based model ( . For system training, we use all provided German-English parallel data, and conducted several pre-processing steps to clean the data.", "labels": [], "entities": [{"text": "German-English translation task of WMT 2014", "start_pos": 7, "end_pos": 50, "type": "TASK", "confidence": 0.6720276971658071}]}, {"text": "In addition, in order to improve the translation quality, we adopted some popular techniques, including three Lexicalized Reordering Models, a 9-gram Operation Sequence Model () and Language Model interpolation on several datasets.", "labels": [], "entities": [{"text": "translation", "start_pos": 37, "end_pos": 48, "type": "TASK", "confidence": 0.9602523446083069}]}, {"text": "And then we use system combination on several systems with different settings to produce the final outputs.", "labels": [], "entities": []}, {"text": "Our phrase-based systems are tuned with k-best MIRA (Cherry and Foster, 2012) on development set.", "labels": [], "entities": [{"text": "MIRA", "start_pos": 47, "end_pos": 51, "type": "METRIC", "confidence": 0.9544088840484619}]}, {"text": "We set the maximum iteration to be 25.", "labels": [], "entities": []}, {"text": "The Language Models in our systems are trained with SRILM).", "labels": [], "entities": [{"text": "SRILM", "start_pos": 52, "end_pos": 57, "type": "METRIC", "confidence": 0.7564751505851746}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Preliminary results on tuning set and test set (newstest 2013). All scores on test set are case- sensitive BLEU[%] scores. And scores on tuning set are case-insensitive BLEU[%] directly from tuning  result. Baseline uses all the data from newstest 2008-2012 for tuning.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 117, "end_pos": 121, "type": "METRIC", "confidence": 0.6886876225471497}, {"text": "BLEU", "start_pos": 179, "end_pos": 183, "type": "METRIC", "confidence": 0.9430409073829651}]}, {"text": " Table 3: System BLEU[%] scores when different LRMs are adopted.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 17, "end_pos": 21, "type": "METRIC", "confidence": 0.9697036743164062}]}, {"text": " Table 4: Experiment results on newstest 2014. We report case-sensitive BLEU[%] score on test set and  case-insensitive BLEU", "labels": [], "entities": [{"text": "newstest 2014", "start_pos": 32, "end_pos": 45, "type": "DATASET", "confidence": 0.9291480183601379}, {"text": "BLEU", "start_pos": 72, "end_pos": 76, "type": "METRIC", "confidence": 0.9722680449485779}, {"text": "BLEU", "start_pos": 120, "end_pos": 124, "type": "METRIC", "confidence": 0.9700930714607239}]}]}