{"title": [{"text": "LIMSI Submission for WMT'14 QE Task", "labels": [], "entities": [{"text": "LIMSI Submission", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.5040965527296066}, {"text": "WMT'14 QE", "start_pos": 21, "end_pos": 30, "type": "TASK", "confidence": 0.5360945463180542}]}], "abstractContent": [{"text": "This paper describes LIMSI participation to the WMT'14 Shared Task on Quality Estimation; we took part to the word-level quality estimation task for English to Spanish translations.", "labels": [], "entities": [{"text": "WMT'14 Shared Task on Quality Estimation", "start_pos": 48, "end_pos": 88, "type": "TASK", "confidence": 0.6438415149847666}, {"text": "word-level quality estimation task", "start_pos": 110, "end_pos": 144, "type": "TASK", "confidence": 0.659196712076664}]}, {"text": "Our system relies on a random forest classifier, an ensemble method that has been shown to be very competitive for this kind of task, when only a few dense and continuous features are used.", "labels": [], "entities": []}, {"text": "Notably, only 16 features are used in our experiments.", "labels": [], "entities": []}, {"text": "These features describe, on the one hand, the quality of the association between the source sentence and each target word and, on the other hand, the fluency of the hypothesis.", "labels": [], "entities": []}, {"text": "Since the evaluation criterion is the f 1 measure, a specific tuning strategy is proposed to select the optimal values for the hyper-parameters.", "labels": [], "entities": []}, {"text": "Overall, our system achieves a 0.67 f 1 score on a randomly extracted test set.", "labels": [], "entities": []}], "introductionContent": [{"text": "This paper describes LIMSI submission to the WMT'14 Shared Task on Quality Estimation.", "labels": [], "entities": [{"text": "WMT'14 Shared Task on Quality Estimation", "start_pos": 45, "end_pos": 85, "type": "TASK", "confidence": 0.6612492005030314}]}, {"text": "We participated in the word-level quality estimation task (Task 2) for the English to Spanish direction.", "labels": [], "entities": [{"text": "word-level quality estimation task", "start_pos": 23, "end_pos": 57, "type": "TASK", "confidence": 0.6375262141227722}]}, {"text": "This task consists in predicting, for each word in a translation hypothesis, whether this word should be post-edited or should rather be kept unchanged.", "labels": [], "entities": []}, {"text": "Predicting translation quality at the word level raises several interesting challenges.", "labels": [], "entities": [{"text": "translation", "start_pos": 11, "end_pos": 22, "type": "TASK", "confidence": 0.9570416808128357}]}, {"text": "First, this is a (relatively) new task and the best way to formulate and evaluate it has still to be established.", "labels": [], "entities": []}, {"text": "Second, as most works on quality estimation have only considered prediction at the sentence level, it is not clear yet which features are really effective to predict quality at the word and a set of baseline features has still to be found.", "labels": [], "entities": [{"text": "quality estimation", "start_pos": 25, "end_pos": 43, "type": "TASK", "confidence": 0.7033581137657166}]}, {"text": "Finally, several characteristic of the task (the limited number of training examples, the unbalanced classes, etc.) makes the use of 'traditional' machine learning algorithms difficult.", "labels": [], "entities": []}, {"text": "This papers describes how we addressed this different issues for our participation to the WMT'14 Shared Task.", "labels": [], "entities": [{"text": "WMT'14 Shared Task", "start_pos": 90, "end_pos": 108, "type": "TASK", "confidence": 0.5478757520516714}]}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 gives an overview of the shared task data that will justify some of the design decisions we made.", "labels": [], "entities": []}, {"text": "Section 3 describes the different features we have considered and Section 4, the learning methods used to estimate the classifiers parameters.", "labels": [], "entities": []}, {"text": "Finally the results of our models are presented and analyzed in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "The features and learning strategies described in the two previous sections were evaluated on the English to Spanish datasets.", "labels": [], "entities": [{"text": "English to Spanish datasets", "start_pos": 98, "end_pos": 125, "type": "DATASET", "confidence": 0.5942126214504242}]}, {"text": "As no official development set was provided by the shared task organizers, we randomly sampled 200 sentences from the training set and use them as a test set throughout the rest of this article.", "labels": [], "entities": []}, {"text": "Preliminary experiments show that the choice of this test has a very low impact on the classification performance.", "labels": [], "entities": [{"text": "classification", "start_pos": 87, "end_pos": 101, "type": "TASK", "confidence": 0.9625424146652222}]}, {"text": "The different hyper-parameters of the training algorithm were chosen by maximizing classification performance (as evaluated by the f 1 score) estimated on 150 sentences of the training set kept apart as a validation set.", "labels": [], "entities": []}, {"text": "Results for the different learning algorithms considered are presented in.", "labels": [], "entities": []}, {"text": "Random forest clearly outperforms a simple logistic regression, which shows the importance of using nonlinear decision functions, a conclusion at pair with our previous results (.", "labels": [], "entities": []}, {"text": "The overall performance, with a f 1 measure of 0.67, is pretty low and in our opinion, not good enough to consider using such a quality estimation system in a computer-assisted post-edition context.", "labels": [], "entities": []}, {"text": "However, as shown in, the prediction performance highly depends on the POS category of the words: it is quite good for 'plain' words (like verb and nouns) but much worse for other categories.", "labels": [], "entities": []}, {"text": "There are two possible explanations for this observation: predicting the correctness of some morpho-syntaxic categories maybe intrinsically harder (e.g. for punctuation the choice of which can be highly controversial) or depend on information that is not currently available to our system.", "labels": [], "entities": []}, {"text": "In particular, we do not consider any information about the structure of the sentence and about the labels of the context, which may explain why our system does not perform well in predicting the labels of determiners and conjunctions.", "labels": [], "entities": []}, {"text": "In both cases, this result brings us to moderate our previous conclusions: as a wrong punctuation sign has not the same impact on translation quality as a wrong verb, our system might, regardless of its f 1 score, be able to provide useful information about the quality of a translation.", "labels": [], "entities": []}, {"text": "This also suggests that we should look fora more 'task-oriented' metric.", "labels": [], "entities": []}, {"text": "Finally, displays the importance of the different features used in our system.", "labels": [], "entities": []}, {"text": "Random forests deliver a quantification of the importance of a feature with respect to the predictability of the target variable.", "labels": [], "entities": []}, {"text": "This quantification is derived from the position of a feature in a decision tree: features used in the top nodes of the trees, which contribute to the final prediction decision of a larger fraction of the input samples, play a more important role than features used near the leaves of the tree.", "labels": [], "entities": []}, {"text": "It appears that, as for our previous experiments ( , the most relevant feature for predicting translation quality is the feature derived from the SOUL language model, even if other fluency features seem to also play an important role.", "labels": [], "entities": [{"text": "predicting translation quality", "start_pos": 83, "end_pos": 113, "type": "TASK", "confidence": 0.8323675990104675}]}, {"text": "Surprisingly enough, features related to the pseudo-reference do not seem to be useful.", "labels": [], "entities": []}, {"text": "Further experiments are needed to explain the reasons of this observation.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Number of examples and distribution of  labels for the different systems on the training set", "labels": [], "entities": []}, {"text": " Table 2: Distribution of labels according to the  POS on the training set", "labels": [], "entities": []}, {"text": " Table 3: Prediction performance for the two learn- ing strategies considered", "labels": [], "entities": [{"text": "Prediction", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.8654770851135254}, {"text": "learn- ing", "start_pos": 45, "end_pos": 55, "type": "TASK", "confidence": 0.6477222343285879}]}, {"text": " Table 4: Prediction performance for each POS tag", "labels": [], "entities": [{"text": "Prediction", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9531704187393188}]}]}