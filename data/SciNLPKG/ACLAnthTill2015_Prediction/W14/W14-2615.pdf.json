{"title": [{"text": "Opinion Mining and Topic Categorization with Novel Term Weighting", "labels": [], "entities": [{"text": "Opinion Mining", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.8407786786556244}]}], "abstractContent": [{"text": "In this paper we investigate the efficiency of the novel term weighting algorithm for opinion mining and topic categorization of articles from newspapers and Internet.", "labels": [], "entities": [{"text": "opinion mining", "start_pos": 86, "end_pos": 100, "type": "TASK", "confidence": 0.7951782941818237}, {"text": "topic categorization of articles from newspapers", "start_pos": 105, "end_pos": 153, "type": "TASK", "confidence": 0.8537333707014719}]}, {"text": "We compare the novel term weighting technique with existing approaches such as TF-IDF and ConfWeight.", "labels": [], "entities": [{"text": "ConfWeight", "start_pos": 90, "end_pos": 100, "type": "DATASET", "confidence": 0.876142680644989}]}, {"text": "The performance on the data from the text-mining campaigns DEFT'07 and DEFT'08 shows that the proposed method can compete with existing information retrieval models in classification quality and that it is computationally faster.", "labels": [], "entities": [{"text": "DEFT'07", "start_pos": 59, "end_pos": 66, "type": "DATASET", "confidence": 0.9457980990409851}, {"text": "DEFT'08", "start_pos": 71, "end_pos": 78, "type": "DATASET", "confidence": 0.6967005729675293}]}, {"text": "The proposed text preprocessing method can be applied in large-scale information retrieval and data mining problems and it can be easily transported to different domains and different languages since it does not require any domain related or linguistic information.", "labels": [], "entities": [{"text": "text preprocessing", "start_pos": 13, "end_pos": 31, "type": "TASK", "confidence": 0.7448699176311493}, {"text": "information retrieval", "start_pos": 69, "end_pos": 90, "type": "TASK", "confidence": 0.7478585243225098}, {"text": "data mining", "start_pos": 95, "end_pos": 106, "type": "TASK", "confidence": 0.6969667375087738}]}], "introductionContent": [{"text": "Nowadays, Internet and social media generate a huge amount of textual information.", "labels": [], "entities": []}, {"text": "It is increasingly important to develop methods of text processing such as text classification.", "labels": [], "entities": [{"text": "text processing", "start_pos": 51, "end_pos": 66, "type": "TASK", "confidence": 0.7156236916780472}, {"text": "text classification", "start_pos": 75, "end_pos": 94, "type": "TASK", "confidence": 0.7933797836303711}]}, {"text": "Text classification is very important for such problems as automatic opining mining (sentiment analysis) and topic categorization of different articles from newspapers and Internet.", "labels": [], "entities": [{"text": "Text classification", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7388485670089722}, {"text": "automatic opining mining", "start_pos": 59, "end_pos": 83, "type": "TASK", "confidence": 0.5997877021630605}, {"text": "sentiment analysis", "start_pos": 85, "end_pos": 103, "type": "TASK", "confidence": 0.8166230916976929}, {"text": "topic categorization of different articles from newspapers", "start_pos": 109, "end_pos": 167, "type": "TASK", "confidence": 0.8445041222231728}]}, {"text": "Text classification can be considered to be apart of natural language understanding, where there is a set of predefined categories and the task is to automatically assign new documents to one of these categories.", "labels": [], "entities": [{"text": "Text classification", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8366206884384155}, {"text": "natural language understanding", "start_pos": 53, "end_pos": 83, "type": "TASK", "confidence": 0.6984031597773234}]}, {"text": "The method of text preprocessing and text representation influences the results that are obtained even with the same classification algorithms.", "labels": [], "entities": [{"text": "text preprocessing", "start_pos": 14, "end_pos": 32, "type": "TASK", "confidence": 0.7548137903213501}, {"text": "text representation", "start_pos": 37, "end_pos": 56, "type": "TASK", "confidence": 0.8071273565292358}]}, {"text": "The most popular model for text classification is vector space model.", "labels": [], "entities": [{"text": "text classification", "start_pos": 27, "end_pos": 46, "type": "TASK", "confidence": 0.8304566740989685}]}, {"text": "In this case text categorization maybe considered as a machine learning problem.", "labels": [], "entities": [{"text": "text categorization", "start_pos": 13, "end_pos": 32, "type": "TASK", "confidence": 0.824611097574234}]}, {"text": "Complexity of text categorization with vector space model is compounded by the need to extract the numerical data from text information before applying machine learning methods.", "labels": [], "entities": [{"text": "text categorization", "start_pos": 14, "end_pos": 33, "type": "TASK", "confidence": 0.7375724017620087}]}, {"text": "Therefore text categorization consists of two parts: text preprocessing and classification using obtained numerical data.", "labels": [], "entities": [{"text": "text categorization", "start_pos": 10, "end_pos": 29, "type": "TASK", "confidence": 0.8306565284729004}]}, {"text": "All text preprocessing methods are based on the idea that the category of the document depends on the words or phrases from this document.", "labels": [], "entities": []}, {"text": "The simplest approach is to take each word of the document as a binary coordinate and the dimension of the feature space will be the number of words in our dictionary.", "labels": [], "entities": []}, {"text": "There exist more advanced approaches for text preprocessing to overcome this problem such as TF-IDF ( and ConfWeight methods (Soucy and.", "labels": [], "entities": [{"text": "text preprocessing", "start_pos": 41, "end_pos": 59, "type": "TASK", "confidence": 0.7734217047691345}]}, {"text": "A novel term weighting method () is also considered, which has some similarities with the ConfWeight method, but has improved computational efficiency.", "labels": [], "entities": []}, {"text": "It is important to notice that we use no morphological or stop-word filtering before text preprocessing.", "labels": [], "entities": []}, {"text": "It means that the text preprocessing can be performed without expert or linguistic knowledge and that the text preprocessing is language-independent.", "labels": [], "entities": []}, {"text": "In this paper we have used k-nearest neighbors algorithm, Bayes Classifier, support vector machine (SVM) generated and optimized with COBRA (Co-Operation of Biology Related Algorithms) which has been proposed by, Rocchio Classifier or Nearest Centroid Algorithm and Neural Network as classification methods.", "labels": [], "entities": []}, {"text": "have been used as implementation software.", "labels": [], "entities": []}, {"text": "For the application of algorithms and comparison of the results we have used the DEFT (\"D\u00e9fi Fouille de Texte\") Evaluation Package 2008 (Proceedings of the 4th DEFT Workshop, 2008) which has been provided by ELRA and publically available corpora from DEFT'07 (Proceedings of the 3rd.", "labels": [], "entities": [{"text": "DEFT (\"D\u00e9fi Fouille de Texte\") Evaluation Package 2008 (Proceedings of the 4th DEFT Workshop, 2008)", "start_pos": 81, "end_pos": 180, "type": "DATASET", "confidence": 0.8549464792013168}, {"text": "ELRA", "start_pos": 208, "end_pos": 212, "type": "DATASET", "confidence": 0.9318158626556396}, {"text": "DEFT'07", "start_pos": 251, "end_pos": 258, "type": "DATASET", "confidence": 0.9322072863578796}]}, {"text": "The main aim of this work is to evaluate the competitiveness of the novel term weighting () in comparison with the state-of-the-art techniques for opining mining and topic categorization.", "labels": [], "entities": []}, {"text": "The criteria using in the evaluation are classification quality and computational efficiency.", "labels": [], "entities": [{"text": "classification", "start_pos": 41, "end_pos": 55, "type": "TASK", "confidence": 0.9608644247055054}]}, {"text": "This paper is organized as follows: in Section 2, we describe details of the corpora.", "labels": [], "entities": []}, {"text": "Section 3 presents text preprocessing methods.", "labels": [], "entities": [{"text": "text preprocessing", "start_pos": 19, "end_pos": 37, "type": "TASK", "confidence": 0.7610531449317932}]}, {"text": "In Section 4 we describe the classification algorithms which we have used to compare different text preprocessing techniques.", "labels": [], "entities": []}, {"text": "Section 5 reports on the experimental results.", "labels": [], "entities": []}, {"text": "Finally, we provide concluding remarks in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "The DEFT (\"D\u00e9fi Fouille de Texte\") Evaluation Package 2008 and publically available corpora from DEFT'07 (Books, Games and Debates) have been used for algorithms application and results comparison.", "labels": [], "entities": [{"text": "DEFT (\"D\u00e9fi Fouille de Texte\") Evaluation Package 2008", "start_pos": 4, "end_pos": 58, "type": "DATASET", "confidence": 0.8690659046173096}, {"text": "DEFT'07", "start_pos": 97, "end_pos": 104, "type": "DATASET", "confidence": 0.8265506625175476}, {"text": "results comparison", "start_pos": 178, "end_pos": 196, "type": "TASK", "confidence": 0.6614434570074081}]}, {"text": "In order to evaluate obtained results with the campaign participants we have to use the same measure of classification quality: precision, recall and F-score.", "labels": [], "entities": [{"text": "precision", "start_pos": 128, "end_pos": 137, "type": "METRIC", "confidence": 0.9997159838676453}, {"text": "recall", "start_pos": 139, "end_pos": 145, "type": "METRIC", "confidence": 0.9992601275444031}, {"text": "F-score", "start_pos": 150, "end_pos": 157, "type": "METRIC", "confidence": 0.9975085258483887}]}, {"text": "Precision for each class i is calculated as the number of correctly classified articles for class i divided by the number of all articles which algorithm assigned for this class.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9878246784210205}]}, {"text": "Recall is the number of correctly classified articles for class i divided by the number of articles that should have been in this class.", "labels": [], "entities": []}, {"text": "Overall precision and recall are calculated as the arithmetic mean of the precisions and recalls for all classes (macroaverage).", "labels": [], "entities": [{"text": "precision", "start_pos": 8, "end_pos": 17, "type": "METRIC", "confidence": 0.9994184970855713}, {"text": "recall", "start_pos": 22, "end_pos": 28, "type": "METRIC", "confidence": 0.9987339377403259}, {"text": "precisions", "start_pos": 74, "end_pos": 84, "type": "METRIC", "confidence": 0.9987651109695435}, {"text": "recalls", "start_pos": 89, "end_pos": 96, "type": "METRIC", "confidence": 0.7148933410644531}]}, {"text": "F-score is calculated as the harmonic mean of precision and recall.", "labels": [], "entities": [{"text": "F-score", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.9809522032737732}, {"text": "precision", "start_pos": 46, "end_pos": 55, "type": "METRIC", "confidence": 0.9996033310890198}, {"text": "recall", "start_pos": 60, "end_pos": 66, "type": "METRIC", "confidence": 0.9977977275848389}]}, {"text": "present the F-scores obtained on the test corpora.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9984076619148254}]}, {"text": "The best values for each problem are shown in bold.", "labels": [], "entities": []}, {"text": "Results of the all classification algorithms are presented with the best parameters.", "labels": [], "entities": []}, {"text": "We also present for each corpus only the best TF-IDF modification.", "labels": [], "entities": []}, {"text": "We can see from that the best F-scores have been obtained with either ConfWeight or novel Term Weighting preprocessing.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9943723678588867}, {"text": "ConfWeight", "start_pos": 70, "end_pos": 80, "type": "DATASET", "confidence": 0.9041048288345337}]}, {"text": "The algorithm performances on the Games and Debates corpora achieved the best results with ConfWeight; however, we can see that the F-scores obtained with novel Term Weighting preprocessing are very similar (0.712 and 0.720 for Games; 0.700 and 0.714 for Debates).", "labels": [], "entities": [{"text": "ConfWeight", "start_pos": 91, "end_pos": 101, "type": "DATASET", "confidence": 0.8855888247489929}, {"text": "F-scores", "start_pos": 132, "end_pos": 140, "type": "METRIC", "confidence": 0.9892234802246094}]}, {"text": "Almost all best results have been obtained with SVM except the Games database where we achieved the highest F-score with k-NN algorithm.", "labels": [], "entities": [{"text": "SVM", "start_pos": 48, "end_pos": 51, "type": "DATASET", "confidence": 0.865847647190094}, {"text": "Games database", "start_pos": 63, "end_pos": 77, "type": "DATASET", "confidence": 0.9859674870967865}, {"text": "F-score", "start_pos": 108, "end_pos": 115, "type": "METRIC", "confidence": 0.9984531402587891}]}, {"text": "This paper focuses on the text preprocessing methods which do not require language or domain-related information; therefore, we have not tried to achieve the best possible classification quality.", "labels": [], "entities": []}, {"text": "However, the result obtained on Books corpus with novel TW preprocessing and SVM (generated using COBRA) as classification algorithm has reached 0.619 F-score which is higher than the best known performance 0.603 (Proceedings of the 3rd.", "labels": [], "entities": [{"text": "Books corpus", "start_pos": 32, "end_pos": 44, "type": "DATASET", "confidence": 0.9868694245815277}, {"text": "F-score", "start_pos": 151, "end_pos": 158, "type": "METRIC", "confidence": 0.9989860653877258}]}, {"text": "Performances on other corpora have achieved close F-score values to the best submissions of the DEFT'07 and DEFT'08 participants.", "labels": [], "entities": [{"text": "F-score", "start_pos": 50, "end_pos": 57, "type": "METRIC", "confidence": 0.9994263648986816}, {"text": "DEFT'07", "start_pos": 96, "end_pos": 103, "type": "DATASET", "confidence": 0.9374995231628418}, {"text": "DEFT'08", "start_pos": 108, "end_pos": 115, "type": "DATASET", "confidence": 0.5775021910667419}]}, {"text": "We have also measured computational efficiency of each text preprocessing technique.", "labels": [], "entities": []}, {"text": "We have run each method 20 times using the Baden-W\u00fcrttemberg Grid (bwGRiD) Cluster Ulm (Every blade comprehends two 4-Core Intel Harpertown CPUs with 2.83 GHz and 16 GByte RAM).", "labels": [], "entities": [{"text": "Baden-W\u00fcrttemberg Grid (bwGRiD) Cluster Ulm", "start_pos": 43, "end_pos": 86, "type": "DATASET", "confidence": 0.716690685067858}, {"text": "Harpertown", "start_pos": 129, "end_pos": 139, "type": "DATASET", "confidence": 0.969150722026825}]}, {"text": "After that we calculated average values and checked statistical significance of the results.", "labels": [], "entities": [{"text": "statistical significance", "start_pos": 52, "end_pos": 76, "type": "METRIC", "confidence": 0.9210990369319916}]}, {"text": "The average value for all TF-IDF modifications is presented because the time variation for the modifications is not significant.", "labels": [], "entities": []}, {"text": "We can see in and that TF-IDF and novel TW require almost the same computational time.", "labels": [], "entities": []}, {"text": "The most time-consuming method is ConfWeight (CW).", "labels": [], "entities": []}, {"text": "It requires approximately six times more time than TF-IDF and novel TW for DEFT'08 corpora and about three-four times more time than TF-IDF and novel TW for DEFT'07 databases.", "labels": [], "entities": [{"text": "DEFT'08", "start_pos": 75, "end_pos": 82, "type": "DATASET", "confidence": 0.814582109451294}, {"text": "DEFT'07 databases", "start_pos": 157, "end_pos": 174, "type": "DATASET", "confidence": 0.8470056354999542}]}], "tableCaptions": [{"text": " Table 1. Corpora description (DEFT'07)", "labels": [], "entities": [{"text": "DEFT'07", "start_pos": 31, "end_pos": 38, "type": "DATASET", "confidence": 0.8225494027137756}]}, {"text": " Table 2. Corpora description (DEFT'08)", "labels": [], "entities": [{"text": "DEFT'08", "start_pos": 31, "end_pos": 38, "type": "DATASET", "confidence": 0.8377315998077393}]}, {"text": " Table 3. Classification results for Books", "labels": [], "entities": []}, {"text": " Table 4. Classification results for Games", "labels": [], "entities": []}, {"text": " Table 5. Classification results for Debates", "labels": [], "entities": [{"text": "Debates", "start_pos": 37, "end_pos": 44, "type": "TASK", "confidence": 0.7330498695373535}]}, {"text": " Table 6. Classification results for T1", "labels": [], "entities": [{"text": "T1", "start_pos": 37, "end_pos": 39, "type": "TASK", "confidence": 0.5604522228240967}]}, {"text": " Table 7. Classification results for T2", "labels": [], "entities": [{"text": "T2", "start_pos": 37, "end_pos": 39, "type": "TASK", "confidence": 0.6507578492164612}]}]}