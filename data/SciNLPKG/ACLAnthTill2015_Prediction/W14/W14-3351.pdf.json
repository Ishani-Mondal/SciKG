{"title": [{"text": "IPA and STOUT: Leveraging Linguistic and Source-based Features for Machine Translation Evaluation", "labels": [], "entities": [{"text": "Machine Translation Evaluation", "start_pos": 67, "end_pos": 97, "type": "TASK", "confidence": 0.8584548830986023}]}], "abstractContent": [{"text": "This paper describes the UPC submissions to the WMT14 Metrics Shared Task: UPC-IPA and UPC-STOUT.", "labels": [], "entities": [{"text": "WMT14 Metrics Shared Task", "start_pos": 48, "end_pos": 73, "type": "TASK", "confidence": 0.641338050365448}, {"text": "UPC-IPA", "start_pos": 75, "end_pos": 82, "type": "DATASET", "confidence": 0.7426257729530334}, {"text": "UPC-STOUT", "start_pos": 87, "end_pos": 96, "type": "DATASET", "confidence": 0.8883494138717651}]}, {"text": "These metrics use a collection of evaluation measures integrated in ASIYA, a toolkit for machine translation evaluation.", "labels": [], "entities": [{"text": "machine translation evaluation", "start_pos": 89, "end_pos": 119, "type": "TASK", "confidence": 0.8959316412607828}]}, {"text": "In addition to some standard metrics, the two submissions take advantage of novel metrics that consider linguistic structures, lexical relationships, and semantics to compare both source and reference translation against the candidate translation.", "labels": [], "entities": []}, {"text": "The new metrics are available for several target languages other than En-glish.", "labels": [], "entities": []}, {"text": "In the the official WMT14 evaluation , UPC-IPA and UPC-STOUT scored above the average in 7 out of 9 language pairs at the system level and 8 out of 9 at the segment level.", "labels": [], "entities": [{"text": "WMT14 evaluation", "start_pos": 20, "end_pos": 36, "type": "DATASET", "confidence": 0.7918180525302887}, {"text": "UPC-IPA", "start_pos": 39, "end_pos": 46, "type": "DATASET", "confidence": 0.8917925953865051}, {"text": "UPC-STOUT", "start_pos": 51, "end_pos": 60, "type": "DATASET", "confidence": 0.85682612657547}]}], "introductionContent": [{"text": "Evaluating Machine Translation (MT) quality is a difficult task, in which even human experts may fail to achieve a high degree of agreement when assessing translations.", "labels": [], "entities": [{"text": "Evaluating Machine Translation (MT)", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.7949076543251673}]}, {"text": "Conducting manual evaluations is impractical during the development cycle of MT systems or for transation applications addressed to general users, such as online translation portals.", "labels": [], "entities": [{"text": "MT", "start_pos": 77, "end_pos": 79, "type": "TASK", "confidence": 0.9772300124168396}]}, {"text": "Automatic evaluation measures bring valuable benefits in such situations.", "labels": [], "entities": []}, {"text": "Compared to manual evaluation, automatic measures are cheap, more objective, and reusable across different test sets and domains.", "labels": [], "entities": []}, {"text": "Nonetheless, automatic metrics are far from perfection: when used in isolation, they tend to stress specific aspects of the translation quality and neglect others (particularly during tuning); they are often unable to capture little system improvements (enhancements in very specific aspects of the translation process); and they may make unfair comparisons when they are notable to reflect real differences among the quality of different MT systems.", "labels": [], "entities": [{"text": "MT", "start_pos": 439, "end_pos": 441, "type": "TASK", "confidence": 0.9660710096359253}]}, {"text": "ASIYA, the core of our approach, is an opensource suite for automatic machine translation evaluation and output analysis.", "labels": [], "entities": [{"text": "machine translation evaluation and output analysis", "start_pos": 70, "end_pos": 120, "type": "TASK", "confidence": 0.7298050969839096}]}, {"text": "It provides a rich set of heterogeneous metrics and tools to evaluate and analyse the quality of automatic translations.", "labels": [], "entities": []}, {"text": "The ASIYA core toolkit was first released in 2009 () and has been continuously improved and extended since then (.", "labels": [], "entities": []}, {"text": "In this paper we first describe the most recent enhancements to ASIYA: (i) linguistic-based metrics for an extended set of source-based metrics for English, Spanish, German, French, Russian, and Czech; and (iii) the integration of mechanisms to exploit the alignments between sources and translations.", "labels": [], "entities": [{"text": "ASIYA", "start_pos": 64, "end_pos": 69, "type": "TASK", "confidence": 0.9242220520973206}]}, {"text": "These enhancements are all available in ASIYA since version 3.0.", "labels": [], "entities": []}, {"text": "We have used them to prepare the UPC submissions to the WMT14 Metrics Task: UPC-IPA and UPC-STOUT, which serve the purpose of testing their usefulness in areal comparative setting.", "labels": [], "entities": [{"text": "WMT14 Metrics Task: UPC-IPA", "start_pos": 56, "end_pos": 83, "type": "DATASET", "confidence": 0.7008936882019043}, {"text": "UPC-STOUT", "start_pos": 88, "end_pos": 97, "type": "DATASET", "confidence": 0.7387664318084717}]}, {"text": "The rest of the paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes the new reference-based metrics developed, including syntactic parsers for languages other than English.", "labels": [], "entities": []}, {"text": "Section 3 gives the details of novel source-based metrics, developed for almost all the language pairs in this challenge.", "labels": [], "entities": []}, {"text": "Section 4 explains our simple metrics combination strategy and analyses the results obtained with both approaches, UPC-IPA and UPC-STOUT, when applied to the WMT13 dataset.", "labels": [], "entities": [{"text": "UPC-IPA", "start_pos": 115, "end_pos": 122, "type": "DATASET", "confidence": 0.73702073097229}, {"text": "UPC-STOUT", "start_pos": 127, "end_pos": 136, "type": "DATASET", "confidence": 0.6960278749465942}, {"text": "WMT13 dataset", "start_pos": 158, "end_pos": 171, "type": "DATASET", "confidence": 0.9801301956176758}]}, {"text": "Finally, Section 5 summarises our main contributions.", "labels": [], "entities": []}], "datasetContent": [{"text": "The tuning and selection of the different metrics to build UPC-IPA and UPC-STOUT was conducted considering the WMT13 Metrics Task dataset, and the resources distributed for the WMT13 Translation Task ( . gives a complete list of these metrics grouped by families.", "labels": [], "entities": [{"text": "UPC-IPA", "start_pos": 59, "end_pos": 66, "type": "DATASET", "confidence": 0.8690919876098633}, {"text": "UPC-STOUT", "start_pos": 71, "end_pos": 80, "type": "DATASET", "confidence": 0.8771941065788269}, {"text": "WMT13 Metrics Task dataset", "start_pos": 111, "end_pos": 137, "type": "DATASET", "confidence": 0.9106215387582779}, {"text": "WMT13 Translation Task", "start_pos": 177, "end_pos": 199, "type": "TASK", "confidence": 0.5924198031425476}]}, {"text": "First, we calculated the Pearson's correlation with the human judgements for all the metrics in the current version of the ASIYA repository, including standard MT evaluation metrics, such as METEOR (), GTM (, -TERp-A (Snover et al., 2009) (a variant of TER tuned towards adequacy), WER () and PER (Tillmann et al., 1997).", "labels": [], "entities": [{"text": "Pearson's correlation", "start_pos": 25, "end_pos": 46, "type": "METRIC", "confidence": 0.9366448720296224}, {"text": "ASIYA repository", "start_pos": 123, "end_pos": 139, "type": "DATASET", "confidence": 0.8337844610214233}, {"text": "MT evaluation", "start_pos": 160, "end_pos": 173, "type": "TASK", "confidence": 0.8727861046791077}, {"text": "METEOR", "start_pos": 191, "end_pos": 197, "type": "METRIC", "confidence": 0.9547467231750488}, {"text": "TERp-A", "start_pos": 210, "end_pos": 216, "type": "METRIC", "confidence": 0.9427977204322815}, {"text": "WER", "start_pos": 282, "end_pos": 285, "type": "METRIC", "confidence": 0.9876028895378113}, {"text": "PER", "start_pos": 293, "end_pos": 296, "type": "METRIC", "confidence": 0.99727863073349}]}, {"text": "We selected the best performing metrics (i.e., those resulting in high Pearson coefficients) in each family across all the From/To English translation language pairs, including the newly developed measures -even if they performed poorly compared to others (see This is how the UPC-STOUT metrics sets for both from English and To English translation pairs were composed 6 (see).", "labels": [], "entities": [{"text": "Pearson", "start_pos": 71, "end_pos": 78, "type": "METRIC", "confidence": 0.9747098088264465}, {"text": "UPC-STOUT metrics sets", "start_pos": 277, "end_pos": 299, "type": "DATASET", "confidence": 0.7967218557993571}]}, {"text": "The metric sets included in UPC-IPA are light versions of the UPC-STOUT ones.", "labels": [], "entities": [{"text": "UPC-IPA", "start_pos": 28, "end_pos": 35, "type": "DATASET", "confidence": 0.8880904316902161}, {"text": "UPC-STOUT", "start_pos": 62, "end_pos": 71, "type": "DATASET", "confidence": 0.9363566637039185}]}, {"text": "They were composed following different criteria, depending on the translation direction.", "labels": [], "entities": []}, {"text": "Parsing-based measures were already available in the previous version of ASIYA when translating into Englishthey are known to be robust across domains and are usually good indicators of translation quality ().", "labels": [], "entities": [{"text": "Parsing-based", "start_pos": 0, "end_pos": 13, "type": "METRIC", "confidence": 0.8923693299293518}]}, {"text": "So, in order to assess the gain achieved with these measures with respect the new ones, UPC-IPA neglects the measures based on structural information obtained from parsers.", "labels": [], "entities": [{"text": "UPC-IPA", "start_pos": 88, "end_pos": 95, "type": "DATASET", "confidence": 0.5170241594314575}]}, {"text": "In contrast, this distinction was not suitable for the From English pairs since the number of resources and measures varies for each language.", "labels": [], "entities": []}, {"text": "Hence, in this latter case, UPC-IPA used only the subset of measures from UPC-STOUT that required no or little resources.", "labels": [], "entities": [{"text": "UPC-IPA", "start_pos": 28, "end_pos": 35, "type": "DATASET", "confidence": 0.8903642892837524}, {"text": "UPC-STOUT", "start_pos": 74, "end_pos": 83, "type": "DATASET", "confidence": 0.9421517252922058}]}, {"text": "In summary, when English is the target language, UPC-IPA uses the baseline evaluation metrics along with the length factor, alignmentsbased metrics, character n-grams, and ESA.", "labels": [], "entities": [{"text": "length factor", "start_pos": 109, "end_pos": 122, "type": "METRIC", "confidence": 0.954388827085495}, {"text": "ESA", "start_pos": 172, "end_pos": 175, "type": "METRIC", "confidence": 0.9919100403785706}]}, {"text": "In addition to the above metrics, UPC-STOUT uses the linguistic-based metrics over parsing trees, named entities, and semantic roles.", "labels": [], "entities": []}, {"text": "When English is the source language, UPC-IPA relies on the basic collection of metrics and character n-grams only.", "labels": [], "entities": []}, {"text": "UPC-STOUT includes the alignment-based metrics, length factor, ESA, and the syntactic parsers applied to both German and French.", "labels": [], "entities": [{"text": "UPC-STOUT", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.9302797913551331}, {"text": "length factor", "start_pos": 48, "end_pos": 61, "type": "METRIC", "confidence": 0.9803565442562103}, {"text": "ESA", "start_pos": 63, "end_pos": 66, "type": "METRIC", "confidence": 0.9946809411048889}]}, {"text": "In all cases (metric sets and language pairs), the translation quality score is computed as the uniformly-averaged linear combination (ULC) of all the individual metrics for each sentence in the testset.", "labels": [], "entities": [{"text": "uniformly-averaged linear combination (ULC)", "start_pos": 96, "end_pos": 139, "type": "METRIC", "confidence": 0.7119618207216263}]}, {"text": "Its calculation implies the normalization of heterogeneous scores (some of them are negative or unbounded), into the range.", "labels": [], "entities": []}, {"text": "As a consequence, the scores of UPC-IPA and UPC-STOUT constitute a natural way of ranking different translations, rather than an overall quality estimation measure.", "labels": [], "entities": [{"text": "UPC-IPA", "start_pos": 32, "end_pos": 39, "type": "DATASET", "confidence": 0.9484505653381348}, {"text": "UPC-STOUT", "start_pos": 44, "end_pos": 53, "type": "DATASET", "confidence": 0.9349001049995422}]}, {"text": "We opt for this linear combination for simplicity.", "labels": [], "entities": []}, {"text": "The discussion below suggests that a more sophisticated method for weight tuning (e.g., relying on machine learning methods) would be required for each language pair, domain and/or task since different metric families perform notably different for each subtask.", "labels": [], "entities": [{"text": "weight tuning", "start_pos": 67, "end_pos": 80, "type": "TASK", "confidence": 0.7832068204879761}]}, {"text": "We complete our experimentation by evaluating more configurations: BAS, a baseline  with standard and commonly used MT metrics; SYN, the reference-based syntactic metrics; SEM, the reference-based semantic metrics; SRC, the source-based metrics; and the combination of BAS with every other configuration: BAS+SYN, BAS+SEM, and BAS+SRC.", "labels": [], "entities": [{"text": "BAS", "start_pos": 67, "end_pos": 70, "type": "METRIC", "confidence": 0.9469249248504639}]}, {"text": "Their purpose is to evaluate the contribution of the newly developed sets of metrics with respect to the baseline.", "labels": [], "entities": []}, {"text": "The composition of the different configurations is summarised in.", "labels": [], "entities": []}, {"text": "For each configuration and language pair, we show the correlation coefficients obtained at the systemand the segment-level, respectively.", "labels": [], "entities": []}, {"text": "As customary with the WMT13 dataset, Pearson correlation was computed at the system-level, whereas Kendall's \u03c4 was used to estimate segment-level rank correlations.", "labels": [], "entities": [{"text": "WMT13 dataset", "start_pos": 22, "end_pos": 35, "type": "DATASET", "confidence": 0.9647735953330994}, {"text": "Pearson correlation", "start_pos": 37, "end_pos": 56, "type": "METRIC", "confidence": 0.9056121408939362}]}, {"text": "Additionally to the two submitted and seven extra configurations, we include the coefficients obtained with the Best and Worst systems reported in the official WMT13 evaluation for each language pair.", "labels": [], "entities": [{"text": "WMT13 evaluation", "start_pos": 160, "end_pos": 176, "type": "DATASET", "confidence": 0.8298207819461823}]}, {"text": "Although the results of our two submitted systems are not radically different to each other, UPC-STOUT consistently outperforms UPC-IPA.", "labels": [], "entities": [{"text": "UPC-STOUT", "start_pos": 93, "end_pos": 102, "type": "DATASET", "confidence": 0.7603060603141785}, {"text": "UPC-IPA", "start_pos": 128, "end_pos": 135, "type": "DATASET", "confidence": 0.9411652684211731}]}, {"text": "The currently available version of ASIYA, including the new metrics, allows fora performance close to the top-performing evaluation measures in last year's challenge, even with our na\u00a8\u0131vena\u00a8\u0131ve combination strategy.", "labels": [], "entities": []}, {"text": "It is worth noting that no configuration behaves the same way throughout the different languages.", "labels": [], "entities": []}, {"text": "In some cases (e.g., with the SRC configuration), the bad performance can be explained by the weaknesses of the necessary resources when computing certain metrics.", "labels": [], "entities": []}, {"text": "When analysed in detail, the cause can be ascribed to different metric families in each case.", "labels": [], "entities": []}, {"text": "As a result, it is clear that specific configurations are necessary for evaluating different languages and domains.", "labels": [], "entities": []}, {"text": "We plan to approach these issues as part of our future work.", "labels": [], "entities": []}, {"text": "When looking at the system-level figures, one can observe that the SEM set allows fora considerable improvement over the baseline system.", "labels": [], "entities": [{"text": "SEM", "start_pos": 67, "end_pos": 70, "type": "METRIC", "confidence": 0.5502378344535828}]}, {"text": "The further inclusion of the SYN set -when available-, tends to increase the quality of the estimations, mainly when English is the source language.", "labels": [], "entities": [{"text": "SYN set", "start_pos": 29, "end_pos": 36, "type": "DATASET", "confidence": 0.6925580054521561}]}, {"text": "These properties impact on some of the UPC-STOUT configurations.", "labels": [], "entities": []}, {"text": "In contrast, when looking at the segment-level scores, while the SYN measures still tend to provide some gain over the baseline, the SEM ones do not.", "labels": [], "entities": []}, {"text": "Finally, it merits some attention the good results achieved by the baseline for translations into English.", "labels": [], "entities": [{"text": "translations into English", "start_pos": 80, "end_pos": 105, "type": "TASK", "confidence": 0.8818463484446207}]}, {"text": "We may remark here that our baseline included also the best performing state-of-the-art metrics, including all the variants of METEOR, that reported good results in the WMT13 challenge.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 127, "end_pos": 133, "type": "METRIC", "confidence": 0.9555541276931763}, {"text": "WMT13 challenge", "start_pos": 169, "end_pos": 184, "type": "TASK", "confidence": 0.5489936172962189}]}, {"text": "show the official results obtained by UPC-IPA and UPC-STOUT in WMT14.", "labels": [], "entities": [{"text": "UPC-IPA", "start_pos": 38, "end_pos": 45, "type": "DATASET", "confidence": 0.9561494588851929}, {"text": "UPC-STOUT", "start_pos": 50, "end_pos": 59, "type": "DATASET", "confidence": 0.9370407462120056}, {"text": "WMT14", "start_pos": 63, "end_pos": 68, "type": "DATASET", "confidence": 0.8845260739326477}]}, {"text": "The best and worst figures for each language pair are included for comparison -the worst performing submission at segment level is neglected as it seems to be a dummy to appear).", "labels": [], "entities": []}, {"text": "Both UPC-IPA and UPC-STOUT configurations resulted in different performances depending on the language pair.", "labels": [], "entities": []}, {"text": "UPC-STOUT scored above the average for all the language pairs except for en-cs at both system and segment level, and en-ru at system level.", "labels": [], "entities": [{"text": "UPC-STOUT", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.931381106376648}]}, {"text": "Although the evaluation results are not directly comparable to the WMT13 ones, one can note that the results were notably better for pairs that involved Czech and Russian, and worse for those that involved French and German at system level.", "labels": [], "entities": [{"text": "WMT13", "start_pos": 67, "end_pos": 72, "type": "DATASET", "confidence": 0.8147419691085815}]}, {"text": "Analysing the impact of the evaluation methods and building comparable results in order to address a study on configurations for different languages is part of our future work.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Length factor parameters as estimated on  the WMT13 parallel corpora.  pair  \u00b5  \u03c3  pair  \u00b5  \u03c3  en-cs 0.972 0.245 cs-en 1.085 0.273  en-de 1.176 0.926 de-en 0.961 0.463  en-fr 1.158 0.411 fr-en 0.914 0.313  en-ru 1.157 0.678 ru-en 1.069 0.668", "labels": [], "entities": [{"text": "Length", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9644327163696289}, {"text": "WMT13 parallel corpora", "start_pos": 56, "end_pos": 78, "type": "DATASET", "confidence": 0.8692571719487509}]}, {"text": " Table 2: Metrics considered in the experiments  separated by families according to the type of  grammatical items they use.", "labels": [], "entities": []}, {"text": " Table 3: Metrics considered in each system. 7", "labels": [], "entities": []}, {"text": " Table 4: System-level Pearson correlation for automatic metrics over translations From/To English.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 23, "end_pos": 42, "type": "METRIC", "confidence": 0.8622591197490692}]}, {"text": " Table 5: Segment-level Kendall's \u03c4 correlation for automatic metrics over translations From/To English.", "labels": [], "entities": [{"text": "Segment-level Kendall's \u03c4 correlation", "start_pos": 10, "end_pos": 47, "type": "METRIC", "confidence": 0.7012577772140502}]}, {"text": " Table 6: System-level Pearson correlation results  in the WMT14 Metrics shared task", "labels": [], "entities": [{"text": "WMT14 Metrics shared task", "start_pos": 59, "end_pos": 84, "type": "DATASET", "confidence": 0.8670735508203506}]}, {"text": " Table 7: Segment-level Kendall's \u03c4 correlation re- sults in the WMT14 Metrics shared task", "labels": [], "entities": [{"text": "Segment-level Kendall's \u03c4 correlation", "start_pos": 10, "end_pos": 47, "type": "METRIC", "confidence": 0.7151493787765503}, {"text": "WMT14 Metrics shared task", "start_pos": 65, "end_pos": 90, "type": "DATASET", "confidence": 0.9398157149553299}]}]}