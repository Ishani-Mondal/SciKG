{"title": [{"text": "Overcoming the Curse of Sentence Length for Neural Machine Translation using Automatic Segmentation", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 44, "end_pos": 70, "type": "TASK", "confidence": 0.6796831289927164}]}], "abstractContent": [{"text": "The authors of (Cho et al., 2014a) have shown that the recently introduced neural network translation systems suffer from a significant drop in translation quality when translating long sentences, unlike existing phrase-based translation systems.", "labels": [], "entities": []}, {"text": "In this paper, we propose away to address this issue by automatically segmenting an input sentence into phrases that can be easily translated by the neural network translation model.", "labels": [], "entities": []}, {"text": "Once each segment has been independently translated by the neu-ral machine translation model, the translated clauses are concatenated to form a final translation.", "labels": [], "entities": [{"text": "neu-ral machine translation", "start_pos": 59, "end_pos": 86, "type": "TASK", "confidence": 0.5759295125802358}]}, {"text": "Empirical results show a significant improvement in translation quality for long sentences.", "labels": [], "entities": [{"text": "translation", "start_pos": 52, "end_pos": 63, "type": "TASK", "confidence": 0.9621012806892395}]}], "introductionContent": [{"text": "Up to now, most research efforts in statistical machine translation (SMT) research have relied on the use of a phrase-based system as suggested in (.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 36, "end_pos": 73, "type": "TASK", "confidence": 0.8125366717576981}]}, {"text": "Recently, however, an entirely new, neural network based approach has been proposed by several research groups, showing promising results, both as a standalone system or as an additional component in the existing phrase-based system.", "labels": [], "entities": []}, {"text": "In this neural network based approach, an encoder 'encodes' a variable-length input sentence into a fixed-length vector and a decoder 'decodes' a variable-length target sentence from the fixedlength encoded vector.", "labels": [], "entities": []}, {"text": "It has been observed in), and) that this neural network approach * Research done while these authors were visiting Universit\u00e9 de Montr\u00e9al works well with short sentences (e.g., 20 words), but has difficulty with long sentences (e.g., 20 words), and particularly with sentences that are longer than those used for training.", "labels": [], "entities": []}, {"text": "Training on long sentences is difficult because few available training corpora include sufficiently many long sentences, and because the computational overhead of each update iteration in training is linearly correlated with the length of training sentences.", "labels": [], "entities": []}, {"text": "Additionally, by the nature of encoding a variablelength sentence into a fixed-size vector representation, the neural network may fail to encode all the important details.", "labels": [], "entities": []}, {"text": "In this paper, hence, we propose to translate sentences piece-wise.", "labels": [], "entities": [{"text": "translate sentences piece-wise", "start_pos": 36, "end_pos": 66, "type": "TASK", "confidence": 0.8751656611760458}]}, {"text": "We segment an input sentence into a number of short clauses that can be confidently translated by the model.", "labels": [], "entities": []}, {"text": "We show empirically that this approach improves translation quality of long sentences, compared to using a neural network to translate a whole sentence without segmentation.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate the proposed approach on the task of English-to-French translation.", "labels": [], "entities": [{"text": "English-to-French translation", "start_pos": 49, "end_pos": 78, "type": "TASK", "confidence": 0.6665265709161758}]}, {"text": "We use a bilingual, parallel corpus of 348M words selected by the method of (Axelrod et al., 2011) from a combination of Europarl (61M), news commentary (5.5M), UN (421M) and two crawled corpora of 90M and 780M words respectively.", "labels": [], "entities": [{"text": "Europarl", "start_pos": 121, "end_pos": 129, "type": "DATASET", "confidence": 0.9833986759185791}]}, {"text": "The performance of our models was tested on news-test2012, news-test2013, and news-test2014.", "labels": [], "entities": [{"text": "news-test2012", "start_pos": 44, "end_pos": 57, "type": "DATASET", "confidence": 0.9529474973678589}, {"text": "news-test2013", "start_pos": 59, "end_pos": 72, "type": "DATASET", "confidence": 0.9338100552558899}, {"text": "news-test2014", "start_pos": 78, "end_pos": 91, "type": "DATASET", "confidence": 0.9597353339195251}]}, {"text": "When comparing with the phrase-based SMT system Moses (, the first two were used as a development set for tuning Moses while news-test2014 was used as our test set.", "labels": [], "entities": [{"text": "SMT", "start_pos": 37, "end_pos": 40, "type": "TASK", "confidence": 0.8961753249168396}]}, {"text": "To train the neural network models, we use only the sentence pairs in the parallel corpus, where both English and French sentences are at most 30 words long.", "labels": [], "entities": []}, {"text": "Furthermore, we limit our vocabulary size to the 30,000 most frequent words for both English and French.", "labels": [], "entities": []}, {"text": "All other words are considered unknown and mapped to a special token ().", "labels": [], "entities": []}, {"text": "In both neural network training and automatic segmentation, we do not incorporate any domainspecific knowledge, except when tokenizing the original text data.", "labels": [], "entities": [{"text": "automatic segmentation", "start_pos": 36, "end_pos": 58, "type": "TASK", "confidence": 0.5147487819194794}]}, {"text": "The datasets and trained Moses models can be downloaded from http://www-lium.univ-lemans.fr/ \u02dc schwenk/cslm_joint_paper/ and the website of ACL 2014 Ninth Workshop on Statistical Machine Translation (WMT 14).", "labels": [], "entities": [{"text": "ACL 2014 Ninth Workshop on Statistical Machine Translation (WMT 14)", "start_pos": 140, "end_pos": 207, "type": "TASK", "confidence": 0.6854526425401369}]}], "tableCaptions": [{"text": " Table 1:  BLEU score computed on  news-test2014 for two control experi- ments. Random segmentation refers to randomly  segmenting a sentence so that the mean and  variance of the segment lengths corresponded to  the ones our best segmentation method. Random  confidence score refers to segmenting a sentence  with randomly generated confidence score for  each segment.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 11, "end_pos": 15, "type": "METRIC", "confidence": 0.9987730383872986}, {"text": "Random  confidence score", "start_pos": 252, "end_pos": 276, "type": "METRIC", "confidence": 0.6562354564666748}]}, {"text": " Table 2: BLEU scores computed on the develop- ment and test sets. See the text for the description  of each approach. Moses refers to the scores by  the conventional phrase-based translation system.  The top five rows consider all sentences of each  data set, whilst the bottom five rows includes only  sentences with no unknown words", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9985699653625488}]}]}