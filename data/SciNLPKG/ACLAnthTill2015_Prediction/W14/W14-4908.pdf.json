{"title": [{"text": "A Web-based Geo-resolution Annotation and Evaluation Tool", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper we present the Edinburgh Geo-annotator, a web-based annotation tool for the manual geo-resolution of location mentions in text using a gazetteer.", "labels": [], "entities": [{"text": "Edinburgh Geo-annotator", "start_pos": 29, "end_pos": 52, "type": "DATASET", "confidence": 0.9216203987598419}]}, {"text": "The annotation tool has an inter-linked text and map interface which lets annotators pick correct candidates within the gazetteer more easily.", "labels": [], "entities": []}, {"text": "The geo-annotator can be used to correct the output of a geoparser or to create gold standard geo-resolution data.", "labels": [], "entities": []}, {"text": "We include accompanying scoring software for geo-resolution evaluation.", "labels": [], "entities": [{"text": "geo-resolution evaluation", "start_pos": 45, "end_pos": 70, "type": "TASK", "confidence": 0.7744154036045074}]}], "introductionContent": [{"text": "Many kinds of digitised content have an important geospatial dimension.", "labels": [], "entities": []}, {"text": "However not all geospatial information is immediately accessible, particularly in the case where it is implicit in place names in text.", "labels": [], "entities": []}, {"text": "The process of geo-resolution (also often referred to as geo-referencing, geoparsing or geotagging) links instances of textual geographic information to location coordinates, enabling searching and linking of digital content using its geospatial properties.", "labels": [], "entities": []}, {"text": "Geo-resolution tools can never be completely accurate and their performance can vary significantly depending on the type and quality of the input texts as well as on the gazetteer resources they consult.", "labels": [], "entities": []}, {"text": "For this reason, users of text collections are frequently disappointed in the results of geo-resolution and, depending on their application and dataset size, they may decide to take remedial action to improve the quality.", "labels": [], "entities": []}, {"text": "The tool we describe here is a web-based, manual annotation tool which can be used to correct the output of geo-resolution.", "labels": [], "entities": []}, {"text": "It has been developed in step with our geo-resolution system, the Edinburgh Geoparser ( , but it could also be used to correct the output of other tools.", "labels": [], "entities": [{"text": "Edinburgh Geoparser", "start_pos": 66, "end_pos": 85, "type": "DATASET", "confidence": 0.9746114313602448}]}, {"text": "In our work, we use the geo-annotator to create gold-standard material for geo-resolution evaluation and have produced accompanying scoring software.", "labels": [], "entities": [{"text": "geo-resolution evaluation", "start_pos": 75, "end_pos": 100, "type": "TASK", "confidence": 0.7722481489181519}]}], "datasetContent": [{"text": "It is important to be able to report the quality of a geo-resolver's performance in concrete and quantifiable terms.", "labels": [], "entities": []}, {"text": "Along with the annotation tool, we are therefore also releasing an evaluation script which compares the manually geo-resolved locations to those predicted by an automatic geoparser.", "labels": [], "entities": []}, {"text": "We follow standard practice in comparing system output to hand-annotated gold standard evaluation data.", "labels": [], "entities": []}, {"text": "The script evaluates the performance of the geo-resolution independently from geo-tagging, meaning that it only considers named entities which were tagged in the input to the manual geo-resolution annotation but not those that were missed.", "labels": [], "entities": []}, {"text": "It is therefore preferable to use input data which contains manually annotated or corrected location mentions.", "labels": [], "entities": []}, {"text": "The evaluation script computes the number of correctly geo-resolved locations and accuracy in percent.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 82, "end_pos": 90, "type": "METRIC", "confidence": 0.9996850490570068}]}, {"text": "Both figures are presented fora strict evaluation of exact match against gazetteer identifier and fora lax evaluation where the grid references of the gold and the system choice have to occur within a small distance of one another to count as a match.", "labels": [], "entities": []}, {"text": "For a pair of location candidates (gold vs. system), we compute the Great-circle distance using a special case of the Vincenty formula which is most accurate for all distances.", "labels": [], "entities": []}, {"text": "The lax evaluation is provided as even with clear annotation guidelines, annotators  can find it difficult to chose between different location types for essentially the same place (e.g. seethe example for Dublin in.", "labels": [], "entities": [{"text": "Dublin", "start_pos": 205, "end_pos": 211, "type": "DATASET", "confidence": 0.9671092629432678}]}, {"text": "During the manual annotation, three special cases can arise.", "labels": [], "entities": []}, {"text": "Some location mentions do not have a candidate in the gazetteer (those appearing in blue), while others do have candidates in the gazetteer but the annotator does not consider any of them correct.", "labels": [], "entities": []}, {"text": "Occasionally there are location mentions with one or more candidates in the gazetteer but an annotator neither chooses one of them nor selects \"none\".", "labels": [], "entities": []}, {"text": "The latter cases are considered to be annotation errors, usually because the annotator has forgotten to resolve them.", "labels": [], "entities": []}, {"text": "The evaluation excludes all three cases when computing accuracy scores but notes them in the evaluation report in order to facilitate error analysis (see sample output in. total: 11 exact: 10 (90.9\\%) within 6.0km 11 (100.0\\%) note: no gold choice for British Empire note: annotator selected \"none\" for Irish Free State: Sample output of the geo-resolution evaluation script.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 55, "end_pos": 63, "type": "METRIC", "confidence": 0.99276202917099}, {"text": "exact", "start_pos": 182, "end_pos": 187, "type": "METRIC", "confidence": 0.9851701855659485}, {"text": "Irish Free State", "start_pos": 303, "end_pos": 319, "type": "DATASET", "confidence": 0.9509341518084208}]}, {"text": "When setting the lax evaluation to 6km, one candidate selected by the system was close enough to the gold candidate to count as a match.", "labels": [], "entities": []}], "tableCaptions": []}