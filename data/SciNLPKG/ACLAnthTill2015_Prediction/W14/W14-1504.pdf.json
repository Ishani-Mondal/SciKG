{"title": [{"text": "Extractive Summarization using Continuous Vector Space Models", "labels": [], "entities": [{"text": "Extractive Summarization", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8659053146839142}]}], "abstractContent": [{"text": "Automatic summarization can help users extract the most important pieces of information from the vast amount of text digitized into electronic form everyday.", "labels": [], "entities": [{"text": "summarization", "start_pos": 10, "end_pos": 23, "type": "TASK", "confidence": 0.9772689342498779}]}, {"text": "Central to automatic summarization is the notion of similarity between sentences in text.", "labels": [], "entities": [{"text": "summarization", "start_pos": 21, "end_pos": 34, "type": "TASK", "confidence": 0.7260283827781677}]}, {"text": "In this paper we propose the use of continuous vector representations for semantically aware representations of sentences as a basis for measuring similarity.", "labels": [], "entities": []}, {"text": "We evaluate different compositions for sentence representation on a standard dataset using the ROUGE evaluation measures.", "labels": [], "entities": [{"text": "sentence representation", "start_pos": 39, "end_pos": 62, "type": "TASK", "confidence": 0.715330183506012}, {"text": "ROUGE", "start_pos": 95, "end_pos": 100, "type": "METRIC", "confidence": 0.8008131384849548}]}, {"text": "Our experiments show that the evaluated methods improve the performance of a state-of-the-art summarization framework and strongly indicate the benefits of continuous word vector representations for automatic summarization.", "labels": [], "entities": [{"text": "summarization", "start_pos": 94, "end_pos": 107, "type": "TASK", "confidence": 0.9664294719696045}]}], "introductionContent": [{"text": "The goal of summarization is to capture the important information contained in large volumes of text, and present it in a brief, representative, and consistent summary.", "labels": [], "entities": [{"text": "summarization", "start_pos": 12, "end_pos": 25, "type": "TASK", "confidence": 0.9914512038230896}]}, {"text": "A well written summary can significantly reduce the amount of work needed to digest large amounts of text on a given topic.", "labels": [], "entities": []}, {"text": "The creation of summaries is currently a task best handled by humans.", "labels": [], "entities": [{"text": "summaries", "start_pos": 16, "end_pos": 25, "type": "TASK", "confidence": 0.8726707696914673}]}, {"text": "However, with the explosion of available textual data, it is no longer financially possible, or feasible, to produce all types of summaries by hand.", "labels": [], "entities": []}, {"text": "This is especially true if the subject matter has a narrow base of interest, either due to the number of potential readers or the duration during which it is of general interest.", "labels": [], "entities": []}, {"text": "A summary describing the events of World War II might for instance be justified to create manually, while a summary of all reviews and comments regarding a certain version of Windows might not.", "labels": [], "entities": []}, {"text": "In such cases, automatic summarization is away forward.", "labels": [], "entities": [{"text": "summarization", "start_pos": 25, "end_pos": 38, "type": "TASK", "confidence": 0.9172969460487366}]}, {"text": "In this paper we introduce a novel application of continuous vector representations to the problem of multi-document summarization.", "labels": [], "entities": [{"text": "multi-document summarization", "start_pos": 102, "end_pos": 130, "type": "TASK", "confidence": 0.6933926641941071}]}, {"text": "We evaluate different compositions for producing sentence representations based on two different word embeddings on a standard dataset using the ROUGE evaluation measures.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 145, "end_pos": 150, "type": "METRIC", "confidence": 0.7872766852378845}]}, {"text": "Our experiments show that the evaluated methods improve the performance of a state-of-the-art summarization framework which strongly indicate the benefits of continuous word vector representations for this tasks.", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to evaluate phrase embeddings for summarization we conduct several experiments and compare different phrase embeddings with tf-idf based vectors.", "labels": [], "entities": [{"text": "summarization", "start_pos": 43, "end_pos": 56, "type": "TASK", "confidence": 0.9895666837692261}]}, {"text": "Seven different configuration were evaluated.", "labels": [], "entities": []}, {"text": "The first configuration provides us with a baseline and is denoted Original for the Lin-Bilmes method described in Sec.", "labels": [], "entities": []}, {"text": "The remaining configurations comprise selected combinations of word embeddings, phrase embeddings, and similarity measures.", "labels": [], "entities": []}, {"text": "The Opinosis dataset ( consists of short user reviews in 51 different topics.", "labels": [], "entities": [{"text": "Opinosis dataset", "start_pos": 4, "end_pos": 20, "type": "DATASET", "confidence": 0.7513818740844727}]}, {"text": "Each of these topics contains between 50 and 575 sentences and area collection of user reviews made by different authors about a certain characteristic of a hotel, car or a product (e.g. \"Location of Holiday Inn, London\" and \"Fonts, Amazon Kindle\").", "labels": [], "entities": []}, {"text": "The dataset is well suited for multidocument summarization (each sentence is considered its own document), and includes between 4 and 5 gold-standard summaries (not sentences chosen from the documents) created by human authors for each topic.", "labels": [], "entities": [{"text": "multidocument summarization", "start_pos": 31, "end_pos": 58, "type": "TASK", "confidence": 0.6237348318099976}]}, {"text": "Each summary is evaluated with ROUGE, that works by counting word overlaps between generated summaries and gold standard summaries.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 31, "end_pos": 36, "type": "METRIC", "confidence": 0.9981213212013245}]}, {"text": "Our results include R-1, R-2, and R-SU4, which counts matches in unigrams, bigrams, and skip-bigrams respectively.", "labels": [], "entities": []}, {"text": "The skip-bigrams allow four words in between).", "labels": [], "entities": []}, {"text": "The measures reported are recall (R), precision (P), and F-score (F), computed for each topic individually and averaged.", "labels": [], "entities": [{"text": "recall (R)", "start_pos": 26, "end_pos": 36, "type": "METRIC", "confidence": 0.9595341235399246}, {"text": "precision (P)", "start_pos": 38, "end_pos": 51, "type": "METRIC", "confidence": 0.9556397795677185}, {"text": "F-score (F)", "start_pos": 57, "end_pos": 68, "type": "METRIC", "confidence": 0.9778476506471634}]}, {"text": "Recall measures what fraction of a human created gold standard summary that is captured, and precision measures what fraction of the generated summary that is in the gold standard.", "labels": [], "entities": [{"text": "Recall", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9428636431694031}, {"text": "precision", "start_pos": 93, "end_pos": 102, "type": "METRIC", "confidence": 0.9996774196624756}]}, {"text": "F-score is a standard way to combine recall and precision, computed as F = 2 P * RP +R .", "labels": [], "entities": [{"text": "F-score", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.9638525247573853}, {"text": "recall", "start_pos": 37, "end_pos": 43, "type": "METRIC", "confidence": 0.9993071556091309}, {"text": "precision", "start_pos": 48, "end_pos": 57, "type": "METRIC", "confidence": 0.9993014335632324}, {"text": "F", "start_pos": 71, "end_pos": 72, "type": "METRIC", "confidence": 0.9898916482925415}, {"text": "RP +R", "start_pos": 81, "end_pos": 86, "type": "METRIC", "confidence": 0.8352342247962952}]}], "tableCaptions": [{"text": " Table 1: ROUGE scores for summaries using dif- ferent similarity measures. OPT constitutes the  optimal ROUGE scores on this dataset.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.8750237822532654}, {"text": "OPT", "start_pos": 76, "end_pos": 79, "type": "METRIC", "confidence": 0.970282793045044}]}]}