{"title": [{"text": "Evaluation Report of the fourth Chinese Parsing Evaluation: CIPS- SIGHAN-ParsEval-2014", "labels": [], "entities": [{"text": "Chinese Parsing Evaluation: CIPS- SIGHAN-ParsEval-2014", "start_pos": 32, "end_pos": 86, "type": "DATASET", "confidence": 0.6604317596980503}]}], "abstractContent": [{"text": "This paper gives the overview of the fourth Chinese parsing evaluation: CIPS-SIGHAN-ParsEval-2014, including its parsing, evaluation metrics, training and test data.", "labels": [], "entities": [{"text": "Chinese parsing evaluation", "start_pos": 44, "end_pos": 70, "type": "TASK", "confidence": 0.6105276147524515}, {"text": "CIPS-SIGHAN-ParsEval-2014", "start_pos": 72, "end_pos": 97, "type": "DATASET", "confidence": 0.8639578223228455}, {"text": "parsing", "start_pos": 113, "end_pos": 120, "type": "TASK", "confidence": 0.9690676331520081}]}, {"text": "The detailed evaluation results and simple discussions will be given to show the difficulties in Chinese syntactic parsing.", "labels": [], "entities": [{"text": "Chinese syntactic parsing", "start_pos": 97, "end_pos": 122, "type": "TASK", "confidence": 0.6260770161946615}]}], "introductionContent": [{"text": "For Chinese parsing evaluations, we have successfully held three times in 2009, 2010 and 2012.", "labels": [], "entities": [{"text": "Chinese parsing evaluations", "start_pos": 4, "end_pos": 31, "type": "TASK", "confidence": 0.6569290558497111}]}, {"text": "They are the CIPS-ParsEval-2009 (, CIPS-SIGHAN-ParsEval-2010 ( and CIPS-SIGHanParsEval-2012 (Zhou, 2012) respectively.", "labels": [], "entities": [{"text": "CIPS-ParsEval-2009", "start_pos": 13, "end_pos": 31, "type": "DATASET", "confidence": 0.9050087928771973}]}, {"text": "Each evaluation has its different theme and goal.", "labels": [], "entities": []}, {"text": "The first ParsEval-2009 focused on Chinese chunk parsing.", "labels": [], "entities": [{"text": "Chinese chunk parsing", "start_pos": 35, "end_pos": 56, "type": "TASK", "confidence": 0.6310485005378723}]}, {"text": "Three kinds of chunking tasks were designed for the Chinese chunks with different descriptive complexities.", "labels": [], "entities": []}, {"text": "The evaluation results showed that as the increasing of the word number and descriptive complexity of the chunks from base chunks (BC) to functional chunks (FC) and event descriptive chunks (EDC), the final F1-value will also decrease about 6 points from 92% to 86% and 80%.", "labels": [], "entities": [{"text": "F1-value", "start_pos": 207, "end_pos": 215, "type": "METRIC", "confidence": 0.9955783486366272}]}, {"text": "The second ParsEval-2010 and third ParsEval-2012 focused on Chinese syntactic parsing.", "labels": [], "entities": [{"text": "Chinese syntactic parsing", "start_pos": 60, "end_pos": 85, "type": "TASK", "confidence": 0.6031074623266856}]}, {"text": "They had different points of emphasis for parse tree evaluation.", "labels": [], "entities": [{"text": "parse tree evaluation", "start_pos": 42, "end_pos": 63, "type": "TASK", "confidence": 0.9119807084401449}]}, {"text": "In ParsEval-2010, we compared the parsing performance differences in two kinds of Chinese sentences.", "labels": [], "entities": []}, {"text": "One is the EDC clauses with about 10 words averagely.", "labels": [], "entities": [{"text": "EDC", "start_pos": 11, "end_pos": 14, "type": "DATASET", "confidence": 0.7785724997520447}]}, {"text": "The other is the complete sentences with about 23 words averagely.", "labels": [], "entities": []}, {"text": "Evaluation results showed that there were about 8% drops for the complete sentence in the labelled F1-score measure.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 99, "end_pos": 107, "type": "METRIC", "confidence": 0.9796385765075684}]}, {"text": "In ParsEval-2012, we compared the parsing performance differences in two kinds of syntactic constituent in Chinese complete sentences.", "labels": [], "entities": []}, {"text": "One is the syntactic constituents with complex internal compound relationships, including event combination and concept composition relations.", "labels": [], "entities": []}, {"text": "The other is the syntactic constituents with ordinary internal relations, such as subject-predicate, predicate-object, modifier-head, etc.", "labels": [], "entities": []}, {"text": "Evaluation results showed that there were 20% drops for the syntactic constituents with complex internal relations in the labelled F1-score measure.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 131, "end_pos": 139, "type": "METRIC", "confidence": 0.993133008480072}]}, {"text": "The above evaluation results in the Chinese clause and sentence levels show that the complex sentence parsing is still a big challenge for the Chinese language.", "labels": [], "entities": [{"text": "complex sentence parsing", "start_pos": 85, "end_pos": 109, "type": "TASK", "confidence": 0.7522087891896566}]}, {"text": "This time we will focus on the deeper parsing evaluation in the Predicate-Argument Structure (PAS) level to test whether the parser can deal with different syntactic alternatives with same event contents.", "labels": [], "entities": []}, {"text": "We will introduce anew lexiconbased Combinatory Categorical Grammar (CCG)) annotation scheme in the evaluation, and propose anew implicit predicate argument (IPA) relation annotation method to build a large scale CCG bank with detailed PAS annotations.", "labels": [], "entities": []}, {"text": "The special lexical dependency pairs automatically extracted from the CCG bank will be used as the final gold-standard data for evaluating parsers' IPA recognition capacity.", "labels": [], "entities": [{"text": "CCG bank", "start_pos": 70, "end_pos": 78, "type": "DATASET", "confidence": 0.9088183641433716}, {"text": "parsers' IPA recognition", "start_pos": 139, "end_pos": 163, "type": "TASK", "confidence": 0.6929503579934438}]}, {"text": "Same with previous ParsEval-2010 and ParsEval-2014, we also set two tracks in the.", "labels": [], "entities": [{"text": "ParsEval-2010", "start_pos": 19, "end_pos": 32, "type": "DATASET", "confidence": 0.8596329689025879}, {"text": "ParsEval-2014", "start_pos": 37, "end_pos": 50, "type": "DATASET", "confidence": 0.8954213261604309}]}, {"text": "One is the Close track in which model parameter estimation is conducted solely on the train data.", "labels": [], "entities": [{"text": "Close", "start_pos": 11, "end_pos": 16, "type": "METRIC", "confidence": 0.8273525834083557}, {"text": "model parameter estimation", "start_pos": 32, "end_pos": 58, "type": "TASK", "confidence": 0.6061908006668091}]}, {"text": "The other is the Open track in which any datasets other than the given training data can be used to estimate model parameters.", "labels": [], "entities": []}, {"text": "We will set separated evaluation ranks for these two tracks.", "labels": [], "entities": []}, {"text": "In addition, we will evaluate following two kinds of methods separately in each track.", "labels": [], "entities": []}, {"text": "1) Single system: parsers that use a single parsing model to finish the parsing task.", "labels": [], "entities": []}, {"text": "2) System combination: participants are allowed to combine multiple models to improve the performance.", "labels": [], "entities": []}, {"text": "Collaborative decoding methods will be regarded as a combination method.", "labels": [], "entities": []}], "datasetContent": [{"text": "Input: A Chinese sentence with correct word segmentations.", "labels": [], "entities": []}, {"text": "The following is an example: \u5c0f\u578b(small) \u6728\u6750(wood) \u52a0\u5de5\u573a(factory) \u5728 (is) \u5fd9(busy) \u7740(-modality) \u5236\u4f5c(build) \u5404 (several) \u79cd(-classifier) \u6728\u5236\u54c1(woodwork) \u3002 (period) (A small wood factory is busy to build several woodworks.)", "labels": [], "entities": []}, {"text": "Parsing goal: Assign appropriate CCG category tags to the words in the sentence and generate CCG derivation tree for the sentence.", "labels": [], "entities": []}, {"text": "Output: The CCG derivation tree with CCG category tags and feature annotations.", "labels": [], "entities": []}, {"text": "There are two parsing stages for the CCG parsers.", "labels": [], "entities": [{"text": "parsing", "start_pos": 14, "end_pos": 21, "type": "TASK", "confidence": 0.9669250845909119}]}, {"text": "One is the syntactic category (CCG category) assignment stage.", "labels": [], "entities": [{"text": "syntactic category (CCG category) assignment", "start_pos": 11, "end_pos": 55, "type": "TASK", "confidence": 0.6116348079272679}]}, {"text": "The other is the parse tree (CCG derivation tree) generation stage.", "labels": [], "entities": []}, {"text": "So we design two different sets of metrics for them.", "labels": [], "entities": []}, {"text": "For the syntactic category (SC) parsing stage, basic metrics are SC tagging precision (SC_P), recall (SC_R) and F1-score(SC_F1).", "labels": [], "entities": [{"text": "syntactic category (SC) parsing", "start_pos": 8, "end_pos": 39, "type": "TASK", "confidence": 0.629896879196167}, {"text": "SC tagging precision (SC_P)", "start_pos": 65, "end_pos": 92, "type": "METRIC", "confidence": 0.7531388700008392}, {"text": "recall (SC_R)", "start_pos": 94, "end_pos": 107, "type": "METRIC", "confidence": 0.9367064734299978}, {"text": "F1-score(SC_F1)", "start_pos": 112, "end_pos": 127, "type": "METRIC", "confidence": 0.9023381074269613}]}, {"text": "\uf06c SC_P= (# of correctly tagged words) / (# of automatically tagged words) * 100% \uf06c SC_R= (# of correctly tagged words) / (# of gold-standard words) * 100% \uf06c SC_F1= 2*SC_P*SC_R / (SC_P + SC_R) The correctly tagged words must have the same syntactic categories with the gold-standard ones.", "labels": [], "entities": []}, {"text": "To obtain detailed evaluation results for different syntactic categories, we will classify all tagged words into different sets and compute different SC_P, SC_R and SC_F1 for them.", "labels": [], "entities": [{"text": "F1", "start_pos": 168, "end_pos": 170, "type": "METRIC", "confidence": 0.8373696208000183}]}, {"text": "The classification condition is as follows.", "labels": [], "entities": [{"text": "classification", "start_pos": 4, "end_pos": 18, "type": "TASK", "confidence": 0.9481636881828308}]}, {"text": "If (SC_Token_Ratio >=10%), then the syntactic tag will be one class with its SC tag, otherwise all other low-frequency SC-tagged words will be classified with a special class with Oth_SC tag.", "labels": [], "entities": []}, {"text": "Where, SC_Token_Ratio= (word token # of one special SC in the test set) / (word token # in the test set) * 100%.", "labels": [], "entities": []}, {"text": "For the CCG derivation tree generation stage, the lexical dependency pairs (LDPs) automatically extracted from the CCG derivation trees will be used as the basic evaluation units.", "labels": [], "entities": [{"text": "CCG derivation tree generation", "start_pos": 8, "end_pos": 38, "type": "TASK", "confidence": 0.5580113083124161}]}, {"text": "Basic metrics for them are LDP precision (LDP_P), recall (LDP_R) and F1-score (LDP_F1).", "labels": [], "entities": [{"text": "LDP precision (LDP_P)", "start_pos": 27, "end_pos": 48, "type": "METRIC", "confidence": 0.86286518403462}, {"text": "recall (LDP_R)", "start_pos": 50, "end_pos": 64, "type": "METRIC", "confidence": 0.9525145490964254}, {"text": "F1-score (LDP_F1)", "start_pos": 69, "end_pos": 86, "type": "METRIC", "confidence": 0.9305804669857025}]}, {"text": "\uf06c LDP_P = (# of correctly labeled LDPs) / (# of automatically parsed LDPs) * 100% The correctly labeled LDPs must have the same annotation information with the goldstandard ones.", "labels": [], "entities": []}, {"text": "To obtain detailed evaluation results for different LDPs, we can classify them into 5 sets and compute different LDP_P, LDP_R and LDP_F1 for them respectively.", "labels": [], "entities": [{"text": "LDP_F1", "start_pos": 130, "end_pos": 136, "type": "METRIC", "confidence": 0.6074847380320231}]}, {"text": "(1) LDPs with complex event relations in the sentence levels; (2) LDPs with concept compound relations in the chunk levels; (3) LDPs with predicate-argument relations in the clause levels, including headcomplement and adjunct-head relations.", "labels": [], "entities": []}, {"text": "(4) LDPs with other non-PA relations in the chunk and clause levels, including modifier-head and operator-complement relations.", "labels": [], "entities": []}, {"text": "(5) All other LDPs.", "labels": [], "entities": []}, {"text": "We compute the weighted average of the F1-scores of the first four sets (Tot4_F1) to obtain the final ranked scores for different proposed parser systems.", "labels": [], "entities": [{"text": "F1-scores", "start_pos": 39, "end_pos": 48, "type": "METRIC", "confidence": 0.9936716556549072}, {"text": "Tot4_F1)", "start_pos": 73, "end_pos": 81, "type": "METRIC", "confidence": 0.6582222282886505}]}, {"text": "The computation formula is as follows: Tot5_F1=\u2211LDP_F1 i * LDP_Ratio i \uff0ci \u2208.", "labels": [], "entities": [{"text": "Tot5_F1", "start_pos": 39, "end_pos": 46, "type": "METRIC", "confidence": 0.7210694750150045}, {"text": "LDP_F1", "start_pos": 48, "end_pos": 54, "type": "METRIC", "confidence": 0.7927854061126709}, {"text": "LDP_Ratio", "start_pos": 59, "end_pos": 68, "type": "METRIC", "confidence": 0.8791578412055969}]}, {"text": "LDP_Ratio i is the distributional ratio for the i th LDP set in the test set.", "labels": [], "entities": [{"text": "LDP_Ratio i", "start_pos": 0, "end_pos": 11, "type": "METRIC", "confidence": 0.8060880750417709}]}, {"text": "It computation formula is: LDP_Ratio i = (# of LDPs in i th set) / (# of all LDPs) * 100% For comparison analysis, we also compute the weighted average of F1-scores of all five sets for ranking reference.", "labels": [], "entities": [{"text": "LDP_Ratio i", "start_pos": 27, "end_pos": 38, "type": "METRIC", "confidence": 0.9105347841978073}, {"text": "F1-scores", "start_pos": 155, "end_pos": 164, "type": "METRIC", "confidence": 0.8900153040885925}]}, {"text": "We used the annotated sentences in the TCT version 1.0 (Zhou, 2004) as the basic resources and designed the following transformation and annotation procedures to obtain the final training and test data for the parsing evaluation task.", "labels": [], "entities": [{"text": "TCT version 1.0 (Zhou, 2004)", "start_pos": 39, "end_pos": 67, "type": "DATASET", "confidence": 0.8722354844212532}, {"text": "parsing evaluation task", "start_pos": 210, "end_pos": 233, "type": "TASK", "confidence": 0.9064100782076517}]}, {"text": "Firstly, we automatically transformed all the TCT parse trees into CCG derivation trees by using the TCT2CCG tool (Zhou, 2011), and built a CCG bank version 1.0 for the TCT data.", "labels": [], "entities": [{"text": "TCT parse", "start_pos": 46, "end_pos": 55, "type": "TASK", "confidence": 0.8868109285831451}, {"text": "TCT data", "start_pos": 169, "end_pos": 177, "type": "DATASET", "confidence": 0.8342582881450653}]}, {"text": "In the bank, most of clauses can be obtained correct CCG derivation trees due to the direct application of the syntax-semantics linking (SSL) principles among the basic syntactic constructions in Chinese sentences.", "labels": [], "entities": []}, {"text": "The above CCG derivation tree (1) in section 2.1 is a good example.", "labels": [], "entities": []}, {"text": "But there are still many syntactic constructions consist of implicit predicate-argument (IPA) relations, such as the topicalization and relative clause constructions.", "labels": [], "entities": []}, {"text": "They can't be automatically transformed into correct CCG derivation trees through the explicit SSL mapping rules.", "labels": [], "entities": []}, {"text": "To deal with the problem, we proposed to manually annotate the IPA relations in these special constructions and restructure the corresponding CCG derivation sub-trees according to these annotated PA tags.", "labels": [], "entities": []}, {"text": "The key for IPA annotation is to find the suitable construction examples that carry the IPA relations in Chinese sentences.", "labels": [], "entities": [{"text": "IPA annotation", "start_pos": 12, "end_pos": 26, "type": "TASK", "confidence": 0.8593757450580597}]}, {"text": "So we classify all the event constructions (ECs) in the Chinese sentences into the following three sets: 1) Basic event constructions (BEC) They are the typical subject-predicate-object constructions in Chinese clause level.", "labels": [], "entities": [{"text": "BEC", "start_pos": 135, "end_pos": 138, "type": "METRIC", "confidence": 0.866207480430603}]}, {"text": "The direct SSL can be found in the constructions.", "labels": [], "entities": []}, {"text": "So the current TCT2CCG tool is OK for them.", "labels": [], "entities": []}, {"text": "A simple example is as follows: \uf06c \u6211(I) \u8bfb\u8fc7(have read) \u8fd9\u672c\u4e66(the book).", "labels": [], "entities": []}, {"text": "(I have read the book.)", "labels": [], "entities": []}, {"text": "2) Derived event constructions (DEC) They are the derived constructions in Chinese clause level due to some special pragmatics purposes or contexts.", "labels": [], "entities": [{"text": "Derived event constructions (DEC)", "start_pos": 3, "end_pos": 36, "type": "TASK", "confidence": 0.6914481172959009}]}, {"text": "Most of them are the topicalization or argument-ellipsis constructions.", "labels": [], "entities": []}, {"text": "The following is a topicalization example: \uf06c \u8fd9\u672c\u4e66(the book) \u6211(I) \u8bfb\u8fc7(have read).", "labels": [], "entities": []}, {"text": "(The book, I have read.)", "labels": [], "entities": []}, {"text": "The topicalized deep object \" \u8fd9\u672c\u4e66(the book)\" should be given special IPA tags to show the detailed SSL relations.", "labels": [], "entities": [{"text": "SSL relations", "start_pos": 99, "end_pos": 112, "type": "TASK", "confidence": 0.8822104930877686}]}, {"text": "3) Transformed event constructions (TEC) Most of them are the relative sub-clauses to describe the special event backgrounds for an ongoing main event predicate.", "labels": [], "entities": [{"text": "Transformed event constructions (TEC)", "start_pos": 3, "end_pos": 40, "type": "TASK", "confidence": 0.7268307507038116}]}, {"text": "The structural particle \u7684(de) is used as the relative marker for them.", "labels": [], "entities": []}, {"text": "The following is a relative sub-clause example (underlined) in a complete clause: \uf06c \u6211(I) \u8bfb\u8fc7(have read) \u7684(de) \u8fd9\u672c\u4e66(the book) \u5f88\u6709\u8da3(very interesting).", "labels": [], "entities": []}, {"text": "(The book that I have read is very interesting.)", "labels": [], "entities": []}, {"text": "It is a big challenge to identify whether the relative noun phrases are the real extracted arguments in TECs or not.", "labels": [], "entities": []}, {"text": "Based on the above event construction classification, we proposed an EC-based IPA annotation scheme.", "labels": [], "entities": [{"text": "event construction classification", "start_pos": 19, "end_pos": 52, "type": "TASK", "confidence": 0.7891424794991811}]}, {"text": "For each DEC or TEC example extracted from Chinese real sentences, two or three independent annotators were asked to select the suitable corresponding BEC menu for them on an IPA annotation platform.", "labels": [], "entities": [{"text": "DEC or TEC example extracted from Chinese real sentences", "start_pos": 9, "end_pos": 65, "type": "TASK", "confidence": 0.5359790821870168}, {"text": "BEC", "start_pos": 151, "end_pos": 154, "type": "METRIC", "confidence": 0.8565059304237366}]}, {"text": "Some detailed information about the IPA annotation procedure can be found in.", "labels": [], "entities": []}, {"text": "After manual IPA annotation, we can obtain the following ECs with IPA tags for the above two DEC and TEC examples:  \uf06c For deep subject: NP \uf0e0 S/(S\\NP) \uf06c For deep object: NP \uf0e0 S/(S/NP) The CCG forward composition rule: S/(S\\NP) (S\\NP)/NP \uf0e0 B S/NP, is used for the SSL of the deep subject.", "labels": [], "entities": []}, {"text": "The special CCG forward application rule: S/(S\\NP) S\\NP \uf0e0 S, is used for the SSL of the topicalized deep object.3) shows the rebuilt CCG derivation tree fora relative sub-clause TEC.", "labels": [], "entities": []}, {"text": "The SSL of the deep subject is same with the above.", "labels": [], "entities": [{"text": "SSL", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.9146451950073242}]}, {"text": "The CCG co-indexing (CI) scheme is used for the SSL of the extracted deep object.", "labels": [], "entities": []}, {"text": "It is assigned as a special feature in the CCG tag of the structure particle \u7684 (de): (NP 1 /NP 2 )\\(S/NP 3 ) [CI:2=3], which means that the modified head (NP 2 ) of the relative clause is co-index with reduced deep object ) in the relative clause.", "labels": [], "entities": []}, {"text": "The rebuilt CCG derivation trees can provide consistent representations for different shallow syntactic alternatives with the same deep PA relations.", "labels": [], "entities": []}, {"text": "Therefore, the same lexical dependency pairs for describing the PA relations in the above three different BEC, DEC and TEC examples can be automatically extracted) from the corresponding rebuilt CCG derivation trees: \uf06c \u8bfb(read), (S\\NP)/NP, 1, \u6211(I) \uf06c \u8bfb(read), (S\\NP)/NP, 2, \u4e66(book) They describe the same event contents consist in the above three EC examples.", "labels": [], "entities": []}, {"text": "So we used these LDPs as the benchmark data for CCG parse tree evaluation.", "labels": [], "entities": [{"text": "CCG parse tree evaluation", "start_pos": 48, "end_pos": 73, "type": "TASK", "confidence": 0.7196377739310265}]}], "tableCaptions": [{"text": " Table 1 Basic statistics of the training and test data:", "labels": [], "entities": []}, {"text": " Table 2 Participant information for ParsEval-2014", "labels": [], "entities": [{"text": "ParsEval-2014", "start_pos": 37, "end_pos": 50, "type": "DATASET", "confidence": 0.600714921951294}]}, {"text": " Table 3 Ranked results in the Open Track of the CCG parsing task", "labels": [], "entities": [{"text": "CCG parsing", "start_pos": 49, "end_pos": 60, "type": "TASK", "confidence": 0.8110080659389496}]}, {"text": " Table 4 Evaluation results of the different classes of LDPs in the Open Track", "labels": [], "entities": [{"text": "Open Track", "start_pos": 68, "end_pos": 78, "type": "DATASET", "confidence": 0.846154510974884}]}]}