{"title": [{"text": "A Usage-Based Model of Early Grammatical Development", "labels": [], "entities": [{"text": "Early Grammatical Development", "start_pos": 23, "end_pos": 52, "type": "TASK", "confidence": 0.6875640551249186}]}], "abstractContent": [{"text": "The representations and processes yielding the limited length and telegraphic style of language production early on in acquisition have received little attention in ac-quisitional modeling.", "labels": [], "entities": []}, {"text": "In this paper, we present a model, starting with minimal linguistic representations, that incrementally builds up an inventory of increasingly long and abstract grammatical representations (form+meaning pairings), inline with the usage-based conception of language acquisition.", "labels": [], "entities": [{"text": "language acquisition", "start_pos": 256, "end_pos": 276, "type": "TASK", "confidence": 0.7438268065452576}]}, {"text": "We explore its performance on a comprehension and a generation task, showing that, overtime, the model better understands the processed utterances, generates longer utterances, and better expresses the situation these utterances intend to refer to.", "labels": [], "entities": []}], "introductionContent": [{"text": "A striking aspect of language acquisition is the difference between children's and adult's utterances.", "labels": [], "entities": [{"text": "language acquisition", "start_pos": 21, "end_pos": 41, "type": "TASK", "confidence": 0.7333636581897736}]}, {"text": "Simulating early grammatical production requires a specification of the nature of the linguistic representations underlying the short, telegraphic utterances of children.", "labels": [], "entities": [{"text": "early grammatical production", "start_pos": 11, "end_pos": 39, "type": "TASK", "confidence": 0.6386470993359884}]}, {"text": "In the usage-based view, young children's grammatical representions are thought to be less abstract than adults', e.g. by having stricter constraints on what can be combined with them (cf. Akhtar and Tomasello 1997;).", "labels": [], "entities": []}, {"text": "The representations and processes yielding the restricted length of these early utterances, however, have received little attention.", "labels": [], "entities": []}, {"text": "Following, we adopt the working hypothesis that the early learner's grammatical representations are more limited in length (or: arity) than those of adults.", "labels": [], "entities": []}, {"text": "Similarly, in computational modeling of grammar acquisition, comprehension has received more attention than language generation.", "labels": [], "entities": [{"text": "computational modeling of grammar acquisition", "start_pos": 14, "end_pos": 59, "type": "TASK", "confidence": 0.6398402512073517}, {"text": "language generation", "start_pos": 108, "end_pos": 127, "type": "TASK", "confidence": 0.7111451774835587}]}, {"text": "In this paper we attempt to make the mechanisms underlying early production explicit within a model that can parse and generate utterances, and that incrementally learns constructions on the basis of its previous parses.", "labels": [], "entities": []}, {"text": "The model's search through the hypothesis space of possible grammatical patterns is highly restricted.", "labels": [], "entities": []}, {"text": "Starting from initially small and concrete representations, it learns incrementally long representations (syntagmatic growth) as well as more abstract ones (paradigmatic growth).", "labels": [], "entities": []}, {"text": "Several models address either paradigmatic) or syntagmatic () growth.", "labels": [], "entities": []}, {"text": "This model aims to explain both, thereby contributing to the understanding of how different learning mechanisms interact.", "labels": [], "entities": []}, {"text": "As opposed to other models involving grammars with semantic representations, but similar to, the model starts without an inventory of mappings of single words to meanings.", "labels": [], "entities": []}, {"text": "Based on motivation from usage-based and construction grammar approaches, we define several learning principles that allow the model to buildup an inventory of linguistic representations.", "labels": [], "entities": []}, {"text": "The model incrementally processes pairs of an utterance U , consisting of a string of words w 1 . .", "labels": [], "entities": []}, {"text": "w n , and a set of situations S, one of which is the situation the speaker intends to refer to.", "labels": [], "entities": []}, {"text": "The other situations contribute to propositional uncertainty (the uncertainty over which proposition the speaker is trying to express; Siskind 1996).", "labels": [], "entities": []}, {"text": "The model tries to identify the intended situation and to understand how parts of the utterance refer to certain parts of that situation.", "labels": [], "entities": []}, {"text": "To do so, the model uses its growing inventory of linguistic representations to analyze U , producing a set of structured semantic analyses or parses; Section 3).", "labels": [], "entities": []}, {"text": "The resulting best parse, U and the selected situation are then stored in a memory buffer (arrow 2), which is used to learn new constructions (arrow 3) using several learning mechanisms (Section 4).", "labels": [], "entities": []}, {"text": "The learned constructions can then be used to generate utterances as well.", "labels": [], "entities": []}, {"text": "We describe two experiments: in the comprehension experiment (Section 5), we evaluate the model's ability to parse the stream of input items.", "labels": [], "entities": []}, {"text": "In the generation experiment (Section 6), the model generates utterances on the basis of a given situation and its linguistic knowledge.", "labels": [], "entities": []}, {"text": "We evaluate the generated utterances given different amounts of training items to consider the development of the model overtime.", "labels": [], "entities": []}], "datasetContent": [{"text": "The model is incrementally presented with U, S pairings based on Alishahi & Stevenson's (2010) generation procedure.", "labels": [], "entities": []}, {"text": "In this procedure, an utterance and a semantic frame expressing its meaning (a situation) are generated.", "labels": [], "entities": []}, {"text": "The generation procedure follows distributions occurring in a corpus of child-directed speech.", "labels": [], "entities": []}, {"text": "As we are interested in the performance of the model under propositional uncertainty, we add a parametrized number of randomly sampled situations, so that S consists of the situation the speaker intends to refer to (s correct ) and a number of situations the speaker does not intend to refer to.", "labels": [], "entities": []}, {"text": "Here, we set the number of ad-ditional situations to be 1 or 5; the other parameter of the model, the size of the memory buffer, is set to 5 exemplars.", "labels": [], "entities": []}, {"text": "For the comprehension experiment, we evaluate the model's performance parsing the input items, averaging over every 50 U, S pairs.", "labels": [], "entities": []}, {"text": "We track the ability to identify the intended situation from S.", "labels": [], "entities": []}, {"text": "Identification succeeds if the best parse maps to s correct , i.e. if s identified = s correct . Next, situation coverage expresses what proportion of s identified has been interpreted and thus how rich the meanings of the used constructions are.", "labels": [], "entities": [{"text": "Identification", "start_pos": 0, "end_pos": 14, "type": "METRIC", "confidence": 0.9353665709495544}]}, {"text": "It is defined as the number of nodes of the interpretation of the best parse, divided by the number of nodes of s identified . Finally, utterance coverage tells us what proportion of U has been parsed with constructions (excluding IGNORED; including BOOT-STRAPPED words).", "labels": [], "entities": [{"text": "BOOT-STRAPPED", "start_pos": 250, "end_pos": 263, "type": "METRIC", "confidence": 0.9225153923034668}]}, {"text": "The measure expresses the proportion of the signal that the learner (correctly or incorrectly) is able to interpret.", "labels": [], "entities": []}, {"text": "For exploring language production, the model receives a situation, and (given the constructicon) finds the most probable, maximally expressive, fully lexicalized derivation expressing it.", "labels": [], "entities": []}, {"text": "That is: among all derivations terminating in phonologically specified constituents, it selects the derivations that cover the most semantic nodes of the given situation.", "labels": [], "entities": []}, {"text": "In the case of multiple such derivations, it selects the most probable one, following the probability model in Section 3.", "labels": [], "entities": []}, {"text": "We only allow for the   fer to words in a given U , and CONCATENATE is a back-off method for analyzing more of U than the constructicon allows for.", "labels": [], "entities": []}, {"text": "The situations used in the generation experiment do not occur in the training items, so that we truly measure the model's ability to generate utterances for novel situations.", "labels": [], "entities": []}, {"text": "The phonologically specified leaf nodes of the best derivation constitute the generated utterance U gen . U gen is evaluated on the basis of its mean length, in number of words, its situation coverage, as defined in the comprehension experiment, and its utterance precision and utterance recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 264, "end_pos": 273, "type": "METRIC", "confidence": 0.9834169149398804}, {"text": "recall", "start_pos": 288, "end_pos": 294, "type": "METRIC", "confidence": 0.9753332138061523}]}, {"text": "To calculate these, we take the maximally overlapping subsequence U overlap between the actual utterance U act associated with the situation and U gen . Utterance precision (how many words are generated correctly) and utterance recall (how many of the correct words are generated) are defined as: Because the U, S-pairs on which the model was trained, are generated randomly, we show results for comprehension and production averaged over 5 simulations.", "labels": [], "entities": [{"text": "precision", "start_pos": 163, "end_pos": 172, "type": "METRIC", "confidence": 0.5761197805404663}, {"text": "recall", "start_pos": 228, "end_pos": 234, "type": "METRIC", "confidence": 0.9233375191688538}]}, {"text": "A central motivation for the development of this model is to account for early grammatical production: can we simulate the developmental pattern of the growth of utterance length and a growing potential for generalization?", "labels": [], "entities": []}, {"text": "The same constructions underlying these productions should, at the same time, also account for the learner's increasing grasp of the meaning of U . To explore the model's performance in both domains, we present a comprehension and a generation experiment.", "labels": [], "entities": []}, {"text": "gives us the results overtime of the comprehension measures given a propositional uncertainty of 1, i.e. one situation besides s correct in S.", "labels": [], "entities": []}, {"text": "Overall, the model understands the utterances increasingly well.", "labels": [], "entities": []}, {"text": "After 2000 input items, the model identifies s correct in 95% of the cases.", "labels": [], "entities": []}, {"text": "With higher levels of propositional uncertainty (not shown here), performance is still relatively robust: given 5 incorrect situations in S, s correct is identified in 62% of all cases (random guessing gives a score of 17%, or 1 6 ).", "labels": [], "entities": []}, {"text": "Similarly, the proportion of the situation interpreted and the proportion of the utterance analyzed go up overtime.", "labels": [], "entities": []}, {"text": "This means that the model builds up an increasing repertoire of constructions that allow it to analyze larger parts of the utterance and the situations it identifies.", "labels": [], "entities": []}, {"text": "It is important to realize that these mea-", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Generations over time t for one situation.", "labels": [], "entities": []}]}