{"title": [{"text": "A language-independent method for the extraction of RDF verbalization templates", "labels": [], "entities": [{"text": "extraction of RDF verbalization templates", "start_pos": 38, "end_pos": 79, "type": "TASK", "confidence": 0.684048080444336}]}], "abstractContent": [{"text": "With the rise of the Semantic Web more and more data become available encoded using the Semantic Web standard RDF.", "labels": [], "entities": []}, {"text": "RDF is faced towards machines: designed to be easily processable by machines it is difficult to be understood by casual users.", "labels": [], "entities": [{"text": "RDF", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.9013851284980774}]}, {"text": "Transforming RDF data into human-comprehensible text would facilitate non-experts to assess this information.", "labels": [], "entities": []}, {"text": "In this paper we present a language-independent method for extracting RDF verbalization templates from a parallel corpus of text and data.", "labels": [], "entities": [{"text": "RDF verbalization templates from a parallel corpus of text and data", "start_pos": 70, "end_pos": 137, "type": "TASK", "confidence": 0.741784621368755}]}, {"text": "Our method is based on distant-supervised simultaneous multi-relation learning and frequent maximal subgraph pattern mining.", "labels": [], "entities": [{"text": "frequent maximal subgraph pattern mining", "start_pos": 83, "end_pos": 123, "type": "TASK", "confidence": 0.6922906041145325}]}, {"text": "We demonstrate the feasibility of our method on a parallel corpus of Wikipedia articles and", "labels": [], "entities": []}], "introductionContent": [{"text": "Natural Language Generation (NLG) systems require resources such as templates (in case of template-based NLG) or rules (in case of rulebased NLG).", "labels": [], "entities": [{"text": "Natural Language Generation (NLG)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7705834011236826}]}, {"text": "Be it template-based or rule-based systems, these resources limit the variability and the domain-specificity of the generated natural language output and manual creation of these resources is tedious work.", "labels": [], "entities": []}, {"text": "We propose a language-independent approach that induces verbalization templates for RDF graphs from example data.", "labels": [], "entities": []}, {"text": "The approach is language-independent since it does not rely on pre-existing language resources such as parsers, grammars or dictionaries.", "labels": [], "entities": []}, {"text": "Input is a corpus of parallel text and data consisting of a set of documents D and an RDF graph G, where D and G are related via a set of entities E where an entity can be described by a document in D and described by data in G.", "labels": [], "entities": []}, {"text": "Output is a set of templates.", "labels": [], "entities": []}, {"text": "Templates consist of a graph pattern that can be applied to query the graph and of a sentence pattern that is a slotted sentence into which parts of the query result are inserted.", "labels": [], "entities": []}, {"text": "A template enables verbalization of a subgraph of G as a complete sentence.", "labels": [], "entities": []}, {"text": "An example is shown in.", "labels": [], "entities": []}, {"text": "The graph pattern GP can be transformed into a SPARQL query Q GP . Querying the data graph G results in the graph G GP . G GP can be verbalized as an English (German) sentence S en (S de ) using the sentence pattern SP en (SP de ).", "labels": [], "entities": []}, {"text": "The approach employs the distant supervision principle) from relation extraction: training data is generated automatically by aligning a database of facts with text; therefore, no hand-labeled data is required.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 61, "end_pos": 80, "type": "TASK", "confidence": 0.8823722302913666}]}, {"text": "We apply simultaneous multi-relation learning) for text-data alignment and frequent maximal subgraph pattern mining to observe commonalities among RDF graph patterns.", "labels": [], "entities": [{"text": "text-data alignment", "start_pos": 51, "end_pos": 70, "type": "TASK", "confidence": 0.7552627623081207}, {"text": "subgraph pattern mining", "start_pos": 92, "end_pos": 115, "type": "TASK", "confidence": 0.7630284229914347}]}, {"text": "Besides the general idea to allow for nonexperts to assess information encoded in RDF, we envision application of these verbalization templates in three scenarios: (1) In query interfaces to semantic databases, casual users -usually not capable of writing formal queries -specify their information needs using keywords ( \"Flash (novel)\"@en \"L. E. Modesitt\"@en \"Science fiction novel\"@en SP de \"L.", "labels": [], "entities": []}, {"text": "E. Modesitt\"@de \"Flash (Roman)\"@de \"Science-Fiction-Roman\"@de \"2004\" ?book_label = \"Flash (novel)\"@en ?book_type_label = \"Science fiction novel\"@en ?author_label = \"L.", "labels": [], "entities": []}, {"text": "E. Modesitt\"@en ?book_pubD = \"2004\" ?book = Flash ?author = L_E_Modesitt ?book_type = SFN \u00b5: A template consists of a graph pattern GP and a sentence pattern SP . The graph pattern GP can be transformed into a SPARQL query Q GP . A result of querying the data graph is the RDF graph G GP with the list of solution mappings \u00b5.", "labels": [], "entities": []}, {"text": "This graph can be verbalized as an English sentence S en using the English sentence pattern SP en or as a German sentence S de using the German sentence pattern SP de . The modifiers, e.g. lcfirst, are explained in et al., 2008).", "labels": [], "entities": []}, {"text": "The system queries an RDF database according to its interpretation of the input.", "labels": [], "entities": []}, {"text": "Query results could be verbalized.", "labels": [], "entities": []}, {"text": "(2) Since the introduction of the Google Knowledge Graph, 2 when searching for an entity such as the city of Karlsruhe via Google, besides the search results shown on the left a table is displayed on the right which provides a short description of the entity taken from Wikipedia.", "labels": [], "entities": []}, {"text": "While these descriptions are decoupled from data in the knowledge graph they could be generated automatically.", "labels": [], "entities": []}, {"text": "(3) The collaboratively-edited knowledge base Wikidata provides machine-readable data which can be used, e.g., by the Wikipedia.", "labels": [], "entities": [{"text": "Wikidata", "start_pos": 46, "end_pos": 54, "type": "DATASET", "confidence": 0.8731225728988647}, {"text": "Wikipedia", "start_pos": 118, "end_pos": 127, "type": "DATASET", "confidence": 0.9414767026901245}]}, {"text": "The Wikidata browsing interface reasonator currently explores the use of template-based NLG in order to provide human-readable descriptions of its entities.", "labels": [], "entities": []}, {"text": "Since the templates are created manually, currently only for few types of entities these verbalizations can be provided.", "labels": [], "entities": []}, {"text": "2 http://googleblog.blogspot.com/2012/ 05/introducing-knowledge-graph-thingsnot.html 3 See for example the page about Johann Sebastian Bach: http://tools.wmflabs.org/reasonator/ ?q=Q1339 (accessed 2014-03-20)", "labels": [], "entities": []}], "datasetContent": [{"text": "We created a multilingual (English, German) parallel text-data corpus using data from DBpedia 6 and documents from the Wikipedia.", "labels": [], "entities": [{"text": "DBpedia 6", "start_pos": 86, "end_pos": 95, "type": "DATASET", "confidence": 0.9251924157142639}]}, {"text": "The graph G consists of 88, 708, 622 triples, the set of documents D consists of 4, 004, 478 English documents and 716, 049 German documents.", "labels": [], "entities": []}, {"text": "The corpus relations and functions are defined as follows: \u2022 document(e, l) := {d|(e, dbo:abstract, \"d\"@l) \u2208 G}.", "labels": [], "entities": []}, {"text": "\u2022 \u03bb(e, l) := {v|(e, rdfs:label, \"v\"@l) \u2208 G} \u2022 The datatype of a literal \"r\u02c6\u02c6t\" is t.", "labels": [], "entities": []}, {"text": "\u2022 The language ll of a literal \"d\"@l isl.", "labels": [], "entities": []}, {"text": "The modifiers we used in the experiment are given in.", "labels": [], "entities": []}, {"text": "Application of date and inte-6 http://wiki.dbpedia.org/Downloads39 We used the files long abstracts en, long abstracts enuris de, mappingbased properties en, raw infobox properties en, article categories en, instance types en, labels en, labels en uris de, category labels en, and category labelsen uris de.", "labels": [], "entities": []}, {"text": "Modifiers are only applied if their application to a literal modifies that literal.", "labels": [], "entities": []}, {"text": "For example, if a string begins with a groups  ger modifiers may also depend on the language of a sentence.", "labels": [], "entities": []}, {"text": "On a value a list of modifiers can be applied.", "labels": [], "entities": []}, {"text": "The list of string modifier lists is shown in.", "labels": [], "entities": []}, {"text": "The table also shows how often each list of modifiers was applied during the abstraction of English and German sentences.", "labels": [], "entities": [{"text": "abstraction of English and German sentences", "start_pos": 77, "end_pos": 120, "type": "TASK", "confidence": 0.7876284619172415}]}, {"text": "We created two sets of entities E en (E de ): those for which an English (German) document exist that consists of at least 100 characters.", "labels": [], "entities": []}, {"text": "E en and E de contain 3, 587, 146 and 613, 027 entities, respectively.", "labels": [], "entities": []}, {"text": "For each entity for each document we split the text into sentences using the Perl module Lingua::Sentence 8 and discarded sentences that do not end with a full stop, an exclamation mark, or a question mark or that were shorter (longer) than 50 (200) characters.", "labels": [], "entities": []}, {"text": "We used the set of string modifiers presented in to identify entities via occurrence of a modified version of their labels in a sentence.", "labels": [], "entities": []}, {"text": "The results are 3, 811, 992 (794, 040) English (German) sentences.", "labels": [], "entities": []}, {"text": "Abstraction resulted in 3,434,108 (530,766) abstracted English (German) sentences whereat least two entities are identified per sentence.", "labels": [], "entities": []}, {"text": "The group size histogram is displayed in.", "labels": [], "entities": []}, {"text": "The majority (90%) of all groups of English (German) sentences contain between 5 and 164 (5 and 39) sentences.", "labels": [], "entities": []}, {"text": "gives for each language the number of groups that contain more than 5 graph patterns, the number of templates we induced, and the number of all groups.", "labels": [], "entities": []}, {"text": "Results of the coverage evaluation cov t (G) are shown as a histogram in.", "labels": [], "entities": []}, {"text": "It shows that for the majority of the templates a high number of subgraphs of G can be verbalized, which means that the templates are not fitted to only a small number of subgraphs: e.g. for 221 English templates verbalize between 10 5 and 10 6 subgraphs, each.", "labels": [], "entities": []}, {"text": "lowercase character, then the lcfirst modifier is inapplicable.", "labels": [], "entities": []}, {"text": "8 http://search.cpan.org/ \u02dc achimru/ Lingua-Sentence-1.05 We cutoff the long tail.", "labels": [], "entities": []}, {"text": "We evaluate the results from the experiment described in the previous section along the dimensions coverage, accuracy, syntactic correctness, and understandability where the latter three are inspired by.", "labels": [], "entities": [{"text": "coverage", "start_pos": 99, "end_pos": 107, "type": "METRIC", "confidence": 0.9802236557006836}, {"text": "accuracy", "start_pos": 109, "end_pos": 117, "type": "METRIC", "confidence": 0.9992474317550659}]}, {"text": "Coverage: we define cov(t, G) of a template t=(sp, gp) regarding a data graph G as the number of subgraphs of G that can be verbalized with that template i.e. match gp.", "labels": [], "entities": []}, {"text": "Accuracy: is measured in two parts: 1.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9955607652664185}]}, {"text": "The extent to which everything that is expressed in gp is also expressed in sp is measured for each triple pattern within the graph pattern on a 4-point scale: (1) The triple pattern is explicitly expressed, (2) The triple pattern is implied, (3) The triple pattern is not expressed, and The verb.", "labels": [], "entities": []}, {"text": "cannot be understood at all.", "labels": [], "entities": []}, {"text": "We evaluated a random sample of 10 English and 10 German templates using a group of 6 evaluators which are experts in the fields of RDF and SPARQL and that are proficient in both English and German.", "labels": [], "entities": []}, {"text": "Each template was evaluated by 3 experts, each expert evaluated 10 templates.", "labels": [], "entities": []}, {"text": "For each template we retrieved a maximum of 100 subgraphs that matched the graph pattern, randomly selected 10 subgraphs and verbalized them.", "labels": [], "entities": []}, {"text": "For each template an evaluator was asked to evaluate accuracy given the graph pattern and given the sentence pattern and, given the list of 10 verbalizations, to evaluate each sentence regarding syntactic correctness and understandability.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.9992627501487732}]}, {"text": "cov(t, G) of all 5066 templates is shown in.", "labels": [], "entities": []}, {"text": "For example, it shows that there are about 300 templates where each template can be used 6 Related work) present a technique for reading sentences and producing sets of hypothetical relations that the sentence maybe expressing.", "labels": [], "entities": []}, {"text": "Given a parallel text-data corpus, entities identified as proper nouns in parsed sentences are replaced with variables.", "labels": [], "entities": []}, {"text": "For each (pattern, set of relations) tuple for each sentence that matches this pattern it is counted in how many sentences that match this pattern a certain relation exists between the two entities identified in the sentence.", "labels": [], "entities": []}, {"text": "This leads to positive weights assigned to patterns.", "labels": [], "entities": []}, {"text": "Negative weights are assigned by applying patterns to sentences, identifying the entities and assigning a negative weight to the relation if the relation expressed by the pattern is not expressed in the data.", "labels": [], "entities": []}, {"text": "In contrast to this approach, our approach 1) does not require to parse input sentences 2) does not only regard relations between proper nouns, 3) constrains candidate entities to the vicinity of already identified entities.", "labels": [], "entities": []}, {"text": "Moreover, 4) our approach takes into account the graph of entities identified in a sentence (hypothesis graphs) compared to sets of relations and can thus express multiple relations between entities.", "labels": [], "entities": []}, {"text": "() present an unsupervised approach to NLG template extraction from a parallel text-data corpus.", "labels": [], "entities": [{"text": "NLG template extraction", "start_pos": 39, "end_pos": 62, "type": "TASK", "confidence": 0.8700773119926453}]}, {"text": "Similar to our approach, text and data are aligned by identifying labels of entities in sentences.", "labels": [], "entities": []}, {"text": "The search space is limited by only allowing to match entities that are directly linked to the entity a text is about.", "labels": [], "entities": []}, {"text": "Sentences are abstracted by replacing the entity with the name of the property that links the entity with the entity the text is about thus limiting the depth of the graph to 1.", "labels": [], "entities": []}, {"text": "Abstracted sentences are parsed and pruned by removing constituents that could not be aligned to the database and by removing constituents of certain classes and then post-processed using manually created rules.", "labels": [], "entities": []}, {"text": "() present an approach to learning natural language representations of predicates from a parallel text-data corpus.", "labels": [], "entities": []}, {"text": "For each predicate where a tuple of entities is identified in a sentence, the predicate's natural language representation is the string between the two entities, e.g. 's acquisition of for the predicate subsidiary and the sentence Google's acquisition of Youtube comes as online video is really starting to hit its stride.", "labels": [], "entities": []}, {"text": "The main differences to our approach are 1) that we do not focus on learning how a single predicate is expressed but rather how a graph, consisting of multiple related entities, can be expressed in natural language and 2) that a relation between two entities is not only expressed by the string between two entities.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: List of modifiers per datatype", "labels": [], "entities": []}, {"text": " Table 3: Number of groups with a cardinality \u2265 5,  the number of induced templates and the number  of all groups.", "labels": [], "entities": []}, {"text": " Table 2: List of lists of string modifiers and their number of applications", "labels": [], "entities": []}]}