{"title": [{"text": "Data Warehouse, Bronze, Gold, STEC, Software", "labels": [], "entities": [{"text": "Data Warehouse", "start_pos": 0, "end_pos": 14, "type": "DATASET", "confidence": 0.8788928687572479}, {"text": "STEC", "start_pos": 30, "end_pos": 34, "type": "METRIC", "confidence": 0.5965791940689087}]}], "abstractContent": [{"text": "We are building an analytical data warehouse for linguistic data-primarily lexicons and phonological data-for languages in the Asia-Pacific region.", "labels": [], "entities": []}, {"text": "This paper briefly outlines the project, making the point that the need for improved technology for endangered and low-density language data extends well beyond completion of fieldwork.", "labels": [], "entities": []}, {"text": "We suggest that shared task evaluation challenges (STECs) are an appropriate model to follow for creating this technology, and that stocking data warehouses with clean bronze-standard data and baseline tools-no mean task-is an effective way to elicit the broad collaboration from linguists and computer scientists needed to create the gold-standard data that STECs require.", "labels": [], "entities": [{"text": "shared task evaluation challenges (STECs)", "start_pos": 16, "end_pos": 57, "type": "TASK", "confidence": 0.6538453911031995}]}], "introductionContent": [{"text": "The call for this workshop mentions the first step of the language documentation process, pointing out that the promise of new technology in documenting endangered languages remains unfulfilled, particularly in the context of modern recording technologies.", "labels": [], "entities": [{"text": "language documentation process", "start_pos": 58, "end_pos": 88, "type": "TASK", "confidence": 0.7805013755957285}]}, {"text": "But lack of tools extends far beyond this first step.", "labels": [], "entities": []}, {"text": "It encompasses the accessibility of data long since gathered and (usually, but not always) published, as well as applications for the data by its most voracious consumer: the study of comparative and historical linguistics.", "labels": [], "entities": [{"text": "comparative and historical linguistics", "start_pos": 184, "end_pos": 222, "type": "TASK", "confidence": 0.6863022893667221}]}, {"text": "We encounter these problems daily in preliminary development of data and software resources fora planned Asia-Pacific Linguistic Data Warehouse.", "labels": [], "entities": [{"text": "Asia-Pacific Linguistic Data Warehouse", "start_pos": 105, "end_pos": 143, "type": "DATASET", "confidence": 0.7927061915397644}]}, {"text": "Briefly, our initial focus is on five phyla (~2,000 languages): Austroasiatic, Austronesian, Hmong-Mien, Kra-Dai, and Sino-Tibetan, which form a Southeast Asian convergence area, and individually extend well into China, India, the Himalayas, and the Pacific.", "labels": [], "entities": []}, {"text": "Data for languages of Australia and New Guinea will follow.", "labels": [], "entities": []}, {"text": "Not all of these languages are endangered, but many are; not all are low-density, but most are.", "labels": [], "entities": []}, {"text": "Our data are preferentially drawn from the sort of lexicography gathered for comparative purposes (ideally 2,500 items per language), and the phonological, semantic, and phylogenetic data that can be found for, or inferred from, them.", "labels": [], "entities": []}, {"text": "These are the only kind of data for which we are likely to find near-complete language representation.", "labels": [], "entities": []}, {"text": "We include smaller lexicons when necessary, and intra-language dialect surveys when available.", "labels": [], "entities": []}, {"text": "All available metadata are incorporated, including typological and phonotactic features, (phylogenetic) character sets, geo-physical and demographic data, details of lexicon coverage, extent, or quality, and bibliographic or source data.", "labels": [], "entities": []}, {"text": "Such data are not always easily found.", "labels": [], "entities": []}, {"text": "Their delivery packages -primarily books and journals -may be discoverable via bibliographic metadata, but details of the datasets themselves are not.", "labels": [], "entities": []}, {"text": "As a result, traditional bibliographic documentation, accessed via portals like OLAC) and Glottolog (), tends to have low recall and precision in regard to data resource discovery.", "labels": [], "entities": [{"text": "OLAC", "start_pos": 80, "end_pos": 84, "type": "DATASET", "confidence": 0.8748688101768494}, {"text": "recall", "start_pos": 122, "end_pos": 128, "type": "METRIC", "confidence": 0.9991220831871033}, {"text": "precision", "start_pos": 133, "end_pos": 142, "type": "METRIC", "confidence": 0.9982424974441528}, {"text": "data resource discovery", "start_pos": 156, "end_pos": 179, "type": "TASK", "confidence": 0.6391410132249197}]}, {"text": "Our experience in acquiring and performing methodical data audits of large quantities of published and unpublished materials reveals sets of lexical, grammatical, phonological, corpus, and other materials that are regular enough inform, and extensive enough in content, to comprise aggregable linguistic data supersets for the Asia-Pacific region.", "labels": [], "entities": []}, {"text": "These ongoing data audits take a three-tiered approach, separately documenting texts (to enable source recovery), their abstract data content (to enable high-recall resource discovery), and any concrete, transcribed data instances (to enable high-precision data aggregation).", "labels": [], "entities": []}, {"text": "Discovery and aggregation only open the door.", "labels": [], "entities": []}, {"text": "Many datasets are hand-crafted fora researcher's specific needs and interests, even if they fall into larger research categories.", "labels": [], "entities": []}, {"text": "Yet far from having reliable algorithms for central concerns (such as proto-language reconstruction, or subgrouping of linguistic phyla in family trees or networks) the field has not yet had to grapple with basic problems -such as normalizing phonological transcription or gloss semantics, or accurately assembling large-scale cognate sets -that will be presented by datasets that include millions of data items for thousands of languages, and many more thousands of dialectal variants.", "labels": [], "entities": [{"text": "proto-language reconstruction", "start_pos": 70, "end_pos": 99, "type": "TASK", "confidence": 0.7322973757982254}, {"text": "subgrouping of linguistic phyla in family trees or networks", "start_pos": 104, "end_pos": 163, "type": "TASK", "confidence": 0.8176439536942376}, {"text": "normalizing phonological transcription", "start_pos": 231, "end_pos": 269, "type": "TASK", "confidence": 0.8313488960266113}]}, {"text": "The central issue we face is the gap between: \uf0b7 the results of published and unpublished fieldwork, and \uf0b7 their usability in downstream research and reference applications.", "labels": [], "entities": []}, {"text": "In some cases this gap is painfully obviousas in the backlog of carefully elicited wordlists still awaiting phonetic transcription.", "labels": [], "entities": []}, {"text": "In others, the gap becomes evident when we begin to assemble large comparable datasets from published data; deceptively difficult, and never accomplished for collections broader than a single language family, or larger than about 200 words per language.", "labels": [], "entities": []}, {"text": "Such tasks are still basically handwork; often requiring the specialized knowledge of the field researcher.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}