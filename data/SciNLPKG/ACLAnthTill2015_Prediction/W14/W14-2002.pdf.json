{"title": [{"text": "Investigating the role of entropy in sentence processing", "labels": [], "entities": [{"text": "sentence processing", "start_pos": 37, "end_pos": 56, "type": "TASK", "confidence": 0.717066079378128}]}], "abstractContent": [{"text": "We outline four ways in which uncertainty might affect comprehension difficulty inhuman sentence processing.", "labels": [], "entities": []}, {"text": "These four hypotheses motivate a self-paced reading experiment, in which we used verb sub-categorization distributions to manipulate the uncertainty over the next step in the syntactic derivation (single step entropy) and the surprisal of the verb's complement.", "labels": [], "entities": []}, {"text": "We additionally estimate word-byword surprisal and total entropy over parses of the sentence using a probabilistic context-free grammar (PCFG).", "labels": [], "entities": []}, {"text": "Surprisal and total entropy, but not single step en-tropy, were significant predictors of reading times in different parts of the sentence.", "labels": [], "entities": [{"text": "Surprisal", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.98114013671875}]}, {"text": "This suggests that a complete model of sentence processing should incorporate both entropy and surprisal.", "labels": [], "entities": [{"text": "sentence processing", "start_pos": 39, "end_pos": 58, "type": "TASK", "confidence": 0.8114522695541382}]}], "introductionContent": [{"text": "Predictable linguistic elements are processed faster than unpredictable ones.", "labels": [], "entities": []}, {"text": "Specifically, processing load on an element A in context C is linearly correlated with its surprisal, \u2212 log 2 P (A|C).", "labels": [], "entities": []}, {"text": "This suggests that readers maintain expectations as to the upcoming elements: likely elements are accessed or constructed in advance of being read.", "labels": [], "entities": []}, {"text": "While there is substantial amount of work on the effect of predictability on processing difficulty, the role (if any) of the distribution over expectations is less well understood.", "labels": [], "entities": []}, {"text": "Surprisal predicts that the distribution over competing predicted elements should not affect reading times: if the conditional probability of a word A is P (A|C), reading times on the word will be proportional to \u2212 log 2 P (A|C), regardless of whether the remaining probability mass is distributed among two or a hundred options.", "labels": [], "entities": []}, {"text": "The entropy reduction hypothesis), on the other hand, accords a central role to the distribution over predicted parses.", "labels": [], "entities": [{"text": "entropy reduction", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.8636299073696136}]}, {"text": "According to this hypothesis, an incoming element is costly to process when it entails a change from a state of high uncertainty (e.g., multiple equiprobable parses) to a state of low uncertainty (e.g., one where a single parse is much more likely than the others).", "labels": [], "entities": []}, {"text": "This contrasts with the entropy reduction hypothesis, according to which processing cost arises when competition is resolved.", "labels": [], "entities": [{"text": "entropy reduction", "start_pos": 24, "end_pos": 41, "type": "TASK", "confidence": 0.8362695574760437}]}, {"text": "Intuitively, the two hypotheses make inversely correlated predictions: on average, there will be less competition following words that reduce entropy.", "labels": [], "entities": []}, {"text": "A recent study found that reading times on w i correlated positively with entropy following w i , providing support for this hypothesis).", "labels": [], "entities": []}, {"text": "The fourth hypothesis we consider, which we term the commitment hypothesis, is derived from the event-related potential (ERP) literature on contextual constraint.", "labels": [], "entities": []}, {"text": "Studies in this tradition have compared the responses to a low-predictability word across two types of context: high-constraint contexts, in which there is a strong expectation fora (different) word, and low-constraint ones, which are not strongly predictive of any individual word.", "labels": [], "entities": []}, {"text": "There is increasing evidence for an ERP component that responds to violations of a strong prediction.", "labels": [], "entities": []}, {"text": "This component can be interpreted as reflecting disproportional commitment to high probability predictions at the expense of lower probability ones, a more extreme version of the proposal that low-probability parses are pruned in the presence of a high-probability parse.", "labels": [], "entities": []}, {"text": "Surprisal is therefore expected to have a larger effect in high constraint contexts, in which entropy was low before the word being read.", "labels": [], "entities": []}, {"text": "Commitment to a high probability prediction may also result in increased processing load at the point at which the commitment is made.", "labels": [], "entities": []}, {"text": "We illustrate these four hypotheses using the simple language sketched in.", "labels": [], "entities": []}, {"text": "Consider the predictions made by the four hypotheses for the sentences ae and be.", "labels": [], "entities": []}, {"text": "Surprisal predicts no difference in reading times between these sentences, since the conditional probabilities of the words in the two sentences are identical (0.5 and 0.25 respectively).", "labels": [], "entities": []}, {"text": "The competition hypothesis predicts increased reading times on the first word in ae compared to be, because the entropy following a is higher than the entropy following b (2 bits compared to 0.71).", "labels": [], "entities": []}, {"text": "Since all sentences in the language are two word long, entropy goes down to 0 after the second word in both sentences.", "labels": [], "entities": [{"text": "entropy", "start_pos": 55, "end_pos": 62, "type": "METRIC", "confidence": 0.98973548412323}]}, {"text": "This hypothesis therefore does not predict a reading time difference on the second word e.", "labels": [], "entities": []}, {"text": "Moving onto the entropy reduction hypothesis, five of the six possible sentences in the language have probability 0.5 \u00d7 0.25, and the sixth one (bf ) has probability 0.5 \u00d7 0.75.", "labels": [], "entities": [{"text": "entropy reduction", "start_pos": 16, "end_pos": 33, "type": "TASK", "confidence": 0.7812399566173553}]}, {"text": "The full entropy of the grammar is therefore 2.4 bits.", "labels": [], "entities": []}, {"text": "The first word reduces entropy in both ae and be (to 2 and 0.71 bits respectively), but entropy reduction is higher when the first word is b.", "labels": [], "entities": []}, {"text": "The entropy reduction hypothesis therefore predicts longer reading times on the first word in be than in ae.", "labels": [], "entities": [{"text": "entropy reduction", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.8954130113124847}]}, {"text": "Conversely, since entropy goes down to 0 in both cases, but from 2 bits in ae compared to 0.71 bits in be, this hypothesis predicts longer reading times one in ae than in be.", "labels": [], "entities": [{"text": "entropy", "start_pos": 18, "end_pos": 25, "type": "METRIC", "confidence": 0.9325632452964783}]}, {"text": "Finally, the commitment hypothesis predicts that after b the reader will become committed to the prediction that the second word will be f . This will lead to longer reading times one in be than in ae, despite the fact that its conditional probability is identical in both cases.", "labels": [], "entities": []}, {"text": "If commitment to a prediction entails additional work, this hypothesis predicts longer reading times on the first word when it is b.", "labels": [], "entities": []}, {"text": "This paper presents an reading time study that aims to test these hypotheses.", "labels": [], "entities": []}, {"text": "Empirical tests of computational theories of sentence processing have employed either reading time corpora or controlled experimental materials.", "labels": [], "entities": [{"text": "sentence processing", "start_pos": 45, "end_pos": 64, "type": "TASK", "confidence": 0.7673527896404266}]}, {"text": "The current paper adopts the latter approach, trading off a decrease in lexical and syntactic heterogeneity for increased control.", "labels": [], "entities": []}, {"text": "This paper is divided into two parts.", "labels": [], "entities": []}, {"text": "Section 2 describes a reading time experiment, which tested the predictions of the surprisal, competition and commitment hypotheses, as applied to the entropy over the next single step in the syntactic derivation.", "labels": [], "entities": []}, {"text": "We then calculate the total entropy (up to an unbounded number of derivation steps) at each word using a PCFG; Section 3 describes how this grammar was constructed, overviews the predictions that it yielded in light of the four hypotheses, and evaluates these predictions on the results of the reading time experiment.", "labels": [], "entities": []}], "datasetContent": [{"text": "To keep syntactic structure constant while manipulating surprisal and entropy over the next derivation step, we took advantage of the fact that verbs vary in the probability distribution of their syntactic complements (subcategorization frames).", "labels": [], "entities": []}, {"text": "Several studies have demonstrated that readers are sensitive to subcategorization probabilities ().", "labels": [], "entities": []}, {"text": "The structure of the experimental materials is shown in.", "labels": [], "entities": []}, {"text": "Ina 2x2x2 factorial design, we crossed the surprisal of a sentential complement (SC) given the verb, the entropy of the verb's subcategorization distribution, and the presence or absence of the complementizer that.", "labels": [], "entities": []}, {"text": "When the complementizer is absent, the region the island is ambiguous between a direct object and an embedded subject.", "labels": [], "entities": []}, {"text": "Surprisal theory predicts an effect of SC surprisal on the disambiguating region in ambiguous sentences (sentences without that), as obtained in previous studies (, and an effect of SC surprisal on the complementizer that in unambiguous sentences.", "labels": [], "entities": []}, {"text": "Reading times should not differ at the verb: in the minimal context we used (the men), the surprisal of the verb should be closely approximated by its lexical frequency, which was matched across conditions.", "labels": [], "entities": []}, {"text": "The competition hypothesis predicts a positive main effect of subcategorization frame entropy (subcategorization frame entropy) at the verb: higher uncertainty over the syntactic category of the complement should result in slower reading times.", "labels": [], "entities": []}, {"text": "The commitment hypothesis predicts that the effect of surprisal in the disambiguating region should be amplified when subcategorization frame entropy is low, since the readers will have committed to the competing high probability frame.", "labels": [], "entities": []}, {"text": "If the commitment step in itself incurs a processing cost, there should be a negative main effect of subcategorization frame entropy at the verb.", "labels": [], "entities": []}, {"text": "This experimental design varies the entropy over the single next derivation step: it assumes that the parser only predicts the identity of the subcategorization frame, but not its internal structure.", "labels": [], "entities": []}, {"text": "Since the predictions of the entropy reduction hypothesis crucially depend on predicting the internal structure as well, we defer the discussion of that hypothesis until Section 3.", "labels": [], "entities": [{"text": "entropy reduction", "start_pos": 29, "end_pos": 46, "type": "TASK", "confidence": 0.829712301492691}]}, {"text": "The men discovered (that) the island mat. subj.", "labels": [], "entities": [{"text": "island mat. subj.", "start_pos": 30, "end_pos": 47, "type": "DATASET", "confidence": 0.9271978378295899}]}, {"text": "verb that emb. subj.", "labels": [], "entities": []}, {"text": "had been invaded by the enemy. emb.", "labels": [], "entities": [{"text": "emb.", "start_pos": 31, "end_pos": 35, "type": "DATASET", "confidence": 0.7979813516139984}]}, {"text": "verb complex rest: Structure of experimental materials (mat.", "labels": [], "entities": []}, {"text": "We tested whether reading times could be predicted by the word-by-word estimates derived from the PCFG.", "labels": [], "entities": [{"text": "PCFG", "start_pos": 98, "end_pos": 102, "type": "DATASET", "confidence": 0.9823655486106873}]}, {"text": "Since total entropy, entropy reduction and surprisal values did not lineup with the factorial design, we used continuous regression instead, again using lme4 with a maximal random effects structure.", "labels": [], "entities": [{"text": "entropy reduction", "start_pos": 21, "end_pos": 38, "type": "TASK", "confidence": 0.6694851964712143}]}, {"text": "We only analyzed words for which the predictions depended on the properties of the verb (as shows, this is only the case fora minority of the words).", "labels": [], "entities": []}, {"text": "As outcome variables, we considered both reading times on the word w i , and a spillover variable computed as the sum of the reading times on w i and the next word w i+1 . The predictors were standardized (separately for each word) to facilitate effect comparison.", "labels": [], "entities": []}, {"text": "Parser-derived entropy reduction values varied the most on the main verb.", "labels": [], "entities": [{"text": "Parser-derived entropy reduction", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.6673510273297628}]}, {"text": "Since the word following the verb differs between the ambiguous and unambiguous conditions, we added a categorical control variable for sentence ambiguity.", "labels": [], "entities": []}, {"text": "In the resulting model, lower entropy (or equivalently, higher entropy reduction values), caused an increase in reading times (no spillover: \u02c6 \u03b2 = 0.014, p = 0.05; one word spillover: \u02c6 \u03b2 = 0.022, p = 0.04).", "labels": [], "entities": []}, {"text": "Our design does not enable us to determine whether the effect of entropy on the verb is due to entropy reduction or simply entropy.", "labels": [], "entities": []}, {"text": "The commitment hypothesis is therefore equally supported by this pattern as is the entropy reduction hypothesis.", "labels": [], "entities": [{"text": "entropy reduction", "start_pos": 83, "end_pos": 100, "type": "TASK", "confidence": 0.786460816860199}]}, {"text": "The only other word on which entropy reduction values varied across verbs was the first word the of the ambiguous region.", "labels": [], "entities": []}, {"text": "Neither entropy reduction nor surprisal were significant predictors of reading times on this word.", "labels": [], "entities": []}, {"text": "There was also some variation across verbs in entropy (though not entropy reduction) on the second word of the embedded subject (island) in ambiguous sentences; however, entropy was not a significant predictor of reading times on that word.", "labels": [], "entities": []}, {"text": "In general, entropy is much higher in the embed-ded subject region in unambiguous than ambiguous sentences, since it is already known that the complement is an SC, and the entropy of an SC is higher.", "labels": [], "entities": []}, {"text": "Yet as mentioned above, reading times on the embedded subject were higher when it was ambiguous (p < 0.001).", "labels": [], "entities": []}, {"text": "Finally, PCFG-based surprisal was a significant predictor of reading times on the disambiguating word in ambiguous sentences (no spillover: n.s.; one word spillover: \u02c6 \u03b2 = 0.037, p = 0.02; twoword spillover: \u02c6 \u03b2 = 0.058, p = 0.001).", "labels": [], "entities": []}, {"text": "In contrast with simple SC surprisal (see Section 2.2.4), PCFG-based surprisal was not a significant predictor of reading times on the complementizer that in unambiguous sentences.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: A example verb from each of the four conditions.  On the left, probabilities of complement types: noun phrase  (NP), infinitive (Inf), prepositional phrase (PP), sentential  complement (SC); on the right, SC surprisal and subcatego- rization frame entropy.", "labels": [], "entities": []}]}