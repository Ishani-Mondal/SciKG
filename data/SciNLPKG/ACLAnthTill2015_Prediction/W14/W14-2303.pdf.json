{"title": [], "abstractContent": [{"text": "Most computational approaches to metaphor detection try to leverage either conceptual metaphor mappings or selec-tional preferences.", "labels": [], "entities": [{"text": "metaphor detection", "start_pos": 33, "end_pos": 51, "type": "TASK", "confidence": 0.9499062597751617}]}, {"text": "Both require extensive knowledge of the mappings/preferences in question, as well as sufficient data for all involved conceptual domains.", "labels": [], "entities": []}, {"text": "Creating these resources is expensive and often limits the scope of these systems.", "labels": [], "entities": []}, {"text": "We propose a statistical approach to metaphor detection that utilizes the rarity of novel metaphors, marking words that do not match a text's typical vocabulary as metaphor candidates.", "labels": [], "entities": [{"text": "metaphor detection", "start_pos": 37, "end_pos": 55, "type": "TASK", "confidence": 0.9128117561340332}]}, {"text": "No knowledge of semantic concepts or the metaphor's source domain is required.", "labels": [], "entities": []}, {"text": "We analyze the performance of this approach as a stand-alone classifier and as a feature in a machine learning model, reporting improvements in F 1 measure over a random baseline of 58% and 68%, respectively.", "labels": [], "entities": [{"text": "F 1 measure", "start_pos": 144, "end_pos": 155, "type": "METRIC", "confidence": 0.9718082944552103}]}, {"text": "We also observe that, as a feature, it appears to be particularly useful when data is sparse, while its effect diminishes as the amount of training data increases.", "labels": [], "entities": []}], "introductionContent": [{"text": "Metaphors are used to replace complicated or unfamiliar ideas with familiar, yet unrelated concepts that share an important attribute with the intended idea.", "labels": [], "entities": []}, {"text": "In NLP, detecting metaphors and other nonliteral figures of speech is necessary to interpret their meaning correctly.", "labels": [], "entities": [{"text": "detecting metaphors and other nonliteral figures of speech", "start_pos": 8, "end_pos": 66, "type": "TASK", "confidence": 0.8125790357589722}]}, {"text": "As metaphors area productive part of language, listing known examples is not sufficient.", "labels": [], "entities": []}, {"text": "Most computational approaches to metaphor detection are based either on the theory of conceptual mappings or that of preference violation.", "labels": [], "entities": [{"text": "metaphor detection", "start_pos": 33, "end_pos": 51, "type": "TASK", "confidence": 0.9628203809261322}, {"text": "preference violation", "start_pos": 117, "end_pos": 137, "type": "TASK", "confidence": 0.758745402097702}]}, {"text": "showed that metaphors have underlying mappings between two conceptual domains: The figurative source domain that the metaphor is taken from and the literal target domain of the surrounding context in which it has to be interpreted.", "labels": [], "entities": []}, {"text": "Various metaphors can be based on the same conceptual metaphor mapping, e.g. both \"The economy is a house of cards\" and \"the stakes of our debates appear small\" match POLITICS IS A GAME.", "labels": [], "entities": [{"text": "POLITICS IS A GAME", "start_pos": 167, "end_pos": 185, "type": "TASK", "confidence": 0.4777740240097046}]}, {"text": "Another attribute of metaphors is that they violate semantic selectional preferences.", "labels": [], "entities": []}, {"text": "The theory of selectional preference observes that verbs constrain their syntactic arguments by the semantic concepts they accept in these positions.", "labels": [], "entities": []}, {"text": "Metaphors violate these constraints, combining incompatible concepts.", "labels": [], "entities": []}, {"text": "To make use of these theories, extensive knowledge of pairings (either mappings or preferences) and the involved conceptual domains is required.", "labels": [], "entities": []}, {"text": "Especially in the case of conceptual mappings, this makes it very difficult for automated systems to achieve appropriate coverage of metaphors.", "labels": [], "entities": []}, {"text": "Even when limited to a single target domain, detecting all metaphors would require knowledge of many metaphoric source domains to coverall relevant mappings (which themselves have to be known, too).", "labels": [], "entities": []}, {"text": "As a result of this, many systems attempt to achieve high precision for specific mappings, rather than provide general coverage.", "labels": [], "entities": [{"text": "precision", "start_pos": 58, "end_pos": 67, "type": "METRIC", "confidence": 0.9972764849662781}]}, {"text": "Many approaches; Krishnakumaran and, and more) make use of manually crafted knowledge bases such as WordNet or FrameNet to establish concept domains.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 100, "end_pos": 107, "type": "DATASET", "confidence": 0.9698012471199036}]}, {"text": "Other recent works establish domains via topic modeling (, ad-hoc clustering ( or by using semantic similarity vectors (.", "labels": [], "entities": [{"text": "topic modeling", "start_pos": 41, "end_pos": 55, "type": "TASK", "confidence": 0.8123352527618408}]}, {"text": "We introduce term relevance as a measure for how \"out of place\" a word is in a given con-text.", "labels": [], "entities": []}, {"text": "Our hypothesis is that words will often be out of place because they are not meant literally, but rather metaphorically.", "labels": [], "entities": []}, {"text": "Term relevance is based on term frequency measures for target domains and mixed-domain data.", "labels": [], "entities": [{"text": "Term relevance", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.8730358183383942}]}, {"text": "The advantage of this approach is that it only requires knowledge of a text's literal target domain, but none about any source domains or conceptual mappings.", "labels": [], "entities": []}, {"text": "As it does not require sentence structure information, it is also resistant to noisy data, allowing the use of large, uncurated corpora.", "labels": [], "entities": []}, {"text": "While some works that utilize domain-mappings circumvent the need for pre-existing source data by generating it themselves (, our approach is truly source-independent.", "labels": [], "entities": []}, {"text": "We present a threshold classifier that uses term relevance as its only metric for metaphor detection.", "labels": [], "entities": [{"text": "metaphor detection", "start_pos": 82, "end_pos": 100, "type": "TASK", "confidence": 0.9367267787456512}]}, {"text": "In addition we evaluate the impact of term relevance at different training sizes.", "labels": [], "entities": []}, {"text": "Our contributions are: \u2022 We present a measure for non-literalness that only requires data for the literal domain(s) of a text.", "labels": [], "entities": []}, {"text": "\u2022 Our approach detects metaphors independently of their source domain.", "labels": [], "entities": []}, {"text": "\u2022 We report improvements for F 1 of 58% (stand-alone) and 68% (multi-feature) over a random baseline.", "labels": [], "entities": [{"text": "F 1", "start_pos": 29, "end_pos": 32, "type": "METRIC", "confidence": 0.9942119121551514}]}], "datasetContent": [{"text": "Evaluation of the two classifiers is done with a corpus of documents related to the concept of governance.", "labels": [], "entities": []}, {"text": "Texts were annotated for metaphoric phrases and phrases that are decidedly in-domain, as well as other factors (e.g. affect) that we will not concern ourselves with.", "labels": [], "entities": []}, {"text": "The focus of annotation was to exhaustively mark metaphors, irrespective of their novelty, but avoid idioms and metonymy.", "labels": [], "entities": []}, {"text": "The corpus is created as part of the MICS: Metaphor Interpretation in terms of Culturallyrelevant Schemas project by the U.S. Intelligence Advanced Research Projects Activity (IARPA).", "labels": [], "entities": [{"text": "MICS: Metaphor Interpretation", "start_pos": 37, "end_pos": 66, "type": "TASK", "confidence": 0.77603580057621}]}, {"text": "We use a snapshot containing 2,510 English sentences, taken from 312 documents.", "labels": [], "entities": []}, {"text": "Of the 2,078 sentences that contain metaphors, 72% contain only a single metaphoric phrase.", "labels": [], "entities": []}, {"text": "The corpus consists of around 48k tokens, 12% of which are parts of metaphors.", "labels": [], "entities": []}, {"text": "Removing stopwords and punctuation reduces it to 23k tokens and slightly skews the distribution, resulting in 15% being metaphors.", "labels": [], "entities": []}, {"text": "We divide the evaluation data into 80% development and 20% test data.", "labels": [], "entities": []}, {"text": "All reported results are based on test data.", "labels": [], "entities": []}, {"text": "Where training data is required for model training (see section 5), ten-fold cross validation is performed on the development set.", "labels": [], "entities": []}, {"text": "We evaluate and optimize our systems for the F 1 metric.", "labels": [], "entities": []}, {"text": "In addition we provide precision and recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 23, "end_pos": 32, "type": "METRIC", "confidence": 0.9995444416999817}, {"text": "recall", "start_pos": 37, "end_pos": 43, "type": "METRIC", "confidence": 0.9995074272155762}]}, {"text": "Accuracy on the other hand proved an inappropriate metric, as the prevalence of literal words in our data resulted in a heavy bias.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9876875281333923}]}, {"text": "We evaluate on a token-basis, as half of the metaphoric phrases consist of a single word and less than 15% are more than three words long (including stopwords, which are filtered out later).", "labels": [], "entities": []}, {"text": "Additionally, evaluating on a phrase-basis would have required grouping non-metaphor sections into phrases of a similar format.", "labels": [], "entities": []}, {"text": "Based on dev set performance, we choose a domain relevance threshold \u03b4 = 0.02 and a common relevance threshold \u03b3 = 0.1.", "labels": [], "entities": []}, {"text": "We provide a random baseline, as well as one that labels all words as metaphors, as they are the most frequently encountered baselines in related works.", "labels": [], "entities": []}, {"text": "Results are shown in table 3.", "labels": [], "entities": []}, {"text": "Both seed sets achieve similar F-scores, beating the baselines by between 39% and 58%, but their precision and recall performance differs notably.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.9982847571372986}, {"text": "precision", "start_pos": 97, "end_pos": 106, "type": "METRIC", "confidence": 0.9996007084846497}, {"text": "recall", "start_pos": 111, "end_pos": 117, "type": "METRIC", "confidence": 0.9994638562202454}]}, {"text": "Both models are significantly better than the baseline and significantly different from one another with p < 0.01.", "labels": [], "entities": []}, {"text": "Significance was computed fora two-tailed t-test using sigf . Using manually chosen seed terms results in a recall rate that is slightly worse than chance, but it is made up by the highest precision.", "labels": [], "entities": [{"text": "Significance", "start_pos": 0, "end_pos": 12, "type": "METRIC", "confidence": 0.9504925608634949}, {"text": "recall rate", "start_pos": 108, "end_pos": 119, "type": "METRIC", "confidence": 0.9911837577819824}, {"text": "precision", "start_pos": 189, "end_pos": 198, "type": "METRIC", "confidence": 0.9924181699752808}]}, {"text": "The fact that this was achieved without expert knowledge or term optimization is encouraging.", "labels": [], "entities": [{"text": "term optimization", "start_pos": 60, "end_pos": 77, "type": "TASK", "confidence": 0.74838587641716}]}, {"text": "The classifier using the fifty best governance terms shows a stronger recall, most likely be-cause the seeds are directly based on the development data, resulting in a domain cluster that more closely resembles the evaluation corpus.", "labels": [], "entities": [{"text": "recall", "start_pos": 70, "end_pos": 76, "type": "METRIC", "confidence": 0.9986893534660339}]}, {"text": "Precision, on the other hand, is slightly below that of the manual seed classifier.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9924370646476746}]}, {"text": "This might bean effect of the coarser granularity that a single domain score offers, as opposed to eight subdomain scores.", "labels": [], "entities": []}, {"text": "Performance of the CRF system (see) seems slightly disappointing at first when compared to our threshold classifier.", "labels": [], "entities": []}, {"text": "The bestperforming CRF beats the threshold classifier by only two points of F-score, despite considerably richer training input.", "labels": [], "entities": [{"text": "F-score", "start_pos": 76, "end_pos": 83, "type": "METRIC", "confidence": 0.9989571571350098}]}, {"text": "Precision and recall performance are reversed, i.e. the CRF provides a higher precision of 0.6, but only detects one out of four metaphor words.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9864665865898132}, {"text": "recall", "start_pos": 14, "end_pos": 20, "type": "METRIC", "confidence": 0.9920478463172913}, {"text": "precision", "start_pos": 78, "end_pos": 87, "type": "METRIC", "confidence": 0.9904003739356995}]}, {"text": "All models provide stable results for all folds, their standard deviation (about 0.01 for F 1 ) being almost equal to that of the baseline.", "labels": [], "entities": [{"text": "standard deviation", "start_pos": 55, "end_pos": 73, "type": "METRIC", "confidence": 0.9010513722896576}, {"text": "F 1 )", "start_pos": 90, "end_pos": 95, "type": "METRIC", "confidence": 0.9714013338088989}]}, {"text": "All results are significantly different from the baseline as well as from each other with p < 0.01, except for the precision scores of the three nonbasic CRF models, which are significantly different from each other with p < 0.05.", "labels": [], "entities": [{"text": "precision", "start_pos": 115, "end_pos": 124, "type": "METRIC", "confidence": 0.9993095397949219}]}, {"text": "Adding term relevance provides a consistent boost of 0.025 to the F-score.", "labels": [], "entities": [{"text": "F-score", "start_pos": 66, "end_pos": 73, "type": "METRIC", "confidence": 0.9944180250167847}]}, {"text": "This boost, however, is rather marginal in comparison to the one provided by part of speech and lexicographer sense.", "labels": [], "entities": []}, {"text": "A possible reason for this could be that the item weights learned during training correspond too closely to our term relevance scores, thus making them obsolete when enough training data is provided.", "labels": [], "entities": []}, {"text": "The next section explores this possibility by comparing different amounts of training data.", "labels": [], "entities": []}, {"text": "With 2000 metaphoric sentences, the dataset we used was already among the largest annotated corpora.", "labels": [], "entities": []}, {"text": "By reducing the amount of training data we evaluate whether term relevance is an efficient feature when data is sparse.", "labels": [], "entities": []}, {"text": "To this end, we repeat our ten-fold cross validations, but withhold some of the folds from each training set.", "labels": [], "entities": []}, {"text": "compares the performance of CRF feature configurations with and without term relevance.", "labels": [], "entities": []}, {"text": "In both cases adding term relevance outperforms the standard configuration's top performance with 400 sentences less, saving about a quarter of the training data.", "labels": [], "entities": []}, {"text": "In we also visualize the relative gain that adding term relevance provides.", "labels": [], "entities": []}, {"text": "As one can see, small datasets profit considerably more from our metric.", "labels": [], "entities": []}, {"text": "Given only 200 sentences, the PosLex model receives 4.7 times the performance gain from term relevance it got at at maximum training size.", "labels": [], "entities": []}, {"text": "The basic model has a factor of 6.8.", "labels": [], "entities": []}, {"text": "This supports our assumption that term relevance is similar to the item weights learned during CRF training.", "labels": [], "entities": []}, {"text": "As labeled training data is considerably more expensive to create than corpora for term relevance, this is an encouraging observation.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Summary of best performing settings  for each threshold classifier model. Bold num- bers indicate best performance; slanted bold num- bers: best threshold classifier recall. All results  are significantly different from the baselines with  p < 0.01.", "labels": [], "entities": [{"text": "recall", "start_pos": 176, "end_pos": 182, "type": "METRIC", "confidence": 0.9410915970802307}]}, {"text": " Table 4: Summary of best performing settings for  each CRF model. Bold numbers indicate best per- formance; slanted bold numbers: best CRF re- call. All results are significantly different from the  baseline with p < 0.01.", "labels": [], "entities": []}]}