{"title": [{"text": "Improving Classification-Based Natural Language Understanding with Non-Expert Annotation", "labels": [], "entities": [{"text": "Improving Classification-Based Natural Language Understanding", "start_pos": 0, "end_pos": 61, "type": "TASK", "confidence": 0.9179556965827942}]}], "abstractContent": [{"text": "Although data-driven techniques are commonly used for Natural Language Understanding in dialogue systems, their efficacy is often hampered by the lack of appropriate annotated training data in sufficient amounts.", "labels": [], "entities": [{"text": "Natural Language Understanding", "start_pos": 54, "end_pos": 84, "type": "TASK", "confidence": 0.651250809431076}]}, {"text": "We present an approach for rapid and cost-effective annotation of training data for classification-based language understanding in conversational dialogue systems.", "labels": [], "entities": [{"text": "classification-based language understanding", "start_pos": 84, "end_pos": 127, "type": "TASK", "confidence": 0.6616608301798502}]}, {"text": "Experiments using a web-accessible conversational character that interacts with a varied user population show that a dramatic improvement in natural language understanding and a substantial reduction inexpert annotation effort can be achieved by leveraging non-expert annotation .", "labels": [], "entities": [{"text": "natural language understanding", "start_pos": 141, "end_pos": 171, "type": "TASK", "confidence": 0.6805574099222819}]}], "introductionContent": [{"text": "Robust Natural Language Understanding (NLU) remains a challenge in conversational dialogue systems that allow arbitrary natural language input from users.", "labels": [], "entities": [{"text": "Robust Natural Language Understanding (NLU)", "start_pos": 0, "end_pos": 43, "type": "TASK", "confidence": 0.7482100938047681}]}, {"text": "Although data-driven approaches are now commonly used to address the NLU problem as one of classification, e.g. (, where input utterances are mapped automatically into system-specific categories, the dependence of such approaches on training data annotated with semantic classes or dialogue acts creates a chicken and egg problem: user utterances are needed to create the annotated training data necessary for NLU by classification, but these cannot be collected without a working system that users can interact with.", "labels": [], "entities": []}, {"text": "Common solutions to this problem include the use of Wizard-of-Oz data collection, where a human expert manually provides the functionality of data-driven modules while data is collected from users, or the use of scenario authors who attempt to anticipate user input to create an initial set of training data.", "labels": [], "entities": []}, {"text": "While these options offer practical ways around the training data acquisition problem, they typically require substantial work from system experts and provide suboptimal solutions: data-driven approaches work best when utterances in the training data are drawn from the same distribution as those encountered in actual system use, but the conditions under which training data is collected (a human expert filling in for systems modules, or a human expert generating possible user utterances) are quite different from those where users interact with the final system.", "labels": [], "entities": [{"text": "training data acquisition", "start_pos": 52, "end_pos": 77, "type": "TASK", "confidence": 0.6887730956077576}]}, {"text": "High quality results are often obtained through an iterative process where an initial training set is authored by a scenario designer, but NLU resources are gradually updated based on real user data overtime (.", "labels": [], "entities": []}, {"text": "Although this can ultimately produce training data composed primarily of real user utterances, and therefore result in better performance from data-driven models, an expert annotator is required to perform manual classification of user utterances.", "labels": [], "entities": []}, {"text": "This is a laborious process that assumes availability and willingness of the annotator for as long as it takes to collect enough user utterances, which may range from weeks to months or even years, depending on the size of the domain and the number and type of utterance categories.", "labels": [], "entities": []}, {"text": "The main question we address is whether annotation by non-experts can be leveraged to speedup utterance classification and lower its cost.", "labels": [], "entities": [{"text": "utterance classification", "start_pos": 94, "end_pos": 118, "type": "TASK", "confidence": 0.8980783224105835}]}, {"text": "We present a technique that frames the annotation of training data as a human intelligence task suitable for crowdsourcing.", "labels": [], "entities": []}, {"text": "Although there are similarities between our technique and active learning (e.g. see), an important difference is that our technique does not reduce the annotation effort by reducing the size of the data to be labeled, but by casting the annotation task into a simpler problem.", "labels": [], "entities": []}, {"text": "This allows us to take advantage of the entire data generated by the users.", "labels": [], "entities": []}, {"text": "Through an experiment with a conversational dia-logue system deployed on the web, we show that a dramatic improvement in the quality of NLU can be achieved with non-expert data annotation, reducing the time required of an expert annotator by 70%.", "labels": [], "entities": []}], "datasetContent": [{"text": "To test our hypothesis that language understanding can be improved with much reduced expert effort, we applied the framework described above to a system that implements a conversational character that talks with users about issues relating to mental and behavioral disorders and presents healthcare options.", "labels": [], "entities": [{"text": "language understanding", "start_pos": 28, "end_pos": 50, "type": "TASK", "confidence": 0.7133864015340805}]}, {"text": "The system is publicly accessible at http://www.simcoach.org, and receives traffic on the order of one hundred users per week.", "labels": [], "entities": []}, {"text": "Of these, about one quarter engage the system in a meaningful dialogue with multiple turns, with the dialogues containing on average 16 user utterances.", "labels": [], "entities": []}, {"text": "Because our process depends crucially on user traffic to generate data for annotation, a webaccessible system is ideally suited for it.", "labels": [], "entities": []}, {"text": "An excerpt from atypical interaction with the system is shown in.", "labels": [], "entities": []}, {"text": "The system and the NLU classifier based on Maximum Entropy models) are described respectively in () and ().", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: NLU accuracy obtained using the initial  training dataset T 0 , after one round of annotation  with T 1 (2,013 utterances), after two rounds of an- notation with T 2 (additional 948 utterances), and  after three rounds with T 3 (additional 1806 utter- ances). Accuracy is estimated on the same heldout  set of dialogues E 3 for all conditions, accounting  for roughly 10% of the annotated data.", "labels": [], "entities": [{"text": "NLU", "start_pos": 10, "end_pos": 13, "type": "DATASET", "confidence": 0.38067173957824707}, {"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9800352454185486}, {"text": "Accuracy", "start_pos": 270, "end_pos": 278, "type": "METRIC", "confidence": 0.9992440938949585}]}]}