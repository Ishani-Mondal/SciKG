{"title": [{"text": "POSTECH Grammatical Error Correction System in the CoNLL- 2014 Shared Task", "labels": [], "entities": [{"text": "POSTECH Grammatical Error Correction", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.5465499982237816}, {"text": "CoNLL- 2014 Shared Task", "start_pos": 51, "end_pos": 74, "type": "DATASET", "confidence": 0.9080679297447205}]}], "abstractContent": [{"text": "This paper describes the POSTECH grammatical error correction system.", "labels": [], "entities": [{"text": "POSTECH grammatical error correction", "start_pos": 25, "end_pos": 61, "type": "TASK", "confidence": 0.8082211911678314}]}, {"text": "Various methods are proposed to correct errors such as rule-based, probability n-gram vector approaches and router-based approach.", "labels": [], "entities": []}, {"text": "Google N-gram count corpus is used mainly as the correction resource.", "labels": [], "entities": [{"text": "Google N-gram count corpus", "start_pos": 0, "end_pos": 26, "type": "DATASET", "confidence": 0.9068857431411743}]}, {"text": "Correction candidates are extracted from NUCLE training data and each candidate is evaluated with development data to extract high precision rules and n-gram frames.", "labels": [], "entities": [{"text": "NUCLE training data", "start_pos": 41, "end_pos": 60, "type": "DATASET", "confidence": 0.8960253397623698}]}, {"text": "Out of 13 participating teams, our system is ranked 4 th on both the original and revised annotation.", "labels": [], "entities": []}], "introductionContent": [{"text": "Automatic grammar error correction (GEC) is widely used by learners of English as a second language (ESL) in written tasks.", "labels": [], "entities": [{"text": "Automatic grammar error correction (GEC)", "start_pos": 0, "end_pos": 40, "type": "TASK", "confidence": 0.7253309530871255}]}, {"text": "Many methods have been proposed to correct grammatical errors; these include methods based on rules, on statistical machine translation), on machine learning, and on n-grams ().", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 104, "end_pos": 135, "type": "TASK", "confidence": 0.6506685415903727}]}, {"text": "Early research () on error correction for nonnative text was based on well-formed corpora.", "labels": [], "entities": [{"text": "error correction", "start_pos": 21, "end_pos": 37, "type": "TASK", "confidence": 0.7422275841236115}]}, {"text": "Most recent work ( has used machine learning methods that rely on a GEtagged corpus such as NUCLE, Japanese English Learner corpus, and Cambridge Learner Corpus (, because well-formed and GEtagged approaches are closely related to each other, can be synergistically combined.", "labels": [], "entities": [{"text": "NUCLE", "start_pos": 92, "end_pos": 97, "type": "DATASET", "confidence": 0.9453887939453125}, {"text": "Japanese English Learner corpus", "start_pos": 99, "end_pos": 130, "type": "DATASET", "confidence": 0.7825323939323425}, {"text": "Cambridge Learner Corpus", "start_pos": 136, "end_pos": 160, "type": "DATASET", "confidence": 0.9665212631225586}]}, {"text": "Therefore, research using both types of data has also been conducted).", "labels": [], "entities": []}, {"text": "Moreover, a meta-classification method using several GEtagged corpora and a native corpus has been proposed to correct the grammatical errors (.", "labels": [], "entities": []}, {"text": "A meta-classifier approach has been proposed to combine a language model and error-specific classification for correction of article and preposition errors.", "labels": [], "entities": []}, {"text": "Web-scale well-formed corpora have been successfully applied to grammar error correction tasks instead of using error-tagged data (.", "labels": [], "entities": [{"text": "grammar error correction tasks", "start_pos": 64, "end_pos": 94, "type": "TASK", "confidence": 0.7048412039875984}]}, {"text": "Especially in the CoNLL-2013 grammar error correction shared task, many of the high-ranked teams () exploited the Google Web-1T n-gram corpus.", "labels": [], "entities": [{"text": "CoNLL-2013 grammar error correction shared task", "start_pos": 18, "end_pos": 65, "type": "TASK", "confidence": 0.7650096615155538}, {"text": "Google Web-1T n-gram corpus", "start_pos": 114, "end_pos": 141, "type": "DATASET", "confidence": 0.848215788602829}]}, {"text": "The major advantage of using these webscale corpora is that extremely large quantities of data are publicly available at no additional costs; thus fewer data sparseness problems arise compared to previous approaches based on errortagged corpora.", "labels": [], "entities": []}, {"text": "We also use the Google Web-1T n-gram corpus.", "labels": [], "entities": [{"text": "Google Web-1T n-gram corpus", "start_pos": 16, "end_pos": 43, "type": "DATASET", "confidence": 0.882285013794899}]}, {"text": "We extract the candidate pairs (original erroneous text and its correction) from NUCLE training data.", "labels": [], "entities": [{"text": "NUCLE training data", "start_pos": 81, "end_pos": 100, "type": "DATASET", "confidence": 0.9382375677426656}]}, {"text": "We use a router to choose the best frame to compare the n-gram score difference between the original and replacement in a given candidate pair.", "labels": [], "entities": []}, {"text": "The intuition of our grammar error correction method is the following: First, if the uni-gram count is less than some threshold, we assume that the word is erroneous.", "labels": [], "entities": [{"text": "grammar error correction", "start_pos": 21, "end_pos": 45, "type": "TASK", "confidence": 0.6143291890621185}]}, {"text": "Second, if the replacement word n-gram has more frequent than the original word n-gram, it presents strong evidence for correction.", "labels": [], "entities": [{"text": "correction", "start_pos": 120, "end_pos": 130, "type": "METRIC", "confidence": 0.8104370832443237}]}, {"text": "Third, depending on the candidate pair, tailored n-gram frames help to correct errors accurately.", "labels": [], "entities": []}, {"text": "Fourth, only high precision method and rules are applied.", "labels": [], "entities": [{"text": "precision", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.9965510368347168}]}, {"text": "If correction precision on a candidate pair is less than 30% in development data, we do not make a correction for the candidate pair at runtime.", "labels": [], "entities": [{"text": "correction precision", "start_pos": 3, "end_pos": 23, "type": "METRIC", "confidence": 0.7404442131519318}]}, {"text": "In the CoNLL-Shared Task, objectives were presented yearly.", "labels": [], "entities": []}, {"text": "In 2012, the objective was to correct article and preposition errors; in 2013, it was to correct article, preposition, noun number, verb form, and subject-verb agreement errors.", "labels": [], "entities": []}, {"text": "This year, the objective is to correct all errors.", "labels": [], "entities": [{"text": "errors", "start_pos": 43, "end_pos": 49, "type": "METRIC", "confidence": 0.9196459054946899}]}, {"text": "Thus, our method should also correct preprocessing and spelling errors.", "labels": [], "entities": [{"text": "preprocessing", "start_pos": 37, "end_pos": 50, "type": "METRIC", "confidence": 0.9709582924842834}]}, {"text": "Detailed description of the shared task setup, data set, and evaluation about the CoNLL-2014 Shared Task is explained in (", "labels": [], "entities": []}], "datasetContent": [{"text": "The CoNLL-2014 training data consist of 1,397 articles together with gold-standard annotation.", "labels": [], "entities": [{"text": "CoNLL-2014 training data", "start_pos": 4, "end_pos": 28, "type": "DATASET", "confidence": 0.9005546172459921}]}, {"text": "original = ngramCount( tok sent ), candidate =ngramCount(tok cand ) 10: If original < candidate then 11: Return tok cand The documents area subset of the NUS Corpus of Learner English (NUCLE).", "labels": [], "entities": [{"text": "NUS Corpus of Learner English (NUCLE)", "start_pos": 154, "end_pos": 191, "type": "DATASET", "confidence": 0.9431396275758743}]}, {"text": "We use the MaxMatch (M2) scorer provided by the CoNLL-2014 Shared Task.", "labels": [], "entities": [{"text": "CoNLL-2014 Shared Task", "start_pos": 48, "end_pos": 70, "type": "DATASET", "confidence": 0.9029532670974731}]}, {"text": "The M2 scorer works by using the set that maximally matches the set of gold-standard edits specified by the annotator as being equal to the set of system edits that are automatically computed and used in scoring.", "labels": [], "entities": []}, {"text": "The official evaluation metric is F0.5, weighting precision twice as much as recall.", "labels": [], "entities": [{"text": "F0.5", "start_pos": 34, "end_pos": 38, "type": "METRIC", "confidence": 0.9991940855979919}, {"text": "precision", "start_pos": 50, "end_pos": 59, "type": "METRIC", "confidence": 0.9045110940933228}, {"text": "recall", "start_pos": 77, "end_pos": 83, "type": "METRIC", "confidence": 0.9989765882492065}]}, {"text": "We achieve F0.5 of 30.88; precision of 34.51; recall of 21.73 in the original annotation.", "labels": [], "entities": [{"text": "F0.5", "start_pos": 11, "end_pos": 15, "type": "METRIC", "confidence": 0.9991670846939087}, {"text": "precision", "start_pos": 26, "end_pos": 35, "type": "METRIC", "confidence": 0.9997640252113342}, {"text": "recall", "start_pos": 46, "end_pos": 52, "type": "METRIC", "confidence": 0.9998001456260681}]}, {"text": "After original official annotations announced by organizers (i.e., only based on the annotations of the two annotators), another set of annotations is offered based on including the additional answers proposed by the 3 teams (CAMB, CUUI, UMC).", "labels": [], "entities": [{"text": "CAMB", "start_pos": 226, "end_pos": 230, "type": "DATASET", "confidence": 0.8999691605567932}, {"text": "CUUI", "start_pos": 232, "end_pos": 236, "type": "DATASET", "confidence": 0.7449322938919067}]}, {"text": "The improvement gap between the original annotation and the revised annotation of our team (POST) is 5.89%.", "labels": [], "entities": [{"text": "improvement gap", "start_pos": 4, "end_pos": 19, "type": "METRIC", "confidence": 0.9700993299484253}]}, {"text": "We obtain the highest improvement rate except for the 3 proposed teams), F0.5 of 36.77; precision of 41.28; recall of 25.59 in the revised annotation.", "labels": [], "entities": [{"text": "F0.5", "start_pos": 73, "end_pos": 77, "type": "METRIC", "confidence": 0.9995530247688293}, {"text": "precision", "start_pos": 88, "end_pos": 97, "type": "METRIC", "confidence": 0.999756395816803}, {"text": "recall", "start_pos": 108, "end_pos": 114, "type": "METRIC", "confidence": 0.9997537732124329}]}, {"text": "Our system achieves the 4 th highest scores of 13 participating teams based on both the original and revised annotations.", "labels": [], "entities": []}, {"text": "To analyze the scores of each of the error types and modules, we apply the method of n-gram vector (Nn), rule-based (Verb, Mec), and router-based (others) separately in both the original and the revised annotation of all error types.", "labels": [], "entities": []}, {"text": "We achieve high precision by rules at the Mec which indicates punctuation, capitalization, spelling, and typos errors.", "labels": [], "entities": [{"text": "precision", "start_pos": 16, "end_pos": 25, "type": "METRIC", "confidence": 0.9992356300354004}]}, {"text": "Additionally, the Nn type has the highest improvement gap between the original and revised annotation (17% \u2192 24.31 of F0.5).", "labels": [], "entities": [{"text": "improvement gap", "start_pos": 42, "end_pos": 57, "type": "METRIC", "confidence": 0.9802689254283905}, {"text": "F0.5", "start_pos": 118, "end_pos": 122, "type": "METRIC", "confidence": 0.9904422163963318}]}, {"text": "In order for our team to improve the high precision in the rulebased approach, we tested potential rules on the development data and kept a rule only if its precision on that data set was 30% or greater.", "labels": [], "entities": [{"text": "precision", "start_pos": 42, "end_pos": 51, "type": "METRIC", "confidence": 0.9991605281829834}, {"text": "precision", "start_pos": 157, "end_pos": 166, "type": "METRIC", "confidence": 0.9954643845558167}]}, {"text": "When we trained router, the same strategy was conducted.", "labels": [], "entities": []}, {"text": "If a frame could not achieve 30% precision, we assigned the candidate pair as \"no correction\" in the router.", "labels": [], "entities": [{"text": "precision", "start_pos": 33, "end_pos": 42, "type": "METRIC", "confidence": 0.9991372227668762}]}, {"text": "These constraints achieve precision of 30 % inmost error types.", "labels": [], "entities": [{"text": "precision", "start_pos": 26, "end_pos": 35, "type": "METRIC", "confidence": 0.9992918968200684}]}], "tableCaptions": [{"text": " Table 4. Performance on each error type", "labels": [], "entities": [{"text": "Performance", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9725020527839661}]}]}