{"title": [{"text": "Word Order Does NOT Differ Significantly Between Chinese and Japanese", "labels": [], "entities": [{"text": "Word Order", "start_pos": 0, "end_pos": 10, "type": "TASK", "confidence": 0.6376423537731171}, {"text": "NOT Differ Significantly", "start_pos": 16, "end_pos": 40, "type": "TASK", "confidence": 0.8028932611147562}]}], "abstractContent": [{"text": "We propose a pre-reordering approach for Japanese-to-Chinese statistical machine translation (SMT).", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 61, "end_pos": 98, "type": "TASK", "confidence": 0.7655034710963567}]}, {"text": "The approach uses dependency structure and manually designed reordering rules to arrange morphemes of Japanese sentences into Chinese-like word order, before a baseline phrase-based (PB) SMT system applied.", "labels": [], "entities": [{"text": "SMT", "start_pos": 187, "end_pos": 190, "type": "TASK", "confidence": 0.8000918626785278}]}, {"text": "Experimental results on the ASPEC-JC data show that the improvement of the proposed pre-reordering approach is slight on BLEU and mediocre on RIBES, compared with the organizer's baseline PB SMT system.", "labels": [], "entities": [{"text": "ASPEC-JC data", "start_pos": 28, "end_pos": 41, "type": "DATASET", "confidence": 0.8159662783145905}, {"text": "BLEU", "start_pos": 121, "end_pos": 125, "type": "METRIC", "confidence": 0.9967633485794067}, {"text": "RIBES", "start_pos": 142, "end_pos": 147, "type": "METRIC", "confidence": 0.9103133082389832}, {"text": "PB SMT", "start_pos": 188, "end_pos": 194, "type": "TASK", "confidence": 0.5354614555835724}]}, {"text": "The approach also shows improvement inhuman evaluation.", "labels": [], "entities": []}, {"text": "We observe the word order does not differ much in the two languages, though Japanese is a subject-object-verb (SOV) language and Chinese is an SVO language.", "labels": [], "entities": []}], "introductionContent": [{"text": "The state-of-the-art techniques of statistical machine translation (SMT) () demonstrate good performance on translation of languages with relatively similar word orders ().", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 35, "end_pos": 72, "type": "TASK", "confidence": 0.7995524903138479}]}, {"text": "However, word reordering is a problematic issue for language pairs with significantly different word orders, such as the translation between a subject-verb-object (SVO) language and a subject-object-verb (SOV) language (.", "labels": [], "entities": [{"text": "word reordering", "start_pos": 9, "end_pos": 24, "type": "TASK", "confidence": 0.7858436405658722}]}, {"text": "To resolve the word reordering problem in SMT, a line of research handles the word reordering as a separate pre-process, which is referred as pre-reordering.", "labels": [], "entities": [{"text": "word reordering", "start_pos": 15, "end_pos": 30, "type": "TASK", "confidence": 0.7064837217330933}, {"text": "SMT", "start_pos": 42, "end_pos": 45, "type": "TASK", "confidence": 0.97807377576828}]}, {"text": "In pre-reordering, the word order on source-side is arranged into the target-side word order, before a standard SMT system is applied, on both training and decoding phases.", "labels": [], "entities": [{"text": "SMT", "start_pos": 112, "end_pos": 115, "type": "TASK", "confidence": 0.9885899424552917}]}, {"text": "An effective rule-based approach, head finalization has been proposed for English-to-Japanese translation ().", "labels": [], "entities": [{"text": "head finalization", "start_pos": 34, "end_pos": 51, "type": "TASK", "confidence": 0.8960456550121307}, {"text": "English-to-Japanese translation", "start_pos": 74, "end_pos": 105, "type": "TASK", "confidence": 0.612397626042366}]}, {"text": "The approach takes advantage of the head final property of Japanese on the target-side.", "labels": [], "entities": []}, {"text": "It designs ahead finalization rule to move the headword based on the parsing result by a head-driven phrase structure grammar (HPSG) parser.", "labels": [], "entities": [{"text": "head-driven phrase structure grammar (HPSG) parser", "start_pos": 89, "end_pos": 139, "type": "TASK", "confidence": 0.6939914152026176}]}, {"text": "Generally, the idea can be applied to other SVO-to-Japanese translation tasks, such as its application in Chinese-toJapanese translation.", "labels": [], "entities": [{"text": "SVO-to-Japanese translation", "start_pos": 44, "end_pos": 71, "type": "TASK", "confidence": 0.7863042950630188}, {"text": "Chinese-toJapanese translation", "start_pos": 106, "end_pos": 136, "type": "TASK", "confidence": 0.6274547278881073}]}, {"text": "However, the head finalization cannot be applied on the reverse translation task, i.e. Japaneseto-SVO translation, which becomes a more difficult task.", "labels": [], "entities": [{"text": "head finalization", "start_pos": 13, "end_pos": 30, "type": "TASK", "confidence": 0.7844825983047485}, {"text": "Japaneseto-SVO translation", "start_pos": 87, "end_pos": 113, "type": "TASK", "confidence": 0.7517869174480438}]}, {"text": "Specifically, Japanese-to-English translation has been studied and several rule-based prereordering approaches have been proposed, taking advantage of the characters of Japanese and English (.", "labels": [], "entities": [{"text": "Japanese-to-English translation", "start_pos": 14, "end_pos": 45, "type": "TASK", "confidence": 0.6600983589887619}]}, {"text": "A comparison of these approaches is reported in.", "labels": [], "entities": []}, {"text": "Because both Chinese and English are SVO languages, to transfer approaches of Japanese-toEnglish to Japanese-to-Chinese translation is a natural idea.", "labels": [], "entities": []}, {"text": "Based on the framework of, we propose dependency-based prereordering rules for Japanese-to-Chinese translation in this paper.", "labels": [], "entities": [{"text": "Japanese-to-Chinese translation", "start_pos": 79, "end_pos": 110, "type": "TASK", "confidence": 0.5695880204439163}]}, {"text": "Contrary to our expectations, from the experimental results on ASPEC-JC data, we discover that the rule-based pre-reordering cannot improve the Japanese-to-Chinese significantly as in the case of Japanese-to-English translation.", "labels": [], "entities": [{"text": "ASPEC-JC data", "start_pos": 63, "end_pos": 76, "type": "DATASET", "confidence": 0.7299088835716248}]}, {"text": "Ina further investigation, we find a basic reason is that the word order is actually similar between Chinese and Japanese.", "labels": [], "entities": []}, {"text": "Chinese and English, though both of them are SVO languages, have very different properties.", "labels": [], "entities": []}], "datasetContent": [{"text": "We tested our approach on the ASPEC-JC data ().", "labels": [], "entities": [{"text": "ASPEC-JC data", "start_pos": 30, "end_pos": 43, "type": "DATASET", "confidence": 0.8364363014698029}]}, {"text": "For the source side Japanese sentences, we used MeCab (IPA dictionary) for morpheme analysis, CaboCha 3 () for chunking and dependency parsing.", "labels": [], "entities": [{"text": "MeCab (IPA dictionary)", "start_pos": 48, "end_pos": 70, "type": "DATASET", "confidence": 0.80528644323349}, {"text": "morpheme analysis", "start_pos": 75, "end_pos": 92, "type": "TASK", "confidence": 0.7230589985847473}, {"text": "dependency parsing", "start_pos": 124, "end_pos": 142, "type": "TASK", "confidence": 0.8197631239891052}]}, {"text": "We used the Stanford Chinese Word Segmenter 4 () with the Chinese Penn Treebank standard (CTB) to seg-ment each Chinese sentence.", "labels": [], "entities": [{"text": "Chinese Penn Treebank standard (CTB)", "start_pos": 58, "end_pos": 94, "type": "DATASET", "confidence": 0.8624004210744586}]}, {"text": "We used the phrasebased (PB) translation system in Moses 5 () as a baseline SMT system.", "labels": [], "entities": [{"text": "phrasebased (PB) translation", "start_pos": 12, "end_pos": 40, "type": "TASK", "confidence": 0.5526196599006653}, {"text": "SMT", "start_pos": 76, "end_pos": 79, "type": "TASK", "confidence": 0.9897928237915039}]}, {"text": "Word alignment was automatically generated by GIZA++ 6) with the default setting of Moses, and symmetrized by the grow-diagfinal-and heuristics (.", "labels": [], "entities": [{"text": "Word alignment", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.6531837582588196}]}, {"text": "In phrase extraction, the max-phrase-length was 7 with GoodTuring option in scoring.", "labels": [], "entities": [{"text": "phrase extraction", "start_pos": 3, "end_pos": 20, "type": "TASK", "confidence": 0.8703773021697998}, {"text": "max-phrase-length", "start_pos": 26, "end_pos": 43, "type": "METRIC", "confidence": 0.9848730564117432}]}, {"text": "The language model used in decoding is an interpolated modified Kneser-Ney discounted 5-gram model, trained on the English side of the training corpus by SRILM 7).", "labels": [], "entities": [{"text": "SRILM 7", "start_pos": 154, "end_pos": 161, "type": "DATASET", "confidence": 0.909478634595871}]}, {"text": "In decoding, the distortion-limit was 9.", "labels": [], "entities": [{"text": "distortion-limit", "start_pos": 17, "end_pos": 33, "type": "METRIC", "confidence": 0.9928279519081116}]}, {"text": "The MERT was used to tune the feature weights on the development set and the translation performance was evaluated on the test set with the tuned weights.", "labels": [], "entities": [{"text": "MERT", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.8591775894165039}, {"text": "translation", "start_pos": 77, "end_pos": 88, "type": "TASK", "confidence": 0.9569584727287292}]}, {"text": "We used identical decoding settings on development and test sets.", "labels": [], "entities": []}, {"text": "Our approach reached a test set BLEU of 28.18 with CTB segmentation in the final evaluation, which had a slight improvement compared with the 28.01 of the organizer's PB SMT baseline.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 32, "end_pos": 36, "type": "METRIC", "confidence": 0.9996930360794067}, {"text": "CTB segmentation", "start_pos": 51, "end_pos": 67, "type": "TASK", "confidence": 0.6503582894802094}, {"text": "PB SMT", "start_pos": 167, "end_pos": 173, "type": "TASK", "confidence": 0.4965676963329315}]}, {"text": "As to the reordering measure RIBES, our approach reached a score of 0.8087, which had a mediocre improvement compared with the organizer's 0.7926.", "labels": [], "entities": [{"text": "RIBES", "start_pos": 29, "end_pos": 34, "type": "METRIC", "confidence": 0.6237025856971741}]}, {"text": "In the human evaluation, our approach also had an improvement of 6.5 percent according to the organizer's evaluation score.", "labels": [], "entities": []}], "tableCaptions": []}