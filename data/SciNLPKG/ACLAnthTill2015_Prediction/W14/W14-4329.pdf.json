{"title": [{"text": "Learning to Re-rank for Interactive Problem Resolution and Query Refinement", "labels": [], "entities": [{"text": "Interactive Problem Resolution", "start_pos": 24, "end_pos": 54, "type": "TASK", "confidence": 0.6472877760728201}, {"text": "Query Refinement", "start_pos": 59, "end_pos": 75, "type": "TASK", "confidence": 0.7069774121046066}]}], "abstractContent": [{"text": "We study the design of an information retrieval (IR) system that assists customer service agents while they interact with end-users.", "labels": [], "entities": [{"text": "information retrieval (IR)", "start_pos": 26, "end_pos": 52, "type": "TASK", "confidence": 0.8476802587509156}]}, {"text": "The type of IR needed is difficult because of the large lexical gap between problems as described by customers , and solutions.", "labels": [], "entities": [{"text": "IR", "start_pos": 12, "end_pos": 14, "type": "TASK", "confidence": 0.9914056062698364}]}, {"text": "We describe an approach that bridges this lexical gap by learning semantic relatedness using tensor representations.", "labels": [], "entities": []}, {"text": "Queries that are short and vague, which are common in practice, result in a large number of documents being retrieved, and a high cognitive load for customer service agents.", "labels": [], "entities": []}, {"text": "We show how to reduce this burden by providing suggestions that are selected based on the learned measures of semantic relatedness.", "labels": [], "entities": []}, {"text": "Experiments show that the approach offers substantial benefit compared to the use of standard lexical similarity.", "labels": [], "entities": []}], "introductionContent": [{"text": "Information retrieval systems help businesses and individuals make decisions by automatically extracting actionable intelligence from large (unstructured) data; Antonio Palma-dos).", "labels": [], "entities": [{"text": "Information retrieval", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.7709938287734985}]}, {"text": "This paper focuses on the application of retrieval systems in a contact centers where the system assists agents while they are helping customers with problem resolution.", "labels": [], "entities": [{"text": "problem resolution", "start_pos": 150, "end_pos": 168, "type": "TASK", "confidence": 0.8193853795528412}]}, {"text": "Currently, most contact center information retrieval use (web based) front-ends to search engines indexed with knowledge sources.", "labels": [], "entities": [{"text": "contact center information retrieval", "start_pos": 16, "end_pos": 52, "type": "TASK", "confidence": 0.5804922059178352}]}, {"text": "Agents enter queries to retrieve documents related to the customer's problem.", "labels": [], "entities": []}, {"text": "These sources are often incomplete as it is unlikely that all possible customer problems can be identified before product release.", "labels": [], "entities": []}, {"text": "This is particularly true for recently released and frequently updated products.", "labels": [], "entities": []}, {"text": "One approach, which we build on here, is to mine problems and resolutions from online discussion forums Yahoo!", "labels": [], "entities": []}, {"text": "Answers 1 Ubuntu Forums 2 and Apple Support Communities . While these often provide useful solutions within hours or days of a problem surfacing, they are semantically noisy (.", "labels": [], "entities": [{"text": "Apple Support Communities", "start_pos": 30, "end_pos": 55, "type": "DATASET", "confidence": 0.8773143092791239}]}, {"text": "Most contact centers and agents are evaluated based on the number of calls they handle over a period ().", "labels": [], "entities": []}, {"text": "As a result, queries entered by agents into the search engine are usually underspecified.", "labels": [], "entities": []}, {"text": "This, together with noise in the database, results in a large number of documents being retrieved as relevant documents.", "labels": [], "entities": []}, {"text": "This in turn, increases the cognitive load on agents, and reduces the effectiveness of the search system and the efficiency of the contact center.", "labels": [], "entities": []}, {"text": "Our first task in this paper is to automatically make candidate suggestions that reduce the search space of relevant documents in a contact center application.", "labels": [], "entities": []}, {"text": "The agent/user then interacts with the system by selecting one of the suggestions.", "labels": [], "entities": []}, {"text": "This is used to expand the original query and the process can be repeated.", "labels": [], "entities": []}, {"text": "We show that even one round of interaction, with a small set of suggestions, can lead to high quality solutions to user problems.", "labels": [], "entities": []}, {"text": "In query expansion, the classical approach is to automatically find suggestions either in the form of words, phrases or similar queries ().", "labels": [], "entities": [{"text": "query expansion", "start_pos": 3, "end_pos": 18, "type": "TASK", "confidence": 0.8525539934635162}]}, {"text": "These can be obtained either from query logs or based on their representativeness of the initial retrieved documents ().", "labels": [], "entities": []}, {"text": "The suggestions are then ranked either based on their frequencies or based on their similarity to the original query (.", "labels": [], "entities": []}, {"text": "For example, if suggestions and queries are represented as term vectors (e.g. term frequency-inverse document frequency or tfidf) their similarity maybe determined using similarity measures such as cosine similarity or inverse of euclidean distance.", "labels": [], "entities": []}, {"text": "However, in question-answering and problemresolution domains, and in contrast to traditional Information Retrieval, most often the query and the suggestions do not have many overlapping words.", "labels": [], "entities": [{"text": "Information Retrieval", "start_pos": 93, "end_pos": 114, "type": "TASK", "confidence": 0.6940547376871109}]}, {"text": "This leads to low similarity scores, even when the suggestion is highly relevant.", "labels": [], "entities": [{"text": "similarity scores", "start_pos": 18, "end_pos": 35, "type": "METRIC", "confidence": 0.9791247248649597}]}, {"text": "Consider the representative example in, taken from our crawled dataset.", "labels": [], "entities": []}, {"text": "Although the suggestions, \"does not support file transfer\", \"connection not stable\", \"pairing failed\" are highly relevant for the problem of \"Bluetooth not working\", their lexical similarity score is zero.", "labels": [], "entities": []}, {"text": "The second task that this paper addresses is how to bridge this lexical chasm between the query and the suggestions.", "labels": [], "entities": []}, {"text": "For this, we learn a measure of semantic-relatedness between the query and the suggestions rather than defining closeness based on lexical similarity.", "labels": [], "entities": []}], "datasetContent": [{"text": "We collected data by crawling forum discussion threads from the Apple Discussion Forum, created during the period 2007-2011, resulting in about 147,000 discussion threads.", "labels": [], "entities": [{"text": "Apple Discussion Forum", "start_pos": 64, "end_pos": 86, "type": "DATASET", "confidence": 0.8648294607798258}]}, {"text": "The underspecified queries and specific queries were created as follows.", "labels": [], "entities": []}, {"text": "Discussion threads were first clustered treating each discussion thread as a data point using a tf-idf representation.", "labels": [], "entities": []}, {"text": "The thread nearest the centroid of the 60 largest clusters were marked as the 'most common' problems.", "labels": [], "entities": []}, {"text": "The first post is used as a proxy for the problem description.", "labels": [], "entities": []}, {"text": "An annotator was asked to then create: Specific Queries generated with the underspecified Query, \"Safari not working\".", "labels": [], "entities": []}, {"text": "a short query (underspecified) from the first post of each of the 60 selected threads.", "labels": [], "entities": []}, {"text": "These queries were given to the Lemur search engine) to retrieve the 50 most similar threads from an index built on the entire set of 147,000 threads.", "labels": [], "entities": [{"text": "Lemur search engine", "start_pos": 32, "end_pos": 51, "type": "DATASET", "confidence": 0.9097156922022501}]}, {"text": "The annotator manually analyzed the first posts of the retrieved threads to create contexts, resulting in a total 200 specific queries.", "labels": [], "entities": []}, {"text": "We give an example to illustrate the data creation in.", "labels": [], "entities": [{"text": "data creation", "start_pos": 37, "end_pos": 50, "type": "TASK", "confidence": 0.784361332654953}]}, {"text": "From an under-specified query \"Safari not working\", the annotator found 5 specific queries.", "labels": [], "entities": []}, {"text": "Two other annotators, were given these specific queries with the search engine's results from the corresponding under-specified query.", "labels": [], "entities": []}, {"text": "They were asked to choose the most relevant results for the specific queries.", "labels": [], "entities": []}, {"text": "The intersection of the choices of the annotators formed our 'gold standard' of relevant documents.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Comparison of the proposed approach  with respect to the Baseline that does not involve  interaction in terms of MRR.", "labels": [], "entities": [{"text": "MRR", "start_pos": 123, "end_pos": 126, "type": "TASK", "confidence": 0.4076181948184967}]}]}