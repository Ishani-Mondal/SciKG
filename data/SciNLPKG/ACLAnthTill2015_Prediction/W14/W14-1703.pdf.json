{"title": [{"text": "The AMU System in the CoNLL-2014 Shared Task: Grammatical Error Correction by Data-Intensive and Feature-Rich Statistical Machine Translation", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 110, "end_pos": 141, "type": "TASK", "confidence": 0.7014926274617513}]}], "abstractContent": [{"text": "Statistical machine translation toolkits like Moses have not been designed with grammatical error correction in mind.", "labels": [], "entities": [{"text": "Statistical machine translation", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.6591889361540476}]}, {"text": "In order to achieve competitive results in this area, it is not enough to simply add more data.", "labels": [], "entities": []}, {"text": "Optimization procedures need to be customized, task-specific features should be introduced.", "labels": [], "entities": []}, {"text": "Only then can the decoder take advantage of relevant data.", "labels": [], "entities": []}, {"text": "We demonstrate the validity of the above claims by combining web-scale language models and large-scale error-corrected texts with parameter tuning according to the task metric and correction-specific features.", "labels": [], "entities": []}, {"text": "Our system achieves a result of 35.0% F 0.5 on the blind CoNLL-2014 test set, ranking on third place.", "labels": [], "entities": [{"text": "CoNLL-2014 test set", "start_pos": 57, "end_pos": 76, "type": "DATASET", "confidence": 0.8893469174702963}]}, {"text": "A similar system , equipped with identical models but without tuned parameters and specialized features, stagnates at 25.4%.", "labels": [], "entities": []}], "introductionContent": [{"text": "There has been an increasing interest in using statistical machine translation (SMT) for the task of grammatical error correction.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 47, "end_pos": 84, "type": "TASK", "confidence": 0.7674989004929861}, {"text": "grammatical error correction", "start_pos": 101, "end_pos": 129, "type": "TASK", "confidence": 0.7063567439715067}]}, {"text": "Among the 16 teams that took part in the, four teams described approaches that fully or partially used SMT in their system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 103, "end_pos": 106, "type": "TASK", "confidence": 0.9886153936386108}]}, {"text": "While in the previous year the correction task was restricted to just five error types, the CoNLL-2014 Shared Task () now requires a participating system to correct all 28 error types present in NUCLE (.", "labels": [], "entities": [{"text": "NUCLE", "start_pos": 195, "end_pos": 200, "type": "DATASET", "confidence": 0.8875495791435242}]}, {"text": "Since the high number of error types has made it harder to target each error category with dedicated components, SMT with its ability to learn generic text transformations is now an even more appealing approach.", "labels": [], "entities": [{"text": "SMT", "start_pos": 113, "end_pos": 116, "type": "TASK", "confidence": 0.9917020201683044}]}, {"text": "With out-of-the-box machine translation toolkits like Moses () being freely available, the application of SMT to grammatical error correction seems straightforward.", "labels": [], "entities": [{"text": "SMT", "start_pos": 106, "end_pos": 109, "type": "TASK", "confidence": 0.9930203557014465}, {"text": "grammatical error correction", "start_pos": 113, "end_pos": 141, "type": "TASK", "confidence": 0.6485436260700226}]}, {"text": "However, Moses has not been designed as a grammar correction system, the standard features and optimization methods are geared towards translation performance measured by the metrics used in the SMT field.", "labels": [], "entities": [{"text": "grammar correction", "start_pos": 42, "end_pos": 60, "type": "TASK", "confidence": 0.7490209639072418}, {"text": "SMT field", "start_pos": 195, "end_pos": 204, "type": "TASK", "confidence": 0.9092962145805359}]}, {"text": "Training Moses on data that is relevant for grammatical error correction is a step in the right direction, but data alone is not enough.", "labels": [], "entities": [{"text": "grammatical error correction", "start_pos": 44, "end_pos": 72, "type": "TASK", "confidence": 0.7257073124249777}]}, {"text": "The decoder needs to be able to judge the data based on relevant features, parameter optimization needs to be performed according to relevant metrics.", "labels": [], "entities": []}, {"text": "This paper constitutes the description of the Adam Mickiewicz University (AMU) submission to the CoNLL-2014 Shared Task on Grammatical Error Correction.", "labels": [], "entities": [{"text": "CoNLL-2014 Shared Task on Grammatical Error Correction", "start_pos": 97, "end_pos": 151, "type": "TASK", "confidence": 0.5841947027615139}]}, {"text": "We explore the interaction of large-scale data, parameter optimization, and taskspecific features in a Moses-based system.", "labels": [], "entities": []}, {"text": "Related work is presented in the next section, the system setup is shortly described in Section 3.", "labels": [], "entities": []}, {"text": "Sections 4 to 7 contain our main contributions.", "labels": [], "entities": []}, {"text": "In Section 4, we describe our implementation of feature weights tuning according to the MaxMatch (M 2 ) metric by which is the evaluation metric of the current CoNLL-2014 Shared Task.", "labels": [], "entities": [{"text": "MaxMatch (M 2 ) metric", "start_pos": 88, "end_pos": 110, "type": "METRIC", "confidence": 0.5927575379610062}]}, {"text": "Sections 5 and 6 deal with the data-intensive aspects of our paper.", "labels": [], "entities": []}, {"text": "We start by extending the baseline system with a Wikipediabased language model and finish with a web-scale language model estimated from CommonCrawl data.", "labels": [], "entities": [{"text": "CommonCrawl data", "start_pos": 137, "end_pos": 153, "type": "DATASET", "confidence": 0.9727599918842316}]}, {"text": "Uncorrected/corrected data from the social language learner's platform Lang-8 is used to extend the translation models of our system.", "labels": [], "entities": [{"text": "Lang-8", "start_pos": 71, "end_pos": 77, "type": "DATASET", "confidence": 0.721570611000061}]}, {"text": "Task-specific dense and sparse features are introduced in Section 7.", "labels": [], "entities": []}, {"text": "These features are meant to raise the \"awareness\" of the decoder for grammatical error correction.", "labels": [], "entities": [{"text": "grammatical error correction", "start_pos": 69, "end_pos": 97, "type": "TASK", "confidence": 0.6827420790990194}]}, {"text": "In Section 8, we discuss the results of our submission and several intermediate systems on the blind CoNLL-2014 test set.", "labels": [], "entities": [{"text": "CoNLL-2014 test set", "start_pos": 101, "end_pos": 120, "type": "DATASET", "confidence": 0.9079207976659139}]}, {"text": "25 use SMT to correct countability errors fora set of 14 mass nouns that pose problems to Chinese ESL learners.", "labels": [], "entities": [{"text": "SMT", "start_pos": 7, "end_pos": 10, "type": "TASK", "confidence": 0.9641748070716858}]}, {"text": "For this very restricted task they achieve a results of 61.81% corrected mistakes.", "labels": [], "entities": [{"text": "corrected mistakes", "start_pos": 63, "end_pos": 81, "type": "METRIC", "confidence": 0.9188766479492188}]}, {"text": "This work mentions minimum error rate tuning according to BLEU.", "labels": [], "entities": [{"text": "error rate tuning", "start_pos": 27, "end_pos": 44, "type": "METRIC", "confidence": 0.887553334236145}, {"text": "BLEU", "start_pos": 58, "end_pos": 62, "type": "METRIC", "confidence": 0.9824735522270203}]}, {"text": "A Moses-based system is described by who correct grammatical errors of learners of Japanese.", "labels": [], "entities": []}, {"text": "This work is continued for English in.", "labels": [], "entities": []}, {"text": "The effect of learner corpus size on various types of grammatical errors is investigated.", "labels": [], "entities": []}, {"text": "The additional largescale data originates from the social learner's platform Lang-8.", "labels": [], "entities": [{"text": "Lang-8", "start_pos": 77, "end_pos": 83, "type": "DATASET", "confidence": 0.7866300344467163}]}, {"text": "Very interesting work is presented by.", "labels": [], "entities": []}, {"text": "A custom beamsearch decoder for grammatical error correction is introduced that incorporates discriminative classifiers for specific error categories such as articles and prepositions.", "labels": [], "entities": [{"text": "grammatical error correction", "start_pos": 32, "end_pos": 60, "type": "TASK", "confidence": 0.6024235089619955}]}, {"text": "The authors perform parameter tuning and find PRO to work better with M 2 1 than MERT 1 . The specialized decoder tuned with M 2 1 is compared to Moses that has been tuned with BLEU.", "labels": [], "entities": [{"text": "PRO", "start_pos": 46, "end_pos": 49, "type": "METRIC", "confidence": 0.8112027049064636}, {"text": "MERT", "start_pos": 81, "end_pos": 85, "type": "METRIC", "confidence": 0.6892130970954895}, {"text": "BLEU", "start_pos": 177, "end_pos": 181, "type": "METRIC", "confidence": 0.9797071218490601}]}, {"text": "As we show in Section 4.2, this cannot be a fair comparison.", "labels": [], "entities": []}, {"text": "The) saw a number of systems based entirely or partially on translation approaches.", "labels": [], "entities": []}, {"text": "apply Moses to all five error types of the shared task and extend the provided training data by adding other learner's corpora.", "labels": [], "entities": []}, {"text": "They also experiment with generating artificial errors.", "labels": [], "entities": []}, {"text": "Improvement over the baseline are small, but their approach to generate errors shows promise.", "labels": [], "entities": []}, {"text": "We successfully re-implement their baseline.", "labels": [], "entities": []}, {"text": "use Moses for two error classes, prepositions and determiners, for other classes they find classifier-based approaches and treelet language models to perform better.", "labels": [], "entities": []}, {"text": "None of the CoNLL-2013 SMT-based systems seems to use parameter tuning.", "labels": [], "entities": [{"text": "SMT-based", "start_pos": 23, "end_pos": 32, "type": "TASK", "confidence": 0.4122374653816223}, {"text": "parameter tuning", "start_pos": 54, "end_pos": 70, "type": "TASK", "confidence": 0.7001074999570847}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Tuning with BLEU and M 2", "labels": [], "entities": [{"text": "Tuning", "start_pos": 10, "end_pos": 16, "type": "TASK", "confidence": 0.9822182059288025}, {"text": "BLEU", "start_pos": 22, "end_pos": 26, "type": "METRIC", "confidence": 0.9986072182655334}]}, {"text": " Table 1. While  BLEU scores increase on both, 4\u00d72-CV and ST- 2013, the effect on M 2  0.5 is catastrophic 3 though  not surprising. The baseline is so weak that it in- troduces more errors than corrections, thus lower- ing the similarity of the output and the reference  below the level of the similarity of the input and  the reference. MERT learns parameter weights  that disable nearly all correction attempts.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 17, "end_pos": 21, "type": "METRIC", "confidence": 0.9990254640579224}, {"text": "ST- 2013", "start_pos": 58, "end_pos": 66, "type": "DATASET", "confidence": 0.761393666267395}, {"text": "MERT", "start_pos": 339, "end_pos": 343, "type": "METRIC", "confidence": 0.8121199011802673}]}, {"text": " Table 3: Results for increasing language models  size on both shared task scenarios", "labels": [], "entities": []}, {"text": " Table 4: Adding parallel data from Lang-8. Top  results are for tuned systems, bottom results for  untuned systems.", "labels": [], "entities": [{"text": "Lang-8", "start_pos": 36, "end_pos": 42, "type": "DATASET", "confidence": 0.9607490301132202}]}, {"text": " Table 5: 20 most frequent patterns extracted from NUCLE 3.0", "labels": [], "entities": [{"text": "NUCLE 3.0", "start_pos": 51, "end_pos": 60, "type": "DATASET", "confidence": 0.762921541929245}]}, {"text": " Table 6: Results of error selection", "labels": [], "entities": []}, {"text": " Table 8: Results for dense Levenshtein distance  (LD) and sparse pattern features (SF). Each com- ponent extends the previous system cumulatively.", "labels": [], "entities": [{"text": "Levenshtein distance  (LD)", "start_pos": 28, "end_pos": 54, "type": "METRIC", "confidence": 0.8826971888542176}, {"text": "sparse pattern features (SF)", "start_pos": 59, "end_pos": 87, "type": "METRIC", "confidence": 0.5925097316503525}]}, {"text": " Table 10: Shared Task results for submission with- out alternative answers. AMU is our result.", "labels": [], "entities": [{"text": "AMU", "start_pos": 77, "end_pos": 80, "type": "METRIC", "confidence": 0.7476431727409363}]}]}