{"title": [{"text": "Syllable and language model based features for detecting non-scorable tests in spoken language proficiency assessment applications", "labels": [], "entities": []}], "abstractContent": [{"text": "This work introduces new methods for detecting non-scorable tests, i.e., tests that cannot be accurately scored automatically, in educational applications of spoken language proficiency assessment.", "labels": [], "entities": [{"text": "spoken language proficiency assessment", "start_pos": 158, "end_pos": 196, "type": "TASK", "confidence": 0.7153655514121056}]}, {"text": "Those include cases of unreliable automatic speech recognition (ASR), often because of noisy, off-topic, foreign or unintelligible speech.", "labels": [], "entities": [{"text": "unreliable automatic speech recognition (ASR)", "start_pos": 23, "end_pos": 68, "type": "TASK", "confidence": 0.7641825761113848}]}, {"text": "We examine features that estimate signal-derived syllable information and compare it with ASR results in order to detect responses with problematic recognition.", "labels": [], "entities": [{"text": "ASR", "start_pos": 90, "end_pos": 93, "type": "TASK", "confidence": 0.48203426599502563}]}, {"text": "Further, we explore the usefulness of language model based features, both for language models that are highly constrained to the spoken task, and for task independent phoneme language models.", "labels": [], "entities": []}, {"text": "We validate our methods on a challenging dataset of young English language learners (ELLs) interacting with an automatic spoken assessment system.", "labels": [], "entities": []}, {"text": "Our proposed methods achieve comparable performance compared to existing non-scorable detection approaches, and lead to a 21% relative performance increase when combined with existing approaches.", "labels": [], "entities": []}], "introductionContent": [{"text": "Automatic language assessment systems are becoming a valuable tool in education, and provide efficient and consistent student assessment that can complement teacher assessment.", "labels": [], "entities": [{"text": "Automatic language assessment", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.5793580810228983}]}, {"text": "Recently, there has been a great increase of English Language Learners (ELLs) in US education).", "labels": [], "entities": []}, {"text": "ELLs are students coming from nonEnglish speaking backgrounds, and often require additional teacher attention.", "labels": [], "entities": []}, {"text": "Thus, assessing ELL student language proficiency is a key issue.", "labels": [], "entities": [{"text": "ELL student language proficiency", "start_pos": 16, "end_pos": 48, "type": "TASK", "confidence": 0.8217945247888565}]}, {"text": "Pearson has developed an automatic spoken assessment system for K-12 students and collected a large dataset of ELL students interacting with the system.", "labels": [], "entities": [{"text": "Pearson", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9630529284477234}]}, {"text": "This is a challenging dataset, containing accented speech and speech from young students.", "labels": [], "entities": []}, {"text": "Thus, fora small percentage of tests, it is technically challenging to compute an accurate automatic score, often because of background/line noise, off-topic or non-English responses or unintelligible speech.", "labels": [], "entities": [{"text": "accurate automatic score", "start_pos": 82, "end_pos": 106, "type": "METRIC", "confidence": 0.7997720241546631}]}, {"text": "Such tests as referred to as non-scorable.", "labels": [], "entities": []}, {"text": "Here, our goal is to propose new methods for better classifying non-scorable tests and describe a system for non-scorable detection.", "labels": [], "entities": [{"text": "non-scorable detection", "start_pos": 109, "end_pos": 131, "type": "TASK", "confidence": 0.7429496347904205}]}, {"text": "We propose two new sets of features: syllable based and language model (LM) based.", "labels": [], "entities": []}, {"text": "The intuition is to contrast information from different sources when processing a test, in order to detect inconsistencies in automatic speech recognition (ASR), that often appear in non-scorable tests.", "labels": [], "entities": [{"text": "automatic speech recognition (ASR)", "start_pos": 126, "end_pos": 160, "type": "TASK", "confidence": 0.8041505813598633}]}, {"text": "Syllable features measure similarity between different estimates of syllable locations, one extracted from ASR and the second from the raw signal.", "labels": [], "entities": []}, {"text": "LM features measure similarity between two ASR results, one using a standard item specific word LM, and the second using a item independent phoneme LM.", "labels": [], "entities": [{"text": "ASR", "start_pos": 43, "end_pos": 46, "type": "TASK", "confidence": 0.9865010380744934}]}, {"text": "Finally, an additional set of ASR confidence scores and log-likelihoods is computed using the proposed phoneme LM.", "labels": [], "entities": [{"text": "ASR confidence scores", "start_pos": 30, "end_pos": 51, "type": "METRIC", "confidence": 0.7817724744478861}]}, {"text": "Compared to existing work, our new methods achieve comparable performance, although they approach the problem from a different perspective.", "labels": [], "entities": []}, {"text": "Furthermore, our proposed features carry complementary information to existing ones, and lead to a 21% relative performance increase when combined with existing work.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our experiments are organized in 5-fold cross validation: we randomly split the 6000 tests into five sets, and each time we use three sets for training the random forest classifier, one set as a development for optimizing the number of trees, and one set for testing non-scorable classification performance.", "labels": [], "entities": []}, {"text": "Performance is computed after merging all test set results.", "labels": [], "entities": []}, {"text": "Because the percentage of nonscorable tests in our dataset is small (approx. 5%) and random forests are trained with a degree of randomness, different runs of an experiment can cause small variations in performance.", "labels": [], "entities": []}, {"text": "To minimize this effect, we repeat each 5-fold cross validation experiment 10 times, and report the average and standard deviation over the 10 runs.", "labels": [], "entities": [{"text": "standard deviation", "start_pos": 112, "end_pos": 130, "type": "METRIC", "confidence": 0.9347889423370361}]}, {"text": "Performance is estimated using the ROC curve of false acceptance rate (FAR) versus false rejec-tion rate (FRR) for the binary (scorable vs nonscorable) classification task.", "labels": [], "entities": [{"text": "ROC curve", "start_pos": 35, "end_pos": 44, "type": "METRIC", "confidence": 0.9602504968643188}, {"text": "false acceptance rate (FAR)", "start_pos": 48, "end_pos": 75, "type": "METRIC", "confidence": 0.8276506463686625}, {"text": "false rejec-tion rate (FRR)", "start_pos": 83, "end_pos": 110, "type": "METRIC", "confidence": 0.8227832068999609}, {"text": "binary (scorable vs nonscorable) classification task", "start_pos": 119, "end_pos": 171, "type": "TASK", "confidence": 0.733302116394043}]}, {"text": "Our goal is to minimize the area under the curve (AUC), e.g., achieve low values for both FAR and FRR.", "labels": [], "entities": [{"text": "AUC)", "start_pos": 50, "end_pos": 54, "type": "METRIC", "confidence": 0.9019921123981476}, {"text": "FAR", "start_pos": 90, "end_pos": 93, "type": "METRIC", "confidence": 0.9608103036880493}, {"text": "FRR", "start_pos": 98, "end_pos": 101, "type": "METRIC", "confidence": 0.9499034881591797}]}, {"text": "Our experiments were performed using the Python ScikitLearn toolbox).", "labels": [], "entities": []}, {"text": "presents the average AUC performance of non-scorable test detection over 10 experiment runs, using different feature sets and random forests.", "labels": [], "entities": [{"text": "AUC", "start_pos": 21, "end_pos": 24, "type": "METRIC", "confidence": 0.6826432943344116}]}, {"text": "'Base' denotes the set of standard ASR-based and signal-based features described in Section 7.", "labels": [], "entities": [{"text": "ASR-based", "start_pos": 35, "end_pos": 44, "type": "TASK", "confidence": 0.9512025713920593}]}, {"text": "Syllable based and LM based denote the similarity features introduced in Sections 4 and 5 respectively.", "labels": [], "entities": [{"text": "Syllable", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9597761631011963}]}, {"text": "Finally, 'confidence' denotes the confidence and log-likelihood features derived from the standard and the proposed phoneme LM, as described in Section 6.", "labels": [], "entities": [{"text": "confidence", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9711408615112305}, {"text": "confidence", "start_pos": 34, "end_pos": 44, "type": "METRIC", "confidence": 0.954021692276001}]}, {"text": "According to our results, 'base' features are the best performing.", "labels": [], "entities": []}, {"text": "However, it is encouraging that our proposed comparisonbased syllable and LM approaches, that approach the problem from a different perspective and only use similarity features, still achieve comparable performance.", "labels": [], "entities": []}, {"text": "also presents the AUC performance after concatenating the feature vectors of different feature sets, under 'Feature Combination'.", "labels": [], "entities": [{"text": "AUC", "start_pos": 18, "end_pos": 21, "type": "DATASET", "confidence": 0.43122872710227966}]}, {"text": "We notice that adding separately each of our proposed syllable based, LM based and confidence features to the base features improves performance by decreasing AUC.", "labels": [], "entities": [{"text": "AUC", "start_pos": 159, "end_pos": 162, "type": "METRIC", "confidence": 0.99009108543396}]}, {"text": "This further indicates that the pro- posed features carry useful information, which is complementary to the 'base' feature set.", "labels": [], "entities": []}, {"text": "Combining all features together leads to a relatively small performance increase, possibly because the large number of features may cause overfitting.", "labels": [], "entities": []}], "tableCaptions": []}