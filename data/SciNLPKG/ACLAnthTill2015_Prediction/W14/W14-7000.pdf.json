{"title": [{"text": "Overview of the 1st Workshop on Asian Translation", "labels": [], "entities": [{"text": "1st Workshop on Asian Translation", "start_pos": 16, "end_pos": 49, "type": "TASK", "confidence": 0.5257952809333801}]}], "abstractContent": [{"text": "This paper presents the results of the 1st workshop on Asian translation (WMT2014) shared tasks, which included J\u2194E translation subtasks and J\u2194C translation subtasks.", "labels": [], "entities": [{"text": "Asian translation (WMT2014) shared tasks", "start_pos": 55, "end_pos": 95, "type": "TASK", "confidence": 0.6870268114975521}]}, {"text": "As the first year of WAT, 12 institutions participated to the shared tasks.", "labels": [], "entities": [{"text": "WAT", "start_pos": 21, "end_pos": 24, "type": "TASK", "confidence": 0.9769366383552551}]}, {"text": "More than 300 translation results have been submitted to the automatic evaluation server, and selected submissions were manually evaluated.", "labels": [], "entities": []}], "introductionContent": [{"text": "The Workshop on Asian Translation (WAT) is anew open evaluation campaign focusing on Asian languages.", "labels": [], "entities": [{"text": "Asian Translation (WAT)", "start_pos": 16, "end_pos": 39, "type": "TASK", "confidence": 0.8391996622085571}]}, {"text": "We would like to invite abroad range of participants and conduct various forms of machine translation experiments and evaluation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 82, "end_pos": 101, "type": "TASK", "confidence": 0.7805513143539429}]}, {"text": "Collecting and sharing our knowledge will allow us to understand the essence of machine translation and the problems to be solved.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 80, "end_pos": 99, "type": "TASK", "confidence": 0.7795912027359009}]}, {"text": "We are working toward the practical use of machine translation among all Asian countries.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 43, "end_pos": 62, "type": "TASK", "confidence": 0.7798082828521729}]}, {"text": "For the 1st WAT, we chose scientific papers as the targeted domain, and selected the languages Japanese, Chinese and English.", "labels": [], "entities": [{"text": "WAT", "start_pos": 12, "end_pos": 15, "type": "TASK", "confidence": 0.8052129745483398}]}, {"text": "What makes WAT unique: \u2022 Open innovation platform The test data is fixed and open, so you can repeat evaluations on the same data and confirm changes in translation accuracy overtime.", "labels": [], "entities": [{"text": "WAT", "start_pos": 11, "end_pos": 14, "type": "TASK", "confidence": 0.9490516781806946}, {"text": "accuracy", "start_pos": 165, "end_pos": 173, "type": "METRIC", "confidence": 0.8784084916114807}]}, {"text": "WAT has no deadline for the automatic translation quality evaluation (continuous evaluation), so you can submit translation results at anytime.", "labels": [], "entities": [{"text": "WAT", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.5274266004562378}]}, {"text": "\u2022  scientific papers as a domain and JapaneseChinese as a language pair.", "labels": [], "entities": [{"text": "JapaneseChinese", "start_pos": 37, "end_pos": 52, "type": "DATASET", "confidence": 0.908991277217865}]}, {"text": "In the future, we will add more Asian languages, such as Korean, Vietnamese, Indonesian, Thai, Myanmar and soon.", "labels": [], "entities": []}, {"text": "\u2022 Evaluation method Evaluation will be done by both automatic and human evaluation.", "labels": [], "entities": []}, {"text": "For human evaluation, WAT will use crowdsourcing, which is low cost and allows multiple evaluations.", "labels": [], "entities": [{"text": "WAT", "start_pos": 22, "end_pos": 25, "type": "TASK", "confidence": 0.9834684133529663}]}], "datasetContent": [{"text": "WAT uses Asian Scientific Paper Excerpt Corpus (ASPEC) 1 as the dataset.", "labels": [], "entities": [{"text": "WAT", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.48872825503349304}, {"text": "Asian Scientific Paper Excerpt Corpus (ASPEC) 1", "start_pos": 9, "end_pos": 56, "type": "DATASET", "confidence": 0.8563110695944892}]}, {"text": "ASPEC is constructed by the Japan Science and Technology Agency (JST) in collaboration with the National Institute of Information and Communications Technology (NICT).", "labels": [], "entities": [{"text": "ASPEC", "start_pos": 0, "end_pos": 5, "type": "TASK", "confidence": 0.8737988471984863}]}, {"text": "It consists of a Japanese-English scientific paper abstract corpus (ASPEC-JE), which is used for J\u2194E subtasks, and a Japanese-Chinese scientific paper excerpt corpus (ASPEC-JC), which is used for J\u2194C subtasks.", "labels": [], "entities": []}, {"text": "The statistics of each corpus are described in Table1.", "labels": [], "entities": []}, {"text": "We calculated automatic evaluation scores of the translation results applying two popular metrics: BLEU () and RIBES (.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 99, "end_pos": 103, "type": "METRIC", "confidence": 0.9988881945610046}, {"text": "RIBES", "start_pos": 111, "end_pos": 116, "type": "METRIC", "confidence": 0.9913989305496216}]}, {"text": "BLEU scores were calculated with multi-bleu.perl distributed with the Moses toolkit (.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9698890447616577}]}, {"text": "RIBES scores were calculated with RIBES.py version 1.02.4 . All scores of each task were calculated using one reference.", "labels": [], "entities": [{"text": "RIBES.py version 1.02.4", "start_pos": 34, "end_pos": 57, "type": "DATASET", "confidence": 0.8734269340833029}]}, {"text": "Before the calculation of the automatic evaluation scores, the translation results have been tokenized with word segmentation tools on each language.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 108, "end_pos": 125, "type": "TASK", "confidence": 0.7018879652023315}]}, {"text": "For Japanese segmentation we use three different tools, which are Juman version 7.0 (, KyTea 0.4.6) with Full SVM model and MeCab 0.996 () with IPA dictionary 2.7.0 8 . For Chinese segmentation we use two different tools, which are KyTea 0.4.6 with Full SVM Model in MSR model and Stanford Word Segmenter version 2014-06-16 with Chinese Penn Treebank (CTB) and Peking University (PKU) model 9 ().", "labels": [], "entities": [{"text": "Japanese segmentation", "start_pos": 4, "end_pos": 25, "type": "TASK", "confidence": 0.5577159523963928}, {"text": "Chinese segmentation", "start_pos": 173, "end_pos": 193, "type": "TASK", "confidence": 0.7336215078830719}, {"text": "Chinese Penn Treebank (CTB)", "start_pos": 329, "end_pos": 356, "type": "DATASET", "confidence": 0.8228392402331034}]}, {"text": "For English segmentation we use tokenizer.perl 10 in the Moses toolkit.", "labels": [], "entities": [{"text": "English segmentation", "start_pos": 4, "end_pos": 24, "type": "TASK", "confidence": 0.7257524132728577}]}, {"text": "The detailed procedures for the automatic evaluation are shown at WAT2014 evaluation web page .  The participants submit the translation results via an automatic evaluation system deployed at WAT2014 web page, which give them automatic evaluation scores of the results they upload.", "labels": [], "entities": [{"text": "WAT2014 evaluation web page", "start_pos": 66, "end_pos": 93, "type": "DATASET", "confidence": 0.9329369217157364}, {"text": "WAT2014 web page", "start_pos": 192, "end_pos": 208, "type": "DATASET", "confidence": 0.9726300636927286}]}, {"text": "shows the submitting interface for participants.", "labels": [], "entities": []}, {"text": "The system requires the participants to provide the following information when they upload the translation results: \u2022 Method (SMT, RBMT, SMT and RBMT,  For the human evaluation, we randomly chose documents from the Test set of ASPEC data, in total 400 sentence pairs for JE and JC.", "labels": [], "entities": [{"text": "RBMT", "start_pos": 145, "end_pos": 149, "type": "METRIC", "confidence": 0.8428317308425903}, {"text": "Test set of ASPEC data", "start_pos": 215, "end_pos": 237, "type": "DATASET", "confidence": 0.6977631866931915}]}, {"text": "We excluded the documents which contains a sentence with longer than 100 Japanese characters.", "labels": [], "entities": []}, {"text": "Each submission is compared with the baseline translation (Phrase-based SMT, described in Section 3) and given HUMAN score.", "labels": [], "entities": [{"text": "Phrase-based SMT", "start_pos": 59, "end_pos": 75, "type": "TASK", "confidence": 0.5545891523361206}, {"text": "HUMAN score", "start_pos": 111, "end_pos": 122, "type": "METRIC", "confidence": 0.9806952774524689}]}, {"text": "We conducted pairwise evaluation of each test sentence of the 400 sentences.", "labels": [], "entities": []}, {"text": "The input sentence and two translations (the baseline and a submission) are shown to the workers, and the workers are asked to judge which translation is better than the other, or they are of the same quality.", "labels": [], "entities": []}, {"text": "The order of the two translations are at random.", "labels": [], "entities": []}, {"text": "shows the illustration of the evaluation.: The combinations of human judgements and the final decision of each sentence pairs from system A and B.", "labels": [], "entities": []}, {"text": "One big benefit of using crowdsourcing is that we can reduce the cost of evaluations.", "labels": [], "entities": []}, {"text": "In WAT2014, one judgement costs 5 JPY.", "labels": [], "entities": [{"text": "WAT2014", "start_pos": 3, "end_pos": 10, "type": "TASK", "confidence": 0.586524248123169}]}, {"text": "The evaluation of a submission requires 3 (judgements) \u00d7 400 (sentence pairs) = 1,200 judgements and it costs 5 \u00d7 1,200 = 6,000 JPY.", "labels": [], "entities": [{"text": "JPY", "start_pos": 128, "end_pos": 131, "type": "DATASET", "confidence": 0.903081476688385}]}, {"text": "The time for the evaluation differs depending on the translation direction.", "labels": [], "entities": []}, {"text": "On the average, one evaluation finished in a couple of days.", "labels": [], "entities": []}, {"text": "shows the list of participants to WAT2014.", "labels": [], "entities": [{"text": "WAT2014", "start_pos": 34, "end_pos": 41, "type": "DATASET", "confidence": 0.6731696724891663}]}, {"text": "There are not only the Japanese organizations, but some organizations came from outside Japan.", "labels": [], "entities": []}, {"text": "12 teams submitted one or more translation results to the automatic evaluation server, and 11 teams submitted one or more translation results to the human evaluation.", "labels": [], "entities": []}, {"text": "In this section, the evaluation results of WAT2014 are reported from several perspectives.", "labels": [], "entities": [{"text": "WAT2014", "start_pos": 43, "end_pos": 50, "type": "TASK", "confidence": 0.7756277918815613}]}, {"text": "Parts of the results of both automatic and human evaluations are also accessible at WAT2014 website 17 . shows the official automatic evaluation results of the representative submissions and baseline systems.", "labels": [], "entities": [{"text": "WAT2014 website", "start_pos": 84, "end_pos": 99, "type": "DATASET", "confidence": 0.8671725392341614}]}, {"text": "The automatic evaluation results of all the submissions are shown in Section Appendix A. shows the official human evaluation results.", "labels": [], "entities": [{"text": "Appendix", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.8947005867958069}]}, {"text": "The error bars in the figures show the 95% confidence interval (see Section 5.2.4).", "labels": [], "entities": [{"text": "confidence interval", "start_pos": 43, "end_pos": 62, "type": "METRIC", "confidence": 0.8391494750976562}]}, {"text": "Note that overlapping the error bars between two submissions does not necessarily mean that there is no significant difference.", "labels": [], "entities": []}, {"text": "If an error bar crosses the x-axis (HUMAN score = 0), it means that there is no significant difference between the submission and the baseline (SMT Phrase).", "labels": [], "entities": [{"text": "HUMAN score", "start_pos": 36, "end_pos": 47, "type": "METRIC", "confidence": 0.9578465521335602}]}], "tableCaptions": [{"text": " Table 1: Statistics of ASPEC.", "labels": [], "entities": [{"text": "ASPEC", "start_pos": 24, "end_pos": 29, "type": "TASK", "confidence": 0.7192220687866211}]}, {"text": " Table 10: The changes of correlations (R 2 ) before  and after removing RBMT and online systems.", "labels": [], "entities": [{"text": "correlations (R 2 )", "start_pos": 26, "end_pos": 45, "type": "METRIC", "confidence": 0.7778033137321472}]}, {"text": " Table 5: Statistical significance testing of JE results.", "labels": [], "entities": [{"text": "JE", "start_pos": 46, "end_pos": 48, "type": "TASK", "confidence": 0.7981089353561401}]}, {"text": " Table 9: The Fleiss' kappa values of human evaluation results.", "labels": [], "entities": []}]}