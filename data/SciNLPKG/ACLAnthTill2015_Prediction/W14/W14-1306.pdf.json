{"title": [{"text": "Aspect Term Extraction for Sentiment Analysis: New Datasets, New Evaluation Measures and an Improved Unsupervised Method", "labels": [], "entities": [{"text": "Aspect Term Extraction", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.81409885485967}, {"text": "Sentiment Analysis", "start_pos": 27, "end_pos": 45, "type": "TASK", "confidence": 0.9558488428592682}]}], "abstractContent": [{"text": "Given a set of texts discussing a particular entity (e.g., customer reviews of a smart-phone), aspect based sentiment analysis (ABSA) identifies prominent aspects of the entity (e.g., battery, screen) and an average sentiment score per aspect.", "labels": [], "entities": [{"text": "aspect based sentiment analysis (ABSA", "start_pos": 95, "end_pos": 132, "type": "TASK", "confidence": 0.5767275094985962}]}, {"text": "We focus on aspect term extraction (ATE), one of the core processing stages of ABSA that extracts terms naming aspects.", "labels": [], "entities": [{"text": "aspect term extraction (ATE)", "start_pos": 12, "end_pos": 40, "type": "TASK", "confidence": 0.7059023678302765}, {"text": "ABSA", "start_pos": 79, "end_pos": 83, "type": "TASK", "confidence": 0.942915678024292}]}, {"text": "We make publicly available three new ATE datasets, arguing that they are better than previously available ones.", "labels": [], "entities": [{"text": "ATE datasets", "start_pos": 37, "end_pos": 49, "type": "DATASET", "confidence": 0.8505919575691223}]}, {"text": "We also introduce new evaluation measures for ATE, again arguing that they are better than previously used ones.", "labels": [], "entities": [{"text": "ATE", "start_pos": 46, "end_pos": 49, "type": "TASK", "confidence": 0.7929022908210754}]}, {"text": "Finally, we show how a popular unsupervised ATE method can be improved by using continuous space vector representations of words and phrases.", "labels": [], "entities": [{"text": "ATE", "start_pos": 44, "end_pos": 47, "type": "TASK", "confidence": 0.8324866890907288}]}], "introductionContent": [{"text": "Before buying a product or service, consumers often search the Web for expert reviews, but increasingly also for opinions of other consumers, expressed in blogs, social networks etc.", "labels": [], "entities": []}, {"text": "Many useful opinions are expressed in text-only form (e.g., in tweets).", "labels": [], "entities": []}, {"text": "It is then desirable to extract aspects (e.g., screen, battery) from the texts that discuss a particular entity (e.g., a smartphone), i.e., figure out what is being discussed, and also estimate aspect sentiment scores, i.e., how positive or negative the (usually average) sentiment for each aspect is.", "labels": [], "entities": []}, {"text": "These two goals are jointly known as Aspect Based Sentiment Analysis (ABSA) (.", "labels": [], "entities": [{"text": "Aspect Based Sentiment Analysis (ABSA)", "start_pos": 37, "end_pos": 75, "type": "TASK", "confidence": 0.6803057534354073}]}, {"text": "In this paper, we consider free text customer reviews of products and services; ABSA, however, is also applicable to texts about other kinds of entities (e.g., politicians, organizations).", "labels": [], "entities": []}, {"text": "We assume that a search engine retrieves customer reviews about a particular target entity (product or: Automatically extracted prominent aspects (shown as clusters of aspect terms) and average aspect sentiment scores of a target entity.", "labels": [], "entities": []}, {"text": "service), that multiple reviews written by different customers are retrieved for each target entity, and that the ultimate goal is to produce a table like the one of, which presents the most prominent aspects and average aspect sentiment scores of the target entity.", "labels": [], "entities": []}, {"text": "Most ABSA systems in effect perform all or some of the following three subtasks: Aspect term extraction: Starting from texts about a particular target entity or entities of the same type as the target entity (e.g., laptop reviews), this stage extracts and possibly ranks by importance aspect terms, i.e., terms naming aspects (e.g., 'battery', 'screen') of the target entity, including multi-word terms (e.g., 'hard disk') (.", "labels": [], "entities": [{"text": "Aspect term extraction", "start_pos": 81, "end_pos": 103, "type": "TASK", "confidence": 0.7001850605010986}]}, {"text": "At the end of this stage, each aspect term is taken to be the name of a different aspect, but aspect terms may subsequently be clustered during aspect aggregation; see below.", "labels": [], "entities": []}, {"text": "Aspect term sentiment estimation: This stage estimates the polarity and possibly also the intensity (e.g., strongly negative, mildly positive) of the opinions for each aspect term of the target entity, usually averaged over several texts.", "labels": [], "entities": [{"text": "Aspect term sentiment estimation", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.6695779040455818}]}, {"text": "Classifying texts by sentiment polarity is a popular research topic ().", "labels": [], "entities": [{"text": "Classifying texts by sentiment polarity", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.7799748063087464}]}, {"text": "The goal, however, in this 44 ABSA subtask is to estimate the (usually average) polarity and intensity of the opinions about particular aspect terms of the target entity.", "labels": [], "entities": []}, {"text": "Aspect aggregation: Some systems group aspect terms that are synonyms or near-synonyms (e.g., 'price', 'cost') or, more generally, cluster aspect terms to obtain aspects of a coarser granularity (e.g., 'chicken', 'steak', and 'fish' may all be replaced by 'food') (.", "labels": [], "entities": []}, {"text": "A polarity (and intensity) score can then be computed for each coarser aspect (e.g., 'food') by combining (e.g., averaging) the polarity scores of the aspect terms that belong in the coarser aspect.", "labels": [], "entities": [{"text": "intensity) score", "start_pos": 16, "end_pos": 32, "type": "METRIC", "confidence": 0.93592369556427}]}, {"text": "In this paper, we focus on aspect term extraction (ATE).", "labels": [], "entities": [{"text": "aspect term extraction (ATE)", "start_pos": 27, "end_pos": 55, "type": "TASK", "confidence": 0.6314247200886408}]}, {"text": "Firstly, we argue (Section 2) that previous ATE datasets are not entirely satisfactory, mostly because they contain reviews from a particular domain only (e.g., consumer electronics), or they contain reviews for very few target entities, or they do not contain annotations for aspect terms.", "labels": [], "entities": [{"text": "ATE datasets", "start_pos": 44, "end_pos": 56, "type": "DATASET", "confidence": 0.7169417589902878}]}, {"text": "We constructed and make publicly available three new ATE datasets with customer reviews fora much larger number of target entities from three domains (restaurants, laptops, hotels), with gold annotations of all the aspect term occurrences; we also measured interannotator agreement, unlike previous datasets.", "labels": [], "entities": [{"text": "ATE datasets", "start_pos": 53, "end_pos": 65, "type": "DATASET", "confidence": 0.7969812154769897}]}, {"text": "Secondly, we argue (Section 3) that commonly used evaluation measures are also not entirely satisfactory.", "labels": [], "entities": []}, {"text": "For example, when precision, recall, and F -measure are computed over distinct aspect terms (types), equal weight is assigned to more and less frequent aspect terms, whereas frequently discussed aspect terms are more important; and when precision, recall, and F -measure are computed over aspect term occurrences (tokens), methods that identify very few, but very frequent aspect terms may appear to perform much better than they actually do.", "labels": [], "entities": [{"text": "precision", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.9982276558876038}, {"text": "recall", "start_pos": 29, "end_pos": 35, "type": "METRIC", "confidence": 0.9681847095489502}, {"text": "F -measure", "start_pos": 41, "end_pos": 51, "type": "METRIC", "confidence": 0.9905039270718893}, {"text": "precision", "start_pos": 237, "end_pos": 246, "type": "METRIC", "confidence": 0.996990442276001}, {"text": "recall", "start_pos": 248, "end_pos": 254, "type": "METRIC", "confidence": 0.9630014896392822}, {"text": "F -measure", "start_pos": 260, "end_pos": 270, "type": "METRIC", "confidence": 0.9736150105794271}]}, {"text": "We propose weighted variants of precision and recall, which take into account the rankings of the distinct aspect terms that are obtained when the distinct aspect terms are ordered by their true and predicted frequencies.", "labels": [], "entities": [{"text": "precision", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.9992684721946716}, {"text": "recall", "start_pos": 46, "end_pos": 52, "type": "METRIC", "confidence": 0.9987133741378784}]}, {"text": "We also compute the average weighted precision over several weighted recall levels.", "labels": [], "entities": [{"text": "precision", "start_pos": 37, "end_pos": 46, "type": "METRIC", "confidence": 0.9581838250160217}, {"text": "recall", "start_pos": 69, "end_pos": 75, "type": "METRIC", "confidence": 0.9950073957443237}]}, {"text": "Thirdly, we show (Section 4) how the popular unsupervised ATE method of, can be extended with continuous space word vectors ().", "labels": [], "entities": []}, {"text": "Using our datasets and evaluation measures, we demonstrate (Section 5) that the extended method performs better.", "labels": [], "entities": []}], "datasetContent": [{"text": "We first discuss previous datasets that have been used for ATE, and we then introduce our own.", "labels": [], "entities": [{"text": "ATE", "start_pos": 59, "end_pos": 62, "type": "TASK", "confidence": 0.9157844185829163}]}, {"text": "So far, ATE methods have been evaluated mainly on customer reviews, often from the consumer electronics domain (.", "labels": [], "entities": [{"text": "ATE", "start_pos": 8, "end_pos": 11, "type": "TASK", "confidence": 0.8820628523826599}]}, {"text": "The most commonly used dataset is that of Hu and, which contains reviews of only five particular electronic products (e.g., Nikon Coolpix 4300).", "labels": [], "entities": [{"text": "Hu and", "start_pos": 42, "end_pos": 48, "type": "DATASET", "confidence": 0.7660864889621735}]}, {"text": "Each sentence is annotated with aspect terms, but inter-annotator agreement has not been reported.", "labels": [], "entities": []}, {"text": "All the sentences appear to have been selected to express clear positive or negative opinions.", "labels": [], "entities": []}, {"text": "There are no sentences expressing conflicting opinions about aspect terms (e.g., \"The screen is clear but small\"), nor are there any sentences that do not express opinions about their aspect terms (e.g., \"It has a 4.8-inch screen\").", "labels": [], "entities": []}, {"text": "Hence, the dataset is not entirely representative of product reviews.", "labels": [], "entities": []}, {"text": "By contrast, our datasets, discussed below, contain reviews from three domains, including sentences that express conflicting or no opinions about aspect terms, they concern many more target entities (not just five), and we have also measured inter-annotator agreement.", "labels": [], "entities": []}, {"text": "The dataset of, on which one of our datasets is based, is also popular.", "labels": [], "entities": []}, {"text": "In the original dataset, each sentence is tagged with coarse aspects ('food', 'service', 'price', 'ambience', 'anecdotes', or 'miscellaneous').", "labels": [], "entities": []}, {"text": "For example, \"The restaurant was expensive, but the menu was great\" would be tagged with the coarse aspects 'price' and 'food'.", "labels": [], "entities": [{"text": "price", "start_pos": 109, "end_pos": 114, "type": "METRIC", "confidence": 0.96390300989151}]}, {"text": "The coarse aspects, however, are not necessarily terms occurring in the sentence, and it is unclear how they were obtained.", "labels": [], "entities": []}, {"text": "By contrast, we asked human annotators to mark the explicit aspect terms of each sentence, leaving the task of clustering the terms to produce coarser aspects for an aspect aggregation stage.", "labels": [], "entities": []}, {"text": "The 'Concept-Level Sentiment Analysis Challenge' of ESWC 2014 uses the dataset of, which contains customer reviews of DVDs, books, kitchen appliances, and electronic products, with an overall sentiment score for each review.", "labels": [], "entities": [{"text": "Concept-Level Sentiment Analysis Challenge' of ESWC 2014", "start_pos": 5, "end_pos": 61, "type": "TASK", "confidence": 0.6680664531886578}]}, {"text": "One of the challenge's tasks requires systems to extract the aspects of each sentence and a sentiment score (positive or negative) per aspect.", "labels": [], "entities": []}, {"text": "The aspects are intended to be concepts from ontologies, not simply aspect terms.", "labels": [], "entities": []}, {"text": "The ontologies to be used, however, are not fully specified and no training dataset with sentences and gold aspects is currently available.", "labels": [], "entities": []}, {"text": "Overall, the previous datasets are not entirely satisfactory, because they contain reviews from a particular domain only, or reviews for very few target entities, or their sentences are not entirely representative of customer reviews, or they do not contain annotations for aspect terms, or no inter-annotator agreement has been reported.", "labels": [], "entities": []}, {"text": "To address these issues, we provide three new ATE datasets, which contain customer reviews of restaurants, hotels, and laptops, respectively.", "labels": [], "entities": [{"text": "ATE datasets", "start_pos": 46, "end_pos": 58, "type": "DATASET", "confidence": 0.7488389015197754}]}, {"text": "3  The restaurants dataset contains 3,710 English sentences from the reviews of.", "labels": [], "entities": [{"text": "restaurants dataset", "start_pos": 7, "end_pos": 26, "type": "DATASET", "confidence": 0.9065114259719849}]}, {"text": "We asked human annotators to tag the aspect terms of each sentence.", "labels": [], "entities": []}, {"text": "In \"The dessert was divine\", for example, the annotators would tag the aspect term 'dessert'.", "labels": [], "entities": []}, {"text": "Ina sentence like \"The restaurant was expensive, but the menu was great\", the annotators were instructed to tag only the explicitly mentioned aspect term 'menu'.", "labels": [], "entities": []}, {"text": "The sentence also refers to the prices, and a possibility would be to add 'price' as an implicit aspect term, but we do not consider implicit aspect terms in this paper.", "labels": [], "entities": []}, {"text": "We used nine annotators for the restaurant reviews.", "labels": [], "entities": []}, {"text": "Each sentence was processed by a single annotator, and each annotator processed approximately the same number of sentences.", "labels": [], "entities": []}, {"text": "Among the 3,710 restaurant sentences, 1,248 contain exactly one aspect term, 872 more than one, and 1,590 no aspect terms.", "labels": [], "entities": []}, {"text": "There are 593 distinct multi-word aspect terms and 452 distinct single-word aspect terms.", "labels": [], "entities": []}, {"text": "Removing aspect terms that occur only once leaves 67 distinct multi-word and 195 distinct single-word aspect terms.", "labels": [], "entities": []}, {"text": "The hotels dataset contains 3,600 English sen-2 See http://2014.eswc-conferences.org/.", "labels": [], "entities": [{"text": "hotels dataset", "start_pos": 4, "end_pos": 18, "type": "DATASET", "confidence": 0.8483148217201233}]}, {"text": "3 Our datasets are available upon request.", "labels": [], "entities": []}, {"text": "The datasets of the ABSA task of SemEval 2014 (http://alt.qcri. org/semeval2014/task4/) are based on our datasets.", "labels": [], "entities": [{"text": "ABSA task of SemEval 2014", "start_pos": 20, "end_pos": 45, "type": "TASK", "confidence": 0.6389252126216889}]}, {"text": "The original dataset of Ganu et al. contains 3,400 sentences, but some of the sentences had not been properly split.", "labels": [], "entities": []}, {"text": "tences from online customer reviews of 30 hotels.", "labels": [], "entities": []}, {"text": "Among the 3,600 hotel sentences, 1,326 contain exactly one aspect term, 652 more than one, and 1,622 none.", "labels": [], "entities": []}, {"text": "There are 199 distinct multi-word aspect terms and 262 distinct single-word aspect terms, of which 24 and 120, respectively, were tagged more than once.", "labels": [], "entities": []}, {"text": "The laptops dataset contains 3,085 English sentences of 394 online customer reviews.", "labels": [], "entities": [{"text": "laptops dataset", "start_pos": 4, "end_pos": 19, "type": "DATASET", "confidence": 0.7674376964569092}]}, {"text": "A single annotator (one of the authors) was used.", "labels": [], "entities": []}, {"text": "Among the 3,085 laptop sentences, 909 contain exactly one aspect term, 416 more than one, and 1,760 none.", "labels": [], "entities": []}, {"text": "There are 350 distinct multi-word and 289 distinct single-word aspect terms, of which 67 and 137, respectively, were tagged more than once.", "labels": [], "entities": []}, {"text": "To measure inter-annotator agreement, we used a sample of 75 restaurant, 75 laptop, and 100 hotel sentences.", "labels": [], "entities": []}, {"text": "Each sentence was processed by two (for restaurants and laptops) or three (for hotels) annotators, other than the annotators used previously.", "labels": [], "entities": []}, {"text": "For each sentence s i , the inter-annotator agreement was measured as the Dice coefficient , where A i , Bi are the sets of aspect term occurrences tagged by the two annotators, respectively, and |S| denotes the cardinality of a set S; for hotels, we use the mean pairwise Di of the three annotators.", "labels": [], "entities": []}, {"text": "The overall interannotator agreement D was taken to be the average Di of the sentences of each sample.", "labels": [], "entities": [{"text": "interannotator agreement D", "start_pos": 12, "end_pos": 38, "type": "METRIC", "confidence": 0.7642192641894022}, {"text": "Di", "start_pos": 67, "end_pos": 69, "type": "METRIC", "confidence": 0.9793947339057922}]}, {"text": "We, thus, obtained D = 0.72, 0.70, 0.69, for restaurants, hotels, and laptops, respectively, which indicate reasonably high inter-annotator agreement.", "labels": [], "entities": [{"text": "D", "start_pos": 19, "end_pos": 20, "type": "METRIC", "confidence": 0.9985454082489014}]}, {"text": "We now discuss previous ATE evaluation measures, also introducing our own.", "labels": [], "entities": [{"text": "ATE evaluation", "start_pos": 24, "end_pos": 38, "type": "TASK", "confidence": 0.583079606294632}]}, {"text": "H&L performs much better than FREQ in all three domains, and our additional pruning (W2V) improves H&L in all three domains.", "labels": [], "entities": [{"text": "FREQ", "start_pos": 30, "end_pos": 34, "type": "METRIC", "confidence": 0.9663403034210205}]}, {"text": "By contrast FREQ benefits from W2V only in the restaurant reviews (but to a smaller degree than H&L), it benefits only marginally in the hotel reviews, and in the laptop reviews FREQ+W2V performs worse than FREQ.", "labels": [], "entities": [{"text": "FREQ", "start_pos": 12, "end_pos": 16, "type": "METRIC", "confidence": 0.9946680068969727}, {"text": "FREQ", "start_pos": 207, "end_pos": 211, "type": "METRIC", "confidence": 0.9894228577613831}]}, {"text": "A possible explanation is that the list of candidate (distinct) aspect terms that FREQ produces already misses many aspect terms in the hotel and laptop datasets; hence, W2V, which can only prune aspect terms, cannot improve the results much, and in the case of laptops W2V has a negative effect, because it prunes several correct candidate aspect terms.", "labels": [], "entities": []}, {"text": "All differences between AWP scores on the same dataset are statistically significant; we use stratified approximate randomization, which indicates p \u2264 0.01 in all cases.", "labels": [], "entities": [{"text": "AWP", "start_pos": 24, "end_pos": 27, "type": "METRIC", "confidence": 0.7795737385749817}]}, {"text": "13 shows the weighted precision and weighted recall curves of the four methods.", "labels": [], "entities": [{"text": "precision", "start_pos": 22, "end_pos": 31, "type": "METRIC", "confidence": 0.9689909815788269}, {"text": "recall", "start_pos": 45, "end_pos": 51, "type": "METRIC", "confidence": 0.9723451137542725}]}, {"text": "In the restaurants dataset, our pruning improves See http://masanjin.net/sigtest.pdf.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Average weighted precision results (%).", "labels": [], "entities": [{"text": "precision", "start_pos": 27, "end_pos": 36, "type": "METRIC", "confidence": 0.7542675137519836}]}]}