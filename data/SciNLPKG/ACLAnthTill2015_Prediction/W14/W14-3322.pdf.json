{"title": [{"text": "CUNI in WMT14: Chimera Still Awaits Bellerophon", "labels": [], "entities": [{"text": "CUNI in WMT14", "start_pos": 0, "end_pos": 13, "type": "DATASET", "confidence": 0.8739943504333496}]}], "abstractContent": [{"text": "We present our English\u2192Czech and English\u2192Hindi submissions for this year's WMT translation task.", "labels": [], "entities": [{"text": "WMT translation task", "start_pos": 75, "end_pos": 95, "type": "TASK", "confidence": 0.9352195858955383}]}, {"text": "For English\u2192Czech, we build upon last year's CHIMERA and evaluate several setups.", "labels": [], "entities": []}, {"text": "English\u2192Hindi is anew language pair for this year.", "labels": [], "entities": []}, {"text": "We experimented with reverse self-training to acquire more (synthetic) parallel data and with modeling target-side morphology.", "labels": [], "entities": []}], "introductionContent": [{"text": "In this paper, we describe translation systems submitted by Charles University (CU or CUNI) to the Translation task of the Ninth Workshop on Statistical Machine Translation (WMT) 2014.", "labels": [], "entities": [{"text": "Charles University (CU or CUNI)", "start_pos": 60, "end_pos": 91, "type": "DATASET", "confidence": 0.9349332707268851}, {"text": "Translation task of the Ninth Workshop on Statistical Machine Translation (WMT) 2014", "start_pos": 99, "end_pos": 183, "type": "TASK", "confidence": 0.8900178372859955}]}, {"text": "In \u00a72, we present our English\u2192Czech systems, CU-TECTOMT, CU-BOJAR, CU-DEPFIX and CU-FUNKY.", "labels": [], "entities": []}, {"text": "The systems are very similar to our submissions () from last year, the main novelty being our experiments with domainspecific and document-specific language models.", "labels": [], "entities": []}, {"text": "In \u00a73, we describe our experiments with English\u2192Hindi translation, which is a translation pair new both to us and to WMT.", "labels": [], "entities": [{"text": "English\u2192Hindi translation", "start_pos": 40, "end_pos": 65, "type": "TASK", "confidence": 0.6195813491940498}, {"text": "WMT", "start_pos": 117, "end_pos": 120, "type": "DATASET", "confidence": 0.8415552973747253}]}, {"text": "We unsuccessfully experimented with reverse self-training and a morphological-tags-based language model, and so our final submission, CU-MOSES, is only a basic instance of Moses.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: English\u2192Czech parallel data.", "labels": [], "entities": []}, {"text": " Table 3: Czech monolingual data.", "labels": [], "entities": [{"text": "Czech monolingual data", "start_pos": 10, "end_pos": 32, "type": "DATASET", "confidence": 0.7924344937006632}]}, {"text": " Table 4: Czech LMs used in CU-BOJAR. The last  small model is described in  \u00a72.2.", "labels": [], "entities": [{"text": "CU-BOJAR", "start_pos": 28, "end_pos": 36, "type": "DATASET", "confidence": 0.8978755474090576}]}, {"text": " Table 5: Scores of automatic metrics and results of  manual evaluation for our systems. The table also  lists the best system according to automatic met- rics and Google Translate as the best-performing  commercial system.", "labels": [], "entities": []}, {"text": " Table 6: Hindi monolingual data.", "labels": [], "entities": [{"text": "Hindi monolingual data", "start_pos": 10, "end_pos": 32, "type": "DATASET", "confidence": 0.6030305822690328}]}, {"text": " Table 8: Options for back-off factors in reverse  self-training and the percentage of their vocabu- lary size compared to surface forms.", "labels": [], "entities": []}]}