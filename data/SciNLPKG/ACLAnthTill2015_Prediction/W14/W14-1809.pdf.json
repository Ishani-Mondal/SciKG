{"title": [{"text": "Automatic evaluation of spoken summaries: the case of language assessment", "labels": [], "entities": [{"text": "evaluation of spoken summaries", "start_pos": 10, "end_pos": 40, "type": "TASK", "confidence": 0.5407572984695435}, {"text": "language assessment", "start_pos": 54, "end_pos": 73, "type": "TASK", "confidence": 0.7129061967134476}]}], "abstractContent": [{"text": "This paper investigates whether ROUGE, a popular metric for the evaluation of automated written summaries, can be applied to the assessment of spoken summaries produced by non-native speakers of English.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 32, "end_pos": 37, "type": "METRIC", "confidence": 0.9947932362556458}, {"text": "evaluation of automated written summaries", "start_pos": 64, "end_pos": 105, "type": "TASK", "confidence": 0.6161887049674988}, {"text": "assessment of spoken summaries produced by non-native speakers of English", "start_pos": 129, "end_pos": 202, "type": "TASK", "confidence": 0.7690438687801361}]}, {"text": "We demonstrate that ROUGE, with its emphasis on the recall of information , is particularly suited to the assessment of the summarization quality of non-native speakers' responses.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 20, "end_pos": 25, "type": "METRIC", "confidence": 0.7150779962539673}, {"text": "recall", "start_pos": 52, "end_pos": 58, "type": "METRIC", "confidence": 0.9570306539535522}, {"text": "summarization", "start_pos": 124, "end_pos": 137, "type": "TASK", "confidence": 0.9604669809341431}]}, {"text": "A standard baseline implementation of ROUGE-1 computed over the output of the automated speech recognizer has a Spear-man correlation of \u03c1 = 0.55 with experts' scores of speakers' proficiency (\u03c1 = 0.51 fora content-vector baseline).", "labels": [], "entities": [{"text": "speech recognizer", "start_pos": 88, "end_pos": 105, "type": "TASK", "confidence": 0.7160474807024002}, {"text": "Spear-man correlation", "start_pos": 112, "end_pos": 133, "type": "METRIC", "confidence": 0.9811551868915558}]}, {"text": "Further increases in agreement with experts' scores can be achieved by using types instead of tokens for the computation of word frequencies for both candidate and reference summaries, as well as by using multiple reference summaries instead of a single one.", "labels": [], "entities": [{"text": "agreement", "start_pos": 21, "end_pos": 30, "type": "METRIC", "confidence": 0.9710075259208679}]}, {"text": "These modifications increase the correlation with experts' scores to a Spear-man correlation of \u03c1 = 0.65.", "labels": [], "entities": [{"text": "Spear-man correlation", "start_pos": 71, "end_pos": 92, "type": "METRIC", "confidence": 0.9699714183807373}]}, {"text": "Furthermore, we found that the choice of reference summaries does not have any impact on performance , and that the adjusted metric is also robust to errors introduced by automated speech recognition (\u03c1 = 0.67 for human transcriptions vs. \u03c1 = 0.65 for speech recognition output).", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 181, "end_pos": 199, "type": "TASK", "confidence": 0.7214074581861496}]}], "introductionContent": [{"text": "In this paper we explore whether metrics commonly used for the automated evaluation of written summaries can be used to evaluate spoken summaries in the context of language assessment.", "labels": [], "entities": []}, {"text": "The performance of automatic summarization systems is routinely evaluated using content metrics such as ROUGE (), which measures the n-gram overlap between the candidate summary and a set of reference summaries (see also for historical background).", "labels": [], "entities": [{"text": "summarization", "start_pos": 29, "end_pos": 42, "type": "TASK", "confidence": 0.7935754656791687}, {"text": "ROUGE", "start_pos": 104, "end_pos": 109, "type": "METRIC", "confidence": 0.9950186014175415}]}, {"text": "ROUGE is a recall-oriented metric inspired by its precision-oriented counterpart BLEU, developed to evaluate machine translations).", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9618558287620544}, {"text": "recall-oriented", "start_pos": 11, "end_pos": 26, "type": "METRIC", "confidence": 0.9956883788108826}, {"text": "precision-oriented", "start_pos": 50, "end_pos": 68, "type": "METRIC", "confidence": 0.9721934199333191}, {"text": "BLEU", "start_pos": 81, "end_pos": 85, "type": "METRIC", "confidence": 0.9601879119873047}]}, {"text": "Recent research in this area has been focused on identifying the most reliable variants of ROUGE and best practices in the application of the metric).", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 91, "end_pos": 96, "type": "METRIC", "confidence": 0.9826468825340271}]}, {"text": "These studies (reviewed in more detail in Section 2.1) showed that less commonly used variants of ROUGE may in fact be more consistent with human judgments, at least in the context of automatic summary evaluation.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 98, "end_pos": 103, "type": "METRIC", "confidence": 0.973003089427948}, {"text": "automatic summary evaluation", "start_pos": 184, "end_pos": 212, "type": "TASK", "confidence": 0.5945912897586823}]}, {"text": "Beyond the research in automatic summarization systems, ROUGE has also been used to evaluate written summaries in the context of educational assessment.", "labels": [], "entities": [{"text": "summarization", "start_pos": 33, "end_pos": 46, "type": "TASK", "confidence": 0.83150315284729}, {"text": "ROUGE", "start_pos": 56, "end_pos": 61, "type": "METRIC", "confidence": 0.7756221294403076}]}, {"text": "showed that one of the variants of ROUGE, in combination with other metrics, performed consistently well for the automated scoring of written responses to summary tasks produced by middle-and highschool students.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 35, "end_pos": 40, "type": "METRIC", "confidence": 0.9887653589248657}, {"text": "automated scoring of written responses to summary tasks produced by middle-and highschool students", "start_pos": 113, "end_pos": 211, "type": "TASK", "confidence": 0.6537445737765386}]}, {"text": "They did not investigate the effect of using other variants of In this paper, we explore whether ROUGE can be used to automatically evaluate the content coverage of spoken summaries produced by non-native speakers in the context of language assessment.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 97, "end_pos": 102, "type": "METRIC", "confidence": 0.9803342223167419}]}, {"text": "As in case of automatic text summaries, the human raters who score these responses are asked to assess whether the summary accurately conveys the information contained in the stimulus.", "labels": [], "entities": [{"text": "automatic text summaries", "start_pos": 14, "end_pos": 38, "type": "TASK", "confidence": 0.6387138664722443}]}, {"text": "While the length of the spoken responses is more loosely constrained than in case of automatic text summaries, human raters do not penalize for extraneously irrelevant language.", "labels": [], "entities": [{"text": "automatic text summaries", "start_pos": 85, "end_pos": 109, "type": "TASK", "confidence": 0.7084518671035767}]}, {"text": "Therefore recalloriented ROUGE is an attractive evaluation metric for this task.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 25, "end_pos": 30, "type": "METRIC", "confidence": 0.9734979867935181}]}, {"text": "At the same time, unlike automatic text sum-maries, spoken summaries are abstractive and often contain ungrammatical sequences, repetitions, repairs, and other disfluencies.", "labels": [], "entities": []}, {"text": "Further 'noise' is introduced by transcription errors generated by the automated speech recognition system.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 81, "end_pos": 99, "type": "TASK", "confidence": 0.6858101040124893}]}, {"text": "In this study, we assess whether (a) ROUGE is robust against this type of noise; (b) how many reference summaries are necessary to obtain reliable evaluation; and (c) how the choice of specific reference summaries affects the performance of the metric (Section 4.1).", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 37, "end_pos": 42, "type": "METRIC", "confidence": 0.9842237830162048}]}, {"text": "We also assess which variants of ROUGE have the most agreement with human judgments on this type of summary and what adjustments can be made to mitigate the effects of disfluencies and errors introduced by automated speech recognition (Section 4.2).", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 33, "end_pos": 38, "type": "METRIC", "confidence": 0.868607223033905}, {"text": "speech recognition", "start_pos": 216, "end_pos": 234, "type": "TASK", "confidence": 0.7356550693511963}]}, {"text": "Finally, we test how well our adjusted variant of ROUGE can predict the human scores on unseen data (Section 4.3).", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 50, "end_pos": 55, "type": "METRIC", "confidence": 0.9702391624450684}]}], "datasetContent": [{"text": "There exist various versions of ROUGE which differ in terms of the length of their n-grams, the use of skip-bigrams, the application of stemming, and the exclusion of stop-words.", "labels": [], "entities": []}, {"text": "Several studies have compared these variants to identify those most consistent with human judgments.", "labels": [], "entities": []}, {"text": "In earlier work, reported that variants based on unigrams and skip-bigrams (ROUGE-SU4) or bigrams alone (ROUGE-2) performed best.", "labels": [], "entities": [{"text": "ROUGE-SU4", "start_pos": 76, "end_pos": 85, "type": "METRIC", "confidence": 0.9113301634788513}]}, {"text": "ROUGE-2 was also identified as the best variant more recently by. found that linear combinations of these metrics with ROUGE based on longer n-grams are more accurate in finding significantly different systems.", "labels": [], "entities": [{"text": "ROUGE-2", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.824356734752655}]}, {"text": "Previous work also explored various methods of text pre-processing prior to the computation of ROUGE, including stemming and the removal of stop-words, neither of which had any substantial effect on the performance of ROUGE ().", "labels": [], "entities": []}, {"text": "reported that the agreement with human judgments was, in fact, higher if the stop-words were retained.", "labels": [], "entities": []}, {"text": "All applications discussed so far used ROUGE to evaluate the textual summarization of written texts.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 39, "end_pos": 44, "type": "METRIC", "confidence": 0.9820581078529358}, {"text": "textual summarization of written texts", "start_pos": 61, "end_pos": 99, "type": "TASK", "confidence": 0.7172145485877991}]}, {"text": "There have also been attempts to apply this metric to text summaries of speech data with mixed results (see fora review).", "labels": [], "entities": [{"text": "text summaries of speech", "start_pos": 54, "end_pos": 78, "type": "TASK", "confidence": 0.7553779482841492}]}, {"text": "ROUGE performed reasonably well for the evaluation of text summaries of spoken presentations (), but was not correlated with the summary accuracy of summaries of meetings or conversations (although see).", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9676101207733154}, {"text": "evaluation of text summaries of spoken presentations", "start_pos": 40, "end_pos": 92, "type": "TASK", "confidence": 0.7397582701274327}, {"text": "accuracy", "start_pos": 137, "end_pos": 145, "type": "METRIC", "confidence": 0.883948564529419}]}, {"text": "Most of this work was performed on extractive summaries produced by summarization systems that used multiple summaries to evaluate each system.", "labels": [], "entities": [{"text": "summarization", "start_pos": 68, "end_pos": 81, "type": "TASK", "confidence": 0.7299696803092957}]}, {"text": "In this study, we explore the application of ROUGE to the evaluation of abstractive summaries produced by students in a language assessment context with an aim of producing a separate evaluation for each summary.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 45, "end_pos": 50, "type": "METRIC", "confidence": 0.7850499153137207}, {"text": "evaluation of abstractive summaries produced by students in a language assessment context", "start_pos": 58, "end_pos": 147, "type": "TASK", "confidence": 0.7532690018415451}]}, {"text": "Furthermore, the fact that these are spoken responses adds an extra layer of complexity to the analysis, therefore the results of previous studies cannot directly be applied to this new context.", "labels": [], "entities": []}, {"text": "The research on the automated scoring of content accuracy in a language assessment has primarily focused on the evaluation of written essays.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.8951250314712524}]}, {"text": "Most previous approaches in this area have used so-called \"bag-of-words\"-based models, gleaned from the discipline of information retrieval.", "labels": [], "entities": []}, {"text": "The basic idea is that an essay is considered to be highly content relevant to a given topic when it contains words that are similar to those seen in previously collected essays with high human-rater scores.", "labels": [], "entities": []}, {"text": "For instance, used a vector-space model to compute the cosine similarities between word vectors found in an essay to be automatically scored and word vectors comprising previously scored essays with the same human-rater score.", "labels": [], "entities": []}, {"text": "Ina similar vein, Foltz et al.", "labels": [], "entities": []}, {"text": "(1999) computed a compressed vector space based on singular value decomposition fora set of document-word vectors, called latent semantic analysis, and then computed similarity scores for essays based on this more compact representation.", "labels": [], "entities": [{"text": "latent semantic analysis", "start_pos": 122, "end_pos": 146, "type": "TASK", "confidence": 0.6592679222424825}]}, {"text": "It should be noted, though, that since all of these models do not take word sequences into account, they must be considered knowledge-poor in that they cannot distinguish between syntactic roles or a list of random words versus a well-formed sentence.", "labels": [], "entities": []}, {"text": "In operational systems, such bag-ofwords similarity features are combined with features which evaluate grammar and other aspects of language use; therefore a random list of content words is unlikely to lead to a high overall score.", "labels": [], "entities": []}, {"text": "However, finer-grained distinctions such as negations or subject-object relationships between words are often lost.", "labels": [], "entities": []}, {"text": "Applications of these methods to spontaneous speech in spoken-language assessments have been conducted much more recently as this domain of language assessment relies on the output of Automatic Speech Recognition systems (ASR) that typically have a fairly high word-error rate.", "labels": [], "entities": []}, {"text": "These errors can negatively affect the accuracy of the methods developed for written responses.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.9990596175193787}]}, {"text": "Furthermore, spoken responses differ in many properties from written ones () and the validity of existing methods for assessing speech needs to be established before they can be used for operational scoring.", "labels": [], "entities": [{"text": "operational scoring", "start_pos": 187, "end_pos": 206, "type": "TASK", "confidence": 0.8502129912376404}]}, {"text": "presented experiments using content features on spontaneous-speech data based on vector-space models, latent semantic analysis, as well as point-wise mutual information.", "labels": [], "entities": []}, {"text": "Some of these content features showed higher correlations with human scores than features measuring other aspects of speaking proficiency, such as fluency or pronunciation.", "labels": [], "entities": []}, {"text": "also used a vector space model for the scoring of spontaneous speech, but extended it by using the ontological information contained in WordNet.", "labels": [], "entities": [{"text": "scoring of spontaneous speech", "start_pos": 39, "end_pos": 68, "type": "TASK", "confidence": 0.8442221581935883}, {"text": "WordNet", "start_pos": 136, "end_pos": 143, "type": "DATASET", "confidence": 0.9755992293357849}]}, {"text": "Finally, used a variety of approaches to capture the content of spontaneous responses from the same corpus that we are investigating in this paper.", "labels": [], "entities": []}, {"text": "Approaches varied from computing the overlap between key words in the stimuli and responses to a more traditional vector space model based on content vector analysis.", "labels": [], "entities": []}, {"text": "While these approaches have good correlations with human scores, they have a number of shortcomings.", "labels": [], "entities": []}, {"text": "The best performing method suggested by requires the manual annotation of the relevant key words for each prompt before the computation of the metric.", "labels": [], "entities": []}, {"text": "Vector space models do not have this limitation, but they require a substantial number of reference summaries to achieve consistent results.", "labels": [], "entities": []}, {"text": "Supporting this point, showed that at least 50 reference responses were necessary to obtain moderate agreement between the cosine similarity measure and human judgments, with further improvement in agreement as the number of reference responses is increased to 200.", "labels": [], "entities": [{"text": "agreement", "start_pos": 198, "end_pos": 207, "type": "METRIC", "confidence": 0.9955495595932007}]}, {"text": "These limitations pose practical difficulties when new items are added to the tests: the computation of content metrics for each new item requires either a manual annotation or a relatively large number of reference responses.", "labels": [], "entities": []}, {"text": "ROUGE appears promising in this context since it does not have either of these limitations.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.6534250974655151}]}, {"text": "First, the computation of ROUGE does not require manual annotation.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 26, "end_pos": 31, "type": "METRIC", "confidence": 0.8914321064949036}]}, {"text": "Second, research on the evaluation of written summaries suggests that relatively few reference summaries maybe necessary to obtain reliable results, e.g., only four references were used for the summary evaluation at the Text Analysis Conference ().", "labels": [], "entities": [{"text": "Text Analysis Conference", "start_pos": 220, "end_pos": 244, "type": "TASK", "confidence": 0.7548734446366628}]}, {"text": "In addition, the recall-based nature of ROUGE is well-aligned with the evaluation criteria for these responses.", "labels": [], "entities": [{"text": "recall-based", "start_pos": 17, "end_pos": 29, "type": "METRIC", "confidence": 0.9984439015388489}, {"text": "ROUGE", "start_pos": 40, "end_pos": 45, "type": "METRIC", "confidence": 0.8804281949996948}]}, {"text": "Therefore in this paper, we explore whether any of the variants of ROUGE can be successfully applied to the content scoring of spoken summaries and what modifications maybe necessary to achieve optimal performance.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 67, "end_pos": 72, "type": "METRIC", "confidence": 0.9625687003135681}, {"text": "content scoring of spoken summaries", "start_pos": 108, "end_pos": 143, "type": "TASK", "confidence": 0.7722653269767761}]}, {"text": "We computed the Spearman's rank correlation between the metric and the holistic score assigned by the first rater to identify the best method of computing ROUGE and the optimal number of references.", "labels": [], "entities": [{"text": "Spearman's rank correlation", "start_pos": 16, "end_pos": 43, "type": "METRIC", "confidence": 0.7847927510738373}, {"text": "ROUGE", "start_pos": 155, "end_pos": 160, "type": "METRIC", "confidence": 0.9632413387298584}]}, {"text": "Performance of the metric maybe affected by properties of the prompt (cf. (), therefore we first analyzed each prompt separately and then selected the variants that achieved the highest performance across all of the prompts.", "labels": [], "entities": []}, {"text": "Since correlation coefficients are not normally distributed, we used several nonparametric methods to identify significant differences including non-parametric bootstrapping and non-parametric ANOVAs.", "labels": [], "entities": []}, {"text": "These analyses were done using the data from the training partition of the corpus.", "labels": [], "entities": []}, {"text": "We then evaluated how well the selected variants of ROUGE predicted human scores using a linear regression model trained on all of the data from the training partition using pooled data from all of the prompts.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 52, "end_pos": 57, "type": "METRIC", "confidence": 0.6823373436927795}]}, {"text": "The model was tested on an unseen test partition that had not been used for any of the analyses.", "labels": [], "entities": []}, {"text": "Finally, we tested whether the new metrics improved the performance of the automated scoring engine for spoken responses.", "labels": [], "entities": []}, {"text": "The current system assigns scores based on the linear combination of features with empirical weights obtained by training scoring models on scores assigned by expert raters ().", "labels": [], "entities": []}, {"text": "Current features measure various aspects of speaking proficiency such as fluency, pronunciation, and grammar usage.", "labels": [], "entities": []}, {"text": "The performance of the system is evaluated with correlations and quadratic kappas between the scores assigned by the human raters and rounded predicted scores.", "labels": [], "entities": []}, {"text": "Analysis by prompt showed that the variants of ROUGE that included unigram counts (ROUGE and ROUGE-SU1-4) had the best correlations with human scores across all prompts.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 83, "end_pos": 88, "type": "METRIC", "confidence": 0.8639966249465942}]}, {"text": "Further improvement in their performance was obtained by counting type frequencies only and by using several reference summaries.", "labels": [], "entities": []}, {"text": "The optimal N references for these variants of ROUGE varied between prompts, but never exceeded four which was therefore selected as the optimal N references for this corpus.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 47, "end_pos": 52, "type": "METRIC", "confidence": 0.6422671675682068}]}, {"text": "Based on these results we computed ROUGE-1 metrics for all responses in the original training partition using four randomly selected, highly scored responses for each prompt and 'types' method of pre-processing.", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 35, "end_pos": 42, "type": "METRIC", "confidence": 0.9819061756134033}]}, {"text": "We then compared it with two baselines: (1) cosine distance (CVA) computed using type frequencies only and the same four references, and (2) na\u00a8\u0131vena\u00a8\u0131ve implementation of ROUGE-1 computed using one randomly selected reference summary and raw frequencies (tokens).", "labels": [], "entities": [{"text": "cosine distance (CVA)", "start_pos": 44, "end_pos": 65, "type": "METRIC", "confidence": 0.8751121401786804}]}, {"text": "The newly adjusted version of ROUGE-1 metrics performed significantly above the baselines (using Zou's method for the comparison of overlapping correlations with confidence intervals constructed at \u03b1 = 0.001).", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 30, "end_pos": 37, "type": "METRIC", "confidence": 0.920356035232544}]}, {"text": "The correlation coefficients are shown in.", "labels": [], "entities": [{"text": "correlation", "start_pos": 4, "end_pos": 15, "type": "METRIC", "confidence": 0.9820899963378906}]}, {"text": "We then trained a standard linear regression model using the human scores as the dependent variables and summarization metrics as independent variables.", "labels": [], "entities": []}, {"text": "The accuracy of prediction was evaluated using two metrics as suggested, for example, by: quadratic weighted kappa (\u03ba) and Pearson's correlation coefficient (r) between the observed and predicted scores.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9992851614952087}, {"text": "quadratic weighted kappa (\u03ba)", "start_pos": 90, "end_pos": 118, "type": "METRIC", "confidence": 0.9559429089228312}, {"text": "Pearson's correlation coefficient (r)", "start_pos": 123, "end_pos": 160, "type": "METRIC", "confidence": 0.9720655339104789}]}, {"text": "For computation of \u03ba, the predicted scores were trimmed to the range of human scores and rounded to the nearest integer.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Average correlation coefficient with hu- man scores (Spearman's \u03c1) across different meth- ods of computation for ROUGE based on n-grams  of different lengths. The table shows the results  for metrics computed based on ASR and manual  transcriptions.", "labels": [], "entities": [{"text": "Average correlation coefficient", "start_pos": 10, "end_pos": 41, "type": "METRIC", "confidence": 0.7566831509272257}, {"text": "Spearman's \u03c1)", "start_pos": 63, "end_pos": 76, "type": "METRIC", "confidence": 0.6828226670622826}, {"text": "ROUGE", "start_pos": 123, "end_pos": 128, "type": "METRIC", "confidence": 0.6467649936676025}, {"text": "ASR", "start_pos": 228, "end_pos": 231, "type": "TASK", "confidence": 0.8799523711204529}]}, {"text": " Table 3: Correlation coefficients with human  scores (Spearman's \u03c1) for the entire training parti- tion for the newly adjusted version of ROUGE and  the baseline metrics. The table shows the results  for metrics computed based on ASR and manual  transcriptions.", "labels": [], "entities": [{"text": "Correlation", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9684259295463562}, {"text": "Spearman's \u03c1)", "start_pos": 55, "end_pos": 68, "type": "METRIC", "confidence": 0.7093981876969337}, {"text": "ROUGE", "start_pos": 139, "end_pos": 144, "type": "METRIC", "confidence": 0.7578829526901245}, {"text": "ASR", "start_pos": 231, "end_pos": 234, "type": "TASK", "confidence": 0.8652405738830566}]}]}