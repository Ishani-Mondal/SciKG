{"title": [{"text": "Randomized Significance Tests in Machine Translation", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 33, "end_pos": 52, "type": "TASK", "confidence": 0.7686000168323517}]}], "abstractContent": [{"text": "Randomized methods of significance testing enable estimation of the probability that an increase in score has occurred simply by chance.", "labels": [], "entities": [{"text": "significance testing", "start_pos": 22, "end_pos": 42, "type": "TASK", "confidence": 0.9214981198310852}]}, {"text": "In this paper, we examine the accuracy of three randomized methods of significance testing in the context of machine translation: paired bootstrap resampling, bootstrap resampling and approximate randomization.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9962300658226013}, {"text": "machine translation", "start_pos": 109, "end_pos": 128, "type": "TASK", "confidence": 0.7472819685935974}]}, {"text": "We carryout a large-scale human evaluation of shared task systems for two language pairs to provide a gold standard for tests.", "labels": [], "entities": []}, {"text": "Results show very little difference inaccuracy across the three methods of significance testing.", "labels": [], "entities": []}, {"text": "Notably, accuracy of all test/metric combinations for evaluation of English-to-Spanish are so low that there is not enough evidence to conclude they are any better than a random coin toss.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 9, "end_pos": 17, "type": "METRIC", "confidence": 0.9993915557861328}]}], "introductionContent": [{"text": "Automatic metrics, such as BLEU (), are widely used in machine translation (MT) as a substitute for human evaluation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 27, "end_pos": 31, "type": "METRIC", "confidence": 0.9987868666648865}, {"text": "machine translation (MT)", "start_pos": 55, "end_pos": 79, "type": "TASK", "confidence": 0.8398975849151611}]}, {"text": "Such metrics commonly take the form of an automatic comparison of MT output text with one or more human reference translations.", "labels": [], "entities": [{"text": "MT output text", "start_pos": 66, "end_pos": 80, "type": "TASK", "confidence": 0.879504919052124}]}, {"text": "Small differences in automatic metric scores can be difficult to interpret, however, and statistical significance testing provides away of estimating the likelihood that a score difference has occurred simply by chance.", "labels": [], "entities": []}, {"text": "For several metrics, such as BLEU, standard significance tests cannot be applied due to scores not comprising the mean of individual sentence scores, justifying the use of randomized methods.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 29, "end_pos": 33, "type": "METRIC", "confidence": 0.9384429454803467}]}, {"text": "Bootstrap resampling was one of the early randomized methods proposed for statistical significance testing of MT, to assess fora pair of systems how likely a difference in BLEU scores occurred by chance.", "labels": [], "entities": [{"text": "Bootstrap resampling", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.6601819396018982}, {"text": "MT", "start_pos": 110, "end_pos": 112, "type": "TASK", "confidence": 0.9537967443466187}, {"text": "BLEU scores", "start_pos": 172, "end_pos": 183, "type": "METRIC", "confidence": 0.9769957959651947}]}, {"text": "Empirical tests detailed in show that even for test sets as small as 300 translations, BLEU confidence intervals can be computed as accurately as if they had been computed on a test set 100 times as large.", "labels": [], "entities": [{"text": "BLEU confidence intervals", "start_pos": 87, "end_pos": 112, "type": "METRIC", "confidence": 0.9735118746757507}]}, {"text": "Approximate randomization was subsequently proposed as an alternate to bootstrap resampling ().", "labels": [], "entities": [{"text": "bootstrap resampling", "start_pos": 71, "end_pos": 91, "type": "TASK", "confidence": 0.73603755235672}]}, {"text": "Theoretically speaking, approximate randomization has an advantage over bootstrap resampling, in that it does not make the assumption that samples are representative of the populations from which they are drawn.", "labels": [], "entities": [{"text": "bootstrap resampling", "start_pos": 72, "end_pos": 92, "type": "TASK", "confidence": 0.7719776630401611}]}, {"text": "Both methods require some adaptation in order to be used for the purpose of MT evaluation, such as combination with an automatic metric, and therefore it cannot betaken for granted that approximate randomization will be more accurate in practice.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 76, "end_pos": 89, "type": "TASK", "confidence": 0.9791474938392639}]}, {"text": "Within MT, approximate randomization for the purpose of statistical testing is also less common.", "labels": [], "entities": [{"text": "MT", "start_pos": 7, "end_pos": 9, "type": "TASK", "confidence": 0.9588475227355957}]}, {"text": "provide a comparison of approximate randomization with bootstrap resampling (distinct from paired bootstrap resampling), and conclude that since approximate randomization produces higher p-values fora set of apparently equally-performing systems, it more conservatively concludes statistically significant differences, and recommend preference of approximate randomization over bootstrap resampling for MT evaluation.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 403, "end_pos": 416, "type": "TASK", "confidence": 0.9506986439228058}]}, {"text": "Conclusions drawn from experiments provided in are oft-cited, with experiments interpreted as evidence that bootstrap resampling is overly optimistic in reporting significant differences ().", "labels": [], "entities": [{"text": "bootstrap resampling", "start_pos": 108, "end_pos": 128, "type": "TASK", "confidence": 0.7708173096179962}]}, {"text": "Our contribution in this paper is to revisit statistical significance tests in MT -namely, bootstrap resampling, paired bootstrap resampling and approximate randomization -and find problems with the published formulations.", "labels": [], "entities": [{"text": "MT", "start_pos": 79, "end_pos": 81, "type": "TASK", "confidence": 0.9765990376472473}, {"text": "bootstrap resampling", "start_pos": 91, "end_pos": 111, "type": "TASK", "confidence": 0.6894687414169312}]}, {"text": "We redress these issues, and apply the tests in statistical testing of two language pairs.", "labels": [], "entities": []}, {"text": "Using human judgments of translation quality, we find only very minor differences in significance levels across the three tests, challenging claims made in the literature about relative merits of tests.", "labels": [], "entities": [{"text": "translation", "start_pos": 25, "end_pos": 36, "type": "TASK", "confidence": 0.9631776809692383}]}], "datasetContent": [{"text": "First, we revisit the formulations of bootstrap resampling and approximate randomization algorithms as presented in.", "labels": [], "entities": [{"text": "bootstrap resampling", "start_pos": 38, "end_pos": 58, "type": "TASK", "confidence": 0.8210451900959015}]}, {"text": "At first glance, both methods appear to be two-tailed tests, with the null hypothesis that the two systems perform equally well.", "labels": [], "entities": []}, {"text": "To facilitate a two-tailed test, absolute values of pseudostatistics are computed before locating the absolute value of the actual statistic (original difference in scores).", "labels": [], "entities": []}, {"text": "Using absolute values of pseudostatistics is not problematic in the approximate randomization algorithm, and results in a reasonable two-tailed significance test.", "labels": [], "entities": []}, {"text": "However, the bootstrap algorithm they provide uses an additional shift-to-zero method of simulating the null hypothesis.", "labels": [], "entities": []}, {"text": "The way in which this shift-to-zero and absolute values of pseudo-statistics are applied is non-standard.", "labels": [], "entities": []}, {"text": "Combining shift-to-zero and absolute values of pseudo-statistics results in all pseudo-statistics that fall below the mean pseudo-statistic to be omitted from computation of counts later used to compute p-values.", "labels": [], "entities": []}, {"text": "The version of the bootstrap algorithm, as provided in the pseudo-code, is effectively a one-tailed test, and since this does not happen in the approximate randomization algorithm, experiments appear to compare p-values from a one-tailed bootstrap test directly with those of a two-tailed approximate randomization test.", "labels": [], "entities": []}, {"text": "This inconsistency is not recognized, however, and p-values are compared as if both tests are two-tailed.", "labels": [], "entities": []}, {"text": "A better comparison of p-values would first require doubling the values of the one-sided bootstrap, leaving those of the two-sided approximate randomization algorithm as-is.", "labels": [], "entities": []}, {"text": "The results of the two tests on this basis are extremely close, and in fact, in two out of the five comparisons, those of the bootstrap would have marginally higher pvalues than those of approximate randomization.", "labels": [], "entities": []}, {"text": "As such, it is conceivable to conclude that the experiments actually show no substantial difference in Type I error between the two tests, which is consistent with results published in other fields of research ().", "labels": [], "entities": [{"text": "Type I error", "start_pos": 103, "end_pos": 115, "type": "METRIC", "confidence": 0.9474856853485107}]}, {"text": "We also note that the pseudo-code contains an unconventional computation of mean pseudo-statistics, \u03c4 B , for shiftto-zero.", "labels": [], "entities": []}, {"text": "Rather than speculate over whether these issues with the original paper were simply presentational glitches or the actual basis of the experiments reported on in the paper, we present a normalized version of the two-sided bootstrap algorithm in, and report on the results of our own experiments in Section 4.", "labels": [], "entities": []}, {"text": "We compare this method with approximate randomization and also paired bootstrap resampling, which is widely used in MT evaluation.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 116, "end_pos": 129, "type": "TASK", "confidence": 0.9657939970493317}]}, {"text": "We carryout evaluation over a range of MT systems, not only including pairs of systems that perform equally well, but also pairs of systems for which one system performs marginally better than the other.", "labels": [], "entities": [{"text": "MT", "start_pos": 39, "end_pos": 41, "type": "TASK", "confidence": 0.9707231521606445}]}, {"text": "This enables evaluation of not only Type I error, but the overall accuracy of the tests.", "labels": [], "entities": [{"text": "Type I error", "start_pos": 36, "end_pos": 48, "type": "METRIC", "confidence": 0.5827639400959015}, {"text": "accuracy", "start_pos": 66, "end_pos": 74, "type": "METRIC", "confidence": 0.9994753003120422}]}, {"text": "We carryout a large-scale human evaluation of all WMT 2012 shared task participating systems for two language pairs, and collect sufficient human judgments to facilitate statistical significance tests.", "labels": [], "entities": [{"text": "WMT 2012 shared task participating", "start_pos": 50, "end_pos": 84, "type": "TASK", "confidence": 0.5936151742935181}]}, {"text": "This human evaluation data then provides a gold-standard against which to compare randomized tests.", "labels": [], "entities": []}, {"text": "Since all randomized tests only function in combination with an automatic MT evaluation metric, we present results of each randomized test across four different MT metrics.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 74, "end_pos": 87, "type": "TASK", "confidence": 0.8771293461322784}]}, {"text": "In order to evaluate the accuracy of the three randomized significance significance tests, we compare conclusions reached in a human evaluation of shared task participant systems.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9981389045715332}]}, {"text": "We carryout a large-scale human evaluation of all participating systems from WMT 2012) for the Spanish-to-English and English-toSpanish translation tasks.", "labels": [], "entities": [{"text": "WMT 2012", "start_pos": 77, "end_pos": 85, "type": "DATASET", "confidence": 0.903725653886795}]}, {"text": "Large numbers of human assessments of translations were collected using Amazon's Mechanical Turk, with strict quality control filtering (.", "labels": [], "entities": [{"text": "Amazon's Mechanical Turk", "start_pos": 72, "end_pos": 96, "type": "DATASET", "confidence": 0.8858659863471985}]}, {"text": "A total of 82,100 human adequacy assessments and 62,400 human fluency assessments were collected.", "labels": [], "entities": []}, {"text": "After the removal of quality control items and filtering of judgments from low-quality workers, this resulted in an average of 1,280 adequacy and 1,013 fluency assessments per system for Spanishto-English (12 systems), and 1,483 adequacy and 1,534 fluency assessments per system for Englishto-Spanish (11 systems).", "labels": [], "entities": []}, {"text": "To remove bias with respect to individual human judge preference scoring severity/leniency, scores provided by each human assessor were standardized according to the mean and standard deviation of all scores provided by that individual.", "labels": [], "entities": []}, {"text": "Significance tests were carried out over the scores for each pair of systems separately for adequacy and fluency assessments using the Wilcoxon rank-sum test.", "labels": [], "entities": []}, {"text": "shows pairwise significance test results for fluency, adequacy and the combination of the two tests, for all pairs of Spanish-to-English systems.", "labels": [], "entities": []}, {"text": "Combined fluency and adequacy significance test results are constructed as follows: if a system's adequacy score is significantly greater than that of another, the combined conclusion is that it is significantly better, at that significance level.", "labels": [], "entities": []}, {"text": "Only when a tie inadequacy scores occurs are fluency judgments used to break the tie.", "labels": [], "entities": []}, {"text": "In this case, p-values from significance tests applied to fluency scores of that system pair are used.", "labels": [], "entities": []}, {"text": "For example, in, adequacy scores of System B are not significantly greater than those of Systems C, D and E, while fluency scores for System B are significantly greater than those of the three other systems.", "labels": [], "entities": []}, {"text": "The combined result for each pair of systems is therefore taken as the p-value from the corresponding fluency significance test.", "labels": [], "entities": []}, {"text": "We use the combined human evaluation pairwise significant tests as a gold standard against which to evaluate the randomized methods of statistical significance testing.", "labels": [], "entities": [{"text": "statistical significance testing", "start_pos": 135, "end_pos": 167, "type": "TASK", "confidence": 0.711745818456014}]}, {"text": "We evaluate paired bootstrap resampling and bootstrap resampling as shown in and approximate randomization as shown in, each in combination with four automatic MT metrics: BLEU (), NIST), METEOR (Banerjee and) and TER ().", "labels": [], "entities": [{"text": "MT", "start_pos": 160, "end_pos": 162, "type": "TASK", "confidence": 0.9677432179450989}, {"text": "BLEU", "start_pos": 172, "end_pos": 176, "type": "METRIC", "confidence": 0.9986966252326965}, {"text": "METEOR", "start_pos": 188, "end_pos": 194, "type": "METRIC", "confidence": 0.9906479716300964}, {"text": "TER", "start_pos": 214, "end_pos": 217, "type": "METRIC", "confidence": 0.9973887801170349}]}, {"text": "shows the outcome of pairwise randomized significance tests for each metric for Spanishto-English systems, and shows numbers of correct conclusions and accuracy of each test.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 152, "end_pos": 160, "type": "METRIC", "confidence": 0.9969224333763123}]}], "tableCaptions": [{"text": " Table 1: Accuracy of randomized significance tests for Spanish-to-English MT with four automatic  metrics, based on the WMT 2012 participant systems.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9595064520835876}, {"text": "MT", "start_pos": 75, "end_pos": 77, "type": "TASK", "confidence": 0.8005629777908325}, {"text": "WMT 2012 participant systems", "start_pos": 121, "end_pos": 149, "type": "DATASET", "confidence": 0.9325203448534012}]}, {"text": " Table 2: Accuracy of randomized significance tests for English-to-Spanish MT with four automatic  metrics, based on the WMT 2012 participant systems.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9607198238372803}, {"text": "MT", "start_pos": 75, "end_pos": 77, "type": "TASK", "confidence": 0.9187372922897339}, {"text": "WMT 2012 participant systems", "start_pos": 121, "end_pos": 149, "type": "DATASET", "confidence": 0.9337373226881027}]}]}