{"title": [{"text": "LAYERED: Metric for Machine Translation Evaluation", "labels": [], "entities": [{"text": "Machine Translation Evaluation", "start_pos": 20, "end_pos": 50, "type": "TASK", "confidence": 0.8595254421234131}]}], "abstractContent": [{"text": "This paper describes the LAYERED metric which is used for the shared WMT'14 metrics task.", "labels": [], "entities": [{"text": "LAYERED", "start_pos": 25, "end_pos": 32, "type": "METRIC", "confidence": 0.9774766564369202}, {"text": "WMT'14 metrics task", "start_pos": 69, "end_pos": 88, "type": "TASK", "confidence": 0.5449963311354319}]}, {"text": "Various metrics exist for MT evaluation: BLEU (Papineni, 2002), METEOR (Alon Lavie, 2007), TER (Snover, 2006) etc., but are found inadequate in quite a few language settings like, for example , in case of free word order languages.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 26, "end_pos": 39, "type": "TASK", "confidence": 0.9569903016090393}, {"text": "BLEU", "start_pos": 41, "end_pos": 45, "type": "METRIC", "confidence": 0.998647153377533}, {"text": "METEOR", "start_pos": 64, "end_pos": 70, "type": "METRIC", "confidence": 0.9963060617446899}, {"text": "TER", "start_pos": 91, "end_pos": 94, "type": "METRIC", "confidence": 0.9973090887069702}]}, {"text": "In this paper, we propose an MT evaluation scheme that is based on the NLP layers: lexical, syntactic and semantic.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 29, "end_pos": 42, "type": "TASK", "confidence": 0.9253454208374023}]}, {"text": "We contend that higher layer met-rics are after all needed.", "labels": [], "entities": []}, {"text": "Results are presented on the corpora of ACL-WMT, 2013 and 2014.", "labels": [], "entities": [{"text": "corpora of ACL-WMT", "start_pos": 29, "end_pos": 47, "type": "DATASET", "confidence": 0.786400039990743}]}, {"text": "We end with a metric which is composed of weighted metrics at individual layers, which correlates very well with human judgment.", "labels": [], "entities": []}], "introductionContent": [{"text": "Evaluation is an integral component of machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 39, "end_pos": 58, "type": "TASK", "confidence": 0.8019870221614838}]}, {"text": "Human evaluation is difficult and time consuming so there is a need fora metric which can give the better evaluation in correlation to human judgement.", "labels": [], "entities": []}, {"text": "There are several existing metrics such as: BLEU, METEOR etc.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 44, "end_pos": 48, "type": "METRIC", "confidence": 0.9991650581359863}, {"text": "METEOR", "start_pos": 50, "end_pos": 56, "type": "METRIC", "confidence": 0.9545483589172363}]}, {"text": "but these only deal with the lexical layer combining bag of words and n-gram based approach.", "labels": [], "entities": []}, {"text": "We present an analysis of BLEU and the higher layer metrics on the ACL WMT 2013 corpora with 3 language pairs: French-English, SpanishEnglish and German-English.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 26, "end_pos": 30, "type": "METRIC", "confidence": 0.9917140603065491}, {"text": "ACL WMT 2013 corpora", "start_pos": 67, "end_pos": 87, "type": "DATASET", "confidence": 0.9094589352607727}]}, {"text": "For syntactic layer, we considered three metrics: Hamming score, Kendall's Tau distance score and the spearman rank score.", "labels": [], "entities": [{"text": "Kendall's Tau distance score", "start_pos": 65, "end_pos": 93, "type": "METRIC", "confidence": 0.5247198879718781}]}, {"text": "Syntactic layer metrics take care of reordering within the words of the sentences so these may play an important role when there is a decision to be made between two MT output sentences of two different systems when both the sentences have same number of n-gram matches wrt the reference sentence but there is a difference in the ordering of the sentence.", "labels": [], "entities": [{"text": "MT output sentences", "start_pos": 166, "end_pos": 185, "type": "TASK", "confidence": 0.8586208820343018}]}, {"text": "We will discuss these metrics in detail in the following sections.", "labels": [], "entities": []}, {"text": "The next NLP layer in consideration is the semantic layer which deals with the meaning of the sentences.", "labels": [], "entities": []}, {"text": "For semantic layer, we considered two metrics: Shallow semantic score and Deep semantic score.", "labels": [], "entities": []}, {"text": "On semantic layer, we considered entailment based measures to get the score.", "labels": [], "entities": []}, {"text": "mentioned some issues in automatic evaluation using BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 52, "end_pos": 56, "type": "METRIC", "confidence": 0.9924626350402832}]}, {"text": "There are some disadvantages of the existing metrics also such as: BLEU does not take care of reordering of the words in the sentence.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 67, "end_pos": 71, "type": "METRIC", "confidence": 0.9984908103942871}]}, {"text": "BLEU-like metrics can give same score by permuting word order.", "labels": [], "entities": [{"text": "BLEU-like", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9709040522575378}]}, {"text": "These metrics can be unreliable at the level of individual sentences because there can be small number of n-grams involved.", "labels": [], "entities": []}, {"text": "We would see in this paper that the correlation of BLEU is lower compared to the semantic layer metrics.", "labels": [], "entities": [{"text": "correlation", "start_pos": 36, "end_pos": 47, "type": "METRIC", "confidence": 0.9617688059806824}, {"text": "BLEU", "start_pos": 51, "end_pos": 55, "type": "METRIC", "confidence": 0.9979579448699951}]}, {"text": "Section 2 presents the study of related work in MT evaluation.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 48, "end_pos": 61, "type": "TASK", "confidence": 0.989443302154541}]}, {"text": "Section 3 presents the importance of each NLP layer in evaluation of MT output.", "labels": [], "entities": [{"text": "MT output", "start_pos": 69, "end_pos": 78, "type": "TASK", "confidence": 0.872801274061203}]}, {"text": "It discusses the metrics that each layer contributes to the achievement of the final result.", "labels": [], "entities": []}, {"text": "In section 4, various experiments are presented with each metric on the top 10 ranking systems of WMT 13 corpora which are ranked on the basis of the human ranking.", "labels": [], "entities": [{"text": "WMT 13 corpora", "start_pos": 98, "end_pos": 112, "type": "DATASET", "confidence": 0.6569383243719736}]}, {"text": "Each metric is discussed with the graphical representation so that it would become clear to analyze the effect of each metric.", "labels": [], "entities": []}, {"text": "In section 5, spearman correlation of the metrics is calculated with human judgement and comparisons are shown.", "labels": [], "entities": [{"text": "correlation", "start_pos": 23, "end_pos": 34, "type": "METRIC", "confidence": 0.8691053986549377}]}, {"text": "In section 6, we discuss the need of a metric which should be a combination of the metrics presented in the above sections and present a weighted metric which is the amalgamation of the metrics at individual layers.", "labels": [], "entities": []}, {"text": "Section 7 presents the results of the proposed metric on WMT 14 data and compares it with other existing metrics.", "labels": [], "entities": [{"text": "WMT 14 data", "start_pos": 57, "end_pos": 68, "type": "DATASET", "confidence": 0.9346524278322855}]}], "datasetContent": [{"text": "In this section, we discuss the different NLP layers and how these are important for evalution of MT output.", "labels": [], "entities": [{"text": "MT output", "start_pos": 98, "end_pos": 107, "type": "TASK", "confidence": 0.9014073312282562}]}, {"text": "We discuss here the significance of three NLP layers: Lexical, Syntactic and Semantic layers.", "labels": [], "entities": []}, {"text": "We conducted the experiments on WMT 13 corpora for French-English, Spanish-English and German-English language pairs.", "labels": [], "entities": [{"text": "WMT 13 corpora", "start_pos": 32, "end_pos": 46, "type": "DATASET", "confidence": 0.7237187425295512}]}, {"text": "We calculated the score of each metric for the top 10 ranking system (wmt, 2013) (as per human judgement) for each language pair.", "labels": [], "entities": []}, {"text": "In the graphs, metric score is multiplied by 100 so that a better view can be captured.", "labels": [], "entities": [{"text": "metric score", "start_pos": 15, "end_pos": 27, "type": "METRIC", "confidence": 0.9838010966777802}]}, {"text": "2. In each graph, the scores of French-English (fren), Spanish-English (es-en) and German-English (de-en) language pairs are represented by red, black and blue lines respectively.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Correlation with different metrics in WMT 14 Results", "labels": [], "entities": [{"text": "WMT", "start_pos": 48, "end_pos": 51, "type": "TASK", "confidence": 0.628578245639801}]}]}