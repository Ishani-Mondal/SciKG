{"title": [{"text": "Sentiment Classification on Polarity Reviews: An Empirical Study Using Rating-based Features", "labels": [], "entities": [{"text": "Sentiment Classification on Polarity Reviews", "start_pos": 0, "end_pos": 44, "type": "TASK", "confidence": 0.8554694890975952}]}], "abstractContent": [{"text": "We present anew feature type named rating-based feature and evaluate the contribution of this feature to the task of document-level sentiment analysis.", "labels": [], "entities": [{"text": "document-level sentiment analysis", "start_pos": 117, "end_pos": 150, "type": "TASK", "confidence": 0.6506028970082601}]}, {"text": "We achieve state-of-the-art results on two publicly available standard polarity movie datasets: on the dataset consisting of 2000 reviews produced by Pang and Lee (2004) we obtain an accuracy of 91.6% while it is 89.87% evaluated on the dataset of 50000 reviews created by Maas et al.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 183, "end_pos": 191, "type": "METRIC", "confidence": 0.9991764426231384}]}, {"text": "We also get a performance at 93.24% on our own dataset consisting of 233600 movie reviews, and we aim to share this dataset for further research in sentiment polarity analysis task.", "labels": [], "entities": [{"text": "sentiment polarity analysis", "start_pos": 148, "end_pos": 175, "type": "TASK", "confidence": 0.8857678969701132}]}], "introductionContent": [{"text": "This paper focuses on document-level sentiment classification on polarity reviews.", "labels": [], "entities": [{"text": "document-level sentiment classification", "start_pos": 22, "end_pos": 61, "type": "TASK", "confidence": 0.6913824379444122}]}, {"text": "Specifically, the document-level sentiment analysis is to identify either a positive or negative opinion in a given opinionated review (.", "labels": [], "entities": []}, {"text": "In early work, proposed an unsupervised learning algorithm to classify reviews by calculating the mutual information between a given phrase and reference words \"excellent\" and \"poor\".", "labels": [], "entities": []}, {"text": "applied supervised learners of Naive Bayes, Maximum Entropy, and Support Vector Machine (SVM) to determine sentiment polarity over movie reviews.", "labels": [], "entities": [{"text": "sentiment polarity over movie reviews", "start_pos": 107, "end_pos": 144, "type": "TASK", "confidence": 0.7656286239624024}]}, {"text": "presented a minimum cut-based approach to detect whether each review' sentence is more likely subjective or not.", "labels": [], "entities": []}, {"text": "Then the sentiment of the whole document review is determined by employing a machine learning method on the document's most-subjective sentences.", "labels": [], "entities": []}, {"text": "Recently, most sentiment polarity classification systems (;;;) have obtained state-of-the-art results by employing machine learning techniques using combination of various features such as Ngrams, syntactic and semantic representations as well as exploiting lexicon resources In this paper, we firstly introduce a novel rating-based feature for the sentiment polarity classification task.", "labels": [], "entities": [{"text": "sentiment polarity classification", "start_pos": 15, "end_pos": 48, "type": "TASK", "confidence": 0.7920618653297424}, {"text": "sentiment polarity classification task", "start_pos": 349, "end_pos": 387, "type": "TASK", "confidence": 0.8337684273719788}]}, {"text": "Our rating-based feature can be seen by that the scores -which users employ to rate entities on review websitescould bring useful information for improving the performance of classifying polarity sentiment.", "labels": [], "entities": [{"text": "classifying polarity sentiment", "start_pos": 175, "end_pos": 205, "type": "TASK", "confidence": 0.8204874396324158}]}, {"text": "For a review with no associated score, we could predict a score for the review in the use of a regression model learned from an external independent dataset of reviews and their actual corresponding scores.", "labels": [], "entities": []}, {"text": "We refer to the predicted score as the rating-based feature for learning sentiment categorization.", "labels": [], "entities": [{"text": "learning sentiment categorization", "start_pos": 64, "end_pos": 97, "type": "TASK", "confidence": 0.7630661924680074}]}, {"text": "By combining the rating-based feature with unigrams, bigrams and trigrams, we then present the results from sentiment classification experiments on the benchmark datasets published by and.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 108, "end_pos": 132, "type": "TASK", "confidence": 0.890585720539093}]}, {"text": "To sum up, the contributions of our study are: \u2022 Propose a novel rating-based feature and describe regression models learned from the external dataset to predict the feature value for the reviews in the two experimental datasets.", "labels": [], "entities": []}, {"text": "\u2022 Achieve state-of-the-art performances in the use of the rating-based feature for the sentiment polarity classification task on the two datasets.", "labels": [], "entities": [{"text": "sentiment polarity classification task", "start_pos": 87, "end_pos": 125, "type": "TASK", "confidence": 0.8449612557888031}]}, {"text": "\u2022 Analyze comprehensively the proficiency of the rating-based feature to the accuracy performance.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.9993844032287598}]}, {"text": "\u2022 Report additional experimental results on our own dataset containing 233600 reviews.", "labels": [], "entities": []}, {"text": "The paper is organized as follows: We provide some related works and describe our approach in section 2 and section 3, respectively.", "labels": [], "entities": []}, {"text": "We detail our experiments in section 4.", "labels": [], "entities": []}, {"text": "Finally, section 5 presents concluding remarks.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conducted experimental evaluations on the polarity dataset PL04 1 of 2000 movie reviews constructed by.", "labels": [], "entities": [{"text": "polarity dataset PL04 1 of 2000 movie reviews", "start_pos": 45, "end_pos": 90, "type": "DATASET", "confidence": 0.834867499768734}]}, {"text": "The dataset PL04 consists of 1000 positive and 1000 negative document reviews in which each review was split into sentences with lowercase normalization.", "labels": [], "entities": [{"text": "PL04", "start_pos": 12, "end_pos": 16, "type": "DATASET", "confidence": 0.8757611513137817}]}, {"text": "In order to compare with other published results, we evaluate our method according to 10-fold cross-validation scheme on the dataset PL04.", "labels": [], "entities": [{"text": "PL04", "start_pos": 133, "end_pos": 137, "type": "DATASET", "confidence": 0.6049644351005554}]}, {"text": "In addition, we carryout experiments on a large dataset IMDB11 2 of 50000 movie reviews produced by Preprocess.", "labels": [], "entities": [{"text": "IMDB11 2 of 50000 movie reviews", "start_pos": 56, "end_pos": 87, "type": "DATASET", "confidence": 0.854705144961675}, {"text": "Preprocess", "start_pos": 100, "end_pos": 110, "type": "DATASET", "confidence": 0.5147535800933838}]}, {"text": "We did not apply stop-word removal, stemming and lemmatization to the dataset in any process in our system, because such stop-words as negation words might indicate sentiment orientation, and as pointed out by stemming and lemmatization processes could be detrimental to accuracy.", "labels": [], "entities": [{"text": "sentiment orientation", "start_pos": 165, "end_pos": 186, "type": "TASK", "confidence": 0.7570878267288208}, {"text": "accuracy", "start_pos": 271, "end_pos": 279, "type": "METRIC", "confidence": 0.9947744011878967}]}, {"text": "In all experiments on PL04, we kept 30000 most frequent N-grams in the training set for each cross-validation run over each polarity class.", "labels": [], "entities": []}, {"text": "After removing duplication, on an average, there are total 39950 N-gram features including 10280 unigrams, 20505 bigrams and 9165 trigrams.", "labels": [], "entities": []}, {"text": "On the dataset IMDB11, it was 40000 most frequent N-grams in each polarity class to be selected for creating feature set of 53724 Ngrams consisting of 13038 unigrams, 26907 bigrams and 13779 trigrams.", "labels": [], "entities": [{"text": "IMDB11", "start_pos": 15, "end_pos": 21, "type": "DATASET", "confidence": 0.9243965148925781}]}, {"text": "We aim to create an independent dataset for learning a regression model to predict the feature RbF's value for each document review in experimental datasets.", "labels": [], "entities": []}, {"text": "(2011) also provided 7091 IMDB movie titles 4 , we used those movie titles to extract all user reviews that their associated scores are not equal to either 5 or 6 from the IMDB website.", "labels": [], "entities": [{"text": "IMDB website", "start_pos": 172, "end_pos": 184, "type": "DATASET", "confidence": 0.9317674338817596}]}, {"text": "Consequently, we created an independent score-associated review dataset (SAR14) 6 of 233600 movie reviews and their accompanying actual scores.", "labels": [], "entities": []}, {"text": "The external dataset SAR14 consists of 167378 user reviews connected to scores valued from 7 to 10, and 66222 reviews linked to 1-4 rated ones (as shown in.", "labels": [], "entities": [{"text": "SAR14", "start_pos": 21, "end_pos": 26, "type": "DATASET", "confidence": 0.5304088592529297}]}, {"text": "Using SAR14, we employed Support Vector Regression algorithm implemented in SV M light package 7 to learn the regression model employing unigram features.", "labels": [], "entities": [{"text": "SAR14", "start_pos": 6, "end_pos": 11, "type": "DATASET", "confidence": 0.9018662571907043}, {"text": "Support Vector Regression", "start_pos": 25, "end_pos": 50, "type": "METRIC", "confidence": 0.8494494160016378}, {"text": "SV M light package 7", "start_pos": 76, "end_pos": 96, "type": "DATASET", "confidence": 0.7639311492443085}]}, {"text": "We then applied the learned model to predict real score values of reviews in the benchmark datasets, and referred to those values as the values of the feature RbF.", "labels": [], "entities": []}, {"text": "Although using N-gram features (consisting of unigrams, bigrams and trigrams) may give better results, we tend to use only unigrams for learning the regression model because of saving the training time on the large size of SAR14.", "labels": [], "entities": [{"text": "SAR14", "start_pos": 223, "end_pos": 228, "type": "DATASET", "confidence": 0.8650211095809937}]}, {"text": "Furthermore, using unigram features is good enough as presented in section 4.4.", "labels": [], "entities": []}, {"text": "To extract the RbF feature's value for each PL04's movie review, the regression model was trained with 20000 most fre-quent unigrams whilst 35000 most frequent unigrams were employed to learn regression model to estimate the RbF feature for each review in the dataset IMDB11.", "labels": [], "entities": [{"text": "IMDB11", "start_pos": 268, "end_pos": 274, "type": "DATASET", "confidence": 0.9297933578491211}]}, {"text": "shows the accuracy results of our method in comparison with other state-of-theart SVM-based performances on the dataset PL04.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9996917247772217}, {"text": "PL04", "start_pos": 120, "end_pos": 124, "type": "DATASET", "confidence": 0.7379307150840759}]}, {"text": "Our method achieves a baseline accuracy of 87.6% which is higher than baselines obtained by all other compared approaches.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.898387610912323}]}, {"text": "The accuracy based on only RbF feature is 88.2% being higher than those published in (.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9997244477272034}]}, {"text": "By exploiting a combination of unigram and RbF features, we gain a result at 89.8% which is comparable with the highest performances reached by;.", "labels": [], "entities": []}, {"text": "It is evident that rising from 87.6% to 89.8% proves the effectiveness of using RbF in sentiment polarity classification.", "labels": [], "entities": [{"text": "sentiment polarity classification", "start_pos": 87, "end_pos": 120, "type": "TASK", "confidence": 0.8970099290211996}]}], "tableCaptions": [{"text": " Table 1: Accuracy results (in %).", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9996806383132935}]}]}