{"title": [{"text": "Tuning a Grammar Correction System for Increased Precision", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper, we propose two enhancements to a statistical machine translation based approach to grammar correction for correcting all error categories.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 48, "end_pos": 79, "type": "TASK", "confidence": 0.6063143213589987}, {"text": "grammar correction", "start_pos": 98, "end_pos": 116, "type": "TASK", "confidence": 0.7365214973688126}]}, {"text": "First, we propose tuning the SMT systems to optimize a metric more suited to the grammar correction task (F-\u03b2 score) rather than the traditional BLEU metric used for tuning language translation tasks.", "labels": [], "entities": [{"text": "SMT", "start_pos": 29, "end_pos": 32, "type": "TASK", "confidence": 0.9899551868438721}, {"text": "grammar correction task", "start_pos": 81, "end_pos": 104, "type": "TASK", "confidence": 0.7473976115385691}, {"text": "F-\u03b2 score)", "start_pos": 106, "end_pos": 116, "type": "METRIC", "confidence": 0.9745941559473673}, {"text": "BLEU", "start_pos": 145, "end_pos": 149, "type": "METRIC", "confidence": 0.9935548901557922}, {"text": "tuning language translation tasks", "start_pos": 166, "end_pos": 199, "type": "TASK", "confidence": 0.703892856836319}]}, {"text": "Since the F-\u03b2 score favours higher precision, tuning to this score can potentially improve precision.", "labels": [], "entities": [{"text": "F-\u03b2 score", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9616480469703674}, {"text": "precision", "start_pos": 35, "end_pos": 44, "type": "METRIC", "confidence": 0.9985587000846863}, {"text": "precision", "start_pos": 91, "end_pos": 100, "type": "METRIC", "confidence": 0.9987758994102478}]}, {"text": "While the results do not indicate improvement due to tuning with the new metric, we believe this could be due to the small number of grammatical errors in the tuning corpus and further investigation is required to answer the question conclusively.", "labels": [], "entities": []}, {"text": "We also explore the combination of custom-engineered grammar correction techniques, which are targeted to specific error categories, with the SMT based method.", "labels": [], "entities": [{"text": "grammar correction", "start_pos": 53, "end_pos": 71, "type": "TASK", "confidence": 0.69309201836586}, {"text": "SMT", "start_pos": 142, "end_pos": 145, "type": "TASK", "confidence": 0.9898714423179626}]}, {"text": "Our simple ensemble methods yield improvements in recall but decrease the precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 50, "end_pos": 56, "type": "METRIC", "confidence": 0.9995442032814026}, {"text": "precision", "start_pos": 74, "end_pos": 83, "type": "METRIC", "confidence": 0.999178946018219}]}, {"text": "Tuning the custom-built techniques can help in increasing the overall accuracy also.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 70, "end_pos": 78, "type": "METRIC", "confidence": 0.9994263648986816}]}], "introductionContent": [{"text": "Grammatical Error Correction (GEC) is an interesting and challenging problem and the existing methods that attempt to solve this problem take recourse to deep linguistic and statistical analysis.", "labels": [], "entities": [{"text": "Grammatical Error Correction (GEC)", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.8722774038712183}]}, {"text": "In general, GEC may partly assist in solving natural language processing (NLP) tasks like Machine Translation, Natural Language Generation etc.", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 90, "end_pos": 109, "type": "TASK", "confidence": 0.8291392922401428}, {"text": "Natural Language Generation", "start_pos": 111, "end_pos": 138, "type": "TASK", "confidence": 0.629577616850535}]}, {"text": "However, a more evident application of GEC is in building automated grammar checkers thereby non-native speakers of a language.", "labels": [], "entities": [{"text": "GEC", "start_pos": 39, "end_pos": 42, "type": "TASK", "confidence": 0.8906986713409424}]}, {"text": "The goal is to have automated tools to help non-native speakers to generate good content by correcting grammatical errors made by them.", "labels": [], "entities": []}, {"text": "The) was focussed towards correcting some of the most frequent categories of grammatical errors.", "labels": [], "entities": []}, {"text": "In contrast, the) set the goal of correcting all grammatical errors in the text.", "labels": [], "entities": []}, {"text": "For correcting specific error categories, custom methods are generally developed, which exploit deep knowledge of the problem to perform the correction (.", "labels": [], "entities": [{"text": "correcting specific error categories", "start_pos": 4, "end_pos": 40, "type": "TASK", "confidence": 0.8168944269418716}]}, {"text": "These methods are generally the state-ofthe-art for the concerned error categories, but a lot of engineering and research effort is required for correcting each error category.", "labels": [], "entities": []}, {"text": "So, the custom development approach is infeasible for correcting a large number of error categories.", "labels": [], "entities": []}, {"text": "Hence, for correction of all the error categories, generic methods have been investigated -generally using language models or statistical machine translation (SMT) systems.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 126, "end_pos": 163, "type": "TASK", "confidence": 0.7764748086531957}]}, {"text": "The language model based method ( scores sentences based on a language model or count ratios of n-grams obtained from a large native text corpus.", "labels": [], "entities": []}, {"text": "But this method still needs a candidate generation mechanism for each error category.", "labels": [], "entities": []}, {"text": "On the other hand, the SMT based method) formulates the grammar correction problem as a problem of translation of incorrect sentences to correct sentences.", "labels": [], "entities": [{"text": "SMT", "start_pos": 23, "end_pos": 26, "type": "TASK", "confidence": 0.9929459691047668}, {"text": "grammar correction", "start_pos": 56, "end_pos": 74, "type": "TASK", "confidence": 0.7064685523509979}, {"text": "translation of incorrect sentences to correct sentences", "start_pos": 99, "end_pos": 154, "type": "TASK", "confidence": 0.8304732271603176}]}, {"text": "SMT provides a natural unsupervised method for identifying candidate corrections in the form of the translation model, and a method for scoring them with a variety of measures including the language model score.", "labels": [], "entities": [{"text": "SMT", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.9455060362815857}]}, {"text": "However, the SMT method requires a lot of parallel non-native learner corpora.", "labels": [], "entities": [{"text": "SMT", "start_pos": 13, "end_pos": 16, "type": "TASK", "confidence": 0.994990885257721}]}, {"text": "In addition, the machinery in phrase based SMT is optimized towards solving the language translation problem.", "labels": [], "entities": [{"text": "phrase based SMT", "start_pos": 30, "end_pos": 46, "type": "TASK", "confidence": 0.48724377155303955}, {"text": "language translation", "start_pos": 80, "end_pos": 100, "type": "TASK", "confidence": 0.7504116594791412}]}, {"text": "Therefore, the community has explored approaches to adapt the SMT method for grammar correction.", "labels": [], "entities": [{"text": "SMT", "start_pos": 62, "end_pos": 65, "type": "TASK", "confidence": 0.9930996894836426}, {"text": "grammar correction", "start_pos": 77, "end_pos": 95, "type": "TASK", "confidence": 0.8134247362613678}]}, {"text": "These include use of factored SMT, syntax based SMT, pruning of the phrase table, disabling or reordering, etc.", "labels": [], "entities": [{"text": "SMT", "start_pos": 30, "end_pos": 33, "type": "TASK", "confidence": 0.7873901128768921}, {"text": "SMT", "start_pos": 48, "end_pos": 51, "type": "TASK", "confidence": 0.886567234992981}]}, {"text": "The generic SMT approach has performed badly as compared to the specific custom made approaches.", "labels": [], "entities": [{"text": "SMT", "start_pos": 12, "end_pos": 15, "type": "TASK", "confidence": 0.9579273462295532}]}, {"text": "Our system also builds upon the SMT methods and tries to address the above mentioned lacunae in two ways: \u2022 Tuning the SMT model to a metric suitable for grammar correction (i.e.F-\u03b2 metric), instead of the BLEU metric.", "labels": [], "entities": [{"text": "SMT", "start_pos": 32, "end_pos": 35, "type": "TASK", "confidence": 0.9873546957969666}, {"text": "SMT", "start_pos": 119, "end_pos": 122, "type": "TASK", "confidence": 0.9815376996994019}, {"text": "grammar correction", "start_pos": 154, "end_pos": 172, "type": "TASK", "confidence": 0.7271285951137543}, {"text": "BLEU", "start_pos": 206, "end_pos": 210, "type": "METRIC", "confidence": 0.9967519044876099}]}, {"text": "\u2022 Combination of custom-engineered methods and SMT based methods, by using classifier based for some error categories.", "labels": [], "entities": [{"text": "SMT", "start_pos": 47, "end_pos": 50, "type": "TASK", "confidence": 0.9936178922653198}]}, {"text": "Section 2 describes our method for tuning the SMT system to optimize the F-\u03b2 metric.", "labels": [], "entities": [{"text": "SMT", "start_pos": 46, "end_pos": 49, "type": "TASK", "confidence": 0.9947834610939026}]}, {"text": "Section 3 explains the combination of classifier based method with the SMT method.", "labels": [], "entities": [{"text": "SMT", "start_pos": 71, "end_pos": 74, "type": "TASK", "confidence": 0.9891290068626404}]}, {"text": "Section 4 lists our experimental setup.", "labels": [], "entities": []}, {"text": "Section 5 analyzes the results of our experiments.", "labels": [], "entities": []}], "datasetContent": [{"text": "We used the NUCLE Corpus v3.1 to build a phrase based SMT system for grammar correction.", "labels": [], "entities": [{"text": "NUCLE Corpus v3.1", "start_pos": 12, "end_pos": 29, "type": "DATASET", "confidence": 0.9420138001441956}, {"text": "SMT", "start_pos": 54, "end_pos": 57, "type": "TASK", "confidence": 0.854020357131958}, {"text": "grammar correction", "start_pos": 69, "end_pos": 87, "type": "TASK", "confidence": 0.7924226820468903}]}, {"text": "The NUCLE Corpus contains 28 error categories, whose details are documented in.", "labels": [], "entities": [{"text": "NUCLE Corpus", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.9789609313011169}]}, {"text": "We split the corpus into training, tuning and test sets are shown in.: Details of data split for SMT training", "labels": [], "entities": [{"text": "SMT training", "start_pos": 97, "end_pos": 109, "type": "TASK", "confidence": 0.9374671876430511}]}], "tableCaptions": [{"text": " Table 1: Details of data split for SMT training", "labels": [], "entities": [{"text": "SMT training", "start_pos": 36, "end_pos": 48, "type": "TASK", "confidence": 0.944164365530014}]}, {"text": " Table 2: Experimental Results for various configurations on the development set", "labels": [], "entities": []}, {"text": " Table 3: Experimental Results for various configurations on the CoNLL-2014 test set without alternatives", "labels": [], "entities": [{"text": "CoNLL-2014 test set", "start_pos": 65, "end_pos": 84, "type": "DATASET", "confidence": 0.9811708132425944}]}]}