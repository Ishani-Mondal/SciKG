{"title": [{"text": "(Digital) Goodies from the ERC Wishing Well: BabelNet, Babelfy, Video Games with a Purpose and the Wikipedia Bitaxonomy", "labels": [], "entities": [{"text": "ERC Wishing Well", "start_pos": 27, "end_pos": 43, "type": "DATASET", "confidence": 0.8472006519635519}, {"text": "Wikipedia Bitaxonomy", "start_pos": 99, "end_pos": 119, "type": "DATASET", "confidence": 0.8857517540454865}]}], "abstractContent": [{"text": "Multilinguality is a key feature of today's Web, and it is this feature that we leverage and exploit in our research work at the Sapienza University of Rome's Linguistic Computing Laboratory, which I am going to overview and showcase in this talk.", "labels": [], "entities": []}, {"text": "I will start by presenting BabelNet 2.5 (Navigli and Ponzetto, 2012), available at http://babelnet.org, a very large multilingual encyclopedic dictionary and semantic network , which covers 50 languages and provides both lexicographic and encyclopedic knowledge for all the open-class parts of speech, thanks to the seamless integration of WordNet, Wikipedia, Wiktionary, OmegaWiki, Wikidata and the Open Multilingual WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 340, "end_pos": 347, "type": "DATASET", "confidence": 0.9349072575569153}]}, {"text": "In order to construct the BabelNet network, we extract at different stages: from WordNet, all available word senses (as concepts) and all the lexical and semantic pointers between synsets (as relations); from Wikipedia, all the Wikipages (i.e., Wikipages, as concepts) and semantically unspecified relations from their hyperlinks.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 81, "end_pos": 88, "type": "DATASET", "confidence": 0.9398321509361267}]}, {"text": "WordNet and Wikipedia overlap both in terms of concepts and relations: this overlap makes the merging between the two resources possible, enabling the creation of a unified knowledge resource.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9383865594863892}]}, {"text": "In order to enable multilinguality, we collect the lexical realizations of the available concepts in different languages.", "labels": [], "entities": []}, {"text": "Finally, we connect the multilingual Babel synsets by establishing semantic relations between them.", "labels": [], "entities": []}, {"text": "Next, I will present Babelfy (Moro et al., 2014), available at http://babelfy.org, a unified approach that leverages BabelNet to perform Word Sense Disambiguation (WSD) and Entity Linking in arbitrary languages, with performance on both tasks on a par with, or surpassing, those of task-specific state-of-the-art supervised systems.", "labels": [], "entities": [{"text": "Word Sense Disambiguation (WSD)", "start_pos": 137, "end_pos": 168, "type": "TASK", "confidence": 0.7256531765063604}, {"text": "Entity Linking", "start_pos": 173, "end_pos": 187, "type": "TASK", "confidence": 0.7213608473539352}]}, {"text": "Babelfy works in three steps: first, given a lexicalized semantic network, we associate with each vertex, i.e., either concept or named entity, a semantic signature, that is, a set of related vertices.", "labels": [], "entities": []}, {"text": "This is a preliminary step which needs to be performed only once, independently of the input text.", "labels": [], "entities": []}, {"text": "Second, given a text, we extract all the linkable fragments from this text and, for each of them, list the possible meanings according to the semantic network.", "labels": [], "entities": []}, {"text": "Third, we create a graph-based semantic interpretation of the whole text by linking the candidate meanings of the extracted fragments using the previously-computed semantic signatures.", "labels": [], "entities": []}, {"text": "We then extract a dense subgraph of this representation and select the best candidate meaning for each fragment.", "labels": [], "entities": []}, {"text": "Our experiments show state-of-the-art performances on both WSD and EL on 6 different datasets, including a multilingual setting.", "labels": [], "entities": [{"text": "WSD", "start_pos": 59, "end_pos": 62, "type": "DATASET", "confidence": 0.6562590599060059}]}, {"text": "In the third part of the talk I will present two novel approaches to large-scale knowledge acquisition and validation developed in my lab.", "labels": [], "entities": [{"text": "knowledge acquisition", "start_pos": 81, "end_pos": 102, "type": "TASK", "confidence": 0.6862989962100983}]}, {"text": "I will first introduce video games with a purpose (Vannella et al., 2014), a novel, powerful paradigm for the large scale acquisition and validation of knowledge and data (http://knowledgeforge.org).", "labels": [], "entities": [{"text": "large scale acquisition and validation of knowledge and data", "start_pos": 110, "end_pos": 170, "type": "TASK", "confidence": 0.7634833521313138}]}, {"text": "We demonstrate that converting games with a purpose into more traditional video games provides a fun component that motivates players to annotate for free, thereby significantly lowering annotation costs below that of crowdsourcing.", "labels": [], "entities": []}, {"text": "Moreover, we show that video games with a purpose produce higher-quality annotations than crowdsourcing.", "labels": [], "entities": []}, {"text": "This work is licensed under a Creative Commons Attribution 4.0 International Licence.", "labels": [], "entities": []}, {"text": "Page numbers and proceedings footer are added by the organisers.", "labels": [], "entities": []}, {"text": "Licence details: http://creativecommons.org/licenses/by/4.0/ 75 Then I will introduce the Wikipedia Bitaxonomy (Flati et al., 2014, WiBi), available at http://wibitaxonomy.org and now integrated into BabelNet.", "labels": [], "entities": []}, {"text": "WiBi is the largest and most accurate currently available taxonomy of Wikipedia pages and taxonomy of categories, aligned to each other.", "labels": [], "entities": [{"text": "WiBi", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8511601686477661}]}, {"text": "WiBi is created in three steps: we first create a taxonomy for the Wikipedia pages by parsing textual definitions, extracting the hypernym(s) and disambiguating them according to the page inventory; next, we leverage the hypernyms in the page taxonomy, together with their links to the corresponding categories, so as to induce a taxonomy over Wikipedia categories while at the same time improving the page taxonomy in an iterative way; finally we employ structural heuristics to overcome inherent problems affecting categories.", "labels": [], "entities": []}, {"text": "The output of our three-phase approach is a bitaxonomy of millions of pages and hundreds of thousands of categories for the English Wikipedia.", "labels": [], "entities": [{"text": "English Wikipedia", "start_pos": 124, "end_pos": 141, "type": "DATASET", "confidence": 0.891819953918457}]}], "introductionContent": [], "datasetContent": [], "tableCaptions": []}