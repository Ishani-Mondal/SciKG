{"title": [{"text": "The SJTU System for Dialog State Tracking Challenge 2", "labels": [], "entities": [{"text": "Dialog State Tracking Challenge", "start_pos": 20, "end_pos": 51, "type": "TASK", "confidence": 0.8895895779132843}]}], "abstractContent": [{"text": "Dialog state tracking challenge provides a common testbed for state tracking algorithms.", "labels": [], "entities": [{"text": "Dialog state tracking challenge", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.7268876060843468}, {"text": "state tracking", "start_pos": 62, "end_pos": 76, "type": "TASK", "confidence": 0.76559978723526}]}, {"text": "This paper describes the SJTU system submitted to the second Dialogue State Tracking Challenge in detail.", "labels": [], "entities": [{"text": "SJTU", "start_pos": 25, "end_pos": 29, "type": "TASK", "confidence": 0.9414234161376953}, {"text": "Dialogue State Tracking Challenge", "start_pos": 61, "end_pos": 94, "type": "TASK", "confidence": 0.7226758375763893}]}, {"text": "In the system, a statistical semantic parser is used to generate refined semantic hypotheses.", "labels": [], "entities": []}, {"text": "A large number of features are then derived based on the semantic hypotheses and the dialogue log information.", "labels": [], "entities": []}, {"text": "The final tracker is a combination of a rule-based model, a maximum entropy and a deep neural network model.", "labels": [], "entities": []}, {"text": "The SJTU system significantly outperformed all the baselines and showed competitive performance in DSTC 2.", "labels": [], "entities": [{"text": "DSTC 2", "start_pos": 99, "end_pos": 105, "type": "TASK", "confidence": 0.5970828384160995}]}], "introductionContent": [{"text": "Dialog state tracking is important because spoken dialog systems rely on it to choose proper actions as spoken dialog systems interact with users.", "labels": [], "entities": [{"text": "Dialog state tracking", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.8945126334826151}]}, {"text": "However, due to automatic speech recognition (ASR) and spoken language understanding (SLU) errors, it is not easy for the dialog manager to maintain the true state of the dialog.", "labels": [], "entities": [{"text": "speech recognition (ASR)", "start_pos": 26, "end_pos": 50, "type": "TASK", "confidence": 0.8292589664459229}, {"text": "spoken language understanding (SLU)", "start_pos": 55, "end_pos": 90, "type": "TASK", "confidence": 0.7170379459857941}]}, {"text": "In recent years, much research has been devoted to dialog state tracking.", "labels": [], "entities": [{"text": "dialog state tracking", "start_pos": 51, "end_pos": 72, "type": "TASK", "confidence": 0.9018253087997437}]}, {"text": "Many approaches have been applied to dialog state tracking, from rule-based to statistical models, from generative models to discriminative models (.", "labels": [], "entities": [{"text": "dialog state tracking", "start_pos": 37, "end_pos": 58, "type": "TASK", "confidence": 0.88315349817276}]}, {"text": "Recently, shared research tasks like the first Dialog State Tracking Challenge (DSTC 1) () have provided a common testbed and evaluation suite for dialog state tracking ().", "labels": [], "entities": [{"text": "Dialog State Tracking Challenge (DSTC 1)", "start_pos": 47, "end_pos": 87, "type": "TASK", "confidence": 0.7626000344753265}, {"text": "dialog state tracking", "start_pos": 147, "end_pos": 168, "type": "TASK", "confidence": 0.7829434474309286}]}, {"text": "Compared with DSTC 1 which is in the bus timetables domain, DSTC 2 introduces more complicated and dynamic dialog states, which may change through the dialog, in anew domain, i.e. restaurants domain ().", "labels": [], "entities": []}, {"text": "For each turn, a tracker is supposed to output a set of distributions for each of the three components of the dialog state: goals, method, and requested slots.", "labels": [], "entities": []}, {"text": "At a given turn, the goals consists of the user's true required value having been revealed for each slot in the dialog up until that turn; the method is the way the user is trying to interact with the system which maybe by name, by constraints, by alternatives or finished; and the requested slots consist of the slots which have been requested by the user and not replied by the system.", "labels": [], "entities": []}, {"text": "For evaluation in DSTC 2, 1-best quality measured by accuracy, probability calibration measured by L2, and discrimination measured by ROC are selected as featured metrics.", "labels": [], "entities": [{"text": "DSTC 2", "start_pos": 18, "end_pos": 24, "type": "DATASET", "confidence": 0.6913983821868896}, {"text": "accuracy", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.9992474317550659}, {"text": "ROC", "start_pos": 134, "end_pos": 137, "type": "METRIC", "confidence": 0.9787295460700989}]}, {"text": "Further details can be found in the DSTC 2 handbook (.", "labels": [], "entities": [{"text": "DSTC 2 handbook", "start_pos": 36, "end_pos": 51, "type": "DATASET", "confidence": 0.9381858309110006}]}, {"text": "Previous research has demonstrated the effectiveness of rule-based (, maximum entropy (MaxEnt) ( and deep neural network (DNN) () models separately.", "labels": [], "entities": []}, {"text": "Motivated by this, the SJTU system employs a combination of a rulebased model, a MaxEnt and a DNN model.", "labels": [], "entities": []}, {"text": "The three models were first trained (if necessary) on the training set and tested for each of the three components of the dialog state, i.e goals, method, and requested slots on the development set.", "labels": [], "entities": []}, {"text": "Then, models with the best performance for each of the three components were selected to form a combined model.", "labels": [], "entities": []}, {"text": "Finally, the combined model was retrained using both training set and development set.", "labels": [], "entities": []}, {"text": "Additionally, as the live SLU was found not good enough with some information lost compared with the live ASR, anew semantic parser was implemented which took the live ASR as input and the SJTU system used the result from the new semantic parser instead of the live SLU.", "labels": [], "entities": []}, {"text": "The remainder of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes the design of the new semantic parser.", "labels": [], "entities": []}, {"text": "Section 3 presents the rule-based model.", "labels": [], "entities": []}, {"text": "Section 4 describes the statistical models including the maximum entropy model and the deep neural network model.", "labels": [], "entities": []}, {"text": "Section 5 shows and discusses the performance of the SJTU system.", "labels": [], "entities": [{"text": "SJTU", "start_pos": 53, "end_pos": 57, "type": "TASK", "confidence": 0.8985179662704468}]}, {"text": "Finally, section 6 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "DSTC 2 provides a training dataset of 1612 dialogues (11677 utterances) and a development set of 506 dialogues (3934 utterances).", "labels": [], "entities": [{"text": "DSTC 2", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9463360905647278}]}, {"text": "The training data was first used to train the semantic parser and the MaxEnt and the DNN models for internal system development as shown in section 5.1 and 5.2.", "labels": [], "entities": [{"text": "MaxEnt", "start_pos": 70, "end_pos": 76, "type": "DATASET", "confidence": 0.9224971532821655}]}, {"text": "These systems were tested on the development data.", "labels": [], "entities": []}, {"text": "Once the system setup and parameters were determined, the training and development set were combined together to train the final submitted system.", "labels": [], "entities": []}, {"text": "The final system was then tested on the final evaluation data as shown in section 5.3.", "labels": [], "entities": []}, {"text": "The official results of the challenge are publicly available and the SJTU team is team 7.", "labels": [], "entities": []}, {"text": "Entry 0, 1, 2, 3 of team 7 is the combined model, the rule-based model, the DNN model and the MaxEnt model respectively.", "labels": [], "entities": [{"text": "MaxEnt", "start_pos": 94, "end_pos": 100, "type": "DATASET", "confidence": 0.7524068355560303}]}, {"text": "They all used the new semantic parser based on live ASR hypotheses.", "labels": [], "entities": [{"text": "ASR", "start_pos": 52, "end_pos": 55, "type": "TASK", "confidence": 0.9378638863563538}]}, {"text": "Entry 4 of team 7 is also a combined model but it does not use the new semantic parser and takes the live SLU as input.", "labels": [], "entities": []}, {"text": "shows the results on the final evaluation test set.", "labels": [], "entities": []}, {"text": "As expected, the semantic parser does work, and the combined model has the best performance for joint goals and method, however, that is not true for requested slots.", "labels": [], "entities": []}, {"text": "Notice that on the development set, the difference of the accuracy of requested slots among the 3 models is significantly smaller than that of joint goals and method.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.9991098046302795}]}, {"text": "One reasonable explanation is that one cannot claim that the rule-based model has better performance for requested slots than the MaxEnt model and the DNN model only with an accuracy advantage less than 0.1%.: Accuracy of the combined model (Combined+) compared with the rule-based model, the MaxEnt model, the DNN model, the combined model without the new semantic parser (Combined-) and four baselines on the test set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 174, "end_pos": 182, "type": "METRIC", "confidence": 0.9989954829216003}]}, {"text": "Four baselines are the baseline tracker (Baseline), the focus tracker (Focus), the HWU tracker (HWU) and the HWU tracker with \"original\" flag set to (HWU+) respectively.", "labels": [], "entities": []}, {"text": "As ROC metric is only comparable between systems of similar accuracy, only accuracy and L2 are compared.", "labels": [], "entities": [{"text": "ROC metric", "start_pos": 3, "end_pos": 13, "type": "METRIC", "confidence": 0.9570739567279816}, {"text": "accuracy", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.9946132302284241}, {"text": "accuracy", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.9996962547302246}, {"text": "L2", "start_pos": 88, "end_pos": 90, "type": "METRIC", "confidence": 0.9429495334625244}]}, {"text": "The results of the combined model is competitive for all the three components, especially for joint goals.", "labels": [], "entities": []}, {"text": "Two strategies and two kinds of features were added to the MaxEnt model for the requested slots after DSTC 2 based on some observations on the training set and development set.", "labels": [], "entities": [{"text": "DSTC 2", "start_pos": 102, "end_pos": 108, "type": "DATASET", "confidence": 0.8716188073158264}]}, {"text": "The first strategy is that the output for the requested slots of the first turn is set to empty by force.", "labels": [], "entities": []}, {"text": "The second strategy is that the output of the confidence is additionally multiplied by (1\u2212C f ), where Cf denotes the confidence given by the MaxEnt model to the method of current turn being finished.", "labels": [], "entities": []}, {"text": "As for the two kinds of features, one is the slot indicator and the other is the acttype-slot tuple.", "labels": [], "entities": []}, {"text": "They are defined as 2 : \u2022 f 5 (f 5,1 , f 5,2 , \u00b7 \u00b7 \u00b7 , f ), where f 5,j slot indicator(i, r, s j ) = 1 if the index of the slot r is j, i.e. s j = r, otherwise 0.", "labels": [], "entities": []}, {"text": "\u2022 f 6 (f 6,1 , f 6,2 , \u00b7 \u00b7 \u00b7 , f ), where f 6,j user act slot(i, r, t j ) = the sum of all the scores assigned by the SLU to the j-th user acttype-slot tuple t j . The acttype-slot tuple is the combination of dialog act type and possible slot, e.g. inf orm-f ood, conf irm-area.", "labels": [], "entities": []}, {"text": "There are 33 user acttype-slot tuples.", "labels": [], "entities": []}, {"text": "The feature number is consistent with that in section 4.1.", "labels": [], "entities": []}, {"text": "\u2022 f 7 (f 7,1 , f 7,2 , \u00b7 \u00b7 \u00b7 , f 7,46 ), where f 7,j sys act slot(i, r, t j ) = the number of occurrences of the j-th machine acttype-slot tuple t j in the dialog acts.", "labels": [], "entities": []}, {"text": "There are 46 machine acttype-slot tuples.", "labels": [], "entities": []}, {"text": "With those strategies and features, the MaxEnt model achieved an accuracy of 0.9769 for the requested slots, which is significantly improved compared with the submitted system.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.9993891716003418}]}], "tableCaptions": [{"text": " Table 1: Performance of semantic parsers with dif- ferent features on the development set.", "labels": [], "entities": []}, {"text": " Table 2: Results for focus baseline tracker with  different parsers", "labels": [], "entities": []}, {"text": " Table 4: Accuracy of the combined model (Com- bined+) compared with the rule-based model,  the MaxEnt model, the DNN model, the com- bined model without the new semantic parser  (Combined-) and four baselines on the test set.  Four baselines are the baseline tracker (Base- line), the focus tracker (Focus), the HWU tracker  (HWU) and the HWU tracker with \"original\" flag  set to (HWU+) respectively.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9797765612602234}]}]}