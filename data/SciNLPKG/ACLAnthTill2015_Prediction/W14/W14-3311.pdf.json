{"title": [{"text": "Phrasal: A Toolkit for New Directions in Statistical Machine Translation", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 41, "end_pos": 72, "type": "TASK", "confidence": 0.8361734747886658}]}], "abstractContent": [{"text": "We present anew version of Phrasal, an open-source toolkit for statistical phrase-based machine translation.", "labels": [], "entities": [{"text": "statistical phrase-based machine translation", "start_pos": 63, "end_pos": 107, "type": "TASK", "confidence": 0.6501650586724281}]}, {"text": "This revision includes features that support emerging research trends such as (a) tuning with large feature sets, (b) tuning on large datasets like the bitext, and (c) web-based interactive machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 190, "end_pos": 209, "type": "TASK", "confidence": 0.6674833446741104}]}, {"text": "A direct comparison with Moses shows favorable results in terms of decoding speed and tuning time.", "labels": [], "entities": []}], "introductionContent": [{"text": "In the early part of the last decade, phrase-based machine translation (MT) () emerged as the preeminent design of statistical MT systems.", "labels": [], "entities": [{"text": "phrase-based machine translation (MT)", "start_pos": 38, "end_pos": 75, "type": "TASK", "confidence": 0.7932591537634531}, {"text": "MT", "start_pos": 127, "end_pos": 129, "type": "TASK", "confidence": 0.74822598695755}]}, {"text": "However, most systems were proprietary or closedsource, so progress was initially constrained by the high engineering barrier to entry into the field.", "labels": [], "entities": []}, {"text": "Then Moses () was released.", "labels": [], "entities": []}, {"text": "What followed was a flowering of work on all aspects of the translation problem, from rule extraction to deployment issues.", "labels": [], "entities": [{"text": "translation problem", "start_pos": 60, "end_pos": 79, "type": "TASK", "confidence": 0.9449847042560577}, {"text": "rule extraction", "start_pos": 86, "end_pos": 101, "type": "TASK", "confidence": 0.8819915056228638}, {"text": "deployment", "start_pos": 105, "end_pos": 115, "type": "TASK", "confidence": 0.9604913592338562}]}, {"text": "Other toolkits appeared including Joshua (), Jane), cdec and the first version of our package,), a Java-based, open source package.", "labels": [], "entities": []}, {"text": "This paper presents a completely re-designed release of Phrasal that lowers the barrier to entry into several exciting areas of MT research.", "labels": [], "entities": [{"text": "Phrasal", "start_pos": 56, "end_pos": 63, "type": "DATASET", "confidence": 0.8044495582580566}, {"text": "MT research", "start_pos": 128, "end_pos": 139, "type": "TASK", "confidence": 0.9572984576225281}]}, {"text": "First, Phrasal exposes a simple yet flexible feature API for building large-scale, feature-rich systems.", "labels": [], "entities": [{"text": "Phrasal", "start_pos": 7, "end_pos": 14, "type": "DATASET", "confidence": 0.8802153468132019}]}, {"text": "Second, Phrasal provides multi-threaded decoding and online tuning for learning feature-rich models on very large datasets, including the bitext.", "labels": [], "entities": []}, {"text": "Third, Phrasal supplies the key ingredients for web-based, interactive MT: an asynchronous RESTful JSON web service implemented as a J2EE servlet, integrated pre-and post-processing, and fast search.", "labels": [], "entities": [{"text": "MT", "start_pos": 71, "end_pos": 73, "type": "TASK", "confidence": 0.9661572575569153}]}, {"text": "Revisions to Phrasal were guided by several design choices.", "labels": [], "entities": [{"text": "Phrasal", "start_pos": 13, "end_pos": 20, "type": "DATASET", "confidence": 0.7670952081680298}]}, {"text": "First, we optimized the system for multi-core architectures, eschewing distributed infrastructure like Hadoop and MapReduce.", "labels": [], "entities": []}, {"text": "While \"scaling-out\" with distributed infrastructure is the conventional industry and academic choice, we find that \"scaling-up\" on a single large-node is an attractive yet overlooked alternative (.", "labels": [], "entities": []}, {"text": "A single \"scale-up\" node is usually competitive in terms of cost and performance, and multi-core code has fewer dependencies in terms of software and expertise.", "labels": [], "entities": []}, {"text": "Second, Phrasal makes extensive use of Java interfaces and reflection.", "labels": [], "entities": []}, {"text": "This is especially helpful in the feature API.", "labels": [], "entities": []}, {"text": "A feature function can be added to the system by simply implementing an interface and specifying the class name on the decoder command line.", "labels": [], "entities": []}, {"text": "There is no need to modify or recompile anything other than the new feature function.", "labels": [], "entities": []}, {"text": "This paper presents a direct comparison of Phrasal and Moses that shows favorable results in terms of decoding speed and tuning time.", "labels": [], "entities": []}, {"text": "An indirect comparison via the WMT2014 shared task showed that Phrasal compares favorably to Moses in an evaluation setting.", "labels": [], "entities": [{"text": "WMT2014 shared task", "start_pos": 31, "end_pos": 50, "type": "DATASET", "confidence": 0.8091492652893066}]}, {"text": "The source code is freely available at: http://nlp.stanford.edu/software/phrasal/", "labels": [], "entities": []}], "datasetContent": [{"text": "All of the error metrics available for tuning can also be invoked for evaluation.", "labels": [], "entities": []}, {"text": "For significance testing, the toolkit includes an implementation of the permutation test of, which was shown to be less susceptible to Type-I error than bootstrap re-sampling: Phrase-based MT as deductive inference.", "labels": [], "entities": [{"text": "significance testing", "start_pos": 4, "end_pos": 24, "type": "TASK", "confidence": 0.9490365087985992}, {"text": "MT", "start_pos": 189, "end_pos": 191, "type": "TASK", "confidence": 0.8122245669364929}]}, {"text": "This notation can be read as follows: if the antecedents on the top are true, then the consequent on the bottom is true subject to the conditions on the right.", "labels": [], "entities": []}, {"text": "The new item dis creating by appending r to the ordered sequence of rules that defined.", "labels": [], "entities": []}, {"text": "Phrasal also includes two truecasing packages.", "labels": [], "entities": [{"text": "Phrasal", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9536818861961365}]}, {"text": "The LM-based truecaser () requires an LM estimated from cased, tokenized text.", "labels": [], "entities": []}, {"text": "A subsequent detokenization step is thus necessary.", "labels": [], "entities": []}, {"text": "A more convenient alternative is the CRF-based postprocessor that can be trained to invert an arbitrary pre-processor.", "labels": [], "entities": []}, {"text": "This post-processor can perform truecasing and detokenization in one pass.", "labels": [], "entities": []}, {"text": "We compare Phrasal and Moses by restricting an existing large-scale system to a set of common features.", "labels": [], "entities": []}, {"text": "We start with the Arabic-English system of , which is built from 6.6M parallel segments.", "labels": [], "entities": []}, {"text": "The system includes a 5-gram English LM estimated from the target-side of the bitext and 990M English monolingual tokens.", "labels": [], "entities": []}, {"text": "The feature set is their dense baseline, but without lexicalized reordering and the two extended phrase table features.", "labels": [], "entities": []}, {"text": "This leaves the nine baseline features also implemented by Moses.", "labels": [], "entities": []}, {"text": "We use the same phrase table, phrase table query limit (20), and distortion limit (5) for both decoders.", "labels": [], "entities": [{"text": "distortion limit (5)", "start_pos": 65, "end_pos": 85, "type": "METRIC", "confidence": 0.9645237803459168}]}, {"text": "The tuning set (mt023568) contains 5,604 segments, and the development set (mt04) contains 1,075 segments.", "labels": [], "entities": []}, {"text": "We ran all experiments on a dedicated server with 16 physical cores and 128GB of memory.", "labels": [], "entities": []}, {"text": "shows single-threaded decoding time of the dev set as a function of the cube pruning pop limit.", "labels": [], "entities": []}, {"text": "At very low limits Moses is faster than Phrasal, but then slows sharply.", "labels": [], "entities": []}, {"text": "In contrast, Phrasal scales linearly and is thus faster at higher pop limits.", "labels": [], "entities": []}, {"text": "shows multi-threaded decoding time of the dev set with the cube pruning pop limit fixed at 1,200.", "labels": [], "entities": []}, {"text": "Here Phrasal is initially faster, but Moses becomes more efficient at four threads.", "labels": [], "entities": []}, {"text": "There are two possible explanations.", "labels": [], "entities": []}, {"text": "First, profiling shows that LM queries account for approximately 75% of the Phrasal CPU-time.", "labels": [], "entities": [{"text": "Phrasal CPU-time", "start_pos": 76, "end_pos": 92, "type": "DATASET", "confidence": 0.9148268699645996}]}, {"text": "KenLM is written in C++, and Phrasal queries it via JNI.", "labels": [], "entities": []}, {"text": "It appears as though multi-threading across this boundary is a source of inefficiency.", "labels": [], "entities": []}, {"text": "Second, we observe that the Java parallel garbage collector (GC) runs up to seven threads, which become increasingly active as the number of decoder threads increases.", "labels": [], "entities": []}, {"text": "These and other Java overhead threads must be scheduled, limiting gains as the number of decoding threads approaches the number of physical cores.", "labels": [], "entities": []}, {"text": "Finally, shows tuning BLEU as a function of wallclock time.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 22, "end_pos": 26, "type": "METRIC", "confidence": 0.9075165390968323}]}, {"text": "For Moses we chose the batch MIRA implementation of Cherry and Foster (2012), which is popular for tuning feature-rich systems.", "labels": [], "entities": [{"text": "MIRA implementation of Cherry and Foster (2012)", "start_pos": 29, "end_pos": 76, "type": "DATASET", "confidence": 0.7045238779650794}]}, {"text": "Phrasal uses the online tuner with the expected BLEU objective ( ).", "labels": [], "entities": [{"text": "Phrasal", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9351891279220581}, {"text": "BLEU", "start_pos": 48, "end_pos": 52, "type": "METRIC", "confidence": 0.9992349147796631}]}, {"text": "Moses achieves a maximum BLEU score of 47.63 after 143 minutes of tuning, while Phrasal reaches this level after just 17 minutes, later reaching a maximum BLEU of 47.75 after 42 minutes.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 25, "end_pos": 35, "type": "METRIC", "confidence": 0.9887827932834625}, {"text": "BLEU", "start_pos": 155, "end_pos": 159, "type": "METRIC", "confidence": 0.9993001222610474}]}, {"text": "Much of the speedup can be attributed to phrase table and LM loading time: the Phrasal tuner loads these data structures just once, while the Moses tuner loads them every epoch.", "labels": [], "entities": [{"text": "Phrasal tuner", "start_pos": 79, "end_pos": 92, "type": "DATASET", "confidence": 0.8704764544963837}]}, {"text": "Of course, this loading time becomes more significant with larger-scale systems.", "labels": [], "entities": []}], "tableCaptions": []}