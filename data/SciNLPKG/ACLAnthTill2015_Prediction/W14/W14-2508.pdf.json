{"title": [{"text": "Fact Checking: Task definition and dataset construction", "labels": [], "entities": [{"text": "Fact Checking", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.8679464757442474}, {"text": "Task definition", "start_pos": 15, "end_pos": 30, "type": "TASK", "confidence": 0.69339619576931}]}], "abstractContent": [{"text": "In this paper we introduce the task of fact checking, i.e. the assessment of the truthfulness of a claim.", "labels": [], "entities": [{"text": "fact checking", "start_pos": 39, "end_pos": 52, "type": "TASK", "confidence": 0.787710428237915}]}, {"text": "The task is commonly performed manually by journalists verifying the claims made by public figures.", "labels": [], "entities": []}, {"text": "Furthermore , ordinary citizens need to assess the truthfulness of the increasing volume of statements they consume.", "labels": [], "entities": []}, {"text": "Thus, developing fact checking systems is likely to be of use to various members of society.", "labels": [], "entities": [{"text": "fact checking", "start_pos": 17, "end_pos": 30, "type": "TASK", "confidence": 0.915298730134964}]}, {"text": "We first define the task and detail the construction of a publicly available dataset using statements fact-checked by journalists available online.", "labels": [], "entities": []}, {"text": "Then, we discuss baseline approaches for the task and the challenges that need to be addressed.", "labels": [], "entities": []}, {"text": "Finally , we discuss how fact checking relates to mainstream natural language processing tasks and can stimulate further research.", "labels": [], "entities": [{"text": "fact checking", "start_pos": 25, "end_pos": 38, "type": "TASK", "confidence": 0.9637345671653748}, {"text": "natural language processing tasks", "start_pos": 61, "end_pos": 94, "type": "TASK", "confidence": 0.7384967803955078}]}], "introductionContent": [], "datasetContent": [{"text": "In order to construct a dataset to develop and evaluate approaches to fact checking, we first surveyed popular fact checking websites.", "labels": [], "entities": [{"text": "fact checking", "start_pos": 70, "end_pos": 83, "type": "TASK", "confidence": 0.8884569704532623}, {"text": "fact checking websites", "start_pos": 111, "end_pos": 133, "type": "TASK", "confidence": 0.77208345135053}]}, {"text": "We decided to consider statements from two of them, the fact checking blog of Channel 4 5 and the Truth-OMeter from PolitiFact.", "labels": [], "entities": [{"text": "Channel 4 5", "start_pos": 78, "end_pos": 89, "type": "DATASET", "confidence": 0.900607168674469}, {"text": "Truth-OMeter from PolitiFact", "start_pos": 98, "end_pos": 126, "type": "DATASET", "confidence": 0.8618614077568054}]}, {"text": "Both websites have large archives of fact-checked statements (more than 1,000 statements each), they cover a wide range of prevalent issues of U.K. and U.S. public life, and they provide detailed verdicts with fine-grained labels such as MOSTLYFALSE and HALFTRUE.", "labels": [], "entities": [{"text": "MOSTLYFALSE", "start_pos": 238, "end_pos": 249, "type": "METRIC", "confidence": 0.9198799729347229}, {"text": "HALFTRUE", "start_pos": 254, "end_pos": 262, "type": "METRIC", "confidence": 0.9528362154960632}]}, {"text": "We examined recent fact-checks from each website at the time of writing.", "labels": [], "entities": []}, {"text": "For each statement, apart from the statement itself, we recorded the date it was made, the speaker, the label of the verdict and the URL.", "labels": [], "entities": []}, {"text": "As the two websites use different labelling schemes, we aligned the labels of the verdicts to a five-point scale: TRUE, MOSTLYTRUE, HALFTRUE, MOSTLYFALSE and FALSE.", "labels": [], "entities": [{"text": "TRUE", "start_pos": 114, "end_pos": 118, "type": "METRIC", "confidence": 0.9964612126350403}, {"text": "MOSTLYTRUE", "start_pos": 120, "end_pos": 130, "type": "METRIC", "confidence": 0.8810222148895264}, {"text": "HALFTRUE", "start_pos": 132, "end_pos": 140, "type": "METRIC", "confidence": 0.9922123551368713}, {"text": "MOSTLYFALSE", "start_pos": 142, "end_pos": 153, "type": "METRIC", "confidence": 0.5348589420318604}, {"text": "FALSE", "start_pos": 158, "end_pos": 163, "type": "METRIC", "confidence": 0.9842466115951538}]}, {"text": "The speakers included, apart from public figures, associations such as the American Beverage Association, activists, even viral FaceBook posts submitted by the public.", "labels": [], "entities": []}, {"text": "We then decided which of the statements should be considered for the task proposed.", "labels": [], "entities": []}, {"text": "As discussed in the previous section we want to avoid statements that cannot be assessed objectively.", "labels": [], "entities": []}, {"text": "Following this, we deemed unsuitable statements: \u2022 assessing causal relations, e.g. whether a statistic should be attributed to a particular law \u2022 concerning the future, e.g. speculations involving oil prices \u2022 not concerning facts, e.g. whether a politician is supporting certain policies For the statements that were considered suitable, we also collected the sources used by the journalists in the analysis provided for the verdict.", "labels": [], "entities": []}, {"text": "Common sources include tables with statistics and reports from governments, think tanks and other organisations, available online.", "labels": [], "entities": []}, {"text": "Automatic identification of the sources needed to fact check a statement is an important stage in the process, which is potentially useful in its own right in the context of assisting journalists in a semi-automated factchecking approach times the verdicts relied on data that were not available online such personal communications; statements whose verdict relied on such data were also deemed unsuitable for the task.", "labels": [], "entities": []}, {"text": "As mentioned earlier, the verdicts on the websites are accompanied by lengthy analyses.", "labels": [], "entities": []}, {"text": "While such analyses could be useful annotation for intermediate stages of the task -e.g. we could use it as supervision to learn how to combine the information extracted from the various sources into a verdict -we noticed that the language used in them is indicative of the verdict.", "labels": [], "entities": []}, {"text": "Thus we decided not to include them in the dataset, as it would enable tackling part of the task as sentiment analysis.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 100, "end_pos": 118, "type": "TASK", "confidence": 0.9569478929042816}]}, {"text": "Out of the 221 fact-checked statements examined, we judged 106 as suitable.", "labels": [], "entities": []}, {"text": "The dataset collected including our suitability judgements is publicly available and we are working on extending it so that it can support the development and the automatic evaluation of fact checking approaches.", "labels": [], "entities": [{"text": "fact checking", "start_pos": 187, "end_pos": 200, "type": "TASK", "confidence": 0.8487125039100647}]}], "tableCaptions": []}