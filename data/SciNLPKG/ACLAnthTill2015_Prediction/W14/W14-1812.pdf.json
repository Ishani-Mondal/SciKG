{"title": [{"text": "Improving Peer Feedback Prediction: The Sentence Level is Right", "labels": [], "entities": [{"text": "Improving Peer Feedback Prediction", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.8244262933731079}]}], "abstractContent": [{"text": "Recent research aims to automatically predict whether peer feedback is of high quality , e.g. suggests solutions to identified problems.", "labels": [], "entities": []}, {"text": "While prior studies have fo-cused on peer review of papers, similar issues arise when reviewing diagrams and other artifacts.", "labels": [], "entities": []}, {"text": "In addition, previous studies have not carefully examined how the level of prediction granularity impacts both accuracy and educational utility.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 111, "end_pos": 119, "type": "METRIC", "confidence": 0.9987180233001709}]}, {"text": "In this paper we develop models for predicting the quality of peer feedback regarding argument diagrams.", "labels": [], "entities": []}, {"text": "We propose to perform prediction at the sentence level, even though the educational task is to label feedback at a multi-sentential comment level.", "labels": [], "entities": []}, {"text": "We first introduce a corpus annotated at a sentence level granularity, then build comment prediction models using this corpus.", "labels": [], "entities": [{"text": "comment prediction", "start_pos": 82, "end_pos": 100, "type": "TASK", "confidence": 0.7226442843675613}]}, {"text": "Our results show that ag-gregating sentence prediction outputs to label comments not only outperforms approaches that directly train on comment annotations, but also provides useful information for enhancing peer review systems with new functionality.", "labels": [], "entities": [{"text": "ag-gregating sentence prediction", "start_pos": 22, "end_pos": 54, "type": "TASK", "confidence": 0.761643668015798}]}], "introductionContent": [{"text": "Peer review systems are increasingly being used to facilitate the teaching and assessment of student writing.", "labels": [], "entities": []}, {"text": "Peer feedback can complement and even be as useful as teacher feedback; students can also benefit by producing peer feedback.", "labels": [], "entities": []}, {"text": "Past research has shown that feedback implementation is significantly correlated to the presence of desirable feedback features such as the description of solutions to problems.", "labels": [], "entities": []}, {"text": "Since it would be very time-consuming for instructors to identify feedback of low quality posthoc, recent research has used natural language processing (NLP) to automatically predict whether peer feedback contains useful content for guiding student revision).", "labels": [], "entities": []}, {"text": "Such realtime predictions have in turn been used to enhance existing online peer-review systems, e.g. by triggering tutoring that is designed to improve feedback quality.", "labels": [], "entities": []}, {"text": "While most prior research of peer review quality has focused on feedback regarding papers, similar issues arise when reviewing other types of artifacts such as program code, graphical diagrams, etc.", "labels": [], "entities": []}, {"text": "(Nguyen and Litman, July 2013).", "labels": [], "entities": []}, {"text": "In addition, previous studies have not carefully examined how the level of prediction granularity (e.g. multisentential review comments versus sentences) impacts both the accuracy and the educational utility of the predictive models.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 171, "end_pos": 179, "type": "METRIC", "confidence": 0.9988045692443848}]}, {"text": "For example, while the tutoring intervention of (Nguyen et al., June 2014) highlighted low versus high quality feedback comments, such a prediction granularity could not support the highlighting of specific text spans that also might have been instructionally useful.", "labels": [], "entities": []}, {"text": "In this paper, we first address the problem of predicting feedback type (i.e. problem, solution, non-criticism) in peer reviews of student argument diagrams.", "labels": [], "entities": [{"text": "predicting feedback type", "start_pos": 47, "end_pos": 71, "type": "TASK", "confidence": 0.8983642260233561}]}, {"text": "In problem feedback, the reviewer describes what is wrong or needs to be improved in the diagram.", "labels": [], "entities": []}, {"text": "In solution feedback, the reviewer provides away to fix a problem or to improve the diagram quality.", "labels": [], "entities": []}, {"text": "Feedback is non-criticism when it is neither a problem nor a solution (e.g. when it provides only positive feedback or summarizes).", "labels": [], "entities": []}, {"text": "The second goal of our research is to design our prediction framework so that it can support realtime tutoring about feedback quality.", "labels": [], "entities": []}, {"text": "Are the relevance, validity, and reason fields in the supportive arcs complete and convincing?", "labels": [], "entities": [{"text": "validity", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9436343908309937}]}, {"text": "If not, indicate where the argument for relevance or validity is missing or unclear.", "labels": [], "entities": []}, {"text": "Suggest ways to make the validity or relevance argument more convincing or sensible.", "labels": [], "entities": []}, {"text": "Not all of these field are filled out, which makes it hard to get a clear idea of how legit these studies are.", "labels": [], "entities": []}, {"text": "Also, some are unclear.", "labels": [], "entities": []}, {"text": "An example is 24-supports where the reason is a question.", "labels": [], "entities": []}, {"text": "I think there should be a substantial reason there instead of a question to convince me why it is relevant.", "labels": [], "entities": []}, {"text": "Is at least one credible opposing Finding, Study, or Theory connected to each Hypothesis?", "labels": [], "entities": []}, {"text": "If there is no opposition, suggest a spot fora potential counterargument.", "labels": [], "entities": []}, {"text": "If there is opposition, is it credible?", "labels": [], "entities": []}, {"text": "If the opposition is not credible, explain why.", "labels": [], "entities": []}, {"text": "There is a good piece of credible opposition, though it is hard to tell from the diagram what the study exactly did.", "labels": [], "entities": []}, {"text": "Are any parts of the diagram hard to understand because they are unclear?", "labels": [], "entities": []}, {"text": "If so, describe any particularly confusing parts of the diagram and suggest ways to increase clarity.", "labels": [], "entities": [{"text": "clarity", "start_pos": 93, "end_pos": 100, "type": "METRIC", "confidence": 0.9623985886573792}]}, {"text": "The argument diagram was easy to follow.", "labels": [], "entities": []}, {"text": "I was able to effortlessly go through the diagram and connect each part.", "labels": [], "entities": []}], "datasetContent": [{"text": "Sections 6 and 7 report the results of two different experiments involving the prediction of feedback types at the comment level.", "labels": [], "entities": []}, {"text": "While each experiment differs in the exact classes to be predicted, both compare the predictive utility of the same two different model-building approaches: \u2022 Trained using comments (CTRAIN): our baseline 6 approach learns comment prediction models using labeled feedback comments for training.", "labels": [], "entities": []}, {"text": "\u2022 Trained using sentences (STRAIN): our proposed approach learns sentence prediction models using labeled sentences, then aggregates sentence prediction outputs to create comment labels.", "labels": [], "entities": [{"text": "STRAIN", "start_pos": 27, "end_pos": 33, "type": "METRIC", "confidence": 0.8671553134918213}, {"text": "sentence prediction", "start_pos": 65, "end_pos": 84, "type": "TASK", "confidence": 0.7588449418544769}]}, {"text": "For example, the aggregation used for the experiment in Section 6 is as follows: if at least one sentence is predicted as Solution/Problem then the comment is assigned Solution/Problem.", "labels": [], "entities": []}, {"text": "We hypothesize that the proposed approach will yield better predictive performance than the baseline because the former takes advantage of cleaner and more discriminative training data.", "labels": [], "entities": []}, {"text": "To make the features of the two approaches comparable, we use the same set of generic linguistic features: \u2022 Ngrams to capture word cues: word unigrams, POS/word bigrams, POS/word trigrams, word and POS pairs, punctuation, word count.", "labels": [], "entities": []}, {"text": "\u2022 Dependency parse to capture structure cues.", "labels": [], "entities": [{"text": "Dependency parse", "start_pos": 2, "end_pos": 18, "type": "TASK", "confidence": 0.7996286749839783}]}, {"text": "We skip domain and course-specific features (e.g. review dimensions, diagram keywords like hypothesis) in order to make the learned model more applicable to different diagram review data.", "labels": [], "entities": []}, {"text": "Instead, we search for diagram keywords in comments and replace them with the string \"KEY-WORD\".", "labels": [], "entities": [{"text": "KEY-WORD", "start_pos": 86, "end_pos": 94, "type": "METRIC", "confidence": 0.9729166626930237}]}, {"text": "The keyword list can be extracted automatically from LASAD's diagram ontology.", "labels": [], "entities": [{"text": "LASAD's diagram ontology", "start_pos": 53, "end_pos": 77, "type": "DATASET", "confidence": 0.8562015891075134}]}, {"text": "Adding metadata features such as comment and sentence ordering did not seem to improve performance so we do not include such features in the experiments below.", "labels": [], "entities": [{"text": "sentence ordering", "start_pos": 45, "end_pos": 62, "type": "TASK", "confidence": 0.7103022336959839}]}, {"text": "Following, we learn prediction models using logistic regression.", "labels": [], "entities": []}, {"text": "However, in our work both feature extraction and model learning are performed using the LightSide 7 toolkit.", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 26, "end_pos": 44, "type": "TASK", "confidence": 0.800935298204422}, {"text": "model learning", "start_pos": 49, "end_pos": 63, "type": "TASK", "confidence": 0.7200464010238647}, {"text": "LightSide 7 toolkit", "start_pos": 88, "end_pos": 107, "type": "DATASET", "confidence": 0.9085447788238525}]}, {"text": "As our data is collected from nine separate sections of the same course, to better evaluate the models, we perform cross-section evaluation in which for each fold we train the model using data from 8 sections and test on the remaining section.", "labels": [], "entities": []}, {"text": "Reported results are averaged over 9-fold cross validations.", "labels": [], "entities": []}, {"text": "Four metrics are used to evaluate prediction performance.", "labels": [], "entities": []}, {"text": "Accuracy (Acc.) and Kappa (\u03ba) are used as standard performance measurements.", "labels": [], "entities": [{"text": "Accuracy (Acc.)", "start_pos": 0, "end_pos": 15, "type": "METRIC", "confidence": 0.890790268778801}, {"text": "Kappa (\u03ba)", "start_pos": 20, "end_pos": 29, "type": "METRIC", "confidence": 0.9465179294347763}]}, {"text": "Since our annotated corpus has imbalanced data which makes the learned models bias to the majority classes, we also report the Precision (Prec.) and Recall (Recl.) of predicting the minor classes.", "labels": [], "entities": [{"text": "Precision (Prec.)", "start_pos": 127, "end_pos": 144, "type": "METRIC", "confidence": 0.9246386140584946}, {"text": "Recall (Recl.)", "start_pos": 149, "end_pos": 163, "type": "METRIC", "confidence": 0.9236961305141449}]}, {"text": "To the best of our knowledge, (Xiong et al.,) contain the only published models developed for predicting feedback types.", "labels": [], "entities": [{"text": "predicting feedback types", "start_pos": 94, "end_pos": 119, "type": "TASK", "confidence": 0.9131084283192953}]}, {"text": "A comment-level solution prediction model has since been deployed in their peer review software to evaluate student reviewer comments in classroom settings, using the following 3-way classification algorithm . Each student comment is classified as either a criticism (i.e. presents problem/solution information) or a non-criticism.", "labels": [], "entities": [{"text": "comment-level solution prediction", "start_pos": 2, "end_pos": 35, "type": "TASK", "confidence": 0.6357690095901489}]}, {"text": "The non-criticism comment is labeled NULL.", "labels": [], "entities": [{"text": "NULL", "start_pos": 37, "end_pos": 41, "type": "DATASET", "confidence": 0.505962073802948}]}, {"text": "The criticism comment is labeled SOLUTION if it contains solution information, and labeled PROBLEM otherwise.", "labels": [], "entities": [{"text": "SOLUTION", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.9773902297019958}]}, {"text": "To evaluate our proposed STRAIN approach in their practically-motivated setting, we follow the description above to relabel peer feedback comments in our corpus to new labels: NULL, PROB-LEM, and SOLUTION.", "labels": [], "entities": [{"text": "NULL", "start_pos": 176, "end_pos": 180, "type": "METRIC", "confidence": 0.8134167194366455}, {"text": "SOLUTION", "start_pos": 196, "end_pos": 204, "type": "METRIC", "confidence": 0.9185703992843628}]}, {"text": "We also asked the authors of () for access to their current model and we were able to run their model on our feedback comment data.", "labels": [], "entities": []}, {"text": "While it is not appropriate to directly compare model performance as Xiong et al. were working with paper (not diagram) review data, we report their model output, named PAPER, to provide a reference baseline.", "labels": [], "entities": [{"text": "PAPER", "start_pos": 169, "end_pos": 174, "type": "METRIC", "confidence": 0.8596314787864685}]}, {"text": "We expect the PAPER model to work on our diagram review data to some extent, particularly due to its predefined seed words for solution and problem cues.", "labels": [], "entities": []}, {"text": "Our CTRAIN baseline, in contrast, trains models regarding the new label set using relabeled diagram comment data, with the same features and learning algorithm from the prior sections.", "labels": [], "entities": [{"text": "CTRAIN baseline", "start_pos": 4, "end_pos": 19, "type": "DATASET", "confidence": 0.850395143032074}]}, {"text": "The majority baseline, MAJOR, assigns all comments the major class label (which is now NULL).", "labels": [], "entities": [{"text": "MAJOR", "start_pos": 23, "end_pos": 28, "type": "METRIC", "confidence": 0.7133625745773315}, {"text": "NULL", "start_pos": 87, "end_pos": 91, "type": "METRIC", "confidence": 0.7366793155670166}]}, {"text": "Regarding our STRAIN sentence level ap-   proach, we propose two aggregation procedures to infer comment labels given sentence prediction output.", "labels": [], "entities": [{"text": "STRAIN sentence level ap-   proach", "start_pos": 14, "end_pos": 48, "type": "DATASET", "confidence": 0.5372957984606425}]}, {"text": "In the first procedure, RELABELFIRST, we infer new sentence labels regarding NULL, PROB-LEM, and SOLUTION using a series of conditional statements.", "labels": [], "entities": [{"text": "RELABELFIRST", "start_pos": 24, "end_pos": 36, "type": "METRIC", "confidence": 0.9898391962051392}, {"text": "NULL", "start_pos": 77, "end_pos": 81, "type": "METRIC", "confidence": 0.914503812789917}, {"text": "PROB-LEM", "start_pos": 83, "end_pos": 91, "type": "METRIC", "confidence": 0.7346254587173462}]}, {"text": "The order of statements is chosen heuristically given the performance of individual models (see) and is described in.", "labels": [], "entities": []}, {"text": "Given the sentences' inferred labels, the comment is labeled SOLUTION if it has at least one SOLUTION sentence.", "labels": [], "entities": [{"text": "SOLUTION", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.8637600541114807}]}, {"text": "Else, it is labeled PROBLEM if at least one of its sentences is PROB-LEM, and labeled NULL otherwise.", "labels": [], "entities": []}, {"text": "Our second aggregation procedure, called INFERFIRST, follows an opposite direction in which we infer comment labels regarding Solution, Problem, and Criticism before re-labeling the comment regarding SOLUTION, PROBLEM, and NULL following the order of conditional statements in the relabel procedure.", "labels": [], "entities": [{"text": "INFERFIRST", "start_pos": 41, "end_pos": 51, "type": "METRIC", "confidence": 0.787975013256073}]}, {"text": "As shown in precision and recall of different models for each class.", "labels": [], "entities": [{"text": "precision", "start_pos": 12, "end_pos": 21, "type": "METRIC", "confidence": 0.9996640682220459}, {"text": "recall", "start_pos": 26, "end_pos": 32, "type": "METRIC", "confidence": 0.9994878768920898}]}, {"text": "The PAPER model achieves high precision but low recall for SOLUTION and PROBLEM classes.", "labels": [], "entities": [{"text": "PAPER", "start_pos": 4, "end_pos": 9, "type": "METRIC", "confidence": 0.8959329128265381}, {"text": "precision", "start_pos": 30, "end_pos": 39, "type": "METRIC", "confidence": 0.9991361498832703}, {"text": "recall", "start_pos": 48, "end_pos": 54, "type": "METRIC", "confidence": 0.9994165897369385}]}, {"text": "We reason that the model's seed words help its precision, but its ngram features, which were trained using paper review data, cannot adequately cover positive instances in our corpus.", "labels": [], "entities": [{"text": "precision", "start_pos": 47, "end_pos": 56, "type": "METRIC", "confidence": 0.9992352724075317}]}, {"text": "The two sentence level models perform better for the PROBLEM class than the other two models, which is consistent with what is reported in Table 6.", "labels": [], "entities": []}, {"text": "Comparing the two sentence level models, INFERFIRST better balances precision and recall than RELABELFIRST.", "labels": [], "entities": [{"text": "INFERFIRST", "start_pos": 41, "end_pos": 51, "type": "METRIC", "confidence": 0.6505070924758911}, {"text": "precision", "start_pos": 68, "end_pos": 77, "type": "METRIC", "confidence": 0.9994826316833496}, {"text": "recall", "start_pos": 82, "end_pos": 88, "type": "METRIC", "confidence": 0.9990054965019226}]}], "tableCaptions": [{"text": " Table 2: Comment label distribution.", "labels": [], "entities": [{"text": "Comment label distribution", "start_pos": 10, "end_pos": 36, "type": "TASK", "confidence": 0.7726704080899557}]}, {"text": " Table 3: Examples of labeled sentences extracted  from the annotated comment.", "labels": [], "entities": []}, {"text": " Table 4: Sentence label distribution.", "labels": [], "entities": [{"text": "Sentence label distribution", "start_pos": 10, "end_pos": 37, "type": "TASK", "confidence": 0.875701387723287}]}, {"text": " Table 5: Prediction performance of three tasks at the sentence level. Comparing STR/STE to CTR/STE:  Italic means higher with p < 0.05, Bold means higher with p < 0.01.", "labels": [], "entities": [{"text": "STR/STE", "start_pos": 81, "end_pos": 88, "type": "METRIC", "confidence": 0.747307022412618}]}, {"text": " Table 6: Prediction performance of three tasks at comment level. Comparing STRAIN to CTRAIN: Italic  means higher with p < 0.1, Bold means higher with p < 0.05.", "labels": [], "entities": [{"text": "STRAIN", "start_pos": 76, "end_pos": 82, "type": "METRIC", "confidence": 0.9787089228630066}, {"text": "CTRAIN", "start_pos": 86, "end_pos": 92, "type": "METRIC", "confidence": 0.8541085124015808}]}, {"text": " Table 9: Precision and recall of different models in a case study.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9918088316917419}, {"text": "recall", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.9970536231994629}]}]}