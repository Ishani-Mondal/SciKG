{"title": [{"text": "Modelling Sarcasm in Twitter, a Novel Approach", "labels": [], "entities": []}], "abstractContent": [{"text": "Automatic detection of figurative language is a challenging task in computational linguistics.", "labels": [], "entities": [{"text": "Automatic detection of figurative language", "start_pos": 0, "end_pos": 42, "type": "TASK", "confidence": 0.867418897151947}, {"text": "computational linguistics", "start_pos": 68, "end_pos": 93, "type": "TASK", "confidence": 0.7155703008174896}]}, {"text": "Recognising both literal and figurative meaning is not trivial fora machine and in some cases it is hard even for humans.", "labels": [], "entities": []}, {"text": "For this reason novel and accurate systems able to recognise figurative languages are necessary.", "labels": [], "entities": []}, {"text": "We present in this paper a novel computational model capable to detect sarcasm in the social network Twitter (a popular microblogging service which allows users to post short messages).", "labels": [], "entities": []}, {"text": "Our model is easy to implement and, unlike previous systems, it does not include patterns of words as features.", "labels": [], "entities": []}, {"text": "Our seven sets of lexical features aim to detect sarcasm by its inner structure (for example unexpectedness, intensity of the terms or imbalance between registers), abstracting from the use of specific terms.", "labels": [], "entities": []}], "introductionContent": [{"text": "Sarcasm is a mode of communication where literal and intended meanings are in opposition.", "labels": [], "entities": []}, {"text": "Sarcasm is often used to express a negative message using positive words.", "labels": [], "entities": []}, {"text": "Automatic detection of sarcasm is then very important in the sentiment analysis field, as a sarcastic phrase that includes positive words conveys a negative message and can be easily misunderstood by an automatic system.", "labels": [], "entities": [{"text": "Automatic detection of sarcasm", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.8005340471863747}, {"text": "sentiment analysis", "start_pos": 61, "end_pos": 79, "type": "TASK", "confidence": 0.9611044824123383}]}, {"text": "A number of systems with the objective of detecting sarcasm have been designed in the past years (.", "labels": [], "entities": [{"text": "detecting sarcasm", "start_pos": 42, "end_pos": 59, "type": "TASK", "confidence": 0.8909413814544678}]}, {"text": "All these computational models have in common the use of frequent and typical sarcastic expressions as features.", "labels": [], "entities": []}, {"text": "This is of course a good approach as some words are used sarcastically more often than others.", "labels": [], "entities": []}, {"text": "Our research seeks to avoid the use of words as features, for two reasons.", "labels": [], "entities": []}, {"text": "Firstly, we want to reduce the complexity of the computational model, decreasing drastically the number of features required for classification.", "labels": [], "entities": []}, {"text": "Secondly, typical sarcastic expressions are often culturally specific (an expression that is considered sarcastic in British English is not necessary sarcastic in American English and vice-versa).", "labels": [], "entities": []}, {"text": "For these reasons we have designed a system that aims to detect sarcasm without the use of words and patterns of words.", "labels": [], "entities": []}, {"text": "We use simple features such as punctuation) and more sophisticated features, that for example detect imbalance between registers (the use of an \"out of context\" word may suggest sarcastic intentions) or the use of very intense terms.", "labels": [], "entities": []}, {"text": "We study sarcasm detection in the microblogging platform Twitter 1 that allows users to send and read text messages (shorter than 140 characters) called tweets, which often do not follow the expected rules of the grammar.", "labels": [], "entities": [{"text": "sarcasm detection", "start_pos": 9, "end_pos": 26, "type": "TASK", "confidence": 0.9230971038341522}]}, {"text": "The dataset we adopted contains positive examples tagged as sarcastic by the users (using the hashtag #sarcasm) and negative examples (tagged with a different hashtag).", "labels": [], "entities": []}, {"text": "This methodology has been previously used in similar studies (.", "labels": [], "entities": []}, {"text": "We presented in a model capable of detecting irony, in this paper we add important features to this model and evaluate anew corpus to determine if our system is capable of detecting tweets marked as sarcastic (#sar-casm).", "labels": [], "entities": [{"text": "detecting irony", "start_pos": 35, "end_pos": 50, "type": "TASK", "confidence": 0.8114989399909973}]}, {"text": "The contributions of this paper are the following: \u2022 Novel set of features to improve the performances of our model \u2022 A new set of experiments to test our model's ability to detect sarcasm \u2022 A corpus to study sarcasm in twitter We will show in the paper that results are positive and the system recognises sarcasm with good accuracy in comparison with the state-of-the-art.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 324, "end_pos": 332, "type": "METRIC", "confidence": 0.996058464050293}]}, {"text": "The rest of the paper is organised as follows: in the next Section we describe related work.", "labels": [], "entities": []}, {"text": "In Section 3 we describes the corpus and text processing tools used and in Section 4 we present our approach to tackle the sarcasm detection problem.", "labels": [], "entities": [{"text": "sarcasm detection problem", "start_pos": 123, "end_pos": 148, "type": "TASK", "confidence": 0.9297793706258138}]}, {"text": "Section 5 describes the experiments while Section 6 interprets the results.", "labels": [], "entities": []}, {"text": "Finally, we close the paper in Section 7 with conclusions and future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to evaluate our system we use five datasets, subsets of the corpus in Section 3: Sarcasm vs Education, Sarcasm vs Humour, Sarcasm vs Irony, Sarcasm vs Newspaper and Sarcasm vs Politics.", "labels": [], "entities": []}, {"text": "Each combination is balanced with 10.000 sarcastic and 10.000 of non-sarcastic examples.", "labels": [], "entities": []}, {"text": "We run the following two types of experiments: 1.", "labels": [], "entities": []}, {"text": "We run in each datasets a 10-fold crossvalidation classification experiment.", "labels": [], "entities": [{"text": "crossvalidation classification", "start_pos": 34, "end_pos": 64, "type": "TASK", "confidence": 0.6901362836360931}]}, {"text": "2. We train the classifier on 75% of positive examples and 75% of negative examples of the same dataset, then we use as test set the rest 25% positive and 25% negative.", "labels": [], "entities": []}, {"text": "We perform this experiment for the five datasets.", "labels": [], "entities": []}, {"text": "In and we show the values of information gain of the five combinations of topics (Sarcasm versus each not-sarcastic topic).", "labels": [], "entities": []}, {"text": "Note that, in the first figure the scale we chose to better visualise all the features truncates the scores of the feature http of Education, Newspaper, and Politics.", "labels": [], "entities": []}, {"text": "These three values are respectively 0.4, 0.7 and 0.4.", "labels": [], "entities": []}, {"text": "includes Precision, Recall, and F-Measure results of Experiment 1 and Experiment 2.", "labels": [], "entities": [{"text": "Precision", "start_pos": 9, "end_pos": 18, "type": "METRIC", "confidence": 0.9976449608802795}, {"text": "Recall", "start_pos": 20, "end_pos": 26, "type": "METRIC", "confidence": 0.9873710870742798}, {"text": "F-Measure", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.9982959628105164}]}], "tableCaptions": [{"text": " Table 1: Precision, Recall and F-Measure of each  topic combination for Experiment 1 (10 cross val- idation). Sarcasm corpus is compared to Educa- tion, Humour, Irony, Newspaper, and Politics cor- pora. The classifier used is Decision Tree", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9957302212715149}, {"text": "Recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9935699701309204}, {"text": "F-Measure", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.9977970123291016}]}, {"text": " Table 2: Precision, Recall and F-Measure of each  topic combination for Experiment 2 (Test set).  Sarcasm corpus is compared to Education, Hu- mour, Irony, Newspaper, and Politics corpora.The  classifier used is Decision Tree", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9961109757423401}, {"text": "Recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9941765069961548}, {"text": "F-Measure", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.9982240796089172}]}]}