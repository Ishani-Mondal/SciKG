{"title": [{"text": "Automated Error Detection in Digitized Cultural Heritage Documents", "labels": [], "entities": [{"text": "Automated Error Detection in Digitized Cultural Heritage Documents", "start_pos": 0, "end_pos": 66, "type": "TASK", "confidence": 0.6607025563716888}]}], "abstractContent": [{"text": "The work reported in this paper aims at performance optimization in the di-gitization of documents pertaining to the cultural heritage domain.", "labels": [], "entities": []}, {"text": "A hybrid method is proposed, combining statistical classification algorithms and linguistic knowledge to automatize post-OCR error detection and correction.", "labels": [], "entities": [{"text": "statistical classification", "start_pos": 39, "end_pos": 65, "type": "TASK", "confidence": 0.737870454788208}, {"text": "post-OCR error detection and correction", "start_pos": 116, "end_pos": 155, "type": "TASK", "confidence": 0.6849138677120209}]}, {"text": "The current paper deals with the integration of linguistic modules and their impact on error detection.", "labels": [], "entities": [{"text": "error detection", "start_pos": 87, "end_pos": 102, "type": "TASK", "confidence": 0.7122601419687271}]}], "introductionContent": [{"text": "Providing wider access to national cultural heritage by massive digitization confronts the actors of the field with a set of new challenges.", "labels": [], "entities": []}, {"text": "State of the art optical character recognition (OCR) software currently achieve an error rate of around 1 to 10% depending on the age and the layout of the text.", "labels": [], "entities": [{"text": "optical character recognition (OCR)", "start_pos": 17, "end_pos": 52, "type": "TASK", "confidence": 0.7720181196928024}, {"text": "error rate", "start_pos": 83, "end_pos": 93, "type": "METRIC", "confidence": 0.9837525188922882}]}, {"text": "While this quality maybe adequate for indexing, documents intended for reading need to meet higher standards.", "labels": [], "entities": [{"text": "indexing", "start_pos": 38, "end_pos": 46, "type": "TASK", "confidence": 0.9835509657859802}]}, {"text": "A reduction of the error rate by a factor of 10 to 100 becomes necessary for the diffusion of digitized books and journals through emerging technologies such as e-books.", "labels": [], "entities": [{"text": "error rate", "start_pos": 19, "end_pos": 29, "type": "METRIC", "confidence": 0.9814582467079163}]}, {"text": "Our paper deals with the automatic post-processing of digitized documents with the aim of reducing the OCR error rate by using contextual information and linguistic processing, by and large absent from current OCR engines.", "labels": [], "entities": [{"text": "OCR error rate", "start_pos": 103, "end_pos": 117, "type": "METRIC", "confidence": 0.7393136322498322}]}, {"text": "In the current stage of the project, we are focusing on French texts from the archives of the French National Library (Biblioth\u00e8que Nationale de France) covering the period from 1646 to 1990.", "labels": [], "entities": [{"text": "French texts from the archives of the French National Library (Biblioth\u00e8que Nationale de France) covering the period from 1646", "start_pos": 56, "end_pos": 182, "type": "DATASET", "confidence": 0.8152985799880255}]}, {"text": "We adopted a hybrid approach, making use of both statistical classification techniques and linguistically motivated modules to detect OCR errors and generate correction candidates.", "labels": [], "entities": [{"text": "statistical classification", "start_pos": 49, "end_pos": 75, "type": "TASK", "confidence": 0.7421469390392303}]}, {"text": "The technology is based on a symbolic linguistic preprocessing, followed by a statistical module which adpots the noisy channel model.", "labels": [], "entities": []}, {"text": "Symbolic methods for error correction allow to target specific phenomena with a high precision, but they typically strongly rely on presumptions about the nature of errors encountered.", "labels": [], "entities": [{"text": "error correction", "start_pos": 21, "end_pos": 37, "type": "TASK", "confidence": 0.734608992934227}, {"text": "precision", "start_pos": 85, "end_pos": 94, "type": "METRIC", "confidence": 0.9867790937423706}]}, {"text": "This drawback can be overcome by using the noisy channel model (.", "labels": [], "entities": []}, {"text": "However, error models in such systems work best if they are created from manually corrected training data, which are not always available.", "labels": [], "entities": []}, {"text": "Other alternatives to OCR error correction include (weighted), voting systems using the output of different OCR engines (), textual alignment combined with dictionary lookup, or heuristic correction methods ().", "labels": [], "entities": [{"text": "OCR error correction", "start_pos": 22, "end_pos": 42, "type": "TASK", "confidence": 0.86905304590861}, {"text": "textual alignment", "start_pos": 124, "end_pos": 141, "type": "TASK", "confidence": 0.6965982019901276}, {"text": "heuristic correction", "start_pos": 178, "end_pos": 198, "type": "TASK", "confidence": 0.6899372488260269}]}, {"text": "While correction systems rely less and lesson pre-existing external dictionaries, a shift can be observed towards methods that dinamically create lexicons either by exploiting the Web ( or from the corpus).", "labels": [], "entities": []}, {"text": "As to linguistically enhanced models, POS tagging was succesfully applied to spelling correction (.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 38, "end_pos": 49, "type": "TASK", "confidence": 0.8076584935188293}, {"text": "spelling correction", "start_pos": 77, "end_pos": 96, "type": "TASK", "confidence": 0.8773886859416962}]}, {"text": "However, to our knowledge, very little work has been done to exploit linguistic analysis for post-OCR correction.", "labels": [], "entities": [{"text": "post-OCR correction", "start_pos": 93, "end_pos": 112, "type": "TASK", "confidence": 0.742191731929779}]}, {"text": "We propose to apply a shallow processing module to detect certain types of named entities (NEs), and a POS tagger trained specifically to deal with NE-tagged input.", "labels": [], "entities": []}, {"text": "Our studies aim to demonstrate that linguistic preprocessing can efficiently contribute to reduce the error rate by 1) detecting false corrections proposed by the statistical correction module, 2) detecting OCR errors which are unlikely to be detected by the statistical correction module.", "labels": [], "entities": [{"text": "error rate", "start_pos": 102, "end_pos": 112, "type": "METRIC", "confidence": 0.9375777840614319}, {"text": "detecting false corrections", "start_pos": 119, "end_pos": 146, "type": "TASK", "confidence": 0.7693421641985575}]}, {"text": "We argue that named entity grammars can be adapted to the correction task at a low cost and they allow to target specific types of errors with a very high precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 155, "end_pos": 164, "type": "METRIC", "confidence": 0.9925069212913513}]}, {"text": "In what follows, we present the global architecture of the post-OCR correction system (2), the named entity recognition module (3), as well as our experiments in named entity-aware POS tagging (4).", "labels": [], "entities": [{"text": "post-OCR correction", "start_pos": 59, "end_pos": 78, "type": "TASK", "confidence": 0.7071049213409424}, {"text": "named entity recognition", "start_pos": 95, "end_pos": 119, "type": "TASK", "confidence": 0.6176643470923106}, {"text": "POS tagging", "start_pos": 181, "end_pos": 192, "type": "TASK", "confidence": 0.7301093935966492}]}, {"text": "The predicted impact of the linguistic modules is illustrated in section 5.", "labels": [], "entities": []}, {"text": "Finally, we present ongoing work and the conclusion (6).", "labels": [], "entities": []}], "datasetContent": [{"text": "A manual, application-independent evaluation was carried out, concentrating primarily on precision for the reasons mentioned in 3.", "labels": [], "entities": [{"text": "precision", "start_pos": 89, "end_pos": 98, "type": "METRIC", "confidence": 0.9993657469749451}]}, {"text": "For four types of NEs, we collected a sample of 200 sentences expected to contain one or more instances of the given entity category, based on the presence of category-specific markers (lexical units, acronyms etc.)", "labels": [], "entities": []}, {"text": "However, chemical formulae were evaluated directly on sentences extracted from the archives of the European Patent Office; no filtering was needed due to the density of formulae in these documents.", "labels": [], "entities": [{"text": "European Patent Office", "start_pos": 99, "end_pos": 121, "type": "DATASET", "confidence": 0.8616376519203186}]}, {"text": "Legal IDs were evaluated on a legal corpus from the Publications Office of the European Union, while the rest of the grammars were evaluated using the BNF corpus.", "labels": [], "entities": [{"text": "BNF corpus", "start_pos": 151, "end_pos": 161, "type": "DATASET", "confidence": 0.898115336894989}]}, {"text": "The following step in the chain is POS tagging using a named entity-aware version of the MElt tagger.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 35, "end_pos": 46, "type": "TASK", "confidence": 0.7571246027946472}]}, {"text": "MElt) is a maximum entropy POS tagger which differs from other systems in that it uses both corpus-based features and a large-coverage lexicon as an external source of information.", "labels": [], "entities": [{"text": "POS tagger", "start_pos": 27, "end_pos": 37, "type": "TASK", "confidence": 0.7015436291694641}]}, {"text": "Its French version, MElt-FR was trained on the Lefff lexicon (Sagot, 2010) and on the French TreeBank (FTB) ().", "labels": [], "entities": [{"text": "Lefff lexicon (Sagot, 2010)", "start_pos": 47, "end_pos": 74, "type": "DATASET", "confidence": 0.9246512992041451}, {"text": "French TreeBank (FTB)", "start_pos": 86, "end_pos": 107, "type": "DATASET", "confidence": 0.9585125565528869}]}, {"text": "The training corpus uses a tagset consisting of 29 tags.", "labels": [], "entities": []}, {"text": "MElt FR yields state of the art results for French, namely 97.8% accuracy on the test set.", "labels": [], "entities": [{"text": "MElt", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9311217665672302}, {"text": "FR", "start_pos": 5, "end_pos": 7, "type": "METRIC", "confidence": 0.6000654101371765}, {"text": "French", "start_pos": 44, "end_pos": 50, "type": "DATASET", "confidence": 0.7023285031318665}, {"text": "accuracy", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.9995297193527222}]}, {"text": "A set of experiments were performed using different sections of the NE-annotated training data.", "labels": [], "entities": [{"text": "NE-annotated training data", "start_pos": 68, "end_pos": 94, "type": "DATASET", "confidence": 0.7036350766817728}]}, {"text": "First, we cutout 100 sentences at random and used them as a test corpus.", "labels": [], "entities": []}, {"text": "From the rest of the sentences, we created diverse random partitionings using 50, 100, 150 and all the 232 sentences as training data.", "labels": [], "entities": []}, {"text": "We trained MElt-h on each training corpus and evaluated it on the test section of the FTB as well as on the 100 NE-annotated sentences.", "labels": [], "entities": [{"text": "MElt-h", "start_pos": 11, "end_pos": 17, "type": "METRIC", "confidence": 0.7482894659042358}, {"text": "FTB", "start_pos": 86, "end_pos": 89, "type": "DATASET", "confidence": 0.9510378241539001}]}, {"text": "The results confirm that adding NE-annotated sentences to the training corpus does not decrease precision on the FTB itself.", "labels": [], "entities": [{"text": "precision", "start_pos": 96, "end_pos": 105, "type": "METRIC", "confidence": 0.999539852142334}, {"text": "FTB", "start_pos": 113, "end_pos": 116, "type": "DATASET", "confidence": 0.8907231688499451}]}, {"text": "Furthermore, we note that the results on the NE corpus are slightly inferior to the results on the FTB, but the figures suggest that the learning curve did not reach a limit for NE-annotated data: adding more NE-annotated sentences will probably increase precision.", "labels": [], "entities": [{"text": "NE corpus", "start_pos": 45, "end_pos": 54, "type": "DATASET", "confidence": 0.6945723295211792}, {"text": "FTB", "start_pos": 99, "end_pos": 102, "type": "DATASET", "confidence": 0.9500264525413513}, {"text": "precision", "start_pos": 255, "end_pos": 264, "type": "METRIC", "confidence": 0.9971960783004761}]}], "tableCaptions": [{"text": " Table 1: Evaluation of NE grammars", "labels": [], "entities": [{"text": "NE grammars", "start_pos": 24, "end_pos": 35, "type": "TASK", "confidence": 0.8211224377155304}]}, {"text": " Table 2: Evaluation of MElt-h on the FTB and on  the NE-annotated corpus", "labels": [], "entities": [{"text": "MElt-h", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.948675274848938}, {"text": "FTB", "start_pos": 38, "end_pos": 41, "type": "DATASET", "confidence": 0.963254988193512}, {"text": "NE-annotated corpus", "start_pos": 54, "end_pos": 73, "type": "DATASET", "confidence": 0.7773146629333496}]}]}