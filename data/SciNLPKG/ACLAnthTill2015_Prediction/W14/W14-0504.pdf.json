{"title": [{"text": "Learning the hyperparameters to learn morphology", "labels": [], "entities": []}], "abstractContent": [{"text": "We perform hyperparameter inference within a model of morphology learning (Goldwater et al., 2011) and find that it affects model behaviour drastically.", "labels": [], "entities": [{"text": "hyperparameter inference", "start_pos": 11, "end_pos": 35, "type": "TASK", "confidence": 0.699861153960228}]}, {"text": "Changing the model structure successfully avoids the unsegmented solution, but results in oversegmentation instead.", "labels": [], "entities": []}], "introductionContent": [{"text": "Bayesian models provide a sound statistical framework in which to explore aspects of language acquisition.", "labels": [], "entities": [{"text": "language acquisition", "start_pos": 85, "end_pos": 105, "type": "TASK", "confidence": 0.7011251598596573}]}, {"text": "Explicitly specifying the causal and computational structure of a model enables the investigation of hypotheses such as the feasibility of learning linguistic structure from the available input), or the interaction of different linguistic levels.", "labels": [], "entities": []}, {"text": "However, these models can be sensitive to small changes in (hyper-)parameters settings.", "labels": [], "entities": []}, {"text": "Robustness in this respect is important, since positing specific parameter values is cognitively implausible.", "labels": [], "entities": []}, {"text": "In this paper we revisit a model of morphology learning presented by Goldwater and colleagues in and.", "labels": [], "entities": [{"text": "morphology learning", "start_pos": 36, "end_pos": 55, "type": "TASK", "confidence": 0.8419540822505951}]}, {"text": "This model demonstrated the effectiveness of non-parametric stochastic processes, specifically the Pitman-Yor Process, for interpolating between types and tokens.", "labels": [], "entities": []}, {"text": "Language learners are exposed to tokens, but many aspects of linguistic structure are lexical; identifying which tokens belong to the same lexical type is crucial.", "labels": [], "entities": []}, {"text": "Surface form is not always sufficient, as in the case of ambiguous words.", "labels": [], "entities": []}, {"text": "Moreover, morphology in particular is influenced by vocabulary-level type statistics, so it is important fora model to operate on both levels: token statistics from realistic (child-directed) input, and type-level statistics based on the token analyses.", "labels": [], "entities": []}, {"text": "The GGJ model learns successfully given fixed hyperparameter values in the Pitman-Yor Process.", "labels": [], "entities": []}, {"text": "However, we show that when these hyperparameters are inferred, it collapses to a token-based model with a trivial morphology.", "labels": [], "entities": []}, {"text": "In this paper we discuss the reasons for this problematic behaviour, which are relevant for other models based on Pitman-Yor Processes with discrete base distributions, common in natural language tasks.", "labels": [], "entities": []}, {"text": "We investigate some potential solutions, by changing the way morphemes are generated within the model.", "labels": [], "entities": []}, {"text": "Our results are mixed; we avoid the hyperparameter problem, but learn overly compact morpheme lexicons.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our datasets consist of the adult utterances from two morphologically annotated corpora from CHILDES, an English corpus, Eve, and a Spanish corpus, Ornat.", "labels": [], "entities": []}, {"text": "Morphology is marked by a grammatical suffix on the stem, e.g. doggy-PL.", "labels": [], "entities": []}, {"text": "Words marked with irregular morphology are unsegmented.", "labels": [], "entities": []}, {"text": "The two languages, while related, have differing degrees of affixation: the English Eve corpus consists of 63 315 tokens (5% suffixed) and 1 988 types (28% suffixed); the Ornat corpus has 43 796 tokens (23% suffixed) and 3 157 types (50% suffixed).", "labels": [], "entities": [{"text": "English Eve corpus", "start_pos": 76, "end_pos": 94, "type": "DATASET", "confidence": 0.9569932818412781}, {"text": "Ornat corpus", "start_pos": 171, "end_pos": 183, "type": "DATASET", "confidence": 0.9301654696464539}]}, {"text": "The English corpus has 17 gold suffix types, while Spanish has 72.", "labels": [], "entities": [{"text": "English corpus", "start_pos": 4, "end_pos": 18, "type": "DATASET", "confidence": 0.7414655685424805}]}, {"text": "We also use the phonologically encoded Eve dataset used by GGJ.", "labels": [], "entities": [{"text": "Eve dataset", "start_pos": 39, "end_pos": 50, "type": "DATASET", "confidence": 0.7824637591838837}, {"text": "GGJ", "start_pos": 59, "end_pos": 62, "type": "DATASET", "confidence": 0.9581288695335388}]}, {"text": "This dataset does not exactly correspond to the orthographic version, due to discrepancies in tokenisation, so we are unable to evaluate this dataset quantitatively.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Final morphology results. 'Fix' refers to models with fixed PYP hyperparameters (a = b = 0.1),  while 'Inf' models have inferred hyperparameters. % Seg shows the percentage of tokens that have a non- null suffix, while |L| is the size of the morpheme lexicon. VM is shown with 95% confidence intervals.", "labels": [], "entities": [{"text": "VM", "start_pos": 270, "end_pos": 272, "type": "DATASET", "confidence": 0.6433402895927429}]}]}