{"title": [{"text": "Focus Annotation in Reading Comprehension Data", "labels": [], "entities": []}], "abstractContent": [{"text": "When characterizing the information structure of sentences, the so-called focus identifies the part of a sentence addressing the current question under discussion in the discourse.", "labels": [], "entities": []}, {"text": "While this notion is precisely defined informal semantics and potentially very useful in theoretical and practical terms, it has turned out to be difficult to reliably annotate focus in corpus data.", "labels": [], "entities": []}, {"text": "We present anew focus annotation effort designed to overcome this problem.", "labels": [], "entities": []}, {"text": "On the one hand, it is based on a task-based corpus providing more explicit context.", "labels": [], "entities": []}, {"text": "The annotation study is based on the CREG corpus (Ott et al., 2012), which consists of answers to explicitly given reading comprehension questions.", "labels": [], "entities": [{"text": "CREG corpus (Ott et al., 2012)", "start_pos": 37, "end_pos": 67, "type": "DATASET", "confidence": 0.8998392555448744}]}, {"text": "On the other hand, we operationalize focus annotation as an incremental process including several substeps which provide guidance, such as explicit answer typing.", "labels": [], "entities": []}, {"text": "We evaluate the focus annotation both intrinsically by calculating agreement between annotators and extrinsically by showing that the focus information substantially improves the automatic meaning assessment of answers in the CoMiC system (Meurers et al., 2011).", "labels": [], "entities": []}], "introductionContent": [{"text": "This paper discusses the interplay of linguistic and computational linguistic aspects in the analysis of focus as a core notion of information structure.", "labels": [], "entities": []}, {"text": "Empirically, our work focuses on analyzing the responses to reading comprehension questions.", "labels": [], "entities": []}, {"text": "In computational linguistics, automatic meaning assessment determining whether a response appropriately answers a given question about a given text has developed into an active field of research.", "labels": [], "entities": [{"text": "automatic meaning assessment determining whether a response appropriately answers a given question about a given text", "start_pos": 30, "end_pos": 147, "type": "TASK", "confidence": 0.5508829280734062}]}, {"text": "Short Answer Assessment recently was also highlighted by the Joint Student Response Analysis and Textual Entailment Challenge (.", "labels": [], "entities": [{"text": "Short Answer Assessment", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.7955742279688517}, {"text": "Joint Student Response Analysis and Textual Entailment Challenge", "start_pos": 61, "end_pos": 125, "type": "TASK", "confidence": 0.5404415670782328}]}, {"text": "Some research in this domain has pointed out the relevance of identifying which parts of a response are given by the question (, with recent work pointing out that the relevant notion here is that of focus as discussed informal pragmatics.", "labels": [], "entities": []}, {"text": "provides an example of answer comparison for meaning assessment, where the focus (marked by square brackets) can effectively be used to zoom in on the information that is relevant for comparing a target answer (TA) with a student answer (SA) given a question (Q).", "labels": [], "entities": [{"text": "answer comparison", "start_pos": 23, "end_pos": 40, "type": "TASK", "confidence": 0.7753069996833801}, {"text": "meaning assessment", "start_pos": 45, "end_pos": 63, "type": "TASK", "confidence": 0.9556746482849121}]}, {"text": "To support this line of research, one needs to be able to identify the focus in a response.", "labels": [], "entities": []}, {"text": "As a first step, we have designed an annotation scheme and manual annotation process for identifying the focus in a corpus of reading comprehension responses.", "labels": [], "entities": []}, {"text": "Focus here is understood in the sense of as indicating the presence of alternatives in the context and being a direct answer to the Question Under Discussion.", "labels": [], "entities": []}, {"text": "This semantic view of focus is essentially language-independent.", "labels": [], "entities": []}, {"text": "Some attempts at systematically identifying focus in authentic data have been made in the past (.", "labels": [], "entities": []}, {"text": "However, most approaches either capture a notion of focus more closely related to particular language features, such as the Topic-Focus Articulation and its relation to the word order in Czech (), or the approaches were not rewarded with much success (.", "labels": [], "entities": []}, {"text": "The latter have tried to identify focus in newspaper text or other data types where no explicit questions are available, making the task of determining the QUD, and thus reliably annotating focus, very hard.", "labels": [], "entities": []}, {"text": "In contrast, in the research presented here, we work with responses to explicitly given questions that are asked about an explicitly given text.", "labels": [], "entities": []}, {"text": "Thus, we can make use of the characteristics of the questions and text to obtain reliable focus annotation for the responses.", "labels": [], "entities": []}, {"text": "Theoretical linguists have discussed the notion of focus for decades, cf., e.g.,,,, and.", "labels": [], "entities": []}, {"text": "However, for insights and arguments from theoretical work to be applicable in computational linguistics, they need to be linked to thorough empirical work -an area where some work remains to be done (cf., e.g., De Kuthy and Meurers, 2012), with some recent research making significant headway (.", "labels": [], "entities": []}, {"text": "As it stands, computational linguists have not yet been able to fully profit from the theoretical debate on focus.", "labels": [], "entities": []}, {"text": "An important reason complementing the one just mentioned is the fact that the context in which the text to be analyzed is produced has rarely been explicitly taken into account and encoded.", "labels": [], "entities": []}, {"text": "Yet, many of the natural tasks in which focus annotation would be relevant actually do contain explicit task and context information of relevance to determining focus.", "labels": [], "entities": []}, {"text": "To move things forward, this paper builds on the availability and relevance of task-based language data and presents an annotation study of focus on authentic reading comprehension data.", "labels": [], "entities": []}, {"text": "As a second component of our proposal, we operationalize the focus annotation in terms of several incremental steps, such as explicit answer typing, which provide relevant information guiding the focus annotation as such.", "labels": [], "entities": []}, {"text": "Overall, the paper tries to accomplish two goals, which are also reflected in the way the annotation is evaluated: i) to present an effective focus annotation scheme and to evaluate how consistently it can be applied, and ii) to explore the possible impact of focus annotation on Short Answer Assessment.", "labels": [], "entities": [{"text": "Short Answer Assessment", "start_pos": 280, "end_pos": 303, "type": "TASK", "confidence": 0.8408541083335876}]}, {"text": "Establishing a focus annotation scheme for question-response pairs from authentic reading comprehension data involves sharpening and linking the concepts and tests from theoretical linguistic with the wide range of properties realized in the authentic reading comprehension data.", "labels": [], "entities": []}, {"text": "The work thus stands to contribute both to an empirical evaluation and enrichment of the linguistic concepts as well as to the development of automatic focus annotation approaches in computational linguistics.", "labels": [], "entities": []}, {"text": "The paper is organized as follows: Section 2 presents the corpus data on which we base the annotation effort and the annotation process.", "labels": [], "entities": []}, {"text": "Section 3 introduces the scheme we developed for annotating the reading comprehension data.", "labels": [], "entities": []}, {"text": "Section 4 then launches into both intrinsic and extrinsic evaluation of the manual annotation, before section 5 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "The approach is evaluated in two ways.", "labels": [], "entities": []}, {"text": "First, the consistency with which the focus annotation scheme was applied is evaluated in section 4.1 by calculating inter-annotator agreement.", "labels": [], "entities": [{"text": "consistency", "start_pos": 11, "end_pos": 22, "type": "METRIC", "confidence": 0.9929003715515137}]}, {"text": "In section 4.2 we then explore the effect of focus annotation on Short Answer Assessment.", "labels": [], "entities": [{"text": "Short Answer Assessment", "start_pos": 65, "end_pos": 88, "type": "TASK", "confidence": 0.7633391817410787}]}, {"text": "For both evaluations, we provide a qualitative discussion of characteristic examples.", "labels": [], "entities": []}, {"text": "It has been pointed out that evaluating an expert annotation of a theoretical linguistic notion only intrinsically is problematic because there is no non-theoretical grounding involved.", "labels": [], "entities": []}, {"text": "Therefore, besides calculating agreement measures, we also evaluated the resulting annotation in a larger computational task, the automatic meaning assessment of answers to reading comprehension questions.", "labels": [], "entities": [{"text": "meaning assessment of answers to reading comprehension questions", "start_pos": 140, "end_pos": 204, "type": "TASK", "confidence": 0.8233509659767151}]}, {"text": "We used the CoMiC system (Comparing Meaning in Context, Meurers et al., 2011) as a testbed for our experiment.", "labels": [], "entities": []}, {"text": "CoMiC is an alignment-based system operating in three stages: 1.", "labels": [], "entities": [{"text": "CoMiC", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9136417508125305}]}, {"text": "Annotating linguistic units (words, chunks and dependencies) in student and target answer on various levels of abstraction 2.", "labels": [], "entities": []}, {"text": "Finding alignments of linguistic units between student and target answer based on annotation 3.", "labels": [], "entities": []}, {"text": "Classifying the student answer based on number and type of alignments, using a supervised machine learning setup with 13 features in total In stage 2, CoMiC integrates a simplistic approach to givenness, excluding all words from alignment that are mentioned in the question.", "labels": [], "entities": []}, {"text": "We transferred the underlying method to the notion of focus and implemented a component that excludes all non-focused words from alignment, resulting in alignments between focused parts of answers only.", "labels": [], "entities": []}, {"text": "We only used the foci where students did not ignore the question according to the annotators.", "labels": [], "entities": []}, {"text": "For the present evaluation, we experimented with three different settings involving the basic givenness filter and our focus annotations: i) using the givenness filter by itself as a baseline, ii) aligning only focused tokens as described above and iii) combining both by producing a givenness and a focus version of each classification feature.", "labels": [], "entities": []}, {"text": "All three settings were tried out for annotator 1 and 2.: Answer classification accuracy with the CoMiC system While this is an encouraging result already, the combination of basic givenness and focus performs substantially better, reaching 90.3% accuracy for annotator 1 and 89.3% for annotator 2.", "labels": [], "entities": [{"text": "Answer classification", "start_pos": 58, "end_pos": 79, "type": "TASK", "confidence": 0.9071953296661377}, {"text": "accuracy", "start_pos": 80, "end_pos": 88, "type": "METRIC", "confidence": 0.9912573099136353}, {"text": "accuracy", "start_pos": 247, "end_pos": 255, "type": "METRIC", "confidence": 0.9984599351882935}]}], "tableCaptions": [{"text": " Table 3: Inter-annotator agreement on student and target answers", "labels": [], "entities": []}, {"text": " Table 4: Answer classification accuracy with the CoMiC system", "labels": [], "entities": [{"text": "Answer classification", "start_pos": 10, "end_pos": 31, "type": "TASK", "confidence": 0.9188545048236847}, {"text": "CoMiC", "start_pos": 50, "end_pos": 55, "type": "DATASET", "confidence": 0.8759196400642395}]}]}