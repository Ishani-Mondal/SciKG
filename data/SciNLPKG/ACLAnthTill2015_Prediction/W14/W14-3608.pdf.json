{"title": [{"text": "Automatic Arabic diacritics restoration based on deep nets", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper, Arabic diacritics restoration problem is tackled under the deep learning framework presenting Confused Subset Resolution (CSR) method to improve the classification accuracy, in addition to Arabic Part-of-Speech (PoS) tagging framework using deep neural nets.", "labels": [], "entities": [{"text": "Arabic diacritics restoration", "start_pos": 15, "end_pos": 44, "type": "TASK", "confidence": 0.6308252314726511}, {"text": "accuracy", "start_pos": 179, "end_pos": 187, "type": "METRIC", "confidence": 0.901699423789978}, {"text": "Arabic Part-of-Speech (PoS) tagging", "start_pos": 204, "end_pos": 239, "type": "TASK", "confidence": 0.6115049223105112}]}, {"text": "Special focus is given to syntactic diacritiza-tion, which still suffer low accuracy as indicated by related works.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 76, "end_pos": 84, "type": "METRIC", "confidence": 0.9987296462059021}]}, {"text": "Evaluation is done versus state-of-the-art systems reported in literature, with quite challenging datasets, collected from different domains.", "labels": [], "entities": []}, {"text": "Standard datasets like LDC Arab-ic Tree Bank is used in addition to custom ones available online for results rep-lication.", "labels": [], "entities": [{"text": "LDC Arab-ic Tree Bank", "start_pos": 23, "end_pos": 44, "type": "DATASET", "confidence": 0.871365875005722}]}, {"text": "Results show significant improvement of the proposed techniques over other approaches, reducing the syntactic classification error to 9.9% and morphological classification error to 3% compared to 12.7% and 3.8% of the best reported results in literature, improving the error by 22% over the best reported systems", "labels": [], "entities": [{"text": "syntactic classification error", "start_pos": 100, "end_pos": 130, "type": "TASK", "confidence": 0.7839911381403605}, {"text": "morphological classification error", "start_pos": 143, "end_pos": 177, "type": "TASK", "confidence": 0.6551855107148489}, {"text": "the error", "start_pos": 265, "end_pos": 274, "type": "METRIC", "confidence": 0.7152369916439056}]}], "introductionContent": [{"text": "Arabic is a widespread language spoken by over 350 million people on the planet.", "labels": [], "entities": []}, {"text": "Arabic alphabet and vocabulary are very rich, with the same word morphology being a candidate of different meanings and pronunciations.", "labels": [], "entities": []}, {"text": "For example the word might bear the meaning of the person name \"Omar\" \u0652\ud97b\udf59 \u064e \ud97b\udf59\u064f or the meaning of \"age\" \u0652\ud97b\udf59\u0652 \ud97b\udf59\u064f . What distinguish them is the diacritization signs assigned to each character of the word.", "labels": [], "entities": []}, {"text": "Diacritics are marks added on the character to reflect its correct pronunciation, according to grammatical, syntactical and morphological rules of the language.", "labels": [], "entities": []}, {"text": "Nowadays, Modern Standard Arabic (MSA) transcripts are written without diacritics, left to the ability of the reader to restore them from the context and knowledge.", "labels": [], "entities": [{"text": "Modern Standard Arabic (MSA) transcripts", "start_pos": 10, "end_pos": 50, "type": "DATASET", "confidence": 0.7181457664285388}]}, {"text": "Diacritics restoration is not an easy task even for knowledgeable, native Arabic speakers.", "labels": [], "entities": [{"text": "Diacritics restoration", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8571318089962006}]}, {"text": "On the other hand, there are many machine learning tasks, like Text-ToSpeech (TTS), translation, spelling correction, word sense disambiguation,\u2026etc, that require diacritizing the script as a pre-processing step before applying the core application technique.", "labels": [], "entities": [{"text": "translation", "start_pos": 84, "end_pos": 95, "type": "TASK", "confidence": 0.9789825677871704}, {"text": "spelling correction", "start_pos": 97, "end_pos": 116, "type": "TASK", "confidence": 0.8762090504169464}, {"text": "word sense disambiguation", "start_pos": 118, "end_pos": 143, "type": "TASK", "confidence": 0.6997791727383932}]}, {"text": "In its basic form, the problem can be reduced to a pattern classification problem, with seven diacritics classes being the targets.", "labels": [], "entities": [{"text": "pattern classification problem", "start_pos": 51, "end_pos": 81, "type": "TASK", "confidence": 0.7913536926110586}]}, {"text": "In addition, the diacritics classification can be divided into syntactical diacritization, caring about caseending and morphological diacritization, caring about the rest of the word diacritics.", "labels": [], "entities": []}, {"text": "So far, morphological part of the problem is almost solved, leaving a marginal error of around 3-4%,).", "labels": [], "entities": []}, {"text": "On the other hand, syntactical diacritization errors are still high, hitting a ceiling that is claimed to be asymptotic and cannot be squeezed any further,.", "labels": [], "entities": []}, {"text": "For this reason, we focus our effort to squeeze this error beyond the least 12.5% error obtained in.", "labels": [], "entities": []}, {"text": "Recently, a significant advancement in the area of deep learning has been witnessed, with the development of a generative model; Deep Belief Nets (DBN), with a fast algorithm for inference of the model parameters.", "labels": [], "entities": []}, {"text": "Deep Neural Networks (DNN) shall be the basic machine learning classifier used in this work, employing the latest results reached in the deep learning field.", "labels": [], "entities": []}, {"text": "An efficient features' vector is designed under the umbrella of deep learning to distinguish different words diacritics.", "labels": [], "entities": []}, {"text": "Features that are tested in the current work are: PoS, morphological quadruple of lexemes, last character and word identity.", "labels": [], "entities": [{"text": "PoS", "start_pos": 50, "end_pos": 53, "type": "METRIC", "confidence": 0.9679365754127502}, {"text": "word identity", "start_pos": 110, "end_pos": 123, "type": "TASK", "confidence": 0.6930881440639496}]}, {"text": "In addition, context features are essential to the diacritization problem.", "labels": [], "entities": []}, {"text": "Context features include, the previous word features, as well as the previous word diacritic.", "labels": [], "entities": []}, {"text": "Part-of-Speech (PoS) features are critical to syntactic diacritization, which is the focus of this work.", "labels": [], "entities": [{"text": "syntactic diacritization", "start_pos": 46, "end_pos": 70, "type": "TASK", "confidence": 0.7735371887683868}]}, {"text": "For some datasets PoS tags are manually annotated by professional linguistics, while for the real case and most datasets, they are not available.", "labels": [], "entities": []}, {"text": "For this reason, standalone PoS taggers are built under the deep learning framework, which can reused in Arabic PoS tagging systems, needed for many other applications, not only for Arabic diacritization.", "labels": [], "entities": [{"text": "PoS taggers", "start_pos": 28, "end_pos": 39, "type": "TASK", "confidence": 0.8433448076248169}, {"text": "PoS tagging", "start_pos": 112, "end_pos": 123, "type": "TASK", "confidence": 0.6229575574398041}]}, {"text": "The deep learning model often hit a performance barrier which cannot be crossed.", "labels": [], "entities": []}, {"text": "Hence, error analysis and diagnosis is run on the confusion matrix results, proposing the Confused Subset Resolution (CSR) method to train subclassifiers to resolve the identified confusions and automatically generate a deep network-ofnetworks composed of the main classifier and the sub-classifiers working together to offer improved accuracy system purified of the identified confusions, offering around 2% error enhancement.", "labels": [], "entities": [{"text": "Confused Subset Resolution (CSR)", "start_pos": 90, "end_pos": 122, "type": "TASK", "confidence": 0.7565815448760986}, {"text": "accuracy", "start_pos": 335, "end_pos": 343, "type": "METRIC", "confidence": 0.9982184767723083}]}, {"text": "Evaluation of the proposed techniques is done on two datasets; the first is a custom one collected from many different sources, which is available online at (http://www.RDIeg.com/RDI/TrainingData is whereto download TRN_DB_II).", "labels": [], "entities": []}, {"text": "Manually extracted PoS and morphological quadruples are available for only apart of this dataset.", "labels": [], "entities": [{"text": "PoS", "start_pos": 19, "end_pos": 22, "type": "METRIC", "confidence": 0.5813664793968201}]}, {"text": "The PoS tags of this part of the dataset were used to build the DNN PoS taggers to tag the rest of the dataset.", "labels": [], "entities": []}, {"text": "The corresponding test set is available online at (http://www.RDI-eg.com/RDI/TestData is whereto download TST_DB), which is quite challenging and collected from different sources than training ones.", "labels": [], "entities": []}, {"text": "The second dataset is the standard LDC Arabic Tree Bank dataset LDC Arabic Tree Bank Part 3, (http://www.ldc.upenn.edu/Catalog/catalogEntry.", "labels": [], "entities": [{"text": "LDC Arabic Tree Bank dataset LDC Arabic Tree Bank Part 3", "start_pos": 35, "end_pos": 91, "type": "DATASET", "confidence": 0.9046871824698015}]}, {"text": "jsp?catalogId=LDC2005T20) used to benchmark the system against state-of-the art systems in Arabic diacritization area.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows: first the related works in literature are surveyed, followed by a formulation of the CSR method.", "labels": [], "entities": []}, {"text": "The next section is dedicated to describing the features used in the system, and how they are encoded and represented in the features' vector followed by the details of building the DNN PoS tagger for Arabic.", "labels": [], "entities": []}, {"text": "The datasets used for evaluation are then described.", "labels": [], "entities": []}, {"text": "The next section describes the system evaluation experiments.", "labels": [], "entities": [{"text": "system evaluation", "start_pos": 31, "end_pos": 48, "type": "TASK", "confidence": 0.8134362697601318}]}, {"text": "Experimental results include an error analysis study of the effect of each feature and method on the system performance, in addition to benchmarking against state-of-the art systems in literature, evaluated on standard datasets.", "labels": [], "entities": []}, {"text": "Finally, the paper is concluded with the main results and conclusion.", "labels": [], "entities": []}], "datasetContent": [{"text": "In all the coming experiments one of the following datasets is used: -TRN_DB_I: This is a 750,000 words dataset, collected from different sources and manually annotated by expert linguistics with every word PoS and Morphological quadruples.", "labels": [], "entities": [{"text": "TRN_DB_I", "start_pos": 70, "end_pos": 78, "type": "METRIC", "confidence": 0.9341799855232239}]}, {"text": "-TRN_DB_II: This is 2500,000 words train set.", "labels": [], "entities": [{"text": "TRN_DB_II", "start_pos": 1, "end_pos": 10, "type": "DATASET", "confidence": 0.8302095770835877}]}, {"text": "-TST_DB: This is 11,000 words test data set.", "labels": [], "entities": [{"text": "TST_DB", "start_pos": 1, "end_pos": 7, "type": "METRIC", "confidence": 0.7467782298723856}]}, {"text": "For more information refer to).", "labels": [], "entities": []}, {"text": "-ATB: LDC Arabic Tree Bank.", "labels": [], "entities": [{"text": "ATB: LDC Arabic Tree Bank", "start_pos": 1, "end_pos": 26, "type": "DATASET", "confidence": 0.9520081381003062}]}, {"text": "For TRN_DB_I, PoS tags are available as ready features added manually.", "labels": [], "entities": [{"text": "TRN_DB_I", "start_pos": 4, "end_pos": 12, "type": "TASK", "confidence": 0.5213491737842559}]}, {"text": "When the manually PoS tags are used as input features, the dataset is referred to as TRN_DB_I -Ready PoS.", "labels": [], "entities": [{"text": "TRN_DB_I -Ready", "start_pos": 85, "end_pos": 100, "type": "METRIC", "confidence": 0.770740977355412}]}, {"text": "While, when our PoS-DNN nets are used, a derivative dataset with only raw text is referred as TRN_DB_I -Raw text.", "labels": [], "entities": []}, {"text": "The objective of this experiment is to show the effect of CSR method.", "labels": [], "entities": []}, {"text": "The test set is TST_DB.", "labels": [], "entities": [{"text": "TST_DB", "start_pos": 16, "end_pos": 22, "type": "DATASET", "confidence": 0.5135394235452017}]}, {"text": "Results in TABLE III show improvement around 2% in all tested datasets.", "labels": [], "entities": [{"text": "TABLE III", "start_pos": 11, "end_pos": 20, "type": "TASK", "confidence": 0.5664306282997131}]}, {"text": "This represents 17.09% improvement of error.", "labels": [], "entities": [{"text": "error", "start_pos": 38, "end_pos": 43, "type": "METRIC", "confidence": 0.9874302744865417}]}], "tableCaptions": []}