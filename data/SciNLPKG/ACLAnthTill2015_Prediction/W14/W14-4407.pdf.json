{"title": [{"text": "A Template-based Abstractive Meeting Summarization: Leveraging Summary and Source Text Relationships", "labels": [], "entities": [{"text": "Template-based Abstractive Meeting Summarization", "start_pos": 2, "end_pos": 50, "type": "TASK", "confidence": 0.6655623018741608}]}], "abstractContent": [{"text": "In this paper, we present an automatic abstractive summarization system of meeting conversations.", "labels": [], "entities": []}, {"text": "Our system extends a novel multi-sentence fusion algorithm in order to generate abstract templates.", "labels": [], "entities": [{"text": "multi-sentence fusion", "start_pos": 27, "end_pos": 48, "type": "TASK", "confidence": 0.7337219715118408}]}, {"text": "It also leverages the relationship between summaries and their source meeting transcripts to select the best templates for generating abstractive summaries of meetings.", "labels": [], "entities": []}, {"text": "Our manual and automatic evaluation results demonstrate the success of our system in achieving higher scores both in readability and in-formativeness.", "labels": [], "entities": []}], "introductionContent": [{"text": "People spend avast amount of time in meetings and these meetings play a prominent role in their lives.", "labels": [], "entities": []}, {"text": "Consequently, study of automatic meeting summarization has been attracting peoples' attention as it can save a great deal of their time and increase their productivity.", "labels": [], "entities": [{"text": "automatic meeting summarization", "start_pos": 23, "end_pos": 54, "type": "TASK", "confidence": 0.6766927142937978}]}, {"text": "The most common approaches to automatic meeting summarization have been extractive.", "labels": [], "entities": [{"text": "meeting summarization", "start_pos": 40, "end_pos": 61, "type": "TASK", "confidence": 0.7435307204723358}]}, {"text": "Since extractive approaches do not require natural language generation techniques, they are arguably simpler to apply and have been extensively investigated.", "labels": [], "entities": [{"text": "natural language generation", "start_pos": 43, "end_pos": 70, "type": "TASK", "confidence": 0.7807770570119222}]}, {"text": "However, a user study conducted by indicates that users prefer abstractive summaries to extractive ones.", "labels": [], "entities": []}, {"text": "Thereafter, more attention has been paid to abstractive meeting summarization systems;.", "labels": [], "entities": [{"text": "abstractive meeting summarization", "start_pos": 44, "end_pos": 77, "type": "TASK", "confidence": 0.5705719987551371}]}, {"text": "However, the approaches introduced in previous studies create summaries by either heavily relying on annotated data or by fusing human utterances which may contain grammatical mistakes.", "labels": [], "entities": [{"text": "summaries", "start_pos": 62, "end_pos": 71, "type": "TASK", "confidence": 0.9778332114219666}]}, {"text": "In this paper, we address these issues by introducing a novel summarization approach that can create readable summaries with less need for annotated data.", "labels": [], "entities": [{"text": "summarization", "start_pos": 62, "end_pos": 75, "type": "TASK", "confidence": 0.9861889481544495}]}, {"text": "Our system first acquires templates from human-authored summaries using a clustering and multi-sentence fusion algorithm.", "labels": [], "entities": []}, {"text": "It then takes a meeting transcript to be summarized, segments the transcript based on topics, and extracts important phrases from it.", "labels": [], "entities": []}, {"text": "Finally, our system selects templates by referring to the relationship between humanauthored summaries and their sources and fills the templates with the phrases to create summaries.", "labels": [], "entities": []}, {"text": "The main contributions of this paper are: 1) The successful adaptation of a word graph algorithm to generate templates from humanauthored summaries; 2) The implementation of a novel template selection algorithm that effectively leverages the relationship between humanauthored summary sentences and their source transcripts; and 3) A comprehensive testing of our approach, comprising both automatic and manual evaluations.", "labels": [], "entities": []}, {"text": "We instantiate our framework on the AMI corpus) and compare our summaries with those created from a state-ofthe-art systems.", "labels": [], "entities": [{"text": "AMI corpus", "start_pos": 36, "end_pos": 46, "type": "DATASET", "confidence": 0.894345223903656}]}, {"text": "The evaluation results demonstrate that our system successfully creates informative and readable summaries.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we describe an evaluation of our system.", "labels": [], "entities": []}, {"text": "First, we describe the corpus data.", "labels": [], "entities": []}, {"text": "Next, the results of the automatic and manual evaluations of our system against various baseline approaches are discussed.", "labels": [], "entities": []}, {"text": "We report the F1-measure of ROUGE-1, ROUGE-2 and ROUGE-SU4 ( to assess the performance of our system.", "labels": [], "entities": [{"text": "F1-measure", "start_pos": 14, "end_pos": 24, "type": "METRIC", "confidence": 0.9992160797119141}, {"text": "ROUGE-1", "start_pos": 28, "end_pos": 35, "type": "METRIC", "confidence": 0.9686000347137451}, {"text": "ROUGE-2", "start_pos": 37, "end_pos": 44, "type": "METRIC", "confidence": 0.8626601099967957}, {"text": "ROUGE-SU4", "start_pos": 49, "end_pos": 58, "type": "METRIC", "confidence": 0.9163132905960083}]}, {"text": "The scores of automatically generated summaries are calculated by comparing them with humanauthored ones.", "labels": [], "entities": []}, {"text": "For our baselines, we use the system introduced by, which creates abstractive summaries from extracted sentences and was proven to be effective in creating abstractive meeting summaries; and TextRank (), a graph based sentence ranker that is suitable for creating extractive summaries.", "labels": [], "entities": []}, {"text": "Our system can create summaries of any length by adjusting the number of segments to be created by LCSeg.", "labels": [], "entities": []}, {"text": "Thus, we create summaries of three different lengths (10, 15, and 20 topic segments) with the average number of words being 100, 137, and 173, respectively.", "labels": [], "entities": []}, {"text": "These numbers generally corresponds to humanauthored summary length in the corpus which varies from 82 to 200 words.", "labels": [], "entities": []}, {"text": "shows the results of our system in comparison with those of the two baselines.", "labels": [], "entities": []}, {"text": "The results show that our model significantly outperforms the two baselines.", "labels": [], "entities": []}, {"text": "Compared with FU-SION, our system with 20 segments achieves about 3 % of improvement in all ROUGE scores.", "labels": [], "entities": [{"text": "FU-SION", "start_pos": 14, "end_pos": 21, "type": "DATASET", "confidence": 0.5160622000694275}, {"text": "ROUGE", "start_pos": 92, "end_pos": 97, "type": "METRIC", "confidence": 0.989800751209259}]}, {"text": "This indicates that our system creates summaries that are more lexically similar to human-authored ones.", "labels": [], "entities": []}, {"text": "Surprisingly, there was not a significant change in our ROUGE scores over the three different summary lengths.", "labels": [], "entities": [{"text": "ROUGE scores", "start_pos": 56, "end_pos": 68, "type": "METRIC", "confidence": 0.9636077284812927}]}, {"text": "This indicates that our system can create summaries of any length without losing its content.", "labels": [], "entities": []}, {"text": "We also conduct manual evaluations utilizing a crowdsourcing tool 1 . In this experiment, our system with 15 segments is compared with FUSION, human-authored summaries (ABS) and, humanannotated extractive summaries (EXT).", "labels": [], "entities": [{"text": "FUSION", "start_pos": 135, "end_pos": 141, "type": "METRIC", "confidence": 0.9871292114257812}]}, {"text": "After randomly selecting 10 meetings, 10 participants were selected for each meeting and given instructions to browse the transcription of the meeting so as to understand its gist.", "labels": [], "entities": []}, {"text": "The results are described in.", "labels": [], "entities": []}, {"text": "Overall, 58 people worldwide, who are among the most reliable contributors accounting for 7 % of overall members and who maintain the highest levels of accuracy on test questions provided in pervious crowd sourcing jobs, participated in this rating task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 152, "end_pos": 160, "type": "METRIC", "confidence": 0.9979562759399414}]}, {"text": "As to statistical significance, we use the 2-tail pairwise t-test to compare our system with the other three approaches.", "labels": [], "entities": []}, {"text": "The results are summarized in.", "labels": [], "entities": []}, {"text": "As expected, for all of the three items, ABS received the highest of all ratings, while our system received the second highest.", "labels": [], "entities": [{"text": "ABS", "start_pos": 41, "end_pos": 44, "type": "METRIC", "confidence": 0.703114926815033}]}, {"text": "The t-test results indicate that the difference in the rating data is statistically significant for all cases except that of informativeness between ours and the extractive summaries.", "labels": [], "entities": []}, {"text": "This can be understood because the extractive summaries were manually created by an annotator and contain all of the important information in the meetings.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: An evaluation of summarization performance  using the F1 measure of ROUGE-1 2, and SU4", "labels": [], "entities": [{"text": "summarization", "start_pos": 27, "end_pos": 40, "type": "TASK", "confidence": 0.9824839234352112}, {"text": "F1 measure", "start_pos": 64, "end_pos": 74, "type": "METRIC", "confidence": 0.9766480326652527}, {"text": "ROUGE-1", "start_pos": 78, "end_pos": 85, "type": "METRIC", "confidence": 0.9685169458389282}, {"text": "SU4", "start_pos": 93, "end_pos": 96, "type": "TASK", "confidence": 0.37645643949508667}]}, {"text": " Table 3. Overall,  58 people worldwide, who are among the most  reliable contributors accounting for 7 % of over- all members and who maintain the highest levels  of accuracy on test questions provided in pervi- ous crowd sourcing jobs, participated in this rat- ing task. As to statistical significance, we use the  2-tail pairwise t-test to compare our system with  the other three approaches. The results are sum- marized in", "labels": [], "entities": [{"text": "accuracy", "start_pos": 167, "end_pos": 175, "type": "METRIC", "confidence": 0.9974283576011658}, {"text": "sum- marized", "start_pos": 413, "end_pos": 425, "type": "METRIC", "confidence": 0.763782799243927}]}, {"text": " Table 4: T-test results of manual evaluation", "labels": [], "entities": [{"text": "T-test", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9972699284553528}]}]}