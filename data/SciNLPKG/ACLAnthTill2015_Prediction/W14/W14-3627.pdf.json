{"title": [{"text": "Domain and Dialect Adaptation for Machine Translation into Egyptian Arabic", "labels": [], "entities": [{"text": "Machine Translation into Egyptian Arabic", "start_pos": 34, "end_pos": 74, "type": "TASK", "confidence": 0.8204702377319336}]}], "abstractContent": [{"text": "In this paper, we present a statistical machine translation system for English to Di-alectal Arabic (DA), using Modern Standard Arabic (MSA) as a pivot.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 28, "end_pos": 59, "type": "TASK", "confidence": 0.6090235511461893}, {"text": "Modern Standard Arabic (MSA)", "start_pos": 112, "end_pos": 140, "type": "DATASET", "confidence": 0.8584970136483511}]}, {"text": "We create a core system to translate from En-glish to MSA using a large bilingual parallel corpus.", "labels": [], "entities": []}, {"text": "Then, we design two separate pathways for translation from MSA into DA: a two-step domain and dialect adaptation system and a one-step simultaneous domain and dialect adaptation system.", "labels": [], "entities": []}, {"text": "Both variants of the adaptation systems are trained on a 100k sentence tri-parallel corpus of English, MSA, and Egyptian Arabic generated by a rule-based transformation.", "labels": [], "entities": []}, {"text": "We test our systems on a held-out Egyp-tian Arabic test set from the 100k sentence corpus and we achieve our best performance using the two-step domain and dialect adaptation system with a BLEU score of 42.9.", "labels": [], "entities": [{"text": "Egyp-tian Arabic test set from the 100k sentence corpus", "start_pos": 34, "end_pos": 89, "type": "DATASET", "confidence": 0.7843863997194502}, {"text": "BLEU", "start_pos": 189, "end_pos": 193, "type": "METRIC", "confidence": 0.9995521903038025}]}], "introductionContent": [{"text": "While MSA is the shared official language of culture, media and education in the Arab world, it is not the native language of any speakers of Arabic.", "labels": [], "entities": []}, {"text": "Most native speakers are unable to produce sustained spontaneous discourse in MSA -they usually resort to repeated code-switching between their dialect and MSA (.", "labels": [], "entities": []}, {"text": "Arabic speakers are quite aware of the contextual factors and the differences between their dialects and MSA, although they may not always be able to pinpoint exact linguistic differences.", "labels": [], "entities": []}, {"text": "In the context of natural language processing (NLP), some Arabic dialects have started receiving increasing attention, particularly in the context of machine translation () and in terms of data collection) and basic enabling technologies ().", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 18, "end_pos": 51, "type": "TASK", "confidence": 0.824353963136673}, {"text": "machine translation", "start_pos": 150, "end_pos": 169, "type": "TASK", "confidence": 0.7652880847454071}]}, {"text": "However, the focus is on a small number of iconic dialects, (e.g., Egyptian).", "labels": [], "entities": []}, {"text": "The Egyptian media industry has traditionally played a dominant role in the Arab world, making the Egyptian dialect the most widely understood and used dialect.", "labels": [], "entities": []}, {"text": "DA is now emerging as the language of informal communication online.", "labels": [], "entities": []}, {"text": "DA differs phonologically, lexically, morphologically, and syntactically from MSA.", "labels": [], "entities": []}, {"text": "And while MSA has an established standard orthography, the dialects do not: people write words reflecting their phonology and sometimes use roman script.", "labels": [], "entities": []}, {"text": "Thus, MSA tools cannot effectively model DA; for instance, over one-third of Levantine verbs cannot be analyzed using an MSA morphological analyzer).", "labels": [], "entities": []}, {"text": "These differences make the direct use of MSA NLP tools and applications for handling dialects impractical.", "labels": [], "entities": []}, {"text": "In this work, we design an MT system for English to Egyptian Arabic translation by using MSA as an intermediary step.", "labels": [], "entities": [{"text": "MT", "start_pos": 27, "end_pos": 29, "type": "TASK", "confidence": 0.9892527461051941}, {"text": "English to Egyptian Arabic translation", "start_pos": 41, "end_pos": 79, "type": "TASK", "confidence": 0.6434369087219238}]}, {"text": "This includes different challenges from those faced when translating into English.", "labels": [], "entities": [{"text": "translating into English", "start_pos": 57, "end_pos": 81, "type": "TASK", "confidence": 0.8686640858650208}]}, {"text": "Because MSA is the formal written variety of Arabic, there is an abundance of written data, including parallel corpora from sources like the United Nations and newspapers, as well as various treebanks.", "labels": [], "entities": []}, {"text": "Using these resources, many researchers have created fairly reliable MSA translation systems.", "labels": [], "entities": [{"text": "MSA translation", "start_pos": 69, "end_pos": 84, "type": "TASK", "confidence": 0.9577482342720032}]}, {"text": "However, these systems are not designed to deal with the other Arabic variants.", "labels": [], "entities": []}, {"text": "Egyptian Arabic is much closer to MSA than it is to English, so one can get a system bet-ter performance by translating first into MSA and then translating from MSA to Egyptian Arabic, which are far more similar.", "labels": [], "entities": []}, {"text": "Our approach consists of a core MT system trained on a large amount of out-of-domain English-MSA parallel data, followed by an adaptation system.", "labels": [], "entities": [{"text": "MT", "start_pos": 32, "end_pos": 34, "type": "TASK", "confidence": 0.9870866537094116}]}, {"text": "We design and implement two adaptation systems: a two-step system first adapts to in-domain MSA and then separately adapts from MSA to Egyptian Arabic, and a one-step system that adapts directly from out-ofdomain MSA to in-domain Egyptian Arabic.", "labels": [], "entities": []}, {"text": "Our research contributions are summarized as follows: (a) We build a machine translation system to translate into, rather than out of, dialectal Arabic (from English), using MSA as a pivot point.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 69, "end_pos": 88, "type": "TASK", "confidence": 0.7252689003944397}]}, {"text": "The remainder of this paper is structured as follows.", "labels": [], "entities": []}, {"text": "We first review the main previous efforts for dealing with DA in NLP, in Section 2.", "labels": [], "entities": [{"text": "DA in NLP", "start_pos": 59, "end_pos": 68, "type": "TASK", "confidence": 0.611744225025177}]}, {"text": "In Section 3,we give a general description about using phrase-based MT as an adaptation system.", "labels": [], "entities": [{"text": "MT", "start_pos": 68, "end_pos": 70, "type": "TASK", "confidence": 0.7653656005859375}]}, {"text": "Section 4 presents the dataset used in the different experiments.", "labels": [], "entities": []}, {"text": "Our approach for translating English text into Egyptian Arabic is explained in Section 5.", "labels": [], "entities": [{"text": "translating English text", "start_pos": 17, "end_pos": 41, "type": "TASK", "confidence": 0.8694558143615723}]}, {"text": "Section 6 presents our experimental setup and the results obtained.", "labels": [], "entities": []}, {"text": "Then, we give an analysis of our system output in Section 7.", "labels": [], "entities": []}, {"text": "Finally, we conclude and describe our future work in Section 8.", "labels": [], "entities": []}], "datasetContent": [{"text": "For evaluation we use multeval) to calculate BLEU (), METEOR (Denkowski and Lavie, 2011), TER (), and length of the test set for each system.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 45, "end_pos": 49, "type": "METRIC", "confidence": 0.9994009733200073}, {"text": "METEOR", "start_pos": 54, "end_pos": 60, "type": "METRIC", "confidence": 0.9984018206596375}, {"text": "TER", "start_pos": 90, "end_pos": 93, "type": "METRIC", "confidence": 0.9992396831512451}, {"text": "length", "start_pos": 102, "end_pos": 108, "type": "METRIC", "confidence": 0.9930093288421631}]}, {"text": "We evaluate the core and adaptation systems on the MSA and Egyptian sides of the test set drawn from the 100k corpus, which we refer to as the 100k sets.", "labels": [], "entities": [{"text": "MSA", "start_pos": 51, "end_pos": 54, "type": "DATASET", "confidence": 0.9057873487472534}]}, {"text": "The data used for evaluation is a genuine Egyptian Arabic generated from MSA, just like the data the systems were trained on.", "labels": [], "entities": [{"text": "MSA", "start_pos": 73, "end_pos": 76, "type": "DATASET", "confidence": 0.8713532090187073}]}, {"text": "It is not practical to evaluate on naturallygenerated Egyptian Arabic in this case because the domain of our datasets is very formal, since most of the text comes from news sources, and dialectal Arabic is generally used in informal situations.", "labels": [], "entities": []}, {"text": "Below we report BLEU scores from our evaluation using tokenized and detokenized system output.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 16, "end_pos": 20, "type": "METRIC", "confidence": 0.9985492825508118}]}, {"text": "We separate our results into the baseline system results, the results of the core, the results of the adaptation systems, and a comparison section.", "labels": [], "entities": []}, {"text": "We specify scores of intermediate system output, such as MSA, as BLEU (A), and the scores of final system output as BLEU (B).", "labels": [], "entities": [{"text": "BLEU (A)", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.9443203657865524}, {"text": "BLEU (B)", "start_pos": 116, "end_pos": 124, "type": "METRIC", "confidence": 0.9563725888729095}]}, {"text": "For error analysis, we use METEOR X-ray () to visualize the alignments of our system results with the references and each other.", "labels": [], "entities": [{"text": "error analysis", "start_pos": 4, "end_pos": 18, "type": "TASK", "confidence": 0.6868945509195328}, {"text": "METEOR X-ray", "start_pos": 27, "end_pos": 39, "type": "METRIC", "confidence": 0.911215603351593}]}, {"text": "For all MT systems we used grow-diag as our symmetrization heuristic.", "labels": [], "entities": [{"text": "MT", "start_pos": 8, "end_pos": 10, "type": "TASK", "confidence": 0.9685695171356201}]}, {"text": "For each system, we report only the BLEU score of the best reordering window variant, which is specified in the caption It is important to note that the Egyptian Arabic data we use is more MSA-like than typical Egyptian because it was generated directly from MSA.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 36, "end_pos": 46, "type": "METRIC", "confidence": 0.9787934720516205}]}, {"text": "The difference in scores between the different reordering window sizes (7, 4, and 0) we tried for the adaptation systems was not large (between 0 and 0.7 BLEU).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 154, "end_pos": 158, "type": "METRIC", "confidence": 0.9986937642097473}]}, {"text": "In the following tables we present the best results for each adaptation system, which is a reordering window size of 7 for all systems, except for the phrase-based onestep domain and dialect adaptation system, which performs better with no reordering (0.2 BLEU better than a window of 7, 0.6 BLEU better than a window of 4), but these small differences in BLEU scores are within noise.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 256, "end_pos": 260, "type": "METRIC", "confidence": 0.9901391267776489}, {"text": "BLEU", "start_pos": 292, "end_pos": 296, "type": "METRIC", "confidence": 0.9925360679626465}, {"text": "BLEU", "start_pos": 356, "end_pos": 360, "type": "METRIC", "confidence": 0.9960915446281433}]}, {"text": "The greatest difference in scores from the reordering windows was in the two-step systems domain adaptation step (MSA to MSA) on top of the phrase-based core, where a reordering window of 7 was 0.7 BLEU better than a window of 0.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 198, "end_pos": 202, "type": "METRIC", "confidence": 0.9984130859375}]}], "tableCaptions": [{"text": " Table 2: Core system (English\u00a8MSAEnglish\u00a8English\u00a8MSA) results  using a reordering window size of 7.", "labels": [], "entities": []}, {"text": " Table 4: Domain Adaptation system (MSA' \u00a8  MSA\") for Two-Step Adaptation System Results  using a reordering window size of 7.", "labels": [], "entities": []}, {"text": " Table 5: Dialect Adaptation system (MSA\" \u00a8  Egyptian) for Two-Step Adaptation System Re- sults using a reordering window size of 7.", "labels": [], "entities": []}, {"text": " Table 7: Comparison of results on 100k MSA test set.", "labels": [], "entities": [{"text": "MSA test set", "start_pos": 40, "end_pos": 52, "type": "DATASET", "confidence": 0.8281638622283936}]}, {"text": " Table 9: Detokenized BLEU, METEOR, TER, and length scores for the best system results.", "labels": [], "entities": [{"text": "Detokenized", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9847997426986694}, {"text": "BLEU", "start_pos": 22, "end_pos": 26, "type": "METRIC", "confidence": 0.9463768601417542}, {"text": "METEOR", "start_pos": 28, "end_pos": 34, "type": "METRIC", "confidence": 0.99703049659729}, {"text": "TER", "start_pos": 36, "end_pos": 39, "type": "METRIC", "confidence": 0.9946223497390747}, {"text": "length", "start_pos": 45, "end_pos": 51, "type": "METRIC", "confidence": 0.9993016719818115}]}, {"text": " Table 10: Detokenized BLEU (B) scores on the 100k EGY test set at different n-gram levels.", "labels": [], "entities": [{"text": "Detokenized", "start_pos": 11, "end_pos": 22, "type": "METRIC", "confidence": 0.9825478196144104}, {"text": "BLEU (B)", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9271330535411835}, {"text": "EGY test set", "start_pos": 51, "end_pos": 63, "type": "DATASET", "confidence": 0.8314716219902039}]}]}