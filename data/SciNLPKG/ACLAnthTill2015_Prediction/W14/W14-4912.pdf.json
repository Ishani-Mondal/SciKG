{"title": [{"text": "Multiple views as aid to linguistic annotation error analysis", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper describes a methodology for supporting the task of annotating sentiment in natural language by detecting borderline cases and inconsistencies.", "labels": [], "entities": [{"text": "annotating sentiment in natural language", "start_pos": 62, "end_pos": 102, "type": "TASK", "confidence": 0.8212498188018799}]}, {"text": "Inspired by the co-training strategy, a number of machine learning models are trained on different views of the same data.", "labels": [], "entities": []}, {"text": "The predictions obtained by these models are then automatically compared in order to bring to light highly uncertain annotations and systematic mistakes.", "labels": [], "entities": []}, {"text": "We tested the methodology against an English corpus annotated according to a fine-grained sentiment analysis annotation schema (SentiML).", "labels": [], "entities": []}, {"text": "We detected that 153 instances (35%) classified differently from the gold standard were acceptable and further 69 instances (16%) suggested that the gold standard should have been improved.", "labels": [], "entities": [{"text": "gold standard", "start_pos": 149, "end_pos": 162, "type": "DATASET", "confidence": 0.7973058521747589}]}], "introductionContent": [{"text": "This work pertains to the phase of testing the reliability of human annotation.", "labels": [], "entities": [{"text": "reliability", "start_pos": 47, "end_pos": 58, "type": "METRIC", "confidence": 0.9576974511146545}]}, {"text": "The strength of our approach relies on the fact that we use multiple supervised machine learning classifiers and analyse their predictions in parallel to automatically identify disagreements.", "labels": [], "entities": []}, {"text": "Those, in fact, ultimately lead to the discovery of borderline cases in the annotation, an expensive task in terms of time when carried out manually.", "labels": [], "entities": []}, {"text": "Predictions with a number of different labels are manually analysed, since they may indicate inconsistencies in the annotation and cases difficult to annotate.", "labels": [], "entities": []}, {"text": "Conversely, cases with high agreement suggest that the annotation schema is reliable.", "labels": [], "entities": []}, {"text": "On the one hand, the analysis of those disagreements, in conjunction with the gold annotations, provides fresh insights about the efficacy of the features provided to the classifiers for the learning phase.", "labels": [], "entities": []}, {"text": "On the other hand, when all the classifiers agree on a wrong annotation, it is a strong signal of ambiguity in the annotation schema and/or guidelines.", "labels": [], "entities": []}, {"text": "In Section 2 we briefly introduce the data to which we apply the methodology described in Section 3.", "labels": [], "entities": []}, {"text": "In Section 4 we report results.", "labels": [], "entities": []}, {"text": "In Section 5 we mention studies related to ours and in Section 6 we draw conclusions and identify steps for future work.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Performance of the classifiers trained on two views, lexical and syntactic. Experiments have  been performed using 10-fold cross-validation.", "labels": [], "entities": []}]}