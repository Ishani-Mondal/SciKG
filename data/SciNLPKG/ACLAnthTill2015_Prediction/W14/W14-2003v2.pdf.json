{"title": [{"text": "Sentence Processing in a Vectorial Model of Working Memory", "labels": [], "entities": [{"text": "Sentence Processing", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.947278767824173}]}], "abstractContent": [{"text": "This paper presents a vectorial incremen-tal parsing model defined using independently posited operations over activation-based working memory and weight-based episodic memory.", "labels": [], "entities": []}, {"text": "This model has the attractive property that it hypothesizes only one unary preterminal rule application and only one binary branching rule application per time step, which allows it to be smoothly integrated into a vector-based recurrence that propagates structural ambiguity from onetime step to the next.", "labels": [], "entities": []}, {"text": "Predictions of this model are calculated on a center-embedded sentence processing task and shown to exhibit decreased processing accuracy in center-embedded constructions .", "labels": [], "entities": [{"text": "accuracy", "start_pos": 129, "end_pos": 137, "type": "METRIC", "confidence": 0.9657818078994751}]}], "introductionContent": [{"text": "Current models of memory) involve a continuous activation-based (or 'working') memory, typically modeled as a vector representing the current firing pattern of neurons or neural clusters in the cortex.", "labels": [], "entities": []}, {"text": "This activation-based memory is then supported by a durable but rapidly mutable weight-based (or 'episodic') memory, typically modeled as one or more matrices formed by summed outer-products of cue and target vectors and cued by simple matrix multiplication, representing variable synaptic connection strengths between neurons or neural clusters.", "labels": [], "entities": []}, {"text": "The lack of discrete memory units in such models makes it difficult to imagine a neural implementation of atypical e.g. chart-based computational account of sentence processing.", "labels": [], "entities": [{"text": "sentence processing", "start_pos": 157, "end_pos": 176, "type": "TASK", "confidence": 0.6725079566240311}]}, {"text": "On the other hand, superposition in vectorial models suggests a natural representation of a parallel incremental processing model.", "labels": [], "entities": []}, {"text": "This paper explores how such an austere model of memory not only might be used to encode a simple probabilistic incremental parser, but also lends itself to naturally implement a vectorial interpreter and coreference resolver.", "labels": [], "entities": [{"text": "coreference resolver", "start_pos": 205, "end_pos": 225, "type": "TASK", "confidence": 0.8961843252182007}]}, {"text": "This model is based on the left-corner parser formulation of van, which has the attractive property of generating exactly one binary-branching rule application after processing each word.", "labels": [], "entities": []}, {"text": "This property greatly simplifies a vectorial implementation because it allows these single grammar rule applications to be superposed in cases of attachment ambiguity.", "labels": [], "entities": []}, {"text": "Predictions of the vectorial model described in this paper are then calculated on a simple centerembedded sentence processing task, producing a lower completion accuracy for center-embedded sentences than for right-branching sentences with the same number of words.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 161, "end_pos": 169, "type": "METRIC", "confidence": 0.6407068967819214}]}, {"text": "As noted by, this kind of memory effect is not easily explained by existing information-theoretic models of frequency effects.", "labels": [], "entities": []}, {"text": "The model described in this paper also provides an explanation for the apparent reality of linguistic objects like categories, grammar rules, discourse referents and dependency relations, as cognitive states in activation-based memory (in the case of categories and discourse referents), or cued associations in weight-based memory (in the case of grammar rules, and dependency relations), without having to posit complex machinery specific to language processing.", "labels": [], "entities": []}, {"text": "In this sense, unlike existing chart-based parsers or connectionist models based on recurrent neural networks, this model integrates familiar notions of grammar and semantic relations with current ideas of activation-based and weight-based memory.", "labels": [], "entities": []}, {"text": "It is also anticipated that this interface to both linguistic and neuroscientific theories will make the model useful as a basis for more nuanced understanding of linguistic phenomena such as ambiguity resolution, semantic representation, and language acquisition.", "labels": [], "entities": [{"text": "ambiguity resolution", "start_pos": 192, "end_pos": 212, "type": "TASK", "confidence": 0.7854878008365631}, {"text": "semantic representation", "start_pos": 214, "end_pos": 237, "type": "TASK", "confidence": 0.7294669449329376}, {"text": "language acquisition", "start_pos": 243, "end_pos": 263, "type": "TASK", "confidence": 0.7163660824298859}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Accuracy of vectorial parser on each sen- tence type.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9961483478546143}]}]}