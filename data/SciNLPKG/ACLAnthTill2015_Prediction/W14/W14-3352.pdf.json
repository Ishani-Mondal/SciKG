{"title": [{"text": "DiscoTK: Using Discourse Structure for Machine Translation Evaluation", "labels": [], "entities": [{"text": "DiscoTK", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.779320240020752}, {"text": "Machine Translation Evaluation", "start_pos": 39, "end_pos": 69, "type": "TASK", "confidence": 0.82796577612559}]}], "abstractContent": [{"text": "We present novel automatic metrics for machine translation evaluation that use discourse structure and convolution kernels to compare the discourse tree of an automatic translation with that of the human reference.", "labels": [], "entities": [{"text": "machine translation evaluation", "start_pos": 39, "end_pos": 69, "type": "TASK", "confidence": 0.8584272861480713}]}, {"text": "We experiment with five transformations and augmentations of abase discourse tree representation based on the rhetorical structure theory, and we combine the kernel scores for each of them into a single score.", "labels": [], "entities": []}, {"text": "Finally, we add other metrics from the ASIYA MT evaluation toolkit, and we tune the weights of the combination on actual human judgments.", "labels": [], "entities": [{"text": "ASIYA MT evaluation", "start_pos": 39, "end_pos": 58, "type": "TASK", "confidence": 0.7483871777852377}]}, {"text": "Experiments on the WMT12 and WMT13 metrics shared task datasets show correlation with human judgments that outper-forms what the best systems that participated in these years achieved, both at the segment and at the system level.", "labels": [], "entities": [{"text": "WMT12", "start_pos": 19, "end_pos": 24, "type": "DATASET", "confidence": 0.9374245405197144}, {"text": "WMT13 metrics shared task datasets", "start_pos": 29, "end_pos": 63, "type": "DATASET", "confidence": 0.8603285670280456}]}], "introductionContent": [{"text": "The rapid development of statistical machine translation (SMT) that we have seen in recent years would not have been possible without automatic metrics for measuring SMT quality.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 25, "end_pos": 62, "type": "TASK", "confidence": 0.8192124168078104}, {"text": "SMT", "start_pos": 166, "end_pos": 169, "type": "TASK", "confidence": 0.9613968729972839}]}, {"text": "In particular, the development of BLEU () revolutionized the SMT field, allowing not only to compare two systems in away that strongly correlates with human judgments, but it also enabled the rise of discriminative log-linear models, which use optimizers such as MERT, and later MIRA ( and PRO, to optimize BLEU, or an approximation thereof, directly.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 34, "end_pos": 38, "type": "METRIC", "confidence": 0.99205082654953}, {"text": "SMT", "start_pos": 61, "end_pos": 64, "type": "TASK", "confidence": 0.9965169429779053}, {"text": "MERT", "start_pos": 263, "end_pos": 267, "type": "METRIC", "confidence": 0.6151331663131714}, {"text": "MIRA", "start_pos": 279, "end_pos": 283, "type": "METRIC", "confidence": 0.7164463996887207}, {"text": "BLEU", "start_pos": 307, "end_pos": 311, "type": "METRIC", "confidence": 0.9802867770195007}]}, {"text": "While over the years other strong metrics such as TER () and Meteor ( have emerged, BLEU remains the de-facto standard, despite its simplicity.", "labels": [], "entities": [{"text": "TER", "start_pos": 50, "end_pos": 53, "type": "METRIC", "confidence": 0.9973516464233398}, {"text": "Meteor", "start_pos": 61, "end_pos": 67, "type": "METRIC", "confidence": 0.5045911073684692}, {"text": "BLEU", "start_pos": 84, "end_pos": 88, "type": "METRIC", "confidence": 0.9980065226554871}]}, {"text": "Recently, there has been steady increase in BLEU scores for well-resourced language pairs such as Spanish-English and Arabic-English.", "labels": [], "entities": [{"text": "BLEU scores", "start_pos": 44, "end_pos": 55, "type": "METRIC", "confidence": 0.9758146107196808}]}, {"text": "However, it was also observed that BLEU-like ngram matching metrics are unreliable for highquality translation output.", "labels": [], "entities": [{"text": "BLEU-like", "start_pos": 35, "end_pos": 44, "type": "METRIC", "confidence": 0.9780473113059998}]}, {"text": "In fact, researchers already worry that BLEU will soon be unable to distinguish automatic from human translations.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 40, "end_pos": 44, "type": "METRIC", "confidence": 0.9924048781394958}, {"text": "distinguish automatic from human translations", "start_pos": 68, "end_pos": 113, "type": "TASK", "confidence": 0.5851926863193512}]}, {"text": "1 This is a problem for most present-day metrics, which cannot tell apart raw machine translation output from a fully fluent professionally post-edited version thereof.", "labels": [], "entities": []}, {"text": "Another concern is that BLEU-like n-gram matching metrics tend to favor phrase-based SMT systems over rule-based systems and other SMT paradigms.", "labels": [], "entities": [{"text": "BLEU-like n-gram matching", "start_pos": 24, "end_pos": 49, "type": "TASK", "confidence": 0.7078766326109568}, {"text": "SMT", "start_pos": 85, "end_pos": 88, "type": "TASK", "confidence": 0.8557582497596741}, {"text": "SMT", "start_pos": 131, "end_pos": 134, "type": "TASK", "confidence": 0.964494526386261}]}, {"text": "In particular, they are unable to capture the syntactic and semantic structure of sentences, and are thus insensitive to improvement in these aspects.", "labels": [], "entities": []}, {"text": "Furthermore, it has been shown that lexical similarity is both insufficient and not strictly necessary for two sentences to convey the same meaning (Culy and).", "labels": [], "entities": []}, {"text": "The above issues have motivated a large amount of work dedicated to design better evaluation metrics.", "labels": [], "entities": []}, {"text": "The Metrics task at the Workshop on Machine Translation (WMT) has been instrumental in this quest.", "labels": [], "entities": [{"text": "Metrics task at the Workshop on Machine Translation (WMT)", "start_pos": 4, "end_pos": 61, "type": "TASK", "confidence": 0.6771306422623721}]}, {"text": "Below we present QCRI's submission to the Metrics task of WMT14, which consists of the DiscoTK family of discourse-based metrics.", "labels": [], "entities": [{"text": "QCRI", "start_pos": 17, "end_pos": 21, "type": "DATASET", "confidence": 0.8629670739173889}, {"text": "WMT14", "start_pos": 58, "end_pos": 63, "type": "DATASET", "confidence": 0.6980198621749878}]}, {"text": "In particular, we experiment with five different transformations and augmentations of a discourse tree representation, and we combine the kernel scores for each of them into a single score which we call DISCOTK light . Next, we add to the combination other metrics from the ASIYA MT evaluation toolkit (, to produce the DISCOTK party metric.", "labels": [], "entities": [{"text": "ASIYA MT evaluation", "start_pos": 274, "end_pos": 293, "type": "TASK", "confidence": 0.6087398926417033}]}, {"text": "Finally, we tune the relative weights of the metrics in the combination using human judgments in a learning-to-rank framework.", "labels": [], "entities": []}, {"text": "This proved to be quite beneficial: the tuned version of the DISCOTK party metric was the best performing metric in the WMT14 Metrics shared task.", "labels": [], "entities": [{"text": "DISCOTK party metric", "start_pos": 61, "end_pos": 81, "type": "DATASET", "confidence": 0.774226983388265}, {"text": "WMT14 Metrics shared task", "start_pos": 120, "end_pos": 145, "type": "DATASET", "confidence": 0.7542030364274979}]}, {"text": "The rest of the paper is organized as follows: Section 2 introduces our basic discourse metrics and the tree representations they are based on.", "labels": [], "entities": []}, {"text": "Section 3 describes our metric combinations.", "labels": [], "entities": []}, {"text": "Section 4 presents our experiments and results on datasets from previous years.", "labels": [], "entities": []}, {"text": "Finally, Section 5 concludes and suggests directions for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we present some of our experiments to decide on the best DiscoTK metric variant and tuning set.", "labels": [], "entities": [{"text": "DiscoTK", "start_pos": 74, "end_pos": 81, "type": "DATASET", "confidence": 0.7884353995323181}]}, {"text": "For tuning, testing and comparison, we worked with some of the datasets available from previous WMT metrics shared tasks, i.e.,), we know that the tuned metrics perform very well on crossvalidation for the same-year dataset.", "labels": [], "entities": [{"text": "WMT metrics shared tasks", "start_pos": 96, "end_pos": 120, "type": "TASK", "confidence": 0.7371193468570709}]}, {"text": "We further know that tuning can be performed by concatenating data from all the into-English language pairs, which yields better results than training separately by language pair.", "labels": [], "entities": []}, {"text": "For the WMT14 metrics task, we investigated in more depth whether the tuned metrics generalize well to new datasets.", "labels": [], "entities": [{"text": "WMT14 metrics task", "start_pos": 8, "end_pos": 26, "type": "TASK", "confidence": 0.5550852417945862}]}, {"text": "Additionally, we tested the effect of concatenating datasets from different years.", "labels": [], "entities": []}, {"text": "shows the main results of our experiments with the DiscoTK metrics.", "labels": [], "entities": [{"text": "DiscoTK metrics", "start_pos": 51, "end_pos": 66, "type": "DATASET", "confidence": 0.9200643002986908}]}, {"text": "We evaluated the performance of the metrics on the WMT12 and WMT13 datasets both at the segment and the system level, and we used WMT11 as an additional tuning dataset.", "labels": [], "entities": [{"text": "WMT12", "start_pos": 51, "end_pos": 56, "type": "DATASET", "confidence": 0.9533343315124512}, {"text": "WMT13 datasets", "start_pos": 61, "end_pos": 75, "type": "DATASET", "confidence": 0.9017594754695892}, {"text": "WMT11", "start_pos": 130, "end_pos": 135, "type": "DATASET", "confidence": 0.9583330154418945}]}, {"text": "We measured the performance of the metrics in terms of correlation with human judgements.", "labels": [], "entities": []}, {"text": "At the segment level, we evaluated using Kendall's Tau (\u03c4 ), recalculated following the WMT14 official Kendall's Tau implementation.", "labels": [], "entities": [{"text": "WMT14 official Kendall's Tau implementation", "start_pos": 88, "end_pos": 131, "type": "DATASET", "confidence": 0.9554783403873444}]}, {"text": "At the system level, we used Spearman's rank correlation (\u03c1) and Pearson's correlation coefficient (r).", "labels": [], "entities": [{"text": "Spearman's rank correlation (\u03c1)", "start_pos": 29, "end_pos": 60, "type": "METRIC", "confidence": 0.7332082603658948}, {"text": "Pearson's correlation coefficient (r)", "start_pos": 65, "end_pos": 102, "type": "METRIC", "confidence": 0.9624751465661185}]}, {"text": "In all cases, we averaged the results overall into-English language pairs.", "labels": [], "entities": []}, {"text": "The symbol '\u2205' represents the untuned versions of our metrics, i.e., applying a uniform linear combination of the individual metrics.", "labels": [], "entities": []}, {"text": "We trained the tuned versions of the DiscoTK measures using different datasets (WMT11, WMT12 and WMT13) in order to study acrosscorpora generalization and the effect of training dataset size.", "labels": [], "entities": [{"text": "DiscoTK", "start_pos": 37, "end_pos": 44, "type": "DATASET", "confidence": 0.7790442705154419}, {"text": "WMT11", "start_pos": 80, "end_pos": 85, "type": "DATASET", "confidence": 0.9333685636520386}, {"text": "WMT12", "start_pos": 87, "end_pos": 92, "type": "DATASET", "confidence": 0.7355279922485352}, {"text": "WMT13", "start_pos": 97, "end_pos": 102, "type": "DATASET", "confidence": 0.8478652834892273}, {"text": "acrosscorpora generalization", "start_pos": 122, "end_pos": 150, "type": "TASK", "confidence": 0.7527463734149933}]}, {"text": "The symbol '+' stands for concatenation of datasets.", "labels": [], "entities": []}, {"text": "We trained the tuned versions at the segment level using Maximum Entropy classifiers for pairwise ranking (cf. Section 3).", "labels": [], "entities": []}, {"text": "For the sake of comparison, the first group of rows contains the results of the best-performing metrics at the WMT12 and WMT13 metrics shared tasks and the last group of rows contains the results of the ASIYA combination of metrics, i.e., DISCOTK party without the discourse components.", "labels": [], "entities": [{"text": "WMT12 and WMT13 metrics shared tasks", "start_pos": 111, "end_pos": 147, "type": "DATASET", "confidence": 0.7606033384799957}]}, {"text": "Several conclusions can be drawn from.", "labels": [], "entities": []}, {"text": "First, DISCOTK party is better than DISCOTK light in all settings, indicating that the discourse-based metrics are very well complemented by the heterogeneous metric set from ASIYA.", "labels": [], "entities": []}, {"text": "DISCOTK light achieves competitive scores at the system level (which would put the metric among the best participants in WMT12 and WMT13); however, as expected, it is not robust enough at the segment level.", "labels": [], "entities": [{"text": "WMT12", "start_pos": 121, "end_pos": 126, "type": "DATASET", "confidence": 0.8827183842658997}, {"text": "WMT13", "start_pos": 131, "end_pos": 136, "type": "DATASET", "confidence": 0.8602649569511414}]}, {"text": "On the other hand, the tuned versions of DISCOTK party are very competitive and improve over the already strong ASIYA in each configuration both at the segment-and the system-level.", "labels": [], "entities": [{"text": "DISCOTK party", "start_pos": 41, "end_pos": 54, "type": "DATASET", "confidence": 0.8735785186290741}]}, {"text": "The improvements are small but consistent, showing that using discourse increases the correlation with human judgments.", "labels": [], "entities": []}, {"text": "Focusing on the results at the segment level, it is clear that the tuned versions offer an advantage over the simple uniform linear combinations.", "labels": [], "entities": []}, {"text": "Interestingly, for the tuned variants, given a test set, the results are consistent across tuning sets, ruling out over-fitting; this shows that the generalization is very good.", "labels": [], "entities": []}, {"text": "This result aligns well with what we observed in our previous studies.", "labels": [], "entities": []}, {"text": "Learning with more data (WMT11+12 or WMT12+13) does not seem to help much, but it does not hurt performance either.", "labels": [], "entities": [{"text": "WMT11+12", "start_pos": 25, "end_pos": 33, "type": "DATASET", "confidence": 0.818764328956604}, {"text": "WMT12", "start_pos": 37, "end_pos": 42, "type": "DATASET", "confidence": 0.8640434145927429}]}, {"text": "Overall, the \u03c4 correlation results obtained with the tuned DISCOTK party metric are much better than the best results of any participant metrics at WMT12 and WMT13 (20.1% and 9.5% relative improvement, respectively).", "labels": [], "entities": [{"text": "\u03c4 correlation", "start_pos": 13, "end_pos": 26, "type": "METRIC", "confidence": 0.820155680179596}, {"text": "DISCOTK party metric", "start_pos": 59, "end_pos": 79, "type": "DATASET", "confidence": 0.7086739738782247}, {"text": "WMT12", "start_pos": 148, "end_pos": 153, "type": "DATASET", "confidence": 0.9466431140899658}, {"text": "WMT13", "start_pos": 158, "end_pos": 163, "type": "DATASET", "confidence": 0.905006468296051}]}, {"text": "At the system level, we observe that tuning over the DISCOTK light metric is not helpful (results are actually slightly lower), while tuning the more complex DISCOTK party metric yields slightly better results.: Evaluation results on WMT12 and WMT13 datasets at segment and system level for the main combined DiscoTK measures proposed in this paper.", "labels": [], "entities": [{"text": "WMT12", "start_pos": 234, "end_pos": 239, "type": "DATASET", "confidence": 0.9172859787940979}, {"text": "WMT13 datasets", "start_pos": 244, "end_pos": 258, "type": "DATASET", "confidence": 0.9279160797595978}]}, {"text": "The scores of our best metric are higher than those of the best participants in WMT12 and WMT13, according to Spearman's \u03c1, which was the official metric in those years.", "labels": [], "entities": [{"text": "WMT12", "start_pos": 80, "end_pos": 85, "type": "DATASET", "confidence": 0.8768087029457092}, {"text": "WMT13", "start_pos": 90, "end_pos": 95, "type": "DATASET", "confidence": 0.857387125492096}, {"text": "Spearman's \u03c1", "start_pos": 110, "end_pos": 122, "type": "METRIC", "confidence": 0.5117403070131937}]}, {"text": "Overall, our metrics are comparable to the state-of-the-art at the system level.", "labels": [], "entities": []}, {"text": "The differences between Spearman's \u03c1 and Pearson's r coefficients are not dramatic, with r values being always higher than \u03c1.", "labels": [], "entities": []}, {"text": "Given the above results, we submitted the following runs to the WMT14 Metrics shared task: (i) DISCOTK party tuned on the concatenation of datasets WMT11+12+13, as our primary run; (ii) Untuned DISCOTK party , to verify that we are not over-fitting the training set; and (iii) Untuned DISCOTK light , to seethe performance of a metric using discourse structures and word unigrams.", "labels": [], "entities": [{"text": "WMT14 Metrics shared task", "start_pos": 64, "end_pos": 89, "type": "DATASET", "confidence": 0.896197110414505}, {"text": "WMT11+12+13", "start_pos": 148, "end_pos": 159, "type": "DATASET", "confidence": 0.9059368252754212}]}, {"text": "The results for the WMT14 Metrics shared task have shown that our primary run, DISCOTK party tuned, was the best-performing metric both at the segment-and at the system-level).", "labels": [], "entities": [{"text": "WMT14 Metrics shared task", "start_pos": 20, "end_pos": 45, "type": "DATASET", "confidence": 0.7350747883319855}]}, {"text": "This metric yielded significantly better results than its untuned counterpart, confirming the importance of weight tuning and the absence of over-fitting during tuning.", "labels": [], "entities": []}, {"text": "Finally, the untuned DISCOTK light achieved relatively competitive, albeit slightly worse results for all language pairs, except for Hindi-English, where system translations resembled a \"word salad\", and were very hard to discourse-parse accurately.", "labels": [], "entities": [{"text": "DISCOTK light", "start_pos": 21, "end_pos": 34, "type": "DATASET", "confidence": 0.7314730882644653}]}], "tableCaptions": [{"text": " Table 1: Evaluation results on WMT12 and WMT13 datasets at segment and system level for the main  combined DiscoTK measures proposed in this paper.", "labels": [], "entities": [{"text": "WMT12", "start_pos": 32, "end_pos": 37, "type": "DATASET", "confidence": 0.8909552097320557}, {"text": "WMT13 datasets", "start_pos": 42, "end_pos": 56, "type": "DATASET", "confidence": 0.938819944858551}, {"text": "DiscoTK", "start_pos": 108, "end_pos": 115, "type": "DATASET", "confidence": 0.5638944506645203}]}]}