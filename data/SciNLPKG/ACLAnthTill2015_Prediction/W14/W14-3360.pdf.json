{"title": [{"text": "An Empirical Comparison of Features and Tuning for Phrase-based Machine Translation", "labels": [], "entities": [{"text": "Phrase-based Machine Translation", "start_pos": 51, "end_pos": 83, "type": "TASK", "confidence": 0.7600739598274231}]}], "abstractContent": [{"text": "Scalable discriminative training methods are now broadly available for estimating phrase-based, feature-rich translation models.", "labels": [], "entities": [{"text": "estimating phrase-based, feature-rich translation models", "start_pos": 71, "end_pos": 127, "type": "TASK", "confidence": 0.6501345882813135}]}, {"text": "However, the sparse feature sets typically appearing in research evaluations are less attractive than standard dense features such as language and translation model probabilities: they often overfit, do not generalize , or require complex and slow feature extractors.", "labels": [], "entities": []}, {"text": "This paper introduces extended features, which are more specific than dense features yet more general than lexicalized sparse features.", "labels": [], "entities": []}, {"text": "Large-scale experiments show that extended features yield robust BLEU gains for both Arabic-English (+1.05) and Chinese-English (+0.67) relative to a strong feature-rich baseline.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 65, "end_pos": 69, "type": "METRIC", "confidence": 0.9986792206764221}]}, {"text": "We also specialize the feature set to specific data domains, identify an objective function that is less prone to overfitting, and release fast, scalable, and language-independent tools for implementing the features.", "labels": [], "entities": []}], "introductionContent": [{"text": "Scalable discriminative algorithm design for machine translation (MT) has lately been a booming enterprise.", "labels": [], "entities": [{"text": "Scalable discriminative algorithm design for machine translation (MT)", "start_pos": 0, "end_pos": 69, "type": "TASK", "confidence": 0.7490421622991562}]}, {"text": "There are now algorithms for every taste: probabilistic and distribution-free, online and batch, regularized and unregularized.", "labels": [], "entities": []}, {"text": "Technical differences aside, the papers that apply these algorithms to phrase-based translation often share a curious empirical characteristic: the algorithms support extra features, but the features do not significantly improve translation.", "labels": [], "entities": [{"text": "phrase-based translation", "start_pos": 71, "end_pos": 95, "type": "TASK", "confidence": 0.8035020232200623}]}, {"text": "For example, showed that PRO with some simple ad hoc features only exceeds the baseline on one of three language pairs.", "labels": [], "entities": [{"text": "PRO", "start_pos": 25, "end_pos": 28, "type": "TASK", "confidence": 0.9536240100860596}]}, {"text": "observed a similar result for both PRO and their ramp-loss algorithm.", "labels": [], "entities": [{"text": "PRO", "start_pos": 35, "end_pos": 38, "type": "TASK", "confidence": 0.9112427234649658}]}, {"text": "found that, at least in the batch case, many algorithms produce similar results, and features only significantly increased quality for one of three language pairs.", "labels": [], "entities": []}, {"text": "Only recently did Cherry (2013) and identify certain features that consistently reduce error.", "labels": [], "entities": [{"text": "error", "start_pos": 87, "end_pos": 92, "type": "METRIC", "confidence": 0.9748401641845703}]}, {"text": "These empirical results suggest that feature design and model fitting, the subjects of this paper, warrant a closer look.", "labels": [], "entities": [{"text": "feature design", "start_pos": 37, "end_pos": 51, "type": "TASK", "confidence": 0.7155502587556839}, {"text": "model fitting", "start_pos": 56, "end_pos": 69, "type": "TASK", "confidence": 0.7503612041473389}]}, {"text": "We introduce an effective extended feature set for phrase-based MT and identify a loss function that is less prone to overfitting.", "labels": [], "entities": [{"text": "MT", "start_pos": 64, "end_pos": 66, "type": "TASK", "confidence": 0.7249849438667297}]}, {"text": "Extended features share three attractive characteristics with the standard Moses dense features (: ease of implementation, language independence, and independence from ancillary corpora like treebanks.", "labels": [], "entities": []}, {"text": "In our experiments, they do not overfit and can be extracted efficiently during decoding.", "labels": [], "entities": []}, {"text": "Because all feature weights are tuned on the development set, the new feature templates are amenable to feature augmentation (, a simple domain adaptation technique that we show works surprisingly well for MT.", "labels": [], "entities": [{"text": "MT", "start_pos": 206, "end_pos": 208, "type": "TASK", "confidence": 0.9610872864723206}]}, {"text": "Extended features are designed according to a principle rather than a rule: they should fire less than standard dense features, which are general, but more than so-called sparse features, which are very specific-they are usually lexicalized-and thus prone to overfitting.", "labels": [], "entities": []}, {"text": "This principle is motivated by analysis, which shows how expressive models can be a mixed blessing in the translation setting.", "labels": [], "entities": []}, {"text": "It is obvious that features allow the model to fit the tuning data more tightly.", "labels": [], "entities": []}, {"text": "For example, sparse lexicalized features could reduce tuning error by learning that the references prefer U.S. over United States, a minor lexical distinction.", "labels": [], "entities": []}, {"text": "Reference choice should matter more than in the dense case, an issue that we quantify.", "labels": [], "entities": []}, {"text": "We also show that frequency cutoffs, which area crude but common form of feature selection, are unnecessary and even detrimental when features follow this principle.", "labels": [], "entities": [{"text": "feature selection", "start_pos": 73, "end_pos": 90, "type": "TASK", "confidence": 0.7000226527452469}]}, {"text": "We report large-scale translation quality experiments relative to both dense and feature-rich baselines.", "labels": [], "entities": []}, {"text": "Our best feature set, which includes domain adaptation features, yields an average +1.05 BLEU improvement for Arabic-English and +0.67 for Chinese-English.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 89, "end_pos": 93, "type": "METRIC", "confidence": 0.9994235038757324}]}, {"text": "In addition to the extended feature set, we show that an online variant of expected error is significantly faster to compute, less prone to overfitting, and nearly as effective as a pairwise loss.", "labels": [], "entities": []}, {"text": "We release all software-feature extractors, and fast word clustering and data selection packages-used in our experiments.", "labels": [], "entities": [{"text": "word clustering", "start_pos": 53, "end_pos": 68, "type": "TASK", "confidence": 0.7071838527917862}]}], "datasetContent": [{"text": "We evaluate and analyze our feature set under a variety of large-scale experimental conditions including multiple domains and references.", "labels": [], "entities": []}, {"text": "To our knowledge, the only language pairs with sufficient research resources to support this protocol are ArabicEnglish (Ar-En) and Chinese-English (Zh-En).", "labels": [], "entities": []}, {"text": "The test, development, and tuning corpora 6 come from the NIST OpenMT and MetricsMATR evaluations.", "labels": [], "entities": [{"text": "NIST OpenMT", "start_pos": 58, "end_pos": 69, "type": "DATASET", "confidence": 0.7647282183170319}]}, {"text": "Extended features benefit from more tuning data, so we concatenated five NIST data sets to build one large tuning set.", "labels": [], "entities": [{"text": "NIST data sets", "start_pos": 73, "end_pos": 87, "type": "DATASET", "confidence": 0.9574227531750997}]}, {"text": "Observe that all test data come from later epochs than the tuning and development data.", "labels": [], "entities": []}, {"text": "From these data we built phrase-based MT systems with Phrasal ( ).", "labels": [], "entities": [{"text": "MT", "start_pos": 38, "end_pos": 40, "type": "TASK", "confidence": 0.8532212972640991}]}, {"text": "We aligned the parallel corpora with the Berkeley aligner () with standard settings and symmetrized via the grow-diag heuristic.", "labels": [], "entities": []}, {"text": "We created separate English LMs for each language pair by concatenating the monolingual Gigaword data with the target-side of the respective bitexts.", "labels": [], "entities": [{"text": "Gigaword data", "start_pos": 88, "end_pos": 101, "type": "DATASET", "confidence": 0.788903146982193}]}, {"text": "For each corpus we estimated unfiltered 5-gram language models with lmplz ( ).", "labels": [], "entities": []}, {"text": "For each condition we ran the learning algorithm for 25 epochs 8 and selected the model according to the maximum uncased, corpus-level BLEU-4 () score on the dev set.", "labels": [], "entities": [{"text": "BLEU-4", "start_pos": 135, "end_pos": 141, "type": "METRIC", "confidence": 0.9858960509300232}]}], "tableCaptions": [{"text": " Table 1: Wallclock time (min.sec) to generate a  mapping from a vocabulary of 63k English words  (3.7M tokens) to 512 classes. All experiments were  run on the same server, which had eight physical  cores. Our Java implementation is multi-threaded;  the C++ baselines are single-threaded.", "labels": [], "entities": []}, {"text": " Table 3: Development, test, and tuning data. Do- main abbreviations: broadcast news (bn), newswire  (nw), and web (wb).", "labels": [], "entities": []}, {"text": " Table 4: Translation quality results (uncased BLEU-4 %). Per-epoch times are in minutes (Min.). Statistical  significance relative to D+SS, the strongest baseline: bold (p < 0.001) and bold-italic (p < 0.05).", "labels": [], "entities": [{"text": "BLEU-4", "start_pos": 47, "end_pos": 53, "type": "METRIC", "confidence": 0.9953420162200928}, {"text": "Statistical  significance", "start_pos": 97, "end_pos": 122, "type": "METRIC", "confidence": 0.87489253282547}]}, {"text": " Table 6: Ar-En learning comparisons.", "labels": [], "entities": [{"text": "Ar-En", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.964089572429657}]}]}