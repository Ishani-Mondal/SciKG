{"title": [{"text": "Wordsyoudontknow: Evaluation of lexicon-based decompounding with unknown handling", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper we present a cross-linguistic evaluation of a lexicon-based decomposition method for decompounding, augmented with a \"guesser\" for unknown components.", "labels": [], "entities": []}, {"text": "Using a gold standard test set, for which the correct decompositions are known, we optimize the method's parameters and show correlations between each parameter and the resulting scores.", "labels": [], "entities": []}, {"text": "The results show that even with optimal parameter settings, the performance on compounds with unknown elements is low in terms of matching the expected lemma components, but much higher in terms of correct string segmentation.", "labels": [], "entities": []}], "introductionContent": [{"text": "Compounding is a productive process that creates new words by combining existing words together in a single string.", "labels": [], "entities": []}, {"text": "It is predominant in Germanic and Scandinavian languages, but is also present in other languages, e.g. Finnish, Korean, or Farsi.", "labels": [], "entities": []}, {"text": "Many languages that are not usually thought of as \"compounding\" nevertheless display marginal presence of compounds, restricted, for instance, to numerical expressions (e.g. Polish czterogodzinny 'four-hour').", "labels": [], "entities": []}, {"text": "Depending on a language, compounding can be a very frequent and productive process, in effect making it impossible to list all the compound words in the dictionary.", "labels": [], "entities": [{"text": "compounding", "start_pos": 25, "end_pos": 36, "type": "TASK", "confidence": 0.966896116733551}]}, {"text": "This creates serious challenges for Natural Language Processing in many areas, including search, Machine Translation, information retrieval and related disciplines that rely on matching multiple occurrences of words to the same underlying representation.", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 97, "end_pos": 116, "type": "TASK", "confidence": 0.7995642721652985}, {"text": "information retrieval", "start_pos": 118, "end_pos": 139, "type": "TASK", "confidence": 0.7590396702289581}]}, {"text": "In this paper, we present a cross-linguistic evaluation of a lexicon-based decomposition method augmented with a \"guesser\" for handling unknown components.", "labels": [], "entities": []}, {"text": "We use existing lexicons developed at Oracle Language Technology in combination with a string scanner parametrized with languagespecific input/output settings.", "labels": [], "entities": []}, {"text": "Our focus is on the evaluation that tries to tease apart string segmentation (i.e. finding boundaries between components) and morphological analysis (i.e. matching component parts to known lemmas).", "labels": [], "entities": [{"text": "string segmentation", "start_pos": 57, "end_pos": 76, "type": "TASK", "confidence": 0.7623583674430847}]}, {"text": "The paper is organized as follows: Section 2 gives an overview of related research; Section 3 describes the compound analyzer used in our experiments; Section 4 presents experimental results; Section 5 contains error analysis and discussion.", "labels": [], "entities": []}, {"text": "Section 6 concludes and suggests future research.", "labels": [], "entities": []}], "datasetContent": [{"text": "Despite all the constraints and settings described above, decompounding is still an imperfect process: there can be multiple competing (i.e. overlapping) decompositions, and many decompositions that are technically possible are incorrect due to semantic reasons.", "labels": [], "entities": []}, {"text": "This problem becomes even more challenging when some of the components are not present in the lexicon.", "labels": [], "entities": []}, {"text": "Since lexicons are limited, and real world text can contain misspellings, proper names, or obscure words, we need to address the issue of decompounding with unknown elements.", "labels": [], "entities": []}, {"text": "Therefore, we set out to evaluate the performance of our lexicon-based method on a gold standard set of known compounds, and compare it to an augmented version that also tries to construct potential components from unknown substrings.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2. Size of corpora per language, count of compounds, distribution of parts.", "labels": [], "entities": []}, {"text": " Table 3. Precision, recall, and f-measure for dynamic decompounding.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.997129499912262}, {"text": "recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9982784986495972}, {"text": "f-measure", "start_pos": 33, "end_pos": 42, "type": "METRIC", "confidence": 0.9765204787254333}]}, {"text": " Table 5. Dynamic decomposition with missing lemmas, optimal settings; string segmentation shows  accuracy score; remaining values are harmonic f-score of precision and recall.", "labels": [], "entities": [{"text": "accuracy score", "start_pos": 98, "end_pos": 112, "type": "METRIC", "confidence": 0.9864644706249237}, {"text": "precision", "start_pos": 155, "end_pos": 164, "type": "METRIC", "confidence": 0.9991940855979919}, {"text": "recall", "start_pos": 169, "end_pos": 175, "type": "METRIC", "confidence": 0.9978518486022949}]}, {"text": " Table 6. Optimal guesser settings and their correlations of settings with the guesser score.", "labels": [], "entities": []}]}