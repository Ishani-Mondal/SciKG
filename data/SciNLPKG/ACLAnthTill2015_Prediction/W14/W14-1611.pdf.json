{"title": [{"text": "Improved Pattern Learning for Bootstrapped Entity Extraction", "labels": [], "entities": [{"text": "Improved Pattern Learning", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.877408484617869}, {"text": "Bootstrapped Entity Extraction", "start_pos": 30, "end_pos": 60, "type": "TASK", "confidence": 0.6213603814442953}]}], "abstractContent": [{"text": "Bootstrapped pattern learning for entity extraction usually starts with seed entities and iteratively learns patterns and entities from unlabeled text.", "labels": [], "entities": [{"text": "Bootstrapped pattern learning", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.6180549462636312}, {"text": "entity extraction", "start_pos": 34, "end_pos": 51, "type": "TASK", "confidence": 0.7348867207765579}]}, {"text": "Patterns are scored by their ability to extract more positive entities and less negative entities.", "labels": [], "entities": []}, {"text": "A problem is that due to the lack of labeled data, unlabeled entities are either assumed to be negative or are ignored by the existing pattern scoring measures.", "labels": [], "entities": []}, {"text": "In this paper, we improve pattern scoring by predicting the labels of unlabeled entities.", "labels": [], "entities": [{"text": "pattern scoring", "start_pos": 26, "end_pos": 41, "type": "TASK", "confidence": 0.8429168462753296}]}, {"text": "We use various unsupervised features based on contrasting domain-specific and general text, and exploiting distributional similarity and edit distances to learned entities.", "labels": [], "entities": []}, {"text": "Our system outperforms existing pattern scoring algorithms for extracting drug-and-treatment entities from four medical forums .", "labels": [], "entities": []}], "introductionContent": [{"text": "This paper considers the problem of building effective entity extractors for custom entity types from specialized domain corpora.", "labels": [], "entities": []}, {"text": "We approach the problem by learning rules bootstrapped using seed sets of entities.", "labels": [], "entities": []}, {"text": "Though entity extraction using machine learning is common in academic research, rule-based systems dominate in commercial use (, mainly because rules are effective, interpretable, and are easy to customize by non-experts to cope with errors.", "labels": [], "entities": [{"text": "entity extraction", "start_pos": 7, "end_pos": 24, "type": "TASK", "confidence": 0.8336746096611023}]}, {"text": "They also have been shown to perform better than state-of-the-art machine learning methods on some specialized domains).", "labels": [], "entities": []}, {"text": "In addition, building supervised machine learning systems fora reasonably large domain-specific corpus would require hand-labeling sufficient data to  train a model, which can be costly and time consuming.", "labels": [], "entities": []}, {"text": "Bootstrapped machine-learned rules can make extraction easier and more efficient on such a corpus.", "labels": [], "entities": []}, {"text": "Ina bootstrapped rule-based entity learning system, seed dictionaries and/or patterns provide weak supervision to label data.", "labels": [], "entities": []}, {"text": "The system iteratively learns new entities belonging to a specific class from unlabeled text.", "labels": [], "entities": []}, {"text": "Rules are typically defined by creating patterns around the entities, such as lexico-syntactic surface word patterns and dependency tree patterns ().", "labels": [], "entities": []}, {"text": "Patterns are scored by their ability to extract more positive entities and less negative entities.", "labels": [], "entities": []}, {"text": "Top ranked patterns are used to extract candidate entities from text.", "labels": [], "entities": []}, {"text": "High scoring candidate entities are added to the dictionaries and are used to generate more candidate patterns around them.", "labels": [], "entities": []}, {"text": "Ina supervised setting, the efficacy of patterns can be judged by their performance on a fully labeled dataset).", "labels": [], "entities": []}, {"text": "Ina bootstrapped system, where the data is not fully labeled, existing systems score patterns by either ignoring the un-labeled entities or assuming them to be negative.", "labels": [], "entities": []}, {"text": "However, these scoring schemes cannot differentiate between patterns that extract good versus bad unlabeled entities.", "labels": [], "entities": []}, {"text": "The problem is similar to the closed world assumption in distantly supervised information extraction systems, when all propositions missing from a knowledge base are considered false (.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 78, "end_pos": 100, "type": "TASK", "confidence": 0.7672702670097351}]}, {"text": "Predicting labels of unlabeled entities can improve scoring patterns.", "labels": [], "entities": []}, {"text": "Consider the example shown in.", "labels": [], "entities": []}, {"text": "Current pattern learning systems would score both patterns equally.", "labels": [], "entities": []}, {"text": "However, features like distributional similarity can predict 'cat' to be closer to {dog} than 'car', and a pattern learning system can use that information to rank 'Pattern 1' higher than ' In this paper, we work on bootstrapping entity extraction using seed sets of entities and an unlabeled text corpus.", "labels": [], "entities": [{"text": "bootstrapping entity extraction", "start_pos": 216, "end_pos": 247, "type": "TASK", "confidence": 0.7265833814938863}]}, {"text": "We improve the scoring of patterns for an entity class by defining a pattern's score by the number of positive entities it extracts and the ratio of number of positive entities to expected number of negative entities it extracts.", "labels": [], "entities": []}, {"text": "Our main contribution is introducing the expected number of negative entities in pattern scoring -we predict probabilities of unlabeled entities belonging to the negative class.", "labels": [], "entities": []}, {"text": "We estimate an unlabeled entity's negative class probability by averaging probabilities from various unsupervised class predictors, such as distributional similarity, string edit distances from learned entities, and TF-IDF scores.", "labels": [], "entities": []}, {"text": "Our system performs significantly better than existing pattern scoring measures for extracting drug-and-treatment entities from four medical forums on MedHelp 1 , a user health discussion website.", "labels": [], "entities": [{"text": "MedHelp 1", "start_pos": 151, "end_pos": 160, "type": "DATASET", "confidence": 0.9102543294429779}]}, {"text": "We release the code for the systems described in this paper at http://nlp.stanford.edu/ software/patternslearning.shtml.", "labels": [], "entities": []}, {"text": "We also release a visualization tool, described in, that visualizes and compares output of multiple pattern-based entity extraction systems.", "labels": [], "entities": []}, {"text": "It can be downloaded at http://nlp.stanford.edu/software/ patternviz.shtml.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our system on extracting drug-andtreatment (DT) entities in sentences from four forums on the MedHelp user health discussion website:, a DT entity is defined as a pharmaceutical drug, or any treatment or intervention mentioned that may help a symptom or a condition.", "labels": [], "entities": [{"text": "MedHelp user health discussion website", "start_pos": 106, "end_pos": 144, "type": "DATASET", "confidence": 0.9004729747772217}]}, {"text": "It includes surgeries, lifestyle changes, alternative treatments, home remedies, and components of daily care and management of a disease, but does not include diagnostic tests and devices.", "labels": [], "entities": []}, {"text": "More information is in the supplemental material.", "labels": [], "entities": []}, {"text": "A few example sentences from the dataset are below.", "labels": [], "entities": []}, {"text": "I plan to start cinnamon and holy basil -known to lower glucose in many people.", "labels": [], "entities": []}, {"text": "She gave me albuteral and symbicort (plus some hayfever meds and asked me to use the peak flow meter.", "labels": [], "entities": [{"text": "peak flow meter", "start_pos": 85, "end_pos": 100, "type": "METRIC", "confidence": 0.6728388766447703}]}, {"text": "My sinus infections were treated electrically, with high voltage million volt electricity, which solved the problem, but the treatment is not FDA approved and generally unavailable, except under experimental treatment protocols.", "labels": [], "entities": []}, {"text": "In these sentences, 'cinanmon', 'holy basil', 'albuteral', 'symbicort', 'meds', 'high voltage million volt electricity', and 'treatment' are DT entities.", "labels": [], "entities": []}, {"text": "We used entities from the following classes as negative: symptoms and conditions (SC), medical specialists, body parts, and common temporal nouns to remove dates and dosage information.", "labels": [], "entities": []}, {"text": "We used the DT and SC seed dictionaries from Gupta and Manning (2014a).", "labels": [], "entities": [{"text": "DT and SC seed dictionaries from Gupta and Manning (2014a)", "start_pos": 12, "end_pos": 70, "type": "DATASET", "confidence": 0.8994734734296799}]}, {"text": "The lists of body parts and temporal nouns were obtained from Wordnet.", "labels": [], "entities": [{"text": "Wordnet", "start_pos": 62, "end_pos": 69, "type": "DATASET", "confidence": 0.9892265796661377}]}, {"text": "The common words list was created using most common words on the web and Twitter.", "labels": [], "entities": []}, {"text": "For evaluation, the first author hand labeled the learned entities pooled from all systems.", "labels": [], "entities": []}, {"text": "A word was evaluated by querying the word and the forum name on Google and manually inspecting the results.", "labels": [], "entities": []}, {"text": "More details on the labeling guidelines are in the Supplement section.", "labels": [], "entities": [{"text": "labeling", "start_pos": 20, "end_pos": 28, "type": "TASK", "confidence": 0.9583674073219299}, {"text": "Supplement", "start_pos": 51, "end_pos": 61, "type": "TASK", "confidence": 0.4319627285003662}]}, {"text": "Inter annotator agreement between the annotator and another researcher was computed on 200 randomly sampled learned entities from each of the Asthma and ENT forum.", "labels": [], "entities": [{"text": "Asthma and ENT forum", "start_pos": 142, "end_pos": 162, "type": "DATASET", "confidence": 0.5710622742772102}]}, {"text": "The agreement for the entities from the Asthma forum was 96% and from the ENT forum was 92.46%.", "labels": [], "entities": [{"text": "Asthma forum", "start_pos": 40, "end_pos": 52, "type": "DATASET", "confidence": 0.7952965795993805}, {"text": "ENT forum", "start_pos": 74, "end_pos": 83, "type": "DATASET", "confidence": 0.9195290207862854}]}, {"text": "The Cohen's kappa scores were 0.91 and 0.83, respectively.", "labels": [], "entities": []}, {"text": "Most disagreements were on food items like 'yogurt', which are hard to label.", "labels": [], "entities": []}, {"text": "Note that we use the hand labeled entities only as a test set for evaluation.", "labels": [], "entities": []}, {"text": "We used the same experimental setup for our system and the baselines.", "labels": [], "entities": []}, {"text": "When matching phrases from a seed dictionary to text, a phrase is labeled with the dictionary's class if the sequence of phrase words or their lemmas match with the sequence of words of a dictionary phrase.", "labels": [], "entities": []}, {"text": "Since our corpora are from online discussion forums, they have many spelling mistakes and morphological variations of entities.", "labels": [], "entities": []}, {"text": "To deal with the variations, we do fuzzy matching of words -if two words are one edit distance away and are more than 6 characters long, then they are considered a match.", "labels": [], "entities": []}, {"text": "We used Stanford TokensRegex () to create and apply surface word patterns to text, and used the Stanford Part-ofSpeech (POS) tagger () to find POS tags of tokens and lemmatize them.", "labels": [], "entities": []}, {"text": "When creating patterns, we discarded patterns whose left or right context was 1 or 2 stop words to avoid generating low precision patterns.", "labels": [], "entities": []}, {"text": "In each iteration, we learned a maximum 20 patterns with ps(r) \u2265 \u03b8 rand maximum 10 words with score \u2265 0.2.", "labels": [], "entities": []}, {"text": "The initial value of \u03b8 r was 1.0, which was reduced to 0.8 \u00d7 \u03b8 r whenever the system did not extract anymore patterns and words.", "labels": [], "entities": []}, {"text": "We discarded patterns that extracted less than 2 positive entities.", "labels": [], "entities": []}, {"text": "We selected these parameters by their performance on the development forum.", "labels": [], "entities": []}, {"text": "For calculating the DistSim feature used for scoring patterns and entities, we clustered all of MedHelp's forum data into 1000 clusters using the Brown clustering algorithm.", "labels": [], "entities": [{"text": "MedHelp's forum data", "start_pos": 96, "end_pos": 116, "type": "DATASET", "confidence": 0.9285097867250443}]}, {"text": "For calculating the Domain Ngram feature for scoring entities, we used n-grams from all user forums in MedHelp as the domain ngrams.", "labels": [], "entities": [{"text": "MedHelp", "start_pos": 103, "end_pos": 110, "type": "DATASET", "confidence": 0.9235358238220215}]}, {"text": "We evaluate systems by their precision and recall in each iteration.", "labels": [], "entities": [{"text": "precision", "start_pos": 29, "end_pos": 38, "type": "METRIC", "confidence": 0.9994307160377502}, {"text": "recall", "start_pos": 43, "end_pos": 49, "type": "METRIC", "confidence": 0.9995092153549194}]}, {"text": "Precision is defined as the fraction of correct entities among the entities extracted.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9383701682090759}]}, {"text": "We stopped learning entities fora system if the precision dropped below 75% to extract entities with reasonably high precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 48, "end_pos": 57, "type": "METRIC", "confidence": 0.9991065859794617}, {"text": "precision", "start_pos": 117, "end_pos": 126, "type": "METRIC", "confidence": 0.9667401313781738}]}, {"text": "Recall is defined as the fraction of correct entities among the total unique correct entities pooled from all systems while maintaining the precision \u2265 75%.", "labels": [], "entities": [{"text": "Recall", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9599208235740662}, {"text": "precision", "start_pos": 140, "end_pos": 149, "type": "METRIC", "confidence": 0.9990028738975525}]}, {"text": "Note that true recall is very hard to compute since our dataset is unlabeled.", "labels": [], "entities": [{"text": "recall", "start_pos": 15, "end_pos": 21, "type": "METRIC", "confidence": 0.992734432220459}]}, {"text": "To compare the systems overall, we calculate the area under the precisionrecall curves (AUC-PR: Area under Precision-Recall curves of the systems.", "labels": [], "entities": [{"text": "precisionrecall", "start_pos": 64, "end_pos": 79, "type": "METRIC", "confidence": 0.9996157884597778}, {"text": "AUC-PR", "start_pos": 88, "end_pos": 94, "type": "METRIC", "confidence": 0.9052327275276184}]}, {"text": "plots the precision and recall of systems.", "labels": [], "entities": [{"text": "precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9995096921920776}, {"text": "recall", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.9982075691223145}]}, {"text": "8 shows AUC-PR scores for all systems.", "labels": [], "entities": [{"text": "AUC-PR", "start_pos": 8, "end_pos": 14, "type": "METRIC", "confidence": 0.933958113193512}]}, {"text": "RlogF-PN and PNOdd have low value for Diabetes because they learned generic patterns in initial iteration, which led them to learn incorrect entities.", "labels": [], "entities": [{"text": "RlogF-PN", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.8017944097518921}]}, {"text": "Overall our system performed significantly better than existing systems.", "labels": [], "entities": []}, {"text": "All systems extract more entities for Acne and ENT because different drugs and treatments are more prevalent in these forums.", "labels": [], "entities": [{"text": "Acne", "start_pos": 38, "end_pos": 42, "type": "METRIC", "confidence": 0.8718647360801697}]}, {"text": "Diabetes and Asthma have more interventions and lifestyle changes that are harder to: Feature ablation study: Area under Precision-Recall curves when individual features are removed from our system during pattern scoring.", "labels": [], "entities": []}, {"text": "The feature is still used for entity scoring.", "labels": [], "entities": [{"text": "entity scoring", "start_pos": 30, "end_pos": 44, "type": "TASK", "confidence": 0.9137104153633118}]}], "tableCaptions": [{"text": " Table 1: Area under Precision-Recall curves of the  systems.", "labels": [], "entities": [{"text": "Area", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.976751983165741}, {"text": "Precision-Recall", "start_pos": 21, "end_pos": 37, "type": "METRIC", "confidence": 0.9530322551727295}]}, {"text": " Table 2: Individual feature effectiveness: Area un- der Precision-Recall curves when our system uses  individual features during pattern scoring. Other  features are still used for entity scoring.", "labels": [], "entities": [{"text": "Area un- der Precision-Recall", "start_pos": 44, "end_pos": 73, "type": "METRIC", "confidence": 0.8864214062690735}, {"text": "entity scoring", "start_pos": 182, "end_pos": 196, "type": "TASK", "confidence": 0.8090608716011047}]}, {"text": " Table 3: Feature ablation study: Area under  Precision-Recall curves when individual features  are removed from our system during pattern scor- ing. The feature is still used for entity scoring.", "labels": [], "entities": [{"text": "Area", "start_pos": 34, "end_pos": 38, "type": "METRIC", "confidence": 0.9564855694770813}, {"text": "pattern scor- ing", "start_pos": 131, "end_pos": 148, "type": "TASK", "confidence": 0.7392879873514175}, {"text": "entity scoring", "start_pos": 180, "end_pos": 194, "type": "TASK", "confidence": 0.8008019030094147}]}]}