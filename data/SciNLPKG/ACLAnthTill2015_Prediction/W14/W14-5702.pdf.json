{"title": [{"text": "Splitting of Compound Terms in non-Prototypical Compounding Languages", "labels": [], "entities": [{"text": "Splitting of Compound Terms", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.8928584009408951}]}], "abstractContent": [{"text": "Compounding is present in a large variety of languages in different proportions.", "labels": [], "entities": []}, {"text": "Compound rate in the text obviously depends on the language, but also on the genre and the domain.", "labels": [], "entities": [{"text": "Compound rate", "start_pos": 0, "end_pos": 13, "type": "METRIC", "confidence": 0.9726661741733551}]}, {"text": "Scientific and technical texts are especially conducive to compounding, even in the languages that are not traditionally admitted as highly compounding ones.", "labels": [], "entities": [{"text": "compounding", "start_pos": 59, "end_pos": 70, "type": "TASK", "confidence": 0.9857349991798401}]}, {"text": "In this article we address compound splitting of specialized terms.", "labels": [], "entities": [{"text": "compound splitting", "start_pos": 27, "end_pos": 45, "type": "TASK", "confidence": 0.7061669379472733}]}, {"text": "We propose a multilingual method of compound recognition and splitting, which uses corpus frequencies, lexical data and optionally linguistic rules.", "labels": [], "entities": [{"text": "compound recognition and splitting", "start_pos": 36, "end_pos": 70, "type": "TASK", "confidence": 0.7552623599767685}]}, {"text": "This is a supervised method which requires a small amount of segmented compounds as input.", "labels": [], "entities": []}, {"text": "We evaluate the method on two languages that rarely serve as a material for automatic splitting systems: English and Russian.", "labels": [], "entities": []}, {"text": "The results obtained are competitive with those of a state-of-the-art corpus-driven approach.", "labels": [], "entities": []}], "introductionContent": [{"text": "Compounding is a method of word formation consisting of a combination of two (or more) lexical elements that form a unit of meaning.", "labels": [], "entities": [{"text": "word formation", "start_pos": 27, "end_pos": 41, "type": "TASK", "confidence": 0.7233792990446091}]}, {"text": "In this work we only handle so called \"closed compounds\"), i.e. those forming also a graphical unit.", "labels": [], "entities": []}, {"text": "A great number of languages resort to this word formation.", "labels": [], "entities": [{"text": "word formation", "start_pos": 43, "end_pos": 57, "type": "TASK", "confidence": 0.7513738870620728}]}, {"text": "In some of them such as German, Dutch (Germanic family), Estonian or Finnish (Uralic family) compounding is very regular and well described.", "labels": [], "entities": []}, {"text": "In other languages it is less productive (e.g. Slavic family), or even marginal (most of Romance languages).", "labels": [], "entities": []}, {"text": "This phenomenon is particularly productive in specialized domains because of the necessity to denote the domain concepts in a very concise and precise way.", "labels": [], "entities": []}, {"text": "In addition, specialized texts contain many neoclassical compounds, i.e. compounds with some elements of Greek or Latin etymological origin: hydro + logy = hydrology.", "labels": [], "entities": []}, {"text": "In this article we discuss processing of compound terms, and we carryout the experiments with English and Russian, which are not prototypical compounding languages.", "labels": [], "entities": [{"text": "processing of compound terms", "start_pos": 27, "end_pos": 55, "type": "TASK", "confidence": 0.8313559740781784}]}, {"text": "As a matter of fact, compounding in English is rather productive and widely investigated in linguistic studies.", "labels": [], "entities": [{"text": "compounding", "start_pos": 21, "end_pos": 32, "type": "TASK", "confidence": 0.9710549712181091}]}, {"text": "But this language is rarely subject to experiments in automatic compound splitting.", "labels": [], "entities": [{"text": "compound splitting", "start_pos": 64, "end_pos": 82, "type": "TASK", "confidence": 0.6889204829931259}]}, {"text": "The first reason is that most of English compounds are formed by simple concatenation (airfoil = air + foil, streamtube = stream + tube), so their splitting is supposed to be straightforward.", "labels": [], "entities": []}, {"text": "The second reason is that many compounds in highly compounding languages should be translated into English as multi-word expressions.", "labels": [], "entities": []}, {"text": "That is why the works addressing automatic compound splitting in the context of machine translation often admit that English contains only few closed compounds ().", "labels": [], "entities": [{"text": "automatic compound splitting", "start_pos": 33, "end_pos": 61, "type": "TASK", "confidence": 0.7170916001001993}, {"text": "machine translation", "start_pos": 80, "end_pos": 99, "type": "TASK", "confidence": 0.6906610280275345}]}, {"text": "In these works, the use of English parallel texts helps to extract the multi-word equivalents of compounds from the texts in highly compounding languages.", "labels": [], "entities": []}, {"text": "We assume that English compound terms are still worth splitting and analyzing.", "labels": [], "entities": []}, {"text": "The first reason is that the assumption of independent occurring of English compound elements fails when we consider neoclassical compounds.", "labels": [], "entities": []}, {"text": "The second ground is that distinguishing between compounds and non-compound out-of-dictionary words (named entities, derivative forms, etc.) can be problematic.", "labels": [], "entities": []}, {"text": "In the Russian language the elements of compounds do not often appear as independent words in texts, for example: \u0432\u043e\u0434\u043e\u0441\u043d\u0430\u0431\u0436\u0435\u043d\u0438\u0435 'water supply' vodosnabzhenie 1 = voda 'water' + snabzhenie 'supply' Here the inflection \"a\" of the first component is omitted and the linking morpheme \"o\" is inserted.", "labels": [], "entities": []}, {"text": "Compounding in Russian is less regular than, for instance, in German.", "labels": [], "entities": []}, {"text": "Therefore most of NLP systems for Russian, to our knowledge, store usual compound parts in the lexicon.", "labels": [], "entities": []}, {"text": "For specialized vocabularies this solution does not seem to be sufficient, since the new compounds terms constantly appear.", "labels": [], "entities": []}, {"text": "To handle compounds in typologically different languages, including languages with non-independent components, we propose a corpus-driven splitting system using also string similarity and able to integrate language-specific rules . The article has the following structure.", "labels": [], "entities": []}, {"text": "Section 2 gives a review of some compound splitting methods proposed in the literature.", "labels": [], "entities": [{"text": "compound splitting", "start_pos": 33, "end_pos": 51, "type": "TASK", "confidence": 0.7041947841644287}]}, {"text": "Section 3 presents our splitting method.", "labels": [], "entities": [{"text": "splitting", "start_pos": 23, "end_pos": 32, "type": "TASK", "confidence": 0.9860731363296509}]}, {"text": "In Section 4 the experiments and data are described.", "labels": [], "entities": []}, {"text": "We discuss the results and analyse the errors.", "labels": [], "entities": []}, {"text": "We also compare our system to the state-of-the-art corpus-based method of.", "labels": [], "entities": []}, {"text": "We conclude with Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use a unified strategy for the two major types of compounds, native and neoclassical.", "labels": [], "entities": []}, {"text": "We also consider in our experiments some prefixed words.", "labels": [], "entities": []}, {"text": "Standard prefixation is, of course, a subtype of derivation, and not of compounding.", "labels": [], "entities": []}, {"text": "However, in some boundary cases it is difficult to catch the difference between prefixed formations and neoclassical or native compounds: compare prefix bi-to neoclassical root uniaccording to.", "labels": [], "entities": []}, {"text": "Moreover, from the operational point of view, some prefixed words can be split in the same manner as compounds.", "labels": [], "entities": []}, {"text": "summarizes the size of the resources used in this work.", "labels": [], "entities": []}, {"text": "Wind Energy corpora 3 were crawled from Web pages by means of Babouk (Groc, 2011), a tool dedicated to automatic compilation of domainspecific corpora, with the use of a key-term list.", "labels": [], "entities": [{"text": "Babouk (Groc, 2011)", "start_pos": 62, "end_pos": 81, "type": "DATASET", "confidence": 0.8633585870265961}]}, {"text": "To compute the domain specificity of a term, we needed its frequency in the general language corpus.", "labels": [], "entities": []}, {"text": "For the English language, we used a subpart of the New York Times corpus 4 . For Russian, we used a frequency list computed from The Russian National Corpus .  The application of our method to the training data allowed us to obtain the parameters required for splitting algorithm (see).", "labels": [], "entities": [{"text": "New York Times corpus", "start_pos": 51, "end_pos": 72, "type": "DATASET", "confidence": 0.8548242896795273}, {"text": "Russian National Corpus", "start_pos": 133, "end_pos": 156, "type": "DATASET", "confidence": 0.7896618247032166}]}, {"text": "The selected coefficients were then used to analyse a test dataset for each language.", "labels": [], "entities": []}, {"text": "The evaluation in terms of recall, precision, and F-measure is presented in.", "labels": [], "entities": [{"text": "recall", "start_pos": 27, "end_pos": 33, "type": "METRIC", "confidence": 0.9995423555374146}, {"text": "precision", "start_pos": 35, "end_pos": 44, "type": "METRIC", "confidence": 0.9995394945144653}, {"text": "F-measure", "start_pos": 50, "end_pos": 59, "type": "METRIC", "confidence": 0.9988968372344971}]}, {"text": "The results vary according to the language.", "labels": [], "entities": []}, {"text": "For Russian the precision turns out to be higher than for English: 81% for Top 1 and 82% for Top 5 for Russian against 74% for Top 1 and 78% for Top 5 for English.", "labels": [], "entities": [{"text": "precision", "start_pos": 16, "end_pos": 25, "type": "METRIC", "confidence": 0.9997159838676453}]}, {"text": "However, the recall is much higher for English than for Russian: 87% for Top 1 and 91% for Top 5 against 52% for Russian.", "labels": [], "entities": [{"text": "recall", "start_pos": 13, "end_pos": 19, "type": "METRIC", "confidence": 0.9987761378288269}]}, {"text": "It can be explained by the abundance of component modifications in the latter and by the lack of correct lemmatization for many terms.", "labels": [], "entities": []}, {"text": "We can also notice that for Russian the gain obtained in the Top 5 of segmentation candidates is very small compared to the Top 1, which means that for this language if the correct candidate was found, it was almost always ranked as the best one.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Extracted Lexicon Size and Compound Rate", "labels": [], "entities": [{"text": "Extracted Lexicon Size", "start_pos": 10, "end_pos": 32, "type": "TASK", "confidence": 0.6932326157887777}]}, {"text": " Table 3: Parameters Learned on the Training Dataset", "labels": [], "entities": [{"text": "Training Dataset", "start_pos": 36, "end_pos": 52, "type": "DATASET", "confidence": 0.7844125330448151}]}, {"text": " Table 4: Splitting quality evaluation in terms of R(ecall), P(precision) and F(-measure).  Highlighting corresponds to the experiments in which our method outperforms (Koehn and Knight 2003).", "labels": [], "entities": [{"text": "Splitting quality evaluation", "start_pos": 10, "end_pos": 38, "type": "TASK", "confidence": 0.8760385711987814}, {"text": "precision", "start_pos": 63, "end_pos": 72, "type": "METRIC", "confidence": 0.9519245028495789}, {"text": "F(-measure)", "start_pos": 78, "end_pos": 89, "type": "METRIC", "confidence": 0.8413230031728745}]}, {"text": " Table 5: Splitting quality by (Koehn and Knight 2003) method.", "labels": [], "entities": [{"text": "Splitting", "start_pos": 10, "end_pos": 19, "type": "TASK", "confidence": 0.9923189282417297}]}]}