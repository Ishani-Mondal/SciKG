{"title": [{"text": "Two-Stage Stochastic Natural Language Generation for Email Synthesis by Modeling Sender Style and Topic Structure", "labels": [], "entities": [{"text": "Stochastic Natural Language Generation", "start_pos": 10, "end_pos": 48, "type": "TASK", "confidence": 0.6152290776371956}, {"text": "Email Synthesis", "start_pos": 53, "end_pos": 68, "type": "TASK", "confidence": 0.9464424252510071}]}], "abstractContent": [{"text": "This paper describes a two-stage process for stochastic generation of email, in which the first stage structures the emails according to sender style and topic structure (high-level generation), and the second stage synthesizes text content based on the particulars of an email element and the goals of a given communication (surface-level realization).", "labels": [], "entities": [{"text": "stochastic generation of email", "start_pos": 45, "end_pos": 75, "type": "TASK", "confidence": 0.7859983444213867}]}, {"text": "Synthesized emails were rated in a preliminary experiment.", "labels": [], "entities": []}, {"text": "The results indicate that sender style can be detected.", "labels": [], "entities": []}, {"text": "In addition we found that stochastic generation performs better if applied at the word level than at an original-sentence level (\"template-based\") in terms of email coherence, sentence fluency , naturalness, and preference.", "labels": [], "entities": [{"text": "stochastic generation", "start_pos": 26, "end_pos": 47, "type": "TASK", "confidence": 0.7146166563034058}]}], "introductionContent": [{"text": "This paper focuses on generating language for the email domain, with the goal of producing mails that reflect sender style and the intent of the communication.", "labels": [], "entities": []}, {"text": "Such a process might be used for the generation of common messages (for example a request fora meeting without direct intervention from the sender).", "labels": [], "entities": []}, {"text": "It can also be used in situations where naturalistic email is needed for other applications.", "labels": [], "entities": []}, {"text": "For instance, our email generator was developed to provide emails to be used as part of synthetic evidence of insider threats for purposes of training, prototyping, and evaluating anomaly detectors).", "labels": [], "entities": []}, {"text": "There are two approaches to natural language generation (NLG), one focuses on generating text using templates or rules (linguistic) methods, the another uses corpus-based statistical techniques.", "labels": [], "entities": [{"text": "natural language generation (NLG)", "start_pos": 28, "end_pos": 61, "type": "TASK", "confidence": 0.8202578326066335}]}, {"text": "showed that stochastic generation benefits from two factors: 1) it takes advantage of the practical language of a domain expert instead of the developer and 2) it restates the problem in terms of classification and labeling, where expertise is not required for developing a rule-based generation system.", "labels": [], "entities": [{"text": "stochastic generation", "start_pos": 12, "end_pos": 33, "type": "TASK", "confidence": 0.7986645996570587}]}, {"text": "They found that naive listeners found such utterances as acceptable as human-generated utterances.", "labels": [], "entities": []}, {"text": "also proposed a probabilistic NLG approach to make systems more robust and components more reusable, reducing manual corpus analysis.", "labels": [], "entities": []}, {"text": "However, most work usually focused on wellstructured documents such as news and Wikipedia, while email messages differ from them, which reflect senders' style and are more spontaneous.", "labels": [], "entities": []}, {"text": "segmented email messages into zones, including sender zones, quoted conversation zones, and boilerplate zones.", "labels": [], "entities": []}, {"text": "This paper only models the text in the sender zone, new content from the current sender.", "labels": [], "entities": []}, {"text": "In the present work, we investigate the use of stochastic techniques for generation of a different class of communications and whether global structures can be convincingly created in the email domain.", "labels": [], "entities": []}, {"text": "A lot of NLG systems are applied in dialogue systems, some of which focus on topic modeling, proposing algorithms to balance local fit of information and global coherence.", "labels": [], "entities": []}, {"text": "However, they seldom consider to model the speaker's characteristics.", "labels": [], "entities": []}, {"text": "considered sentiment such as openness and neuroticism to specify characters for dialogue generation.", "labels": [], "entities": [{"text": "dialogue generation", "start_pos": 80, "end_pos": 99, "type": "TASK", "confidence": 0.7923067510128021}]}, {"text": "In stead of modeling authors' attitudes, this paper proposes the first approach of synthesizing emails by modeling their writing patterns.", "labels": [], "entities": []}, {"text": "Specifically we investigate whether stochastic techniques can be used to acceptably model longer texts and individual speaker characteristics in the emails, both of which may require higher cohesion to be acceptable.", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate the performance of sender style, 7 subjects were given 5 real emails from each sender and then 9 synthesized emails.", "labels": [], "entities": []}, {"text": "They were asked to rate each synthesized email for each sender on a scale of 1 (highly confident that the email is not from the sender) to 5 (highly confident that the email is from that sender).", "labels": [], "entities": []}, {"text": "With \u03b1 = 0.75 in (1) for predicting mixture models (higher weight for sender-specific model), average normalized scores the corresponding senders receives account for 45%; this is above chance (which would be 33%).", "labels": [], "entities": [{"text": "predicting mixture", "start_pos": 25, "end_pos": 43, "type": "TASK", "confidence": 0.8818432986736298}]}, {"text": "This suggests that sender style can be noticed by subjects, although the effect is weak, and we are in the process of designing a larger evaluation.", "labels": [], "entities": []}, {"text": "Ina followup questionnaire, subjects indicated that their ratings were based on greeting usage, politeness, the length of email and other characteristics.", "labels": [], "entities": []}, {"text": "We conduct a comparative evaluation of two different generation algorithms, template-based generation and stochastic generation, on the same email structures.", "labels": [], "entities": []}, {"text": "The average number of sentences in synthesized emails is 3.8, because our data is about daily business and has relatively short emails.", "labels": [], "entities": []}, {"text": "Given a structural label, template-based generation consisted of randomly selecting an intact whole sentence with the target structural label.", "labels": [], "entities": []}, {"text": "This could be termed sentence-level NLG, while stochastic generation is word-level NLG.", "labels": [], "entities": []}, {"text": "We presented 30 pairs of (sentence-, word-) synthesized emails, and 7 subjects were asked to compare the overall coherence of an email, its sentence fluency and naturalness; then select their preference.", "labels": [], "entities": []}, {"text": "shows subjects' preference according to the rating criteria.", "labels": [], "entities": []}, {"text": "The word-based stochastic generation outperforms or performs as well as the template-based algorithm for all criteria, where a t-test on an email as a random variable shows no significant improvement but p-value is close to 0.05 (p = 0.051).", "labels": [], "entities": []}, {"text": "Subjects indicated that emails from word-based stochastic generation are more natural; word-level generation is less likely to produce an unusual sentences from the real data; word-level generation produces more conventional sentences.", "labels": [], "entities": []}, {"text": "Some subjects noted that neither email seemed human-written, perhaps an artifact of our experimental design.", "labels": [], "entities": []}, {"text": "Nevertheless, we believe that this stochastic approach would require less effort compared to most rule-based or template-based systems in terms of knowledge engineering.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Generation algorithm comparison (%).", "labels": [], "entities": []}]}