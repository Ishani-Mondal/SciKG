{"title": [{"text": "Paraphrase Detection for Short Answer Scoring", "labels": [], "entities": [{"text": "Paraphrase Detection", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8479134142398834}, {"text": "Short Answer Scoring", "start_pos": 25, "end_pos": 45, "type": "TASK", "confidence": 0.7010495265324911}]}], "abstractContent": [{"text": "We describe a system that grades learner answers in reading comprehension tests in the context of foreign language learning.", "labels": [], "entities": []}, {"text": "This task, also known as short answer scoring, essentially requires determining whether a semantic entailment relationship holds between an individual learner answer and a target answer; thus semantic information is a necessary part of any automatic short answer scoring system.", "labels": [], "entities": [{"text": "short answer scoring", "start_pos": 25, "end_pos": 45, "type": "TASK", "confidence": 0.7394336462020874}]}, {"text": "At the same time the method must be robust to the particularities of learner language.", "labels": [], "entities": []}, {"text": "We propose using paraphrase detection, a method that meets both requirements.", "labels": [], "entities": [{"text": "paraphrase detection", "start_pos": 17, "end_pos": 37, "type": "TASK", "confidence": 0.9185557663440704}]}, {"text": "The basis for our specific paraphrasing method is word alignment learned from parallel corpora which we create from the available data in the CREG corpus (Corpus for Reading Comprehension Exercises for German).", "labels": [], "entities": [{"text": "word alignment learned", "start_pos": 50, "end_pos": 72, "type": "TASK", "confidence": 0.8196891744931539}, {"text": "CREG corpus", "start_pos": 142, "end_pos": 153, "type": "DATASET", "confidence": 0.8712962567806244}]}, {"text": "We show the usefulness of this kind of information for the task of short answer scoring.", "labels": [], "entities": [{"text": "short answer scoring", "start_pos": 67, "end_pos": 87, "type": "TASK", "confidence": 0.7494767109553019}]}, {"text": "Combining our results with existing approaches we obtain an improvement tendency.", "labels": [], "entities": []}], "introductionContent": [{"text": "Reading comprehension exercises area common means of assessment for language teaching: students read a text in the language they are learning and are then asked to answer questions about the text.", "labels": [], "entities": []}, {"text": "Answers to such questions typically consist of one sentence, sometimes two or three.", "labels": [], "entities": []}, {"text": "They are graded taking the semantic content into consideration, ignoring spelling or grammatical errors.", "labels": [], "entities": []}, {"text": "Developing methods for the automatic scoring of answers (in short: short answer scoring) is a task of considerable practical relevance, in particular with regard to the increasing availability of online language courses.", "labels": [], "entities": [{"text": "automatic scoring of answers (in short: short answer scoring)", "start_pos": 27, "end_pos": 88, "type": "TASK", "confidence": 0.600866973400116}]}, {"text": "At the same time, it is an interesting challenge for computational semantics, and it calls for the use of methods from semantics-focused natural language processing.", "labels": [], "entities": []}, {"text": "The short answer scoring (SAS) task stands in a close relationship to the task of recognizing textual entailment (RTE): A correct student answer should entail (ideally, be identical in content with) one of the target answers, i.e., the sample solutions created by a teacher.", "labels": [], "entities": [{"text": "short answer scoring (SAS)", "start_pos": 4, "end_pos": 30, "type": "TASK", "confidence": 0.5444655269384384}, {"text": "recognizing textual entailment (RTE)", "start_pos": 82, "end_pos": 118, "type": "TASK", "confidence": 0.7087644189596176}]}, {"text": "Moreover, the student answer should be entailed by the text.", "labels": [], "entities": []}, {"text": "shows an example of a passage of a reading text, a question about the text, the target answer and both a correct and incorrect learner answer.", "labels": [], "entities": []}, {"text": "Note that the first learner answer is graded as correct because it is a paraphrase of the target answer, despite the errors it contains.", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate precision of the extracted paraphrases, we again follow and.", "labels": [], "entities": [{"text": "precision", "start_pos": 12, "end_pos": 21, "type": "METRIC", "confidence": 0.9990967512130737}]}, {"text": "For each of the two systems, 300 fragment pairs are randomly extracted, half with the unidirectional version and half with the bidirectional.", "labels": [], "entities": []}, {"text": "These are evenly distributed across LA-TA pairs and answer-text sentence pairs.", "labels": [], "entities": []}, {"text": "Each fragment is labeled by two annotators with one of four categories: paraphrase, related, unrelated, or invalid.", "labels": [], "entities": []}, {"text": "The label related is assigned when there is overlap between the two fragments, but they are not unidirectional bidirectional basic 0.78 0.74 chunk-based 0.69 0.71: Precision of paraphrase fragment detection paraphrases, and invalid is assigned if one or both fragments are completely ungrammatical or not readable.", "labels": [], "entities": [{"text": "paraphrase fragment detection", "start_pos": 177, "end_pos": 206, "type": "TASK", "confidence": 0.6895721157391866}]}, {"text": "Annotators were not told the type of the sentence pair, and they were instructed to ignore spelling and grammatical errors in evaluating paraphrases.", "labels": [], "entities": []}, {"text": "shows the inter-annotator agreement in 2 conditions: if we consider all 4 labels separately, and if we instead merge paraphrase and related as well as unrelated and invalid.", "labels": [], "entities": []}, {"text": "Results are along the lines of (Regneri and Wang, 2012) who report Kappa values of 0.55 for four-label annotation and 0.71 fora two-label condition.", "labels": [], "entities": [{"text": "Kappa", "start_pos": 67, "end_pos": 72, "type": "METRIC", "confidence": 0.987261950969696}]}, {"text": "Our basic system shows worse agreement than the chunk-based.", "labels": [], "entities": [{"text": "agreement", "start_pos": 29, "end_pos": 38, "type": "METRIC", "confidence": 0.9601548314094543}]}, {"text": "This is due to the fact that basic fragments are often linguistically not well-formed and are therefore harder to annotate.", "labels": [], "entities": []}, {"text": "For the final gold-standard, all conflicts have been resolved by a third annotator.", "labels": [], "entities": []}, {"text": "This gold-standard annotation is then used for evaluating the quality of the fragments.", "labels": [], "entities": []}, {"text": "For measuring the precision of the extracted paraphrases, i.e. for measuring what percentage of the fragment pairs identified should be considered as paraphrases or related, we use the two-label condition.", "labels": [], "entities": [{"text": "precision", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.9987528324127197}]}, {"text": "Results are presented in table 2.", "labels": [], "entities": []}, {"text": "Precision on our dataset is in the same range as that reported by (62 to 67%) on a monolingual comparable corpus.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9850955605506897}]}, {"text": "Note however that this evaluation covers only a very small dataset as compared to the overall parallel corpus.", "labels": [], "entities": []}, {"text": "Overall the performance of the basic system is better than the chunk-based.", "labels": [], "entities": []}, {"text": "This is an unexpected result because the chunk-based system was developed specifically to improve the quality of the basic fragments.", "labels": [], "entities": []}, {"text": "However, missing tokens like prepositions that are added to a fragment by the chunk system can change its meaning and as a consequence the fragments are no longer related.", "labels": [], "entities": []}, {"text": "Between the unidirectional and bidirectional approaches there is no stastically significant difference, according to a chi-squared test.", "labels": [], "entities": []}, {"text": "For the application of the extracted paraphrase fragments to short answer scoring, the unidirectional approach is used, because it gave us the best results for the generally better basic version of the system.", "labels": [], "entities": [{"text": "short answer scoring", "start_pos": 61, "end_pos": 81, "type": "TASK", "confidence": 0.5862572193145752}]}, {"text": "We expect variability across correct and incorrect answers, because in scoring a learner answer, strict paraphrases are not always necessary.", "labels": [], "entities": []}, {"text": "For example a question in the corpus asking \"Wer war an der T\u00fcr\" (Who was by the door?) with the target answer \"Drei Soldaten (three soldiers) waren an der T\u00fcr\" the learner answer \"Drei M\u00e4nner (three men) waren an der T\u00fcr\", although less specific, was also graded as correct by the teachers.", "labels": [], "entities": []}, {"text": "To investigate this variability, we look at the distribution of the four categories across the various subcorpora.", "labels": [], "entities": []}, {"text": "depicts the distribution of the labels -exemplarily for the chunk-based versionshowing how often each annotation label occured within the five subcorpora TA -correct LA, TA -incorrect LA, TA -text sentence, correct LA -text sentence, incorrect LA -text sentence.", "labels": [], "entities": []}, {"text": "We can see that correct learner answers lead to more paraphrases of the target answer (18) than do incorrect learner answers (2).", "labels": [], "entities": []}, {"text": "Incorrect learner answers, however, have a much higher degree of unrelated fragments with the target answer (41 vs 15).", "labels": [], "entities": []}, {"text": "Correctness has not much influence on the validity.", "labels": [], "entities": [{"text": "Correctness", "start_pos": 0, "end_pos": 11, "type": "METRIC", "confidence": 0.94044029712677}, {"text": "validity", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.8517007827758789}]}, {"text": "In the subcorpora involving text sentences, both correct and incorrect learner answers have a similarly high degree of paraphrase and related cases.", "labels": [], "entities": []}, {"text": "That is the case because both correct and incorrect learner answers are often paraphrases of some part of the text.", "labels": [], "entities": []}, {"text": "In the case of an correct answer, the target answer is often a paraphrase of the same text sentences as the text sentence for the learner answer, in the case of an incorrect learner answer, the student often erroneously paraphrased a text sentence that has nothing to do with the correct answer.", "labels": [], "entities": []}, {"text": "We compare our approach to both the alignment model (as in () and the deep semantic model by.", "labels": [], "entities": []}, {"text": "We re-implement the alignment model using features for token and chunk alignment reaching an accuracy of 86.8% on the CREG corpus (compared to 84.6% in the () model).", "labels": [], "entities": [{"text": "token and chunk alignment", "start_pos": 55, "end_pos": 80, "type": "TASK", "confidence": 0.6198423206806183}, {"text": "accuracy", "start_pos": 93, "end_pos": 101, "type": "METRIC", "confidence": 0.9987770915031433}, {"text": "CREG corpus", "start_pos": 118, "end_pos": 129, "type": "DATASET", "confidence": 0.9235334992408752}]}, {"text": "The deep semantic model reaches an accuracy of 86.3%, also on the CREG data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9995834231376648}, {"text": "CREG data", "start_pos": 66, "end_pos": 75, "type": "DATASET", "confidence": 0.9301584959030151}]}, {"text": "We make direct comparison against these two scores; a random baseline for this balanced data set is 50%.", "labels": [], "entities": []}, {"text": "We evaluate using tenfold cross-validation, running the complete paraphrase fragment detection method (Section 3) on nine folds for training.", "labels": [], "entities": [{"text": "paraphrase fragment detection", "start_pos": 65, "end_pos": 94, "type": "TASK", "confidence": 0.6124899486700693}]}, {"text": "For the test corpus, of course, we don't know ahead of time whether answers are corrector not.", "labels": [], "entities": []}, {"text": "Thus we build our input corpus without taking advantage of this information.", "labels": [], "entities": []}, {"text": "In this setting, each pair involving a LA or TA is included 10 times, regardless of the answer's correctness.", "labels": [], "entities": [{"text": "LA", "start_pos": 39, "end_pos": 41, "type": "METRIC", "confidence": 0.9829676747322083}, {"text": "TA", "start_pos": 45, "end_pos": 47, "type": "METRIC", "confidence": 0.5176487565040588}]}, {"text": "We evaluate our model alone and using additional features from the other two models, as is shown in table 5: In order to seethe contribution of the direct and indirect feature sets, we evaluate those sets individually (paraphrases direct and paraphrases indirect) and together (paraphrases combined).", "labels": [], "entities": []}, {"text": "For combining with the other models, we always use the combined set of paraphrase features.", "labels": [], "entities": []}, {"text": "To evaluate our model in combination with the alignment model (paraphrases + aligment system), we add the features from our reimplementation.", "labels": [], "entities": []}, {"text": "We also combine our model with both of the other two models (paraphrases + aligment model + deep semantics), using the semantic scores obtained by summarizes our results: We can see that our system alone, while being far from reaching the state of the art, can reasonably differentiate between correct and incorrect answers.", "labels": [], "entities": []}, {"text": "The direct comparison of learner answer and target answer (paraphrases direct) works better than just the indirect comparison via fragments obtained from alignment with the text.", "labels": [], "entities": []}, {"text": "In combination, the indirect features still contribute to the performance paraphrases combined, although not in a statistically significant way.", "labels": [], "entities": []}, {"text": "When combining the paraphrase features with the features from the alignment system, we don't get an improvement over the alignment system (86.8%).", "labels": [], "entities": []}, {"text": "When additionally adding the semantic score to both feature sets, we reach our best result with an accuracy of 88.9% which is not significantly better (\u03b1=0.25 according to a McNemar test) than the comparison figure of 86.8%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 99, "end_pos": 107, "type": "METRIC", "confidence": 0.9993399977684021}]}, {"text": "When comparing the goldlink to the autolink condition, we see an advantage of having the optimal information about the best matching sentence in the indirect feature set.", "labels": [], "entities": []}, {"text": "There is no clear trend as to whether the basic or the chunk-based system performs better.", "labels": [], "entities": []}, {"text": "The paraphrase fragments model on its own is not good enough to beat the other methods.", "labels": [], "entities": []}, {"text": "However, combining the three systems gives an improvement of 2.1%, which is an indication of complementary information provided by the different feature sets.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 5: Accuracy on CREG balanced corpus with various model combinations", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9970706701278687}, {"text": "CREG balanced corpus", "start_pos": 22, "end_pos": 42, "type": "DATASET", "confidence": 0.6369524498780569}]}]}