{"title": [{"text": "Exploring Consensus in Machine Translation for Quality Estimation", "labels": [], "entities": [{"text": "Machine Translation for Quality Estimation", "start_pos": 23, "end_pos": 65, "type": "TASK", "confidence": 0.7054665565490723}]}], "abstractContent": [{"text": "This paper presents the use of consensus among Machine Translation (MT) systems for the WMT14 Quality Estimation shared task.", "labels": [], "entities": [{"text": "Machine Translation (MT)", "start_pos": 47, "end_pos": 71, "type": "TASK", "confidence": 0.846091628074646}, {"text": "WMT14 Quality Estimation shared task", "start_pos": 88, "end_pos": 124, "type": "TASK", "confidence": 0.875079357624054}]}, {"text": "Consensus is explored hereby comparing the MT system output against several alternative machine translations using standard evaluation metrics.", "labels": [], "entities": [{"text": "MT", "start_pos": 43, "end_pos": 45, "type": "TASK", "confidence": 0.9769366383552551}]}, {"text": "Figures extracted from such metrics are used as features to complement baseline prediction models.", "labels": [], "entities": []}, {"text": "The hypothesis is that knowing whether the translation of interest is similar or dissimilar to translations from multiple different MT systems can provide useful information regarding the quality of such a translation.", "labels": [], "entities": []}], "introductionContent": [{"text": "While Machine Translation (MT) evaluation metrics can rely on the similarity of the MT system output to reference (human) translations as a proxy to quality assessment, this is not possible for MT systems in use, translating unseen texts.", "labels": [], "entities": [{"text": "Machine Translation (MT) evaluation", "start_pos": 6, "end_pos": 41, "type": "TASK", "confidence": 0.854684645930926}]}, {"text": "Quality Estimation (QE) metrics are used in such settings as away of predicting translation quality.", "labels": [], "entities": [{"text": "predicting translation", "start_pos": 69, "end_pos": 91, "type": "TASK", "confidence": 0.8793426156044006}]}, {"text": "While reference translations are not available for QE, previous work has explored the so called pseudoreferences.", "labels": [], "entities": [{"text": "QE", "start_pos": 51, "end_pos": 53, "type": "TASK", "confidence": 0.7745895981788635}]}, {"text": "Pseudo-references are alternative translations produced by MT systems different from the system that we intend to predict quality for).", "labels": [], "entities": [{"text": "MT", "start_pos": 59, "end_pos": 61, "type": "TASK", "confidence": 0.9509243965148926}]}, {"text": "These can be used to provide additional features to train QE models.", "labels": [], "entities": []}, {"text": "Such features are normally figures resulting from automatic metrics (such as BLEU,) computed between pseudo-references and the output of the given MT system.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 77, "end_pos": 81, "type": "METRIC", "confidence": 0.9985407590866089}]}, {"text": "explore pseudoreferences for document-level QE prediction to rank outputs from an MT system.", "labels": [], "entities": [{"text": "QE prediction", "start_pos": 44, "end_pos": 57, "type": "TASK", "confidence": 0.8751382231712341}]}, {"text": "The pseudoreferences-based features are BLEU scores extracted by comparing the output of the MT system under investigation and the output of an offthe-shelf MT system, for both the target and the source texts.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 40, "end_pos": 44, "type": "METRIC", "confidence": 0.9981253743171692}, {"text": "MT", "start_pos": 93, "end_pos": 95, "type": "TASK", "confidence": 0.9268565773963928}]}, {"text": "The statistical MT system training data is also used as pseudo-references to compute training data-based features.", "labels": [], "entities": [{"text": "MT", "start_pos": 16, "end_pos": 18, "type": "TASK", "confidence": 0.9529746770858765}]}, {"text": "The use of pseudoreferences has been shown to outperform strong baseline results.", "labels": [], "entities": []}, {"text": "propose a method that uses sentence-level prediction models for document-level QE.", "labels": [], "entities": []}, {"text": "They also use a pseudo-references-based feature (based in BLEU) and claim that this feature is one of the most powerful in the framework.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 58, "end_pos": 62, "type": "METRIC", "confidence": 0.9066656827926636}]}, {"text": "For QE at sentence-level,  use BLEU based on pseudo-references combined with other features to build the best QE system of the WMT12 QE shared task.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 31, "end_pos": 35, "type": "METRIC", "confidence": 0.9978072047233582}, {"text": "WMT12 QE shared task", "start_pos": 127, "end_pos": 147, "type": "DATASET", "confidence": 0.8360795378684998}]}, {"text": "1  use pseudo-references in the same way to extract a BLEU feature for sentence-level prediction.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 54, "end_pos": 58, "type": "METRIC", "confidence": 0.9958437085151672}, {"text": "sentence-level prediction", "start_pos": 71, "end_pos": 96, "type": "TASK", "confidence": 0.7355000078678131}]}, {"text": "Feature analysis on a number of datasets showed that this feature contributed the most across all datasets.", "labels": [], "entities": []}, {"text": "Louis and Nenkova (2013) apply pseudoreferences for summary evaluation.", "labels": [], "entities": [{"text": "summary evaluation", "start_pos": 52, "end_pos": 70, "type": "TASK", "confidence": 0.844281792640686}]}, {"text": "They use six systems classified as \"best systems\", \"mediocre systems\" or \"worst systems\" to make the comparison, with ROUGE () as quality score.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 118, "end_pos": 123, "type": "METRIC", "confidence": 0.9984834790229797}]}, {"text": "They also experiment with a combination of the \"best systems\" and the \"worst systems\".", "labels": [], "entities": []}, {"text": "The use of only \"best systems\" led to the best results.", "labels": [], "entities": []}, {"text": "Examples of \"bad summaries\" are said not to be very useful because a summary close to the worst systems outputs can mean that either it is bad or it is too different from the best systems outputs in terms of content.", "labels": [], "entities": []}, {"text": "use pseudo-references to improve MT evaluation by combining them with a single human reference.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 33, "end_pos": 46, "type": "TASK", "confidence": 0.9731082320213318}]}, {"text": "They show that the use of pseudo-references im-proves the correlation with human judgements.", "labels": [], "entities": []}, {"text": "claim that pseudoreferences should be produced by systems as different as possible from the MT system of interest.", "labels": [], "entities": [{"text": "MT", "start_pos": 92, "end_pos": 94, "type": "TASK", "confidence": 0.9174626469612122}]}, {"text": "This ensures that the similarities found among the systems' translations are not related to the similarities of the systems themselves.", "labels": [], "entities": []}, {"text": "Therefore, the assumption that a translation from system X shares some characteristics with a translation from system Y is not a mere coincidence.", "labels": [], "entities": []}, {"text": "Another way to make the most of pseudo-references is to use an MT system known as generally better (or worse) than the MT system of interest.", "labels": [], "entities": []}, {"text": "In that case, the comparison will lead to whether the MT system of interest is similar to a good (or bad) MT system.", "labels": [], "entities": [{"text": "MT", "start_pos": 54, "end_pos": 56, "type": "TASK", "confidence": 0.9669796228408813}]}, {"text": "However, inmost scenarios it is difficult to rely on the average translation quality of a given system as an absolute indicator of its quality.", "labels": [], "entities": []}, {"text": "This is particularly true for sentence-level QE, where the quality of a given system can vary significantly across sentences.", "labels": [], "entities": []}, {"text": "Finding translations from MT systems that are considerably different can also be a challenge.", "labels": [], "entities": [{"text": "MT", "start_pos": 26, "end_pos": 28, "type": "TASK", "confidence": 0.7437351942062378}]}, {"text": "In this paper we exploit pseudo-references in a different way: measuring the consensus among different MT systems in the translations they produce.", "labels": [], "entities": [{"text": "MT", "start_pos": 103, "end_pos": 105, "type": "TASK", "confidence": 0.9652093052864075}]}, {"text": "As sources of pseudoreferences, we use translations given in a multitranslation dataset or those produced by the participants in the WMT translation task for the same data.", "labels": [], "entities": [{"text": "WMT translation task", "start_pos": 133, "end_pos": 153, "type": "TASK", "confidence": 0.9096141457557678}]}, {"text": "While some MT systems can be similar to each other, for some language pairs, such as English-Spanish, a wide range of MT systems with different average qualities are available.", "labels": [], "entities": [{"text": "MT", "start_pos": 11, "end_pos": 13, "type": "TASK", "confidence": 0.9722499847412109}, {"text": "MT", "start_pos": 118, "end_pos": 120, "type": "TASK", "confidence": 0.9468264579772949}]}, {"text": "Our hypothesis is that by using translations from several MT systems we can find consensual information (even if some of the systems are similar to the one of interest).", "labels": [], "entities": []}, {"text": "The use of more than one MT system is expected to smooth out the effect of \"coincidences\" in the similarities between systems' translations.", "labels": [], "entities": [{"text": "MT", "start_pos": 25, "end_pos": 27, "type": "TASK", "confidence": 0.9553934931755066}]}, {"text": "This paper describes the use of consensual information for the WMT14 QE shared task (USHEFF-consensus system), simulating a scenario where we do not know the quality of the pseudo-references, nor the characteristics of any MT systems (the system of interest or the systems which generated the pseudo-references).", "labels": [], "entities": [{"text": "WMT14 QE shared task (USHEFF-consensus system)", "start_pos": 63, "end_pos": 109, "type": "DATASET", "confidence": 0.6866274699568748}]}, {"text": "We participated in all variants of Task 1, sentence-level QE, for both for scoring and ranking.", "labels": [], "entities": []}, {"text": "Section 2 explains how we extracted consensual information for all tasks.", "labels": [], "entities": []}, {"text": "Section 3 shows our official results compared to the baselines provided.", "labels": [], "entities": []}, {"text": "Section 4 presents some conclusions.", "labels": [], "entities": []}], "datasetContent": [{"text": "The results reported herein are the official shared task results, that is, they were computed using the true scores of the test set made available by the organisers after our submission.", "labels": [], "entities": []}, {"text": "For training the QE models, we used Support Vector Machines (SVM) regression algorithm with a radial basis function (RBF) kernel with the hyperparameters optimised via grid search.", "labels": [], "entities": []}, {"text": "The scikit-learn algorithm available in the QuEst Framework) was used for that.", "labels": [], "entities": [{"text": "QuEst Framework", "start_pos": 44, "end_pos": 59, "type": "DATASET", "confidence": 0.8931458592414856}]}, {"text": "We compared the results obtained against using only the QuEst baseline (BL) features, which is the same system used as the official baseline for the shared task.", "labels": [], "entities": [{"text": "QuEst baseline (BL)", "start_pos": 56, "end_pos": 75, "type": "METRIC", "confidence": 0.6470807075500489}]}, {"text": "For the scoring variant we also compare our results against a baseline that \"predicts\" the average of the true scores of the training set as scores for each sentence of the test set (Mean -each sentence has the same predicted score).", "labels": [], "entities": []}, {"text": "For all language pairs in Task 1.1, shows the average results for the scoring variant using MAE (Mean Absolute Error) as evaluation metric, while shows the results for the ranking variant using DeltaAvg.", "labels": [], "entities": [{"text": "MAE (Mean Absolute Error)", "start_pos": 92, "end_pos": 117, "type": "METRIC", "confidence": 0.9303022027015686}]}, {"text": "The results for scoring improved over the baselines with the use of consensual information for language pairs DE-EN and EN-ES.", "labels": [], "entities": []}, {"text": "For EN-DE and ES-EN the consensual features achieved similar results to BL.", "labels": [], "entities": [{"text": "BL", "start_pos": 72, "end_pos": 74, "type": "METRIC", "confidence": 0.8716058731079102}]}, {"text": "The best result for consensual information features was achieved with EN-ES (0.03 of MAE difference from BL).", "labels": [], "entities": [{"text": "EN-ES", "start_pos": 70, "end_pos": 75, "type": "METRIC", "confidence": 0.9615505337715149}, {"text": "MAE", "start_pos": 85, "end_pos": 88, "type": "METRIC", "confidence": 0.9965406060218811}, {"text": "BL", "start_pos": 105, "end_pos": 107, "type": "METRIC", "confidence": 0.9572544097900391}]}, {"text": "For the ranking variant, the consensual information improved the results for all language pairs.", "labels": [], "entities": []}, {"text": "The largest improvement from consensual-based features was achieved for ES-EN, with a difference of 0.11 from the baseline.", "labels": [], "entities": [{"text": "ES-EN", "start_pos": 72, "end_pos": 77, "type": "DATASET", "confidence": 0.7203352451324463}]}, {"text": "It is worth mentioning that for ES-EN our system achieved the best ranking result in Task 1.1.", "labels": [], "entities": []}, {"text": "Since the results varied for different languages pairs, we further inspected them for each language pair.", "labels": [], "entities": []}, {"text": "First, we looked at the true scores distribution and realised that the first batch of translations for each language pair was probably the human reference since the percentage of 1s -the best quality score -was much higher for this system (see.", "labels": [], "entities": []}, {"text": "By using this human translation as a reference for the other MT systems, we computed BLEU for each sentence.", "labels": [], "entities": [{"text": "MT", "start_pos": 61, "end_pos": 63, "type": "TASK", "confidence": 0.9423053860664368}, {"text": "BLEU", "start_pos": 85, "end_pos": 89, "type": "METRIC", "confidence": 0.9990028738975525}]}, {"text": "For DE-EN, EN-DE and EN-ES, the various systems appeared to be less dissimilar in terms of BLEU, when compared to ES-EN.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 91, "end_pos": 95, "type": "METRIC", "confidence": 0.9994677901268005}]}, {"text": "For ES-EN, the difference between the two MT systems was higher than for other language pairs (0.12 for the test set and 0.11 for the training set).", "labels": [], "entities": [{"text": "MT", "start_pos": 42, "end_pos": 44, "type": "TASK", "confidence": 0.963420569896698}]}, {"text": "Moreover, for DE-EN, EN-DE and EN-ES, the difference between the averaged BLEU score of the training set and the average BLEU score of the test set is very small (smaller than 0.01).", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 74, "end_pos": 84, "type": "METRIC", "confidence": 0.9759059548377991}, {"text": "BLEU score", "start_pos": 121, "end_pos": 131, "type": "METRIC", "confidence": 0.9792693555355072}]}, {"text": "For ES-EN, however, the difference between the scores for the training and test sets was also higher (0.04 for System1 and 0.03 for System2).", "labels": [], "entities": [{"text": "ES-EN", "start_pos": 4, "end_pos": 9, "type": "DATASET", "confidence": 0.6546333432197571}]}, {"text": "This can be one reason why the consensual features did not show improvements for this language pair.", "labels": [], "entities": []}, {"text": "Since the systems are considerably different and also there is a considerable difference between training and test sets, the data can be too noisy to be used as pseudo-references.", "labels": [], "entities": []}, {"text": "For EN-DE, the reasons for the bad performance of consensual features are not clear.", "labels": [], "entities": [{"text": "EN-DE", "start_pos": 4, "end_pos": 9, "type": "DATASET", "confidence": 0.7560405135154724}]}, {"text": "This language pair showed the worst average quality scores for all systems.", "labels": [], "entities": []}, {"text": "Reasons for this can include characteristics of German language, such as compound words which are not well treated in MT, and complex grammar.", "labels": [], "entities": [{"text": "MT", "start_pos": 118, "end_pos": 120, "type": "TASK", "confidence": 0.9471421837806702}]}, {"text": "One hypothesis is that these low BLEU scores (as shows) introduce noise instead of useful information for QE.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 33, "end_pos": 37, "type": "METRIC", "confidence": 0.998789370059967}, {"text": "QE", "start_pos": 106, "end_pos": 108, "type": "TASK", "confidence": 0.5460425615310669}]}, {"text": "Another difference that appeared only in EN-DE was the distributions of the scores across the different systems.", "labels": [], "entities": []}, {"text": "As shows, System1 has a distribution considerably different from the other two systems.", "labels": [], "entities": []}, {"text": "For the other language pairs, the distributions across different systems were more uniform.", "labels": [], "entities": []}, {"text": "This difference can be another factor influencing the results for this language pair.", "labels": [], "entities": []}, {"text": "shows the results for scoring (MAE) and   For Tasks 1.2 and 1.3 the use of consensual information only slightly improved the baseline results for scoring.", "labels": [], "entities": [{"text": "scoring (MAE)", "start_pos": 22, "end_pos": 35, "type": "METRIC", "confidence": 0.6760602816939354}]}, {"text": "For the ranking variant, BL+Consensus achieved better results, but only significantly so for Task 1.2.", "labels": [], "entities": [{"text": "BL", "start_pos": 25, "end_pos": 27, "type": "METRIC", "confidence": 0.9647234678268433}]}, {"text": "Therefore, consensual information seems useful to rank sentences according to predicted HTER, its contribution to predicting actual HTER is not noticeable.", "labels": [], "entities": [{"text": "predicting actual HTER", "start_pos": 114, "end_pos": 136, "type": "TASK", "confidence": 0.7782274087270101}]}, {"text": "For post-editing time as quality labels, the improvement achieved with the use of consensual information was marginal.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Scoring results for Task 1.1 in terms of MAE", "labels": [], "entities": [{"text": "MAE", "start_pos": 51, "end_pos": 54, "type": "DATASET", "confidence": 0.7960935235023499}]}, {"text": " Table 2: Ranking results for Task 1.1 in terms of DeltaAvg", "labels": [], "entities": [{"text": "DeltaAvg", "start_pos": 51, "end_pos": 59, "type": "DATASET", "confidence": 0.8800978660583496}]}, {"text": " Table 4: Scoring results of Tasks 1.2 and 1.3 in  terms of MAE", "labels": [], "entities": [{"text": "MAE", "start_pos": 60, "end_pos": 63, "type": "DATASET", "confidence": 0.8223397731781006}]}, {"text": " Table 5: Ranking results of Tasks 1.2 and 1.3 in  terms of DeltaAvg", "labels": [], "entities": [{"text": "DeltaAvg", "start_pos": 60, "end_pos": 68, "type": "DATASET", "confidence": 0.8854795098304749}]}]}