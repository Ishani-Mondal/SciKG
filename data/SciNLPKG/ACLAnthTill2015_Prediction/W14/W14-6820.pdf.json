{"title": [{"text": "Overview of SIGHAN 2014 Bake-off for Chinese Spelling Check", "labels": [], "entities": [{"text": "SIGHAN 2014 Bake-off", "start_pos": 12, "end_pos": 32, "type": "TASK", "confidence": 0.5997518102327982}, {"text": "Chinese Spelling Check", "start_pos": 37, "end_pos": 59, "type": "TASK", "confidence": 0.6779367129007975}]}], "abstractContent": [{"text": "This paper introduces a Chinese Spelling Check campaign organized for the SIGHAN 2014 bake-off, including task description, data preparation, performance metrics, and evaluation results based on essays written by Chinese as a foreign language learners.", "labels": [], "entities": [{"text": "Chinese Spelling Check", "start_pos": 24, "end_pos": 46, "type": "TASK", "confidence": 0.654264916976293}, {"text": "SIGHAN 2014 bake-off", "start_pos": 74, "end_pos": 94, "type": "TASK", "confidence": 0.5230586230754852}]}, {"text": "The hope is that such evaluations can produce more advanced Chinese spelling check techniques .", "labels": [], "entities": [{"text": "Chinese spelling check", "start_pos": 60, "end_pos": 82, "type": "TASK", "confidence": 0.6453286608060201}]}], "introductionContent": [{"text": "Chinese spelling errors frequently arise from confusion between multiple Chinese characters which are phonologically and visually similar, but semantically distinct ().", "labels": [], "entities": []}, {"text": "The SIGHAN 2013 Chinese Spelling Check Bakeoff was the first campaign to provide data sets as benchmarks for the objective performance evaluation of Chinese spelling checkers ( ).", "labels": [], "entities": [{"text": "SIGHAN 2013 Chinese Spelling Check Bakeoff", "start_pos": 4, "end_pos": 46, "type": "TASK", "confidence": 0.6026092916727066}]}, {"text": "The collected data set is publicly available at http://ir.itc.ntnu.edu.tw/lre/sighan7csc.htm.", "labels": [], "entities": []}, {"text": "The competition resulted in the integration of effective NLP techniques in the development of Chinese spelling checkers.", "labels": [], "entities": [{"text": "Chinese spelling checkers", "start_pos": 94, "end_pos": 119, "type": "TASK", "confidence": 0.6039309899012247}]}, {"text": "Language modeling was used to glean extra semantic clues and collect web resources together to identify and correct spelling errors ( . A hybrid model was proposed to combine language models and statistical machine translation for spelling error correction ( ).", "labels": [], "entities": [{"text": "spelling error correction", "start_pos": 231, "end_pos": 256, "type": "TASK", "confidence": 0.752147893110911}]}, {"text": "A linear regression model was trained using phonological and orthographic similarities to correct misspelled characters ().", "labels": [], "entities": []}, {"text": "Web-based measures were adopted to score candidates for Chinese spelling error correction ( . A graph model was used to represent the sentence, using the single source shortest path algorithm for correcting spelling errors () SIGHAN 2014 Bake-off, again features a Chinese Spelling Check task, providing an evaluation platform for the development and implementation of automatic Chinese spelling checkers.", "labels": [], "entities": [{"text": "Chinese spelling error correction", "start_pos": 56, "end_pos": 89, "type": "TASK", "confidence": 0.7089083716273308}, {"text": "SIGHAN 2014 Bake-off", "start_pos": 226, "end_pos": 246, "type": "DATASET", "confidence": 0.6683655381202698}, {"text": "Chinese Spelling Check task", "start_pos": 265, "end_pos": 292, "type": "TASK", "confidence": 0.64107745885849}]}, {"text": "Given a passage composed of several sentences, the checker should identify all possible spelling errors, highlight their locations and suggest possible corrections.", "labels": [], "entities": []}, {"text": "While previous tasks were based on essays written by native Chinese speakers, the current task is based on essays written by learners of Chinese as a Foreign Language (CFL), which should provide a greater challenge The rest of this article is organized as follows.", "labels": [], "entities": [{"text": "Chinese as a Foreign Language (CFL)", "start_pos": 137, "end_pos": 172, "type": "TASK", "confidence": 0.6491773203015327}]}, {"text": "Section 2 provides an overview of the SIGHAN 2014 Bake-off Chinese Spelling Check task.", "labels": [], "entities": [{"text": "SIGHAN 2014 Bake-off Chinese Spelling Check task", "start_pos": 38, "end_pos": 86, "type": "TASK", "confidence": 0.7435000113078526}]}, {"text": "Section 3 introduces the data sets used for evaluation.", "labels": [], "entities": []}, {"text": "Section 4 proposes evaluation metrics.", "labels": [], "entities": []}, {"text": "Section 5 compares results for the various contestants.", "labels": [], "entities": []}, {"text": "Finally, we conclude this with findings and future research directions in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "Detection-level evaluations are designed to identify spelling errors and highlight their locations in the input passages.", "labels": [], "entities": []}, {"text": "Accuracy is a key performance criterion, but accuracy can be affected by the distribution of testing instances.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9933971166610718}, {"text": "accuracy", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.9990119934082031}]}, {"text": "A neutral baseline can be easily achieved by always reporting all testing errors are correct without errors.", "labels": [], "entities": []}, {"text": "According to the test data distribution, the baseline system can achieve an accuracy level of 0.5.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 76, "end_pos": 84, "type": "METRIC", "confidence": 0.9994970560073853}]}, {"text": "Some systems (i.e., CAS, KUAS, and NCYU) achieved promising results exceeding 0.6.", "labels": [], "entities": [{"text": "KUAS", "start_pos": 25, "end_pos": 29, "type": "DATASET", "confidence": 0.7849733829498291}, {"text": "NCYU", "start_pos": 35, "end_pos": 39, "type": "DATASET", "confidence": 0.9612166881561279}]}, {"text": "Each participating team was allowed submit up to three iterative runs based on the same input, and several teams sent different runs aimed at optimizing either recall or precision rates.", "labels": [], "entities": [{"text": "recall", "start_pos": 160, "end_pos": 166, "type": "METRIC", "confidence": 0.9976676106452942}, {"text": "precision", "start_pos": 170, "end_pos": 179, "type": "METRIC", "confidence": 0.9628874659538269}]}, {"text": "We thus used the F1 score to reflect the tradeoff between precision and recall.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.9846515357494354}, {"text": "precision", "start_pos": 58, "end_pos": 67, "type": "METRIC", "confidence": 0.9995062351226807}, {"text": "recall", "start_pos": 72, "end_pos": 78, "type": "METRIC", "confidence": 0.9962331652641296}]}, {"text": "In the testing results, KUAS provided the best error detection results, providing a high F1 score of 0.633.", "labels": [], "entities": [{"text": "KUAS", "start_pos": 24, "end_pos": 28, "type": "DATASET", "confidence": 0.670430600643158}, {"text": "F1 score", "start_pos": 89, "end_pos": 97, "type": "METRIC", "confidence": 0.9888549447059631}]}, {"text": "For correction-level evaluations, the systems need to locate errors in the passages and indicate the corresponding correct characters.", "labels": [], "entities": [{"text": "correction-level evaluations", "start_pos": 4, "end_pos": 32, "type": "TASK", "confidence": 0.8860855400562286}]}, {"text": "The correction accuracy provided by the KUAS submission (0.7081) significantly outperformed the other teams.", "labels": [], "entities": [{"text": "correction", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9916548728942871}, {"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.8408357501029968}, {"text": "KUAS submission", "start_pos": 40, "end_pos": 55, "type": "DATASET", "confidence": 0.8026664853096008}]}, {"text": "However, in terms of correction precision, the spelling checker developed by KUAS and NCYU outperforms the others at 0.8.", "labels": [], "entities": [{"text": "correction", "start_pos": 21, "end_pos": 31, "type": "METRIC", "confidence": 0.9767658710479736}, {"text": "precision", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.7347719669342041}, {"text": "spelling checker", "start_pos": 47, "end_pos": 63, "type": "TASK", "confidence": 0.658152848482132}, {"text": "KUAS", "start_pos": 77, "end_pos": 81, "type": "DATASET", "confidence": 0.8879534602165222}, {"text": "NCYU", "start_pos": 86, "end_pos": 90, "type": "DATASET", "confidence": 0.8046804070472717}]}, {"text": "Most systems were unable to effectively correct spelling errors, with the better systems (CAS, and KUAS) achieving a correction recall rate of slightly above 0.3.", "labels": [], "entities": [{"text": "correction", "start_pos": 117, "end_pos": 127, "type": "METRIC", "confidence": 0.9918213486671448}, {"text": "recall rate", "start_pos": 128, "end_pos": 139, "type": "METRIC", "confidence": 0.8782296180725098}]}, {"text": "The system developed by KUAS provided the highest F1 score of 0.6125 for spelling error correction.", "labels": [], "entities": [{"text": "KUAS", "start_pos": 24, "end_pos": 28, "type": "DATASET", "confidence": 0.8500324487686157}, {"text": "F1 score", "start_pos": 50, "end_pos": 58, "type": "METRIC", "confidence": 0.9898467659950256}, {"text": "spelling error correction", "start_pos": 73, "end_pos": 98, "type": "TASK", "confidence": 0.7163658340771993}]}, {"text": "It is difficult to correct all spelling errors found in the input passages, since some sentences contain multiple errors and only correcting some of them are regarded as a wrong case.", "labels": [], "entities": []}, {"text": "In summary, none of the submitted systems provided superior performance in all metrics, though those submitted by KUAS, NCYU, and CAS provided best overall performance..", "labels": [], "entities": [{"text": "KUAS", "start_pos": 114, "end_pos": 118, "type": "DATASET", "confidence": 0.9378652572631836}, {"text": "NCYU", "start_pos": 120, "end_pos": 124, "type": "DATASET", "confidence": 0.930352509021759}]}, {"text": "Testing results of our Chinese spelling check task.", "labels": [], "entities": [{"text": "Chinese spelling check task", "start_pos": 23, "end_pos": 50, "type": "TASK", "confidence": 0.7868150025606155}]}], "tableCaptions": [{"text": " Table 4. Submission statistics for all participants", "labels": [], "entities": [{"text": "Submission", "start_pos": 10, "end_pos": 20, "type": "TASK", "confidence": 0.9697069525718689}]}, {"text": " Table 6. Testing results of our Chinese spelling check task.", "labels": [], "entities": [{"text": "Chinese spelling check task", "start_pos": 33, "end_pos": 60, "type": "TASK", "confidence": 0.7255508452653885}]}]}