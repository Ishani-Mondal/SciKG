{"title": [{"text": "Lexical Acquisition for Opinion Inference: A Sense-Level Lexicon of Benefactive and Malefactive Events", "labels": [], "entities": [{"text": "Opinion Inference", "start_pos": 24, "end_pos": 41, "type": "TASK", "confidence": 0.7266725897789001}]}], "abstractContent": [{"text": "Opinion inference arises when opinions are expressed toward states and events which positive or negatively affect entities, i.e., benefactive and malefactive events.", "labels": [], "entities": [{"text": "Opinion inference", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9009757936000824}]}, {"text": "This paper addresses creating a lexicon of such events, which would be helpful to infer opinions.", "labels": [], "entities": []}, {"text": "Verbs maybe ambiguous, in that some meanings maybe benefac-tive and others maybe malefactive or neither.", "labels": [], "entities": []}, {"text": "Thus, we use WordNet to create a sense-level lexicon.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 13, "end_pos": 20, "type": "DATASET", "confidence": 0.9526991844177246}]}, {"text": "We begin with seed senses culled from FrameNet and expand the lexicon using WordNet relationships.", "labels": [], "entities": []}, {"text": "The evaluations show that the accuracy of the approach is well above baseline accuracy .", "labels": [], "entities": [{"text": "accuracy", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9996758699417114}, {"text": "accuracy", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.8826483488082886}]}], "introductionContent": [{"text": "Opinions are commonly expressed in many kinds of written and spoken text such as blogs, reviews, new articles, and conversation.", "labels": [], "entities": []}, {"text": "Recently, there have been a surge in reserach in opinion analysis (sentiment analysis) research.", "labels": [], "entities": [{"text": "opinion analysis (sentiment analysis) research", "start_pos": 49, "end_pos": 95, "type": "TASK", "confidence": 0.8695314824581146}]}, {"text": "While most past researches have mainly addressed explicit opinion expressions, there area few researches for implicit opinions expressed via implicatures.", "labels": [], "entities": []}, {"text": "showed how sentiments toward one entity maybe propagated to other entities via opinion implicature rules.", "labels": [], "entities": []}, {"text": "Consider The bill would curb skyrocketing healthcare costs.", "labels": [], "entities": []}, {"text": "Note that curb costs is bad for the object costs since the costs are reduced.", "labels": [], "entities": []}, {"text": "We can reason that the writer is positive toward the event curb since the event is bad for the object healthcare costs which the writer expresses an explicit negative sentiment (skyrocketing).", "labels": [], "entities": []}, {"text": "We can reason from there that the writer is positive toward the bill, since it is the agent of the positive event.", "labels": [], "entities": []}, {"text": "These implicature rules involve events that positively or negatively affect the object.", "labels": [], "entities": []}, {"text": "Such events are called malefactive and benefactive, or, for ease of writing, goodFor (gf ) and badFor (bf ) (hereafter gfbf).", "labels": [], "entities": []}, {"text": "The list of gfbf events and their polarities (gf or bf) are necessary to develop a fully automatic opinion inference system.", "labels": [], "entities": []}, {"text": "On first thought, one might think that we only need lists of gfbf words.", "labels": [], "entities": []}, {"text": "However, it turns out that gfbf terms maybe ambiguous -a single word may have both gf and bf meanings.", "labels": [], "entities": []}, {"text": "Thus, in this work, we take a sense-level approach to acquire gfbf lexicon knowledge, leading us to employ lexical resources with finegrained sense rather than word representations.", "labels": [], "entities": []}, {"text": "For that, we adopt an automatic bootstrapping method which disambiguates gfbf polarity at the sense-level utilizing WordNet, a widely-used lexical resource.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 116, "end_pos": 123, "type": "DATASET", "confidence": 0.9622365832328796}]}, {"text": "Starting from the seed set manually generated from FrameNet, a rich lexicon in which words are organized by semantic frames, we explore how gfbf terms are organized in WordNet via semantic relations and expand the seed set based on those semantic relations.", "labels": [], "entities": []}, {"text": "The expanded lexicon is evaluated in two ways.", "labels": [], "entities": []}, {"text": "First, the lexicon is evaluated against a corpus that has been annotated with gfbf information at the word level.", "labels": [], "entities": []}, {"text": "Second, samples from the expanded lexicon are manually annotated at the sense level, which gives some idea of the prevalence of gfbf lexical ambiguity and provides a basis for senselevel evaluation.", "labels": [], "entities": []}, {"text": "Also, we conduct the agreement study.", "labels": [], "entities": []}, {"text": "The results show that the expanded lexicon covers more than half of the gfbf instances in the gfbf corpus, and the system's accuracy, as measured against the sense-level gold standard, is substantially higher than baseline.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 124, "end_pos": 132, "type": "METRIC", "confidence": 0.9995388984680176}]}, {"text": "In addition, in the agreement study, the annotators achieve good agreement, providing evidence that the annotation task is feasible and that the concept of gfbf gives us a natural coarse-grained grouping of senses.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we use the gfbf annotations in the corpus as a gold standard.", "labels": [], "entities": [{"text": "gfbf annotations in the corpus", "start_pos": 28, "end_pos": 58, "type": "DATASET", "confidence": 0.6762217462062836}]}, {"text": "The annotations in the corpus are at the word level.", "labels": [], "entities": []}, {"text": "To use the annotations as a sense-level gold standard, all the senses of a word marked gf (bf) in the corpus are considered to be gf (bf).", "labels": [], "entities": []}, {"text": "While this is not ideal, this allows us to evaluate the lexicon against the only corpus evidence available.", "labels": [], "entities": []}, {"text": "The 196 words that appear in gf instances in the corpus have a total of 897 senses, and the 286 words that appear in bf instances have a total of 1,154 senses.", "labels": [], "entities": []}, {"text": "Among them, 125 senses are conflicted: a sense of a word marked gf in the corpus could be a member of the same synset as a sense of a word marked bf in the corpus.", "labels": [], "entities": []}, {"text": "For a more reliable gold-standard set, we ignored these conflicted senses.", "labels": [], "entities": []}, {"text": "Thus, the gold-standard set contains 772 gf senses and 1,029 bf senses.", "labels": [], "entities": []}, {"text": "shows the results after five iterations of lexicon expansion.", "labels": [], "entities": []}, {"text": "In total, the gf lexicon contains 4,157 senses and the bf lexicon contains 5,071 senses.", "labels": [], "entities": []}, {"text": "The top half gives the results for the gf lexicon and the bottom half gives the results for the bf lexicon.", "labels": [], "entities": []}, {"text": "In the table, gfOverlap means the overlap between the senses in the lexicon in that row and the gold-standard gf set, while bfOverlap is the overlap between the senses in the lexicon in that row and the gold-standard bf set.", "labels": [], "entities": []}, {"text": "That is, of the 772 senses in the gf gold standard, 449 (58%) are in the gf expanded lexicon while 105 (14%) are in the bf expanded lexicon.", "labels": [], "entities": [{"text": "gf gold standard", "start_pos": 34, "end_pos": 50, "type": "DATASET", "confidence": 0.7297365069389343}]}, {"text": "Accuracy (Acc) for gf is calculated as #gfOver-lap / (#gfOverlap + #bfOverlap) and bf is calculated as #bfOverlap / (#gfOverlap + #bfOverlap).", "labels": [], "entities": [{"text": "Accuracy (Acc)", "start_pos": 0, "end_pos": 14, "type": "METRIC", "confidence": 0.8725908547639847}]}, {"text": "Overall, accuracy is higher for the bf than the gf lexicon.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 9, "end_pos": 17, "type": "METRIC", "confidence": 0.9995980858802795}]}, {"text": "The results in the table are broken down by semantic relation.", "labels": [], "entities": []}, {"text": "Note that the individual counts do not sum to the totals because senses of different words may actually be the same sense in WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 125, "end_pos": 132, "type": "DATASET", "confidence": 0.9776966571807861}]}, {"text": "The results for the bf lexicon are consistently high overall semantic relations.", "labels": [], "entities": []}, {"text": "The results for the gf lexicon are more mixed, but all relations are valuable.", "labels": [], "entities": []}, {"text": "The WordNet Similarity is advantageous because it detects similar senses automatically, so may provide coverage beyond the semantic relations coded in WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 151, "end_pos": 158, "type": "DATASET", "confidence": 0.9559240341186523}]}, {"text": "Overall, the verb group is the most informative relation, as we suspected.", "labels": [], "entities": []}, {"text": "Although the gf-lexicon accuracy for the troponym relation is not high, it has the advantage is that it yields the most number of senses.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.986362874507904}]}, {"text": "Its lower accuracy doesn't support our original hypothesis.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9992768168449402}]}, {"text": "We first thought that verbs lower down in the hierarchy would tend to have the same polarity since they express specific manners characterizing an event.", "labels": [], "entities": []}, {"text": "However, this hypothesis is wrong.", "labels": [], "entities": []}, {"text": "Even though most troponyms have the same polarity, there are many exceptions.", "labels": [], "entities": []}, {"text": "For example, protect#v#1, which means the first sense of the verb protect, has 18 direct troponyms such as cover for#v#1, overprotect#v#2, and soon.", "labels": [], "entities": [{"text": "cover", "start_pos": 107, "end_pos": 112, "type": "METRIC", "confidence": 0.9568910002708435}]}, {"text": "protect#v#1 is a gf event because the meaning is \"shielding from danger\" and most troponyms are also gf events.", "labels": [], "entities": []}, {"text": "However, overprotect#v#2, which is one of troponyms of protect#v#1, is a bf event.", "labels": [], "entities": []}, {"text": "For the hypernym relation, the number of detected senses is not large because many were already detected in previous iterations (in general, there are fewer nodes on each level as hypernym links are traversed).", "labels": [], "entities": []}, {"text": "For a more direct evaluation, two annotators, who are co-authors, independently annotated a sample of senses.", "labels": [], "entities": []}, {"text": "The total number of senses is 151; 64 senses are classified as gf, 56 senses are classified as bf, and 31 senses are not classified.", "labels": [], "entities": []}, {"text": "We included more mixed than pure words to make the results of the study more informative.", "labels": [], "entities": []}, {"text": "Further, we wanted to included non-classified senses as decoys for the annotators.", "labels": [], "entities": []}, {"text": "The annotators only saw the sense entries from WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 47, "end_pos": 54, "type": "DATASET", "confidence": 0.9694981575012207}]}, {"text": "They didn't know whether the system classified a sense as gf or bf or whether it didn't classify it at all.", "labels": [], "entities": []}, {"text": "evaluates the lexicons against the manual annotations, and in comparison to the majority class baseline.", "labels": [], "entities": []}, {"text": "The top half of the table shows results when treating Anno1's annotations as the gold standard, and the bottom half shows the results when treating Anno2's as the gold standard.", "labels": [], "entities": []}, {"text": "Among 151 senses, Anno1 annotated 56 senses (37%) as gf, 51 senses (34%) as bf, and 44 senses (29%) as neutral.", "labels": [], "entities": []}, {"text": "Anno2 annotated 66 senses (44%) as gf, 55 senses (36%) as bf, and 30 (20%) senses as neutral.", "labels": [], "entities": []}, {"text": "The incorrect cases are divided into two sets: incorrect opposite consists of senses that are classified as the opposite polarity by the expansion method (e.g., the sense is classified into gf, but annotator annotates it as bf), and incorrect neutral consists of senses that the expansion method classifies as gf or bf, but the annotator marked it as neutral.", "labels": [], "entities": []}, {"text": "We report the accuracy and the percentage of cases for each incorrect case.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.99970942735672}]}, {"text": "The accuracies substantially improve over baseline for both annotators and for both classes.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9992327690124512}]}, {"text": "In, we breakdown the results into gfbf classes.", "labels": [], "entities": []}, {"text": "The gf accuracy measures the percentage of correct gf senses out of all senses annotated as gf according to the annotations (same as bf accuracy).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 7, "end_pos": 15, "type": "METRIC", "confidence": 0.9544203877449036}, {"text": "accuracy", "start_pos": 136, "end_pos": 144, "type": "METRIC", "confidence": 0.8914193511009216}]}, {"text": "As we can see, accuracy is higher for the bf than the gf.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9997252821922302}]}, {"text": "The conclusion is consistent with what we have discovered in Section 4.3.", "labels": [], "entities": []}, {"text": "By Anno1, 8 words are detected as mixed words, that is, they contain both gf and bf senses.", "labels": [], "entities": [{"text": "Anno1", "start_pos": 3, "end_pos": 8, "type": "DATASET", "confidence": 0.8325652480125427}]}, {"text": "By Anno2, 9 words are mixed words (this set includes the 8 mixed words of Anno1).", "labels": [], "entities": [{"text": "Anno2", "start_pos": 3, "end_pos": 8, "type": "DATASET", "confidence": 0.8853111267089844}, {"text": "Anno1", "start_pos": 74, "end_pos": 79, "type": "DATASET", "confidence": 0.8966158628463745}]}, {"text": "Among the randomly selected 60 words, the proportion of mixed words range from 13.3% to 15%, according to the two annotators.", "labels": [], "entities": []}, {"text": "This shows that gfbf lexical ambiguity does exist.", "labels": [], "entities": []}, {"text": "To measure agreement between the annotators, we calculate two measures: percent agreement and \u03ba (", "labels": [], "entities": [{"text": "percent agreement", "start_pos": 72, "end_pos": 89, "type": "METRIC", "confidence": 0.7089285254478455}]}], "tableCaptions": [{"text": " Table 1: Results after lexicon expansion", "labels": [], "entities": []}, {"text": " Table 2: Results against sense-annotated data", "labels": [], "entities": []}, {"text": " Table 3: Accuracy broken down for gfbf", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9994974136352539}]}]}