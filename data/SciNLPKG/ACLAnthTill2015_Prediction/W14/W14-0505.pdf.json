{"title": [{"text": "An explicit statistical model of learning lexical segmentation using multiple cues", "labels": [], "entities": [{"text": "learning lexical segmentation", "start_pos": 33, "end_pos": 62, "type": "TASK", "confidence": 0.7155752778053284}]}], "abstractContent": [{"text": "This paper presents an unsupervised and incremental model of learning segmenta-tion that combines multiple cues whose use by children and adults were attested by experimental studies.", "labels": [], "entities": []}, {"text": "The cues we exploit in this study are predictability statistics, phonotactics, lexical stress and partial lexical information.", "labels": [], "entities": []}, {"text": "The performance of the model presented in this paper is competitive with the state-of-the-art segmentation models in the literature, while following the child language acquisition more faithfully.", "labels": [], "entities": []}, {"text": "Besides the performance improvements over the similar models in the literature , the cues are combined in an explicit manner, allowing easier interpretation of what the model learns.", "labels": [], "entities": []}], "introductionContent": [{"text": "Segmenting the continuous speech stream into lexical units is one of the challenges we face while listening to other speakers.", "labels": [], "entities": []}, {"text": "For competent language users, probably the biggest aid in identifying the word boundaries is the knowledge of the words.", "labels": [], "entities": [{"text": "identifying the word boundaries", "start_pos": 58, "end_pos": 89, "type": "TASK", "confidence": 0.80846107006073}]}, {"text": "Not surprisingly, the models of adult word recognition depend heavily on a lexicon (see, fora recent review).", "labels": [], "entities": [{"text": "word recognition", "start_pos": 38, "end_pos": 54, "type": "TASK", "confidence": 0.7283281683921814}]}, {"text": "The same can be observed in speech and language technology where all automatic speech recognition systems make use of a comprehensive lexicon.", "labels": [], "entities": []}, {"text": "Even with a comprehensive lexicon and an error-free representation of the acoustic input, the problem is not trivial, since the input is often compatible with multiple segmentations spanning the complete utterance.", "labels": [], "entities": []}, {"text": "The problem, however, is even more difficult fora learner who starts with no lexicon.", "labels": [], "entities": []}, {"text": "Fortunately, the lexicon is not the only aid for segmentation.", "labels": [], "entities": []}, {"text": "Experimental research within last two decades has revealed an array of cues that are used by adults and children for lexical segmentation.", "labels": [], "entities": [{"text": "lexical segmentation", "start_pos": 117, "end_pos": 137, "type": "TASK", "confidence": 0.7042799592018127}]}, {"text": "These cues include, but are not limited to, lexical stress), phonotactics, predictability statistics (, allophonic differences), coarticulation (E. K., and vowel harmony ().", "labels": [], "entities": []}, {"text": "The relative utility or dominance of these cues is a matter of current debate.", "labels": [], "entities": []}, {"text": "However, it seems uncontroversial that none of these cues solves the segmentation problem alone and, when available, they are used in conjunction.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 69, "end_pos": 81, "type": "TASK", "confidence": 0.9622884392738342}]}, {"text": "Along with experimental research on segmentation, a large number of computational models have been proposed in the literature.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 36, "end_pos": 48, "type": "TASK", "confidence": 0.9831798076629639}]}, {"text": "The early studies typically made use of connectionist models (e.g.,.", "labels": [], "entities": []}, {"text": "Of these studies, is particularly interesting for the present study since it incorporates most of the cues used in this study.", "labels": [], "entities": []}, {"text": "Using a simple recurrent network), demonstrated the usefulness of lexical stress, predictability statistics (included implicitly in any SRN model), and utterance boundaries, and showed that combining the cues improves the performance.", "labels": [], "entities": []}, {"text": "The connectionist models have been instrumental in investigating a large number of cognitive phenomena.", "labels": [], "entities": []}, {"text": "However, they have also been subject to the criticism that what a connectionist model learns is rather difficult to interpret.", "labels": [], "entities": []}, {"text": "Furthermore, the performance achieved using connectionist models is far lower than that is expected from humans.", "labels": [], "entities": []}, {"text": "Models that use explicit representations in combination with statistical procedures (e.g.,) avoid both problems: these models perform better, and it is easier to reason about what they learn.", "labels": [], "entities": []}, {"text": "Although these models were also instrumental in our understanding of the problem, they lack at least two aspects of con-nectionist models that fit human processing better.", "labels": [], "entities": []}, {"text": "First, even though we know that human segmentation is incremental and predictive, most of these models process their input either in a batch fashion, or they require the complete utterance to be presented before attempting to segment the input.", "labels": [], "entities": []}, {"text": "Second, it is generally difficult to incorporate arbitrary cues into most of these models.", "labels": [], "entities": []}, {"text": "Models that use explicit representations with incremental models exist (e.g.,), but are rather rare.", "labels": [], "entities": []}, {"text": "Furthermore, the investigation of cues and cue combination in segmentation is also relatively scarce within the recent studies (exceptions include the investigation of various suprevised models by.", "labels": [], "entities": []}, {"text": "The present paper introduces a strictly incremental, unsupervised method for learning segmentation where the learning method and internal representations are explicitly defined.", "labels": [], "entities": [{"text": "learning segmentation", "start_pos": 77, "end_pos": 98, "type": "TASK", "confidence": 0.6980966627597809}]}, {"text": "Crucially, we use a set of cues demonstrated to be used by humans in solving the segmentation problem.", "labels": [], "entities": [{"text": "segmentation problem", "start_pos": 81, "end_pos": 101, "type": "TASK", "confidence": 0.8941526710987091}]}, {"text": "The simulations results that we present are based on the same child-directed speech input used by many other studies in the literature.", "labels": [], "entities": []}, {"text": "The rest of this article is organized as follows: in the next section, we present a method for combining cues.", "labels": [], "entities": []}, {"text": "Section 3 describes the cues used in this study.", "labels": [], "entities": []}, {"text": "The simulations are described and results are presented in Section 4.", "labels": [], "entities": []}, {"text": "A general discussion of the modeling framework and the simulation results are given in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "Two quantitative measures, precision (P), recall (R) and their harmonic mean F 1 -score (F-score, or F, for short), have become the standard evaluation measures for computational simulations.", "labels": [], "entities": [{"text": "precision (P)", "start_pos": 27, "end_pos": 40, "type": "METRIC", "confidence": 0.9403034895658493}, {"text": "recall (R)", "start_pos": 42, "end_pos": 52, "type": "METRIC", "confidence": 0.9596634656190872}, {"text": "harmonic mean F 1 -score (F-score, or F", "start_pos": 63, "end_pos": 102, "type": "METRIC", "confidence": 0.8436654047532515}]}, {"text": "Following recent studies in the literature we present precision recall and F-scores for boundaries (BP, BR, BF), word tokens (WP, WR, WF) and word types or lexicon (LP, LR, LF).", "labels": [], "entities": [{"text": "precision", "start_pos": 54, "end_pos": 63, "type": "METRIC", "confidence": 0.9972085356712341}, {"text": "recall", "start_pos": 64, "end_pos": 70, "type": "METRIC", "confidence": 0.7953628897666931}, {"text": "F-scores", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.9664008617401123}]}, {"text": "Besides precision and recall, we also present two error measures, oversegmentation (E o ) and undersegmentation (E u ) errors, defined as E o = FP/(FP + TN) and Eu = FN/(FN + TP), where TP, FP, TN and FN are true positives, false positives, true negatives, and false negatives respectively.", "labels": [], "entities": [{"text": "precision", "start_pos": 8, "end_pos": 17, "type": "METRIC", "confidence": 0.9993269443511963}, {"text": "recall", "start_pos": 22, "end_pos": 28, "type": "METRIC", "confidence": 0.9991044402122498}, {"text": "oversegmentation (E o )", "start_pos": 66, "end_pos": 89, "type": "METRIC", "confidence": 0.8921295285224915}, {"text": "undersegmentation (E u ) errors", "start_pos": 94, "end_pos": 125, "type": "METRIC", "confidence": 0.7488582332928976}, {"text": "FP", "start_pos": 144, "end_pos": 146, "type": "METRIC", "confidence": 0.9783138632774353}]}, {"text": "In plain words, E o is the number of the false boundaries inserted by the model divided by the total number of word internal positions in the corpus.", "labels": [], "entities": [{"text": "E o", "start_pos": 16, "end_pos": 19, "type": "METRIC", "confidence": 0.9308522939682007}]}, {"text": "Similarly, Eu is the ratio of boundaries missed to the total number of boundaries.", "labels": [], "entities": [{"text": "Eu", "start_pos": 11, "end_pos": 13, "type": "METRIC", "confidence": 0.9964132905006409}]}, {"text": "Although these error measures are related to precision and recall, they provide different, and sometimes better, insights into the model's behavior.", "labels": [], "entities": [{"text": "precision", "start_pos": 45, "end_pos": 54, "type": "METRIC", "confidence": 0.9993065595626831}, {"text": "recall", "start_pos": 59, "end_pos": 65, "type": "METRIC", "confidence": 0.9982907176017761}]}, {"text": "This section reports results of a set of simulations using the modeling framework described so far.", "labels": [], "entities": []}, {"text": "All experiments are run on the BR corpus.", "labels": [], "entities": [{"text": "BR corpus", "start_pos": 31, "end_pos": 40, "type": "DATASET", "confidence": 0.9530695974826813}]}, {"text": "For all the results reported below, each cue is represented by a set indicators as described in Section 3, multiple indicators for each phoneme n-gram of length one and three are used for left (l) and right (r) contexts, for all measures that are calculated over phoneme n-grams surrounding the potential boundary.", "labels": [], "entities": []}, {"text": "The use of lexical information and lexical stress as standalone strategies are similar to the 'lexicon-building' strategy.", "labels": [], "entities": []}, {"text": "The learner inserts complete utterances to the lexicon when the strategy cannot segment the utterance.", "labels": [], "entities": []}, {"text": "As the learner starts to learn (from the edges of the sequences in the lexicon) what the edges of words look like, it uses this information to segment later utterances in the input.", "labels": [], "entities": []}, {"text": "We first report the performance results of individual cues, namely, predictability (P), utterance boundaries (U), lexical information (W) and lexical stress (S) in.", "labels": [], "entities": []}, {"text": "Using the predictability cue alone leads to a segmentation performance lower than but close to the state-of-the-art reference model LM.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 46, "end_pos": 58, "type": "TASK", "confidence": 0.9572651982307434}]}, {"text": "Although these results are not directly comparable to the earlier studies in the literature, the performance scores presented in are the best scores presented to date for models using the predictability cue alone.", "labels": [], "entities": []}, {"text": "Graphs presented by indicates about 50%-60% WP and WR and 20%-30% LP for his baseline model utilizing mutual information on the BR corpus.", "labels": [], "entities": [{"text": "WP", "start_pos": 44, "end_pos": 46, "type": "METRIC", "confidence": 0.9778477549552917}, {"text": "WR", "start_pos": 51, "end_pos": 53, "type": "METRIC", "confidence": 0.9656816720962524}, {"text": "LP", "start_pos": 66, "end_pos": 68, "type": "METRIC", "confidence": 0.9908148646354675}, {"text": "BR corpus", "start_pos": 128, "end_pos": 137, "type": "DATASET", "confidence": 0.8503227233886719}]}, {"text": "report 76% BP, and 75% BR on George Orwell's 1984.", "labels": [], "entities": [{"text": "BP", "start_pos": 11, "end_pos": 13, "type": "METRIC", "confidence": 0.9579704403877258}, {"text": "BR", "start_pos": 23, "end_pos": 25, "type": "METRIC", "confidence": 0.999103307723999}, {"text": "Orwell's 1984.", "start_pos": 36, "end_pos": 50, "type": "DATASET", "confidence": 0.8588961213827133}]}, {"text": "report 37% WP and 40% WR with an SRN using phonotactics and utterance boundary cues on another child-directed speech corpus.", "labels": [], "entities": [{"text": "WP", "start_pos": 11, "end_pos": 13, "type": "METRIC", "confidence": 0.9765402674674988}, {"text": "WR", "start_pos": 22, "end_pos": 24, "type": "METRIC", "confidence": 0.9975540041923523}]}, {"text": "The model that learns from the utterance boundaries seems to perform the best.", "labels": [], "entities": []}, {"text": "The results are comparable, and in some cases better than the LM.", "labels": [], "entities": []}, {"text": "Furthermore, the overall scores are also higher than the scores reported by, where the boundary, word and lexical F-scores were 82.9%, 70.7% and 36.6%, respectively.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 114, "end_pos": 122, "type": "METRIC", "confidence": 0.8069917559623718}]}, {"text": "Although it is somewhat behind both predictability and utterance boundary cues, the lexical information alone certainly performs better than random.", "labels": [], "entities": []}, {"text": "The lower performance of this model in comparison to 'U' suggests that, at least in this setting, phonotactics learned from word tokens found at the utterance edges leads to a better performance compared to the phonotactics learned from the word types in the learner's lexicon.", "labels": [], "entities": []}, {"text": "The experiment that takes only the stress cue into account yields the worst overall results.", "labels": [], "entities": []}, {"text": "It seems, when the cue indicates a boundary, it is extremely precise.", "labels": [], "entities": [{"text": "precise", "start_pos": 61, "end_pos": 68, "type": "METRIC", "confidence": 0.9482574462890625}]}, {"text": "However, it is also very conservative.", "labels": [], "entities": []}, {"text": "This seems to be due to the fact that the model learns to segment at weak-strong transitions, which is expected to be precise.", "labels": [], "entities": []}, {"text": "However, since majority of the stress transitions are strongstrong, this covers rather a small portion of the boundaries.: Results of combination of strategies based on four cues: starting with predictability and utterance boundaries (PU), addition of lexicon (PUW) and lexical stress (PUWS).", "labels": [], "entities": []}, {"text": "The rows labeled LM and RM are scores of reference models repeated for ease of comparison.", "labels": [], "entities": [{"text": "RM", "start_pos": 24, "end_pos": 26, "type": "METRIC", "confidence": 0.9316567182540894}]}, {"text": "presents combination of predictability and utterance boundaries, followed by lexical information and stress.", "labels": [], "entities": []}, {"text": "Here all indicators are combined in a flat, non-hierarchical manner.", "labels": [], "entities": []}, {"text": "The combination of predictability and utterance boundaries results in higher F-scores, and it results in more balanced under-and over-segmentation errors.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.9973838925361633}]}, {"text": "The addition of the lexical information provide a small but consistent improvement.", "labels": [], "entities": []}, {"text": "However, adding stress information seems to have an adverse effect.", "labels": [], "entities": []}, {"text": "Despite the increased boundary and word precision, all other performance scores go down substantially when we add the stress cue.", "labels": [], "entities": [{"text": "precision", "start_pos": 40, "end_pos": 49, "type": "METRIC", "confidence": 0.8735491037368774}]}, {"text": "The scores in are obtained over the complete corpus.", "labels": [], "entities": []}, {"text": "As noted in Section 4.3, these scores do not reflect the 'learned' state of the models.", "labels": [], "entities": []}, {"text": "Furthermore, we are interested in the progress of a learner as more input is provided.", "labels": [], "entities": []}, {"text": "To demonstrate both, E o and Eu for all combined models are plotted in for each 500 utterances.", "labels": [], "entities": [{"text": "E o", "start_pos": 21, "end_pos": 24, "type": "METRIC", "confidence": 0.9677335619926453}]}, {"text": "An interesting observation that can be made in these graphs is that the models without the stress cue make fewer undersegmentation errors, with the cost of slightly higher oversegmentation.", "labels": [], "entities": []}, {"text": "However, the strategy that combines all cues keeps, but measured for the last 290-utterances (last block in an incremental experiment with 500-utterance increments).", "labels": [], "entities": []}, {"text": "oversegmentation errors low throughout the learning process, and towards the end, it makes fewer undersegmentation errors as well.", "labels": [], "entities": []}, {"text": "This suggests that the model combining all cues, including the stress, maybe doing better as it collects more evidence.", "labels": [], "entities": []}, {"text": "To demonstrate this further, presents the same results presented in, calculated on the last block of an experiment where performance scores were calculated after every 500 input utterances.", "labels": [], "entities": []}, {"text": "Besides demonstrating the increase in performance scores when calculated at later stages of learning, the differences between tables 3 and 4 show clearly that despite the fact that it has a detrimental affect when scores are calculated over the complete corpus, the stress cue has a positive effect at the end of the learning process.", "labels": [], "entities": []}, {"text": "This suggests that the combined model using stress cue learns slower and makes more mistakes at the beginning.", "labels": [], "entities": []}, {"text": "However as evidence accumulates, it starts to be useful, and increases the overall performance of the combined model.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Performance scores of the reference mod- els LM and RM in comparison with some of the  earlier scores reported in the literature. If there  were multiple models reported in a study, the re- sult with the highest lexicon F-score is presented.  All scores are obtained on the BR corpus.", "labels": [], "entities": [{"text": "RM", "start_pos": 62, "end_pos": 64, "type": "METRIC", "confidence": 0.8864010572433472}, {"text": "F-score", "start_pos": 230, "end_pos": 237, "type": "METRIC", "confidence": 0.9720579981803894}, {"text": "BR corpus", "start_pos": 284, "end_pos": 293, "type": "DATASET", "confidence": 0.9055770039558411}]}, {"text": " Table 2: Results of simulations using individual  cues: predictability (P), utterance boundaries (U),  lexicon (W) and lexical stress (S). The rows la- beled LM and RM are scores of reference models  repeated for ease of comparison.", "labels": [], "entities": [{"text": "RM", "start_pos": 166, "end_pos": 168, "type": "METRIC", "confidence": 0.9358142614364624}]}, {"text": " Table 3: Results of combination of strategies  based on four cues: starting with predictability  and utterance boundaries (PU), addition of lexi- con (PUW) and lexical stress (PUWS). The rows  labeled LM and RM are scores of reference mod- els repeated for ease of comparison.", "labels": [], "entities": [{"text": "addition of lexi- con (PUW)", "start_pos": 129, "end_pos": 156, "type": "METRIC", "confidence": 0.8069585934281349}]}, {"text": " Table 4: The same results presented in", "labels": [], "entities": []}]}