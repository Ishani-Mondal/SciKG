{"title": [], "abstractContent": [{"text": "In this paper, we introduce a novel graph based technique for topic based multi-document summarization.", "labels": [], "entities": [{"text": "multi-document summarization", "start_pos": 74, "end_pos": 102, "type": "TASK", "confidence": 0.5477118790149689}]}, {"text": "We transform documents into a bipartite graph where one set of nodes represents entities and the other set of nodes represents sentences.", "labels": [], "entities": []}, {"text": "To obtain the summary we apply a ranking technique to the bipartite graph which is followed by an optimization step.", "labels": [], "entities": []}, {"text": "We test the performance of our method on several DUC datasets and compare it to the state-of-the-art.", "labels": [], "entities": [{"text": "DUC datasets", "start_pos": 49, "end_pos": 61, "type": "DATASET", "confidence": 0.9455564320087433}]}], "introductionContent": [{"text": "Topic-based multi-document summarization aims to create a single summary from a set of given documents while considering the topic of interest.", "labels": [], "entities": [{"text": "Topic-based multi-document summarization", "start_pos": 0, "end_pos": 40, "type": "TASK", "confidence": 0.5476442277431488}]}, {"text": "The input documents can be created by querying an information retrieval or search engine fora particular topic and retaining highly ranked documents, or by clustering documents of a large collection and then using each cluster as a set of input documents (.", "labels": [], "entities": []}, {"text": "Here, each cluster of the set of documents contains a representative topic.", "labels": [], "entities": []}, {"text": "A summary extracted from a set of input documents must be related to the topic of that set.", "labels": [], "entities": []}, {"text": "If textual units (or sentences) extracted from different documents convey the same information, then those units are called redundant.", "labels": [], "entities": []}, {"text": "Ideally, the multi-document summary should be nonredundant.", "labels": [], "entities": []}, {"text": "Hence each textual unit in a summary should convey unique information.", "labels": [], "entities": []}, {"text": "Still, all extracted textual units should be related to the topic.", "labels": [], "entities": []}, {"text": "They should also makeup a coherent summary.", "labels": [], "entities": []}, {"text": "When building summaries from multiple documents belonging to different sets, a system should attempt to optimize these three basic properties: 1.", "labels": [], "entities": []}, {"text": "Relevance: A summary should contain only those textual units which are relevant to the topic and provide useful information.", "labels": [], "entities": [{"text": "Relevance", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9233874082565308}]}, {"text": "2. Non-redundancy: A summary should not contain the same information twice.", "labels": [], "entities": []}, {"text": "3. Readability: A summary should have good readability (syntactically well formed, no dangling pronouns, coherent, . .", "labels": [], "entities": []}, {"text": "). Generally, multi-document summarization systems differ from each other on the basis of document representation, sentence selection method or on the requirements for the output summary.", "labels": [], "entities": [{"text": "sentence selection", "start_pos": 115, "end_pos": 133, "type": "TASK", "confidence": 0.6859331578016281}]}, {"text": "Popular methods for document representation include graph-based representations (e.g.) and TextRank (Mihalcea and Tarau, 2004)) and tf-idf vector-based representations).", "labels": [], "entities": [{"text": "document representation", "start_pos": 20, "end_pos": 43, "type": "TASK", "confidence": 0.7436603009700775}]}, {"text": "These document representations act as input for the next phase and provide information about the importance of individual sentences.", "labels": [], "entities": []}, {"text": "Sentence selection is the crucial phase of the summarizer where sentence redundancy must be handled in an efficient way.", "labels": [], "entities": [{"text": "Sentence selection", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9540954530239105}, {"text": "summarizer", "start_pos": 47, "end_pos": 57, "type": "TASK", "confidence": 0.97050940990448}, {"text": "sentence redundancy", "start_pos": 64, "end_pos": 83, "type": "TASK", "confidence": 0.7009331583976746}]}, {"text": "A widely used technique is the greedy approach introduced by and.", "labels": [], "entities": []}, {"text": "They compute a relevance score for all sentences with regard to the topic, start by extracting the most relevant sentence, and then iteratively extract further sentences which are relevant to the topic and at the same time most dissimilar to already extracted sentences.", "labels": [], "entities": [{"text": "relevance score", "start_pos": 15, "end_pos": 30, "type": "METRIC", "confidence": 0.9493699073791504}]}, {"text": "Later more fundamental optimization methods have been widely used in multi-document summarization, e.g. Integer Linear Programming (ILP)).", "labels": [], "entities": [{"text": "multi-document summarization", "start_pos": 69, "end_pos": 97, "type": "TASK", "confidence": 0.5719094276428223}]}, {"text": "Unlike most other approaches () has also taken into account the readability of the final summary.", "labels": [], "entities": []}, {"text": "In this work, we introduce an extractive topic based multi-document summarization system which represents documents graphically and optimizes the importance of sentences and nonredundancy.", "labels": [], "entities": []}, {"text": "The importance of sentences is obtained by means of applying the Hubs and Authorities ranking algorithm) on the unweighted bipartite graph whereas redundancy in the final summary is dealt with entities in a graph.", "labels": [], "entities": []}, {"text": "In Section 2 we introduce the state-of-the-art in topic based multi-document summarizaton.", "labels": [], "entities": []}, {"text": "Section 3 provides a detailed description of our approach.", "labels": [], "entities": []}, {"text": "Experiments are described in Section 4 where we also briefly describe the datasets used and the results.", "labels": [], "entities": []}, {"text": "Section 5 discusses the results of our approach, and in Section 6 we finally give conclusions.", "labels": [], "entities": []}], "datasetContent": [{"text": "We perform experiments on various DUC datasets to compare the results with state-of-the-art systems.", "labels": [], "entities": [{"text": "DUC datasets", "start_pos": 34, "end_pos": 46, "type": "DATASET", "confidence": 0.9236544370651245}]}, {"text": "Datasets used for our experiments are DUC2005), DUC2006) and DUC2007 1 . Each dataset contains group of related documents.", "labels": [], "entities": [{"text": "DUC2005", "start_pos": 38, "end_pos": 45, "type": "DATASET", "confidence": 0.9102889895439148}, {"text": "DUC2006", "start_pos": 48, "end_pos": 55, "type": "DATASET", "confidence": 0.8882392644882202}, {"text": "DUC2007 1", "start_pos": 61, "end_pos": 70, "type": "DATASET", "confidence": 0.7102320492267609}]}, {"text": "Each group of documents contains one related topic or a query consisting of a few sentences.", "labels": [], "entities": []}, {"text": "In DUC, the final summary should respond to the corresponding topic.", "labels": [], "entities": []}, {"text": "Also, the summary cannot exceed the maximum allowed length.", "labels": [], "entities": [{"text": "length", "start_pos": 52, "end_pos": 58, "type": "METRIC", "confidence": 0.9008429646492004}]}, {"text": "For instance, in DUC2005, 250 words are allowed in the final summary.", "labels": [], "entities": [{"text": "DUC2005", "start_pos": 17, "end_pos": 24, "type": "DATASET", "confidence": 0.9192835688591003}]}, {"text": "Every document cluster has corresponding human summaries for evaluating system summaries on the basis of ROUGE scores).", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 105, "end_pos": 110, "type": "METRIC", "confidence": 0.9871252179145813}]}, {"text": "The sources of DUC datasets are Los Angeles Times, Financial Times of London, Associated Press, New York Times and Xinhua news agency.", "labels": [], "entities": [{"text": "DUC datasets", "start_pos": 15, "end_pos": 27, "type": "DATASET", "confidence": 0.8119679689407349}, {"text": "Financial Times of London", "start_pos": 51, "end_pos": 76, "type": "DATASET", "confidence": 0.9461175948381424}, {"text": "Xinhua news agency", "start_pos": 115, "end_pos": 133, "type": "DATASET", "confidence": 0.8336278796195984}]}, {"text": "We employ ROUGE SU4 and ROUGE 2 as evaluation metrics.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.8936586976051331}, {"text": "ROUGE 2", "start_pos": 24, "end_pos": 31, "type": "METRIC", "confidence": 0.9186683595180511}]}, {"text": "ROUGE returns recall, precision and F-score of a system, but usually only recall is used in for evaluating automatic summarization systems, because the final summary does not contain many words.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9496597647666931}, {"text": "recall", "start_pos": 14, "end_pos": 20, "type": "METRIC", "confidence": 0.9990938901901245}, {"text": "precision", "start_pos": 22, "end_pos": 31, "type": "METRIC", "confidence": 0.9992964267730713}, {"text": "F-score", "start_pos": 36, "end_pos": 43, "type": "METRIC", "confidence": 0.9986786246299744}, {"text": "recall", "start_pos": 74, "end_pos": 80, "type": "METRIC", "confidence": 0.9975427389144897}, {"text": "summarization", "start_pos": 117, "end_pos": 130, "type": "TASK", "confidence": 0.913964033126831}]}, {"text": "Hence, if the recall is high then the summarization system is working well.", "labels": [], "entities": [{"text": "recall", "start_pos": 14, "end_pos": 20, "type": "METRIC", "confidence": 0.9996767044067383}, {"text": "summarization", "start_pos": 38, "end_pos": 51, "type": "TASK", "confidence": 0.9657604694366455}]}, {"text": "Document statistics is provided in.", "labels": [], "entities": []}, {"text": "We use raw documents from the various DUC datasets as input for our system.", "labels": [], "entities": [{"text": "DUC datasets", "start_pos": 38, "end_pos": 50, "type": "DATASET", "confidence": 0.9626882672309875}]}, {"text": "We remove nonalphabetical characters from the documents.", "labels": [], "entities": []}, {"text": "Then we obtain a clean sentence split by means of the Stanford parser ( so that the sentences are compatible with the next steps.", "labels": [], "entities": []}, {"text": "We use the Brown coherence toolkit) to convert the documents into the entity grid representation from which the bipartite graph is constructed (.", "labels": [], "entities": []}, {"text": "Entities in the graph correspond to head nouns of noun phrase mentioned in the sentences.", "labels": [], "entities": []}, {"text": "The ranking algorithm from Section 3.2 is applied to this graph and returns the importance score of a sentence as required by the objective function given in Equation 3.", "labels": [], "entities": [{"text": "importance score", "start_pos": 80, "end_pos": 96, "type": "METRIC", "confidence": 0.9432602822780609}]}, {"text": "Next optimization using ILP is performed as described in Section 3.3.", "labels": [], "entities": []}, {"text": "We use GUROBI Optimizer 2 for performing ILP.", "labels": [], "entities": [{"text": "GUROBI", "start_pos": 7, "end_pos": 13, "type": "METRIC", "confidence": 0.8222401142120361}]}, {"text": "ILP returns a binary value, i.e., if a sentence should be included in the summary it returns 1, if not it returns 0.", "labels": [], "entities": []}, {"text": "We set \u03bb 1 = 0.7 and \u03bb 2 = 0.3 for all datasets.", "labels": [], "entities": []}, {"text": "We did not choose the optimal values, but rather opted for ones which favor importance over non-redundancy.", "labels": [], "entities": []}, {"text": "We did not observe significant differences between different \u03bb values as long as \u03bb 1 > \u03bb 2 (see).", "labels": [], "entities": []}, {"text": "The sentences in the output summary are ordered according to their ranks.", "labels": [], "entities": []}, {"text": "If the output summary contains pronouns, we perform pronoun resolution in the source documents using the coreference resolution system by.", "labels": [], "entities": [{"text": "pronoun resolution", "start_pos": 52, "end_pos": 70, "type": "TASK", "confidence": 0.7303740978240967}, {"text": "coreference resolution", "start_pos": 105, "end_pos": 127, "type": "TASK", "confidence": 0.8441435396671295}]}, {"text": "If pronoun and antecedent occur in the same sentence, we leave the pronoun.", "labels": [], "entities": []}, {"text": "If the antecedent occurs in an earlier sentence, we replace the pronoun in the summary by the first element of the coreference chain the pronoun belongs to.", "labels": [], "entities": []}, {"text": "Except for setting \u03bb 1 and \u03bb 2 on DUC 2005, our approach is unsupervised, as there is no traning data required.", "labels": [], "entities": [{"text": "DUC 2005", "start_pos": 34, "end_pos": 42, "type": "DATASET", "confidence": 0.9860859513282776}]}, {"text": "The recall (ROUGE) scores on different datasets are shown in. shows that our system would have performed very well in the DUC 2005 and DUC 2006 competitions with ranks in the top 3 and well in the DUC 2007 competition.", "labels": [], "entities": [{"text": "recall (ROUGE)", "start_pos": 4, "end_pos": 18, "type": "METRIC", "confidence": 0.9125380665063858}, {"text": "DUC 2005 and DUC 2006 competitions", "start_pos": 122, "end_pos": 156, "type": "DATASET", "confidence": 0.890525629123052}, {"text": "DUC 2007 competition", "start_pos": 197, "end_pos": 217, "type": "DATASET", "confidence": 0.9077008763949076}]}, {"text": "Since the competitions date awhile back, we compare in addition to the current state-of-art in multi-document summarization.", "labels": [], "entities": []}, {"text": "To our knowledge Galanis et al.", "labels": [], "entities": []}, {"text": "The best ROUGE-1 score reported to date has been reported by with 0.456.", "labels": [], "entities": [{"text": "ROUGE-1 score", "start_pos": 9, "end_pos": 22, "type": "METRIC", "confidence": 0.9768542647361755}]}, {"text": "The difference between this score and our score of 0.448 is rather small.", "labels": [], "entities": []}], "tableCaptions": []}