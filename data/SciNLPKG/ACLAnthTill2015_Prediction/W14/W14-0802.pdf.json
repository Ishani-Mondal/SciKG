{"title": [{"text": "A Supervised Model for Extraction of Multiword Expressions Based on Statistical Context Features", "labels": [], "entities": [{"text": "Extraction of Multiword Expressions", "start_pos": 23, "end_pos": 58, "type": "TASK", "confidence": 0.8261741399765015}]}], "abstractContent": [{"text": "We present a method for extracting Multi-word Expressions (MWEs) based on the immediate context they occur in, using a supervised model.", "labels": [], "entities": [{"text": "extracting Multi-word Expressions (MWEs)", "start_pos": 24, "end_pos": 64, "type": "TASK", "confidence": 0.833947479724884}]}, {"text": "We show some of these contextual features can be very discrim-inant and combining them with MWE-specific features results in a relatively accurate extraction.", "labels": [], "entities": []}, {"text": "We define context as a sequential structure and not a bag of words, consequently, it becomes much more informative about MWEs.", "labels": [], "entities": []}], "introductionContent": [{"text": "Multiword Expressions (MWEs) are an important research topic in the area of Natural Language Processing (NLP).", "labels": [], "entities": [{"text": "Multiword Expressions (MWEs)", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8629655718803406}, {"text": "Natural Language Processing (NLP)", "start_pos": 76, "end_pos": 109, "type": "TASK", "confidence": 0.7162863413492838}]}, {"text": "Efficient and effective extraction and interpretation of MWEs is crucial inmost NLP tasks.", "labels": [], "entities": [{"text": "extraction and interpretation of MWEs", "start_pos": 24, "end_pos": 61, "type": "TASK", "confidence": 0.7158282279968262}]}, {"text": "They exist in many types of text and cause major problems in all kinds of natural language processing applications ().", "labels": [], "entities": []}, {"text": "However, identifying and lexicalizing these important but hard to identify structures need to be improved inmost major computational lexicons).", "labels": [], "entities": []}, {"text": "Jackendoff (1997) estimates that the number of MWEs is equal to the number of single words in a speaker's lexicon, while believe that the number is even greater than this.", "labels": [], "entities": []}, {"text": "Moreover, as a language evolves, the number of MWEs consistently increases.", "labels": [], "entities": []}, {"text": "MWEs area powerful way of extending languages' lexicons.", "labels": [], "entities": []}, {"text": "Their role in language evolution is so important that according to, \"It is highly doubtful that any language would evolve without MWEs of some description\".", "labels": [], "entities": [{"text": "language evolution", "start_pos": 14, "end_pos": 32, "type": "TASK", "confidence": 0.7117402255535126}]}, {"text": "The efficient identification and extraction of MWEs can positively influence many other NLP tasks, e.g., part of speech tagging, parsing, syntactic disambiguation, semantic tagging, machine translation, and natural language generation.", "labels": [], "entities": [{"text": "identification and extraction of MWEs", "start_pos": 14, "end_pos": 51, "type": "TASK", "confidence": 0.7714335441589355}, {"text": "speech tagging", "start_pos": 113, "end_pos": 127, "type": "TASK", "confidence": 0.6848292052745819}, {"text": "parsing", "start_pos": 129, "end_pos": 136, "type": "TASK", "confidence": 0.9544354677200317}, {"text": "syntactic disambiguation", "start_pos": 138, "end_pos": 162, "type": "TASK", "confidence": 0.7095840573310852}, {"text": "semantic tagging", "start_pos": 164, "end_pos": 180, "type": "TASK", "confidence": 0.7126671075820923}, {"text": "machine translation", "start_pos": 182, "end_pos": 201, "type": "TASK", "confidence": 0.8079145848751068}, {"text": "natural language generation", "start_pos": 207, "end_pos": 234, "type": "TASK", "confidence": 0.6432288785775503}]}, {"text": "MWEs also have important applications outside NLP.", "labels": [], "entities": []}, {"text": "For instance in document indexing, information retrieval, and cross lingual information retrieval.", "labels": [], "entities": [{"text": "document indexing", "start_pos": 16, "end_pos": 33, "type": "TASK", "confidence": 0.7317445278167725}, {"text": "information retrieval", "start_pos": 35, "end_pos": 56, "type": "TASK", "confidence": 0.8380417227745056}, {"text": "cross lingual information retrieval", "start_pos": 62, "end_pos": 97, "type": "TASK", "confidence": 0.6472975313663483}]}, {"text": "In this paper we present a method of extracting MWEs which is relatively different from most of the state of the art approaches.", "labels": [], "entities": [{"text": "extracting MWEs", "start_pos": 37, "end_pos": 52, "type": "TASK", "confidence": 0.8618631958961487}]}, {"text": "We characterize MWEs based on the statistical properties of the immediate context they occur in.", "labels": [], "entities": []}, {"text": "For each possible MWE candidate we define a set of contextual features (e.g., prefixes, suffixes, etc.).", "labels": [], "entities": []}, {"text": "The contextual feature vector is then enriched with a few MWE-specific features such as the frequency of its components, type frequency of the candidate MWE, and the association between these two (which is learned by a supervised model).", "labels": [], "entities": []}, {"text": "Subsequently the MWEhood of the extracted candidates is predicted based on this feature representation, using a Support Vector Machine (SVM).", "labels": [], "entities": [{"text": "MWEhood", "start_pos": 17, "end_pos": 24, "type": "TASK", "confidence": 0.8053861856460571}]}, {"text": "The system reaches a relatively high accuracy of predicting MWEs on unseen data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.9995039701461792}, {"text": "predicting MWEs", "start_pos": 49, "end_pos": 64, "type": "TASK", "confidence": 0.9123019874095917}]}], "datasetContent": [{"text": "A set of \u2248 10K negative and positive English MWE examples were annotated.", "labels": [], "entities": []}, {"text": "This set does not particularly belong in any specific genre, as the examples were chosen randomly from across a general-purpose corpus.", "labels": [], "entities": []}, {"text": "This set comprises an equal number of positive and negative annotations.", "labels": [], "entities": []}, {"text": "Part of it was annotated manually at UNDL foundation, and part of it was acquired from the manually examined MWE lexicon presented in.", "labels": [], "entities": [{"text": "UNDL foundation", "start_pos": 37, "end_pos": 52, "type": "DATASET", "confidence": 0.9838590025901794}, {"text": "MWE lexicon", "start_pos": 109, "end_pos": 120, "type": "DATASET", "confidence": 0.8932513296604156}]}, {"text": "The set of positive and negative annotated n-grams is detailed in  This set was divided into 1/3 test and 2/3 training data, which were selected randomly but were evenly distributed with respect to positive and negative examples.", "labels": [], "entities": []}, {"text": "The test set remains completely unseen to the model during the learning phase.", "labels": [], "entities": []}, {"text": "We then train a linear SVM: Where h(y) is a discriminant hyperplane, w is the weight vector, and y is a set of MWE examples, where each example is defined as: y j = x 1 , ..., x 11 . illustrates the trained model's predictions on a set of randomly selected test examples.", "labels": [], "entities": [{"text": "SVM", "start_pos": 23, "end_pos": 26, "type": "TASK", "confidence": 0.9339972734451294}]}, {"text": "The overall performance of the model is shown in the form of a precision-recall curve in: Sample SVM's output on unseen data.", "labels": [], "entities": [{"text": "precision-recall", "start_pos": 63, "end_pos": 79, "type": "METRIC", "confidence": 0.994130551815033}]}, {"text": "A t-test ranks the significance of the defined features in classifying n-grams into MWE, and non-MWE classes, as illustrated in.", "labels": [], "entities": []}, {"text": "The most The significance of x 9 is due to the fact that in the training set majority of MWEs are bigrams.", "labels": [], "entities": []}, {"text": "Therefore, by the SVM, being a bigram is considered as a substantial feature of MWEs.", "labels": [], "entities": [{"text": "SVM", "start_pos": 18, "end_pos": 21, "type": "DATASET", "confidence": 0.9137578010559082}]}, {"text": "Nevertheless since the number of negative and positive examples which are bigrams are approximately the same, the bias toward x 9 in discriminating MWEs from non-MWE balances out.", "labels": [], "entities": []}, {"text": "However its association with other features which is implicitly learned still has an impact on discriminating these two classes.", "labels": [], "entities": []}, {"text": "x 7 and x 8 are the next two important features, as we expected.", "labels": [], "entities": []}, {"text": "These two are the features whose magnitude suggests the presence or lack of contexts such as (the..of ).", "labels": [], "entities": []}, {"text": "The class separability of MWE, and non-MWE (\u22121) examples can be seen in, where the bidimentional projection of the examples of two classes is visualized.", "labels": [], "entities": []}, {"text": "A star plot of a sample of 50 manually annotated examples is shown in.", "labels": [], "entities": []}, {"text": "In many cases, but not always, non-MWEs can be discriminated from MWEs, in this eleven dimensional visualization.", "labels": [], "entities": []}, {"text": "Same pattern was observed in the visualization of 500 examples (which would be hard to demonstrate in the present paper's scale).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4. The bias  toward bigrams is due to the fact that the majority  of manually verified MWEs that could be obtained  are bigrams.", "labels": [], "entities": []}, {"text": " Table 5: Performance of the SVM which learns the  MWEhood based on contextual and specific fea- tures (x 1 \u2212 x 11 )", "labels": [], "entities": []}, {"text": " Table 6: Sample SVM's output on unseen data.", "labels": [], "entities": [{"text": "Sample SVM", "start_pos": 10, "end_pos": 20, "type": "TASK", "confidence": 0.7934065759181976}]}]}