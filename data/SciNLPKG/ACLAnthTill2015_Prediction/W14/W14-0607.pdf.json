{"title": [{"text": "A Hybrid Disambiguation Measure for Inaccurate Cultural Heritage Data", "labels": [], "entities": []}], "abstractContent": [{"text": "Cultural heritage data is always associated with inaccurate information and different types of ambiguities.", "labels": [], "entities": []}, {"text": "For instance, names of persons, occupations or places mentioned in historical documents are not standardized and contain numerous variations.", "labels": [], "entities": []}, {"text": "This article examines in detail various existing similarity functions and proposes a hybrid technique for the following task: among the list of possible names, occupations and places extracted from historical documents, identify those that are variations of the same person name, occupation and place respectively.", "labels": [], "entities": []}, {"text": "The performance of our method is evaluated on three manually constructed datasets and one public dataset in terms of precision, recall and F-measure.", "labels": [], "entities": [{"text": "precision", "start_pos": 117, "end_pos": 126, "type": "METRIC", "confidence": 0.9996722936630249}, {"text": "recall", "start_pos": 128, "end_pos": 134, "type": "METRIC", "confidence": 0.9992326498031616}, {"text": "F-measure", "start_pos": 139, "end_pos": 148, "type": "METRIC", "confidence": 0.9973616003990173}]}, {"text": "The results demonstrate that the hybrid technique outper-forms current methods and allows to significantly improve the quality of cultural heritage data.", "labels": [], "entities": []}], "introductionContent": [{"text": "Inaccurate information and lack of common identifiers are problems encountered when combining information from heterogeneous sources.", "labels": [], "entities": []}, {"text": "There area number of reasons that can cause inaccurate information such as spelling variations, abbreviations, translation from one language into another and modifying long names into shorter ones.", "labels": [], "entities": []}, {"text": "Inaccurate information often occurs in many domains, for example, during information extraction from the Web or when attributing a publication to its proper author.", "labels": [], "entities": []}, {"text": "Inaccurate information is very typical in cultural heritage data as well.", "labels": [], "entities": []}, {"text": "In historical documents areal person could be mentioned many times, for instance in civil certificates such as birth, marriage and death certificates or in property transfer records and tax declarations.", "labels": [], "entities": []}, {"text": "The name of the same person, his occupation and the place in such documents varies a lot.", "labels": [], "entities": []}, {"text": "When working with such information, researchers have to identify which person references mentioned in different historical documents belong to the same person entity.", "labels": [], "entities": []}, {"text": "This problem has been referred to in literature in many different ways but is best known as entity resolution (ER), record linkage or duplicate detection (.", "labels": [], "entities": [{"text": "entity resolution (ER)", "start_pos": 92, "end_pos": 114, "type": "TASK", "confidence": 0.7794279813766479}, {"text": "duplicate detection", "start_pos": 134, "end_pos": 153, "type": "TASK", "confidence": 0.6829234659671783}]}, {"text": "The process of ER in historical documents is always accompanied by inaccurate information as well.", "labels": [], "entities": [{"text": "ER", "start_pos": 15, "end_pos": 17, "type": "TASK", "confidence": 0.8602861166000366}]}, {"text": "As an example, there are more than 100 variants of the first name Jan, such as Johan, Johannes, Janis, Jean or the profession musician in historical documents can be spelled as musikant, muzikant or even muzikant bij de tiende afd.", "labels": [], "entities": []}, {"text": "The latter means the musician in the 10th department.", "labels": [], "entities": []}, {"text": "The past few decades have seen a large research interest in the problem of inaccurate information.", "labels": [], "entities": []}, {"text": "As a result, a large number of methods for comparing string has been developed.", "labels": [], "entities": []}, {"text": "These standard methods are called string similarity functions.", "labels": [], "entities": []}, {"text": "Some of those well known techniques are character-based, token-based or based on phonetic functions, for instance Levenshtein Edit distance, Jaro Winkler distance, Monge Elkan distance, Smith Waterman distance,.", "labels": [], "entities": [{"text": "Levenshtein Edit distance", "start_pos": 114, "end_pos": 139, "type": "METRIC", "confidence": 0.6176564991474152}]}, {"text": "Each of the mentioned similarity functions perform optimally fora particular dataset domain.", "labels": [], "entities": []}, {"text": "For example, the phonetic function Soundex works great for encoding names by sound as it pronounced in English, but nevertheless sometimes it is also used to encode names in other European languages.", "labels": [], "entities": []}, {"text": "However, only little work has been done in studying combinations of similarity functions, and in their simultaneous use for achieving more reliable results.", "labels": [], "entities": []}, {"text": "in his work computes names similarity with affine gaps to train the Support Vector Machines classifier.", "labels": [], "entities": []}, {"text": "designed a learnable Levenshtein distance for solving the string similarity problem.", "labels": [], "entities": [{"text": "string similarity problem", "start_pos": 58, "end_pos": 83, "type": "TASK", "confidence": 0.7330130934715271}]}, {"text": "learned weights of different types of string transformations.", "labels": [], "entities": []}, {"text": "In this paper we explore various traditional string similarity functions for solving data ambiguities and also design a supervised hybrid technique.", "labels": [], "entities": []}, {"text": "We carryout our experiments on three manually constructed datasets: Dutch names, occupations and places, and also on one publicly available dataset of restaurants.", "labels": [], "entities": []}, {"text": "The clarified function, that will allow us to recognize difficult ambiguities in textual fields, later will be incorporated into the overall ER process fora large historical database.", "labels": [], "entities": []}, {"text": "The main contributions of this paper is a practical study of existing techniques and the design and the extensive analysis of a hybrid technique that allow us to achieve a significant improvement in results.", "labels": [], "entities": []}, {"text": "The remainder of this paper is structured as follows.", "labels": [], "entities": []}, {"text": "In Section 2 we begin by presenting typical ambiguities in real-life cultural heritage data.", "labels": [], "entities": []}, {"text": "in Section 3 we give an overview of standard string similarity functions.", "labels": [], "entities": []}, {"text": "We describe the general hybrid approach in Section 4.", "labels": [], "entities": []}, {"text": "In Section 5 we describe the prediction models that we use in the hybrid approach.", "labels": [], "entities": []}, {"text": "In Section 6 we provide details about carrying out the experiments.", "labels": [], "entities": []}, {"text": "In Section 7 we present an evaluation of the results.", "labels": [], "entities": []}, {"text": "Section 8 offers a discussion about applying the designed approach to real-world data.", "labels": [], "entities": []}, {"text": "Concluding remarks are given in Section 9.", "labels": [], "entities": [{"text": "Section 9", "start_pos": 32, "end_pos": 41, "type": "DATASET", "confidence": 0.8220109045505524}]}], "datasetContent": [{"text": "Our experiments are conducted on four datasets.", "labels": [], "entities": []}, {"text": "Three datasets, namely names, occupations and places variations are manually constructed from Cultural Heritage Data.", "labels": [], "entities": [{"text": "Cultural Heritage Data", "start_pos": 94, "end_pos": 116, "type": "DATASET", "confidence": 0.834122896194458}]}, {"text": "They are discussed in detail in Section 2.", "labels": [], "entities": []}, {"text": "The fourth dataset is a public dataset called Restaurant.", "labels": [], "entities": []}, {"text": "It is a standard benchmark dataset which is widely used in data matching studies.", "labels": [], "entities": [{"text": "data matching", "start_pos": 59, "end_pos": 72, "type": "TASK", "confidence": 0.7840946018695831}]}, {"text": "It contains information about 864 restaurant names and addresses where 112 records are duplicated.", "labels": [], "entities": []}, {"text": "It was obtained by integrating records from two sources: Fodors and Zagats guidebooks.", "labels": [], "entities": [{"text": "Fodors and Zagats guidebooks", "start_pos": 57, "end_pos": 85, "type": "DATASET", "confidence": 0.8181710541248322}]}, {"text": "The Restaurant dataset was taken from the SecondString toolkit 4 . We carried out our experiments in accordance to the algorithm described in Section 4.", "labels": [], "entities": [{"text": "Restaurant dataset", "start_pos": 4, "end_pos": 22, "type": "DATASET", "confidence": 0.8995977640151978}, {"text": "SecondString toolkit", "start_pos": 42, "end_pos": 62, "type": "DATASET", "confidence": 0.9714000523090363}]}, {"text": "At first, we convert each dataset into a dataset of variant pairs using random combinations of records.", "labels": [], "entities": []}, {"text": "Then for each pair of records we compute string similarity functions.", "labels": [], "entities": []}, {"text": "We randomly divided all available data into two subsets, namely training and test sets.", "labels": [], "entities": []}, {"text": "To construct the set of string similarities we use 70% of the training set to learn RF importance rate and the other 30% of the training set to validate results under stepwise selection procedure as it was described in the algorithm in Section 4.", "labels": [], "entities": [{"text": "RF importance rate", "start_pos": 84, "end_pos": 102, "type": "METRIC", "confidence": 0.9482722679773966}]}, {"text": "The resulting set of selected string similarities for each dataset is shown in.", "labels": [], "entities": []}, {"text": "After constructing the set of string similarities we learn the classifier on the complete training set and then evaluate it on the test set.", "labels": [], "entities": []}, {"text": "In order to assess the performance of our results, we apply a 10-fold cross-validation method.", "labels": [], "entities": []}, {"text": "We randomly partition the available dataset into 10 equal size subsets.", "labels": [], "entities": []}, {"text": "Then one subset was chosen as the validation data for testing the classifier, and the remaining subsets are used for training the classifier.", "labels": [], "entities": []}, {"text": "Then the crossvalidation process is repeated 10 times, with each of the 10 subsets used exactly once as the validation dataset.", "labels": [], "entities": []}, {"text": "In order to evaluate the performance of standard string similarity functions and the applied hybrid approach, we compute the sets of True Positives (TP), False Positives (FP) and False Negatives (FN) as the correctly identified, incorrectly identified and incorrectly rejected matches, respectively.", "labels": [], "entities": [{"text": "False Positives (FP) and False Negatives (FN)", "start_pos": 154, "end_pos": 199, "type": "METRIC", "confidence": 0.8380723676898263}]}, {"text": "demonstrates the performance of standard and hybrid approaches on four examined datasets.", "labels": [], "entities": []}, {"text": "The logistic regression as well as SVM classifiers which are used in the hybrid approach on each of the dataset outperform standard string similarities.", "labels": [], "entities": []}, {"text": "The improvement in results is significant, especially it is clearly seen on the dataset of occupations.", "labels": [], "entities": []}, {"text": "For a more detailed analysis, shows the evaluation of results in terms of F-measure and the threshold value for all continuous methods.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 74, "end_pos": 83, "type": "METRIC", "confidence": 0.99128258228302}]}, {"text": "Moreover,  In addition to analyzing the hybrid approach, in this section we investigate in more detail functions which demonstrate not the typical behavior on the precision and recall plots.", "labels": [], "entities": [{"text": "precision", "start_pos": 163, "end_pos": 172, "type": "METRIC", "confidence": 0.9983667731285095}, {"text": "recall", "start_pos": 177, "end_pos": 183, "type": "METRIC", "confidence": 0.9227961301803589}]}, {"text": "For instance, on for SW, GH and ME similarities the simultaneous growth of the precision is accomplished by the growth in the recall on the interval (0, 0.3).", "labels": [], "entities": [{"text": "ME", "start_pos": 32, "end_pos": 34, "type": "METRIC", "confidence": 0.9838795065879822}, {"text": "precision", "start_pos": 79, "end_pos": 88, "type": "METRIC", "confidence": 0.998849630355835}, {"text": "recall", "start_pos": 126, "end_pos": 132, "type": "METRIC", "confidence": 0.9975206255912781}]}, {"text": "The same situation occurs for SW and GH similarities on datasets of occupations and places.", "labels": [], "entities": [{"text": "SW", "start_pos": 30, "end_pos": 32, "type": "TASK", "confidence": 0.9473788738250732}, {"text": "GH similarities", "start_pos": 37, "end_pos": 52, "type": "METRIC", "confidence": 0.8945344686508179}]}, {"text": "In for SW similarity on the dataset on names for three levels of the threshold we show its performance indicators, namely TP,: Evaluation measures for SW similarity for 3 levels of the threshold the same name.", "labels": [], "entities": [{"text": "SW similarity", "start_pos": 7, "end_pos": 20, "type": "TASK", "confidence": 0.695147305727005}, {"text": "TP", "start_pos": 122, "end_pos": 124, "type": "METRIC", "confidence": 0.9791474938392639}]}, {"text": "Therefore, to make it absolutely clear, in we gave an example of such pairs of names that are included into 99 FP and cause the simultaneous grows of precision and recall.", "labels": [], "entities": [{"text": "99 FP", "start_pos": 108, "end_pos": 113, "type": "DATASET", "confidence": 0.5994129478931427}, {"text": "precision", "start_pos": 150, "end_pos": 159, "type": "METRIC", "confidence": 0.9994317889213562}, {"text": "recall", "start_pos": 164, "end_pos": 170, "type": "METRIC", "confidence": 0.99803227186203}]}], "tableCaptions": [{"text": " Table 1: An example of a name variation dataset", "labels": [], "entities": [{"text": "name variation", "start_pos": 26, "end_pos": 40, "type": "TASK", "confidence": 0.7869665622711182}]}, {"text": " Table 3: An example of character and token-based  similarities", "labels": [], "entities": []}, {"text": " Table 4: An example of term pair-variants", "labels": [], "entities": []}, {"text": " Table 7: Evaluation measures for SW similarity  for 3 levels of the threshold", "labels": [], "entities": [{"text": "SW", "start_pos": 34, "end_pos": 36, "type": "TASK", "confidence": 0.9235895276069641}, {"text": "similarity", "start_pos": 37, "end_pos": 47, "type": "METRIC", "confidence": 0.48468348383903503}]}, {"text": " Table 8: Example of FP pairs of names according  to the maximum value of SW, GH and ME func- tions", "labels": [], "entities": [{"text": "ME func- tions", "start_pos": 85, "end_pos": 99, "type": "METRIC", "confidence": 0.8680183738470078}]}]}