{"title": [{"text": "Unsupervised Adaptation for Statistical Machine Translation", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 28, "end_pos": 59, "type": "TASK", "confidence": 0.8669144113858541}]}], "abstractContent": [{"text": "In this work, we tackle the problem of language and translation models domain-adaptation without explicit bilingual in-domain training data.", "labels": [], "entities": []}, {"text": "In such a scenario, the only information about the domain can be induced from the source-language test corpus.", "labels": [], "entities": []}, {"text": "We explore unsupervised adaptation, where the source-language test corpus is combined with the corresponding hypotheses generated by the translation system to perform adaptation.", "labels": [], "entities": []}, {"text": "We compare unsupervised adaptation to supervised and pseudo supervised adaptation.", "labels": [], "entities": []}, {"text": "Our results show that the choice of the adaptation (target) set is crucial for successful application of adaptation methods.", "labels": [], "entities": []}, {"text": "Evaluation is conducted over the German-to-English WMT newswire translation task.", "labels": [], "entities": [{"text": "WMT newswire translation", "start_pos": 51, "end_pos": 75, "type": "TASK", "confidence": 0.6987206141153971}]}, {"text": "The experiments show that the unsupervised adaptation method generates the best translation quality as well as generalizes well to unseen test sets.", "labels": [], "entities": []}], "introductionContent": [{"text": "Over the last few years, large amounts of statistical machine translation (SMT) monolingual and bilingual corpora were collected.", "labels": [], "entities": [{"text": "statistical machine translation (SMT) monolingual", "start_pos": 42, "end_pos": 91, "type": "TASK", "confidence": 0.797998675278255}]}, {"text": "Early years focused on structured data translation such as newswire.", "labels": [], "entities": [{"text": "structured data translation", "start_pos": 23, "end_pos": 50, "type": "TASK", "confidence": 0.7682637770970663}]}, {"text": "Nowadays, due to the relative success of SMT, new domains of translation are being explored, such as lecture and patent translation (.", "labels": [], "entities": [{"text": "SMT", "start_pos": 41, "end_pos": 44, "type": "TASK", "confidence": 0.9929463267326355}, {"text": "patent translation", "start_pos": 113, "end_pos": 131, "type": "TASK", "confidence": 0.6872531026601791}]}, {"text": "The task of domain adaptation tackles the problem of utilizing existing resources mainly drawn from one domain (e.g. parliamentary discussion) to maximize the performance on the target (test) domain (e.g. newswire).", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 12, "end_pos": 29, "type": "TASK", "confidence": 0.7344887852668762}]}, {"text": "To be able to perform adaptation, a target set representing the test domain is used to manipulate the general-domain models.", "labels": [], "entities": [{"text": "adaptation", "start_pos": 22, "end_pos": 32, "type": "TASK", "confidence": 0.9611319899559021}]}, {"text": "Previous work on SMT adaptation focused on the scenario where (small) bilingual in-domain or pseudo in-domain training data are available.", "labels": [], "entities": [{"text": "SMT adaptation", "start_pos": 17, "end_pos": 31, "type": "TASK", "confidence": 0.9970456063747406}]}, {"text": "Furthermore, small attention was given to the choice of the target set for adaptation.", "labels": [], "entities": []}, {"text": "In this work, we explore the problem of adaptation where no explicit bilingual data from the test domain is available for training, and the only resource encapsulating information about the domain is the source-language test corpus itself.", "labels": [], "entities": []}, {"text": "We explore how to utilize the source-language test corpus for adapting the language model (LM) and the translation model (TM).", "labels": [], "entities": []}, {"text": "A combination of source and automatically translated target of the test set is compared to using the source side only for TM adaptation.", "labels": [], "entities": [{"text": "TM adaptation", "start_pos": 122, "end_pos": 135, "type": "TASK", "confidence": 0.9651716649532318}]}, {"text": "Furthermore, we compare using the test set to using in-domain data and a pseudo in-domain data (e.g. news-commentary as opposed to newswire).", "labels": [], "entities": []}, {"text": "Experiments are done on the WMT 2013 German-to-English newswire translation task.", "labels": [], "entities": [{"text": "WMT 2013 German-to-English newswire translation task", "start_pos": 28, "end_pos": 80, "type": "TASK", "confidence": 0.7896283268928528}]}, {"text": "Our best adaptation method shows competitive results to the best submissions of the evaluation.", "labels": [], "entities": []}, {"text": "This paper is structured as follows.", "labels": [], "entities": []}, {"text": "We review related work in Section 2 and introduce the basic adaptation methods in Section 3.", "labels": [], "entities": []}, {"text": "The experimental setup is described in Section 4, results are discussed in Section 5 and we conclude in Section 6.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: German-English bilingual training and  test data statistics: the number of sentence pairs  (Sent), German (De) and English (En) words are  given.", "labels": [], "entities": []}, {"text": " Table 2: German-English monolingual corpora  statistics: the number of tokens is given in millions", "labels": [], "entities": []}, {"text": " Table 3: Optimal size portion and resulting per- plexities, across adaptation sets (NC, REF and  HYP) and monolingual LM training corpora.", "labels": [], "entities": []}, {"text": " Table 4: German-English LM filtering results using different adaptation sets. The LM perplexity over  the blind test set nestest13, as well as BLEU and TER percentages are presented.", "labels": [], "entities": [{"text": "LM filtering", "start_pos": 25, "end_pos": 37, "type": "TASK", "confidence": 0.7529333829879761}, {"text": "BLEU", "start_pos": 144, "end_pos": 148, "type": "METRIC", "confidence": 0.9990354776382446}, {"text": "TER percentages", "start_pos": 153, "end_pos": 168, "type": "METRIC", "confidence": 0.9746793806552887}]}, {"text": " Table 5: German-English TM filtering and weighting results using different adaptation sets. The results  are given in BLEU and TER percentages. Significance is measured over the full system (first row).", "labels": [], "entities": [{"text": "TM filtering", "start_pos": 25, "end_pos": 37, "type": "TASK", "confidence": 0.904015451669693}, {"text": "BLEU", "start_pos": 119, "end_pos": 123, "type": "METRIC", "confidence": 0.9992130994796753}, {"text": "TER", "start_pos": 128, "end_pos": 131, "type": "METRIC", "confidence": 0.99031001329422}]}]}