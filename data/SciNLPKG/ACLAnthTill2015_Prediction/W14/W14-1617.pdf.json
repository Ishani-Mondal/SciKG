{"title": [{"text": "Hallucinating Phrase Translations for Low Resource MT", "labels": [], "entities": [{"text": "Hallucinating Phrase Translations", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.629375676314036}, {"text": "MT", "start_pos": 51, "end_pos": 53, "type": "TASK", "confidence": 0.7372721433639526}]}], "abstractContent": [{"text": "We demonstrate that \"hallucinating\" phrasal translations can significantly improve the quality of machine translation in low resource conditions.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 98, "end_pos": 117, "type": "TASK", "confidence": 0.7422473132610321}]}, {"text": "Our hallucinated phrase tables consist of entries composed from multiple unigram translations drawn from the baseline phrase table and from translations that are induced from mono-lingual corpora.", "labels": [], "entities": []}, {"text": "The hallucinated phrase table is very noisy.", "labels": [], "entities": []}, {"text": "Its translations are low precision but high recall.", "labels": [], "entities": [{"text": "translations", "start_pos": 4, "end_pos": 16, "type": "TASK", "confidence": 0.9589763283729553}, {"text": "precision", "start_pos": 25, "end_pos": 34, "type": "METRIC", "confidence": 0.9991724491119385}, {"text": "recall", "start_pos": 44, "end_pos": 50, "type": "METRIC", "confidence": 0.9991042017936707}]}, {"text": "We counter this by introducing 30 new feature functions (including a variety of monolingually-estimated features) and by aggressively pruning the phrase table.", "labels": [], "entities": []}, {"text": "Our analysis evaluates the intrinsic quality of our hallucinated phrase pairs as well as their impact in end-to-end Spanish-English and Hindi-English MT.", "labels": [], "entities": [{"text": "Hindi-English MT", "start_pos": 136, "end_pos": 152, "type": "TASK", "confidence": 0.46257151663303375}]}], "introductionContent": [{"text": "In this work, we augment the translation model fora low-resource phrase-based SMT system by automatically expanding its phrase table.", "labels": [], "entities": [{"text": "SMT", "start_pos": 78, "end_pos": 81, "type": "TASK", "confidence": 0.8955031633377075}]}, {"text": "We \"hallucinate\" new phrase table entries by composing the unigram translations from the baseline system's phrase table and translations learned from comparable monolingual corpora.", "labels": [], "entities": []}, {"text": "The composition process yields a very large number of new phrase pair translations, which are high recall but low precision.", "labels": [], "entities": [{"text": "phrase pair translations", "start_pos": 58, "end_pos": 82, "type": "TASK", "confidence": 0.6691295901934305}, {"text": "recall", "start_pos": 99, "end_pos": 105, "type": "METRIC", "confidence": 0.9989050626754761}, {"text": "precision", "start_pos": 114, "end_pos": 123, "type": "METRIC", "confidence": 0.9952552318572998}]}, {"text": "We filter the phrase table using anew set of feature functions estimated from monolingual corpora.", "labels": [], "entities": []}, {"text": "We evaluate the hallucinated phrase pairs intrinsically as well as in end-to-end machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 81, "end_pos": 100, "type": "TASK", "confidence": 0.744478315114975}]}, {"text": "The augmented phrase table provides more coverage than the original phrase table, while being high quality enough to improve translation performance.", "labels": [], "entities": [{"text": "translation", "start_pos": 125, "end_pos": 136, "type": "TASK", "confidence": 0.9620687961578369}]}, {"text": "We propose a four-part approach to hallucinating and using new phrase pair translations: 1.", "labels": [], "entities": []}, {"text": "Learn potential translations for out-ofvocabulary (OOV) words from comparable monolingual corpora 2.", "labels": [], "entities": []}, {"text": "\"Hallucinate\" a large, noisy set of phrase translations by composing unigram translations from the baseline model and from the monolingually-induced bilingual dictionary 3.", "labels": [], "entities": []}, {"text": "Use comparable monolingual corpora to score, rank, and prune the huge number of hallucinated translations 4. Augment the baseline phrase table with hallucinated translations and new feature functions estimated from monolingual corpora We define an algorithm for generating loosely compositional phrase pairs, which we use to hallucinate new translations.", "labels": [], "entities": []}, {"text": "In oracle experiments, we show that such loosely compositional phrase pairs contribute substantially to the performance of end-to-end SMT, beyond that of component unigram translations.", "labels": [], "entities": [{"text": "SMT", "start_pos": 134, "end_pos": 137, "type": "TASK", "confidence": 0.9586317539215088}]}, {"text": "In our non-oracle experiments, we show that adding a judiciously pruned set of automatically hallucinated phrase pairs to an endto-end baseline SMT model results in a significant improvement in translation quality for both Spanish-English and Hindi-English.", "labels": [], "entities": [{"text": "SMT", "start_pos": 144, "end_pos": 147, "type": "TASK", "confidence": 0.9309220910072327}]}], "datasetContent": [{"text": "In all of our experiments, we assume that we have access to only a small parallel corpus.", "labels": [], "entities": []}, {"text": "For our Spanish experiments, we randomly sample 2, 000 sentence pairs (about 57, 000 Spanish words) from the Spanish-English Europarl v5 parallel corpus ().", "labels": [], "entities": [{"text": "Europarl v5 parallel corpus", "start_pos": 125, "end_pos": 152, "type": "DATASET", "confidence": 0.8957296758890152}]}, {"text": "For Hindi, we use the parallel corpora released by.", "labels": [], "entities": []}, {"text": "Again, we randomly sample 2, 000 sentence pairs from the training corpus (about 39, 000 Hindi words).", "labels": [], "entities": []}, {"text": "We expect that this amount of parallel text could be compiled fora single text domain and any pair of modern languages.", "labels": [], "entities": []}, {"text": "Additionally, we use approximately 2, 500 and 1, 000 single-reference parallel sentences each for tuning and testing our Spanish and Hindi models, respectively.", "labels": [], "entities": []}, {"text": "Spanish tuning and test sets are newswire articles taken from the 2010 WMT shared task.", "labels": [], "entities": [{"text": "WMT shared task", "start_pos": 71, "end_pos": 86, "type": "DATASET", "confidence": 0.8261183500289917}]}, {"text": "We use the Hindi development and testing splits released by.", "labels": [], "entities": []}, {"text": "Before moving to the results of our proposed approach for composing phrase translations, we present an oracle experiment to answer these research questions: Would a low resource translation model benefit from composing its unigram translations into phrases?", "labels": [], "entities": [{"text": "composing phrase translations", "start_pos": 58, "end_pos": 87, "type": "TASK", "confidence": 0.7273982663949331}]}, {"text": "Would this be further improved by adding unigram translations that are learned from monolingual texts?", "labels": [], "entities": []}, {"text": "We answer these questions by starting with our lowresource Spanish-English and Hindi-English baselines and augmenting each with (1) phrasal translations composed from baseline model unigram translations, and (2) phrasal translations composed of a mix of baseline model unigram translations and the monolingually-induced unigrams.", "labels": [], "entities": []}, {"text": "illustrates how our hallucinated phrasetable entries can result in improved translation quality for Spanish to English translation.", "labels": [], "entities": [{"text": "Spanish to English translation", "start_pos": 100, "end_pos": 130, "type": "TASK", "confidence": 0.6165379583835602}]}, {"text": "Since the baseline model is trained from such a small amount of data, it typically translates individual words instead of phrases.", "labels": [], "entities": []}, {"text": "In our augmented system, we compose a translation of was no one from habia nadie, since habia translates as was in the baseline model, nadie translates as one, and no is a stop word.", "labels": [], "entities": []}, {"text": "We are able to monolingually-induce translations for the OOVs centros and electorales before composing the phrase translation polling stations for centros electorales.", "labels": [], "entities": []}, {"text": "In our oracle experiments, composed translations are only added to the phrase table if they are contained in the reference.", "labels": [], "entities": []}, {"text": "This eliminates the huge number of noisy translations that our compositional algorithm generates.", "labels": [], "entities": []}, {"text": "We augment baseline models with translations for the same sets of source language phrases described in Section 4.", "labels": [], "entities": []}, {"text": "We use GIZA++ to word align our tuning and test sets and use a standard phrase pair extraction heuristic 6 to identify oracle phrase translations.", "labels": [], "entities": [{"text": "identify oracle phrase translations", "start_pos": 110, "end_pos": 145, "type": "TASK", "confidence": 0.669643297791481}]}, {"text": "We add oracle translations to each baseline model without bilingually estimated translation scores 7 because such scores are not available for our automatically induced translations.", "labels": [], "entities": []}, {"text": "Instead, we score the oracle phrase pairs using the 30 new phrase table features described in Section 3.3.", "labels": [], "entities": []}, {"text": "shows the results of our oracle experiments.", "labels": [], "entities": []}, {"text": "Augmenting the baselines with the subset of oracle translations which are composed given the unigram translations in the baseline models themselves (i.e. in the small training sets) yields not having dependent on the centros electorales . no hab\u00eda nadie en los centros electorales, which translates correctly as there was nobody at the voting offices.", "labels": [], "entities": []}, {"text": "The full oracle is augmented with translations composed from the seed model as well as induced unigram translations.", "labels": [], "entities": []}, {"text": "The phrase was no one is composeable from hab\u00eda nadie given the seed model.", "labels": [], "entities": []}, {"text": "In contrast, the phrase polling stations is composeable from centros electorales using induced translations.", "labels": [], "entities": []}, {"text": "For each translation, the phrase segmentations used by the decoder are highlighted.", "labels": [], "entities": []}, {"text": "a BLEU score improvement of about 1.4 points for Spanish and about 0.6 for Hindi.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 2, "end_pos": 12, "type": "METRIC", "confidence": 0.955575168132782}]}, {"text": "This finding itself is noteworthy, and we investigated the reason for it.", "labels": [], "entities": []}, {"text": "A representative example of a compositional oracle translation that was added to the Spanish model is para evitarlos, which translates as to prevent them.", "labels": [], "entities": [{"text": "compositional oracle translation", "start_pos": 30, "end_pos": 62, "type": "TASK", "confidence": 0.6933358311653137}]}, {"text": "In the training corpus, para translates far more frequently as for than to.", "labels": [], "entities": []}, {"text": "Thus, it is useful for the translation model to know that, in the context of evitarlos, para should translate as to and not for.", "labels": [], "entities": []}, {"text": "Additionally, evitarlos was observed only translating as the unigram prevent.", "labels": [], "entities": []}, {"text": "The small model fails to align the adjoined clitic los with its translation them.", "labels": [], "entities": []}, {"text": "However, our loose definition of compositionality allows the English stop word them to appear anywhere in the target translation.", "labels": [], "entities": []}, {"text": "In the first result, composeable translations do not include those that contain new, induced word translations.", "labels": [], "entities": []}, {"text": "Using the baseline model and induced unigram translations to compose phrase translations results in a 2 and 1.6 BLEU point gain for Spanish and Hindi, respectively.", "labels": [], "entities": [{"text": "BLEU point", "start_pos": 112, "end_pos": 122, "type": "METRIC", "confidence": 0.9698812365531921}]}, {"text": "The second column of shows the results of augmenting the baseline models with the same oracle phrase pairs as well as the new features estimated overall phrase pairs.", "labels": [], "entities": []}, {"text": "Although the features do not improve the performance of the baseline models, this diverse set of scores improves performance dramatically when new, oracle phrase pairs are added.", "labels": [], "entities": []}, {"text": "Adding all oracle translations and the new feature set results in a total gain of about 2.6 BLEU points for Spanish and about 1.9 for Hindi.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 92, "end_pos": 96, "type": "METRIC", "confidence": 0.9994843006134033}]}, {"text": "These gains are the maximum that we could hope to achieve by augmenting models with our hallucinated translations and new feature set.", "labels": [], "entities": []}, {"text": "shows examples of top ranked translations for several Spanish words.", "labels": [], "entities": []}, {"text": "Although performance is generally quite good, we do observe some instances of false cognates, for example the top ranked translation for aburridos, which translates correctly as bored, is burritos.", "labels": [], "entities": []}, {"text": "Using automatic word alignments as a reference, we find that 44% of Spanish tuning set unigrams have a correct translation in their top-10 ranked lists and 62% in the top-100.", "labels": [], "entities": []}, {"text": "For Hindi, 31% of tuning set unigrams have a correct translation in their top-10 ranked lists and 43% in the top-100.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Motivating Experiment: BLEU results using the", "labels": [], "entities": [{"text": "Motivating", "start_pos": 10, "end_pos": 20, "type": "TASK", "confidence": 0.9657469987869263}, {"text": "BLEU", "start_pos": 33, "end_pos": 37, "type": "METRIC", "confidence": 0.9959127306938171}]}, {"text": " Table 2: Top five induced translations for several source words. Correct translations are bolded. aceite translates as oil.", "labels": [], "entities": []}, {"text": " Table 3: Top three compositional translations for several", "labels": [], "entities": [{"text": "compositional translations", "start_pos": 20, "end_pos": 46, "type": "TASK", "confidence": 0.8379531800746918}]}, {"text": " Table 4: Experimental results. First, the baseline models", "labels": [], "entities": []}]}