{"title": [{"text": "The CoNLL-2014 Shared Task on Grammatical Error Correction", "labels": [], "entities": []}], "abstractContent": [{"text": "The CoNLL-2014 shared task was devoted to grammatical error correction of all error types.", "labels": [], "entities": [{"text": "grammatical error correction", "start_pos": 42, "end_pos": 70, "type": "TASK", "confidence": 0.5902078747749329}]}, {"text": "In this paper, we give the task definition , present the data sets, and describe the evaluation metric and scorer used in the shared task.", "labels": [], "entities": []}, {"text": "We also give an overview of the various approaches adopted by the participating teams, and present the evaluation results.", "labels": [], "entities": []}, {"text": "Compared to the CoNLL-2013 shared task, we have introduced the following changes in CoNLL-2014: (1) A participating system is expected to detect and correct grammatical errors of all types, instead of just the five error types in CoNLL-2013; (2) The evaluation metric was changed from F 1 to F 0.5 , to emphasize precision over recall; and (3) We have two human annotators who independently annotated the test essays, compared to just one human annotator in CoNLL-2013.", "labels": [], "entities": [{"text": "CoNLL-2014", "start_pos": 84, "end_pos": 94, "type": "DATASET", "confidence": 0.9238191843032837}, {"text": "CoNLL-2013", "start_pos": 230, "end_pos": 240, "type": "DATASET", "confidence": 0.925014853477478}, {"text": "F 1", "start_pos": 285, "end_pos": 288, "type": "METRIC", "confidence": 0.9708835184574127}, {"text": "precision", "start_pos": 313, "end_pos": 322, "type": "METRIC", "confidence": 0.9939654469490051}, {"text": "recall", "start_pos": 328, "end_pos": 334, "type": "METRIC", "confidence": 0.976093590259552}, {"text": "CoNLL-2013", "start_pos": 458, "end_pos": 468, "type": "DATASET", "confidence": 0.9572027325630188}]}], "introductionContent": [{"text": "Grammatical error correction is the shared task of the Eighteenth Conference on Computational Natural Language Learning in 2014.", "labels": [], "entities": [{"text": "Grammatical error correction", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8551906744639078}, {"text": "Eighteenth Conference on Computational Natural Language Learning in 2014", "start_pos": 55, "end_pos": 127, "type": "TASK", "confidence": 0.610724151134491}]}, {"text": "In this task, given an English essay written by a learner of English as a second language, the goal is to detect and correct the grammatical errors of all error types present in the essay, and return the corrected essay.", "labels": [], "entities": []}, {"text": "This task has attracted much recent research interest, with two shared tasks Helping Our Own (HOO) organized in 2011 and 2012, and a CoNLL shared task on grammatical error correction organized in . In contrast to previous CoNLL shared tasks which focused on particular subtasks of natural language processing, such as named entity recognition, semantic role labeling, dependency parsing, or coreference resolution, grammatical error correction aims at building a complete end-to-end application.", "labels": [], "entities": [{"text": "grammatical error correction", "start_pos": 154, "end_pos": 182, "type": "TASK", "confidence": 0.5810504655043284}, {"text": "named entity recognition", "start_pos": 318, "end_pos": 342, "type": "TASK", "confidence": 0.6244970460732778}, {"text": "semantic role labeling", "start_pos": 344, "end_pos": 366, "type": "TASK", "confidence": 0.6634267270565033}, {"text": "dependency parsing", "start_pos": 368, "end_pos": 386, "type": "TASK", "confidence": 0.8059934675693512}, {"text": "coreference resolution", "start_pos": 391, "end_pos": 413, "type": "TASK", "confidence": 0.9167815446853638}, {"text": "grammatical error correction", "start_pos": 415, "end_pos": 443, "type": "TASK", "confidence": 0.7749109864234924}]}, {"text": "This task is challenging since for many error types, current grammatical error correction systems do not achieve high performance and much research is still needed.", "labels": [], "entities": [{"text": "grammatical error correction", "start_pos": 61, "end_pos": 89, "type": "TASK", "confidence": 0.636378655831019}]}, {"text": "Also, tackling this task has farreaching impact, since it is estimated that hundreds of millions of people worldwide are learning English and they benefit directly from an automated grammar checker.", "labels": [], "entities": []}, {"text": "The CoNLL-2014 shared task provides a forum for participating teams to work on the same grammatical error correction task, with evaluation on the same blind test set using the same evaluation metric and scorer.", "labels": [], "entities": [{"text": "grammatical error correction task", "start_pos": 88, "end_pos": 121, "type": "TASK", "confidence": 0.7115506380796432}]}, {"text": "This overview paper contains a detailed description of the shared task, and is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 provides the task definition.", "labels": [], "entities": []}, {"text": "Section 3 describes the annotated training data provided and the blind test data.", "labels": [], "entities": []}, {"text": "Section 4 describes the evaluation metric and the scorer.", "labels": [], "entities": [{"text": "scorer", "start_pos": 50, "end_pos": 56, "type": "METRIC", "confidence": 0.9633030891418457}]}, {"text": "Section 5 lists the participating teams and outlines the approaches to grammatical error correction used by the teams.", "labels": [], "entities": [{"text": "grammatical error correction", "start_pos": 71, "end_pos": 99, "type": "TASK", "confidence": 0.712521493434906}]}, {"text": "Section 6 presents the results of the shared task, including a discussion on cross annotator comparison.", "labels": [], "entities": [{"text": "cross annotator comparison", "start_pos": 77, "end_pos": 103, "type": "TASK", "confidence": 0.6700410644213358}]}, {"text": "Section 7 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "A grammatical error correction system is evaluated by how well its proposed corrections or edits match the gold-standard edits.", "labels": [], "entities": [{"text": "grammatical error correction", "start_pos": 2, "end_pos": 30, "type": "TASK", "confidence": 0.6007265746593475}]}, {"text": "An essay is first sentence-segmented and tokenized before evaluation is carried out on the essay.", "labels": [], "entities": []}, {"text": "To illustrate, consider the following tokenized sentence S written by an English learner: <MISTAKE start par=\"5\" start off=\"11\" end par=\"5\" end off=\"19\"> <TYPE>Wform</TYPE> <CORRECTION>absolutely</CORRECTION> </MISTAKE>: Error type distribution of the training and test data.", "labels": [], "entities": [{"text": "MISTAKE start par", "start_pos": 91, "end_pos": 108, "type": "METRIC", "confidence": 0.7877525289853414}]}, {"text": "The test data were annotated independently by two annotators.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Statistics of training and test data.", "labels": [], "entities": []}, {"text": " Table 3: Error type distribution of the training and test data. The test data were annotated independently  by two annotators.", "labels": [], "entities": [{"text": "Error", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9655327200889587}]}, {"text": " Table 7: Scores (in %) without alternative an- swers. The teams that submitted their system out- put after the deadline have an asterisk affixed after  their team names.", "labels": [], "entities": []}, {"text": " Table 8: Scores (in %) with alternative answers.  The teams that submitted their system output af- ter the deadline have an asterisk affixed after their  team names.", "labels": [], "entities": []}, {"text": " Table 11: Performance (in %) for each team's output scored against the annotations of a single annotator.", "labels": [], "entities": []}, {"text": " Table 12: Performance (in %) for output of one gold standard annotation scored against the other gold  standard annotation: (a) The score of Annotator 1 if Annotator 2 was the gold standard, (b) The score of  Annotator 2 if Annotator 1 was the gold standard.", "labels": [], "entities": []}]}