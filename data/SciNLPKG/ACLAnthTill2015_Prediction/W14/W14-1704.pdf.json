{"title": [{"text": "The Illinois-Columbia System in the CoNLL-2014 Shared Task", "labels": [], "entities": [{"text": "CoNLL-2014 Shared Task", "start_pos": 36, "end_pos": 58, "type": "DATASET", "confidence": 0.8734392722447714}]}], "abstractContent": [{"text": "The CoNLL-2014 shared task is an extension of last year's shared task and fo-cuses on correcting grammatical errors in essays written by non-native learners of English.", "labels": [], "entities": [{"text": "correcting grammatical errors in essays written by non-native learners of English", "start_pos": 86, "end_pos": 167, "type": "TASK", "confidence": 0.8580641529776833}]}, {"text": "In this paper, we describe the Illinois-Columbia system that participated in the shared task.", "labels": [], "entities": []}, {"text": "Our system ranked second on the original annotations and first on the revised annotations.", "labels": [], "entities": []}, {"text": "The core of the system is based on the University of Illinois model that placed first in the CoNLL-2013 shared task.", "labels": [], "entities": []}, {"text": "This baseline model has been improved and expanded for this year's competition in several respects.", "labels": [], "entities": []}, {"text": "We describe our underlying approach, which relates to our previous work, and describe the novel aspects of the system in more detail.", "labels": [], "entities": []}], "introductionContent": [{"text": "The topic of text correction has seen a lot of interest in the past several years, with a focus on correcting grammatical errors made by English as a Second Language (ESL) learners.", "labels": [], "entities": [{"text": "text correction", "start_pos": 13, "end_pos": 28, "type": "TASK", "confidence": 0.8220347166061401}, {"text": "correcting grammatical errors made by English as a Second Language (ESL) learners", "start_pos": 99, "end_pos": 180, "type": "TASK", "confidence": 0.8181365558079311}]}, {"text": "ESL error correction is an important problem since most writers of English are not native English speakers.", "labels": [], "entities": [{"text": "ESL error correction", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.7158660689989725}]}, {"text": "The increased interest in this topic can be seen not only from the number of papers published on the topic but also from the three competitions devoted to grammatical error correction for non-native writers that have recently taken place:), HOO-2012 (, and the CoNLL-2013 shared task ( . In all three shared tasks, the participating systems performed at a level that is considered extremely low compared to performance obtained in other areas of NLP: even the best systems attained F1 scores in the range of 20-30 points.", "labels": [], "entities": [{"text": "grammatical error correction", "start_pos": 155, "end_pos": 183, "type": "TASK", "confidence": 0.5932542184988657}, {"text": "HOO-2012", "start_pos": 241, "end_pos": 249, "type": "METRIC", "confidence": 0.6527330279350281}, {"text": "F1 scores", "start_pos": 482, "end_pos": 491, "type": "METRIC", "confidence": 0.9820968210697174}]}, {"text": "The key reason that text correction is a difficult task is that even for non-native English speakers, writing accuracy is very high, as errors are very sparse.", "labels": [], "entities": [{"text": "text correction", "start_pos": 20, "end_pos": 35, "type": "TASK", "confidence": 0.8672331273555756}, {"text": "accuracy", "start_pos": 110, "end_pos": 118, "type": "METRIC", "confidence": 0.9844610095024109}]}, {"text": "Even for some of the most common types of errors, such as article and preposition usage, the majority of the words in these categories (over 90%) are used correctly.", "labels": [], "entities": []}, {"text": "For instance, in the CoNLL training data, only 2% of prepositions are incorrectly used.", "labels": [], "entities": [{"text": "CoNLL training data", "start_pos": 21, "end_pos": 40, "type": "DATASET", "confidence": 0.9394644697507223}]}, {"text": "Because errors are so sparse, it is more difficult fora system to identify a mistake accurately and without introducing many false alarms.", "labels": [], "entities": []}, {"text": "The CoNLL-2014 shared task () is an extension of the CoNLL-2013 shared task ( . Both competitions make use of essays written by ESL learners at the National University of Singapore.", "labels": [], "entities": []}, {"text": "However, while the first one focused on five kinds of mistakes that are commonly made by ESL writers -article, preposition, noun number, verb agreement, and verb formthis year's competition covers all errors occurring in the data.", "labels": [], "entities": []}, {"text": "Errors outside the target group were present in the task corpora last year as well, but were not evaluated.", "labels": [], "entities": [{"text": "Errors", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9619928002357483}]}, {"text": "Our system extends the one developed by the University of Illinois ( ) that placed first in the For this year's shared task, the system has been extended and improved in several respects: we extended the set of errors addressed by the system, developed a general approach for improving the error-specific models, and added a joint inference component to address interaction among errors.", "labels": [], "entities": []}, {"text": "See Rozovskaya and Roth (2013) for more detail.", "labels": [], "entities": []}, {"text": "We briefly discuss the task (Section 2) and give an overview of the baseline Illinois system (Section 3).", "labels": [], "entities": [{"text": "Illinois system", "start_pos": 77, "end_pos": 92, "type": "DATASET", "confidence": 0.8952087461948395}]}, {"text": "Section 4 presents the novel aspects of the system.", "labels": [], "entities": []}, {"text": "In Section 5, we evaluate the complete system on the development data and show the results obtained on test.", "labels": [], "entities": []}, {"text": "We offer error analysis and a brief discussion in Section 6.", "labels": [], "entities": []}, {"text": "In the parentheses, the error codes used in the shared task are shown.", "labels": [], "entities": []}, {"text": "Note that only the errors exemplifying the relevant phenomena are marked in the table; the sentences may contain other mistakes.", "labels": [], "entities": []}, {"text": "Errors marked as verb form include multiple grammatical phenomena that may characterize verbs.", "labels": [], "entities": []}, {"text": "Our system addresses all of the error types except \"Wrong Collocation\" and \"Local Redundancy\".", "labels": [], "entities": []}], "datasetContent": [{"text": "In Sections 3 and 4, we described the individual system components that address different types of errors.", "labels": [], "entities": []}, {"text": "In this section, we show how the system improves when each component is added into the system.", "labels": [], "entities": []}, {"text": "In this year's competition, systems are compared using F0.5 measure instead of F1.", "labels": [], "entities": [{"text": "F0.5 measure", "start_pos": 55, "end_pos": 67, "type": "METRIC", "confidence": 0.9861165285110474}, {"text": "F1", "start_pos": 79, "end_pos": 81, "type": "METRIC", "confidence": 0.9984937906265259}]}, {"text": "This is because in error correction good precision is more important than having a high recall, and the F0.5 reflects that by weighing precision twice as much as recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 41, "end_pos": 50, "type": "METRIC", "confidence": 0.9988278746604919}, {"text": "recall", "start_pos": 88, "end_pos": 94, "type": "METRIC", "confidence": 0.9990629553794861}, {"text": "F0.5", "start_pos": 104, "end_pos": 108, "type": "METRIC", "confidence": 0.9971099495887756}, {"text": "precision", "start_pos": 135, "end_pos": 144, "type": "METRIC", "confidence": 0.8970606327056885}, {"text": "recall", "start_pos": 162, "end_pos": 168, "type": "METRIC", "confidence": 0.9923418760299683}]}, {"text": "System output is scored with the M2 scorer.", "labels": [], "entities": []}, {"text": "reports performance results of each individual classifier.", "labels": [], "entities": []}, {"text": "In the final system, the article, preposition, noun number, and verb agree-    ment classifiers use combined models, each consisting of a classifier trained on the learner data and a classifier trained on native data.", "labels": [], "entities": []}, {"text": "We report performance of each such component separately and when they are combined.", "labels": [], "entities": []}, {"text": "The results show that combining models boosts the performance of each classifier: for example, the performance of the article classifier improves by more than 2 F0.5 points.", "labels": [], "entities": [{"text": "F0.5", "start_pos": 161, "end_pos": 165, "type": "METRIC", "confidence": 0.9924013614654541}]}, {"text": "It should be noted that results are computed with respect to all errors present in the data.", "labels": [], "entities": []}, {"text": "For this reason, recall is low.", "labels": [], "entities": [{"text": "recall", "start_pos": 17, "end_pos": 23, "type": "METRIC", "confidence": 0.9996892213821411}]}, {"text": "Next, in, we show the contribution of the novel components over the baseline system on the development set.", "labels": [], "entities": []}, {"text": "As described in Section 3, the baseline Illinois system consists of five individual components; their performance is shown in the top part of the table.", "labels": [], "entities": [{"text": "Illinois system", "start_pos": 40, "end_pos": 55, "type": "DATASET", "confidence": 0.8806663453578949}]}, {"text": "Note that although for the development set we make use of last year's test set, these results are not comparable to the performance results reported in last year's competition that used the F1 measure.", "labels": [], "entities": [{"text": "F1 measure", "start_pos": 190, "end_pos": 200, "type": "METRIC", "confidence": 0.9719279110431671}]}, {"text": "Overall, the baseline system achieves an F0.5 score of 33.17 on the development set.", "labels": [], "entities": [{"text": "F0.5 score", "start_pos": 41, "end_pos": 51, "type": "METRIC", "confidence": 0.9837805330753326}]}, {"text": "Then, by applying the model combination technique introduced in Section 4.3, the performance is improved to 34.92.", "labels": [], "entities": []}, {"text": "By adding modules to target three additional error types, the overall performance becomes 37.09.", "labels": [], "entities": []}, {"text": "Finally, the joint inference technique (see Section 4.4) slightly improves the performance further.", "labels": [], "entities": []}, {"text": "The final system achieves an F0.5 score of 37.13.", "labels": [], "entities": [{"text": "F0.5 score", "start_pos": 29, "end_pos": 39, "type": "METRIC", "confidence": 0.9800081849098206}]}, {"text": "shows the results on the test set provided by the organizers.", "labels": [], "entities": []}, {"text": "As was done previously, the organizers also offered another set of annotations based on the combination of revised official annotations and accepted alternative annotations proposed by participants.", "labels": [], "entities": []}, {"text": "Performance results on this set are also shown in.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: Performance of classifiers targeting  specific errors.", "labels": [], "entities": []}, {"text": " Table 5: Results on the development data. The  top part of the table shows the performance of the  baseline (Illinois) system from last year.", "labels": [], "entities": [{"text": "Illinois) system", "start_pos": 110, "end_pos": 126, "type": "DATASET", "confidence": 0.891741673151652}]}, {"text": " Table 6: Results on Test.", "labels": [], "entities": [{"text": "Test", "start_pos": 21, "end_pos": 25, "type": "DATASET", "confidence": 0.958400547504425}]}]}