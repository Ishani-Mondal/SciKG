{"title": [{"text": "Credibility Adjusted Term Frequency: A Supervised Term Weighting Scheme for Sentiment Analysis and Text Classification", "labels": [], "entities": [{"text": "Credibility Adjusted Term Frequency", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.6659766510128975}, {"text": "Sentiment Analysis", "start_pos": 76, "end_pos": 94, "type": "TASK", "confidence": 0.970455676317215}, {"text": "Text Classification", "start_pos": 99, "end_pos": 118, "type": "TASK", "confidence": 0.7248165458440781}]}], "abstractContent": [{"text": "We provide a simple but novel supervised weighting scheme for adjusting term frequency in tf-idf for sentiment analysis and text classification.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 101, "end_pos": 119, "type": "TASK", "confidence": 0.969382107257843}, {"text": "text classification", "start_pos": 124, "end_pos": 143, "type": "TASK", "confidence": 0.804896205663681}]}, {"text": "We compare our method to baseline weighting schemes and find that it outperforms them on multiple benchmarks.", "labels": [], "entities": []}, {"text": "The method is robust and works well on both snippets and longer documents.", "labels": [], "entities": []}], "introductionContent": [{"text": "Baseline discriminative methods for text classification usually involve training a linear classifier over bag-of-words (BoW) representations of documents.", "labels": [], "entities": [{"text": "text classification", "start_pos": 36, "end_pos": 55, "type": "TASK", "confidence": 0.7313147336244583}]}, {"text": "In BoW representations (also known as Vector Space Models), a document is represented as a vector where each entry is a count (or binary count) of tokens that occurred in the document.", "labels": [], "entities": []}, {"text": "Given that some tokens are more informative than others, a common technique is to apply a weighting scheme to give more weight to discriminative tokens and less weight to non-discriminative ones.", "labels": [], "entities": []}, {"text": "Term frequency-inverse document frequency (tfidf )) is an unsupervised weighting technique that is commonly employed.", "labels": [], "entities": [{"text": "Term frequency-inverse document frequency (tfidf ))", "start_pos": 0, "end_pos": 51, "type": "METRIC", "confidence": 0.7225027510098049}]}, {"text": "In tf-idf, each token i in document dis assigned the following weight, where tf i,d is the number of times token i occurred in document d, N is the number of documents in the corpus, and df i is the number of documents in which token i occurred.", "labels": [], "entities": []}, {"text": "Many supervised and unsupervised variants of tf-idf exist; Martineau and;).", "labels": [], "entities": []}, {"text": "The purpose of this paper is not to perform an exhaustive comparison of existing weighting schemes, and hence we do not list them here.", "labels": [], "entities": []}, {"text": "Interested readers are directed to and for comprehensive reviews of the different schemes.", "labels": [], "entities": []}, {"text": "In the present work, we propose a simple but novel supervised method to adjust the term frequency portion in tf-idf by assigning a credibility adjusted score to each token.", "labels": [], "entities": []}, {"text": "We find that it outperforms the traditional unsupervised tf-idf weighting scheme on multiple benchmarks.", "labels": [], "entities": []}, {"text": "The benchmarks include both snippets and longer documents.", "labels": [], "entities": []}, {"text": "We also compare our method against's Naive-Bayes Support Vector Machine (NBSVM), which has achieved state-of-the-art results (or close to it) on many datasets, and find that it performs competitively against NBSVM.", "labels": [], "entities": []}, {"text": "We additionally find that the traditional tf-idf performs competitively against other, more sophisticated methods when used with the right scaling and normalization parameters.", "labels": [], "entities": []}], "datasetContent": [{"text": "We test our method on both long and short text classification tasks, all of which were used to establish baselines in. has summary statistics of the datasets.", "labels": [], "entities": []}, {"text": "The snippet datasets are: \u2022 PL-sh: Short movie reviews with one sentence per review.", "labels": [], "entities": [{"text": "PL-sh", "start_pos": 28, "end_pos": 33, "type": "METRIC", "confidence": 0.9017028212547302}]}, {"text": "Classification involves detecting whether a review is positive or negative.", "labels": [], "entities": [{"text": "Classification", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.9219374060630798}]}, {"text": "2 \u2022 PL-sub: Dataset with short subjective movie reviews and objective plot summaries.", "labels": [], "entities": [{"text": "PL-sub", "start_pos": 4, "end_pos": 10, "type": "METRIC", "confidence": 0.7495750188827515}]}, {"text": "Classification task is to detect whether the sentence is objective or subjective.", "labels": [], "entities": [{"text": "Classification", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.9034786820411682}]}, {"text": "And the longer document datasets are: 1 Wang and Manning (2012) use the same \u03b1 but they differ from our NBSVM in two ways.", "labels": [], "entities": [{"text": "NBSVM", "start_pos": 104, "end_pos": 109, "type": "DATASET", "confidence": 0.9548707008361816}]}, {"text": "One, they use l2 hinge loss (as opposed to l1 loss in this paper \u2022 PL-2k: 2000 full-length movie reviews that has become the de facto benchmark for sentiment analysis ().", "labels": [], "entities": [{"text": "PL-2k: 2000 full-length movie reviews", "start_pos": 67, "end_pos": 104, "type": "DATASET", "confidence": 0.7936427990595499}, {"text": "sentiment analysis", "start_pos": 148, "end_pos": 166, "type": "TASK", "confidence": 0.9748176038265228}]}, {"text": "\u2022 IMDB: 50k full-length movie reviews (25k training, 25k test), from IMDB (Maas et al., 2011).", "labels": [], "entities": [{"text": "IMDB", "start_pos": 2, "end_pos": 6, "type": "DATASET", "confidence": 0.5885283946990967}, {"text": "IMDB", "start_pos": 69, "end_pos": 73, "type": "DATASET", "confidence": 0.9327893257141113}]}, {"text": "3 \u2022 AthR, XGraph: The 20-Newsgroup dataset, 2nd version with headers removed.", "labels": [], "entities": [{"text": "20-Newsgroup dataset", "start_pos": 22, "end_pos": 42, "type": "DATASET", "confidence": 0.691101923584938}]}, {"text": "Classification task is to classify which topic a document belongs to.", "labels": [], "entities": [{"text": "Classification", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.9515557289123535}]}, {"text": "AthR: alt.atheism vs religion.misc, XGraph: comp.windows.x vs comp.graphics.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Summary statistics for the datasets.  Length is the average number of unigram tokens  (including punctuation) per document. Pos/Neg is  the number of positive/negative documents in the  training set. Test is the number of documents in  the test set (CV means that there is no separate  test set for this dataset and thus a 10-fold cross- validation was used to calculate errors).", "labels": [], "entities": [{"text": "Length", "start_pos": 48, "end_pos": 54, "type": "METRIC", "confidence": 0.9912723898887634}]}, {"text": " Table 2: Results of our method (cred-tf-idf ) against baselines (tf-idf, NBSVM), using unigrams and  bigrams. cred-tf-idf and tf-idf both use log scaling and l 2 normalization. Best results (that do not use  external sources) are underlined, while top three are in bold. Rows 7-11 are MNB and NBSVM results  from", "labels": [], "entities": [{"text": "NBSVM", "start_pos": 74, "end_pos": 79, "type": "DATASET", "confidence": 0.9562562704086304}, {"text": "MNB", "start_pos": 286, "end_pos": 289, "type": "DATASET", "confidence": 0.9038676619529724}]}]}