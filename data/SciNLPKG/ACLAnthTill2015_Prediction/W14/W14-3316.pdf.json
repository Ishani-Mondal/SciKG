{"title": [], "abstractContent": [{"text": "We describe Stanford's participation in the French-English and English-German tracks of the 2014 Workshop on Statistical Machine Translation (WMT).", "labels": [], "entities": [{"text": "Statistical Machine Translation (WMT)", "start_pos": 109, "end_pos": 146, "type": "TASK", "confidence": 0.7863152424494425}]}, {"text": "Our systems used large feature sets, word classes, and an optional unconstrained language model.", "labels": [], "entities": []}, {"text": "Among constrained systems, ours performed the best according to uncased BLEU: 36.0% for French-English and 20.9% for English-German.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 72, "end_pos": 76, "type": "METRIC", "confidence": 0.991145670413971}]}], "introductionContent": [{"text": "Phrasal () is a phrase-based machine translation system () with an online, adaptive tuning algorithm () which allows efficient tuning of featurerich translation models.", "labels": [], "entities": [{"text": "phrase-based machine translation", "start_pos": 16, "end_pos": 48, "type": "TASK", "confidence": 0.6269708176453909}]}, {"text": "We improved upon the basic Phrasal system with sparse features over word classes, class-based language models, and a webscale language model.", "labels": [], "entities": []}, {"text": "We submitted one constrained French-English (Fr-En) system, one unconstrained English-German (En-De) system with a huge language model, and one constrained English-German system without it.", "labels": [], "entities": []}, {"text": "Each system was built using over 100,000 features and was tuned on over 10,000 sentences.", "labels": [], "entities": []}, {"text": "This paper describes our submitted systems and discusses how the improvements affect translation quality.", "labels": [], "entities": [{"text": "translation", "start_pos": 85, "end_pos": 96, "type": "TASK", "confidence": 0.95890212059021}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 3: Number of unique n-grams, in millions,  appearing in the Common Crawl German language  model.", "labels": [], "entities": [{"text": "Common Crawl German language  model", "start_pos": 67, "end_pos": 102, "type": "DATASET", "confidence": 0.8969821095466614}]}, {"text": " Table 4: Official results in terms of cased and uncased BLEU of our submitted systems compared to the  best systems for each track. The ranks for the unconstrained system are calculated relative to all primary  submissions for the language pair, whereas the ranks for the constrained systems are relative to only the  constrained systems submitted.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 57, "end_pos": 61, "type": "METRIC", "confidence": 0.9944345355033875}]}, {"text": " Table 5: En-De BLEU results. The tuning set is newstest2008-2012. Scores on newstest2014 were  computed after the system submission deadline using the released references.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 16, "end_pos": 20, "type": "METRIC", "confidence": 0.8784341812133789}, {"text": "newstest2008-2012", "start_pos": 48, "end_pos": 65, "type": "DATASET", "confidence": 0.9666736125946045}]}, {"text": " Table 6: Fr-En BLEU results. The tuning set is newstest2008-2012. Scores on newstest2014 were  computed after the system submission deadline using the released references.", "labels": [], "entities": [{"text": "Fr-En", "start_pos": 10, "end_pos": 15, "type": "DATASET", "confidence": 0.6354138851165771}, {"text": "BLEU", "start_pos": 16, "end_pos": 20, "type": "METRIC", "confidence": 0.9689744114875793}, {"text": "newstest2008-2012", "start_pos": 48, "end_pos": 65, "type": "DATASET", "confidence": 0.967231810092926}]}, {"text": " Table 7: Casing results on newstest2014 performed  after the evaluation. The oracle scores are uncased  BLEU (%) while all other scores are cased. Sub- mitted systems are shown in italic.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 105, "end_pos": 109, "type": "METRIC", "confidence": 0.9902533888816833}]}]}