{"title": [{"text": "Assessing the Readability of Sentences: Which Corpora and Features?", "labels": [], "entities": [{"text": "Assessing the Readability of Sentences", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.8711015582084656}]}], "abstractContent": [{"text": "The paper investigates the problem of sentence readability assessment, which is modelled as a classification task, with a specific view to text simplification.", "labels": [], "entities": [{"text": "sentence readability assessment", "start_pos": 38, "end_pos": 69, "type": "TASK", "confidence": 0.7821817000706991}, {"text": "text simplification", "start_pos": 139, "end_pos": 158, "type": "TASK", "confidence": 0.7387818992137909}]}, {"text": "In particular , it addresses two open issues connected with it, i.e. the corpora to be used for training, and the identification of the most effective features to determine sentence readability.", "labels": [], "entities": []}, {"text": "An existing readabil-ity assessment tool developed for Italian was specialized at the level of training corpus and learning algorithm.", "labels": [], "entities": []}, {"text": "A maximum entropy-based feature selection and ranking algorithm (grafting) was used to identify to the most relevant features: it turned out that assessing the readability of sentences is a complex task, requiring a high number of features, mainly syntactic ones.", "labels": [], "entities": []}], "introductionContent": [{"text": "Over the last ten years, work on automatic readability assessment employed sophisticated NLP techniques (such as syntactic parsing and statistical language modeling) to capture highly complex linguistic features, and used statistical machine learning to build readability assessment tools.", "labels": [], "entities": [{"text": "automatic readability assessment", "start_pos": 33, "end_pos": 65, "type": "TASK", "confidence": 0.6164549092451731}, {"text": "syntactic parsing", "start_pos": 113, "end_pos": 130, "type": "TASK", "confidence": 0.8026252090930939}, {"text": "statistical language modeling", "start_pos": 135, "end_pos": 164, "type": "TASK", "confidence": 0.6395930250485738}]}, {"text": "A variety of different NLP-based approaches has been proposed so far in the literature, differing at the level of the number of identified readability classes, the typology of features taken into account, the intended audience of the texts under evaluation, or the application within which readability assessment is carried out, etc.", "labels": [], "entities": []}, {"text": "Research focused so far on readability assessment at the document level.", "labels": [], "entities": []}, {"text": "However, as pointed out by, methods developed perform well when the task is characterizing the readability level of an entire document, while they are unreliable for short texts, including single sentences.", "labels": [], "entities": []}, {"text": "Yet, for specific applications, assessing the readability level of individual sentences would be desirable.", "labels": [], "entities": []}, {"text": "This is the case, for instance, for text simplification: in current approaches, text readability is typically assessed with respect to the entire document, while text simplification is carried out at the sentence level, as e.g. done in, and.", "labels": [], "entities": [{"text": "text simplification", "start_pos": 36, "end_pos": 55, "type": "TASK", "confidence": 0.7939267456531525}, {"text": "text simplification", "start_pos": 162, "end_pos": 181, "type": "TASK", "confidence": 0.7234805524349213}]}, {"text": "By decoupling the readability assessment and simplification processes, the impact of simplification operations on the overall readability level of a given text may not always be clear.", "labels": [], "entities": []}, {"text": "With sentence-based readability assessment, this is expected to be no longer a problem.", "labels": [], "entities": []}, {"text": "Sentence readability assessment thus represents an open issue in the literature which is worth being further explored.", "labels": [], "entities": [{"text": "Sentence readability assessment", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.9398789008458456}]}, {"text": "To our knowledge, the only attempts in this direction are represented by and for the Italian and Swedish languages respectively, followed more recently by dealing with English.", "labels": [], "entities": []}, {"text": "In this paper, we tackle the challenge of assessing the readability of individual sentences as a first step towards text simplification.", "labels": [], "entities": [{"text": "text simplification", "start_pos": 116, "end_pos": 135, "type": "TASK", "confidence": 0.7699310481548309}]}, {"text": "The task is modelled as a classification task, with the final aim of shedding light on two open issues connected with it, namely the reference corpora to be used for training (i.e. collections of sentences classified according to their readability level), and the identification of the most effective features to determine sentence readability.", "labels": [], "entities": []}, {"text": "For what concerns the former, sentence readability assessment poses the remarkable issue of classifying sentences according to their difficulty: if all sentences occurring in simplified texts can be assumed to be easy-to-read sentences, the reverse does not necessarily hold since not all sentences occurring in complex texts are to be assumed difficult-to-read.", "labels": [], "entities": [{"text": "sentence readability assessment", "start_pos": 30, "end_pos": 61, "type": "TASK", "confidence": 0.8155310352643331}]}, {"text": "This fact has important implications at the level of the composition of the corpora to be used for training.", "labels": [], "entities": []}, {"text": "The sec-ond issue is concerned with whether and to what extent the features playing a significant role in the assessment of readability at the sentence level coincide with those exploited at the level of document.", "labels": [], "entities": []}, {"text": "In particular, the following research questions are addressed: 1.", "labels": [], "entities": []}, {"text": "in assessing sentence readability, is it better to use a small gold standard training corpus of manually classified sentences or a much bigger training corpus automatically constructed from readability-tagged documents possibly containing misclassified sentences?", "labels": [], "entities": []}, {"text": "2. which are the features maximizing sentence readability assessment?", "labels": [], "entities": [{"text": "sentence readability assessment", "start_pos": 37, "end_pos": 68, "type": "TASK", "confidence": 0.6845716238021851}]}, {"text": "3. to what extent do important features for sentence readability classification match those playing a role in the document readability classification?", "labels": [], "entities": [{"text": "sentence readability classification", "start_pos": 44, "end_pos": 79, "type": "TASK", "confidence": 0.7262090146541595}, {"text": "document readability classification", "start_pos": 114, "end_pos": 149, "type": "TASK", "confidence": 0.6444625854492188}]}, {"text": "We will try to answer these questions by working on Italian, which is a less-resourced language as far as readability is concerned.", "labels": [], "entities": []}, {"text": "To this end, READ-IT (), which represents the first NLP-based readability assessment tool for Italian, was specialized in different respects, namely at the level of the training corpus and of the learning algorithm; to investigate questions 2. and 3.", "labels": [], "entities": [{"text": "READ-IT", "start_pos": 13, "end_pos": 20, "type": "METRIC", "confidence": 0.9558866620063782}]}, {"text": "above, a maximum entropy-based feature selection and ranking algorithm (i.e. grafting) was selected.", "labels": [], "entities": []}, {"text": "The specific target audience of readers addressed in this study is represented by people characterised by low literacy skills and/or by mild cognitive impairment.", "labels": [], "entities": []}, {"text": "The paper is organized as follows: Section 2 describes the background literature, Section 3 introduces our approach to the task, in terms of used corpora, features and learning algorithm.", "labels": [], "entities": []}, {"text": "Finally, Sections 4 and 5 describe the experimental setting and discuss achieved results.", "labels": [], "entities": []}], "datasetContent": [{"text": "In all experiments, the corpora were automatically tagged by the part-of-speech tagger described in Dell'Orletta (2009) and dependency-parsed by the DeSR parser) using Support Vector Machines as learning algorithm.", "labels": [], "entities": []}, {"text": "We devised two different experiments, aimed at exploring the research questions investigated in this paper.", "labels": [], "entities": []}, {"text": "To this end, READ-IT was adapted by integrating a specialized training corpus and a maximum entropy-based feature selection and ranking algorithm (i.e. grafting).", "labels": [], "entities": [{"text": "READ-IT", "start_pos": 13, "end_pos": 20, "type": "TASK", "confidence": 0.7632828950881958}]}, {"text": "This experiment, investigating the first research question, is aimed at identifying what is the most effective training data for sentence readability assessment.", "labels": [], "entities": [{"text": "sentence readability assessment", "start_pos": 129, "end_pos": 160, "type": "TASK", "confidence": 0.8224518497784933}]}, {"text": "In particular, the goal is to compare the results on the basis of using a small set of gold standard data with respect to a (potentially larger, but) noisy data set (i.e. without manual revision) where every Rep sentence was assumed to be difficult-to-read.", "labels": [], "entities": []}, {"text": "To assess similarities and differences at the level of the different corpora used for training in this experiment, in we report a selection of linguistic features (see Section 3.2) characterizing the four datasets with respect to the whole 2Par corpus.We can observe that 2Par differs from all four Rep corpora for all reported features, and that the four Rep corpora show similar trends.", "labels": [], "entities": [{"text": "2Par corpus.We", "start_pos": 240, "end_pos": 254, "type": "DATASET", "confidence": 0.7616972625255585}]}, {"text": "Interestingly, however, the Rep Gold corpus is almost always the most distant one from 2Par (i.e. at the level of sentence length, word length, distribution of adjectives and subjects, average length of dependency links and parse tree depth).", "labels": [], "entities": [{"text": "Rep Gold corpus", "start_pos": 28, "end_pos": 43, "type": "DATASET", "confidence": 0.8349947532018026}]}, {"text": "On the basis of the four Rep datasets, four models were built which we evaluated using a heldout test set consisting of 435 sentences from 2Par and 435 manually classified difficult-to-read sentences from Rep.", "labels": [], "entities": [{"text": "Rep datasets", "start_pos": 25, "end_pos": 37, "type": "DATASET", "confidence": 0.7027999311685562}, {"text": "2Par", "start_pos": 139, "end_pos": 143, "type": "DATASET", "confidence": 0.9634921550750732}]}, {"text": "Using the grafting method, we calculated the classification score for each sentence in our test set on the basis of an increasing number of features (ranging from 1 to all non-zero weighted features for the specific dataset): sentences with a score below 0.5 were classified as easy-to-read, whereas sentences having a score greater or equal to 0.5 were classified as difficultto-read.", "labels": [], "entities": []}, {"text": "This procedure was repeated for each of the four models.", "labels": [], "entities": []}, {"text": "The second experiment is aimed at answering our second and third research questions, focusing on the features relevant for sentence readability, and the relationship of those features with document readability classification.", "labels": [], "entities": [{"text": "document readability classification", "start_pos": 189, "end_pos": 224, "type": "TASK", "confidence": 0.6241627832253774}]}, {"text": "For this purpose, we compared sentence-and document-based readability classification results.", "labels": [], "entities": [{"text": "sentence-and document-based readability classification", "start_pos": 30, "end_pos": 84, "type": "TASK", "confidence": 0.523182787001133}]}, {"text": "In particular, we compared the features used by the sentence-based readability model trained on the gold standard data and the features used by the document-based model trained on Rep and 2Par.", "labels": [], "entities": [{"text": "gold standard data", "start_pos": 100, "end_pos": 118, "type": "DATASET", "confidence": 0.781201163927714}]}, {"text": "With respect to the document classification, we used a corpus of 638 documents (319 extracted from 2Par representating easy-to-read texts, and 319 extracted from Rep representing difficult-to-read texts) with 20% of the documents constituting the held-out test set.", "labels": [], "entities": []}, {"text": "reports the results for the sentence classification task using the four training datasets described above.", "labels": [], "entities": [{"text": "sentence classification task", "start_pos": 28, "end_pos": 56, "type": "TASK", "confidence": 0.8463386495908102}]}, {"text": "Results are reported in terms of both overall accuracy (calculated as the proportion of correct answers against all answers) and precision within each readability class (when using all features), defined as the number of easy or difficult sentences correctly identified as such (in their respective columns).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.9931640028953552}, {"text": "precision", "start_pos": 129, "end_pos": 138, "type": "METRIC", "confidence": 0.9995077848434448}]}, {"text": "Accuracy was computed for all training models tested using an increasing number of features (2, 10, 30, 50 and all features) as resulting from the GRAFTING-based ranking and detailed in.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.994118332862854}, {"text": "GRAFTING-based", "start_pos": 147, "end_pos": 161, "type": "METRIC", "confidence": 0.7843261957168579}]}, {"text": "Note that the first two features correspond in all cases to the traditional readability features of sentence length and word length.", "labels": [], "entities": []}, {"text": "The classification model trained on the small gold standard dataset turned out to almost always outperform all other models: it achieved the best accuracy (83.7%) using a relatively small number of features, and also fora fixed number of features (i.e. 2, 30 and 50).", "labels": [], "entities": [{"text": "gold standard dataset", "start_pos": 46, "end_pos": 67, "type": "DATASET", "confidence": 0.7998632788658142}, {"text": "accuracy", "start_pos": 146, "end_pos": 154, "type": "METRIC", "confidence": 0.9993108510971069}]}, {"text": "Only when using the top-10 features, the uncorrected balanced large dataset slightly outperformed the gold standard dataset.", "labels": [], "entities": []}, {"text": "The accuracy when using the unbalanced dataset for training was always significantly (p < 0.05) worse (using McNemar's test) than the accuracy based on the other training data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9995939135551453}, {"text": "McNemar's test", "start_pos": 109, "end_pos": 123, "type": "DATASET", "confidence": 0.8530193567276001}, {"text": "accuracy", "start_pos": 134, "end_pos": 142, "type": "METRIC", "confidence": 0.9977779984474182}]}, {"text": "The only other significant difference existed between the balanced small and large dataset for 10 features.", "labels": [], "entities": []}, {"text": "All other differences are non-significant.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: raw text features, lexical  features, morpho-syntactic features and syntactic  features, shortly described below. 2", "labels": [], "entities": []}, {"text": " Table 2: Distribution of some linguistic features in Rep and 2Par training data", "labels": [], "entities": [{"text": "2Par training data", "start_pos": 62, "end_pos": 80, "type": "DATASET", "confidence": 0.7727918823560079}]}, {"text": " Table 3: Sentence classification results using four training datasets and a varying number of features", "labels": [], "entities": [{"text": "Sentence classification", "start_pos": 10, "end_pos": 33, "type": "TASK", "confidence": 0.9896898567676544}]}]}