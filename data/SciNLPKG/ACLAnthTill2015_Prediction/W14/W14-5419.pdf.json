{"title": [{"text": "Multi-layered Image Representation for Image Interpretation", "labels": [], "entities": [{"text": "Multi-layered Image Representation", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.6114078561464945}, {"text": "Image Interpretation", "start_pos": 39, "end_pos": 59, "type": "TASK", "confidence": 0.6882384568452835}]}], "abstractContent": [{"text": "In order to bridge the semantic gap between the visual context of an image and semantic concepts people would use to interpret it, we propose a multi-layered image representation model considering different amounts of knowledge needed for the interpretation of the image at each layer.", "labels": [], "entities": []}, {"text": "Interpretation results on different semantic layers of Corel images related to outdoor scenes are presented and compared.", "labels": [], "entities": []}, {"text": "Obtained results show positive correlation of precision and recall with the abstract level of classes used for image annotation, i.e. more generalized classes have achieved better results.", "labels": [], "entities": [{"text": "precision", "start_pos": 46, "end_pos": 55, "type": "METRIC", "confidence": 0.9994578957557678}, {"text": "recall", "start_pos": 60, "end_pos": 66, "type": "METRIC", "confidence": 0.9992703795433044}]}], "introductionContent": [{"text": "Image captions and surrounding text can facilitate the retrieval of images if they exist, but the vast majority of images are not annotated with words.", "labels": [], "entities": []}, {"text": "A number of methods have been developed in recent years to automatically annotate images with words that users might intuitively use when searching for them.", "labels": [], "entities": []}, {"text": "This problem is challenging because different people will most likely interpret the same image with different words on different levels of abstraction.", "labels": [], "entities": []}, {"text": "Used words reflect their knowledge about the context of the image, experience, cultural background, etc.", "labels": [], "entities": []}, {"text": "On the other hand, annotation methods deal with visual features such as color, texture and shape that can be extracted from raw image data, so the major goal is to bridge the semantic gap between the available features and the interpretation of the images in the way humans do.", "labels": [], "entities": []}, {"text": "The idea is to define an image representation model that will reflect the semantic levels of words used in image interpretation.", "labels": [], "entities": [{"text": "image interpretation", "start_pos": 107, "end_pos": 127, "type": "TASK", "confidence": 0.7320509552955627}]}], "datasetContent": [{"text": "Our goal was to compare image interpretation results on different semantic layers.", "labels": [], "entities": [{"text": "image interpretation", "start_pos": 24, "end_pos": 44, "type": "TASK", "confidence": 0.7194489240646362}]}, {"text": "We have used apart of the Corel image database related to outdoor scenes.", "labels": [], "entities": [{"text": "Corel image database", "start_pos": 26, "end_pos": 46, "type": "DATASET", "confidence": 0.9623752037684122}]}, {"text": "The data set consisted of 500 images segmented with the n-cuts algorithm.", "labels": [], "entities": []}, {"text": "For each image segment a 16D feature vector was computed based on CIE L*a*b* colour model and geometric properties (size, position, height, width and shape of the area) of image segments ().", "labels": [], "entities": [{"text": "CIE", "start_pos": 66, "end_pos": 69, "type": "DATASET", "confidence": 0.916212797164917}]}, {"text": "The segments were labeled with one of the 28 keywords related to natural and artificial objects such as 'airplane', 'bird', 'lion', 'train' etc. and background objects like 'ground', 'sky', 'water' etc.", "labels": [], "entities": []}, {"text": "The keywords correspond to the elementary classes.", "labels": [], "entities": []}, {"text": "Some image segments were too small and couldn't be labeled manually and were excluded from data.", "labels": [], "entities": []}, {"text": "The final data set used for the experiment consists of 3960 segments.", "labels": [], "entities": []}, {"text": "The data was divided into training (3160) and testing (800) subsets by a 10-fold cross validation with 20% of the observations for the holdout cross-validation.", "labels": [], "entities": []}, {"text": "For image classification into elementary classes Bayesian classifier was used according to the maximum posterior probability ( ): (1) The conditional probability of a feature vector for the given elementary classes and the prior probability are estimated according to data in the training set.", "labels": [], "entities": [{"text": "image classification", "start_pos": 4, "end_pos": 24, "type": "TASK", "confidence": 0.8382841944694519}]}, {"text": "It is taken into account that the evidence factor is a scale factor that does not influence the classification results and is not calculated.", "labels": [], "entities": [{"text": "classification", "start_pos": 96, "end_pos": 110, "type": "TASK", "confidence": 0.9573260545730591}]}, {"text": "The results of the image-segments classification are compared with the ground truth and the precision and recall measures are calculated.", "labels": [], "entities": [{"text": "precision", "start_pos": 92, "end_pos": 101, "type": "METRIC", "confidence": 0.99970942735672}, {"text": "recall", "start_pos": 106, "end_pos": 112, "type": "METRIC", "confidence": 0.9986905455589294}]}, {"text": "The achieved average precision for classification of elementary classes is 32.6% and average recall is 27.5%.", "labels": [], "entities": [{"text": "precision", "start_pos": 21, "end_pos": 30, "type": "METRIC", "confidence": 0.9878847002983093}, {"text": "classification of elementary classes", "start_pos": 35, "end_pos": 71, "type": "TASK", "confidence": 0.8774327337741852}, {"text": "recall", "start_pos": 93, "end_pos": 99, "type": "METRIC", "confidence": 0.9712481498718262}]}, {"text": "To predict concepts on layers MI 2 and higher we have used the knowledge representation scheme based on fuzzy Petri nets with an integrated fuzzy inference engine ().", "labels": [], "entities": []}, {"text": "The fuzzy knowledge base contains the following main components: fuzzy spatial and co-occurrence relationships between elementary classes, fuzzy aggregation relationships between elementary classes and scene classes, and fuzzy generalization relationships between scene classes and generalization classes.", "labels": [], "entities": []}, {"text": "The knowledge chunks considering spatial and co-occurrence relationships as well as aggregation relationships are computed from the training set.", "labels": [], "entities": []}, {"text": "The training set is also used to estimated the truth of these relationships.", "labels": [], "entities": []}, {"text": "The hierarchical and generalization relationships are defined according to expert knowledge and so is their truth.", "labels": [], "entities": []}, {"text": "There were 15 scene classes defined in the knowledge base such as Scene Lion, Scene Shuttle and Seaside and 13 generalization classes on different levels of abstraction, such as Wild Cats, Wildlife, Natural Scene, and Man-Made Objects.", "labels": [], "entities": []}, {"text": "The obtained results show positive correlation of precision and recall with the abstract level of semantic concepts used for image interpretation.", "labels": [], "entities": [{"text": "precision", "start_pos": 50, "end_pos": 59, "type": "METRIC", "confidence": 0.9994829893112183}, {"text": "recall", "start_pos": 64, "end_pos": 70, "type": "METRIC", "confidence": 0.9989044666290283}, {"text": "image interpretation", "start_pos": 125, "end_pos": 145, "type": "TASK", "confidence": 0.729675218462944}]}, {"text": "For scene classes achieved results are little bit higher than for elementary classes, with precision of 37% and 31% for recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 91, "end_pos": 100, "type": "METRIC", "confidence": 0.9995893836021423}, {"text": "recall", "start_pos": 120, "end_pos": 126, "type": "METRIC", "confidence": 0.9974096417427063}]}, {"text": "For generalised classes the obtained results are significantly better, with precision of 52% and recall of 42%.", "labels": [], "entities": [{"text": "precision", "start_pos": 76, "end_pos": 85, "type": "METRIC", "confidence": 0.9997597336769104}, {"text": "recall", "start_pos": 97, "end_pos": 103, "type": "METRIC", "confidence": 0.9997639060020447}]}], "tableCaptions": []}