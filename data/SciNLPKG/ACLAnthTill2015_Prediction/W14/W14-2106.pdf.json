{"title": [{"text": "Analyzing Argumentative Discourse Units in Online Interactions", "labels": [], "entities": [{"text": "Analyzing Argumentative Discourse Units", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.7721841186285019}]}], "abstractContent": [{"text": "Argument mining of online interactions is in its infancy.", "labels": [], "entities": [{"text": "Argument mining of online interactions", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.8108346164226532}]}, {"text": "One reason is the lack of annotated corpora in this genre.", "labels": [], "entities": []}, {"text": "To make progress, we need to develop a principled and scalable way of determining which portions of texts are argumentative and what is the nature of argumentation.", "labels": [], "entities": []}, {"text": "We propose a two-tiered approach to achieve this goal and report on several initial studies to assess its potential.", "labels": [], "entities": []}], "introductionContent": [{"text": "An increasing portion of information and opinion exchange occurs in online interactions such as discussion forums, blogs, and webpage comments.", "labels": [], "entities": []}, {"text": "This type of user-generated conversational data provides a wealth of naturally occurring arguments.", "labels": [], "entities": []}, {"text": "Argument mining of online interactions, however, is still in its infancy.", "labels": [], "entities": [{"text": "Argument mining of online interactions", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.8254308104515076}]}, {"text": "One reason is the lack of annotated corpora in this genre.", "labels": [], "entities": []}, {"text": "To make progress, we need to develop a principled and scalable way of determining which portions of texts are argumentative and what is the nature of argumentation.", "labels": [], "entities": []}, {"text": "We propose a multi-step coding approach grounded in findings from argumentation research on managing the difficulties of coding arguments ().", "labels": [], "entities": []}, {"text": "In the first step, trained expert annotators identify basic argumentative features (coarse-grained analysis) in full-length threads.", "labels": [], "entities": []}, {"text": "In the second step, we explore the feasibility of using crowdsourcing and novice annotators to identify finer details and nuances of the basic argumentative units focusing on limited thread context.", "labels": [], "entities": []}, {"text": "Our coarse-grained scheme for argumentation is based on Pragmatic Argumentation Theory (PAT); Hutchby,.", "labels": [], "entities": [{"text": "argumentation", "start_pos": 30, "end_pos": 43, "type": "TASK", "confidence": 0.9637124538421631}, {"text": "Hutchby", "start_pos": 94, "end_pos": 101, "type": "DATASET", "confidence": 0.5691677927970886}]}, {"text": "PAT states that an argument can arise at any point when two or more actors engage in calling out and making problematic some aspect of another actor's prior contribution for what it (could have) said or meant.", "labels": [], "entities": []}, {"text": "The argumentative relationships among contributions to a discussion are indicated through links between what is targeted and how it is called-out.", "labels": [], "entities": []}, {"text": "shows an example of two Callouts that refer back to the same Target.", "labels": [], "entities": []}, {"text": "The annotation task performed by the trained annotators includes three subtasks that Peldszus and Stede (2013a) identify as part of the argument mining problem: 1) Segmentation, 2) Segment classification, and 3) Relationship identification.", "labels": [], "entities": [{"text": "argument mining", "start_pos": 136, "end_pos": 151, "type": "TASK", "confidence": 0.7575353682041168}, {"text": "Segment classification", "start_pos": 181, "end_pos": 203, "type": "TASK", "confidence": 0.9468158483505249}, {"text": "Relationship identification", "start_pos": 212, "end_pos": 239, "type": "TASK", "confidence": 0.8493747711181641}]}, {"text": "In the language of, Callouts and Targets are the basic Argument Discourse Units (ADUs) that are segmented, classified, and linked.", "labels": [], "entities": []}, {"text": "There are two key advantages of our coarse-grained annotation scheme: 1) It does not initially prescribe what constitutes an argumentative text; 2) It makes it possible for Expert Annotators (EAs) to find ADUs in long threads.", "labels": [], "entities": []}, {"text": "Assigning finer grained (more complex) labels would have unduly increased the already heavy cognitive load for the EAs.", "labels": [], "entities": []}, {"text": "In Section 2 we present the corpus, describe the annotation scheme and task, calculate Inter Annotator Agreement (IAA), and propose a hierarchical clustering approach to identify text segments that the EAs found easier or harder to annotate.", "labels": [], "entities": [{"text": "Inter Annotator Agreement (IAA)", "start_pos": 87, "end_pos": 118, "type": "METRIC", "confidence": 0.9160704016685486}]}, {"text": "In Section 3, we report on two Amazon Mechanical Turk (MTurk) experiments, which demonstrate that crowdsourcing is a feasible way to obtain finer grained annotations of basic ADUs, especially on the text segments that were easier for the EAs to code.", "labels": [], "entities": []}, {"text": "In the first crowd sourcing study, the Turkers (the workers at MTurk, who we consider novice annotators) assigned labels (Agree/Disagree/Other) to the relations between Callout and Target identified by the EAs.", "labels": [], "entities": [{"text": "crowd sourcing", "start_pos": 13, "end_pos": 27, "type": "TASK", "confidence": 0.7306409478187561}, {"text": "MTurk", "start_pos": 63, "end_pos": 68, "type": "DATASET", "confidence": 0.928198516368866}]}, {"text": "In the second study, Turkers labeled segments of Callouts as Stance or Rationale.", "labels": [], "entities": [{"text": "Stance", "start_pos": 61, "end_pos": 67, "type": "METRIC", "confidence": 0.9513979554176331}]}, {"text": "Turkers saw only a limited context of the threaded discussion, i.e. a particular Callout-Target pair identified by the EA(s) who had analyzed the entire thread.", "labels": [], "entities": []}, {"text": "In addition we report on initial classification experiments to detect agreement/disagreement, with the best F1 of 66.9% for the Agree class and 62.6% for the Disagree class.", "labels": [], "entities": [{"text": "F1", "start_pos": 108, "end_pos": 110, "type": "METRIC", "confidence": 0.9985860586166382}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Number of Callouts by threads and EA", "labels": [], "entities": []}, {"text": " Table 2: IAA for 5 EA: F1 and alpha values per  thread", "labels": [], "entities": [{"text": "IAA", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9835799336433411}, {"text": "F1", "start_pos": 24, "end_pos": 26, "type": "METRIC", "confidence": 0.9991538524627686}]}, {"text": " Table 3: Examples of Callouts lusters and their corresponding Targets", "labels": [], "entities": []}, {"text": " Table 4: Number of clusters for each cluster type", "labels": [], "entities": []}, {"text": " Table 5: Percentage of Relation labels per EA  cluster type", "labels": [], "entities": []}, {"text": " Table 6: Classification of Agree/Disagree", "labels": [], "entities": [{"text": "Classification of Agree/Disagree", "start_pos": 10, "end_pos": 42, "type": "TASK", "confidence": 0.8525663375854492}]}, {"text": " Table 7: Importance of Lexical Features", "labels": [], "entities": [{"text": "Importance", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9586189389228821}]}, {"text": " Table 8: Difficulty judgments by Turkers com- pared to number of EAs who selected a cluster", "labels": [], "entities": [{"text": "Difficulty", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.8841858506202698}]}, {"text": " Table 9: Difficulty judgment (majority voting)", "labels": [], "entities": [{"text": "Difficulty", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9777136445045471}]}]}