{"title": [{"text": "VPCTagger: Detecting Verb-Particle Constructions With Syntax-Based Methods", "labels": [], "entities": [{"text": "Detecting Verb-Particle Constructions", "start_pos": 11, "end_pos": 48, "type": "TASK", "confidence": 0.8870638012886047}]}], "abstractContent": [{"text": "Verb-particle combinations (VPCs) consist of a verbal and a preposition/particle component, which often have some additional meaning compared to the meaning of their parts.", "labels": [], "entities": [{"text": "Verb-particle combinations (VPCs)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.617423665523529}]}, {"text": "If a data-driven morphological parser or a syntactic parser is trained on a dataset annotated with extra information for VPCs, they will be able to identify VPCs in raw texts.", "labels": [], "entities": []}, {"text": "In this paper, we examine how syntactic parsers perform on this task and we introduce VPCTag-ger, a machine learning-based tool that is able to identify English VPCs in context.", "labels": [], "entities": []}, {"text": "Our method consists of two steps: it first selects VPC candidates on the basis of syntactic information and then selects genuine VPCs among them by exploiting new features like semantic and contextual ones.", "labels": [], "entities": []}, {"text": "Based on our results, we see that VPC-Tagger outperforms state-of-the-art methods in the VPC detection task.", "labels": [], "entities": [{"text": "VPC detection task", "start_pos": 89, "end_pos": 107, "type": "TASK", "confidence": 0.9658418297767639}]}], "introductionContent": [{"text": "Verb-particle constructions (VPCs) area subclass of multiword expressions (MWEs) that contain more than one meaningful tokens but the whole unit exhibits syntactic, semantic or pragmatic idiosyncracies ().", "labels": [], "entities": []}, {"text": "VPCs consist of a verb and a preposition/particle (like hand in or go out) and they are very characteristic of the English language.", "labels": [], "entities": []}, {"text": "The particle modifies the meaning of the verb: it may add aspectual information, may refer to motion or location or may totally change the meaning of the expression.", "labels": [], "entities": []}, {"text": "Thus, the meaning of VPCs can be compositional, i.e. it can be computed on the basis of the meaning of the verb and the particle (go out) or it can be idiomatic; i.e. a combination of the given verb and particle results in a(n unexpected) new meaning (do in \"kill\").", "labels": [], "entities": []}, {"text": "Moreover, as their syntactic surface structure is very similar to verb -prepositional phrase combinations, it is not straightforward to determine whether a given verb + preposition/particle combination functions as a VPC or not and contextual information plays a very important role here.", "labels": [], "entities": []}, {"text": "For instance, compare the following examples: The hitman did in the president and What he did in the garden was unbelievable.", "labels": [], "entities": []}, {"text": "Both sentences contain the sequence did in, but it is only in the first sentence where it functions as a VPC and in the second case, it is a simple verbprepositional phrase combination.", "labels": [], "entities": []}, {"text": "For these reasons, VPCs are of great interest for natural language processing applications like machine translation or information extraction, where it is necessary to grab the meaning of the text.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 96, "end_pos": 115, "type": "TASK", "confidence": 0.8040486872196198}, {"text": "information extraction", "start_pos": 119, "end_pos": 141, "type": "TASK", "confidence": 0.8060195744037628}]}, {"text": "The special relation of the verb and particle within a VPC is often distinctively marked at several annotation layers in treebanks.", "labels": [], "entities": []}, {"text": "For instance, in the Penn Treebank, the particle is assigned a specific part of speech tag (RP) and it also has a specific syntactic label (PRT), see also.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 21, "end_pos": 34, "type": "DATASET", "confidence": 0.9944382309913635}]}, {"text": "This entails that if a datadriven morphological parser or a syntactic parser is trained on a dataset annotated with extra information for VPCs, it will be able to assign these kind of tags as well.", "labels": [], "entities": []}, {"text": "In other words, the morphological/syntactic parser itself will be able to identify VPCs in texts.", "labels": [], "entities": []}, {"text": "In this paper, we seek to identify VPCs on the basis of syntactic information.", "labels": [], "entities": []}, {"text": "We first examine how syntactic parsers perform on), a dataset manually annotated for different types of MWEs, including VPCs.", "labels": [], "entities": []}, {"text": "We then present our syntax-based tool called VPCTagger to identify VPCs, which consists of two steps: first, we select VPC candidates (i.e. verbpreposition/particle pairs) from the text and then we apply a machine learning-based technique to classify them as genuine VPCs or not.", "labels": [], "entities": []}, {"text": "This The hitman did in the president . method is based on a rich feature set with new features like semantic or contextual features.", "labels": [], "entities": []}, {"text": "We compare the performance of the parsers with that of our approach and we discuss the reasons for any possible differences.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Statistical data on the corpora.", "labels": [], "entities": []}, {"text": " Table 2: Edge types in the Wiki50 corpus. prt: par- ticle. prep: preposition. advmod: adverbial mod- ifier. other: other dependency labels. none: no  direct syntactic connection between the verb and  particle.", "labels": [], "entities": [{"text": "Wiki50 corpus", "start_pos": 28, "end_pos": 41, "type": "DATASET", "confidence": 0.8349658846855164}]}, {"text": " Table 3: The frequency of verbs on the Wiki50  corpus used by Tu and Roth (2012).", "labels": [], "entities": [{"text": "Wiki50  corpus", "start_pos": 40, "end_pos": 54, "type": "DATASET", "confidence": 0.9029765427112579}]}, {"text": " Table 4: The most frequent VPCs and verbal com- ponents on the Wiki50 corpus.", "labels": [], "entities": [{"text": "Wiki50 corpus", "start_pos": 64, "end_pos": 77, "type": "DATASET", "confidence": 0.950368195772171}]}, {"text": " Table 6: 5-fold cross validation results on the  Tu&Roth dataset in terms of accuracy and F-score.", "labels": [], "entities": [{"text": "Tu&Roth dataset", "start_pos": 50, "end_pos": 65, "type": "DATASET", "confidence": 0.8299490809440613}, {"text": "accuracy", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.9996334314346313}, {"text": "F-score", "start_pos": 91, "end_pos": 98, "type": "METRIC", "confidence": 0.9987026453018188}]}, {"text": " Table 7: The usefulness of individual features in  terms of precision, recall and F-score using the  Wiki50 corpus.", "labels": [], "entities": [{"text": "precision", "start_pos": 61, "end_pos": 70, "type": "METRIC", "confidence": 0.9995197057723999}, {"text": "recall", "start_pos": 72, "end_pos": 78, "type": "METRIC", "confidence": 0.9995892643928528}, {"text": "F-score", "start_pos": 83, "end_pos": 90, "type": "METRIC", "confidence": 0.9986132383346558}, {"text": "Wiki50 corpus", "start_pos": 102, "end_pos": 115, "type": "DATASET", "confidence": 0.9564071893692017}]}]}