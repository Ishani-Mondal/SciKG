{"title": [{"text": "Transduction Recursive Auto-Associative Memory: Learning Bilingual Compositional Distributed Vector Representations of Inversion Transduction Grammars", "labels": [], "entities": [{"text": "Transduction Recursive Auto-Associative Memory", "start_pos": 0, "end_pos": 46, "type": "TASK", "confidence": 0.8488382548093796}, {"text": "Learning Bilingual Compositional Distributed Vector Representations of Inversion Transduction Grammars", "start_pos": 48, "end_pos": 150, "type": "TASK", "confidence": 0.7564466923475266}]}], "abstractContent": [{"text": "We introduce TRAAM, or Transduction RAAM, a fully bilingual generalization of Pollack's (1990) monolingual Recur-sive Auto-Associative Memory neural network model, in which each distributed vector represents a bilingual constituent-i.e., an instance of a transduction rule, which specifies a relation between two monolin-gual constituents and how their subcon-stituents should be permuted.", "labels": [], "entities": [{"text": "TRAAM", "start_pos": 13, "end_pos": 18, "type": "METRIC", "confidence": 0.9890496730804443}]}, {"text": "Bilingual terminals are special cases of bilingual constituents, where a vector represents either (1) a bilingual token-a token-to-token or \"word-to-word\" translation rule-or (2) a bilingual segment-a segment-to-segment or \"phrase-to-phrase\" translation rule.", "labels": [], "entities": []}, {"text": "TRAAMs have properties that appear attractive for bilingual grammar induction and statistical machine translation applications.", "labels": [], "entities": [{"text": "TRAAMs", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.8026409149169922}, {"text": "bilingual grammar induction", "start_pos": 50, "end_pos": 77, "type": "TASK", "confidence": 0.7890892227490743}, {"text": "statistical machine translation", "start_pos": 82, "end_pos": 113, "type": "TASK", "confidence": 0.6816976269086202}]}, {"text": "Training of TRAAM drives both the autoencoder weights and the vector representations to evolve, such that similar bilingual constituents tend to have more similar vectors.", "labels": [], "entities": [{"text": "TRAAM", "start_pos": 12, "end_pos": 17, "type": "METRIC", "confidence": 0.8136016130447388}]}], "introductionContent": [{"text": "We introduce Transduction RAAM-or TRAAM for short-a recurrent neural network model that generalizes the monolingual RAAM model of to a distributed vector representation of compositionally structured transduction grammars () that is fully bilingual from top to bottom.", "labels": [], "entities": [{"text": "Transduction RAAM-or TRAAM", "start_pos": 13, "end_pos": 39, "type": "TASK", "confidence": 0.7756582498550415}]}, {"text": "In RAAM, which stands for Recursive Auto-Associative Memory, using feature vectors to characterize constituents at every level of a parse tree has the advantages that (1) the entire context of all subtrees inside the constituent can be efficiently captured in the feature vectors, (2) the learned representations generalize well because similar feature vectors represent similar constituents or segments, and (3) representations can be automatically learned so as to maximize prediction accuracy for various tasks using semi-supervised learning.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 487, "end_pos": 495, "type": "METRIC", "confidence": 0.9509354829788208}]}, {"text": "We argue that different, but analogous, properties are desirable for bilingual structured translation models.", "labels": [], "entities": [{"text": "bilingual structured translation", "start_pos": 69, "end_pos": 101, "type": "TASK", "confidence": 0.5890370309352875}]}, {"text": "Unlike RAAM, where each distributed vector represents a monolingual token or constituent, each distributed vector in TRAAM represents a bilingual constituent or biconstituent-that is, an instance of a transduction rule, which asserts a relation between two monolingual constituents, as well as specifying how to permute their subconstituents in translation.", "labels": [], "entities": [{"text": "RAAM", "start_pos": 7, "end_pos": 11, "type": "TASK", "confidence": 0.9339049458503723}]}, {"text": "Bilingual terminals, or biterminals, are special cases of biconstituents where a vector represents either (1) a bitoken-a token-to-token or \"word-to-word\" translation rule -or (2) a bisegment-a segment-to-segment or \"phrase-to-phrase\" translation rule.", "labels": [], "entities": []}, {"text": "The properties of TRAAMs are attractive for machine translation applications.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 44, "end_pos": 63, "type": "TASK", "confidence": 0.7778405547142029}]}, {"text": "As with RAAM, TRAAMs can be trained via backpropagation training, which simultaneously evolves both the autoencoder weights and the biconstituent vector representations.", "labels": [], "entities": []}, {"text": "As with RAAM, the evolution of the vector representations within the hidden layer performs automatic feature induction, and for many applications can obviate the need for manual feature engineering.", "labels": [], "entities": [{"text": "RAAM", "start_pos": 8, "end_pos": 12, "type": "TASK", "confidence": 0.9234901666641235}, {"text": "feature induction", "start_pos": 101, "end_pos": 118, "type": "TASK", "confidence": 0.7135974168777466}]}, {"text": "However, the result is that similar vectors tend to represent similar biconstituents, rather than monolingual constituents.", "labels": [], "entities": []}, {"text": "The learned vector representations thus tend to form clusters of similar translation relations instead of merely similar strings.", "labels": [], "entities": []}, {"text": "That is, TRAAM clusters represent soft nonterminal categories of cross-lingual relations and translation patterns, as opposed to soft nonterminal categories of monolingual strings as in RAAM.", "labels": [], "entities": [{"text": "TRAAM", "start_pos": 9, "end_pos": 14, "type": "METRIC", "confidence": 0.9090983867645264}]}, {"text": "Also, TRAAMs inherently make full simultaneous use of both input and output language fea-tures, recursively, in an elegant integrated fashion.", "labels": [], "entities": []}, {"text": "TRAAM does not make restrictive a priori assumptions of conditional independence between input and output language features.", "labels": [], "entities": [{"text": "TRAAM", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.6262965202331543}]}, {"text": "When evolving the biconstituent vector representations, generalization occurs over similar input and output structural characteristics simultaneously.", "labels": [], "entities": []}, {"text": "In most recurrent neural network applications to machine translation to date, only input side features or only output language features are used.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 49, "end_pos": 68, "type": "TASK", "confidence": 0.7839018702507019}]}, {"text": "Even in the few previous cases where recurrent neural networks have employed both input and output language features for machine translation, the models have typically been factored so that their recursive portion is applied only to either the input or output language, but not both.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 121, "end_pos": 140, "type": "TASK", "confidence": 0.7428340911865234}]}, {"text": "As with RAAM, the objective criteria for training can be adjusted to reflect accuracy on numerous different kinds of tasks, biasing the direction that vector representations evolve toward.", "labels": [], "entities": [{"text": "RAAM", "start_pos": 8, "end_pos": 12, "type": "TASK", "confidence": 0.9595206379890442}, {"text": "accuracy", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.996928870677948}]}, {"text": "But again, TRAAM's learned vector representations support making predictions that simultaneously make use of both input and output structural characteristics.", "labels": [], "entities": []}, {"text": "For example, TRAAM has the ability to take into account the structure of both input and output subtree characteristics while making predictions on reordering them.", "labels": [], "entities": []}, {"text": "Similarly, for specific cross-lingual tasks such as word alignment, sense disambiguation, or machine translation, classifiers can simultaneously be trained in conjunction with evolving the vector representations to optimize task-specific accuracy.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 52, "end_pos": 66, "type": "TASK", "confidence": 0.7812322676181793}, {"text": "sense disambiguation", "start_pos": 68, "end_pos": 88, "type": "TASK", "confidence": 0.7032335996627808}, {"text": "machine translation", "start_pos": 93, "end_pos": 112, "type": "TASK", "confidence": 0.7652455270290375}, {"text": "accuracy", "start_pos": 238, "end_pos": 246, "type": "METRIC", "confidence": 0.9681833982467651}]}, {"text": "In this paper we use as examples binary biparse trees consistent with transduction grammars in a 2-normal form, which by definition are inversion transduction grammars (Wu, 1997) since they are binary rank.", "labels": [], "entities": []}, {"text": "This is not a requirement for TRAAM, which in general can be formed for transduction grammars of any rank.", "labels": [], "entities": [{"text": "TRAAM", "start_pos": 30, "end_pos": 35, "type": "METRIC", "confidence": 0.8937111496925354}]}, {"text": "Moreover, with distributed vector representations, the notion of nonterminal categories in TRAAM is that of soft membership, unlike in symbolically represented transduction grammars.", "labels": [], "entities": []}, {"text": "We start with bracketed training data that contains no bilingual category labels (like training data for Bracketing ITGs or BITGs).", "labels": [], "entities": [{"text": "Bracketing ITGs or BITGs", "start_pos": 105, "end_pos": 129, "type": "DATASET", "confidence": 0.7732598036527634}]}, {"text": "Training results in self-organizing clusters that have been automatically induced, representing soft nonterminal categories (unlike BITGs, which do not have differentiated nonterminal categories).", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}