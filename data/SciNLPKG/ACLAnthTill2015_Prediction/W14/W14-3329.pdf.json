{"title": [{"text": "The DCU Terminology Translation System for the Medical Query Subtask at WMT14", "labels": [], "entities": [{"text": "DCU Terminology Translation", "start_pos": 4, "end_pos": 31, "type": "TASK", "confidence": 0.8483360409736633}, {"text": "WMT14", "start_pos": 72, "end_pos": 77, "type": "DATASET", "confidence": 0.8012890815734863}]}], "abstractContent": [{"text": "This paper describes the Dublin City University terminology translation system used for our participation in the query translation subtask in the medical translation task in the Workshop on Statistical Machine Translation (WMT14).", "labels": [], "entities": [{"text": "Dublin City University", "start_pos": 25, "end_pos": 47, "type": "DATASET", "confidence": 0.968889037768046}, {"text": "terminology translation", "start_pos": 48, "end_pos": 71, "type": "TASK", "confidence": 0.7436190247535706}, {"text": "query translation subtask", "start_pos": 113, "end_pos": 138, "type": "TASK", "confidence": 0.7665826777617136}, {"text": "medical translation task", "start_pos": 146, "end_pos": 170, "type": "TASK", "confidence": 0.8101016283035278}, {"text": "Statistical Machine Translation (WMT14)", "start_pos": 190, "end_pos": 229, "type": "TASK", "confidence": 0.7684671630462011}]}, {"text": "We deployed six different kinds of terminology extraction methods, and participated in three different tasks: FR-EN and EN-FR query tasks, and the CLIR task.", "labels": [], "entities": [{"text": "terminology extraction", "start_pos": 35, "end_pos": 57, "type": "TASK", "confidence": 0.8929472863674164}, {"text": "FR-EN", "start_pos": 110, "end_pos": 115, "type": "METRIC", "confidence": 0.831021249294281}]}, {"text": "We obtained 36.2 BLEU points absolute for FR-EN and 28.8 BLEU points absolute for EN-FR tasks where we obtained the first place in both tasks.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 17, "end_pos": 21, "type": "METRIC", "confidence": 0.9985601305961609}, {"text": "FR-EN", "start_pos": 42, "end_pos": 47, "type": "METRIC", "confidence": 0.6836020350456238}, {"text": "BLEU", "start_pos": 57, "end_pos": 61, "type": "METRIC", "confidence": 0.9978834986686707}]}, {"text": "We obtained 51.8 BLEU points absolute for the CLIR task.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 17, "end_pos": 21, "type": "METRIC", "confidence": 0.9975570440292358}]}], "introductionContent": [{"text": "This paper describes the terminology translation system developed at Dublin City University for our participation in the query translation subtask at the Workshop on Statistical Machine Translation (WMT14).", "labels": [], "entities": [{"text": "terminology translation", "start_pos": 25, "end_pos": 48, "type": "TASK", "confidence": 0.7563362717628479}, {"text": "Dublin City University", "start_pos": 69, "end_pos": 91, "type": "DATASET", "confidence": 0.9709046880404154}, {"text": "query translation subtask at the Workshop on Statistical Machine Translation (WMT14)", "start_pos": 121, "end_pos": 205, "type": "TASK", "confidence": 0.7478144466876984}]}, {"text": "We developed six kinds of terminology extraction methods for the problem of medical terminology translation, especially where rare and new words are considered.", "labels": [], "entities": [{"text": "terminology extraction", "start_pos": 26, "end_pos": 48, "type": "TASK", "confidence": 0.8649637401103973}, {"text": "medical terminology translation", "start_pos": 76, "end_pos": 107, "type": "TASK", "confidence": 0.6684001882870992}]}, {"text": "We have several motivations which we address before providing a description of the actual algorithms undeprinning our work.", "labels": [], "entities": []}, {"text": "First, terminology translation cannot be seen just as a simple extension of the translation process if we use an analogy from human translation.", "labels": [], "entities": [{"text": "terminology translation", "start_pos": 7, "end_pos": 30, "type": "TASK", "confidence": 0.8821718990802765}]}, {"text": "Terminology translation can be considered as more important and a quite different task than translation per se, so we need a considerably different way of solving this particular problem.", "labels": [], "entities": [{"text": "Terminology translation", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.9410552084445953}]}, {"text": "Bilingual terminology selection has been claimed to be the touchstone inhuman translation, especially where scientific and legal translation are concerned.", "labels": [], "entities": [{"text": "Bilingual terminology selection", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.8446508447329203}, {"text": "scientific and legal translation", "start_pos": 108, "end_pos": 140, "type": "TASK", "confidence": 0.8023814260959625}]}, {"text": "Terminology selection is often the hardest and most time-consuming process in the translation workflow.", "labels": [], "entities": [{"text": "Terminology selection", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.9536892473697662}, {"text": "translation workflow", "start_pos": 82, "end_pos": 102, "type": "TASK", "confidence": 0.90994793176651}]}, {"text": "Depending on the particular requirements of the use-case, users may not object to disfluent translations, but will invariably be very sensitive to the wrong selection of terminology, even if the meaning of the chosen terms is correct.", "labels": [], "entities": []}, {"text": "This is especially true if this selected terminology does not match with that preferred by the users themselves, in which case users are likely to express some kind of complaint; it may even be that the entire translation is rejected as sub-standard or inappropriate on such grounds.", "labels": [], "entities": []}, {"text": "Second, we look at how to handle new and rare words.", "labels": [], "entities": []}, {"text": "If we inspect the process of human translation more closely, it is easy to identify several differences compared to the methods used in statistical MT (SMT).", "labels": [], "entities": [{"text": "human translation", "start_pos": 29, "end_pos": 46, "type": "TASK", "confidence": 0.6914409101009369}, {"text": "MT (SMT)", "start_pos": 148, "end_pos": 156, "type": "TASK", "confidence": 0.8425314426422119}]}, {"text": "Unless stipulated by the client, the selection of bilingual terminology can be a highly subjective process.", "labels": [], "entities": []}, {"text": "Accordingly, it is not necessarily the bilingual term-pair with the highest probability that is chosen by the human translator.", "labels": [], "entities": []}, {"text": "It is often the case that statistical methods often forget about or delete less frequent n-grams, but rely on more frequent n-grams using maximum likelihood or Maximum A Priori (MAP) methods.", "labels": [], "entities": []}, {"text": "If some terminology is highly suitable, a human translator can use it quite freely.", "labels": [], "entities": []}, {"text": "Furthermore, there area lot of new words in reality for which new target equivalents have to be created by the translators themselves, so the question arises as to how human translators actually select appropriate new terminology.", "labels": [], "entities": []}, {"text": "Transliteration, which is often supported by many Asian languages including Hindi, Japanese, and Chinese, is perhaps the easiest things to do under such circumstances.", "labels": [], "entities": []}, {"text": "Slight modifications of alphabets/accented characters can sometimes successfully create a valid new term, even for European languages.", "labels": [], "entities": []}, {"text": "The remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes our algorithms.", "labels": [], "entities": []}, {"text": "Our decoding strategy in Section 3.", "labels": [], "entities": []}, {"text": "Our experimen-tal settings and results are presented in Section 4, and we conclude in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "The baseline is obtained in the following way.", "labels": [], "entities": []}, {"text": "The GIZA++ implementation of IBM Model 4 is used as the baseline for word alignment: Model 4 is incrementally trained by performing 5 iterations of Model 1, 5 iterations of HMM, 3 iterations of Model 3, and 3 iterations of Model 4.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 69, "end_pos": 83, "type": "TASK", "confidence": 0.8243005275726318}]}, {"text": "For phrase extraction the grow-diagfinal heuristics described in () is used to derive the refined alignment from bidirectional alignments.", "labels": [], "entities": [{"text": "phrase extraction", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.8533483445644379}]}, {"text": "We then perform MERT which optimizes parameter settings using the BLEU metric (), while a 5-gram language model is derived with Kneser-Ney smoothing) trained using SRILM).", "labels": [], "entities": [{"text": "MERT", "start_pos": 16, "end_pos": 20, "type": "METRIC", "confidence": 0.8810882568359375}, {"text": "BLEU", "start_pos": 66, "end_pos": 70, "type": "METRIC", "confidence": 0.9977110624313354}, {"text": "SRILM", "start_pos": 164, "end_pos": 169, "type": "DATASET", "confidence": 0.49003949761390686}]}, {"text": "We use the whole training corpora including the WMT14 translation task corpora as well as medical domain data.", "labels": [], "entities": [{"text": "WMT14 translation task corpora", "start_pos": 48, "end_pos": 78, "type": "DATASET", "confidence": 0.6902193278074265}]}, {"text": "UMLS and Wikipedia are used just as training corpora for the baseline.", "labels": [], "entities": [{"text": "UMLS", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9639362692832947}]}, {"text": "For the extraction from parallel corpora (cf. Section 2.2), we used Genia tagger) and the Berkeley parser.", "labels": [], "entities": []}, {"text": "For the zero-shot learning (cf. Section 2.6) we used scikit learn), word2vec (, and a recurrent neural network.", "labels": [], "entities": []}, {"text": "Other tools used are in-house software.", "labels": [], "entities": []}, {"text": "shows the results for the FR-EN query task.", "labels": [], "entities": [{"text": "FR-EN query task", "start_pos": 26, "end_pos": 42, "type": "TASK", "confidence": 0.4568501015504201}]}, {"text": "We obtained 36.2 BLEU points absolute, which is an improvement of 6.3 BLEU point absolute (21.1% relative) over the baseline.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 17, "end_pos": 21, "type": "METRIC", "confidence": 0.9994582533836365}, {"text": "BLEU point absolute", "start_pos": 70, "end_pos": 89, "type": "METRIC", "confidence": 0.9757020870844523}]}, {"text": "shows the results for the EN-FR query task.", "labels": [], "entities": []}, {"text": "We obtained 28.8 BLEU points absolute, which is an improvement of 8.7 BLEU points abso-lute (43% relative) over the baseline.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 17, "end_pos": 21, "type": "METRIC", "confidence": 0.9989870190620422}, {"text": "BLEU", "start_pos": 70, "end_pos": 74, "type": "METRIC", "confidence": 0.9974071383476257}]}, {"text": "Our system was the best system for both of these tasks.", "labels": [], "entities": []}, {"text": "These improvements over the baseline were statistically significant by a paired bootstrap test, the effects of extraction methods, language model and MERT process.", "labels": [], "entities": [{"text": "MERT", "start_pos": 150, "end_pos": 154, "type": "METRIC", "confidence": 0.6501284241676331}]}, {"text": "All the measurements are by BLEU (cased).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 28, "end_pos": 32, "type": "METRIC", "confidence": 0.999188244342804}]}, {"text": "In this table, \"medical\" indicates a language model built on all the medical corpora while \"WMT\" indicates a language model built on all the non-medical corpora.", "labels": [], "entities": []}, {"text": "Note that some sentence in testset can be considered as non-medical domain.", "labels": [], "entities": []}, {"text": "Extraction methods (1) -(6) correspond to those described in Section 2.1 -2.6.", "labels": [], "entities": []}, {"text": "shows the results for CLIR task.", "labels": [], "entities": []}, {"text": "We obtained 51.8 BLEU points absolute, which is an improvement of 9.4 BLEU point absolute (22.2% relative) over the baseline.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 17, "end_pos": 21, "type": "METRIC", "confidence": 0.9996188879013062}, {"text": "BLEU point absolute", "start_pos": 70, "end_pos": 89, "type": "METRIC", "confidence": 0.9773126045862833}]}, {"text": "Although CLIR task allowed 10-best lists, our submission included only 1-best list.", "labels": [], "entities": [{"text": "CLIR task", "start_pos": 9, "end_pos": 18, "type": "DATASET", "confidence": 0.7034211158752441}]}, {"text": "This resulted in the score of P@5 of 0.348 and P@10 of 0.346 which correspond to the second place, despite a good result in terms of BLEU.", "labels": [], "entities": [{"text": "P", "start_pos": 30, "end_pos": 31, "type": "METRIC", "confidence": 0.9958451390266418}, {"text": "P@10", "start_pos": 47, "end_pos": 51, "type": "METRIC", "confidence": 0.9720659255981445}, {"text": "BLEU", "start_pos": 133, "end_pos": 137, "type": "METRIC", "confidence": 0.9972858428955078}]}, {"text": "This is since unlike BLEU score P@5 and P@10 measure whether the whole elements in reference and hypothesis are matched or not.", "labels": [], "entities": [{"text": "BLEU score P@5", "start_pos": 21, "end_pos": 35, "type": "METRIC", "confidence": 0.9255629777908325}]}, {"text": "We noticed that our submission included a lot of: Results for EN-FR query task.", "labels": [], "entities": [{"text": "EN-FR query task", "start_pos": 62, "end_pos": 78, "type": "TASK", "confidence": 0.5862352848052979}]}, {"text": "near miss sentences only in terms of capitalization: \"abnominal pain and Helicobacter pylori and cancer\" (reference) and \"abnominal pain and helicobacter pylori and cancer\" (submission).", "labels": [], "entities": []}, {"text": "These are counted as incorrect in terms of P@5 and P@10. 3 Noted that after submission we obtained the revised score of P@5 of 0.560 and P@10 of 0.560 with the same method but with 2-best lists which handles the capitalization varieties.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results for FR-EN query task.", "labels": [], "entities": [{"text": "FR-EN query task", "start_pos": 22, "end_pos": 38, "type": "TASK", "confidence": 0.5798953572909037}]}, {"text": " Table 3: Results for EN-FR query task.", "labels": [], "entities": [{"text": "EN-FR query task", "start_pos": 22, "end_pos": 38, "type": "TASK", "confidence": 0.639684796333313}]}, {"text": " Table 4: Results for CLIR task.", "labels": [], "entities": [{"text": "CLIR task", "start_pos": 22, "end_pos": 31, "type": "TASK", "confidence": 0.6984223127365112}]}]}