{"title": [{"text": "Efforts on Machine Learning over Human-mediated Translation Edit Rate", "labels": [], "entities": [{"text": "Human-mediated Translation Edit", "start_pos": 33, "end_pos": 64, "type": "TASK", "confidence": 0.7453901569048563}]}], "abstractContent": [{"text": "In this paper we describe experiments on predicting HTER, as part of our submission in the Shared Task on Quality Estimation , in the frame of the 9th Workshop on Statistical Machine Translation.", "labels": [], "entities": [{"text": "predicting HTER", "start_pos": 41, "end_pos": 56, "type": "TASK", "confidence": 0.829991489648819}, {"text": "Shared Task on Quality Estimation", "start_pos": 91, "end_pos": 124, "type": "TASK", "confidence": 0.6292996406555176}, {"text": "Statistical Machine Translation", "start_pos": 163, "end_pos": 194, "type": "TASK", "confidence": 0.7173279325167338}]}, {"text": "In our experiment we check whether it is possible to achieve better HTER prediction by training four individual regression models for each one of the edit types (deletions, insertions, substitutions, shifts), however no improvements were yielded.", "labels": [], "entities": [{"text": "HTER prediction", "start_pos": 68, "end_pos": 83, "type": "TASK", "confidence": 0.9746495187282562}]}, {"text": "We also had no improvements when investigating the possibility of adding more data from other non-minimally post-edited and freely translated datasets.", "labels": [], "entities": []}, {"text": "Best HTER prediction was achieved by adding dedupli-cated WMT13 data and additional features such as (a) rule-based language corrections (language tool) (b) PCFG parsing statistics and count of tree labels (c) position statistics of parsing labels (d) position statistics of tri-grams with low probability.", "labels": [], "entities": [{"text": "HTER prediction", "start_pos": 5, "end_pos": 20, "type": "TASK", "confidence": 0.8386403918266296}, {"text": "WMT13 data", "start_pos": 58, "end_pos": 68, "type": "DATASET", "confidence": 0.9154218435287476}, {"text": "rule-based language corrections", "start_pos": 105, "end_pos": 136, "type": "TASK", "confidence": 0.5946801503499349}, {"text": "PCFG parsing", "start_pos": 157, "end_pos": 169, "type": "TASK", "confidence": 0.6571201086044312}]}], "introductionContent": [{"text": "As Machine Translation (MT) gets integrated into regular translation workflows, its use as base for post-editing is radically increased.", "labels": [], "entities": [{"text": "Machine Translation (MT)", "start_pos": 3, "end_pos": 27, "type": "TASK", "confidence": 0.8420451402664184}]}, {"text": "As a result, there is a great demand for methods that can automatically assess the MT outcome and ensure that it is useful for the translator and can lead to more productive translation work.", "labels": [], "entities": [{"text": "MT", "start_pos": 83, "end_pos": 85, "type": "TASK", "confidence": 0.9885932207107544}]}, {"text": "Although many agree that the quality of the MT output itself is not adequate for the professional standards, there has not yet been a widelyaccepted way to measure its quality on par with human translations.", "labels": [], "entities": [{"text": "MT", "start_pos": 44, "end_pos": 46, "type": "TASK", "confidence": 0.9745751619338989}]}, {"text": "One such metric, the Human Translation Edit Rate (HTER)), is the focus of the current submission.", "labels": [], "entities": [{"text": "Human Translation Edit Rate (HTER))", "start_pos": 21, "end_pos": 56, "type": "METRIC", "confidence": 0.7894304394721985}]}, {"text": "HTER is highly relevant to the need of adapting MT to the needs of translators, as it aims to measure how far it is from an acceptable equivalent translation done by humans.", "labels": [], "entities": [{"text": "MT", "start_pos": 48, "end_pos": 50, "type": "TASK", "confidence": 0.8786043524742126}]}, {"text": "HTER is used herein the frame of Quality Estimation, i.e. having the goal of being able to predict the post-editing effort in areal case environment, right before the translation is given to the user, without real access to the correct translation.", "labels": [], "entities": [{"text": "HTER", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.43579167127609253}]}, {"text": "For this purpose the text of the source and the produced translation is analyzed by automatic tools in order to infer indications (numerical features) that maybe relevant to the quality of the translation.", "labels": [], "entities": []}, {"text": "These features are used in a statistical model whose parameters are estimated with common supervised Machine Learning techniques.", "labels": [], "entities": []}, {"text": "This work presents an extensive search over various set-ups and parameters for such techniques, aiming to build a model that better predicts HTER over the data of the Shared Task of the 9th Workshop on Statistical Machine Translation.", "labels": [], "entities": [{"text": "Shared Task of the 9th Workshop on Statistical Machine Translation", "start_pos": 167, "end_pos": 233, "type": "TASK", "confidence": 0.5026944756507874}]}, {"text": "2 New approaches being tested 2.1 Break HTER apart HTER is a complex metric, in the sense that it is calculated as a linear function over specific types of edit distance.", "labels": [], "entities": [{"text": "Break HTER apart HTER", "start_pos": 34, "end_pos": 55, "type": "METRIC", "confidence": 0.8077778667211533}]}, {"text": "The official algorithm performs a comparison between the MT output and the corrected version of this output by a human translator, who performed the minimum number of changes.", "labels": [], "entities": [{"text": "MT", "start_pos": 57, "end_pos": 59, "type": "TASK", "confidence": 0.9156380295753479}]}, {"text": "The comparison results in counting the number of insertions, deletions, substitutions and shifts (e.g. reordering).", "labels": [], "entities": []}, {"text": "The final HTER score is the total number of edits divided by the number of reference words.", "labels": [], "entities": [{"text": "HTER score", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9292062819004059}]}, {"text": "HTER = #insertions + #dels + #subs + #shifts #reference words We notice that the metric is clearly based on four edit types that are seemingly independent of each other.", "labels": [], "entities": [{"text": "HTER", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9245194792747498}]}, {"text": "This poses the question whether the existing approach of learning the entire metric altogether introduces way too much complexity in the machine learning process.", "labels": [], "entities": []}, {"text": "Instead, we test the hypothesis that it is more effective to build a separate model for each error type and then put the output of each model on the overall HTER fraction shown above.", "labels": [], "entities": []}, {"text": "Following this idea, we score the given translations again in order to produce all four HTER factors (insertions, deletions, substitutions and shifts) and we train four regression models accordingly.", "labels": [], "entities": []}, {"text": "This way, each model can be optimized separately, in order to better fit the particular error type, unaffected by the noise that other error types may infer.", "labels": [], "entities": []}], "datasetContent": [{"text": "All specific model parameters were tested with cross validation with 10 equal folds on the training data.", "labels": [], "entities": []}, {"text": "Cross validation is useful as it reduces the possibility of overfitting, yet using the entire amount of data.", "labels": [], "entities": [{"text": "Cross validation", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.7442362308502197}]}, {"text": "The regression task is evaluated in terms of Mean Average Error (MAE).", "labels": [], "entities": [{"text": "Mean Average Error (MAE)", "start_pos": 45, "end_pos": 69, "type": "METRIC", "confidence": 0.9749480883280436}]}, {"text": "The open source language tool 1 is used to annotate source and target sentences with automatically detected monolingual error tags.", "labels": [], "entities": []}, {"text": "Language model features are computed with the SRILM toolkit) with an order of 5, based on monolingual training material from Europarl v7.0 ( trained over an English and a Spanish treebank).", "labels": [], "entities": [{"text": "SRILM toolkit", "start_pos": 46, "end_pos": 59, "type": "DATASET", "confidence": 0.8532353043556213}, {"text": "Europarl", "start_pos": 125, "end_pos": 133, "type": "DATASET", "confidence": 0.9713156223297119}]}, {"text": "2 Baseline features are extracted using Quest and HTER edits and scores are recalculated by modifying the original TERp code.", "labels": [], "entities": [{"text": "HTER", "start_pos": 50, "end_pos": 54, "type": "DATASET", "confidence": 0.5720723271369934}, {"text": "TERp code", "start_pos": 115, "end_pos": 124, "type": "DATASET", "confidence": 0.7578219473361969}]}, {"text": "The annotation process is organised with the Ruffus library and the learning algorithms are executed using the Scikit Learn toolkit (Pedregosa et al., 2011).", "labels": [], "entities": [{"text": "Ruffus library", "start_pos": 45, "end_pos": 59, "type": "DATASET", "confidence": 0.9074565768241882}]}], "tableCaptions": [{"text": " Table 2: Comparing models built with several dif- ferent feature sets, including various combinations  of the features described in section 3.2. All models  trained on combination of WMT14 and WMT13  data", "labels": [], "entities": [{"text": "WMT14", "start_pos": 184, "end_pos": 189, "type": "DATASET", "confidence": 0.9444906711578369}, {"text": "WMT13", "start_pos": 194, "end_pos": 199, "type": "DATASET", "confidence": 0.9025185704231262}]}, {"text": " Table 3: The combination of 4 different estima- tors (combined) does not bring any improvement,  when compared to the single HTER estimator.  Models trained on both WMT14 and WMT13 data", "labels": [], "entities": [{"text": "WMT14", "start_pos": 166, "end_pos": 171, "type": "DATASET", "confidence": 0.9661209583282471}, {"text": "WMT13 data", "start_pos": 176, "end_pos": 186, "type": "DATASET", "confidence": 0.9388949871063232}]}]}