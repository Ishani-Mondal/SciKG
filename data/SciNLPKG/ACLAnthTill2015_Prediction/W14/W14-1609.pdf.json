{"title": [{"text": "Lexicon Infused Phrase Embeddings for Named Entity Resolution", "labels": [], "entities": []}], "abstractContent": [{"text": "Most state-of-the-art approaches for named-entity recognition (NER) use semi supervised information in the form of word clusters and lexicons.", "labels": [], "entities": [{"text": "named-entity recognition (NER)", "start_pos": 37, "end_pos": 67, "type": "TASK", "confidence": 0.8610923647880554}]}, {"text": "Recently neural network-based language models have been explored, as they as a byprod-uct generate highly informative vector representations for words, known as word embeddings.", "labels": [], "entities": []}, {"text": "In this paper we present two contributions: anew form of learning word embeddings that can leverage information from relevant lexicons to improve the representations, and the first system to use neural word embeddings to achieve state-of-the-art results on named-entity recognition in both CoNLL and Ontonotes NER.", "labels": [], "entities": [{"text": "named-entity recognition", "start_pos": 257, "end_pos": 281, "type": "TASK", "confidence": 0.7242133617401123}, {"text": "CoNLL", "start_pos": 290, "end_pos": 295, "type": "DATASET", "confidence": 0.8959653377532959}, {"text": "Ontonotes NER", "start_pos": 300, "end_pos": 313, "type": "DATASET", "confidence": 0.9060202538967133}]}, {"text": "Our system achieves an F1 score of 90.90 on the test set for CoNLL 2003-significantly better than any previous system trained on public data, and matching a system employing massive private industrial query-log data.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9888212978839874}, {"text": "CoNLL 2003-significantly", "start_pos": 61, "end_pos": 85, "type": "DATASET", "confidence": 0.8546004891395569}]}], "introductionContent": [{"text": "In many natural language processing tasks, such as named-entity recognition or coreference resolution, syntax alone is not enough to build a high performance system; some external source of information is required.", "labels": [], "entities": [{"text": "named-entity recognition", "start_pos": 51, "end_pos": 75, "type": "TASK", "confidence": 0.7643428146839142}, {"text": "coreference resolution", "start_pos": 79, "end_pos": 101, "type": "TASK", "confidence": 0.9415504932403564}]}, {"text": "In most state-of-the-art systems for named-entity recognition (NER) this knowledge comes in two forms: domain-specific lexicons (lists of word types related to the desired named entity types) and word representations (either clusterings or vectorial representations of word types which capture some of their syntactic and semantic behavior and allow generalizing to unseen word types).", "labels": [], "entities": [{"text": "named-entity recognition (NER)", "start_pos": 37, "end_pos": 67, "type": "TASK", "confidence": 0.8625357985496521}]}, {"text": "Current state-of-the-art named entity recognition systems use Brown clusters as the form of word representation, or other cluster-based representations computed from private data (.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 25, "end_pos": 49, "type": "TASK", "confidence": 0.6597905258337656}]}, {"text": "While very attractive due to their simplicity, generality, and hierarchical structure, Brown clusters are limited because the computational complexity of fitting a model scales quadratically with the number of words in the corpus, or the number of \"base clusters\" in some efficient implementations, making it infeasible to train it on large corpora or with millions of word types.", "labels": [], "entities": []}, {"text": "Although some attempts have been made to train named-entity recognition systems with other forms of word representations, most notably those obtained from training neural language models (, these systems have historically underperformed simple applications of Brown clusters.", "labels": [], "entities": [{"text": "named-entity recognition", "start_pos": 47, "end_pos": 71, "type": "TASK", "confidence": 0.7174464762210846}]}, {"text": "A disadvantage of neural language models is that, while they are inherently more scalable than Brown clusters, training large neural networks is still often expensive; for example, report that some models took multiple days or weeks to produce acceptable representations.", "labels": [], "entities": []}, {"text": "Moreover, language embeddings learned from neural networks tend to behave in a \"nonlinear\" fashion, as they are trained to encourage a many-layered neural network to assign high probability to the data.", "labels": [], "entities": []}, {"text": "These neural networks can detect nonlinear relationships between the embeddings, which is not possible in a log-linear model such as a conditional random field, and therefore limiting how much information from the embeddings can be actually leveraged.", "labels": [], "entities": []}, {"text": "Recently Mikolov et al () proposed two simple loglinear language models, the CBOW model and the Skip-Gram model, that are simplifications of neural language models, and which can be very efficiently trained on large amounts of data.", "labels": [], "entities": []}, {"text": "For example it is possible to train a Skip-gram model over more than a billion tokens with a single machine in less than half a day.", "labels": [], "entities": []}, {"text": "These embeddings can also be trained on phrases instead of individual word types, allowing for fine granularity of meaning.", "labels": [], "entities": []}, {"text": "In this paper we make the following contributions.", "labels": [], "entities": []}, {"text": "(1) We show how to extend the Skip-Gram language model by injecting supervisory training signal from a collection of curated lexiconseffectively encouraging training to learn similar embeddings for phrases which occur in the same lexicons.", "labels": [], "entities": []}, {"text": "(2) We demonstrate that this method outperforms a simple application of the SkipGram model on the semantic similarity task on which it was originally tested.", "labels": [], "entities": []}, {"text": "(3) We show that a linear-chain CRF is able to successfully use these log-linearly-trained embeddings better than the other neural-network-trained embeddings.", "labels": [], "entities": []}, {"text": "(4) We show that lexicon-infused embeddings let us easily build anew highest-performing named entity recognition system on CoNLL 2003 data which is trained using only publicly available data.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 88, "end_pos": 112, "type": "TASK", "confidence": 0.6541154285271963}, {"text": "CoNLL 2003 data", "start_pos": 123, "end_pos": 138, "type": "DATASET", "confidence": 0.9717844128608704}]}, {"text": "(5) We also present results on the relatively understudied Ontonotes NER task), where we show that our embeddings outperform Brown clusters.", "labels": [], "entities": [{"text": "Ontonotes NER task", "start_pos": 59, "end_pos": 77, "type": "DATASET", "confidence": 0.8222083250681559}]}], "datasetContent": [{"text": "Our phrase embeddings are learned on the combination of English Wikipedia and the RCV1 Corpus ().", "labels": [], "entities": [{"text": "English Wikipedia", "start_pos": 56, "end_pos": 73, "type": "DATASET", "confidence": 0.9405297935009003}, {"text": "RCV1 Corpus", "start_pos": 82, "end_pos": 93, "type": "DATASET", "confidence": 0.9568964838981628}]}, {"text": "Wikipedia contains 8M articles, and RCV1 contains 946K.", "labels": [], "entities": []}, {"text": "To get candidate phrases we first select bigrams which have a pointwise mutual information score larger than 1000.", "labels": [], "entities": []}, {"text": "We discard bigrams with stopwords from a manually selected list.", "labels": [], "entities": []}, {"text": "If two bigrams share a token we add its corresponding trigram to our phrase list.", "labels": [], "entities": []}, {"text": "We further add page titles from the English Wikipedia to the list of candidate phrases, as well as all word types.", "labels": [], "entities": []}, {"text": "We get a total of about 10M phrases.", "labels": [], "entities": []}, {"text": "We restrict the vocabulary to the most frequent 1M phrases.", "labels": [], "entities": []}, {"text": "All our reported experiments are on 50-dimensional embeddings.", "labels": [], "entities": []}, {"text": "Longer embeddings, while performing better on the semantic similarity task, as seen in Mikolov et al (2013a;: Accuracy for Semantic-Syntactic task, when restricted to Top 30K words.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 110, "end_pos": 118, "type": "METRIC", "confidence": 0.9981607794761658}]}, {"text": "Lex-0.01 refers to a model trained with lexicons, where 0.01% of negative examples were used for training.", "labels": [], "entities": [{"text": "Lex-0.01", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9421522617340088}]}, {"text": "2013b), did not perform as well on NER.", "labels": [], "entities": [{"text": "NER", "start_pos": 35, "end_pos": 38, "type": "DATASET", "confidence": 0.9359573125839233}]}, {"text": "To train phrase embeddings, we use a context of length 21.", "labels": [], "entities": []}, {"text": "We use lexicons derived from Wikipedia categories and data from the US Census, totaling K = 22 lexicon classes.", "labels": [], "entities": []}, {"text": "We use a randomly selected 0.01% of negative training examples for lexicons.", "labels": [], "entities": []}, {"text": "We perform two sets of experiments.", "labels": [], "entities": []}, {"text": "First, we validate our lexicon-infused phrase embeddings on a semantic similarity task, similar to Mikolov et al ().", "labels": [], "entities": []}, {"text": "Then we evaluate their utility on two named-entity recognition tasks.", "labels": [], "entities": [{"text": "named-entity recognition tasks", "start_pos": 38, "end_pos": 68, "type": "TASK", "confidence": 0.8015568057696024}]}, {"text": "For the NER Experiments, we use the baseline system as described in Section 2.3.1.", "labels": [], "entities": [{"text": "NER Experiments", "start_pos": 8, "end_pos": 23, "type": "TASK", "confidence": 0.6860904395580292}]}, {"text": "NER systems marked as \"Skip-gram\" consider phrase embeddings; \"LexEmb\" consider lexicon-infused embeddings; \"Brown\" use Brown clusters, and \"Gaz\" use our lexicons as features.", "labels": [], "entities": [{"text": "LexEmb", "start_pos": 63, "end_pos": 69, "type": "DATASET", "confidence": 0.9345093369483948}]}], "tableCaptions": [{"text": " Table 1: Accuracy for Semantic-Syntactic task,  when restricted to Top 30K words. Lex-0.01 refers  to a model trained with lexicons, where 0.01% of  negative examples were used for training.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.996982991695404}]}, {"text": " Table 2: Final NER F1 scores for the CoNLL 2003  shared task. On the top are the systems presented  in this paper, and on the bottom we have base- line systems. The best results within each area are  highlighted in bold. Lin and Wu 2009 use massive  private industrial query-log data in training.", "labels": [], "entities": [{"text": "NER", "start_pos": 16, "end_pos": 19, "type": "TASK", "confidence": 0.7501960396766663}, {"text": "F1", "start_pos": 20, "end_pos": 22, "type": "METRIC", "confidence": 0.5482041239738464}, {"text": "CoNLL 2003  shared task", "start_pos": 38, "end_pos": 61, "type": "DATASET", "confidence": 0.8723440319299698}]}, {"text": " Table 3: Final NER F1 scores for Ontonotes 5.0  dataset. The results in bold face are the best on  each evaluation set.", "labels": [], "entities": [{"text": "Final NER F1", "start_pos": 10, "end_pos": 22, "type": "METRIC", "confidence": 0.6877181927363077}, {"text": "Ontonotes 5.0  dataset", "start_pos": 34, "end_pos": 56, "type": "DATASET", "confidence": 0.8868736028671265}]}]}