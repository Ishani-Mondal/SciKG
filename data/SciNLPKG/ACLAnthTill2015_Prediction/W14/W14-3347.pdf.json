{"title": [{"text": "VERTa participation in the WMT14 Metrics Task", "labels": [], "entities": [{"text": "VERTa", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.6609340906143188}, {"text": "WMT14 Metrics", "start_pos": 27, "end_pos": 40, "type": "DATASET", "confidence": 0.8170202970504761}]}], "abstractContent": [{"text": "In this paper we present VERTa, a linguistically motivated metric that combines linguistic features at different levels.", "labels": [], "entities": [{"text": "VERTa", "start_pos": 25, "end_pos": 30, "type": "METRIC", "confidence": 0.9612765908241272}]}, {"text": "We provide the linguistic motivation on which the metric is based, as well as describe the different modules in VERTa and how they are combined.", "labels": [], "entities": [{"text": "VERTa", "start_pos": 112, "end_pos": 117, "type": "DATASET", "confidence": 0.9253683090209961}]}, {"text": "Finally, we describe the two versions of VERTa, VERTa-EQ and VERTa-W, sent to WMT14 and report results obtained in the experiments conducted with the WMT12 and WMT13 data into English.", "labels": [], "entities": [{"text": "VERTa", "start_pos": 41, "end_pos": 46, "type": "METRIC", "confidence": 0.7639766931533813}, {"text": "VERTa-EQ", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.7462301254272461}, {"text": "VERTa-W", "start_pos": 61, "end_pos": 68, "type": "METRIC", "confidence": 0.6230530738830566}, {"text": "WMT14", "start_pos": 78, "end_pos": 83, "type": "DATASET", "confidence": 0.9769914746284485}, {"text": "WMT12", "start_pos": 150, "end_pos": 155, "type": "DATASET", "confidence": 0.9463984370231628}, {"text": "WMT13 data", "start_pos": 160, "end_pos": 170, "type": "DATASET", "confidence": 0.844069629907608}]}], "introductionContent": [{"text": "In the Machine Translation (MT) process, the evaluation of MT systems plays a key role both in their development and improvement.", "labels": [], "entities": [{"text": "Machine Translation (MT) process", "start_pos": 7, "end_pos": 39, "type": "TASK", "confidence": 0.8668657938639323}]}, {"text": "From the MT metrics that have been developed during the last decades, BLEU () is one of the most well-known and widely used, since it is fast and easy to use.", "labels": [], "entities": [{"text": "MT", "start_pos": 9, "end_pos": 11, "type": "TASK", "confidence": 0.9250061511993408}, {"text": "BLEU", "start_pos": 70, "end_pos": 74, "type": "METRIC", "confidence": 0.9984143972396851}]}, {"text": "Nonetheless, researchers such as) and) have claimed its weaknesses regarding translation quality and its tendency to favour statistically-based MT systems.", "labels": [], "entities": [{"text": "translation", "start_pos": 77, "end_pos": 88, "type": "TASK", "confidence": 0.9594622254371643}, {"text": "MT", "start_pos": 144, "end_pos": 146, "type": "TASK", "confidence": 0.9544203877449036}]}, {"text": "As a consequence, other more complex metrics that use linguistic information have been developed.", "labels": [], "entities": []}, {"text": "Some use linguistic information at lexical level, such as METEOR (; others rely on syntactic information, either using constituent () or dependency analysis; others use more complex information such as semantic roles (.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 58, "end_pos": 64, "type": "METRIC", "confidence": 0.8974649906158447}]}, {"text": "All these metrics focus on partial aspects of language; however, other researchers have tried to combine information at different linguistic levels in order to follow a more holistic approach.", "labels": [], "entities": []}, {"text": "Some of these metrics follow a machine-learning approach (), others combine a wide variety of metrics in a simple and straightforward way).", "labels": [], "entities": []}, {"text": "However, very little research has been performed on the impact of the linguistic features used and how to combine this information from a linguistic point of view.", "labels": [], "entities": []}, {"text": "Hence, our proposal is a linguistically-based metric, VERTa (, which uses a wide variety of linguistic features at different levels, and aims at combining them in order to provide a wider and more accurate coverage than those metrics working at a specific linguistic level.", "labels": [], "entities": [{"text": "VERTa", "start_pos": 54, "end_pos": 59, "type": "METRIC", "confidence": 0.6653488278388977}]}, {"text": "In this paper we provide a description of the linguistic information used in VERTa, the different modules that form VERTa and how they are combined according to the language evaluated and the type of evaluation performed.", "labels": [], "entities": [{"text": "VERTa", "start_pos": 77, "end_pos": 82, "type": "DATASET", "confidence": 0.8959177732467651}, {"text": "VERTa", "start_pos": 116, "end_pos": 121, "type": "DATASET", "confidence": 0.9086526036262512}]}, {"text": "Moreover, the two versions of VERTa participating in WMT14, VERTa-EQ and VERTa-W are described.", "labels": [], "entities": [{"text": "WMT14", "start_pos": 53, "end_pos": 58, "type": "TASK", "confidence": 0.5065164566040039}, {"text": "VERTa-EQ", "start_pos": 60, "end_pos": 68, "type": "DATASET", "confidence": 0.7078537940979004}, {"text": "VERTa-W", "start_pos": 73, "end_pos": 80, "type": "DATASET", "confidence": 0.7593880295753479}]}, {"text": "Finally, for the sake of comparison, we use the data available in WMT12 and WMT13 to compare both versions to the metrics participating in those shared tasks.", "labels": [], "entities": [{"text": "WMT12", "start_pos": 66, "end_pos": 71, "type": "DATASET", "confidence": 0.9802040457725525}, {"text": "WMT13", "start_pos": 76, "end_pos": 81, "type": "DATASET", "confidence": 0.9373606443405151}]}], "datasetContent": [{"text": "Experiments were carried out on WMT data, specifically on WMT12 and WMT13 data, all languages into English.", "labels": [], "entities": [{"text": "WMT data", "start_pos": 32, "end_pos": 40, "type": "DATASET", "confidence": 0.8916497230529785}, {"text": "WMT12", "start_pos": 58, "end_pos": 63, "type": "DATASET", "confidence": 0.9839124083518982}, {"text": "WMT13 data", "start_pos": 68, "end_pos": 78, "type": "DATASET", "confidence": 0.933373749256134}]}, {"text": "Languages \"all\" include French, German, Spanish and Czech for WMT12 and French, German, Spanish, Czech and Russian for WMT13.", "labels": [], "entities": [{"text": "WMT12", "start_pos": 62, "end_pos": 67, "type": "DATASET", "confidence": 0.9512398838996887}, {"text": "WMT13", "start_pos": 119, "end_pos": 124, "type": "DATASET", "confidence": 0.9674656391143799}]}, {"text": "Both segment and system level evaluations were performed.", "labels": [], "entities": []}, {"text": "Evaluation sets provided by WMT organizers were used to calculate both segment and system level correlations.", "labels": [], "entities": [{"text": "WMT organizers", "start_pos": 28, "end_pos": 42, "type": "DATASET", "confidence": 0.6900166124105453}]}, {"text": "Since VERTa has been mainly designed to assess either adequacy or fluency separately, our goal for WMT14 was to find the best combination in order to evaluate whole translation quality.", "labels": [], "entities": [{"text": "VERTa", "start_pos": 6, "end_pos": 11, "type": "DATASET", "confidence": 0.6172170639038086}, {"text": "WMT14", "start_pos": 99, "end_pos": 104, "type": "DATASET", "confidence": 0.6416919827461243}]}, {"text": "Firstly we decided to explore the influence of each module separately.", "labels": [], "entities": []}, {"text": "To this aim, all modules described above, except for the semantics one were used and tested separately.", "labels": [], "entities": []}, {"text": "Secondly, all modules were assigned the same weight and tested in combination (VERTa-EQ).", "labels": [], "entities": [{"text": "VERTa-EQ", "start_pos": 79, "end_pos": 87, "type": "METRIC", "confidence": 0.9863211512565613}]}, {"text": "The reason why the semantics module was disregarded is that it does not usually correlate well with human judgements, as stated above.", "labels": [], "entities": []}, {"text": "Each module was set as follows: Lexical module.", "labels": [], "entities": []}, {"text": "As described above, except for the use of hypernyms/hyponyms matches that were disregarded.", "labels": [], "entities": []}, {"text": "As described above, except for the lemma-PoS match and the hypernyms/hyponyms-PoS match.", "labels": [], "entities": []}, {"text": "As described above, using a 2-gram length.", "labels": [], "entities": []}, {"text": "Finally, we used the module combination aimed at evaluating adequacy, which is mainly based on the dependency and lexical modules, but with a stronger influence of the ngram module in order to control word order (VERTa-W).", "labels": [], "entities": [{"text": "VERTa-W", "start_pos": 213, "end_pos": 220, "type": "METRIC", "confidence": 0.864059567451477}]}, {"text": "Weights were manually assigned, based on results obtained in previous experiments conducted for adequacy and fluency), as follows: Lexical module: 0.41 Morphological module: 0 Dependency module: 0.40 Ngram module: 0.19 Experiments aimed at evaluating the influence of each module (see and show that the dependency module, in the case of WMT12 data, and the lexical module in the case of WMT13 data, are the most effective ones.", "labels": [], "entities": [{"text": "WMT12 data", "start_pos": 337, "end_pos": 347, "type": "DATASET", "confidence": 0.9687475562095642}, {"text": "WMT13 data", "start_pos": 387, "end_pos": 397, "type": "DATASET", "confidence": 0.9752956926822662}]}, {"text": "However, the influence of the ngram module and the morphological module varies depending on the source language.", "labels": [], "entities": []}, {"text": "The fact that the dependency module correlates better with human judgements than others might be due to its flexibility to capture different syntactic constructions that convey the same meaning.", "labels": [], "entities": []}, {"text": "In addition, the good performance of the lexical module is due to the use of lexical semantic relations.", "labels": [], "entities": []}, {"text": "On the other hand, in general the morphological module shows a better performance than the ngram one, which might be due to the type of source languages and the possible translation mistakes.", "labels": [], "entities": []}, {"text": "All source languages are highly-inflected languages and this might cause problems when translating into English, since its inflectional morphology is not as rich as theirs.", "labels": [], "entities": []}, {"text": "As for the low performance of the ngram module in the cs-en (especially, in WMT12 data), it might be due to the fact that Czech word order is unrestricted, whereas English shows a stricter word order and this might cause translation issues.", "labels": [], "entities": [{"text": "WMT12 data", "start_pos": 76, "end_pos": 86, "type": "DATASET", "confidence": 0.9440911114215851}, {"text": "translation", "start_pos": 221, "end_pos": 232, "type": "TASK", "confidence": 0.9546817541122437}]}, {"text": "A longer ngram distance might have been more appropriate to control word order in this case.", "labels": [], "entities": []}, {"text": "Finally, two versions of VERTa were compared: the unweighted combination (VERTa-EQ) and the weighted one (VERTa-W).", "labels": [], "entities": [{"text": "VERTa", "start_pos": 25, "end_pos": 30, "type": "DATASET", "confidence": 0.5639179944992065}, {"text": "VERTa-EQ", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.8261035084724426}, {"text": "VERTa-W", "start_pos": 106, "end_pos": 113, "type": "METRIC", "confidence": 0.6800004243850708}]}, {"text": "These two versions were also compared to some of the best performing metrics in WMT12 (see and) and WMT13 (see and): Spede07-pP, METEOR, SEMPOR and AMBER; SIMPBLEU-RECALL, METEOR and DEPREF-ALIGN 8 ).", "labels": [], "entities": [{"text": "WMT12", "start_pos": 80, "end_pos": 85, "type": "DATASET", "confidence": 0.9534403085708618}, {"text": "WMT13", "start_pos": 100, "end_pos": 105, "type": "DATASET", "confidence": 0.9687023758888245}, {"text": "METEOR", "start_pos": 129, "end_pos": 135, "type": "METRIC", "confidence": 0.8227683305740356}, {"text": "SEMPOR", "start_pos": 137, "end_pos": 143, "type": "METRIC", "confidence": 0.9605255722999573}, {"text": "AMBER", "start_pos": 148, "end_pos": 153, "type": "METRIC", "confidence": 0.8845229744911194}, {"text": "DEPREF-ALIGN", "start_pos": 183, "end_pos": 195, "type": "METRIC", "confidence": 0.7791364192962646}]}, {"text": "As regards WMT12 data at segment level, the unweighted version achieves similar results to those obtained by the best performing metrics.", "labels": [], "entities": [{"text": "WMT12 data", "start_pos": 11, "end_pos": 21, "type": "DATASET", "confidence": 0.7564511001110077}]}, {"text": "On the other hand, VERTa-W's results are slightly worse, especially for fr-en and es-en pairs, which is due to the fact that the morphological module has been disregarded in this ver-sion.", "labels": [], "entities": []}, {"text": "Regarding system level correlation, neither VERTa-EQ nor VERTa-W achieves a high correlation with human judgements.", "labels": [], "entities": [{"text": "VERTa-EQ", "start_pos": 44, "end_pos": 52, "type": "DATASET", "confidence": 0.514985978603363}, {"text": "VERTa-W", "start_pos": 57, "end_pos": 64, "type": "DATASET", "confidence": 0.4969240128993988}]}], "tableCaptions": [{"text": " Table 1. Lexical matches and examples.", "labels": [], "entities": []}, {"text": " Table 4. Segment-level Kendall's tau correla- tion per module with WMT12 data.", "labels": [], "entities": [{"text": "WMT12 data", "start_pos": 68, "end_pos": 78, "type": "DATASET", "confidence": 0.946559339761734}]}, {"text": " Table 5. Segment-level Kendall's tau correla- tion per module with WMT13 data.", "labels": [], "entities": [{"text": "WMT13 data", "start_pos": 68, "end_pos": 78, "type": "DATASET", "confidence": 0.9593636691570282}]}, {"text": " Table 6. Segment-level Kendall's tau correla- tion WMT12.", "labels": [], "entities": [{"text": "Segment-level Kendall's tau correla- tion WMT12", "start_pos": 10, "end_pos": 57, "type": "METRIC", "confidence": 0.5267065204679966}]}, {"text": " Table 7. System-level Spearman's rho correla- tion WMT12.", "labels": [], "entities": [{"text": "Spearman's rho correla- tion WMT12", "start_pos": 23, "end_pos": 57, "type": "DATASET", "confidence": 0.3797220417431423}]}, {"text": " Table 8. Segment-level Kendall's tau correlation WMT13.", "labels": [], "entities": [{"text": "Segment-level Kendall's tau correlation WMT13", "start_pos": 10, "end_pos": 55, "type": "METRIC", "confidence": 0.6696545581022898}]}, {"text": " Table 9. System-level Spearman's rho correlation WMT13.", "labels": [], "entities": [{"text": "Spearman's rho correlation WMT13", "start_pos": 23, "end_pos": 55, "type": "DATASET", "confidence": 0.42592806816101075}]}]}