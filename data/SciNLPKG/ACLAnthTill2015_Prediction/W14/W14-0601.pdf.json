{"title": [{"text": "A New Implementation for Canonical Text Services", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper introduces anew implementation of the Canonical Text Services (CTS) protocol intended to be capable of handling thousands of editions.", "labels": [], "entities": []}, {"text": "CTS was introduced for the Digital Humanities and is based on a hierarchical structuring of texts down to the level of individual words mirroring traditional practices of citing.", "labels": [], "entities": []}, {"text": "The paper gives an overview of CTS for those that are unfamiliar and establishes its place in the Digital Humanities research.", "labels": [], "entities": [{"text": "CTS", "start_pos": 31, "end_pos": 34, "type": "TASK", "confidence": 0.8723705410957336}]}, {"text": "Some existing CTS implementations are discussed and it is explained why there is a need for one that is able to scale to much larger text collections.", "labels": [], "entities": []}, {"text": "Evaluations are given that can be used to illustrate the performance of the new implementation.", "labels": [], "entities": []}], "introductionContent": [{"text": "Canonical Text Services (CTS) 1 is a standard that resulted from research in the Digital Humanities community on citation in a digital context.", "labels": [], "entities": [{"text": "Canonical Text Services (CTS) 1", "start_pos": 0, "end_pos": 31, "type": "DATASET", "confidence": 0.5305331902844566}]}, {"text": "It consists of two parts: an URN scheme that is used to express citations and a protocol for the interaction of a client and a server to identify text passages and retrieve them.", "labels": [], "entities": []}, {"text": "CTS is an attempt to formalize citation practices which allow fora persistent identification of text passages and citations which express an ontology of texts as well as links between texts.", "labels": [], "entities": []}, {"text": "The same citation scheme can be used across different versions of a text, even across language borders.", "labels": [], "entities": []}, {"text": "All these properties make CTS attractive as an approach to the presentation of large, structured collections of texts.", "labels": [], "entities": [{"text": "CTS", "start_pos": 26, "end_pos": 29, "type": "TASK", "confidence": 0.9690552949905396}, {"text": "presentation of large, structured collections of texts", "start_pos": 63, "end_pos": 117, "type": "TASK", "confidence": 0.7181528806686401}]}, {"text": "The framework will have little impact however as long as there is no implementation that can scale to the amount of texts currently available for Digital Humanities research and still perform at a level that makes automatic processing of texts attractive.", "labels": [], "entities": []}, {"text": "Therefore the implementation of the scheme presented here allows for large repositories without becoming infeasibly slow.", "labels": [], "entities": []}], "datasetContent": [{"text": "Here we want to show that our implementation is able to scale to the large amounts of data potentially available to Digital Humanities researchers today and that it can handle the large amounts of data potentially generated by cooperative editing.", "labels": [], "entities": []}, {"text": "In order to do this we designed tests that can be used to access the performance of our Canonical Text Services implementation.", "labels": [], "entities": [{"text": "Canonical Text Services implementation", "start_pos": 88, "end_pos": 126, "type": "DATASET", "confidence": 0.7189896255731583}]}, {"text": "The following Tests were used: 1.", "labels": [], "entities": []}, {"text": "retrieve a list of all editions, then get all the valid URNs and the full passage for each edition 2.", "labels": [], "entities": []}, {"text": "collect the first 1000 editions, then obtain the first URN at the lowest level within each edition and its second neighbour, retrieve the first full word for both 17 , finally get the subsection between both words.", "labels": [], "entities": [{"text": "URN", "start_pos": 55, "end_pos": 58, "type": "METRIC", "confidence": 0.9672098159790039}]}, {"text": "Test 1 measures the speed with which the data store can be explored even with a large number of editions and how quickly a passage spanning the whole edition can be constructed.", "labels": [], "entities": []}, {"text": "It can be assumed that the time needed to execute will increase with the amount of editions that are added to a repository and with the length of the individual texts.", "labels": [], "entities": []}, {"text": "Test 2 checks how quickly the implementation can find subsections and is not expected to take substantially longer for our implementation as the number of editions increases.", "labels": [], "entities": []}, {"text": "It is mainly intended to show that behaviour on single texts is not impacted by the number of editions managed and that the construction of larger passages from elementary chunks is handled efficiently.", "labels": [], "entities": []}, {"text": "Both tests were run by using a small seed of data 18 that was copied repeatedly in order to arrive at the number of necessary editions.", "labels": [], "entities": []}, {"text": "The data will be made available.", "labels": [], "entities": []}, {"text": "Our implementation ran on a server with a 2.4 GHz CPU and 1GB of RAM.", "labels": [], "entities": [{"text": "RAM", "start_pos": 65, "end_pos": 68, "type": "METRIC", "confidence": 0.9620664119720459}]}, {"text": "The requests necessary for our tests ran on a different machine in order to factor in the problem of communication.", "labels": [], "entities": []}, {"text": "In future tests it would be possible to distribute the requests between different clients to focus more on this point.", "labels": [], "entities": []}, {"text": "contains the results for Test 1.", "labels": [], "entities": []}, {"text": "The amount of time taken is linear in the number of editions since every new text was generated once.", "labels": [], "entities": []}, {"text": "While the construction of all the texts took several hours for the larger collections, the list of all editions was retrieved within a second or less.", "labels": [], "entities": []}, {"text": "There is a surprising spike that could be due to factors external to our program which could have a strong impact on such comparatively short time measurements.", "labels": [], "entities": []}, {"text": "gives the results for Test 2.", "labels": [], "entities": []}, {"text": "As expected the behaviour is not greatly impacted by the number of editions in the collection.", "labels": [], "entities": []}, {"text": "The variation between the different numbers of editions is within a second for the complete task and the average time needed per retrieval task varies by only ten milliseconds.", "labels": [], "entities": []}, {"text": "A word not containing special characters and longer than 2 characters.", "labels": [], "entities": []}, {"text": "Both measures show a slight increase as the number of editions goes over 3000 but then stabilise.", "labels": [], "entities": []}, {"text": "Overall the experiments show that handling thousands of text is indeed feasible with our implementation on a relatively modest server even for the hardest possible task of reconstructing all the texts in the collection from their smallest parts.", "labels": [], "entities": []}, {"text": "Subtasks that do not require retrieving all the texts show little impact from increasing the number of editions.", "labels": [], "entities": []}], "tableCaptions": []}