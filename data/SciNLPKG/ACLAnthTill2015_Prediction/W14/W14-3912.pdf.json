{"title": [{"text": "The IUCL+ System: Word-Level Language Identification via Extended Markov Models", "labels": [], "entities": [{"text": "Word-Level Language Identification", "start_pos": 18, "end_pos": 52, "type": "TASK", "confidence": 0.6132511794567108}]}], "abstractContent": [{"text": "We describe the IUCL+ system for the shared task of the First Workshop on Computational Approaches to Code Switching (Solorio et al., 2014), in which participants were challenged to label each word in Twitter texts as a named entity or one of two candidate languages.", "labels": [], "entities": [{"text": "First Workshop on Computational Approaches to Code Switching", "start_pos": 56, "end_pos": 116, "type": "TASK", "confidence": 0.5025056935846806}]}, {"text": "Our system combines character n-gram probabilities , lexical probabilities, word label transition probabilities and existing named entity recognition tools within a Markov model framework that weights these components and assigns a label.", "labels": [], "entities": []}, {"text": "Our approach is language-independent, and we submitted results for all data sets (five test sets and three \"surprise\" sets, covering four language pairs), earning the highest accuracy score on the tweet level on two language pairs (Mandarin-English, Arabic-dialects 1 & 2) and one of the surprise sets (Arabic-dialects).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 175, "end_pos": 183, "type": "METRIC", "confidence": 0.9987819790840149}]}], "introductionContent": [{"text": "This shared task challenged participants to perform word level analysis on short, potentially bilingual Twitter and blog texts covering four language pairs: NepaliEnglish, Spanish-English, Mandarin-English and Modern Standard Arabic-Arabic dialects.", "labels": [], "entities": [{"text": "NepaliEnglish", "start_pos": 157, "end_pos": 170, "type": "DATASET", "confidence": 0.8254223465919495}]}, {"text": "Training sets ranging from 1,000 to roughly 11,000 tweets were provided for the language pairs, where the content of the tweets was tokenized and labeled with one of six labels.", "labels": [], "entities": []}, {"text": "The goal of the task is to accurately replicate this annotation automatically on pre-tokenized texts.", "labels": [], "entities": [{"text": "replicate this annotation", "start_pos": 38, "end_pos": 63, "type": "TASK", "confidence": 0.8627047737439474}]}, {"text": "With an inventory of six labels, however, the task is more than a simple binary classification task.", "labels": [], "entities": []}, {"text": "In general, the most common labels observed in the training data are lang1 and lang2, with other (mainly covering punctuation and emoticons) also common.", "labels": [], "entities": []}, {"text": "Named entities (ne) are also frequent, and accounting for them adds a significant complication to the task.", "labels": [], "entities": []}, {"text": "Less common are mixed (to account for words that may e.g., apply L1 morphology to an L2 word), and ambiguous (to cover a word that could exist in either language, e.g., no in the Spanish-English data).", "labels": [], "entities": []}, {"text": "Traditionally, language identification is performed on the document level, i.e., on longer segments of text than what is available in tweets.", "labels": [], "entities": [{"text": "language identification", "start_pos": 15, "end_pos": 38, "type": "TASK", "confidence": 0.7590592801570892}]}, {"text": "These methods are based on variants of character n-grams.", "labels": [], "entities": []}, {"text": "Seminal work in this area is by and.", "labels": [], "entities": []}, {"text": "showed that character n-grams also perform on Twitter messages.", "labels": [], "entities": []}, {"text": "One of a few recent approaches working on individual words is by, who worked on historical data; see also work by and.", "labels": [], "entities": []}, {"text": "Our system is an adaptation of a Markov model, which integrates lexical, character n-gram, and label transition probabilities (all trained on the provided data) in addition to the output of pre-existing NER tools.", "labels": [], "entities": []}, {"text": "All the information sources are weighted in the Markov model.", "labels": [], "entities": []}, {"text": "One advantage of our approach is that it is languageindependent.", "labels": [], "entities": []}, {"text": "We use the exact same architecture for all language pairs, and the only difference for the individual language pairs lies in a manual, non-exhaustive search for the best weights.", "labels": [], "entities": []}, {"text": "Our results show that the approach works well for the one language pair with different writing systems (Mandarin-English) as well as for the most complex language pair, the Arabic set.", "labels": [], "entities": []}, {"text": "In the latter data set, the major difficulty consists in the extreme skewing with an overwhelming dominance of words in Modern Standard Arabic.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Tweet level results in comparison to the system with (next-)highest accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.9970306158065796}]}, {"text": " Table 2: Token level results in comparison to the system with highest accuracy (results for ambiguous and  other are not reported).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 71, "end_pos": 79, "type": "METRIC", "confidence": 0.9985842704772949}]}, {"text": " Table 3: Token level results for the out-of-domain data.", "labels": [], "entities": []}]}