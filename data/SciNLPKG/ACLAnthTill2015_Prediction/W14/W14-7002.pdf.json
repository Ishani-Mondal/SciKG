{"title": [{"text": "Forest-to-String SMT for Asian Language Translation: NAIST at WAT 2014", "labels": [], "entities": [{"text": "SMT", "start_pos": 17, "end_pos": 20, "type": "TASK", "confidence": 0.7425833940505981}, {"text": "Asian Language Translation", "start_pos": 25, "end_pos": 51, "type": "TASK", "confidence": 0.5988652209440867}, {"text": "NAIST at WAT 2014", "start_pos": 53, "end_pos": 70, "type": "DATASET", "confidence": 0.9173928648233414}]}], "abstractContent": [{"text": "This paper describes the Nara Institute of Science and Technology's (NAIST) submission to the 2014 Workshop on Asian Translation's four translation tasks.", "labels": [], "entities": [{"text": "Nara Institute of Science and Technology's (NAIST) submission to the 2014 Workshop on Asian Translation's four translation tasks", "start_pos": 25, "end_pos": 153, "type": "TASK", "confidence": 0.8508008068258112}]}, {"text": "All systems are based on forest-to-string (F2S) translation, in which the input sentence is first parsed using a syntactic parser, then a forest of possible syntactic analyses is translated into the target language.", "labels": [], "entities": [{"text": "forest-to-string (F2S) translation", "start_pos": 25, "end_pos": 59, "type": "TASK", "confidence": 0.6690723121166229}]}, {"text": "In addition to the baseline F2S system, we add rescoring using a recurrent neural network language model (RNNLM), which allows for more fluent output.", "labels": [], "entities": []}, {"text": "The resulting system achieved the highest results in both automatic and manual evaluation for all four of the language pairs targeted by the workshop.", "labels": [], "entities": []}], "introductionContent": [{"text": "The Workshop on Asian Translation (WAT) 2014) included a translation task over four language pairs, all involving translating Japanese (ja), a language with SOV word order, to/from English (en) or Chinese (zh), languages with SVO word order.", "labels": [], "entities": [{"text": "Asian Translation (WAT) 2014)", "start_pos": 16, "end_pos": 45, "type": "TASK", "confidence": 0.8692718999726432}]}, {"text": "Because of this, it can be expected that one of the major challenges facing translation systems in this task is the proper reordering of the words between the source and target languages.", "labels": [], "entities": []}, {"text": "One promising way to tackle the reordering problem is through the use of tree-to-string (T2S) translation, a translation formalism where the source sentence is first parsed using a syntactic parser, then sub-structures of the parse tree are translated into target-side strings ().", "labels": [], "entities": [{"text": "tree-to-string (T2S) translation", "start_pos": 73, "end_pos": 105, "type": "TASK", "confidence": 0.7742120504379273}]}, {"text": "have also demonstrated that forest-to-string (F2S) translation allows for more robust use of source-side syntax by not considering a 1-best parse tree, but a myriad of parse candidates stored efficiently in a packed-forest data structure.", "labels": [], "entities": [{"text": "forest-to-string (F2S) translation", "start_pos": 28, "end_pos": 62, "type": "TASK", "confidence": 0.6735472917556763}]}, {"text": "In our previous work), we have shown that F2S translation is effective for en-ja and ja-en translation, and can outperform alternative methods such as preor post-ordering.", "labels": [], "entities": [{"text": "F2S translation", "start_pos": 42, "end_pos": 57, "type": "TASK", "confidence": 0.6896976828575134}]}, {"text": "Thus, in our WAT submission, we choose this formalism, and specifically it's implementation in the open-source Travatar decoder 1 as the base of our system.", "labels": [], "entities": [{"text": "WAT submission", "start_pos": 13, "end_pos": 27, "type": "TASK", "confidence": 0.873280793428421}]}, {"text": "Another promising development over the past couple years is the use of continuous-space representations of language combined with neuralnetwork-based probabilistic models.", "labels": [], "entities": []}, {"text": "These have been incorporated into translation as either language models (LMs) () or translation models (TMs) (), allowing for large increases in translation accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 157, "end_pos": 165, "type": "METRIC", "confidence": 0.8515724539756775}]}, {"text": "In our submission, we incorporate this continuousspace representation by training a recurrent neural network language model (RNNLM;) and using its scores as a feature in n-best hypothesis rescoring.", "labels": [], "entities": [{"text": "n-best hypothesis rescoring", "start_pos": 170, "end_pos": 197, "type": "TASK", "confidence": 0.6497411926587423}]}, {"text": "We also made a few small improvements to our ja-en system, mainly in an attempt to reduce the number of unknown words.", "labels": [], "entities": []}, {"text": "Specifically, we perform compound splitting ( of unknown words to help reduce the effects of under-segmentation, perform one small word substitution to regularize for the peculiarities of the development/test data, and add large external dictionaries.", "labels": [], "entities": [{"text": "compound splitting", "start_pos": 25, "end_pos": 43, "type": "TASK", "confidence": 0.732355073094368}]}, {"text": "As a result of the incorporation of F2S translation and RNNLMs, we see a large gain inaccuracy over a baseline phrase-based machine translation model.", "labels": [], "entities": [{"text": "F2S translation", "start_pos": 36, "end_pos": 51, "type": "TASK", "confidence": 0.7471010386943817}, {"text": "phrase-based machine translation", "start_pos": 111, "end_pos": 143, "type": "TASK", "confidence": 0.6988000075022379}]}, {"text": "Specifically, we see again in BLEU of 8.21 for en-ja, 5.44 for ja-en, 4.71 for zh-ja, and 2.47 for ja-zh.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 30, "end_pos": 34, "type": "METRIC", "confidence": 0.999193012714386}]}, {"text": "In addition, according to the official automatic evaluation, our system outperformed all other submitted systems in all tracks.", "labels": [], "entities": []}, {"text": "Scripts to largely reproduce our experiments will be released open source.", "labels": [], "entities": []}], "datasetContent": [{"text": "In we show the results for our systems with and without the RNNLM, and tuning with BLEU or BLEU+RIBES.", "labels": [], "entities": [{"text": "RNNLM", "start_pos": 60, "end_pos": 65, "type": "DATASET", "confidence": 0.5489851832389832}, {"text": "BLEU", "start_pos": 83, "end_pos": 87, "type": "METRIC", "confidence": 0.998306393623352}, {"text": "BLEU+RIBES", "start_pos": 91, "end_pos": 101, "type": "METRIC", "confidence": 0.8728125095367432}]}, {"text": "In addition, we show the results fora PBMT system trained using the Moses toolkit (, with the same data as the F2S system and the default settings except fora reordering limit of 18, which gave better results on all language pairs than the default of 6.", "labels": [], "entities": [{"text": "F2S", "start_pos": 111, "end_pos": 114, "type": "DATASET", "confidence": 0.9322206377983093}]}, {"text": "From this table we can first see that the F2S translation greatly outperforms PBMT.", "labels": [], "entities": [{"text": "PBMT", "start_pos": 78, "end_pos": 82, "type": "DATASET", "confidence": 0.8674018979072571}]}, {"text": "The trend is more prominent in the translation to or from English, a result of the fact that the amount of reordering is greater between English and Japanese than between Chinese and Japanese.", "labels": [], "entities": [{"text": "translation to or from English", "start_pos": 35, "end_pos": 65, "type": "TASK", "confidence": 0.845341157913208}]}, {"text": "In addition, we can see that the gain over PBMT is smaller: Overall BLEU and RIBES results fora baseline Moses, and five of our systems without and with the RNNLM rescoring, tuning for BLEU or BLEU+RIBES, and with/without dictionaries.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 68, "end_pos": 72, "type": "METRIC", "confidence": 0.9982726573944092}, {"text": "RIBES", "start_pos": 77, "end_pos": 82, "type": "METRIC", "confidence": 0.9405732750892639}, {"text": "BLEU", "start_pos": 185, "end_pos": 189, "type": "METRIC", "confidence": 0.9952254295349121}, {"text": "BLEU", "start_pos": 193, "end_pos": 197, "type": "METRIC", "confidence": 0.9509285688400269}]}, {"text": "Bold indicates systems not statistically different from the best system according to bootstrap resampling.", "labels": [], "entities": []}, {"text": "in pairs where the source is Japanese.", "labels": [], "entities": []}, {"text": "We account this to the fact that the syntactic parsing accuracy is lower, partly due to the fact that the training data for the parser is smaller (approximately 6,000 sentences), and partly due to the fact that we have done very little grammar engineering for Japanese, in contrast to the more carefully thought-out phrase structure of the English and Chinese treebanks.", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 37, "end_pos": 54, "type": "TASK", "confidence": 0.6664947122335434}, {"text": "accuracy", "start_pos": 55, "end_pos": 63, "type": "METRIC", "confidence": 0.9018273949623108}]}, {"text": "Next, taking a look at the results with RNNLM, we can see that adding RNNLM helps across all language pairs on the order of 0.7-1.0 BLEU points, with slightly smaller gains for RIBES.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 132, "end_pos": 136, "type": "METRIC", "confidence": 0.998772919178009}]}, {"text": "These consistent gains are in concert with previous results, adding further evidence to the observation that continuous space language models are beneficial for translation.", "labels": [], "entities": [{"text": "translation", "start_pos": 161, "end_pos": 172, "type": "TASK", "confidence": 0.9712021350860596}]}, {"text": "We can also see that adding RIBES to the evaluation function used in parameter optimization leads to a significant increase in RIBES across all data sets.", "labels": [], "entities": [{"text": "parameter optimization", "start_pos": 69, "end_pos": 91, "type": "TASK", "confidence": 0.7605533599853516}, {"text": "RIBES", "start_pos": 127, "end_pos": 132, "type": "METRIC", "confidence": 0.8958290815353394}]}, {"text": "On the other hand, it also leads to a decrease in BLEU for the ja-zh and zh-ja data sets, although no significant decrease is observed in the ja-en and en-ja data sets.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 50, "end_pos": 54, "type": "METRIC", "confidence": 0.9997126460075378}]}, {"text": "It should also be noted that the systems tuned for BLEU+RIBES tend to result in significantly shorter translation outputs than other systems.", "labels": [], "entities": [{"text": "BLEU+RIBES", "start_pos": 51, "end_pos": 61, "type": "METRIC", "confidence": 0.7460763851801554}]}, {"text": "shows the average length of sentences for each of the systems (using RNNLM in all cases).", "labels": [], "entities": [{"text": "RNNLM", "start_pos": 69, "end_pos": 74, "type": "DATASET", "confidence": 0.8049498796463013}]}, {"text": "From this, we can see that for all language pairs except ja-en, BLEU-tuned systems tend to largely match the length of the reference hypotheses, while the BLEU+RIBES tuned systems are significantly shorter.", "labels": [], "entities": [{"text": "BLEU-tuned", "start_pos": 64, "end_pos": 74, "type": "METRIC", "confidence": 0.9943113327026367}, {"text": "BLEU", "start_pos": 155, "end_pos": 159, "type": "METRIC", "confidence": 0.9785754680633545}]}], "tableCaptions": [{"text": " Table 1: Overall BLEU and RIBES results for a baseline Moses, and five of our systems without and with  the RNNLM rescoring, tuning for BLEU or BLEU+RIBES, and with/without dictionaries. Bold indi- cates systems not statistically different from the best system according to bootstrap resampling", "labels": [], "entities": [{"text": "BLEU", "start_pos": 18, "end_pos": 22, "type": "METRIC", "confidence": 0.9981592297554016}, {"text": "RIBES", "start_pos": 27, "end_pos": 32, "type": "METRIC", "confidence": 0.8724240660667419}, {"text": "BLEU", "start_pos": 137, "end_pos": 141, "type": "METRIC", "confidence": 0.997468113899231}, {"text": "BLEU", "start_pos": 145, "end_pos": 149, "type": "METRIC", "confidence": 0.9539851546287537}]}, {"text": " Table 2: The average number of words per sen- tence according to different tuning objectives, as  well as for the reference.", "labels": [], "entities": []}, {"text": " Table 3: BLEU, RIBES, and HUMAN evaluation according to the official evaluation results. We also  show the best competing systems other than ours according to each evaluation metric. Bold indicates  systems within 0.3 of the best system for BLEU and RIBES, and systems that do not show a significant  decrease from the best system according to Student's t-test for HUMAN (p < 0.05).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9976521134376526}, {"text": "BLEU", "start_pos": 242, "end_pos": 246, "type": "METRIC", "confidence": 0.9934972524642944}]}]}