{"title": [{"text": "InproTK S : A Toolkit for Incremental Situated Processing", "labels": [], "entities": [{"text": "Incremental Situated Processing", "start_pos": 26, "end_pos": 57, "type": "TASK", "confidence": 0.6960430145263672}]}], "abstractContent": [], "introductionContent": [{"text": "Realising incremental processing of speech inand output -a prerequisite to interpretation and possibly production of speech concurrently with the other dialogue participant -requires some fundamental changes in the way that components of dialogue systems operate and communicate with each other (.", "labels": [], "entities": []}, {"text": "Processing situated communication, that is, communication that requires reference to the physical setting in which it occurs, makes it necessary to accept (and fuse) information from various different sensors, each tracking different aspects of the physical situation, making the system multimodal (.", "labels": [], "entities": [{"text": "Processing situated communication", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.6233837703863779}]}, {"text": "Incremental situated processing brings together these requirements.", "labels": [], "entities": []}, {"text": "In this paper, we present a collection of extensions to the incremental processing toolkit INPROTK () that make it capable of processing situated communication in an incremental fashion: we have developed a general architecture for plugging in multimodal sensors whith we denote INPROTK S , which includes instantiations for motion capture (via e.g. via Microsoft Kinect and Leap Motion) and eye tracking (Seeingmachines FaceLAB).", "labels": [], "entities": [{"text": "motion capture", "start_pos": 325, "end_pos": 339, "type": "TASK", "confidence": 0.7202350497245789}, {"text": "eye tracking", "start_pos": 392, "end_pos": 404, "type": "TASK", "confidence": 0.7968328893184662}, {"text": "Seeingmachines FaceLAB", "start_pos": 406, "end_pos": 428, "type": "DATASET", "confidence": 0.8134346306324005}]}, {"text": "We also describe anew module we built that makes it possible to perform (large vocabulary, open domain) speech recognition via the Google Web Speech API.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 104, "end_pos": 122, "type": "TASK", "confidence": 0.6915685385465622}]}, {"text": "We describe these components individually and give as use-cases in a driving simulation setup, as well as real-time gaze and gesture recognition.", "labels": [], "entities": [{"text": "gesture recognition", "start_pos": 125, "end_pos": 144, "type": "TASK", "confidence": 0.796509325504303}]}, {"text": "In the next section, we will give some background on incremental processing, then describe the new methods of plugging in multimodal sensors, specifically using XML-RPC, the Robotics Service Bus, and the InstantReality framework.", "labels": [], "entities": []}, {"text": "We then explain how we incorporated the Google Web Speech API into InproTK, offer some use cases for these new modules, and conclude.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}