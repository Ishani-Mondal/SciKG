{"title": [{"text": "The Role of Polarity in Inferring Acceptance and Rejection in Dialogue", "labels": [], "entities": [{"text": "Inferring Acceptance and Rejection in Dialogue", "start_pos": 24, "end_pos": 70, "type": "TASK", "confidence": 0.8048816621303558}]}], "abstractContent": [{"text": "We study the role that logical polarity plays in determining the rejection or acceptance function of an utterance in dialogue.", "labels": [], "entities": [{"text": "rejection or acceptance function of an utterance in dialogue", "start_pos": 65, "end_pos": 125, "type": "TASK", "confidence": 0.7224601440959506}]}, {"text": "We develop a model inspired by recent work on the semantics of negation and polarity particles and test it on annotated data from two spoken dialogue corpora: the Switchboard Corpus and the AMI Meeting Corpus.", "labels": [], "entities": [{"text": "AMI Meeting Corpus", "start_pos": 190, "end_pos": 208, "type": "DATASET", "confidence": 0.9037728508313497}]}, {"text": "Our experiments show that taking into account the relative polarity of a proposal under discussion and of its response greatly helps to distinguish rejections from acceptances in both corpora.", "labels": [], "entities": []}], "introductionContent": [{"text": "In order to establish and maintain coherence, dialogue participants need to keep track of the information they jointly take for granted-their common ground.", "labels": [], "entities": []}, {"text": "As a dialogue progresses, the common ground typically evolves.", "labels": [], "entities": []}, {"text": "New information becomes shared as the interlocutors exchange moves (such as assertions, questions, acceptances, and rejections) through the collaborative process of grounding.", "labels": [], "entities": []}, {"text": "To keep track of the common ground, speakers must identify which information is accepted or rejected by their addressees.", "labels": [], "entities": []}, {"text": "The basic idea is simple: If a proposal is rejected, its content does not enter the common ground, while if it is accepted, its content does become common belief.", "labels": [], "entities": []}, {"text": "Yet, determining whether a response to a move counts as an acceptance or a rejection is far from trivial.", "labels": [], "entities": []}, {"text": "In many cases, the surface form of an utterance is not explicit enough to determine its acceptance or rejection force and inference is required.", "labels": [], "entities": []}, {"text": "For instance, B's utterance in (1), extracted from the AMI Meeting Corpus, exemplifies what Walker (1996) calls implicature rejection (the rejection arises from an inferred scalar implicature: \"normal\" implicates \"not interesting\"; see also).", "labels": [], "entities": [{"text": "AMI Meeting Corpus", "start_pos": 55, "end_pos": 73, "type": "DATASET", "confidence": 0.9494148095448812}]}, {"text": "(1) A: This is a very interesting design.", "labels": [], "entities": [{"text": "A", "start_pos": 4, "end_pos": 5, "type": "METRIC", "confidence": 0.9913743138313293}]}, {"text": "B: It's just the same as normal.", "labels": [], "entities": [{"text": "B", "start_pos": 0, "end_pos": 1, "type": "METRIC", "confidence": 0.9552465081214905}, {"text": "It", "start_pos": 3, "end_pos": 5, "type": "METRIC", "confidence": 0.7664408087730408}]}, {"text": "The goal of this paper is to investigate the role of logical polarity in distinguishing rejections from acceptances.", "labels": [], "entities": [{"text": "distinguishing rejections from acceptances", "start_pos": 73, "end_pos": 115, "type": "TASK", "confidence": 0.7725412249565125}]}, {"text": "Consider the following dialogue excerpts, again from AMI, where the same utterance form (\"Yes it is\") acts as an acceptance in (2) and as a rejection in (3): (2) A: But it's uh yeah it's uh original idea.", "labels": [], "entities": [{"text": "AMI", "start_pos": 53, "end_pos": 56, "type": "DATASET", "confidence": 0.8686643242835999}]}, {"text": "B: Yes it is.", "labels": [], "entities": [{"text": "B", "start_pos": 0, "end_pos": 1, "type": "METRIC", "confidence": 0.9528499245643616}]}, {"text": "(3) A: the shape of a banana is not it's not really handy . B: Yes it is.", "labels": [], "entities": []}, {"text": "To determine whether B's utterance in either case above functions as an acceptance or a rejection, it is critical to not only look beyond the utterance itself and take into account the proposal under discussion (A's utterance), but also to specify (a) the polarity (positive vs. negative) of both the proposal and the response, and (b) how these polarities interact to give rise to a particular interpretation.", "labels": [], "entities": []}, {"text": "Our aim in this paper is to develop a model of how logical polarity influences acceptance/rejection interpretation, inspired by recent work on the semantics of negation and polarity particles, and to test it on annotated data from two spoken dialogue corpora: the Switchboard Corpus () and the AMI Meeting Corpus.", "labels": [], "entities": [{"text": "acceptance/rejection interpretation", "start_pos": 79, "end_pos": 114, "type": "TASK", "confidence": 0.7708816677331924}, {"text": "AMI Meeting Corpus", "start_pos": 294, "end_pos": 312, "type": "DATASET", "confidence": 0.9149465362230936}]}, {"text": "In the next section, we give an overview of related computational work on acceptance/rejection detection.", "labels": [], "entities": [{"text": "acceptance/rejection detection", "start_pos": 74, "end_pos": 104, "type": "TASK", "confidence": 0.9504922032356262}]}, {"text": "In Section 3, we first briefly review recent formal semantics approaches to polarity and then present our model of logical polarity in acceptance and rejection moves.", "labels": [], "entities": []}, {"text": "Section 4 describes our experiments: We derive machine learning features from our polarity theory and test them in Switchboard and AMI datasets, achieving competitive F -scores of around 60 on the task of retrieving rejections.", "labels": [], "entities": [{"text": "AMI datasets", "start_pos": 131, "end_pos": 143, "type": "DATASET", "confidence": 0.8688173890113831}, {"text": "F -scores", "start_pos": 167, "end_pos": 176, "type": "METRIC", "confidence": 0.9914907415707906}]}, {"text": "We conclude in Section 5 with a discussion of our results.", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to automatically test the extent to which logical polarity plays a role in determining the function of naturally occurring acceptances and rejections, we conduct machine learning experiments on dialogue corpus data.", "labels": [], "entities": []}, {"text": "We first explain how we create our dataset, then describe how we devise features that encode polarity information, and finally report the results obtained.", "labels": [], "entities": []}, {"text": "We test our model on two different corpora: The Switchboard Corpus (SWB) ( and the AMI Meeting Corpus.", "labels": [], "entities": [{"text": "AMI Meeting Corpus", "start_pos": 83, "end_pos": 101, "type": "DATASET", "confidence": 0.9091793696085612}]}, {"text": "SWD is a collection of around 2400 recorded and transcribed telephone conversations between two dialogue participants.", "labels": [], "entities": []}, {"text": "The speakers are provided with a topic and then converse freely.", "labels": [], "entities": []}, {"text": "In contrast, AMI contains transcriptions from around 100 hours of recorded multiparty conversations amongst four dialogue participants who interact face-to-face in a meeting setting.", "labels": [], "entities": []}, {"text": "The speakers converse freely, but they play roles (such as industrial designer or project manager) in a fictitious design team whose goal is to design a remote control.", "labels": [], "entities": []}, {"text": "Therefore the dialogue is mildly task-oriented.", "labels": [], "entities": []}, {"text": "Both corpora have been annotated with dialogue acts (DAs), albeit with slightly different DA annotation schemes: SWD is annotated with the SWBD-DAMSL tagset (, while AMI uses a coarser-grained tagset but includes relations between some DAs (loosely called adjacency pair annotations).", "labels": [], "entities": [{"text": "SWBD-DAMSL tagset", "start_pos": 139, "end_pos": 156, "type": "DATASET", "confidence": 0.8206267654895782}]}, {"text": "acceptances rejections total P -R SWB 4534 (97%) 145 (3%) 4679 AMI 7405 (91%) 697 (9%) 8102: Class distribution in our datasets.", "labels": [], "entities": [{"text": "P -R SWB 4534", "start_pos": 29, "end_pos": 42, "type": "METRIC", "confidence": 0.6845370650291442}, {"text": "AMI 7405", "start_pos": 63, "end_pos": 71, "type": "DATASET", "confidence": 0.620671272277832}]}, {"text": "We use the DA annotations to extract a dataset of proposal-response (P -R) pairs for each corpus as follows.", "labels": [], "entities": []}, {"text": "To construct the SWB dataset, we extract all utterances u annotated as Agree/Accept or Reject that are turninitial and that are immediately preceded by a turn whose last utterance u 0 is annotated as Statement-non-opinion, Statementopinion or Summarize/Reformulate.", "labels": [], "entities": [{"text": "SWB dataset", "start_pos": 17, "end_pos": 28, "type": "DATASET", "confidence": 0.7479846477508545}]}, {"text": "To construct the AMI dataset, we extract all utterances u annotated as Assessment that are turn initial and that are linked with the relations Support/Positive Assessment or Objection/Negative Assessment to an earlier utterance u 0 that is not annotated as Elicit Inform or Elicit Assessment (i.e., that is not a question).", "labels": [], "entities": [{"text": "AMI dataset", "start_pos": 17, "end_pos": 28, "type": "DATASET", "confidence": 0.9125405251979828}]}, {"text": "In both cases, P corresponds to u 0 and R to the first five words of u.", "labels": [], "entities": []}, {"text": "We consider R an acceptance if u is annotated as Agree/Accept in SWB or as Support/Positive Assessment in AMI, and a rejection if it is annotated as Reject in SWB or as Objection/Negative Assessment in AMI.", "labels": [], "entities": [{"text": "Agree/Accept", "start_pos": 49, "end_pos": 61, "type": "METRIC", "confidence": 0.8005178968111674}, {"text": "AMI", "start_pos": 106, "end_pos": 109, "type": "DATASET", "confidence": 0.8784146904945374}, {"text": "AMI", "start_pos": 202, "end_pos": 205, "type": "DATASET", "confidence": 0.9405457377433777}]}, {"text": "We take the first five words of a turn-initial utterance to be the most relevant ones for conveying acceptance or rejection.", "labels": [], "entities": [{"text": "conveying acceptance or rejection", "start_pos": 90, "end_pos": 123, "type": "TASK", "confidence": 0.8897368907928467}]}, {"text": "This is motivated by the fact that dialogue participants typically provide evidence of understanding-and, by extension, of agreement or disagreement-at the earliest opportunity in order to avoid misunderstandings on what they take to be common ground.", "labels": [], "entities": []}, {"text": "However, when extracting our P -R pairs we retain the entire utterance u (of which R is a prefix) in order to be able to take its length into account in the automatic classification experiment, as explained in the next section.", "labels": [], "entities": [{"text": "automatic classification", "start_pos": 157, "end_pos": 181, "type": "TASK", "confidence": 0.6551379561424255}]}, {"text": "Finally, we observe that in the two corpora all the P -R pairs where R is just a single \"yeah\" are acceptances.", "labels": [], "entities": []}, {"text": "Thus, in the terminology we introduced in Section 3.2, bare \"yeah\" seems to bean absolute response type, whose acceptance function is independent of the polarity of P (in contrast to the relative response types in).", "labels": [], "entities": []}, {"text": "Since identification of these acceptances is trivial, we discard them from our datasets.", "labels": [], "entities": [{"text": "identification of these acceptances", "start_pos": 6, "end_pos": 41, "type": "TASK", "confidence": 0.8159453421831131}]}, {"text": "The final distribution of acceptances and rejections in each of the datasets is shown in.", "labels": [], "entities": [{"text": "acceptances and rejections", "start_pos": 26, "end_pos": 52, "type": "TASK", "confidence": 0.7078130046526591}]}, {"text": "As can be seen, the data is highly skewed, with less than 10% of P -R pairs corresponding to rejections.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: Precision, Recall, and F -scores for rejection identification.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9992227554321289}, {"text": "Recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9831554889678955}, {"text": "F -scores", "start_pos": 33, "end_pos": 42, "type": "METRIC", "confidence": 0.992061714331309}, {"text": "rejection identification", "start_pos": 47, "end_pos": 71, "type": "TASK", "confidence": 0.9439246952533722}]}]}