{"title": [{"text": "Syntax and Semantics in Quality Estimation of Machine Translation", "labels": [], "entities": [{"text": "Quality Estimation of Machine Translation", "start_pos": 24, "end_pos": 65, "type": "TASK", "confidence": 0.6694245576858521}]}], "abstractContent": [{"text": "We employ syntactic and semantic information in estimating the quality of machine translation from anew data set which contains source text from English customer support forums and target text consisting of its machine translation into French.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 74, "end_pos": 93, "type": "TASK", "confidence": 0.7286155521869659}]}, {"text": "These translations have been both post-edited and evaluated by professional translators.", "labels": [], "entities": []}, {"text": "We find that quality estimation using syntactic and semantic information on this data set can hardly improve over a baseline which uses only surface features.", "labels": [], "entities": []}, {"text": "However, the performance can be improved when they are combined with such surface features.", "labels": [], "entities": []}, {"text": "We also introduce a novel metric to measure translation adequacy based on predicate-argument structure match using word alignments.", "labels": [], "entities": []}, {"text": "While word alignments can be reliably used, the two main factors affecting the performance of all semantic-based methods seems to be the low quality of semantic role labelling (especially on ill-formed text) and the lack of nominal predicate annotation .", "labels": [], "entities": [{"text": "word alignments", "start_pos": 6, "end_pos": 21, "type": "TASK", "confidence": 0.7427213788032532}, {"text": "semantic role labelling", "start_pos": 152, "end_pos": 175, "type": "TASK", "confidence": 0.6393444637457529}]}], "introductionContent": [{"text": "The problem of evaluating machine translation output without reference translations is called quality estimation (QE) and has recently been the centre of attention () following the seminal work of.", "labels": [], "entities": [{"text": "evaluating machine translation output", "start_pos": 15, "end_pos": 52, "type": "TASK", "confidence": 0.7031512334942818}, {"text": "quality estimation (QE)", "start_pos": 94, "end_pos": 117, "type": "TASK", "confidence": 0.6407544374465942}]}, {"text": "Most QE studies have focused on surface and languagemodel-based features of the source and target.", "labels": [], "entities": []}, {"text": "The quality of translation is however closely related to the syntax and semantics of the languages, the former concerning fluency and the latter adequacy.", "labels": [], "entities": []}, {"text": "While there have been some attempts to utilize syntax in this task, semantics has been paid less attention.", "labels": [], "entities": []}, {"text": "In this work, we aim to exploit both syntax and semantics in QE, with a particular focus on the latter.", "labels": [], "entities": []}, {"text": "We use shallow semantic analysis obtained via semantic role labelling (SRL) and employ this information in QE in various ways including statistical learning using both tree kernels and hand-crafted features.", "labels": [], "entities": [{"text": "semantic role labelling (SRL)", "start_pos": 46, "end_pos": 75, "type": "TASK", "confidence": 0.7180678844451904}]}, {"text": "We also design a QE metric which is based on the Predicate-Argument structure Match (PAM ) between the source and its translation.", "labels": [], "entities": [{"text": "Predicate-Argument structure Match (PAM )", "start_pos": 49, "end_pos": 90, "type": "METRIC", "confidence": 0.8958672086397806}]}, {"text": "The semantic-based system is then combined with the syntax-based system to evaluate the full power of structural linguistic information.", "labels": [], "entities": []}, {"text": "We also combine this system with a baseline system consisting of effective surface features.", "labels": [], "entities": []}, {"text": "A second contribution of the paper is the release of anew data set for QE.", "labels": [], "entities": [{"text": "QE", "start_pos": 71, "end_pos": 73, "type": "DATASET", "confidence": 0.6897224187850952}]}, {"text": "This data set comprises a set of 4.5K sentences chosen from customer support forum text.", "labels": [], "entities": []}, {"text": "The machine translation of the sentences are not only evaluated in terms of adequacy and fluency, but also manually post-edited allowing various metrics of interest to be applied to measure different aspects of quality.", "labels": [], "entities": []}, {"text": "All experiments are carried out on this data set.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows: after reviewing the related work, the data is described and the semantic role labelling approach is explained.", "labels": [], "entities": [{"text": "semantic role labelling", "start_pos": 111, "end_pos": 134, "type": "TASK", "confidence": 0.5921754141648611}]}, {"text": "The baseline is then introduced, followed by the experiments with tree kernels, handcrafted features, the PAM metric and finally the combination of all methods.", "labels": [], "entities": []}, {"text": "The paper ends with a summary and suggestions for future work.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Average and standard deviation of the evaluation scores for the entire data set", "labels": [], "entities": [{"text": "Average", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9829750657081604}, {"text": "standard", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9432372450828552}]}, {"text": " Table 5: RMSE and Pearson r of the 17 baseline  features (WMT17) and hand-crafted features", "labels": [], "entities": [{"text": "RMSE", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.4842906892299652}, {"text": "Pearson r", "start_pos": 19, "end_pos": 28, "type": "METRIC", "confidence": 0.9421100318431854}, {"text": "WMT17", "start_pos": 59, "end_pos": 64, "type": "DATASET", "confidence": 0.8323699831962585}]}, {"text": " Table 7: RMSE and Pearson r of PAM unlabelled  and labelled F 1 scores as estimation of the MT  evaluation metrics", "labels": [], "entities": [{"text": "RMSE", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.8094429969787598}, {"text": "Pearson r", "start_pos": 19, "end_pos": 28, "type": "METRIC", "confidence": 0.9675404131412506}, {"text": "F 1 scores", "start_pos": 61, "end_pos": 71, "type": "METRIC", "confidence": 0.9198355674743652}, {"text": "MT  evaluation", "start_pos": 93, "end_pos": 107, "type": "TASK", "confidence": 0.8658102452754974}]}, {"text": " Table 8: RMSE and Pearson r of PAM scores as  features, alone and combined (PAM)", "labels": [], "entities": [{"text": "RMSE", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.6363420486450195}, {"text": "Pearson r", "start_pos": 19, "end_pos": 28, "type": "METRIC", "confidence": 0.9644462168216705}, {"text": "PAM", "start_pos": 32, "end_pos": 35, "type": "TASK", "confidence": 0.9052152633666992}]}, {"text": " Table 9: RMSE and Pearson r of the 17 baseline  features (WMT17) and system combinations", "labels": [], "entities": [{"text": "RMSE", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.444378137588501}, {"text": "Pearson r", "start_pos": 19, "end_pos": 28, "type": "METRIC", "confidence": 0.9496015012264252}, {"text": "WMT17", "start_pos": 59, "end_pos": 64, "type": "DATASET", "confidence": 0.9319961071014404}]}]}