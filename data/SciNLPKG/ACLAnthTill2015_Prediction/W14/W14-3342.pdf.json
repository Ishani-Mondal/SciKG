{"title": [{"text": "LIG System for Word Level QE task at WMT14", "labels": [], "entities": [{"text": "Word Level QE", "start_pos": 15, "end_pos": 28, "type": "TASK", "confidence": 0.5048693716526031}, {"text": "WMT14", "start_pos": 37, "end_pos": 42, "type": "DATASET", "confidence": 0.9203884601593018}]}], "abstractContent": [{"text": "This paper describes our Word-level QE system for WMT 2014 shared task on Spanish-English pair.", "labels": [], "entities": [{"text": "WMT 2014 shared task", "start_pos": 50, "end_pos": 70, "type": "TASK", "confidence": 0.6620268523693085}]}, {"text": "Compared to WMT 2013, this year's task is different due to the lack of SMT setting information and additional resources.", "labels": [], "entities": [{"text": "WMT 2013", "start_pos": 12, "end_pos": 20, "type": "DATASET", "confidence": 0.7811130881309509}, {"text": "SMT setting", "start_pos": 71, "end_pos": 82, "type": "TASK", "confidence": 0.9841106235980988}]}, {"text": "We report how we overcome this challenge to retain most of the important features which performed well last year in our system.", "labels": [], "entities": []}, {"text": "Novel features related to the availability of multiple systems output (new point of this year) are also proposed and experimented along with baseline set.", "labels": [], "entities": []}, {"text": "The system is optimized by several ways: tuning the classification threshold, combining with WMT 2013 data, and refining using Feature Selection strategy on our development set, before dealing with the test set for submission.", "labels": [], "entities": [{"text": "WMT 2013 data", "start_pos": 93, "end_pos": 106, "type": "DATASET", "confidence": 0.9547502398490906}]}], "introductionContent": [], "datasetContent": [{"text": "System quality in Precision (Pr), Recall (Rc) and F score (F) are shown in.", "labels": [], "entities": [{"text": "Precision (Pr)", "start_pos": 18, "end_pos": 32, "type": "METRIC", "confidence": 0.9528502225875854}, {"text": "Recall (Rc)", "start_pos": 34, "end_pos": 45, "type": "METRIC", "confidence": 0.9722804576158524}, {"text": "F score (F)", "start_pos": 50, "end_pos": 61, "type": "METRIC", "confidence": 0.9760485887527466}]}, {"text": "It can be observed that promising results are found in binary variant where both BL(bin) and BL+WMT(bin) are able to reach at least 50% F score in detecting errors (BAD class), meanwhile the performances in \"OK\" class go far beyond (73.51% and 75.01% respectively).", "labels": [], "entities": [{"text": "BL", "start_pos": 81, "end_pos": 83, "type": "METRIC", "confidence": 0.9828406572341919}, {"text": "BL+WMT(bin)", "start_pos": 93, "end_pos": 104, "type": "METRIC", "confidence": 0.8876824776331583}, {"text": "F score", "start_pos": 136, "end_pos": 143, "type": "METRIC", "confidence": 0.9883499443531036}, {"text": "BAD class)", "start_pos": 165, "end_pos": 175, "type": "METRIC", "confidence": 0.929057240486145}]}, {"text": "Interestingly, the combination with WMT13 data boosts the baseline prediction capability in both labels: BL+WMT13(bin) outperforms BL(bin) in 1.10% ( 3.89%) for OK (BAD) label.", "labels": [], "entities": [{"text": "WMT13 data", "start_pos": 36, "end_pos": 46, "type": "DATASET", "confidence": 0.8923282027244568}, {"text": "BL", "start_pos": 105, "end_pos": 107, "type": "METRIC", "confidence": 0.987190306186676}, {"text": "WMT13", "start_pos": 108, "end_pos": 113, "type": "DATASET", "confidence": 0.7836498022079468}, {"text": "BL(bin)", "start_pos": 131, "end_pos": 138, "type": "METRIC", "confidence": 0.9170612543821335}, {"text": "OK (BAD) label", "start_pos": 161, "end_pos": 175, "type": "DATASET", "confidence": 0.6599757730960846}]}, {"text": "Nevertheless, level 1 and multi-class systems maintain only good score for \"OK\" class.", "labels": [], "entities": [{"text": "OK", "start_pos": 76, "end_pos": 78, "type": "METRIC", "confidence": 0.9756819605827332}]}, {"text": "In addition, BL(mult) seems suffer seriously from its class imbalance, as well as the lack of training data for each, resulting in the inability of prediction for several among them (not all are reported in ).", "labels": [], "entities": [{"text": "BL", "start_pos": 13, "end_pos": 15, "type": "METRIC", "confidence": 0.9376583099365234}]}], "tableCaptions": [{"text": " Table 1: Statistics of corpora used in LIG's system. We use the notion name+year to indicate the dataset.  For instance, train14 stands for the training set of WMT14", "labels": [], "entities": [{"text": "LIG", "start_pos": 40, "end_pos": 43, "type": "DATASET", "confidence": 0.7766549587249756}, {"text": "WMT14", "start_pos": 161, "end_pos": 166, "type": "DATASET", "confidence": 0.7645524144172668}]}, {"text": " Table 2: Average Pr, Rc and F for labels  of all-feature binary and multi-class systems,  obtained on our WMT 2014 dev set (200  sentences). In BL(multi), classes with zero value  for Pr or Rc will not be reported", "labels": [], "entities": [{"text": "F", "start_pos": 29, "end_pos": 30, "type": "METRIC", "confidence": 0.9366737008094788}, {"text": "WMT 2014 dev set", "start_pos": 107, "end_pos": 123, "type": "DATASET", "confidence": 0.9652936011552811}]}, {"text": " Table 3: The rank of each feature (in term of usefulness) in WMT2014 and WMT2013 systems. The  bold ones perform well in both cases. Note that feature sets are not exactly the same for 2013 and 2014  (see explanations in section 3).", "labels": [], "entities": [{"text": "WMT2014", "start_pos": 62, "end_pos": 69, "type": "DATASET", "confidence": 0.9558891654014587}, {"text": "WMT2013", "start_pos": 74, "end_pos": 81, "type": "DATASET", "confidence": 0.8677195906639099}]}, {"text": " Table 4: The F scores for \"OK\" class and the  average F scores for the remaining classes (official  WMT14 metric) , obtained on test set.", "labels": [], "entities": [{"text": "F", "start_pos": 14, "end_pos": 15, "type": "METRIC", "confidence": 0.9981953501701355}, {"text": "F", "start_pos": 55, "end_pos": 56, "type": "METRIC", "confidence": 0.9967861175537109}, {"text": "WMT14", "start_pos": 101, "end_pos": 106, "type": "DATASET", "confidence": 0.7115740776062012}]}]}