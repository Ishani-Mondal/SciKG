{"title": [{"text": "ETS Lexical Associations System for the COGALEX-4 Shared Task", "labels": [], "entities": [{"text": "ETS Lexical Associations", "start_pos": 0, "end_pos": 24, "type": "DATASET", "confidence": 0.746441106001536}, {"text": "COGALEX-4 Shared Task", "start_pos": 40, "end_pos": 61, "type": "DATASET", "confidence": 0.7747334440549215}]}], "abstractContent": [{"text": "We present an automated system that computes multi-cue associations and generates associated-word suggestions, using lexical co-occurrence data from a large corpus of English texts.", "labels": [], "entities": []}, {"text": "The system performs expansion of cue words to their inflectional variants, retrieves candidate words from corpus data, finds maximal associations between candidates and cues, computes an aggregate score for each candidate, and outputs an n-best list of candidates.", "labels": [], "entities": []}, {"text": "We present experiments using several measures of statistical association, two methods of score aggregation, ablation of resources and applying additional filters on retrieved candidates.", "labels": [], "entities": []}, {"text": "The system achieves 18.6% precision on the COGALEX-4 shared task data.", "labels": [], "entities": [{"text": "precision", "start_pos": 26, "end_pos": 35, "type": "METRIC", "confidence": 0.9996297359466553}, {"text": "COGALEX-4 shared task data", "start_pos": 43, "end_pos": 69, "type": "DATASET", "confidence": 0.8745354264974594}]}, {"text": "Results with additional evaluation methods are presented.", "labels": [], "entities": []}, {"text": "We also describe an annotation experiment which suggests that the shared task may underestimate the appropriateness of candidate words produced by the corpus-based system.", "labels": [], "entities": []}], "introductionContent": [{"text": "The COGALEX-4 shared task is a multi-cue association task: finding a target word that is associated with a set of cue words.", "labels": [], "entities": []}, {"text": "The task is motivated, for example, by a tip-of-the-tongue search application, as described by the organizers: \"Suppose, we were looking fora word expressing the following ideas: 'superior dark coffee made of beans from Arabia', but could not remember the intended word 'mocha'.", "labels": [], "entities": []}, {"text": "Since people always remember something concerning the elusive word, it would be nice to have a system accepting this kind of input, to propose then a number of candidates for the target word.", "labels": [], "entities": []}, {"text": "Given the above example, we might enter 'dark', 'coffee', 'beans', and 'Arabia', and the system would be supposed to come up with one or several associated words such as 'mocha', 'espresso', or 'cappuccino'.\"", "labels": [], "entities": []}, {"text": "The data for the shared task were sampled from the Edinburgh Associative Thesaurus (EAThttp://www.eat.rl.ac.uk).", "labels": [], "entities": [{"text": "Edinburgh Associative Thesaurus (EAThttp://www.eat.rl.ac.uk)", "start_pos": 51, "end_pos": 111, "type": "DATASET", "confidence": 0.923301562666893}]}, {"text": "For each of about 8,000 stimulus words, the EAT lists the associations (words) provided by human respondents, sorted according to the number of respondents who provided the respective word.", "labels": [], "entities": [{"text": "EAT", "start_pos": 44, "end_pos": 47, "type": "TASK", "confidence": 0.7713689208030701}]}, {"text": "Generally, when more people provided the same response, the underlying association is considered to be stronger ().", "labels": [], "entities": []}, {"text": "For the COGALEX-4 shared task, the cues were the five strongest responses to an unknown stimulus word, and the task was to recover (guess) the stimulus word (henceforth, target word).", "labels": [], "entities": []}, {"text": "The data for the task consisted of a training set of 2000 items (for which target words were provided), and a test set of 2000 items.", "labels": [], "entities": []}, {"text": "The origin of the data was not disclosed before or during the system development and evaluation phases of the shared task competition.", "labels": [], "entities": []}, {"text": "The ETS entry consisted of a system that uses corpus-based distributional information about pairs of words in English.", "labels": [], "entities": [{"text": "ETS entry", "start_pos": 4, "end_pos": 13, "type": "DATASET", "confidence": 0.8268415927886963}]}, {"text": "No use was made of human association data (EAT or other), nor of any other information such as the order of importance of the cue words, or any special preference for the British spelling often used in the EAT.", "labels": [], "entities": []}], "datasetContent": [{"text": "We investigated how the restriction of resources impacts the performance on this task.", "labels": [], "entities": []}, {"text": "Specifically we restricted the resources as follows.", "labels": [], "entities": []}, {"text": "In one condition we used only the bigrams data, retrieving candidates only from the vectors of left co-occurring words (immediate preceding words) of each cue word (condition NL -n-grams left).", "labels": [], "entities": []}, {"text": "A similar restriction is when candidates are retrieved only from right (immediate successor) words (condition NR -n-grams right).", "labels": [], "entities": []}, {"text": "A third condition still uses only bigrams, but admits candidates from both left and right vectors (condition NL+NR).", "labels": [], "entities": []}, {"text": "Under the fourth condition (DSM), n-grams data is not used at all, only the DSM resource is used.", "labels": [], "entities": [{"text": "DSM resource", "start_pos": 76, "end_pos": 88, "type": "DATASET", "confidence": 0.921989381313324}]}, {"text": "In the fifth and sixth conditions we combine candidates from DSM with n-gram candidates (left or right vectors onlyrespectively).", "labels": [], "entities": [{"text": "DSM", "start_pos": 61, "end_pos": 64, "type": "DATASET", "confidence": 0.9416375160217285}]}, {"text": "The seventh condition is our standard -candidates from DSM and both left and right neighbors from bigrams are admitted.", "labels": [], "entities": [{"text": "DSM", "start_pos": 55, "end_pos": 58, "type": "DATASET", "confidence": 0.9415850043296814}]}, {"text": "For those experiments, we used NPMI association measure with MR aggregation, and included inflections in evaluation.", "labels": [], "entities": [{"text": "NPMI association measure", "start_pos": 31, "end_pos": 55, "type": "METRIC", "confidence": 0.6403627097606659}, {"text": "MR aggregation", "start_pos": 61, "end_pos": 75, "type": "METRIC", "confidence": 0.8850255310535431}]}, {"text": "The results are presented in.", "labels": [], "entities": []}, {"text": "Using only right-hand associates (typical textual successors of cue words) provides very low performance (precision@1 is 2.95%).", "labels": [], "entities": [{"text": "precision", "start_pos": 106, "end_pos": 115, "type": "METRIC", "confidence": 0.9989854693412781}]}, {"text": "Using only left-hand associates (typical textual predecessors of cue words) provides slightly better performance (precision@1 is 4.5%).", "labels": [], "entities": [{"text": "precision", "start_pos": 114, "end_pos": 123, "type": "METRIC", "confidence": 0.9990699887275696}]}, {"text": "However, it is notable that there are some items in the EAT data where all cues are strong bigrams with the target, e.g. {orange, fruit, lemon, apple, tomato} with target 'juice'.", "labels": [], "entities": [{"text": "EAT data", "start_pos": 56, "end_pos": 64, "type": "DATASET", "confidence": 0.7526077330112457}]}, {"text": "Combining these two resources (condition NL+NR) provides much better performance: precision@1 is 8.5%.", "labels": [], "entities": [{"text": "precision", "start_pos": 82, "end_pos": 91, "type": "METRIC", "confidence": 0.9993647933006287}]}, {"text": "Using just the DSM, the system achieves 10.5% precision@1, which may seem rather close to the combined NL+NR 8.5%.", "labels": [], "entities": [{"text": "DSM", "start_pos": 15, "end_pos": 18, "type": "DATASET", "confidence": 0.9556089639663696}, {"text": "precision", "start_pos": 46, "end_pos": 55, "type": "METRIC", "confidence": 0.9993492960929871}]}, {"text": "However, with DSM, for n-best lists precision rises quite sharply (e.g. 24.35% for precision@5), while for the NL+NR setting precision tends to be under 17% for all values of n up to 25.", "labels": [], "entities": [{"text": "DSM", "start_pos": 14, "end_pos": 17, "type": "DATASET", "confidence": 0.8734268546104431}, {"text": "precision", "start_pos": 36, "end_pos": 45, "type": "METRIC", "confidence": 0.9984906911849976}, {"text": "precision", "start_pos": 83, "end_pos": 92, "type": "METRIC", "confidence": 0.9882459044456482}, {"text": "precision", "start_pos": 125, "end_pos": 134, "type": "METRIC", "confidence": 0.889819324016571}]}, {"text": "Since our DSM and bigrams resources are built on the same corpus of text, for any given set of cues the DSM produces all the candidates that the bigrams resource does (but with different association values) and a lot of other candidates.", "labels": [], "entities": []}, {"text": "However, results for DSM+NR and DSM+NL settings (which are better than DSM alone) indicate that association values from bigrams contribute substantially to overall performance.", "labels": [], "entities": [{"text": "DSM+NR", "start_pos": 21, "end_pos": 27, "type": "DATASET", "confidence": 0.7616385022799174}, {"text": "DSM+NL", "start_pos": 32, "end_pos": 38, "type": "DATASET", "confidence": 0.7293870449066162}]}, {"text": "The best result in this experiment is achieved by a setting that combines candidates (and association values) from all three resources, indicating further that associations from sequential word combinations (bigrams) provide a substantial contribution to performance in this task.", "labels": [], "entities": []}, {"text": "Inspecting results from training-set data, we observed a number of cases where the system produced very plausible targets which however were struck down as incorrect (not matching the gold-standard).", "labels": [], "entities": []}, {"text": "For example, for the cue set {music, piano, play, player, instrument} the gold-standard target was 'accordion'.", "labels": [], "entities": [{"text": "accordion", "start_pos": 100, "end_pos": 109, "type": "METRIC", "confidence": 0.9187743067741394}]}, {"text": "But why not 'violin' or 'trombone'?", "labels": [], "entities": []}, {"text": "To provide a more in-depth evaluation of the results, we sampled 180 items at random from the test set, along with the candidate targets produced by our system, 9 and submitted those to evaluation by two research assistants.", "labels": [], "entities": []}, {"text": "For each item, evaluators were given the five cue words and the best candidate target generated by the system.", "labels": [], "entities": []}, {"text": "They were told that the word is supposed to be a common associate of the five cues, and asked to indicate, for each item, whether the candidate was (a) Just Right; or (b) OK; or (c) Inadequate; (a,b,c are on ordinal scale).", "labels": [], "entities": [{"text": "Inadequate", "start_pos": 182, "end_pos": 192, "type": "METRIC", "confidence": 0.9637203216552734}]}, {"text": "Out of the 180 items, 80 were judged by both annotators.", "labels": [], "entities": []}, {"text": "presents the agreement matrix between the two annotators.", "labels": [], "entities": []}, {"text": "Agreement on the 3 classes was kappa=0.49.", "labels": [], "entities": [{"text": "kappa", "start_pos": 31, "end_pos": 36, "type": "METRIC", "confidence": 0.9872213006019592}]}, {"text": "If Just Right and OK are collapsed, the agreement is kappa=0.60.", "labels": [], "entities": [{"text": "OK", "start_pos": 18, "end_pos": 20, "type": "METRIC", "confidence": 0.9687254428863525}, {"text": "kappa", "start_pos": 53, "end_pos": 58, "type": "METRIC", "confidence": 0.9882346391677856}]}, {"text": "The discrepancy is largely due to a substantial number of instances that one annotator judged OK and the other -Just Right.", "labels": [], "entities": []}, {"text": "We note that one annotator commented on a difficulty making a decision in a number of cases where the cues area list of mostly adjectives or possessives, and the target produced by the system is an adverb.", "labels": [], "entities": []}, {"text": "For example, the cue set {busy, house, vacant, engaged, empty} with the proposed candidate target 'currently'; the cue set {food, thirsty, tired, empty, starving} with the proposed candidate 'perpetually'; the cue set {fat, short, build, thick, built} with the proposed candidate 'slightly'; the cue set {mine, yours, his, is, theirs} with the proposed target 'rightfully'.", "labels": [], "entities": []}, {"text": "This annotator felt that these responses were OK, while the other annotator rejected them.", "labels": [], "entities": []}, {"text": "We merged the two annotations to provide a single annotation for the full set of 180 items by taking one annotator's judgment on single-annotated cases and taking the lower of the two judgments for the double annotated disagreed cases (thus, OK and Inadequate are merged to Inadequate; Just Right and OK are merged to OK).", "labels": [], "entities": []}, {"text": "We next compare these annotations to the EAT gold standard.", "labels": [], "entities": [{"text": "EAT gold standard", "start_pos": 41, "end_pos": 58, "type": "DATASET", "confidence": 0.9337286551793417}]}, {"text": "shows the confusion matrix between the \"gold label\" from EAT and our annotation.", "labels": [], "entities": [{"text": "EAT", "start_pos": 57, "end_pos": 60, "type": "DATASET", "confidence": 0.906444787979126}]}, {"text": "We observe that the totals for Just Right and EAT-match are almost identical (43 vs 42); however, only 17 items were both Just Right and EAT-matches.", "labels": [], "entities": [{"text": "Just", "start_pos": 31, "end_pos": 35, "type": "DATASET", "confidence": 0.8166757822036743}, {"text": "EAT-match", "start_pos": 46, "end_pos": 55, "type": "DATASET", "confidence": 0.5462817549705505}]}, {"text": "There were 24 EAT matches that were judged as OK by the annotators (presumably, these did not quite create the \"just right\" impression for at least one annotator).", "labels": [], "entities": []}, {"text": "These results might reflect cultural differences between original EAT respondents (British undergraduates circa year 1970) and present-day American young adults who, e.g. might not know much about cricket.", "labels": [], "entities": []}, {"text": "Another possibility is that in the EAT collection, the 5 th cue sometimes corresponds to a very weak associate provided by just a single respondent out of 100, as in brewingbear and primary-alcohol cases.", "labels": [], "entities": [{"text": "EAT collection", "start_pos": 35, "end_pos": 49, "type": "DATASET", "confidence": 0.8466120958328247}]}, {"text": "Interestingly, the weak cues did not confuse the system, but replicability of the human judgments for such cases is doubtful.", "labels": [], "entities": []}, {"text": "There were also 26 instances that were judged as Just Right yet were not EAT-matches.", "labels": [], "entities": [{"text": "EAT-matches", "start_pos": 73, "end_pos": 84, "type": "DATASET", "confidence": 0.6732917428016663}]}, {"text": "Three of these were derivationally related, like 'build' (EAT target) vs 'buildings' (proposed) for the cue set {house, up, construct, destroy, bricks}, the others were 'dwell In the rest of the cases, the generated candidates seemed as good as, or better, than the EAT words.", "labels": [], "entities": []}, {"text": "For example, the cue set {ships, boat, sea, ship, ocean} had 'liners' as the EAT target, whereas the system proposed 'cruise'.", "labels": [], "entities": [{"text": "EAT target", "start_pos": 77, "end_pos": 87, "type": "TASK", "confidence": 0.5512735247612}]}, {"text": "For the cue set {natural, animal, nature, birds, fear}, the gold-standard EAT target is 'instinct', whereas the system proposed 'predatory'.", "labels": [], "entities": [{"text": "EAT", "start_pos": 74, "end_pos": 77, "type": "METRIC", "confidence": 0.6454922556877136}]}, {"text": "For the cue set {sound, speak, sing, noise, speech} the gold-standard EAT target is 'voice', while the system produced 'louder'.", "labels": [], "entities": [{"text": "EAT", "start_pos": 70, "end_pos": 73, "type": "METRIC", "confidence": 0.6918496489524841}]}, {"text": "For the cue set {music, band, noise, club, folk} the target was 'jazz', whereas the system proposed 'dance'.", "labels": [], "entities": []}, {"text": "For the cue set {violin, music, orchestra, bow, instrument} the target was 'cello', while the system produced 'stringed'.", "labels": [], "entities": []}, {"text": "Furthermore, in as many as 58 cases (32%) the response produced by the system did not match the target from EAT, but was OK-ed by the annotators.", "labels": [], "entities": [{"text": "EAT", "start_pos": 108, "end_pos": 111, "type": "DATASET", "confidence": 0.7246673107147217}, {"text": "OK-ed", "start_pos": 121, "end_pos": 126, "type": "METRIC", "confidence": 0.9791694283485413}]}, {"text": "Some examples include: the cue set {fool, loaf, idiot, lout, lazy} with proposed candidate 'ignorant'; the cue set {hard, problems, work, hardship, trouble} with proposed candidate 'economic'; {interesting, intriguing, amazing, book, exciting} with proposed candidate 'discoveries'; {lazy, chair, about, lying, sitting} with proposed candidate 'motionless'.", "labels": [], "entities": []}, {"text": "In all, if the system were evaluated by counting Just Right and OK annotations as correct, the precison@1 would have been (43+82)/180 = 69%.", "labels": [], "entities": [{"text": "OK", "start_pos": 64, "end_pos": 66, "type": "METRIC", "confidence": 0.9181171655654907}, {"text": "precison@1", "start_pos": 95, "end_pos": 105, "type": "METRIC", "confidence": 0.939608613650004}]}, {"text": "The estimation of performance based on gold-standard EAT data for this set is 42/180 = 23%, exactly one-third of what annotators found to be reasonable responses.", "labels": [], "entities": [{"text": "estimation", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9855198264122009}]}, {"text": "This suggests that evaluation of multi-cued retrieval on targets from EAT rejects many good semantic associates, and thus might be considered too harsh.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. Inter-annotator agreement matrix for a subset of items from the test-set.", "labels": [], "entities": []}, {"text": " Table 2. Annotated data vs. gold-standard matches for a set of 180 items.", "labels": [], "entities": []}]}