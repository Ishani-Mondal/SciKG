{"title": [{"text": "TUHOI: Trento Universal Human Object Interaction Dataset", "labels": [], "entities": [{"text": "Trento Universal Human Object Interaction Dataset", "start_pos": 7, "end_pos": 56, "type": "DATASET", "confidence": 0.6634271989266077}]}], "abstractContent": [{"text": "This paper describes the Trento Universal Human Object Interaction dataset, TUHOI, which is dedicated to human object interactions in images.", "labels": [], "entities": [{"text": "Trento Universal Human Object Interaction dataset, TUHOI", "start_pos": 25, "end_pos": 81, "type": "DATASET", "confidence": 0.7073655724525452}]}, {"text": "1 Recognizing human actions is an important yet challenging task.", "labels": [], "entities": [{"text": "Recognizing human actions", "start_pos": 2, "end_pos": 27, "type": "TASK", "confidence": 0.9009907444318136}]}, {"text": "Most available datasets in this field are limited in numbers of actions and objects.", "labels": [], "entities": []}, {"text": "A large dataset with various actions and human object interactions is needed for training and evaluating complicated and robust human action recognition systems, especially systems that combine knowledge learned from language and vision.", "labels": [], "entities": []}, {"text": "We introduce an image collection with more than two thousand actions which have been annotated through crowdsourcing.", "labels": [], "entities": []}, {"text": "We review publicly available datasets, describe the annotation process of our image collection and some statistics of this dataset.", "labels": [], "entities": []}, {"text": "Finally, experimental results on the dataset including human action recognition based on objects and an analysis of the relation between human-object positions in images and prepositions in language are presented.", "labels": [], "entities": [{"text": "human action recognition", "start_pos": 55, "end_pos": 79, "type": "TASK", "confidence": 0.6277501185735067}]}], "introductionContent": [{"text": "Visual action recognition is generally studied on datasets with a limited number of predefined actions represented in many training images or videos).", "labels": [], "entities": [{"text": "Visual action recognition", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.6877409219741821}]}, {"text": "Common methods using holistic image or video representation such as Bagof-Words have achieved successful results in retrieval settings.", "labels": [], "entities": []}, {"text": "Though these predefined lists of actions are good for many computer vision problems, this cannot work when one wants to recognize all possible actions.", "labels": [], "entities": []}, {"text": "Firstly, the same action can be phrased in several ways.", "labels": [], "entities": []}, {"text": "Secondly, the number of actions that such systems would have to recognize in real life data is huge: the number of possible interactions with all possible objects is bounded by the cartesian product of numbers of verbs and objects.", "labels": [], "entities": []}, {"text": "Therefore, the task of collecting images or videos of each individual action becomes infeasible with this growing number.", "labels": [], "entities": []}, {"text": "By necessity this means that for some actions only few examples will be available.", "labels": [], "entities": []}, {"text": "In this paper we want to enable studies in the direction of recognizing all possible actions, for which we provide anew, suitable human-object interaction dataset.", "labels": [], "entities": []}, {"text": "A human action can be defined as a human, object, and the relation between them.", "labels": [], "entities": []}, {"text": "Therefore, an action is naturally recognized through its individual components.", "labels": [], "entities": []}, {"text": "Recent advances in computer vision have led to reasonable accuracy for object and human recognition, which makes recognizing the components feasible.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.9991611242294312}, {"text": "object and human recognition", "start_pos": 71, "end_pos": 99, "type": "TASK", "confidence": 0.6447723656892776}]}, {"text": "Additionally, language can help determining how components are combined.", "labels": [], "entities": []}, {"text": "Furthermore, the relative position between human and object can be used to disambiguate different human actions.", "labels": [], "entities": []}, {"text": "Perhaps prepositions in natural language can be linked to this relative position between the object and human (e.g., step out of a car).", "labels": [], "entities": []}, {"text": "To transfer this knowledge from language to vision, it is important that the distribution of the visual actions are sampled similarly as the language data.", "labels": [], "entities": []}, {"text": "This requirement is fulfilled when the action frequencies in the dataset mirror the frequencies in which they occur in real life.", "labels": [], "entities": []}], "datasetContent": [{"text": "A common approach to human action recognition is to exploit visual features using bag-of-features or part-based representation and treat action recognition as a general classification problem).", "labels": [], "entities": [{"text": "human action recognition", "start_pos": 21, "end_pos": 45, "type": "TASK", "confidence": 0.6118420958518982}, {"text": "action recognition", "start_pos": 137, "end_pos": 155, "type": "TASK", "confidence": 0.738086074590683}]}, {"text": "For common actions, it has been shown that learning the joint appearance of the human-object interaction can be beneficial ().", "labels": [], "entities": []}, {"text": "Other studies recognize actions by their components such as objects, human poses, scenes) jointly models attributes and parts, where attributes are verbs and parts are objects and local body parts.", "labels": [], "entities": []}, {"text": "These studies rely on suitable training data fora set of predefined actions: () tests on a 6 sport action dataset, attempts to distinguish images where a human plays a musical instrument from images where he/she does not, () classifies images to one of the seven everyday actions, and) introduces a dataset containing 40 human actions.", "labels": [], "entities": []}, {"text": "Most of these datasets were obtained using web search results such as Google, Bing, Flickr, etc.", "labels": [], "entities": []}, {"text": "The number of images varies from 300 to more than 9K images.", "labels": [], "entities": []}, {"text": "A comparison of the publicly available datasets with respect to the number of actions and their related objects is given in.: A comparison of available human action datasets in terms of number of objects and actions As can be seen in, the Stanford 40 action dataset contains quite a big number of images with 40 different actions.", "labels": [], "entities": [{"text": "Stanford 40 action dataset", "start_pos": 239, "end_pos": 265, "type": "DATASET", "confidence": 0.8923264741897583}]}, {"text": "This dataset is good for visually training action recognizers since there are enough images collected for each actions divided into training and test sets.", "labels": [], "entities": [{"text": "visually training action recognizers", "start_pos": 25, "end_pos": 61, "type": "TASK", "confidence": 0.6163811534643173}]}, {"text": "There are some dataset in which human action does not involved any object, these actions are for instance running, walking, or actions where objects are not specified such as catching, throwing.", "labels": [], "entities": [{"text": "catching, throwing", "start_pos": 175, "end_pos": 193, "type": "TASK", "confidence": 0.6698943078517914}]}, {"text": "These types of actions are not the target domain of our dataset.", "labels": [], "entities": []}, {"text": "We aim at recognizing the human object interactions based on objects.", "labels": [], "entities": []}, {"text": "With the same object, some actions are also more common than other actions: for example, sitting on a chair is more commonly observed than standing on a chair.", "labels": [], "entities": []}, {"text": "We want to capture such information in our dataset which can reflect the human action distributions on common objects, aiming to sample human actions related to objects in the visual world.", "labels": [], "entities": []}, {"text": "Furthermore, how actions can be phrased in different ways, or how verbs can have different meanings when interacting with different objects should also be considered.", "labels": [], "entities": []}, {"text": "Some actions can only be performed on some particular objects and are not applicable to some other objects: a person can ride a horse, ride a bike, can feed a horse, but cannot feed a bike.", "labels": [], "entities": []}, {"text": "This problem of ambiguity and different word uses have been widely studied in computational linguistics, but have received little attention from the computer vision community.", "labels": [], "entities": []}, {"text": "With the aim of creating a dataset that covers these requirements, we collect our dataset starting from images where humans and objects co-occur together and define the actions we observe in each image instead of collecting images for some predefined human actions.", "labels": [], "entities": []}, {"text": "This way of annotating actions in images is more natural and helps creating a more realistic dataset with various human actions that can occur in images generally.", "labels": [], "entities": []}, {"text": "Recently, some good works attempted to generate descriptive sentences from images ().", "labels": [], "entities": []}, {"text": "In our dataset we focus on human actions, which, if present, are often the main topic of interest within an image.", "labels": [], "entities": []}, {"text": "As such, our dataset can be used as an important steppingstone for generating full image descriptions as it allows for more rigorous evaluation than free-form text.", "labels": [], "entities": []}, {"text": "ImageNet is a hierarchical image database built upon the WordNet structure.", "labels": [], "entities": [{"text": "WordNet structure", "start_pos": 57, "end_pos": 74, "type": "DATASET", "confidence": 0.9573986530303955}]}, {"text": "The DET dataset in the ImageNet large scale object recognition challenge 2013 2 contains 200 objects for training and evaluation.", "labels": [], "entities": [{"text": "DET dataset", "start_pos": 4, "end_pos": 15, "type": "DATASET", "confidence": 0.7588264644145966}, {"text": "ImageNet large scale object recognition challenge 2013", "start_pos": 23, "end_pos": 77, "type": "TASK", "confidence": 0.7011294492653438}]}, {"text": "With the idea of starting from images with humans and common objects, we chose to use this DET dataset as a starting point to build our human action data.", "labels": [], "entities": [{"text": "DET dataset", "start_pos": 91, "end_pos": 102, "type": "DATASET", "confidence": 0.7402248978614807}]}, {"text": "The 200 objects in the DET dataset are general, basic-level categories (e.g., monitor, waffle iron, sofa, spatula, starfish).", "labels": [], "entities": [{"text": "DET dataset", "start_pos": 23, "end_pos": 34, "type": "DATASET", "confidence": 0.780265748500824}]}, {"text": "Each object corresponds to a synset (set of synonymous nouns) in WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 65, "end_pos": 72, "type": "DATASET", "confidence": 0.9638558626174927}]}, {"text": "The DET training set consists of single topic images where only the target object is annotated.", "labels": [], "entities": [{"text": "DET training set", "start_pos": 4, "end_pos": 20, "type": "DATASET", "confidence": 0.8279766043027242}]}, {"text": "As such, most images only contain primarily the object of interest and few actions.", "labels": [], "entities": []}, {"text": "It is good for learning object classifiers but is not suitable for learning action recognition.", "labels": [], "entities": [{"text": "learning object classifiers", "start_pos": 15, "end_pos": 42, "type": "TASK", "confidence": 0.6895939807097117}, {"text": "learning action recognition", "start_pos": 67, "end_pos": 94, "type": "TASK", "confidence": 0.7732763290405273}]}, {"text": "In contrast, the validation dataset contains various images where all object instances are annotated with a bounding box.", "labels": [], "entities": []}, {"text": "Many of these images contain actions.", "labels": [], "entities": []}, {"text": "Therefore we start the annotation from the validation set.", "labels": [], "entities": []}, {"text": "As can be seen in, there are 15,668 images having human and 31,081 human instances in these images.", "labels": [], "entities": []}, {"text": "We select only images having human since we want to annotate this dataset with human object interactions.", "labels": [], "entities": []}, {"text": "Objects related to clothes such as bathing cap, miniskirt, tie, etc. are not interesting for human actions (most of the time, the action associated with these objects is \"to wear\").", "labels": [], "entities": []}, {"text": "Therefore, we excluded all these objects from the list of 200 objects above, which are: bathing cap, bow tie, bow, brassiere, hat with a wide brim, helmet, maillot, miniskirt, neck brace, sunglasses, tie.", "labels": [], "entities": []}, {"text": "In this part, we use our newly collected dataset for building a general human action classifier based on objects.", "labels": [], "entities": []}, {"text": "We analyze the relative positions between humans and objects in each image and use this information to help classifying human actions.", "labels": [], "entities": []}, {"text": "Finally, we discuss the relations between human-object positions with prepositions that are used in language for describing human actions.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: A comparison of available human action datasets in terms of number of objects and actions", "labels": [], "entities": []}, {"text": " Table 2: The statistics of the DET dataset", "labels": [], "entities": [{"text": "DET dataset", "start_pos": 32, "end_pos": 43, "type": "DATASET", "confidence": 0.8393070697784424}]}, {"text": " Table 4: Results of the classifier with and without position information", "labels": [], "entities": []}, {"text": " Table 5: Objects with higher accuracy when using position information", "labels": [], "entities": [{"text": "accuracy", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9986774325370789}]}, {"text": " Table 6: Actions that can be disambiguated by positions (Group 1) vs. actions that cannot be disam- biguated by positions (Group 2) and their links in the language model", "labels": [], "entities": []}]}