{"title": [{"text": "Identifying Narrative Clause Types in Personal Stories", "labels": [], "entities": [{"text": "Identifying Narrative Clause Types in Personal Stories", "start_pos": 0, "end_pos": 54, "type": "TASK", "confidence": 0.9002820338521685}]}], "abstractContent": [{"text": "Today was a very eventful workday.", "labels": [], "entities": []}], "introductionContent": [{"text": "Sharing personal experiences by storytelling is a fundamental aspect of human social behavior ().", "labels": [], "entities": [{"text": "Sharing personal experiences", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.876991073290507}]}, {"text": "Humans appear to be wired to engage with information that is narratively structured, and telling stories provides a critical developmental and societal function, by serving as a means to reinforce community value systems and to define individual identity (.", "labels": [], "entities": []}, {"text": "This has led some theorists to claim that \"the stories they tell\" is the defining aspect of both individuals and cultures.", "labels": [], "entities": []}, {"text": "Unlike any prior time inhuman history, personal narratives about many life experiences are being told online, and are widely available in social media sources such as weblogs.", "labels": [], "entities": []}, {"text": "A personal narrative about an arrest is shown in, and one about a protest is in.", "labels": [], "entities": []}, {"text": "Narratives such as these provide a valuable resource for learning a wealth of commonsense knowledge about people, the types of activities they engage in, and the attitudes they hold.", "labels": [], "entities": []}, {"text": "They are also well suited to learning about causal and temporal relationships between events because narrative interpretation explicitly depends on the coherence of these relationships.", "labels": [], "entities": []}, {"text": "This paper applies and tests a narrative clause labeling scheme to personal narratives.", "labels": [], "entities": []}, {"text": "Our scheme is derived from Labov & Waletzky's (henceforth L&W) theory of oral narrative).", "labels": [], "entities": []}, {"text": "L&W's theory distinguishes (1) clauses that indicate causal relationships (AC-TION), from (2) clauses that provide traits or properties of the setting or characters (ORIENTATION), from (3) clauses describing the story characters' emotional reactions to the events (EVALUATION).", "labels": [], "entities": [{"text": "L&W", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.855344831943512}, {"text": "ORIENTATION", "start_pos": 166, "end_pos": 177, "type": "METRIC", "confidence": 0.9845475554466248}, {"text": "EVALUATION", "start_pos": 265, "end_pos": 275, "type": "METRIC", "confidence": 0.9621958136558533}]}, {"text": "We adopt L&W's theory for three reasons.", "labels": [], "entities": []}, {"text": "First, we believe that the narrative structure of personal narratives posted on weblogs will be more similar to oral narrative than they are to classical stories.", "labels": [], "entities": []}, {"text": "Second, we believe that any narrative discourse typology must at least distinguish ACTION, from ORI-ENTATION, and EVALUATION.", "labels": [], "entities": [{"text": "ORI-ENTATION", "start_pos": 96, "end_pos": 108, "type": "METRIC", "confidence": 0.9451097846031189}]}, {"text": "Third, personal stories found on the web are often noisy and difficult to interpret; they do not always clearly follow well defined narrative conventions.", "labels": [], "entities": []}, {"text": "A deep analysis  and annotation scheme, such as the one employed by) that extends theories of narrative structure and plot units (, offers many advantages.", "labels": [], "entities": []}, {"text": "However, acquiring this level of analysis on user generated content, such as blog stories, is resource intensive.", "labels": [], "entities": []}, {"text": "Research on computational models of narrative structure typically focus on inferring the causal and temporal relationships between events ().", "labels": [], "entities": []}, {"text": "Yet L&W point out that stories are not just about the events that occur.", "labels": [], "entities": [{"text": "L&W", "start_pos": 4, "end_pos": 7, "type": "DATASET", "confidence": 0.9502662420272827}]}, {"text": "In fact, L&W say that stories that are only about events are boring.", "labels": [], "entities": [{"text": "L&W", "start_pos": 9, "end_pos": 12, "type": "DATASET", "confidence": 0.9543115893999735}]}, {"text": "Current methods for inferring narrative structure, including our own (, do not distinguish event clauses from other narrative clause types.", "labels": [], "entities": [{"text": "inferring narrative structure", "start_pos": 20, "end_pos": 49, "type": "TASK", "confidence": 0.6602751811345419}]}, {"text": "But note that actions only constitute about one third of the clauses in the narratives in.", "labels": [], "entities": []}, {"text": "2 provides more detail about L&W's theory.", "labels": [], "entities": [{"text": "L&W's theory", "start_pos": 29, "end_pos": 41, "type": "DATASET", "confidence": 0.822075092792511}]}, {"text": "3 describes the annotation experiments and efforts to improve annotation reliability.", "labels": [], "entities": [{"text": "reliability", "start_pos": 73, "end_pos": 84, "type": "METRIC", "confidence": 0.7424007654190063}]}, {"text": "4 presents experiments on learning to automatically classify L&W categories, where we examine the the most predictive features, and the effect of annotator agreement on classification accuracy.", "labels": [], "entities": [{"text": "classify L&W categories", "start_pos": 52, "end_pos": 75, "type": "TASK", "confidence": 0.7630429983139038}, {"text": "accuracy", "start_pos": 184, "end_pos": 192, "type": "METRIC", "confidence": 0.8923948407173157}]}, {"text": "We achieve a best average F-score of .658, which rises to an Fscore of .767 on the cases with the highest annotator agreement.", "labels": [], "entities": [{"text": "F-score", "start_pos": 26, "end_pos": 33, "type": "METRIC", "confidence": 0.9925674200057983}, {"text": "Fscore", "start_pos": 61, "end_pos": 67, "type": "METRIC", "confidence": 0.9990981817245483}]}, {"text": "We analyze the types of errors the classifier makes in Sec.", "labels": [], "entities": []}, {"text": "5.1 and conclude in Sec.", "labels": [], "entities": []}], "datasetContent": [{"text": "Previous work showed that about 5% of all weblog entries are personal stories describing an event in the author's daily life.", "labels": [], "entities": []}, {"text": "They developed an automatic classifier for identifying personal narratives from a random sample of 5,000 posts from a corpus of 44 million entries available as part of the ICWSM 2010 dataset challenge ().", "labels": [], "entities": [{"text": "ICWSM 2010 dataset challenge", "start_pos": 172, "end_pos": 200, "type": "DATASET", "confidence": 0.9232604652643204}]}, {"text": "229 of these posts were manually labeled as personal stories.", "labels": [], "entities": []}, {"text": "Our experiments are based on 50 of these 229 stories.", "labels": [], "entities": []}, {"text": "L&W's theory applies to subsentence discourse units in a narrative.", "labels": [], "entities": [{"text": "L&W", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9136706391970316}]}, {"text": "It is an open question what level of phrasal granularity is appropriate to apply to written narratives.", "labels": [], "entities": []}, {"text": "Here, we treat each independent clause as the basic unit of discourse and manually segment each story in our dataset using this definition.", "labels": [], "entities": []}, {"text": "This results in a collection of 1,602 independent clauses.", "labels": [], "entities": []}, {"text": "We then divided the 50 stories into 4 groups and annotated them in batches among 3 annotators in order to refine our annotation guidelines and process.", "labels": [], "entities": []}, {"text": "This dataset is freely available at https://nlds.soe.ucsc.edu/lw.", "labels": [], "entities": []}, {"text": "Previous work has applied L&W's theory to Aesop's fables and achieved high levels of interannotator agreement and extremely high machine learning accuracies ( . However personal narratives clearly provide a more challenging context for annotation.", "labels": [], "entities": [{"text": "L&W's theory", "start_pos": 26, "end_pos": 38, "type": "DATASET", "confidence": 0.8177303791046142}, {"text": "Aesop's fables", "start_pos": 42, "end_pos": 56, "type": "DATASET", "confidence": 0.6315239568551382}, {"text": "agreement", "start_pos": 100, "end_pos": 109, "type": "METRIC", "confidence": 0.769111692905426}]}, {"text": "There was a high level of disagreement after the initial round of annotation.", "labels": [], "entities": []}, {"text": "We found at least 6 primary sources of disagreements: \u2022 Clauses of more than one category are common with rising action and evaluation, e.g. a clause containing elements of orientation, action, and evaluation: After leaving the apartment at 6:45 AM, flying 2 hours, taking a cab to Seattle, and then driving seven hours up to Whistler including a border crossing, it's safe to say that I felt pretty much like a dick with legs.", "labels": [], "entities": []}, {"text": "\u2022 Actions that are implied but not explicitly stated in the text.", "labels": [], "entities": []}, {"text": "\u2022 Stative descriptions of the world as a result of an action that are not intuitively orientation.", "labels": [], "entities": []}, {"text": "\u2022 Stative descriptions of the world that are localized to a specific place in the narrative, which is problematic to L&W's definition of orientation.", "labels": [], "entities": [{"text": "L&W", "start_pos": 117, "end_pos": 120, "type": "DATASET", "confidence": 0.797551174958547}]}, {"text": "\u2022 Subjective clauses in scene setting are usually orientation, but are lexically similar to evaluation.", "labels": [], "entities": []}, {"text": "\u2022 Disambiguating the functional purpose of clauses that describe actions, but maybe intended to set the scene as opposed to the rising action.", "labels": [], "entities": []}, {"text": "\u2022 Disambiguating the functional purpose of subjective language in the description of an event or state, e.g., The gigantic tree outside my window, The radiant blue of the sky.", "labels": [], "entities": []}, {"text": "After several rounds of annotation we stabilized on a labeling scheme that hierarchically extends the original L&W categories, along with annotation guidelines that annotators could use to disambiguate recurring problematic cases.", "labels": [], "entities": []}, {"text": "The final set of extended category labels along with two reduced hierarchical mappings are shown in.", "labels": [], "entities": []}, {"text": "STATIVE-LOCAL CONTEXT is a category for distinguishing stative descriptions of the world, that are not intuitively orientation.", "labels": [], "entities": [{"text": "STATIVE-LOCAL CONTEXT", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.41915765404701233}]}, {"text": "For example: \u2022 I saw the sign I expected to turn south on Hwy 138.", "labels": [], "entities": [{"text": "Hwy 138", "start_pos": 58, "end_pos": 65, "type": "DATASET", "confidence": 0.9791580736637115}]}, {"text": "The traffic sign pointed to Sutherlin and Roseburg, The clause in italics is a stative that describes the sign seen in the previous action.", "labels": [], "entities": [{"text": "Sutherlin and Roseburg", "start_pos": 28, "end_pos": 50, "type": "TASK", "confidence": 0.6870219310124716}]}, {"text": "It is clearly not an action or evaluation, but is not intuitively an orientation, because it is so locally dependent.", "labels": [], "entities": []}, {"text": "STATIVE-IMPLIED ACTIONS are clauses, which do not explicitly mention an action or event, but imply one that is necessary to maintain the causal or temporal coherence of the remaining story.", "labels": [], "entities": [{"text": "STATIVE-IMPLIED ACTIONS", "start_pos": 0, "end_pos": 23, "type": "METRIC", "confidence": 0.7182019650936127}]}, {"text": "For example: After that, we decided to walk some more.", "labels": [], "entities": []}, {"text": "In the context of the story it is necessary to know that they actually did walk some more in order to interpret the other actions described in the narrative.", "labels": [], "entities": []}, {"text": "Implied actions are often passive constructions that describe a state of the world that could only be true if an action had taken place.", "labels": [], "entities": []}, {"text": "For example: We were at the convention center in about 10 minutes.", "labels": [], "entities": []}, {"text": "STATIVE-CONSEQUENCE is a category that describes the state of the world that has resulted as a consequence of an action, but does not directly evaluate the goals, intentions or desires of the participants.", "labels": [], "entities": []}, {"text": "For example, clause 23 in.", "labels": [], "entities": []}, {"text": "Using this extended label set we were able to achieve an inter-annotator agreement between the 3 annotators of 0.582 using Fleiss' \u03ba on assigning categories to clauses.", "labels": [], "entities": []}, {"text": "We also mapped the full set of labels to a smaller subsets to see if the finer grained distinctions helped improve reliability on more coarse grained labeling schemes.", "labels": [], "entities": [{"text": "reliability", "start_pos": 115, "end_pos": 126, "type": "METRIC", "confidence": 0.9792094230651855}]}, {"text": "The extended labels we included were generally different types of stative descriptions of the world, which were all mapped to a single category for the Stative label set.", "labels": [], "entities": []}, {"text": "Finally, we mapped each extended label to an original L&W category that we thought best fit the original definitions.", "labels": [], "entities": []}, {"text": "When mapping back to these reduced label sets we were able to increase the \u03ba to 0.597 for the stative set and 0.630 for the original L&W categories.", "labels": [], "entities": []}, {"text": "This result indicates that we can achieve higher reliability by ensuring that the annotators think carefully about particular kinds of distinctions between different stative clauses.", "labels": [], "entities": [{"text": "reliability", "start_pos": 49, "end_pos": 60, "type": "METRIC", "confidence": 0.9865638017654419}]}, {"text": "Gold standard labels were selected based on a simple majority of the annotator assignments.", "labels": [], "entities": []}, {"text": "When no annotators agreed on a label, one of the selected labels was chosen at random.", "labels": [], "entities": []}, {"text": "Once completed there were 424 action clauses, 702 evaluations, 26 not stories, 306 orientation, 17 stative consequences, 12 implied actions and 115 local contexts.", "labels": [], "entities": []}, {"text": "Note that EVALUATION and ORIENTATION clauses that would not be distinguished from ACTION by previous work constitute two thirds of the clauses.", "labels": [], "entities": [{"text": "EVALUATION", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9026498794555664}, {"text": "ORIENTATION", "start_pos": 25, "end_pos": 36, "type": "METRIC", "confidence": 0.9788919687271118}]}, {"text": "The triply annotated dataset described above was used as training and test data for experiments on learning to automatically label narrative clauses.", "labels": [], "entities": []}, {"text": "40 narratives were randomly selected to be used as training and development data and the remaining 10 narratives for test data.", "labels": [], "entities": []}, {"text": "To derive feature representations of each type of narrative clause we started with the features presented in ( ).", "labels": [], "entities": []}, {"text": "We refined these by examining L&W's descriptions of distinguishing features of each category.", "labels": [], "entities": []}, {"text": "summarizes the features we automatically extracted from all narrative clauses in the weblogs.", "labels": [], "entities": []}, {"text": "First, we used the Stanford Parser to distinguish independent and dependent clauses and kept track separately of features that occurred in both types of clause.", "labels": [], "entities": []}, {"text": "This is because L&W state that the unit of analysis should bean independent clause with its subordinate clauses, but we felt that these were exactly the cases that often caused difficulties during annotation.", "labels": [], "entities": [{"text": "L&W", "start_pos": 16, "end_pos": 19, "type": "DATASET", "confidence": 0.8519601623217264}]}, {"text": "However distinguishing between the features occurring in the two clause types would allow us to determine if and when the features of the subordinate clause were relevant or more informative for automatic classification.", "labels": [], "entities": []}, {"text": "Then, within both dependent and independent clauses, we distinguished the part-of-speech of the main verb (POS), whether the clause contained a negation (Negate), lexical semantic categories from LIWC (), dependency relations (DEP), lexical unigrams (STEM), and whether the verb was one of a class of verbs that are likely to be stative.", "labels": [], "entities": []}, {"text": "We also developed a set of features describing the relative position of the clause in the story (BinPosition, FirstClause, LastClause), because different story regions are often associated with different: The 10 most highly correlated features with each label and cumulatively overall the labels using mutual information and information gain.", "labels": [], "entities": [{"text": "FirstClause", "start_pos": 110, "end_pos": 121, "type": "DATASET", "confidence": 0.6194912195205688}, {"text": "LastClause", "start_pos": 123, "end_pos": 133, "type": "METRIC", "confidence": 0.7866953015327454}]}, {"text": "For example, in and, the beginning of the story contains more ORIENTATION clauses, while ACTION clauses are concentrated in the middle of the story.", "labels": [], "entities": [{"text": "ORIENTATION", "start_pos": 62, "end_pos": 73, "type": "METRIC", "confidence": 0.9934493899345398}, {"text": "ACTION", "start_pos": 89, "end_pos": 95, "type": "METRIC", "confidence": 0.9585107564926147}]}, {"text": "The EVALUATION clauses typically occur part-way through the story where they provide the narrator's reaction to story events.", "labels": [], "entities": [{"text": "EVALUATION", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.8484877347946167}]}, {"text": "In total there were 3,510 unique binary valued features extracted from our training dataset.", "labels": [], "entities": []}, {"text": "We used mutual information to find the features that had the highest correlation with each category and the information gain overall the labels.", "labels": [], "entities": []}, {"text": "The 10 highest valued features are in, e.g. the top feature is when the part-of-speech (POS) of the main verb in the independent clause (IND) is past tense (VBD).", "labels": [], "entities": []}, {"text": "This feature encoding was used for machine learning experiments with classification algorithms from Mallet): Naive Bayes (NB)), Confidence Weighted Linear Classifier (CWLC) (, Maximum Entropy (ME)) and a sequential classifier (CRF) ().", "labels": [], "entities": [{"text": "Mallet", "start_pos": 100, "end_pos": 106, "type": "DATASET", "confidence": 0.9754564166069031}]}, {"text": "We evaluate the performance of our classifiers with experiments using the 50 annotated stories.", "labels": [], "entities": []}, {"text": "Using the 40 stories in the training set we calculated the information gain for each feature (see).", "labels": [], "entities": []}, {"text": "For each subset of the highest valued features (in the range of 2 2 -2 12 ), we performed a 10-fold crossvalidation on the training data and assessed the performance of each classifier to find the right number of features.", "labels": [], "entities": []}, {"text": "Within each fold of the cross-validation we also perform a simple grid search for the optimal hyper-parameters of the model (e.g., the prior in the ME and CRF models) using only the data within the training fold.: The optimal number of features found for each model and the average F-score obtained using a 10-fold cross-validation on the training data.", "labels": [], "entities": [{"text": "F-score", "start_pos": 282, "end_pos": 289, "type": "METRIC", "confidence": 0.9986599683761597}]}, {"text": "We report the optimal number of features and the corresponding macro F-score, weighted by the relative frequency of each category, for each algorithm and label set.", "labels": [], "entities": [{"text": "F-score", "start_pos": 69, "end_pos": 76, "type": "METRIC", "confidence": 0.8768062591552734}]}, {"text": "For all algorithms, performance increases for label sets with higher levels of abstraction.", "labels": [], "entities": []}, {"text": "The Naive Bayes and CRF models perform better with a small subset of the features, while the ME and CWLC algorithms use a much larger subset.", "labels": [], "entities": []}, {"text": "Surprisingly the sequential classifier has the lowest F-score and Naive Bayes performs the best.", "labels": [], "entities": [{"text": "F-score", "start_pos": 54, "end_pos": 61, "type": "METRIC", "confidence": 0.998982846736908}]}, {"text": "A * indicates a significant improvement over CRF at p < 0.05 using a two-sided t-test.", "labels": [], "entities": [{"text": "A", "start_pos": 0, "end_pos": 1, "type": "METRIC", "confidence": 0.9920852780342102}, {"text": "CRF", "start_pos": 45, "end_pos": 48, "type": "METRIC", "confidence": 0.8491576910018921}]}, {"text": "No other differences were significant.", "labels": [], "entities": []}, {"text": "Using the optimal number of features obtained from this search we trained a model for each algorithm using the entire training dataset and selecting the hyper-parameters as before.", "labels": [], "entities": []}, {"text": "We applied these models to the unseen test data and evaluated the performance of each classifier as applied to the entire set of clauses and to individual narratives.", "labels": [], "entities": []}, {"text": "We first computed the precision, recall and F-score aggregated overall the clauses in the test set.", "labels": [], "entities": [{"text": "precision", "start_pos": 22, "end_pos": 31, "type": "METRIC", "confidence": 0.9997120499610901}, {"text": "recall", "start_pos": 33, "end_pos": 39, "type": "METRIC", "confidence": 0.9995098114013672}, {"text": "F-score", "start_pos": 44, "end_pos": 51, "type": "METRIC", "confidence": 0.9988227486610413}]}, {"text": "Table 5 summarizes the results for each classifier and label set.", "labels": [], "entities": []}, {"text": "The left hand side of the table shows the macro precision, recall and F-score weighted by the relative frequency of each label.", "labels": [], "entities": [{"text": "precision", "start_pos": 48, "end_pos": 57, "type": "METRIC", "confidence": 0.9238472580909729}, {"text": "recall", "start_pos": 59, "end_pos": 65, "type": "METRIC", "confidence": 0.9995074272155762}, {"text": "F-score", "start_pos": 70, "end_pos": 77, "type": "METRIC", "confidence": 0.9992638230323792}]}, {"text": "The right hand side of the table shows the F-score of each individual label separately.", "labels": [], "entities": [{"text": "F-score", "start_pos": 43, "end_pos": 50, "type": "METRIC", "confidence": 0.9976218342781067}]}, {"text": "On this evaluation, Naive Bayes outperforms all other methods on all label sets.", "labels": [], "entities": []}, {"text": "Overall, precision and recall are relatively balanced achieving a maximum F-score of 0.689 when the labels are mapped back to the original L&W categories.", "labels": [], "entities": [{"text": "precision", "start_pos": 9, "end_pos": 18, "type": "METRIC", "confidence": 0.9996490478515625}, {"text": "recall", "start_pos": 23, "end_pos": 29, "type": "METRIC", "confidence": 0.9996734857559204}, {"text": "F-score", "start_pos": 74, "end_pos": 81, "type": "METRIC", "confidence": 0.9994361996650696}]}, {"text": "The CRF does surprisingly well considering its poor performance during the feature selection search.", "labels": [], "entities": [{"text": "feature selection search", "start_pos": 75, "end_pos": 99, "type": "TASK", "confidence": 0.7574166655540466}]}, {"text": "The classifiers perform the poorest on orientation clauses and the best on evaluation clauses.", "labels": [], "entities": []}, {"text": "As mentioned above, the annotation task is highly subjective, requiring interpreting the narrative and the author's intention, which prevents us from obtaining high levels of inter-rater agreement.", "labels": [], "entities": []}, {"text": "Because of the noise in the annotations, the standard evalua-   tion metrics may hide information about the ability of the classifiers to learn from our feature set.", "labels": [], "entities": []}, {"text": "For example, the best performing classifier (NB) incorrectly labeled 127 clauses out of 430 possible in the test set.", "labels": [], "entities": []}, {"text": "However, 44 of these errors agreed with at least one annotator, but were counted as entirely incorrect in the previous evaluations.", "labels": [], "entities": []}, {"text": "To address these concerns we also evaluated the performance of the the best performing classifier based on the level of agreement of each instance using two different approaches.", "labels": [], "entities": []}, {"text": "The first approach was inspired by the approach in () where the clauses in the test set are binned based on the number of annotators who agreed with the gold standard label.", "labels": [], "entities": []}, {"text": "The performance is then calculated for each bin.", "labels": [], "entities": []}, {"text": "The first three rows of show the performance for the different levels of agreement in the dataset.", "labels": [], "entities": []}, {"text": "There were only 15 clauses in the test set where there was no agreement at all.", "labels": [], "entities": []}, {"text": "It is unsurprising that when the annotators could not agree on a label the system performed near chance levels.", "labels": [], "entities": []}, {"text": "However, when all three annotators agreed on the gold standard label the F-score improved to 0.767.", "labels": [], "entities": [{"text": "F-score", "start_pos": 73, "end_pos": 80, "type": "METRIC", "confidence": 0.9988393187522888}]}, {"text": "As a comparison, the F-score of the entire test set was 0.689 as shown in the row labeled All.: Summary statistics of the F-score, with 95% confidence intervals, when evaluating stories.", "labels": [], "entities": [{"text": "F-score", "start_pos": 21, "end_pos": 28, "type": "METRIC", "confidence": 0.9969425797462463}, {"text": "F-score", "start_pos": 122, "end_pos": 129, "type": "METRIC", "confidence": 0.9743853211402893}]}, {"text": "duce a modification to the standard precision, recall and F-scores that takes into account the level of agreement of each instance, where the values of true-positives and false-negatives are assigned fractional counts based on the proportion of annotators who assigned that label.", "labels": [], "entities": [{"text": "precision", "start_pos": 36, "end_pos": 45, "type": "METRIC", "confidence": 0.9992247819900513}, {"text": "recall", "start_pos": 47, "end_pos": 53, "type": "METRIC", "confidence": 0.9987198114395142}, {"text": "F-scores", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.9894717335700989}, {"text": "agreement", "start_pos": 104, "end_pos": 113, "type": "METRIC", "confidence": 0.8885138630867004}]}, {"text": "The final row of provides the results using these adjusted values.", "labels": [], "entities": []}, {"text": "We also investigated the performance of the classifiers when evaluating each story separately.", "labels": [], "entities": []}, {"text": "Each classifier was applied to the clauses of the 10 narratives in the test set and the F-score was computed for each narrative individually.", "labels": [], "entities": [{"text": "F-score", "start_pos": 88, "end_pos": 95, "type": "METRIC", "confidence": 0.998803973197937}]}, {"text": "The table shows the minimum, maximum and average F-score with 95% confidence in- tervals over the 10 narratives.", "labels": [], "entities": [{"text": "F-score", "start_pos": 49, "end_pos": 56, "type": "METRIC", "confidence": 0.9986605644226074}]}, {"text": "The CWLC performed the best on this test and the performance of all the algorithms generally improved using the higher-level label sets.", "labels": [], "entities": []}, {"text": "The results also show that there is a high variance in performance between stories, with a minimum F-score of 0.458 and a maximum of 0.877 for the CWLC on the L&W label set.", "labels": [], "entities": [{"text": "F-score", "start_pos": 99, "end_pos": 106, "type": "METRIC", "confidence": 0.9991501569747925}, {"text": "CWLC", "start_pos": 147, "end_pos": 151, "type": "DATASET", "confidence": 0.9149850010871887}, {"text": "L&W label set", "start_pos": 159, "end_pos": 172, "type": "DATASET", "confidence": 0.9749123096466065}]}, {"text": "This indicates that some clauses are ambiguous and difficult to label, but also that some stories are more difficult to classify.", "labels": [], "entities": []}, {"text": "To assess whether more annotated data could improve performance, we ran a series of learning curves in.", "labels": [], "entities": []}, {"text": "Only the training data was used for these experiments.", "labels": [], "entities": []}, {"text": "The curves were created by randomly sampling 90% of the data for training and 10% for testing.", "labels": [], "entities": []}, {"text": "A model was trained, using the same process as above, on successively larger subsets of the data and applied to the 10% held out clauses.", "labels": [], "entities": []}, {"text": "This process was repeated 10 times and the mean F-Score is reported.", "labels": [], "entities": [{"text": "F-Score", "start_pos": 48, "end_pos": 55, "type": "METRIC", "confidence": 0.9666111469268799}]}, {"text": "In nearly all cases, the performance of classifiers is still increasing when all of the data is used indicating that we have not exhausted the expressive power of our features and more annotated data would be useful.", "labels": [], "entities": []}, {"text": "However, we also see we can reach about 93% of our maximum performance with only a few hundred examples.", "labels": [], "entities": []}, {"text": "We plan to increase the size of our annotated data set in future work.", "labels": [], "entities": []}, {"text": "And this all happened right in front of my store 16 Evaluation which was kind of scary 17 Evaluation but it was kind of interesting 18 Coda since I've never seen a riot before.", "labels": [], "entities": [{"text": "Evaluation", "start_pos": 52, "end_pos": 62, "type": "METRIC", "confidence": 0.9899094104766846}, {"text": "Evaluation", "start_pos": 90, "end_pos": 100, "type": "METRIC", "confidence": 0.9811612963676453}]}], "tableCaptions": [{"text": " Table 3: The 10 most highly correlated features with  each label and cumulatively over all the labels using  mutual information and information gain.", "labels": [], "entities": []}, {"text": " Table 4: The optimal number of features found for  each model and the average F-score obtained using  a 10-fold cross-validation on the training data.", "labels": [], "entities": [{"text": "F-score", "start_pos": 79, "end_pos": 86, "type": "METRIC", "confidence": 0.9992325305938721}]}, {"text": " Table 5: The performance of each of classifier on the test set when all clauses are aggregated together.", "labels": [], "entities": []}, {"text": " Table 6: Performance measures for different levels of agreement among the annotators.", "labels": [], "entities": []}, {"text": " Table 6. The first  approach was inspired by the approach in (", "labels": [], "entities": []}]}