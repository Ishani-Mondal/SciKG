{"title": [], "abstractContent": [{"text": "In web corpus construction, crawling is a necessary step, and it is probably the most costly of all, because it requires expensive bandwidth usage, and excess crawling increases storage requirements.", "labels": [], "entities": [{"text": "web corpus construction", "start_pos": 3, "end_pos": 26, "type": "TASK", "confidence": 0.6754138569037119}]}, {"text": "Excess crawling results from the fact that the web contains a lot of redundant content (duplicates and near-duplicates), as well as other material not suitable or desirable for inclusion in web corpora or web indexes (for example, pages with little text or virtually no text at all).", "labels": [], "entities": [{"text": "Excess crawling", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.7264775633811951}]}, {"text": "An optimized crawler for web corpus construction would ideally avoid crawling such content in the first place, saving bandwidth, storage, and post-processing costs.", "labels": [], "entities": [{"text": "web corpus construction", "start_pos": 25, "end_pos": 48, "type": "TASK", "confidence": 0.6324352125326792}]}, {"text": "In this paper, we show in three experiments that two simple scores are suitable to improve the ratio between corpus size and crawling effort for web corpus construction.", "labels": [], "entities": [{"text": "web corpus construction", "start_pos": 145, "end_pos": 168, "type": "TASK", "confidence": 0.688165565331777}]}, {"text": "The first score is related to overall text quality of the page containing the link, the other one is related to the likelihood that the local block enclosing a link is boilerplate.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "In this experiment, we examine the correlation between the yield ratio of crawler seed URLs and the yield ratio of short Breadth-First Search (BFS) crawls based on those URLs.", "labels": [], "entities": []}, {"text": "We used the Heritrix (1.14) web crawler () and an older version of the texrex web page cleaning toolkit).", "labels": [], "entities": [{"text": "Heritrix (1.14) web crawler", "start_pos": 12, "end_pos": 39, "type": "DATASET", "confidence": 0.8833509584267935}, {"text": "texrex web page cleaning toolkit", "start_pos": 71, "end_pos": 103, "type": "TASK", "confidence": 0.6018091142177582}]}, {"text": "The tools perform, among other things, boilerplate detection and text quality evaluation in the form of the so-called Badness score ( . A document receives a low Badness score if the most frequent function words of the target language have a high enough frequency in the document.", "labels": [], "entities": [{"text": "boilerplate detection", "start_pos": 39, "end_pos": 60, "type": "TASK", "confidence": 0.9102059602737427}]}, {"text": "The Badness score is based on previous ideas from language identification and web document filtering.", "labels": [], "entities": [{"text": "Badness score", "start_pos": 4, "end_pos": 17, "type": "METRIC", "confidence": 0.8433632552623749}, {"text": "language identification", "start_pos": 50, "end_pos": 73, "type": "TASK", "confidence": 0.7355207651853561}, {"text": "web document filtering", "start_pos": 78, "end_pos": 100, "type": "TASK", "confidence": 0.6436421871185303}]}, {"text": "Originally, this experiment was carried out in the context of an evaluation of sources of different seed URLs for crawls.", "labels": [], "entities": []}, {"text": "Ina preliminary step, we began by collecting seed URLs from various sources: 1.", "labels": [], "entities": []}, {"text": "the Etools meta search engine 3.", "labels": [], "entities": [{"text": "Etools meta search engine 3", "start_pos": 4, "end_pos": 31, "type": "DATASET", "confidence": 0.8995699882507324}]}, {"text": "the FriendFeed social service aggregator 4.", "labels": [], "entities": [{"text": "FriendFeed social service aggregator", "start_pos": 4, "end_pos": 40, "type": "DATASET", "confidence": 0.9206059873104095}]}, {"text": "the identi.ca social bookmarking service 5.", "labels": [], "entities": []}, {"text": "Wikipedia dumps We scraped the content behind the URLs and ran a state-of-the-art language identifier (Lui and Baldwin, 2012) on it in order to obtain languageclassified seed URLs (.", "labels": [], "entities": []}, {"text": "We then looked specifically at the following languages associated as the single dominant language with at least one top-level domain (TLD): We randomly sampled 1, 000 seed URLs for each of the 20 permutations of seed sources and languages/TLDs, downloaded them and used texrex to determine the document yield ratio for the documents behind the 1, 000 seeds.", "labels": [], "entities": []}, {"text": "The software was configured to perform boilerplate removal, removal of documents based on high Badness scores, perfect duplicate removal, and deletion of documents shorter than 1, 000 characters (after boilerplate removal).", "labels": [], "entities": [{"text": "boilerplate removal", "start_pos": 39, "end_pos": 58, "type": "TASK", "confidence": 0.8977445363998413}, {"text": "perfect duplicate removal", "start_pos": 111, "end_pos": 136, "type": "TASK", "confidence": 0.7689910133679708}]}, {"text": "Then, we crawled the respective TLDs, starting the crawls with the 1, 000 seed URLs, respectively.", "labels": [], "entities": []}, {"text": "In each crawl, we downloaded 2 GB of raw data, cleaned them, and calculated the document yield ratio using the same configuration of texrex as we used for cleaning the seed documents.", "labels": [], "entities": []}, {"text": "plots the data and an appropriate linear model.", "labels": [], "entities": []}, {"text": "We see that there is a strong correlation (adjusted R 2 = 0.7831) between the yield ratio of the documents behind the seed URLs and the yield ratio of the documents found by using the seeds for BFS crawling.", "labels": [], "entities": [{"text": "R", "start_pos": 52, "end_pos": 53, "type": "METRIC", "confidence": 0.9786208868026733}, {"text": "BFS crawling", "start_pos": 194, "end_pos": 206, "type": "TASK", "confidence": 0.6560824513435364}]}, {"text": "It follows that giving high priority to links from pages which are themselves considered high-quality documents by the postprocessing tools will likely lead to more efficient crawling.", "labels": [], "entities": []}, {"text": "Since there is no fundamental distinction between initial URL seeds and URLs harvested at a later time during the crawl, this effect is likely to extend to the whole run time of a crawl.", "labels": [], "entities": []}, {"text": "Using the same configuration of tools as in Section 2, we performed a crawl targeting Flemish documents in the Belgian .be national TLD, which hosts both Flemish and French documents in substantial proportions.", "labels": [], "entities": [{"text": "Belgian .be national TLD", "start_pos": 111, "end_pos": 135, "type": "DATASET", "confidence": 0.9265737175941468}]}, {"text": "Usually, even under more favorable conditions (i. e., when we crawl a TLD which contains mostly documents in the target language), the yield ratio of a BFS crawl decreases rapidly in the initial phase, then staying at a low level (.", "labels": [], "entities": [{"text": "yield", "start_pos": 135, "end_pos": 140, "type": "METRIC", "confidence": 0.9619414210319519}]}, {"text": "illustrates this with an analysis of a .de BFS crawl from late 2011, also processed with the same tools as mentioned in Section 2.", "labels": [], "entities": []}, {"text": "Notice that the .de domain hosts German documents almost exclusively.", "labels": [], "entities": []}, {"text": "The interesting complication in this experiment is thus the non-target language present in the TLD scope of the crawler and the related question whether, simply speaking, predominantly Flemish documents link to other predominantly Flemish documents rather than French documents.", "labels": [], "entities": []}, {"text": "Since the Badness score (calculated as described in Section 2) includes a form of language identification, the yield ratio takes into account this additional complication.", "labels": [], "entities": [{"text": "Badness score", "start_pos": 10, "end_pos": 23, "type": "METRIC", "confidence": 0.8038245737552643}, {"text": "language identification", "start_pos": 82, "end_pos": 105, "type": "TASK", "confidence": 0.7162471115589142}, {"text": "yield", "start_pos": 111, "end_pos": 116, "type": "METRIC", "confidence": 0.9637849926948547}]}, {"text": "We tested whether the decline of the yield ratio could be compensated for by selecting \"high quality\" URLs in the following manner: The crawl progressed in five phases.", "labels": [], "entities": []}, {"text": "In the first short burnin phase, we crawled 1, 000, 000 documents, and in each of the second to fifth phase, we crawled 10, 000, 000 documents.", "labels": [], "entities": []}, {"text": "After each phase, the crawl was halted, the crawler frontier was emptied, and the crawl was then re-started with a selection of the URLs harvested in the previous phase.", "labels": [], "entities": []}, {"text": "Only those URLs were used which came from documents with a Badness score of 10 or lower (= documents in which the distribution of the most frequent function words fits the expected distribution for Flemish very well, cf. Section 2), and from text blocks with a boilerplate score () in [0.5, 1] (= likely not boilerplate).", "labels": [], "entities": [{"text": "Badness score", "start_pos": 59, "end_pos": 72, "type": "METRIC", "confidence": 0.9262226819992065}]}, {"text": "Additionally, it was made sure that no URLs were re-used between the five phases.", "labels": [], "entities": []}, {"text": "The very promising results are plotted in.: Fit of linear models for the decrease in the yield ratios of the first 100 snapshots in each of the five phases of the .be crawl.", "labels": [], "entities": []}, {"text": "For the first phase, only 50 snapshots were crawled and fitted.", "labels": [], "entities": []}, {"text": "The decline of the yield ratio is almost linear for the first 100 snapshots in the five phases (cf.), where each phase has roughly 500 snapshots in total, and one snapshot corresponds to 400 MB of downloaded raw data.", "labels": [], "entities": []}, {"text": "After this decline, the yield ratio remains at low levels around 0.05.", "labels": [], "entities": [{"text": "yield ratio", "start_pos": 24, "end_pos": 35, "type": "METRIC", "confidence": 0.9758883714675903}]}, {"text": "Cyclic URL selection, however, repeatedly manages to push the yield ratio to above 0.2 fora short period.", "labels": [], "entities": [{"text": "yield", "start_pos": 62, "end_pos": 67, "type": "METRIC", "confidence": 0.9702900648117065}]}, {"text": "The subsequent sharp decline shows that link selection/prioritization should rather be implemented in the crawler frontier management in order to achieve a constant effect over longer crawls (cf. Section 5).", "labels": [], "entities": [{"text": "link selection", "start_pos": 40, "end_pos": 54, "type": "TASK", "confidence": 0.7436082363128662}]}, {"text": "For the last experiment, we used the most recent version of the texrex toolkit, which writes full link structures for the processed documents as a by-product.", "labels": [], "entities": []}, {"text": "An internal analysis of a small portion of a crawled data set from the German TLD was performed, which is part of the raw material of the DECOW corpus ().", "labels": [], "entities": [{"text": "crawled data set from the German TLD", "start_pos": 45, "end_pos": 81, "type": "DATASET", "confidence": 0.8424795099667141}, {"text": "DECOW corpus", "start_pos": 138, "end_pos": 150, "type": "DATASET", "confidence": 0.9600296914577484}]}, {"text": "The data set contains 11, 557, 695 crawled HTML documents and 81, 255, 876 http links extracted from the crawled documents (only <a> tags).", "labels": [], "entities": []}, {"text": "Among the link URLs in the sample, 711, 092 are actually links to documents in the sample, so we could analyze exactly those 711, 092 links.", "labels": [], "entities": []}, {"text": "It should be noticed that we only looked at links to different hosts, such that hostinternal links (navigation to \"Home\", etc.) are not included in the analysis.", "labels": [], "entities": []}, {"text": "In this experiment, we were interested specifically in the many documents which we usually discard right away simply because they are either very short (below 2 KB of unstripped HTML) or perfect duplicates of other documents.", "labels": [], "entities": []}, {"text": "This is a The new version (release name hyperhyper) has been released and documented at http://texrex.sf.net/.", "labels": [], "entities": []}, {"text": "The observable correlation between the quality of a link's context and the quality of the page behind the link is stronger for the boilerplate score than for the Badness score.", "labels": [], "entities": []}, {"text": "For example, had we only followed links from documents with a Badness score of 10 or lower (= better), then show a confusion matrix fora reasonable Badness threshold (10) and a reasonable boilerplate threshold (0.5).", "labels": [], "entities": []}, {"text": "Obviously, if we use Badness and boilerplate scores of the link context to make a binary download decision, the accuracy is much too low, which is why we suggest to merely prioritize URLs instead of discarding them, cf. Section 5.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 112, "end_pos": 120, "type": "METRIC", "confidence": 0.9993502497673035}]}], "tableCaptions": [{"text": " Table 1: Fit of linear models for the decrease in  the yield ratios of the first 100 snapshots in each  of the five phases of the .be crawl. For the first  phase, only 50 snapshots were crawled and fitted.", "labels": [], "entities": []}, {"text": " Table 2: Confusion matrix for binary download  decisions based on the Badness of the document  containing the URL for the DECOW crawl sam- ple described in Section 4. Badness threshold at  10. Precision=0.225, Recall=0.530, F 1 =0.316.", "labels": [], "entities": [{"text": "DECOW crawl sam- ple", "start_pos": 123, "end_pos": 143, "type": "DATASET", "confidence": 0.8922599673271179}, {"text": "Badness threshold", "start_pos": 168, "end_pos": 185, "type": "METRIC", "confidence": 0.9338157474994659}, {"text": "Precision", "start_pos": 194, "end_pos": 203, "type": "METRIC", "confidence": 0.9984739422798157}, {"text": "Recall", "start_pos": 211, "end_pos": 217, "type": "METRIC", "confidence": 0.9994589686393738}, {"text": "F 1", "start_pos": 225, "end_pos": 228, "type": "METRIC", "confidence": 0.9956304132938385}]}, {"text": " Table 3: Confusion matrix for binary down- load decisions based on the boilerplate score of  the block containing the URL for the DECOW  crawl sample described in Section 4. Boilerplate  threshold at 0.5. Precision=0.590, Recall=0.640,  F 1 =0.614.", "labels": [], "entities": [{"text": "DECOW  crawl sample", "start_pos": 131, "end_pos": 150, "type": "DATASET", "confidence": 0.8781441648801168}, {"text": "Boilerplate  threshold", "start_pos": 175, "end_pos": 197, "type": "METRIC", "confidence": 0.9678966701030731}, {"text": "Precision", "start_pos": 206, "end_pos": 215, "type": "METRIC", "confidence": 0.9965624213218689}, {"text": "Recall", "start_pos": 223, "end_pos": 229, "type": "METRIC", "confidence": 0.9994848966598511}, {"text": "F 1", "start_pos": 238, "end_pos": 241, "type": "METRIC", "confidence": 0.9954501688480377}]}]}