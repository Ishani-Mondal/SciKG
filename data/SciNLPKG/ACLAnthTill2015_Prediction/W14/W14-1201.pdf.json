{"title": [{"text": "One Step Closer to Automatic Evaluation of Text Simplification Systems", "labels": [], "entities": [{"text": "Automatic Evaluation of Text Simplification", "start_pos": 19, "end_pos": 62, "type": "TASK", "confidence": 0.6119855165481567}]}], "abstractContent": [{"text": "This study explores the possibility of replacing the costly and time-consuming human evaluation of the grammaticality and meaning preservation of the output of text simplification (TS) systems with some automatic measures.", "labels": [], "entities": [{"text": "text simplification (TS)", "start_pos": 160, "end_pos": 184, "type": "TASK", "confidence": 0.7929578959941864}]}, {"text": "The focus is on six widely used machine translation (MT) evaluation metrics and their correlation with human judgements of grammatical-ity and meaning preservation in text snippets.", "labels": [], "entities": [{"text": "machine translation (MT) evaluation", "start_pos": 32, "end_pos": 67, "type": "TASK", "confidence": 0.8724998831748962}, {"text": "meaning preservation in text snippets", "start_pos": 143, "end_pos": 180, "type": "TASK", "confidence": 0.8226291179656983}]}, {"text": "As the results show a significant correlation between them, we go further and try to classify simplified sentences into: (1) those which are acceptable; (2) those which need minimal post-editing; and (3) those which should be discarded.", "labels": [], "entities": []}, {"text": "The preliminary results, reported in this paper, are promising.", "labels": [], "entities": []}], "introductionContent": [{"text": "Lexically and syntactically complex sentences can be difficult to understand for non-native speakers, and for people with language impairments, e.g. people diagnosed with aphasia, autism spectrum disorder), dyslexia, congenital deafness, and intellectual disability.", "labels": [], "entities": []}, {"text": "At the same time, long and complex sentences are also a stumbling block for many NLP tasks and applications such as parsing, machine translation, information retrieval, and summarisation (.", "labels": [], "entities": [{"text": "parsing", "start_pos": 116, "end_pos": 123, "type": "TASK", "confidence": 0.9799455404281616}, {"text": "machine translation", "start_pos": 125, "end_pos": 144, "type": "TASK", "confidence": 0.7966248393058777}, {"text": "information retrieval", "start_pos": 146, "end_pos": 167, "type": "TASK", "confidence": 0.8253044188022614}, {"text": "summarisation", "start_pos": 173, "end_pos": 186, "type": "TASK", "confidence": 0.9930155277252197}]}, {"text": "This justifies the need for Text Simplification (TS) systems which would convert such sentences into their simpler and easier-to-read variants, while at the same time preserving the original meaning.", "labels": [], "entities": [{"text": "Text Simplification (TS)", "start_pos": 28, "end_pos": 52, "type": "TASK", "confidence": 0.8327736735343934}]}, {"text": "So far, TS systems have been developed for English),), and Portuguese (, with recent attempts at Basque (), Swedish (),, and Italian (.", "labels": [], "entities": [{"text": "TS", "start_pos": 8, "end_pos": 10, "type": "TASK", "confidence": 0.9326432943344116}]}, {"text": "Usually, TS systems are either evaluated for: (1) the quality of the generated output, or (2) the effectiveness/usefulness of such simplification on reading speed and comprehension of the target population.", "labels": [], "entities": [{"text": "TS", "start_pos": 9, "end_pos": 11, "type": "TASK", "confidence": 0.9758256077766418}]}, {"text": "For the purpose of this study we focused only on the former.", "labels": [], "entities": []}, {"text": "The quality of the output generated by TS systems is commonly evaluated by using a combination of readability metrics (measuring the degree of simplification) and human assessment (measuring the grammaticality and meaning preservation).", "labels": [], "entities": [{"text": "TS", "start_pos": 39, "end_pos": 41, "type": "TASK", "confidence": 0.9500478506088257}, {"text": "meaning preservation", "start_pos": 214, "end_pos": 234, "type": "TASK", "confidence": 0.6649782210588455}]}, {"text": "Despite the noticeable similarity between evaluation of the fluency and adequacy of a machine translation (MT) output, and evaluation of grammaticality and meaning preservation of a TS system output, there have been no works exploring whether any of the MT evaluation metrics are well correlated with the latter, and could thus replace the time-consuming human assessment.", "labels": [], "entities": [{"text": "machine translation (MT) output", "start_pos": 86, "end_pos": 117, "type": "TASK", "confidence": 0.7956513663132986}, {"text": "MT evaluation", "start_pos": 254, "end_pos": 267, "type": "TASK", "confidence": 0.910174548625946}]}, {"text": "The contributions of the present work are the following: \u2022 It is the first study to explore the possibility of replacing human assessment of the quality of TS system output with automatic evaluation.", "labels": [], "entities": [{"text": "TS system output", "start_pos": 156, "end_pos": 172, "type": "TASK", "confidence": 0.845158855120341}]}, {"text": "\u2022 It is the first study to investigate the correlation of human assessment of TS system output with MT evaluation metrics.", "labels": [], "entities": [{"text": "TS system output", "start_pos": 78, "end_pos": 94, "type": "TASK", "confidence": 0.8189409176508585}, {"text": "MT evaluation", "start_pos": 100, "end_pos": 113, "type": "TASK", "confidence": 0.8953084349632263}]}, {"text": "\u2022 It proposes a decision-making procedure for the classification of simplified sentences into: (1) those which are acceptable; (2) those which need further post-editing; and (3) those which should be discarded.", "labels": [], "entities": [{"text": "classification of simplified sentences", "start_pos": 50, "end_pos": 88, "type": "TASK", "confidence": 0.8419444113969803}]}], "datasetContent": [{"text": "The dataset contains 280 pairs of original sentences and their corresponding simplified versions annotated by humans for grammaticality, meaning preservation, and simplicity of the simplified version.", "labels": [], "entities": [{"text": "meaning preservation", "start_pos": 137, "end_pos": 157, "type": "TASK", "confidence": 0.7611169517040253}, {"text": "simplicity", "start_pos": 163, "end_pos": 173, "type": "METRIC", "confidence": 0.9744307398796082}]}, {"text": "We used all sentence pairs, focusing only on four out of eight available features: (1) the original text, (2) the simplified text, (3) the grammaticality score, and  The simplified versions of original sentences were obtained by using four different simplification methods: baseline, sentence-wise, eventwise, and pronominal anaphora.", "labels": [], "entities": []}, {"text": "The baseline retains only the main clause of a sentence, and discards all subordinate clauses, based on the output of the Stanford constituency parser.", "labels": [], "entities": []}, {"text": "Sentence-wise simplification eliminates all those tokens in the original sentence that do not belong to any of the extracted factual event mentions, while the event-wise simplification transforms each factual event mention into a separate sentence of the output.", "labels": [], "entities": []}, {"text": "The last simplification scheme (pronominal anaphora) additionally employs pronominal anaphora resolution on top of the event-wise simplification scheme.", "labels": [], "entities": [{"text": "pronominal anaphora resolution", "start_pos": 74, "end_pos": 104, "type": "TASK", "confidence": 0.626427561044693}]}, {"text": "The original dataset (Original) contains separate scores for grammaticality (G), meaning preservation (M), and simplicity (S), each of them on a 1-3 scale.", "labels": [], "entities": [{"text": "meaning preservation (M)", "start_pos": 81, "end_pos": 105, "type": "METRIC", "confidence": 0.6984151482582093}, {"text": "simplicity (S)", "start_pos": 111, "end_pos": 125, "type": "METRIC", "confidence": 0.9434168040752411}]}, {"text": "From this dataset we derived two additional ones: Total3 and Total2.", "labels": [], "entities": [{"text": "Total3", "start_pos": 50, "end_pos": 56, "type": "DATASET", "confidence": 0.8433914184570312}, {"text": "Total2", "start_pos": 61, "end_pos": 67, "type": "DATASET", "confidence": 0.9239816665649414}]}, {"text": "The Total3 dataset contains three marks (OKuse as it is, PE -post-editing required, and Dis -discard) derived from G and Min the Original dataset.", "labels": [], "entities": [{"text": "Total3 dataset", "start_pos": 4, "end_pos": 18, "type": "DATASET", "confidence": 0.978352427482605}, {"text": "PE -post-editing required", "start_pos": 57, "end_pos": 82, "type": "METRIC", "confidence": 0.9426318854093552}, {"text": "Dis -discard)", "start_pos": 88, "end_pos": 101, "type": "METRIC", "confidence": 0.9600629806518555}, {"text": "Original dataset", "start_pos": 129, "end_pos": 145, "type": "DATASET", "confidence": 0.8264748752117157}]}, {"text": "Those simplified sentences which scored '3' for both meaning preservation (M) and grammaticality (G) are placed in the OK class as they do not need any kind of post-editing.", "labels": [], "entities": [{"text": "meaning preservation (M", "start_pos": 53, "end_pos": 76, "type": "TASK", "confidence": 0.667559027671814}, {"text": "grammaticality (G)", "start_pos": 82, "end_pos": 100, "type": "METRIC", "confidence": 0.791834369301796}]}, {"text": "A closer look at the remaining sentences suggests that any simplified sentence which got a score '2' or '3' for meaning preservation (M) could be easily postedited, i.e. it requires minimal changes which are obvious from its comparison to the corresponding original.", "labels": [], "entities": [{"text": "meaning preservation (M)", "start_pos": 112, "end_pos": 136, "type": "TASK", "confidence": 0.6912248730659485}]}, {"text": "For instance, in the sentence (b) in Table 2 the only change that needs to be made is adding the word \"up\" after \"signed\".", "labels": [], "entities": []}, {"text": "Those sentences which scored '2' for meaning need slightly more, albeit simple modification.", "labels": [], "entities": []}, {"text": "The simplified text snippet (c) in would need \"but did not enter a plea\" added at the end of the last sentence.", "labels": [], "entities": []}, {"text": "The next sentence (d) in the same table needs a few more changes, but still very minor ones: adding the word \"capture\" after \"had evaded\", adding the preposition \"on\" before \"the run\", and adding \"when\" after \"last year\".", "labels": [], "entities": []}, {"text": "Therefore, we grouped all those sentences into one class -PE (sentences which require a minimal postediting effort).", "labels": [], "entities": [{"text": "PE", "start_pos": 58, "end_pos": 60, "type": "METRIC", "confidence": 0.9497249722480774}]}, {"text": "Those sentences which scored '1' for meaning need to either be left in their original form or simplified from scratch.", "labels": [], "entities": []}, {"text": "We thus classify them as Dis.", "labels": [], "entities": []}, {"text": "This newly created dataset (Total3) allows us to investigate whether we could automatically classify simplified sentences into those three categories, taking into account both grammaticality and meaning preservation at the same time.", "labels": [], "entities": []}, {"text": "The Total2 dataset contains only two marks ('0' and '1') which correspond to the sentences which should be discarded ('0') and those which should be retained ('1'), where '0' corresponds to Dis in Total3, and '1' corresponds to the union of OK and PE in Total3.", "labels": [], "entities": [{"text": "Total2 dataset", "start_pos": 4, "end_pos": 18, "type": "DATASET", "confidence": 0.960361659526825}]}, {"text": "The derivation procedure for both datasets is presented in.", "labels": [], "entities": []}, {"text": "We wanted to investigate whether the classification task would be simpler (better performed) if there were only two classes instead of three.", "labels": [], "entities": [{"text": "classification task", "start_pos": 37, "end_pos": 56, "type": "TASK", "confidence": 0.9119932651519775}]}, {"text": "In the case that such classification could be performed with satisfactory accuracy, all sentences classified as '0' would be left in their original form or simplified with some different simplification strategy, while those classified as '1' would be sent fora quick human postediting procedure.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.9930881857872009}]}, {"text": "Original Here it is important to mention that we decided not to use human scores for simplicity (S) for several reasons.", "labels": [], "entities": [{"text": "simplicity", "start_pos": 85, "end_pos": 95, "type": "METRIC", "confidence": 0.9927356243133545}]}, {"text": "First, simplicity was defined as the amount of irrelevant information which was eliminated.", "labels": [], "entities": [{"text": "simplicity", "start_pos": 7, "end_pos": 17, "type": "METRIC", "confidence": 0.9982424974441528}]}, {"text": "Therefore, we cannot expect that any of the six MT evaluation metrics would have a significant correlation with this score (except maybe TERp and, in particular, one of its parts -'number of deletions'.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 48, "end_pos": 61, "type": "TASK", "confidence": 0.8749048113822937}, {"text": "TERp", "start_pos": 137, "end_pos": 141, "type": "METRIC", "confidence": 0.9970180988311768}]}, {"text": "However, none of the two demonstrated any significant correlation with the simplicity score, and those results are thus not reported in this paper).", "labels": [], "entities": [{"text": "simplicity score", "start_pos": 75, "end_pos": 91, "type": "METRIC", "confidence": 0.9819144010543823}]}, {"text": "Second, the output sentences with a low simplicity score are not as detrimental for the TS system as those with a low grammaticality or meaning preservation score.", "labels": [], "entities": [{"text": "simplicity score", "start_pos": 40, "end_pos": 56, "type": "METRIC", "confidence": 0.974296361207962}, {"text": "TS", "start_pos": 88, "end_pos": 90, "type": "TASK", "confidence": 0.873159646987915}, {"text": "meaning preservation", "start_pos": 136, "end_pos": 156, "type": "TASK", "confidence": 0.6156724095344543}]}, {"text": "The sentences with a low simplicity score would simply not help the target user read faster or understand better, but would not do any harm either.", "labels": [], "entities": [{"text": "simplicity score", "start_pos": 25, "end_pos": 41, "type": "METRIC", "confidence": 0.9814348220825195}]}, {"text": "Alternatively, if the target \"user\" is an MT or information extraction (IE) system, or a parser for example, such sentences would not lower the performance of the system; they would just not improve it.", "labels": [], "entities": [{"text": "MT or information extraction (IE)", "start_pos": 42, "end_pos": 75, "type": "TASK", "confidence": 0.8076050281524658}]}, {"text": "Low scores for G and M, however, would lead to a worse performance for such NLP systems, longer reading time, and a worse or erroneous understanding of the text.", "labels": [], "entities": []}, {"text": "Third, the simplicity of the output (or complexity reduction performed by a TS system) could be evaluated separately, in a fully automatic manner -using some readability measures or average sentence length as features (as in) for example).", "labels": [], "entities": [{"text": "simplicity", "start_pos": 11, "end_pos": 21, "type": "METRIC", "confidence": 0.9755882024765015}]}, {"text": "In all experiments, we focused on six commonly used MT evaluation metrics.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 52, "end_pos": 65, "type": "TASK", "confidence": 0.9280882775783539}]}, {"text": "These are cosine similarity (using the bag-of-words representation), METEOR (Denkowski and Lavie, 2011), TERp (Snover et al., 2009), TINE (Rios et al., 2011), and two components of TINE: T-BLEU (which differs from the standard BLEU () by using 3-grams, 2-grams, and 1-grams when there are no 4-grams found, where the \"original\" BLEU would give score '0') and SRL (which is the component of TINE based on semantic role labeling using SENNA 4 ).", "labels": [], "entities": [{"text": "METEOR", "start_pos": 69, "end_pos": 75, "type": "METRIC", "confidence": 0.9810309410095215}, {"text": "TERp", "start_pos": 105, "end_pos": 109, "type": "METRIC", "confidence": 0.9950060248374939}, {"text": "TINE", "start_pos": 133, "end_pos": 137, "type": "METRIC", "confidence": 0.8698903918266296}, {"text": "T-BLEU", "start_pos": 187, "end_pos": 193, "type": "METRIC", "confidence": 0.8565442562103271}, {"text": "BLEU", "start_pos": 227, "end_pos": 231, "type": "METRIC", "confidence": 0.9927417039871216}, {"text": "BLEU", "start_pos": 328, "end_pos": 332, "type": "METRIC", "confidence": 0.979762613773346}, {"text": "SRL", "start_pos": 359, "end_pos": 362, "type": "METRIC", "confidence": 0.9882718920707703}]}, {"text": "Although these two components contribute equally to TINE (thus being linearly correlated with TINE), we wanted to investigate which one of them contributes more to the correlation of TINE with human judgements.", "labels": [], "entities": [{"text": "TINE", "start_pos": 52, "end_pos": 56, "type": "METRIC", "confidence": 0.8782817721366882}]}, {"text": "Given their different natures, we expect T-BLEU to contribute more to the correlation of TINE with human judgements of grammaticality, and SRL to contribute more to the correlation of TINE with human judgements of meaning preservation.", "labels": [], "entities": [{"text": "SRL", "start_pos": 139, "end_pos": 142, "type": "METRIC", "confidence": 0.5118108987808228}, {"text": "meaning preservation", "start_pos": 214, "end_pos": 234, "type": "TASK", "confidence": 0.7840020060539246}]}, {"text": "As we do not have the reference for the simplified sentence, all metrics are applied in a slightly different way than in MT.", "labels": [], "entities": []}, {"text": "Instead of evaluating the translation hypothesis (output of the automatic TS system in our case) with the corresponding reference translation (which would be a 'gold standard' simplified sentence), we apply the metrics to the output of the automatic TS system comparing it with the corresponding original sentence.", "labels": [], "entities": []}, {"text": "Given that the simplified sentences in the used dataset are usually shorter than the original ones (due to the elimination of irrelevant content which was the main focus of the TS system proposed by Glava\u0161 and\u0160tajnerand\u02c7and\u0160tajner (2013)), we expect low scores of T-BLEU and METEOR which apply a brevity penalty.", "labels": [], "entities": [{"text": "T-BLEU", "start_pos": 264, "end_pos": 270, "type": "METRIC", "confidence": 0.8646298050880432}, {"text": "METEOR", "start_pos": 275, "end_pos": 281, "type": "METRIC", "confidence": 0.9783636331558228}]}, {"text": "However, our dataset does not contain any kind of lexical simplification, but rather copies all relevant information from the original sentence . Therefore, we expect the exact matches of word forms and semantic role labels (which are components of the MT evaluation metrics) to have a good correlation to human judgements of grammaticality and meaning preservation.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 253, "end_pos": 266, "type": "TASK", "confidence": 0.8862158060073853}, {"text": "meaning preservation", "start_pos": 345, "end_pos": 365, "type": "TASK", "confidence": 0.7207896411418915}]}, {"text": "http://ml.nec-labs.com/senna/ The exceptions being changes of gerundive forms into past tense, and anaphoric pronoun resolution in some simplification schemes.", "labels": [], "entities": [{"text": "anaphoric pronoun resolution", "start_pos": 99, "end_pos": 127, "type": "TASK", "confidence": 0.7204806208610535}]}, {"text": "See Section 3.1 and (Glava\u0161 and\u0160tajnerand\u02c7and\u0160tajner, 2013) for more details.", "labels": [], "entities": []}, {"text": "The six experiments conducted in this study are presented in.", "labels": [], "entities": []}, {"text": "The first two experiments had the aim of answering the first question (Section 3.5) as to whether the chosen MT metrics correlate with the human judgements of grammaticality (G) and meaning preservation (M) of the TS system output.", "labels": [], "entities": [{"text": "MT", "start_pos": 109, "end_pos": 111, "type": "TASK", "confidence": 0.9458657503128052}]}, {"text": "The results were obtained in terms of Pearson's, Kendall's and Spearman's correlation coefficients.", "labels": [], "entities": [{"text": "Pearson", "start_pos": 38, "end_pos": 45, "type": "METRIC", "confidence": 0.8696731328964233}, {"text": "Spearman's correlation", "start_pos": 63, "end_pos": 85, "type": "METRIC", "confidence": 0.6031430065631866}]}, {"text": "The third and the fourth experiments  Correlation of the six automatic MT metrics with the human scores for Meaning preservation 3.", "labels": [], "entities": [{"text": "MT", "start_pos": 71, "end_pos": 73, "type": "TASK", "confidence": 0.9765080213546753}, {"text": "Meaning preservation", "start_pos": 108, "end_pos": 128, "type": "TASK", "confidence": 0.9799338579177856}]}, {"text": "Classification of the simplified sentences into 3 classes ('1' -Bad, '2' -Medium, and '3' -Good) according to their Grammaticality 4.", "labels": [], "entities": [{"text": "Grammaticality", "start_pos": 116, "end_pos": 130, "type": "METRIC", "confidence": 0.9173052310943604}]}, {"text": "Classification of the simplified sentences into 3 classes ('1' -Bad, '2' -Medium, and '3' -Good) according to their Meaning preservation 5.", "labels": [], "entities": []}, {"text": "Classification of the simplified sentences into 3 classes (OK, PE, Dis) according to their Total3 score 6.", "labels": [], "entities": [{"text": "PE", "start_pos": 63, "end_pos": 65, "type": "METRIC", "confidence": 0.8808432817459106}, {"text": "Dis", "start_pos": 67, "end_pos": 70, "type": "METRIC", "confidence": 0.894426703453064}, {"text": "Total3 score", "start_pos": 91, "end_pos": 103, "type": "METRIC", "confidence": 0.8288795948028564}]}, {"text": "Classification of the simplified sentences into 2 classes ('1' -Retain, '0' -Discard) according to their Total2 score: Experiments as to whether we could automatically classify the simplified sentences into those which are: (1) correct (OK), (2) require minimal post-editing (PE), and (3) incorrect and need to be discarded (Dis).", "labels": [], "entities": [{"text": "Total2 score", "start_pos": 105, "end_pos": 117, "type": "METRIC", "confidence": 0.5573872476816177}]}, {"text": "The last experiment was conducted with the aim of exploring whether the classification of simplified sentences into only two classes -Retain (for further post-editing) and Discard -would lead to better results than the classification into three classes (OK, PE, and Dis) in the fifth experiment.", "labels": [], "entities": [{"text": "Retain", "start_pos": 134, "end_pos": 140, "type": "METRIC", "confidence": 0.9351152181625366}, {"text": "PE", "start_pos": 258, "end_pos": 260, "type": "METRIC", "confidence": 0.7615476250648499}, {"text": "Dis", "start_pos": 266, "end_pos": 269, "type": "METRIC", "confidence": 0.9119102954864502}]}, {"text": "All classification experiments were performed in Weka workbench), using seven classification algorithms in a 10-fold cross-validation setup: \u2022 NB -NaiveBayes (), \u2022 SMO -Weka implementation of Support Vector Machines () with normalisation (n) or with standardisation (s), \u2022 Logistic (le Cessie and van Houwelingen, 1992), \u2022 Lazy.IBk -K-nearest neighbours, \u2022 JRip -a propositional rule learner, \u2022 J48 -Weka implementation of C4.5 (Quinlan, 1993).", "labels": [], "entities": []}, {"text": "As a baseline we use the classifier which assigns the most frequent (majority) class to all instances.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: IAA from (Glava\u0161 and\u0160tajnerand\u02c7and\u0160tajner, 2013)", "labels": [], "entities": [{"text": "IAA from (Glava\u0161 and\u0160tajnerand\u02c7and\u0160tajner, 2013)", "start_pos": 10, "end_pos": 58, "type": "DATASET", "confidence": 0.6760244742035866}]}, {"text": " Table 2: Human evaluation examples (G, M, and S correspond to the human scores for grammaticality,  meaning preservation and simplicity, and SM denotes the simplification method used: B -baseline, S - sentence-wise, E -event-wise, and P -pronominal anaphora)", "labels": [], "entities": [{"text": "meaning preservation", "start_pos": 101, "end_pos": 121, "type": "TASK", "confidence": 0.7418746948242188}, {"text": "simplicity", "start_pos": 126, "end_pos": 136, "type": "METRIC", "confidence": 0.9904616475105286}]}, {"text": " Table 5: Correlation between automatic evaluation  metrics and human scores for grammaticality", "labels": [], "entities": []}, {"text": " Table 6: Correlation between automatic evaluation  metrics and human scores for meaning preserva- tion", "labels": [], "entities": [{"text": "meaning preserva- tion", "start_pos": 81, "end_pos": 103, "type": "TASK", "confidence": 0.7706919983029366}]}, {"text": " Table 7: Classification results (the best performances are shown in bold; baseline uses the majority class)", "labels": [], "entities": []}, {"text": " Table 8: Confusion matrices for the best classifications according to Grammaticality (Lazy.IBk) and  Meaning (Logistic). The number of \"severe\" classification mistakes (classifying Good as Bad or vice  versa) are presented in bold.", "labels": [], "entities": [{"text": "Grammaticality", "start_pos": 71, "end_pos": 85, "type": "METRIC", "confidence": 0.9388228058815002}, {"text": "Meaning", "start_pos": 102, "end_pos": 109, "type": "METRIC", "confidence": 0.9775388836860657}]}, {"text": " Table 9: Confusion matrix for the best classifica- tion according to Total3 (Logistic). The number of  \"severe\" classification mistakes (classifying Good  as Bad or vice versa) are presented in bold.", "labels": [], "entities": [{"text": "Confusion", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.944513738155365}, {"text": "Total3", "start_pos": 70, "end_pos": 76, "type": "DATASET", "confidence": 0.9536650776863098}]}, {"text": " Table 10: Confusion matrix for the best classifi- cation according to Total2 (Logistic). The num- ber of \"severe\" classification mistakes (classifying  Retain as Discard or vice versa) are presented in  bold.", "labels": [], "entities": [{"text": "Total2", "start_pos": 71, "end_pos": 77, "type": "DATASET", "confidence": 0.9343146085739136}]}]}