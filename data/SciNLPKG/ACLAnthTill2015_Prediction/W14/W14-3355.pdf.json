{"title": [{"text": "RED: DCU-CASICT Participation in WMT2014 Metrics Task", "labels": [], "entities": [{"text": "WMT2014 Metrics", "start_pos": 33, "end_pos": 48, "type": "TASK", "confidence": 0.5931141078472137}]}], "abstractContent": [{"text": "Based on the last year's DCU-CASIST participation on WMT metrics task, we further improve our model in the following ways: 1) parameter tuning 2) support languages other than English.", "labels": [], "entities": [{"text": "DCU-CASIST", "start_pos": 25, "end_pos": 35, "type": "DATASET", "confidence": 0.8859917521476746}, {"text": "WMT", "start_pos": 53, "end_pos": 56, "type": "TASK", "confidence": 0.6388561725616455}]}, {"text": "We tuned our system on all the data of WMT 2010, 2012 and 2013.", "labels": [], "entities": [{"text": "WMT 2010", "start_pos": 39, "end_pos": 47, "type": "DATASET", "confidence": 0.9085563123226166}]}, {"text": "The tuning results as well as the WMT 2014 test results are reported.", "labels": [], "entities": [{"text": "WMT 2014 test", "start_pos": 34, "end_pos": 47, "type": "DATASET", "confidence": 0.8788743019104004}]}], "introductionContent": [{"text": "Automatic evaluation plays a more and more important role in the evolution of machine translation.", "labels": [], "entities": [{"text": "Automatic evaluation", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.7096973359584808}, {"text": "machine translation", "start_pos": 78, "end_pos": 97, "type": "TASK", "confidence": 0.7957275211811066}]}, {"text": "There are roughly two categories can be seen: namely lexical information based and structural information based.", "labels": [], "entities": []}, {"text": "1) Lexical information based approaches, among which, BLEU (?), Translation Error Rate (TER) (?) and METEOR (?) are the most popular ones and, with simplicity as their merits, cannot adequately reflect the structural level similarity.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 54, "end_pos": 58, "type": "METRIC", "confidence": 0.9986631870269775}, {"text": "Translation Error Rate (TER)", "start_pos": 64, "end_pos": 92, "type": "METRIC", "confidence": 0.8471863468488058}, {"text": "METEOR", "start_pos": 101, "end_pos": 107, "type": "METRIC", "confidence": 0.9744659066200256}]}, {"text": "2) A lot of structural level based methods have been exploited to overcome the weakness of the lexical based methods, including Syntactic Tree Model(STM)(?), a constituent tree based approach, and Head Word Chain Model(HWCM)(?), a dependency tree based approach.", "labels": [], "entities": []}, {"text": "Both of the methods compute the similarity between the sub-trees of the hypothesis and the reference.", "labels": [], "entities": [{"text": "similarity", "start_pos": 32, "end_pos": 42, "type": "METRIC", "confidence": 0.9701852798461914}]}, {"text": "presented a method using the Lexical-Functional Grammar (LFG) dependency tree.", "labels": [], "entities": []}, {"text": "MAXSIM (?) and the method proposed by Zhu et al (?) also employed the syntactic information in association with lexical information.", "labels": [], "entities": [{"text": "MAXSIM", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.8036584854125977}]}, {"text": "As we know that the hypothesis is potentially noisy, and these errors are enlarged through the parsing process.", "labels": [], "entities": []}, {"text": "Thus the power of syntactic information could be considerably weakened.", "labels": [], "entities": []}, {"text": "In this paper, based on our attempt on WMT metrics task 2013 (?), we propose a metrics named RED ( REference Dependency based automatic evaluation method).", "labels": [], "entities": [{"text": "WMT metrics task 2013", "start_pos": 39, "end_pos": 60, "type": "DATASET", "confidence": 0.6719929277896881}, {"text": "RED", "start_pos": 93, "end_pos": 96, "type": "METRIC", "confidence": 0.9440257549285889}]}, {"text": "Our metrics employs only the reference dependency tree which contains both the lexical and syntactic information, leaving the hypothesis side unparsed to avoid error propagation.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we report the experimental results of RED on the tuning set, which is the combination of WMT2010, WMT2012 and WMT2013 data set, as well as the test results on the WMT2014.", "labels": [], "entities": [{"text": "RED", "start_pos": 54, "end_pos": 57, "type": "METRIC", "confidence": 0.5053271651268005}, {"text": "WMT2010", "start_pos": 105, "end_pos": 112, "type": "DATASET", "confidence": 0.9741570949554443}, {"text": "WMT2012", "start_pos": 114, "end_pos": 121, "type": "DATASET", "confidence": 0.9168059229850769}, {"text": "WMT2013 data set", "start_pos": 126, "end_pos": 142, "type": "DATASET", "confidence": 0.9777595400810242}, {"text": "WMT2014", "start_pos": 179, "end_pos": 186, "type": "DATASET", "confidence": 0.9883676171302795}]}, {"text": "Both the sentence level evaluation and the system level evaluation are conducted to assess the performance of our automatic metrics.", "labels": [], "entities": []}, {"text": "At the sentence level evaluation, Kendall's rank correlation coefficient \u03c4 is used.", "labels": [], "entities": [{"text": "Kendall's rank correlation coefficient \u03c4", "start_pos": 34, "end_pos": 74, "type": "METRIC", "confidence": 0.7303315997123718}]}, {"text": "At the system level evaluation, the Spearman's rank correlation coefficient \u03c1 is used.", "labels": [], "entities": [{"text": "Spearman's rank correlation coefficient \u03c1", "start_pos": 36, "end_pos": 77, "type": "METRIC", "confidence": 0.7492324759562811}]}, {"text": "Kendall's rank correlation coefficient \u03c4 is employed to evaluate the correlation of all the MT evaluation metrics and human judgements at the sentence level.", "labels": [], "entities": [{"text": "rank correlation coefficient \u03c4", "start_pos": 10, "end_pos": 40, "type": "METRIC", "confidence": 0.7965972051024437}, {"text": "MT evaluation", "start_pos": 92, "end_pos": 105, "type": "TASK", "confidence": 0.898564338684082}]}, {"text": "A higher value of \u03c4 means a better ranking similarity with the human judges.", "labels": [], "entities": []}, {"text": "The correlation scores of are shown in.", "labels": [], "entities": [{"text": "correlation", "start_pos": 4, "end_pos": 15, "type": "METRIC", "confidence": 0.9891806244850159}]}, {"text": "Our method performs best when maximum length of dep-n-gram is set to 3, so we present only the results when the maximum length of dep-n-gram equals 3.", "labels": [], "entities": []}, {"text": "We also evaluated the RED scores with the human rankings at the system level to further investigate the effectiveness of our metrics.", "labels": [], "entities": [{"text": "RED", "start_pos": 22, "end_pos": 25, "type": "METRIC", "confidence": 0.5423741936683655}]}, {"text": "The matching of the words in RED is correlated with the position of the words, so the traditional way of computing system level score, like what BLEU does, is not feasible for RED.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 145, "end_pos": 149, "type": "METRIC", "confidence": 0.9944226145744324}, {"text": "RED", "start_pos": 176, "end_pos": 179, "type": "TASK", "confidence": 0.9037184119224548}]}, {"text": "Therefore, we resort to the way of adding the sentence level scores together to obtain the system level score.", "labels": [], "entities": []}, {"text": "At system level evaluation, we employ Spearman's rank correlation coefficient \u03c1.", "labels": [], "entities": [{"text": "Spearman's rank correlation coefficient \u03c1", "start_pos": 38, "end_pos": 79, "type": "METRIC", "confidence": 0.6537339289983114}]}, {"text": "The correlations and the average scores are shown in.", "labels": [], "entities": []}, {"text": "From, we can see similar trends as in with the following difference: firstly, without parameter tuning, RED perform comparably with METEOR on all the three tuning sets; secondly, on test set, RED also perform comparably with METEOR.", "labels": [], "entities": [{"text": "RED", "start_pos": 104, "end_pos": 107, "type": "METRIC", "confidence": 0.9672300219535828}, {"text": "METEOR", "start_pos": 132, "end_pos": 138, "type": "METRIC", "confidence": 0.842134952545166}, {"text": "METEOR", "start_pos": 225, "end_pos": 231, "type": "DATASET", "confidence": 0.8327300548553467}]}, {"text": "thirdly, RED perform very bad on Hindi-English, which is a newly introduced task this year.", "labels": [], "entities": [{"text": "RED", "start_pos": 9, "end_pos": 12, "type": "TASK", "confidence": 0.6155391335487366}]}, {"text": "We evaluate both sentence level and system level score of RED on English to French and German.", "labels": [], "entities": [{"text": "RED", "start_pos": 58, "end_pos": 61, "type": "DATASET", "confidence": 0.36160150170326233}]}, {"text": "The reason we only conduct these two languages are that the dependency parsers are more reliable in these two languages.", "labels": [], "entities": []}, {"text": "The results are shown in and 4.", "labels": [], "entities": []}, {"text": "From and 4 we can see that the tuned version of RED still perform slightly better than METEOR with the only exception of system level en-de.", "labels": [], "entities": [{"text": "RED", "start_pos": 48, "end_pos": 51, "type": "METRIC", "confidence": 0.7619131207466125}, {"text": "METEOR", "start_pos": 87, "end_pos": 93, "type": "METRIC", "confidence": 0.9690915942192078}]}], "tableCaptions": [{"text": " Table 1: Sentence level correlations tuned on WMT 2010, 2012 and 2013; tested on WMT 2014. The  value in bold is the best result in each raw. ave stands for the average result of the language pairs on each  year. RED stands for our untuned system, RED-sent is G.sent.2, RED-syssent is G.sent.1", "labels": [], "entities": [{"text": "WMT 2010", "start_pos": 47, "end_pos": 55, "type": "DATASET", "confidence": 0.9499340653419495}, {"text": "WMT", "start_pos": 82, "end_pos": 85, "type": "DATASET", "confidence": 0.9744606018066406}]}, {"text": " Table 2: System level correlations tuned on WMT 2010, 2012 and 2013, tested on 2014. The value in  bold is the best result in each raw. ave stands for the average result of the language pairs on each year.  RED stands for our untuned system, RED-sys is G.sys.2, RED-syssent is G.sys.1", "labels": [], "entities": [{"text": "WMT 2010", "start_pos": 45, "end_pos": 53, "type": "DATASET", "confidence": 0.9466077089309692}, {"text": "RED", "start_pos": 208, "end_pos": 211, "type": "METRIC", "confidence": 0.9444509744644165}]}, {"text": " Table 3: Sentence level correlations tuned on WMT 2010, 2012 and 2013, and tested on 2014. The  value in bold is the best result in each raw. RED stands for our untuned system, RED-sent is G.sent.2,  RED-syssent is G.sent.1", "labels": [], "entities": [{"text": "WMT 2010", "start_pos": 47, "end_pos": 55, "type": "DATASET", "confidence": 0.9499756693840027}, {"text": "RED", "start_pos": 143, "end_pos": 146, "type": "METRIC", "confidence": 0.9413455724716187}]}, {"text": " Table 4: System level correlations for English to Franch and German, tuned on WMT 2010, 2012 and  2013; tested on WMT 2014. The value in bold is the best result in each raw. RED stands for our untuned  system, RED-sys is G.sys.2, RED-syssent is G.sys.1", "labels": [], "entities": [{"text": "WMT 2010", "start_pos": 79, "end_pos": 87, "type": "DATASET", "confidence": 0.9518381953239441}, {"text": "WMT", "start_pos": 115, "end_pos": 118, "type": "DATASET", "confidence": 0.9835470914840698}, {"text": "RED", "start_pos": 175, "end_pos": 178, "type": "METRIC", "confidence": 0.9714113473892212}, {"text": "RED-syssent", "start_pos": 231, "end_pos": 242, "type": "METRIC", "confidence": 0.784148633480072}, {"text": "G.sys.1", "start_pos": 246, "end_pos": 253, "type": "DATASET", "confidence": 0.6447044610977173}]}]}