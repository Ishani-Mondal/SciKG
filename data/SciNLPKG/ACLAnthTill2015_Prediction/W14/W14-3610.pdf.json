{"title": [{"text": "Named Entity Recognition for Dialectal Arabic", "labels": [], "entities": [{"text": "Entity Recognition", "start_pos": 6, "end_pos": 24, "type": "TASK", "confidence": 0.6829746067523956}]}], "abstractContent": [{"text": "To date, majority of research for Ara-bic Named Entity Recognition (NER) addresses the task for Modern Standard Ara-bic (MSA) and mainly focuses on the newswire genre.", "labels": [], "entities": [{"text": "Ara-bic Named Entity Recognition (NER)", "start_pos": 34, "end_pos": 72, "type": "TASK", "confidence": 0.725304297038487}, {"text": "Modern Standard Ara-bic (MSA)", "start_pos": 96, "end_pos": 125, "type": "DATASET", "confidence": 0.8126491606235504}]}, {"text": "Despite some common characteristics between MSA and Dialec-tal Arabic (DA), the significant differences between the two language varieties hinder such MSA specific systems from solving NER for Dialectal Arabic.", "labels": [], "entities": [{"text": "NER", "start_pos": 185, "end_pos": 188, "type": "TASK", "confidence": 0.8796742558479309}]}, {"text": "In this paper, we present an NER system for DA specifically focusing on the Egyptian Dialect (EGY).", "labels": [], "entities": [{"text": "NER", "start_pos": 29, "end_pos": 32, "type": "TASK", "confidence": 0.9626689553260803}, {"text": "DA", "start_pos": 44, "end_pos": 46, "type": "TASK", "confidence": 0.9795041680335999}, {"text": "Egyptian Dialect (EGY)", "start_pos": 76, "end_pos": 98, "type": "DATASET", "confidence": 0.7207173585891724}]}, {"text": "Our system delivers \u2248 16% improvement in F1-score over state-of-the-art features.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.9996342658996582}]}], "introductionContent": [{"text": "Named Entity Recognition (NER) aims to identify predefined set of named entities types (e.g. in open-domain text (.", "labels": [], "entities": [{"text": "Named Entity Recognition (NER)", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.8061698377132416}]}, {"text": "NER has proven to bean essential component in many Natural Language Processing (NLP) and Information Retrieval tasks.", "labels": [], "entities": [{"text": "Information Retrieval tasks", "start_pos": 89, "end_pos": 116, "type": "TASK", "confidence": 0.8558856050173441}]}, {"text": "In, the authors show the significant impact NER imposes on the retrieval performance, given the fact that names occur with high frequency in text.", "labels": [], "entities": [{"text": "NER", "start_pos": 44, "end_pos": 47, "type": "TASK", "confidence": 0.9377760887145996}]}, {"text": "Moreover, in Question Answering, () report that Questions on average contain \u2248 85% Named Entities.", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 13, "end_pos": 31, "type": "TASK", "confidence": 0.7577154040336609}]}, {"text": "Although NER has been well studied in the literature, but the majority of the work primarily focuses on English in the newswire genre, with nearhuman performance (f-score\u2248 93% in MUC-7).", "labels": [], "entities": [{"text": "NER", "start_pos": 9, "end_pos": 12, "type": "TASK", "confidence": 0.9429156184196472}, {"text": "f-score", "start_pos": 163, "end_pos": 170, "type": "METRIC", "confidence": 0.9760708808898926}]}, {"text": "Arabic NER has gained significant attention in the NLP community with the increased availability of annotated datasets.", "labels": [], "entities": [{"text": "Arabic NER", "start_pos": 0, "end_pos": 10, "type": "TASK", "confidence": 0.5502683818340302}]}, {"text": "For instance: Determiners appear as prefixes as in Al (AlqAhrp 'Cairo'), likewise with affixival prepositions such as l meaning 'for' (ldm$q -'to/from Damascus'-), as well as prefixed conjunctions such as w meaning 'and' (wAlqds -'and Jerusalem'-); \u2022 Absence of Short Vowels (Diacritics): Written MSA, even in newswire, is undiacritized; resulting in ambiguity that can only be resolved using contextual information).", "labels": [], "entities": [{"text": "Absence", "start_pos": 251, "end_pos": 258, "type": "METRIC", "confidence": 0.9821674823760986}]}, {"text": "Instances of such phenomena: mSr, which is underspecified for short vowels, can refer to miSor 'Egypt' or muSir 'insistent'; qTr maybe 'Qatar' if qaTar, 'sugar syrup' if qaTor, 'diameter' if quTor.", "labels": [], "entities": [{"text": "diameter", "start_pos": 178, "end_pos": 186, "type": "METRIC", "confidence": 0.9438698887825012}]}, {"text": "Previously proposed Arabic NER systems (Benajiba et al., 2007) and ( were developed exclusively for MSA and primarily address the problem in the newswire genre.", "labels": [], "entities": []}, {"text": "Nevertheless, with the extensive use of social networking and web blogs, DA NLP is gaining more attention, yielding a more urgent need for DA NER systems.", "labels": [], "entities": [{"text": "DA NLP", "start_pos": 73, "end_pos": 79, "type": "TASK", "confidence": 0.6135778427124023}, {"text": "DA NER", "start_pos": 139, "end_pos": 145, "type": "TASK", "confidence": 0.6038654148578644}]}, {"text": "Furthermore, applying NLP tools, such as NER, that are designed for MSA on DA results in considerably low performance, thus the need to build resources and tools that specifically target DA (.", "labels": [], "entities": []}, {"text": "In this paper, we propose a DA NER systemusing Egyptian Arabic (EGY) as an example dialect.", "labels": [], "entities": [{"text": "DA NER systemusing Egyptian Arabic (EGY)", "start_pos": 28, "end_pos": 68, "type": "TASK", "confidence": 0.5298209302127361}]}, {"text": "Our contributions are as follows: \u2022 Provide an annotated dataset for EGY NER; \u2022 To the best of our knowledge, our system is one of the few systems that specifically targets DA.", "labels": [], "entities": [{"text": "EGY NER", "start_pos": 69, "end_pos": 76, "type": "TASK", "confidence": 0.6949016451835632}]}], "datasetContent": [{"text": "Evaluation Data Due to the very limited resources in DA for NER, we manually annotate a portion of the DA data collected and provided by the LDC from web blogs.", "labels": [], "entities": [{"text": "NER", "start_pos": 60, "end_pos": 63, "type": "TASK", "confidence": 0.9399098753929138}]}, {"text": "The annotated data was chosen from a set of web blogs that are manually identified by LDC as Egyptian dialect and contains nearly 40k tokens.", "labels": [], "entities": []}, {"text": "The data was annotated by one native Arabic speaker annotator who followed the Linguistics Data Consortium (LDC) guidelines for NE tagging.", "labels": [], "entities": [{"text": "NE tagging", "start_pos": 128, "end_pos": 138, "type": "TASK", "confidence": 0.9526055157184601}]}, {"text": "Our dataset is relatively small and contains 285 PER, 153 LOC, and 10 ORG instances.", "labels": [], "entities": [{"text": "PER", "start_pos": 49, "end_pos": 52, "type": "METRIC", "confidence": 0.9622867703437805}, {"text": "ORG", "start_pos": 70, "end_pos": 73, "type": "METRIC", "confidence": 0.9418563842773438}]}, {"text": "Brown Clustering Data In our work, we run Brown Clustering on BOLT Phase1 Egyptian Arabic Treebank (ARZ) , where the chosen number of clusters is 500.", "labels": [], "entities": [{"text": "Brown Clustering Data", "start_pos": 0, "end_pos": 21, "type": "DATASET", "confidence": 0.8237200975418091}, {"text": "BOLT Phase1 Egyptian Arabic Treebank (ARZ)", "start_pos": 62, "end_pos": 104, "type": "DATASET", "confidence": 0.9092105105519295}]}, {"text": "MADAMIRA (): For tokenization and other features such as lemmas, gender and Part of Speech (POS) tags, and other morphological features; 2.", "labels": [], "entities": [{"text": "MADAMIRA", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.6649371981620789}, {"text": "tokenization", "start_pos": 17, "end_pos": 29, "type": "TASK", "confidence": 0.9652662873268127}]}, {"text": "CRFSuite implementation (Okazaki, 2007).", "labels": [], "entities": [{"text": "CRFSuite implementation (Okazaki, 2007)", "start_pos": 0, "end_pos": 39, "type": "DATASET", "confidence": 0.8227343814713615}]}, {"text": "We choose precision (PREC), recall (REC), and harmonic F-measure (F1) metrics to evaluate the performance of our NER system over accuracy.", "labels": [], "entities": [{"text": "precision (PREC)", "start_pos": 10, "end_pos": 26, "type": "METRIC", "confidence": 0.8760083764791489}, {"text": "recall (REC)", "start_pos": 28, "end_pos": 40, "type": "METRIC", "confidence": 0.9513323307037354}, {"text": "harmonic F-measure (F1)", "start_pos": 46, "end_pos": 69, "type": "METRIC", "confidence": 0.819721794128418}, {"text": "accuracy", "start_pos": 129, "end_pos": 137, "type": "METRIC", "confidence": 0.9977235198020935}]}, {"text": "This decision is based on the observation that the baseline accuracy on the token level in NER is not a fair assessment, since NER accuracy is always high as the majority of the tokens in free text are not named entities.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.9699338674545288}, {"text": "accuracy", "start_pos": 131, "end_pos": 139, "type": "METRIC", "confidence": 0.9456502199172974}]}], "tableCaptions": [{"text": " Table 3. FEA6 delivers the  best NER performance of F1-score=70.305%", "labels": [], "entities": [{"text": "FEA6", "start_pos": 10, "end_pos": 14, "type": "DATASET", "confidence": 0.5323231816291809}, {"text": "NER", "start_pos": 34, "end_pos": 37, "type": "TASK", "confidence": 0.5441458821296692}, {"text": "F1-score", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.9985785484313965}]}, {"text": " Table 2: Baseline NER performance", "labels": [], "entities": [{"text": "NER", "start_pos": 19, "end_pos": 22, "type": "TASK", "confidence": 0.7106785774230957}]}]}