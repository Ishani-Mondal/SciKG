{"title": [{"text": "MVA: The Multimodal Virtual Assistant", "labels": [], "entities": [{"text": "Multimodal Virtual Assistant", "start_pos": 9, "end_pos": 37, "type": "TASK", "confidence": 0.5971701840559641}]}], "abstractContent": [{"text": "The Multimodal Virtual Assistant (MVA) is an application that enables users to plan an outing through an interactive multi-modal dialog with a mobile device.", "labels": [], "entities": [{"text": "Multimodal Virtual Assistant (MVA)", "start_pos": 4, "end_pos": 38, "type": "TASK", "confidence": 0.6443779915571213}]}, {"text": "MVA demonstrates how a cloud-based multi-modal language processing infrastructure can support mobile multimodal interaction.", "labels": [], "entities": [{"text": "MVA", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7341738343238831}]}, {"text": "This demonstration will highlight in-cremental recognition, multimodal speech and gesture input, contextually-aware language understanding, and the targeted clarification of potentially incorrect segments within user input.", "labels": [], "entities": [{"text": "in-cremental recognition", "start_pos": 34, "end_pos": 58, "type": "TASK", "confidence": 0.7670750617980957}, {"text": "contextually-aware language understanding", "start_pos": 97, "end_pos": 138, "type": "TASK", "confidence": 0.6179451843102773}]}], "introductionContent": [{"text": "With the recent launch of virtual assistant applications such as Siri, Google Now, S-Voice, and Vlingo, spoken access to information and services on mobile devices has become commonplace.", "labels": [], "entities": []}, {"text": "The Multimodal Virtual Assistant (MVA) project explores the application of multimodal dialog technology in the virtual assistant landscape.", "labels": [], "entities": [{"text": "Multimodal Virtual Assistant (MVA)", "start_pos": 4, "end_pos": 38, "type": "TASK", "confidence": 0.6752479821443558}]}, {"text": "MVA departs from the existing paradigm for dialog-based mobile virtual assistants that display the unfolding dialog as a chat display.", "labels": [], "entities": []}, {"text": "Instead, the MVA prototype situates the interaction directly within a touch-based interface that combines a map with visual information displays.", "labels": [], "entities": []}, {"text": "Users can interact using combinations of speech and gesture inputs, and the interpretation of user commands depends on both map and GUI display manipulation and the physical location of the device.", "labels": [], "entities": []}, {"text": "MVA is a mobile application that allows users to plan a day or evening outwith friends using natural language and gesture input.", "labels": [], "entities": []}, {"text": "Users can search and browse over multiple interconnected domains, including music events, movie showings, and places to eat.", "labels": [], "entities": []}, {"text": "They can specify multiple parameters in natural language, such as \"Jazz concerts around San Francisco next Saturday\".", "labels": [], "entities": []}, {"text": "As users find interesting events and places, they can be collected together into plans and shared with others.", "labels": [], "entities": []}, {"text": "The central components of the graphical user interface area dynamic map showing business and event locations, and an information display showing the current recognition, system prompts, search result listing, or plans ().", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}