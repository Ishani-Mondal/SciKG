{"title": [{"text": "The CMU Submission for the Shared Task on Language Identification in Code-Switched Data", "labels": [], "entities": [{"text": "Language Identification in Code-Switched Data", "start_pos": 42, "end_pos": 87, "type": "TASK", "confidence": 0.7913648009300231}]}], "abstractContent": [{"text": "We describe the CMU submission for the 2014 shared task on language identification in code-switched data.", "labels": [], "entities": [{"text": "language identification", "start_pos": 59, "end_pos": 82, "type": "TASK", "confidence": 0.6880653500556946}]}, {"text": "We participated in all four language pairs: Spanish-English, Mandarin-English, Nepali-English, and Modern Standard Arabic-Arabic dialects.", "labels": [], "entities": []}, {"text": "After describing our CRF-based baseline system, we discuss three extensions for learning from unlabeled data: semi-supervised learning, word embeddings, and word lists.", "labels": [], "entities": []}], "introductionContent": [{"text": "Code switching (CS) occurs when a multilingual speaker uses more than one language in the same conversation or discourse.", "labels": [], "entities": [{"text": "Code switching (CS)", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.859451949596405}]}, {"text": "Automatic idenefication of the points at which code switching occurs is important for two reasons: to help sociolinguists analyze the frequency, circumstances and motivations related to code switching, and (2) to automatically determine which language-specific NLP models to use for analyzing segments of text or speech.", "labels": [], "entities": [{"text": "code switching", "start_pos": 47, "end_pos": 61, "type": "TASK", "confidence": 0.7075604647397995}, {"text": "code switching", "start_pos": 186, "end_pos": 200, "type": "TASK", "confidence": 0.7428040504455566}]}, {"text": "CS is pervasive in social media due to its informal nature ).", "labels": [], "entities": []}, {"text": "The first workshop on computational approaches to code switching in EMNLP 2014 organized a shared task () on identifying code switching, providing training data of multilingual tweets with token-level language-ID annotations.", "labels": [], "entities": [{"text": "code switching in EMNLP 2014", "start_pos": 50, "end_pos": 78, "type": "TASK", "confidence": 0.7474442481994629}, {"text": "identifying code switching", "start_pos": 109, "end_pos": 135, "type": "TASK", "confidence": 0.7997053066889445}]}, {"text": "See \u00a72 fora detailed description of the shared task.", "labels": [], "entities": []}, {"text": "This short paper documents our submission in the shared task.", "labels": [], "entities": []}, {"text": "We note that constructing a CS data set that is annotated at the token level requires remarkable manual effort.", "labels": [], "entities": []}, {"text": "However, collecting raw tweets is easy and fast.", "labels": [], "entities": []}, {"text": "We propose leveraging both labeled and unlabeled data in a unified framework; conditional random field autoencoders ().", "labels": [], "entities": []}, {"text": "The CRF autoencoder framework consists of an encoding model and a reconstruction model.", "labels": [], "entities": []}, {"text": "The encoding model is a linear-chain conditional random field (CRF) () which generates a sequence of labels, conditional on a token sequence.", "labels": [], "entities": []}, {"text": "Importantly, the parameters of the encoding model can be interpreted in the same way a CRF model would.", "labels": [], "entities": []}, {"text": "This is in contrary to generative model parameters which explain both the observation sequence and the label sequence.", "labels": [], "entities": []}, {"text": "The reconstruction model, on the other hand, independently generates the tokens conditional on the corresponding labels.", "labels": [], "entities": []}, {"text": "Both labeled and unlabeled data can be efficiently used to fit parameters of this model, minimizing regularized log loss.", "labels": [], "entities": [{"text": "regularized log loss", "start_pos": 100, "end_pos": 120, "type": "METRIC", "confidence": 0.7634485761324564}]}, {"text": "See \u00a74.1 for more details.", "labels": [], "entities": []}, {"text": "After modeling unlabeled token sequences, we explore two other ways of leveraging unlabeled data: word embeddings and word lists.", "labels": [], "entities": []}, {"text": "The word embeddings we use capture monolingual distributional similarities and therefore maybe indicative of a language (see \u00a74.2).", "labels": [], "entities": []}, {"text": "A word list, on the other hand, is a collection of words which have been manually or automatically constructed and share some property (see \u00a74.3).", "labels": [], "entities": []}, {"text": "For example, we extract the set of surface forms in monolingual corpora.", "labels": [], "entities": []}, {"text": "In \u00a75, we describe the experiments and discuss results.", "labels": [], "entities": []}, {"text": "According to the results, modeling unlabeled data using CRF autoencoders did not improve prediction accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 100, "end_pos": 108, "type": "METRIC", "confidence": 0.8806025981903076}]}, {"text": "Nevertheless, more experiments need to be run before we can conclude this setting.", "labels": [], "entities": []}, {"text": "On the positive side, word embeddings and word lists have been shown to improve CS prediction accuracy, provided they have decent coverage of tokens in the test set.", "labels": [], "entities": [{"text": "CS prediction", "start_pos": 80, "end_pos": 93, "type": "TASK", "confidence": 0.8636389076709747}, {"text": "accuracy", "start_pos": 94, "end_pos": 102, "type": "METRIC", "confidence": 0.908340573310852}]}], "datasetContent": [{"text": "We compare the performance of five models for each language pair, which correspond to the five lines in.", "labels": [], "entities": []}, {"text": "The first model, \"CRF\" is the baseline model described in \u00a73.", "labels": [], "entities": []}, {"text": "The second \"CRF + U test \" and the third \"CRF + U all \" are CRF autoencoder models (see \u00a74.1) with two sets of unlabeled data: (1) U test which only includes the test set, 7 and (2) U all which includes the test set as well as all tweets by the set of users who contributed any tweets in L.", "labels": [], "entities": []}, {"text": "The fourth model \"CRF + U all + emb.\" is a CRF autoencoder which uses word embedding features (see \u00a74.2), as well as the features described in \u00a73.2.", "labels": [], "entities": []}, {"text": "Finally, the fifth model \"CRF + U all + emb.", "labels": [], "entities": []}, {"text": "+ lists\" further adds word list features (see \u00a74.3).", "labels": [], "entities": []}, {"text": "In all but the \"CRF\" model, we adopt a transductive learning setup.", "labels": [], "entities": []}, {"text": "Since the CRF baseline is used as the encoding part of the CRF autoencoder model, we use the supervisedly-trained CRF parameters to initialize the CRF autoencoder models.", "labels": [], "entities": []}, {"text": "The categorical distributions of the reconstruction model are initialized with discrete uniforms.", "labels": [], "entities": []}, {"text": "We set the weight of the labeled data log-likelihood c labeled = 0.5, the weight of the unlabeled data log-likelihood c unlabeled = 0.5, the L 2 regularization strength c L 2 = 0.3, the concentration parameter of the Dirichlet prior \u03b1 = 0.1, the number of L-BFGS iterations c LBFGS = 4, and the number of EM iterations c EM = 4. 8 We stop training after 50 iterations of block coordinate descent.: Confusion between MSA and ARZ in the Baseline configuration.", "labels": [], "entities": []}, {"text": "The CRF baseline results are reported in the first line in.", "labels": [], "entities": [{"text": "CRF baseline", "start_pos": 4, "end_pos": 16, "type": "METRIC", "confidence": 0.7837336957454681}]}, {"text": "For three language pairs, the overall token-level accuracy ranges between 94.6% and 95.2%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 50, "end_pos": 58, "type": "METRIC", "confidence": 0.9908149838447571}]}, {"text": "In the fourth language pair, MSA-ARZ, the baseline accuracy is 80.5% which indicates the relative difficulty of this task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.9469409584999084}]}, {"text": "The second and third lines in show the results when we use CRF autoencoders with the unlabeled test set (U test ), and with all unlabeled tweets (U all ), respectively.", "labels": [], "entities": []}, {"text": "While semi-supervised learning did not hurt accuracy on any of the languages, it only resulted in a tiny increase inaccuracy for the Arabic dialects task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 44, "end_pos": 52, "type": "METRIC", "confidence": 0.9981470108032227}]}, {"text": "The fourth line in extends the CRF autoencoder model (third line) by adding unsupervised word embedding features.", "labels": [], "entities": []}, {"text": "This results in an improvement of 0.6% for MSA-ARZ, 0.5% for En-Es, 0.1% for En-Ne and Zh-En.", "labels": [], "entities": [{"text": "MSA-ARZ", "start_pos": 43, "end_pos": 50, "type": "DATASET", "confidence": 0.7583967447280884}]}, {"text": "The fifth line builds on the fourth line by adding word list features.", "labels": [], "entities": []}, {"text": "This results in an improvement of 1.7% in En-Ne, 1.6% in En-Es, 0.4% in Zh-En, and degradation of 0.1% in MSA-ARZ.", "labels": [], "entities": [{"text": "degradation", "start_pos": 83, "end_pos": 94, "type": "METRIC", "confidence": 0.981019914150238}, {"text": "MSA-ARZ", "start_pos": 106, "end_pos": 113, "type": "DATASET", "confidence": 0.9476238489151001}]}], "tableCaptions": [{"text": " Table 2: The type-level coverage of annotated data  according to word embeddings (second column)  and according to word lists (third column), per lan- guage.", "labels": [], "entities": []}, {"text": " Table 3: Token level accuracy results for each of  the four language pairs.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.938776969909668}]}, {"text": " Table 4: Confusion between MSA and ARZ in the  Baseline configuration.", "labels": [], "entities": [{"text": "ARZ", "start_pos": 36, "end_pos": 39, "type": "METRIC", "confidence": 0.6013829112052917}]}, {"text": " Table 5: Number of tweets in L, U test and U all used  for semi-supervised learning of CRF autoencoders  models.", "labels": [], "entities": []}]}