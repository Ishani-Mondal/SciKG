{"title": [{"text": "CoNLL 2014 Shared Task: Grammatical Error Correction with a Syntactic N-gram Language Model from a Big Corpora", "labels": [], "entities": [{"text": "CoNLL 2014 Shared Task", "start_pos": 0, "end_pos": 22, "type": "DATASET", "confidence": 0.8476999998092651}, {"text": "Grammatical Error Correction", "start_pos": 24, "end_pos": 52, "type": "TASK", "confidence": 0.7980710665384928}]}], "abstractContent": [{"text": "We describe our approach to grammatical error correction presented in the CoNLL Shared Task 2014.", "labels": [], "entities": [{"text": "grammatical error correction", "start_pos": 28, "end_pos": 56, "type": "TASK", "confidence": 0.6603350043296814}, {"text": "CoNLL Shared Task 2014", "start_pos": 74, "end_pos": 96, "type": "DATASET", "confidence": 0.8665012568235397}]}, {"text": "Our work is focused on error detection in sentences with a language model based on syntactic tri-grams and bi-grams extracted from dependency trees generated from 90% of the English Wikipedia.", "labels": [], "entities": [{"text": "error detection", "start_pos": 23, "end_pos": 38, "type": "TASK", "confidence": 0.7032349407672882}]}, {"text": "Also, we add a na\u00efve module to error correction that outputs a set of possible answers, those sentences are scored using a syntactic n-gram language model.", "labels": [], "entities": [{"text": "error correction", "start_pos": 31, "end_pos": 47, "type": "TASK", "confidence": 0.6615868657827377}]}, {"text": "The sentence with the best score is the final suggestion of the system.", "labels": [], "entities": []}, {"text": "The system was ranked 11th, evidently this is a very simple approach, but since the beginning our main goal was to test the syntactic n-gram language model with a big corpus to future comparison.", "labels": [], "entities": []}], "introductionContent": [{"text": "Grammatical error correction is a difficult task to solve even for humans, because there area lot of phenomena that can occur in a sentence.", "labels": [], "entities": [{"text": "Grammatical error correction", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.7938099304835001}]}, {"text": "One example of the difficulty of the task is that the annotators of the training and test data in the NUCLE () differs in the corrections that they made to the sentences, those differences in the annotations are mostly because depend on uncontrolled conditions, such knowledge, emotional state and the environment of the annotator at the moment that the task is performed.", "labels": [], "entities": [{"text": "NUCLE", "start_pos": 102, "end_pos": 107, "type": "DATASET", "confidence": 0.899826169013977}]}, {"text": "This time the shared task is more difficult than the last year ( ) that considered only five types of errors, and this time the task consist into correct all the grammatical errors in the NUCLE ().", "labels": [], "entities": [{"text": "NUCLE", "start_pos": 188, "end_pos": 193, "type": "DATASET", "confidence": 0.9619356393814087}]}, {"text": "We are interested into test the behaviour of different methods used in different NLP task with the syntactic n-grams as a resource, in order to set a baseline to future work.", "labels": [], "entities": []}, {"text": "There is work that probes that there is an improvement using syntactic n-grams in) where the author uses syntactic n-grams as machine learning features, another example of the use of syntactic ngrams occurred in the), but they used a different approach from us.", "labels": [], "entities": []}, {"text": "Until the moment we do not have a comparison with the same method that we used in this task using normal n-grams, still our hypothesis is that syntactic n-grams allow us to relate words that in a common n-gram model wouldn't be related and that can outperform the results.", "labels": [], "entities": []}, {"text": "For example, in the sentence: \"Genetic risk refers more to your chance of inheriting a disorder or disease .\" Some common tri-grams are \"to your chance\", \"your chance of\", \"chance of inheriting\".", "labels": [], "entities": []}, {"text": "The word chance cannot be related to the words \"disorder\" or \"disease\", unless we use 5-grams or 7-grams, unlike with the syntactic tri-grams that as can be appreciated in the the relation between this words are normally included.", "labels": [], "entities": []}, {"text": "Another hypothesis is that a low probability in a syntactic n-gram is an indicator that exist a wrong token in the portion of a dependency tree.", "labels": [], "entities": []}, {"text": "A simple example of this intuition can be seen in the for the sentence \"This will , if not already , caused problems as there are very limited spaces for us .\" from the training data in the NUCLE.", "labels": [], "entities": [{"text": "NUCLE", "start_pos": 190, "end_pos": 195, "type": "DATASET", "confidence": 0.9177499413490295}]}, {"text": "The bold words are wrong tokens annotated in the training data and the numbers are the token number in the sentence.", "labels": [], "entities": []}, {"text": "As can be observed the low probability syntactic tri-grams include the wrong tokens.", "labels": [], "entities": []}, {"text": "The problem is to establish a threshold in the prob- 'not-5 if-4 already-6 False': Ordered probabilities of syntactic trigrams.", "labels": [], "entities": [{"text": "False", "start_pos": 75, "end_pos": 80, "type": "METRIC", "confidence": 0.9878387451171875}]}, {"text": "The wrong tokens are \"caused\", \"are\" and \"spaces\".", "labels": [], "entities": []}, {"text": "abilities to consider as wrong a syntactic trigram and separate the wrong tokens from the correct ones.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our official results in the CoNLL 2014 Shared Task on grammatical error correction of the NUCLE and evaluated with the official scorer   The scoring without alternative answers was made with gold edits of the annotators and the scoring with alternative annotation includes answers proposed by 3 teams that participated on the Shared Task and were judged by the annotators.", "labels": [], "entities": [{"text": "CoNLL 2014 Shared Task", "start_pos": 28, "end_pos": 50, "type": "DATASET", "confidence": 0.8152681142091751}, {"text": "grammatical error correction", "start_pos": 54, "end_pos": 82, "type": "TASK", "confidence": 0.5581590235233307}, {"text": "NUCLE", "start_pos": 90, "end_pos": 95, "type": "DATASET", "confidence": 0.6326925754547119}]}], "tableCaptions": [{"text": " Table 1: Ordered probabilities of syntactic tri- grams. The wrong tokens are \"caused\", \"are\"  and \"spaces\".", "labels": [], "entities": []}, {"text": " Table 8: Results in the CoNLL 2014 Shared  Task .", "labels": [], "entities": [{"text": "CoNLL 2014 Shared  Task", "start_pos": 25, "end_pos": 48, "type": "DATASET", "confidence": 0.812295064330101}]}]}