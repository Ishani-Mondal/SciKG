{"title": [{"text": "A Hybrid Approach to Multi-document Summarization of Opinions in Reviews", "labels": [], "entities": [{"text": "Multi-document Summarization of Opinions", "start_pos": 21, "end_pos": 61, "type": "TASK", "confidence": 0.7808119803667068}]}], "abstractContent": [{"text": "We present a hybrid method to generate summaries of product and services reviews by combining natural language generation and salient sentence selection techniques.", "labels": [], "entities": [{"text": "summaries of product and services reviews", "start_pos": 39, "end_pos": 80, "type": "TASK", "confidence": 0.7659565011660258}, {"text": "natural language generation", "start_pos": 94, "end_pos": 121, "type": "TASK", "confidence": 0.7490321596463522}]}, {"text": "Our system, STARLET-H, receives as input textual reviews with associated rated topics, and produces as output a natural language document summarizing the opinions expressed in the reviews.", "labels": [], "entities": [{"text": "STARLET-H", "start_pos": 12, "end_pos": 21, "type": "METRIC", "confidence": 0.43453189730644226}]}, {"text": "STARLET-H operates as a hybrid abstractive/extractive summarizer: using extractive summarization techniques, it selects salient quotes from the input reviews and embeds them into an automatically generated abstractive summary to provide evidence for, exemplify or justify positive or negative opinions.", "labels": [], "entities": []}, {"text": "We demonstrate that, compared to extractive methods , summaries generated with abstractive and hybrid summarization approaches are more readable and compact.", "labels": [], "entities": [{"text": "summaries", "start_pos": 54, "end_pos": 63, "type": "TASK", "confidence": 0.9747214913368225}]}], "introductionContent": [{"text": "Text summarization is a well-established area of research.", "labels": [], "entities": [{"text": "Text summarization", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8009813725948334}]}, {"text": "Many approaches are extractive, that is, they select and stitch together pieces of text from the input documents ().", "labels": [], "entities": []}, {"text": "Other approaches are abstractive; they use natural language generation (NLG) techniques to paraphrase and condense the content of the input documents ().", "labels": [], "entities": [{"text": "natural language generation (NLG)", "start_pos": 43, "end_pos": 76, "type": "TASK", "confidence": 0.8349393407503763}]}, {"text": "Most summarization methods focus on distilling factual information by identifying the input documents' main topics, removing redundancies, and coherently ordering extracted phrases or sentences.", "labels": [], "entities": [{"text": "summarization", "start_pos": 5, "end_pos": 18, "type": "TASK", "confidence": 0.9798033833503723}]}, {"text": "Summarization of sentiment-laden text (e.g., product or service reviews) is substantially different from the traditional text summarization task: instead of presenting facts, the summarizer must present the range of opinions and the consensus opinion (if any), and instead of focusing on one topic, the summarizer must present information about multiple aspects of the target entity.", "labels": [], "entities": [{"text": "Summarization of sentiment-laden text (e.g., product or service reviews)", "start_pos": 0, "end_pos": 72, "type": "TASK", "confidence": 0.7972410122553507}, {"text": "text summarization", "start_pos": 121, "end_pos": 139, "type": "TASK", "confidence": 0.7430658936500549}]}, {"text": "* This work was conducted when in AT&T Labs Research In addition, traditional summarization techniques discard redundancies, while for summarization of sentiment-laden text, similar opinions mentioned multiple times across documents are crucial indicators of the overall strength of the sentiments expressed by the writers ().", "labels": [], "entities": [{"text": "AT&T Labs Research", "start_pos": 34, "end_pos": 52, "type": "DATASET", "confidence": 0.8054974675178528}, {"text": "summarization", "start_pos": 78, "end_pos": 91, "type": "TASK", "confidence": 0.9885141253471375}, {"text": "summarization of sentiment-laden text", "start_pos": 135, "end_pos": 172, "type": "TASK", "confidence": 0.8351214677095413}]}, {"text": "Extractive summaries are linguistically interesting and can be both informative and concise.", "labels": [], "entities": [{"text": "Extractive summaries", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.7656306326389313}]}, {"text": "Extractive summarizers also require less engineering effort.", "labels": [], "entities": [{"text": "Extractive summarizers", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.7077118456363678}]}, {"text": "On the other hand, abstractive summaries tend to have better coverage fora particular level of conciseness, and to be less redundant and more coherent).", "labels": [], "entities": []}, {"text": "They also can be constructed to target particular discourse goals, such as summarization, comparison or recommendation.", "labels": [], "entities": [{"text": "summarization", "start_pos": 75, "end_pos": 88, "type": "TASK", "confidence": 0.9711631536483765}]}, {"text": "Although in theory, it is possible to produce user-targeted extractive summaries, userspecific review summarization has only been explored in the context of abstractive summarization.", "labels": [], "entities": [{"text": "userspecific review summarization", "start_pos": 82, "end_pos": 115, "type": "TASK", "confidence": 0.5606364111105601}]}, {"text": "Current systems for summarizing sentimentladen text use information about the attributes of the target entity (or entities); the range, mean and median of the ratings of each attribute; relationships between the attributes; and links between ratings/attributes and text elements in the input documents.", "labels": [], "entities": [{"text": "summarizing sentimentladen text", "start_pos": 20, "end_pos": 51, "type": "TASK", "confidence": 0.9036212762196859}]}, {"text": "However, there is other information that no summarizer currently takes into account.", "labels": [], "entities": []}, {"text": "This includes temporal features (in particular, depending on how old the documents are, products and services evaluated features may changeover time) and social features (in particular, social or demographic similarities or relationships between document authors and the reader of the summary).", "labels": [], "entities": []}, {"text": "In addition, there is an essential contradiction at the heart of current review summarization systems: the system is authoring the review, but the opinions contained therein are really attributable to one or more human authors, and those attributions are not retained in the review summary.", "labels": [], "entities": []}, {"text": "For example, consider the extractive summary generated with STARLET-E: \"Delicious.", "labels": [], "entities": [{"text": "STARLET-E", "start_pos": 60, "end_pos": 69, "type": "METRIC", "confidence": 0.7908904552459717}]}, {"text": "Can't wait for my next trip to Buffalo.", "labels": [], "entities": []}, {"text": "I have rearranged business trips so that I could stop in and have a helping or two of their wings\".", "labels": [], "entities": []}, {"text": "We were seated promptly and the staff was courteous.", "labels": [], "entities": []}, {"text": "The summary is generated by selecting sentences from reviews to reflect topics and rating distributions contained in the input review set.", "labels": [], "entities": []}, {"text": "Do the two sentences about wings reflect one (repeated) opinion from a single reviewer, or two opinions from two separate reviewers?", "labels": [], "entities": []}, {"text": "The ability to attribute subjective statements to known sources can make them more trustworthy; conversely, in the absence of the ability to attribute, a reader may become skeptical or confused about the content of the review summary.", "labels": [], "entities": []}, {"text": "We term this summarization issue opinion holder attribution.", "labels": [], "entities": []}, {"text": "In this paper we present STARLET-H, a hybrid review summarizer that combines the advantages of the abstractive and extractive approaches to summarization and implements a solution to the opinion holder attribution problem.", "labels": [], "entities": [{"text": "STARLET-H", "start_pos": 25, "end_pos": 34, "type": "METRIC", "confidence": 0.8360993266105652}, {"text": "summarization", "start_pos": 140, "end_pos": 153, "type": "TASK", "confidence": 0.97554612159729}]}, {"text": "STARLET-H takes as input a set of reviews, each review of which is labeled with aspect ratings and authorship.", "labels": [], "entities": [{"text": "STARLET-H", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.5424619913101196}]}, {"text": "It generates hybrid abstractive/extractive reviews that: 1) are informative (achieve broad coverage of the input opinions); 2) are concise and avoid redundancy; 3) are readable and coherent (of high linguistic quality); 4) can be targeted to the reader; and 5) address the opinion holder attribution problem by directly referring to reviewers authorship when embedding phrases from reviews.", "labels": [], "entities": []}, {"text": "We demonstrate through a comparative evaluation of STARLET-H and other review summarizers that hybrid review summarization is preferred over extractive summarization for readability, correctness, completeness (achieving broad coverage of the input opinions) and compactness.", "labels": [], "entities": [{"text": "STARLET-H", "start_pos": 51, "end_pos": 60, "type": "DATASET", "confidence": 0.7113374471664429}, {"text": "correctness", "start_pos": 183, "end_pos": 194, "type": "METRIC", "confidence": 0.938606321811676}]}], "datasetContent": [{"text": "Evaluating an abstractive review summarizer involves measuring how accurately the opinion content present in the reviews is reflected in the summary and how understandable the generated content is to the reader.", "labels": [], "entities": []}, {"text": "Traditional multi-document summarization evaluation techniques utilize both qualitative and quantitative metrics.", "labels": [], "entities": [{"text": "multi-document summarization evaluation", "start_pos": 12, "end_pos": 51, "type": "TASK", "confidence": 0.6809133291244507}]}, {"text": "The former require human subjects to rate different evaluative characteristics on a Likert-like scale, while the latter relies on automatic metrics such as ROUGE, which is based on the common number of n-grams between a peer, and one or several gold-standard reference summaries.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 156, "end_pos": 161, "type": "METRIC", "confidence": 0.9972645044326782}]}, {"text": "To evaluate our abstractive summarizer, we used a qualitative metric approach and compared four review summarizers: 1) the open source MEAD system, designed for extractive summarization of general text (); 2) STARLET-E, an extractive summarizer based on KL-divergence and language modeling features that is described in Di Fabbrizio et al.", "labels": [], "entities": [{"text": "extractive summarization of general text", "start_pos": 161, "end_pos": 201, "type": "TASK", "confidence": 0.7313692688941955}, {"text": "STARLET-E", "start_pos": 209, "end_pos": 218, "type": "METRIC", "confidence": 0.741959810256958}]}, {"text": "(2011); 3) STARLET-A, the abstractive summarizer presented in this paper, without the quote selection module; and 4) the hybrid summarizer STARLET-H.", "labels": [], "entities": []}, {"text": "We used the Amazon Mechanical Turk 3 crowd-sourcing system to post subjective evaluation tasks, or HITs, for 20 restaurant summaries.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk 3 crowd-sourcing system", "start_pos": 12, "end_pos": 58, "type": "DATASET", "confidence": 0.8730271756649017}, {"text": "HITs", "start_pos": 99, "end_pos": 103, "type": "METRIC", "confidence": 0.9316326975822449}]}, {"text": "Each HIT consists of a set often randomly ordered reviews for one restaurant, and four randomly ordered summaries of reviews for that restaurant, each one accompanied by a set of evaluation widgets for the different evaluation metrics described below.", "labels": [], "entities": []}, {"text": "To minimize reading order bias, both reviews and summaries were shuffled each time a task was presented.", "labels": [], "entities": []}, {"text": "We chose to carryout a qualitative evaluation in the first instance as n-gram metrics, such as ROUGE, are not necessarily appropriate for assessing abstractive summaries.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 95, "end_pos": 100, "type": "METRIC", "confidence": 0.9699047207832336}]}, {"text": "We requested five evaluators for each HIT.", "labels": [], "entities": [{"text": "HIT", "start_pos": 38, "end_pos": 41, "type": "DATASET", "confidence": 0.8529399633407593}]}, {"text": "To increase the chances of getting accurate evaluations, we required evaluators to be located in the USA and have an approval rate of 90% or higher (i.e., have a history of 90% or more approved HITs).", "labels": [], "entities": [{"text": "approval rate", "start_pos": 117, "end_pos": 130, "type": "METRIC", "confidence": 0.9275336563587189}]}, {"text": "Manual examinations of the evaluation responses did not show evidence of tampered data, but statistical analysis showed unusually widely spread rating ranges.", "labels": [], "entities": []}, {"text": "We noticed that most evaluators only evaluated one or two HITs; this may imply that they tried a few HITs and then decided not to continue because they found the task too long or the instructions unclear.", "labels": [], "entities": []}, {"text": "We then re-opened the evaluation and directly contacted three additional evaluators, explaining in detail the instructions and the evaluation scales.", "labels": [], "entities": []}, {"text": "For consistency, we asked these evaluators to complete the evaluation for all HITs.", "labels": [], "entities": [{"text": "consistency", "start_pos": 4, "end_pos": 15, "type": "METRIC", "confidence": 0.9349563717842102}]}, {"text": "In our analysis, we only included the five evaluators (two from the first round of evaluation, and three from the second) who completed all HITs.", "labels": [], "entities": []}, {"text": "For each evaluation metric, the five workers evaluated each of the 20 summaries, fora total of 100 ratings.", "labels": [], "entities": []}, {"text": "shows an example output of the four summarization methods fora single set of restaurant review documents.: Example of MEAD-based, extractive, abstractive and hybrid summaries from the restaurant domain MEAD Summary a truly fun resturant everyone who like spicy food should try the rattoes and fora mixed drink the worm burner really good food and a fun place to meet your friends.", "labels": [], "entities": [{"text": "MEAD-based", "start_pos": 118, "end_pos": 128, "type": "METRIC", "confidence": 0.873688280582428}]}, {"text": "We were attracted by the great big frog on the exterior of the building and the fun RAZZOO S logo during a trip to the mall.", "labels": [], "entities": [{"text": "RAZZOO", "start_pos": 84, "end_pos": 90, "type": "METRIC", "confidence": 0.7674817442893982}]}, {"text": "it was great the waitress was excellent very prompt and courteous and friendly to all areal complement to razzoo 's way of service her name was Tabitha.", "labels": [], "entities": []}, {"text": "The best spicy food restaurant with great server and fast service.", "labels": [], "entities": []}, {"text": "The evaluation results are presented in.", "labels": [], "entities": []}, {"text": "Each evaluation metric is considered separately.", "labels": [], "entities": []}, {"text": "Average values for STARLET-E, STARLET-A and STARLET-H are better than for MEAD across the board, suggesting a preference for summaries of sentiment-laden text that take opinion into account.", "labels": [], "entities": [{"text": "MEAD", "start_pos": 74, "end_pos": 78, "type": "METRIC", "confidence": 0.7273505330085754}, {"text": "summaries of sentiment-laden text", "start_pos": 125, "end_pos": 158, "type": "TASK", "confidence": 0.8029210716485977}]}, {"text": "To validate this hypothesis, we first computed the non-parametric Kruskal-Wallis statistic for each evaluation metric, using a chi-square test to establish significance.", "labels": [], "entities": []}, {"text": "The results were not significant for any of the metrics.", "labels": [], "entities": []}, {"text": "However, when we conducted pairwise Wilcoxon signed-rank tests considering two summarization methods at a time, we found some significant differences (p < 0.05).", "labels": [], "entities": [{"text": "summarization", "start_pos": 79, "end_pos": 92, "type": "TASK", "confidence": 0.9690850973129272}]}, {"text": "As predicted, also found that the differences between extractive and abstractive approaches are even more significant in the case of controversial content, where the abstractive system is able to more effectively convey the full range of opinions.", "labels": [], "entities": []}, {"text": "propose a method to extract salient sentence fragments that are both highly frequent and syntactically well-formed by using a graph-based data structure to eliminate redundancies.", "labels": [], "entities": []}, {"text": "However, this approach assumes that the input sentences are already selected in terms of aspect and with highly redundant opinion content.", "labels": [], "entities": []}, {"text": "Also, the generated summaries are very short and cannot be compared to a full-length output of atypical multi-document summarizer (e.g., 100-200 words).", "labels": [], "entities": []}, {"text": "A similar approach is described in, where very short phrases (from two to five words) are collated together to generate what the authors call ultra-concise summaries.", "labels": [], "entities": []}, {"text": "The most complete contribution to evaluative text summarization is described in and it closely relates to this work.", "labels": [], "entities": [{"text": "evaluative text summarization", "start_pos": 34, "end_pos": 63, "type": "TASK", "confidence": 0.629724899927775}]}, {"text": "compare an extractive summarization system, MEAD* -a modified version of the open source summarization system MEAD () -with SEA, an abstractive summarization system, demonstrating that both systems perform equally well.", "labels": [], "entities": [{"text": "MEAD", "start_pos": 44, "end_pos": 48, "type": "METRIC", "confidence": 0.7631540894508362}, {"text": "SEA", "start_pos": 124, "end_pos": 127, "type": "METRIC", "confidence": 0.8754506707191467}]}, {"text": "The SEA approach, although better than traditional MEAD, has a few drawbacks.", "labels": [], "entities": [{"text": "SEA", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.8235408663749695}, {"text": "MEAD", "start_pos": 51, "end_pos": 55, "type": "TASK", "confidence": 0.49524468183517456}]}, {"text": "Firstly, the sentence selection mechanism only considers the most frequently discussed aspects, leaving the decision about whereto stop the selection process to the maximum summary length parameter.", "labels": [], "entities": [{"text": "sentence selection", "start_pos": 13, "end_pos": 31, "type": "TASK", "confidence": 0.7472116351127625}]}, {"text": "This could leave out interesting opinions that do not appear with sufficient frequency in the source documents.", "labels": [], "entities": []}, {"text": "Ideally, all opinions should be represented in the summary according to the overall distribution of the input reviews.", "labels": [], "entities": []}, {"text": "Secondly, use the absolute value of the sum of positive and negative contributions to determine the relevance of a sentence in terms of opinion content.", "labels": [], "entities": []}, {"text": "This flattens the aspect distributions since sentences with very negative or very positive polarity or with numerous opinions, but with moderate polarity strengths, will get the same score, regardless.", "labels": [], "entities": []}, {"text": "Finally, it does not address the opinion holder attribution problem leaving the source of opinion undefined.", "labels": [], "entities": []}, {"text": "In contrast, STARLET-H follows reviews aspect rating distributions both to select quotable sentences and to summarize relevant aspects.", "labels": [], "entities": []}, {"text": "Moreover, it explicitly mentions the opinion source in the embedded quoted sentences.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Quote-annotated dataset statistics", "labels": [], "entities": [{"text": "Quote-annotated dataset", "start_pos": 10, "end_pos": 33, "type": "DATASET", "confidence": 0.6667295843362808}]}, {"text": " Table 2: Summarizer lexicon for most frequent adjective phrases by aspect and polarity", "labels": [], "entities": [{"text": "Summarizer lexicon", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.9198835492134094}]}, {"text": " Table 4: Quote, aspect, and polarity classification  performances for the restaurant domain", "labels": [], "entities": []}, {"text": " Table 6: Qualitative evaluation results", "labels": [], "entities": []}]}