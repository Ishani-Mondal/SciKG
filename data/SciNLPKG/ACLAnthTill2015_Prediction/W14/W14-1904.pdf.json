{"title": [{"text": "Speech recognition in Alzheimer's disease with personal assistive robots", "labels": [], "entities": [{"text": "Speech recognition", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7949172854423523}]}], "abstractContent": [{"text": "To help individuals with Alzheimer's disease live at home for longer, we are developing a mobile robotic platform, called ED, intended to be used as a personal care-giver to help with the performance of activities of daily living.", "labels": [], "entities": []}, {"text": "Ina series of experiments , we study speech-based interactions between each of 10 older adults with Alzheimers disease and ED as the former makes tea in a simulated home environment.", "labels": [], "entities": []}, {"text": "Analysis reveals that speech recognition remains a challenge for this recording environment, with word-level accuracies between 5.8% and 19.2% during household tasks with individuals with Alzheimer's disease.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 22, "end_pos": 40, "type": "TASK", "confidence": 0.7083072364330292}, {"text": "accuracies", "start_pos": 109, "end_pos": 119, "type": "METRIC", "confidence": 0.6800655722618103}]}, {"text": "This work provides a baseline assessment for the types of technical and communicative challenges that will need to be overcome in human-robot interaction for this population.", "labels": [], "entities": []}], "introductionContent": [{"text": "Alzheimer's disease (AD) is a progressive neurodegenerative disorder primarily impairing memory, followed by declines in language, ability to carryout motor tasks, object recognition, and executive functioning (American Psychiatric Association,;).", "labels": [], "entities": [{"text": "Alzheimer's disease (AD)", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.6608306467533112}, {"text": "object recognition", "start_pos": 164, "end_pos": 182, "type": "TASK", "confidence": 0.7525345087051392}]}, {"text": "An accurate measure of functional decline comes from performance in activities of daily living (ADLs), such as shopping, finances, housework, and selfcare tasks.", "labels": [], "entities": []}, {"text": "The deterioration in language comprehension and/or production resulting from specific brain damage, also known as aphasia, is a common feature of AD and other related conditions.", "labels": [], "entities": [{"text": "aphasia", "start_pos": 114, "end_pos": 121, "type": "METRIC", "confidence": 0.9510486125946045}, {"text": "AD", "start_pos": 146, "end_pos": 148, "type": "TASK", "confidence": 0.9578122496604919}]}, {"text": "Language changes observed clinically in older adults with dementia include increasing word-finding difficulties, loss of ability to verbally express information in detail, increasing use of generic references (e.g., \"it\"), and progressing difficulties understanding information presented verbally).", "labels": [], "entities": []}, {"text": "Many nations are facing healthcare crises in the lack of capacity to support rapidly aging populations nor the chronic conditions associated with aging, including dementia.", "labels": [], "entities": []}, {"text": "The current healthcare model of removing older adults from their homes and placing them into long-term care facilities is neither financially sustainable in this scenario (, nor is it desirable.", "labels": [], "entities": []}, {"text": "Our team has been developing \"smart home\" systems at the Toronto Rehabilitation Institute (TRI, part of the University Health Network) to help older adults \"age-in-place\" by providing different types of support, such as step-by-step prompts for daily tasks (, responses to emergency situations (, and means to communicate with family and friends.", "labels": [], "entities": []}, {"text": "These systems are being evaluated within a completely functional re-creation of a one-bedroom apartment located within The TRI hospital, called HomeLab.", "labels": [], "entities": [{"text": "TRI hospital", "start_pos": 123, "end_pos": 135, "type": "DATASET", "confidence": 0.8603321611881256}, {"text": "HomeLab", "start_pos": 144, "end_pos": 151, "type": "DATASET", "confidence": 0.9568354487419128}]}, {"text": "These smart home technologies use advanced sensing techniques and machine learning to autonomously react to their users, but they are fixed and embedded into the environment, e.g., as cameras in the ceiling.", "labels": [], "entities": []}, {"text": "Fixing the location of these technologies carries a tradeoff between utility and feasibility -installing multiple hardware units at all locations where assistance could be required (e.g., bathroom, kitchen, and bedroom) can be expensive and cumbersome, but installing too few units will present gaps where a user's activity will not be detected.", "labels": [], "entities": []}, {"text": "Alternatively, integrating personal mobile robots with smart homes can overcome some of these tradeoffs.", "labels": [], "entities": []}, {"text": "Moreover, assistance provided via a physically embodied robot is often more acceptable than that provided by an embedded system ().", "labels": [], "entities": []}, {"text": "With these potential advantages in mind, we conducted a 'Wizard-of-Oz' study to explore the 20 feasibility and usability of a mobile assistive robot that uses the step-by-step prompting approaches for daily activities originally applied to our smart home research ().", "labels": [], "entities": []}, {"text": "We conducted the study with older adults with mild or moderate AD and the tasks of hand washing and tea making.", "labels": [], "entities": [{"text": "AD", "start_pos": 63, "end_pos": 65, "type": "METRIC", "confidence": 0.9934507608413696}, {"text": "hand washing", "start_pos": 83, "end_pos": 95, "type": "TASK", "confidence": 0.8599015176296234}, {"text": "tea making", "start_pos": 100, "end_pos": 110, "type": "TASK", "confidence": 0.8333785235881805}]}, {"text": "Our preliminary data analysis showed that the participants reacted well to the robot itself and the prompts that it provided, suggesting the feasibility of using personal robots for this application (.", "labels": [], "entities": []}, {"text": "One important identified issue is the need for an automatic speech recognition system to detect and understand utterances specifically from older adults with AD.", "labels": [], "entities": []}, {"text": "The development of such a system will enable the assistive robot to better understand the behaviours and needs of these users for effective interactions and will further enhance environmentalbased smart home systems.", "labels": [], "entities": []}, {"text": "This paper presents an analysis of the speech data collected from our participants with AD when interacting with the robot.", "labels": [], "entities": []}, {"text": "Ina series of experiments, we measure the performance of modern speech recognition with this population and with their younger caregivers with and without signal preprocessing.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 64, "end_pos": 82, "type": "TASK", "confidence": 0.7638244330883026}]}, {"text": "This work will serve as the basis for further studies by identifying some of the development needs of a speech-based interface for robotic caregivers for older adults with AD.", "labels": [], "entities": []}], "datasetContent": [{"text": "Automatic speech recognition given these data is complicated by several factors, including a preponderance of utterances in which human caregivers speak concurrently with the participants, as well as inordinately challenging levels of noise.", "labels": [], "entities": [{"text": "Automatic speech recognition", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.688367505868276}]}, {"text": "The estimated signal-to-noise ratio (SNR) across utterances range from \u22123.42 dB to 8.14 dB, which is extremely low compared to typical SNR of 40 dB in clean speech.", "labels": [], "entities": [{"text": "signal-to-noise ratio (SNR)", "start_pos": 14, "end_pos": 41, "type": "METRIC", "confidence": 0.8469224393367767}]}, {"text": "One cause of this low SNR is that microphones are placed in the environment, rather than on the robot (so the distance to the microphone is variable, but relatively large) and that the participant often has their back turned to the microphone, as shown in.", "labels": [], "entities": [{"text": "SNR", "start_pos": 22, "end_pos": 25, "type": "TASK", "confidence": 0.831979513168335}]}, {"text": "As in previous work (Rudzicz et al., 2012), we enhance speech signals with the log-spectral amplitude estimator (LSAE) which minimizes the mean squared error of the log spectra given a model for the source speech X k = A k e ( j\u03c9 k ), where A k is the spectral amplitude.", "labels": [], "entities": [{"text": "log-spectral amplitude estimator (LSAE)", "start_pos": 79, "end_pos": 118, "type": "METRIC", "confidence": 0.8319681783517202}, {"text": "mean squared error", "start_pos": 139, "end_pos": 157, "type": "METRIC", "confidence": 0.7316404183705648}]}, {"text": "The LSAE method is a modification of the short-time spectral amplitude estimator that finds an estimate of the spectral amplitude, \u02c6 A k , that minimizes the distortion such that the log-spectral amplitude estimate is\u02c6A where \u03be k is the a priori SNR, R k is the noisy spectral amplitude, v k = \u03be k 1+\u03be k \u03b3 k , and \u03b3 k is the a posteriori SNR (.", "labels": [], "entities": []}, {"text": "Often this is based on a Gaussian model of noise, as it is here.", "labels": [], "entities": []}, {"text": "As mentioned, there are many utterances in which human caregivers speak concurrently with the participants.", "labels": [], "entities": []}, {"text": "This is partially confounded by the fact that utterances by individuals with AD tend to be shorter, so more of their utterance is lost, proportionally.", "labels": [], "entities": [{"text": "AD", "start_pos": 77, "end_pos": 79, "type": "METRIC", "confidence": 0.9601554274559021}]}, {"text": "Examples of this type where the caregiver's voice is louder than the participant's voice are discarded, amounting to about 10% of all utterances.", "labels": [], "entities": []}, {"text": "In the following analyses, function words (i.e., prepositions, subordinating conjunctions, and determiners) are removed from consideration, although interjections are kept.", "labels": [], "entities": []}, {"text": "Proper names are also omitted.", "labels": [], "entities": []}, {"text": "We use the HTK) toolchain, which provides an implementation of a semicontinuous hidden Markov model (HMM) that allows state-tying and represents output densities by mixtures of Gaussians.", "labels": [], "entities": [{"text": "HTK) toolchain", "start_pos": 11, "end_pos": 25, "type": "DATASET", "confidence": 0.8885493477185568}]}, {"text": "Features consisted of the first 13 Mel-frequency cepstral coefficients, their first (\u03b4) and second (\u03b4\u03b4) derivatives, and the log energy component, for 42 dimensions.", "labels": [], "entities": []}, {"text": "Our own data were z-scaled regardless of whether LSAE noise reduction was applied.", "labels": [], "entities": [{"text": "LSAE noise reduction", "start_pos": 49, "end_pos": 69, "type": "TASK", "confidence": 0.6186771591504415}]}, {"text": "Two language models (LMs) are used, both trigram models derived from the English Gigaword corpus, which contains 1200 word tokens (.", "labels": [], "entities": [{"text": "English Gigaword corpus", "start_pos": 73, "end_pos": 96, "type": "DATASET", "confidence": 0.8129295508066813}]}, {"text": "The first LM uses the first 5000 most frequent words and the second uses the first 64,000 most frequent words of that corpus.", "labels": [], "entities": []}, {"text": "Five acoustic models (AMs) are used with 1, 2, 4, 8, and 16 Gaussians per output density respectively.", "labels": [], "entities": []}, {"text": "These are trained with approximately 211 hours of spoken transcripts of the Wall Street Journal (WSJ) from over one hundred non-pathological speakers).", "labels": [], "entities": [{"text": "Wall Street Journal (WSJ)", "start_pos": 76, "end_pos": 101, "type": "DATASET", "confidence": 0.9415494600931803}]}, {"text": "shows, for the small-and largevocabulary LMs, the word-level accuracies of the baseline HTK ASR system, as determined by the inverse of the Levenshtein edit distance, for two scenarios (sit-down interviews vs. during the task), with and without LSAE noise reduction, for speech from individuals with AD and for their caregivers.", "labels": [], "entities": [{"text": "HTK ASR", "start_pos": 88, "end_pos": 95, "type": "TASK", "confidence": 0.5209272503852844}]}, {"text": "These values are computed overall complexities of acoustic model and are consistent with other tasks of this type (i.e., with the challenges associated with the population and recording set up), with this type of relatively unconstrained ASR (.", "labels": [], "entities": [{"text": "ASR", "start_pos": 238, "end_pos": 241, "type": "TASK", "confidence": 0.8407658338546753}]}, {"text": "Applying LSAE results in a significant increase inaccuracy for both the small-vocabulary (right-tailed homoscedastic t(58) = 3.9, p < 0.005, CI =) and large-vocabulary (right-tailed homoscedastic t(58) = 2.4, p < 0.01, CI = [2.58, \u221e]) tasks.", "labels": [], "entities": []}, {"text": "For the participants with AD, ASR accuracy is significantly higher in interviews (paired t(39) = 8.7, p < 0.0001, CI = [13.8, \u221e]), which is expected due in large part to the closer proximity of the microphone.", "labels": [], "entities": [{"text": "ASR", "start_pos": 30, "end_pos": 33, "type": "TASK", "confidence": 0.9250410199165344}, {"text": "accuracy", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9812417030334473}, {"text": "paired t(39)", "start_pos": 82, "end_pos": 94, "type": "METRIC", "confidence": 0.905353581905365}]}, {"text": "Surprisingly, ASR accuracy on participants with ASR was not significantly different than on caregivers (two-tailed heteroscedastic t(78) = \u22120.32, p = 0.75, CI = [\u22125.54, 4.0]).", "labels": [], "entities": [{"text": "ASR", "start_pos": 14, "end_pos": 17, "type": "TASK", "confidence": 0.9812599420547485}, {"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9731648564338684}, {"text": "two-tailed heteroscedastic t(78)", "start_pos": 104, "end_pos": 136, "type": "METRIC", "confidence": 0.7753925422827402}]}, {"text": "shows the mean ASR accuracy, with standard error (\u03c3/ \u221a n), for each of the smallvocabulary and large-vocabulary ASR systems.: ASR accuracy (means, and std. dev.) across speakers, scenario (interviews vs. during the task), and presence of noise reduction for the small and large language models.", "labels": [], "entities": [{"text": "ASR", "start_pos": 15, "end_pos": 18, "type": "TASK", "confidence": 0.9419358968734741}, {"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.851020336151123}, {"text": "ASR", "start_pos": 126, "end_pos": 129, "type": "TASK", "confidence": 0.6573739051818848}, {"text": "accuracy", "start_pos": 130, "end_pos": 138, "type": "METRIC", "confidence": 0.7911812663078308}]}], "tableCaptions": [{"text": " Table 1 summarizes relevant demographics.", "labels": [], "entities": []}]}