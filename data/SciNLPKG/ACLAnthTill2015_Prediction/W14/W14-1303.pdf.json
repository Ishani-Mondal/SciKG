{"title": [{"text": "Accurate Language Identification of Twitter Messages", "labels": [], "entities": [{"text": "Accurate Language Identification of Twitter Messages", "start_pos": 0, "end_pos": 52, "type": "TASK", "confidence": 0.8514401217301687}]}], "abstractContent": [{"text": "We present an evaluation of \"off-the-shelf\" language identification systems as applied to microblog messages from Twit-ter.", "labels": [], "entities": [{"text": "language identification", "start_pos": 44, "end_pos": 67, "type": "TASK", "confidence": 0.7550878524780273}]}, {"text": "A key challenge is the lack of an adequate corpus of messages annotated for language that reflects the linguistic diversity present on Twitter.", "labels": [], "entities": []}, {"text": "We overcome this through a \"mostly-automated\" approach to gathering language-labeled Twitter messages for evaluating language identification.", "labels": [], "entities": [{"text": "evaluating language identification", "start_pos": 106, "end_pos": 140, "type": "TASK", "confidence": 0.6071960826714834}]}, {"text": "We present the method to construct this dataset, as well as empirical results over existing datasets and off-the-shelf language identifiers.", "labels": [], "entities": []}, {"text": "We also test techniques that have been proposed in the literature to boost language identification performance over Twitter messages.", "labels": [], "entities": [{"text": "language identification", "start_pos": 75, "end_pos": 98, "type": "TASK", "confidence": 0.7601699829101562}]}, {"text": "We find that simple voting over three specific systems consistently outperforms any specific system, and achieves state-of-the-art accuracy on the task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 131, "end_pos": 139, "type": "METRIC", "confidence": 0.9981576800346375}]}], "introductionContent": [{"text": "Twitter 1 has captured the attention of various research communities as a potent data source, because of the immediacy of the information presented, the volume and variability of the data contained, the potential to analyze networking effects within the data, and the ability to (where GPS data is available) geolocate messages (.", "labels": [], "entities": []}, {"text": "Although individual messages range from inane through mundane right up to insane, the aggregate of these messages can lead to profound insights in real-time.", "labels": [], "entities": []}, {"text": "Examples include real-time detection of earthquakes (Sakaki have collected a substantial cache of Twitter data from before the availability of built-in predictions.", "labels": [], "entities": [{"text": "real-time detection of earthquakes", "start_pos": 17, "end_pos": 51, "type": "TASK", "confidence": 0.8495157659053802}]}, {"text": "Motivated by the need to work with monolingual subsets of historical data, we investigate the most practical means of carrying out LangID of Twitter messages, balancing accuracy with ease of implementation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 169, "end_pos": 177, "type": "METRIC", "confidence": 0.998448371887207}]}, {"text": "In this work, we present an evaluation of \"off-the-shelf\" language identifiers, combined with techniques that have been proposed for boosting accuracy on Twitter messages.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 142, "end_pos": 150, "type": "METRIC", "confidence": 0.99648118019104}]}, {"text": "A major challenge that we have had to overcome is the lack of annotated data for evaluation.", "labels": [], "entities": []}, {"text": "point out that in LangID research on microblog messages to date, only a small number of European languages has been considered.", "labels": [], "entities": [{"text": "LangID", "start_pos": 18, "end_pos": 24, "type": "DATASET", "confidence": 0.7740862965583801}]}, {"text": "showed that, when considering full documents, good performance on just European languages does not necessarily imply equally good performance when a larger set of languages is considered.", "labels": [], "entities": []}, {"text": "This does not detract from work to date on European languages (), but rather highlights the need for further research in LangID for microblog messages.", "labels": [], "entities": [{"text": "LangID", "start_pos": 121, "end_pos": 127, "type": "DATASET", "confidence": 0.8457199931144714}]}, {"text": "Manual annotation of Twitter messages is a challenging and laborious process.", "labels": [], "entities": [{"text": "Manual annotation of Twitter messages", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.8388957619667053}]}, {"text": "Furthermore, Twitter is highly multilingual, making it very difficult to obtain annotators for all of the languages represented.", "labels": [], "entities": []}, {"text": "Previous work has attempted to crowdsource part of this process (), but such an approach requires substantial monetary investment, as well as care in ensuring the quality of the final annotations.", "labels": [], "entities": []}, {"text": "In this paper, we propose an alternative, \"mostly-automated\" approach to gathering language-labeled Twitter messages for evaluating LangID.", "labels": [], "entities": [{"text": "LangID", "start_pos": 132, "end_pos": 138, "type": "DATASET", "confidence": 0.7705649137496948}]}, {"text": "A corpus constructed by direct application of automatic LangID to Twitter messages would obviously be unsuitable for evaluating the accuracy of LangID tools.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 132, "end_pos": 140, "type": "METRIC", "confidence": 0.9971007704734802}]}, {"text": "Even with manual post-filtering, the remaining dataset would be biased towards messages that are easy for automated systems to classify correctly.", "labels": [], "entities": []}, {"text": "The novelty of our approach is to leverage user identity, allowing us to construct a corpus of language-labeled Twitter messages without using automated tools to determine the languages of the messages.", "labels": [], "entities": []}, {"text": "This quality makes the corpus suitable for use in the evaluation of automated LangID of Twitter messages.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Inter-annotator agreement measured us- ing Fleiss' kappa (Fleiss, 1971) over annotations  for TWITTER.", "labels": [], "entities": [{"text": "Fleiss' kappa (Fleiss, 1971)", "start_pos": 53, "end_pos": 81, "type": "DATASET", "confidence": 0.8902835590498788}]}, {"text": " Table 2: Macro-averaged F-Score on manually-annotated Twitter datasets. Italics denotes results where  the dataset contains languages not supported by the identifier.", "labels": [], "entities": [{"text": "F-Score", "start_pos": 25, "end_pos": 32, "type": "METRIC", "confidence": 0.9599143266677856}]}, {"text": " Table 3: System combination by majority voting. All combinations of 3, 5 and 7 systems were con- sidered. For each dataset, we report the single-best system, the best combination, and F-score of the  majority-vote combination of langid.py, CLD2 and LangDetect.", "labels": [], "entities": [{"text": "F-score", "start_pos": 185, "end_pos": 192, "type": "METRIC", "confidence": 0.9980218410491943}, {"text": "CLD2", "start_pos": 241, "end_pos": 245, "type": "DATASET", "confidence": 0.865648627281189}, {"text": "LangDetect", "start_pos": 250, "end_pos": 260, "type": "DATASET", "confidence": 0.9359526038169861}]}, {"text": " Table 4: Macro-averaged Precision/Recall/F-Score, as well as message-level accuracy for each system  on TWITUSER. The right side of the table reports results after applying message-level cleaning (Tromp", "labels": [], "entities": [{"text": "Precision/Recall", "start_pos": 25, "end_pos": 41, "type": "METRIC", "confidence": 0.7387171586354574}, {"text": "F-Score", "start_pos": 42, "end_pos": 49, "type": "METRIC", "confidence": 0.5184407234191895}, {"text": "accuracy", "start_pos": 76, "end_pos": 84, "type": "METRIC", "confidence": 0.992021381855011}, {"text": "TWITUSER", "start_pos": 105, "end_pos": 113, "type": "DATASET", "confidence": 0.8867719173431396}]}, {"text": " Table 5: Proportion of messages from each dataset  that were still accessible as of August 2013.", "labels": [], "entities": []}]}