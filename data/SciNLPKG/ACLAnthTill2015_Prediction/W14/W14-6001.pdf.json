{"title": [{"text": "Investigating Context Parameters in Technology Term Recognition", "labels": [], "entities": [{"text": "Investigating Context Parameters", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7508225838343302}, {"text": "Technology Term Recognition", "start_pos": 36, "end_pos": 63, "type": "TASK", "confidence": 0.7485831479231516}]}], "abstractContent": [{"text": "We propose and evaluate the task of technology term recognition: a method to extract technology terms at a synchronic level from a corpus of scientific publications.", "labels": [], "entities": [{"text": "technology term recognition", "start_pos": 36, "end_pos": 63, "type": "TASK", "confidence": 0.628775546948115}]}, {"text": "The proposed method is built on the principles of terminology extraction and distributional semantics.", "labels": [], "entities": [{"text": "terminology extraction", "start_pos": 50, "end_pos": 72, "type": "TASK", "confidence": 0.8936181366443634}]}, {"text": "It is realized as a regression task in a vector space model.", "labels": [], "entities": []}, {"text": "In this method, candidate terms are first extracted from text.", "labels": [], "entities": []}, {"text": "Subsequently, using the random indexing technique, the extracted candidate terms are represented as vectors in a Euclidean vector space of reduced dimensionality.", "labels": [], "entities": []}, {"text": "These vectors are derived from the frequency of co-occurrences of candidate terms and words in windows of text surrounding candidate terms in the input corpus (context window).", "labels": [], "entities": []}, {"text": "The constructed vector space and a set of manually tagged technology terms (reference vectors) in a k-nearest neighbours regression framework is then used to identify terms that signify technology concepts.", "labels": [], "entities": []}, {"text": "We examine a number of factors that play roles in the performance of the proposed method, i.e. the configuration of context windows, neighborhood size (k) selection, and reference vector size.", "labels": [], "entities": []}], "introductionContent": [{"text": "Technology terms and their corresponding concepts are part and parcel of any system that tries to capture competitive technological intelligence).", "labels": [], "entities": []}, {"text": "We propose a method of technology term recognition (TTR) at a synchronic level, i.e., the identification of terms that correspond to technological concepts from a corpus of scientific publications.", "labels": [], "entities": [{"text": "technology term recognition (TTR)", "start_pos": 23, "end_pos": 56, "type": "TASK", "confidence": 0.8206675052642822}, {"text": "identification of terms that correspond to technological concepts from a corpus of scientific publications", "start_pos": 90, "end_pos": 196, "type": "TASK", "confidence": 0.6388508677482605}]}, {"text": "TTR can be viewed as a kind of automatic term recognition (ATR) task.", "labels": [], "entities": [{"text": "TTR", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.7981268167495728}, {"text": "term recognition (ATR) task", "start_pos": 41, "end_pos": 68, "type": "TASK", "confidence": 0.8233768592278162}]}, {"text": "The input of ATR is a large collection of documents, i.e. a domain-specific corpus, and the output is a terminological resource.", "labels": [], "entities": []}, {"text": "The generated terminological resource embraces terms that signify a wide spectrum of concepts in domain knowledge represented by the input corpus.", "labels": [], "entities": []}, {"text": "The extracted terms and their corresponding concepts, however, can be further organized in several categories; each category characterizes a group of 'similar' concepts (e.g. technology) in domain knowledge.", "labels": [], "entities": []}, {"text": "1 TTR, therefore, goes beyond ATR and targets a subset of terms that characterizes the category of technological concepts in domain knowledge).", "labels": [], "entities": [{"text": "ATR", "start_pos": 30, "end_pos": 33, "type": "METRIC", "confidence": 0.5357451438903809}]}, {"text": "Establishing a precise definition of technology-and subsequently finding its corresponding terms-is a fundamental problem studied in philosophy of science.", "labels": [], "entities": []}, {"text": "The most simplistic definition of technology, perhaps, can be found in a dictionary.", "labels": [], "entities": []}, {"text": "For example, Oxford dictionary defines technology as the 'application of scientific knowledge for practical purposes'.", "labels": [], "entities": [{"text": "Oxford dictionary", "start_pos": 13, "end_pos": 30, "type": "DATASET", "confidence": 0.9835557639598846}]}, {"text": "As to our understanding, technology terms signal concepts that involve processes-a series of actions taken in order to achieve a particular goal-e.g. as manifested in practical applications of a research.", "labels": [], "entities": []}, {"text": "Consequently, technology terms should not be confused with other categories of terms, e.g. terms that signify research subjects or problems.", "labels": [], "entities": []}, {"text": "For example, in computational linguistics literature, both 'language resource' and 'natural language processing' are This work is licensed under a Creative Commons Attribution 4.0 International Licence.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 84, "end_pos": 111, "type": "TASK", "confidence": 0.7103168765703837}]}, {"text": "Page numbers and proceedings footer are added by the organizers.", "labels": [], "entities": []}, {"text": "Licence details: http://creativecommons.org/licenses/by/4.0/ 1 Or, contrariwise, a group of similar concepts can form a category.", "labels": [], "entities": []}, {"text": "Even though, these category of terms are strongly correlated.", "labels": [], "entities": []}, {"text": "For instance, a technology may provide a solution fora research problem and can be defined in the scope of a research subject.", "labels": [], "entities": []}, {"text": "Therefore, it is important to note that a research problem or a research subject is not a technology.", "labels": [], "entities": []}, {"text": "valid terms; however, we only recognize the latter as a valid technology term.", "labels": [], "entities": []}, {"text": "In this example, 'language resource' signals artefacts such as lexicons and corpora.", "labels": [], "entities": []}, {"text": "Although the process of creating these artefacts involves several technologies, we do not consider them-and subsequently the term 'language resource'-as technology.", "labels": [], "entities": []}, {"text": "In the absence of an analytical answer to the question 'what is technology?', we suggest exploiting the context of terms in order to identify technology terms among them.", "labels": [], "entities": []}, {"text": "We believe that technology terms tend to appear in similar linguistic contexts.", "labels": [], "entities": []}, {"text": "By extending Harris's (1954) distributional hypothesis, we claim that the context of (previously) known technology terms can be modelled and used in order to identify new unknown technology terms.", "labels": [], "entities": []}, {"text": "We thus take a distributional approach to the problem of technology term recognition.", "labels": [], "entities": [{"text": "technology term recognition", "start_pos": 57, "end_pos": 84, "type": "TASK", "confidence": 0.6521642506122589}]}, {"text": "Consequently, we tie the context of terms to their meaning by quantification of their distributional similarities.", "labels": [], "entities": []}, {"text": "We employ vector spaces to model such distributional similarities.", "labels": [], "entities": []}, {"text": "Consequently, the proposed method for TTR is realized as a term classification task in a vector space model (VSM).", "labels": [], "entities": [{"text": "TTR", "start_pos": 38, "end_pos": 41, "type": "TASK", "confidence": 0.9881742000579834}, {"text": "term classification", "start_pos": 59, "end_pos": 78, "type": "TASK", "confidence": 0.7284749150276184}]}, {"text": "The proposed method employs the prevalent mechanism of terminology extraction in the form of a two-step procedure: candidate term extraction followed by term scoring and ranking).", "labels": [], "entities": [{"text": "terminology extraction", "start_pos": 55, "end_pos": 77, "type": "TASK", "confidence": 0.8331217169761658}, {"text": "candidate term extraction", "start_pos": 115, "end_pos": 140, "type": "TASK", "confidence": 0.5903478463490804}]}, {"text": "Candidate term extraction deals with the term formation and the extraction of term candidates.", "labels": [], "entities": [{"text": "Candidate term extraction", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.6749077439308167}]}, {"text": "We employ a linguistic filtering based on part-of-speech (PoS) tag sequences for the extraction of candidate terms.", "labels": [], "entities": []}, {"text": "Subsequent to candidate term extraction, a scoring procedure-which can be seen as a semantic weighting mechanism-is employed to indicate how likely it is that a candidate term is a technology term.", "labels": [], "entities": [{"text": "candidate term extraction", "start_pos": 14, "end_pos": 39, "type": "TASK", "confidence": 0.742664615313212}]}, {"text": "As suggested in, the scoring procedure in ATR usually combines scores that are known as termhood and unithood.", "labels": [], "entities": [{"text": "ATR", "start_pos": 42, "end_pos": 45, "type": "TASK", "confidence": 0.8354905247688293}]}, {"text": "Unithood indicates the degree to which a sequence of tokens can be combined to form a complex term (a lexical unit that is made of more than one token).", "labels": [], "entities": []}, {"text": "Unithood is, thus, a measure of the syntagmatic relation between the constituents of complex terms: a lexical association measure to identify collocations.", "labels": [], "entities": []}, {"text": "Termhood, on the other hand, 'is the degree to which a stable lexical unit is related to some domain-specific concepts' (.", "labels": [], "entities": []}, {"text": "It characterizes a paradigmatic relation between lexical units-either simple (made of one token) or complex terms-and the communicative context that verbalizes domain-concepts.", "labels": [], "entities": []}, {"text": "In this paper, in order to simplify the evaluation framework, we assume that the PoS-based approach to candidate term extraction implicitly characterizes the unithood score.", "labels": [], "entities": [{"text": "candidate term extraction", "start_pos": 103, "end_pos": 128, "type": "TASK", "confidence": 0.6851473848025004}]}, {"text": "The focus is thus on the termhood measure.", "labels": [], "entities": []}, {"text": "We devise a termhood measure to distinguish technology terms from the set of extracted candidate terms.", "labels": [], "entities": []}, {"text": "We assume that the association of a term to a technology concept (i.e. what termhood determines) is a kind of paradigmatic relation that can be characterized using the syntagmatic relations of the term and its co-occurred, surrounding words in a context window).", "labels": [], "entities": []}, {"text": "Words appeared in context win-  dows are represented by the elements of the standard basis of a vector space.", "labels": [], "entities": []}, {"text": "The frequency of words in context windows of a candidate term (in the whole corpus) then determines the coordinates of the vector that represent the candidate term.", "labels": [], "entities": []}, {"text": "To avoid the curse of dimensionality, the VSMs are constructed at reduced dimensionality using the random indexing technique . In this VSM, we characterize the category of technology terms using a set of reference terms, i.e. previously known technology terms.", "labels": [], "entities": []}, {"text": "Consequently, the proximity of vectors that represent candidate terms to the vectors that represent reference terms determines the association of candidate terms to the category of technology terms.", "labels": [], "entities": []}, {"text": "This association is measured using a k-nearest neighbours (k-nn) regression algorithm.", "labels": [], "entities": []}, {"text": "In the proposed technique, finding context window's properties that best characterize technology terms is the major research concern that should be investigated.", "labels": [], "entities": []}, {"text": "These properties are the size of the co-occurrence region, the position of a term in the context window and the direction in which the neighbourhood is extended (see.", "labels": [], "entities": []}, {"text": "To find the most discriminative context window, we construct several VSMs; each VSM represents a context window of a certain configuration (i.e. size, direction and the word order information).", "labels": [], "entities": []}, {"text": "We then examine the discriminative power of context windows by reporting the performance of the k-nn regression algorithm in these VSMs.", "labels": [], "entities": []}, {"text": "Furthermore, to examine the role of the number of reference vectors in the performance of the classification task, we repeat these experiments using various numbers of reference vectors.", "labels": [], "entities": [{"text": "classification task", "start_pos": 94, "end_pos": 113, "type": "TASK", "confidence": 0.9098898768424988}]}, {"text": "We report the results of similar evaluation methodology, however, using a k-nn voting algorithm in.", "labels": [], "entities": []}, {"text": "In the rest of this paper, we first detail the evaluation framework in Section 2: the employed corpus for the evaluation in Section 2.1, the construction of vector spaces in Section 2.2, the scoring procedure in Section 2.3 and the evaluation methodology in Section 2.4.", "labels": [], "entities": []}, {"text": "Subsequently, we report the observed results in Section 3 and conclude in Section 4.", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to evaluate the proposed method, we employ the ACL anthology reference corpus (ACL ARC) () and the ACL reference dataset for terminology extraction and classification (AC RD-TEC) (Zadeh and Handschuh, 2014a).", "labels": [], "entities": [{"text": "ACL anthology reference corpus (ACL ARC)", "start_pos": 56, "end_pos": 96, "type": "DATASET", "confidence": 0.8509042486548424}, {"text": "ACL reference dataset", "start_pos": 108, "end_pos": 129, "type": "DATASET", "confidence": 0.7881050308545431}, {"text": "terminology extraction and classification", "start_pos": 134, "end_pos": 175, "type": "TASK", "confidence": 0.8378147929906845}]}, {"text": "The ACL ARC has been developed with the aim of providing a platform for benchmarking methods of scholarly document processing.", "labels": [], "entities": [{"text": "ACL ARC", "start_pos": 4, "end_pos": 11, "type": "DATASET", "confidence": 0.892080545425415}, {"text": "scholarly document processing", "start_pos": 96, "end_pos": 125, "type": "TASK", "confidence": 0.5907761553923289}]}, {"text": "It consists of 10,922 articles that were published between 1965 to 2006 in the domain of computational linguistics.", "labels": [], "entities": [{"text": "computational linguistics", "start_pos": 89, "end_pos": 114, "type": "TASK", "confidence": 0.7261833548545837}]}, {"text": "These articles are digitized and enriched with bibliography metadata.", "labels": [], "entities": []}, {"text": "The provided resources in the ACL ARC consist of three layers: (a) source publications in portable document format (PDF), (b) automatically extracted text from the articles and (c) bibliographic metadata and citation network.", "labels": [], "entities": [{"text": "ACL ARC", "start_pos": 30, "end_pos": 37, "type": "DATASET", "confidence": 0.8098752200603485}]}, {"text": "Each of the articles in the collection is assigned to a unique identifier that indicates the source (e.g. journal or conference) ParsCit tool for the automatic identification of logical text sections in ACL ARC's raw text files.", "labels": [], "entities": [{"text": "automatic identification of logical text sections", "start_pos": 150, "end_pos": 199, "type": "TASK", "confidence": 0.7082622845967611}, {"text": "ACL ARC's raw text files", "start_pos": 203, "end_pos": 227, "type": "DATASET", "confidence": 0.6509098559617996}]}, {"text": "The resulting segmented text units are cleansed using a set of heuristics; for instance, broken words and text segments are joined, footnotes and captions are removed, and sections are organised into paragraphs.", "labels": [], "entities": []}, {"text": "Text sections are further segmented into PoS-tagged sentences and each linguistically welldefined unit, e.g. types (i.e. PoS-tagged and lemmatized words), sentences, paragraphs and (sub)sections, is assigned to a unique identifier.", "labels": [], "entities": []}, {"text": "These text units are stored and presented in inverted index files, in a tabseparated format.", "labels": [], "entities": []}, {"text": "Hence, text units can be easily traced back to the contexts and, eventually, publications that they appeared in. shows the statistics of text segments in the dataset.", "labels": [], "entities": []}, {"text": "The ACL RD-TEC consists of manual annotations that can be used for the evaluation of ATR and term classification tasks.", "labels": [], "entities": [{"text": "ACL RD-TEC", "start_pos": 4, "end_pos": 14, "type": "DATASET", "confidence": 0.7378482222557068}, {"text": "ATR", "start_pos": 85, "end_pos": 88, "type": "TASK", "confidence": 0.917137861251831}, {"text": "term classification tasks", "start_pos": 93, "end_pos": 118, "type": "TASK", "confidence": 0.8082232276598612}]}, {"text": "In its current release, more than 80,000 lexical units 7 are annotated as either valid or invalid terms.", "labels": [], "entities": []}, {"text": "For a given lexical form t, if t refers to a significant concept in the computational linguistics domain, it is annotated as valid.", "labels": [], "entities": []}, {"text": "8 Examples of valid terms are 'natural language' and 'terminology'.", "labels": [], "entities": []}, {"text": "In addition, valid terms are classified as those that can signal a technology concept.", "labels": [], "entities": []}, {"text": "Technology terms indicate a method or a process that is employed to accomplish a task; examples of these terms are 'parsing' and 'information retrieval', and more delicate terms such as 'linear interpolation'.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 130, "end_pos": 151, "type": "TASK", "confidence": 0.6811753213405609}]}, {"text": "Similar to the valid terms, terms that are annotated as technology terms do not exclusively belong to this class.", "labels": [], "entities": []}, {"text": "For example, 'computational linguistics' is a lexical form that can be classified as a technology term, e.g., in '\u00b7 \u00b7 \u00b7 promising area of application of computational linguistics techniques\u00b7 \u00b7 \u00b7 '.", "labels": [], "entities": [{"text": "computational linguistics'", "start_pos": 14, "end_pos": 40, "type": "TASK", "confidence": 0.747727781534195}]}, {"text": "However, it can also signal other concepts such as a scientific discipline as well as a community, e.g., in '\u00b7 \u00b7 \u00b7 theoretical work in computational linguistics\u00b7 \u00b7 \u00b7 ' and '\u00b7 \u00b7 \u00b7 pursued by the computational linguistics community \u00b7 \u00b7 \u00b7 ', respectively.", "labels": [], "entities": []}, {"text": "As reported in, the observed agreement between 4 participants in the manual annotation of technology terms from a small set of randomly selected candidate terms is 0.828; the obtained Cohen's kappa coefficient for inter annotator agreement is 0.627.", "labels": [], "entities": []}, {"text": "shows the current statistics of the annotated terms.", "labels": [], "entities": []}, {"text": "In the reported evaluation framework, the procedure described in Section 2.2 is performed to construct several vector spaces of various context configurations, which are described in Section 2.4.1.", "labels": [], "entities": []}, {"text": "The described procedure for term scoring in Section 2.3 is then employed to assign scores to the extracted candidate terms in all the constructed vector spaces.", "labels": [], "entities": [{"text": "term scoring", "start_pos": 28, "end_pos": 40, "type": "TASK", "confidence": 0.7160035818815231}]}, {"text": "In each experiment, candidate terms are sorted in descending order by their assigned scores.", "labels": [], "entities": []}, {"text": "The proportion of technology terms in the list of the top n terms (we start with 250 terms) is reported for the comparison of the performance of the evaluated context configurations.", "labels": [], "entities": []}, {"text": "We further investigate the role of the neighbourhood size selection k as well as the number of reference terms R sin the performance of the scoring task.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Summary statistics of the dataset derived from automatic processing of the ACL ARC.", "labels": [], "entities": [{"text": "ACL ARC", "start_pos": 85, "end_pos": 92, "type": "DATASET", "confidence": 0.8959639072418213}]}, {"text": " Table 2: Summary statistics of the annotated candidate terms.", "labels": [], "entities": [{"text": "Summary", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.6480916738510132}]}, {"text": " Table 3: Summary statistics of the reference terms R s . f (x) denotes the accumulative frequency of  occurrences of all the terms in R s in text segments of type x.", "labels": [], "entities": []}, {"text": " Table 4: The observed results from the performed evaluations. The number columns show the proportion  of technology terms in the top 250 terms for various values of k.", "labels": [], "entities": []}]}