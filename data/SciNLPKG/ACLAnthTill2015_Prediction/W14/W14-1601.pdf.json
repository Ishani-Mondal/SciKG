{"title": [{"text": "What's in a p-value in NLP?", "labels": [], "entities": []}], "abstractContent": [{"text": "In NLP, we need to document that our proposed methods perform significantly better with respect to standard metrics than previous approaches, typically by reporting p-values obtained by rank-or randomization-based tests.", "labels": [], "entities": []}, {"text": "We show that significance results following current research standards are unreliable and, in addition , very sensitive to sample size, co-variates such as sentence length, as well as to the existence of multiple metrics.", "labels": [], "entities": []}, {"text": "We estimate that under the assumption of perfect metrics and unbiased data, we need a significance cutoff at \u21e00.0025 to reduce the risk of false positive results to <5%.", "labels": [], "entities": [{"text": "significance cutoff", "start_pos": 86, "end_pos": 105, "type": "METRIC", "confidence": 0.9740098416805267}]}, {"text": "Since in practice we often have considerable selection bias and poor metrics, this, however, will not do alone.", "labels": [], "entities": []}], "introductionContent": [{"text": "In NLP, we try to improve upon state of the art language technologies, guided by experience and intuition, as well as error analysis from previous experiments, and research findings often consist in system comparisons showing that System A is better than System B. Effect size, i.e., one system's improvements over another, can be seen as a random variable.", "labels": [], "entities": []}, {"text": "If the random variable follows a known distribution, e.g., a normal distribution, we can use parametric tests to estimate whether System A is better than System B. If it follows a normal distribution, we can use Student's t-test, for example.", "labels": [], "entities": []}, {"text": "Effect sizes in NLP are generally not normally distributed or follow any of the other wellstudied distributions.", "labels": [], "entities": []}, {"text": "The standard significance testing methods in NLP are therefore rank-or randomization-based nonparametric tests.", "labels": [], "entities": []}, {"text": "Specifically, most system comparisons across words, sentences or documents use bootstrap tests or approximate randomization, while studies that compare performance across data sets use rank-based tests such as Wilcoxon's test.", "labels": [], "entities": []}, {"text": "The question we wish to address here is: how likely is a research finding in NLP to be false?", "labels": [], "entities": []}, {"text": "Naively, we would expect all reported findings to be true, but significance tests have their weaknesses, and sometimes researchers are forced to violate test assumptions and basic statistical methodology, e.g., when there is no one established metric, when we can't run our models on full-length sentences, or when data is biased.", "labels": [], "entities": []}, {"text": "For example, one such well-known bias from the tagging and parsing literature is what we may refer to as the WSJ FALLACY.", "labels": [], "entities": [{"text": "tagging and parsing", "start_pos": 47, "end_pos": 66, "type": "TASK", "confidence": 0.8590071002642313}, {"text": "WSJ", "start_pos": 109, "end_pos": 112, "type": "DATASET", "confidence": 0.7353448271751404}, {"text": "FALLACY", "start_pos": 113, "end_pos": 120, "type": "METRIC", "confidence": 0.8141565918922424}]}, {"text": "This is the false belief that performance on the test section of the Wall Street Journal (WSJ) part of the English Penn treebank is representative for performance on other texts in English.", "labels": [], "entities": [{"text": "test section of the Wall Street Journal (WSJ) part of the English Penn treebank", "start_pos": 49, "end_pos": 128, "type": "DATASET", "confidence": 0.8760264702141285}]}, {"text": "In other words, it is the belief that our samples are always representative.", "labels": [], "entities": []}, {"text": "However, (the unawareness of) selection bias is not the only reason research findings in NLP maybe false.", "labels": [], "entities": []}, {"text": "In this paper, we critically examine significance results in NLP by simulations, as well as running a series of experiments comparing state-of-the-art POS taggers, dependency parsers, and NER systems, focusing on the sensitivity of p-values to various factors.", "labels": [], "entities": [{"text": "POS taggers", "start_pos": 151, "end_pos": 162, "type": "TASK", "confidence": 0.6725775599479675}, {"text": "dependency parsers", "start_pos": 164, "end_pos": 182, "type": "TASK", "confidence": 0.7140579521656036}]}, {"text": "Specifically, we address three important factors: Sample size.", "labels": [], "entities": [{"text": "Sample size", "start_pos": 50, "end_pos": 61, "type": "TASK", "confidence": 0.8254318535327911}]}, {"text": "When system A is reported to be better than system B, this may not hold across domains (cf. WSJ FALLACY).", "labels": [], "entities": [{"text": "WSJ", "start_pos": 92, "end_pos": 95, "type": "DATASET", "confidence": 0.4531453251838684}, {"text": "FALLACY", "start_pos": 96, "end_pos": 103, "type": "METRIC", "confidence": 0.8148943781852722}]}, {"text": "More importantly, though, it may not even hold on a sub-sample of the test data, or if we added more data points to the test set.", "labels": [], "entities": []}, {"text": "Below, we show that in 6/10 of our POS tagger evaluations, significant effects become insignificant by (randomly) adding more test data.", "labels": [], "entities": [{"text": "POS tagger", "start_pos": 35, "end_pos": 45, "type": "TASK", "confidence": 0.7276158034801483}]}], "datasetContent": [{"text": "Throughout the rest of the paper, we use four running examples: a synthetic toy example and three standard experimental NLP tasks, namely POS tagging, dependency parsing and NER.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 138, "end_pos": 149, "type": "TASK", "confidence": 0.7841928601264954}, {"text": "dependency parsing", "start_pos": 151, "end_pos": 169, "type": "TASK", "confidence": 0.8707737624645233}]}, {"text": "The toy example is supposed to illustrate the logic behind our reasoning and is not specific to NLP.", "labels": [], "entities": []}, {"text": "It shows how likely we are to obtain a low p-value for the difference in means when sampling from exactly the same (Gaussian) distributions.", "labels": [], "entities": []}, {"text": "For the NLP setups (2-4), we use off-the-shelf models or available runs, as described next.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: POS tagging p-values across tagging ac- curacy (TA), accuracy for unseen words (UA) and  sentence-level accuracy (SA) with bootstrap (b)  and Wilcoxon (w) (p < 0.05 gray-shaded).", "labels": [], "entities": [{"text": "POS tagging p-values", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.7494318087895712}, {"text": "ac- curacy (TA)", "start_pos": 46, "end_pos": 61, "type": "METRIC", "confidence": 0.8906999925772349}, {"text": "accuracy for unseen words (UA)", "start_pos": 63, "end_pos": 93, "type": "METRIC", "confidence": 0.64848564352308}, {"text": "accuracy (SA)", "start_pos": 114, "end_pos": 127, "type": "METRIC", "confidence": 0.9193252921104431}]}, {"text": " Table 4: Ratio of positive results (p < 0.05) for MALT-LIN VS. STANFORD-RNN at sample sizes (N )", "labels": [], "entities": [{"text": "Ratio", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9886255264282227}, {"text": "MALT-LIN VS", "start_pos": 51, "end_pos": 62, "type": "METRIC", "confidence": 0.6835760176181793}, {"text": "STANFORD-RNN", "start_pos": 64, "end_pos": 76, "type": "METRIC", "confidence": 0.7679787874221802}]}]}