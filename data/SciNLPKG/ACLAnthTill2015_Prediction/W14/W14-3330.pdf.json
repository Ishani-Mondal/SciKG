{"title": [{"text": "LIMSI @ WMT'14 Medical Translation Task", "labels": [], "entities": [{"text": "WMT'14 Medical Translation", "start_pos": 8, "end_pos": 34, "type": "TASK", "confidence": 0.7968685229619344}]}], "abstractContent": [{"text": "This paper describes LIMSI's submission to the first medical translation task at WMT'14.", "labels": [], "entities": [{"text": "medical translation task at WMT'14", "start_pos": 53, "end_pos": 87, "type": "TASK", "confidence": 0.777997636795044}]}, {"text": "We report results for English-French on the subtask of sentence translation from summaries of medical articles.", "labels": [], "entities": [{"text": "sentence translation from summaries of medical articles", "start_pos": 55, "end_pos": 110, "type": "TASK", "confidence": 0.8280078257833209}]}, {"text": "Our main submission uses a combination of NCODE (n-gram-based) and MOSES (phrase-based) output and continuous-space language models used in a post-processing step for each system.", "labels": [], "entities": []}, {"text": "Other characteristics of our submission include: the use of sampling for building MOSES' phrase table; the implementation of the vector space model proposed by Chen et al.", "labels": [], "entities": []}, {"text": "(2013); adaptation of the POS-tagger used by NCODE to the medical domain ; and a report of error analysis based on the typology of Vilar et al.", "labels": [], "entities": []}], "introductionContent": [{"text": "This paper describes LIMSI's submission to the first medical translation task at WMT'14.", "labels": [], "entities": [{"text": "medical translation task at WMT'14", "start_pos": 53, "end_pos": 87, "type": "TASK", "confidence": 0.777997636795044}]}, {"text": "This task is characterized by high-quality input text and the availability of large amounts of training data from the same domain, yielding unusually high translation performance.", "labels": [], "entities": []}, {"text": "This prompted us to experiment with two systems exploring different translation spaces, the n-gram-based NCODE ( \u00a72.1) and an on-the-fly variant of the phrasebased MOSES ( \u00a72.2), and to later combine their output.", "labels": [], "entities": []}, {"text": "Further attempts at improving translation quality were made by resorting to continuous language model rescoring ( \u00a72.4), vector space subcorpus adaptation ( \u00a72.3), and POS-tagging adaptation to the medical domain ( \u00a73.3).", "labels": [], "entities": [{"text": "translation", "start_pos": 30, "end_pos": 41, "type": "TASK", "confidence": 0.9652594923973083}, {"text": "continuous language model rescoring", "start_pos": 76, "end_pos": 111, "type": "TASK", "confidence": 0.6336404457688332}, {"text": "vector space subcorpus adaptation", "start_pos": 121, "end_pos": 154, "type": "TASK", "confidence": 0.6802636831998825}, {"text": "POS-tagging adaptation", "start_pos": 168, "end_pos": 190, "type": "TASK", "confidence": 0.7928508222103119}]}, {"text": "We also performed a small-scale error analysis of the outputs of some of our systems ( \u00a75).", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Parallel corpora used in this work, along with the number of sentences and the number of English  and French tokens, respectively. Weights (\u03bb k ) from our best NCODE configuration are indicated for each  sub-corpora's bilingual word language model (wrd-lm) and POS factor language model (pos-lm).", "labels": [], "entities": []}, {"text": " Table 2: Impact of the optimization method during  the tuning process on BLEU score, for a baseline  NCODE system.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 74, "end_pos": 84, "type": "METRIC", "confidence": 0.9753370583057404}]}, {"text": " Table 3: BLEU scores obtained by NCODE trained  on medical data only, WMT'13 data only, or both.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.999146580696106}, {"text": "NCODE", "start_pos": 34, "end_pos": 39, "type": "DATASET", "confidence": 0.9273778200149536}, {"text": "WMT'13 data", "start_pos": 71, "end_pos": 82, "type": "DATASET", "confidence": 0.8072726130485535}]}, {"text": " Table 4: BLEU results when using a standard POS  tagging (std) or our medical adapted specialized  method (spec), either for the reordering rule mech- anism (Reordering) or for the POS-POS bilingual  language models features (Factor model).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9987139701843262}, {"text": "POS  tagging", "start_pos": 45, "end_pos": 57, "type": "TASK", "confidence": 0.6845824718475342}]}, {"text": " Table 5: Influence of the choice of the develop- ment set when using our baseline NCODE system.  Each row corresponds to the choice of a develop- ment set used in the tuning process, indicated by a  surrounded BLEU score.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 211, "end_pos": 221, "type": "METRIC", "confidence": 0.9621299505233765}]}, {"text": " Table 6: Contrast of our two main systems and  their combination, when adding SOUL language  (LM) and translation (TM) models. Stars indicate  an adapted LM. BLEU results for the best run on  the development set are reported.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 159, "end_pos": 163, "type": "METRIC", "confidence": 0.9986733198165894}]}, {"text": " Table 7: Results for manual error analysis following (Vilar et al., 2006) for the first 100 test sentences.", "labels": [], "entities": []}]}