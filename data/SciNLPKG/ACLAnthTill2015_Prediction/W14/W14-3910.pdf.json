{"title": [], "abstractContent": [{"text": "This paper describes a CRF based token level language identification system entry to Language Identification in Code-Switched (CS) Data task of CodeSwitch 2014.", "labels": [], "entities": [{"text": "token level language identification", "start_pos": 33, "end_pos": 68, "type": "TASK", "confidence": 0.5663866102695465}, {"text": "Code-Switched (CS) Data task of CodeSwitch 2014", "start_pos": 112, "end_pos": 159, "type": "DATASET", "confidence": 0.6256133019924164}]}, {"text": "Our system hinges on using conditional posterior probabilities for the individual codes (words) in code-switched data to solve the language identification task.", "labels": [], "entities": [{"text": "language identification task", "start_pos": 131, "end_pos": 159, "type": "TASK", "confidence": 0.8266568978627523}]}, {"text": "We also experiment with other linguistically motivated language specific as well as generic features to train the CRF based sequence labeling algorithm achieving reasonable results.", "labels": [], "entities": []}], "introductionContent": [{"text": "This paper describes our participation in the Language Identification in Code-Switched Data task at CodeSwitch 2014 ().", "labels": [], "entities": [{"text": "Language Identification in Code-Switched Data task at CodeSwitch 2014", "start_pos": 46, "end_pos": 115, "type": "TASK", "confidence": 0.7579216361045837}]}, {"text": "The workshop focuses on NLP approaches for the analysis and processing of mixed-language data with a focus on intra sentential code-switching, while the shared task focuses on the identification of the language of each word in a codeswitched data, which is a prerequisite for analyzing/processing such data.", "labels": [], "entities": []}, {"text": "Code-switching is a sociolinguistics phenomenon, where multilingual speakers switchback and forth between two or more common languages or language-varieties, in the context of a single written or spoken conversation.", "labels": [], "entities": []}, {"text": "Natural language analysis of codeswitched (henceforth CS) data for various NLP tasks like Parsing, Machine Translation (MT), Automatic Speech Recognition (ASR), Information Retrieval (IR) and Extraction (IE) and Semantic Processing, is more complex than monolingual data.", "labels": [], "entities": [{"text": "Natural language analysis of codeswitched (henceforth CS)", "start_pos": 0, "end_pos": 57, "type": "TASK", "confidence": 0.7635011441177793}, {"text": "Machine Translation (MT)", "start_pos": 99, "end_pos": 123, "type": "TASK", "confidence": 0.8646069288253784}, {"text": "Automatic Speech Recognition (ASR)", "start_pos": 125, "end_pos": 159, "type": "TASK", "confidence": 0.7692776421705881}, {"text": "Information Retrieval (IR) and Extraction (IE)", "start_pos": 161, "end_pos": 207, "type": "TASK", "confidence": 0.8707500040531159}]}, {"text": "Traditional NLP techniques perform miserably when processing mixed language data.", "labels": [], "entities": []}, {"text": "The performance degrades at a rate proportional to the amount and level of code-switching present in the data.", "labels": [], "entities": []}, {"text": "Therefore, in order to process such data, a separate language identification component is needed, to first identify the language of individual words.", "labels": [], "entities": [{"text": "language identification", "start_pos": 53, "end_pos": 76, "type": "TASK", "confidence": 0.7267434895038605}]}, {"text": "Language identification in code-switched data can bethought of as a sub-task of a document level language identification task.", "labels": [], "entities": [{"text": "Language identification", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.7242362201213837}, {"text": "document level language identification task", "start_pos": 82, "end_pos": 125, "type": "TASK", "confidence": 0.6421014666557312}]}, {"text": "The latter aims to identify the language a given document is written in, while the former addresses the same problem, however at the token level.", "labels": [], "entities": []}, {"text": "Although, both the problems have separate goals, they can fundamentally be modeled with a similar set of features and techniques.", "labels": [], "entities": []}, {"text": "However, language identification at the word level is more challenging than atypical document level language identification problem.", "labels": [], "entities": [{"text": "language identification", "start_pos": 9, "end_pos": 32, "type": "TASK", "confidence": 0.7694087028503418}, {"text": "language identification", "start_pos": 100, "end_pos": 123, "type": "TASK", "confidence": 0.7504498064517975}]}, {"text": "The number of features available at document level is much higher than at word level.", "labels": [], "entities": []}, {"text": "The available features for word level identification are word morphology, syllable structure and phonemic (letter) inventory of the language(s).", "labels": [], "entities": [{"text": "word level identification", "start_pos": 27, "end_pos": 52, "type": "TASK", "confidence": 0.6649862130482992}]}, {"text": "Since these features are related to the structure of a word, letter based n-gram models have been reported to give reasonably accurate and comparable results.", "labels": [], "entities": []}, {"text": "In this work, we present a token level language identification system which mainly hinges on the posterior probabilities computed using n-gram based language models.", "labels": [], "entities": [{"text": "token level language identification", "start_pos": 27, "end_pos": 62, "type": "TASK", "confidence": 0.6460876539349556}]}, {"text": "The rest of the paper is organized as follows: In Section 2, we discuss about the data of the shared task.", "labels": [], "entities": []}, {"text": "In Section 3, we discuss the methodology we adapted to address the problem of language identification, in detail.", "labels": [], "entities": [{"text": "language identification", "start_pos": 78, "end_pos": 101, "type": "TASK", "confidence": 0.7426023185253143}]}, {"text": "Experiments based on our methodology are discussed in Section 4.", "labels": [], "entities": []}, {"text": "In Section 5, we present the results obtained, with a brief discussion.", "labels": [], "entities": []}, {"text": "Finally we conclude in Section 6 with some future directions.", "labels": [], "entities": []}], "datasetContent": [{"text": "The pipeline mentioned in Section 3 was used for the language identification task for all the LPs.", "labels": [], "entities": [{"text": "language identification task", "start_pos": 53, "end_pos": 81, "type": "TASK", "confidence": 0.8777007460594177}]}, {"text": "We carried out a series of experiments with preprocessing to clean the training data and also to synchronize the testing data.", "labels": [], "entities": []}, {"text": "We also did some post-processing to handle language and tag specific cases.", "labels": [], "entities": []}, {"text": "In order to generate language model scores, we trained 6 language models (one for each language/dialect) on the filtered-out training data as mentioned in.", "labels": [], "entities": []}, {"text": "We experimented with different values of n-gram to select the optimal value based on the F1-measure.", "labels": [], "entities": [{"text": "F1-measure", "start_pos": 89, "end_pos": 99, "type": "METRIC", "confidence": 0.9950335025787354}]}, {"text": "shows the optimal order of n-gram, selected corresponding to the highest value of F1-score.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 82, "end_pos": 90, "type": "METRIC", "confidence": 0.9965705871582031}]}, {"text": "Using the optimal value of n-gram, language models have been trained and then posterior probabilities have been calculated using equation (1).", "labels": [], "entities": []}, {"text": "Finally, we trained separate CRF models for each LP, using the CRF++ 9 toolkit based on the features described in Section 3.3.1 and the feature template in Section 3.3.2.", "labels": [], "entities": []}, {"text": "To empirically find the relevance of features we also performed leave-one out experiments so as to decide the optimal features for the language identification task (more details in Section 4.1).", "labels": [], "entities": [{"text": "language identification task", "start_pos": 135, "end_pos": 163, "type": "TASK", "confidence": 0.8591646154721578}]}, {"text": "Then, using these CRF models, tags were predicted on the testing and surprise genre datasets.", "labels": [], "entities": []}, {"text": "Language-Pair N-gram: Optimal Value of N-gram", "labels": [], "entities": [{"text": "Optimal Value", "start_pos": 22, "end_pos": 35, "type": "METRIC", "confidence": 0.9359562397003174}]}], "tableCaptions": [{"text": " Table 2: Data Statistics after Filtering", "labels": [], "entities": []}, {"text": " Table 4: Token Level Results", "labels": [], "entities": [{"text": "Token Level", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.5987527668476105}]}, {"text": " Table 6: Comment/Post/Tweet Level Results", "labels": [], "entities": [{"text": "Comment/Post/Tweet Level", "start_pos": 10, "end_pos": 34, "type": "TASK", "confidence": 0.6095143755276998}]}]}