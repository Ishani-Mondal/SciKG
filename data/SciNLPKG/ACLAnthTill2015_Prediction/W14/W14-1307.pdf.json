{"title": [{"text": "Vowel and Diacritic Restoration for Social Media Texts", "labels": [], "entities": [{"text": "Vowel and Diacritic Restoration", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.6212312206625938}]}], "abstractContent": [{"text": "In this paper, we focus on two important problems of social media text normaliza-tion, namely: vowel and diacritic restoration.", "labels": [], "entities": [{"text": "vowel and diacritic restoration", "start_pos": 95, "end_pos": 126, "type": "TASK", "confidence": 0.6624597907066345}]}, {"text": "For these two problems, we propose a hybrid model consisting both a dis-criminative sequence classifier and a language validator in order to select one of the morphologically valid outputs of the first stage.", "labels": [], "entities": []}, {"text": "The proposed model is language independent and has no need for manual annotation of the training data.", "labels": [], "entities": []}, {"text": "We measured the performance both on synthetic data specifically produced for these two problems and on real social media data.", "labels": [], "entities": []}, {"text": "Our model (with 97.06% on synthetic data) improves the state of the art results for diacritization of Turkish by 3.65 percentage points on ambiguous cases and for the vowel restoration by 45.77 percentage points over a rule based baseline with 62.66% accuracy.", "labels": [], "entities": [{"text": "vowel restoration", "start_pos": 167, "end_pos": 184, "type": "TASK", "confidence": 0.6787426471710205}, {"text": "accuracy", "start_pos": 251, "end_pos": 259, "type": "METRIC", "confidence": 0.9977706670761108}]}, {"text": "The results on real data are 95.43% and 69.56% accordingly.", "labels": [], "entities": []}], "introductionContent": [{"text": "In recent years, with the high usage of computers and social networks like Facebook and Twitter, the analysis of the social media language has become a very popular and crucial form of business intelligence.", "labels": [], "entities": []}, {"text": "But unfortunately, this language is very different from the well edited written texts and much more similar to the spoken language, so that, the available NLP tools do not perform well on this new platform.", "labels": [], "entities": []}, {"text": "As we all know, Twitter announced (at April 1st, 2013) 1 that it is shifting to a two-tiered service where the basic free service 'Twttr\" will only allow to use consonants in the tweets.", "labels": [], "entities": [{"text": "Twitter announced (at April 1st, 2013) 1", "start_pos": 16, "end_pos": 56, "type": "DATASET", "confidence": 0.6905712842941284}]}, {"text": "Although, this is a very funny joke, people nowadays are already very used to use this style of writing without vowels in order to fit their messages into 140 characters Twitter or 160 characters SMS messages.", "labels": [], "entities": []}, {"text": "As a result, the vowelization problem (Twttr \u21d2 Twitter) is no more limited with some specific language families (e.g.semitic languages)) but it became a problem of social media text normalization in general.", "labels": [], "entities": [{"text": "social media text normalization", "start_pos": 164, "end_pos": 195, "type": "TASK", "confidence": 0.7083289548754692}]}, {"text": "Diacritics are some marks (e.g. accents, dots, curves) added to the characters and have a wide usage in many languages.", "labels": [], "entities": []}, {"text": "The absence of these marks in Web2.0 language is very common and posses a big problem in the automatic processing of this data by NLP tools.", "labels": [], "entities": []}, {"text": "Although, in the literature, the term \"diacritization\" is used both for vowel and diacritic restoration for semitic languages, in this paper, we use this term only for the task of converting an ASCII text to its proper form (with accents and special characters).", "labels": [], "entities": []}, {"text": "A Turkish example is the word \"dondu\" (it is frozen) which maybe the ascii form of both \"dondu\"(it is frozen) or \"d\u00f6nd\u00fc\" (it returned) where the ambiguity should be resolved according to the context.", "labels": [], "entities": []}, {"text": "In some studies, this task is also referred as \"unicodification\" or \"deasciification\".", "labels": [], "entities": []}, {"text": "In this paper, we focus on these two important problems of social text normalization, namely: diacritization and vowelization.", "labels": [], "entities": [{"text": "social text normalization", "start_pos": 59, "end_pos": 84, "type": "TASK", "confidence": 0.687724749247233}]}, {"text": "These two problems compose almost the quarter (26.5%) of the normalization errors within a 25K Turkish Tweeter Data Set.", "labels": [], "entities": [{"text": "normalization", "start_pos": 61, "end_pos": 74, "type": "TASK", "confidence": 0.9324368238449097}, {"text": "Turkish Tweeter Data Set", "start_pos": 95, "end_pos": 119, "type": "DATASET", "confidence": 0.9442186206579208}]}, {"text": "We propose a two stage hybrid model: firstly a discriminative model as a sequence classification task by using CRFs (Conditional Random Fields) and secondly a language validator over the first stage's results.", "labels": [], "entities": [{"text": "sequence classification", "start_pos": 73, "end_pos": 96, "type": "TASK", "confidence": 0.7127356678247452}]}, {"text": "Although in this paper, we presented our results on Turkish (which is a highly agglutinative language with very long words full of un-ascii characters), the proposed model is totally language independent and has no need for manual annotation of the training data.", "labels": [], "entities": []}, {"text": "For morphologically simpler languages, it would be enough to use a lexicon lookup for the language validation stage (whereas we used a morphological analyzer for the case of Turkish).", "labels": [], "entities": []}, {"text": "With our proposed model, we obtained the highest results in the literature for Turkish diacritization and vowelization.", "labels": [], "entities": [{"text": "Turkish diacritization", "start_pos": 79, "end_pos": 101, "type": "TASK", "confidence": 0.6389653980731964}]}, {"text": "The remaining of the paper is structured as follows: Section 2 discusses the related work, Section 3 tries to show the complexity of diacritization and the vowelization tasks by giving examples from an agglutinative language; Turkish.", "labels": [], "entities": []}, {"text": "Section 4 introduces our proposed model and Section 5 presents our experiments and results.", "labels": [], "entities": []}, {"text": "The conclusion is given in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we first present our datasets and evaluation strategy.", "labels": [], "entities": []}, {"text": "We then continue with the diacritization experiments and finally we share the results of our vowelization experiments.", "labels": [], "entities": []}, {"text": "For both of the diacritization and vowelization tasks, creating the labeled data is a straightforward task since the reverse operations for these (converting from formally written text to their Ascii form or to a form without vowels) can be accomplished automatically for most of the languages (except semitic languages where the vowels do not appear in the formal form).", "labels": [], "entities": []}, {"text": "To give an example from Turkish, the word \"oldu\u02d8 gunu\" maybe automatically converted to the form \"OldUGUnU\" for diacritization and \" l d g n \" for vowelization experiments.", "labels": [], "entities": []}, {"text": "We used data from three different corpora: METU Corpus () and two web corpora from Y\u0131ld\u0131z and Tantu\u02d8 g (2012) and.", "labels": [], "entities": [{"text": "METU Corpus", "start_pos": 43, "end_pos": 54, "type": "DATASET", "confidence": 0.9519588947296143}, {"text": "Y\u0131ld\u0131z and Tantu\u02d8 g (2012)", "start_pos": 83, "end_pos": 109, "type": "DATASET", "confidence": 0.7903029024600983}]}, {"text": "In order to judge different approaches fairly, we aimed to create a decently though test set.", "labels": [], "entities": []}, {"text": "Since the vowelization task already comprises a very high ambiguity, we focused to the ambiguous diacritization samples.", "labels": [], "entities": []}, {"text": "With this purpose, we first took the Turkish dictionary and converted all the lemmas within the dictionary into their Ascii forms.", "labels": [], "entities": [{"text": "Turkish dictionary", "start_pos": 37, "end_pos": 55, "type": "DATASET", "confidence": 0.9089235365390778}]}, {"text": "We then created the possible diacritized forms) and created a list of all ambiguous lemmas (1221 lemmas in total) by finding all the lemmas which could be produced as the output of diacritization.", "labels": [], "entities": []}, {"text": "For example \"a\u00e7\u0131\" and \"ac\u0131\" are put into this list after this operation.", "labels": [], "entities": []}, {"text": "Although this ambiguous lemmas list maybe extended by also considering interfusing surface forms, for the sake of simplicity we just considered to take the ambiguous lemmas from the dictionary.", "labels": [], "entities": []}, {"text": "We then searched our three corpora (and the WEB where not available in these) for the words with an ambiguous lemma and created our test set so that for each ambiguous lemma there is exactly one sentence consisting of it.", "labels": [], "entities": [{"text": "WEB", "start_pos": 44, "end_pos": 47, "type": "METRIC", "confidence": 0.4769640564918518}]}, {"text": "As a result, we collected a test set of 1157 sentences (17923 tokens) consisting of 1871 ambiguous words 10 in total.", "labels": [], "entities": []}, {"text": "The remaining sentences from the corpora are used during training.", "labels": [], "entities": []}, {"text": "Since the feature set size directly affects the amount of usable training data, for different experiment sets we used different size of training data each time trying to use the data from the three corpora in equal amounts.", "labels": [], "entities": []}, {"text": "After evaluating with synthetically produced training and test sets, we also tested our best performed models on real data collected from social media (25K tweets with at least one erroneous token) and normalized manually . This data consists 58836 tokens that have text normalization problems where 3.75% is due to missing vowels and 22.8% is due to misuse of Turkish characters.", "labels": [], "entities": [{"text": "text normalization", "start_pos": 266, "end_pos": 284, "type": "TASK", "confidence": 0.6841996163129807}]}, {"text": "In order to separate these specific error sets, we first automatically aligned the original and normalized tweets and then applied some filters over the aligned data: e.g. Deasciification errors are selected so that the character length of the original word and its normalized form should be the same and the differing letters should only be the versions of the same Turkish characters.", "labels": [], "entities": []}, {"text": "For the evaluation of diacritization, we provide two accuracy scores: Accuracy over the entire words (Acc Overall Equation 1) and accuracy over the ambiguous words alone (Acc Amb Equation 2 over 1871 ambiguous words in the test set).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.9989450573921204}, {"text": "Accuracy", "start_pos": 70, "end_pos": 78, "type": "METRIC", "confidence": 0.9988082647323608}, {"text": "Acc Overall Equation 1)", "start_pos": 102, "end_pos": 125, "type": "METRIC", "confidence": 0.9419473052024842}, {"text": "accuracy", "start_pos": 130, "end_pos": 138, "type": "METRIC", "confidence": 0.9986299276351929}, {"text": "Acc Amb Equation 2", "start_pos": 171, "end_pos": 189, "type": "METRIC", "confidence": 0.9083324819803238}]}, {"text": "Since the vowelization problem is almost entirely ambiguous, the two scores are almost the same for the entire tests (# of words \u2248 # of amb. words).", "labels": [], "entities": []}, {"text": "That is why, for the vowelization task we provide only Acc Overall . Acc Overall = # of corr.", "labels": [], "entities": [{"text": "Acc", "start_pos": 55, "end_pos": 58, "type": "METRIC", "confidence": 0.9978343844413757}, {"text": "Acc Overall", "start_pos": 69, "end_pos": 80, "type": "METRIC", "confidence": 0.9701243937015533}]}, {"text": "diacritized words # of words Acc Amb = # of corr.", "labels": [], "entities": [{"text": "Acc Amb", "start_pos": 29, "end_pos": 36, "type": "METRIC", "confidence": 0.928583174943924}]}, {"text": "words (2) In the work of T\u00fcr, the accuracy score is provided as the correctly determined characters which we do not find useful for the given tasks: . This score gives credit to the systems although the produced output word is not a valid Turkish word.", "labels": [], "entities": [{"text": "accuracy score", "start_pos": 34, "end_pos": 48, "type": "METRIC", "confidence": 0.9814402461051941}]}, {"text": "For example, if a vowelization system produces an invalid output as \"oldgn\" for the input \" l d g n \", it will have a 1/5 (one correct character over 5 possible positions) score whereas in our evaluation this output will be totally penalized.", "labels": [], "entities": []}, {"text": "For diacritization, we designed four sets of experiments.", "labels": [], "entities": [{"text": "diacritization", "start_pos": 4, "end_pos": 18, "type": "TASK", "confidence": 0.9746280312538147}]}, {"text": "The first set of experiments presents the results of our baseline systems.", "labels": [], "entities": []}, {"text": "We provide four baseline systems.", "labels": [], "entities": []}, {"text": "The first one is a rule based diacritics restorer which creates all the possible diacritics fora given input and outputs the first morphologically valid one.", "labels": [], "entities": []}, {"text": "As the proposed model does, the rule based system also validates its outputs by using the morphological analyzer introduced in Section 4.2.", "labels": [], "entities": []}, {"text": "One can see from the table that the accuracy on the ambiguous words of this system is nearly 70%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.9996744394302368}]}, {"text": "Our second baseline uses a unigram language model in order to select the most probable valid output of the morphological analyzer.", "labels": [], "entities": []}, {"text": "Our third baseline is a baseline for our discriminative classifier (with only \u00b12 neighboring characters) without the language validator component.", "labels": [], "entities": []}, {"text": "In this model, the topmost output of the CRF is accepted as the correct output word.", "labels": [], "entities": []}, {"text": "One can observe that this baseline although it performs better than the rule based system, it is worse than the second baseline with a language model component.", "labels": [], "entities": []}, {"text": "Our last baseline is the baseline for the proposed system in this paper with a discriminative classifier (using only \u00b12 neighboring characters) and a language validator which chooses the first valid output within the top 5 results of the classifier.", "labels": [], "entities": []}, {"text": "It outperforms all the previous baselines.", "labels": [], "entities": []}, {"text": "The second set of experiments given in is for the feature selection of the proposed model.", "labels": [], "entities": []}, {"text": "We test with the neighboring characters up to \u00b13 and together with the surface form of the current token sf orm curr and/or the first n characters of the current token f irstnch curr as lemma feature.", "labels": [], "entities": []}, {"text": "For both of the first two sets of experiments) we used a training data of size 4591K (the max.", "labels": [], "entities": []}, {"text": "possible size for the most complex feature set in these experiments; (last line of).", "labels": [], "entities": []}, {"text": "It can be observed from that although \u00b13ch (2nd line) performs better than \u00b12ch (1st line), when we use these together with sf orm curr we obtain better results with \u00b12ch (3rd line).", "labels": [], "entities": []}, {"text": "Since \u00b13ch (7 characters in total) will be very close to the whole number of characters within the surface form, the new feature's help is more modest in \u00b13ch model.", "labels": [], "entities": []}, {"text": "In these experiments we try to optimize on the overall accuracies.", "labels": [], "entities": []}, {"text": "Our highest score in this table is with the \u00b12ch + sf orm curr + f irst5ch curr (last line) but since the difference between this and \u00b12ch + sf orm curr is not statistically significant (with McNemar's test p<0.01) and the size of the maximum possible training data could still be improved for the latter model, we decided to continue with \u00b12ch + sf orm curr .  For the vowelization, we designed similar set of experiments.", "labels": [], "entities": [{"text": "McNemar's test p", "start_pos": 192, "end_pos": 208, "type": "DATASET", "confidence": 0.7192144095897675}]}, {"text": "In, we provide the results fora rulebased baseline and our proposed model with \u00b12ch.", "labels": [], "entities": []}, {"text": "It is certainly a very time consuming process to produce all the possible forms for the vowelization task (see Section 3.3).", "labels": [], "entities": []}, {"text": "Thus, for the rule based baseline we stopped the generation process once we find a valid output.", "labels": [], "entities": []}, {"text": "The baseline of the proposed model provides a 28.44 percentage points improvements over the rule based system.", "labels": [], "entities": []}, {"text": "We did not try to compare our results with the work of T\u00fcr (an HMM model on character level) firstly because the developed model was not available for testing, secondly because the provided evaluation (see Section 5.1) was useless for our purposes and finally because our \u00b13 character model provided in the second line of gives the feature selection tests' results similarly to the previous section.", "labels": [], "entities": []}, {"text": "This time we obtained the highest score with \u00b13ch + sf orm curr 59.17%.", "labels": [], "entities": []}, {"text": "In this set of experiments, we used 4445K of training data.", "labels": [], "entities": []}, {"text": "In order to investigate the impact of neighboring tokens, in the experiments given in 50.89 +sf orm 49.60 +sf orm 31.84 +sf orm 49.41 +sf orm 47.78 +sf orm 48.98 +sf orm 47.88 +sf orm 47.21: Vowelization Feature Selection II 971K of training data.", "labels": [], "entities": []}, {"text": "We could not obtain any improvement with the neighboring tokens.", "labels": [], "entities": []}, {"text": "We relate these results to the fact that the neighboring tokens are also in vowel-less form in the training data so that this information do not help the disambiguation of the current token.", "labels": [], "entities": []}, {"text": "Since we could not add the word based features to this task by this model, for future work we are planning to apply a word based language model over the proposed model's possible output sequences.", "labels": [], "entities": []}, {"text": "In the final experiment set given in, we trained our best performing model \u00b13ch + sf orm curr with the maximum possible training data (6653K).", "labels": [], "entities": []}, {"text": "We also tested with different N values of CRF output.", "labels": [], "entities": []}, {"text": "Although there is a slight increase on the overall accuracy by passing from N=5 to N=10, the increase is much higher when we evaluate with Acc topN . Equation 3 gives the calculation of this score which basically calculates the highest score that could be obtained after perfect reranking of the top N results.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.9994035959243774}, {"text": "Acc", "start_pos": 139, "end_pos": 142, "type": "METRIC", "confidence": 0.9826987981796265}]}, {"text": "In this score the system is credited if the correct vowelized answer is within the top N results of the system.", "labels": [], "entities": []}, {"text": "We see from the table that there is still a margin for the improvement in top 10 results (up to 85.09% for the best model).", "labels": [], "entities": []}, {"text": "This strengthens our believe for the need of a word based language model over the proposed model outputs.", "labels": [], "entities": []}, {"text": "Our vowelization model in its current state achieves an accuracy score of 62.66% with a 45.77 percentage points improvements over the rule based baseline.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.9997671246528625}]}, {"text": "Finally we test our best models on vowelization and diacritization errors from our Tweeter data set and obtained 95.43% for diacritization and 69.56% for vowelization.", "labels": [], "entities": [{"text": "Tweeter data set", "start_pos": 83, "end_pos": 99, "type": "DATASET", "confidence": 0.9606531262397766}]}], "tableCaptions": [{"text": " Table 3: Diacritization Baseline Results", "labels": [], "entities": [{"text": "Diacritization Baseline", "start_pos": 10, "end_pos": 33, "type": "TASK", "confidence": 0.8355641067028046}]}, {"text": " Table 4: Diacritization Feature Selection I", "labels": [], "entities": [{"text": "Diacritization Feature", "start_pos": 10, "end_pos": 32, "type": "TASK", "confidence": 0.8839420676231384}]}, {"text": " Table 5: Diacritization Feature Selection II", "labels": [], "entities": [{"text": "Diacritization Feature Selection", "start_pos": 10, "end_pos": 42, "type": "TASK", "confidence": 0.892521878083547}]}, {"text": " Table 6: Diacritization Results Comparison with  Previous Work", "labels": [], "entities": [{"text": "Diacritization", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.9767962694168091}]}, {"text": " Table 8: Vowelization Feature Selection I", "labels": [], "entities": []}, {"text": " Table 9: Vowelization Feature Selection II", "labels": [], "entities": []}, {"text": " Table 10: Vowelization Top N Results", "labels": [], "entities": [{"text": "Vowelization Top N", "start_pos": 11, "end_pos": 29, "type": "METRIC", "confidence": 0.6887103716532389}]}]}