{"title": [{"text": "Dependency-based Automatic Enumeration of Semantically Equivalent Word Orders for Evaluating Japanese Translations", "labels": [], "entities": [{"text": "Dependency-based Automatic Enumeration of Semantically Equivalent Word Orders", "start_pos": 0, "end_pos": 77, "type": "TASK", "confidence": 0.7540483586490154}, {"text": "Evaluating Japanese Translations", "start_pos": 82, "end_pos": 114, "type": "TASK", "confidence": 0.7610964775085449}]}], "abstractContent": [{"text": "Scrambling is acceptable reordering of verb arguments in languages such as Japanese and German.", "labels": [], "entities": []}, {"text": "In automatic evaluation of translation quality, BLEU is the de facto standard method, but BLEU has only very weak correlation with human judgements in case of Japanese-to-English/English-to-Japanese translations.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 48, "end_pos": 52, "type": "METRIC", "confidence": 0.9977777600288391}, {"text": "BLEU", "start_pos": 90, "end_pos": 94, "type": "METRIC", "confidence": 0.9977697134017944}]}, {"text": "Therefore, alternative methods, IMPACT and RIBES, were proposed and they have shown much stronger correlation than BLEU.", "labels": [], "entities": [{"text": "IMPACT", "start_pos": 32, "end_pos": 38, "type": "METRIC", "confidence": 0.7580540776252747}, {"text": "RIBES", "start_pos": 43, "end_pos": 48, "type": "METRIC", "confidence": 0.9780861735343933}, {"text": "BLEU", "start_pos": 115, "end_pos": 119, "type": "METRIC", "confidence": 0.9956384301185608}]}, {"text": "Now, RIBES is widely used in recent papers on Japanese-related translations.", "labels": [], "entities": [{"text": "RIBES", "start_pos": 5, "end_pos": 10, "type": "METRIC", "confidence": 0.8610032796859741}]}, {"text": "RIBES compares word order of MT output with manually translated reference sentences but it does not regard scrambling at all.", "labels": [], "entities": [{"text": "RIBES", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.6782812476158142}, {"text": "MT output", "start_pos": 29, "end_pos": 38, "type": "TASK", "confidence": 0.9074064493179321}]}, {"text": "In this paper, we present a method to enumerate scrambled sentences from dependency trees of reference sentences.", "labels": [], "entities": []}, {"text": "Our experiments based on NTCIR Patent MT data show that the method improves sentence-level correlation between RIBES and human-judged adequacy.", "labels": [], "entities": [{"text": "NTCIR Patent MT data", "start_pos": 25, "end_pos": 45, "type": "DATASET", "confidence": 0.9255173802375793}, {"text": "RIBES", "start_pos": 111, "end_pos": 116, "type": "METRIC", "confidence": 0.6139990091323853}]}], "introductionContent": [{"text": "Statistical Machine Translation has grown with an automatic evaluation method BLEU ().", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.7267974615097046}, {"text": "BLEU", "start_pos": 78, "end_pos": 82, "type": "METRIC", "confidence": 0.9977731108665466}]}, {"text": "BLEU measures local word order by ngrams and does not care about global word order.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9617083668708801}]}, {"text": "In JE/EJ translations, this insensitivity degrades BLEU's correlation with human judgements.", "labels": [], "entities": [{"text": "JE/EJ translations", "start_pos": 3, "end_pos": 21, "type": "TASK", "confidence": 0.6136590838432312}, {"text": "BLEU", "start_pos": 51, "end_pos": 55, "type": "METRIC", "confidence": 0.9972328543663025}]}, {"text": "Therefore, alternative automatic evaluation methods are proposed.", "labels": [], "entities": []}, {"text": "presented the idea of RIBES.", "labels": [], "entities": [{"text": "RIBES", "start_pos": 22, "end_pos": 27, "type": "DATASET", "confidence": 0.6243124604225159}]}, {"text": "named this method \"RIBES\" (Rank-based Intuitive Bilingual Evaluation Score).", "labels": [], "entities": [{"text": "RIBES", "start_pos": 19, "end_pos": 24, "type": "METRIC", "confidence": 0.9776487946510315}]}, {"text": "This version of RIBES was defined as follows: RIBES = NKT \u00d7 P \u03b1.", "labels": [], "entities": [{"text": "RIBES", "start_pos": 46, "end_pos": 51, "type": "METRIC", "confidence": 0.7689077854156494}]}, {"text": "According to this table, RIBES with \u03b1 = 0.2 has a very strong correlation (Spearman's \u03c1 = 0.947) with human-judged adequacy.", "labels": [], "entities": [{"text": "Spearman's \u03c1 = 0.947)", "start_pos": 75, "end_pos": 96, "type": "METRIC", "confidence": 0.8510106603304545}]}, {"text": "For each sentence, we use the average of adequacy scores of three judges.", "labels": [], "entities": []}, {"text": "Here, we call this average \"Adequacy\".", "labels": [], "entities": [{"text": "Adequacy", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.9588515758514404}]}, {"text": "We focus on Adequacy because current SMT systems tend to output inadequate sentences.", "labels": [], "entities": [{"text": "Adequacy", "start_pos": 12, "end_pos": 20, "type": "TASK", "confidence": 0.9381669163703918}, {"text": "SMT", "start_pos": 37, "end_pos": 40, "type": "TASK", "confidence": 0.9862807989120483}]}, {"text": "Note that only single reference translations are available for this task although use of multiple references is common for BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 123, "end_pos": 127, "type": "METRIC", "confidence": 0.8817852139472961}]}, {"text": "RIBES is publicly available from http:// www.kecl.ntt.co.jp/icl/lirg/ribes/ and was used as a standard quality measure in recent NTCIR PatentMT tasks.", "labels": [], "entities": [{"text": "RIBES", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.7529616951942444}, {"text": "NTCIR PatentMT tasks", "start_pos": 129, "end_pos": 149, "type": "TASK", "confidence": 0.480136513710022}]}, {"text": "shows the result of metaevaluation at NICTR-9/10 PatentMT.", "labels": [], "entities": [{"text": "NICTR-9/10 PatentMT", "start_pos": 38, "end_pos": 57, "type": "DATASET", "confidence": 0.907793328166008}]}, {"text": "The table shows that RIBES is more reliable than BLEU and NIST.", "labels": [], "entities": [{"text": "RIBES", "start_pos": 21, "end_pos": 26, "type": "METRIC", "confidence": 0.9286407828330994}, {"text": "BLEU", "start_pos": 49, "end_pos": 53, "type": "METRIC", "confidence": 0.9904373288154602}, {"text": "NIST", "start_pos": 58, "end_pos": 62, "type": "DATASET", "confidence": 0.9589352011680603}]}, {"text": "Current RIBES has the following improvements.", "labels": [], "entities": [{"text": "RIBES", "start_pos": 8, "end_pos": 13, "type": "DATASET", "confidence": 0.8158382773399353}]}, {"text": "\u2022 BLEU's Brevity Penalty (BP) was introduced where \u03b1 = 0.25 and \u03b2 = 0.10.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 2, "end_pos": 6, "type": "METRIC", "confidence": 0.9854879975318909}, {"text": "Brevity Penalty (BP)", "start_pos": 9, "end_pos": 29, "type": "METRIC", "confidence": 0.9769111633300781}]}, {"text": "BLEU uses BP for the entire test corpus, but RIBES uses it for each sentence.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.8528025150299072}, {"text": "BP", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9930065870285034}, {"text": "RIBES", "start_pos": 45, "end_pos": 50, "type": "DATASET", "confidence": 0.900947630405426}]}, {"text": "\u2022 The word alignment algorithm in the original RIBES used only bigrams for disambiguation when the same word appears twice or more in one sentence.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 6, "end_pos": 20, "type": "TASK", "confidence": 0.7587464451789856}, {"text": "RIBES", "start_pos": 47, "end_pos": 52, "type": "DATASET", "confidence": 0.9082170724868774}]}, {"text": "This restriction is now removed, and longer n-grams are used to get a better alignment.", "labels": [], "entities": []}, {"text": "RIBES is widely used in recent Annual Meeings of the (Japanese) Association for NLP.", "labels": [], "entities": [{"text": "RIBES", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.7879943251609802}]}, {"text": "International conference papers on Japanese-related translations also use RIBES.", "labels": [], "entities": [{"text": "RIBES", "start_pos": 74, "end_pos": 79, "type": "DATASET", "confidence": 0.6926372051239014}]}, {"text": "(. uses RIBES for Chinese-to-Japanese translation.", "labels": [], "entities": [{"text": "RIBES", "start_pos": 8, "end_pos": 13, "type": "METRIC", "confidence": 0.5128958225250244}]}, {"text": "However, we have to take \"scrambling\" into account when we think of Japanese word order.", "labels": [], "entities": []}, {"text": "Scrambling is also observed in other languages such as German.", "labels": [], "entities": [{"text": "Scrambling", "start_pos": 0, "end_pos": 10, "type": "TASK", "confidence": 0.878812313079834}]}, {"text": "Current RIBES does not regard this fact.", "labels": [], "entities": [{"text": "RIBES", "start_pos": 8, "end_pos": 13, "type": "DATASET", "confidence": 0.7723596096038818}]}], "datasetContent": [{"text": "As we mentioned before, RIBES measures global word order similarity between machine-translated sentences and reference sentences.", "labels": [], "entities": [{"text": "RIBES", "start_pos": 24, "end_pos": 29, "type": "METRIC", "confidence": 0.5627080798149109}]}, {"text": "It does not regard scrambling at all.", "labels": [], "entities": []}, {"text": "When the target language allows scrambling just like Japanese, RIBES should consider scrambling.", "labels": [], "entities": [{"text": "RIBES", "start_pos": 63, "end_pos": 68, "type": "TASK", "confidence": 0.4749920070171356}]}, {"text": "Once we have a correct dependency tree of the reference sentence, we enumerate scrambled sentences by reordering children of each node.", "labels": [], "entities": []}, {"text": "The number of the reordered sentences depend on the structure of the dependency tree.", "labels": [], "entities": []}, {"text": "Current RIBES code (RIBES-1.02.4) assumes that every sentence has a fixed number of references, but here the number of automatically generated reference sentences depends on the dependency structure of the original reference sentence.", "labels": [], "entities": [{"text": "RIBES code (RIBES-1.02.4", "start_pos": 8, "end_pos": 32, "type": "DATASET", "confidence": 0.6934851184487343}]}, {"text": "Therefore, we modified the code for variable numbers of reference sentences.", "labels": [], "entities": []}, {"text": "RIBES-1.02.4 simply uses the maximum value of the scores for different reference sentences, and we followed it.", "labels": [], "entities": [{"text": "RIBES-1.02.4", "start_pos": 0, "end_pos": 12, "type": "DATASET", "confidence": 0.9509392380714417}]}, {"text": "Here, we compare the following four methods.", "labels": [], "entities": []}, {"text": "\u2022 single: We use only single reference translations provided by the NTCIR organizers.", "labels": [], "entities": [{"text": "NTCIR organizers", "start_pos": 68, "end_pos": 84, "type": "DATASET", "confidence": 0.9679434299468994}]}, {"text": "\u2022 postOrder: We generate all permutations of the given reference sentence generated by post-order traversals of its dependency tree.", "labels": [], "entities": []}, {"text": "This can be achieved by the following two steps.", "labels": [], "entities": []}, {"text": "First, we enumerate all permutations of child nodes at each node.", "labels": [], "entities": []}, {"text": "Then, we combine these permutations.", "labels": [], "entities": []}, {"text": "This is implemented by cartesian products of the permutation sets.", "labels": [], "entities": []}, {"text": "\u2022 caseMarkers: We reorder only \"case marker (kaku joshi) phrases\".", "labels": [], "entities": []}, {"text": "Here, a \"case marker phrase\" is post-order traversal of a subtree rooted at a case marker bunsetsu.", "labels": [], "entities": []}, {"text": "For instance, the root of the following sentence S3 has a non-case marker child \"kaburi ,\" (wear) between case marker children, \"jon ga\" and \"zubon wo\" (Trousers are the object).", "labels": [], "entities": []}, {"text": "jon ga shiroi boushi wo kaburi , kuroi zubon wo hai te iru.", "labels": [], "entities": []}, {"text": "(John wears a white hat and wears black trousers.)", "labels": [], "entities": []}, {"text": "This is implemented by removing non-case marker nodes from the set of child nodes to be reordered in the above \"postOrder\" method.", "labels": [], "entities": []}, {"text": "For simplicity, we do not reorder other markers such as the topic marker \"wa\" here.", "labels": [], "entities": []}, {"text": "\u2022 proposed: We reorder only contiguous case marker children of anode, and we accept sentences that satisfy the aforementioned Simple Case Marker Constraint.", "labels": [], "entities": []}, {"text": "S3's root node has two case marker children, but they are not contiguous.", "labels": [], "entities": []}, {"text": "Therefore, we do not reorder them.", "labels": [], "entities": []}, {"text": "We expect that the constraint inhibit generation of incomprehensible or misleading sentences.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Meta-evaluation of NTCIR-7 JE task data", "labels": [], "entities": [{"text": "NTCIR-7 JE task data", "start_pos": 29, "end_pos": 49, "type": "DATASET", "confidence": 0.6153539195656776}]}, {"text": " Table 2: Meta-evaluation at NTCIR-9/10  PatentMT (Spearman's \u03c1, Goto et al. 2011, 2013)  BLEU  NIST RIBES  NTCIR-9 JE \u22120.042 \u22120.114  0.632  NTCIR-9 EJ \u22120.029 \u22120.074  0.716  NTCIR-10 JE  0.31  0.36  0.88  NTCIR-10 EJ  0.36  0.22  0.79", "labels": [], "entities": [{"text": "NTCIR-9/10  PatentMT", "start_pos": 29, "end_pos": 49, "type": "DATASET", "confidence": 0.7276936620473862}, {"text": "BLEU  NIST RIBES  NTCIR-9 JE \u22120.042 \u22120.114  0.632  NTCIR-9 EJ \u22120.029 \u22120.074  0.716  NTCIR-10 JE  0.31  0.36  0.88  NTCIR-10 EJ  0.36  0.22  0.79", "start_pos": 90, "end_pos": 234, "type": "METRIC", "confidence": 0.7536563840177324}]}, {"text": " Table 4: Improvement of sentence-level correla- tion between Adequacy and RIBES for human- judged NTCIR-7 EJ systems (MAIN RESULT)  Pearson's r  Spearman's \u03c1  single \u2192 proposed single \u2192 proposed  tsbmt 0.466 \u2192 0.472  0.439 \u2192 0.452  Moses 0.607 \u2192 0.663  0.607 \u2192 0.670  NTT 0.709 \u2192 0.735  0.692 \u2192 0.727", "labels": [], "entities": []}, {"text": " Table 5: Increase of averaged RIBES scores  Adeq.  RIBES  system  single proposed caseMarkers postOrder  tsbmt 3.527 0.715 0.7188  0.719 0.7569  moses 2.897 0.706 0.7192  0.722 0.781  NTT 2.740 0.671 0.683  0.686 0.7565", "labels": [], "entities": []}]}