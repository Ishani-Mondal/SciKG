{"title": [{"text": "From Visualisation to Hypothesis Construction for Second Language Acquisition", "labels": [], "entities": [{"text": "Second Language Acquisition", "start_pos": 50, "end_pos": 77, "type": "TASK", "confidence": 0.6450532376766205}]}], "abstractContent": [{"text": "One research goal in Second Language Acquisition (SLA) is to formulate and test hypotheses about errors and the environments in which they are made, a process which often involves substantial effort; large amounts of data and computational visualisation techniques promise help here.", "labels": [], "entities": [{"text": "Second Language Acquisition (SLA)", "start_pos": 21, "end_pos": 54, "type": "TASK", "confidence": 0.8250575959682465}]}, {"text": "In this paper we have defined anew task for finding contexts for errors that vary with the native language of the speaker that are potentially useful for SLA research.", "labels": [], "entities": [{"text": "SLA research", "start_pos": 154, "end_pos": 166, "type": "TASK", "confidence": 0.9524996280670166}]}, {"text": "We propose four models for approaching this task, and find that one based only on error-feature co-occurrence and another based on determining maximum weight cliques in a feature association graph discover strongly distinguishing contexts, with an apparent trade-off between false positives and very specific contexts.", "labels": [], "entities": []}], "introductionContent": [{"text": "SLA researchers are interested in a wide variety of aspects of humans learning anew language (L2) different from their native one (L1): cognitive issues and developmental sequences for learners, sociocultural factors, and soon.", "labels": [], "entities": []}, {"text": "One longstanding question, dating back to at least, is expressed by in the following way: \"What is the role played by first language in L2 development, vis-` a-vis the role of other universal development forces?\"", "labels": [], "entities": []}, {"text": "An example of SLA research that looks at this question is the study of, comparing Chinese and Spanish learners of English with respect to the English article system (a, an, the) using corpora of essays by native and non-native speakers of English.", "labels": [], "entities": [{"text": "SLA", "start_pos": 14, "end_pos": 17, "type": "TASK", "confidence": 0.947196900844574}]}, {"text": "Drawing on the 175 nonnative texts, they take a particular theoretical analysis (the so-called Bickerton semantic wheel), use the simple Wordsmith tools designed to extract data for lexicographers to identify errors in a semi-automatic way, and evaluate whether Chinese and Spanish L1 speakers do behave differently via hypothesis testing (ANOVA, chisquare and z-tests, in their case).", "labels": [], "entities": [{"text": "ANOVA", "start_pos": 340, "end_pos": 345, "type": "METRIC", "confidence": 0.946697473526001}]}, {"text": "They conclude that Chinese and Spanish do have characteristic differences, with patterns of zero article and definite article use differing according to semantic context.", "labels": [], "entities": []}, {"text": "Such studies are typically carried out on relatively small datasets, and use fairly elementary tools.", "labels": [], "entities": []}, {"text": "Sources such as and give good overviews of such studies and of SLA research in general.", "labels": [], "entities": [{"text": "SLA", "start_pos": 63, "end_pos": 66, "type": "TASK", "confidence": 0.9828541278839111}]}, {"text": "A goal of this paper is to investigate a particular way in which Natural Language Processing (NLP) can usefully contribute to SLA.", "labels": [], "entities": [{"text": "SLA", "start_pos": 126, "end_pos": 129, "type": "TASK", "confidence": 0.9916316866874695}]}, {"text": "In terms of existing work, the subfield of Native Language Identification (NLI) has been quite active recently, which looks at predicting the L1 of writers writing in a common L2 within a classification task framework; see for example the recent NLI shared task with 29 entrants.", "labels": [], "entities": [{"text": "Native Language Identification (NLI)", "start_pos": 43, "end_pos": 79, "type": "TASK", "confidence": 0.8275049726168314}]}, {"text": "From within linguistics, there has been much interest in how data-driven approaches can contribute to SLA.", "labels": [], "entities": [{"text": "SLA", "start_pos": 102, "end_pos": 105, "type": "TASK", "confidence": 0.9876848459243774}]}, {"text": "Granger (2011) discusses a body of work based on the the methodology of carrying out corpus-based approaches to SLA with a focus on NLP tools; in an edited collection present recent work by linguists who extend the corpus-based setup by using a text classification approach, looking at what feature selection might say for SLA.", "labels": [], "entities": [{"text": "SLA", "start_pos": 112, "end_pos": 115, "type": "TASK", "confidence": 0.9620614051818848}, {"text": "text classification", "start_pos": 245, "end_pos": 264, "type": "TASK", "confidence": 0.7057424634695053}]}, {"text": "From within NLP, and take a data-driven approach to SLA investigations much in the spirit of this work.", "labels": [], "entities": [{"text": "SLA investigations", "start_pos": 52, "end_pos": 70, "type": "TASK", "confidence": 0.9716047644615173}]}, {"text": "One particular approach to finding aspects of texts characteristic of their L1s that has motivated the present work is described in, the goal of which is to develop visualisation tools for SLA researchers.", "labels": [], "entities": [{"text": "SLA researchers", "start_pos": 189, "end_pos": 204, "type": "TASK", "confidence": 0.9173482060432434}]}, {"text": "They present graphs of the relationships between errors and their contexts, such that SLA researchers can navigate through the graphs to find contexts for particular errors that can lead to hypotheses like that of above.", "labels": [], "entities": []}, {"text": "In this paper, we look at approaches to finding such hypothesis candidates automatically in the context of L1-L2 interaction by analysing the graphs used in the visualisations of.", "labels": [], "entities": []}, {"text": "Specifically, we do the following: \u2022 We propose anew task that is more directly oriented to SLA research than NLI has been for the most part, with the goal of identifying error-related contexts that are characteristic of L1s.", "labels": [], "entities": [{"text": "SLA research", "start_pos": 92, "end_pos": 104, "type": "TASK", "confidence": 0.9218501448631287}]}, {"text": "\u2022 We evaluate a number of models for finding such contexts, ranging from a simple baseline to treating the problem as a graph-theoretic maximum weighted clique one.", "labels": [], "entities": []}, {"text": "\u2022 We examine the results of some of the models to see how the task and the models might contribute to SLA research.", "labels": [], "entities": [{"text": "SLA research", "start_pos": 102, "end_pos": 114, "type": "TASK", "confidence": 0.9613543450832367}]}, {"text": "Because we draw heavily on the work of Yannakoudakis et al., we first review relevant aspects of that work in \u00a72; we then present our task definition and experimental setup in \u00a73; we give results along with a discussion in \u00a74; we follow with some more detail on related work in \u00a75; and we conclude in \u00a76.", "labels": [], "entities": []}], "datasetContent": [{"text": "At a general level, our goal is to find which kinds of constructions (in a loose sense) centred around errors are particularly characteristic of various L1s.", "labels": [], "entities": []}, {"text": "The specific task we define for this paper, then, is to select a set of features (in the terminology of)-which we refer to as the ERROR CONTEXT-that, when combined with the error, show a strong association with L1, in a manner we describe below.", "labels": [], "entities": [{"text": "ERROR CONTEXT-that", "start_pos": 130, "end_pos": 148, "type": "METRIC", "confidence": 0.779862254858017}]}, {"text": "So, for example, this may involve finding that an MD error in the context of RG_JJ_NN1, JJ_NN1_II and VBZ_RG shows a strong association with L1.", "labels": [], "entities": [{"text": "VBZ_RG", "start_pos": 102, "end_pos": 108, "type": "DATASET", "confidence": 0.7248693903287252}]}, {"text": "We investigate a number of models for this selection process: the task then is the identification of which models produce poor error contexts (which will not rank highly in hypothesis testing) and which produce good ones (potentially worth considering by an SLA researcher).", "labels": [], "entities": []}, {"text": "Below we discuss the data we use, the measure of association for an error and its context, the set of errors chosen, and the models for selecting context.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: FCESUB, broken down by language", "labels": [], "entities": [{"text": "FCESUB", "start_pos": 10, "end_pos": 16, "type": "DATASET", "confidence": 0.8280242085456848}]}, {"text": " Table 2: ANOVA results giving mean score (number  of sentences with MD error per 10 sentences) for each  language, the ANOVA F-statistic, and significance value", "labels": [], "entities": [{"text": "ANOVA", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.7719581127166748}, {"text": "mean score", "start_pos": 31, "end_pos": 41, "type": "METRIC", "confidence": 0.9754809737205505}, {"text": "MD error", "start_pos": 69, "end_pos": 77, "type": "METRIC", "confidence": 0.845552384853363}, {"text": "ANOVA F-statistic", "start_pos": 120, "end_pos": 137, "type": "METRIC", "confidence": 0.6256878077983856}, {"text": "significance", "start_pos": 143, "end_pos": 155, "type": "METRIC", "confidence": 0.9704439640045166}]}, {"text": " Table 3: Error types chosen for evaluation, including F- statistic, ANOVA p-value and corpus count of sentences  containing error.", "labels": [], "entities": [{"text": "F- statistic", "start_pos": 55, "end_pos": 67, "type": "METRIC", "confidence": 0.9617690046628317}, {"text": "ANOVA", "start_pos": 69, "end_pos": 74, "type": "METRIC", "confidence": 0.9862048029899597}]}, {"text": " Table 4: Average correlation coefficient r between F- statistic and \u03c7 2 -statistic for each model", "labels": [], "entities": [{"text": "Average correlation coefficient r", "start_pos": 10, "end_pos": 43, "type": "METRIC", "confidence": 0.9007387310266495}, {"text": "F", "start_pos": 52, "end_pos": 53, "type": "METRIC", "confidence": 0.9847339987754822}]}]}