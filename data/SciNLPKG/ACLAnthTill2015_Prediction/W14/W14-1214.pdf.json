{"title": [{"text": "An Analysis of Crowdsourced Text Simplifications", "labels": [], "entities": [{"text": "Crowdsourced Text Simplifications", "start_pos": 15, "end_pos": 48, "type": "TASK", "confidence": 0.5870326260725657}]}], "abstractContent": [{"text": "We present a study on the text simplification operations undertaken collaboratively by Simple English Wikipedia contributors.", "labels": [], "entities": [{"text": "text simplification", "start_pos": 26, "end_pos": 45, "type": "TASK", "confidence": 0.7944606840610504}]}, {"text": "The aim is to understand whether a complex-simple parallel corpus involving this version of Wikipedia is appropriate as data source to induce simplification rules, and whether we can automatically categorise the different operations performed by humans.", "labels": [], "entities": []}, {"text": "A subset of the corpus was first manually analysed to identify its transformation operations.", "labels": [], "entities": []}, {"text": "We then built machine learning models to attempt to automatically classify segments based on such transformations.", "labels": [], "entities": []}, {"text": "This classification could be used, e.g., to filter out potentially noisy transformations.", "labels": [], "entities": []}, {"text": "Our results show that the most common transformation operations performed by humans are paraphrasing (39.80%) and drop of information (26.76%), which are some of the most difficult operations to generalise from data.", "labels": [], "entities": []}, {"text": "They are also the most difficult operations to identify automatically, with the lowest overall classifier accuracy among all operations (73% and 59%, respectively).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 106, "end_pos": 114, "type": "METRIC", "confidence": 0.9659442901611328}]}], "introductionContent": [{"text": "Understanding written texts in a variety of forms (newspapers, educational books, etc.) can be a challenge for certain groups of readers).", "labels": [], "entities": []}, {"text": "Among these readers we can cite second language learners, language-impaired people (e.g. aphasic and dyslexic), and the elderly.", "labels": [], "entities": []}, {"text": "Sentences with multiple clauses, unusual word order and rare vocabulary are some of the linguistic phenomena that should be avoided in texts written for these audiences.", "labels": [], "entities": [{"text": "Sentences with multiple clauses", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.8077983111143112}]}, {"text": "Although initiatives like the Plain English have long advocated for the use of clear and concise language, these have only been adopted in limited cases (UK government bodies, for example).", "labels": [], "entities": [{"text": "Plain English", "start_pos": 30, "end_pos": 43, "type": "DATASET", "confidence": 0.8544633090496063}]}, {"text": "The vast majority of texts which are aimed at the broad population, such as news, are often too complex to be processed by a large proportion of the population.", "labels": [], "entities": []}, {"text": "Adapting texts into their simpler variants is an expensive task.", "labels": [], "entities": [{"text": "Adapting texts", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.9210409224033356}]}, {"text": "Work on automating this process only started in recent years.", "labels": [], "entities": []}, {"text": "However, already in the 1920's Lively and Pressey created a method to distinguish simple from complex texts based on readability measures.", "labels": [], "entities": []}, {"text": "Using such measures, publishers were able to grade texts according to reading levels ( so that readers could focus on texts that were appropriate to them.", "labels": [], "entities": []}, {"text": "The first attempt to automate the process of simplification of texts was devised by.", "labels": [], "entities": []}, {"text": "This pioneer work has shown that it was possible to simplify texts automatically through hand-crafted linguistic rules.", "labels": [], "entities": []}, {"text": "In further work, developed a method to extract these rules from data.", "labels": [], "entities": []}, {"text": "defines Text Simplification as any method or process that simplifies text while maintaining its information.", "labels": [], "entities": [{"text": "Text Simplification", "start_pos": 8, "end_pos": 27, "type": "TASK", "confidence": 0.7563552558422089}]}, {"text": "Instead of handcrafted rules, recent methodologies are mostly data-driven, i.e., based on the induction of simplification rules from parallel corpora of complex segments and their corresponding simpler variants. and model the task using the Statistical Machine Translation framework, where simplified sentences are considered the \"target language\".", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 241, "end_pos": 272, "type": "TASK", "confidence": 0.7315548062324524}]}, {"text": "construct a simplification model based on edits in the Simple English Wikipedia.", "labels": [], "entities": [{"text": "Simple English Wikipedia", "start_pos": 55, "end_pos": 79, "type": "DATASET", "confidence": 0.8006331920623779}]}, {"text": "Woodsend and Lapata (2011) adopt a quasi-synchronous grammar with optimisation via integer linear programming.", "labels": [], "entities": []}, {"text": "This research focuses the corpus used by most of previous data-driven Text Simplification work: the parallel corpus of the main and simple English Wikipedia.", "labels": [], "entities": [{"text": "Text Simplification", "start_pos": 70, "end_pos": 89, "type": "TASK", "confidence": 0.7227378785610199}]}, {"text": "Following the collaborative nature of Wikipedia, a subset of the Main English Wikipedia (MainEW) has been edited by volunteers to make the texts more readable to a broader audience.", "labels": [], "entities": [{"text": "Main English Wikipedia (MainEW)", "start_pos": 65, "end_pos": 96, "type": "DATASET", "confidence": 0.8962811628977457}]}, {"text": "This resulted in the Simple English Wikipedia (SimpleEW) 1 , which we consider a crowdsourced text simplification corpus.", "labels": [], "entities": [{"text": "Simple English Wikipedia (SimpleEW) 1", "start_pos": 21, "end_pos": 58, "type": "DATASET", "confidence": 0.8072501165526254}]}, {"text": "paired articles from these two versions and automatically extracted parallel paragraphs and sentences from them (ParallelSEW).", "labels": [], "entities": []}, {"text": "The first task was accomplished in a straightforward way, given that corresponding articles have the same title as unique identification.", "labels": [], "entities": []}, {"text": "The paragraph alignment was performed selecting paragraphs when their normalised TF-IDF weighted cosine distance reached a minimum threshold.", "labels": [], "entities": [{"text": "paragraph alignment", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.8254304230213165}]}, {"text": "Sentence alignment was performed using monolingual alignment techniques () based on a dynamic programming algorithm.", "labels": [], "entities": [{"text": "Sentence alignment", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.946789026260376}]}, {"text": "In total, 137, 000 sentences were found to be parallel.", "labels": [], "entities": []}, {"text": "The resulting parallel corpora contains transformation operations of various types, including rewording, reordering, insertion and deletion.", "labels": [], "entities": []}, {"text": "In our experiments we analyse the distribution of these operations and perform some further analysis on their nature.", "labels": [], "entities": []}, {"text": "Most studies on data-driven Text Simplification have focused on the learning of the operations, with no or little qualitative analysis of the Text Simplification corpora used (.", "labels": [], "entities": [{"text": "Text Simplification", "start_pos": 28, "end_pos": 47, "type": "TASK", "confidence": 0.7248178571462631}]}, {"text": "As in any other area, the quality of machine learning models for Text Simplification will depend on the size and quality of the training dataset.", "labels": [], "entities": [{"text": "Text Simplification", "start_pos": 65, "end_pos": 84, "type": "TASK", "confidence": 0.8437454998493195}]}, {"text": "Our study takes a step back to carefully look at the most common simplification corpus and: (i) understand the most common transformation operations performed by humans and judge whether this corpus is adequate to induce simplification rules from, and (ii) automatically categorise transformation operations such as to further process and \"clean\" the corpus, for example to allow the modelling of specific simplification phenomena or groups of phenomena individually.", "labels": [], "entities": []}, {"text": "After reviewing some of the relevant related work (Section 2), in Section 3, we present the manual analysis of a subset of the ParallelSEW corpus.", "labels": [], "entities": [{"text": "ParallelSEW corpus", "start_pos": 127, "end_pos": 145, "type": "DATASET", "confidence": 0.7615907192230225}]}], "datasetContent": [{"text": "Our ultimate goal of this experiment is to select parts of the ParallelSWE corpus that are more adequate for the learning of certain simplification rules.", "labels": [], "entities": [{"text": "ParallelSWE corpus", "start_pos": 63, "end_pos": 81, "type": "DATASET", "confidence": 0.6915477961301804}]}, {"text": "While it may seem that simplification operations comprise a small set which is already known based on previous work, we would like to focus on the learning of fine-grained, lexicalized rules.", "labels": [], "entities": []}, {"text": "In other words, we are interested in the learning of more specific rules based on lexical items in addition to more general information such as POS tags and syntactic structures.", "labels": [], "entities": []}, {"text": "The learning of such rules could benefit from a high quality corpus that is not only noise-free, but also for which one already has some information about the general operation(s) covered.", "labels": [], "entities": []}, {"text": "In an ideal scenario, one could for example use a subset of the corpus that contains only sentence splitting operations to learn very specific and accurate rules to perform different types of sentence splitting in unseen data.", "labels": [], "entities": [{"text": "sentence splitting", "start_pos": 90, "end_pos": 108, "type": "TASK", "confidence": 0.7582881450653076}, {"text": "sentence splitting", "start_pos": 192, "end_pos": 210, "type": "TASK", "confidence": 0.7793481945991516}]}, {"text": "Selecting a subset of the corpus that contain only one transformation operation per segment is also appealing as it would facilitate the learning.", "labels": [], "entities": []}, {"text": "The process of manually annotating the corpus with the corresponding transformation operations is however a laborious task.", "labels": [], "entities": []}, {"text": "For this reason, we have trained classifiers on the labelled data described in the previous section with two purposes: \u2022 Decide over the six main transformation operations presented in the previous section; and \u2022 Decide whether a sentence was simplified by one operation only, or by more than one operation.", "labels": [], "entities": []}, {"text": "The features used in both experiments are described in Section 4.1 and the algorithms and results are presented in Section 4.2.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Counts of transformation operations in  the Parallel143 corpus", "labels": [], "entities": []}, {"text": " Table 2: Mode of transformation operations in the  Parallel143 corpus", "labels": [], "entities": []}, {"text": " Table 3: Main transformation operations found in  the Parallel143 corpus", "labels": [], "entities": []}, {"text": " Table 4. These are compared  to the accuracy of the majority class baseline (i.e.,  the class with the highest frequency in the train- ing set).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.999498724937439}]}, {"text": " Table 4: Baselines and classifiers accuracy of the  transformation operations", "labels": [], "entities": []}]}