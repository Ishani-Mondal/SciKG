{"title": [{"text": "Linear Mixture Models for Robust Machine Translation", "labels": [], "entities": []}], "abstractContent": [{"text": "As larger and more diverse parallel texts become available, how can we leverage heterogeneous data to train robust machine translation systems that achieve good translation quality on various test domains?", "labels": [], "entities": [{"text": "machine translation", "start_pos": 115, "end_pos": 134, "type": "TASK", "confidence": 0.7477545738220215}]}, {"text": "This challenge has been addressed so far by repurposing techniques developed for domain adaptation, such as linear mixture models which combine estimates learned on homogeneous sub-domains.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 81, "end_pos": 98, "type": "TASK", "confidence": 0.7240549623966217}]}, {"text": "However, learning from large heterogeneous corpora is quite different from standard adaptation tasks with clear domain distinctions.", "labels": [], "entities": []}, {"text": "In this paper, we show that linear mixture models can reliably improve translation quality in very heterogeneous training conditions, even if the mixtures do not use any domain knowledge and attempt to learn generic models rather than adapt them to the target domain.", "labels": [], "entities": []}, {"text": "This surprising finding opens new perspectives for using mixture models in machine translation beyond clear cut domain adaptation tasks.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 75, "end_pos": 94, "type": "TASK", "confidence": 0.7667041122913361}]}], "introductionContent": [{"text": "While machine translation tasks used to be defined by drawing training and test data from a single well-defined domain, current systems have to deal with increasingly heterogeneous data, both at training and attest time.", "labels": [], "entities": [{"text": "machine translation tasks", "start_pos": 6, "end_pos": 31, "type": "TASK", "confidence": 0.8541486461957296}]}, {"text": "As larger and more diverse parallel texts become available, how can we leverage heterogeneous data to train statistical machine translation (SMT) systems that achieve good translation quality on various test domains?", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 108, "end_pos": 145, "type": "TASK", "confidence": 0.779900019367536}]}, {"text": "So far, this challenge has been addressed by repurposing techniques developed for more clear-cut domain adaptation scenarios, such as linear mixture models).", "labels": [], "entities": []}, {"text": "Instead of estimating models on the whole training corpus at once, linear mixture models are built as follows: (1) partition the training corpus into homogeneous domain-based component, (2) train one model per component, (3) linearly mix models using weights learned to adapt to the test domain, (4) replace resulting model in translation system.", "labels": [], "entities": []}, {"text": "In this paper, we aim to gain a better understanding of the benefits of linear mixture models in heterogeneous data conditions, by examining key untested assumptions: \u2022 Should mixture component capture domain information?", "labels": [], "entities": []}, {"text": "Previous work assumes that training data should be organized into domains.", "labels": [], "entities": []}, {"text": "When manual domain distinctions are not available, previous work uses clustering approaches to approximate manual domain distinctions.", "labels": [], "entities": []}, {"text": "However, it is unclear whether it is necessary to use or mimic domain distinctions in order to define mixture components.", "labels": [], "entities": []}, {"text": "\u2022 Mixture models are usually assumed to improve translation quality by giving more weight to parts of the training corpus that are more relevant to the test domain.", "labels": [], "entities": [{"text": "Mixture", "start_pos": 2, "end_pos": 9, "type": "TASK", "confidence": 0.9648357629776001}, {"text": "translation", "start_pos": 48, "end_pos": 59, "type": "TASK", "confidence": 0.9691945910453796}]}, {"text": "Is this intuition still valid in our more complex heterogeneous training conditions?", "labels": [], "entities": []}, {"text": "If not, how do mixture models affect translation probability estimates?", "labels": [], "entities": []}, {"text": "In order to answer these questions, we propose to study several variants of linear mixture models that reflect different modeling assumptions and different levels of domain knowledge.", "labels": [], "entities": []}, {"text": "We first consider two methods for setting mixture weights: adaptation to the test domain via maximum likelihood, and uniform mixtures that make no assumption about the domain of interest (Section 2).", "labels": [], "entities": []}, {"text": "Then, we will describe a wide range of techniques that can be used to define mixture components (Section 3).", "labels": [], "entities": []}, {"text": "Again, these techniques reflect opposite modeling assumptions: manually defined domains and automatic clusters attempt to organize heterogeneous training sets into homogeneous groups that represent distinct domains, while random samples capture no domain information and simply provide different views of the training set.", "labels": [], "entities": []}, {"text": "We present an empirical investigation of all the variations outlined above using a strong system trained on large and diverse training corpora, for two language pairs and two distinct test domains.", "labels": [], "entities": []}, {"text": "Our results show that linear mixtures reliably and robustly improve the quality of machine translation (Section 5).", "labels": [], "entities": [{"text": "machine translation", "start_pos": 83, "end_pos": 102, "type": "TASK", "confidence": 0.7632933557033539}]}, {"text": "While they were originally developed for domain adaptation tasks, linear mixtures that have no domain knowledge can perform as well as traditional mixtures meant to perform domain adaptation.", "labels": [], "entities": [{"text": "domain adaptation tasks", "start_pos": 41, "end_pos": 64, "type": "TASK", "confidence": 0.8136678536732992}, {"text": "domain adaptation", "start_pos": 173, "end_pos": 190, "type": "TASK", "confidence": 0.7135374695062637}]}, {"text": "This suggests that improvements do not stem from domain modeling per se, but from better generic estimates from the heterogeneous training data.", "labels": [], "entities": []}, {"text": "Further analysis shows that the linear mixture estimates are very different from estimates obtained using more explicit smoothing schemes (Section 6).", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our linear mixture models on two different language pairs, Arabic-English and Chinese-English, and two different test domains.: Statistics for Chinese-English data: Number of segments (segs), source tokens (src) and English tokens (en) for each corpus.", "labels": [], "entities": []}, {"text": "For English dev and test sets, word counts averaged across 4 references.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics for Arabic-English data: Num- ber of segments (segs), source tokens (src) and En- glish tokens (en) for each corpus. For English dev  and test sets, word counts averaged across 2 refer- ences.", "labels": [], "entities": []}, {"text": " Table 3: Impact of mixture type on translation  quality as measured by BLEU.", "labels": [], "entities": [{"text": "translation", "start_pos": 36, "end_pos": 47, "type": "TASK", "confidence": 0.9696193933486938}, {"text": "BLEU", "start_pos": 72, "end_pos": 76, "type": "METRIC", "confidence": 0.9868139028549194}]}, {"text": " Table 4: Impact of mixture component definition  on BLEU score: there is no clear benefit to explic- itly modeling domains.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 53, "end_pos": 63, "type": "METRIC", "confidence": 0.9741775095462799}]}, {"text": " Table 5: Impact of linear mixture weights on trans- lation quality as measured by BLEU: using do- main knowledge when setting weights has an un- reliable impact.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 83, "end_pos": 87, "type": "METRIC", "confidence": 0.9959782361984253}]}]}