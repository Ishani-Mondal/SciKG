{"title": [{"text": "Japanese to English Machine Translation using Preordering and Compositional Distributed Semantics", "labels": [], "entities": [{"text": "Japanese to English Machine Translation", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.5357061386108398}]}], "abstractContent": [{"text": "The pipeline of modern statistical machine translation (SMT) systems consists of several stages, presenting interesting opportunities to tune it towards improved performance on distant language pairs like Japanese and English.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 23, "end_pos": 60, "type": "TASK", "confidence": 0.8003233869870504}]}, {"text": "We explore modifications to several parts of this pipeline.", "labels": [], "entities": []}, {"text": "We include a preordering method in the pre-processing stage, a neural network based model in the tuning stage and a recurrent neural network language model in the post-processing stage.", "labels": [], "entities": []}, {"text": "To our knowledge this is the first work tightly integrating a neural network based model into the tuning stage of a SMT system for the Japanese-English language pair.", "labels": [], "entities": [{"text": "SMT", "start_pos": 116, "end_pos": 119, "type": "TASK", "confidence": 0.9941980838775635}]}, {"text": "As a first step in this direction we provide several insights into how this integration should be approached and give rise to future work in this area.", "labels": [], "entities": []}], "introductionContent": [{"text": "Modern models for statistical machine translation constitute a pipeline of different components.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 18, "end_pos": 49, "type": "TASK", "confidence": 0.7449281016985575}]}, {"text": "This pipeline usually involves a preprocessing part, a language model, a translation model and a postprocessing part.", "labels": [], "entities": []}, {"text": "While this or a similar structure is basis of most systems and generally agreed upon, a lot of research has been focusing on modifying and extending the individual components.", "labels": [], "entities": []}, {"text": "Many parts rely on probabilities acquired through word frequencies in fixed contexts, discarding additional syntactical and semantical information.", "labels": [], "entities": []}, {"text": "Problems arising from these strong assumptions become particularly apparent when dealing with distant language pairs like Japanese and English.", "labels": [], "entities": []}, {"text": "We focus exclusively on translating Japanese sentences to English and take several measures to inject additional information into the pipeline of our baseline system.", "labels": [], "entities": [{"text": "translating Japanese sentences to English", "start_pos": 24, "end_pos": 65, "type": "TASK", "confidence": 0.8677079439163208}]}, {"text": "\u2022 We apply preordering to the input text as away of compensating for syntactic differences between English and Japanese.", "labels": [], "entities": []}, {"text": "\u2022 We insert scores into the translation model of our baseline system that are computed from semantically meaningful distributed vector representations.", "labels": [], "entities": []}, {"text": "\u2022 As postprocessing, we utilize a recurrent neural network language model to re-score the 100 best translation candidates for each output sentence of our system.", "labels": [], "entities": []}, {"text": "Being able to handle variable length context, it complements the n-gram based language model used within the pipeline.", "labels": [], "entities": []}], "datasetContent": [{"text": "RIBES scores and submission to WAT 2014 human evaluation  We evaluated our results on the test split provided with the ASPEC corpus.", "labels": [], "entities": [{"text": "RIBES", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.5826985239982605}, {"text": "WAT 2014 human evaluation", "start_pos": 31, "end_pos": 56, "type": "DATASET", "confidence": 0.7317223846912384}, {"text": "ASPEC corpus", "start_pos": 119, "end_pos": 131, "type": "DATASET", "confidence": 0.9082828760147095}]}, {"text": "To avoid tokenization mismatches we applied the same tokenization method to the test set that we had previously used on the training set and computed BLEU and RIBES scores with the Travatar  matic evaluation website differ due to tokenization mismatches.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 150, "end_pos": 154, "type": "METRIC", "confidence": 0.9918017983436584}, {"text": "RIBES", "start_pos": 159, "end_pos": 164, "type": "METRIC", "confidence": 0.9733713865280151}, {"text": "Travatar  matic evaluation website", "start_pos": 181, "end_pos": 215, "type": "DATASET", "confidence": 0.5524245500564575}]}, {"text": "As listed in our baseline achieved a score of 19.16 BLEU and 63.41 RIBES.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 52, "end_pos": 56, "type": "METRIC", "confidence": 0.999187171459198}, {"text": "RIBES", "start_pos": 67, "end_pos": 72, "type": "METRIC", "confidence": 0.9990999698638916}]}, {"text": "Both of these scores can be considered low compared to results acquired e.g. on the Chinese to Japanese task of WAT 2014 where the phrase-based baseline model provided by the organizers achieved a BLEU score of 27.96.", "labels": [], "entities": [{"text": "WAT 2014", "start_pos": 112, "end_pos": 120, "type": "TASK", "confidence": 0.5093519687652588}, {"text": "BLEU score", "start_pos": 197, "end_pos": 207, "type": "METRIC", "confidence": 0.9816702008247375}]}, {"text": "The same baseline system only achieved 18.45 on the Japanese to English task which confirms that the Japanese to English translation task is highly difficult.", "labels": [], "entities": [{"text": "Japanese to English translation task", "start_pos": 101, "end_pos": 137, "type": "TASK", "confidence": 0.6730017781257629}]}, {"text": "Settings with preordering consistently perform worse than their counterparts without preordering, even though the employed preordering method has proven very successful () on the NTCIR patent corpus before.", "labels": [], "entities": [{"text": "NTCIR patent corpus", "start_pos": 179, "end_pos": 198, "type": "DATASET", "confidence": 0.9740713238716125}]}, {"text": "We suspect the main cause to be the different domain of the text.", "labels": [], "entities": []}, {"text": "The NTCIR workshop revolved around text from the patent and legal domain while WAT 2014 is based on the ASPEC corpus comprising abstracts from scientific papers.", "labels": [], "entities": [{"text": "NTCIR workshop", "start_pos": 4, "end_pos": 18, "type": "DATASET", "confidence": 0.918643981218338}, {"text": "WAT 2014", "start_pos": 79, "end_pos": 87, "type": "TASK", "confidence": 0.5974187254905701}, {"text": "ASPEC corpus", "start_pos": 104, "end_pos": 116, "type": "DATASET", "confidence": 0.723871573805809}]}, {"text": "Sentences in patent and legal text must be phrased in a very concise and unambiguous way and therefore feature a lot of recurring phrases.", "labels": [], "entities": []}, {"text": "Writers of scientific text have more freedom to express content in English and can draw from a larger set of ways to phrase their ideas.", "labels": [], "entities": []}, {"text": "This looser structure leads to mismatches when applying the heuristic preordering rules.", "labels": [], "entities": []}, {"text": "Both ways to integrate the scores computed with the neural network model into the Moses pipeline perform roughly equally well, falling just a little short of the baseline but failing to beat it.", "labels": [], "entities": []}, {"text": "(seefull) and Feature Function) Examining phrase pairs and their scores revealed that the scores make sense in general, even identifying transliterations correctly with a high confidence.", "labels": [], "entities": []}, {"text": "However, the model fails to capture an important property that prevents it from contributing information to Moses that is not already covered by the default features.", "labels": [], "entities": []}, {"text": "It almost entirely neglects syntactic structure.", "labels": [], "entities": []}, {"text": "Because of its objective to only distinguish valid translations from invalid ones, the model learns to ignore special characters, determiners and other words that do not possess a high discriminative value.", "labels": [], "entities": []}, {"text": "The translation hypotheses of our baseline system, however, include many cases where the recognition of syntax and special characters is crucial to assign sensible scores.", "labels": [], "entities": []}, {"text": "The values of word vectors of many stop words, determiners and special characters reveal that most of them are in close proximity to the zero vector.", "labels": [], "entities": []}, {"text": "This vector consisting only of zeros constitutes the neutral element of the mean function as well as the hyperbolic tangent.", "labels": [], "entities": []}, {"text": "Therefore, having or not having such a token in a sentence will only marginally change the composed representation.", "labels": [], "entities": []}, {"text": "This issue becomes particularly problematic for pairs of the form Japanese English \u30c8\u30fc\u30af\u30f3 token \u30c8\u30fc\u30af\u30f3 token ! \u30c8\u30fc\u30af\u30f3 the token the the All three of these examples will have an almost identical score since the vectors of \"!\" and \"the\" were learned to be very close to the zero vector.", "labels": [], "entities": []}, {"text": "In an information retrieval context where processed sentences are sensible and sane, this property does not pose a huge problem.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 6, "end_pos": 27, "type": "TASK", "confidence": 0.7659558057785034}]}, {"text": "However, in the case of a phrase table or hypotheses in the tuning stage of an SMT system, cases like the one illustrated above occur regularly and should exhibit not the same but very different scores.", "labels": [], "entities": [{"text": "SMT", "start_pos": 79, "end_pos": 82, "type": "TASK", "confidence": 0.9925873875617981}]}, {"text": "The Phrase  the neural model score can substitute the default Moses scores.", "labels": [], "entities": []}, {"text": "The BLEU and RIBES scores on the official evaluation board as reported in differ largely from the results presented in.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.9984791874885559}, {"text": "RIBES", "start_pos": 13, "end_pos": 18, "type": "METRIC", "confidence": 0.9917826652526855}]}, {"text": "This is due to tokenization mismatches.", "labels": [], "entities": []}, {"text": "Since our baseline system differs slightly from the system provided by the WAT 2014 organizers we submitted our baseline as a point of reference for our own experiments to the human evaluation.", "labels": [], "entities": [{"text": "WAT 2014 organizers", "start_pos": 75, "end_pos": 94, "type": "DATASET", "confidence": 0.7859498461087545}]}, {"text": "As our second submission to the human evaluation we chose our baseline with preordering.", "labels": [], "entities": []}, {"text": "Preordering has proven successful in previous NTCIR workshops but has failed to improve upon the baseline in WAT 2014.", "labels": [], "entities": [{"text": "Preordering", "start_pos": 0, "end_pos": 11, "type": "METRIC", "confidence": 0.9318152666091919}, {"text": "NTCIR", "start_pos": 46, "end_pos": 51, "type": "DATASET", "confidence": 0.6230881810188293}, {"text": "WAT 2014", "start_pos": 109, "end_pos": 117, "type": "TASK", "confidence": 0.6756344735622406}]}, {"text": "shows that our baseline performs slightly worse than the baseline of the organizers.", "labels": [], "entities": []}, {"text": "We are primarily interested in the difference between our baseline and our baseline with preordering.", "labels": [], "entities": []}, {"text": "The human evaluation confirms that preordering has a negative effect when applied to the AS-PEC corpus.", "labels": [], "entities": [{"text": "preordering", "start_pos": 35, "end_pos": 46, "type": "METRIC", "confidence": 0.9224284887313843}, {"text": "AS-PEC corpus", "start_pos": 89, "end_pos": 102, "type": "DATASET", "confidence": 0.7738753259181976}]}, {"text": "According to the WAT 2014 homepage, the scores of the human evaluation are calculated by repeatedly comparing sentences from the submissions to their corresponding outputs of the baseline system provided by the organizers.", "labels": [], "entities": [{"text": "WAT 2014 homepage", "start_pos": 17, "end_pos": 34, "type": "DATASET", "confidence": 0.9004849394162496}]}, {"text": "They therefore indicate how a submission performs in comparison to the organizer's baseline.", "labels": [], "entities": []}, {"text": "The fine grained results of the human evaluation in WAT 2014 offer a good opportunity to investigate the effects of our preordering method.", "labels": [], "entities": [{"text": "WAT 2014", "start_pos": 52, "end_pos": 60, "type": "TASK", "confidence": 0.6565829813480377}]}, {"text": "Examples illustrating the effect of the applied preordering method are shown in.", "labels": [], "entities": []}, {"text": "We only picked examples featuring clear annotator agreement.", "labels": [], "entities": []}, {"text": "-1 indicates that the organizers' baseline was assessed as superior to our system, 1 that it was evaluated to be inferior.", "labels": [], "entities": []}, {"text": "The biggest change in both of the examples shown in was induced by the same preordering rule (Rule 2).", "labels": [], "entities": []}, {"text": "Due to the lack of a topic marker in both source sentences Rule 2 caused the main verb to be brought to the front of the sentence.", "labels": [], "entities": []}, {"text": "Despite the similarity in the preprocessing step the quality of the translations differs heavily.", "labels": [], "entities": []}, {"text": "While Example 2 was evaluated as superior to the organizers' baseline, Example 1 was received as just the opposite.", "labels": [], "entities": []}, {"text": "In example 1, the preordered main verb (\"discussed\") was omitted entirely in the output of our system, changing the meaning from a discussion to a statement.", "labels": [], "entities": []}, {"text": "On the contrary, the output of our system for ex- Organizers' The titled new in-line flowmeter was developed.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Results for training the RNN LM with  different hyperparameters applying it to re-rank  the 100 best translation candidates for each out- put sentence of our SMT model. We varied the  number of hidden units (hidden), the number of di- rect connections from input to output layer (direct)  and kept the number of classes for the factored  softmax fixed at 220 (following the recommenda- tion in", "labels": [], "entities": [{"text": "SMT", "start_pos": 168, "end_pos": 171, "type": "TASK", "confidence": 0.985878586769104}]}, {"text": " Table 4: Scores from WAT 2014 evaluation board.", "labels": [], "entities": [{"text": "WAT 2014 evaluation board", "start_pos": 22, "end_pos": 47, "type": "DATASET", "confidence": 0.7811277806758881}]}]}