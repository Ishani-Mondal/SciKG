{"title": [{"text": "POS error detection in automatically annotated corpora", "labels": [], "entities": [{"text": "POS error detection", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7918210029602051}]}], "abstractContent": [{"text": "Recent work on error detection has shown that the quality of manually annotated corpora can be substantially improved by applying consistency checks to the data and automatically identifying incorrectly labelled instances.", "labels": [], "entities": [{"text": "error detection", "start_pos": 15, "end_pos": 30, "type": "TASK", "confidence": 0.699900209903717}]}, {"text": "These methods, however, cannot be used for automatically annotated corpora where errors are systematic and cannot easily be identified by looking at the variance in the data.", "labels": [], "entities": []}, {"text": "This paper targets the detection of POS errors in automatically annotated corpora, so-called silver standards, showing that by combining different measures sensitive to annotation quality we can identify a large part of the errors and obtain a substantial increase inaccuracy.", "labels": [], "entities": []}], "introductionContent": [{"text": "Today, linguistically annotated corpora are an indispensable resource for many areas of linguistic research.", "labels": [], "entities": []}, {"text": "However, since the emergence of the first digitised corpora in the 60s, the field has changed considerably.", "labels": [], "entities": []}, {"text": "What was considered \"very large\" in the last decades is now considered to be rather small.", "labels": [], "entities": []}, {"text": "Through the emergence of Web 2.0 and the spread of user-generated content, more and more data is accessible for building corpora for specific purposes.", "labels": [], "entities": []}, {"text": "This presents us with new challenges for automatic preprocessing and annotation.", "labels": [], "entities": []}, {"text": "While conventional corpora mostly include written text which complies to grammatical standards, the new generation of corpora contain texts from very different varieties, displaying features of spoken language, regional variety, ungrammatical content, typos and non-canonical spelling.", "labels": [], "entities": []}, {"text": "A large portion of the vocabulary are unknown words (that is, not included in the training data).", "labels": [], "entities": []}, {"text": "As a result, the accuracy of state-of-the-art NLP tools on this type of data is often rather low.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.9994062185287476}]}, {"text": "In combination with the increasing corpus sizes, it seems that we have to lower our expectations with respect to the quality of the annotations.", "labels": [], "entities": []}, {"text": "Timeconsuming double annotation or a manual correction of the whole corpus is often not feasible.", "labels": [], "entities": [{"text": "Timeconsuming", "start_pos": 0, "end_pos": 13, "type": "METRIC", "confidence": 0.9036710858345032}]}, {"text": "Thus, the use of so-called silver standards has been discussed (, along with their adequacy to replace carefully hand-crafted gold standard corpora.", "labels": [], "entities": []}, {"text": "Other approaches to address this problem come from the areas of domain adaptation and error detection.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 64, "end_pos": 81, "type": "TASK", "confidence": 0.7655458748340607}, {"text": "error detection", "start_pos": 86, "end_pos": 101, "type": "TASK", "confidence": 0.6725812703371048}]}, {"text": "In the first field, the focus is on adapting NLP tools or algorithms to data from new domains, thus increasing the accuracy of the tools.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 115, "end_pos": 123, "type": "METRIC", "confidence": 0.9988718628883362}]}, {"text": "In error detection, the goal is to automatically identify erroneous labels in the data and either hand those instances to a human annotator for manual correction, or to automatically correct those cases.", "labels": [], "entities": [{"text": "error detection", "start_pos": 3, "end_pos": 18, "type": "TASK", "confidence": 0.7077607363462448}]}, {"text": "Here, the focus is not on improving the tools but on increasing the quality of the corpus and, at the same time, reducing human effort.", "labels": [], "entities": []}, {"text": "These approaches are not mutually exclusive but can be seen as complementary methods for building high-quality language resources at a reasonable expense.", "labels": [], "entities": []}, {"text": "We position our work at the interface of these fields.", "labels": [], "entities": []}, {"text": "Our general objective is to build a high-quality linguistic resource for informal spoken youth language, annotated with parts of speech (POS) information.", "labels": [], "entities": []}, {"text": "As we do not have the resources for proofing the whole corpus, we aim at building a silver standard where the quality of the annotations is high enough to be useful for linguistic research.", "labels": [], "entities": []}, {"text": "For automatic preprocessing, we use tagging models adapted to our data.", "labels": [], "entities": []}, {"text": "The main contribution of this paper is in developing and evaluating methods for POS error detection in automatically annotated corpora.", "labels": [], "entities": [{"text": "POS error detection", "start_pos": 80, "end_pos": 99, "type": "TASK", "confidence": 0.8784243067105612}]}, {"text": "We show that our approach not only works for our data but can also be applied to canonical text from the newspaper domain, where the POS accuracy of standard NLP tools is quite high.", "labels": [], "entities": [{"text": "POS", "start_pos": 133, "end_pos": 136, "type": "METRIC", "confidence": 0.9063860177993774}, {"text": "accuracy", "start_pos": 137, "end_pos": 145, "type": "METRIC", "confidence": 0.6970204710960388}]}, {"text": "The paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 reviews related work on detecting annotation errors in corpora.", "labels": [], "entities": []}, {"text": "Section 3 describes the underlying assumptions of our approach.", "labels": [], "entities": []}, {"text": "In Section 4, we describe the experimental setup and data used in our experiments, and we present our results in Section 5.", "labels": [], "entities": []}, {"text": "We conclude in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "The data we use in our experiments comes from two sources, i) from a corpus of informal, spoken German youth language (The KiezDeutsch Korpus (KiDKo) Release 1.0) (), and ii) from the TIGER corpus (), a German newspaper corpus.", "labels": [], "entities": [{"text": "The KiezDeutsch Korpus (KiDKo) Release 1.0", "start_pos": 119, "end_pos": 161, "type": "DATASET", "confidence": 0.7078342474997044}, {"text": "TIGER corpus", "start_pos": 184, "end_pos": 196, "type": "DATASET", "confidence": 0.8839563131332397}]}], "tableCaptions": [{"text": " Table 1: Baseline results for different taggers on KiDKo and TIGER (results on KiDKo are given for a  5-fold cross validation (5-fold) and for the development and test set)", "labels": [], "entities": [{"text": "TIGER", "start_pos": 62, "end_pos": 67, "type": "METRIC", "confidence": 0.6562009453773499}]}, {"text": " Table 2: Number of error candidates identified by the disagreements in the ensemble tagger predictions  (baseline)", "labels": [], "entities": []}, {"text": " Table 4: Number of error candidates identified by the classifier, precision (prec) and recall (rec)", "labels": [], "entities": [{"text": "precision (prec)", "start_pos": 67, "end_pos": 83, "type": "METRIC", "confidence": 0.9090529978275299}, {"text": "recall", "start_pos": 88, "end_pos": 94, "type": "METRIC", "confidence": 0.9932242631912231}]}, {"text": " Table 5: Number of error candidates identified by the classifier using a marginal probability threshold", "labels": [], "entities": []}]}