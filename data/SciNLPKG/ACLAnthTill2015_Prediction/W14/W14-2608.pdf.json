{"title": [{"text": "An Impact Analysis of Features in a Classification Approach to Irony Detection in Product Reviews", "labels": [], "entities": [{"text": "Irony Detection in Product Reviews", "start_pos": 63, "end_pos": 97, "type": "TASK", "confidence": 0.7583846747875214}]}], "abstractContent": [{"text": "Irony is an important device inhuman communication , both in everyday spoken conversations as well as in written texts including books, websites, chats, reviews, and Twitter messages among others.", "labels": [], "entities": []}, {"text": "Specific cases of irony and sarcasm have been studied in different contexts but, to the best of our knowledge, only recently the first publicly available corpus including annotations about whether a text is ironic or not has been published by Filatova (2012).", "labels": [], "entities": []}, {"text": "However , no baseline for classification of ironic or sarcastic reviews has been provided.", "labels": [], "entities": [{"text": "classification of ironic or sarcastic reviews", "start_pos": 26, "end_pos": 71, "type": "TASK", "confidence": 0.8548769354820251}]}, {"text": "With this paper, we aim at closing this gap.", "labels": [], "entities": []}, {"text": "We formulate the problem as a supervised classification task and evaluate different classifiers, reaching an F 1-measure of up to 74 % using logistic regression.", "labels": [], "entities": [{"text": "F 1-measure", "start_pos": 109, "end_pos": 120, "type": "METRIC", "confidence": 0.9818252325057983}]}, {"text": "We analyze the impact of a number of features which have been proposed in previous research as well as combinations of them.", "labels": [], "entities": []}], "introductionContent": [{"text": "Irony is often understood as \"the use of words that mean the opposite of what you really think especially in order to be funny\" or \"a situation that is strange or funny because things happen in away that seems to be the opposite\" of what is expected.", "labels": [], "entities": []}, {"text": "Many dictionaries make this difference between verbal irony and situational irony).", "labels": [], "entities": [{"text": "situational irony", "start_pos": 64, "end_pos": 81, "type": "TASK", "confidence": 0.7302385121583939}]}, {"text": "The German Duden (2014) mentions sarcasm as synonym to irony, while the comprehension of sarcasm as a special case of irony might be more common.", "labels": [], "entities": [{"text": "German Duden (2014)", "start_pos": 4, "end_pos": 23, "type": "DATASET", "confidence": 0.9359105110168457}]}, {"text": "For instance, the Merriam Webster Dictionary (2014) defines sarcasm as \"a sharp and often satirical or ironic utterance designed to cut or give pain\".", "labels": [], "entities": [{"text": "Merriam Webster Dictionary (2014)", "start_pos": 18, "end_pos": 51, "type": "DATASET", "confidence": 0.9461318651835123}]}, {"text": "Irony is a frequent phenomenon within human communication, occurring both in spoken and written discourse including books, websites, fora, chats, Twitter messages, Facebook posts, news articles and product reviews.", "labels": [], "entities": []}, {"text": "Even for humans it is sometimes difficult to recognize irony.", "labels": [], "entities": [{"text": "recognize irony", "start_pos": 45, "end_pos": 60, "type": "TASK", "confidence": 0.7464083135128021}]}, {"text": "Irony markers are thus often used inhuman communication, supporting the correct interpretation).", "labels": [], "entities": []}, {"text": "The automatic identification of ironic formulations in written text is a very challenging as well as important task as shown by the comment \"Read the book!\" which in the context of a movie review could be regarded as ironic and as conveying the fact that the film was far worse compared to the book.", "labels": [], "entities": [{"text": "automatic identification of ironic formulations in written text", "start_pos": 4, "end_pos": 67, "type": "TASK", "confidence": 0.8053216561675072}]}, {"text": "Another example is taken from a review for the book \"Great Expectations\" by Charles Dickens: \"i would recomend this book to friends who have insomnia or those who i absolutely despise.\"", "labels": [], "entities": [{"text": "Great Expectations\" by Charles Dickens", "start_pos": 53, "end_pos": 91, "type": "TASK", "confidence": 0.7402747869491577}]}, {"text": "The standard approach of recommending X implies that X is worthwhile is clearly not valid in the given context as the author is stating that she disliked the book.", "labels": [], "entities": []}, {"text": "In real world applications of sentiment analysis, large data sets are automatically classified into positive statements or negative statements and such output is used to generate summaries of the sentiment about a product.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 30, "end_pos": 48, "type": "TASK", "confidence": 0.9250801205635071}]}, {"text": "In order to increase the accurateness of such systems, ironic or sarcastic statements need to be identified in order to infer the actual communicative intention of the author.", "labels": [], "entities": [{"text": "accurateness", "start_pos": 25, "end_pos": 37, "type": "METRIC", "confidence": 0.9703372120857239}]}, {"text": "In this paper, we are concerned with approaches for the automatic detection of irony in texts, which is an important task in a variety of applications, including the automatic interpretation of text-based chats, computer interaction or sentiment analysis and opinion mining.", "labels": [], "entities": [{"text": "automatic detection of irony in texts", "start_pos": 56, "end_pos": 93, "type": "TASK", "confidence": 0.8531168897946676}, {"text": "automatic interpretation of text-based chats", "start_pos": 166, "end_pos": 210, "type": "TASK", "confidence": 0.7898952603340149}, {"text": "computer interaction or sentiment analysis", "start_pos": 212, "end_pos": 254, "type": "TASK", "confidence": 0.6116279065608978}, {"text": "opinion mining", "start_pos": 259, "end_pos": 273, "type": "TASK", "confidence": 0.8640778958797455}]}, {"text": "In the latter case, the detection is of outmost importance in order to correctly assign a polarity score to an aspect of a reviewed product or a person mentioned in a Twitter message.", "labels": [], "entities": []}, {"text": "In addition, the automatic detection of irony or sarcasm in text requires an operational definition and has therefore the potential to contribute to a deeper understanding of the linguistic properties of irony and sarcasm as linguistic phenomena and their corpus based evaluation and verification.", "labels": [], "entities": [{"text": "automatic detection of irony or sarcasm in text", "start_pos": 17, "end_pos": 64, "type": "TASK", "confidence": 0.8116136640310287}]}, {"text": "The rest of this paper is structured as follows: We introduce the background and theories on irony in Section 1.1 and discuss previous work in the area of automatically recognizing irony in Section 1.2.", "labels": [], "entities": [{"text": "automatically recognizing irony", "start_pos": 155, "end_pos": 186, "type": "TASK", "confidence": 0.6365246077378591}]}, {"text": "In the methods part in Section 2, we present our set of features (Section 2.1) and the classifiers we take into account (Section 2.2).", "labels": [], "entities": []}, {"text": "In Section 3, we discuss the data set used in this work in more detail (Section 3.1), present our experimental setting (Section 3.2) and show the evaluation of our approach (Section 3.3).", "labels": [], "entities": []}, {"text": "We conclude with a discussion and summary (Section 4) and with an outlook on possible future work (Section 5).", "labels": [], "entities": []}], "datasetContent": [{"text": "We run experiments for three baselines: The starrating baseline relies only on the number of stars assigned in the review as a feature.", "labels": [], "entities": []}, {"text": "The bag-ofwords baseline exploits only the unigrams in the text as features.", "labels": [], "entities": []}, {"text": "The sentiment word count only uses the information whether the number of positive words in the text is larger than the number of negative words.", "labels": [], "entities": []}, {"text": "We emphasize that the first baseline is only of limited applicability as it requires the explicit availability of a star-rating.", "labels": [], "entities": []}, {"text": "The second baseline relies on standard text classification features that are not specific for the task.", "labels": [], "entities": []}, {"text": "The third baseline relies on a classical feature used in sentiment analysis, but is not specific for irony detection.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 57, "end_pos": 75, "type": "TASK", "confidence": 0.9506037533283234}, {"text": "irony detection", "start_pos": 101, "end_pos": 116, "type": "TASK", "confidence": 0.9056092798709869}]}, {"text": "We refer to the feature set \"All\" encompassing all features described in Section 2.1, including bagof-words and the set \"Specific Features\".", "labels": [], "entities": []}, {"text": "In order to understand the impact of a specific feature A, we run three sets of experiments: \u2022 Using all features with the exception of A.", "labels": [], "entities": []}, {"text": "\u2022 Using all specific features with the exception of A.", "labels": [], "entities": [{"text": "A", "start_pos": 52, "end_pos": 53, "type": "METRIC", "confidence": 0.9631844162940979}]}, {"text": "\u2022 Using A as the only feature.", "labels": [], "entities": [{"text": "A", "start_pos": 8, "end_pos": 9, "type": "METRIC", "confidence": 0.9888717532157898}]}, {"text": "In addition to evaluating each single feature as described above, we evaluate the set of positive and negative instantiations of features when using the sentiment dictionary.", "labels": [], "entities": []}, {"text": "The \"Positive set\" and \"Negative set\" take into account the respective subsets of all specific features.", "labels": [], "entities": []}, {"text": "Each experiment is performed in a 10-fold crossvalidation setting on document level.", "labels": [], "entities": []}, {"text": "We report recall, precision and F 1 -measure for each of the classifiers.: Comparison of different classification methods using different feature sets.", "labels": [], "entities": [{"text": "recall", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9994459748268127}, {"text": "precision", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.9993768334388733}, {"text": "F 1 -measure", "start_pos": 32, "end_pos": 44, "type": "METRIC", "confidence": 0.9883301258087158}]}, {"text": "\"All\" refers to the features described in Section 2 including bag-of-words (\"BOW\").", "labels": [], "entities": [{"text": "BOW", "start_pos": 77, "end_pos": 80, "type": "METRIC", "confidence": 0.9823982119560242}]}, {"text": "Features\" are \"All\" without \"BOW\".", "labels": [], "entities": [{"text": "BOW", "start_pos": 29, "end_pos": 32, "type": "METRIC", "confidence": 0.9904464483261108}]}, {"text": "a positive rating by the author, as explained by Table 2, which shows the more real-world compatible result of a rich feature set in addition.", "labels": [], "entities": []}, {"text": "Obviously, the depicted distribution is very similar to the distribution of the manually annotated data set, which can obviously not be achieved by the star-rating feature alone.", "labels": [], "entities": []}, {"text": "The best result is achieved by using the starrating together with bag-of-words and specific features with a logistic regression approach (leading to an F 1 -measure of 74 %).", "labels": [], "entities": [{"text": "F 1 -measure", "start_pos": 152, "end_pos": 164, "type": "METRIC", "confidence": 0.9870669096708298}]}, {"text": "The SVM and decision tree have a comparable performance on the task, which is albeit lower compared to the performance of the logistic regression approach.", "labels": [], "entities": []}, {"text": "Using the task-agnostic pure bag-of-words approach leads to a performance of 68.8 % for logistic regression; this classifier has the property of dealing well with correlated features and the additional specific features cannot contribute positively to the result.", "labels": [], "entities": []}, {"text": "Similarly, the F 1 -measure of 64.1 % produced by the SVM cannot be increased by including additional features.", "labels": [], "entities": [{"text": "F 1 -measure", "start_pos": 15, "end_pos": 27, "type": "METRIC", "confidence": 0.9893105775117874}, {"text": "SVM", "start_pos": 54, "end_pos": 57, "type": "DATASET", "confidence": 0.8715238571166992}]}, {"text": "In contrast, a positive impact of additional features can be observed for the decision tree in the case that specific features are combined with bag-of-word-based features, reaching close to 59 % F 1 in comparison to 53.4 % F 1 for bag-of-words alone.", "labels": [], "entities": [{"text": "F 1", "start_pos": 196, "end_pos": 199, "type": "METRIC", "confidence": 0.9882746636867523}, {"text": "F 1", "start_pos": 224, "end_pos": 227, "type": "METRIC", "confidence": 0.9860500991344452}]}, {"text": "It would be desirable to have a model only or mainly based on the problem-specific features, as this leads to a much more compact and therefore ef-ficient representation than taking all words into account.", "labels": [], "entities": []}, {"text": "In addition, the model would be easier to understand.", "labels": [], "entities": []}, {"text": "By exploiting task-specific features alone, the performance reaches at most an F 1 -measure of 50.9 %, which shows that task-agnostic features such as unigram features are needed.", "labels": [], "entities": [{"text": "F 1 -measure", "start_pos": 79, "end_pos": 91, "type": "METRIC", "confidence": 0.9907475262880325}]}, {"text": "A significant drop in performance when leaving out a feature or feature set can be observed for the Imbalance feature and the Positive set.", "labels": [], "entities": []}, {"text": "Both these feature sets take into account the star-rating.", "labels": [], "entities": []}, {"text": "The task-specific features alone yield high precision results at the expense of a very low recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 44, "end_pos": 53, "type": "METRIC", "confidence": 0.9979076385498047}, {"text": "recall", "start_pos": 91, "end_pos": 97, "type": "METRIC", "confidence": 0.9989332556724548}]}, {"text": "This clearly shows that task-specific features should be used with standard, task-independent features (the bag-of-words).", "labels": [], "entities": []}, {"text": "The most helpful task-specific features are: Imbalance, Positive set, Quotes and Pos/Neg&Ellipses.", "labels": [], "entities": [{"text": "Imbalance", "start_pos": 45, "end_pos": 54, "type": "METRIC", "confidence": 0.9849141240119934}]}], "tableCaptions": [{"text": " Table 1: Comparison of different classification methods using different feature sets. \"All\" refers to the  features described in Section 2 including bag-of-words (\"BOW\"). \"Sp. Features\" are \"All\" without  \"BOW\".", "labels": [], "entities": []}, {"text": " Table 2: Frequencies for the different star-ratings  of a review, as annotated, and according to the  logistic regression classifier with the feature set  \"All \u2212 Imbalance\".", "labels": [], "entities": [{"text": "Frequencies", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9928876757621765}]}]}