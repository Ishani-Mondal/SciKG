{"title": [{"text": "Mining Lexical Variants from Microblogs: An Unsupervised Multilingual Approach", "labels": [], "entities": []}], "abstractContent": [{"text": "User-generated content has become a recurrent resource for NLP tools and applications , hence many efforts have been made lately in order to handle the noise present in short social media texts.", "labels": [], "entities": []}, {"text": "The use of normalisation techniques has been proven useful for identifying and replacing lexical variants on some of the most informal genres such as microblogs.", "labels": [], "entities": []}, {"text": "But annotated data is needed in order to train and evaluate these systems, which usually involves a costly process.", "labels": [], "entities": []}, {"text": "Until now, most of these approaches have been fo-cused on English and they were not taking into account demographic variables such as the user location and gender.", "labels": [], "entities": []}, {"text": "In this paper we describe the methodology used for automatically mining a corpus of variant and normalisation pairs from English and Spanish tweets.", "labels": [], "entities": [{"text": "automatically mining a corpus of variant and normalisation pairs from English and Spanish tweets", "start_pos": 51, "end_pos": 147, "type": "TASK", "confidence": 0.6168935064758573}]}], "introductionContent": [{"text": "User-generated content (UGC), and specially the microblog genre, has become an interesting resource for Natural Language Processing (NLP) tools and applications.", "labels": [], "entities": [{"text": "User-generated content (UGC)", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.5477661311626434}]}, {"text": "Many are the advantages of exploiting this real-time stream of multilingual textual data.", "labels": [], "entities": []}, {"text": "Popular applications such as Twitter has an heterogeneous user base of almost 600 million users that generate more than 60 million new tweets everyday.", "labels": [], "entities": []}, {"text": "For this reason, Twitter has become one of the most used sources of textual data for NLP with several applications such as sentiment analysis or realtime event detection (.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 123, "end_pos": 141, "type": "TASK", "confidence": 0.9627499878406525}, {"text": "realtime event detection", "start_pos": 145, "end_pos": 169, "type": "TASK", "confidence": 0.6386248370011648}]}, {"text": "Recent advances on machine translation or information retrieval systems have been also making an extensive use of UGC for both training and evaluation purposes.", "labels": [], "entities": [{"text": "machine translation or information retrieval", "start_pos": 19, "end_pos": 63, "type": "TASK", "confidence": 0.7758012533187866}]}, {"text": "However, tweets can be very noisy and sometimes hard to understand for both humans ( ) and NLP applications (, so an additional preprocessing step is usually required.", "labels": [], "entities": []}, {"text": "There have been different perceptions regarding the lexical quality of social media) ( ) and even others suggested that 40% of the messages of Twitter were \"pointless babble\").", "labels": [], "entities": []}, {"text": "Most of the out of vocabulary (OOV) words present in social media texts can be catalogued as lexical variants (e.g. \"See u 2moro\" \u2192 \"See you tomorrow\"), that are words lexically related with their canonic form.", "labels": [], "entities": []}, {"text": "The use of text normalisation techniques has been proven useful in order to clean short and informal texts such as tweets.", "labels": [], "entities": [{"text": "text normalisation", "start_pos": 11, "end_pos": 29, "type": "TASK", "confidence": 0.7613833546638489}]}, {"text": "However, the evaluation of these systems requires annotated data, which usually involves costly human annotations.", "labels": [], "entities": []}, {"text": "There are previous works about automatically constructing normalisation dictionaries, but until now, most of these approaches have been focused on English and they were not taking into account demographic variants.", "labels": [], "entities": [{"text": "automatically constructing normalisation dictionaries", "start_pos": 31, "end_pos": 84, "type": "TASK", "confidence": 0.5961092859506607}]}, {"text": "In this paper we describe the methodology used for automatically mining lexical variants from English and Spanish tweets associated to a set of headwords.", "labels": [], "entities": [{"text": "automatically mining lexical variants from English and Spanish tweets associated to a set of headwords", "start_pos": 51, "end_pos": 153, "type": "TASK", "confidence": 0.6421482026576996}]}, {"text": "These formal and informal pairs can be later used to train and evaluate existing social media text normalisation systems.", "labels": [], "entities": [{"text": "social media text normalisation", "start_pos": 81, "end_pos": 112, "type": "TASK", "confidence": 0.7284097671508789}]}, {"text": "Additional metadata from Twitter such as geographic location and user gender is also collected, opening the possibility to model and analyse gender or location-specific variants.", "labels": [], "entities": []}, {"text": "This paper is organised as follows.", "labels": [], "entities": []}, {"text": "We describe the related work in Section 2.", "labels": [], "entities": []}, {"text": "We then describe our variant mining methodology in Section 3.", "labels": [], "entities": []}, {"text": "The obtained results are presented in Section 4.", "labels": [], "entities": []}, {"text": "Section 5, draws the conclusions and future work.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 5: Example of formal/informal pairs and the  extract transformations.", "labels": [], "entities": []}, {"text": " Table 6: Cross-validation results of intentionality  classification with examples.", "labels": [], "entities": [{"text": "intentionality  classification", "start_pos": 38, "end_pos": 68, "type": "TASK", "confidence": 0.8408345580101013}]}]}