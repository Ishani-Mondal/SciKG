{"title": [{"text": "Word Confidence Estimation for SMT N-best List Re-ranking", "labels": [], "entities": [{"text": "Word Confidence Estimation", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.6458527346452078}, {"text": "SMT", "start_pos": 31, "end_pos": 34, "type": "TASK", "confidence": 0.9833083152770996}, {"text": "Re-ranking", "start_pos": 47, "end_pos": 57, "type": "TASK", "confidence": 0.26664528250694275}]}], "abstractContent": [{"text": "This paper proposes to use Word Confidence Estimation (WCE) information to improve MT outputs via N-best list re-ranking.", "labels": [], "entities": [{"text": "Word Confidence Estimation (WCE)", "start_pos": 27, "end_pos": 59, "type": "TASK", "confidence": 0.7589334001143774}, {"text": "MT outputs", "start_pos": 83, "end_pos": 93, "type": "TASK", "confidence": 0.890243798494339}]}, {"text": "From the confidence label assigned for each word in the MT hypothesis , we add six scores to the baseline log-linear model in order to re-rank the N-best list.", "labels": [], "entities": [{"text": "MT", "start_pos": 56, "end_pos": 58, "type": "TASK", "confidence": 0.982568085193634}]}, {"text": "Firstly, the correlation between the WCE-based sentence-level scores and the conventional evaluation scores (BLEU, TER, TERp-A) is investigated.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 109, "end_pos": 113, "type": "METRIC", "confidence": 0.9983086585998535}, {"text": "TER", "start_pos": 115, "end_pos": 118, "type": "METRIC", "confidence": 0.9689016342163086}, {"text": "TERp-A", "start_pos": 120, "end_pos": 126, "type": "METRIC", "confidence": 0.9616857767105103}]}, {"text": "Then, the N-best list re-ranking is evaluated over different WCE system performance levels: from our real and efficient WCE system (ranked 1st during last WMT 2013 Quality Estimation Task) to an oracle WCE (which simulates an interactive scenario where a user simply validates words of a MT hypothesis and the new output will be automatically regenerated).", "labels": [], "entities": [{"text": "WMT 2013 Quality Estimation Task", "start_pos": 155, "end_pos": 187, "type": "TASK", "confidence": 0.6047223567962646}, {"text": "MT hypothesis", "start_pos": 288, "end_pos": 301, "type": "TASK", "confidence": 0.865636795759201}]}, {"text": "The results suggest that our real WCE system slightly (but significantly) improves the baseline while the oracle one extremely boosts it; and better WCE leads to better MT quality.", "labels": [], "entities": [{"text": "MT", "start_pos": 169, "end_pos": 171, "type": "TASK", "confidence": 0.9800318479537964}]}], "introductionContent": [{"text": "A number of methods to improve MT hypotheses after decoding have been proposed in the past, such as: post-editing, re-ranking or re-decoding.", "labels": [], "entities": [{"text": "MT hypotheses", "start_pos": 31, "end_pos": 44, "type": "TASK", "confidence": 0.9098665118217468}]}, {"text": "Post-editing () is a humaninspired task where the machine post edits translations in a second automatic pass.", "labels": [], "entities": []}, {"text": "In re-ranking (), more features are used along with the multiple model scores for re-determining the 1-best among N-best list.", "labels": [], "entities": []}, {"text": "Meanwhile, redecoding process ( intervenes directly into the decoder's search graph (e.g. adds more reward or penalty scores), driving it to another better path.", "labels": [], "entities": []}, {"text": "This work aims at re-ranking the N-best list to improve MT quality.", "labels": [], "entities": [{"text": "MT", "start_pos": 56, "end_pos": 58, "type": "TASK", "confidence": 0.9898067116737366}]}, {"text": "Generally, during the translation task, the decoder traverses through paths in its search space, computes the objective function values for them and outputs the one with highest score as the best hypothesis.", "labels": [], "entities": [{"text": "translation task", "start_pos": 22, "end_pos": 38, "type": "TASK", "confidence": 0.9144777953624725}]}, {"text": "Besides, those with lower scores can also be generated in a socalled N-best list.", "labels": [], "entities": []}, {"text": "The decoder's function consists of parameters from different models, such as translation, distortion, word penalties, reordering, language models, etc.", "labels": [], "entities": []}, {"text": "In the N-best list, although the current 1-best beats the remains in terms of model score, it might not be exactly the closest to the human reference.", "labels": [], "entities": []}, {"text": "Therefore, adding more decoder independent features would be expected to raise up a better candidate.", "labels": [], "entities": []}, {"text": "In this work, we build six additional features based on the labels predicted by our Word Confidence Estimation (WCE) system, then integrate them with the existing decoder scores for re-ranking hypotheses in the N-best list.", "labels": [], "entities": [{"text": "Word Confidence Estimation (WCE)", "start_pos": 84, "end_pos": 116, "type": "TASK", "confidence": 0.7322293072938919}]}, {"text": "More precisely, in the second pass, our reranker aggregates over decoder and WCE-based weighted scores and utilizes the obtained sum to sort out the best candidate.", "labels": [], "entities": []}, {"text": "The novelty of this paper lies on the following contributions: the correlation between WCE-based sentence-level scores and conventional evaluation scores (BLEU, TER, TERp-A) is first investigated.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 155, "end_pos": 159, "type": "METRIC", "confidence": 0.9968247413635254}, {"text": "TER", "start_pos": 161, "end_pos": 164, "type": "METRIC", "confidence": 0.9125022888183594}, {"text": "TERp-A", "start_pos": 166, "end_pos": 172, "type": "METRIC", "confidence": 0.8550629019737244}]}, {"text": "Then, we conduct the N-best list re-ranking over different WCE system performance levels: starting by areal WCE, passing through several gradually improved (simulated) systems and finally the \"oracle\" one.", "labels": [], "entities": []}, {"text": "From these in-depth experiments, the role of WCE in improving MT quality via re-ranking N-best list is confirmed and reinforced.", "labels": [], "entities": [{"text": "MT", "start_pos": 62, "end_pos": 64, "type": "TASK", "confidence": 0.9917091727256775}]}, {"text": "The remaining parts of this article are organized as follows: in section 2 we summarize some outstanding approaches in N-best list re-ranking as well as in WCE.", "labels": [], "entities": [{"text": "WCE", "start_pos": 156, "end_pos": 159, "type": "DATASET", "confidence": 0.7650691270828247}]}, {"text": "Section 3 describes our WCE system construction, followed by proposed features.", "labels": [], "entities": [{"text": "WCE system construction", "start_pos": 24, "end_pos": 47, "type": "TASK", "confidence": 0.7641113003094991}]}], "datasetContent": [{"text": "As described in Section 3.2, our SMT system generates 1000-best list for each source sentence, and among them, the best hypothesis was determined by using the objective function based on 14 decoder scores, including: 7 reordering scores, 1 language model score, 5 translation model scores and 1 word penalty score.", "labels": [], "entities": [{"text": "SMT", "start_pos": 33, "end_pos": 36, "type": "TASK", "confidence": 0.9896184802055359}]}, {"text": "Initially, all six additional WCE-based scores are weighted as 1.0.", "labels": [], "entities": []}, {"text": "Then, two optimization methods: MERT and Margin Infused Relaxed Algorithm (MIRA) () are applied to optimize the weights of all 20 scores of the re-ranker.", "labels": [], "entities": [{"text": "MERT", "start_pos": 32, "end_pos": 36, "type": "METRIC", "confidence": 0.954652726650238}, {"text": "Margin Infused Relaxed Algorithm (MIRA)", "start_pos": 41, "end_pos": 80, "type": "METRIC", "confidence": 0.916913024016789}]}, {"text": "In both methods, we carryout a 2-fold cross validation on the N-best: Translation quality of the baseline system (only decoder scores) and that with additional scores from real \"WCE\" or \"oracle\" WCE system  test set.", "labels": [], "entities": [{"text": "WCE system  test set", "start_pos": 195, "end_pos": 215, "type": "DATASET", "confidence": 0.6141936331987381}]}, {"text": "In other words, we split our N-best test set into two equivalent subsets: S1 and S2.", "labels": [], "entities": []}, {"text": "Playing the role of a development set, S1 will be used to optimize the 20 weights for re-ranking S2 (and vice versa).", "labels": [], "entities": []}, {"text": "Finally two result subsets (new 1-best after re-ranking process) are merged for evaluation.", "labels": [], "entities": []}, {"text": "To better acknowledge the impact of the proposed scores, we calculate them not only using our real WCE system, but also using an oracle WCE (further called \"WCE scores\" and \"oracle scores\", respectively).", "labels": [], "entities": []}, {"text": "To summarize, we experiment with the three following systems: \u2022 BL: Baseline SMT system with 14 above decoder scores \u2022 BL+WCE: Baseline + 6 real WCE scores \u2022 BL+OR: Baseline + 6 oracle WCE scores (simulating an interactive scenario).", "labels": [], "entities": [{"text": "BL", "start_pos": 64, "end_pos": 66, "type": "METRIC", "confidence": 0.9696011543273926}, {"text": "SMT", "start_pos": 77, "end_pos": 80, "type": "TASK", "confidence": 0.8004688620567322}]}], "tableCaptions": [{"text": " Table 1: Pr, Rc and F for \"G\" and \"B\" labels of  our WCE system", "labels": [], "entities": []}, {"text": " Table 2: Translation quality of the baseline system (only decoder scores) and that with additional scores  from real \"WCE\" or \"oracle\" WCE system", "labels": [], "entities": [{"text": "Translation", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.5531664490699768}]}, {"text": " Table 3: Quality comparison (measured by TER) between the baseline and two integrated systems in  details (How many sentences are improved, kept equivalent or degraded, out of 881 test sentences?", "labels": [], "entities": [{"text": "TER", "start_pos": 42, "end_pos": 45, "type": "METRIC", "confidence": 0.9985703229904175}]}, {"text": " Table 4: The performances (Fscore) of simulated  WCE systems", "labels": [], "entities": [{"text": "Fscore", "start_pos": 28, "end_pos": 34, "type": "METRIC", "confidence": 0.9918708801269531}]}]}