{"title": [{"text": "Incremental N-gram Approach for Language Identification in Code-Switched Text", "labels": [], "entities": [{"text": "Approach", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9370886087417603}, {"text": "Language Identification in Code-Switched Text", "start_pos": 32, "end_pos": 77, "type": "TASK", "confidence": 0.7582607805728913}]}], "abstractContent": [{"text": "A multilingual person writing a sentence or apiece of text tends to switch between languages s/he is proficient in.", "labels": [], "entities": []}, {"text": "This alteration between languages, commonly known as code-switching, presents us with the problem of determining the correct language of each word in the text.", "labels": [], "entities": []}, {"text": "My method uses a variety of techniques based upon the observed differences in the formation of words in these languages.", "labels": [], "entities": []}, {"text": "My system was able to obtain third position in both tweet and token level for the main test dataset as well as first position in the token level evaluation for the surprise dataset both consisting of Nepali-English code-switched texts.", "labels": [], "entities": []}], "introductionContent": [{"text": "Nowadays, it is common for people to be able to speak in two or more languages.", "labels": [], "entities": []}, {"text": "So, the propensity to use code-switching in spoken as well as in written text has increased.", "labels": [], "entities": []}, {"text": "Code-switching occurs when a person uses two or more than two languages in a single piece of text.", "labels": [], "entities": []}, {"text": "According to, the phenomenon where speakers switch between multiple languages between the same utterance or across utterances within the same conversation is referred to as Linguistic Code Switching.", "labels": [], "entities": [{"text": "Linguistic Code Switching", "start_pos": 173, "end_pos": 198, "type": "TASK", "confidence": 0.6117207606633505}]}, {"text": "English, being an universal language is highly likely to be code-switched with some other language.", "labels": [], "entities": []}, {"text": "This is specially true when English is studied or spoken in the community as the second language by a person.", "labels": [], "entities": []}, {"text": "Ina such case, the person is likely to use English words with his/her native language to form codeswitched, yet, syntactically correct and meaningful sentences.", "labels": [], "entities": []}, {"text": "This paper deals with the code-switching that occurs when English is used with Spanish or Nepali.", "labels": [], "entities": []}, {"text": "The problem of identifying codeswitching is closely tied with figuring out how a language is acquired or learned.", "labels": [], "entities": []}, {"text": "identified the phenomenon of how Italians, who were raised in Germany developed fluctuation and variation in their native language as well as in German.", "labels": [], "entities": []}, {"text": "They were also noticed to have a strong tendency to have a conversation dominated by the German words.", "labels": [], "entities": []}, {"text": "This phenomenon was also observed by.", "labels": [], "entities": []}, {"text": "The strong influence of Bollywood in the Indian culture and the high amount of code-switching with English in movie dialogues and song lyrics, led to Hindi-English code-switching, being common for the average Indian.", "labels": [], "entities": []}, {"text": "Finding out the points in the text where people are most likely to code-switch, what word of a certain language is more likely to be used than a word with the same meaning of another language and which languages are more likely to be used in code-switching than others are all important research questions.", "labels": [], "entities": []}, {"text": "Although my paper deals only with finding out the language a certain token in a code-switched text belongs to, this is a first step towards answering those other questions.", "labels": [], "entities": [{"text": "finding out the language a certain token in a code-switched text", "start_pos": 34, "end_pos": 98, "type": "TASK", "confidence": 0.7208560109138489}]}, {"text": "The main aim of this paper is to describe my system submission to the Computational Approaches to Code Switching task ().", "labels": [], "entities": [{"text": "Computational Approaches to Code Switching task", "start_pos": 70, "end_pos": 117, "type": "TASK", "confidence": 0.628727580110232}]}, {"text": "The training dataset provided for the classification task were tweets composed of Spanish and English words or Nepali and English words.", "labels": [], "entities": [{"text": "classification task", "start_pos": 38, "end_pos": 57, "type": "TASK", "confidence": 0.8881588280200958}]}, {"text": "The test dataset also consisted of similar tweets.", "labels": [], "entities": []}, {"text": "In addition to this, there was also a surprise dataset consisting of Facebook posts and comments in the place of tweets.", "labels": [], "entities": []}, {"text": "My system for this task performs language identification by using a number of techniques.", "labels": [], "entities": [{"text": "language identification", "start_pos": 33, "end_pos": 56, "type": "TASK", "confidence": 0.8005341291427612}]}, {"text": "The first one is based upon an assumption that words of different languages have varying sets of n-gram prefixes that occur predominantly throughout the language.", "labels": [], "entities": []}, {"text": "There has been prior research on language identification through the use of n-grams.", "labels": [], "entities": [{"text": "language identification", "start_pos": 33, "end_pos": 56, "type": "TASK", "confidence": 0.7191366702318192}]}, {"text": "have ap-proached the task of identifying the language of an electronic mail taken from Usenet newsgroups with the use of n-grams.", "labels": [], "entities": [{"text": "identifying the language of an electronic mail taken from Usenet newsgroups", "start_pos": 29, "end_pos": 104, "type": "TASK", "confidence": 0.6872671246528625}]}, {"text": "They obtained training sets for each language to be classified, which acted as language category samples.", "labels": [], "entities": []}, {"text": "They computed n-gram frequency profiles on these training sets.", "labels": [], "entities": []}, {"text": "They found that the top 300 n-grams of each language are used most frequently to form the words of the language.", "labels": [], "entities": []}, {"text": "have used dictionary search and a n-gram based language model to identify the language on word-level of forum posts with Dutch and Turkish code-switching.", "labels": [], "entities": []}, {"text": "found that data collected from social media to detect code-switching contained a lot of non-standard spellings of words and unnecessary capitalization.", "labels": [], "entities": []}, {"text": "It was also true for this dataset.", "labels": [], "entities": []}, {"text": "So, I made use of a lightweight spellchecker in the event that the word was not spelled correctly and hence not categorised into any language.", "labels": [], "entities": []}, {"text": "I have also used a rule based classification system that can also be used for named entities and non-alphanumeric language classes.", "labels": [], "entities": []}, {"text": "With the system that I built based on these ideas, I achieved an accuracy of above 94% for English-Nepali and above 80% for English-Spanish in the token level evaluation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.9996649026870728}]}, {"text": "As the system works as a pipeline of smaller systems, it was time consuming.", "labels": [], "entities": []}, {"text": "So, in order to improve speed, it is built to run on a multithreaded environment.", "labels": [], "entities": [{"text": "speed", "start_pos": 24, "end_pos": 29, "type": "METRIC", "confidence": 0.9808438420295715}]}, {"text": "Language identification by using these techniques overcomes the drawback of other simpler methods like extracting a token's characters and then using its Unicode value to determine its language.", "labels": [], "entities": [{"text": "Language identification", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.7431877255439758}]}, {"text": "But most of the time the words are not written in its own script by using Unicode, but rather, its Romanized form is used.", "labels": [], "entities": []}, {"text": "Some languages like Spanish are almost fully written in roman letters, with exception being only a small subset of accented characters.", "labels": [], "entities": []}, {"text": "Precisely these kinds of words require more robust classification techniques.", "labels": [], "entities": []}, {"text": "Another alternative is manual classification but it has the downside of being time consuming and an uneconomical alternative.", "labels": [], "entities": [{"text": "manual classification", "start_pos": 23, "end_pos": 44, "type": "TASK", "confidence": 0.6524333208799362}]}, {"text": "There is a need of an application that can overcome these drawbacks and create a system that can be used for similar sets of data.", "labels": [], "entities": []}], "datasetContent": [{"text": "For all my experiments, I divided the training data into a ratio of 70:30 for training and crossvalidation.", "labels": [], "entities": []}, {"text": "In order to tune the different parameters, I had to repeat the experiments multiple times.", "labels": [], "entities": []}, {"text": "So, in order to improve the runtime performance, I made use of multithreading.", "labels": [], "entities": []}, {"text": "I tested the application by setting the first ngram length in the Incremental N-Gram Model to 3 and 4.", "labels": [], "entities": [{"text": "ngram length", "start_pos": 46, "end_pos": 58, "type": "METRIC", "confidence": 0.832895815372467}]}, {"text": "I varied the criteria of the least number of characters that should match between two tokens, in order for the two tokens to be similar.", "labels": [], "entities": []}, {"text": "I observed the highest accuracy of above 94% in Nepali-English classification when the First n-gram length was 4.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9993526339530945}]}, {"text": "In the case of Spanish-English token classification, I observed the highest accuracy of 88% when the n-gram length was 3.", "labels": [], "entities": [{"text": "Spanish-English token classification", "start_pos": 15, "end_pos": 51, "type": "TASK", "confidence": 0.5342000623544058}, {"text": "accuracy", "start_pos": 76, "end_pos": 84, "type": "METRIC", "confidence": 0.9995201826095581}]}, {"text": "The spellchecker gave the best results when it had the above mentioned similarity criteria.", "labels": [], "entities": [{"text": "similarity", "start_pos": 71, "end_pos": 81, "type": "METRIC", "confidence": 0.9876794815063477}]}, {"text": "The whole classifying task was sure to take along time so I built it to scale with the increasing number of CPUs.", "labels": [], "entities": [{"text": "classifying task", "start_pos": 10, "end_pos": 26, "type": "TASK", "confidence": 0.8893467485904694}]}, {"text": "I performed the experiments on a 1st Generation Core i7 (Eight Logical Cores) CPU and a Core 2 Duo CPU (2 logical Cores).", "labels": [], "entities": []}, {"text": "I observed the best performance when the application created the number of threads equal to the number of available CPU cores.", "labels": [], "entities": []}, {"text": "The classification task completed in the i7 CPU with 8 active threads in 13 minutes compared to almost 35 minutes with 2 active threads on the Core 2 Duo CPU.", "labels": [], "entities": []}, {"text": "The task completed in around 38 minutes in the i7 CPU with 2 active threads.", "labels": [], "entities": []}, {"text": "the Facebook post-level evaluation of EnglishNepali test tweets.", "labels": [], "entities": [{"text": "EnglishNepali test tweets", "start_pos": 38, "end_pos": 63, "type": "DATASET", "confidence": 0.8435751795768738}]}, {"text": "Although, it was third in tweetlevel evaluation, it was only 0.7% behind the best tweet-level system in terms of accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 113, "end_pos": 121, "type": "METRIC", "confidence": 0.9979141354560852}]}, {"text": "My system was second in Facebook post-level evaluation by 6.9%.", "labels": [], "entities": []}, {"text": "It had an accuracy of 94.6% and 86.5% in the token level evaluation of English-Nepali test tweets and Facebook posts respectively.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.999504804611206}]}, {"text": "The model was third in the tweet-token evaluation but stood first in the Facebook-post token evaluation.", "labels": [], "entities": []}, {"text": "These results align with the hypothesis of the Incremental N-Gram Occurrence Model that token belonging to the same language will have more overlap of the preceding characters.", "labels": [], "entities": []}, {"text": "My system obtained an accuracy of 69.9% in the tweet-level evaluation and 70.0% accuracy in the Facebook post-level evaluation of the EnglishSpanish test data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9994410872459412}, {"text": "accuracy", "start_pos": 80, "end_pos": 88, "type": "METRIC", "confidence": 0.9978294968605042}, {"text": "EnglishSpanish test data", "start_pos": 134, "end_pos": 158, "type": "DATASET", "confidence": 0.9668654799461365}]}, {"text": "It was the least effective in both the evaluation tasks.", "labels": [], "entities": []}, {"text": "My system had an accuracy of 80.3% and 87.6% in the token level evaluation of English-Spanish test tweets and Facebook posts respectively.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.9994520545005798}]}, {"text": "The model was again the least effective in both the token level evaluation task but by a smaller margin.", "labels": [], "entities": []}, {"text": "The results do not exactly follow the hypothesis, but we can say it supports it because English and Spanish languages share a lot of common word prefixes.", "labels": [], "entities": []}, {"text": "Hence my method is more likely to incorrectly predict some Spanish words as English and vice-versa.", "labels": [], "entities": []}, {"text": "It is evident from the results that this model is suitable when the languages being classified are   highly dissimilar in syntax and structure.", "labels": [], "entities": []}, {"text": "As English and Nepali language do not have the same ancestry they have very different syntax and structure.", "labels": [], "entities": []}, {"text": "The word prefixes used frequently to form Nepali words and the syntax of forming various parts of speech in Nepali language is quite different than in the English language.", "labels": [], "entities": []}, {"text": "In both the training and test datasets, the ratio of code-switched to monolingual tweets is higher in Nepali than in Spanish, which probably led to my system performing worse on tweet level for Spanish.", "labels": [], "entities": []}, {"text": "Although, this distribution can be anticipated because English is taught from primary schooling levels in Nepal.", "labels": [], "entities": []}, {"text": "Almost all the literate population can communicate pretty well in English.", "labels": [], "entities": []}, {"text": "Nepal is a country that relies heavily in the tourism industry, and English being a universal language is a second language in major cities and travel destinations of the country.", "labels": [], "entities": []}, {"text": "All these factors have led to a lot of code switching in tweets Nepali tweets.", "labels": [], "entities": [{"text": "code switching", "start_pos": 39, "end_pos": 53, "type": "TASK", "confidence": 0.7168021202087402}]}, {"text": "On the other hand, Spanish is a widely spoken language itself.", "labels": [], "entities": []}, {"text": "The people who know Spanish rarely need to learn a second language.", "labels": [], "entities": []}, {"text": "This might be the reason that there are less code-switched tweets for Spanish.", "labels": [], "entities": []}, {"text": "My model also has a drawback, which is also demonstrated by my evaluation results.", "labels": [], "entities": []}, {"text": "Spanish and English languages do share a lot of common prefixes.", "labels": [], "entities": []}, {"text": "This maybe due to their shared IndoEuropean ancestry and the fact that English language has borrowed a significant number of words from the French language, which is very similar to the Spanish language.", "labels": [], "entities": []}, {"text": "The word \"precious\" and \"bilingual\" in English is spelled \"precioso\" and \"bilingue\" in Spanish.", "labels": [], "entities": []}, {"text": "This similarity of prefixes leads the Incremental N-gram model to classify tokens wrongly based upon the recurrence of the  Facebook posts to be verified as code switched because, just one token in a tweet that is wrongly classified as belonging to another language class, will validate the tweet as code-switched.", "labels": [], "entities": []}, {"text": "To counter this drawback, when classifying words of the language that have the same ancestry and similar structure and syntax, only the prefixes should not be considered.", "labels": [], "entities": []}, {"text": "Another important thing to note is that the task of evaluation is very taxing on the CPU and takes a lot of time.", "labels": [], "entities": []}, {"text": "Various evaluation techniques are applied to a token before its correct class is determined.", "labels": [], "entities": []}, {"text": "This time consuming process can be accelerated significantly by designing a system that follows the data and task parallelism principles i.e. multithreading.", "labels": [], "entities": []}, {"text": "The redesign of the system to support multithreading made the training process almost 3 times faster.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Incremental N-gram Classification Ex- ample", "labels": [], "entities": [{"text": "Incremental N-gram Classification", "start_pos": 10, "end_pos": 43, "type": "TASK", "confidence": 0.7892585198084513}]}, {"text": " Table 2: Tweet level results on the test data.", "labels": [], "entities": []}, {"text": " Table 3: Token level results on the test data for  Nepali-English.", "labels": [], "entities": []}, {"text": " Table 4: Token level results on the test data for  Spanish-English.", "labels": [], "entities": []}, {"text": " Table 5: Tweet level results on the surprise data.", "labels": [], "entities": []}, {"text": " Table 6: Token level results on the surprise data  for Nepali-English.", "labels": [], "entities": []}, {"text": " Table 7: Token level results on the surprise data  for Spanish-English.", "labels": [], "entities": []}]}