{"title": [{"text": "Weakly-Supervised Bayesian Learning of a CCG Supertagger", "labels": [], "entities": []}], "abstractContent": [{"text": "We present a Bayesian formulation for weakly-supervised learning of a Combina-tory Categorial Grammar (CCG) supertag-ger with an HMM.", "labels": [], "entities": []}, {"text": "We assume supervision in the form of a tag dictionary, and our prior encourages the use of cross-linguistically common category structures as well as transitions between tags that can combine locally according to CCG's combinators.", "labels": [], "entities": []}, {"text": "Our prior is theoretically appealing since it is motivated by language-independent, universal properties of the CCG formalism.", "labels": [], "entities": []}, {"text": "Empirically, we show that it yields substantial improvements over previous work that used similar biases to initialize an EM-based learner.", "labels": [], "entities": []}, {"text": "Additional gains are obtained by further shaping the prior with corpus-specific information that is extracted automatically from raw text and a tag dictionary.", "labels": [], "entities": []}], "introductionContent": [{"text": "Unsupervised part-of-speech (POS) induction is a classic problem in NLP.", "labels": [], "entities": [{"text": "Unsupervised part-of-speech (POS) induction", "start_pos": 0, "end_pos": 43, "type": "TASK", "confidence": 0.5894972681999207}]}, {"text": "Many proposed solutions are based on Hidden Markov models (HMMs), with various improvements obtainable through: inductive bias in the form of tag dictionaries, sparsity constraints (, careful initialization of parameters (, feature based representations, and priors on model parameters.", "labels": [], "entities": []}, {"text": "When tag dictionaries are available, a situation we will call type-supervision, POS induction from unlabeled corpora can be relatively successful; however, as the number of possible tags increases, performance drops ().", "labels": [], "entities": [{"text": "POS induction from unlabeled corpora", "start_pos": 80, "end_pos": 116, "type": "TASK", "confidence": 0.8360217452049256}]}, {"text": "In such cases, there area large number of possible labels for each token, so picking the right one simply by chance is unlikely; the parameter space tends to be large; and devising good initial parameters is difficult.", "labels": [], "entities": []}, {"text": "Therefore, it is unsurprising that the unsupervised (or even weaklysupervised) learning of a Combinatory Categorial Grammar (CCG) supertagger, which labels each word with one of a large (possibly unbounded) number of structured categories called supertags, is a considerable challenge.", "labels": [], "entities": []}, {"text": "Despite the apparent complexity of the task, supertag sequences have regularities due to universal properties of the CCG formalism ( \u00a72) that can be used to reduce the complexity of the problem; previous work showed promising results by using these regularities to initialize an HMM that is then refined with EM.", "labels": [], "entities": [{"text": "EM", "start_pos": 309, "end_pos": 311, "type": "METRIC", "confidence": 0.8271085023880005}]}, {"text": "Here, we exploit CCG's category structure to motivate a novel prior over HMM parameters for use in Bayesian learning ( \u00a73).", "labels": [], "entities": []}, {"text": "This prior encourages (i) crosslinguistically common tag types, (ii) tag bigrams that can combine using CCG's combinators, and (iii) sparse transition distributions.", "labels": [], "entities": []}, {"text": "We also go beyond the use of these universals to show how additional, corpus-specific information can be automatically extracted from a combination of the tag dictionary and raw data, and how that information can be combined with the universal knowledge for integration into the model to improve the prior.", "labels": [], "entities": []}, {"text": "We use a blocked sampling algorithm to sample supertag sequences for the sentences in the training data, proportional to their posterior probability ( \u00a74).", "labels": [], "entities": []}, {"text": "We experimentally verify that our Bayesian formulation is effective and substantially outperforms the state-of-the-art baseline initialization/EM strategy in several languages ( \u00a75).", "labels": [], "entities": []}, {"text": "We also evaluate using tag dictionaries that are unpruned and have only partial word coverage, finding even greater improvements in these more realistic scenarios.", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate our approach, we used CCGBank, which is a transformation of the English Penn Treebank (; the CTB-CCG (Tse and Curran, 2010) transformation of the Penn Chinese Treebank (); and the CCG-TUT corpus (, built from the TUT corpus of Italian text ().", "labels": [], "entities": [{"text": "English Penn Treebank", "start_pos": 76, "end_pos": 97, "type": "DATASET", "confidence": 0.8181785345077515}, {"text": "CTB-CCG (Tse and Curran, 2010) transformation", "start_pos": 105, "end_pos": 150, "type": "DATASET", "confidence": 0.8991167810228136}, {"text": "Penn Chinese Treebank", "start_pos": 158, "end_pos": 179, "type": "DATASET", "confidence": 0.9109940131505331}, {"text": "CCG-TUT corpus", "start_pos": 192, "end_pos": 206, "type": "DATASET", "confidence": 0.8120675981044769}, {"text": "TUT corpus of Italian text", "start_pos": 225, "end_pos": 251, "type": "DATASET", "confidence": 0.8681299924850464}]}, {"text": "Statistics on the size and ambiguity of these datasets are shown in.", "labels": [], "entities": []}, {"text": "For CCGBank, sections 00-15 were used for extracting the tag dictionary, 16-18 for the raw corpus, 19-21 for development data, and 22-24 for test data.", "labels": [], "entities": [{"text": "CCGBank", "start_pos": 4, "end_pos": 11, "type": "DATASET", "confidence": 0.9528763890266418}]}, {"text": "For TUT, the first 150 sentences of each of the CIVIL LAW and NEWSPAPER sections were used for raw data, the next sentences 150-249 of each was used for development, and the sentences 250-349 were used for test; the remaining data, 457 sentences from CIVIL LAW and 548 from NEWSPAPER, plus the much smaller 132-sentence JRC ACQUIS data, was used for the tag dictionary.", "labels": [], "entities": [{"text": "TUT", "start_pos": 4, "end_pos": 7, "type": "DATASET", "confidence": 0.5138309001922607}, {"text": "CIVIL LAW", "start_pos": 48, "end_pos": 57, "type": "DATASET", "confidence": 0.9272743463516235}, {"text": "CIVIL LAW", "start_pos": 251, "end_pos": 260, "type": "DATASET", "confidence": 0.9591178894042969}, {"text": "JRC ACQUIS data", "start_pos": 320, "end_pos": 335, "type": "DATASET", "confidence": 0.811166008313497}]}, {"text": "For CTB-CCG, sections 00-11 were used for the tag dictionary, 20-24 for raw, 25-27 for dev, and 28-31 for test.", "labels": [], "entities": [{"text": "CTB-CCG", "start_pos": 4, "end_pos": 11, "type": "DATASET", "confidence": 0.9274448752403259}]}, {"text": "Because we are interested in showing the relative gains that our ideas provide over Baldridge (2008), we reimplemented the initialization procedure from that paper, allowing us to evaluate all approaches consistently.", "labels": [], "entities": []}, {"text": "For each dataset, we ran a series of experiments in which we made further changes from the original work.", "labels": [], "entities": []}, {"text": "We first ran a baseline experiment with uniform transition and emission initialization of EM (indicated as \"1.\" in) followed by our reimplementation of the initialization procedure by is a reimplementation of. is Bayesian formulation using only the ideas from Baldridge: P CPLX , P \u03ba , and uniform emissions.", "labels": [], "entities": []}, {"text": "(4-6) are our enhancements to the prior: using our category grammar in PG instead of P CPLX , using P tr \u03ba instead of P \u03ba , and using P em instead of uniform.", "labels": [], "entities": []}, {"text": "experimented with the Bayesian formulation, first using the same information used by Baldridge, and then adding our enhancements: using our category grammar in PG , using P tr \u03ba as the transition compatability distribution, and using P em as \u03c6 0 t (w).", "labels": [], "entities": []}, {"text": "For each dataset, we ran experiments using four different levels of tag dictionary pruning.", "labels": [], "entities": []}, {"text": "Pruning is the process of artificially removing noise from the tag dictionary by using token-level annotation counts to discard low-probability tags; for each word, for cutoff x, any tag with probability less than x is excluded.", "labels": [], "entities": []}, {"text": "Tag dictionary pruning is a standard procedure in type-supervised training, but because it requires information that does not truly conform to the type-supervised scenario, we felt that it was critical to demonstrate the performance of our approach under situations of less pruning, including no artificial pruning at all.", "labels": [], "entities": []}, {"text": "We emphasize that unlike inmost previous work, we use incomplete tag dictionaries.", "labels": [], "entities": []}, {"text": "Most previous work makes the unrealistic assumption that the tag dictionary contains an entry for every word that appears in either the training or testing data.", "labels": [], "entities": []}, {"text": "This is a poor approximation of areal tagging system, which will never have complete lexical knowledge about the test data.", "labels": [], "entities": [{"text": "areal tagging", "start_pos": 32, "end_pos": 45, "type": "TASK", "confidence": 0.5074920952320099}]}, {"text": "Even work that only assumes complete knowledge of the tagging possibilities for the lexical items in the training corpus is problematic.", "labels": [], "entities": []}, {"text": "This still makes learning unrealistically easy since it dramatically reduces the ambiguity of words that would have been unseen, and, in the case of CCG, introduces additional tags that would not have otherwise been known.", "labels": [], "entities": []}, {"text": "To ensure that our experiments are more realistic, we draw our tag dictionary entries from data that is totally disjoint from both the raw and test corpora.", "labels": [], "entities": []}, {"text": "During learning, any unknown words (words not appearing in the tag dictionary) are unconstrained so that they may take any tag, and are, thus, maximally ambiguous.", "labels": [], "entities": []}, {"text": "We only performed minimal parameter tuning, choosing instead to stay consistent with Baldridge (2008) and simply pick reasonable-seeming values for any additional parameters.", "labels": [], "entities": [{"text": "Baldridge (2008)", "start_pos": 85, "end_pos": 101, "type": "DATASET", "confidence": 0.8113787919282913}]}, {"text": "Any tuning that was performed was done with simple hill-climbing on the development data of English CCGBank.", "labels": [], "entities": [{"text": "English CCGBank", "start_pos": 92, "end_pos": 107, "type": "DATASET", "confidence": 0.9215589463710785}]}, {"text": "All parameters were held consistent across experiments, including across languages.", "labels": [], "entities": []}, {"text": "For EM, we used 50 iterations; for FFBS we used 100 burnin iterations and 200 sampling iterations.", "labels": [], "entities": [{"text": "FFBS", "start_pos": 35, "end_pos": 39, "type": "DATASET", "confidence": 0.873027503490448}]}, {"text": "For all experiments, we used \u03c3 = 0.95 for P (tr) \u03ba and \u03bb = 0.5 for \u03c0 0 t to be consistent with previous work, \u03b1 \u03c0 = 3000, \u03b1 \u03c6 = 7000, p term = 0.6, p fw = 0.5, p mod = 0.8, and \u03b4 = 1000 for p atom . Test data was run only once, for the final figures.", "labels": [], "entities": []}, {"text": "The final results reported were achieved by using the following training sequence: initialize parameters according to the scenario, train an HMM using EM or FFBS starting with that set of parameters, tag the raw corpus with the trained HMM, add-0.1 smooth counts from the now-tagged raw corpus, and train a maximum entropy Markov model (MEMM) from this \"auto-supervised\" data.", "labels": [], "entities": []}, {"text": "Most notably, the contributions described in this paper improve results in nearly every experimental scenario.", "labels": [], "entities": []}, {"text": "We can see immediate, often sizable, gains inmost cases simply by using the Bayesian formulation.", "labels": [], "entities": []}, {"text": "Further gains are seen from adding each of the other various contributions of this paper.", "labels": [], "entities": []}, {"text": "Perhaps most interestingly, the gains are only minimal with maximum pruning, but the gains increase as the pruning becomes less aggressive -as the scenarios become more realistic.", "labels": [], "entities": []}, {"text": "This indicates that our improvements make the overall procedure more robust.", "labels": [], "entities": []}, {"text": "Error Analysis Like POS-taggers, the learned supertagger frequently confuses nouns (N) and their modifiers (N/N), but the most frequent error made by the English (6) experiment was (((S\\NP)\\(S\\NP))/N) instead of (NP nb /N).", "labels": [], "entities": []}, {"text": "However, these are both determiner types, indicating an interesting problem for the supertagger: it often predicts an object type-raised determiner instead of the vanilla NP/N, but in many contexts, both categories are equally valid.", "labels": [], "entities": []}, {"text": "(In fact, for parsers that use type-raising as a rule, this distinction in lexical categories does not exist.) also improved upon the work by by using integer linear programming to find a minimal model of supertag transitions, thereby generating a better starting point for EM than the grammatical constraints alone could provide.", "labels": [], "entities": []}, {"text": "This approach is complementary to the work presented here, and because we have shown that our work yields gains under tag dictionaries of various levels of cleanliness, it is probable that employing minimization to set the base distribution for sampling could lead to still higher gains.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics for the various corpora used. CCGBank is English, CCG-CTB is Chinese, and TUT  is Italian. The number of tags includes only those tags found in the tag dictionary (TD). Ambiguity rates  are the average number of entries in the unpruned tag dictionary for each word in the raw corpus. English  POS statistics are shown only for comparison; only CCG experiments were run.", "labels": [], "entities": [{"text": "TUT", "start_pos": 95, "end_pos": 98, "type": "METRIC", "confidence": 0.9446491003036499}, {"text": "Ambiguity", "start_pos": 190, "end_pos": 199, "type": "METRIC", "confidence": 0.9515365362167358}]}, {"text": " Table 2: Experimental results: test-set per-token supertag accuracies. \"TD cutoff\" indicates the level of  tag dictionary pruning; see text. (1) is uniform EM initialization.", "labels": [], "entities": [{"text": "TD cutoff", "start_pos": 73, "end_pos": 82, "type": "METRIC", "confidence": 0.9616549015045166}, {"text": "EM initialization", "start_pos": 157, "end_pos": 174, "type": "TASK", "confidence": 0.8664525151252747}]}]}