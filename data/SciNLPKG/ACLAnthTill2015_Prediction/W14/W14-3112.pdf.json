{"title": [{"text": "Concurrent Visualization of Relationships between Words and Topics in Topic Models", "labels": [], "entities": []}], "abstractContent": [{"text": "Analysis tools based on topic models are often used as a means to explore large amounts of unstructured data.", "labels": [], "entities": []}, {"text": "Users often reason about the correctness of a model using relationships between words within the topics or topics within the model.", "labels": [], "entities": []}, {"text": "We compute this useful contextual information as term co-occurrence and topic co-variance and overlay it on top of standard topic model output via an intuitive interactive visualization.", "labels": [], "entities": []}, {"text": "This is a work in progress with the end goal to combine the visual representation with interactions and online learning, so the users can directly explore (a) why a model may not align with their intuition and (b) modify the model as needed.", "labels": [], "entities": []}], "introductionContent": [{"text": "Topic modeling is a popular technique for analyzing large text corpora.", "labels": [], "entities": [{"text": "Topic modeling", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.8132133781909943}]}, {"text": "A user is unlikely to have the time required to understand and exploit the raw results of topic modeling for analysis of a corpus.", "labels": [], "entities": []}, {"text": "Therefore, an interesting and intuitive visualization is required fora topic model to provide added value.", "labels": [], "entities": []}, {"text": "A common topic modeling technique is Latent Dirichlet Allocation (LDA) (, which is an unsupervised algorithm for performing statistical topic modeling that uses a \"bag of words\" approach.", "labels": [], "entities": [{"text": "topic modeling", "start_pos": 9, "end_pos": 23, "type": "TASK", "confidence": 0.7248808890581131}, {"text": "Latent Dirichlet Allocation (LDA", "start_pos": 37, "end_pos": 69, "type": "METRIC", "confidence": 0.7997632026672363}, {"text": "statistical topic modeling", "start_pos": 124, "end_pos": 150, "type": "TASK", "confidence": 0.6365841627120972}]}, {"text": "The resulting topic model represents the corpus as an unrelated set of topics where each topic is a probability distribution over words.", "labels": [], "entities": []}, {"text": "Experienced users who have worked with a text corpus for an extended period of time often think of the thematic relationships in the corpus in terms of higher-level statistics such as (a) inter-topic correlations or (b) word correlations.", "labels": [], "entities": []}, {"text": "However, standard topic models do not explicitly provide such contextual information to the users.", "labels": [], "entities": []}, {"text": "Existing tools based on topic models, such as Topical Guide (, TopicViz (, and the topic visualization of) support topic-based corpus browsing and understanding.", "labels": [], "entities": []}, {"text": "Visualizations of this type typically represent standard topic models as a sea of word clouds; the individual topics within the model are presented as an unordered set of word clouds -or something similar -of the top words for the topic 1 where word size is proportional to the probability of the word for the topic.", "labels": [], "entities": []}, {"text": "A primary issue with word clouds is that they can hinder understanding (Harris, 2011) due to the fact that they lack information about the relationships between words.", "labels": [], "entities": []}, {"text": "Additionally, topic model visualizations that display topics in a random layout can lead to a huge, inefficiently organized search space, which is not always helpful in providing a quick corpus overview or assisting the user to diagnose possible problems with the model.", "labels": [], "entities": []}, {"text": "The authors of Correlated Topic Models (CTM) () recognize the limitation of existing topic models to directly model the correlation between topics, and present an alternative algorithm, CTM, which models the correlation between topics discovered fora corpus by using a more flexible distribution for the topic proportions in the model.", "labels": [], "entities": []}, {"text": "Topical n-gram models (TNG) () discover phrases in addition to topics.", "labels": [], "entities": []}, {"text": "TNG is a probabilistic model which assigns words and n-grams based on surrounding context, instead of for all references in the corpus.", "labels": [], "entities": []}, {"text": "These models independently account for the two limitations of statistical topic modeling discussed in this paper by modifying the underlying topic modeling algorithm.", "labels": [], "entities": [{"text": "statistical topic modeling", "start_pos": 62, "end_pos": 88, "type": "TASK", "confidence": 0.634846031665802}]}, {"text": "Our work aims to provide a low-cost method for incorporating this information as well as visualizing it in an effective way.", "labels": [], "entities": []}, {"text": "We compute summary statistics, term co-occurrence and topic covariance, which can be overlaid on top of any traditional topic model.", "labels": [], "entities": []}, {"text": "As a number of application-specific LDA implementations exist, we propose a meta-technique which can be applied to any underlying algorithm.", "labels": [], "entities": []}, {"text": "We present a relationship-enriched visualization to help users explore topic models through word and topic correlations.", "labels": [], "entities": []}, {"text": "We propose interactions to support user understanding, validation, and refinement of the models.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}