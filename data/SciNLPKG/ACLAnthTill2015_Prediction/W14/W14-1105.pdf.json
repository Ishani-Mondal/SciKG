{"title": [], "abstractContent": [{"text": "Discourse relation parsing is an important task with the goal of understanding text beyond the sentence boundaries.", "labels": [], "entities": [{"text": "Discourse relation parsing", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8382904926935831}]}, {"text": "With the availability of annotated corpora (Penn Discourse Treebank) statistical discourse parsers were developed.", "labels": [], "entities": [{"text": "Penn Discourse Treebank", "start_pos": 44, "end_pos": 67, "type": "DATASET", "confidence": 0.9722097516059875}, {"text": "statistical discourse parsers", "start_pos": 69, "end_pos": 98, "type": "TASK", "confidence": 0.5968303481737772}]}, {"text": "In the literature it was shown that the discourse parsing subtasks of discourse connective detection and relation sense classification do not generalize well across domains.", "labels": [], "entities": [{"text": "discourse parsing", "start_pos": 40, "end_pos": 57, "type": "TASK", "confidence": 0.7396828830242157}, {"text": "discourse connective detection", "start_pos": 70, "end_pos": 100, "type": "TASK", "confidence": 0.6546197732289633}, {"text": "relation sense classification", "start_pos": 105, "end_pos": 134, "type": "TASK", "confidence": 0.7783506512641907}]}, {"text": "The biomedical domain is of particular interest due to the availability of Biomedical Discourse Relation Bank (BioDRB).", "labels": [], "entities": []}, {"text": "In this paper we present cross-domain evaluation of PDTB trained discourse relation parser and evaluate feature-level domain adaptation techniques on the argument span extraction subtask.", "labels": [], "entities": [{"text": "discourse relation parser", "start_pos": 65, "end_pos": 90, "type": "TASK", "confidence": 0.6830390890439352}, {"text": "feature-level domain adaptation", "start_pos": 104, "end_pos": 135, "type": "TASK", "confidence": 0.643974224726359}, {"text": "argument span extraction", "start_pos": 154, "end_pos": 178, "type": "TASK", "confidence": 0.6624664068222046}]}, {"text": "We demonstrate that the subtask generalizes well across domains.", "labels": [], "entities": []}], "introductionContent": [{"text": "Discourse analysis is one of the most challenging tasks in Natural Language Processing that has applications in many language technology areas such as opinion mining, summarization, information extraction, etc.", "labels": [], "entities": [{"text": "Discourse analysis", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8768556714057922}, {"text": "opinion mining", "start_pos": 151, "end_pos": 165, "type": "TASK", "confidence": 0.8278818726539612}, {"text": "summarization", "start_pos": 167, "end_pos": 180, "type": "TASK", "confidence": 0.9841347336769104}, {"text": "information extraction", "start_pos": 182, "end_pos": 204, "type": "TASK", "confidence": 0.8502534627914429}]}, {"text": "(see and) for detailed review).", "labels": [], "entities": []}, {"text": "The release of the large discourse relation annotated corpora, such as Penn Discourse Treebank (PDTB) (, marked the development of statistical discourse parsers (.", "labels": [], "entities": [{"text": "Penn Discourse Treebank (PDTB)", "start_pos": 71, "end_pos": 101, "type": "DATASET", "confidence": 0.9528390864531199}, {"text": "statistical discourse parsers", "start_pos": 131, "end_pos": 160, "type": "TASK", "confidence": 0.6281556586424509}]}, {"text": "Recently, PDTB-style discourse annotation was applied to biomedical domain and Biomedical Discourse Relation Bank (BioDRB)) was released.", "labels": [], "entities": []}, {"text": "This milestone marks the beginning of the research on cross-domain evaluation and domain adaptation of PDTB-style discourse parsers.", "labels": [], "entities": [{"text": "domain adaptation of PDTB-style discourse parsers", "start_pos": 82, "end_pos": 131, "type": "TASK", "confidence": 0.627608448266983}]}, {"text": "In this paper we address the question of how well PDTB-trained discourse parser (news-wire domain) can extract argument spans of explicit discourse relations in BioDRB (biomedical domain).", "labels": [], "entities": [{"text": "PDTB-trained discourse parser", "start_pos": 50, "end_pos": 79, "type": "TASK", "confidence": 0.7770828008651733}]}, {"text": "The use cases of discourse parsing in biomedical domain are discussed in detail in.", "labels": [], "entities": [{"text": "discourse parsing", "start_pos": 17, "end_pos": 34, "type": "TASK", "confidence": 0.7484119236469269}]}, {"text": "Here, on the other hand, we provide very general connection between the two.", "labels": [], "entities": []}, {"text": "The goal of Biomedical Text Mining (BioNLP) is to retrieve and organize biomedical knowledge from scientific publications; and detecting discourse relations such as contrast and causality is an important step towards this goal).", "labels": [], "entities": [{"text": "Biomedical Text Mining (BioNLP)", "start_pos": 12, "end_pos": 43, "type": "TASK", "confidence": 0.7068458745876948}]}, {"text": "To illustrate this point consider a quote from), given below.", "labels": [], "entities": []}, {"text": "In the example, the discourse connective since signals a causal relation between the clauses it connects.", "labels": [], "entities": []}, {"text": "That is, the reason why 'the addition of an anti-Oct2 antibody did not interfere with complex formation' is 'HeLa cells' not expressing Oct2'.", "labels": [], "entities": []}, {"text": "PDTB adopts non-hierarchical binary view on discourse relations: Argument 1 (Arg1) (in italics in the example) and Argument 2 (Arg2), which is syntactically attached to a discourse connective (in bold).", "labels": [], "entities": [{"text": "PDTB", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9027981162071228}]}, {"text": "Thus, a discourse relation is a triplet of a connective and its two arguments.", "labels": [], "entities": []}, {"text": "In the literature () PDTB-style discourse parsing is partitioned into discourse relation detection, argument position classification, argument span extraction, and relation sense classification.", "labels": [], "entities": [{"text": "PDTB-style discourse parsing", "start_pos": 21, "end_pos": 49, "type": "TASK", "confidence": 0.7620768745740255}, {"text": "discourse relation detection", "start_pos": 70, "end_pos": 98, "type": "TASK", "confidence": 0.612809548775355}, {"text": "argument position classification", "start_pos": 100, "end_pos": 132, "type": "TASK", "confidence": 0.6840530435244242}, {"text": "argument span extraction", "start_pos": 134, "end_pos": 158, "type": "TASK", "confidence": 0.6561202804247538}, {"text": "relation sense classification", "start_pos": 164, "end_pos": 193, "type": "TASK", "confidence": 0.7474397818247477}]}, {"text": "For the explicit discourse relations (i.e. signaled by a connective), discourse relation detection is cast as classification of connectives as discourse and non-discourse.", "labels": [], "entities": [{"text": "discourse relation detection", "start_pos": 70, "end_pos": 98, "type": "TASK", "confidence": 0.6655195454756418}]}, {"text": "Argument position classification, on the other hand, involves detection of the location of Arg1 with re- spect to Arg2, that is to detect whether a relation is inter-or intra-sentential.", "labels": [], "entities": [{"text": "Argument position classification", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.6761404474576315}, {"text": "Arg1", "start_pos": 91, "end_pos": 95, "type": "METRIC", "confidence": 0.9694185853004456}, {"text": "Arg2", "start_pos": 114, "end_pos": 118, "type": "METRIC", "confidence": 0.925493597984314}]}, {"text": "Argument span extraction is the extraction (labeling) of text segments that belong to each of the arguments.", "labels": [], "entities": [{"text": "Argument span extraction", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.7009943624337515}, {"text": "extraction (labeling) of text segments", "start_pos": 32, "end_pos": 70, "type": "TASK", "confidence": 0.7333483951432365}]}, {"text": "Finally, relation sense classification is the annotation of relations with the senses from the sense hierarchy (PDTB or BioDRB).", "labels": [], "entities": [{"text": "relation sense classification", "start_pos": 9, "end_pos": 38, "type": "TASK", "confidence": 0.7287031610806783}, {"text": "BioDRB", "start_pos": 120, "end_pos": 126, "type": "METRIC", "confidence": 0.7634989619255066}]}, {"text": "To the best of our knowledge, the only subtasks that were addressed cross-domain are the detection of explicit discourse connectives and relation sense classification).", "labels": [], "entities": [{"text": "detection of explicit discourse connectives", "start_pos": 89, "end_pos": 132, "type": "TASK", "confidence": 0.6363137900829315}, {"text": "relation sense classification", "start_pos": 137, "end_pos": 166, "type": "TASK", "confidence": 0.692702849706014}]}, {"text": "While the discourse parser of Faiz and Mercer (2013) 1 provides models for both domains and does identification of argument head words in the style of; there is no decision made on arguments spans.", "labels": [], "entities": []}, {"text": "Moreover, there is no cross-domain evaluation available for each of the models.", "labels": [], "entities": []}, {"text": "In this paper we address the task of cross-domain argument span extraction of explicit discourse relations.", "labels": [], "entities": [{"text": "cross-domain argument span extraction of explicit discourse relations", "start_pos": 37, "end_pos": 106, "type": "TASK", "confidence": 0.735517218708992}]}, {"text": "Additionally, we provide evaluation for cross-domain argument position classification as far as the data allows, since BioDRB lacks manual sentence segmentation.", "labels": [], "entities": [{"text": "cross-domain argument position classification", "start_pos": 40, "end_pos": 85, "type": "TASK", "confidence": 0.711228609085083}, {"text": "BioDRB", "start_pos": 119, "end_pos": 125, "type": "DATASET", "confidence": 0.7889406085014343}, {"text": "sentence segmentation", "start_pos": 139, "end_pos": 160, "type": "TASK", "confidence": 0.7473757565021515}]}, {"text": "The paper is structured as follows.", "labels": [], "entities": []}, {"text": "In Section 2 we present the comparative analysis of PDTB and BioDRB corpora and the relevant works on crossdomain discourse parsing.", "labels": [], "entities": [{"text": "crossdomain discourse parsing", "start_pos": 102, "end_pos": 131, "type": "TASK", "confidence": 0.7537238597869873}]}, {"text": "In Section 3 we describe the PDTB discourse parser used for crossdomain experiments.", "labels": [], "entities": [{"text": "PDTB discourse parser", "start_pos": 29, "end_pos": 50, "type": "TASK", "confidence": 0.7484596172968546}]}, {"text": "In Section 4 we present the evaluation methodology and the experimental results.", "labels": [], "entities": []}, {"text": "Section 5 provides concluding remarks.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this Section we first describe the evaluation methodology and then the experiments on crossdomain evaluation of argument position classification and argument span extraction models.", "labels": [], "entities": [{"text": "argument position classification", "start_pos": 115, "end_pos": 147, "type": "TASK", "confidence": 0.6428545117378235}, {"text": "argument span extraction", "start_pos": 152, "end_pos": 176, "type": "TASK", "confidence": 0.6347156365712484}]}, {"text": "The experimental settings for PDTB are the following: Sections 02-22 are used for training and Sections 23-24 for testing.", "labels": [], "entities": [{"text": "PDTB", "start_pos": 30, "end_pos": 34, "type": "TASK", "confidence": 0.846921980381012}]}, {"text": "For BioDRB, on the other hand, 12 fold cross-validation is used (2 documents in each fold, since in BioDRB there are 24 documents).", "labels": [], "entities": [{"text": "BioDRB", "start_pos": 4, "end_pos": 10, "type": "DATASET", "confidence": 0.7833495140075684}]}, {"text": "The performance of Argument Span Extraction is evaluated in terms of precision (p), recall (r), and F-measure (F 1 ) using the equations 1 -3.", "labels": [], "entities": [{"text": "Argument Span Extraction", "start_pos": 19, "end_pos": 43, "type": "TASK", "confidence": 0.6860100428263346}, {"text": "precision (p)", "start_pos": 69, "end_pos": 82, "type": "METRIC", "confidence": 0.9350078701972961}, {"text": "recall (r)", "start_pos": 84, "end_pos": 94, "type": "METRIC", "confidence": 0.9596029371023178}, {"text": "F-measure (F 1 )", "start_pos": 100, "end_pos": 116, "type": "METRIC", "confidence": 0.9195277214050293}]}, {"text": "An argument span is considered to be correct, if it exactly matches the reference string.", "labels": [], "entities": []}, {"text": "In the equations, Exact Match is the count of correctly tagged argument spans; No Match is the count of argument spans that do not match the reference string exactly, i.e. even a single token difference is counted as an error; and References in Gold is the total number of arguments in the reference.", "labels": [], "entities": [{"text": "Exact Match", "start_pos": 18, "end_pos": 29, "type": "METRIC", "confidence": 0.9721823036670685}]}, {"text": "Since argument span extraction is applied after argument position classification, the classification error is propagated.", "labels": [], "entities": [{"text": "argument span extraction", "start_pos": 6, "end_pos": 30, "type": "TASK", "confidence": 0.7032450636227926}, {"text": "argument position classification", "start_pos": 48, "end_pos": 80, "type": "TASK", "confidence": 0.6692785720030466}]}, {"text": "Thus, for the evaluation of argument span extraction, misclassified instances are reflected in the counts of Exact Matches and No Matches.", "labels": [], "entities": [{"text": "argument span extraction", "start_pos": 28, "end_pos": 52, "type": "TASK", "confidence": 0.6262831687927246}]}, {"text": "For example, misclassified same sentence relation results in that both its arguments are: In-domain performance of the PDTBtrained argument span extraction models on the test set with 'Gold' and 'Automatic' sentence splitting, tokenization, and syntactic features.", "labels": [], "entities": [{"text": "PDTBtrained argument span extraction", "start_pos": 119, "end_pos": 155, "type": "TASK", "confidence": 0.7576675415039062}, {"text": "sentence splitting", "start_pos": 207, "end_pos": 225, "type": "TASK", "confidence": 0.7425369620323181}]}, {"text": "The results are reported together with the error propagation from argument position classification for Same Sentence (SS), Previous Sentence (PS) models and joined results (ALL) as precision (P), recall (R) and F-measure (F1).", "labels": [], "entities": [{"text": "precision (P)", "start_pos": 181, "end_pos": 194, "type": "METRIC", "confidence": 0.9322400242090225}, {"text": "recall (R)", "start_pos": 196, "end_pos": 206, "type": "METRIC", "confidence": 0.9401233345270157}, {"text": "F-measure (F1)", "start_pos": 211, "end_pos": 225, "type": "METRIC", "confidence": 0.9439591467380524}]}, {"text": "considered as not recalled for the SS, and for the PS they are considered as No Match.", "labels": [], "entities": [{"text": "SS", "start_pos": 35, "end_pos": 37, "type": "TASK", "confidence": 0.5298101305961609}, {"text": "PS", "start_pos": 51, "end_pos": 53, "type": "TASK", "confidence": 0.8295323848724365}]}, {"text": "However, we do not propagate error in crossdomain evaluation on BioDRB, since there is no reference information.", "labels": [], "entities": [{"text": "BioDRB", "start_pos": 64, "end_pos": 70, "type": "DATASET", "confidence": 0.9072151780128479}]}, {"text": "Additionally, while Arg1 span extraction models are trained on Gold Arg2 features, for testing they are always automatic.", "labels": [], "entities": [{"text": "Arg1 span extraction", "start_pos": 20, "end_pos": 40, "type": "TASK", "confidence": 0.569824606180191}]}], "tableCaptions": [{"text": " Table 2: In-domain performance of the PDTB- trained argument span extraction models on the  test set with 'Gold' and 'Automatic' sentence  splitting, tokenization, and syntactic features. The  results are reported together with the error prop- agation from argument position classification for  Same Sentence (SS), Previous Sentence (PS) mod- els and joined results (ALL) as precision (P), recall  (R) and F-measure (F1).", "labels": [], "entities": [{"text": "argument span extraction", "start_pos": 53, "end_pos": 77, "type": "TASK", "confidence": 0.59907199939092}, {"text": "sentence  splitting", "start_pos": 130, "end_pos": 149, "type": "TASK", "confidence": 0.7351344525814056}, {"text": "precision (P)", "start_pos": 376, "end_pos": 389, "type": "METRIC", "confidence": 0.9281828105449677}, {"text": "recall  (R)", "start_pos": 391, "end_pos": 402, "type": "METRIC", "confidence": 0.9446506649255753}, {"text": "F-measure (F1)", "start_pos": 407, "end_pos": 421, "type": "METRIC", "confidence": 0.9524128139019012}]}, {"text": " Table 3: In-domain performance of the BioDRB- trained argument span extraction models. Both  training and testing are on automatic sentence  splitting, tokenization, and syntactic features. The  results are reported for Same Sentence (SS) and  Previous Sentence (PS) models, and the joined re- sults for each of the arguments (ALL) as average  precision (P), recall (R), and F-measure (F1) of  12-fold cross-validation.", "labels": [], "entities": [{"text": "BioDRB- trained argument span extraction", "start_pos": 39, "end_pos": 79, "type": "TASK", "confidence": 0.5531731645266215}, {"text": "sentence  splitting", "start_pos": 132, "end_pos": 151, "type": "TASK", "confidence": 0.7410262227058411}, {"text": "precision (P)", "start_pos": 345, "end_pos": 358, "type": "METRIC", "confidence": 0.9336623698472977}, {"text": "recall (R)", "start_pos": 360, "end_pos": 370, "type": "METRIC", "confidence": 0.9544554352760315}, {"text": "F-measure (F1)", "start_pos": 376, "end_pos": 390, "type": "METRIC", "confidence": 0.9333930611610413}]}, {"text": " Table 5: Cross-domain performance of the PDTB- trained argument span extraction models on Bio- DRB. For the 'Syntactic' setting the models are  trained on only syntactic features (POS-tag + IOB- chain) and 'connective labels'. For 'No Relation  Sense', the models are trained by replacing con- nective sense with 'connective labels'. The 'Base- line' is repeated from", "labels": [], "entities": [{"text": "argument span extraction", "start_pos": 56, "end_pos": 80, "type": "TASK", "confidence": 0.619107057650884}, {"text": "Bio- DRB", "start_pos": 91, "end_pos": 99, "type": "DATASET", "confidence": 0.7934836347897848}]}, {"text": " Table 6: Cross-domain performance of the PDTB- trained argument span extraction model on uni- gram and bigrams of token, POS-tag, IOB-chain  and 'connective label'. The results are reported for  Same Sentence (SS) and Previous Sentence (PS)  models, and the joined results for each of the argu- ments (ALL) as average precision (P), recall (R),  and F-measure (F1) of 12-fold cross-validation.", "labels": [], "entities": [{"text": "PDTB- trained argument span extraction", "start_pos": 42, "end_pos": 80, "type": "TASK", "confidence": 0.6211595982313156}, {"text": "precision (P)", "start_pos": 319, "end_pos": 332, "type": "METRIC", "confidence": 0.9448864161968231}, {"text": "recall (R)", "start_pos": 334, "end_pos": 344, "type": "METRIC", "confidence": 0.9534859657287598}, {"text": "F-measure (F1)", "start_pos": 351, "end_pos": 365, "type": "METRIC", "confidence": 0.9484584480524063}]}]}