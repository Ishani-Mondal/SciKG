{"title": [{"text": "Extracting Higher Order Relations From Biomedical Text", "labels": [], "entities": [{"text": "Extracting Higher Order Relations", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.850595086812973}]}], "abstractContent": [{"text": "Argumentation in a scientific article is composed of unexpressed and explicit statements of old and new knowledge combined into a logically coherent tex-tual argument.", "labels": [], "entities": []}, {"text": "Discourse relations, linguistic coherence relations that connect discourse segments, help to communicate an argument's logical steps.", "labels": [], "entities": []}, {"text": "A biomedi-cal relation exhibits a relationship between biomedical entities.", "labels": [], "entities": []}, {"text": "In this paper, we are primarily concerned with the extraction of connections between biomedical relations , a connection that we calla higher order relation.", "labels": [], "entities": []}, {"text": "We combine two methods, namely biomedical relation extraction and discourse relation parsing, to extract such higher order relations from biomedical research articles.", "labels": [], "entities": [{"text": "biomedical relation extraction", "start_pos": 31, "end_pos": 61, "type": "TASK", "confidence": 0.7032056649525961}, {"text": "discourse relation parsing", "start_pos": 66, "end_pos": 92, "type": "TASK", "confidence": 0.7089513738950094}]}, {"text": "Finding and extracting these relations can assist in automatically understanding the scientific arguments expressed by the author in the text.", "labels": [], "entities": []}], "introductionContent": [{"text": "We use the term higher order relation to denote a relation that relates two biomedical relations.", "labels": [], "entities": []}, {"text": "Consider, for example, the following sentence: (1) Aspirin appeared to prevent VCAM-1 transcription, since it dose-dependently inhibited induction of VCAM-1 mRNA by TNF.", "labels": [], "entities": [{"text": "VCAM-1 transcription", "start_pos": 79, "end_pos": 99, "type": "TASK", "confidence": 0.9587082266807556}]}, {"text": "We can find two biomedical relations involving Aspirin: Aspirin-prevents-VCAM-1 transcription and Aspirin-inhibits-induction of VCAM-1 mRNA.", "labels": [], "entities": []}, {"text": "These two relations are connected by the word since.", "labels": [], "entities": []}, {"text": "The higher order relation conveys a causal sense, which indicates that the latter relation causes the earlier one.", "labels": [], "entities": []}, {"text": "In genetic transcription mRNA is generated (a process known by the reader, so not expressed in the argument).", "labels": [], "entities": []}, {"text": "This piece of the author's argument is that by observing inhibition of mRNA induction (the genetic process that activates transcription) by different doses of aspirin, the inference that aspirin prevents the transcription can be made.", "labels": [], "entities": []}, {"text": "This inference is textually signalled by the discourse connective since.", "labels": [], "entities": []}, {"text": "Formally, we define a higher order relation as a binary relation that relates one biomedical relation with another biomedical relation.", "labels": [], "entities": []}, {"text": "In this paper we propose a method for these extracting higher order relations using discourse relation parsing and biomedical relation extraction.", "labels": [], "entities": [{"text": "extracting higher order relations", "start_pos": 44, "end_pos": 77, "type": "TASK", "confidence": 0.8168440014123917}, {"text": "discourse relation parsing", "start_pos": 84, "end_pos": 110, "type": "TASK", "confidence": 0.6660537719726562}, {"text": "biomedical relation extraction", "start_pos": 115, "end_pos": 145, "type": "TASK", "confidence": 0.6937516530354818}]}], "datasetContent": [{"text": "Our algorithm for extracting higher order relations depends on discourse parsing and biomedical relation extraction.", "labels": [], "entities": [{"text": "extracting higher order relations", "start_pos": 18, "end_pos": 51, "type": "TASK", "confidence": 0.8336934596300125}, {"text": "discourse parsing", "start_pos": 63, "end_pos": 80, "type": "TASK", "confidence": 0.7056597024202347}, {"text": "biomedical relation extraction", "start_pos": 85, "end_pos": 115, "type": "TASK", "confidence": 0.6443037291367849}]}, {"text": "We have discussed our implementation of these components and evaluated their performance in previous work.", "labels": [], "entities": []}, {"text": "We have evaluated the algorithm we present in this paper in terms of how accurately it can use those components in order to find higher order relations.", "labels": [], "entities": []}, {"text": "More specifically, we will measure how accurately it can determine the part of the full argument extent that contains the biomedical entities in it.", "labels": [], "entities": []}, {"text": "For this evaluation we used the AIMed corpus ().", "labels": [], "entities": [{"text": "AIMed corpus", "start_pos": 32, "end_pos": 44, "type": "DATASET", "confidence": 0.872835099697113}]}, {"text": "This corpus contains an annotation for protein-protein interactions.", "labels": [], "entities": []}, {"text": "From this corpus we collected 69 discourse relations.", "labels": [], "entities": []}, {"text": "For both ARG1 and ARG2 we performed two tests.", "labels": [], "entities": [{"text": "ARG1", "start_pos": 9, "end_pos": 13, "type": "DATASET", "confidence": 0.8976958990097046}, {"text": "ARG2", "start_pos": 18, "end_pos": 22, "type": "DATASET", "confidence": 0.8831995129585266}]}, {"text": "We measured from the argument heads how many protein mentions occurring within the argument extent (the True Positives) are found and how many protein mentions that lie beyond the argument extent (the False Positives) are found.", "labels": [], "entities": [{"text": "False", "start_pos": 201, "end_pos": 206, "type": "METRIC", "confidence": 0.9660699963569641}]}, {"text": "For ARG1, we found that our algorithm missed only one protein mention and incorrectly found three proteins from outside the argument extent, a precision of 98% and a recall of 99.32%.", "labels": [], "entities": [{"text": "ARG1", "start_pos": 4, "end_pos": 8, "type": "DATASET", "confidence": 0.836514949798584}, {"text": "precision", "start_pos": 143, "end_pos": 152, "type": "METRIC", "confidence": 0.9991649389266968}, {"text": "recall", "start_pos": 166, "end_pos": 172, "type": "METRIC", "confidence": 0.9996540546417236}]}, {"text": "For ARG2, we obtained a 100% precision and a 99% recall.", "labels": [], "entities": [{"text": "ARG2", "start_pos": 4, "end_pos": 8, "type": "DATASET", "confidence": 0.9072917103767395}, {"text": "precision", "start_pos": 29, "end_pos": 38, "type": "METRIC", "confidence": 0.9991455078125}, {"text": "recall", "start_pos": 49, "end_pos": 55, "type": "METRIC", "confidence": 0.9972041845321655}]}, {"text": "We conducted another experiment, which is similar to the previous one except that now instead of counting only the protein mentions, we counted all the words that can be reached from an argument head.", "labels": [], "entities": []}, {"text": "In other words, this experiment evaluates our algorithm in terms of how accurately it can identify the full argument extent (i.e., the words in it).", "labels": [], "entities": []}, {"text": "For ARG1 and ARG2 we got an F-score of 91.98% and 92.98% respectively.", "labels": [], "entities": [{"text": "ARG1", "start_pos": 4, "end_pos": 8, "type": "DATASET", "confidence": 0.9465100169181824}, {"text": "ARG2", "start_pos": 13, "end_pos": 17, "type": "DATASET", "confidence": 0.9404090046882629}, {"text": "F-score", "start_pos": 28, "end_pos": 35, "type": "METRIC", "confidence": 0.9997187256813049}]}], "tableCaptions": []}