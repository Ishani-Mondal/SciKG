{"title": [{"text": "Extractive Summarization and Dialogue Act Modeling on Email Threads: An Integrated Probabilistic Approach", "labels": [], "entities": [{"text": "Extractive Summarization", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8424745500087738}, {"text": "Dialogue Act Modeling on Email Threads", "start_pos": 29, "end_pos": 67, "type": "TASK", "confidence": 0.6942561666170756}]}], "abstractContent": [{"text": "In this paper, we present a novel supervised approach to the problem of summarizing email conversations and modeling dialogue acts.", "labels": [], "entities": [{"text": "summarizing email conversations", "start_pos": 72, "end_pos": 103, "type": "TASK", "confidence": 0.9133225282033285}]}, {"text": "We assume that there is a relationship between dialogue acts and important sentences.", "labels": [], "entities": []}, {"text": "Based on this assumption, we introduce a sequential graphical model approach which simultaneously summarizes email conversation and models dialogue acts.", "labels": [], "entities": []}, {"text": "We compare our model with sequential and non-sequential models, which independently conduct the tasks of extractive summari-zation and dialogue act modeling.", "labels": [], "entities": [{"text": "dialogue act modeling", "start_pos": 135, "end_pos": 156, "type": "TASK", "confidence": 0.6652209858099619}]}, {"text": "An empirical evaluation shows that our approach significantly outperforms all base-lines in classifying correct summary sentences without losing performance on dialogue act modeling task.", "labels": [], "entities": [{"text": "classifying correct summary sentences", "start_pos": 92, "end_pos": 129, "type": "TASK", "confidence": 0.7999746352434158}, {"text": "dialogue act modeling task", "start_pos": 160, "end_pos": 186, "type": "TASK", "confidence": 0.7588300853967667}]}], "introductionContent": [{"text": "Nowadays, an overwhelming amount of text information can be found on the web.", "labels": [], "entities": []}, {"text": "Most of this information is redundant and thus the task of document summarization has attracted much attention.", "labels": [], "entities": [{"text": "document summarization", "start_pos": 59, "end_pos": 81, "type": "TASK", "confidence": 0.5227340906858444}]}, {"text": "Since emails in particular are used fora wide variety of purposes, the process of automatically summarizing emails might be of great benefit in dealing with this excessive amount of information.", "labels": [], "entities": [{"text": "summarizing emails", "start_pos": 96, "end_pos": 114, "type": "TASK", "confidence": 0.8391009569168091}]}, {"text": "Much work has already been conducted on email summarization.", "labels": [], "entities": [{"text": "email summarization", "start_pos": 40, "end_pos": 59, "type": "TASK", "confidence": 0.7194976806640625}]}, {"text": "The first research on this topic was conducted by, who took a supervised learning approach to extracting important sentences.", "labels": [], "entities": []}, {"text": "A study on the supervised summarization of email threads was also performed by.", "labels": [], "entities": [{"text": "summarization of email threads", "start_pos": 26, "end_pos": 56, "type": "TASK", "confidence": 0.8645443171262741}]}, {"text": "This study used the regression-based method for classification.", "labels": [], "entities": [{"text": "classification", "start_pos": 48, "end_pos": 62, "type": "TASK", "confidence": 0.9610469937324524}]}, {"text": "There have been studies on unsupervised summarization of email threads as well.", "labels": [], "entities": [{"text": "summarization of email threads", "start_pos": 40, "end_pos": 70, "type": "TASK", "confidence": 0.841888353228569}]}, {"text": "proposed a graph-based unsupervised approach to email conversation summarization using clue words, i.e., recurring words contained in replies.", "labels": [], "entities": [{"text": "email conversation summarization", "start_pos": 48, "end_pos": 80, "type": "TASK", "confidence": 0.696684738000234}]}, {"text": "In addition, the task of labeling sentences with dialogue acts has become important and has been employed in many conversation analysis systems.", "labels": [], "entities": [{"text": "labeling sentences with dialogue acts", "start_pos": 25, "end_pos": 62, "type": "TASK", "confidence": 0.8445903539657593}, {"text": "conversation analysis", "start_pos": 114, "end_pos": 135, "type": "TASK", "confidence": 0.7364450097084045}]}, {"text": "For example, applications such as meeting summarization and collaborative task learning agents use dialogue acts as their underlying structure.", "labels": [], "entities": [{"text": "meeting summarization", "start_pos": 34, "end_pos": 55, "type": "TASK", "confidence": 0.8885693848133087}]}, {"text": "Ina previous work, defined a set of \"email acts\" and employed text classification methods to detect these acts in emails.", "labels": [], "entities": [{"text": "text classification", "start_pos": 62, "end_pos": 81, "type": "TASK", "confidence": 0.6943312585353851}]}, {"text": "employed a combination of n-gram sequences as features and then used a supervised machine learning method to improve the accuracy of this email act classification.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 121, "end_pos": 129, "type": "METRIC", "confidence": 0.999168872833252}, {"text": "email act classification", "start_pos": 138, "end_pos": 162, "type": "TASK", "confidence": 0.5700761675834656}]}, {"text": "In addition, presented unsupervised dialogue act labeling methods.", "labels": [], "entities": [{"text": "dialogue act labeling", "start_pos": 36, "end_pos": 57, "type": "TASK", "confidence": 0.6460209588209788}]}, {"text": "In their work, they introduced a graph-based method and two probabilistic sequence-labeling methods for modeling dialogue acts.", "labels": [], "entities": []}, {"text": "However, little work has been done on discovering the relationship between dialogue acts and extractive summaries.", "labels": [], "entities": []}, {"text": "If there is a relationship between them, combining these approaches so as to model both simultaneously will yield better results.", "labels": [], "entities": []}, {"text": "In this paper, we investigate this hypothesis by introducing anew sequential graphical model approach that performs dialogue act modeling and extractive summarization jointly on email threads.", "labels": [], "entities": [{"text": "dialogue act modeling", "start_pos": 116, "end_pos": 137, "type": "TASK", "confidence": 0.7385424574216207}, {"text": "extractive summarization", "start_pos": 142, "end_pos": 166, "type": "TASK", "confidence": 0.6220227181911469}]}], "datasetContent": [{"text": "In our experiment, the publically available BC3 corpus) is used for training and evaluation purposes.", "labels": [], "entities": [{"text": "BC3 corpus", "start_pos": 44, "end_pos": 54, "type": "DATASET", "confidence": 0.8647695183753967}]}, {"text": "The corpus contains email threads from the World Wide Web Consortium (W3C) mailing list.", "labels": [], "entities": [{"text": "World Wide Web Consortium (W3C) mailing list", "start_pos": 43, "end_pos": 87, "type": "DATASET", "confidence": 0.5184309581915537}]}, {"text": "It consists of 40 threads with an average of five emails per thread.", "labels": [], "entities": []}, {"text": "The corpus provides extractive summaries of each email thread, all of which were annotated by three annotators.", "labels": [], "entities": []}, {"text": "Hence, we use sentences that are selected by more than one annotator as the gold standard summary for each conversation.", "labels": [], "entities": []}, {"text": "In addition, all sentences in the 39 out of 40 threads are annotated for dialogue act tags.", "labels": [], "entities": [{"text": "dialogue act tags", "start_pos": 73, "end_pos": 90, "type": "TASK", "confidence": 0.680846373240153}]}, {"text": "The tagset consists of five general and 12 specific tags.", "labels": [], "entities": []}, {"text": "All of these tags are based on.", "labels": [], "entities": []}, {"text": "For our experiment, considering that our data is relatively small, we decide to use the coarser five tag set.", "labels": [], "entities": []}, {"text": "The details are shown in   After removing quoted sentences and redundant information such as senders and addresses, 1300 distinct sentences remain in the 39 email threads.", "labels": [], "entities": []}, {"text": "The detailed content of the corpus is summarized in.", "labels": [], "entities": []}, {"text": "No. of Threads 39 No. of Sentences 1300 No. of Extractive Summary Sentences 521 No. of S Sentences 959 No. of Q Sentences 103 No. of R Sentences 68 No. of Su Sentences 73 No. of M Sentences 97: Detailed content of the BC3 corpus  Here, we introduce evaluation metrics for our joint model of extractive summarization and dialogue act recognition.", "labels": [], "entities": [{"text": "BC3 corpus", "start_pos": 218, "end_pos": 228, "type": "DATASET", "confidence": 0.8917713165283203}, {"text": "extractive summarization", "start_pos": 291, "end_pos": 315, "type": "TASK", "confidence": 0.605889081954956}, {"text": "dialogue act recognition", "start_pos": 320, "end_pos": 344, "type": "TASK", "confidence": 0.6728378136952718}]}, {"text": "The CRF model has been shown to be the effective one in both dialogue act modeling and extractive summarization.", "labels": [], "entities": [{"text": "dialogue act modeling", "start_pos": 61, "end_pos": 82, "type": "TASK", "confidence": 0.8174812992413839}, {"text": "extractive summarization", "start_pos": 87, "end_pos": 111, "type": "TASK", "confidence": 0.810863733291626}]}, {"text": "Hence, for comparison, we implement two different CRFs, one for extractive summarization and the other for dialogue act modeling.", "labels": [], "entities": [{"text": "extractive summarization", "start_pos": 64, "end_pos": 88, "type": "TASK", "confidence": 0.755001574754715}, {"text": "dialogue act modeling", "start_pos": 107, "end_pos": 128, "type": "TASK", "confidence": 0.8281399408976237}]}, {"text": "When classifying extractive summaries using the CRF, we only use its extractive summarization features.", "labels": [], "entities": [{"text": "classifying extractive summaries", "start_pos": 5, "end_pos": 37, "type": "TASK", "confidence": 0.6208375493685404}]}, {"text": "Similarly, when modeling dialogue acts, we only use its dialogue act features.", "labels": [], "entities": []}, {"text": "In addition, we also com-pare our system with a non-sequential classifier, a support vector machine (SVM), with the same settings as those described above.", "labels": [], "entities": []}, {"text": "For these implementations, we use Mallet and SVM-light package 3.", "labels": [], "entities": [{"text": "Mallet", "start_pos": 34, "end_pos": 40, "type": "DATASET", "confidence": 0.9602503776550293}]}, {"text": "In our experiment, we first measure separately the performance of extractive summarization and dialogue act modeling.", "labels": [], "entities": [{"text": "extractive summarization", "start_pos": 66, "end_pos": 90, "type": "TASK", "confidence": 0.6130922436714172}, {"text": "dialogue act modeling", "start_pos": 95, "end_pos": 116, "type": "TASK", "confidence": 0.7019127806027731}]}, {"text": "The performance of extractive summarization is measured by its averaged precision, recall, and F-measure.", "labels": [], "entities": [{"text": "extractive summarization", "start_pos": 19, "end_pos": 43, "type": "TASK", "confidence": 0.6608543694019318}, {"text": "precision", "start_pos": 72, "end_pos": 81, "type": "METRIC", "confidence": 0.9847438931465149}, {"text": "recall", "start_pos": 83, "end_pos": 89, "type": "METRIC", "confidence": 0.9996495246887207}, {"text": "F-measure", "start_pos": 95, "end_pos": 104, "type": "METRIC", "confidence": 0.9981129169464111}]}, {"text": "For dialogue acts, we report the averaged-micro and macro accuracies as well as the averaged accuracies of each dialogue act.", "labels": [], "entities": []}, {"text": "Second, we evaluate the combined performance of extractive summarization and dialogue act modeling tasks.", "labels": [], "entities": [{"text": "extractive summarization", "start_pos": 48, "end_pos": 72, "type": "TASK", "confidence": 0.6429179012775421}, {"text": "dialogue act modeling", "start_pos": 77, "end_pos": 98, "type": "TASK", "confidence": 0.717926025390625}]}, {"text": "In general, we are interested in the dialogue acts in summary sentences because they can be later used as input for other natural language processing applications such as automatic abstractive summarization.", "labels": [], "entities": [{"text": "automatic abstractive summarization", "start_pos": 171, "end_pos": 206, "type": "TASK", "confidence": 0.5256626208623251}]}, {"text": "Therefore, we measure the performance of our model with the following modified precision (Pre'), recall (Rec'), and F-measure (F'): where a correctly classified sentence refers to a true summary sentence that is classified as such and whose dialogue acts are also correctly classified.", "labels": [], "entities": [{"text": "precision", "start_pos": 79, "end_pos": 88, "type": "METRIC", "confidence": 0.9969694018363953}, {"text": "Pre", "start_pos": 90, "end_pos": 93, "type": "METRIC", "confidence": 0.923015832901001}, {"text": "recall (Rec')", "start_pos": 97, "end_pos": 110, "type": "METRIC", "confidence": 0.7933967262506485}, {"text": "F-measure (F')", "start_pos": 116, "end_pos": 130, "type": "METRIC", "confidence": 0.7956590503454208}]}, {"text": "For all cases, we run five sets of 10-fold cross validation to train and test the classifiers on a shuffled dataset and calculate the average of the results.", "labels": [], "entities": []}, {"text": "For each cross validation run, we extract all features following the process described in Section 4 on the training set.", "labels": [], "entities": []}, {"text": "When comparing these two baselines with our model, we report pvalues obtained from a student paired t-test on the results to determine their significance.", "labels": [], "entities": []}, {"text": "3 http://www.cs.cornell.edu/people/tj/svm_light", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 5: A comparison of the extractive summariza- tion performance of our DCRF model and the two  baselines based on precision, recall, and F-measure", "labels": [], "entities": [{"text": "precision", "start_pos": 119, "end_pos": 128, "type": "METRIC", "confidence": 0.999575674533844}, {"text": "recall", "start_pos": 130, "end_pos": 136, "type": "METRIC", "confidence": 0.9984310269355774}, {"text": "F-measure", "start_pos": 142, "end_pos": 151, "type": "METRIC", "confidence": 0.9976633787155151}]}, {"text": " Table 6: A comparison of the dialogue act modeling  performance of our DCRF model and the two base- lines based on averaged accuracies", "labels": [], "entities": [{"text": "dialogue act modeling", "start_pos": 30, "end_pos": 51, "type": "TASK", "confidence": 0.7114201188087463}]}, {"text": " Table 7: A comparison of the overall performance of  our DCRF model and the two baselines based on  modified precision, recall and F-measure", "labels": [], "entities": [{"text": "precision", "start_pos": 110, "end_pos": 119, "type": "METRIC", "confidence": 0.9994404911994934}, {"text": "recall", "start_pos": 121, "end_pos": 127, "type": "METRIC", "confidence": 0.9996463060379028}, {"text": "F-measure", "start_pos": 132, "end_pos": 141, "type": "METRIC", "confidence": 0.9966334700584412}]}]}