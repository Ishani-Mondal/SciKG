{"title": [{"text": "Large-scale Exact Decoding: The IMS-TTT submission to WMT14 *", "labels": [], "entities": [{"text": "Exact Decoding", "start_pos": 12, "end_pos": 26, "type": "TASK", "confidence": 0.9213260412216187}, {"text": "IMS-TTT submission to WMT14", "start_pos": 32, "end_pos": 59, "type": "DATASET", "confidence": 0.7146206051111221}]}], "abstractContent": [{"text": "We present the IMS-TTT submission to WMT14, an experimental statistical tree-to-tree machine translation system based on the multi-bottom up tree transducer including rule extraction, tuning and decoding.", "labels": [], "entities": [{"text": "WMT14", "start_pos": 37, "end_pos": 42, "type": "DATASET", "confidence": 0.8181638717651367}, {"text": "statistical tree-to-tree machine translation", "start_pos": 60, "end_pos": 104, "type": "TASK", "confidence": 0.5912235528230667}, {"text": "rule extraction", "start_pos": 167, "end_pos": 182, "type": "TASK", "confidence": 0.7338780909776688}]}, {"text": "Thanks to input parse forests and a \"no pruning\" strategy during decoding, the obtained translations are competitive.", "labels": [], "entities": []}, {"text": "The drawbacks area restricted coverage of 70% on test data, in part due to exact input parse tree matching, and a relatively high runtime.", "labels": [], "entities": [{"text": "coverage", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9606112837791443}, {"text": "exact input parse tree matching", "start_pos": 75, "end_pos": 106, "type": "TASK", "confidence": 0.5643904864788055}]}, {"text": "Advantages include easy redecoding with a different weight vector, since the full translation forests can be stored after the first decoding pass.", "labels": [], "entities": []}], "introductionContent": [{"text": "In this contribution, we present an implementation of a translation model that is based on MBOT (the multi bottom-up tree transducer of and).", "labels": [], "entities": [{"text": "MBOT", "start_pos": 91, "end_pos": 95, "type": "METRIC", "confidence": 0.5358377695083618}]}, {"text": "Intuitively, an MBOT is asynchronous tree sequence substitution grammar (STSSG,;;) that has discontiguities only on the target side.", "labels": [], "entities": []}, {"text": "From an algorithmic point of view, this makes the MBOT more appealing than STSSG as demonstrated by.", "labels": [], "entities": [{"text": "MBOT", "start_pos": 50, "end_pos": 54, "type": "DATASET", "confidence": 0.7153613567352295}]}, {"text": "Formally, MBOT is expressive enough to express all sensible translations (Maletti, 2012) . displays sample rules of the MBOT variant, called MBOT, * This work was supported by Deutsche Forschungsgemeinschaft grants Models of Morphosyntax for Statistical Machine Translation (Phase 2) and MA/4959/1-1.", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 242, "end_pos": 273, "type": "TASK", "confidence": 0.7111356655756632}, {"text": "MA/4959/1-1", "start_pos": 288, "end_pos": 299, "type": "DATASET", "confidence": 0.9087493300437928}]}, {"text": "A translation is sensible if it is of linear size increase and can be computed by some (potentially copying) top-down tree transducer. that we use (in a graphical representation of the trees and the alignment).", "labels": [], "entities": []}, {"text": "Recently, a shallow version of MBOT has been integrated into the popular Moses toolkit (.", "labels": [], "entities": [{"text": "MBOT", "start_pos": 31, "end_pos": 35, "type": "DATASET", "confidence": 0.8425566554069519}]}, {"text": "Our implementation is exact in the sense that it does absolutely no pruning during decoding and thus preserves all translation candidates, while having no mechanism to handle unknown structures.", "labels": [], "entities": []}, {"text": "(We added dummy rules that leave unseen lexical material untranslated.)", "labels": [], "entities": []}, {"text": "The coverage is thus limited, but still considerably high.", "labels": [], "entities": [{"text": "coverage", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9737609624862671}]}, {"text": "Source-side and targetside syntax restrict the search space so that decoding stays tractable.", "labels": [], "entities": []}, {"text": "Only the language model scoring is implemented as a separate reranker 2 . This has several advantages: (1) We can use input parse forests (.", "labels": [], "entities": []}, {"text": "(2) Not only is the output optimal with regard to the theoretical model, also the space of translation candidates can be efficiently stored as a weighted regular tree grammar.", "labels": [], "entities": []}, {"text": "The best translations can then be extracted using the k-best algorithm by.", "labels": [], "entities": []}, {"text": "Rule weights can be changed without the need for explicit redecoding, the parameters of the log-linear model can be changed, and even new features can be added.", "labels": [], "entities": []}, {"text": "These properties are especially helpful in tuning, where only the k-best algorithm has to be re-run in each iteration.", "labels": [], "entities": [{"text": "tuning", "start_pos": 43, "end_pos": 49, "type": "TASK", "confidence": 0.9588013887405396}]}, {"text": "A model in similar spirit has been described by; however, it used target syntax only (using a top-down tree-to-string transducer backwards), and was restricted to sentences of length at most 25.", "labels": [], "entities": []}, {"text": "We do not make such restrictions.", "labels": [], "entities": []}, {"text": "The theoretical aspects of MBOT and their use in our translation model are presented in Section 2.", "labels": [], "entities": [{"text": "MBOT", "start_pos": 27, "end_pos": 31, "type": "DATASET", "confidence": 0.8066461682319641}, {"text": "translation", "start_pos": 53, "end_pos": 64, "type": "TASK", "confidence": 0.9670889377593994}]}, {"text": "Based on this, we implemented a machine translation system that we are going to make available to the public.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 32, "end_pos": 51, "type": "TASK", "confidence": 0.7170863896608353}]}, {"text": "Section 4 presents the most important components of our MBOT implementation, and Section 5 presents our submission to the WMT14 shared translation task.", "labels": [], "entities": [{"text": "MBOT implementation", "start_pos": 56, "end_pos": 75, "type": "TASK", "confidence": 0.554555356502533}, {"text": "WMT14 shared translation task", "start_pos": 122, "end_pos": 151, "type": "TASK", "confidence": 0.7187455594539642}]}], "datasetContent": [{"text": "We used the training data that was made available for the WMT14 shared translation task on English-German 8 . It consists of three parallel corpora (1.9M sentences of European parliament proceedings, 201K sentences of newswire text, and 2M sentences of web text) and additional monolingual news data for language model training.", "labels": [], "entities": [{"text": "WMT14 shared translation task", "start_pos": 58, "end_pos": 87, "type": "TASK", "confidence": 0.7412175834178925}]}, {"text": "The English half of the parallel data was parsed using Egret 9 which is a re-implementation of the Berkeley parser ().", "labels": [], "entities": []}, {"text": "For the German parse, we used the BitPar parser).", "labels": [], "entities": []}, {"text": "The BitPar German grammar is highly detailed, which makes the syntactic information contained in the parses extremely useful.", "labels": [], "entities": [{"text": "BitPar German grammar", "start_pos": 4, "end_pos": 25, "type": "DATASET", "confidence": 0.9283221960067749}]}, {"text": "Part-of-speech tags and category label are augmented by case, number and gender information, as can be seen in the German parse tree in.", "labels": [], "entities": [{"text": "German parse tree", "start_pos": 115, "end_pos": 132, "type": "DATASET", "confidence": 0.8165911237398783}]}, {"text": "We only kept the best parse for each sentence during training.", "labels": [], "entities": []}, {"text": "After parsing, we prepared three versions of the German corpus: a) RAW, with no morphological post-processing; b) UNSPLIT, using SMOR, a rule-based morphological analyser ( ), to reduce words to their base form; c) SPLIT, using SMOR to reduce words to their base form and split compound nouns.", "labels": [], "entities": [{"text": "German corpus", "start_pos": 49, "end_pos": 62, "type": "DATASET", "confidence": 0.8998059034347534}, {"text": "RAW", "start_pos": 67, "end_pos": 70, "type": "METRIC", "confidence": 0.7032813429832458}, {"text": "UNSPLIT", "start_pos": 114, "end_pos": 121, "type": "METRIC", "confidence": 0.8923225998878479}, {"text": "SPLIT", "start_pos": 215, "end_pos": 220, "type": "TASK", "confidence": 0.7039422392845154}]}, {"text": "After translation, compounds were merged again, and words were re-inflected.", "labels": [], "entities": []}, {"text": "Previous experiments using SMOR to lemmatise and split compounds in phrase-based SMT showed improved translation performances, see (Cap et al., 2014a) for details.", "labels": [], "entities": [{"text": "SMT", "start_pos": 81, "end_pos": 84, "type": "TASK", "confidence": 0.68129962682724}]}, {"text": "We then trained three 5-gram language models on monolingual data using KenLM to appear) for the three setups.", "labels": [], "entities": []}, {"text": "For SPLIT and UNSPLIT, we were only able to use the German side of the parallel data, since parsing is a prerequisite for our morphological post-processing and we did not have the resources to parse more data.", "labels": [], "entities": [{"text": "UNSPLIT", "start_pos": 14, "end_pos": 21, "type": "DATASET", "confidence": 0.8150526881217957}, {"text": "parsing", "start_pos": 92, "end_pos": 99, "type": "TASK", "confidence": 0.9738847613334656}]}, {"text": "For RAW, we additionally used the monolingual German data that was distributed for the shared task.", "labels": [], "entities": [{"text": "RAW", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.8672555088996887}]}, {"text": "Word alignment for all three setups was achieved using GIZA++ . As usual, we discarded sentence pairs where one sentence was significantly longer than the other, as well as those that were too long or too short.", "labels": [], "entities": [{"text": "Word alignment", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.755422830581665}]}, {"text": "For tuning, we chose the WMT12 test set (3,003 sentences of newswire text), available as part of the development data for the WMT13 shared translation task.", "labels": [], "entities": [{"text": "WMT12 test set", "start_pos": 25, "end_pos": 39, "type": "DATASET", "confidence": 0.9753928184509277}, {"text": "WMT13 shared translation task", "start_pos": 126, "end_pos": 155, "type": "TASK", "confidence": 0.8010230958461761}]}, {"text": "Since our system had limited coverage on this tuning set, we limited ourselves to the first a subset of sentences we could translate.", "labels": [], "entities": []}, {"text": "When translating the test set, our models used parse trees delivered by the Egret parser.", "labels": [], "entities": []}, {"text": "After translation, recasing was done by examining the output syntax tree, using a simple heuristics looking for nouns and sentence boundaries.", "labels": [], "entities": []}, {"text": "Since coverage on the test set was also limited, we used the systems as described in as a fallback to translate sentences that our system was notable to translate.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: BLEU and TER scores of the submitted  systems.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9992190599441528}, {"text": "TER scores", "start_pos": 19, "end_pos": 29, "type": "METRIC", "confidence": 0.959455132484436}]}]}