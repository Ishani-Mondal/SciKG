{"title": [{"text": "GWU-HASP: Hybrid Arabic Spelling and Punctuation Corrector 1", "labels": [], "entities": [{"text": "GWU-HASP", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.8530936241149902}, {"text": "Hybrid Arabic Spelling and Punctuation Corrector", "start_pos": 10, "end_pos": 58, "type": "TASK", "confidence": 0.6965591410795847}]}], "abstractContent": [{"text": "In this paper, we describe our Hybrid Ar-abic Spelling and Punctuation Corrector (HASP).", "labels": [], "entities": [{"text": "Ar-abic Spelling and Punctuation Corrector (HASP)", "start_pos": 38, "end_pos": 87, "type": "METRIC", "confidence": 0.7456184774637222}]}, {"text": "HASP was one of the systems participating in the QALB-2014 Shared Task on Arabic Error Correction.", "labels": [], "entities": [{"text": "HASP", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8234862089157104}, {"text": "QALB-2014 Shared Task on Arabic Error Correction", "start_pos": 49, "end_pos": 97, "type": "TASK", "confidence": 0.6464620402881077}]}, {"text": "The system uses a CRF (Conditional Random Fields) classifier for correcting punctuation errors, an open-source dictionary (or word list) for detecting errors and generating and filtering candidates, an n-gram language model for selecting the best candidates, and a set of deterministic rules for text normalization (such as removing diacritics and kashida and converting Hindi numbers into Arabic numerals).", "labels": [], "entities": [{"text": "text normalization", "start_pos": 296, "end_pos": 314, "type": "TASK", "confidence": 0.72918701171875}]}, {"text": "We also experiment with word alignment for spelling correction at the character level and report some preliminary results.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 24, "end_pos": 38, "type": "TASK", "confidence": 0.7805943191051483}, {"text": "spelling correction", "start_pos": 43, "end_pos": 62, "type": "TASK", "confidence": 0.7914040088653564}]}], "introductionContent": [{"text": "In this paper 1 we describe our system for Arabic spelling error detection and correction, Hybrid Arabic Spelling and Punctuation Corrector (HASP).", "labels": [], "entities": [{"text": "Arabic spelling error detection and correction", "start_pos": 43, "end_pos": 89, "type": "TASK", "confidence": 0.5982272326946259}, {"text": "Hybrid Arabic Spelling and Punctuation Corrector (HASP)", "start_pos": 91, "end_pos": 146, "type": "TASK", "confidence": 0.7160449160469903}]}, {"text": "We participate with HASP in the QALB-2014 Shared Task on Arabic Error Correction () as part of the Arabic Natural Language Processing Workshop (ANLP) taking place at EMNLP 2014.", "labels": [], "entities": [{"text": "QALB-2014 Shared Task on Arabic Error Correction", "start_pos": 32, "end_pos": 80, "type": "TASK", "confidence": 0.5337897879736764}, {"text": "Arabic Natural Language Processing Workshop (ANLP)", "start_pos": 99, "end_pos": 149, "type": "TASK", "confidence": 0.585422720760107}]}, {"text": "The shared task data deals with \"errors\" in the general sense which comprise: a) punctuation errors; b) non-word errors; c) real-word spelling errors; d) grammatical errors; and, e) orthographical errors such as elongation (kashida) and speech effects such as character multiplication for emphasis.", "labels": [], "entities": []}, {"text": "HASP in its current stage only handles types (a), (b), and (e) errors.", "labels": [], "entities": [{"text": "HASP", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8091500401496887}]}, {"text": "We assume that the various error types are too distinct to be treated with the same computational technique.", "labels": [], "entities": []}, {"text": "Therefore, we treat each problem separately, and for each problem we select the approach that seems most efficient, and ultimately all components are integrated in a single framework.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1. Distribution Statistics on Error Types", "labels": [], "entities": [{"text": "Distribution", "start_pos": 10, "end_pos": 22, "type": "TASK", "confidence": 0.9160009026527405}]}, {"text": " Table 3. Yamcha results on the development set", "labels": [], "entities": []}, {"text": " Table 4. CRF results on the development set  * with full tokens; other experiments use stems  only, i.e., clitics are removed.", "labels": [], "entities": []}, {"text": " Table 5. Merged words and their splits", "labels": [], "entities": [{"text": "Merged words and their splits", "start_pos": 10, "end_pos": 39, "type": "TASK", "confidence": 0.7075694859027862}]}, {"text": " Table 6. Results of dictionary error detection", "labels": [], "entities": [{"text": "dictionary error detection", "start_pos": 21, "end_pos": 47, "type": "TASK", "confidence": 0.6607806285222372}]}, {"text": " Table 7. Error types in the training set", "labels": [], "entities": [{"text": "Error", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9510424137115479}]}, {"text": " Table 8. The option split-1 refers  to using the splitting algorithm \u00ed \u00b5\u00ed\u00b1\u0099 \u2212 3 as ex- plained in Section 2.3.1, while split-2 refers to  using the splitting algorithm 2 !!! .", "labels": [], "entities": [{"text": "splitting algorithm \u00ed \u00b5\u00ed\u00b1\u0099 \u2212 3", "start_pos": 50, "end_pos": 80, "type": "METRIC", "confidence": 0.6923782314573016}]}, {"text": " Table 8. LM correction with 3 candidates", "labels": [], "entities": [{"text": "LM correction", "start_pos": 10, "end_pos": 23, "type": "TASK", "confidence": 0.5807717442512512}]}, {"text": " Table 9. Final official results on the test set pro- vided by the Shared Task", "labels": [], "entities": []}, {"text": " Table 10. Results of character-based alignment", "labels": [], "entities": [{"text": "character-based alignment", "start_pos": 22, "end_pos": 47, "type": "TASK", "confidence": 0.8370546400547028}]}]}