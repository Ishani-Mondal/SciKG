{"title": [{"text": "Introduction to BIT Chinese Spelling Correction System at CLP 2014 Bake-off", "labels": [], "entities": [{"text": "BIT Chinese Spelling Correction", "start_pos": 16, "end_pos": 47, "type": "TASK", "confidence": 0.6545104756951332}, {"text": "CLP 2014", "start_pos": 58, "end_pos": 66, "type": "DATASET", "confidence": 0.768091082572937}, {"text": "Bake-off", "start_pos": 67, "end_pos": 75, "type": "TASK", "confidence": 0.7842475771903992}]}], "abstractContent": [{"text": "This paper describes the Chinese spelling correction system submitted by BIT at CLP Bake-off 2014 task 2.", "labels": [], "entities": [{"text": "Chinese spelling correction", "start_pos": 25, "end_pos": 52, "type": "TASK", "confidence": 0.6581485569477081}, {"text": "BIT at CLP Bake-off 2014 task 2", "start_pos": 73, "end_pos": 104, "type": "DATASET", "confidence": 0.8437814286776951}]}, {"text": "The system mainly includes two parts: 1) N-gram model is adopted to retrieve the non-words which are wrongly separated byword segmentation.", "labels": [], "entities": [{"text": "byword segmentation", "start_pos": 119, "end_pos": 138, "type": "TASK", "confidence": 0.7603535652160645}]}, {"text": "The non-words are then corrected in terms of word frequency , pronunciation similarity, shape similarity and POS (part of speech) tag.", "labels": [], "entities": [{"text": "POS", "start_pos": 109, "end_pos": 112, "type": "METRIC", "confidence": 0.8535816073417664}]}, {"text": "2) For wrong words, abnormal POS tag is used to indicate their location and dependency relation matching is employed to correct them.", "labels": [], "entities": [{"text": "POS tag", "start_pos": 29, "end_pos": 36, "type": "METRIC", "confidence": 0.8333833515644073}]}, {"text": "Experiment results demonstrate the effectiveness of our system.", "labels": [], "entities": []}], "introductionContent": [{"text": "Spelling check, which is an automatic mechanism to detect and correct human spelling errors, is a common task in every written language.", "labels": [], "entities": [{"text": "Spelling check", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7130043208599091}]}, {"text": "The number of people learning Chinese as a Foreign Language (CFL) is booming in recent decades and this number is expected to become even larger for the years to come.", "labels": [], "entities": [{"text": "Chinese as a Foreign Language (CFL)", "start_pos": 30, "end_pos": 65, "type": "TASK", "confidence": 0.7060845196247101}]}, {"text": "However, unlike English learning environment where many learning techniques have been developed, tools to support CFL learners are relatively rare, especially those that could automatically detect and correct Chinese spelling and grammatical errors.", "labels": [], "entities": []}, {"text": "For example, Microsoft Word \u00ae has not yet supported these functions for Chinese, although it supports English for years.", "labels": [], "entities": []}, {"text": "In CLP Bake-off 2014, essays written by CFL learners were collected for developing automatic spelling checkers.", "labels": [], "entities": [{"text": "CLP Bake-off 2014", "start_pos": 3, "end_pos": 20, "type": "DATASET", "confidence": 0.9498502214749655}, {"text": "automatic spelling checkers", "start_pos": 83, "end_pos": 110, "type": "TASK", "confidence": 0.6245974202950796}]}, {"text": "The aims are that through such evaluation campaigns, more innovative computer assisted techniques will be developed, more effective Chinese learning resources will be built, and the state-of-art NLP techniques will be advanced for the educational applications.", "labels": [], "entities": []}, {"text": "By analyzing the training data released by the CLP 2014 Bake-off task2 and the test data used in SIGHAN Bake-off 2013 2 , we find that the main errors focus on two types: One is wrong characters which result in \"non-words\" that are similar to OOV (out-of-vocabulary).", "labels": [], "entities": [{"text": "CLP 2014 Bake-off task2", "start_pos": 47, "end_pos": 70, "type": "DATASET", "confidence": 0.9250885099172592}, {"text": "SIGHAN Bake-off 2013 2", "start_pos": 97, "end_pos": 119, "type": "DATASET", "confidence": 0.8115500956773758}, {"text": "OOV", "start_pos": 243, "end_pos": 246, "type": "METRIC", "confidence": 0.9627764821052551}]}, {"text": "For example, the writer may misspell \"\u8eab\u908a\" as \"\u751f\u908a\", and \"\u6839\u64da\" as \"\u6839\u8655\" (The former appears because of the words' similar pronunciation and the latter comes up due to their similar shape).", "labels": [], "entities": []}, {"text": "These are even not words and of course do not exist in the vocabulary.", "labels": [], "entities": []}, {"text": "The other type is words which are correct in the dictionary but incorrect in the sentence.", "labels": [], "entities": []}, {"text": "Some of them maybe misspelled, like \"\u60c5 \u611b\" in phrase \"\u60c5\u611b\u7684\u738b\u5b9c\u5bb6\", which is a misspelling of word \"\u89aa\u611b\".", "labels": [], "entities": []}, {"text": "But we can find \"\u60c5\u611b\" in the dictionary and it is not a non-word.", "labels": [], "entities": []}, {"text": "Others are words which are not used correctly.", "labels": [], "entities": []}, {"text": "This usually happens when the writer does not understand their meaning clearly.", "labels": [], "entities": []}, {"text": "For example, writers often confuse \"\u5728\" and \"\u518d\", such as \"\u9ad8\u96c4\u662f \u518d\u53f0\u7063\u5357\u90e8\u4e00\u500b\u73fe\u4ee3\u5316\u57ce\u5e02\".", "labels": [], "entities": []}, {"text": "Here, it is \"\u5728\" but not \" \u518d \" the right one.", "labels": [], "entities": []}, {"text": "Different from non-words, we call these words \"wrong words\".", "labels": [], "entities": []}, {"text": "According to the statistics obtained from the training data of CLP 2014 Back-off, there are nearly 3,400 wrong words which are about twice more than non-words, 1,800 ones.", "labels": [], "entities": [{"text": "CLP 2014 Back-off", "start_pos": 63, "end_pos": 80, "type": "DATASET", "confidence": 0.8683700760205587}]}, {"text": "Spelling check and correction is a traditional task in natural language processing.", "labels": [], "entities": [{"text": "Spelling check and correction", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.7021103277802467}, {"text": "natural language processing", "start_pos": 55, "end_pos": 82, "type": "TASK", "confidence": 0.6406276722749075}]}, {"text": "Pollock and built a misspelling dictionary for spelling check.", "labels": [], "entities": [{"text": "spelling check", "start_pos": 47, "end_pos": 61, "type": "TASK", "confidence": 0.9438410699367523}]}, {"text": "Chang (1995) adopted a bi-gram language model to substitute the confusing character.", "labels": [], "entities": []}, {"text": "proposed an approximate word matching method to detect and correct spelling errors.", "labels": [], "entities": [{"text": "word matching", "start_pos": 24, "end_pos": 37, "type": "TASK", "confidence": 0.7026837915182114}]}, {"text": "Compared with the test data in SIGHAN Bake-off 2013, there are more wrong words and the text is more colloquial in the current Bake-off, which make the correction task more challenging.", "labels": [], "entities": [{"text": "SIGHAN Bake-off 2013", "start_pos": 31, "end_pos": 51, "type": "DATASET", "confidence": 0.8403778672218323}]}], "datasetContent": [{"text": "In this section, several experiments are conducted to verify the proposed methods described in Section 2.", "labels": [], "entities": []}, {"text": "The final official provided test dataset consists of 1,062 sentences with or without spelling errors in traditional Chinese.", "labels": [], "entities": []}, {"text": "Since the released training data are hardly employed to train models in our system, we regard it as a development set where some parameters are settled.", "labels": [], "entities": []}, {"text": "There are some different settings in our previous experiments on the development set (the released training data) and we apply three of them to the final test file.", "labels": [], "entities": []}, {"text": "BIT Run1: All modules are employed except the abnormal POS detection and dependency relation matching.", "labels": [], "entities": [{"text": "BIT", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.9397060871124268}, {"text": "POS detection", "start_pos": 55, "end_pos": 68, "type": "TASK", "confidence": 0.8381683230400085}, {"text": "dependency relation matching", "start_pos": 73, "end_pos": 101, "type": "TASK", "confidence": 0.6506606638431549}]}, {"text": "The threshold of the n-gram transfer probability at non-word retrieval step is set as 0.008.", "labels": [], "entities": []}, {"text": "The frequency threshold of the \"dependable\" word is set as 80.", "labels": [], "entities": []}, {"text": "That is to say the quasi non-word will not be retrieved if its \"dependable\" word appears less than 80 times in the dictionary.", "labels": [], "entities": []}, {"text": "BIT Run2: Abnormal POS detection and dependency relation matching are included.", "labels": [], "entities": [{"text": "BIT Run2", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.7069073915481567}, {"text": "POS detection", "start_pos": 19, "end_pos": 32, "type": "TASK", "confidence": 0.7031184881925583}, {"text": "dependency relation matching", "start_pos": 37, "end_pos": 65, "type": "TASK", "confidence": 0.6402825117111206}]}, {"text": "BIT Run3: \"De\" is a frequently used word in Chinese texts.", "labels": [], "entities": [{"text": "BIT Run3", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.7289658486843109}]}, {"text": "Due to the low parsing accuracy, plenty of \"De\" were wrongly replaced in our experiments.", "labels": [], "entities": [{"text": "parsing", "start_pos": 15, "end_pos": 22, "type": "TASK", "confidence": 0.955624520778656}, {"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9337354302406311}]}, {"text": "To avoid this type of noise, the heuristic rules about the correction of \"De\" are removed in Run3.", "labels": [], "entities": [{"text": "correction of \"De\"", "start_pos": 59, "end_pos": 77, "type": "METRIC", "confidence": 0.9338812589645386}, {"text": "Run3", "start_pos": 93, "end_pos": 97, "type": "DATASET", "confidence": 0.9642541408538818}]}, {"text": "Moreover, the transfer probability and the frequency threshold is changed to 0.001 and 100 respectively to tighten the retrieval.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: A summary of approaches and resources employed in our correction system", "labels": [], "entities": []}, {"text": " Table 2: The results of the submitted two runs", "labels": [], "entities": []}, {"text": " Table 3: The results of Run3", "labels": [], "entities": [{"text": "Run3", "start_pos": 25, "end_pos": 29, "type": "DATASET", "confidence": 0.9075992107391357}]}]}