{"title": [{"text": "Facilitating Multi-Lingual Sense Annotation: Human Mediated Lemmatizer", "labels": [], "entities": [{"text": "Facilitating Multi-Lingual Sense Annotation", "start_pos": 0, "end_pos": 43, "type": "TASK", "confidence": 0.6704904362559319}]}], "abstractContent": [{"text": "Sense marked corpora is essential for supervised word sense disambiguation (WSD).", "labels": [], "entities": [{"text": "supervised word sense disambiguation (WSD)", "start_pos": 38, "end_pos": 80, "type": "TASK", "confidence": 0.7576227613857814}]}, {"text": "The marked sense ids come from wordnets.", "labels": [], "entities": []}, {"text": "However , words in corpora appear in morphed forms, while wordnets store lemma.", "labels": [], "entities": []}, {"text": "This situation calls for accurate lemmatizers.", "labels": [], "entities": []}, {"text": "The lemma is the gateway to the wordnet.", "labels": [], "entities": []}, {"text": "However , the problem is that for many languages, lemmatizers do not exist, and this problem is not easy to solve, since rule based lemmatiz-ers take time and require highly skilled linguists .Satistical stemmers on the other hand do not return legitimate lemma.", "labels": [], "entities": []}, {"text": "We present here a novel scheme for creating accurate lemmatizers quickly.", "labels": [], "entities": []}, {"text": "These lemma-tizers are human mediated.", "labels": [], "entities": []}, {"text": "The key idea is that a trie is created out of the vocabulary of the language.", "labels": [], "entities": []}, {"text": "The lemmatizing process consists in navigating the trie, trying to find a match between the input word and an entry in the trie.", "labels": [], "entities": []}, {"text": "At the point of first mismatch, the yield of the subtree rooted at the partially matched node is output as the list of possible lemma.", "labels": [], "entities": []}, {"text": "If the correct lemma does not appear in the list-as noted by a human lexicographer-backtracking is initiated.", "labels": [], "entities": []}, {"text": "This can output more possibilities.", "labels": [], "entities": []}, {"text": "A ranking function filters and orders the output list of lemma.", "labels": [], "entities": []}, {"text": "We have evaluated the performance of this human mediated lemmatizer for eighteen In-dian Languages and five European languages.", "labels": [], "entities": []}, {"text": "We have compared accuracy values against well known lemmatizers/stemmers like Mor-pha, Morfessor and Snowball stemmers, and observed superior performance in all cases.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.9994348883628845}, {"text": "Snowball stemmers", "start_pos": 101, "end_pos": 118, "type": "DATASET", "confidence": 0.9094478189945221}]}, {"text": "Our work shows away of speedily creating human assisted accurate lemmatizers, thereby removing a difficult roadblock in many NLP tasks, e.g., sense annotation.", "labels": [], "entities": [{"text": "sense annotation", "start_pos": 142, "end_pos": 158, "type": "TASK", "confidence": 0.7505761981010437}]}], "introductionContent": [{"text": "Supervised WSD-the ruling paradigm for high accuracy sense determination-requires sense marked corpus in large quantity.", "labels": [], "entities": [{"text": "WSD-the", "start_pos": 11, "end_pos": 18, "type": "TASK", "confidence": 0.9400255084037781}, {"text": "sense determination-requires sense", "start_pos": 53, "end_pos": 87, "type": "TASK", "confidence": 0.6998738547166189}]}, {"text": "Sense annotation is a difficult job, requiring linguistic expertise, knowledge of the topic and domain, and most importantly a fine sense of word meanings.", "labels": [], "entities": [{"text": "Sense annotation", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.85787633061409}]}, {"text": "Most often the sense annotation task is accomplished by using a Sense Marker Tool, like the one described in.", "labels": [], "entities": [{"text": "sense annotation task", "start_pos": 15, "end_pos": 36, "type": "TASK", "confidence": 0.785636822382609}]}, {"text": "This particular tool, equipped with an easy to use GUI facilitates the task of manually marking each word with the correct sense of the word, as available in the wordnet of the language.", "labels": [], "entities": []}, {"text": "The tool has the wordnet sense repository resident in its memory.", "labels": [], "entities": []}, {"text": "It displays the senses of word for the human annotator to choose from.", "labels": [], "entities": []}, {"text": "The mentioned tool is extensively used by a number of language groups in India to produce high quality sense marked corpus.", "labels": [], "entities": []}, {"text": "shows the Sense Marker Tool, where a user is marking the sense of the word \"banks\" in English.", "labels": [], "entities": []}, {"text": "Now it is obvious that if the wordnet is to be accessed for the senses to be displayed, the lemma of the words must be available.", "labels": [], "entities": []}, {"text": "The lemma is the gateway to the wordnet.", "labels": [], "entities": []}, {"text": "Lemmatization is an important activity in many NLP applications.", "labels": [], "entities": [{"text": "Lemmatization", "start_pos": 0, "end_pos": 13, "type": "METRIC", "confidence": 0.7411528825759888}]}, {"text": "We stress at this point that our focus of attention is lemmatization and not stemming.", "labels": [], "entities": []}, {"text": "As a process, stemming aims to reduce a set of words into a cannonical form which mayor may not be a dictionary word of the language.", "labels": [], "entities": [{"text": "stemming", "start_pos": 14, "end_pos": 22, "type": "TASK", "confidence": 0.9782887697219849}]}, {"text": "Lemmatization, on the other hand, always produces a legal root word of the language.", "labels": [], "entities": []}, {"text": "To give an example, a stemmer can give rise to \"ladi\" from \"ladies\".", "labels": [], "entities": []}, {"text": "But a lemmatizer will have to produce \"lady\".", "labels": [], "entities": []}, {"text": "And if the senses of 'ladies' has to be found, the lemma 'lady' is required.", "labels": [], "entities": []}, {"text": "There are three basic approaches to lemmatization, viz., affix removal (rule based systems), statistical (supervised/unsupervised) and hybrid (rule based + statistical).", "labels": [], "entities": [{"text": "affix removal", "start_pos": 57, "end_pos": 70, "type": "TASK", "confidence": 0.7071566730737686}]}, {"text": "Developing a rule based system is an uphill task, requiring a number of language experts and enormous amount of time.", "labels": [], "entities": []}, {"text": "Purely statistical systems fail exploit linguistic features, and produce non-dictionary lexemes.", "labels": [], "entities": []}, {"text": "A hybrid system utilizes both the above mentioned approaches.", "labels": [], "entities": []}, {"text": "In this paper, we discuss an alternative approach to lemmatization which is quick, takes user help and is exact.", "labels": [], "entities": []}, {"text": "The key idea is that a trie is created out of the vocabulary of the language.", "labels": [], "entities": []}, {"text": "The lemmatizing process consists in navigating the trie, trying to find a match between the input word and an entry in the trie.", "labels": [], "entities": []}, {"text": "At the point of first mismatch-i.e., maximum prefix matching, the yield of the subtree rooted at the partially matched node is output as the list of possible lemma.", "labels": [], "entities": []}, {"text": "If the correct lemma does not appear in the list-as noted by a human lexicographer-a backtracking is initiated.", "labels": [], "entities": []}, {"text": "This can output more possibilities, since the yield of anode at a higher level of the trie is output.", "labels": [], "entities": []}, {"text": "A ranking function filters and orders the output list of lemma.", "labels": [], "entities": []}, {"text": "Our ultimate goal is to integrate the human mediated lemmatizer with the Sense Marking Tool.Currently, the tool supports the following 9 languages: English, Hindi, Marathi, Tamil, Telugu, Kannada, Malayalam, Bengali and Punjabi.", "labels": [], "entities": [{"text": "Sense Marking", "start_pos": 73, "end_pos": 86, "type": "TASK", "confidence": 0.7007764726877213}]}, {"text": "We give an example to give a feel for how our lemmatiser works.", "labels": [], "entities": []}, {"text": "When a linguist tries to mark the correct sense of the inflected Assamese word as shown in ('gharalai' meaning 'in the house'), in the current scenario no output is displayed, but with our lemmatizer integrated, it will show the possible lemma of that word.", "labels": [], "entities": []}, {"text": "Here, the correct lemma is shown at the top of the list, and the lemma can pullout the senses from the wordnet for the linguist to tag with.", "labels": [], "entities": []}, {"text": "The remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "We describe related work and background in section 2.", "labels": [], "entities": []}, {"text": "Section 3 explains the core of our human mediated lemmatiser.", "labels": [], "entities": []}, {"text": "Implementation details are in Section 4.", "labels": [], "entities": [{"text": "Implementation", "start_pos": 0, "end_pos": 14, "type": "METRIC", "confidence": 0.934185266494751}]}, {"text": "Experiments and results are discussed in Section 5.", "labels": [], "entities": [{"text": "Section 5", "start_pos": 41, "end_pos": 50, "type": "DATASET", "confidence": 0.8274596333503723}]}, {"text": "Comparison with existing stemmers/lemmatizers are in Section 6.", "labels": [], "entities": []}, {"text": "Section 7 concludes the paper and points to future directions.", "labels": [], "entities": []}], "datasetContent": [{"text": "We performed several experiments to evaluate the performance of the lemmatizer.", "labels": [], "entities": []}, {"text": "The basis of our: Online Interface -Language Independent Human Mediated Lemmatizer evaluation was that for every inflected input word, a set of output root forms will be generated by the lemmatizer.", "labels": [], "entities": []}, {"text": "Even if one of the result in the top 10 from this set matches the root in the gold standard, then we consider our result to be correct.", "labels": [], "entities": []}, {"text": "Following this approach the accuracy of inflected nouns undergoing stemming is very high.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.9993624091148376}]}, {"text": "Although, due to readjustment in verbs, for the first iteration without backtracking, our lemmatizer gives a fairly low accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 120, "end_pos": 128, "type": "METRIC", "confidence": 0.997511625289917}]}, {"text": "This can be further improved through backtracking, when we traversed the trie, one level up at a time.", "labels": [], "entities": []}, {"text": "The first iteration of stemming gives result among the best five outputs and the backtracking approach gives among the best ten outputs.", "labels": [], "entities": []}, {"text": "Interestingly, for inflected words in Italian our lemmatizer performs better when we include the results of one level backtrack as compared to a non-backtrack variant.", "labels": [], "entities": []}, {"text": "The results for various languages are shown in based on the lemmatizer's default variant, i.e., without using backtrack feature.", "labels": [], "entities": []}, {"text": "We compared performance of our system against three most commonly used lemmatizers/stemmers, viz.", "labels": [], "entities": []}, {"text": "Morpha (), Snowball and Morfessor.", "labels": [], "entities": [{"text": "Snowball", "start_pos": 11, "end_pos": 19, "type": "DATASET", "confidence": 0.9812931418418884}]}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "Our lemmatizer works better than Morfessor for Hindi (up by 64%) and Marathi (up by 59%).", "labels": [], "entities": []}, {"text": "For English, our lemmatizer outperforms Snowball by almost 36% and Morfessor by almost 10%.", "labels": [], "entities": [{"text": "English", "start_pos": 4, "end_pos": 11, "type": "DATASET", "confidence": 0.8241211175918579}, {"text": "Snowball", "start_pos": 40, "end_pos": 48, "type": "DATASET", "confidence": 0.9829338192939758}]}, {"text": "Although, as an exception, Morpha lemmatizer works better by about 1% in this case.", "labels": [], "entities": []}, {"text": "Snowball and Morpha lemmatizers are not available for Indian Languages and thus the results are marked with 'NA'.", "labels": [], "entities": [{"text": "Snowball", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9206748008728027}]}, {"text": "Morfessor being statistical in nature does not capture all the linguistic and morphological phenomenons associated with Indian languages and Snowball and Morpha are strictly rule based in nature and do not use any linguistic resource to validate the output.", "labels": [], "entities": [{"text": "Snowball", "start_pos": 141, "end_pos": 149, "type": "DATASET", "confidence": 0.9330027103424072}]}, {"text": "We have, of course, compared our lemmatizer with in-house rule based Hindi and Marathi morph analyzers.", "labels": [], "entities": []}, {"text": "The Marathi Morphological Analyzer () has an accuracy of 72.18%, with a usability of 94.33%, whereas the in-house Hindi Morphological Analyzer 6 accuracy is close to 100% with average usability around 96.5% (Table 3).", "labels": [], "entities": [{"text": "Marathi Morphological Analyzer", "start_pos": 4, "end_pos": 34, "type": "DATASET", "confidence": 0.8556076884269714}, {"text": "accuracy", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.9994151592254639}, {"text": "accuracy", "start_pos": 145, "end_pos": 153, "type": "METRIC", "confidence": 0.9454770684242249}]}, {"text": "Here, usability is the percentage of total number of words analyzed out of total words in corpus.", "labels": [], "entities": []}, {"text": "However, these morph analysers have taken years to build with many false starts and false hits and misses.", "labels": [], "entities": []}, {"text": "Compared to this, our human medi-  Once it was realized that we need lemmatizers for accessing senses of words, the system was builtin no time.", "labels": [], "entities": []}, {"text": "It is the preparation of gold data, generation of accuracy values and comparison with existing systems that took time.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 50, "end_pos": 58, "type": "METRIC", "confidence": 0.9953061938285828}]}], "tableCaptions": [{"text": " Table 1: Precision calculation for output produced  by our lemmatizer based on the first variant, i.e.,  without using backtrack feature. * denotes manual  evaluation and # denotes one level backtracking.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9895897507667542}]}, {"text": " Table 2: Comparative Evaluation (precision values) of Human Mediated Lemmatizer [Without using  Backtracking] against other classic stemming systems like Morpha, Snowball and Morfessor", "labels": [], "entities": [{"text": "precision", "start_pos": 34, "end_pos": 43, "type": "METRIC", "confidence": 0.9924980998039246}, {"text": "Snowball", "start_pos": 163, "end_pos": 171, "type": "DATASET", "confidence": 0.9549700617790222}, {"text": "Morfessor", "start_pos": 176, "end_pos": 185, "type": "DATASET", "confidence": 0.6677310466766357}]}]}