{"title": [], "abstractContent": [{"text": "Many machine reading approaches, from shallow information extraction to deep semantic parsing, map natural language to symbolic representations of meaning.", "labels": [], "entities": [{"text": "shallow information extraction", "start_pos": 38, "end_pos": 68, "type": "TASK", "confidence": 0.6818618277708689}, {"text": "deep semantic parsing", "start_pos": 72, "end_pos": 93, "type": "TASK", "confidence": 0.72363809744517}]}, {"text": "Representations such as first-order logic capture the richness of natural language and support complex reasoning, but often fail in practice due to their reliance on logical background knowledge and the difficulty of scaling up inference.", "labels": [], "entities": []}, {"text": "In contrast, low-dimensional embeddings (i.e. distri-butional representations) are efficient and enable generalization, but it is unclear how reasoning with embeddings could support the full power of symbolic representations such as first-order logic.", "labels": [], "entities": []}, {"text": "In this proof-of-concept paper we address this by learning embeddings that simulate the behavior of first-order logic.", "labels": [], "entities": []}], "introductionContent": [{"text": "Much of the work in machine reading follows an approach that is, at its heart, symbolic: language is transformed, possibly in a probabilistic way, into a symbolic world model such as a relational database or a knowledge base of first-order formulae.", "labels": [], "entities": [{"text": "machine reading", "start_pos": 20, "end_pos": 35, "type": "TASK", "confidence": 0.7537754476070404}]}, {"text": "For example, a statistical relation extractor reads texts and populates relational tables ().", "labels": [], "entities": [{"text": "statistical relation extractor reads texts", "start_pos": 15, "end_pos": 57, "type": "TASK", "confidence": 0.7267260432243348}]}, {"text": "Likewise, a semantic parser can turn sentences into complex first-order logic statements ().", "labels": [], "entities": []}, {"text": "Several properties make symbolic representations of knowledge attractive as a target of machine reading.", "labels": [], "entities": [{"text": "machine reading", "start_pos": 88, "end_pos": 103, "type": "TASK", "confidence": 0.7434969842433929}]}, {"text": "They support a range of well understood symbolic reasoning processes, capture semantic concepts such as determiners, negations and tense, can be interpreted, edited and curated by humans to inject prior knowledge.", "labels": [], "entities": []}, {"text": "However, on practical applications fully symbolic approaches have often shown low recall (e.g.) as they are affected by the limited coverage of ontologies such as WordNet.", "labels": [], "entities": [{"text": "recall", "start_pos": 82, "end_pos": 88, "type": "METRIC", "confidence": 0.9993228912353516}, {"text": "WordNet", "start_pos": 163, "end_pos": 170, "type": "DATASET", "confidence": 0.9606127142906189}]}, {"text": "Moreover, due to their deterministic nature they often cannot cope with noise and uncertainty inherent to real world data, and inference with such representations is difficult to scale up.", "labels": [], "entities": []}, {"text": "Embedding-based approaches address some of the concerns above.", "labels": [], "entities": []}, {"text": "Here relational worlds are described using low-dimensional embeddings of entities and relations based on relational evidence in knowledge bases) or surfaceform relationships mentioned in text (.", "labels": [], "entities": []}, {"text": "To overcome the generalization bottleneck, these approaches learn to embed similar entities and relations as vectors close in distance.", "labels": [], "entities": []}, {"text": "Subsequently, unseen facts can be inferred by simple and efficient linear algebra operations (e.g. dot products).", "labels": [], "entities": []}, {"text": "The core argument against embeddings is their supposed inability to capture deeper semantics, and more complex patterns of reasoning such as those enabled by first-order logic ().", "labels": [], "entities": []}, {"text": "Here we argue that this does not need to be true.", "labels": [], "entities": []}, {"text": "We present an approach that enables us to learn low-dimensional embeddings such that the model behaves as if it follows a complex first-order reasoning process-but still operates in terms of simple vector and matrix representations.", "labels": [], "entities": []}, {"text": "In this view, machine reading becomes the process of taking (inherently symbolic) knowledge in language and injecting this knowledge into a sub-symbolic distributional world model.", "labels": [], "entities": [{"text": "machine reading", "start_pos": 14, "end_pos": 29, "type": "TASK", "confidence": 0.7973395586013794}]}, {"text": "For example, one could envision a semantic parser that turns a sentence into a first-order logic statement,  just to then inject this statement into the embeddings of relations and entities mentioned in the sentence.", "labels": [], "entities": []}, {"text": "We assume a domain of a set of entities, such as SMITH and CAMBRIDGE, and relations among these (e.g. profAt(\u00b7, \u00b7)).", "labels": [], "entities": [{"text": "SMITH", "start_pos": 49, "end_pos": 54, "type": "DATASET", "confidence": 0.7386766076087952}]}, {"text": "We start from a knowledge base of observed logical statements, e.g., profAt(SMITH, CAMBRIDGE) or \u2200x, y : profAt(x, y) =\u21d2 worksFor (x, y).", "labels": [], "entities": []}, {"text": "These statements can be extracted from text through information extraction (for factual statements), be the output from a semantic parsing (for first-order statements) or come from human curators or external knowledge bases.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 52, "end_pos": 74, "type": "TASK", "confidence": 0.7569211423397064}]}], "datasetContent": [{"text": "We perform experiments on synthetic data defined over 7 entity pairs and 6 relations.", "labels": [], "entities": []}, {"text": "We fix the embedding size k to 4 and train the model for 100 epochs using SGD with 2 -regularization on the values of the embeddings.", "labels": [], "entities": []}, {"text": "The learning rate and the regularization parameter are set to 0.05.", "labels": [], "entities": []}, {"text": "The left part of shows the observed (bold) and inferred truth values fora set of factual staments of the form R(p), mapped to R as discussed above.", "labels": [], "entities": []}, {"text": "Due to the generalization obtained by low-dimensional embeddings, the model infers that, for example, SMITH is an employee at CAMBRIDGE and DAVIES lives in LONDON.", "labels": [], "entities": [{"text": "SMITH", "start_pos": 102, "end_pos": 107, "type": "METRIC", "confidence": 0.7798042297363281}, {"text": "CAMBRIDGE", "start_pos": 126, "end_pos": 135, "type": "DATASET", "confidence": 0.921051561832428}, {"text": "LONDON", "start_pos": 156, "end_pos": 162, "type": "DATASET", "confidence": 0.7704302072525024}]}, {"text": "However, we would like the model to also capture that every professor works for his or her university  and that, when somebody is registered in a city, he or she also lives in that city.", "labels": [], "entities": []}, {"text": "When including such first-order constraints (right part of), the model's predictions improve concerning different aspects.", "labels": [], "entities": []}, {"text": "First, the model gets the implication right, demonstrating that the low-dimensional embeddings encode firstorder knowledge.", "labels": [], "entities": []}, {"text": "Second, this implication transitively improves the predictions on other columns (e.g. SMITH is an employee at CAMBRIDGE).", "labels": [], "entities": [{"text": "SMITH", "start_pos": 86, "end_pos": 91, "type": "DATASET", "confidence": 0.491772323846817}, {"text": "CAMBRIDGE", "start_pos": 110, "end_pos": 119, "type": "DATASET", "confidence": 0.8608484268188477}]}, {"text": "Third, the implication works indeed in an asymmetric way, e.g., the model does not predict that WILLIAMS is a professor at OXFORD just because she is working there.", "labels": [], "entities": [{"text": "OXFORD", "start_pos": 123, "end_pos": 129, "type": "DATASET", "confidence": 0.7405638098716736}]}], "tableCaptions": [{"text": " Table 1: Reconstructed matrix without (left) and with (right) the first-order constraints profAt =\u21d2  worksFor and registeredIn =\u21d2 livesIn. Predictions for training cells of factual constraints [R(p)] =  are shown in bold, and true and false test cells are denoted by and \u22a5 respectively.", "labels": [], "entities": []}]}