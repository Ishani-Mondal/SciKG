{"title": [{"text": "Expanding the Language model in a low-resource hybrid MT system", "labels": [], "entities": [{"text": "MT", "start_pos": 54, "end_pos": 56, "type": "TASK", "confidence": 0.9488493204116821}]}], "abstractContent": [{"text": "The present article investigates the fusion of different language models to improve translation accuracy.", "labels": [], "entities": [{"text": "translation", "start_pos": 84, "end_pos": 95, "type": "TASK", "confidence": 0.9544680714607239}, {"text": "accuracy", "start_pos": 96, "end_pos": 104, "type": "METRIC", "confidence": 0.6924503445625305}]}, {"text": "A hybrid MT system, recently-developed in the European Commission-funded PRESEMT project that combines example based MT and Statistical MT principles is used as a starting point.", "labels": [], "entities": [{"text": "MT", "start_pos": 9, "end_pos": 11, "type": "TASK", "confidence": 0.9776082634925842}, {"text": "MT", "start_pos": 117, "end_pos": 119, "type": "TASK", "confidence": 0.9457976222038269}, {"text": "Statistical MT", "start_pos": 124, "end_pos": 138, "type": "TASK", "confidence": 0.5342168509960175}]}, {"text": "In this article, the syntactically-defined phrasal language models (NPs, VPs etc.) used by this MT system are supplemented by n-gram language models to improve translation accuracy.", "labels": [], "entities": [{"text": "MT", "start_pos": 96, "end_pos": 98, "type": "TASK", "confidence": 0.9730967879295349}, {"text": "accuracy", "start_pos": 172, "end_pos": 180, "type": "METRIC", "confidence": 0.7766223549842834}]}, {"text": "For specific structural patterns, n-gram statistics are consulted to determine whether the pattern instantiations are corroborated.", "labels": [], "entities": []}, {"text": "Experiments indicate improvements in translation accuracy.", "labels": [], "entities": [{"text": "translation", "start_pos": 37, "end_pos": 48, "type": "TASK", "confidence": 0.9731236696243286}, {"text": "accuracy", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.874854326248169}]}], "introductionContent": [{"text": "Currently a major part of cutting-edge research in MT revolves around the statistical machine translation (SMT) paradigm.", "labels": [], "entities": [{"text": "MT", "start_pos": 51, "end_pos": 53, "type": "TASK", "confidence": 0.9946720004081726}, {"text": "statistical machine translation (SMT)", "start_pos": 74, "end_pos": 111, "type": "TASK", "confidence": 0.8063568621873856}]}, {"text": "SMT has been inspired by the use of statistical methods to create language models fora number of applications including speech recognition.", "labels": [], "entities": [{"text": "SMT", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.985176682472229}, {"text": "speech recognition", "start_pos": 120, "end_pos": 138, "type": "TASK", "confidence": 0.8117363154888153}]}, {"text": "A number of different translation models of increasing complexity and translation accuracy have been developed.", "labels": [], "entities": [{"text": "translation", "start_pos": 22, "end_pos": 33, "type": "TASK", "confidence": 0.9792159199714661}, {"text": "translation", "start_pos": 70, "end_pos": 81, "type": "TASK", "confidence": 0.9347550868988037}, {"text": "accuracy", "start_pos": 82, "end_pos": 90, "type": "METRIC", "confidence": 0.9494019746780396}]}, {"text": "Today, several packages for developing statistical language models are available for free use, including SRI (Stolke et al., 2011), thus supporting research into statistical methods.", "labels": [], "entities": [{"text": "SRI", "start_pos": 105, "end_pos": 108, "type": "METRIC", "confidence": 0.5471630692481995}]}, {"text": "A main reason for the widespread adoption of SMT is that it is directly amenable to new language pairs using the same algorithms.", "labels": [], "entities": [{"text": "SMT", "start_pos": 45, "end_pos": 48, "type": "TASK", "confidence": 0.9931557774543762}]}, {"text": "An integrated framework (MOSES) has been developed for the creation of SMT systems ( . The more recent developments of SMT are summarised by.", "labels": [], "entities": [{"text": "SMT", "start_pos": 71, "end_pos": 74, "type": "TASK", "confidence": 0.9904929995536804}, {"text": "SMT", "start_pos": 119, "end_pos": 122, "type": "TASK", "confidence": 0.9651543498039246}]}, {"text": "One particular advance in SMT has been the integration of syntactically motivated phrases in order to establish correspondences between source language (SL) and target language (TL) (.", "labels": [], "entities": [{"text": "SMT", "start_pos": 26, "end_pos": 29, "type": "TASK", "confidence": 0.9956862926483154}]}, {"text": "Recently SMT has been enhanced by using different levels of abstraction e.g. word, lemma or part-of-speech (PoS), in factored SMT models so as to improve SMT performance . The drawback of SMT is that SL-to-TL parallel corpora of the order of millions of tokens are required to extract meaningful models for translation.", "labels": [], "entities": [{"text": "SMT", "start_pos": 9, "end_pos": 12, "type": "TASK", "confidence": 0.989732563495636}, {"text": "SMT", "start_pos": 126, "end_pos": 129, "type": "TASK", "confidence": 0.9388893246650696}, {"text": "SMT", "start_pos": 154, "end_pos": 157, "type": "TASK", "confidence": 0.9945454597473145}, {"text": "SMT", "start_pos": 188, "end_pos": 191, "type": "TASK", "confidence": 0.9886808395385742}]}, {"text": "Such corpora are hard to obtain, particularly for less resourced languages.", "labels": [], "entities": []}, {"text": "For this reason, SMT researchers are increasingly investigating the extraction of information from monolingual corpora, including lexica), restructuring) and topic-specific information (.", "labels": [], "entities": [{"text": "SMT", "start_pos": 17, "end_pos": 20, "type": "TASK", "confidence": 0.9913220405578613}]}, {"text": "As an alternative to pure SMT, the use of less specialised but more readily available resources has been proposed.", "labels": [], "entities": [{"text": "SMT", "start_pos": 26, "end_pos": 29, "type": "TASK", "confidence": 0.991671085357666}]}, {"text": "Even if such approaches do not provide a translation quality as high as SMT, their ability to develop MT systems with very limited resources confers to them an important advantage.", "labels": [], "entities": [{"text": "SMT", "start_pos": 72, "end_pos": 75, "type": "TASK", "confidence": 0.9711112976074219}, {"text": "MT", "start_pos": 102, "end_pos": 104, "type": "TASK", "confidence": 0.9786286354064941}]}, {"text": "have proposed an MT method that requires no parallel text, but relies on a full-form bilingual dictionary and a decoder using long-range context.", "labels": [], "entities": [{"text": "MT", "start_pos": 17, "end_pos": 19, "type": "TASK", "confidence": 0.9913676381111145}]}, {"text": "Other systems using low-cost resources include METIS ( and METIS-II, which are based only on large monolingual corpora to translate SL texts.", "labels": [], "entities": []}, {"text": "Another recent trend in MT has been towards hybrid MT systems, which combine characteristics from multiple MT paradigms.", "labels": [], "entities": [{"text": "MT", "start_pos": 24, "end_pos": 26, "type": "TASK", "confidence": 0.9946407079696655}, {"text": "MT", "start_pos": 51, "end_pos": 53, "type": "TASK", "confidence": 0.9786807894706726}]}, {"text": "The idea is that by fusing characteristics from different paradigms, a better translation performance can be attained ().", "labels": [], "entities": []}, {"text": "In the present article, the PRESEMT hybrid MT method using predominantly monolingual corpora () is extended by integrating n-gram information to improve the translation accuracy.", "labels": [], "entities": [{"text": "PRESEMT hybrid MT", "start_pos": 28, "end_pos": 45, "type": "TASK", "confidence": 0.5751112699508667}, {"text": "accuracy", "start_pos": 169, "end_pos": 177, "type": "METRIC", "confidence": 0.8707391023635864}]}, {"text": "The focus of the article is on how to extract, as comprehensively as possible, information from monolingual corpora by combining multiple models, to allow a higher quality translation.", "labels": [], "entities": []}, {"text": "A review of the base MT system is performed in section 2.", "labels": [], "entities": [{"text": "MT", "start_pos": 21, "end_pos": 23, "type": "TASK", "confidence": 0.9706408381462097}]}, {"text": "The TL language model is then detailed, allowing new work to be presented in section 3.", "labels": [], "entities": []}, {"text": "More specifically, via an error analysis, ngram based extensions are proposed to augment the language model.", "labels": [], "entities": []}, {"text": "Experiments are presented in section 4 and discussed in section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "The experiments reported in the present article focus on the Greek -English language pair, the reason being that this is the language pair for which the most extensive experimentation has been reported for the PRESEMT system).", "labels": [], "entities": []}, {"text": "Thus, improvements in the translation accuracy will be more difficult to attain.", "labels": [], "entities": [{"text": "translation", "start_pos": 26, "end_pos": 37, "type": "TASK", "confidence": 0.9571435451507568}, {"text": "accuracy", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.9110672473907471}]}, {"text": "Two datasets are used to evaluate translation accuracy, a development set (dataset1) and a test set (dataset2), each containing 200 sentences of length ranging from 7 to 40 tokens.", "labels": [], "entities": [{"text": "translation", "start_pos": 34, "end_pos": 45, "type": "TASK", "confidence": 0.957320511341095}, {"text": "accuracy", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.7293030023574829}]}, {"text": "These sets of sentences are readily available for download over the project website 2 . Two versions of the bilingual lexicon have been used, abase version and an expanded one.", "labels": [], "entities": []}, {"text": "Both sets are manually translated by Greek native speakers and then cross-checked by English native speakers, with one reference translation per sentence.", "labels": [], "entities": []}, {"text": "A range of evaluation metrics are employed, namely BLEU (), NIST (NIST 2002),) and TER ().", "labels": [], "entities": [{"text": "BLEU", "start_pos": 51, "end_pos": 55, "type": "METRIC", "confidence": 0.9985120892524719}, {"text": "NIST (NIST 2002),)", "start_pos": 60, "end_pos": 78, "type": "DATASET", "confidence": 0.9052263975143433}, {"text": "TER", "start_pos": 83, "end_pos": 86, "type": "METRIC", "confidence": 0.9977549910545349}]}, {"text": "The exact sequence with which hypotheses are tested affects the results of the translation, since only one hypothesis is allowed to be applied to each sentence token at present.", "labels": [], "entities": []}, {"text": "This simplifies the evaluation of the hypotheses' effectiveness.", "labels": [], "entities": []}, {"text": "As a result, hypotheses are applied in strict order (i.e. first H 1 , then H 2 etc.).", "labels": [], "entities": []}, {"text": "The threshold values of were settled upon via limited experimentation using sentences from dataset1.", "labels": [], "entities": []}, {"text": "Hypothesis testing was applied to both datasets.", "labels": [], "entities": [{"text": "Hypothesis", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9545732140541077}]}, {"text": "Notably, dataset1 has been used in the development of the MT systems and thus the results obtained with dataset2 should be considered the most representative ones, as they are com-pletely unbiased and the set of sentences was unseen before the experiment and was only translated once.", "labels": [], "entities": [{"text": "MT", "start_pos": 58, "end_pos": 60, "type": "TASK", "confidence": 0.9637562036514282}]}, {"text": "The number of times each hypothesis is tested for each dataset is quoted in, for both the standard (denoted as \"stand\") and the enriched resources (\"enrich\").", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. Values of performance metrics for data- set1, using the baseline version of the proposed  method and other established systems.", "labels": [], "entities": []}, {"text": " Table 4. Metric scores for dataset1, using the  standard language resources, for the baseline sys- tem and for different hypotheses.", "labels": [], "entities": []}, {"text": " Table 5. Metric scores for dataset1, using en- riched language resources, for different systems.", "labels": [], "entities": []}, {"text": " Table 6. Metric scores for dataset2, using stan- dard language resources, for different systems.", "labels": [], "entities": []}, {"text": " Table 7. Metric scores for dataset2, using en- riched language resources, for different systems.", "labels": [], "entities": []}]}