{"title": [{"text": "Word's Vector Representations meet Machine Translation", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 35, "end_pos": 54, "type": "TASK", "confidence": 0.728656604886055}]}], "abstractContent": [{"text": "Distributed vector representations of words are useful in various NLP tasks.", "labels": [], "entities": []}, {"text": "We briefly review the CBOW approach and propose a bilingual application of this architecture with the aim to improve consistency and coherence of Machine Translation.", "labels": [], "entities": [{"text": "consistency", "start_pos": 117, "end_pos": 128, "type": "METRIC", "confidence": 0.967781126499176}, {"text": "Machine Translation", "start_pos": 146, "end_pos": 165, "type": "TASK", "confidence": 0.7653343975543976}]}, {"text": "The primary goal of the bilingual extension is to handle ambiguous words for which the different senses are conflated in the monolingual setup.", "labels": [], "entities": []}], "introductionContent": [{"text": "Machine Translation (MT) systems are nowadays achieving a high-quality performance.", "labels": [], "entities": [{"text": "Machine Translation (MT)", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.886401093006134}]}, {"text": "However, they are typically developed at sentence level using only local information and ignoring the document-level one.", "labels": [], "entities": []}, {"text": "Recent work claims that discourse-wide context can help to translate individual words in away that leads to more coherent translations).", "labels": [], "entities": []}, {"text": "Standard SMT systems use n-gram models to represent words in the target language.", "labels": [], "entities": [{"text": "SMT", "start_pos": 9, "end_pos": 12, "type": "TASK", "confidence": 0.9919965863227844}]}, {"text": "However, there are other word representation techniques that use vectors of contextual information.", "labels": [], "entities": [{"text": "word representation", "start_pos": 25, "end_pos": 44, "type": "TASK", "confidence": 0.7177311778068542}]}, {"text": "Recently, several distributed word representation models have been introduced that have interesting properties regarding to the semantic information that they capture.", "labels": [], "entities": [{"text": "distributed word representation", "start_pos": 18, "end_pos": 49, "type": "TASK", "confidence": 0.6073878506819407}]}, {"text": "In particular, we are interested in the word2vec package available in ().", "labels": [], "entities": []}, {"text": "These models proved to be robust and powerful for predicting semantic relations between words and even across languages.", "labels": [], "entities": [{"text": "predicting semantic relations between words", "start_pos": 50, "end_pos": 93, "type": "TASK", "confidence": 0.8764930725097656}]}, {"text": "However, they are notable to handle lexical ambiguity as they conflate word senses of polysemous words into one common representation.", "labels": [], "entities": []}, {"text": "This limitation is already discussed in () and in (, in which bilingual extensions of the word2vec architecture are proposed.", "labels": [], "entities": []}, {"text": "In contrast to their approach, we are not interested in monolingual applications but instead like to concentrate directly on the bilingual casein connection with MT.", "labels": [], "entities": [{"text": "MT", "start_pos": 162, "end_pos": 164, "type": "TASK", "confidence": 0.8685718774795532}]}, {"text": "We built bilingual word representation models based on word-aligned parallel corpora by an application of the Continuous Bag-of-Words (CBOW) algorithm to the bilingual case (Section 2).", "labels": [], "entities": [{"text": "bilingual word representation", "start_pos": 9, "end_pos": 38, "type": "TASK", "confidence": 0.6291057268778483}]}, {"text": "We made a twofold preliminary evaluation of the acquired word-pair representations on two different tasks (Section 3): predicting semantically related words (3.1) and cross-lingual lexical substitution (3.2).", "labels": [], "entities": [{"text": "predicting semantically related words", "start_pos": 119, "end_pos": 156, "type": "TASK", "confidence": 0.895864263176918}, {"text": "cross-lingual lexical substitution", "start_pos": 167, "end_pos": 201, "type": "TASK", "confidence": 0.6950603127479553}]}, {"text": "Section 4 draws the conclusions and sets the future work in a direct application of these models to MT.", "labels": [], "entities": [{"text": "MT", "start_pos": 100, "end_pos": 102, "type": "TASK", "confidence": 0.9774160981178284}]}], "datasetContent": [{"text": "The semantic models are built using a combination of freely available corpora for English and Spanish (EuropalV7, United Nations and Multilingual United Nations, and Subtitles2012).", "labels": [], "entities": [{"text": "EuropalV7", "start_pos": 103, "end_pos": 112, "type": "DATASET", "confidence": 0.960620641708374}, {"text": "United Nations and Multilingual United Nations", "start_pos": 114, "end_pos": 160, "type": "DATASET", "confidence": 0.7156123220920563}]}, {"text": "They can be found in the Opus site.We trained vectors to represent word pairs forms using this corpora with the word2vec CBOW implementation.", "labels": [], "entities": [{"text": "Opus site.We", "start_pos": 25, "end_pos": 37, "type": "DATASET", "confidence": 0.9747824966907501}]}, {"text": "We built a training set of almost 600 million words and used 600-dimension vectors in the training.", "labels": [], "entities": []}, {"text": "Regarding to the alignments, we only used word-to-word ones to avoid noise.", "labels": [], "entities": []}], "tableCaptions": []}