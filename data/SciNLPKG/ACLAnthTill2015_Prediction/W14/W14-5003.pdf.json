{"title": [{"text": "Crowdsourcing Language Generation Templates for Dialogue Systems", "labels": [], "entities": [{"text": "Crowdsourcing Language Generation Templates", "start_pos": 0, "end_pos": 43, "type": "TASK", "confidence": 0.6758222952485085}]}], "abstractContent": [{"text": "We explore the use of crowdsourcing to generate natural language in spoken dialogue systems.", "labels": [], "entities": []}, {"text": "We introduce a methodology to elicit novel templates from the crowd based on a dialogue seed corpus, and investigate the effect that the amount of surrounding dialogue context has on the generation task.", "labels": [], "entities": [{"text": "dialogue seed corpus", "start_pos": 79, "end_pos": 99, "type": "DATASET", "confidence": 0.7474162479241689}]}, {"text": "Evaluation is performed both with a crowd and with a system developer to assess the naturalness and suit-ability of the elicited phrases.", "labels": [], "entities": []}, {"text": "Results indicate that the crowd is able to provide reasonable and diverse templates within this methodology.", "labels": [], "entities": []}, {"text": "More work is necessary before elicited templates can be automatically plugged into the system.", "labels": [], "entities": []}], "introductionContent": [{"text": "A common approach for natural language generation in task-oriented spoken dialogue systems is template-based generation: a set of templates is manually constructed by system developers, and instantiated with slot values at runtime.", "labels": [], "entities": [{"text": "natural language generation in task-oriented spoken dialogue", "start_pos": 22, "end_pos": 82, "type": "TASK", "confidence": 0.6662176038537707}, {"text": "template-based generation", "start_pos": 94, "end_pos": 119, "type": "TASK", "confidence": 0.7344490885734558}]}, {"text": "When the set of templates is limited, frequent interactions with the system can quickly become repetitive, and the naturalness of the interaction is lost.", "labels": [], "entities": []}, {"text": "In this work, we propose and investigate a methodology for developing a corpus of natural language generation templates fora spoken dialogue system via crowdsourcing.", "labels": [], "entities": []}, {"text": "We use an existing dialogue system that generates utterances from templates, and explore how well a crowd can generate reliable paraphrases given snippets from the system's original dialogues.", "labels": [], "entities": []}, {"text": "By utilizing dialogue data collected from interactions with an existing system, we can begin to learn different ways to converse while controlling the crowd to stay within the scope of the original system.", "labels": [], "entities": []}, {"text": "The proposed approach aims to leverage the system's existing capabilities together with the power of the crowd to expand the system's natural language repertoire and create richer interactions.", "labels": [], "entities": []}, {"text": "Our methodology begins with an existing corpus of dialogues, extracted from a spoken dialogue system that gives directions in a building.", "labels": [], "entities": []}, {"text": "Further details on this system are given in \u00a74.1.", "labels": [], "entities": []}, {"text": "The extracted dialogue corpus contains phrases the system has generated, and crowd-workers construct alternates for these phrases, which can be plugged back into the system as crowd templates.", "labels": [], "entities": []}, {"text": "We investigate via crowdsourcing the effect of the amount of surrounding context provided to workers on the perceived meaning, naturalness, and diversity of the alternates they produce, and study the acceptability of these alternates from a system developer viewpoint.", "labels": [], "entities": []}, {"text": "Our results indicate that the crowd provides reasonable and diverse templates with this methodology.", "labels": [], "entities": []}, {"text": "The developer evaluation suggests that additional work is necessary before we can automatically plug crowdsourced templates directly into the system.", "labels": [], "entities": []}, {"text": "We begin by discussing related work in \u00a72.", "labels": [], "entities": []}, {"text": "In \u00a73, we detail the proposed methodology.", "labels": [], "entities": []}, {"text": "In \u00a74, we describe the experimental setup and results.", "labels": [], "entities": []}, {"text": "Directions for future work are discussed in \u00a75.", "labels": [], "entities": []}], "datasetContent": [{"text": "A good crowd template must minimally satisfy two criteria: (1) It should maintain the meaning of the original template; and (2) It should sound natural in any dialogue context where the original template was used by the dialogue manager, i.e., it should generalize well, beyond the specifics of the dialogue from which it was elicited.", "labels": [], "entities": []}, {"text": "To assess crowd template quality, we construct evaluation HITs for each crowd template.", "labels": [], "entities": []}, {"text": "Instantiated versions of the original template and the crowd template are displayed as options A and B (with randomized assignment) and highlighted as part of the entire dialogue in which the original template was used (see).", "labels": [], "entities": []}, {"text": "In this incontext (IC) evaluation HIT, the worker is asked whether the instantiated crowd template has the same meaning as the original, and which is more natural.", "labels": [], "entities": []}, {"text": "In addition, because the original dialogues were sometimes incoherent (see \u00a73.1), we also asked the evaluation workers to judge whether the given phrases made sense in the given context.", "labels": [], "entities": []}, {"text": "Finally, in order to assess how well the crowd template generalizes across different dialogues, we use a second, out-of-context (OOC) evaluation HIT.", "labels": [], "entities": []}, {"text": "For each crowd template, we randomly selected anew dialogue where the template t appeared.", "labels": [], "entities": []}, {"text": "The out-of-context evaluation HIT presents the instantiated original template and crowd template in this new dialogue.", "labels": [], "entities": []}, {"text": "The crowdworkers thus assess the crowd template in a dialogue context different from the one in which it was collected.", "labels": [], "entities": []}, {"text": "We describe the evaluation HITs in further detail in \u00a74.", "labels": [], "entities": []}, {"text": "We now describe our experiments and results.", "labels": [], "entities": []}, {"text": "We aim to discover whether there is an effect of the amount of surrounding context on perceived crowd template naturalness.", "labels": [], "entities": []}, {"text": "We additionally explore whether the crowd template retains the meaning of the original template, whether they both make sense in the given context, and the diversity of the templates that the crowd produced for each template type.", "labels": [], "entities": []}, {"text": "We report results when the templates are instantiated in-context, in the original dialogue; and out-of-context, in anew dialogue.", "labels": [], "entities": []}, {"text": "We first describe the experimental test-bed and the corpora used and collected below.", "labels": [], "entities": []}, {"text": "The test-bed for our experiments is Directions Robot, a situated dialogue system that provides directions to peoples' offices, conference rooms, and other locations in our building ().", "labels": [], "entities": []}, {"text": "The system couples a Nao humanoid robot with a software infrastructure for multimodal, physically situated dialogue) and has been deployed for several months in an open space, in front of the elevator bank on the 3 rd floor of our building (see).", "labels": [], "entities": []}, {"text": "While some of the interactions are need-based, e.g., visitors coming to the building for meetings, many are also driven by curiosity about the robot.", "labels": [], "entities": []}, {"text": "The purpose crowdsourcing marketplace, the Universal Human Relevance System (UHRS).", "labels": [], "entities": []}, {"text": "The marketplace connects human intelligence tasks with a large population of workers across the globe.", "labels": [], "entities": []}, {"text": "It provides controls for selecting the country of residence and native languages for workers, and for limiting the maximum number of tasks that can be done by a single worker.", "labels": [], "entities": []}, {"text": "Evaluation HITs To keep the crowd evaluation tractable, we randomly sampled 25% of the paraphrases generated for all conditions to produce evaluation HITs.", "labels": [], "entities": []}, {"text": "We excluded paraphrases from seeds that did not receive paraphrases from all 3 workers or were missing required slots.", "labels": [], "entities": []}, {"text": "As discussed in \u00a73, paraphrases were converted to crowd templates, and each crowd template was instantiated in the original dialogue, in-context (IC) and in a randomly selected out-of-context (OOC) dialogue.", "labels": [], "entities": []}, {"text": "The OOC templates were instantiated with slots relevant to the chosen dialogue.", "labels": [], "entities": []}, {"text": "This process yielded 2281 paraphrases, placed into each of the two contexts.", "labels": [], "entities": []}, {"text": "Crowd evaluation process As discussed in \u00a73.3, instantiated templates (crowd and original) were displayed as options A and B, with randomized assignment (see).", "labels": [], "entities": []}, {"text": "Workers were asked to judge whether the original and the crowd template had the same meaning, and whether they made sense in the dialogue context.", "labels": [], "entities": []}, {"text": "Workers then rated which was more natural on a 5-point ordinal scale ranging from -2 to 2, where a -2 rating marked that the original was much more natural than the crowd template.", "labels": [], "entities": []}, {"text": "Statistics on the judgments collected in the evaluation HITs are shown in.", "labels": [], "entities": []}, {"text": "Workers were paid 7 cents for each HIT.", "labels": [], "entities": [{"text": "HIT", "start_pos": 35, "end_pos": 38, "type": "DATASET", "confidence": 0.7179213762283325}]}, {"text": "Each worker could complete at most 5% of all HITs, and each HIT was completed by 5 unique workers.", "labels": [], "entities": []}, {"text": "Outlier elimination One challenge with crowdsourced evaluations is noise introduced by spammers.", "labels": [], "entities": [{"text": "Outlier elimination", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8443953394889832}]}, {"text": "While questions with known answers maybe used to detect spammers in objective tasks, the subjective nature of our evaluation tasks makes this difficult: a worker who does not agree with the majority may simply have different opinions about the paraphrase meaning or naturalness.", "labels": [], "entities": []}, {"text": "Instead of spam detection, we therefore seek to identify and eliminate outliers; in addition, as previously discussed, each HIT was performed by 5 workers, in an effort to increase robustness.", "labels": [], "entities": [{"text": "spam detection", "start_pos": 11, "end_pos": 25, "type": "TASK", "confidence": 0.9596588611602783}, {"text": "HIT", "start_pos": 124, "end_pos": 127, "type": "METRIC", "confidence": 0.7630228996276855}]}, {"text": "We focused attention on workers who performed at least 20 HITs (151 of 230 workers, covering 98% of the total number of HITs).", "labels": [], "entities": []}, {"text": "Since we randomized the A/B assignment of instantiated original templates and crowd templates, we expect to see asymmetric distribution over the relative naturalness scores of all judgments produced by a worker.", "labels": [], "entities": []}, {"text": "To identify workers violating this expectation, we computed a score that reflected the symmetry of the histogram of the naturalness votes for each worker.", "labels": [], "entities": []}, {"text": "We considered as outliers 6 workers that were more than z=1.96 standard deviations away from the mean on this metric (corresponding to a 95% confidence interval).", "labels": [], "entities": []}, {"text": "Secondly, we computed a score that reflected the percentage of tasks where a worker was in a minority, i.e., had the single opposing vote to the other workers on the same meaning question.", "labels": [], "entities": []}, {"text": "We eliminated 4 workers, who fell in the top 97.5 percentile of this distribution.", "labels": [], "entities": []}, {"text": "We corroborated these analyses with a visual inspection of scatterplots showing these two metrics against the number of tasks performed by each judge.", "labels": [], "entities": []}, {"text": "As one worker failed on both criteria, overall, 9 workers (covering 9% of all judgements) were considered outliers and their responses were excluded.", "labels": [], "entities": []}, {"text": "Meaning and Sense Across conditions, we find that most crowd templates are evaluated as having the same meaning as the original and making sense by the majority of workers.", "labels": [], "entities": []}, {"text": "Evaluation percentages are shown in, and are around 90% across the board.", "labels": [], "entities": []}, {"text": "This suggests that inmost cases, the generation task yields crowd templates that meet the goal of preserving the meaning of the original template.", "labels": [], "entities": []}, {"text": "Naturalness To evaluate whether the amount of surrounding context has an effect on the perceived naturalness of a paraphrase relative to the original phrase, we use a Kruskal-Wallis (KW) test on the mean scores for each of the paraphrases, setting our significance level to .05.", "labels": [], "entities": []}, {"text": "A Kruskal-Wallis testis a non-parametric test useful for significance testing when the independent variable is categorical and the data is not assumed to be normally distributed.", "labels": [], "entities": [{"text": "significance testing", "start_pos": 57, "end_pos": 77, "type": "TASK", "confidence": 0.9390055537223816}]}, {"text": "We find that there is an effect of condition on the relative naturalness score (KW chisquared = 15.9156, df = 5, p = 0.007) when crowd: % same meaning, % makes sense, and average relative naturalness (standard deviation in parentheses), measured in-context (IC) and out-of-context (OOC); crowd-based and developer-based diversity score (D-score); developer acceptance rate computed overall templates, and those seen more than once.", "labels": [], "entities": [{"text": "developer-based diversity score (D-score)", "start_pos": 304, "end_pos": 345, "type": "METRIC", "confidence": 0.7342899839083353}]}, {"text": "The susuS condition yields the most diverse templates using crowd-based metrics; removing templates seen once in the evaluation corpus, this condition has the highest acceptance in the developer evaluation.", "labels": [], "entities": []}, {"text": "templates are evaluated in-context, but not out-ofcontext (KW chi-squared = 9.4102, df = 5, p-value = 0.09378).", "labels": [], "entities": []}, {"text": "Average relative naturalness scores in each condition are shown in.", "labels": [], "entities": []}, {"text": "Diversity We also assess the diversity of the templates elicited from the crowd, based on the evaluation set.", "labels": [], "entities": []}, {"text": "Specifically, we calculate a diversity score (D-score) for each template type t.", "labels": [], "entities": [{"text": "diversity score (D-score)", "start_pos": 29, "end_pos": 54, "type": "METRIC", "confidence": 0.8454398155212403}]}, {"text": "We calculate this score as the number of unique crowd template types fort voted to make sense and have the same meaning as the original by the majority, divided by the total number of seeds fort with evaluated crowd templates.", "labels": [], "entities": []}, {"text": "More formally, let P be the original template instantiations that have evaluated crowd templates, M the set of unique crowd template types voted as having the same meaning as the original template by the majority of workers, and S the set of unique crowd template types voted as making sense in the dialogue by the majority of workers.", "labels": [], "entities": []}, {"text": "Then: D-score(t) = |M \\ S| |P | The average diversity scores across all templates for each condition are shown in.", "labels": [], "entities": [{"text": "D-score", "start_pos": 6, "end_pos": 13, "type": "METRIC", "confidence": 0.9498994946479797}]}, {"text": "We find the templates that yield the most diverse crowd templates include WL Retry \"Where are you trying to get to in this building?\" and OK Help, \"Okay, I think I can help you with that\", which have a diversity rating of 1.0 in several conditions: for each template instance we instantiate (i.e., each generation HIT), we get anew, unique crowd template back.", "labels": [], "entities": [{"text": "WL Retry", "start_pos": 74, "end_pos": 82, "type": "DATASET", "confidence": 0.8591223359107971}]}, {"text": "Example crowd templates for the OK Help category include \"I believe I can help you find that\" and \"I can help you ok\".", "labels": [], "entities": [{"text": "OK Help category", "start_pos": 32, "end_pos": 48, "type": "DATASET", "confidence": 0.8471856315930685}]}, {"text": "The templates with the least diversity are those for Hi, which has a D-score around 0.2 in the Sand P hrase conditions.", "labels": [], "entities": [{"text": "D-score", "start_pos": 69, "end_pos": 76, "type": "METRIC", "confidence": 0.9797143340110779}]}, {"text": "We now turn to an investigation of whether statistics from the crowd-based generation and evaluation processes can be used to automatically filter crowd templates.", "labels": [], "entities": []}, {"text": "Specifically, we look at two heuristics, with results plotted in.", "labels": [], "entities": []}, {"text": "These heuristics are applied across the evaluation corpus, collating data from all conditions.", "labels": [], "entities": []}, {"text": "The first heuristic, Heur 1 , uses a simple threshold on the number of times a crowd template occurred in the evaluation corpus.", "labels": [], "entities": []}, {"text": "We hypothesize that more frequent paraphrases are more likely to be acceptable to the developer, and in fact, as we increase the frequency threshold, precision increases and recall decreases.", "labels": [], "entities": [{"text": "precision", "start_pos": 150, "end_pos": 159, "type": "METRIC", "confidence": 0.9995443224906921}, {"text": "recall", "start_pos": 174, "end_pos": 180, "type": "METRIC", "confidence": 0.9995331764221191}]}, {"text": "The second heuristic, Heur 2 , combines the threshold on counts with additional scores collected in the out-of-context crowd-evaluation: It only considers templates with an aggregated judgment on the same meaning question greater than 50% (i.e., the majority of the crowd thought the paraphrase had the same meaning as the original), and with an aggregated relative naturalness score above the overall mean.", "labels": [], "entities": []}, {"text": "As illustrates, different tradeoffs between precision and recall can be achieved via these heuristics, and by varying the count threshold.", "labels": [], "entities": [{"text": "precision", "start_pos": 44, "end_pos": 53, "type": "METRIC", "confidence": 0.9992294311523438}, {"text": "recall", "start_pos": 58, "end_pos": 64, "type": "METRIC", "confidence": 0.9983664155006409}]}, {"text": "These results indicate that developer filtering remains a necessary step for adding new dialogue system templates, as the filtering process cannot yet be replaced by the crowd-evaluation.", "labels": [], "entities": [{"text": "developer filtering", "start_pos": 28, "end_pos": 47, "type": "TASK", "confidence": 0.773084968328476}]}, {"text": "This is not surprising since the evaluation HITs did not Since the evaluation corpus randomly sampled 25% of the generation HITs output, this is a proxy for the frequency with which that template was generated by the crowd.", "labels": [], "entities": []}, {"text": "express all the different factors that we found the developer took into account when selecting templates, such as style decisions and how phrases are combined in the system to form a dialogue.", "labels": [], "entities": []}, {"text": "Future work may consider expanding evaluation HITs to reflect some of these aspects.", "labels": [], "entities": []}, {"text": "By using signals acquired through crowd generation and evaluation, we should be able to reduce the load for the developer by presenting a smaller and more precise candidate list at the expense of reductions in recall.", "labels": [], "entities": [{"text": "crowd generation", "start_pos": 34, "end_pos": 50, "type": "TASK", "confidence": 0.7059256136417389}, {"text": "recall", "start_pos": 210, "end_pos": 216, "type": "METRIC", "confidence": 0.9978278279304504}]}], "tableCaptions": [{"text": " Table 1: Statistics for the crowd-based generation  and evaluation processes. Each generation HIT  was seen by 3 unique workers and each evaluation  HIT was seen by 5 unique workers. #w represents  number of workers. For evaluation, #w = 231.", "labels": [], "entities": []}, {"text": " Table 1. Each worker  could complete at most 1/6 of the total HITs for  that condition. We paid 3 cents for each genera-", "labels": [], "entities": []}, {"text": " Table 2: % same meaning, % makes sense, and average relative naturalness (standard deviation in paren- theses), measured in-context (IC) and out-of-context (OOC); crowd-based and developer-based diversity  score (D-score); developer acceptance rate computed over all templates, and those seen more than once.  The susuS condition yields the most diverse templates using crowd-based metrics; removing templates  seen once in the evaluation corpus, this condition has the highest acceptance in the developer evaluation.", "labels": [], "entities": [{"text": "developer-based diversity  score (D-score)", "start_pos": 180, "end_pos": 222, "type": "METRIC", "confidence": 0.7721545100212097}, {"text": "acceptance rate computed", "start_pos": 234, "end_pos": 258, "type": "METRIC", "confidence": 0.8763749798138937}]}]}