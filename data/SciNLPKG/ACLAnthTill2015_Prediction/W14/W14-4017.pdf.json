{"title": [{"text": "Evaluating Word Order Recursively over Permutation-Forests", "labels": [], "entities": [{"text": "Evaluating Word Order Recursively", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7381468489766121}, {"text": "Permutation-Forests", "start_pos": 39, "end_pos": 58, "type": "TASK", "confidence": 0.4287623167037964}]}], "abstractContent": [{"text": "Automatically evaluating word order of MT system output at the sentence-level is challenging.", "labels": [], "entities": [{"text": "MT system output", "start_pos": 39, "end_pos": 55, "type": "TASK", "confidence": 0.863761842250824}]}, {"text": "At the sentence-level, ngram counts are rather sparse which makes it difficult to measure word order quality effectively using lexicalized units.", "labels": [], "entities": []}, {"text": "Recent approaches abstract away from lexicaliza-tion by assigning a score to the permutation representing how word positions in system output move around relative to a reference translation.", "labels": [], "entities": []}, {"text": "Metrics over permutations exist (e.g., Kendal tau or Spear-man Rho) and have been shown to be useful in earlier work.", "labels": [], "entities": []}, {"text": "However, none of the existing metrics over permutations groups word positions recursively into larger phrase-like blocks, which makes it difficult to account for long-distance reordering phenomena.", "labels": [], "entities": []}, {"text": "In this paper we explore novel metrics computed over Permutation Forests (PEFs), packed charts of Permutation Trees (PETs), which are tree decompositions of a permutation into primitive ordering units.", "labels": [], "entities": []}, {"text": "We empirically compare PEFs metric against five known reordering metrics on WMT13 data for ten language pairs.", "labels": [], "entities": [{"text": "PEFs", "start_pos": 23, "end_pos": 27, "type": "METRIC", "confidence": 0.767814040184021}, {"text": "WMT13 data", "start_pos": 76, "end_pos": 86, "type": "DATASET", "confidence": 0.9626913964748383}]}, {"text": "The PEFs metric shows better correlation with human ranking than the other metrics almost on all language pairs.", "labels": [], "entities": [{"text": "PEFs", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.6740254759788513}]}, {"text": "None of the other metrics exhibits as stable behavior across language pairs.", "labels": [], "entities": []}], "introductionContent": [{"text": "Evaluating word order (also reordering) in MT is one of the main ingredients in automatic MT evaluation, e.g.,).", "labels": [], "entities": [{"text": "MT", "start_pos": 43, "end_pos": 45, "type": "TASK", "confidence": 0.9661220908164978}, {"text": "MT evaluation", "start_pos": 90, "end_pos": 103, "type": "TASK", "confidence": 0.9558593034744263}]}, {"text": "To monitor progress on evaluating reordering, recent work explores dedicated reordering evaluation metrics, cf. ().", "labels": [], "entities": []}, {"text": "Existing work computes the correlation between the ranking of the outputs of different systems by an evaluation metric to human ranking, on e.g., the WMT evaluation data.", "labels": [], "entities": [{"text": "WMT evaluation data", "start_pos": 150, "end_pos": 169, "type": "DATASET", "confidence": 0.7934666872024536}]}, {"text": "For evaluating reordering, it is necessary to word align system output with the corresponding reference translation.", "labels": [], "entities": []}, {"text": "For convenience, a 1:1 alignment (a permutation) is induced between the words on both sides (, possibly leaving words unaligned on either side.", "labels": [], "entities": []}, {"text": "Existing work then concentrates on defining measures of reordering over permutations, cf. ().", "labels": [], "entities": []}, {"text": "Popular metrics over permutations are: Kendall's tau, Spearman, Hamming distance, Ulam and Fuzzy score.", "labels": [], "entities": [{"text": "Spearman", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.9915680289268494}, {"text": "Ulam", "start_pos": 82, "end_pos": 86, "type": "METRIC", "confidence": 0.9649636745452881}, {"text": "Fuzzy score", "start_pos": 91, "end_pos": 102, "type": "METRIC", "confidence": 0.8573898673057556}]}, {"text": "These metrics treat a permutation as a flat sequence of integers or blocks, disregarding the possibility of hierarchical grouping into phrase-like units, making it difficult to measure long-range order divergence.", "labels": [], "entities": []}, {"text": "Next we will show by example that permutations also contain latent atomic units that govern the recursive reordering of phrase-like units.", "labels": [], "entities": []}, {"text": "Accounting for these latent reorderings could actually be far simpler than the flat view of a permutation.", "labels": [], "entities": []}, {"text": "argue that the conventional metrics cannot measure well the long distance reordering between an English reference sentence \"A because B\" and a Japanese-English hypothesis translation \"B because A\", where A and B are blocks of any length with internal monotonic alignments.", "labels": [], "entities": []}, {"text": "In this paper we explore the idea of factorizing permutations into permutation-trees (PETs) () and defining new A because B Our PET-based metrics interpolate the scores over the two inversion operators 2, 1 with the internal scores for A and B, incorporating a weight for subtree height.", "labels": [], "entities": []}, {"text": "If both A and B are large blocks, internally monotonically (also known as straight) aligned, then our measure will not count every single reordering of a word in A or B, but will consider this case as block reordering.", "labels": [], "entities": []}, {"text": "From a PET perspective, the distance of the reordering is far smaller than when looking at a flat permutation.", "labels": [], "entities": []}, {"text": "But does this hierarchical view of reordering cohere better with human judgement than string-based metrics?", "labels": [], "entities": []}, {"text": "The example above also shows that a permutation may factorize into different PETs, each corresponding to a different segmentation of a sentence pair into phrase-pairs.", "labels": [], "entities": []}, {"text": "In this paper we introduce permutation forests (PEFs); a PEF is a hypergraph that compactly packs the set of PETs that factorize a permutation.", "labels": [], "entities": []}, {"text": "There is yet a more profoud reasoning behind PETs than only accounting for long-range reorderings.", "labels": [], "entities": []}, {"text": "The example in gives the flavor of PETs.", "labels": [], "entities": []}, {"text": "Observe how every internal node in this PET dominates a subtree whose fringe 1 is itself a permutation over an integer sub-range of the original permutation.", "labels": [], "entities": []}, {"text": "Every node is decorated with a permutation over the child positions (called operator).", "labels": [], "entities": []}, {"text": "For example 4, 5, 6 constitutes a contiguous range of integers (corresponding to a phrase pair), and hence will be grouped into a subtree; 1 Ordered sequence of leaf nodes.", "labels": [], "entities": []}, {"text": "which in turn can be internally re-grouped into a binary branching subtree.", "labels": [], "entities": []}, {"text": "Every node in a PET is minimum branching, i.e., the permutation factorizes into a minimum number of adjacent permutations over integer sub-ranges).", "labels": [], "entities": []}, {"text": "The node operators in a PET are known to be the atomic building blocks of all permutations (called primal permutations).", "labels": [], "entities": []}, {"text": "Because these are building atomic units of reordering, it makes sense to want to measure reordering as a function of the individual cost of these operators.", "labels": [], "entities": []}, {"text": "In this work we propose to compute new reordering measures that aggregate over the individual node-permutations in these PETs.", "labels": [], "entities": []}, {"text": "While PETs where exploited rather recently for extracting features used in the BEER metric system description) in the official WMT 2014 competition, this work is the first to propose integral recursive metrics over PETs and PEFs solely for measuring reordering (as opposed to individual non-recursive features in a full metric that measures at the same time both fluency and adequacy).", "labels": [], "entities": [{"text": "BEER metric system description", "start_pos": 79, "end_pos": 109, "type": "DATASET", "confidence": 0.884404644370079}, {"text": "WMT 2014 competition", "start_pos": 127, "end_pos": 147, "type": "TASK", "confidence": 0.5952903827031454}]}, {"text": "We empirically show that a PEF-based evaluation measure correlates better with human rankings than the string-based measures on eight of the ten language pairs in WMT13 data.", "labels": [], "entities": [{"text": "WMT13 data", "start_pos": 163, "end_pos": 173, "type": "DATASET", "confidence": 0.9612396061420441}]}, {"text": "For the 9 th language pair it is close to best, and for the 10 th (English-Czech) we find a likely explanation in the Findings of the 2013 WMT (.", "labels": [], "entities": [{"text": "Findings of the 2013 WMT", "start_pos": 118, "end_pos": 142, "type": "DATASET", "confidence": 0.7456018686294555}]}, {"text": "Crucially, the PEF-based measure shows more stable ranking across language pairs than any of the other measures.", "labels": [], "entities": []}, {"text": "The metric is available online as free software 2 .", "labels": [], "entities": []}], "datasetContent": [{"text": "Data The data that was used for experiments are human rankings of translations from WMT13 ().", "labels": [], "entities": [{"text": "WMT13", "start_pos": 84, "end_pos": 89, "type": "DATASET", "confidence": 0.9708355069160461}]}, {"text": "The data covers 10 language pairs with a diverse set of systems used for translation.", "labels": [], "entities": []}, {"text": "Each human evaluator was presented with 5 different translations, source sentence and a reference translation and asked to rank system translations by their quality (ties were allowed).", "labels": [], "entities": []}, {"text": "We extract pairs of translations from human evaluated data and compute their scores with all metrics.", "labels": [], "entities": []}, {"text": "If the ranking assigned by a metric is the same as the ranking assigned by a human evaluator then that pair is considered concordant, otherwise it is a discordant pair.", "labels": [], "entities": []}, {"text": "All pairs which have the same score by the metric or are judged as ties by human evaluators are not used in meta-evaluation.", "labels": [], "entities": []}, {"text": "The formula that was used for computing Kendall's tau correlation coefficient is shown in Equation 1.", "labels": [], "entities": [{"text": "correlation coefficient", "start_pos": 54, "end_pos": 77, "type": "METRIC", "confidence": 0.8075865805149078}, {"text": "Equation", "start_pos": 90, "end_pos": 98, "type": "METRIC", "confidence": 0.9860199093818665}]}, {"text": "Note that the formula for Kendall tau rank correlation coefficient that is used in meta-evaluation is different from the Kendall tau similarity function used for evaluating permutations.", "labels": [], "entities": [{"text": "Kendall tau rank correlation coefficient", "start_pos": 26, "end_pos": 66, "type": "METRIC", "confidence": 0.574862003326416}]}, {"text": "The values that it returns are in the range, where \u22121 means that order is always opposite from the human judgment while the value 1 means that metric ranks the system translations in the same way as humans do.", "labels": [], "entities": []}, {"text": "Evaluating reordering Since system translations do not differ only in the word order but also in lexical choice, we follow  and interpolate the score given by each reordering metric with the same lexical score.", "labels": [], "entities": []}, {"text": "For lexical scoring we use unigram BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 35, "end_pos": 39, "type": "METRIC", "confidence": 0.9439341425895691}]}, {"text": "The parameter that balances the weights for these two metrics \u03b1 is chosen to be 0.5 so it would not underestimate the lexical differences between translations (\u03b1 0.5) but also would not turn the whole metric into unigram BLEU (\u03b1 0.5).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 221, "end_pos": 225, "type": "METRIC", "confidence": 0.9804114103317261}]}, {"text": "The equation for this interpolation is shown in Equation 2. 4 Where \u03c0(ref, sys) is the permutation representing the word alignment from sys to ref . The effect of \u03b1 on the German-English evaluation is visible on.", "labels": [], "entities": []}, {"text": "The PET and PEF measures have an extra parameter \u03b2 that gives importance to the long distance errors that also needs to be tuned.", "labels": [], "entities": [{"text": "PET", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.5087230205535889}]}, {"text": "On we can seethe effect of \u03b2 on GermanEnglish for \u03b1 = 0.5.", "labels": [], "entities": [{"text": "GermanEnglish", "start_pos": 32, "end_pos": 45, "type": "DATASET", "confidence": 0.9658257365226746}]}, {"text": "For all language pairs for \u03b2 = 0.6 both PETs and PEFs get good results so we picked that as value for \u03b2 in our experiments.", "labels": [], "entities": []}, {"text": "Choice of word alignments The issue we did not discuss so far is how to find a permutation from system and reference translations.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 10, "end_pos": 25, "type": "TASK", "confidence": 0.7014263421297073}]}, {"text": "One way is to first get alignments between the source sentence and the system translation (from a decoder or by automatically aligning sentences), and also alignments between the source sentence and the reference translation (manually or automatically aligned).", "labels": [], "entities": []}, {"text": "Subsequently we must make those alignments 1-to-1 and merge them into a permutation.", "labels": [], "entities": []}, {"text": "That is the approach that was followed in previous work (Birch and Osborne, 2011; Talbot et al.,).", "labels": [], "entities": []}, {"text": "Alternatively, we may align system and reference translations directly.", "labels": [], "entities": []}, {"text": "One of the simplest ways to do that is by finding exact matches between words and bigrams between system and reference translation as done in ().", "labels": [], "entities": []}, {"text": "The way we align system and reference translations is by using the aligner supplied with ME-TEOR (Denkowski and Lavie, 2011) for finding 1-to-1 alignments which are later converted to a permutation.", "labels": [], "entities": [{"text": "ME-TEOR", "start_pos": 89, "end_pos": 96, "type": "METRIC", "confidence": 0.817370593547821}]}, {"text": "The advantage of this method is that it can do non-exact matching by stemming or using additional sources for semantic similarity such as WordNets and paraphrase tables.", "labels": [], "entities": [{"text": "WordNets", "start_pos": 138, "end_pos": 146, "type": "DATASET", "confidence": 0.9362183213233948}]}, {"text": "Since we will not have a perfect permutation as input, because many words in the reference or system translations might not be aligned, we introduce a brevity penalty (bp(\u00b7, \u00b7) in Equation 2) for the ordering component as in (.", "labels": [], "entities": []}, {"text": "The brevity penalty is the same as in BLEU with the small difference that instead of taking the length of system and reference translation as its parameters, it takes the length of the system permutation and the length of the reference.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 38, "end_pos": 42, "type": "METRIC", "confidence": 0.9698940515518188}]}, {"text": "The results in suggest that the PEFscore which uses hierarchy over permutations outperforms the string based permutation metrics in the majority of the language pairs.", "labels": [], "entities": []}, {"text": "The main exception is the English-Czech language pair in which both PETs and PEFs based metric do not give good results compared to some other metrics.", "labels": [], "entities": []}, {"text": "For discussion about English-Czech look at the section 6.1.", "labels": [], "entities": []}, {"text": "So far we have shown that PEFs outperform the existing metrics over the majority of language pairs.", "labels": [], "entities": [{"text": "PEFs", "start_pos": 26, "end_pos": 30, "type": "METRIC", "confidence": 0.742546558380127}]}, {"text": "There are two pending issues to discuss.", "labels": [], "entities": []}, {"text": "Why is English-Czech seemingly so difficult?", "labels": [], "entities": []}, {"text": "And does preferring inversion over non-binary branching correlate better with human judgement.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Sentence level Kendall tau scores for  translation out of English with \u03b1 = 0.5 and \u03b2 =  0.6", "labels": [], "entities": [{"text": "translation out of English", "start_pos": 49, "end_pos": 75, "type": "TASK", "confidence": 0.8800092488527298}]}, {"text": " Table 2: Sentence level Kendall tau scores for  translation into English with \u03b1 = 0.5 and \u03b2 = 0.6", "labels": [], "entities": [{"text": "translation", "start_pos": 49, "end_pos": 60, "type": "TASK", "confidence": 0.9781599640846252}]}, {"text": " Table 3: Average ranks and average Kendall  scores for each tested metrics over all language  pairs", "labels": [], "entities": [{"text": "average Kendall  scores", "start_pos": 28, "end_pos": 51, "type": "METRIC", "confidence": 0.7724328637123108}]}, {"text": " Table 4: Sentence level Kendall tau score for  translation out of English different \u03b3 with \u03b1 = 0.5  and \u03b2 = 0.6", "labels": [], "entities": []}, {"text": " Table 5: Sentence level Kendall tau score for  translation into English for different \u03b3 with \u03b1 =  0.5 and \u03b2 = 0.6", "labels": [], "entities": [{"text": "Sentence", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.926546573638916}, {"text": "translation", "start_pos": 48, "end_pos": 59, "type": "TASK", "confidence": 0.9647862315177917}]}]}