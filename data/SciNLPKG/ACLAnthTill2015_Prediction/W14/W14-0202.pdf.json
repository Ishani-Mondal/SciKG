{"title": [{"text": "IBM's Belief Tracker: Results On Dialog State Tracking Challenge Datasets", "labels": [], "entities": [{"text": "IBM's Belief Tracker", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.707798644900322}, {"text": "Dialog State Tracking Challenge", "start_pos": 33, "end_pos": 64, "type": "TASK", "confidence": 0.6433802396059036}]}], "abstractContent": [{"text": "Accurate dialog state tracking is crucial for the design of an efficient spoken dialog system.", "labels": [], "entities": [{"text": "dialog state tracking", "start_pos": 9, "end_pos": 30, "type": "TASK", "confidence": 0.7362544139226278}]}, {"text": "Until recently, quantitative comparison of different state tracking methods was difficult.", "labels": [], "entities": [{"text": "state tracking", "start_pos": 53, "end_pos": 67, "type": "TASK", "confidence": 0.7575659155845642}]}, {"text": "However the 2013 Dialog State Tracking Challenge (DSTC) introduced a common dataset and metrics that allow to evaluate the performance of trackers on a standardized task.", "labels": [], "entities": [{"text": "Dialog State Tracking Challenge (DSTC)", "start_pos": 17, "end_pos": 55, "type": "TASK", "confidence": 0.7980367754186902}]}, {"text": "In this paper we present our belief tracker based on the Hidden Information State (HIS) model with an adjusted user model component.", "labels": [], "entities": []}, {"text": "Further, we report the results of our tracker on test3 dataset from DSTC.", "labels": [], "entities": [{"text": "test3 dataset from DSTC", "start_pos": 49, "end_pos": 72, "type": "DATASET", "confidence": 0.8481178432703018}]}, {"text": "Our tracker is competitive with trackers submitted to DSTC, even without training it achieves the best results in L2 metrics and it performs between second and third place inaccuracy.", "labels": [], "entities": [{"text": "DSTC", "start_pos": 54, "end_pos": 58, "type": "DATASET", "confidence": 0.9444382190704346}]}, {"text": "After adjusting the tracker using the provided data it outperformed the other submissions also inaccuracy and yet improved in L2.", "labels": [], "entities": []}, {"text": "Additionally we present preliminary results on another two datasets, test1 and test2, used in the DSTC.", "labels": [], "entities": [{"text": "DSTC", "start_pos": 98, "end_pos": 102, "type": "DATASET", "confidence": 0.8161624073982239}]}, {"text": "Strong performance in L2 metric means that our tracker produces well calibrated hypotheses probabilities.", "labels": [], "entities": []}], "introductionContent": [{"text": "Spoken dialog systems need to keep a representation of the dialog state and the user goal to follow an efficient interaction path.", "labels": [], "entities": []}, {"text": "The performance of state-of-the-art speech recognition systems varies widely with domain and environment with word accuracy rates ranging from less than 70% to 98%, which often leads to misinterpretation of the user's intention.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 36, "end_pos": 54, "type": "TASK", "confidence": 0.7786052227020264}, {"text": "accuracy", "start_pos": 115, "end_pos": 123, "type": "METRIC", "confidence": 0.9021779298782349}]}, {"text": "Dialog state tracking methods need to cope with such error-prone automatic speech recognition (ASR) and spoken language understanding (SLU) outputs.", "labels": [], "entities": [{"text": "Dialog state tracking", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.8647338350613912}, {"text": "automatic speech recognition (ASR) and spoken language understanding (SLU) outputs", "start_pos": 65, "end_pos": 147, "type": "TASK", "confidence": 0.739649025457246}]}, {"text": "Traditional dialog systems use hand-crafted rules to select from the SLU outputs based on their confidence scores.", "labels": [], "entities": []}, {"text": "Recently, several data-driven approaches to dialog state tracking were developed as apart of end-to-end spoken dialog systems.", "labels": [], "entities": [{"text": "dialog state tracking", "start_pos": 44, "end_pos": 65, "type": "TASK", "confidence": 0.8982439835866293}]}, {"text": "However, specifics of these systems render comparison of dialog state tracking methods difficult.", "labels": [], "entities": [{"text": "dialog state tracking", "start_pos": 57, "end_pos": 78, "type": "TASK", "confidence": 0.7592083215713501}]}, {"text": "The Dialog State Tracking Challenge (DSTC) () provides a shared testbed with datasets and tools for evaluation of dialog state tracking methods.", "labels": [], "entities": [{"text": "Dialog State Tracking Challenge (DSTC)", "start_pos": 4, "end_pos": 42, "type": "TASK", "confidence": 0.8095117253916604}, {"text": "dialog state tracking", "start_pos": 114, "end_pos": 135, "type": "TASK", "confidence": 0.7039947509765625}]}, {"text": "It abstracts from subsystems of end-to-end spoken dialog systems focusing only on the dialog state estimation and tracking.", "labels": [], "entities": [{"text": "dialog state estimation", "start_pos": 86, "end_pos": 109, "type": "TASK", "confidence": 0.7287556926409403}]}, {"text": "It does so by providing datasets of ASR and SLU outputs with reference transcriptions together with annotation on the level of dialog acts.", "labels": [], "entities": [{"text": "ASR", "start_pos": 36, "end_pos": 39, "type": "TASK", "confidence": 0.9100160598754883}]}, {"text": "In this paper we report initial encouraging results of our generative belief state tracker.", "labels": [], "entities": [{"text": "generative belief state tracker", "start_pos": 59, "end_pos": 90, "type": "TASK", "confidence": 0.9107464253902435}]}, {"text": "We plan to investigate discriminative approaches in the future.", "labels": [], "entities": []}, {"text": "The rest of the paper continues as follows.", "labels": [], "entities": []}, {"text": "In the next section we formally introduce the dialog tracking task together with datasets used in the DSTC.", "labels": [], "entities": [{"text": "dialog tracking task", "start_pos": 46, "end_pos": 66, "type": "TASK", "confidence": 0.9176149566968282}, {"text": "DSTC", "start_pos": 102, "end_pos": 106, "type": "DATASET", "confidence": 0.6005278825759888}]}, {"text": "Then in Section 3 we discuss related work.", "labels": [], "entities": []}, {"text": "Section 4 describes the belief update equations of our tracker.", "labels": [], "entities": [{"text": "belief update", "start_pos": 24, "end_pos": 37, "type": "TASK", "confidence": 0.7569940984249115}]}, {"text": "After that we introduce the design of our whole tracking system, especially how we trained the system in a supervised setting on the train dataset and in an unsupervised setting on the test dataset.", "labels": [], "entities": []}, {"text": "In Section 6 we show results of our trackers, compare them to other DSTC participants, and discuss the results in the context of design choices and task characteristics.", "labels": [], "entities": []}], "datasetContent": [{"text": "The task of the DSTC can be formally defined as computing P(g t |u 0:t , a 0:t ).", "labels": [], "entities": []}, {"text": "That is, for each time step t of the dialog compute the probability distribution over the user's hidden goal g given a sequence of SLU hypotheses from the  beginning of the dialog up to the time t denoted as u 0:t and a sequence of machine actions a 0:t . It is assumed that the goal is fixed through the dialog, unless the user is informed that the requested goal does not exist.", "labels": [], "entities": []}, {"text": "In DSTC the user's goal consist of nine slots: route, from.desc, from.neighborhood, from.monument, to.desc, to.neighborhood, to.monument, date, time.", "labels": [], "entities": []}, {"text": "The dialog datasets in the DSTC are partitioned into five training sets and four test sets.", "labels": [], "entities": []}, {"text": "Details and differences of the datasets are summarized in and 2.", "labels": [], "entities": []}, {"text": "The datasets come from dialog systems deployed by three teams denoted as A, B and C.", "labels": [], "entities": []}, {"text": "All the training datasets were transcribed but only three of them were annotated on the level of dialog acts.", "labels": [], "entities": []}, {"text": "The SLU confidence scores from system B are relatively well calibrated, meaning that confidences can be directly interpreted as probabilities of observing the SLU hypothesis.", "labels": [], "entities": []}, {"text": "Confidence scores from the system A are not well calibrated as noted by several DSTC participants.", "labels": [], "entities": [{"text": "Confidence", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9737887978553772}]}, {"text": "The evaluation protocol is briefly described in Section 6.", "labels": [], "entities": []}, {"text": "Its detailed description can be found in (), its evaluation in (.", "labels": [], "entities": []}, {"text": "In 2013, nine teams with 27 trackers participated in the challenge.", "labels": [], "entities": []}, {"text": "The results of the best trackers will be discussed together with the results of our tracker later in Section 6.", "labels": [], "entities": []}, {"text": "We evaluated all our tracker variants on the DSTC test3 dataset using the protocol designed for the challenge participants.", "labels": [], "entities": [{"text": "DSTC test3 dataset", "start_pos": 45, "end_pos": 63, "type": "DATASET", "confidence": 0.9813965360323588}]}, {"text": "We also present initial results of the basic IBM indep uniform and IBM jointly uniform trackers for test1 and test2 datasets.", "labels": [], "entities": []}, {"text": "Several quantities were measured in three different schedules, which defines, which moments of the dialog the evaluation is performed.", "labels": [], "entities": []}, {"text": "Here we report results for schedule 2 and 3.", "labels": [], "entities": []}, {"text": "Schedule 2 takes into account all turns when the relevant concept appeared on user's SLU list or was mentioned by the dialog system.", "labels": [], "entities": []}, {"text": "Schedule 3 evaluates belief at the end of the dialog, i.e. at the moment when the queried information is presented to the user.", "labels": [], "entities": []}, {"text": "We report accuracy, which is the ratio of dialogs where the user goal was correctly estimated, and the L2 score, which is the Euclidean distance of the vector of the resulting belief from a vector having 1 for the correct hypothesis and 0s for the others.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9978054165840149}, {"text": "L2 score", "start_pos": 103, "end_pos": 111, "type": "METRIC", "confidence": 0.9672795832157135}]}, {"text": "For both of these the average values overall tracked slot is reported as well as the value for the joint hypotheses.", "labels": [], "entities": [{"text": "tracked", "start_pos": 45, "end_pos": 52, "type": "METRIC", "confidence": 0.9335933327674866}, {"text": "slot", "start_pos": 53, "end_pos": 57, "type": "METRIC", "confidence": 0.5429179668426514}]}, {"text": "The accuracy informs us how often the correct query to the database will be made.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.999618649482727}]}, {"text": "The L2 score tells us how well-calibrated the results are, which can be important for disambiguation and for statistical policy optimization.", "labels": [], "entities": [{"text": "disambiguation", "start_pos": 86, "end_pos": 100, "type": "TASK", "confidence": 0.9819577932357788}, {"text": "statistical policy optimization", "start_pos": 109, "end_pos": 140, "type": "TASK", "confidence": 0.7727901339530945}]}], "tableCaptions": [{"text": " Table 5: Preliminary results for schedule 3 on the  DSTC test set 1 of our two trackers compared to  three overall well performing teams. For teams 6  and 9 see", "labels": [], "entities": [{"text": "DSTC test set", "start_pos": 53, "end_pos": 66, "type": "DATASET", "confidence": 0.9212124546368917}]}, {"text": " Table 6: Preliminary results for schedule 3 on the  DSTC test set 2. For teams see", "labels": [], "entities": [{"text": "DSTC test set 2", "start_pos": 53, "end_pos": 68, "type": "DATASET", "confidence": 0.9652537405490875}]}]}