{"title": [{"text": "Towards Surface Realization with CCGs Induced from Dependencies", "labels": [], "entities": [{"text": "Surface Realization", "start_pos": 8, "end_pos": 27, "type": "TASK", "confidence": 0.694149911403656}]}], "abstractContent": [{"text": "We present a novel algorithm for inducing Combinatory Categorial Grammars from dependency treebanks, along with initial experiments showing that it can be used to achieve competitive realization results using an enhanced version of the surface realization shared task data.", "labels": [], "entities": []}], "introductionContent": [{"text": "In the first surface realization shared task, no grammar-based systems achieved competitive results, as input conversion turned out to be more difficult than anticipated.", "labels": [], "entities": []}, {"text": "Since then, have shown that grammar-based systems can be substantially improved with error mining techniques.", "labels": [], "entities": [{"text": "error mining", "start_pos": 85, "end_pos": 97, "type": "TASK", "confidence": 0.7326544225215912}]}, {"text": "In this paper, inspired by recent work on converting dependency treebanks () and semantic parsing () with Combinatory Categorial Grammar (CCG), we pursue the alternative strategy of inducing a CCG from an enhanced version of the shared task dependencies, with initial experiments showing even better results.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 81, "end_pos": 97, "type": "TASK", "confidence": 0.7278682291507721}]}, {"text": "A silver lining of the failure of grammar-based systems in the shared task is that it revealed some problems with the data.", "labels": [], "entities": []}, {"text": "In particular, it became evident that in cases where a constituent is annotated with multiple roles in the Penn Treebank (PTB), the partial nature of Propbank annotation and the restriction to syntactic dependency trees meant that information was lost between the surface and deep representations, leading grammarbased systems to fail for good reason.", "labels": [], "entities": [{"text": "Penn Treebank (PTB)", "start_pos": 107, "end_pos": 126, "type": "DATASET", "confidence": 0.968598461151123}]}, {"text": "For example, shows that with free object relatives, only one of the two roles played by how much manufacturing strength is captured in the deep representation, making it difficult to linearize this phrase correctly.", "labels": [], "entities": []}, {"text": "By contrast,  Economists are divided as to [how much manufacturing strength] i they expect to see ti in September reports on industrial production and capacity utilization , also due tomorrow (wsj 2400.6, \"deep\" representation) shows an experimental version of the shallow representation intended to capture all the syntactic dependencies in the PTB, including the additional object role played by this phrase here.", "labels": [], "entities": [{"text": "PTB", "start_pos": 346, "end_pos": 349, "type": "DATASET", "confidence": 0.9259821176528931}]}, {"text": "1 Including all PTB syntactic dependencies in the shallow representation makes it feasible to define a compatible CCG; at the bottom of the figure, a corresponding CCG derivation for these dependencies is shown.", "labels": [], "entities": []}, {"text": "In the next section, we present an algorithm for inducing such derivations.", "labels": [], "entities": []}, {"text": "In contrast to approach, the algorithm integrates the proposal of candidate lexical categories with the derivational process, making it possible to derive categories involving unsaturated arguments, such ass e,dcl \\np x /(s e ,to \\np x ); it also makes greater use of unary type-changing rules, as with Artzi & Zettlemoyer's (2013) approach.", "labels": [], "entities": []}, {"text": "Unlike their approach though, it works in abroad coverage setting, and makes use of all the combinators standardly used with CCG, including ones for type-raising.", "labels": [], "entities": []}], "datasetContent": [{"text": "We ran the induction algorithm over the standard PTB training sections (02-21), recovering complete derivations more than 90% of the time for most sections.", "labels": [], "entities": [{"text": "PTB training sections", "start_pos": 49, "end_pos": 70, "type": "DATASET", "confidence": 0.9058408737182617}]}, {"text": "Robust treatment of coordination, including argument cluster coordination and gapping, remains a known issue; other causes of derivation failures remain to be investigated.", "labels": [], "entities": [{"text": "argument cluster coordination", "start_pos": 44, "end_pos": 73, "type": "TASK", "confidence": 0.5996530751387278}]}, {"text": "To select preferred derivations, we used a complexity metric that simply counts the number of steps and the number of slashes in the categories.", "labels": [], "entities": []}, {"text": "We then trained a generative syntactic model) and used it along with a composite language model to generate nbest realizations for reranking), additionally using a large-scale (gigaword) language model.", "labels": [], "entities": []}, {"text": "Development and test results appear in.", "labels": [], "entities": []}, {"text": "Perhaps because of the expanded use of type-changing rules with simple lexical categories, the generative model and hypertagger) performed worse than expected.", "labels": [], "entities": []}, {"text": "Combining the generative syntactic model and composite language model (GEN) with equal weight yielded a devtest BLEU score of only 0.4513, while discriminatively training the generative component models (GLOBAL) increased the score to 0.7679.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 112, "end_pos": 122, "type": "METRIC", "confidence": 0.9713433980941772}]}, {"text": "Using all features increased the score to 0.8083, while doubling the beam size (ALL+) pushed the score to 0.8210, indicating that search errors maybe an issue.", "labels": [], "entities": [{"text": "beam size (ALL+)", "start_pos": 69, "end_pos": 85, "type": "METRIC", "confidence": 0.8289063811302185}]}, {"text": "Ablation results show that leaving out the large-scale language model (NO-BIGLM) and dependency-ordering features (NO-DEPORD) substantially drops the score.", "labels": [], "entities": []}, {"text": "3 Focusing only on the 80.5% of the sentences for which a complete derivation was found (COMPLETE) yielded a score of 0.8668.", "labels": [], "entities": [{"text": "COMPLETE)", "start_pos": 89, "end_pos": 98, "type": "METRIC", "confidence": 0.906430184841156}]}, {"text": "By comparison, realization with the 3 All differences were statistically significant at p < 0.01 with paired bootstrap resampling on all sentences (NATIVE) yields a score more than five BLEU points higher, despite using inputs with more semantically-oriented relations and leaving out many function words, indicating that there is likely substantial room for improvement in the pre-processing and grammar induction process.", "labels": [], "entities": [{"text": "NATIVE", "start_pos": 148, "end_pos": 154, "type": "METRIC", "confidence": 0.8890169262886047}, {"text": "BLEU", "start_pos": 186, "end_pos": 190, "type": "METRIC", "confidence": 0.9992713332176208}, {"text": "grammar induction", "start_pos": 397, "end_pos": 414, "type": "TASK", "confidence": 0.690296933054924}]}, {"text": "Towards that end, we tried selecting the best derivations using several rounds of Viterbi EM with the generative syntactic model, but doing so did not improve realization quality.", "labels": [], "entities": []}, {"text": "A similar pattern is seen in the Section 23 results, with a competitive BLEU score of 0.8260 with the expanded beam, much higher than score of 0.675 with 38.8% coverage, the best previous score with a grammar-based system.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 72, "end_pos": 82, "type": "METRIC", "confidence": 0.9786833226680756}, {"text": "coverage", "start_pos": 160, "end_pos": 168, "type": "METRIC", "confidence": 0.9644628167152405}]}, {"text": "This score still trails the shared task scores of the top statistical dependency realizers by several points (STUMABA-S at 0.8911 and DCU at 0.8575), though it exceeds the score of a purpose-built system using no external resources (ATT at 0.6701).", "labels": [], "entities": [{"text": "DCU", "start_pos": 134, "end_pos": 137, "type": "DATASET", "confidence": 0.7546367049217224}]}, {"text": "In future work, we hope to close the gap with the top systems by integrating an improved ranking model into the induction process and resolving the remaining representational issues with problematic constructions.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Development set (Section 00) & test set  (Section 23) results, including exact match and  complete derivation percentages and BLEU scores", "labels": [], "entities": [{"text": "BLEU", "start_pos": 136, "end_pos": 140, "type": "METRIC", "confidence": 0.9982751607894897}]}]}