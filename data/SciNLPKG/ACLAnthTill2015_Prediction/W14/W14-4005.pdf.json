{"title": [], "abstractContent": [{"text": "In this paper we report the results of first experiments with HMEANT (a semi-automatic evaluation metric that assesses translation utility by matching semantic role fillers) on the Russian language.", "labels": [], "entities": [{"text": "HMEANT", "start_pos": 62, "end_pos": 68, "type": "METRIC", "confidence": 0.8640879392623901}]}, {"text": "We developed a web-based annotation interface and with its help evaluated practicability of this metric in the MT research and development process.", "labels": [], "entities": [{"text": "MT research and development process", "start_pos": 111, "end_pos": 146, "type": "TASK", "confidence": 0.8993000149726867}]}, {"text": "We studied reliability , language independence, labor cost and discriminatory power of HMEANT by evaluating English-Russian translation of several MT systems.", "labels": [], "entities": [{"text": "reliability", "start_pos": 11, "end_pos": 22, "type": "METRIC", "confidence": 0.9553656578063965}, {"text": "MT", "start_pos": 147, "end_pos": 149, "type": "TASK", "confidence": 0.9641602635383606}]}, {"text": "Role labeling and alignment were done by two groups of annotators-with linguistic background and without it.", "labels": [], "entities": [{"text": "Role labeling", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.8013885617256165}]}, {"text": "Experimental results were not univocal and changed from very high inter-annotator agreement in role labeling to much lower values at role alignment stage, good correlation of HMEANT with human ranking at the system level significantly decreased at the sentence level.", "labels": [], "entities": [{"text": "role labeling", "start_pos": 95, "end_pos": 108, "type": "TASK", "confidence": 0.7395155429840088}]}, {"text": "Analysis of experimental results and anno-tators' feedback suggests that HMEANT annotation guidelines need some adaptation for Russian.", "labels": [], "entities": []}], "introductionContent": [{"text": "Measuring translation quality is one of the most important tasks in MT, its history began long ago but most of the currently used approaches and metrics have been developed during the last two decades.", "labels": [], "entities": [{"text": "MT", "start_pos": 68, "end_pos": 70, "type": "TASK", "confidence": 0.9931255578994751}]}, {"text": "BLEU (), NIST) and METEOR ()metric require reference translation to compare it with MT output in fully automatic mode, which resulted in a dramatical speed-up for MT research and development.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9817636609077454}, {"text": "NIST", "start_pos": 9, "end_pos": 13, "type": "DATASET", "confidence": 0.838341474533081}, {"text": "METEOR", "start_pos": 19, "end_pos": 25, "type": "METRIC", "confidence": 0.9642860889434814}, {"text": "MT", "start_pos": 84, "end_pos": 86, "type": "TASK", "confidence": 0.9157819151878357}, {"text": "MT", "start_pos": 163, "end_pos": 165, "type": "TASK", "confidence": 0.9893430471420288}]}, {"text": "These metrics correlate with manual MT evaluation and provide reliable evaluation for many languages and for different types of MT systems.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 36, "end_pos": 49, "type": "TASK", "confidence": 0.9611636996269226}, {"text": "MT", "start_pos": 128, "end_pos": 130, "type": "TASK", "confidence": 0.9792124032974243}]}, {"text": "However, the major problem of popular MT evaluation metrics is that they aim to capture lexical similarity of MT output and reference translation (fluency), but fail to evaluate the semantics of translation according to the semantics of reference (adequacy) ().", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 38, "end_pos": 51, "type": "TASK", "confidence": 0.9151412844657898}, {"text": "MT output", "start_pos": 110, "end_pos": 119, "type": "TASK", "confidence": 0.8773873448371887}]}, {"text": "An alternative approach that is worth mentioning is the one proposed by, known as HTER, which measures the quality of machine translation in terms of post-editing.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 118, "end_pos": 137, "type": "TASK", "confidence": 0.723771721124649}]}, {"text": "This method was proved to correlate well with human adequacy judgments, though it was not designed fora task of gisting.", "labels": [], "entities": []}, {"text": "Moreover, HTER is not widely used in machine translation evaluation because of its high labor intensity.", "labels": [], "entities": [{"text": "HTER", "start_pos": 10, "end_pos": 14, "type": "TASK", "confidence": 0.6569387316703796}, {"text": "machine translation evaluation", "start_pos": 37, "end_pos": 67, "type": "TASK", "confidence": 0.8872809807459513}]}, {"text": "A family of metrics called MEANT was proposed in 2011 (, which approaches MT evaluation differently: it measures how much of an event structure of reference does machine translation preserve, utilizing shallow semantic parsing (MEANT metric) or human annotation (HMEANT) as a gold standard.", "labels": [], "entities": [{"text": "MEANT", "start_pos": 27, "end_pos": 32, "type": "METRIC", "confidence": 0.6512264609336853}, {"text": "MT evaluation", "start_pos": 74, "end_pos": 87, "type": "TASK", "confidence": 0.9443113505840302}, {"text": "shallow semantic parsing", "start_pos": 202, "end_pos": 226, "type": "TASK", "confidence": 0.6632873316605886}]}, {"text": "We applied HMEANT to anew languageRussian -and evaluated the usefulness of metric.", "labels": [], "entities": []}, {"text": "The practicability for the Russian language was studied with respect to the following criteria provided by: Reliability -measured as inter-annotator agreement for individual stages of evaluation task.", "labels": [], "entities": [{"text": "Reliability", "start_pos": 108, "end_pos": 119, "type": "METRIC", "confidence": 0.9852080345153809}]}, {"text": "Discriminatory Power -the correlation of rankings of four MT systems (by manual evaluation, BLEU and HMEANT) measured on a sentence and test set levels.", "labels": [], "entities": [{"text": "MT", "start_pos": 58, "end_pos": 60, "type": "TASK", "confidence": 0.9789101481437683}, {"text": "BLEU", "start_pos": 92, "end_pos": 96, "type": "METRIC", "confidence": 0.9986720085144043}, {"text": "HMEANT", "start_pos": 101, "end_pos": 107, "type": "METRIC", "confidence": 0.8667502403259277}]}, {"text": "Language Independence -we collected the problems with the original method and guidelines and compared these problems to those reported by and.", "labels": [], "entities": [{"text": "Language Independence", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.73110032081604}]}, {"text": "Efficiency -we studied the labor cost of annotation task, i. e. average time required to evaluate translations with HMEANT.", "labels": [], "entities": [{"text": "Efficiency", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.965349018573761}]}, {"text": "Besides, we tested the statement that semantic role labeling (SRL) does not require experienced annotators (in our case, with linguistic background).", "labels": [], "entities": [{"text": "semantic role labeling (SRL)", "start_pos": 38, "end_pos": 66, "type": "TASK", "confidence": 0.813791553179423}]}, {"text": "Although the problems of HMEANT were outlined before (by and) and several improvements were proposed, we decided to step back and conduct experiments with HMEANT in its original form.", "labels": [], "entities": [{"text": "HMEANT", "start_pos": 25, "end_pos": 31, "type": "DATASET", "confidence": 0.7685041427612305}]}, {"text": "No changes to the metric, except for the annotation interface enhancements, were made.", "labels": [], "entities": []}, {"text": "This paper has the following structure.", "labels": [], "entities": []}, {"text": "Section 2 reports the previous experiments with HMEANT; section 3 summarizes the methods behind HMEANT; section 4 -the settings for our own experiments; sections 5 and 6 are dedicated to results and discussion.", "labels": [], "entities": [{"text": "HMEANT", "start_pos": 48, "end_pos": 54, "type": "DATASET", "confidence": 0.8397470712661743}]}], "datasetContent": [{"text": "The underlying annotation cycle of HMEANT consists of two stages: semantic role labeling (SRL) and alignment.", "labels": [], "entities": [{"text": "semantic role labeling (SRL)", "start_pos": 66, "end_pos": 94, "type": "TASK", "confidence": 0.7190732210874557}]}, {"text": "During the SRL stage, each annotator is asked to mark all the frames (a predicate and associated roles) in reference translation and hypothesis translation.", "labels": [], "entities": [{"text": "SRL stage", "start_pos": 11, "end_pos": 20, "type": "TASK", "confidence": 0.9168327748775482}, {"text": "reference translation", "start_pos": 107, "end_pos": 128, "type": "TASK", "confidence": 0.7813241481781006}, {"text": "hypothesis translation", "start_pos": 133, "end_pos": 155, "type": "TASK", "confidence": 0.7205245643854141}]}, {"text": "To annotate a frame, one has to mark the frame head -predicate (which is a verb, but not a modal verb) and its arguments, role fillers, which are linked to that predicate.", "labels": [], "entities": []}, {"text": "These role fillers are given a role from the inventory of 11 roles (Lo and Wu, 2011a).", "labels": [], "entities": []}, {"text": "The role inventory is presented in, where each role corresponds to a specific question about the whole frame.", "labels": [], "entities": []}, {"text": "On the second stage, the annotators are asked to align the elements of frames from reference and hypothesis translations.", "labels": [], "entities": []}, {"text": "The annotators link both actions and roles, and these alignments can be matched as \"Correct\" or \"Partially Correct\" depending on how well the meaning was preserved.", "labels": [], "entities": []}, {"text": "We have used the original minimalistic guidelines for the SRL and alignment provided by in English with a small set of Russian examples.", "labels": [], "entities": [{"text": "SRL", "start_pos": 58, "end_pos": 61, "type": "TASK", "confidence": 0.9903088212013245}, {"text": "alignment", "start_pos": 66, "end_pos": 75, "type": "TASK", "confidence": 0.9627165198326111}]}], "tableCaptions": [{"text": " Table 2. The top four MT systems for the en-ru  translation task at WMT13. The scores were  calculated for the subset of translations which we  used in experiments.", "labels": [], "entities": [{"text": "MT", "start_pos": 23, "end_pos": 25, "type": "TASK", "confidence": 0.9813767075538635}, {"text": "en-ru  translation task", "start_pos": 42, "end_pos": 65, "type": "TASK", "confidence": 0.6466349065303802}, {"text": "WMT13", "start_pos": 69, "end_pos": 74, "type": "DATASET", "confidence": 0.945228099822998}]}, {"text": " Table 4. EWS over manual assessments, EWS  over HMEANT and BLEU scores for MT  systems.", "labels": [], "entities": [{"text": "EWS", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9956657290458679}, {"text": "EWS", "start_pos": 39, "end_pos": 42, "type": "METRIC", "confidence": 0.9969438910484314}, {"text": "HMEANT", "start_pos": 49, "end_pos": 55, "type": "METRIC", "confidence": 0.8020055890083313}, {"text": "BLEU", "start_pos": 60, "end_pos": 64, "type": "METRIC", "confidence": 0.994066596031189}, {"text": "MT", "start_pos": 76, "end_pos": 78, "type": "TASK", "confidence": 0.9815036058425903}]}, {"text": " Table 5. The rank correlation coefficients for  HMEANT and human judgments. Reliable  results (with p-value >0.05) are in bold.", "labels": [], "entities": [{"text": "rank correlation", "start_pos": 14, "end_pos": 30, "type": "METRIC", "confidence": 0.785872757434845}, {"text": "Reliable", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.9707177877426147}]}, {"text": " Table 6. The inter-annotator agreement for the  individual stages of annotation and alignment  procedures. Id, class, align stand for  identification, classification and alignment  respectively.", "labels": [], "entities": [{"text": "Id", "start_pos": 108, "end_pos": 110, "type": "METRIC", "confidence": 0.9559097290039062}]}, {"text": " Table 7. Most common role disagreements. Last  columns (L for linguists, D for developers) stand  for the ratio of times Role A was confused with  Role B across all the label types (roles, predicate,  none).", "labels": [], "entities": []}]}