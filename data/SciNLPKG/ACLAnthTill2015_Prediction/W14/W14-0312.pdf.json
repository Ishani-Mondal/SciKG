{"title": [{"text": "Confidence-based Active Learning Methods for Machine Translation", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 45, "end_pos": 64, "type": "TASK", "confidence": 0.7508576214313507}]}], "abstractContent": [{"text": "The paper presents experiments with active learning methods for the acquisition of training data in the context of machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 115, "end_pos": 134, "type": "TASK", "confidence": 0.7878762781620026}]}, {"text": "We propose a confidence-based method which is superior to the state-of-the-art method both in terms of quality and complexity.", "labels": [], "entities": []}, {"text": "Additionally, we discovered that oracle selection techniques that use real quality scores lead to poor results, making the effectiveness of confidence-driven methods of active learning for machine translation questionable.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 189, "end_pos": 208, "type": "TASK", "confidence": 0.7482070028781891}]}], "introductionContent": [{"text": "Active learning (AL) is a technique for the automatic selection of data which is most useful for model building.", "labels": [], "entities": [{"text": "Active learning (AL)", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8078593075275421}, {"text": "model building", "start_pos": 97, "end_pos": 111, "type": "TASK", "confidence": 0.7110813707113266}]}, {"text": "In the context of machine translation (MT), AL is particularly important as the acquisition of data often has a high cost, i.e. new source texts need to be translated manually.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 18, "end_pos": 42, "type": "TASK", "confidence": 0.8584186673164368}, {"text": "AL", "start_pos": 44, "end_pos": 46, "type": "TASK", "confidence": 0.5914565324783325}]}, {"text": "Thus it is beneficial to select for manual translation sentences which can lead to better translation quality.", "labels": [], "entities": []}, {"text": "The majority of AL methods for MT is based on the (dis)similarity of sentences with respect to the training data, with particular focus on domain adaptation.", "labels": [], "entities": [{"text": "MT", "start_pos": 31, "end_pos": 33, "type": "TASK", "confidence": 0.9965336322784424}, {"text": "domain adaptation", "start_pos": 139, "end_pos": 156, "type": "TASK", "confidence": 0.7152857333421707}]}, {"text": "suggest a TF-IDF metric to choose sentences with words absent in the training corpus.", "labels": [], "entities": []}, {"text": "propose a metric of informativeness relying on unseen ngrams.", "labels": [], "entities": []}, {"text": "Bloodgood and Callison-Burch (2010) use ngram frequency and coverage of the additional data as selection criteria.", "labels": [], "entities": [{"text": "Bloodgood", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.9560097455978394}, {"text": "ngram frequency", "start_pos": 40, "end_pos": 55, "type": "METRIC", "confidence": 0.7879031002521515}]}, {"text": "Their technique solicits translations for phrases instead of entire sentences, which saves user effort and leads to quality improvements even if the initial dataset is already sizeable.", "labels": [], "entities": []}, {"text": "A recent trend is to select source sentences based on an estimate of the quality of their translation by a baseline MT system.", "labels": [], "entities": [{"text": "MT", "start_pos": 116, "end_pos": 118, "type": "TASK", "confidence": 0.9640122652053833}]}, {"text": "It is assumed that if a sentence has been translated well with the existing data, it will not contribute to improving the translation quality.", "labels": [], "entities": []}, {"text": "If however a sentence has been translated erroneously, it might have words or phrases that are absent or incorrectly represented.", "labels": [], "entities": []}, {"text": "train a classifier to define the sentences to select.", "labels": [], "entities": []}, {"text": "The classifier uses a set of features of the source sentences and their automatic translations: n-grams and phrases frequency, MT model score, etc.", "labels": [], "entities": [{"text": "MT model score", "start_pos": 127, "end_pos": 141, "type": "METRIC", "confidence": 0.6346102555592855}]}, {"text": "build a pairwise classifier that ranks sentences according to the proportion of n-grams they contain that can cause errors.", "labels": [], "entities": []}, {"text": "For quality estimation, train language models of well and badly translated sentences.", "labels": [], "entities": []}, {"text": "The usefulness of a sentence is measured as the difference of its perplexities in these two language models.", "labels": [], "entities": []}, {"text": "In this research we also explore a quality-based AL technique.", "labels": [], "entities": []}, {"text": "Compared to its predecessors, our method is based on a more complex and therefore potentially more reliable quality estimation framework.", "labels": [], "entities": []}, {"text": "It uses wider range of features, which go beyond those used in previous work, covering information from both source and target sentences.", "labels": [], "entities": []}, {"text": "Another important novel feature in our work is the addition of real post-editions to the MT training data, as opposed to simulated post-editions (human reference translations) as in previous work on AL for MT.", "labels": [], "entities": [{"text": "MT training", "start_pos": 89, "end_pos": 100, "type": "TASK", "confidence": 0.8703270852565765}, {"text": "MT", "start_pos": 206, "end_pos": 208, "type": "TASK", "confidence": 0.966866135597229}]}, {"text": "As we show in section 3.2, adding post-editions leads to superior translation quality improvements.", "labels": [], "entities": []}, {"text": "Additionally, this is a suitable solution for \"human in the loop\" settings, as postediting automatically translated sentences tends to be faster and easier than translation from scratch (.", "labels": [], "entities": [{"text": "postediting automatically translated sentences", "start_pos": 79, "end_pos": 125, "type": "TASK", "confidence": 0.8930212259292603}]}, {"text": "Also, different from previous work, we do not focus on domain adaptation: our experiments involve only in-domain data.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 55, "end_pos": 72, "type": "TASK", "confidence": 0.7433429956436157}]}, {"text": "Compared to previous work on confidencedriven AL, our approach has led to better results, but these proved to be highly dependent on a sentence length bias.", "labels": [], "entities": [{"text": "confidencedriven AL", "start_pos": 29, "end_pos": 48, "type": "TASK", "confidence": 0.6025943011045456}]}, {"text": "However, an oracle-based selec-tion using true quality scores has not been shown to perform well.", "labels": [], "entities": []}, {"text": "This indicates that the usefulness of quality scores as AL selection criterion in the context of MT needs to be further investigated.", "labels": [], "entities": [{"text": "MT", "start_pos": 97, "end_pos": 99, "type": "TASK", "confidence": 0.9876518249511719}]}], "datasetContent": [{"text": "For the AL data selection experiment, two datasets are necessary: parallel sentences to train an initial, baseline MT system, and an additional pool of parallel sentences to select from.", "labels": [], "entities": [{"text": "AL data selection", "start_pos": 8, "end_pos": 25, "type": "TASK", "confidence": 0.7286515931288401}, {"text": "MT", "start_pos": 115, "end_pos": 117, "type": "TASK", "confidence": 0.9444106817245483}]}, {"text": "Our goal was to study potential improvements in the baseline MT system in a realistic \"human in the loop\" scenario, where source sentences are translated by the baseline system and post-edited by humans before they are added to the system.", "labels": [], "entities": [{"text": "MT", "start_pos": 61, "end_pos": 63, "type": "TASK", "confidence": 0.9507011771202087}]}, {"text": "As it has been shown in), post-editions tend to be closer to source sentences than freely created translations.", "labels": [], "entities": []}, {"text": "One of our research questions was to investigate whether they would be more useful to improve MT quality.", "labels": [], "entities": [{"text": "MT", "start_pos": 94, "end_pos": 96, "type": "TASK", "confidence": 0.9951220154762268}]}, {"text": "We chose the biggest corpus with machine translations and post-editions available to date: the LIG French-English post-editions corpus.", "labels": [], "entities": [{"text": "LIG French-English post-editions corpus", "start_pos": 95, "end_pos": 134, "type": "DATASET", "confidence": 0.8730277121067047}]}, {"text": "It contains 10,881 quadruples of the type: <source sentence, reference translation, automatic translation, post-edited automatic translation>.", "labels": [], "entities": []}, {"text": "Out of these, we selected 9,000 as the pool to be added to be baseline MT system, and the remaining 1,881 to train the QE system for the experiments with AL.", "labels": [], "entities": [{"text": "MT", "start_pos": 71, "end_pos": 73, "type": "TASK", "confidence": 0.849700927734375}]}, {"text": "For QE training, we use the HTER scores between MT and its post-edited version as computed by the TERp tool.", "labels": [], "entities": [{"text": "QE training", "start_pos": 4, "end_pos": 15, "type": "TASK", "confidence": 0.910778820514679}, {"text": "HTER", "start_pos": 28, "end_pos": 32, "type": "METRIC", "confidence": 0.9507445096969604}, {"text": "MT", "start_pos": 48, "end_pos": 50, "type": "TASK", "confidence": 0.551713228225708}]}, {"text": "We use the Moses toolkit with standard settings 2 to build the (baseline) statistical MT systems.", "labels": [], "entities": [{"text": "MT", "start_pos": 86, "end_pos": 88, "type": "TASK", "confidence": 0.8080036044120789}]}, {"text": "As training data, we use the FrenchEnglish News Commentary corpus released by the WMT13 shared task ().", "labels": [], "entities": [{"text": "FrenchEnglish News Commentary corpus released", "start_pos": 29, "end_pos": 74, "type": "DATASET", "confidence": 0.9828843355178833}, {"text": "WMT13 shared task", "start_pos": 82, "end_pos": 99, "type": "DATASET", "confidence": 0.8148018916447958}]}, {"text": "For the AL experiments, the size of the pool of additional data (10,000) poses a limitation.", "labels": [], "entities": []}, {"text": "To examine improvements obtained by adding fractions of up to only 9,000 sentences, we took a small random subset of the WMT13 data for these experiments (Table 1).", "labels": [], "entities": [{"text": "WMT13 data", "start_pos": 121, "end_pos": 131, "type": "DATASET", "confidence": 0.9347675442695618}]}, {"text": "Although these figures may seem small, the settings are realistic for many language pairs and text domains where larger data sets are simply not available.", "labels": [], "entities": []}, {"text": "We should also note that all the data used in our experiments belongs to the same domain: the LIG SMT system which produced sentences for the post-editions corpus was trained on Europarl and News commentary datasets (), but the post-edited sentences themselves were taken from news test sets released for WMT shared tasks in different years.", "labels": [], "entities": [{"text": "LIG", "start_pos": 94, "end_pos": 97, "type": "METRIC", "confidence": 0.8800033330917358}, {"text": "SMT", "start_pos": 98, "end_pos": 101, "type": "TASK", "confidence": 0.7614939212799072}, {"text": "Europarl", "start_pos": 178, "end_pos": 186, "type": "DATASET", "confidence": 0.988491415977478}, {"text": "News commentary datasets", "start_pos": 191, "end_pos": 215, "type": "DATASET", "confidence": 0.9257956544558207}, {"text": "WMT shared tasks", "start_pos": 305, "end_pos": 321, "type": "TASK", "confidence": 0.5900479555130005}]}, {"text": "Our baseline system is trained on a fraction of the news commentary corpus.", "labels": [], "entities": [{"text": "news commentary corpus", "start_pos": 52, "end_pos": 74, "type": "DATASET", "confidence": 0.6181273957093557}]}, {"text": "Finally, we tune and test all our systems on WMT shared task news news datasets (those which do not overlap with the post-editions corpus).", "labels": [], "entities": [{"text": "WMT shared task news news datasets", "start_pos": 45, "end_pos": 79, "type": "DATASET", "confidence": 0.8485955893993378}]}, {"text": "show that adding post-editions results in significantly better quality than adding the same number of reference translations . This effect can be seen even when the additional data corresponds to only a small fraction of the training data.", "labels": [], "entities": []}, {"text": "In addition, it does not seem to matter which MT system produced the translations which were then post-edited in the post-edition corpus.", "labels": [], "entities": [{"text": "MT", "start_pos": 46, "end_pos": 48, "type": "TASK", "confidence": 0.8899826407432556}]}, {"text": "Even if the output of a third-party system was used (as in our case), it improves the quality of machine translations for unseen data.", "labels": [], "entities": []}, {"text": "We assume that since posteditions tend to be closer to original sentences than free translations (Potet et al., 2012), they generally help produce better source-target alignments, leading to the extraction of good quality phrases.", "labels": [], "entities": []}, {"text": "In order to check the two hypotheses put forward in the previous section, we conduct two other sets of AL experiments: (i) a selection strategy that chooses longer sentences first (denoted as Length) and (ii) a selection strategy that chooses sentences with larger numbers of errors first (Errors).", "labels": [], "entities": [{"text": "Errors", "start_pos": 290, "end_pos": 296, "type": "METRIC", "confidence": 0.994216799736023}]}, {"text": "shows that a simple length-based strategy yields better results than any of the other tested strategies.", "labels": [], "entities": []}, {"text": "Therefore, in cases when the corpus has sufficient variation in sentence length, length-based selection might perform at least as well as other more sophisticated criteria.", "labels": [], "entities": []}, {"text": "The experiments with confidence-based selection described in) were free of this length bias, as sentences much longer or shorter than average were deliberately filtered out.", "labels": [], "entities": []}, {"text": "Interestingly, results for the Errors strategy are slightly worse than those for QuEst, although the former is guaranteed to choose sentences with the largest number of errors and has even stronger length bias than QuEst (see.", "labels": [], "entities": []}, {"text": "Therefore, the reasons hypothesised to be behind the superiority of QuEst over Oracle (longer sentences and larger number of errors) are actually not the only factors that influence the quality of an AL strategy.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Influence of post-edited and reference  translations on MT quality. Ref: baseline system  with added free references, PE: baseline system  with added post-editions.", "labels": [], "entities": [{"text": "MT", "start_pos": 66, "end_pos": 68, "type": "TASK", "confidence": 0.9556301236152649}]}]}