{"title": [{"text": "Crowdsourcing High-Quality Parallel Data Extraction from Twitter *", "labels": [], "entities": [{"text": "Crowdsourcing High-Quality Parallel Data Extraction", "start_pos": 0, "end_pos": 51, "type": "TASK", "confidence": 0.5090556144714355}]}], "abstractContent": [{"text": "High-quality parallel data is crucial fora range of multilingual applications, from tuning and evaluating machine translation systems to cross-lingual annotation projection.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 106, "end_pos": 125, "type": "TASK", "confidence": 0.6673110127449036}, {"text": "cross-lingual annotation projection", "start_pos": 137, "end_pos": 172, "type": "TASK", "confidence": 0.7529635230700175}]}, {"text": "Unfortunately, automatically obtained parallel data (which is available in relative abundance) tends to be quite noisy.", "labels": [], "entities": []}, {"text": "To obtain high-quality parallel data, we introduce a crowdsourcing paradigm in which workers with only basic bilingual proficiency identify translations from an automatically extracted corpus of parallel microblog messages.", "labels": [], "entities": []}, {"text": "For less than $350, we obtained over 5000 parallel segments in five language pairs.", "labels": [], "entities": []}, {"text": "Evaluated against expert annotations, the quality of the crowdsourced corpus is significantly better than existing automatic methods: it obtains an performance comparable to expert annotations when used in MERT tuning of a microblog MT system; and training a parallel sentence classifier with it leads also to improved results.", "labels": [], "entities": [{"text": "MERT tuning", "start_pos": 206, "end_pos": 217, "type": "TASK", "confidence": 0.8880490660667419}]}, {"text": "The crowdsourced corpora will be made available in", "labels": [], "entities": []}], "introductionContent": [{"text": "High-quality parallel data is essential for tuning and evaluating statistical MT systems, and it plays a role in a wide range of multilingual NLP applications, such as word sense disambiguation (), paraphrasing), annotation projection (, and other language-specific applications (Schwarck et al., * A sample of the crowdsourced corpora and the interfaces used are available as supplementary material.;.", "labels": [], "entities": [{"text": "MT", "start_pos": 78, "end_pos": 80, "type": "TASK", "confidence": 0.8284505009651184}, {"text": "word sense disambiguation", "start_pos": 168, "end_pos": 193, "type": "TASK", "confidence": 0.6498233477274576}, {"text": "annotation projection", "start_pos": 213, "end_pos": 234, "type": "TASK", "confidence": 0.7619379162788391}]}, {"text": "While large amounts of parallel data can be easily obtained by mining the web, comparable corpora (, and even social media sites (, automatically extracted parallel tends to be noisy, and, as a result, \"evaluation-quality\" parallel corpora have generally been produced at considerable expense by targeted translation efforts (.", "labels": [], "entities": []}, {"text": "Unfortunately, in some domains such as microblogs, the only corpora that are available are automatically extracted and noisy.", "labels": [], "entities": []}, {"text": "While phrase-based translation models can effectively learn translation rules from noisy parallel data (, having a subset of highquality parallel segments is nevertheless crucial.", "labels": [], "entities": [{"text": "phrase-based translation", "start_pos": 6, "end_pos": 30, "type": "TASK", "confidence": 0.6003032326698303}]}, {"text": "Firstly, the automatic parallel data extraction system's parameters can be tuned by optimizing on the gold standard data.", "labels": [], "entities": [{"text": "parallel data extraction", "start_pos": 23, "end_pos": 47, "type": "TASK", "confidence": 0.639498899380366}]}, {"text": "Secondly, even though the parallel data used to train MT systems can contain a considerable amount of noise, it is conventional to use human annotated parallel data to tune and evaluate the system.", "labels": [], "entities": [{"text": "MT", "start_pos": 54, "end_pos": 56, "type": "TASK", "confidence": 0.9890690445899963}]}, {"text": "Finally, other NLP applications may not be as noise-robust as MT.", "labels": [], "entities": [{"text": "MT", "start_pos": 62, "end_pos": 64, "type": "TASK", "confidence": 0.8815978169441223}]}, {"text": "We introduce anew crowdsourcing protocol for obtaining high-quality parallel data from noisy, automatically extracted parallel data ( \u00a73), focusing on the challenging case of identifying parallel data in microblog messages (.", "labels": [], "entities": []}, {"text": "In contrast to previous attempts to use crowdsourcing to obtain parallel data, in which workers performed translation, our approach only requires that they identify whether a candidate message contains a translation, and if so, what the spans of the translated segments are.", "labels": [], "entities": []}, {"text": "This is a much simpler task than translation, and one that can often be completed by workers with only a basic proficiency in the source and target languages.", "labels": [], "entities": [{"text": "translation", "start_pos": 33, "end_pos": 44, "type": "TASK", "confidence": 0.9784244298934937}]}, {"text": "For evaluation ( \u00a74), we use our protocol to build parallel datasets on a Chinese-English corpus originally extracted from Sina Weibo and for which we have expert annotations.", "labels": [], "entities": []}, {"text": "This lets us quantify the effectiveness of our method under different task variations.", "labels": [], "entities": []}, {"text": "We also show that the crowdsourced corpus performs as well as expert annotation (and better than the automatically extracted corpus) for tuning an MT system with MERT.", "labels": [], "entities": [{"text": "MT", "start_pos": 147, "end_pos": 149, "type": "TASK", "confidence": 0.9787535667419434}]}, {"text": "We next apply our method on a corpus of five language pairs (enar, en-ja, en-ko, en-ru, en-zh) extracted from Twitter ( \u00a75), for which we have no gold-standard data.", "labels": [], "entities": []}, {"text": "Using this data in a cross-validation setup, we train and evaluate a maxent classifier for detecting parallel data ( \u00a76), and then we conclude ( \u00a77).", "labels": [], "entities": []}], "datasetContent": [{"text": "To obtain results on the effectiveness of the methods described in Section 3, we will first perform experiments using pre-annotated data.", "labels": [], "entities": []}, {"text": "We use the annotated dataset with tweets in Mandarin-English from Sina Weibo created in (.", "labels": [], "entities": []}, {"text": "It consists of approximately 4000 tweets crawled from Sina Weibo that were annotated on whether they contained parallel data and the location of the parallel segments.", "labels": [], "entities": [{"text": "Sina Weibo", "start_pos": 54, "end_pos": 64, "type": "DATASET", "confidence": 0.899311751127243}]}, {"text": "In our experiment, we sample 1000 tweets from this dataset, where 602 tweets were parallel and 398 were not.", "labels": [], "entities": []}, {"text": "We will not submit the same tasks using different setups, since we would have to pay the cost of the tasks multiple times.", "labels": [], "entities": []}, {"text": "Furthermore, we know the answers for all the questions in this controlled experiment, the quality of a job can be evaluated precisely by using all questions as references.", "labels": [], "entities": []}, {"text": "Thus, we will perform the task once, with a larger number of workers and accepting and rejecting jobs based on their real quality.", "labels": [], "entities": []}, {"text": "Then, we will use the resulting datasets and simulate the conditions using different setups.: Agreement with the expert annotations for different acceptors.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Agreement with the expert annotations  for different acceptors.", "labels": [], "entities": []}, {"text": " Table 2: Parallel post prediction scores using dif- ferent acceptors.", "labels": [], "entities": []}, {"text": " Table 3: Identification scores for different n.", "labels": [], "entities": [{"text": "Identification scores", "start_pos": 10, "end_pos": 31, "type": "METRIC", "confidence": 0.9132451117038727}]}, {"text": " Table 4: Parallel data location scores for different  acceptors (rows) and different numbers of work- ers. Each cell denotes the WER for that setup.", "labels": [], "entities": [{"text": "WER", "start_pos": 130, "end_pos": 133, "type": "METRIC", "confidence": 0.9959139227867126}]}, {"text": " Table 5: BLEU score comparison using different  corpora for MERT tuning. The Size row denotes  the number of sentences of each corpus, and the  EN-ZH and ZH-EN rows denote the BLEU scores  of the respective language pair and tuning dataset.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9745845794677734}, {"text": "MERT tuning", "start_pos": 61, "end_pos": 72, "type": "TASK", "confidence": 0.9484165906906128}, {"text": "BLEU", "start_pos": 177, "end_pos": 181, "type": "METRIC", "confidence": 0.9978210926055908}]}, {"text": " Table 7: AMT costs for crowdsourced corpora  from Twitter.", "labels": [], "entities": [{"text": "AMT", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.6578248143196106}]}, {"text": " Table 6: Classification Results using a 10-fold cross validation over different datasets. Each cell contains  the F-measure using a given dataset and an incremental set of features.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 115, "end_pos": 124, "type": "METRIC", "confidence": 0.9518940448760986}]}]}