{"title": [{"text": "Exploration of the Impact of Maximum Entropy in Recurrent Neural Network Language Models for Code-Switching Speech", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper presents our latest investigations of the jointly trained maximum en-tropy and recurrent neural network language models for Code-Switching speech.", "labels": [], "entities": []}, {"text": "First, we explore extensively the integration of part-of-speech tags and language identifier information in recurrent neu-ral network language models for Code-Switching.", "labels": [], "entities": []}, {"text": "Second, the importance of the maximum entropy model is demonstrated along with a various of experimental results.", "labels": [], "entities": []}, {"text": "Finally, we propose to adapt the recurrent neural network language model to different Code-Switching behaviors and use them to generate artificial Code-Switching text data.", "labels": [], "entities": []}], "introductionContent": [{"text": "The term Code-Switching (CS) denotes speech which contains more than one language.", "labels": [], "entities": []}, {"text": "Speakers switch their language while they are talking.", "labels": [], "entities": []}, {"text": "This phenomenon appears very often in multilingual communities, such as in India, Hong Kong or Singapore.", "labels": [], "entities": []}, {"text": "Furthermore, it increasingly occurs in former monolingual cultures due to the strong growth of globalization.", "labels": [], "entities": []}, {"text": "In many contexts and domains, speakers switch more often between their native language and English within their utterances than in the past.", "labels": [], "entities": []}, {"text": "This is a challenge for speech recognition systems which are typically monolingual.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 24, "end_pos": 42, "type": "TASK", "confidence": 0.7831640541553497}]}, {"text": "While there have been promising approaches to handle Code-Switching in the field of acoustic modeling, language modeling is still a great challenge.", "labels": [], "entities": [{"text": "acoustic modeling", "start_pos": 84, "end_pos": 101, "type": "TASK", "confidence": 0.7548371851444244}, {"text": "language modeling", "start_pos": 103, "end_pos": 120, "type": "TASK", "confidence": 0.7227489501237869}]}, {"text": "The main reason is a shortage of training data.", "labels": [], "entities": []}, {"text": "Whereas about 50h of training data might be sufficient for the estimation of acoustic models, the transcriptions of these data are not enough to build reliable language models.", "labels": [], "entities": []}, {"text": "In this paper, we focus on exploring and improving the language model for Code-switching speech and as a result improve the automatic speech recognition (ASR) system on Code-Switching speech.", "labels": [], "entities": [{"text": "automatic speech recognition (ASR)", "start_pos": 124, "end_pos": 158, "type": "TASK", "confidence": 0.7761765718460083}]}, {"text": "The main contribution of the paper is the extensive investigation of jointly trained maximum entropy (ME) and recurrent neural language models (RNN LMs) for Code-Switching speech.", "labels": [], "entities": []}, {"text": "We revisit the integration of part-of-speech (POS) tags and language identifier (LID) information in recurrent neural network language models and the impact of maximum entropy on the language model performance.", "labels": [], "entities": []}, {"text": "As follow-up to our previous work in (Adel, ), here we investigate whether a recurrent neural network alone without using ME is a suitable model for Code-Switching speech.", "labels": [], "entities": []}, {"text": "Afterwards, to directly use the RNN LM in the decoding process of an ASR system, we convert the RNN LM into the n-gram language model using the text generation approach); Furthermore motivated by the fact that Code-Switching is speaker dependent, we first adapt the recurrent neural network language model to different Code-Switching behaviors and then generate artificial Code-Switching text data.", "labels": [], "entities": [{"text": "ASR", "start_pos": 69, "end_pos": 72, "type": "TASK", "confidence": 0.9515272378921509}, {"text": "text generation", "start_pos": 144, "end_pos": 159, "type": "TASK", "confidence": 0.7160724103450775}]}, {"text": "This allows us to train an accurate n-gram model which can be used directly during decoding to improve ASR performance.", "labels": [], "entities": [{"text": "ASR", "start_pos": 103, "end_pos": 106, "type": "TASK", "confidence": 0.9883612990379333}]}, {"text": "The paper is organized as follows: Section 2 gives a short overview of related works.", "labels": [], "entities": []}, {"text": "In Section 3, we describe the jointly trained maximum entropy and recurrent neural network language models and their extension for Code-Switching speech.", "labels": [], "entities": []}, {"text": "Section 4 gives a short description of the SEAME corpus.", "labels": [], "entities": [{"text": "SEAME corpus", "start_pos": 43, "end_pos": 55, "type": "DATASET", "confidence": 0.8064140677452087}]}, {"text": "In Section 5, we summarize the most important experiments and results.", "labels": [], "entities": []}, {"text": "The study is concluded in Section 6 with a summary.", "labels": [], "entities": []}], "datasetContent": [{"text": "This section presents all the experiments and results regarding language models and ASR on the development and the evaluation set of the SEAME corpus.", "labels": [], "entities": [{"text": "ASR", "start_pos": 84, "end_pos": 87, "type": "TASK", "confidence": 0.8905942440032959}, {"text": "SEAME corpus", "start_pos": 137, "end_pos": 149, "type": "DATASET", "confidence": 0.7820232212543488}]}, {"text": "However, the parameters were tuned only on the development set.", "labels": [], "entities": []}, {"text": "For the ASR experiments, we applied BioKIT, a dynamic one-pass decoder ().", "labels": [], "entities": [{"text": "ASR", "start_pos": 8, "end_pos": 11, "type": "TASK", "confidence": 0.9831050038337708}, {"text": "BioKIT", "start_pos": 36, "end_pos": 42, "type": "METRIC", "confidence": 0.9609213471412659}]}, {"text": "The acoustic model is speaker independent and has been trained with all the training data.", "labels": [], "entities": []}, {"text": "To extract the features, we first trained a multilayer perceptron (MLP) with a small hidden layer with 40 nodes.", "labels": [], "entities": []}, {"text": "The output of this hidden layer is called bottleneck features and is used to train the acoustic model.", "labels": [], "entities": []}, {"text": "The MLP has been initialized with a multilingual multilayer perceptron as described in ( ).", "labels": [], "entities": []}, {"text": "The phone set contains English and Mandarin phones, filler models for continuous speech (+noise+, +breath+, +laugh+) and an additional phone +particle+ for Singaporean and Malayan particles.", "labels": [], "entities": []}, {"text": "The acoustic model applied a fully-continuous 3-state left-to-right HMM.", "labels": [], "entities": []}, {"text": "The emission probabilities were modeled with Gaussian mixture models.", "labels": [], "entities": []}, {"text": "We used a context dependent acoustic model with 3,500 quintphones.", "labels": [], "entities": []}, {"text": "Mergeand-split training was applied followed by six iterations of Viterbi training.", "labels": [], "entities": []}, {"text": "To obtain a dictionary, the CMU English (CMU Dictionary, 2014) and Mandarin () pronunciation dictionaries were merged into one bilingual pronunciation dictionary.", "labels": [], "entities": [{"text": "CMU English (CMU Dictionary, 2014)", "start_pos": 28, "end_pos": 62, "type": "DATASET", "confidence": 0.9503295570611954}]}, {"text": "Additionally, several rules from were applied which generate pronunciation variants for Singaporean English.", "labels": [], "entities": []}, {"text": "As a performance measure for decoding CodeSwitching speech, we used the mixed error rate (MER) which applies word error rates to English and character error rates to Mandarin segments ( ).", "labels": [], "entities": [{"text": "mixed error rate (MER)", "start_pos": 72, "end_pos": 94, "type": "METRIC", "confidence": 0.9261848032474518}]}, {"text": "With character error rates for Mandarin, the performance can be compared across different word segmentations.", "labels": [], "entities": []}, {"text": "Table 9 shows the results of the baseline CS 3-gram LM, the 3-gram LM trained with 235M artificial words interpolated with CS 3-gram LM and the final 3-gram LM described in the previous section.", "labels": [], "entities": []}, {"text": "Compared to the baseline system, we are able to improve the MER by up to 3% relative.", "labels": [], "entities": [{"text": "MER", "start_pos": 60, "end_pos": 63, "type": "METRIC", "confidence": 0.9918501377105713}]}, {"text": "Furthermore, a very small gain can be observed by using the Code-Switching attitude dependent language model compared to the unadapted best RNN-ME LM.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of the SEAME corpus", "labels": [], "entities": [{"text": "SEAME", "start_pos": 28, "end_pos": 33, "type": "TASK", "confidence": 0.5226992964744568}]}, {"text": " Table 2: Effect of output layer factorization", "labels": [], "entities": []}, {"text": " Table 3: Effect of ME on the POS integration into  the input layer", "labels": [], "entities": [{"text": "ME", "start_pos": 20, "end_pos": 22, "type": "METRIC", "confidence": 0.9919689297676086}]}, {"text": " Table 4: Effect of ME on the integration of POS  and the output layer factorization using LID", "labels": [], "entities": [{"text": "ME", "start_pos": 20, "end_pos": 22, "type": "METRIC", "confidence": 0.9955589771270752}]}, {"text": " Table 5: Effect of the BPTT step", "labels": [], "entities": [{"text": "BPTT", "start_pos": 24, "end_pos": 28, "type": "METRIC", "confidence": 0.7577340602874756}]}, {"text": " Table 6: Effect of the direct order", "labels": [], "entities": []}, {"text": " Table 7: Effect of the number of direct connections", "labels": [], "entities": []}, {"text": " Table 8: PPL of the N-gram models trained with  artificial text data", "labels": [], "entities": []}, {"text": " Table 9: ASR results on SEAME data", "labels": [], "entities": [{"text": "ASR", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.869893491268158}, {"text": "SEAME", "start_pos": 25, "end_pos": 30, "type": "TASK", "confidence": 0.5181530117988586}]}]}