{"title": [{"text": "Beyond Linguistic Equivalence. An Empirical Study of Translation Evaluation in a Translation Learner Corpus", "labels": [], "entities": [{"text": "Translation Evaluation", "start_pos": 53, "end_pos": 75, "type": "TASK", "confidence": 0.9678517580032349}]}], "abstractContent": [{"text": "The realisation that fully automatic translation in many settings is still far from producing output that is equal or superior to human translation has lead to an intense interest in translation evaluation in the MT community.", "labels": [], "entities": [{"text": "translation evaluation", "start_pos": 183, "end_pos": 205, "type": "TASK", "confidence": 0.9758173823356628}, {"text": "MT", "start_pos": 213, "end_pos": 215, "type": "TASK", "confidence": 0.9821135401725769}]}, {"text": "However, research in this field, by now, has not only largely ignored the tremendous amount of relevant knowledge available in a closely related discipline, namely translation studies, but also failed to provide a deeper understanding of the nature of \"translation errors\" and \"translation quality\".", "labels": [], "entities": [{"text": "translation studies", "start_pos": 164, "end_pos": 183, "type": "TASK", "confidence": 0.9665760397911072}]}, {"text": "This paper presents an empirical take on the latter concept, translation quality, by comparing human and automatic evaluations of learner translations in the KOPTE corpus.", "labels": [], "entities": [{"text": "translation quality", "start_pos": 61, "end_pos": 80, "type": "TASK", "confidence": 0.7736740112304688}, {"text": "KOPTE corpus", "start_pos": 158, "end_pos": 170, "type": "DATASET", "confidence": 0.8634450435638428}]}, {"text": "We will show that translation studies provide sophisticated concepts for translation quality estimation and error annotation.", "labels": [], "entities": [{"text": "translation", "start_pos": 18, "end_pos": 29, "type": "TASK", "confidence": 0.9623822569847107}, {"text": "translation quality estimation", "start_pos": 73, "end_pos": 103, "type": "TASK", "confidence": 0.7938438057899475}]}, {"text": "Moreover , by applying well-established MT evaluation scores, namely BLEU and Meteor , to KOPTE learner translations that were graded by a human expert, we hope to shed light on properties (and potential shortcomings) of these scores.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 40, "end_pos": 53, "type": "TASK", "confidence": 0.8617212772369385}, {"text": "BLEU", "start_pos": 69, "end_pos": 73, "type": "METRIC", "confidence": 0.997407853603363}, {"text": "KOPTE learner translations", "start_pos": 90, "end_pos": 116, "type": "TASK", "confidence": 0.5783676107724508}]}], "introductionContent": [], "datasetContent": [{"text": "Human evaluation of MT output is performed in different ways.", "labels": [], "entities": [{"text": "MT", "start_pos": 20, "end_pos": 22, "type": "TASK", "confidence": 0.9735677242279053}]}, {"text": "The most frequently used evaluation method seems to be a simple ranking of translated sentences by a \"reasonable number of evaluators\").", "labels": [], "entities": []}, {"text": "According to, this form of evaluation was used, among others, during the last STATMT workshops and can thus be considered rather popular.", "labels": [], "entities": [{"text": "STATMT", "start_pos": 78, "end_pos": 84, "type": "TASK", "confidence": 0.6319239735603333}]}, {"text": "AP-PRAISE) is a tool that can be used for such as task, since it allows for the manual ranking of sentences, quality estimation, error annotation and post-editing.", "labels": [], "entities": []}, {"text": "Other forms of evaluation, however, exist.", "labels": [], "entities": []}, {"text": "For example, propose HMEANT, an evaluation score based on MEANT (Lo and Wu, 2011), a semi-automatic MT quality score that measures the degree of meaning preservation by comparing verb frames and semantic roles of hypothesis translations to their respective counterparts in the reference translation(s).", "labels": [], "entities": [{"text": "MEANT", "start_pos": 58, "end_pos": 63, "type": "METRIC", "confidence": 0.9845600128173828}, {"text": "MT", "start_pos": 100, "end_pos": 102, "type": "TASK", "confidence": 0.9733150005340576}, {"text": "meaning preservation", "start_pos": 145, "end_pos": 165, "type": "TASK", "confidence": 0.7197243571281433}]}, {"text": "Unfortunately, report difficulty in producing coherent role alignments between hypotheses and translations, a problem that affects the final HMEANT score calculation.", "labels": [], "entities": [{"text": "HMEANT score", "start_pos": 141, "end_pos": 153, "type": "METRIC", "confidence": 0.5897429436445236}]}, {"text": "This, however, seems hardly surprising given the difficulty of the annotation task (although, following the authors' description, some familiarity of the annotators with the linguistic key concepts can be assumed) and the fact that guidelines and training are meant to be minimal.", "labels": [], "entities": []}, {"text": "Another (indirect) human evaluation method for MT that is also employed for error analysis are reading comprehension tests (e.g.,).", "labels": [], "entities": [{"text": "MT", "start_pos": 47, "end_pos": 49, "type": "TASK", "confidence": 0.9966779947280884}]}, {"text": "Moreover, HTER) is a TER-based repair-oriented metric which uses human annotators (the only apparent qualificational requirement being fluency in the target language) to generate \"targeted\" reference translations by post-editing the MT output or the existing reference translations, following the goal to find the shortest path between the hypothesis and a \"correct\" reference.", "labels": [], "entities": [{"text": "HTER", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.6113726496696472}]}, {"text": "report a high correlation between evaluation with HTER and traditional human adequacy and fluency judgements.", "labels": [], "entities": []}, {"text": "Last but not least, Somers (2011) mentions other repairoriented measures such as post-editing effort measured by the amount of key-strokes or time spent on producing a \"correct\" translation on the basis of MT output.", "labels": [], "entities": [{"text": "repairoriented", "start_pos": 49, "end_pos": 63, "type": "METRIC", "confidence": 0.8935158252716064}, {"text": "MT", "start_pos": 206, "end_pos": 208, "type": "TASK", "confidence": 0.9421917200088501}]}, {"text": "Student translations were evaluated by one of the authors, an experienced translation teacher, with the aim of giving feedback to students.", "labels": [], "entities": []}, {"text": "All translations were graded and errors as well as good solutions were marked in the text according to a fine-grained evaluation scheme.", "labels": [], "entities": []}, {"text": "In this scheme, the weight of evaluated items is indicated through numbers ranging from plus/minus 1 (minor) to plus/minus 8 (major).", "labels": [], "entities": []}, {"text": "Based on these evaluations, each translation was assigned a final grade according to the German grading system on a scale ranging from 1 (\"very good\") to 6 (\"highly erroneous\") with in-between intervals at the levels of .0, .3 and .7.", "labels": [], "entities": []}, {"text": "To calculate this grade, positive and negative evaluations were summed up separately, before the negative score was subtracted from the positive one.", "labels": [], "entities": []}, {"text": "A score of around zero corresponds to the grade \"good\" (=2), to achieve \"very good\" (=1) the student needs a surplus of positive evaluations.", "labels": [], "entities": []}, {"text": "The evaluation scheme based on which student translations are graded is divided into external and internal factors.", "labels": [], "entities": []}, {"text": "External characteristics describe the communicative situation given by the source text and the translation brief (author, recipient, medium, location, time).", "labels": [], "entities": []}, {"text": "Internal factors, on the other hand, comprise eight categories: form, structure, cohesion, stylistics/register, grammar, lexis/semantics, translation-specific problems, function.", "labels": [], "entities": []}, {"text": "These categories are containers for more fine-grained criteria which can be applied to segments of the (source or target) text or even to the whole text, depending on the nature of the criterion.", "labels": [], "entities": []}, {"text": "Some internal subcriteria of the scheme are summarised in.", "labels": [], "entities": []}, {"text": "A quantitative analysis of error types in KOPTE shows that semantic/lexical errors are by far the most common error in the student translations.", "labels": [], "entities": [{"text": "KOPTE", "start_pos": 42, "end_pos": 47, "type": "DATASET", "confidence": 0.700782060623169}]}, {"text": "Evaluations in KOPTE were carried out by just one evaluator for the reason that, in a classroom setting, multiple evaluations are not feasible.", "labels": [], "entities": [{"text": "KOPTE", "start_pos": 15, "end_pos": 20, "type": "DATASET", "confidence": 0.691776692867279}]}, {"text": "Although multiple evaluations would have been considered highly valuable, the data available from KOPTE was evaluated by an experienced translation scholar with long-standing experience in teaching translation.", "labels": [], "entities": [{"text": "KOPTE", "start_pos": 98, "end_pos": 103, "type": "DATASET", "confidence": 0.8554213047027588}]}, {"text": "Moreover, the evaluation scheme is much more detailed than error annotation schemes that are normally described in the literature and it is theoretically well-motivated.", "labels": [], "entities": []}, {"text": "An analysis of the median grades in our data sample (compare Tables 2-4) shows that grading varies only slightly between different texts, considering the maximum variation potential ranging from 1 to 6, and thus can be considered consistent.", "labels": [], "entities": []}, {"text": "The goal of our experiments was to study whether the human translation expert judgements in KOPTE can be mimicked using simple automatic quality metrics as used in MT, namely BLEU and Meteor.", "labels": [], "entities": [{"text": "MT", "start_pos": 164, "end_pos": 166, "type": "TASK", "confidence": 0.7303346395492554}, {"text": "BLEU", "start_pos": 175, "end_pos": 179, "type": "METRIC", "confidence": 0.9898607730865479}]}, {"text": "More specifically, we aim at: \u2022 studying how automatic evaluation scores relate to fine-grained human expert evaluations, \u2022 investigating whether a higher number of references improves the automatic scores and why (or why not), \u2022 examining whether a higher number of references provides more reliable evaluation scores as measured by an improved correlation with the human expert judgments.", "labels": [], "entities": []}, {"text": "In order to study the behaviour of automatic MT evaluation scores, we conducted three experiments by applying IBM BLEU () and Meteor) to a sample of KOPTE translations that were produced by translation students preparing for their final master's exams.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 45, "end_pos": 58, "type": "TASK", "confidence": 0.9155349433422089}, {"text": "IBM", "start_pos": 110, "end_pos": 113, "type": "DATASET", "confidence": 0.4652848243713379}, {"text": "BLEU", "start_pos": 114, "end_pos": 118, "type": "METRIC", "confidence": 0.9378130435943604}]}, {"text": "Scores were calculated on the complete texts.", "labels": [], "entities": []}, {"text": "To evaluate the overall performance of the automatic evaluation scores on these texts, we calculated Kendall's rank correlation coefficient for each text following the procedure described in.", "labels": [], "entities": [{"text": "Kendall's rank correlation coefficient", "start_pos": 101, "end_pos": 139, "type": "METRIC", "confidence": 0.8515807628631592}]}, {"text": "Correlations were calculated for: \u2022 the human expert grades and BLEU scores for each translation, \u2022 the human expert grades and Meteor scores for each translation, \u2022 BLEU and Meteor scores for each translation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 64, "end_pos": 68, "type": "METRIC", "confidence": 0.9988403916358948}, {"text": "BLEU", "start_pos": 166, "end_pos": 170, "type": "METRIC", "confidence": 0.9984927177429199}]}, {"text": "Ina first experiment, we applied the automatic evaluation scores to the source texts given in Table 2, choosing, for each text, the student translation with the best human grade as reference translation.", "labels": [], "entities": []}, {"text": "The median human grades as well as mean BLEU and Meteor and correlation scores obtained for each text (excluding the reference translation) are included in.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 40, "end_pos": 44, "type": "METRIC", "confidence": 0.9859390258789062}, {"text": "Meteor", "start_pos": 49, "end_pos": 55, "type": "DATASET", "confidence": 0.714248776435852}, {"text": "correlation", "start_pos": 60, "end_pos": 71, "type": "METRIC", "confidence": 0.9960920214653015}]}, {"text": "Ina second experiment, we repeated this procedure, however, using a set of three reference translations.", "labels": [], "entities": []}, {"text": "Finally, in a last experiment we used five reference translations selected according to their human expert grade.", "labels": [], "entities": []}, {"text": "In both steps, source texts for which less than four hypotheses were available were excluded from the data sets.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Source texts, number of human translations per source text, median of the obtained grade per source text, mean of the  BLEU and Meteor scores per source text and Kendall's rank correlation coefficients for the first experiment.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 129, "end_pos": 133, "type": "METRIC", "confidence": 0.9967555403709412}, {"text": "Kendall's rank correlation", "start_pos": 172, "end_pos": 198, "type": "METRIC", "confidence": 0.5818064957857132}]}, {"text": " Table 3: Source texts, number of human translations per source text, median of the obtained grade per source text, mean of the  BLEU and Meteor scores per source text and Kendall's rank correlation coefficients for the second experiment.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 129, "end_pos": 133, "type": "METRIC", "confidence": 0.996238112449646}, {"text": "Kendall's rank correlation", "start_pos": 172, "end_pos": 198, "type": "METRIC", "confidence": 0.6013043597340584}]}, {"text": " Table 4: Source texts, number of human translations per source text, median of the obtained grade per source text, mean of the  BLEU and Meteor scores per source text and Kendall's rank correlation coefficients for the third experiment.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 129, "end_pos": 133, "type": "METRIC", "confidence": 0.996330201625824}, {"text": "Kendall's rank correlation", "start_pos": 172, "end_pos": 198, "type": "METRIC", "confidence": 0.5947154611349106}]}]}