{"title": [{"text": "Sequential Labeling for Tracking Dynamic Dialog States", "labels": [], "entities": [{"text": "Sequential Labeling", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8946655094623566}]}], "abstractContent": [{"text": "This paper presents a sequential labeling approach for tracking the dialog states for the cases of goal changes in a dialog session.", "labels": [], "entities": []}, {"text": "The tracking models are trained using linear-chain conditional random fields with the features obtained from the results of SLU.", "labels": [], "entities": []}, {"text": "The experimental results show that our proposed approach can improve the performances of the sub-tasks of the second dialog state tracking challenge.", "labels": [], "entities": [{"text": "dialog state tracking challenge", "start_pos": 117, "end_pos": 148, "type": "TASK", "confidence": 0.8014863282442093}]}], "introductionContent": [{"text": "A dialog manager is one of the key components of a dialog system, which aims at determining the system actions to generate appropriate responses to users.", "labels": [], "entities": []}, {"text": "To make the system capable of conducting a dialog in a more natural and effective manner, the dialog manager should take into account not only a given user utterance itself, but also the dialog state which represents various conversational situations obtained from the dialog session progress.", "labels": [], "entities": []}, {"text": "Dialog state tracking is a sub-task of dialog management that analyzes and maintains this dialog state at each moment.", "labels": [], "entities": [{"text": "Dialog state tracking", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.7896676063537598}, {"text": "dialog management", "start_pos": 39, "end_pos": 56, "type": "TASK", "confidence": 0.8757171034812927}]}, {"text": "The major obstacle to dialog state tracking is that the inputs to the tracker are likely to be noisy because of the errors produced by automatic speech recognition (ASR) and spoken language understanding (SLU) processes which are required to be performed prior to the tracking.", "labels": [], "entities": [{"text": "dialog state tracking", "start_pos": 22, "end_pos": 43, "type": "TASK", "confidence": 0.8597496946652731}, {"text": "automatic speech recognition (ASR) and spoken language understanding (SLU)", "start_pos": 135, "end_pos": 209, "type": "TASK", "confidence": 0.7527955495394193}]}, {"text": "Thus, many researchers have focused on improving the robustness of dialog state trackers against ASR and SLU errors.", "labels": [], "entities": [{"text": "dialog state trackers", "start_pos": 67, "end_pos": 88, "type": "TASK", "confidence": 0.7303836146990458}, {"text": "ASR", "start_pos": 97, "end_pos": 100, "type": "TASK", "confidence": 0.8890644907951355}]}, {"text": "The simplest ways to tackle this problem have been based on handcrafted rules mainly on the confidence scores obtained from ASR and SLU modules (;.", "labels": [], "entities": [{"text": "ASR", "start_pos": 124, "end_pos": 127, "type": "TASK", "confidence": 0.8282105326652527}]}, {"text": "However, these approaches have the limitation that building the quality rules manually is expensive and, what is worse, the confidence scores could be unreliable and inconsistent in some cases.", "labels": [], "entities": []}, {"text": "The other direction of dialog state tracking approaches have utilized statistical machine learning techniques to obtain the distribution over a set of hypotheses.", "labels": [], "entities": [{"text": "dialog state tracking", "start_pos": 23, "end_pos": 44, "type": "TASK", "confidence": 0.8323168357213339}]}, {"text": "Although the most widely studied approaches have been based on generative models (), recently, some researchers have reported that discriminative models) achieved comparable, or even better, performances than generative models, especially in the tasks of the first dialog state tracking challenge (DSTC) (.", "labels": [], "entities": [{"text": "generative", "start_pos": 63, "end_pos": 73, "type": "TASK", "confidence": 0.9612765908241272}, {"text": "dialog state tracking challenge (DSTC)", "start_pos": 265, "end_pos": 303, "type": "TASK", "confidence": 0.7926402006830487}]}, {"text": "This work focuses on the second phase of DSTC ().", "labels": [], "entities": [{"text": "DSTC", "start_pos": 41, "end_pos": 45, "type": "TASK", "confidence": 0.8099292516708374}]}, {"text": "The major difference of DSTC 2 from the previous challenge is that user goals can be changed even in a single dialog session.", "labels": [], "entities": []}, {"text": "This aspect can cause the limitations of the previous approaches assuming the fixed user goal for each session.", "labels": [], "entities": []}, {"text": "To solve this dynamic state tracking problem, we propose a sequential labeling approach using linear-chain conditional random fields (CRFs) ().", "labels": [], "entities": [{"text": "dynamic state tracking", "start_pos": 14, "end_pos": 36, "type": "TASK", "confidence": 0.6719841957092285}]}, {"text": "This approach aims to improve the performances of the tracker in the case of goal changes by jointly performing prediction and segmentation of dialog states.", "labels": [], "entities": []}], "datasetContent": [{"text": "To demonstrate the effectiveness of our proposed sequential labeling approach for dialog state tracking, we performed experiments on the DSTC 2 dataset which consists of 3,235 dialog sessions on restaurant information domain which were collected using Amazon Mechanical Turk.", "labels": [], "entities": [{"text": "dialog state tracking", "start_pos": 82, "end_pos": 103, "type": "TASK", "confidence": 0.8562770287195841}, {"text": "DSTC 2 dataset", "start_pos": 137, "end_pos": 151, "type": "DATASET", "confidence": 0.9262193441390991}, {"text": "Amazon Mechanical Turk", "start_pos": 252, "end_pos": 274, "type": "DATASET", "confidence": 0.9427731037139893}]}, {"text": "The results of ASR and SLU are annotated for every turn in the dataset, as well as the gold standard annotations are also provided for evaluation.", "labels": [], "entities": [{"text": "ASR", "start_pos": 15, "end_pos": 18, "type": "TASK", "confidence": 0.7394476532936096}, {"text": "SLU", "start_pos": 23, "end_pos": 26, "type": "METRIC", "confidence": 0.7732330560684204}]}, {"text": "We used this dataset following the original division into training/development/test sets, which have 1,612/506/1,117 sessions, respectively.", "labels": [], "entities": []}, {"text": "Using this dataset, we trained two different types of models: one is based on CRFs for our proposed sequential labeling approach; and the other is a baseline using maximum entropy (ME) that performs the prediction for each individual turn separately from others in a given session.", "labels": [], "entities": [{"text": "maximum entropy (ME)", "start_pos": 164, "end_pos": 184, "type": "METRIC", "confidence": 0.641878855228424}]}, {"text": "All the models for both approaches were trained on the training set with the same feature functions defined in Section 3.3 using MALLET 1 toolkit.", "labels": [], "entities": [{"text": "MALLET 1 toolkit", "start_pos": 129, "end_pos": 145, "type": "DATASET", "confidence": 0.6872820655504862}]}, {"text": "The trained models were used for predicting goals, method, and requested slots of each turn in the development and test sets, the results of which were then organized into a tracker output object defined as the input format to the evaluation script of DSTC 2.", "labels": [], "entities": [{"text": "DSTC 2", "start_pos": 252, "end_pos": 258, "type": "DATASET", "confidence": 0.8916336894035339}]}, {"text": "Since we omitted the joint goals distributions in the output, the evaluations on the joint goals were performed on the independent combinations of the slot distributions.", "labels": [], "entities": []}, {"text": "Among the various combinations of evaluation variables listed in the results of the evaluation script, the following three featured metrics were selected to report the performances of the tracker in this paper: Accuracy, L2 norm, and ROC CA 5.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 211, "end_pos": 219, "type": "METRIC", "confidence": 0.9994482398033142}, {"text": "ROC CA 5", "start_pos": 234, "end_pos": 242, "type": "METRIC", "confidence": 0.9824103713035583}]}, {"text": "All these metrics were computed for the predicted joint goals, method and requested slots.", "labels": [], "entities": []}, {"text": "compares the performances of our proposed approach (CRF) and the baseline method (ME) for three sub-tasks on the development and test sets.", "labels": [], "entities": [{"text": "baseline method (ME)", "start_pos": 65, "end_pos": 85, "type": "METRIC", "confidence": 0.6739098906517029}]}, {"text": "The results indicate that our proposed sequential labeling approach achieved better performances than the baseline for most cases.", "labels": [], "entities": [{"text": "sequential labeling", "start_pos": 39, "end_pos": 58, "type": "TASK", "confidence": 0.7111101448535919}]}, {"text": "Especially, CRF models produced better joint goals and method predictions in terms of accuracy and L2 norm on both development and test sets.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 86, "end_pos": 94, "type": "METRIC", "confidence": 0.9995251893997192}]}, {"text": "For the requested slots task, our proposed approach failed to generate better results than the baseline on the development set.", "labels": [], "entities": []}, {"text": "However, this situation was reversed on the test set, which means our proposed approach achieved better performances on all three sub-tasks on the test set in two of the three evaluation metrics.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Comparisons of dialog state tracking performances", "labels": [], "entities": []}]}