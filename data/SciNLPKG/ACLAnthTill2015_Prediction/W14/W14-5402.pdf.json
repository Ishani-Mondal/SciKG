{"title": [], "abstractContent": [{"text": "The prospect of human commanders teaming with mobile robots \"smart enough\" to undertake joint exploratory tasks-especially tasks that neither commander nor robot could perform alone-requires novel methods of preparing and testing human-robot teams for these ventures prior to real-time operations.", "labels": [], "entities": []}, {"text": "In this paper, we report work-in-progress that maintains face validity of selected configurations of resources and people, as would be available in emergency circumstances.", "labels": [], "entities": []}, {"text": "More specifically, from an off-site post, we ask human commanders (C) to perform an exploratory task in collaboration with a remotely located human robot-navigator (Rn) who controls the navigation of, but cannot seethe physical robot (R).", "labels": [], "entities": []}, {"text": "We impose network bandwidth restrictions in two mission scenarios comparable to real circumstances by varying the availability of sensor, image, and video signals to Rn, in effect limiting the human Rn to function as an automation stand-in.", "labels": [], "entities": []}, {"text": "To better understand the capabilities and language required in such configurations , we constructed multi-modal corpora of time-synced dialog, video, and LIDAR files recorded during task sessions.", "labels": [], "entities": []}, {"text": "We can now examine commander/robot dialogs while replaying what C and Rn saw, to assess their task performance under these varied conditions.", "labels": [], "entities": []}], "introductionContent": [{"text": "Our research addresses a paradoxical situation in developing a robot capable of teaming with humans.", "labels": [], "entities": []}, {"text": "To know what capabilities such a robot needs, we seek to determine how a human commander would interact -choice of vocabulary and sentence types, expected capabilities and world knowledge, resources used to accomplish tasks efficiently, etc.", "labels": [], "entities": []}, {"text": "But without such a robot to interact with, we cannot know how a commander would behave.", "labels": [], "entities": []}, {"text": "The prospect of human commanders teaming with mobile robots that are \"smart enough\" to undertake joint exploratory tasks requires novel methods of preparing and testing actual human-robot teams for these ventures, in advance of actual real-time operations.", "labels": [], "entities": []}, {"text": "Furthermore, given the need for human/robot teams during emergencies (such as Japan's tsunami/Fukishima disaster), we are interested in particular in the feasibility of commander/robot shared tasks that include NL communication specifically for network contexts when bandwidth is limited by emergencies.", "labels": [], "entities": [{"text": "NL communication", "start_pos": 211, "end_pos": 227, "type": "TASK", "confidence": 0.88932204246521}]}, {"text": "Here we ask, how can multimodal data, as collected and processed by robots, and the robots themselves contribute real-time alerts and responses to human commanders over geographically-distributed networks?", "labels": [], "entities": []}, {"text": "The first phase of our approach is to introduce a human stand-in who navigates the robot, posing as an intelligent control system.", "labels": [], "entities": []}, {"text": "At this stage, following our prior work (), we seek to determine how the commander communicates to accomplish different tasks with the robot, while we limit the information made available in passing from the robot's sensors and camera to the commander byway of the stand-in.", "labels": [], "entities": []}, {"text": "In future phases, we will progressively automate away this actor's role, replacing the audio that the stand-in hears with what is \"understood\" by automatic natural language semantic interpretation within a dialog manager, and replacing the joystick that it uses to navigate as the robot with \"actions\" as automatically generated from micro-controller commands produced by transformation of semantic commands.", "labels": [], "entities": []}, {"text": "In this paper, we report work-in-progress that maintains face validity of selected configurations of resources and people, as would be available in emergency circumstances.", "labels": [], "entities": []}, {"text": "From an off-site post, we ask human commanders (C) to perform an exploratory task in collaboration with a remotely located human robot-navigator (Rn) who actually controls the navigation of, but cannot see, the physical robot (R).", "labels": [], "entities": []}, {"text": "We restrict the information Rn receives from R by imposing network bandwidth restrictions comparable to real circumstances which limit what Rn is able to communicate to C.", "labels": [], "entities": []}, {"text": "We then examine the commander/robot dialogs and task performance under these varied conditions.", "labels": [], "entities": []}, {"text": "To better understand the capabilities and language required in such configurations, we constructed multi-modal corpora of time-synced dialog, video, and LIDAR files recorded during task sessions.", "labels": [], "entities": []}, {"text": "We can now examine commander/robot dialogs while replaying what C and Rn saw, to identify the impact of varying the shared visual information on discourse, and to assess task performance under these varied conditions.", "labels": [], "entities": []}, {"text": "We hypothesized that more explicit, mututally available information (visual or verbal) between participants would yield better understanding with more common ground, leading to more task success.", "labels": [], "entities": []}, {"text": "We also hypothesized that exploration in a more complex physical environment would lead both to more dialog, as needed in resolving references to more locations, and also then on occasion, to less overall task success.", "labels": [], "entities": []}, {"text": "We have found in preliminary analyses that, with more explicit visual information, some Cs reduce their level of communication, with fewer requests for images from Rn.", "labels": [], "entities": []}, {"text": "In one such case, this led to the Rn getting lost.", "labels": [], "entities": []}, {"text": "We also noticed that some Cs increased their level of verbal communication, requesting far more still images from the robot when Rn could not itself seethe robot's images (as opposed to when Rn had access to sent images).", "labels": [], "entities": []}, {"text": "Taken together, these observations suggest-contrary to our hypothesis that more information is better, especially in a complex environment-that there maybe a \"teeter totter\" effect in the communication between C and Rn as visual information varies.", "labels": [], "entities": []}, {"text": "When Rn has access to more of the robot's visual information, C communicates less with Rn, possibly assuming more shared information than is correct.", "labels": [], "entities": []}, {"text": "Whereas when Rn is able to see less, C communicates more with Rn, possibly compensating for the lack of certainty Rn expresses.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Mission 1 sessions: These training sessions provided the robot-navigators (Rn) with \"full\"  real-time vision, i.e., their screens displayed all sensed data, as collected by the physical robot (R)", "labels": [], "entities": []}, {"text": " Table 3: Mission 2 per-session events: request and reference types, task success.", "labels": [], "entities": []}]}