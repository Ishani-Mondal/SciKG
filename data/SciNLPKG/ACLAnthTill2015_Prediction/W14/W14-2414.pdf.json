{"title": [{"text": "Efficient Logical Inference for Semantic Processing", "labels": [], "entities": [{"text": "Efficient Logical Inference", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.6967142621676127}]}], "abstractContent": [{"text": "Dependency-based Compositional Semantics (DCS) provides a precise and expressive way to model semantics of natural language queries on relational databases, by simple dependency-like trees.", "labels": [], "entities": [{"text": "Dependency-based Compositional Semantics (DCS)", "start_pos": 0, "end_pos": 46, "type": "TASK", "confidence": 0.7693837583065033}]}, {"text": "Recently abstract denotation is proposed to enable generic logical inference on DCS.", "labels": [], "entities": [{"text": "generic logical inference", "start_pos": 51, "end_pos": 76, "type": "TASK", "confidence": 0.7532008091608683}]}, {"text": "In this paper, we discuss some other possibilities to equip DCS with logical inference, and we discuss further on how logical inference can help textual entailment recognition, or other semantic precessing tasks.", "labels": [], "entities": [{"text": "textual entailment recognition", "start_pos": 145, "end_pos": 175, "type": "TASK", "confidence": 0.8248055577278137}]}], "introductionContent": [{"text": "Dependency-based Compositional Semantics (DCS) was proposed as an interface for querying relational databases by natural language.", "labels": [], "entities": [{"text": "Dependency-based Compositional Semantics (DCS)", "start_pos": 0, "end_pos": 46, "type": "TASK", "confidence": 0.7873692164818445}]}, {"text": "It features DCS trees as semantic representation, with a structure similar to dependency trees.", "labels": [], "entities": []}, {"text": "In its basic version, anode of a DCS tree indicates a table in the database, and an edge indicates a join relation.", "labels": [], "entities": []}, {"text": "Both ends of an edge are labeled by afield of the corresponding table ().", "labels": [], "entities": []}, {"text": "However, when DCS is applied to logical inference on unrestricted texts, it is unrealistic to assume an explicit database, because we cannot prepare a database for everything in the world.", "labels": [], "entities": []}, {"text": "For this reason, DCS trees are detached from any specific relational database, in away that each node of a DCS tree indicates a content word in a sentence (thus no fixed set of possible word labels fora DCS tree node), and each edge indicates a semantic relation between two words.", "labels": [], "entities": []}, {"text": "Labels on the two ends of an edge, initially indicating fields of tables in a database, are considered as semantic roles of the corresponding words.", "labels": [], "entities": []}, {"text": "Abstract denotation is proposed to capture the meaning of this abstract version of DCS tree, and a textual inference system based on abstract denotation is built (.", "labels": [], "entities": []}, {"text": "It is quite natural to apply DCS trees, a simple and expressive semantic representation, to textual inference; however the use of abstract denotations to convey logical inference is somehow unusual.", "labels": [], "entities": []}, {"text": "There are two seemingly obvious way to equip DCS with logical inference: (i) at the tree level, by defining a set of logically sound transformations of DCS trees; or (ii) at the logic level, by converting DCS trees to first order predicate logic (FOL) formulas and then utilizing a theorem prover.", "labels": [], "entities": []}, {"text": "For (i), it may not be easy to enumerate all types of logically sound transformations, but tree transformations can be seen as an approximation of logical inference.", "labels": [], "entities": []}, {"text": "For (ii), abstract denotation is more efficient than FOL formula, because abstract denotation eliminates quantifiers and meanings of natural language texts can be represented by atomic sentences.", "labels": [], "entities": [{"text": "FOL", "start_pos": 53, "end_pos": 56, "type": "METRIC", "confidence": 0.4589906632900238}]}, {"text": "To elaborate the above discussion and to provide more topics to the literature, in this paper we discuss the following four questions: ( \u00a72) How well can tree transformation approximate logical inference?", "labels": [], "entities": []}, {"text": "( \u00a73) With rigorous inference on DCS trees, where does logic contribute in the system of?", "labels": [], "entities": []}, {"text": "( \u00a74) Does logical inference have further potentials in Recognizing Textual Entailment (RTE) task? and ( \u00a75) How efficient is abstract denotation compared to FOL formula?", "labels": [], "entities": [{"text": "Recognizing Textual Entailment (RTE) task", "start_pos": 56, "end_pos": 97, "type": "TASK", "confidence": 0.7948810373033796}]}, {"text": "We provide examples or experimental results to the above questions.", "labels": [], "entities": []}, {"text": "In the tree transformation based approach to RTE, it has been realized that some gaps between T and H cannot be filled even by a large number of tree transformation rules extracted from corpus ().", "labels": [], "entities": [{"text": "RTE", "start_pos": 45, "end_pos": 48, "type": "TASK", "confidence": 0.9676687121391296}]}, {"text": "For example in, it is possible to extract the rule blamed for death \u2192 cause loss of life, but not easy to extract tropical storm Debby \u2192 storm, because \"Debby\" could bean arbitrary name which may not even appear in the corpus.", "labels": [], "entities": []}, {"text": "This kind of gaps was typically addressed by approximate matching methods, for example by counting common sub-graphs of T and H, or by computing a cost of tree edits that convert T to H.", "labels": [], "entities": []}, {"text": "In the example of, we would expect that T is \"similar enough\" (i.e. has many common sub-graphs) with H, or the cost to convert T into H (e.g. by deleting the node Debby and then add the node storm) is low.", "labels": [], "entities": []}, {"text": "As for how similar is enough, or how the cost is evaluated, we will need a statistical model to train on RTE development set.", "labels": [], "entities": [{"text": "RTE development set", "start_pos": 105, "end_pos": 124, "type": "DATASET", "confidence": 0.8392583330472311}]}, {"text": "It was neglected that some combinations of tree edits are logical (while some are not).", "labels": [], "entities": []}, {"text": "The entailment pair in can be easily treated by logical inference, as long as the apposition tropical storm = Debby is appropriately handled.", "labels": [], "entities": []}, {"text": "In contrast to graph matching or tree edit models which theoretically admit arbitrary tree transformation, logical inference clearly discriminate sound transformations from unsound ones.", "labels": [], "entities": []}, {"text": "In this sense, there would be no need to train on RTE data.", "labels": [], "entities": [{"text": "RTE data", "start_pos": 50, "end_pos": 58, "type": "DATASET", "confidence": 0.7362326681613922}]}, {"text": "When coreference is considered, logically sound tree transformations can be quite complicated.", "labels": [], "entities": []}, {"text": "The following is a modified example from RTE2-dev: T: Hurricane Isabel, which caused significant damage, was a tropical storm when she entered Virginia.", "labels": [], "entities": [{"text": "RTE2-dev", "start_pos": 41, "end_pos": 49, "type": "DATASET", "confidence": 0.9167066812515259}, {"text": "T", "start_pos": 51, "end_pos": 52, "type": "METRIC", "confidence": 0.9895763397216797}]}, {"text": "Note the coreference between Hurricane Isabel and she, suggesting us to copy the subtree of Hurricane Isabel to she, in a tree edit approach.", "labels": [], "entities": []}, {"text": "This is not enough yet, because the head storm in T is not placed at the subject of cause.", "labels": [], "entities": []}, {"text": "The issue is indeed very logical: from \"Hurricane Isabel = she\", \"Hurricane Isabel = storm\", \"she = subject of enter\" and \"Hurricane Isabel = subject of cause\", we can imply that \"storm = subject of enter = subject of cause\".", "labels": [], "entities": []}, {"text": "3 Alignment with logical clues proposed away to generate onthe-fly knowledge to fill knowledge gaps: if H is not proven, compare DCS trees of T and H to generate path alignments (e.g. blamed for death \u223c cause loss of life, as underscored in; evaluate the path alignments by a similarity score function; and path alignments with a score greater than a threshold (0.4) are accepted and converted to inference rules.", "labels": [], "entities": []}, {"text": "The word vectors use to calculate similarities are reported able to capture semantic compositions by simple additions and subtractions.", "labels": [], "entities": []}, {"text": "This is also the case when used as knowledge resource for RTE, for example the similarities between blamed+death and cause+loss+life, or between found+shot+dead and killed, are computed > 0.4.", "labels": [], "entities": [{"text": "RTE", "start_pos": 58, "end_pos": 61, "type": "TASK", "confidence": 0.719740629196167}]}, {"text": "However, generally such kind of similarity is very noisy.", "labels": [], "entities": []}, {"text": "used some logical clues to filter out irrelevant path alignments, which helps to keep a high precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 93, "end_pos": 102, "type": "METRIC", "confidence": 0.998508632183075}]}, {"text": "To evaluate the effect of such logical filters, we compare it with some other alignment strategies, the performance of which on RTE5-test data is shown in.", "labels": [], "entities": [{"text": "RTE5-test data", "start_pos": 128, "end_pos": 142, "type": "DATASET", "confidence": 0.9004901051521301}]}, {"text": "Each strategy is described in the following.", "labels": [], "entities": []}, {"text": ", which use logical clues to filter out irrelevant path alignments, and apply accepted path alignments as inference rules.", "labels": [], "entities": []}, {"text": "LexNoun + Inference The same system as above, except that we only align paths between lexically aligned nouns.", "labels": [], "entities": [{"text": "LexNoun", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.8784504532814026}]}, {"text": "Two nouns are aligned if and only if they are synonyms, hyponyms or derivatively related in WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 92, "end_pos": 99, "type": "DATASET", "confidence": 0.9634537696838379}]}, {"text": "LexNoun + Coverage As above, paths between lexically aligned nouns are aligned, and aligned paths with similarity score > 0.4 are accepted.", "labels": [], "entities": [{"text": "LexNoun", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9161056280136108}]}, {"text": "If all nodes in H can be covered by some accepted path alignments, then output \"Y\".", "labels": [], "entities": []}, {"text": "This is very similar to the system described in.", "labels": [], "entities": []}, {"text": "NoFilter + Coverage Same as above, but all paths alignments with similarity score > 0.4 are accepted.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Comparison of different alignment strate- gies", "labels": [], "entities": []}, {"text": " Table 2: Proportion (%) of exit status of Prover9", "labels": [], "entities": [{"text": "Proportion", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9933900833129883}, {"text": "exit status", "start_pos": 28, "end_pos": 39, "type": "METRIC", "confidence": 0.9584943652153015}, {"text": "Prover9", "start_pos": 43, "end_pos": 50, "type": "DATASET", "confidence": 0.7541549205780029}]}]}