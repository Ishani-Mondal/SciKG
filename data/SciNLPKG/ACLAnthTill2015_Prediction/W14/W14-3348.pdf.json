{"title": [{"text": "Meteor Universal: Language Specific Translation Evaluation for Any Target Language", "labels": [], "entities": [{"text": "Meteor Universal", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.5815638601779938}, {"text": "Language Specific Translation Evaluation", "start_pos": 18, "end_pos": 58, "type": "TASK", "confidence": 0.7666470259428024}]}], "abstractContent": [{"text": "This paper describes Meteor Universal, released for the 2014 ACL Workshop on Statistical Machine Translation.", "labels": [], "entities": [{"text": "ACL Workshop on Statistical Machine Translation", "start_pos": 61, "end_pos": 108, "type": "TASK", "confidence": 0.5452127059300741}]}, {"text": "Meteor Universal brings language specific evaluation to previously unsupported target languages by (1) automatically extracting linguistic resources (paraphrase tables and function word lists) from the bitext used to train MT systems and (2) using a universal parameter set learned from pooling human judgments of translation quality from several language directions.", "labels": [], "entities": [{"text": "MT", "start_pos": 223, "end_pos": 225, "type": "TASK", "confidence": 0.964637815952301}]}, {"text": "Meteor Universal is shown to significantly outperform baseline BLEU on two new languages, Russian (WMT13) and Hindi (WMT14).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 63, "end_pos": 67, "type": "METRIC", "confidence": 0.9918013215065002}]}], "introductionContent": [{"text": "Recent WMT evaluations have seen a variety of metrics employ language specific resources to replicate human translation rankings far better than simple baselines;.", "labels": [], "entities": [{"text": "WMT", "start_pos": 7, "end_pos": 10, "type": "TASK", "confidence": 0.9800615906715393}, {"text": "replicate human translation", "start_pos": 92, "end_pos": 119, "type": "TASK", "confidence": 0.7792567610740662}]}, {"text": "While the wealth of linguistic resources for the WMT languages allows the development of sophisticated metrics, most of the world's 7,000+ languages lack the prerequisites for building advanced metrics.", "labels": [], "entities": []}, {"text": "Researchers working on low resource languages are usually limited to baseline BLEU () for evaluating translation quality.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 78, "end_pos": 82, "type": "METRIC", "confidence": 0.9920070171356201}]}, {"text": "Meteor Universal brings language specific evaluation to any target language by combining linguistic resources automatically learned from MT system training data with a universal metric parameter set that generalizes across languages.", "labels": [], "entities": []}, {"text": "Given only the bitext used to build a standard phrase-based translation system, Meteor Universal learns a paraphrase table and function word list, two of the most consistently beneficial language specific resources employed in versions of Meteor.", "labels": [], "entities": [{"text": "phrase-based translation", "start_pos": 47, "end_pos": 71, "type": "TASK", "confidence": 0.669315904378891}]}, {"text": "Whereas previous versions of Meteor require human ranking judgments in the target language to learn parameters, Meteor Universal uses a single parameter set learned from pooling judgments from several languages.", "labels": [], "entities": []}, {"text": "This universal parameter set captures general preferences shown by human evaluators across languages.", "labels": [], "entities": []}, {"text": "We show this approach to significantly outperform baseline BLEU for two new languages, Russian and Hindi.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 59, "end_pos": 63, "type": "METRIC", "confidence": 0.9823458790779114}]}, {"text": "The following sections review Meteor's scoring function ( \u00a72), describe the automatic extraction of language specific resources ( \u00a73), discuss training of the universal parameter set ( \u00a74), report experimental results ( \u00a75), describe released software ( \u00a76), and conclude ( \u00a77).", "labels": [], "entities": [{"text": "conclude", "start_pos": 263, "end_pos": 271, "type": "METRIC", "confidence": 0.9652730822563171}]}], "datasetContent": [{"text": "We evaluate the Universal version of Meteor against full language dedicated versions of Meteor and baseline BLEU on the WMT13 rankings.", "labels": [], "entities": [{"text": "Universal version of Meteor", "start_pos": 16, "end_pos": 43, "type": "DATASET", "confidence": 0.7816112637519836}, {"text": "Meteor", "start_pos": 88, "end_pos": 94, "type": "DATASET", "confidence": 0.9750953912734985}, {"text": "BLEU", "start_pos": 108, "end_pos": 112, "type": "METRIC", "confidence": 0.9897143244743347}, {"text": "WMT13 rankings", "start_pos": 120, "end_pos": 134, "type": "DATASET", "confidence": 0.9595312476158142}]}, {"text": "Results for English, Czech, German, Spanish, and French are biased in favor of Meteor Universal since rankings for these target languages are included in the training data while Russian constitutes a true held out test.", "labels": [], "entities": [{"text": "Meteor Universal", "start_pos": 79, "end_pos": 95, "type": "DATASET", "confidence": 0.7606680691242218}]}, {"text": "We also report the results of the WMT14 Hindi evaluation task.", "labels": [], "entities": [{"text": "WMT14 Hindi evaluation task", "start_pos": 34, "end_pos": 61, "type": "TASK", "confidence": 0.5611562132835388}]}, {"text": "Shown, Meteor Universal significantly outperforms baseline BLEU in all cases while suffering only slight degradation compared to versions of Meteor tuned for individual languages.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 59, "end_pos": 63, "type": "METRIC", "confidence": 0.9929813146591187}]}, {"text": "For Russian, correlation is nearly double that of BLEU.", "labels": [], "entities": [{"text": "correlation", "start_pos": 13, "end_pos": 24, "type": "METRIC", "confidence": 0.9991400241851807}, {"text": "BLEU", "start_pos": 50, "end_pos": 54, "type": "METRIC", "confidence": 0.9976108074188232}]}, {"text": "This provides substantial evidence that Meteor Universal will further generalize, bringing improved evaluation accuracy to new target languages currently limited to baseline language independent metrics.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 111, "end_pos": 119, "type": "METRIC", "confidence": 0.9822834134101868}]}, {"text": "For the WMT14 evaluation, we use the traditional language specific versions of Meteor for all language directions except Hindi.", "labels": [], "entities": [{"text": "WMT14", "start_pos": 8, "end_pos": 13, "type": "DATASET", "confidence": 0.5220279097557068}, {"text": "Meteor", "start_pos": 79, "end_pos": 85, "type": "DATASET", "confidence": 0.916784942150116}]}, {"text": "This includes Russian, for which additional language specific resources (a Snowball word stemmer) help significantly.", "labels": [], "entities": [{"text": "Snowball word stemmer", "start_pos": 75, "end_pos": 96, "type": "DATASET", "confidence": 0.9192219972610474}]}, {"text": "For Hindi, we use the release version of Meteor Universal to extract linguistic resources from the constrained training bitext provided for the shared translation task.", "labels": [], "entities": []}, {"text": "These resources are used with the universal parameter set to score all system outputs for the English-Hindi direction.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Comparison of parameters for language specific and universal versions of Meteor.", "labels": [], "entities": []}, {"text": " Table 3: Sentence-level correlation with human  rankings (Kendall's \u03c4 ) for Meteor (language spe- cific versions), Meteor Universal, and BLEU", "labels": [], "entities": [{"text": "Kendall's \u03c4 )", "start_pos": 59, "end_pos": 72, "type": "METRIC", "confidence": 0.7030077800154686}, {"text": "Meteor", "start_pos": 77, "end_pos": 83, "type": "DATASET", "confidence": 0.9262347221374512}, {"text": "BLEU", "start_pos": 138, "end_pos": 142, "type": "METRIC", "confidence": 0.9834769368171692}]}]}