{"title": [{"text": "Inducing Domain-specific Noun Polarity Guided by Domain-independent Polarity Preferences of Adjectives", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper, we discuss how domain-specific noun polarity lexicons can be induced.", "labels": [], "entities": [{"text": "noun polarity lexicons", "start_pos": 46, "end_pos": 68, "type": "TASK", "confidence": 0.7356570859750112}]}, {"text": "We focus on the generation of good candidates and compare two machine learning scenarios in order to establish an approach that produces high precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 142, "end_pos": 151, "type": "METRIC", "confidence": 0.9944638609886169}]}, {"text": "Candidates are generated on the basis of polarity preferences of adjectives derived from a large domain-independent corpus.", "labels": [], "entities": []}, {"text": "The polarity preference of a word, here an adjective, reflects the distribution of positive, negative and neutral arguments the word takes (here: its nominal head).", "labels": [], "entities": []}, {"text": "Given a noun modified by some adjectives, a vote among the polarity preferences of these adjectives establishes a good indicator of the polarity of the noun.", "labels": [], "entities": []}, {"text": "In our experiments with five domains, we achieved f-measure of 59% up to 88% on the basis of two machine learning approaches carried out on top of the preference votes.", "labels": [], "entities": [{"text": "f-measure", "start_pos": 50, "end_pos": 59, "type": "METRIC", "confidence": 0.9953399896621704}]}], "introductionContent": [{"text": "Polarity lexicons are crucial for fine-grained sentiment analysis.", "labels": [], "entities": [{"text": "fine-grained sentiment analysis", "start_pos": 34, "end_pos": 65, "type": "TASK", "confidence": 0.6548539400100708}]}, {"text": "For instance, in approaches carrying out sentiment composition, where phrase-level polarity is composed out of word level polarity (e.g. disappointed \u2212 hope + \u2192 NP \u2212 ).", "labels": [], "entities": [{"text": "sentiment composition", "start_pos": 41, "end_pos": 62, "type": "TASK", "confidence": 0.924859493970871}]}, {"text": "However, often freely available lexicons are domain-independent, which is a problem with domain-specific texts, since lexical gaps reduce composition anchors.", "labels": [], "entities": []}, {"text": "But how many domain-specific words do we have to expect?", "labels": [], "entities": []}, {"text": "Is it areal or rather a marginal problem?", "labels": [], "entities": []}, {"text": "In our experiments, we found that domain-specific nouns do occur quite often -so they do matter.", "labels": [], "entities": []}, {"text": "In one of our domains, we identified about 1000 negative nouns, 409 were domain-specific.", "labels": [], "entities": []}, {"text": "In that domain, the finance sector, more than 13'000 noun types exist that do not occur at all in the DeWac corpus -a large Web corpus (in German) with over 90 Million sentences.", "labels": [], "entities": [{"text": "DeWac corpus -a large Web corpus", "start_pos": 102, "end_pos": 134, "type": "DATASET", "confidence": 0.8881834489958627}]}, {"text": "Thus, most of them must be regarded as domain-specific.", "labels": [], "entities": []}, {"text": "It would be quite time-consuming to go through all of them in order to identify and annotate the polar ones.", "labels": [], "entities": []}, {"text": "Could we, rather, predict good candidates?", "labels": [], "entities": []}, {"text": "We would need polarity predictors -words that take other, polar words e.g. as their heads.", "labels": [], "entities": [{"text": "polarity predictors", "start_pos": 14, "end_pos": 33, "type": "TASK", "confidence": 0.6510238200426102}]}, {"text": "If they, moreover, had a clear-cut preference, i.e. they mostly took one kind of polar words, say negative, then they were perfect predictors of the polarity of nouns.", "labels": [], "entities": []}, {"text": "We found that adjectives (e.g. acute) can be used as such polarity predictors (e.g. acute mostly takes negative nouns, denoted n \u2212 , e.g. acute pain)).", "labels": [], "entities": []}, {"text": "Our hypothesis is that the polarity preferences of adjectives are (more or less) domainindependent.", "labels": [], "entities": []}, {"text": "We can learn the preferences from domain-independent texts and apply it to domainspecific texts and get good candidates of domainspecific polar nouns.", "labels": [], "entities": []}, {"text": "Clearly, if the polarity preferences of an adjective are balanced (0.33 for each polarity), than the predictions could not help at all.", "labels": [], "entities": []}, {"text": "But if one polarity clearly prevails, we might even get a good performance by just classifying the polarity of unknown nouns in a domain according to the dominant polarity preference of the adjectives they co-occur with.", "labels": [], "entities": []}, {"text": "In this paper, we show how to generate such a preference model on the basis of a large, domain-independent German corpus and a domain-independent German polarity lexicon.", "labels": [], "entities": []}, {"text": "We use this model to generate candidate nouns from five domain-specific text collections -ranging from 3'200 up to 37'000 texts per domain.", "labels": [], "entities": []}, {"text": "In order to see how far an automatic induction of a domain-specific noun lexicon could go, we also experimented with machine learning scenarios on the output of the baseline system.", "labels": [], "entities": []}, {"text": "We experimented with a distributional feature setting on the basis of unigrams and used the Maximum En-tropy learner, Megam), to learn a classifier.", "labels": [], "entities": []}, {"text": "We also worked with Weka () and features derived from the German polarity lexicon.", "labels": [], "entities": [{"text": "Weka", "start_pos": 20, "end_pos": 24, "type": "DATASET", "confidence": 0.9497096538543701}]}, {"text": "Both approaches yield significant gains in terms of precision -so they realize a highprecision scenario.", "labels": [], "entities": [{"text": "precision", "start_pos": 52, "end_pos": 61, "type": "METRIC", "confidence": 0.9993783235549927}]}], "datasetContent": [{"text": "The goal of our experiments were the prediction of positive and negative domain-specific nouns in five domains.", "labels": [], "entities": [{"text": "prediction of positive and negative domain-specific nouns", "start_pos": 37, "end_pos": 94, "type": "TASK", "confidence": 0.681386683668409}]}, {"text": "We used our preference model to generate candidates.", "labels": [], "entities": []}, {"text": "Then we manually annotated the results in order to obtain a domain-specific gold standard.", "labels": [], "entities": []}, {"text": "We evaluated the output of the preference model relative to the new gold standards and we run our experiments with Megam and Weka's Simple Logistic Regression (SRL).", "labels": [], "entities": [{"text": "Simple Logistic Regression (SRL)", "start_pos": 132, "end_pos": 164, "type": "TASK", "confidence": 0.5492275257905325}]}, {"text": "Megam and Weka's SLR were trained on the basis of the positive, negative and neutral nouns from PoLex and the DeWac corpus.", "labels": [], "entities": [{"text": "DeWac corpus", "start_pos": 110, "end_pos": 122, "type": "DATASET", "confidence": 0.8892923593521118}]}, {"text": "#PM gives the number of nouns predicted by the preference model to be negative (e.g. 220 in the politics domain).", "labels": [], "entities": [{"text": "PM", "start_pos": 1, "end_pos": 3, "type": "METRIC", "confidence": 0.9863974452018738}]}, {"text": "These are the nouns we annotated for polarity and that formed our gold standard afterwards (e.g. 75.90 out of 110 predicted are true negative nouns and are kept as the gold standard).", "labels": [], "entities": []}, {"text": "Since the generation of the gold standard is based on the preference model's output, its recall is 1.", "labels": [], "entities": [{"text": "recall", "start_pos": 89, "end_pos": 95, "type": "METRIC", "confidence": 0.9997218251228333}]}, {"text": "We cannot fix the real recall since this would require to manually classify all nouns occurring in those texts (e.g. 13'000 in the banks domain).", "labels": [], "entities": [{"text": "recall", "start_pos": 23, "end_pos": 29, "type": "METRIC", "confidence": 0.973173201084137}]}, {"text": "However, since we wanted to compare the machine learning performance with the preference model, we had to mea- Figure 3: Prediction of Negative Nouns sure recall, otherwise we could not determine the overall performance.", "labels": [], "entities": [{"text": "recall", "start_pos": 155, "end_pos": 161, "type": "METRIC", "confidence": 0.9708334803581238}]}, {"text": "From we can see that the preference model (PM) performs best in terms of f-measure (in bold).", "labels": [], "entities": []}, {"text": "Of course, recall (i.e. 1, not shown) is idealized, since we took the output of the preference model to generate the gold standard.", "labels": [], "entities": [{"text": "recall", "start_pos": 11, "end_pos": 17, "type": "METRIC", "confidence": 0.999244213104248}]}, {"text": "Note however that this was our premise, that we needed an approach that delivers good candidates, otherwise we were lost given the vast amount of candidate nouns (e.g. remember the 13'000 nouns in the finance sector).: Domain-specific Negative Nouns shows examples of negative nouns from two domains: banks and pharma.", "labels": [], "entities": []}, {"text": "But: are all found nouns domain-specific negative nouns?", "labels": [], "entities": []}, {"text": "In the bank domain, we have manually annotated for domain specificity: out of 1013 nouns predicted to be negative by the model, 409 actually were domain-specific (40.3 %) . The other nouns could also be in a domain-independent polarity lexicon.", "labels": [], "entities": []}, {"text": "Now, we turn to the prediction of positive domain-specific nouns.", "labels": [], "entities": [{"text": "prediction of positive domain-specific nouns", "start_pos": 20, "end_pos": 64, "type": "TASK", "confidence": 0.8321022510528564}]}, {"text": "It is not really surprising that the preference model is unbalanced -that there are far more negative than positive polarity predictors: 401 compared to 105.", "labels": [], "entities": []}, {"text": "PoLex, the pool of nouns used for learning of the polarity preferences already is unbalanced (2100 negative compared to 1250 positive nouns).", "labels": [], "entities": [{"text": "PoLex", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.7816081643104553}]}, {"text": "Also, the majority of the texts in our five domains are negative (all texts are annotated for document-level polarity).", "labels": [], "entities": []}, {"text": "It is obvious then that our model is better in the prediction of negative than positive polarity.", "labels": [], "entities": []}, {"text": "Actually, our base model comprising 105 positive polarity predictors does not trigger often within the whole corpus.", "labels": [], "entities": []}, {"text": "For instance, only 10 predictions were made in the banks domain, despite the 37'346 texts.", "labels": [], "entities": []}, {"text": "Clearly, newspaper texts often are critical and thus more negative than positive vocabulary is used.", "labels": [], "entities": []}, {"text": "This explains the very low recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 27, "end_pos": 33, "type": "METRIC", "confidence": 0.999332845211029}]}], "tableCaptions": []}