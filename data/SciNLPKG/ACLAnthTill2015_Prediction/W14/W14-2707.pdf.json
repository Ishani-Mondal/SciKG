{"title": [{"text": "Detecting and Evaluating Local Text Reuse in Social Networks", "labels": [], "entities": [{"text": "Detecting and Evaluating Local Text Reuse", "start_pos": 0, "end_pos": 41, "type": "TASK", "confidence": 0.8748120864232382}]}], "abstractContent": [{"text": "Texts propagate among participants in many social networks and provide evidence for network structure.", "labels": [], "entities": []}, {"text": "We describe intrinsic and extrinsic evaluations for algorithms that detect clusters of reused passages embedded within longer documents in large collections.", "labels": [], "entities": []}, {"text": "We explore applications of these approaches to two case studies: the culture of free reprinting in the nineteenth-century United States and the use of similar language in the public statements of U.S. members of Congress.", "labels": [], "entities": []}], "introductionContent": [{"text": "While many studies of social networks use surveys and direct observation to catalogue actors (nodes) and their interactions (edges), we often cannot directly observe network links.", "labels": [], "entities": []}, {"text": "Instead, we might observe behavior by network participants that provides indirect evidence for social ties.", "labels": [], "entities": []}, {"text": "One revealing form of shared behavior is the reuse of text by different social actors.", "labels": [], "entities": []}, {"text": "Methods to uncover invisible links among sources of text methods would have broad applicability because of the very general nature of the problemsources of text include websites, newspapers, individuals, corporations, political parties, and soon.", "labels": [], "entities": []}, {"text": "Further, discerning those hidden links between sources would provide more effective ways of identifying the provenance and diverse sources of information, and to build predictive models of the diffusion of information.", "labels": [], "entities": []}, {"text": "There are substantial challenges, however, in building a methodology to study text reuse, including: scalable detection of reused passages; identification of appropriate statistical models of text mutation; inference methods for characterizing missing nodes that originate or mediate text transmission; link inference conditioned on textual topics; and the development of testbeds through which predictions of the resulting models might be validated against some broader understanding of the processes of transmission.", "labels": [], "entities": [{"text": "text mutation", "start_pos": 192, "end_pos": 205, "type": "TASK", "confidence": 0.6886850744485855}, {"text": "characterizing missing nodes that originate or mediate text transmission", "start_pos": 229, "end_pos": 301, "type": "TASK", "confidence": 0.790140363905165}]}, {"text": "In this paper, we sketch relevant features of our two testbed collections ( \u00a72) and then describe initial progress on developing algorithms for detecting reused passages embedded within the larger text output of social network nodes ( \u00a73).", "labels": [], "entities": [{"text": "detecting reused passages embedded within the larger text output of social network nodes", "start_pos": 144, "end_pos": 232, "type": "TASK", "confidence": 0.7106020267193134}]}, {"text": "We then describe an intrinsic evaluation of the efficiency of these techniques for scaling up text reuse detection ( \u00a74).", "labels": [], "entities": [{"text": "text reuse detection", "start_pos": 94, "end_pos": 114, "type": "TASK", "confidence": 0.8075488408406576}]}, {"text": "Finally, we perform an extrinsic evaluation of the network links inferred from text reuse by correlating them with side information about the underlying social networks ( \u00a75).", "labels": [], "entities": []}, {"text": "A preliminary version of the text reuse detection system was presented fora single, smaller corpus in), but without the extrinsic or much of the intrinsic evaluation and without data on the underlying networks.", "labels": [], "entities": [{"text": "text reuse detection", "start_pos": 29, "end_pos": 49, "type": "TASK", "confidence": 0.8465903401374817}]}], "datasetContent": [{"text": "To evaluate the precision and recall of text reuse detection, we create a pseudo-relevant set of document pairs by pooling the results of several runs with different parameter settings.", "labels": [], "entities": [{"text": "precision", "start_pos": 16, "end_pos": 25, "type": "METRIC", "confidence": 0.9986608028411865}, {"text": "recall", "start_pos": 30, "end_pos": 36, "type": "METRIC", "confidence": 0.9969221949577332}, {"text": "text reuse detection", "start_pos": 40, "end_pos": 60, "type": "TASK", "confidence": 0.8170307874679565}]}, {"text": "For each document pair found in the union of these runs, we observe the length, in matching characters, of the longest local alignment.", "labels": [], "entities": []}, {"text": "(Using matching character length allows us to abstract somewhat from the precise cost matrix.)", "labels": [], "entities": []}, {"text": "We can then observe how many aligned passages each method retrieves that are at least 50,000 character matches in length, at least 20,000 character matches in length, and soon.", "labels": [], "entities": []}, {"text": "The candidate pairs are sorted by the number of overlapping n-grams; we measure the pseudorecall at several length cutoffs.", "labels": [], "entities": []}, {"text": "For each position in a ranked list of document pairs, we then measure the precision: what proportion of documents retrieved are in fact 50k, 20k, etc., in length?", "labels": [], "entities": [{"text": "precision", "start_pos": 74, "end_pos": 83, "type": "METRIC", "confidence": 0.9996570348739624}]}, {"text": "Since we wish to rank documents by the length of the aligned passages they contain, this is a reasonable metric.", "labels": [], "entities": []}, {"text": "One summary of these various values is the average precision: the mean of the precision at every rank position that contains an actually relevant document pair.", "labels": [], "entities": [{"text": "precision", "start_pos": 51, "end_pos": 60, "type": "METRIC", "confidence": 0.7262120842933655}, {"text": "precision", "start_pos": 78, "end_pos": 87, "type": "METRIC", "confidence": 0.9918940663337708}]}, {"text": "One of the few earlier evaluations of local text reuse, by, compared fingerprinting methods to a trigram baseline.", "labels": [], "entities": []}, {"text": "Since their corpus contained short individual news articles, the extent of the reused passages was evaluated qualitatively rather than by alignment.", "labels": [], "entities": []}, {"text": "shows the average precision of different parameter settings on the newspaper collection, ranked by the number of pairs each returns.", "labels": [], "entities": [{"text": "precision", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.997714638710022}, {"text": "newspaper collection", "start_pos": 67, "end_pos": 87, "type": "DATASET", "confidence": 0.9333186149597168}]}, {"text": "If the pairwise document step returns a large number of pairs, we will have to perform a large number of more costly Smith-Waterman alignments.", "labels": [], "entities": []}, {"text": "On this collection, a good tradeoff between space 5e+06 1e+07 2e+07 5e+07 Pairs examined). and speed is achieved by skip bigram features.", "labels": [], "entities": [{"text": "speed", "start_pos": 95, "end_pos": 100, "type": "METRIC", "confidence": 0.9740228056907654}]}, {"text": "In the best case, we look at bigrams where there is a gap of at least 95, and not more than 105, words between the first and second terms (n=2 u=100 w=105 g=95).", "labels": [], "entities": []}, {"text": "While average precision is a good summary of the quality of the ranked list at anyone point, many applications will simply be concerned with the total recall after some fixed amount of processing.", "labels": [], "entities": [{"text": "precision", "start_pos": 14, "end_pos": 23, "type": "METRIC", "confidence": 0.7296810150146484}, {"text": "recall", "start_pos": 151, "end_pos": 157, "type": "METRIC", "confidence": 0.9346579909324646}]}, {"text": "also summarizes these recall results by the absolute number of document pairs examined.", "labels": [], "entities": [{"text": "recall", "start_pos": 22, "end_pos": 28, "type": "METRIC", "confidence": 0.9977297186851501}]}, {"text": "From these results, it is clear the several good settings perform well at retrieving all reprinted passages of at least 5000 characters.", "labels": [], "entities": []}, {"text": "Even using the pseudo-recall metric, however, even the best operating points fail in the end to retrieve about 10% of the reprints detected by some other setting for all documents of at least 1000 characters.", "labels": [], "entities": []}, {"text": "While political scientists, historians, and literary scholars will, we hope, find these techniques useful and perform close reading and manual analysis on texts of interest, we would like to validate our results without a costly annotation campaign.", "labels": [], "entities": []}, {"text": "In this paper, we explore the correlation of patterns of text reuse with what is already known from other sources about the connections among Members of Congress, newspaper editors, and soon.", "labels": [], "entities": []}, {"text": "This idea was inspired by, who used these techniques to test rhetorical theories of \"semantic organizing processes\" on the congressional statements corpus.", "labels": [], "entities": []}, {"text": "The approach is quite simple: measure the correlation between some metric of text reuse between actors in asocial network and other features of the network links between those actors.", "labels": [], "entities": []}, {"text": "The metric of text reuse might be simply the number of exact n-grams shared by the language of two authors (; alternately, it might be the absolute or relative length of all the aligned passages shared by two authors or the tree distance between them in a phylogenetic reconstruction.", "labels": [], "entities": []}, {"text": "To measure the correlation of a text reuse metric with a single network, we can simply use Pearson's correlation; for more networks, we can use multivariate regression.", "labels": [], "entities": [{"text": "Pearson's correlation", "start_pos": 91, "end_pos": 112, "type": "METRIC", "confidence": 0.8575461109479269}]}, {"text": "Due to, for instance, autocorrelation among edges arising from a particular node, we cannot proceed as if the weight of each edge in the text reuse network can be compared independently to the weight of the corresponding edges in other networks.", "labels": [], "entities": []}, {"text": "We therefore use nonparametric permutation tests using the quadratic assignment procedure (QAP) to resample several networks with the same structure but different labels and weights.", "labels": [], "entities": []}, {"text": "The QAP achieves this by reordering the rows and columns of one network's adjacency matrix according to the same permutation.", "labels": [], "entities": []}, {"text": "The permuted network then has the same structure-e.g., degree distribution-but should no longer exhibit the same correlations with the other network(s).", "labels": [], "entities": []}, {"text": "We can run QAP to generate confidence intervals for both single and multiple correlations.", "labels": [], "entities": [{"text": "QAP", "start_pos": 11, "end_pos": 14, "type": "TASK", "confidence": 0.5812389254570007}]}], "tableCaptions": [{"text": " Table 2: Correlations between log length of aligned text and other author networks in public statements  by Members of Congress.  * p < .05,  *   *  p < .01,  *   *   * p < .001", "labels": [], "entities": []}, {"text": " Table 3: Correlations between shared reprints between 19c newspapers and political and other affinities.  While many Whig papers became Republican, they do not completely overlap in our dataset; the identical  number of pairs is coincidental.", "labels": [], "entities": []}]}