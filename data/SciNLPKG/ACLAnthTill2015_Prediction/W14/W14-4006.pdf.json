{"title": [{"text": "Reducing the Impact of Data Sparsity in Statistical Machine Translation", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 40, "end_pos": 71, "type": "TASK", "confidence": 0.8482266068458557}]}], "abstractContent": [{"text": "Morphologically rich languages generally require large amounts of parallel data to adequately estimate parameters in a statistical Machine Translation(SMT) system.", "labels": [], "entities": [{"text": "statistical Machine Translation(SMT)", "start_pos": 119, "end_pos": 155, "type": "TASK", "confidence": 0.7804824113845825}]}, {"text": "However, it is time consuming and expensive to create large collections of parallel data.", "labels": [], "entities": []}, {"text": "In this paper, we explore two strategies for circumventing sparsity caused by lack of large parallel corpora.", "labels": [], "entities": []}, {"text": "First, we explore the use of distributed representations in an Recurrent Neural Network based language model with different morphological features and second, we explore the use of lexical resources such as WordNet to overcome sparsity of content words.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 207, "end_pos": 214, "type": "DATASET", "confidence": 0.9584658145904541}]}], "introductionContent": [{"text": "Statistical machine translation (SMT) models estimate parameters (lexical models, and distortion model) from parallel corpora.", "labels": [], "entities": [{"text": "Statistical machine translation (SMT)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.7943570762872696}]}, {"text": "The reliability of these parameter estimates is dependent on the size of the corpora.", "labels": [], "entities": [{"text": "reliability", "start_pos": 4, "end_pos": 15, "type": "METRIC", "confidence": 0.973788321018219}]}, {"text": "In morphologically rich languages, this sparsity is compounded further due to lack of large parallel corpora.", "labels": [], "entities": []}, {"text": "In this paper, we present two approaches that address the issue of sparsity in SMT models for morphologically rich languages.", "labels": [], "entities": [{"text": "SMT", "start_pos": 79, "end_pos": 82, "type": "TASK", "confidence": 0.9939572811126709}]}, {"text": "First, we use an Recurrent Neural Network (RNN) based language model (LM) to re-rank the output of a phrasebased SMT (PB-SMT) system and second we use lexical resources such as WordNet to minimize the impact of Out-of-Vocabulary(OOV) words on MT quality.", "labels": [], "entities": [{"text": "MT", "start_pos": 243, "end_pos": 245, "type": "TASK", "confidence": 0.9901346564292908}]}, {"text": "We further improve the accuracy of MT using a model combination approach.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9994708895683289}, {"text": "MT", "start_pos": 35, "end_pos": 37, "type": "TASK", "confidence": 0.9917085766792297}]}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "We first present our approach of training the baseline model and source side reordering.", "labels": [], "entities": []}, {"text": "In Section 4, we present our experiments and results on reranking the MT output using RNNLM.", "labels": [], "entities": [{"text": "MT", "start_pos": 70, "end_pos": 72, "type": "TASK", "confidence": 0.8254844546318054}, {"text": "RNNLM", "start_pos": 86, "end_pos": 91, "type": "DATASET", "confidence": 0.9353483319282532}]}, {"text": "In Section 5, we discuss our approach to increase the coverage of the model by using synset ID's from the English WordNet (EWN).", "labels": [], "entities": [{"text": "English WordNet (EWN)", "start_pos": 106, "end_pos": 127, "type": "DATASET", "confidence": 0.810777747631073}]}, {"text": "Section 6 describes our experiments on combining the model with synset ID's and baseline model to further improve the translation accuracy followed by results and observations sections.We conclude the paper with future work and conclusions.", "labels": [], "entities": [{"text": "translation", "start_pos": 118, "end_pos": 129, "type": "TASK", "confidence": 0.9334589242935181}, {"text": "accuracy", "start_pos": 130, "end_pos": 138, "type": "METRIC", "confidence": 0.8989037871360779}]}], "datasetContent": [{"text": "In this section, we describe the results of reranking the output of the translation model using Recurrent Neural Networks (RNN) based language models using the same data which is used for language modelling in the baseline models.", "labels": [], "entities": []}, {"text": "Unlike traditional n-gram based discrete language models, RNN do not make the Markov assumption and potentially can take into account long-term dependencies between words.", "labels": [], "entities": []}, {"text": "Since the words in RNNs are represented as continuous valued vectors in low dimensions allowing for the possibility of smoothing using syntactic and semantic features.", "labels": [], "entities": []}, {"text": "In practice, however, learning long-term dependencies with gradient descent is difficult as described by due to diminishing gradients.", "labels": [], "entities": []}, {"text": "We have integrated the approach of re-scoring n-best output using RNNLM which has also been shown to be helpful by ().", "labels": [], "entities": [{"text": "RNNLM", "start_pos": 66, "end_pos": 71, "type": "DATASET", "confidence": 0.7400442361831665}]}, {"text": "Shi (2012) also showed the benefits of using RNNLM with contextual and linguistic features.", "labels": [], "entities": []}, {"text": "Following their work, we used three type of features for building an RNNLM for Hindi : lemma (root), POS, NC (number-case).", "labels": [], "entities": [{"text": "POS", "start_pos": 101, "end_pos": 104, "type": "METRIC", "confidence": 0.9371246099472046}]}, {"text": "The data used was a Wikipedia dump, MT training data, news articles which had approximately 500,000 Hindi sentences.", "labels": [], "entities": [{"text": "MT training", "start_pos": 36, "end_pos": 47, "type": "TASK", "confidence": 0.8176866173744202}]}, {"text": "Features were extracted using paradigmbased Hindi Morphological Analyzer 1 illustrates the results of re-ranking performed using RNNLM trained with various features.", "labels": [], "entities": []}, {"text": "The Oracle score is the highest achievable score in a re-ranking experiment.", "labels": [], "entities": []}, {"text": "This score is computed based on the best translation out of nbest translations.", "labels": [], "entities": []}, {"text": "The best translation is found using the cosine similarity between the hypothesis and the reference translation.", "labels": [], "entities": []}, {"text": "It can be seen from, that the LM with only word and POS information is inferior to all other models.", "labels": [], "entities": []}, {"text": "However, morphological features like lemma, number and case information help in re-ranking the hypothesis significantly.", "labels": [], "entities": []}, {"text": "The RNNLM which uses all the features performed the best for the re-ranking experiments achieving a BLEU score of 26.91, after rescoring 500-best obtained from the pre-order SMT model.", "labels": [], "entities": [{"text": "RNNLM", "start_pos": 4, "end_pos": 9, "type": "DATASET", "confidence": 0.9361577033996582}, {"text": "BLEU score", "start_pos": 100, "end_pos": 110, "type": "METRIC", "confidence": 0.9846470952033997}, {"text": "SMT", "start_pos": 174, "end_pos": 177, "type": "TASK", "confidence": 0.96551513671875}]}, {"text": "We have used the HCU morph-analyzer.: Rescoring results of 500-best hypotheses using RNNLM with different features  As can be seen in, the model with synset information led to reduction in OOV words.", "labels": [], "entities": [{"text": "RNNLM", "start_pos": 85, "end_pos": 90, "type": "DATASET", "confidence": 0.8040473461151123}]}, {"text": "Even though BLEU score decreased, but METEOR score improved for all the experiments based on using synset IDs in the source sentence, but it has been shown by) that METEOR is a better evaluation metrics for morphologically rich languages.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 12, "end_pos": 22, "type": "METRIC", "confidence": 0.9801367819309235}, {"text": "METEOR score", "start_pos": 38, "end_pos": 50, "type": "METRIC", "confidence": 0.9606208503246307}]}, {"text": "Also, when synset ID\u00e2s are used instead of words in the source language, the system makes incorrect morphological choices.", "labels": [], "entities": []}, {"text": "Example : going and goes will be replaced by same synset ID\u00e2SynsetID\u02c6ID\u00e2Synset(go.v.1)\u02c6 a, so this has lead to loss of information in the phrase-: Results for the model in which there were Synset ID's instead of word in English data results with significant reduction in OOV words and also some gains in METEOR score.", "labels": [], "entities": [{"text": "METEOR score", "start_pos": 304, "end_pos": 316, "type": "METRIC", "confidence": 0.9798862040042877}]}], "tableCaptions": [{"text": " Table 1: Baseline Scores for Phrase-based Moses  Model", "labels": [], "entities": []}, {"text": " Table 2: Rescoring results of 500-best hypotheses  using RNNLM with different features", "labels": [], "entities": [{"text": "Rescoring", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.5147532820701599}, {"text": "RNNLM", "start_pos": 58, "end_pos": 63, "type": "DATASET", "confidence": 0.877368152141571}]}, {"text": " Table 3: Results for the model in which there were Synset ID's instead of word in English data", "labels": [], "entities": []}, {"text": " Table 4: OOV words in Different Models", "labels": [], "entities": [{"text": "OOV", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.7067630290985107}]}]}