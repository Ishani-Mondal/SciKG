{"title": [{"text": "Distributional Composition using Higher-Order Dependency Vectors", "labels": [], "entities": [{"text": "Distributional Composition", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.9594691097736359}]}], "abstractContent": [{"text": "This paper concerns how to apply compo-sitional methods to vectors based on grammatical dependency relation vectors.", "labels": [], "entities": []}, {"text": "We demonstrate the potential of a novel approach which uses higher-order grammatical dependency relations as features.", "labels": [], "entities": []}, {"text": "We apply the approach to adjective-noun compounds with promising results in the prediction of the vectors for (held-out) observed phrases.", "labels": [], "entities": []}], "introductionContent": [{"text": "Vector space models of semantics characterise the meaning of a word in terms of distributional features derived from word co-occurrences.", "labels": [], "entities": []}, {"text": "The most widely adopted basis for word co-occurrence is proximity, i.e. that two words (or more generally lexemes) are taken to co-occur when they occur together within a certain sized window, or within the same sentence, paragraph, or document., in contrast, took the syntactic relationship between co-occurring words into account: the distributional features of a word are based on the word's grammatical dependents as found in a dependency parsed corpus.", "labels": [], "entities": []}, {"text": "For example, observing that the word glass appears as the indirect object of the verb fill, provides evidence that the word glass has the distributional feature iobj:fill, where iobj denotes the inverse indirect object grammatical relation.", "labels": [], "entities": []}, {"text": "The use of grammatical dependents as word features has been exploited in the discovery of tight semantic relations, such as synonymy and hypernymy, where an evaluation against a gold standard such as WordNet can be made).", "labels": [], "entities": [{"text": "WordNet", "start_pos": 200, "end_pos": 207, "type": "DATASET", "confidence": 0.9322179555892944}]}, {"text": "took this further by considering not just direct grammatical dependents, but also including indirect dependents.", "labels": [], "entities": []}, {"text": "Thus, observing the sentence She filled her glass slowly would provide evidence that the word glass has the distributional feature iobj:advmod:slowly where iobj:advmod captures the indirect dependency relationship between glass and slowly in the sentence.", "labels": [], "entities": []}, {"text": "Note that included a basis mapping function that gave their framework flexibility as to how to map paths such as iobj:advmod:slowly onto the basis of the vector space.", "labels": [], "entities": []}, {"text": "Indeed, the instantiation of their framework that they adopt in their experiments uses a basis mapping function that removes the dependency path to leave just the word, so iobj:advmod:slowly would be mapped to slowly.", "labels": [], "entities": []}, {"text": "In this paper, we are concerned with the problem of distributional semantic composition.", "labels": [], "entities": [{"text": "distributional semantic composition", "start_pos": 52, "end_pos": 87, "type": "TASK", "confidence": 0.6908051172892252}]}, {"text": "We show that the idea that the distributional semantics of a word can be captured with higher-order dependency relationships, provides the basis fora simple approach to compositional distributional semantics.", "labels": [], "entities": []}, {"text": "While our approach is quite general, dealing with arbitrarily high-order dependency relationships, and the composition of arbitrary phrases, in this paper we consider only first and second order dependency relations, and adjective-noun composition.", "labels": [], "entities": []}, {"text": "In Section 2, we illustrate our proposal by showing how second order dependency relations can play a role in computing the semantics of adjective-noun composition.", "labels": [], "entities": []}, {"text": "In Section 3 we describe a number of experiments that are intended to evaluate the approach, with the results presented in Section 4.", "labels": [], "entities": []}, {"text": "The basis for our evaluation follows and.", "labels": [], "entities": []}, {"text": "Typically, compositional distributional semantic models can be used to generate an (inferred) distributional vector fora phrase from the (observed) distributional vectors of the phrase's constituents.", "labels": [], "entities": []}, {"text": "One of the motivations for doing this is that the observed distributional vectors for most phrases tend to be very sparse, a consequence of the frequency with which typical phrases occur in even large corpora.", "labels": [], "entities": []}, {"text": "However, there are phrases that occur sufficiently frequently that a reasonable characterisation of their meaning can be captured with their observed distributional vector.", "labels": [], "entities": []}, {"text": "Such phrases can be exploited in order to assess the quality of a model of composition.", "labels": [], "entities": []}, {"text": "This is achieved by measuring the distributional similarity of the observed and inferred distributional vectors for these high frequency phrases.", "labels": [], "entities": []}, {"text": "The contributions of this paper are as follows.", "labels": [], "entities": []}, {"text": "We propose a novel approach to phrasal composition which uses higher order grammatical dependency relations as features.", "labels": [], "entities": [{"text": "phrasal composition", "start_pos": 31, "end_pos": 50, "type": "TASK", "confidence": 0.81427001953125}]}, {"text": "We demonstrate its potential in the context of adjective-noun composition by comparing (held-out) observed and inferred phrasal vectors.", "labels": [], "entities": []}, {"text": "Further, we compare different vector operations, different feature association scores and investigate the effect of weighting features before or after composition.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our corpus is a mid-2011 dump of WikiPedia.", "labels": [], "entities": []}, {"text": "This has been part-of-speech tagged, lemmatised and dependency parsed using the Malt Parser).", "labels": [], "entities": []}, {"text": "All major grammatical dependency relations involving open class parts of speech (nsubj, dobj, iobj, conj, amod, advmod, nnmod) have been extracted for all POS-tagged and lemmatised nouns and adjectives occurring 100 or more times.", "labels": [], "entities": []}, {"text": "In past work with conventional dependency relation vectors we found that using a feature threshold of 100, weighting features with PPMI and a cosine similarity score work well.", "labels": [], "entities": []}, {"text": "For experimental purposes, we have taken: Adjectives considered 32 of the most frequently occurring adjectives (see).", "labels": [], "entities": []}, {"text": "These adjectives include ones which would generally be considered intersective (e.g., female), subsective (e.g,, long) and nonsubsective/intensional (e.g., former) . For all of these adjectives there are at least 100 adjective-noun phrases which occur at least 100 times in the corpus.", "labels": [], "entities": []}, {"text": "We randomly selected 50 of the phrases for each adjective.", "labels": [], "entities": []}, {"text": "Note that our proposed method does not require any hyper parameters to beset during training, nor does it require a certain number of phrases per adjective.", "labels": [], "entities": []}, {"text": "For the purpose of these experiments we have a list of 1600 adjective-noun phrases, all of which occur at least 100 times in WikiPedia.", "labels": [], "entities": []}], "tableCaptions": []}