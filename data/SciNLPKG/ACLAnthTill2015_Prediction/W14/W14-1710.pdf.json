{"title": [{"text": "Grammatical Error Detection and Correction using a Single Maximum Entropy Model *", "labels": [], "entities": [{"text": "Grammatical Error Detection", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.8985660672187805}]}], "abstractContent": [{"text": "This paper describes the system of Shang-hai Jiao Tong Unvierity team in the CoNLL-2014 shared task.", "labels": [], "entities": [{"text": "Shang-hai Jiao Tong Unvierity team", "start_pos": 35, "end_pos": 69, "type": "DATASET", "confidence": 0.9094428300857544}, {"text": "CoNLL-2014 shared task", "start_pos": 77, "end_pos": 99, "type": "DATASET", "confidence": 0.7732803821563721}]}, {"text": "Error correction operations are encoded as a group of predefined labels and therefore the task is formulized as a multi-label classification task.", "labels": [], "entities": [{"text": "Error correction", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.714523121714592}]}, {"text": "For training, labels are obtained through a strict rule-based approach.", "labels": [], "entities": []}, {"text": "For decoding, errors are detected and corrected according to the classification results.", "labels": [], "entities": []}, {"text": "A single maximum entropy model is used for the classification implementation incorporated with an improved feature selection algorithm.", "labels": [], "entities": []}, {"text": "Our system achieved precision of 29.83, recall of 5.16 and F 0.5 of 15.24 in the official evaluation.", "labels": [], "entities": [{"text": "precision", "start_pos": 20, "end_pos": 29, "type": "METRIC", "confidence": 0.9998456239700317}, {"text": "recall", "start_pos": 40, "end_pos": 46, "type": "METRIC", "confidence": 0.9998767375946045}, {"text": "F 0.5", "start_pos": 59, "end_pos": 64, "type": "METRIC", "confidence": 0.9914485812187195}]}], "introductionContent": [{"text": "The task of CoNLL-2014 is grammatical error correction which consists of detecting and correcting the grammatical errors in English essays written by non-native speakers ().", "labels": [], "entities": [{"text": "CoNLL-2014", "start_pos": 12, "end_pos": 22, "type": "DATASET", "confidence": 0.8108749389648438}, {"text": "grammatical error correction", "start_pos": 26, "end_pos": 54, "type": "TASK", "confidence": 0.6137543519337972}, {"text": "detecting and correcting the grammatical errors in English essays written by non-native speakers", "start_pos": 73, "end_pos": 169, "type": "TASK", "confidence": 0.7206066388350266}]}, {"text": "The research of grammatical error correction can potentially help millions of people in the world who are learning English as foreign language.", "labels": [], "entities": [{"text": "grammatical error correction", "start_pos": 16, "end_pos": 44, "type": "TASK", "confidence": 0.7046654224395752}]}, {"text": "Although there have been many works on grammatical error correction, the current approaches mainly focus on very limited error types and the result is far from satisfactory.", "labels": [], "entities": [{"text": "grammatical error correction", "start_pos": 39, "end_pos": 67, "type": "TASK", "confidence": 0.6130684614181519}]}, {"text": "The CoNLL-2014 shared task, compared with the previous Help Our Own (HOO) tasks) considering only determiner and preposition errors and the CoNLL-2013 shared task fo- * This work was partially supported by the National Natural Science Foundation of China, the National Basic Research Program of China, the Science and Technology Commission of Shanghai Municipality, and the European Union Seventh Framework Program.", "labels": [], "entities": [{"text": "European Union Seventh Framework Program", "start_pos": 374, "end_pos": 414, "type": "DATASET", "confidence": 0.7444382548332215}]}, {"text": "\u2020 Corresponding author cusing on five major types of errors, requires to correct all 28 types of errors (.", "labels": [], "entities": [{"text": "author cusing", "start_pos": 16, "end_pos": 29, "type": "TASK", "confidence": 0.627287432551384}]}, {"text": "One traditional strategy is designing a system combined of a set of sub-models, where each submodel is specialized fora specific subtask, for example, correcting one type of errors.", "labels": [], "entities": []}, {"text": "This strategy is computationally efficient and can adopt different favorable features for each subtask.", "labels": [], "entities": []}, {"text": "Top ranked systems in) are based on this strategy.", "labels": [], "entities": []}, {"text": "However, the division of the model relies on prior-knowledges and the designing of different features for each sub-model requires a large amount of manual works.", "labels": [], "entities": []}, {"text": "This shortage is especially notable in CoNLL-2014 shared task, since the number of error types is much larger and the composition of errors is more complicated than before.", "labels": [], "entities": []}, {"text": "In contrast, we follow the work in (, integrating everything into one model.", "labels": [], "entities": []}, {"text": "This integrated system holds a merit that a one-way feature selection benefits the whole system and no additional process is needed to deal with the conflict or error propagation of every sub-models.", "labels": [], "entities": []}, {"text": "Here is a glance of this method: A set of more detailed error types are generated automatically from the original 28 types of errors.", "labels": [], "entities": []}, {"text": "The detailed error type can be regarded as the label of a word, thus the task of grammatical error detection is transformed to a multi-label classification task using maximum entropy model.", "labels": [], "entities": [{"text": "grammatical error detection", "start_pos": 81, "end_pos": 108, "type": "TASK", "confidence": 0.6661247809727987}]}, {"text": "A feature selection approach is introduced to get effective features from large amounts of feature candidates.", "labels": [], "entities": [{"text": "feature selection", "start_pos": 2, "end_pos": 19, "type": "TASK", "confidence": 0.813318282365799}]}, {"text": "Once errors are detected through word label classification, a rule-based method is used to make corrections according to their labels.", "labels": [], "entities": [{"text": "word label classification", "start_pos": 33, "end_pos": 58, "type": "TASK", "confidence": 0.6673179864883423}]}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes the system architecture.", "labels": [], "entities": []}, {"text": "Section 3 introduces the feature selection approach and the features we used.", "labels": [], "entities": [{"text": "feature selection", "start_pos": 25, "end_pos": 42, "type": "TASK", "confidence": 0.8281137943267822}]}, {"text": "Experiments and results are presented in section 5, followed by conclusion.", "labels": [], "entities": []}], "datasetContent": [{"text": "The final system we use is sys 220 with refined training data, the performance of our system on the developing corpus and the blind official test data is presented in.", "labels": [], "entities": []}, {"text": "The score is calculated using M 2 scorer.", "labels": [], "entities": [{"text": "M 2 scorer", "start_pos": 30, "end_pos": 40, "type": "METRIC", "confidence": 0.8933593233426412}]}], "tableCaptions": [{"text": " Table 5: Labels whose count is larger than 15.", "labels": [], "entities": []}]}