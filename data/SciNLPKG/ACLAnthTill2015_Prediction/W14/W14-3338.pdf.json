{"title": [{"text": "SHEF-Lite 2.0: Sparse Multi-task Gaussian Processes for Translation Quality Estimation", "labels": [], "entities": [{"text": "Translation Quality Estimation", "start_pos": 56, "end_pos": 86, "type": "TASK", "confidence": 0.9331908623377482}]}], "abstractContent": [{"text": "We describe our systems for the WMT14 Shared Task on Quality Estimation (sub-tasks 1.1, 1.2 and 1.3).", "labels": [], "entities": [{"text": "WMT14 Shared Task on Quality Estimation", "start_pos": 32, "end_pos": 71, "type": "TASK", "confidence": 0.5517582595348358}]}, {"text": "Our submissions use the framework of Multi-task Gaus-sian Processes, where we combine multiple datasets in a multi-task setting.", "labels": [], "entities": []}, {"text": "Due to the large size of our datasets we also experiment with Sparse Gaussian Processes, which aim to speedup training and prediction by providing sensible sparse approximations .", "labels": [], "entities": []}], "introductionContent": [{"text": "The purpose of machine translation (MT) quality estimation (QE) is to provide a quality prediction for new, unseen machine translated texts, without relying on reference translations (.", "labels": [], "entities": [{"text": "machine translation (MT) quality estimation (QE)", "start_pos": 15, "end_pos": 63, "type": "TASK", "confidence": 0.859917038679123}]}, {"text": "A common use of quality predictions is the decision between post-editing a given machine translated sentence and translating its source from scratch, based on whether its post-editing effort is estimated to be lower than the effort of translating the source sentence.", "labels": [], "entities": []}, {"text": "The WMT 2014 QE shared task defined a group of tasks related to QE.", "labels": [], "entities": [{"text": "WMT 2014 QE shared task", "start_pos": 4, "end_pos": 27, "type": "DATASET", "confidence": 0.8248589754104614}]}, {"text": "In this paper, we describe our submissions for subtasks 1.1, 1.2 and 1.3.", "labels": [], "entities": []}, {"text": "Our models are based on Gaussian Processes (GPs)), a non-parametric kernelised probabilistic framework.", "labels": [], "entities": []}, {"text": "We propose to combine multiple datasets to improve our QE models by applying GPs in a multi-task setting.", "labels": [], "entities": []}, {"text": "Our hypothesis is that using sensible multi-task learning settings gives improvements over simply pooling all datasets together.", "labels": [], "entities": []}, {"text": "Task 1.1 focuses on predicting post-editing effort for four language pairs: English-Spanish (en-es), Spanish-English (es-en), English-German (en-de), and German-English (de-en).", "labels": [], "entities": []}, {"text": "Each contains a different number of source sentences and their human translations, as well as 2-3 versions of machine translations: by a statistical (SMT) system, a rule-based system (RBMT) system and, for en-es/de only, a hybrid system.", "labels": [], "entities": []}, {"text": "Source sentences were extracted from tests sets of WMT13 and WMT12, and the translations were produced by top MT systems of each type and a human translator.", "labels": [], "entities": [{"text": "WMT13", "start_pos": 51, "end_pos": 56, "type": "DATASET", "confidence": 0.966494619846344}, {"text": "WMT12", "start_pos": 61, "end_pos": 66, "type": "DATASET", "confidence": 0.912261962890625}, {"text": "MT", "start_pos": 110, "end_pos": 112, "type": "TASK", "confidence": 0.9059156775474548}]}, {"text": "Labels range from 1 to 3, with 1 indicating a perfect translation and 3, a low quality translation.", "labels": [], "entities": []}, {"text": "The purpose of task 1.2 is to predict HTER scores (Human Translation Error Rate)) using a dataset composed of 896 English-Spanish sentences translated by a MT system and post-edited by a professional translator.", "labels": [], "entities": [{"text": "HTER scores (Human Translation Error Rate))", "start_pos": 38, "end_pos": 81, "type": "METRIC", "confidence": 0.713318757712841}]}, {"text": "Finally, task 1.3 aims at predicting post-editing time, using a subset of 650 sentences from the Task 1.2 dataset.", "labels": [], "entities": [{"text": "predicting post-editing", "start_pos": 26, "end_pos": 49, "type": "TASK", "confidence": 0.8211149871349335}]}, {"text": "For each task, participants can submit two types of results: scoring and ranking.", "labels": [], "entities": []}, {"text": "For scoring, evaluation is made in terms of Mean Absolute Error (MAE) and Root Mean Square Error (RMSE).", "labels": [], "entities": [{"text": "Mean Absolute Error (MAE)", "start_pos": 44, "end_pos": 69, "type": "METRIC", "confidence": 0.9626488089561462}, {"text": "Root Mean Square Error (RMSE)", "start_pos": 74, "end_pos": 103, "type": "METRIC", "confidence": 0.8582662088530404}]}, {"text": "For ranking, DeltaAvg and Spearman's rank correlation were used as evaluation metrics.", "labels": [], "entities": [{"text": "DeltaAvg", "start_pos": 13, "end_pos": 21, "type": "DATASET", "confidence": 0.5270174741744995}, {"text": "Spearman's rank correlation", "start_pos": 26, "end_pos": 53, "type": "METRIC", "confidence": 0.4940052554011345}]}], "datasetContent": [{"text": "Our submissions are based on multi-task settings.", "labels": [], "entities": []}, {"text": "For task 1.1, we consider each language pair as a different task, training one model for all pairs.", "labels": [], "entities": []}, {"text": "For tasks 1.2 and 1.3, we used additional datasets and encoded each one as a different task (totalling 3 tasks): WMT13: these are the datasets provided in last year's QE shared task ().", "labels": [], "entities": [{"text": "WMT13", "start_pos": 113, "end_pos": 118, "type": "DATASET", "confidence": 0.8529946804046631}]}, {"text": "We combined training and test sets, totalling 2, 754 sentences for HTER prediction and 1, 003 sentences for post-editing time prediction, both for English-Spanish.", "labels": [], "entities": [{"text": "HTER prediction", "start_pos": 67, "end_pos": 82, "type": "TASK", "confidence": 0.8217858374118805}, {"text": "post-editing time prediction", "start_pos": 108, "end_pos": 136, "type": "TASK", "confidence": 0.6329322457313538}]}, {"text": "EAMT11: this dataset is provided by Specia (2011) and is composed of 1, 000 EnglishSpanish sentences annotated in terms of HTER and post-editing time.", "labels": [], "entities": [{"text": "EAMT11", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.8485608696937561}, {"text": "HTER", "start_pos": 123, "end_pos": 127, "type": "METRIC", "confidence": 0.6926019787788391}]}, {"text": "For each task we prepared two submissions: one trained on a standard GP with the full 80 features set and another one trained on a sparse GP with a subset of 40 features.", "labels": [], "entities": []}, {"text": "The features were chosen by training a smaller model on a subset of 400 instances and following the procedure explained in Section 2.3 for feature selection, with a pre-define cutoff point on the number of features, based on previous experiments.", "labels": [], "entities": [{"text": "feature selection", "start_pos": 139, "end_pos": 156, "type": "TASK", "confidence": 0.6740315556526184}]}, {"text": "The sparse models were trained using 400 inducing inputs.", "labels": [], "entities": []}, {"text": "To select an appropriate multi-task setting for our submissions we performed preliminary experiments using a 90%/10% split on the corresponding training set for each task.", "labels": [], "entities": []}, {"text": "The resulting MAE scores are shown in: MAE results for preliminary experiments on standard GPs.", "labels": [], "entities": [{"text": "MAE", "start_pos": 14, "end_pos": 17, "type": "METRIC", "confidence": 0.8398249745368958}, {"text": "MAE", "start_pos": 39, "end_pos": 42, "type": "METRIC", "confidence": 0.84336256980896}]}, {"text": "Post-editing time scores for task 1.3 are shown on log time per word.: MAE results for preliminary experiments on sparse GPs.", "labels": [], "entities": [{"text": "MAE", "start_pos": 71, "end_pos": 74, "type": "METRIC", "confidence": 0.9542211890220642}]}, {"text": "Post-editing time scores for task 1.3 are shown on log time per word.", "labels": [], "entities": []}, {"text": "official submissions, after re-training on the corresponding full training sets.", "labels": [], "entities": []}, {"text": "To check the speed-ups obtained from using sparse GPs, we measured wall clock times for training and prediction in Task 1.1 using the \"Independent\" multi-task setting.", "labels": [], "entities": []}, {"text": "shows the resulting times and the corresponding speed-ups when comparing to the standard GP.", "labels": [], "entities": []}, {"text": "For comparison, we also trained a model using 200 inducing inputs, although we did not use the results of this model in our submissions.", "labels": [], "entities": []}, {"text": "Time (secs) Speed-up Standard GP 12122 -Sparse GP (m=400) 3376 3.59x Sparse GP (m=200) 978 12.39x: Wall clock times and speed-ups for GPs training and prediction: full versus sparse GPs.", "labels": [], "entities": [{"text": "Time", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9047315716743469}, {"text": "GP 12122 -Sparse GP", "start_pos": 30, "end_pos": 49, "type": "DATASET", "confidence": 0.6249020397663116}, {"text": "prediction", "start_pos": 151, "end_pos": 161, "type": "TASK", "confidence": 0.9667823314666748}]}, {"text": "shows the results for Task 1.1.", "labels": [], "entities": []}, {"text": "Using standard GPs we obtained improved results over the baseline for English-Spanish and EnglishGerman only, with particularly substantial improvements for English-Spanish, which also happens for sparse GPs.", "labels": [], "entities": []}, {"text": "This maybe related to the larger size of this dataset when compared to the others.", "labels": [], "entities": []}, {"text": "Our results here are mostly inconclusive though and we plan to investigate this setting more in depth in the future.", "labels": [], "entities": []}, {"text": "Specifically, due to the coarse behaviour of the labels, ordinal regression GP models (like the one proposed in ()) could be useful for this task.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: MAE results for preliminary experiments on standard GPs. Post-editing time scores for task 1.3  are shown on log time per word.", "labels": [], "entities": [{"text": "MAE", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.881263792514801}]}, {"text": " Table 2: MAE results for preliminary experiments on sparse GPs. Post-editing time scores for task 1.3  are shown on log time per word.", "labels": [], "entities": [{"text": "MAE", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9017763137817383}]}, {"text": " Table 4: Official results for task 1.1. The top table shows results for the ranking subtask (\u2206: DeltaAvg;  \u03c1: Spearman's correlation). The bottom table shows results for the scoring subtask.", "labels": [], "entities": [{"text": "DeltaAvg;  \u03c1: Spearman's correlation)", "start_pos": 97, "end_pos": 134, "type": "METRIC", "confidence": 0.6722828969359398}]}, {"text": " Table 5: Official results for task 1.2.", "labels": [], "entities": []}, {"text": " Table 6: Official results for task 1.3.", "labels": [], "entities": []}]}