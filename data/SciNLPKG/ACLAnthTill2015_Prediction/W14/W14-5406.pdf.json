{"title": [{"text": "A Poodle or a Dog? Evaluating Automatic Image Annotation Using Human Descriptions at Different Levels of Granularity", "labels": [], "entities": [{"text": "Evaluating Automatic Image Annotation", "start_pos": 19, "end_pos": 56, "type": "TASK", "confidence": 0.8338110148906708}]}], "abstractContent": [{"text": "Different people may describe the same object in different ways, and at varied levels of granular-ity (\"poodle\", \"dog\", \"pet\" or \"animal\"?)", "labels": [], "entities": []}, {"text": "In this paper, we propose the idea of 'granularity-aware' groupings where semantically related concepts are grouped across different levels of granularity to capture the variation in how different people describe the same image content.", "labels": [], "entities": []}, {"text": "The idea is demonstrated in the task of automatic image annotation, where these semantic groupings are used to alter the results of image annotation in a manner that affords different insights from its initial, category-independent rankings.", "labels": [], "entities": []}, {"text": "The semantic groupings are also incorporated during evaluation against image descriptions written by humans.", "labels": [], "entities": []}, {"text": "Our experiments show that semantic groupings result in image annotations that are more informative and flexible than without groupings, although being too flexible may result in image annotations that are less informative.", "labels": [], "entities": []}], "introductionContent": [{"text": "Describing the content of an image is essential for various tasks such as image indexing and retrieval, and the organization and browsing of large image collections.", "labels": [], "entities": [{"text": "Describing the content of an image", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.8702058295408884}, {"text": "image indexing and retrieval", "start_pos": 74, "end_pos": 102, "type": "TASK", "confidence": 0.7537656277418137}]}, {"text": "Recent years have seen substantial progress in the field of visual object recognition, allowing systems to automatically annotate an image with a list of terms representing concepts depicted in the image.", "labels": [], "entities": [{"text": "visual object recognition", "start_pos": 60, "end_pos": 85, "type": "TASK", "confidence": 0.7140771945317587}]}, {"text": "Fueled by advances in recognition algorithms and the availability of large scale datasets such as ImageNet (, current systems are able to recognize thousands of object categories with reasonable accuracy, for example achieving an error rate of 0.11 in classifying 1, 000 categories in the ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC13) (.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 195, "end_pos": 203, "type": "METRIC", "confidence": 0.9811216592788696}, {"text": "error rate", "start_pos": 230, "end_pos": 240, "type": "METRIC", "confidence": 0.9549985826015472}, {"text": "ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC13)", "start_pos": 289, "end_pos": 354, "type": "TASK", "confidence": 0.6906098872423172}]}, {"text": "However, the ILSVRC13 classification challenge assumes each image is annotated with only one correct label, although systems are allowed up to five guesses per image to make the correct prediction (or rather, to match the ground truth label).", "labels": [], "entities": [{"text": "ILSVRC13 classification", "start_pos": 13, "end_pos": 36, "type": "TASK", "confidence": 0.7101821601390839}]}, {"text": "The problem with this is that it becomes difficult to guess what the 'correct' label is, especially when many other categories can equally be considered correct.", "labels": [], "entities": []}, {"text": "For instance, should a system label an image containing an instance of a dog (and possibly some other objects like a ball and a couch) as \"dog\", \"poodle\", \"puppy\", \"pet\", \"domestic dog\", \"canine\" or even \"animal\" (in addition to \"ball\", \"tennis ball\", \"toy\", \"couch\", \"sofa\", etc.)?", "labels": [], "entities": [{"text": "label an image containing an instance of a dog (and possibly some other objects like a ball and a couch) as \"dog\", \"poodle\", \"puppy\", \"pet\", \"domestic dog\", \"canine\" or even \"animal\" (in addition to \"ball\", \"tennis ball\", \"toy\", \"couch\", \"sofa\", etc.", "start_pos": 30, "end_pos": 280, "type": "Description", "confidence": 0.8368507524331411}]}, {"text": "The problem becomes even harder when the number of possible ways to refer to the same object instance increases, but the number of prediction slots to fill remains limited.", "labels": [], "entities": []}, {"text": "With so many options from which to choose, how do we know what the 'correct' annotation is supposed to be?", "labels": [], "entities": []}, {"text": "In this paper, we take a human-centric view of the problem, motivated by the observation that humans are likely to be the end-users or consumers of such linguistic image annotations.", "labels": [], "entities": []}, {"text": "In particular, we investigate the effects of grouping semantically related concepts that may refer to the same object instance in an image.", "labels": [], "entities": []}, {"text": "Our work is related to the idea of basic-level categories in Linguistics, where most people have a natural preference to classify certain object categories at a particular level of granularity, e.g. \"bird\" instead of \"sparrow\" or \"animal\".", "labels": [], "entities": []}, {"text": "However, we argue that what one person considers 'basic-level' may not necessarily be 'basic-level' to another, depending on the person's knowledge, expertise, interest, or the context of the task at hand.", "labels": [], "entities": []}, {"text": "For example, shows that users label groups of images and describe individual images differently with regards to the level of abstraction.", "labels": [], "entities": []}, {"text": "The key idea behind our proposed 'granularity-aware' approach is to group semantically related categories across different levels of granularity to account for how different people would describe content in an image differently.", "labels": [], "entities": []}, {"text": "We demonstrate the benefits of the 'granularity-aware' approach by producing a re-ranking of visual classifier outputs for groups of concept nodes, e.g. WordNet synsets.", "labels": [], "entities": []}, {"text": "The concept nodes are grouped across different levels of specificity within a semantic hierarchy (Section 3.1).", "labels": [], "entities": []}, {"text": "This models better the richness of the vocabulary and lexical semantic relations in natural language.", "labels": [], "entities": []}, {"text": "In this sense these groupings are used to alter the results of image annotation in a manner that affords different insights from its initial, category-independent rankings.", "labels": [], "entities": []}, {"text": "For example, if the annotation mentions only \"dog\" but not \"poodle\", a system ranking \"poodle\" at 1 and \"dog\" at 20 will have a lower overall score than a system ranking \"dog\" at 1, although both are equally correct.", "labels": [], "entities": []}, {"text": "Grouping (\"poodle\" or \"dog\") however will allow a fairer evaluation and comparison where both systems are now considered equally good.", "labels": [], "entities": []}, {"text": "The 'granularityaware' groupings will also be used in evaluating these re-rankings using textual descriptions written by humans, rather than a keyword-based gold-standard annotation.", "labels": [], "entities": []}, {"text": "The hypothesis is that by modeling the variation in granularity levels for different concepts, we can gain a more informative insight as to how the output of image annotation systems can relate to how a person describes what he or she perceives in an image, and consequently produce image annotation systems that are more human-centric.", "labels": [], "entities": []}, {"text": "The remainder of the paper is organized as follows: Section 2 discusses related work.", "labels": [], "entities": []}, {"text": "Section 3 describes our proposed 'granularity-aware' approach to group related concepts across different levels of granularity.", "labels": [], "entities": []}, {"text": "It also discusses how to apply the idea both in automatic image annotation, by re-ranking noisy visual classifier outputs in a 'granularity-aware' manner, and in evaluation of classifier outputs against human descriptions of images.", "labels": [], "entities": [{"text": "automatic image annotation", "start_pos": 48, "end_pos": 74, "type": "TASK", "confidence": 0.6200883487860361}]}, {"text": "The results of the proposed method are reported in Section 4.", "labels": [], "entities": []}, {"text": "Finally, Section 5 offers conclusions and proposes possible future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "The image dataset used in our experiments (Section 4.1) is annotated with five full-sentence captions per image but not keyword labels.", "labels": [], "entities": []}, {"text": "Although an option would be to obtain keyword annotations via crowdsourcing, it is time consuming and expensive and also requires validating the annotation quality.", "labels": [], "entities": []}, {"text": "Instead, we exploit the existing full-sentence captions from the dataset to automatically generate a gold standard keyword annotation for evaluating our ranked classifier outputs.", "labels": [], "entities": []}, {"text": "The use of such captions is also inline with our goal of making the evaluation of image annotation systems more human-centric.", "labels": [], "entities": []}, {"text": "For each caption, we extract nouns using the open source tool For each image, each noun is assigned an individual relevance score, which is the number of captions that mentions the noun.", "labels": [], "entities": []}, {"text": "This upweights important objects while downweighting less important objects (or errors from the annotator or the parser).", "labels": [], "entities": []}, {"text": "The result is a list of nouns that humans use to describe objects present in the image, each weighted by its relevance score.", "labels": [], "entities": []}, {"text": "We assume nouns that appear in the same WordNet synset (\"bicycle\" and \"bike\") are synonyms and that they refer to the same object instance in the image.", "labels": [], "entities": []}, {"text": "Hence, we group them as a single label-group, with the relevance score taken to be the maximum relevance score among the nouns in the group.", "labels": [], "entities": [{"text": "relevance score", "start_pos": 55, "end_pos": 70, "type": "METRIC", "confidence": 0.9725819528102875}]}, {"text": "Since there are only five captions per image, the proposed approach will result in a sparse set of keywords.", "labels": [], "entities": []}, {"text": "This mirrors the problem described in Section 1 where systems have to 'guess' the so-called 'correct' labels, thus allowing us to demonstrate the effectiveness of our 'granularity-aware' re-rankings.", "labels": [], "entities": []}, {"text": "In order to compare the annotations against the re-rankings, we will need to map the keywords to the semantic groupings.", "labels": [], "entities": []}, {"text": "This is done by matching the nouns to any of the terms in a semantic group, with a corresponding label (G \u03bb w ) for each group (Section 3.2).", "labels": [], "entities": []}, {"text": "Nouns assigned the same label are merged, with the new relevance score being the maximum relevance score among the nouns.", "labels": [], "entities": [{"text": "relevance score", "start_pos": 55, "end_pos": 70, "type": "METRIC", "confidence": 0.9701200723648071}, {"text": "relevance score", "start_pos": 89, "end_pos": 104, "type": "METRIC", "confidence": 0.9291859567165375}]}, {"text": "If a noun matches more than one semantic group (polyseme/homonym), we treat all groups as relevant and divide the relevance score uniformly among the groups.", "labels": [], "entities": []}, {"text": "Evaluation is then performed by matching the semantic group labels against the image annotation output.", "labels": [], "entities": []}, {"text": "Our proposed method is evaluated on the dataset and categories as will be described in Section 4.1, by reranking the output of the visual classifiers in Section 4.2.", "labels": [], "entities": []}, {"text": "The effects of semantic groupings are explored using different settings of \u03bb (see Section 3.1).", "labels": [], "entities": []}, {"text": "To ensure any improvements in scores are not purely as a result of having a shorter list of concepts to rank, we compare the results to a set of baseline groupings where synsets are grouped in a random manner.", "labels": [], "entities": []}, {"text": "For a fair comparison the baselines contain the same number of groups and cluster size distributions as our semantic groupings.", "labels": [], "entities": []}, {"text": "The Flickr8k dataset) is used in our image annotation experiments.", "labels": [], "entities": [{"text": "Flickr8k dataset", "start_pos": 4, "end_pos": 20, "type": "DATASET", "confidence": 0.9262400567531586}]}, {"text": "The dataset contains 8,091 images, each annotated with five textual descriptions.", "labels": [], "entities": []}, {"text": "To demonstrate the notion of granularity in large-scale object hierarchies, we use as object categories synset nodes from WordNet).", "labels": [], "entities": [{"text": "WordNet", "start_pos": 122, "end_pos": 129, "type": "DATASET", "confidence": 0.9500569105148315}]}, {"text": "Ideally, we would like to be able to train visual classifiers for all synset categories in ImageNet ().", "labels": [], "entities": []}, {"text": "However, we limit the categories to only synsets with terms occurring in the textual descriptions of the Flickr8k dataset to reduce computational complexity, and regard the use of more categories as future work.", "labels": [], "entities": [{"text": "Flickr8k dataset", "start_pos": 105, "end_pos": 121, "type": "DATASET", "confidence": 0.9864821135997772}]}, {"text": "This results in a total of 1,372 synsets to be used in our experiments.", "labels": [], "entities": []}, {"text": "The synsets include both leaf nodes as well as internal nodes in the WordNet hierarchy.", "labels": [], "entities": [{"text": "WordNet hierarchy", "start_pos": 69, "end_pos": 86, "type": "DATASET", "confidence": 0.9278464317321777}]}, {"text": "The systems are evaluated using the Normalized Discounted Cumulative Gain (NDCG) ( measure.", "labels": [], "entities": []}, {"text": "This measure is commonly used in Information Retrieval (IR) to evaluate ranked retrieval results where each document is assigned a relevance score.", "labels": [], "entities": [{"text": "Information Retrieval (IR)", "start_pos": 33, "end_pos": 59, "type": "TASK", "confidence": 0.860964560508728}]}, {"text": "This measure favours rankings where the most relevant items are ranked ahead of less relevant items, and does not penalize irrelevant items.", "labels": [], "entities": []}, {"text": "The NDCG at position k, N DCG k , fora set of test images I is defined as: where R p is the relevance score of the concept at position p, and IDCG k (i) is the ideal discounted cumulative gain fora perfect ranking algorithm at position k, which normalizes the overall measure to be between 0.0 to 1.0.", "labels": [], "entities": []}, {"text": "This makes the scores comparable across rankings regardless of the number of synset groups involved.", "labels": [], "entities": []}, {"text": "For each grouping, we report the results of N DCG k for the largest possible k (i.e. the number of synset groups), which gives the overall performance of the rankings.", "labels": [], "entities": []}, {"text": "shows the results of re-ranking the output of the visual classifiers (Section 4.2), with different semantic groupings formed by varying \u03bb.", "labels": [], "entities": []}, {"text": "The effects of the proposed groupings is apparent when compared to the random baseline groupings.", "labels": [], "entities": []}, {"text": "As we increase the value of \u03bb (allowing groups to have a larger range of granularity), the NDCG scores also consistently increase.", "labels": [], "entities": []}, {"text": "However, higher NDCG scores do not necessarily equate to better groupings, as semantic groups with too much flexibility in granularity levels may end up being less informative, for example by annotating a \"being\" in an image.", "labels": [], "entities": []}, {"text": "The informativeness of the groupings is a subjective issue depending on the context, and makes an interesting open question.", "labels": [], "entities": []}, {"text": "To provide insight into the effects of our groupings, shows an example whereat low levels of \u03bb (rigid flexibility), the various dog species are highly ranked but none of them is considered relevant by the evaluation system.", "labels": [], "entities": []}, {"text": "However, at \u03bb = 0.5 most dog species are grouped as a \"dog\" semantic group resulting in a highly relevant prediction, while at the same time allowing the \"sidewalk\" group to rise higher in the rankings.", "labels": [], "entities": []}, {"text": "At higher levels of \u03bb, however, the semantic groupings become less informative when superordinate groups like \"being\", \"artifact\" and \"equipment\" are formed, suggesting that higher flexibility with granularity levels may not always be more informative.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results of re-ranking with semantic groupings. The first two rows show the average NDCG  scores for the proposed groupings and the random baseline groupings, for different groupings formed by  varying \u03bb. The bottom row shows the number of semantic groups formed for different values of \u03bb.", "labels": [], "entities": []}]}