{"title": [{"text": "Exploring Options for Fast Domain Adaptation of Dependency Parsers", "labels": [], "entities": [{"text": "Fast Domain Adaptation of Dependency Parsers", "start_pos": 22, "end_pos": 66, "type": "TASK", "confidence": 0.7668113708496094}]}], "abstractContent": [{"text": "The paper explores different domain-independent techniques to adapt a dependency parser trained on a general-language corpus to parse web texts (online reviews, newsgroup posts, we-blogs): co-training, word clusters, and a crowd-sourced dictionary.", "labels": [], "entities": [{"text": "parse web texts (online reviews, newsgroup posts, we-blogs)", "start_pos": 128, "end_pos": 187, "type": "TASK", "confidence": 0.7602904886007309}]}, {"text": "We examine the relative utility of these techniques as well as different ways to put them together to achieve maximum parsing accuracy.", "labels": [], "entities": [{"text": "parsing", "start_pos": 118, "end_pos": 125, "type": "TASK", "confidence": 0.9702908992767334}, {"text": "accuracy", "start_pos": 126, "end_pos": 134, "type": "METRIC", "confidence": 0.8343828320503235}]}, {"text": "While we find that co-training and word clusters produce the most promising results, there is little additive improvement when combining the two techniques, which suggests that in the absence of large grammatical discrepancies between the training and test domains, they address largely the same problem, that of unknown vocabulary, with word clusters being a somewhat more effective solution for it.", "labels": [], "entities": []}, {"text": "Our highest results were achieved by a combination of word clusters and co-training, significantly improving on the baseline, by up to 1.67%.", "labels": [], "entities": []}, {"text": "Evaluation of the best configurations on the SANCL-2012 test data (Petrov and McDonald, 2012) showed that they outperform all the shared task submissions that used a single parser to parse test data, averaging the results across all the test sets.", "labels": [], "entities": [{"text": "SANCL-2012 test data", "start_pos": 45, "end_pos": 65, "type": "DATASET", "confidence": 0.9373823404312134}]}], "introductionContent": [{"text": "Domain adaptation of a statistical dependency parser is a problem that is of much importance for many practical NLP applications.", "labels": [], "entities": [{"text": "Domain adaptation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7930397093296051}, {"text": "statistical dependency parser", "start_pos": 23, "end_pos": 52, "type": "TASK", "confidence": 0.6406628787517548}]}, {"text": "Previous research has shown that the accuracy of parsing significantly drops when a general-language model is applied to narrow domains like financial news, biomedical texts (), web data (, or patents (.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.9991481304168701}, {"text": "parsing", "start_pos": 49, "end_pos": 56, "type": "TASK", "confidence": 0.9802672863006592}]}, {"text": "Ina preliminary experiment, we looked at the effect of cross-domain parsing on three state-ofthe-art parsers -Malt, MST), and Mate parser) -trained on the CoNLL09 dataset and tested on texts from different domains in the OntoNotes v5.0 corpus as well as the in-domain CoNLL09 test set.", "labels": [], "entities": [{"text": "cross-domain parsing", "start_pos": 55, "end_pos": 75, "type": "TASK", "confidence": 0.6762763410806656}, {"text": "CoNLL09 dataset", "start_pos": 155, "end_pos": 170, "type": "DATASET", "confidence": 0.960114061832428}, {"text": "OntoNotes v5.0 corpus", "start_pos": 221, "end_pos": 242, "type": "DATASET", "confidence": 0.8298139174779257}, {"text": "CoNLL09 test set", "start_pos": 268, "end_pos": 284, "type": "DATASET", "confidence": 0.9552669723828634}]}, {"text": "The results (see) indicate that depending on the application domain, the parsing accuracy can suffer an absolute drop of as much as 16%.", "labels": [], "entities": [{"text": "parsing", "start_pos": 73, "end_pos": 80, "type": "TASK", "confidence": 0.9731036424636841}, {"text": "accuracy", "start_pos": 81, "end_pos": 89, "type": "METRIC", "confidence": 0.9497740864753723}, {"text": "absolute", "start_pos": 104, "end_pos": 112, "type": "METRIC", "confidence": 0.968643307685852}]}, {"text": "Ina typical domain adaptation scenario, there are in-domain texts that are manually annotated and that are used to train a general-language parser, and out-of-domain or target domain texts that are This work is licensed under a Creative Commons Attribution 4.0 International Licence.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 12, "end_pos": 29, "type": "TASK", "confidence": 0.7502822577953339}]}, {"text": "Page numbers and proceedings footer are added by the organisers.", "labels": [], "entities": []}, {"text": "Licence details: http://creativecommons.org/licenses/by/4.0/ parsed during parser testing.", "labels": [], "entities": []}, {"text": "In addition, a certain amount of unlabelled target domain texts maybe available that can be leveraged in this or that way to facilitate domain adaptation.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 136, "end_pos": 153, "type": "TASK", "confidence": 0.7393047213554382}]}, {"text": "To address the problem of domain adaption, previous work focused on weakly supervised methods to re-train parsers on automatically parsed out-of-domain texts, through techniques such as co-training, self-training (, and uptraining (; selecting or weighting sentences from annotated in-domain data that fit best with the target domain).", "labels": [], "entities": [{"text": "domain adaption", "start_pos": 26, "end_pos": 41, "type": "TASK", "confidence": 0.7166780829429626}]}, {"text": "Another line of research aims specifically to overcome the lexical gap between the training data and the target domain texts.", "labels": [], "entities": []}, {"text": "These approaches include techniques such as text pre-processing and normalization, the use of external lexica and morphological clues to predict PoS tags of unknown target domain words), discrete or continuous word clusters computed from unlabelled target domain texts), selectional preferences modelled from word co-occurrences obtained from unannotated texts ().", "labels": [], "entities": []}], "datasetContent": [{"text": "As a measure of parser accuracy, we report labeled attachment scores (LAS), the percentage of dependencies which are attached and labeled correctly.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9516526460647583}, {"text": "labeled attachment scores (LAS)", "start_pos": 43, "end_pos": 74, "type": "METRIC", "confidence": 0.8556792537371317}]}, {"text": "Significance testing was performed using paired t-test.", "labels": [], "entities": [{"text": "Significance", "start_pos": 0, "end_pos": 12, "type": "METRIC", "confidence": 0.8726586699485779}]}], "tableCaptions": [{"text": " Table 1: Labelled accuracy scores achieved by the MST, Malt, and Mate parsers trained on CoNLL09  data and tested on different specialist domains.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9268767237663269}, {"text": "MST", "start_pos": 51, "end_pos": 54, "type": "DATASET", "confidence": 0.6818622350692749}, {"text": "CoNLL09  data", "start_pos": 90, "end_pos": 103, "type": "DATASET", "confidence": 0.9581448137760162}]}, {"text": " Table 2: The size of OntoNotes train and test datasets.", "labels": [], "entities": [{"text": "OntoNotes train and test datasets", "start_pos": 22, "end_pos": 55, "type": "DATASET", "confidence": 0.6316267073154449}]}, {"text": " Table 3: The size of SANCL train and test datasets.", "labels": [], "entities": [{"text": "SANCL train and test datasets", "start_pos": 22, "end_pos": 51, "type": "DATASET", "confidence": 0.728507137298584}]}, {"text": " Table 4: The size of unlabelled datasets.", "labels": [], "entities": []}, {"text": " Table 5: The effect of the number of word clusters on in-and out-of-domain parsing, using the reviews  and weblogs subsets of the SANCL-2012 unlabelled data.", "labels": [], "entities": [{"text": "SANCL-2012 unlabelled data", "start_pos": 131, "end_pos": 157, "type": "DATASET", "confidence": 0.8830871979395548}]}, {"text": " Table 6: The effect of filtering out rare words on word clusters, using the reviews and weblogs subsets  of the SANCL-2012 unlabelled data.", "labels": [], "entities": [{"text": "word clusters", "start_pos": 52, "end_pos": 65, "type": "TASK", "confidence": 0.6619993150234222}, {"text": "SANCL-2012 unlabelled data", "start_pos": 113, "end_pos": 139, "type": "DATASET", "confidence": 0.9070593516031901}]}, {"text": " Table 8: The effect of the domain of unlabelled data on word clusters, discarding word types with count  less than 3.", "labels": [], "entities": []}, {"text": " Table 9: The effect of the Wiktionary lexicon on parsing accuracy.", "labels": [], "entities": [{"text": "parsing", "start_pos": 50, "end_pos": 57, "type": "TASK", "confidence": 0.9540654420852661}, {"text": "accuracy", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.9395122528076172}]}, {"text": " Table 10: The baselines of parsers used in co-training experiments.", "labels": [], "entities": []}, {"text": " Table 12: The effect of removing short sentences from generated training data.", "labels": [], "entities": []}, {"text": " Table 13: Accuracy scores for tri-training (Mate+Malt+MST) and the best two-parser co-training algo- rithm (Mate+Malt).", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.9994534850120544}]}, {"text": " Table 15: Combination of co-training with word clusters and an external lexicon, SANCL test set.", "labels": [], "entities": [{"text": "SANCL test set", "start_pos": 82, "end_pos": 96, "type": "DATASET", "confidence": 0.7922527392705282}]}]}