{"title": [{"text": "Grammatical error correction using hybrid systems and type filtering", "labels": [], "entities": [{"text": "Grammatical error correction", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8872087399164835}, {"text": "type filtering", "start_pos": 54, "end_pos": 68, "type": "TASK", "confidence": 0.841732531785965}]}], "abstractContent": [{"text": "This paper describes our submission to the CoNLL 2014 shared task on grammatical error correction using a hybrid approach, which includes both a rule-based and an SMT system augmented by a large web-based language model.", "labels": [], "entities": [{"text": "CoNLL 2014 shared task on grammatical error correction", "start_pos": 43, "end_pos": 97, "type": "TASK", "confidence": 0.6717204041779041}]}, {"text": "Furthermore, we demonstrate that correction type estimation can be used to remove unnecessary corrections, improving precision without harming recall.", "labels": [], "entities": [{"text": "correction type estimation", "start_pos": 33, "end_pos": 59, "type": "TASK", "confidence": 0.5184385478496552}, {"text": "precision", "start_pos": 117, "end_pos": 126, "type": "METRIC", "confidence": 0.9990967512130737}, {"text": "recall", "start_pos": 143, "end_pos": 149, "type": "METRIC", "confidence": 0.9971767663955688}]}, {"text": "Our best hybrid system achieves state-of-the-art results, ranking first on the original test set and second on the test set with alternative annotations.", "labels": [], "entities": []}], "introductionContent": [{"text": "Grammatical error correction has attracted considerable interest in the last few years, especially through a series of 'shared tasks'.", "labels": [], "entities": [{"text": "Grammatical error correction", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8867649038632711}]}, {"text": "These efforts have helped to provide a common ground for evaluating and comparing systems while encouraging research in the field.", "labels": [], "entities": []}, {"text": "These shared tasks have primarily focused on English as a second or foreign language and addressed different error types.", "labels": [], "entities": []}, {"text": "The, for example, included all error types whereas HOO 2012 () and the CoNLL 2013 shared task ( ) were restricted to only two and five types respectively.", "labels": [], "entities": [{"text": "HOO 2012", "start_pos": 51, "end_pos": 59, "type": "DATASET", "confidence": 0.8347141742706299}, {"text": "CoNLL 2013 shared task", "start_pos": 71, "end_pos": 93, "type": "DATASET", "confidence": 0.8336839824914932}]}, {"text": "In this paper, we describe our submission to the CoNLL 2014 shared task (), which involves correcting all the errors in essays written in English by students at the National University of Singapore.", "labels": [], "entities": [{"text": "CoNLL 2014 shared task", "start_pos": 49, "end_pos": 71, "type": "DATASET", "confidence": 0.8087045550346375}, {"text": "correcting all the errors in essays written in English by students at the National University of Singapore", "start_pos": 91, "end_pos": 197, "type": "TASK", "confidence": 0.6655583136221942}]}, {"text": "An all-type task poses a greater challenge, since correcting open-class types (such as spelling or collocation errors) requires different correction strategies than those in closed classes (such as determiners or prepositions).", "labels": [], "entities": []}, {"text": "In this scenario, hybrid systems or combinations of correction modules seem more appropriate and typically produce good results.", "labels": [], "entities": []}, {"text": "In fact, most of the participating teams in previous shared tasks have used a combination of modules or systems for their submissions, even for correcting closedclass types.", "labels": [], "entities": [{"text": "correcting closedclass types", "start_pos": 144, "end_pos": 172, "type": "TASK", "confidence": 0.856412390867869}]}, {"text": "In line with previous research, we present a hybrid approach that employs a rule-based error correction system and an ad-hoc statistical machine translation (SMT) system, as well as a large-scale language model to rank alternative corrections and an error type filtering technique.", "labels": [], "entities": [{"text": "rule-based error correction", "start_pos": 76, "end_pos": 103, "type": "TASK", "confidence": 0.5972073376178741}, {"text": "statistical machine translation (SMT)", "start_pos": 125, "end_pos": 162, "type": "TASK", "confidence": 0.8230337997277578}, {"text": "error type filtering", "start_pos": 250, "end_pos": 270, "type": "TASK", "confidence": 0.5982480148474375}]}, {"text": "The remainder of this paper is organised as follows: Section 2 describes our approach and each component in detail, Section 3 presents our experiments using the CoNLL 2014 shared task development set and Section 4 reports our official results on the test set.", "labels": [], "entities": [{"text": "CoNLL 2014 shared task development set", "start_pos": 161, "end_pos": 199, "type": "DATASET", "confidence": 0.9280546108881632}]}, {"text": "Finally, we discuss the performance of our system and present an error analysis in Section 5 and conclude in Section 6.", "labels": [], "entities": [{"text": "error analysis", "start_pos": 65, "end_pos": 79, "type": "METRIC", "confidence": 0.9435750842094421}]}], "datasetContent": [{"text": "We carried out a series of experiments on the development set using different pipelines and combinations of systems in order to find an optimal setting.", "labels": [], "entities": []}, {"text": "The following sections describe them in detail.", "labels": [], "entities": []}, {"text": "Our submission to the CoNLL 2014 shared task is the result of our best hybrid system, described in the previous section and summarised in.", "labels": [], "entities": [{"text": "CoNLL 2014 shared task", "start_pos": 22, "end_pos": 44, "type": "DATASET", "confidence": 0.854905292391777}]}, {"text": "The official test set comprised 50 new essays tokens in 1,312 sentences) written in response to two prompts, one of which was also included in the training data.", "labels": [], "entities": []}, {"text": "Systems were evaluated using the M 2 Scorer, which uses F 0.5 as its overall measure.", "labels": [], "entities": [{"text": "M 2 Scorer", "start_pos": 33, "end_pos": 43, "type": "METRIC", "confidence": 0.49235374728838605}, {"text": "F 0.5", "start_pos": 56, "end_pos": 61, "type": "METRIC", "confidence": 0.9781964421272278}]}, {"text": "As in previous years, there were two evaluation rounds.", "labels": [], "entities": []}, {"text": "The first one was based on the original gold-standard annotations provided by the shared-task organisers whereas the second was based on a revised version including alternative annotations submitted by the participating teams.", "labels": [], "entities": []}, {"text": "Our submitted system achieved the first and second place respectively.", "labels": [], "entities": []}, {"text": "The official results of our submission in both evaluation rounds are reported in.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Performance of language models on the  development set after ranking the SMT system's  10-best candidates per sentence. CE: correct ed- its, ME: missed edits, UE: unnecessary edits, P:  precision, R: recall.", "labels": [], "entities": [{"text": "SMT", "start_pos": 83, "end_pos": 86, "type": "TASK", "confidence": 0.9802626967430115}, {"text": "CE: correct ed- its", "start_pos": 130, "end_pos": 149, "type": "METRIC", "confidence": 0.8166985710461935}, {"text": "UE", "start_pos": 169, "end_pos": 171, "type": "METRIC", "confidence": 0.9503784775733948}, {"text": "precision", "start_pos": 196, "end_pos": 205, "type": "METRIC", "confidence": 0.9364177584648132}, {"text": "recall", "start_pos": 210, "end_pos": 216, "type": "METRIC", "confidence": 0.8977545499801636}]}, {"text": " Table 2: Results of individual systems on the de- velopment set.", "labels": [], "entities": []}, {"text": " Table 3: Results for different system pipelines on the development set.", "labels": [], "entities": []}, {"text": " Table 4: Results for individual systems on the development set.", "labels": [], "entities": []}, {"text": " Table 5: Type-specific performance of our best hy- brid system on the development set. Types with  zero precision are marked in bold.", "labels": [], "entities": [{"text": "precision", "start_pos": 105, "end_pos": 114, "type": "METRIC", "confidence": 0.987676739692688}]}, {"text": " Table 6: Official results of our system on the orig- inal and revised test sets.", "labels": [], "entities": []}, {"text": " Table 7: Type-specific performance of our submit- ted system on the original test set.", "labels": [], "entities": []}]}