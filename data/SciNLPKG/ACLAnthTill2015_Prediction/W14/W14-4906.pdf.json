{"title": [{"text": "Finding your \"inner-annotator\": An experiment in annotator independence for rating discourse coherence quality in essays", "labels": [], "entities": []}], "abstractContent": [{"text": "An experimental annotation method is described, showing promise fora subjective labeling task-discourse coherence quality of essays.", "labels": [], "entities": []}, {"text": "Annotators developed personal protocols, reducing front-end resources: protocol development and annotator training.", "labels": [], "entities": [{"text": "protocol development", "start_pos": 71, "end_pos": 91, "type": "TASK", "confidence": 0.7678562998771667}]}, {"text": "Substantial inter-annotator agreement was achieved fora 4-point scale.", "labels": [], "entities": []}, {"text": "Correlational analyses revealed how unique linguistic phenomena were considered in annotation.", "labels": [], "entities": []}, {"text": "Systems trained with the annotator data demonstrated utility of the data.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "Annotation scoring protocols from 2 annotators for coherence quality are evaluated and described.", "labels": [], "entities": []}, {"text": "A one-week pilot study was conducted.", "labels": [], "entities": []}, {"text": "To provide some initial grounding, annotators received a 1-page task description that offered a high-level explanation of \"coherence\" describing the end-points of a potential protocol.", "labels": [], "entities": []}, {"text": "(This description was written in about an hour.)", "labels": [], "entities": []}, {"text": "It indicated that high coherence is associated with an essay that can be easily understood, and low coherence is associated with an incomprehensible essay.", "labels": [], "entities": []}, {"text": "Each annotator developed her own protocol: for each score point she wrote descriptive text illustrating a set of defining characteristics for each score point of coherence quality (e.g., \"The writer's point is difficult to understand.\").", "labels": [], "entities": []}, {"text": "Annotator 1 (A1) developed a 4-point scale; Feature Type  To evaluate the utility of the annotated data, two evaluations were conducted: one built classifiers with all system features (Sys_All), and a second with the GUM features (Sys_GUM).", "labels": [], "entities": []}, {"text": "Using 10-fold crossvalidation with a gradient boosting regression learner, four classifiers were trained to predict coherence quality ratings on a 4-point scale, using the respective annotator data sets: A1 and A2 Sys_All, and A1 and A2 Sys_GUM systems.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. Six item types & populations in the experimental annotation task. NNES-Univ = non-native  English speakers, university applicants", "labels": [], "entities": [{"text": "NNES-Univ", "start_pos": 76, "end_pos": 85, "type": "DATASET", "confidence": 0.9104192852973938}]}, {"text": " Table 2. Pearson r between annotator discourse coherence scores and features. All correlations are  significant at p < .0001, except for A2's long-distance sentence-pair similarity at p < .05.", "labels": [], "entities": [{"text": "Pearson r", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9517404735088348}, {"text": "A2's long-distance sentence-pair similarity", "start_pos": 138, "end_pos": 181, "type": "METRIC", "confidence": 0.7984119176864624}]}]}