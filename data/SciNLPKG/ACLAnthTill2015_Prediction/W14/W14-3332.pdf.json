{"title": [{"text": "Experiments in Medical Translation Shared Task at WMT 2014", "labels": [], "entities": [{"text": "Medical Translation Shared Task", "start_pos": 15, "end_pos": 46, "type": "TASK", "confidence": 0.8079326003789902}, {"text": "WMT 2014", "start_pos": 50, "end_pos": 58, "type": "DATASET", "confidence": 0.7989122271537781}]}], "abstractContent": [{"text": "This paper describes Dublin City Uni-versity's (DCU) submission to the WMT 2014 Medical Summary task.", "labels": [], "entities": [{"text": "Dublin City Uni-versity", "start_pos": 21, "end_pos": 44, "type": "DATASET", "confidence": 0.9457718729972839}, {"text": "WMT 2014 Medical Summary task", "start_pos": 71, "end_pos": 100, "type": "TASK", "confidence": 0.7008914828300477}]}, {"text": "We report our results on the test data set in the French to English translation direction.", "labels": [], "entities": []}, {"text": "We also report statistics collected from the corpora used to train our translation system.", "labels": [], "entities": []}, {"text": "We conducted our experiment on the Moses 1.0 phrase-based translation system framework.", "labels": [], "entities": [{"text": "phrase-based translation system", "start_pos": 45, "end_pos": 76, "type": "TASK", "confidence": 0.6323218842347463}]}, {"text": "We performed a variety of experiments on translation models, reordering models, operation sequence model and language model.", "labels": [], "entities": [{"text": "translation models", "start_pos": 41, "end_pos": 59, "type": "TASK", "confidence": 0.9237663745880127}]}, {"text": "We also experimented with data selection and removal the length constraint for phrase-pair extraction.", "labels": [], "entities": [{"text": "phrase-pair extraction", "start_pos": 79, "end_pos": 101, "type": "TASK", "confidence": 0.7813965678215027}]}], "introductionContent": [], "datasetContent": [], "tableCaptions": [{"text": " Table 1: WMT 2014 Medical Translation shared  task parallel training data before preprocessing.", "labels": [], "entities": [{"text": "WMT 2014 Medical Translation", "start_pos": 10, "end_pos": 38, "type": "TASK", "confidence": 0.7242885380983353}]}, {"text": " Table 2: WMT 2014 Medical Translation shared  task parallel training data after preprocessing  steps.", "labels": [], "entities": [{"text": "WMT 2014 Medical Translation", "start_pos": 10, "end_pos": 38, "type": "TASK", "confidence": 0.7255592197179794}]}, {"text": " Table 3: Feature decay algorithm transductive  learning selection on all in-domain data using ex- tracted features from the source side of the test  data set. We choose system uses 1/8 proportions  of the in-domain data as our baseline system.", "labels": [], "entities": []}, {"text": " Table 4: -max-phrase-length setting experiment,  where phrase table entries is in millions. * indi- actes statistically significant improvement at the p  = 0.05 level. 1", "labels": [], "entities": []}, {"text": " Table 5: Reordering Model or/and OSM results", "labels": [], "entities": [{"text": "OSM", "start_pos": 34, "end_pos": 37, "type": "TASK", "confidence": 0.7038140892982483}]}, {"text": " Table 7: Language model experiment results", "labels": [], "entities": []}]}