{"title": [{"text": "RACAI GEC -A hybrid approach to Grammatical Error Correction", "labels": [], "entities": [{"text": "RACAI GEC", "start_pos": 0, "end_pos": 9, "type": "TASK", "confidence": 0.9172195494174957}, {"text": "Grammatical Error Correction", "start_pos": 32, "end_pos": 60, "type": "TASK", "confidence": 0.9220318794250488}]}], "abstractContent": [{"text": "This paper describes RACAI's (Research Institute for Artificial Intelligence) hybrid grammatical error correction system.", "labels": [], "entities": [{"text": "RACAI's (Research Institute for Artificial Intelligence) hybrid grammatical error correction", "start_pos": 21, "end_pos": 113, "type": "TASK", "confidence": 0.5096442653582647}]}, {"text": "This system was validated during the participation into the CONLL'14 Shared Task on Grammatical Error Correction.", "labels": [], "entities": [{"text": "CONLL'14 Shared Task on Grammatical Error Correction", "start_pos": 60, "end_pos": 112, "type": "TASK", "confidence": 0.6481928655079433}]}, {"text": "We offer an analysis of the types of errors detected and corrected by our system , we present the necessary steps to reproduce our experiment and also the results we obtained.", "labels": [], "entities": []}], "introductionContent": [{"text": "Grammatical error correction (GEC) is a complex task mainly because of the natural dependencies between the words of a sentence both at the lexical and the semantic levels, leave it aside the morphologic and syntactic levels, an intrinsic and complex attribute specific to the human language.", "labels": [], "entities": [{"text": "Grammatical error correction (GEC)", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.8448654413223267}]}, {"text": "Grammatical error detection and correction received a significant level of interest from various research groups both from the academic and commercial environments.", "labels": [], "entities": [{"text": "Grammatical error detection and correction", "start_pos": 0, "end_pos": 42, "type": "TASK", "confidence": 0.8285347580909729}]}, {"text": "A testament to the importance of this task is the long history of challenges (e.g. Microsoft Speller Challenge and CONLL Shared Task) () that had the primary objective of proving a common testing ground (i.e. resources, tools and gold standards) in order to assess the performance of various methods and tools for GEC, when applied to identical input data.", "labels": [], "entities": [{"text": "GEC", "start_pos": 314, "end_pos": 317, "type": "TASK", "confidence": 0.6165337562561035}]}, {"text": "In the task of GEC, one can easily distinguish two separate tasks: grammatical error detection and grammatical error correction.", "labels": [], "entities": [{"text": "GEC", "start_pos": 15, "end_pos": 18, "type": "TASK", "confidence": 0.8818614482879639}, {"text": "grammatical error detection", "start_pos": 67, "end_pos": 94, "type": "TASK", "confidence": 0.6155422826608022}, {"text": "grammatical error correction", "start_pos": 99, "end_pos": 127, "type": "TASK", "confidence": 0.6147301296393076}]}, {"text": "Typically, there are three types of approaches: statistical, rule-based and hybrid.", "labels": [], "entities": []}, {"text": "The difficulty of detecting and correcting an error depends on its class.", "labels": [], "entities": [{"text": "detecting and correcting", "start_pos": 18, "end_pos": 42, "type": "TASK", "confidence": 0.729476273059845}]}, {"text": "(a) Statistical approaches rely on building statistical models (using surface forms or syntactic labels) that are used for detecting and correcting local errors.", "labels": [], "entities": []}, {"text": "The typical statistical approach is to model how likely the occurrence of an event is, given a history of preceding events.", "labels": [], "entities": []}, {"text": "Thus, statistical approaches easily adaptable to any language (requiring only training data in the form of raw or syntactically labeled text) are very good guessers when it comes to detecting and correcting collocations, idioms, typos and small grammatical inadvertences such as the local gender and case agreements.", "labels": [], "entities": [{"text": "detecting and correcting collocations, idioms, typos and small grammatical inadvertences such as the local gender and case agreements", "start_pos": 182, "end_pos": 315, "type": "Description", "confidence": 0.7988658308982849}]}, {"text": "The main impediments of such systems are two-fold: (1) they are resource consuming techniques (memory/storage) and they are highly dependent on data -large and domain adapted datasets are required in order to avoid the data-scarceness specific issue and currently they rely only on a limited horizon of events; (2) they usually lack semantic information and favoring highoccurring events is not always the best way of detecting and correcting grammatical errors.", "labels": [], "entities": [{"text": "detecting and correcting grammatical errors", "start_pos": 418, "end_pos": 461, "type": "TASK", "confidence": 0.7212124109268189}]}, {"text": "(b) Rule-based approaches embed linguistic knowledge in the form of machine parsable rules that are used to detect errors and describe via transformations how various error types should be corrected.", "labels": [], "entities": []}, {"text": "The drawbacks to rule-based system are (1) the extensive effort required to build the ruleset, (2) regardless of the size of the ruleset, given the variability of the human language it is virtually impossible to capture all possible errors and (3) the large number of exceptions to rules.", "labels": [], "entities": []}, {"text": "(c) Hybrid systems that combine both rulebased and statistical approaches are plausible to overcome the weaknesses of the two methodologies if the mixture of the two components is done properly.", "labels": [], "entities": []}, {"text": "Detection of errors can be achieved statistically and rule-based, the task of the hybrid approach being to resolve any conflicts that arise between the outputs of the two approaches.", "labels": [], "entities": [{"text": "Detection of errors", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8569437861442566}]}, {"text": "However, even the most advanced systems are only able to distinguish between a limited number of error types and the task of correcting an error is even more difficult.", "labels": [], "entities": []}, {"text": "Along the typical set of errors that are handled by typical correction systems (punctuation, capitalization, spelling, typos, verb tense, missing verb, etc.), CONLL's GEC task introduces some hard cases which require a level of semantic analysis: local redundancy, unclear meaning, parallelism, etc.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1. The most frequent types of mistakes  in the training data  There are also some less frequent errors: pro- noun form, noun possessive form, word order of  adjectives and adverbs, etc. Some of these can be  solved by means of rules, others by accessing  lexical resources and others are extremely diffi- cult to deal with.  As far as the test data are concerned, the error  distribution according to their types is the fol- lowing:  Type of error  Occurrences  in official- 2014.0.m2", "labels": [], "entities": [{"text": "Occurrences", "start_pos": 456, "end_pos": 467, "type": "METRIC", "confidence": 0.5992763638496399}]}, {"text": " Table 2. The most frequent types of mistakes  in the test data  Roughly, the same types of mistakes are more  frequent in the test set, just like as in the training  set.", "labels": [], "entities": []}]}