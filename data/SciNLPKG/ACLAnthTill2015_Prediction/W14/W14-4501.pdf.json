{"title": [{"text": "Application-Driven Relation Extraction with Limited Distant Supervision", "labels": [], "entities": [{"text": "Application-Driven Relation Extraction", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.7438847819964091}]}], "abstractContent": [{"text": "Recent approaches to relation extraction following the distant supervision paradigm have focused on exploiting large knowledge bases, from which they extract substantial amount of supervision.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 21, "end_pos": 40, "type": "TASK", "confidence": 0.9476715624332428}]}, {"text": "However, for many relations in real-world applications, there are few instances available to seed the relation extraction process, and appropriate named entity recognizers which are necessary for pre-processing do not exist.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 102, "end_pos": 121, "type": "TASK", "confidence": 0.7522871196269989}]}, {"text": "To overcome this issue, we learn entity filters jointly with relation extraction using imitation learning.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 61, "end_pos": 80, "type": "TASK", "confidence": 0.8869564533233643}]}, {"text": "We evaluate our approach on architect names and building completion years, using only around 30 seed instances for each relation and show that the jointly learned entity filters improved the performance by 30 and 7 points in average precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 233, "end_pos": 242, "type": "METRIC", "confidence": 0.993498682975769}]}], "introductionContent": [{"text": "In this paper we focus on relation extraction in the context of a real-world application.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 26, "end_pos": 45, "type": "TASK", "confidence": 0.9485379159450531}]}, {"text": "The application is a dialog-based city tour guide, based in Edinburgh.", "labels": [], "entities": [{"text": "Edinburgh", "start_pos": 60, "end_pos": 69, "type": "DATASET", "confidence": 0.9514633417129517}]}, {"text": "One of the features of the system is its pro-active nature, offering information which maybe of interest to the user.", "labels": [], "entities": []}, {"text": "In order to be pro-active in this way, as well as answer users' questions, the system requires a large amount of knowledge about the city.", "labels": [], "entities": []}, {"text": "Part of that knowledge is stored in a database, which is time-consuming and difficult to populate manually.", "labels": [], "entities": []}, {"text": "Hence, we have explored the use of an automatic knowledge base population technique based on distant supervision.", "labels": [], "entities": []}, {"text": "The attraction of this approach is that the only input required is a list of seed instances of the relation in question and a corpus of sentences expressing new instances of that relation.", "labels": [], "entities": []}, {"text": "However, existing studies typically assume a large seed set, whereas in our application such sets are often not readily available, e.g. reported using 7K-140K seed instances per relation as input.", "labels": [], "entities": []}, {"text": "In this paper, the two relations that we evaluate on are architect name and completion year of buildings.", "labels": [], "entities": []}, {"text": "These were chosen because they are highly relevant to our application, but also somewhat non-standard compared to the existing literature; and crucially they do not come with a readily-available set of seed instances.", "labels": [], "entities": []}, {"text": "Furthermore, previous approaches typically assume named entity recognition (NER) as a preprocessing step in order to construct the training and testing instances.", "labels": [], "entities": [{"text": "named entity recognition (NER)", "start_pos": 50, "end_pos": 80, "type": "TASK", "confidence": 0.7966877818107605}]}, {"text": "However, since these tools are not tailored to the relations of interest, they introduce spurious entity matches that are harmful to performance as shown by and.", "labels": [], "entities": []}, {"text": "These authors ameliorated this issue by learning fine-grained entity recognizers and filters using supervised learning.", "labels": [], "entities": []}, {"text": "The labeled data used was extracted from the anchor text of entity mentions annotated in Wikipedia, however this is not possible for entities not annotated in this resource.", "labels": [], "entities": []}, {"text": "In this work, instead of relying on labeled data to construct entity filters, we learn them jointly with the relation extraction component.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 109, "end_pos": 128, "type": "TASK", "confidence": 0.737311065196991}]}, {"text": "For this purpose we use the imitation learning algorithm DAGGER), which can handle the dependencies between actions taken in a sequence, and use supervision for later actions to learn how to take actions earlier in the sequence.", "labels": [], "entities": [{"text": "DAGGER", "start_pos": 57, "end_pos": 63, "type": "METRIC", "confidence": 0.8344075679779053}]}, {"text": "We evaluate our approach using around 30 seed instances per relation and show that the jointly learned entity filters result in gains of 7 and 30 points in average precision for the completion year and the architect name relations respectively.", "labels": [], "entities": [{"text": "precision", "start_pos": 164, "end_pos": 173, "type": "METRIC", "confidence": 0.9767704606056213}]}], "datasetContent": [{"text": "The relations used for evaluation are building-architect and building-completion year, for the reasons given in Sec.", "labels": [], "entities": []}, {"text": "1. For each of the 138 listed historical buildings in Wikipedia, we found the correct answers, resulting in 60 building-completion year and 68 building-architect pairs.", "labels": [], "entities": []}, {"text": "We split the data into two equal parts for training/development and testing.", "labels": [], "entities": []}, {"text": "We then collected relevant web pages querying the web as described in Sec.", "labels": [], "entities": []}, {"text": "2. The queries were submitted to Bing via its Search API and the top 300 results for each query were obtained.", "labels": [], "entities": []}, {"text": "We downloaded the corresponding pages and extracted their textual content with BoilerPipe (.", "labels": [], "entities": [{"text": "BoilerPipe", "start_pos": 79, "end_pos": 89, "type": "DATASET", "confidence": 0.8620961904525757}]}, {"text": "We then processed the texts using the Stanford CoreNLP toolkit.", "labels": [], "entities": [{"text": "Stanford CoreNLP toolkit", "start_pos": 38, "end_pos": 62, "type": "DATASET", "confidence": 0.9478348890940348}]}, {"text": "We tried to match the question entity with tokens in each of the sentences, allowing for minor differences in tokenization, whitespace and capitalization.", "labels": [], "entities": []}, {"text": "If a sentence contained the question entity and a candidate answer, we parsed it using the parser.", "labels": [], "entities": []}, {"text": "The instances generated were labeled using the distant supervision assumption, resulting in 974K and 4.5M labeled instances for the completion year and the architect relation, respectively.", "labels": [], "entities": []}, {"text": "We ran experiments with three systems; the jointly learned entity filtering-relation extraction approach using imitation learning (henceforth 2stage), the one-stage classification approach using the features for both entity filtering and relation extraction (henceforth 1stage), and a baseline that for each question entity returns all candidate answers for the relation ranked by the number of times they appeared with the question entity and ignoring all other information (henceforth Base).", "labels": [], "entities": [{"text": "entity filtering-relation extraction", "start_pos": 59, "end_pos": 95, "type": "TASK", "confidence": 0.7749687830607096}, {"text": "relation extraction", "start_pos": 238, "end_pos": 257, "type": "TASK", "confidence": 0.7291368693113327}, {"text": "Base", "start_pos": 487, "end_pos": 491, "type": "METRIC", "confidence": 0.9666446447372437}]}, {"text": "Following four-fold crossvalidations experiment on the development data, we used 12 iterations for learning with DAGGER.", "labels": [], "entities": [{"text": "DAGGER", "start_pos": 113, "end_pos": 119, "type": "DATASET", "confidence": 0.6063898801803589}]}, {"text": "Each system returns a list of answers ranked according to the number of instances classified as positive for that answer.", "labels": [], "entities": []}, {"text": "We used two evaluation modes.", "labels": [], "entities": []}, {"text": "The first considers only the top-ranked answer (top), whereas the second considers all answers returned until either the correct one is found or they are exhausted (all).", "labels": [], "entities": []}, {"text": "In all we define recall as the number of correct answers over the total number of question entities, and precision as the chance of finding the correct answer while traversing those returned.", "labels": [], "entities": [{"text": "recall", "start_pos": 17, "end_pos": 23, "type": "METRIC", "confidence": 0.9991681575775146}, {"text": "precision", "start_pos": 105, "end_pos": 114, "type": "METRIC", "confidence": 0.9994938373565674}]}, {"text": "Results by all models are reported for both relations in.", "labels": [], "entities": []}, {"text": "A first observation is that the architect name relation is substantially harder to extract since all models achieve worse scores than for the completion year relation.", "labels": [], "entities": []}, {"text": "More specifically, Base achieves respectable scores in top mode in completion year extraction, but it fails completely in architect name.", "labels": [], "entities": [{"text": "completion year extraction", "start_pos": 67, "end_pos": 93, "type": "TASK", "confidence": 0.5499143203099569}]}, {"text": "This is due to the existence of many other names: Test set precision-recall curves in all mode for year completed (left) and architect name (right). that appear more frequently together with a building than that of its architect, while the completion year is sometimes the number most frequently mentioned in the same sentence with the building.", "labels": [], "entities": [{"text": "precision-recall", "start_pos": 59, "end_pos": 75, "type": "METRIC", "confidence": 0.8253538012504578}]}, {"text": "In addition, Base achieves the maximum possible all recall by construction, since if there is a sentence containing the correct answer fora question entity it will be returned.", "labels": [], "entities": [{"text": "recall", "start_pos": 52, "end_pos": 58, "type": "METRIC", "confidence": 0.8832700252532959}]}, {"text": "However this comes at a cost of low precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 36, "end_pos": 45, "type": "METRIC", "confidence": 0.9992153644561768}]}, {"text": "Both the machine-learned models improve upon Base substantially on both datasets, with the 2stage model being substantially better in architect name extraction, especially in terms of precision.", "labels": [], "entities": [{"text": "Base", "start_pos": 45, "end_pos": 49, "type": "METRIC", "confidence": 0.9598137736320496}, {"text": "architect name extraction", "start_pos": 134, "end_pos": 159, "type": "TASK", "confidence": 0.6590338349342346}, {"text": "precision", "start_pos": 184, "end_pos": 193, "type": "METRIC", "confidence": 0.997368335723877}]}, {"text": "In completion year extraction the differences are smaller, with 1stage being slightly better.", "labels": [], "entities": [{"text": "completion year extraction", "start_pos": 3, "end_pos": 29, "type": "TASK", "confidence": 0.6160977184772491}, {"text": "1stage", "start_pos": 64, "end_pos": 70, "type": "METRIC", "confidence": 0.960461437702179}]}, {"text": "These small differences are expected since recognizing completion years is much easier than recognizing architect names, thus learning a separate entity filtering model for them is less likely to be useful.", "labels": [], "entities": []}, {"text": "Nevertheless, inspecting the weights learned by the 2stage model showed that some useful distinctions were learned, e.g. being preceded by the word \"between\" as in \"built between 1849 and 1852\" renders a numberless likely to be a completion year.", "labels": [], "entities": []}, {"text": "Finally, we examined the quality of the learned models further by generating precisionrecall curves for the all mode by adjusting the classification thresholds used by 1stage and 2stage.", "labels": [], "entities": [{"text": "precisionrecall", "start_pos": 77, "end_pos": 92, "type": "METRIC", "confidence": 0.9994102716445923}]}, {"text": "As shown in the plots of Table 2, 2stage achieves higher precision than 1stage at most recall levels for both relations, with the benefits being more pronounced in the architect name relation.", "labels": [], "entities": [{"text": "2stage", "start_pos": 34, "end_pos": 40, "type": "METRIC", "confidence": 0.9060782194137573}, {"text": "precision", "start_pos": 57, "end_pos": 66, "type": "METRIC", "confidence": 0.9990721940994263}, {"text": "recall", "start_pos": 87, "end_pos": 93, "type": "METRIC", "confidence": 0.9956711530685425}]}, {"text": "Summarizing these curves using average precision (, the scores were 0.69 and 0.76 for the completion year, and 0.21 and 0.51 for the architect, for the 1stage and the 2stage models respectively, thus confirming the usefulness of separating the entity filtering features from relation extraction.", "labels": [], "entities": [{"text": "precision", "start_pos": 39, "end_pos": 48, "type": "METRIC", "confidence": 0.9004027843475342}, {"text": "relation extraction", "start_pos": 275, "end_pos": 294, "type": "TASK", "confidence": 0.7636661231517792}]}], "tableCaptions": [{"text": " Table 1: Test set results for the 3 systems on year completed (top) and architect name (bottom).", "labels": [], "entities": []}, {"text": " Table 2: Test set precision-recall curves in all mode for year completed (left) and architect name (right).", "labels": [], "entities": [{"text": "precision-recall", "start_pos": 19, "end_pos": 35, "type": "METRIC", "confidence": 0.9960856437683105}]}]}