{"title": [{"text": "Application of Prize based on Sentence Length in Chunk-based Automatic Evaluation of Machine Translation", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 85, "end_pos": 104, "type": "TASK", "confidence": 0.7149907201528549}]}], "abstractContent": [{"text": "As described in this paper, we propose anew automatic evaluation metric for machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 76, "end_pos": 95, "type": "TASK", "confidence": 0.811632364988327}]}, {"text": "Our metric is based on chunking between the reference and candidate translation.", "labels": [], "entities": []}, {"text": "Moreover, we apply a prize based on sentence-length to the metric, dissimilar from penalties in BLEU or NIST.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 96, "end_pos": 100, "type": "METRIC", "confidence": 0.9620290994644165}, {"text": "NIST", "start_pos": 104, "end_pos": 108, "type": "DATASET", "confidence": 0.9554855227470398}]}, {"text": "We designate this metric as Automatic Evaluation of Machine Translation in which the Prize is Applied to a Chunk-based metric (APAC).", "labels": [], "entities": [{"text": "Automatic Evaluation of Machine Translation", "start_pos": 28, "end_pos": 71, "type": "TASK", "confidence": 0.4941648840904236}]}, {"text": "Through meta-evaluation experiments and comparison with several metrics, we confirmed that our metric shows stable correlation with human judgment.", "labels": [], "entities": []}], "introductionContent": [{"text": "In the field of machine translation, various automatic evaluation metrics have been proposed.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 16, "end_pos": 35, "type": "TASK", "confidence": 0.8042388260364532}]}, {"text": "Among them, chunk-based metrics such as METEOR(A., ROUGE-L(), and IMPACT(H. Echizen-ya and K. are effective.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 40, "end_pos": 46, "type": "METRIC", "confidence": 0.9843898415565491}, {"text": "ROUGE-L", "start_pos": 51, "end_pos": 58, "type": "METRIC", "confidence": 0.9950290322303772}, {"text": "IMPACT", "start_pos": 66, "end_pos": 72, "type": "METRIC", "confidence": 0.9240955114364624}]}, {"text": "In general, BLEU(K.), NIST), and RIBES(H.) use a penalty for calculation of scores because the high score is often given extremely when the candidate translation is short.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 12, "end_pos": 16, "type": "METRIC", "confidence": 0.9973329305648804}, {"text": "NIST", "start_pos": 22, "end_pos": 26, "type": "DATASET", "confidence": 0.9286150932312012}, {"text": "RIBES", "start_pos": 33, "end_pos": 38, "type": "METRIC", "confidence": 0.9488344192504883}]}, {"text": "Therefore, the penalty is effective to obtain high correlation with human judgment.", "labels": [], "entities": []}, {"text": "On the other hand, almost all chunkbased metrics use the F -measure based on a precision by candidate translation and a recall by reference.", "labels": [], "entities": [{"text": "F -measure", "start_pos": 57, "end_pos": 67, "type": "METRIC", "confidence": 0.9924246470133463}, {"text": "precision", "start_pos": 79, "end_pos": 88, "type": "METRIC", "confidence": 0.9920147657394409}, {"text": "recall", "start_pos": 120, "end_pos": 126, "type": "METRIC", "confidence": 0.9939233660697937}]}, {"text": "Moreover, they assign a penalty for the difference of chunk order between the candidate translation and the reference, not the penalty for the difference of sentence length.", "labels": [], "entities": []}, {"text": "Nevertheless, it is also important for chunk-based metrics to examine the sentence length.", "labels": [], "entities": []}, {"text": "In chunk-based metrics, each word's weight depends on the sentence length.", "labels": [], "entities": []}, {"text": "For example, the weight of each word is 0.2 (=1/5) when the number of words in a sentence is 5; it is 0.1 (=1/10) when the number of words in a sentence is 10.", "labels": [], "entities": []}, {"text": "Therefore, the weight of the non-matched word in the short sentence is large.", "labels": [], "entities": []}, {"text": "To resolve this problem, it is effective for short sentences to give a prize based on the sentence length in the chunk-based metrics.", "labels": [], "entities": []}, {"text": "Therefore, we propose anew metric using a prize based on the sentence length.", "labels": [], "entities": []}, {"text": "We designate this metric as Automatic Evaluation of Machine Translation in which the Prize is Applied to a Chunk-based metric (APAC).", "labels": [], "entities": [{"text": "Automatic Evaluation of Machine Translation", "start_pos": 28, "end_pos": 71, "type": "TASK", "confidence": 0.4941648840904236}]}, {"text": "In our metric, the weight of a non-matched word becomes small for the short sentence by awarding of the prize.", "labels": [], "entities": []}, {"text": "It is almost identical to that fora long sentence by awarding of the prize.", "labels": [], "entities": []}, {"text": "Therefore, our metric does not depend heavily on sentence length because the weight of nonmatched words is constantly small.", "labels": [], "entities": []}, {"text": "We confirmed the effectiveness of APAC using metaevaluation experiments.", "labels": [], "entities": [{"text": "APAC", "start_pos": 34, "end_pos": 38, "type": "TASK", "confidence": 0.6748599410057068}]}], "datasetContent": [{"text": "In APAC, 0.1 and 1.2 were used as the values of parameters \u03b1 and \u03b2 by the preliminarily experimentally obtained results.", "labels": [], "entities": [{"text": "APAC", "start_pos": 3, "end_pos": 7, "type": "DATASET", "confidence": 0.5316457748413086}]}, {"text": "In, \"Rank\" denotes the ranking based on \"Avg.\"", "labels": [], "entities": [{"text": "Avg", "start_pos": 41, "end_pos": 44, "type": "METRIC", "confidence": 0.9909955859184265}]}, {"text": "The value of \"()\" denotes the number of MT systems in, 5, and 7.", "labels": [], "entities": [{"text": "MT", "start_pos": 40, "end_pos": 42, "type": "TASK", "confidence": 0.9683364033699036}]}, {"text": "The value of \"()\" represents the number of sentence pairs in, 6, and 8.", "labels": [], "entities": []}, {"text": "These values depend on the data.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Spearman's rank correlation coefficient of system-level in WMT2012 data.", "labels": [], "entities": [{"text": "Spearman's rank correlation coefficient", "start_pos": 10, "end_pos": 49, "type": "METRIC", "confidence": 0.6504188179969788}, {"text": "WMT2012 data", "start_pos": 69, "end_pos": 81, "type": "DATASET", "confidence": 0.9597823917865753}]}, {"text": " Table 2: Kendall tau rank correlation coefficient of the segment level in WMT2012 data.", "labels": [], "entities": [{"text": "Kendall tau rank correlation coefficient", "start_pos": 10, "end_pos": 50, "type": "METRIC", "confidence": 0.7067060172557831}, {"text": "WMT2012 data", "start_pos": 75, "end_pos": 87, "type": "DATASET", "confidence": 0.961272269487381}]}, {"text": " Table 3: Spearman's rank correlation coefficient of the system level in WMT2013 data.", "labels": [], "entities": [{"text": "Spearman's rank correlation coefficient", "start_pos": 10, "end_pos": 49, "type": "METRIC", "confidence": 0.6613454282283783}, {"text": "WMT2013 data", "start_pos": 73, "end_pos": 85, "type": "DATASET", "confidence": 0.9626017212867737}]}, {"text": " Table 4: Kendall tau rank correlation coefficient of the segment level in WMT2013 data.", "labels": [], "entities": [{"text": "Kendall tau rank correlation coefficient", "start_pos": 10, "end_pos": 50, "type": "METRIC", "confidence": 0.7136293172836303}, {"text": "WMT2013 data", "start_pos": 75, "end_pos": 87, "type": "DATASET", "confidence": 0.961199164390564}]}, {"text": " Table 5: Spearman's rank correlation coefficient of the system level in NTCIR-7 data.", "labels": [], "entities": [{"text": "Spearman's rank correlation coefficient", "start_pos": 10, "end_pos": 49, "type": "METRIC", "confidence": 0.6633823931217193}, {"text": "NTCIR-7 data", "start_pos": 73, "end_pos": 85, "type": "DATASET", "confidence": 0.9559073746204376}]}, {"text": " Table 6: Kendall tau rank correlation coefficient of the segment level in NTCIR-7 data.", "labels": [], "entities": [{"text": "Kendall tau rank correlation coefficient", "start_pos": 10, "end_pos": 50, "type": "METRIC", "confidence": 0.7164940416812897}, {"text": "NTCIR-7 data", "start_pos": 75, "end_pos": 87, "type": "DATASET", "confidence": 0.9660001993179321}]}, {"text": " Table 7: Spearman's rank correlation coefficient of the system level in NTCIR-9 data.", "labels": [], "entities": [{"text": "Spearman's rank correlation coefficient", "start_pos": 10, "end_pos": 49, "type": "METRIC", "confidence": 0.6532525658607483}, {"text": "NTCIR-9 data", "start_pos": 73, "end_pos": 85, "type": "DATASET", "confidence": 0.9600951075553894}]}, {"text": " Table 8: Kendall tau rank correlation coefficient of segment-level in NTCIR-9 data.", "labels": [], "entities": [{"text": "Kendall tau rank correlation coefficient", "start_pos": 10, "end_pos": 50, "type": "METRIC", "confidence": 0.7223669409751892}, {"text": "NTCIR-9 data", "start_pos": 71, "end_pos": 83, "type": "DATASET", "confidence": 0.9709140658378601}]}]}