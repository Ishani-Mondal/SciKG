{"title": [{"text": "Semantic Parsing using Distributional Semantics and Probabilistic Logic", "labels": [], "entities": [{"text": "Semantic Parsing", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.9116575419902802}]}], "abstractContent": [{"text": "We propose anew approach to semantic parsing that is not constrained by a fixed formal ontology and purely logical inference.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 28, "end_pos": 44, "type": "TASK", "confidence": 0.7916541397571564}]}, {"text": "Instead, we use distributional semantics to generate only the relevant part of an on-the-fly ontology.", "labels": [], "entities": []}, {"text": "Sentences and the on-the-fly ontology are represented in probabilistic logic.", "labels": [], "entities": []}, {"text": "For inference, we use probabilistic logic frameworks like Markov Logic Networks (MLN) and Prob-abilistic Soft Logic (PSL).", "labels": [], "entities": []}, {"text": "This semantic parsing approach is evaluated on two tasks, Textual Entitlement (RTE) and Tex-tual Similarity (STS), both accomplished using inference in probabilistic logic.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 5, "end_pos": 21, "type": "TASK", "confidence": 0.725464716553688}, {"text": "Tex-tual Similarity (STS)", "start_pos": 88, "end_pos": 113, "type": "TASK", "confidence": 0.6896286904811859}]}, {"text": "Experiments show the potential of the approach .", "labels": [], "entities": []}], "introductionContent": [{"text": "Semantic Parsing is probably best defined as the task of representing the meaning of a natural language sentence in some formal knowledge representation language that supports automated inference.", "labels": [], "entities": [{"text": "Semantic Parsing", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8083164393901825}, {"text": "representing the meaning of a natural language sentence", "start_pos": 57, "end_pos": 112, "type": "TASK", "confidence": 0.7135452628135681}]}, {"text": "A semantic parser is best defined as having three parts, a formal language, an ontology, and an inference mechanism.", "labels": [], "entities": []}, {"text": "Both the formal language (e.g. first-order logic) and the ontology define the formal knowledge representation.", "labels": [], "entities": []}, {"text": "The formal language uses predicate symbols from the ontology, and the ontology provides them with meanings by defining the relations between them.", "labels": [], "entities": []}, {"text": "A formal expression by itself without an ontology is insufficient for semantic interpretation; we call it uninterpreted logical form.", "labels": [], "entities": [{"text": "semantic interpretation", "start_pos": 70, "end_pos": 93, "type": "TASK", "confidence": 0.7964678406715393}]}, {"text": "An uninterpreted logical form is not enough as a knowledge representation because the predicate symbols do not have meaning in themselves, they get this meaning from the ontology.", "labels": [], "entities": []}, {"text": "Inference is what takes a problem represented in the formal knowledge representation and the ontology and performs the target task (e.g. textual entailment, question answering, etc.).", "labels": [], "entities": [{"text": "textual entailment", "start_pos": 137, "end_pos": 155, "type": "TASK", "confidence": 0.7024406641721725}, {"text": "question answering", "start_pos": 157, "end_pos": 175, "type": "TASK", "confidence": 0.7445870637893677}]}, {"text": "Prior work in standard semantic parsing uses a pre-defined set of predicates in a fixed ontology.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 23, "end_pos": 39, "type": "TASK", "confidence": 0.7227579206228256}]}, {"text": "However, it is difficult to construct formal ontologies of properties and relations that have broad coverage, and very difficult to do semantic parsing based on such an ontology.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 135, "end_pos": 151, "type": "TASK", "confidence": 0.723873496055603}]}, {"text": "Consequently, current semantic parsers are mostly restricted to fairly limited domains, such as querying a specific database (.", "labels": [], "entities": []}, {"text": "We propose a semantic parser that is not restricted to a predefined ontology.", "labels": [], "entities": []}, {"text": "Instead, we use distributional semantics to generate the needed part of an on-the-fly ontology.", "labels": [], "entities": []}, {"text": "Distributional semantics is a statistical technique that represents the meaning of words and phrases as distributions over context words.", "labels": [], "entities": []}, {"text": "Distributional information can be used to predict semantic relations like synonymy and hyponymy between words and phrases of interest (.", "labels": [], "entities": []}, {"text": "The collection of predicted semantic relations is the \"on-the-fly ontology\" our semantic parser uses.", "labels": [], "entities": []}, {"text": "A distributional semantics is relatively easy to build from a large corpus of raw text, and provides the wide coverage that formal ontologies lack.", "labels": [], "entities": []}, {"text": "The formal language we would like to use in the semantic parser is first-order logic.", "labels": [], "entities": []}, {"text": "However, distributional information is graded in nature, so the on-the-fly ontology and its predicted semantic relations are also graded.", "labels": [], "entities": []}, {"text": "This means, that standard first-order logic is insufficient because it is binary by nature.", "labels": [], "entities": []}, {"text": "Probabilistic logic solves this problem because it accepts weighted first order logic formulas.", "labels": [], "entities": []}, {"text": "For example, in probabilistic logic, the synonymy relation between \"man\" and \"guy\" is represented by: \u2200x. man(x) \u21d4 guy(x) | w 1 and the hyponymy relation between \"car\" and \"vehicle\" is: \u2200x. car(x) \u21d2 vehicle(x) | w 2 where w 1 and w 1 are some certainty measure estimated from the distributional semantics.", "labels": [], "entities": []}, {"text": "For inference, we use probabilistic logic frameworks like Markov Logic Networks (MLN) () and Probabilistic Soft Logic (PSL) ().", "labels": [], "entities": []}, {"text": "They are Statistical Relational Learning (SRL) techniques) that combine logical and statistical knowledge in one uniform framework, and provide a mechanism for coherent probabilistic inference.", "labels": [], "entities": [{"text": "Statistical Relational Learning (SRL)", "start_pos": 9, "end_pos": 46, "type": "TASK", "confidence": 0.875686893860499}]}, {"text": "We implemented this semantic parser () and used it to perform two tasks that require deep semantic analysis, Recognizing Textual Entailment (RTE), and Semantic Textual Similarity (STS).", "labels": [], "entities": [{"text": "Recognizing Textual Entailment (RTE)", "start_pos": 109, "end_pos": 145, "type": "TASK", "confidence": 0.605680912733078}, {"text": "Semantic Textual Similarity (STS)", "start_pos": 151, "end_pos": 184, "type": "TASK", "confidence": 0.696413924296697}]}, {"text": "The rest of the paper is organized as follows: section 2 presents background material, section 3 explains the three components of the semantic parser, section 4 shows how this semantic parser can be used for RTE and STS tasks, section 5 presents the evaluation and 6 concludes.", "labels": [], "entities": [{"text": "RTE and STS tasks", "start_pos": 208, "end_pos": 225, "type": "TASK", "confidence": 0.7661828845739365}]}], "datasetContent": [{"text": "The dataset used for evaluation is SICK: Sentences Involving Compositional Knowledge dataset, a task for SemEval 2014.", "labels": [], "entities": [{"text": "SemEval 2014", "start_pos": 105, "end_pos": 117, "type": "TASK", "confidence": 0.7298654913902283}]}, {"text": "The initial data release for the competition consists of 5,000 pairs of sentences which are annotated for both RTE and STS.", "labels": [], "entities": [{"text": "RTE", "start_pos": 111, "end_pos": 114, "type": "DATASET", "confidence": 0.8012424111366272}, {"text": "STS", "start_pos": 119, "end_pos": 122, "type": "DATASET", "confidence": 0.5853312611579895}]}, {"text": "For this evaluation, we performed 10-fold cross validation on this initial data.", "labels": [], "entities": []}, {"text": "shows results comparing our full approach (logic+dist) to two baselines, a distributional-only baseline (dist) that uses vector addition, and a probabilistic logic-only baseline (logic) which is our semantic parser without distributional inference rules.", "labels": [], "entities": []}, {"text": "The integrated approach (logic+dist) out-performs both baselines.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: RTE accuracy and STS Correlation", "labels": [], "entities": [{"text": "RTE", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.8984594941139221}, {"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9225268959999084}, {"text": "STS Correlation", "start_pos": 27, "end_pos": 42, "type": "METRIC", "confidence": 0.9108034074306488}]}]}