{"title": [{"text": "Tolerant BLEU: a Submission to the WMT14 Metrics Task", "labels": [], "entities": [{"text": "BLEU", "start_pos": 9, "end_pos": 13, "type": "METRIC", "confidence": 0.9662445783615112}, {"text": "WMT14 Metrics", "start_pos": 35, "end_pos": 48, "type": "DATASET", "confidence": 0.8914671838283539}]}], "abstractContent": [{"text": "This paper describes a machine translation metric submitted to the WMT14 Metrics Task.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 23, "end_pos": 42, "type": "TASK", "confidence": 0.7329613864421844}, {"text": "WMT14 Metrics Task", "start_pos": 67, "end_pos": 85, "type": "TASK", "confidence": 0.6196314295132955}]}, {"text": "It is a simple modification of the standard BLEU metric using a monolin-gual alignment of reference and test sentences.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 44, "end_pos": 48, "type": "METRIC", "confidence": 0.9723474979400635}]}, {"text": "The alignment is computed as a minimum weighted maximum bipartite matching of the translated and the reference sentence words with respect to the relative edit distance of the word prefixes and suffixes.", "labels": [], "entities": []}, {"text": "The aligned words are included in the n-gram precision computation with a penalty proportional to the matching distance.", "labels": [], "entities": [{"text": "precision", "start_pos": 45, "end_pos": 54, "type": "METRIC", "confidence": 0.8994753360748291}]}, {"text": "The proposed tBLEU metric is designed to be more tolerant to errors in inflection, which usually does not effect the understandability of a sentence, and therefore be more suitable for measuring quality of translation into morphologically richer languages.", "labels": [], "entities": []}], "introductionContent": [{"text": "Automatic evaluation of machine translation (MT) quality is an important part of the machine translation pipeline.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 24, "end_pos": 48, "type": "TASK", "confidence": 0.8302624702453614}, {"text": "machine translation pipeline", "start_pos": 85, "end_pos": 113, "type": "TASK", "confidence": 0.8370372454325358}]}, {"text": "The possibility to run an evaluation algorithm many times while training a system enables the system to be optimized with respect to such a metric (e.g., by Minimum Error Rate Training).", "labels": [], "entities": []}, {"text": "By achieving a high correlation of the metric with human judgment, we expect the system performance to be optimized also with respect to the human perception of translation quality.", "labels": [], "entities": []}, {"text": "In this paper, we propose an MT metric called tBLEU (tolerant BLEU) that is based on the standard BLEU () and designed to suit better when translation into morphologically richer languages.", "labels": [], "entities": [{"text": "MT", "start_pos": 29, "end_pos": 31, "type": "TASK", "confidence": 0.977442741394043}, {"text": "BLEU)", "start_pos": 62, "end_pos": 67, "type": "METRIC", "confidence": 0.8713668286800385}, {"text": "BLEU", "start_pos": 98, "end_pos": 102, "type": "METRIC", "confidence": 0.983919084072113}]}, {"text": "We aim to have a simple language independent metric that correlates with human judgment better than the standard BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 113, "end_pos": 117, "type": "METRIC", "confidence": 0.9895346760749817}]}, {"text": "Several metrics try to address this problem as well and usually succeed to gain a higher correlation with human judgment (e.g. ME-TEOR (), TerrorCat ().", "labels": [], "entities": [{"text": "TerrorCat", "start_pos": 139, "end_pos": 148, "type": "DATASET", "confidence": 0.8760392069816589}]}, {"text": "However, they usually use some language-dependent tools and resources (METEOR uses stemmer and parahprasing tables, TerrorCat uses lemmatization and needs training data for each language pair) which prevent them from being widely adopted.", "labels": [], "entities": [{"text": "TerrorCat", "start_pos": 116, "end_pos": 125, "type": "DATASET", "confidence": 0.9212076663970947}]}, {"text": "In the next section, the previous work is briefly summarized.", "labels": [], "entities": []}, {"text": "Section 3 describes the metric in detail.", "labels": [], "entities": []}, {"text": "The experiments with the metric are described in Section 4 and their results are summarized in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated the proposed metric on the dataset used for the WMT13 Metrics Task.", "labels": [], "entities": [{"text": "WMT13 Metrics Task", "start_pos": 61, "end_pos": 79, "type": "DATASET", "confidence": 0.6983118255933126}]}, {"text": "The dataset consists of 135 systems' outputs in 10 directions (5 into English 5 out of English).", "labels": [], "entities": []}, {"text": "Each system's output and the reference translation contain 3000 sentences.", "labels": [], "entities": []}, {"text": "According to the WMT14 guidelines, we report the the Pearson's correlation coefficient instead of the Spearman's coefficient that was used in the last years.", "labels": [], "entities": [{"text": "WMT14", "start_pos": 17, "end_pos": 22, "type": "DATASET", "confidence": 0.8669589161872864}, {"text": "Pearson's correlation coefficient", "start_pos": 53, "end_pos": 86, "type": "METRIC", "confidence": 0.9617408365011215}, {"text": "Spearman's coefficient", "start_pos": 102, "end_pos": 124, "type": "METRIC", "confidence": 0.6412873963514963}]}, {"text": "Twenty values of the affix distance threshold were tested in order to estimate what is the most suitable threshold setting.", "labels": [], "entities": [{"text": "affix distance threshold", "start_pos": 21, "end_pos": 45, "type": "METRIC", "confidence": 0.9063034454981486}]}, {"text": "We report only the system level correlation because the metric is designed to compare only the whole system outputs.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: System level Pearson's correlation with  the human judgment for systems translating from  English computed on the WMT13 dataset.", "labels": [], "entities": [{"text": "Pearson's correlation", "start_pos": 23, "end_pos": 44, "type": "METRIC", "confidence": 0.8129549026489258}, {"text": "WMT13 dataset", "start_pos": 124, "end_pos": 137, "type": "DATASET", "confidence": 0.9922240674495697}]}, {"text": " Table 2: System level Pearson's correlation with  the human judgment for systems translating to En- glish computed on the WMT13 dataset.", "labels": [], "entities": [{"text": "Pearson's correlation", "start_pos": 23, "end_pos": 44, "type": "METRIC", "confidence": 0.8446030418078104}, {"text": "WMT13 dataset", "start_pos": 123, "end_pos": 136, "type": "DATASET", "confidence": 0.9925436675548553}]}]}