{"title": [{"text": "Experiments for Dependency Parsing of Greek", "labels": [], "entities": [{"text": "Dependency Parsing of Greek", "start_pos": 16, "end_pos": 43, "type": "TASK", "confidence": 0.85353784263134}]}], "abstractContent": [{"text": "This paper describes experiments for statistical dependency parsing using two different parsers trained on a recently extended dependency treebank for Greek, a language with a moderately rich morphology.", "labels": [], "entities": [{"text": "statistical dependency parsing", "start_pos": 37, "end_pos": 67, "type": "TASK", "confidence": 0.7087519665559133}]}, {"text": "We show how scores obtained by the two parsers are influenced by morphology and dependency types as well as sentence and arc length.", "labels": [], "entities": []}, {"text": "The best LAS obtained in these experiments was 80.16 on a test set with manually validated POS tags and lemmas.", "labels": [], "entities": [{"text": "LAS", "start_pos": 9, "end_pos": 12, "type": "METRIC", "confidence": 0.9047509431838989}]}], "introductionContent": [{"text": "This work describes experiments for statistical dependency parsing using a recently extended dependency treebank for Greek, a language with a moderately rich morphology.", "labels": [], "entities": [{"text": "statistical dependency parsing", "start_pos": 36, "end_pos": 66, "type": "TASK", "confidence": 0.7148128847281138}]}, {"text": "Relatively small training resources like the one we use here can set severe sparsity obstacles for languages with flexible word order and a relatively rich morphology like Greek.", "labels": [], "entities": []}, {"text": "This work presents ongoing efforts for evaluating ways of improving this situation.", "labels": [], "entities": []}, {"text": "The rest of this paper is structured as follows: We describe the treebank and the tools for preprocessing it in section 2.", "labels": [], "entities": []}, {"text": "After mentioning some relevant work, we present in section 4 different settings for experiments involving manually validated and automatically pre-processed data for morphology and lemmas.", "labels": [], "entities": []}, {"text": "In section 5 we include a comparison of the output of two well-known statistical parsers in reference to a set of criteria.", "labels": [], "entities": []}, {"text": "Section 6 describes work on using sentences from relatively large auto-parsed resources as additional training data.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we report on experiments using statistical parsers trained on automatically preprocessed and manually validated versions of GDT-2014.", "labels": [], "entities": [{"text": "GDT-2014", "start_pos": 141, "end_pos": 149, "type": "DATASET", "confidence": 0.9760342240333557}]}, {"text": "In all experiments we report the Labeled and Unlabeled Attachment Scores (LAS and UAS) and the Label Accuracy (LACC), with punctuation tokens counting as scoring tokens.", "labels": [], "entities": [{"text": "Unlabeled Attachment Scores (LAS and UAS)", "start_pos": 45, "end_pos": 86, "type": "METRIC", "confidence": 0.752154815942049}, {"text": "Label Accuracy (LACC)", "start_pos": 95, "end_pos": 116, "type": "METRIC", "confidence": 0.873626983165741}]}, {"text": "We split the data of GDT-2014 in 90% and 10% training and test sets (5,101/567 sentences; 117,581/13,172 tokens).", "labels": [], "entities": [{"text": "GDT-2014", "start_pos": 21, "end_pos": 29, "type": "DATASET", "confidence": 0.8893768787384033}]}, {"text": "In this partitioning scheme, unknown tokens and lemmas when parsing the test set are 27% and 16%, respectively.", "labels": [], "entities": []}, {"text": "We performed experiments with the transition-based Maltparser () and the graph-based Mateparser.", "labels": [], "entities": []}, {"text": "For Maltparser, a 5-fold cross validation on the training set using MaltOptimizer (Ballesteros and Nivre, 2012) resulted in the selection of the non-projective stacklazy parsing algorithm as the one yielding an average best 78.96 LAS.", "labels": [], "entities": [{"text": "LAS", "start_pos": 230, "end_pos": 233, "type": "METRIC", "confidence": 0.9573286771774292}]}, {"text": "provides an abbreviated overview of the selected feature model, which is dominated by the top and first three elements in the parser's stack and its lookahead list.", "labels": [], "entities": []}, {"text": "For Mateparser we used default settings.", "labels": [], "entities": []}, {"text": "summarizes the results of our experiments.", "labels": [], "entities": []}, {"text": "We observe a better 79.74 LAS with Mateparser with a larger difference in UAS than in LACC (2.37 vs 1.26).", "labels": [], "entities": [{"text": "79.74", "start_pos": 20, "end_pos": 25, "type": "METRIC", "confidence": 0.9513095617294312}, {"text": "LAS", "start_pos": 26, "end_pos": 29, "type": "METRIC", "confidence": 0.6594072580337524}, {"text": "UAS", "start_pos": 74, "end_pos": 77, "type": "METRIC", "confidence": 0.9969539642333984}]}, {"text": "This may suggest that the two parsers agree on the labels they assign but differ more in discovering node heads.", "labels": [], "entities": []}, {"text": "Not surprisingly, testing in a more realistic scenario of using automatic PoS, features and lemmas produces more errors).", "labels": [], "entities": []}, {"text": "Maltparser shows a relatively smaller decrease inaccuracy (-3.05 vs -3.45) in this context.", "labels": [], "entities": [{"text": "Maltparser", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.9096570611000061}, {"text": "inaccuracy", "start_pos": 47, "end_pos": 57, "type": "METRIC", "confidence": 0.9721781015396118}]}, {"text": "In the next two experiments with Mateparser, we see that in automatic pre-processing scenarios, the tagger clearly contributes more to error increase (-3.34) compared to the lemmatizer (-0.06).", "labels": [], "entities": [{"text": "error increase", "start_pos": 135, "end_pos": 149, "type": "METRIC", "confidence": 0.9884611964225769}]}, {"text": "We also trained Mateparser in the MPL setting with POS tagsets of varying granularity, by removing features that were intuitively deemed to increase sparsity without contributing to parsing accuracy.", "labels": [], "entities": [{"text": "parsing", "start_pos": 182, "end_pos": 189, "type": "TASK", "confidence": 0.9629976749420166}, {"text": "accuracy", "start_pos": 190, "end_pos": 198, "type": "METRIC", "confidence": 0.7944450974464417}]}, {"text": "More specifically, we experimented with several combinations of removing for aspect and tense of verbs, gender of nominal elements, definiteness of articles and degree of adjectives.", "labels": [], "entities": []}, {"text": "A best LAS of 80.16 (cf. the two final columns of) was observed after removing features for degree and definiteness.", "labels": [], "entities": [{"text": "LAS", "start_pos": 7, "end_pos": 10, "type": "METRIC", "confidence": 0.9974195957183838}, {"text": "degree", "start_pos": 92, "end_pos": 98, "type": "METRIC", "confidence": 0.9859386086463928}]}, {"text": "Finally, and in order to examine how the expansion of the treebank has affected performance, we also", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: Results from parsing GDT with Malt and Mate parsers: MPL refers to training and testing on  manually validated POS, morphological features and lemmas; APL is evaluation on automatic POS, fea- tures and lemmas; APML is evaluation on automatic morphology and gold lemmas; MPAL on gold mor- phology and automatic lemmas. APL-AUTO is APL with training data including automatically parsed  sentences. MFR1 is MPL after removing features for tense, aspect, degree and definiteness. MFR2 is  MPL after removing features for degree and definiteness.", "labels": [], "entities": [{"text": "parsing GDT", "start_pos": 23, "end_pos": 34, "type": "TASK", "confidence": 0.6800356954336166}]}]}