{"title": [{"text": "\"My Curiosity was Satisfied, but not in a Good Way\": Predicting User Ratings for Online Recipes", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper, we develop an approach to automatically predict user ratings for recipes at Epicuri-ous.com, based on the recipes' reviews.", "labels": [], "entities": []}, {"text": "We investigate two distributional methods for feature selection , Information Gain and Bi-Normal Separation; we also compare distributionally selected features to linguistically motivated features and two types of frameworks: a one-layer system where we aggregate all reviews and predict the rating vs. a two-layer system where ratings of individual reviews are predicted and then aggregated.", "labels": [], "entities": [{"text": "Bi-Normal Separation", "start_pos": 87, "end_pos": 107, "type": "TASK", "confidence": 0.9073649346828461}]}, {"text": "We obtain our best results by using the two-layer architecture, in combination with 5 000 features selected by Information Gain.", "labels": [], "entities": []}, {"text": "This setup reaches an overall accuracy of 65.60%, given an upper bound of 82.57%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9997066855430603}]}], "introductionContent": [{"text": "Exchanging recipes over the internet has become popular over the last decade.", "labels": [], "entities": []}, {"text": "There are numerous sites that allow us to upload our own recipes, to search for and to download others, as well as to rate and review recipes.", "labels": [], "entities": []}, {"text": "Such sites aggregate invaluable information.", "labels": [], "entities": []}, {"text": "This raises the question how such sites can select good recipes to present to users.", "labels": [], "entities": []}, {"text": "Thus, we need to automatically predict their ratings.", "labels": [], "entities": []}, {"text": "Previous work () has shown that the reviews are the best rating predictors, in comparison to ingredients, preparation steps, and metadata.", "labels": [], "entities": []}, {"text": "In this paper, we follow their approach and investigate how to use the information contained in the reviews to its fullest potential.", "labels": [], "entities": []}, {"text": "Given that the rating classes are discrete and that the distances between adjacent classes are not necessarily equivalent, we frame this task as a classification problem, in which the class distribution is highly skewed, posing the question of how to improve precision and recall especially for the minority classes to achieve higher overall accuracy.", "labels": [], "entities": [{"text": "precision", "start_pos": 259, "end_pos": 268, "type": "METRIC", "confidence": 0.9992102384567261}, {"text": "recall", "start_pos": 273, "end_pos": 279, "type": "METRIC", "confidence": 0.9988340735435486}, {"text": "accuracy", "start_pos": 342, "end_pos": 350, "type": "METRIC", "confidence": 0.9832130074501038}]}, {"text": "One approach is to identify n-gram features of the highest discriminating power among ratings, from a large number of features, many of which are equally distributed over ratings.", "labels": [], "entities": []}, {"text": "An alternative strategy is to select less surface-oriented, but rather linguistically motivated features.", "labels": [], "entities": []}, {"text": "Our second question concerns the rating predictor architecture.", "labels": [], "entities": []}, {"text": "One possibility is to aggregate all reviews fora recipe, utilizing rich textual information atone step (one-layer architecture).", "labels": [], "entities": []}, {"text": "The other possibility is to rate individual reviews first, using shorter but more precise language clues, and then aggregate them (two-layer).", "labels": [], "entities": []}, {"text": "The latter approach avoids the problem of contradictory reviews fora given review, but it raises the question on how to aggregate over individual ratings.", "labels": [], "entities": []}, {"text": "We will investigate all these approaches.", "labels": [], "entities": []}, {"text": "The remainder of the paper is structured as follows: First, we review related work in section 2.", "labels": [], "entities": []}, {"text": "Then, in section 3, we motivate our research questions in more detail.", "labels": [], "entities": []}, {"text": "Section 4 describes the experimental setup, including the data preparation, feature extraction, classifier, and evaluation.", "labels": [], "entities": [{"text": "data preparation", "start_pos": 58, "end_pos": 74, "type": "TASK", "confidence": 0.7083160728216171}, {"text": "feature extraction", "start_pos": 76, "end_pos": 94, "type": "TASK", "confidence": 0.6982129216194153}]}, {"text": "In section 5, we present the results for the one-layer experiments, and in section 6 for the two-layer experiments.", "labels": [], "entities": []}, {"text": "Section 7 investigates a more realistic gold standard.", "labels": [], "entities": []}, {"text": "We then conclude in section 8.", "labels": [], "entities": []}], "datasetContent": [{"text": "Evaluation was performed using 3-fold cross validation.", "labels": [], "entities": []}, {"text": "Since the data is skewed, we report Precision (P), Recall (R), and F-Scores (F) for all classes across each experiment, along with standard accuracy.", "labels": [], "entities": [{"text": "Precision (P)", "start_pos": 36, "end_pos": 49, "type": "METRIC", "confidence": 0.9653730094432831}, {"text": "Recall (R)", "start_pos": 51, "end_pos": 61, "type": "METRIC", "confidence": 0.958611011505127}, {"text": "F-Scores (F)", "start_pos": 67, "end_pos": 79, "type": "METRIC", "confidence": 0.9629114717245102}, {"text": "accuracy", "start_pos": 140, "end_pos": 148, "type": "METRIC", "confidence": 0.996480405330658}]}], "tableCaptions": [{"text": " Table 2: Results for feature selection based on Bi-Normal Separation (BNS) and Information Gain (IG).", "labels": [], "entities": [{"text": "feature selection", "start_pos": 22, "end_pos": 39, "type": "TASK", "confidence": 0.7367193400859833}, {"text": "Bi-Normal Separation (BNS)", "start_pos": 49, "end_pos": 75, "type": "TASK", "confidence": 0.6818310379981994}]}, {"text": " Table 3: Results for manually selected features.", "labels": [], "entities": []}, {"text": " Table 4: Results on individual reviews for the two-layer experiments.", "labels": [], "entities": []}, {"text": " Table 5: Results on aggregating reviews for the two-layer experiments.", "labels": [], "entities": []}, {"text": " Table 6: Evaluation on a more realistic gold standard for two-layer experiments.", "labels": [], "entities": []}, {"text": " Table 7: Evaluation on a more realistic gold standard for one-layer experiments.", "labels": [], "entities": []}]}