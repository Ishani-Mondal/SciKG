{"title": [{"text": "Unsupervised Approach to Extracting Problem Phrases from User Reviews of Products", "labels": [], "entities": [{"text": "Extracting Problem Phrases from User Reviews of Products", "start_pos": 25, "end_pos": 81, "type": "TASK", "confidence": 0.8899932727217674}]}], "abstractContent": [{"text": "This paper describes an approach to problem phrase extraction from texts that contain user experience with products.", "labels": [], "entities": [{"text": "problem phrase extraction from texts that contain user experience with products", "start_pos": 36, "end_pos": 115, "type": "TASK", "confidence": 0.8223206373778257}]}, {"text": "In contrast to other works, we propose a straightforward approach to problem phrase extraction based on syntactic and semantic connections between a problem indicator and mentions about the problem targets.", "labels": [], "entities": [{"text": "problem phrase extraction", "start_pos": 69, "end_pos": 94, "type": "TASK", "confidence": 0.7512590289115906}]}, {"text": "In this paper, we discuss (i) grammatical dependencies between the target and the problem indicators and (ii) a number of domain-specific targets that were extracted using problem phrase structure and additional world knowledge.", "labels": [], "entities": []}, {"text": "The algorithm achieves an average F1-measure of 77%, evaluated on reviews about electronic and automobile products.", "labels": [], "entities": [{"text": "F1-measure", "start_pos": 34, "end_pos": 44, "type": "METRIC", "confidence": 0.9994798302650452}]}], "introductionContent": [{"text": "Automatic analysis of reviews can increase information about product effectiveness.", "labels": [], "entities": []}, {"text": "This is especially important to a company if the information can be obtained with minimal costs.", "labels": [], "entities": []}, {"text": "Customers write reviews regarding product issues that are too difficult to handle without technical support.", "labels": [], "entities": []}, {"text": "In this paper, we present a study about connections between a product (the target of a problem phrase) and words (problem indicators), describing unexpected situations specific to products.", "labels": [], "entities": []}, {"text": "We define problem indicators as words describing phrases that contain obvious links to a problem (e.g., problem, issue).", "labels": [], "entities": []}, {"text": "We also define problem indicators as words that mention implicit problems (e.g., after, sometimes).", "labels": [], "entities": []}, {"text": "The problem indicator maybe presented as an action verb with a negation expressing product failure.", "labels": [], "entities": []}, {"text": "The task is to identify which noun phrases (NPs) referred to the problem target in the sentence.", "labels": [], "entities": []}, {"text": "The task is divided into two subtasks: (1) identify what phrase potentially contains information about a problem and (2) find possible targets using the set of nouns fora given problem expression.", "labels": [], "entities": []}, {"text": "The first subtask, problem phrase identification, determines whether a given sentence contains problem phrases.", "labels": [], "entities": [{"text": "problem phrase identification", "start_pos": 19, "end_pos": 48, "type": "TASK", "confidence": 0.6850939591725668}]}, {"text": "The second subtask, target phrase extraction, identifies the targets of a given problem phrase.", "labels": [], "entities": [{"text": "target phrase extraction", "start_pos": 20, "end_pos": 44, "type": "TASK", "confidence": 0.6212328573067983}]}, {"text": "The problem indicators are significant for problem sentences where the device doesn't work correctly.", "labels": [], "entities": []}, {"text": "However, the presence of indicators in the sentence may have insufficient context to determine the problem's existence.", "labels": [], "entities": []}, {"text": "In examples 1 and 2, the object that receives the action isn't defined (\"I have not seen one\", \"something hasn't been right\").", "labels": [], "entities": []}, {"text": "1. After looking for months, I have not seen one that I like at the local store and have expanded to other local stores.", "labels": [], "entities": []}, {"text": "2. Something hasn't been right for sometimes now -we have advised the support staff several times about this issue.", "labels": [], "entities": []}, {"text": "We present the dependency-based approach for extracting problem phrases and its target from user reviews of products.", "labels": [], "entities": []}, {"text": "We suppose that problem indicators have a syntax connection with the target of a problem phrase.", "labels": [], "entities": []}, {"text": "Domain-specific targets are extracted by determining a domain category of related target words in WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 98, "end_pos": 105, "type": "DATASET", "confidence": 0.9727096557617188}]}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "In section 2, we introduce related work in different areas of text classification of reviews and target detection.", "labels": [], "entities": [{"text": "text classification of reviews", "start_pos": 62, "end_pos": 92, "type": "TASK", "confidence": 0.8271088898181915}, {"text": "target detection", "start_pos": 97, "end_pos": 113, "type": "TASK", "confidence": 0.7455716729164124}]}, {"text": "Section 3 describes our approach and classifies dependency relations between problem indicators and targets of problem phrases.", "labels": [], "entities": []}, {"text": "We present experimental results in section 4.", "labels": [], "entities": []}, {"text": "Section 5 presents our conclusions and future extensions of this work.", "labels": [], "entities": []}], "datasetContent": [{"text": "For our experiments we collected 734 sentences from the HP website 2 . We employed 953 sentences from Amazon reviews 3 about automobile products.", "labels": [], "entities": [{"text": "HP website", "start_pos": 56, "end_pos": 66, "type": "DATASET", "confidence": 0.9207465946674347}]}, {"text": "Of the total sentences, 1,288 sentences (506 + 782 from electronic and automobile domains) were classified as problem sentences, and 399 (228 + 171) sentences were labeled as part of the no-problem class.", "labels": [], "entities": []}, {"text": "Class labels for each sentence were obtained by using the Amazon Mechanical Turk service.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk service", "start_pos": 58, "end_pos": 88, "type": "DATASET", "confidence": 0.9577893316745758}]}, {"text": "Each sentence does not have any particular label for targets and contains at least one problem indicator that the approach can find.", "labels": [], "entities": []}, {"text": "We propose that the problem phrase with the problem indicator always has targets, but not necessarily domain-specific targets.", "labels": [], "entities": []}, {"text": "For our performance metrics, we view that a text classification task is to identify whether a target is a domainspecific problem target.", "labels": [], "entities": [{"text": "text classification task", "start_pos": 44, "end_pos": 68, "type": "TASK", "confidence": 0.8072620232899984}]}, {"text": "We computed precision (P), recall (R), accuracy (Acc.) and F1-mesure (F1).", "labels": [], "entities": [{"text": "precision (P)", "start_pos": 12, "end_pos": 25, "type": "METRIC", "confidence": 0.9475439637899399}, {"text": "recall (R)", "start_pos": 27, "end_pos": 37, "type": "METRIC", "confidence": 0.9509969353675842}, {"text": "accuracy", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.9989820122718811}, {"text": "Acc.", "start_pos": 49, "end_pos": 53, "type": "METRIC", "confidence": 0.836652934551239}, {"text": "F1-mesure (F1)", "start_pos": 59, "end_pos": 73, "type": "METRIC", "confidence": 0.833556056022644}]}], "tableCaptions": [{"text": " Table 2: Performance metrics of the dependency-based approach.", "labels": [], "entities": []}]}