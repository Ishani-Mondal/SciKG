{"title": [{"text": "Bayesian Reordering Model with Feature Selection", "labels": [], "entities": []}], "abstractContent": [{"text": "In phrase-based statistical machine translation systems, variation in grammatical structures between source and target languages can cause large movements of phrases.", "labels": [], "entities": [{"text": "phrase-based statistical machine translation", "start_pos": 3, "end_pos": 47, "type": "TASK", "confidence": 0.6062444001436234}]}, {"text": "Modeling such movements is crucial in achieving translations of long sentences that appear natural in the target language.", "labels": [], "entities": [{"text": "translations of long sentences", "start_pos": 48, "end_pos": 78, "type": "TASK", "confidence": 0.8302098959684372}]}, {"text": "We explore generative learning approach to phrase reordering in Arabic to English.", "labels": [], "entities": [{"text": "phrase reordering", "start_pos": 43, "end_pos": 60, "type": "TASK", "confidence": 0.7840083539485931}]}, {"text": "Formulating the reordering problem as a classification problem and using naive Bayes with feature selection, we achieve an improvement in the BLEU score over a lexicalized reordering model.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 142, "end_pos": 152, "type": "METRIC", "confidence": 0.9778308570384979}]}, {"text": "The proposed model is compact, fast and scalable to a large corpus.", "labels": [], "entities": []}], "introductionContent": [{"text": "Currently, the dominant approach to machine translation is statistical, starting from the mathematical formulations and algorithms for parameter estimation, further extended in).", "labels": [], "entities": [{"text": "machine translation", "start_pos": 36, "end_pos": 55, "type": "TASK", "confidence": 0.7882104516029358}]}, {"text": "These early models, widely known as the IBM models, were wordbased.", "labels": [], "entities": [{"text": "IBM models", "start_pos": 40, "end_pos": 50, "type": "DATASET", "confidence": 0.8788663446903229}]}, {"text": "Recent extensions note that a better approach is to group collections of words, or phrases, for translation together, resulting in a significant focus these days on phrase-based statistical machine translation systems.", "labels": [], "entities": [{"text": "phrase-based statistical machine translation", "start_pos": 165, "end_pos": 209, "type": "TASK", "confidence": 0.5757855847477913}]}, {"text": "To deal with the alignment problem of oneto-many word alignments in the IBM model formulation, whereas phrase-based models may have many-to-many translation relationships, IBM models are trained in both directions, source to target and target to source, and their word alignments are combined ().", "labels": [], "entities": [{"text": "word alignments", "start_pos": 49, "end_pos": 64, "type": "TASK", "confidence": 0.7143779098987579}]}, {"text": "While phrase-based systems area significant improvement over word-based approaches, a particular issue that emerges is long-range reorderings at the phrase level ().", "labels": [], "entities": []}, {"text": "Analogous to speech recognition systems, translation systems relied on language models to produce more fluent translation.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 13, "end_pos": 31, "type": "TASK", "confidence": 0.7067202925682068}, {"text": "translation", "start_pos": 41, "end_pos": 52, "type": "TASK", "confidence": 0.9731425046920776}]}, {"text": "While early work penalized phrase movements without considering reorderings arising from vastly differing grammatical structures across language pairs like ArabicEnglish, many researchers considered lexical reordering models that attempted to learn orientation based on content).", "labels": [], "entities": [{"text": "phrase movements", "start_pos": 27, "end_pos": 43, "type": "TASK", "confidence": 0.7244792133569717}]}, {"text": "These approaches may suffer from the data sparseness problem since many phrase pairs occur only once.", "labels": [], "entities": []}, {"text": "As an alternative way of exploiting function approximation capabilities offered by machine learning methods, there is recent interest in formulating a learning problem that aims to predict reordering from linguistic features that capture their context.", "labels": [], "entities": [{"text": "function approximation", "start_pos": 36, "end_pos": 58, "type": "TASK", "confidence": 0.7617875933647156}]}, {"text": "An example of this is the maximum entropy method used by).", "labels": [], "entities": []}, {"text": "In this work we apply a naive Bayes classifier, combined with feature selection to address the reordering problem.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, this simple model of classification has not been used in this context previously.", "labels": [], "entities": []}, {"text": "We present empirical results comparing our work and previously proposed lexicalized reordering model.", "labels": [], "entities": []}, {"text": "We show that our model is scalable to large corpora.", "labels": [], "entities": []}, {"text": "The remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 discusses previous work in the field and how that is related to our paper.", "labels": [], "entities": []}, {"text": "Section 3 gives an overview of the baseline translation system.", "labels": [], "entities": []}, {"text": "Section 4 introduces the Bayesian reordering model and gives details of different inference methods, while, Section 5 describes feature selection method.", "labels": [], "entities": []}, {"text": "Section 6 presents the experiments and reports the results evaluated as classification and translation problems.", "labels": [], "entities": [{"text": "translation", "start_pos": 91, "end_pos": 102, "type": "TASK", "confidence": 0.8683364987373352}]}, {"text": "Finally, we end the paper with a summary of our conclusions and perspectives.", "labels": [], "entities": []}], "datasetContent": [{"text": "The corpus used in our experiments is MultiUN which is a large-scale parallel corpus extracted from the United Nations website 1.", "labels": [], "entities": [{"text": "United Nations website 1", "start_pos": 104, "end_pos": 128, "type": "DATASET", "confidence": 0.8527391254901886}]}, {"text": "We have used Arabic and English portion of MultiUN.", "labels": [], "entities": [{"text": "MultiUN", "start_pos": 43, "end_pos": 50, "type": "DATASET", "confidence": 0.8906156420707703}]}, {"text": "We simplify the problem by classifying phrase movements into three categories (monotone, swap, discontinuous).", "labels": [], "entities": []}, {"text": "To train the reordering models, we used GIZA++ to produce word alignments).", "labels": [], "entities": [{"text": "word alignments", "start_pos": 58, "end_pos": 73, "type": "TASK", "confidence": 0.6916535794734955}]}, {"text": "Then, we used the extract tool that comes with the Moses 2 toolkit () in order to extract phrase pairs along with their orientation classes.", "labels": [], "entities": []}, {"text": "Each extracted phrase pair is represented by linguistic features as follows: \u2022 Aligned source and target words in a phrase pair.", "labels": [], "entities": []}, {"text": "Each word alignment is a feature.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 5, "end_pos": 19, "type": "TASK", "confidence": 0.650780662894249}]}, {"text": "\u2022 Words within a window around the source phrase to capture the context.", "labels": [], "entities": []}, {"text": "We choose adjacent words of the phrase boundary.", "labels": [], "entities": []}, {"text": "Most researchers build one reordering model for the whole training set ().) simplified the learning problem to have as many submodels as source phrases.", "labels": [], "entities": []}, {"text": "Training data were divided into small independent sets where samples having the same source phrase are considered a training set.", "labels": [], "entities": []}, {"text": "In our experiments, we have chosen the first method.", "labels": [], "entities": []}, {"text": "We compare lexicalized and Bayesian reordering models in two phases.", "labels": [], "entities": []}, {"text": "In the classification phase, we seethe performance of the models as a classification problem.", "labels": [], "entities": [{"text": "classification", "start_pos": 7, "end_pos": 21, "type": "TASK", "confidence": 0.9711024165153503}]}, {"text": "In the translation phase, we test the actual impact of these reordering models in a translation system.", "labels": [], "entities": [{"text": "translation", "start_pos": 7, "end_pos": 18, "type": "TASK", "confidence": 0.9809431433677673}]}, {"text": "We used the Moses toolkit () with its default settings.", "labels": [], "entities": []}, {"text": "The language model is a 5-gram with interpolation and Kneser-Ney smoothing.", "labels": [], "entities": []}, {"text": "We tuned the system by using MERT technique.", "labels": [], "entities": [{"text": "MERT", "start_pos": 29, "end_pos": 33, "type": "METRIC", "confidence": 0.9329274296760559}]}, {"text": "We built four Arabic-English translation systems.", "labels": [], "entities": [{"text": "Arabic-English translation", "start_pos": 14, "end_pos": 40, "type": "TASK", "confidence": 0.674839437007904}]}, {"text": "Three systems differ in how their reordering models were estimated and the fourth system is a baseline system without reordering model.", "labels": [], "entities": []}, {"text": "In all cases, orientation extraction is hierarchical-based since it is the best approach while orientations are monotone, swap and discontinuous.", "labels": [], "entities": [{"text": "orientation extraction", "start_pos": 14, "end_pos": 36, "type": "TASK", "confidence": 0.9404180645942688}]}, {"text": "The model is trained in Moses by specifying the configuration string hier-msd-backward-fe. As commonly used in statistical machine translation, we evaluated the translation performance by BLEU score ().", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 111, "end_pos": 142, "type": "TASK", "confidence": 0.6200405657291412}, {"text": "BLEU score", "start_pos": 188, "end_pos": 198, "type": "METRIC", "confidence": 0.9808871150016785}]}, {"text": "The test sets are NIST MT06 and NIST MT08.", "labels": [], "entities": [{"text": "NIST", "start_pos": 18, "end_pos": 22, "type": "DATASET", "confidence": 0.9761674404144287}, {"text": "MT06", "start_pos": 23, "end_pos": 27, "type": "DATASET", "confidence": 0.47332748770713806}, {"text": "NIST", "start_pos": 32, "end_pos": 36, "type": "DATASET", "confidence": 0.9884147644042969}, {"text": "MT08", "start_pos": 37, "end_pos": 41, "type": "DATASET", "confidence": 0.5286622643470764}]}, {"text": "shows statistics of development and test sets.", "labels": [], "entities": []}, {"text": "We also computed statistical significance for the proposed models using the paired bootstrap resampling method: Statistics of development and test sets.", "labels": [], "entities": []}, {"text": "The English side in NIST is larger because there are four translations for each Arabic sentence.", "labels": [], "entities": [{"text": "NIST", "start_pos": 20, "end_pos": 24, "type": "DATASET", "confidence": 0.9375684857368469}]}], "tableCaptions": [{"text": " Table 2: General statistics of Arabic-English Mul- tiUN (M: million, K: thousand).", "labels": [], "entities": [{"text": "Mul", "start_pos": 47, "end_pos": 50, "type": "METRIC", "confidence": 0.8990015983581543}]}, {"text": " Table 4: Statistics of development and test sets.  The English side in NIST is larger because there  are four translations for each Arabic sentence.", "labels": [], "entities": [{"text": "NIST", "start_pos": 72, "end_pos": 76, "type": "DATASET", "confidence": 0.9353595972061157}]}, {"text": " Table 6: BLEU scores for Arabic-English trans- lation systems (*: better than the baseline with at  least 95% statistical significance).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9989495873451233}, {"text": "trans- lation", "start_pos": 41, "end_pos": 54, "type": "TASK", "confidence": 0.6812925537427267}]}]}