{"title": [{"text": "Factored Markov Translation with Robust Modeling", "labels": [], "entities": []}], "abstractContent": [{"text": "Phrase-based translation models usually memorize local translation literally and make independent assumption between phrases which makes it neither generalize well on unseen data nor model sentence-level effects between phrases.", "labels": [], "entities": [{"text": "Phrase-based translation", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8212001919746399}]}, {"text": "In this paper we present anew method to model correlations between phrases as a Markov model and meanwhile employ a robust smoothing strategy to provide better generalization.", "labels": [], "entities": []}, {"text": "This method defines a re-cursive estimation process and backs off in parallel paths to infer richer structures.", "labels": [], "entities": []}, {"text": "Our evaluation shows an 1.1-3.2% BLEU improvement over competitive baselines for Chinese-English and Arabic-English translation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 33, "end_pos": 37, "type": "METRIC", "confidence": 0.9997432827949524}]}], "introductionContent": [{"text": "Phrase-based methods to machine translation () have drastically improved beyond word-based approaches, primarily by using phrase-pairs as translation units, which can memorize local lexical context and reordering patterns.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 24, "end_pos": 43, "type": "TASK", "confidence": 0.7502649128437042}]}, {"text": "However, this literal memorization mechanism makes it generalize poorly to unseen data.", "labels": [], "entities": []}, {"text": "Moreover, phrase-based models make an independent assumption, stating that the application of phrases in a derivation is independent to each other which conflicts with the underlying truth that the translation decisions of phrases should be dependent on context.", "labels": [], "entities": []}, {"text": "There are some work aiming to solve the two problems.", "labels": [], "entities": []}, {"text": "propose a word-based Markov model to integrate translation and reordering into one model and use the sophisticated hierarchical Pitman-Yor process which backs off from larger to smaller context to provide dynamic adaptive smoothing.", "labels": [], "entities": []}, {"text": "This model shows good generalization to unseen data while it uses words as the translation unit which cannot handle multiple-to-multiple links in real word alignments. and propose an operation sequence model (OSM) which models correlations between minimal translation units (MTUs) and evaluates probabilities with modified Kneser-Ney smoothing.", "labels": [], "entities": []}, {"text": "On one hand the use of MTUs can help retain the multiple-to-multiple alignments, on the other hand its definition of operations where source words and target words are bundled into one operation makes it subjected to sparsity.", "labels": [], "entities": []}, {"text": "The common feature of the above two methods is they both back off in one fixed path by dropping least recent events first which precludes some useful structures.", "labels": [], "entities": []}, {"text": "For the segment pairs <b\u02c7a<b\u02c7a t\u00af a k\u02c7ao\u00ec v j` \u0131nq\u00f9, take it into account> in, the more common structure is <b\u02c7a<b\u02c7a ...", "labels": [], "entities": []}, {"text": "k\u02c7ao\u00ec v j` \u0131nq\u00f9, take ... into account>.", "labels": [], "entities": []}, {"text": "If we always drop the least recent events first, then we can only learn the pattern <...", "labels": [], "entities": []}, {"text": "t\u00af a k\u02c7ao\u00ec v j` \u0131nq\u00f9, ...", "labels": [], "entities": []}, {"text": "On these grounds, we propose a method with new definition of correlations and more robust probability modeling.", "labels": [], "entities": []}, {"text": "This method defines a Markov model over correlations between minimal phrases where each is decomposed into three factors (source, target and jump).", "labels": [], "entities": []}, {"text": "In the meantime it employs a fancier smoothing strategy for the Markov model which backs off by dropping multiple conditioning factors in parallel in order to learn richer structures.", "labels": [], "entities": []}, {"text": "Both the uses of factors and parallel backoff give rise to robust modeling against sparsity.", "labels": [], "entities": []}, {"text": "In addition, modeling bilingual information and reorderings into one model instead of adding them to the linear model as separate features allows for using more sophisticated estimation methods rather than get a loose weight for each feature from tuning algorithms.", "labels": [], "entities": []}, {"text": "We compare the performance of our model with that of the phrase-based model and the hierarchical phrase-based model on the Chinese-English and Arabic-English NIST test sets, and get an im-", "labels": [], "entities": [{"text": "NIST test sets", "start_pos": 158, "end_pos": 172, "type": "DATASET", "confidence": 0.9497629602750143}]}], "datasetContent": [{"text": "We design experiments to first compare our method with the phrase-based model (PB), the operation sequence model (OSM) and the hierarchical phrase-based model (HPB), then we present several experiments to test: 1.", "labels": [], "entities": []}, {"text": "how each of the factors in our model and parallel backoff affect overall performance; 2.", "labels": [], "entities": []}, {"text": "how the language model order affects the relative gains, in order to test if we are just learning a high order LM, or something more useful; 3.", "labels": [], "entities": []}, {"text": "how the Markov model interplay with the distortion and lexical reordering models of Moses, and are they complemenatary; 4. whether using MPs as translation units is better in our approach than the simpler tactic of using only word pairs.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: BLEU % scores on the Chinese-English  data set.", "labels": [], "entities": [{"text": "BLEU % scores", "start_pos": 10, "end_pos": 23, "type": "METRIC", "confidence": 0.9601323207219442}, {"text": "Chinese-English  data set", "start_pos": 31, "end_pos": 56, "type": "DATASET", "confidence": 0.8591090043385824}]}, {"text": " Table 2: BLEU % scores on the Arabic-English  data set.", "labels": [], "entities": [{"text": "BLEU % scores", "start_pos": 10, "end_pos": 23, "type": "METRIC", "confidence": 0.9582260251045227}, {"text": "Arabic-English  data set", "start_pos": 31, "end_pos": 55, "type": "DATASET", "confidence": 0.7815980811913809}]}, {"text": " Table 3: The impact of factors and parallel back- off. Key: t-target, j-jump, s-source, p-parallel  backoff.", "labels": [], "entities": []}, {"text": " Table 4: The impact of the order of the standard  language models.", "labels": [], "entities": []}, {"text": " Table 5: Comparison between our Markov model  (denoted as M) and the lexical reordering model  of Moses.", "labels": [], "entities": []}, {"text": " Table 5. Comparing  the results of Moses-l and Moses-d, we can see that  the lexical reordering model outperforms the dis- tortion model by a margin of 1.5% BLEU. Com- paring Moses-d+M with Moses-l, our Markov  model provides further improvements of 2.0%", "labels": [], "entities": [{"text": "BLEU", "start_pos": 158, "end_pos": 162, "type": "METRIC", "confidence": 0.9996113181114197}]}]}