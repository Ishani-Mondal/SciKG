{"title": [{"text": "Learning to Rank Answer Candidates for Automatic Resolution of Crossword Puzzles", "labels": [], "entities": [{"text": "Automatic Resolution of Crossword Puzzles", "start_pos": 39, "end_pos": 80, "type": "TASK", "confidence": 0.8160803973674774}]}], "abstractContent": [{"text": "In this paper, we study the impact of rela-tional and syntactic representations for an interesting and challenging task: the automatic resolution of crossword puzzles.", "labels": [], "entities": [{"text": "automatic resolution of crossword puzzles", "start_pos": 125, "end_pos": 166, "type": "TASK", "confidence": 0.7554034411907196}]}, {"text": "Automatic solvers are typically based on two answer retrieval modules: (i) a web search engine, e.g., Google, Bing, etc. and (ii) a database (DB) system for access-ing previously resolved crossword puzzles.", "labels": [], "entities": [{"text": "Automatic solvers", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7032286524772644}]}, {"text": "We show that learning to rank models based on relational syntactic structures defined between the clues and the answer can improve both modules above.", "labels": [], "entities": []}, {"text": "In particular , our approach accesses the DB using a search engine and reranks its output by modeling paraphrasing.", "labels": [], "entities": []}, {"text": "This improves on the MRR of previous system up to 53% in ranking answer candidates and greatly impacts on the resolution accuracy of crossword puzzles up to 15%.", "labels": [], "entities": [{"text": "MRR", "start_pos": 21, "end_pos": 24, "type": "METRIC", "confidence": 0.9984719157218933}, {"text": "accuracy", "start_pos": 121, "end_pos": 129, "type": "METRIC", "confidence": 0.6951859593391418}]}], "introductionContent": [{"text": "Crossword puzzles (CPs) are probably the most popular language games played around the world.", "labels": [], "entities": [{"text": "Crossword puzzles (CPs)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.866089379787445}]}, {"text": "It is very challenging for human intelligence as it requires high level of general knowledge, logical thinking, intuition and the ability to deal with ambiguities and puns.", "labels": [], "entities": []}, {"text": "CPs normally have the form of a square or rectangular grid of white and black shaded squares.", "labels": [], "entities": []}, {"text": "The white squares on the border of the grid or adjacent to the black ones are associated with clues.", "labels": [], "entities": []}, {"text": "The goal of the game is to fill the sequences of white squares with words answering the clues.", "labels": [], "entities": []}, {"text": "There have been many attempts to build automatic CP solving systems, which have also participated in competitions such as The American Crossword Puzzle Tournament (ACPT).", "labels": [], "entities": [{"text": "CP solving", "start_pos": 49, "end_pos": 59, "type": "TASK", "confidence": 0.9100784063339233}]}, {"text": "This is the oldest and largest CP tournament for crossword experts held in the United States.", "labels": [], "entities": []}, {"text": "The goal of such systems is to outperform human players in solving crosswords more accurately and in less time.", "labels": [], "entities": []}, {"text": "Automatic CP solvers have been mainly targeted by the artificial intelligence (AI) community, who has mostly focused on AI techniques for filling the puzzle grid, given a set of answer candidates for each clue.", "labels": [], "entities": [{"text": "Automatic CP solvers", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.6851280331611633}]}, {"text": "The basic idea is to optimize the overall probability of correctly filling the entire grid by exploiting the likelihood of each candidate answer, fulfilling at the same time the grid constraints.", "labels": [], "entities": []}, {"text": "After several failures in approaching the human expert performance, it has become clear that designing more accurate solvers would not have provided a winning system.", "labels": [], "entities": [{"text": "solvers", "start_pos": 117, "end_pos": 124, "type": "TASK", "confidence": 0.9559116959571838}]}, {"text": "In contrast, the Precision and Recall of the answer candidates are obviously a key factor: a very high value for both of them would enable the solver to quickly find the correct solution.", "labels": [], "entities": [{"text": "Precision", "start_pos": 17, "end_pos": 26, "type": "METRIC", "confidence": 0.9989157915115356}, {"text": "Recall", "start_pos": 31, "end_pos": 37, "type": "METRIC", "confidence": 0.9765332341194153}, {"text": "solver", "start_pos": 143, "end_pos": 149, "type": "TASK", "confidence": 0.9643325209617615}]}, {"text": "This basically suggests that, similarly to the Jeopardy!", "labels": [], "entities": [{"text": "Jeopardy!", "start_pos": 47, "end_pos": 56, "type": "DATASET", "confidence": 0.6675384640693665}]}, {"text": "challenge case), the solution relies on Question Answering (QA) research.", "labels": [], "entities": [{"text": "Question Answering (QA)", "start_pos": 40, "end_pos": 63, "type": "TASK", "confidence": 0.8272960364818573}]}, {"text": "In this paper, we study methods for improving the quality of automatic extraction of answer candidate lists for automatic CP resolution.", "labels": [], "entities": [{"text": "automatic extraction of answer candidate lists", "start_pos": 61, "end_pos": 107, "type": "TASK", "confidence": 0.7733979423840841}, {"text": "CP resolution", "start_pos": 122, "end_pos": 135, "type": "TASK", "confidence": 0.8771742880344391}]}, {"text": "For this purpose, we designed learning to rank models for reordering the answers produced with two different techniques typically used in CP systems: (i) searching the Web with clue representations, e.g., exploiting Bing search engine querying the DB of previously resolved CP clues, e.g., using standard SQL techniques.", "labels": [], "entities": []}, {"text": "We rerank the text snippets returned by Bing by means of SVM preference ranking () for improving the first technique.", "labels": [], "entities": []}, {"text": "One interesting contribution is that our model exploits a syntactic representation of clues to improve Web search.", "labels": [], "entities": []}, {"text": "More in detail, we use structural kernels (e.g., see) in SVMs applied to our syntactic representation of pairs, formed by clues with their candidate snippets.", "labels": [], "entities": []}, {"text": "Regarding the DB approach, we provide a completely novel solution by substituting it and the SQL function with a search engine for retrieving clues similar to the target one.", "labels": [], "entities": []}, {"text": "Then, we rerank the retrieved clues by applying SVMs and structural kernels to the syntactic representation of clue pairs.", "labels": [], "entities": []}, {"text": "This way, SVMs learn to choose the best candidate among similar clues that are available in the DB.", "labels": [], "entities": []}, {"text": "The syntactic representation captures clue paraphrasing properties.", "labels": [], "entities": []}, {"text": "In order to carryout our study, we created two different corpora, one for each task: (i) a snippets reranking dataset and (ii) a clue similarity dataset.", "labels": [], "entities": []}, {"text": "The first includes 21,000 clues, each associated with 150 candidate snippets whereas the latter comprises 794,190 clues.", "labels": [], "entities": []}, {"text": "These datasets constitute interesting resources that we made available to the research community 2 . We compare our methods with one of the best systems for automatic CP resolution,).", "labels": [], "entities": [{"text": "CP resolution", "start_pos": 167, "end_pos": 180, "type": "TASK", "confidence": 0.8387909531593323}]}, {"text": "Such system does use the two approaches mentioned before.", "labels": [], "entities": []}, {"text": "Regarding snippet reranking, our structural models improve on the basic approach of WebCrow based on Bing by more than 4 absolute percent points in MRR, fora relative improvement of 23%.", "labels": [], "entities": [{"text": "MRR", "start_pos": 148, "end_pos": 151, "type": "METRIC", "confidence": 0.7796909213066101}]}, {"text": "Concerning the similar clues retrieval, our methods improve on the one used by WebCrow, based on DBs, by 25% absolute, i.e., about 53% of error reduction whereas the answer accuracy at first position improves up to 70%.", "labels": [], "entities": [{"text": "error reduction", "start_pos": 138, "end_pos": 153, "type": "METRIC", "confidence": 0.9764630496501923}, {"text": "accuracy", "start_pos": 173, "end_pos": 181, "type": "METRIC", "confidence": 0.7779675722122192}]}, {"text": "Given such promising results, we used our clue reranking method in WebCrow, and obtained an average improvement of 15% in resolving complete CPs.", "labels": [], "entities": [{"text": "WebCrow", "start_pos": 67, "end_pos": 74, "type": "DATASET", "confidence": 0.9470250606536865}]}, {"text": "This demonstrates that advanced QA methods such as those based on syntactic structures and learning to rank methods can help to win the CP resolution challenge.", "labels": [], "entities": [{"text": "CP resolution challenge", "start_pos": 136, "end_pos": 159, "type": "TASK", "confidence": 0.9268006483713785}]}, {"text": "In the reminder of this paper, Sec.", "labels": [], "entities": []}, {"text": "2 introduces the automatic CP resolution task in the context of the related work, Sec.", "labels": [], "entities": [{"text": "CP resolution task", "start_pos": 27, "end_pos": 45, "type": "TASK", "confidence": 0.8690619270006815}]}, {"text": "3 introduces WebCrow, Sec.", "labels": [], "entities": []}, {"text": "4 illustrates our models for snippets reranking and similar clue retrieval using kernel methods, syntactic structures, and traditional feature vectors, Sec.", "labels": [], "entities": [{"text": "clue retrieval", "start_pos": 60, "end_pos": 74, "type": "TASK", "confidence": 0.7196122407913208}]}, {"text": "5 describes our experiments, and finally, Sec.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our experiments aim at demonstrating the effectiveness of our models on two different tasks: (i) Snippet Reranking and (ii) Similar Clue Retrieval (SCR).", "labels": [], "entities": [{"text": "Snippet Reranking", "start_pos": 97, "end_pos": 114, "type": "TASK", "confidence": 0.7954055964946747}, {"text": "Similar Clue Retrieval (SCR)", "start_pos": 124, "end_pos": 152, "type": "TASK", "confidence": 0.7280374417702357}]}, {"text": "Additionally, we measured the impact of our best model for SCR in the WebCrow system by comparing with it.", "labels": [], "entities": [{"text": "SCR", "start_pos": 59, "end_pos": 62, "type": "TASK", "confidence": 0.9761868715286255}, {"text": "WebCrow system", "start_pos": 70, "end_pos": 84, "type": "DATASET", "confidence": 0.9274992644786835}]}, {"text": "Our referring database of clues is composed by 1,158,202 clues, which belong to eight different crossword editors (downloaded from the Web 6 ).", "labels": [], "entities": []}, {"text": "We use the latter to create one dataset for snippet reranking and one dataset for clues retrieval.", "labels": [], "entities": [{"text": "clues retrieval", "start_pos": 82, "end_pos": 97, "type": "TASK", "confidence": 0.7568711638450623}]}, {"text": "To train our models, we adopted SVM-light-TK 7 , which enables the use of structural kernels) in SVM-light), with default parameters.", "labels": [], "entities": [{"text": "SVM-light-TK 7", "start_pos": 32, "end_pos": 46, "type": "DATASET", "confidence": 0.8409123122692108}]}, {"text": "We applied a polynomial kernel of degree 3 to the explicit feature vectors,   as we believe feature combinations can be valuable.", "labels": [], "entities": []}, {"text": "To measure the impact of the rerankers as well as the baselines, we used well known metrics for assessing the accuracy of QA and retrieval systems, i.e.: Recall at rank 1 (R@1 and 5), Mean Reciprocal Rank (MRR), Mean Average Precision (MAP), the average Recall (AvgRec).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 110, "end_pos": 118, "type": "METRIC", "confidence": 0.9985783100128174}, {"text": "Recall", "start_pos": 154, "end_pos": 160, "type": "METRIC", "confidence": 0.9819429516792297}, {"text": "Mean Reciprocal Rank (MRR)", "start_pos": 184, "end_pos": 210, "type": "METRIC", "confidence": 0.9459890027840933}, {"text": "Mean Average Precision (MAP)", "start_pos": 212, "end_pos": 240, "type": "METRIC", "confidence": 0.9715146422386169}, {"text": "average Recall (AvgRec)", "start_pos": 246, "end_pos": 269, "type": "METRIC", "confidence": 0.9115788102149963}]}, {"text": "R@k is the percentage of questions with a correct answer ranked at the first position.", "labels": [], "entities": [{"text": "R@k", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.9500221411387125}]}, {"text": "MRR is computed as follows: , where rank(q) is the position of the first correct answer in the candidate list.", "labels": [], "entities": [{"text": "MRR", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.4039941132068634}]}, {"text": "For a set of queries Q, MAP is the mean over the average precision scores for each query: 1 Q Q q=1 AveP (q).", "labels": [], "entities": [{"text": "MAP", "start_pos": 24, "end_pos": 27, "type": "METRIC", "confidence": 0.9960596561431885}, {"text": "precision", "start_pos": 57, "end_pos": 66, "type": "METRIC", "confidence": 0.9917148947715759}, {"text": "AveP", "start_pos": 100, "end_pos": 104, "type": "METRIC", "confidence": 0.9333517551422119}]}, {"text": "AvgRec and all the measures are evaluated on the first 10 retrieved snippets/clues.", "labels": [], "entities": [{"text": "AvgRec", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.7378324866294861}]}, {"text": "For training and testing the reranker, only the first 10 snippets/clues retrieved by the search engine are used.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Reranking of similar clues.", "labels": [], "entities": [{"text": "Reranking", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.5778457522392273}]}, {"text": " Table 4: Performance on the word list candidates  averaged over the clues of 10 entire CPs", "labels": [], "entities": []}, {"text": " Table 5: Performance given in terms of correct  words and letters averaged on the 10 CPs", "labels": [], "entities": []}]}