{"title": [{"text": "Workshop on Computational Linguistics and Clinical Psychology: From Linguistic Signal Challenges in Automating Maze Detection", "labels": [], "entities": [{"text": "Maze Detection", "start_pos": 111, "end_pos": 125, "type": "TASK", "confidence": 0.8281373083591461}]}], "abstractContent": [{"text": "SALT is a widely used annotation approach for analyzing natural language transcripts of children.", "labels": [], "entities": []}, {"text": "Nine annotated corpora are distributed along with scoring software to provide norming data.", "labels": [], "entities": []}, {"text": "We explore automatic identification of mazes-SALT's version of disfluency annotations-and find that cross-corpus generalization is very poor.", "labels": [], "entities": [{"text": "automatic identification of mazes-SALT", "start_pos": 11, "end_pos": 49, "type": "TASK", "confidence": 0.7018103078007698}, {"text": "cross-corpus generalization", "start_pos": 100, "end_pos": 127, "type": "TASK", "confidence": 0.7570336759090424}]}, {"text": "This surprising lack of cross-corpus generalization suggests substantial differences between the corpora.", "labels": [], "entities": [{"text": "cross-corpus generalization", "start_pos": 24, "end_pos": 51, "type": "TASK", "confidence": 0.6890476793050766}]}, {"text": "This is the first paper to investigate the SALT corpora from the lens of natural language processing , and to compare the utility of different corpora collected in a clinical setting to train an automatic annotation system .", "labels": [], "entities": [{"text": "SALT", "start_pos": 43, "end_pos": 47, "type": "TASK", "confidence": 0.8987758755683899}]}], "introductionContent": [{"text": "Assessing a child's linguistic abilities is a critical component of diagnosing developmental disorders such as Specific Language Impairment or Autism Spectrum Disorder, and for evaluating progress made with remediation.", "labels": [], "entities": [{"text": "Specific Language Impairment or Autism Spectrum Disorder", "start_pos": 111, "end_pos": 167, "type": "TASK", "confidence": 0.6130297183990479}]}, {"text": "Structured instruments (\"tests\") that elicit brief, easy to score, responses to a sequence of items area popular way of performing such assessment.", "labels": [], "entities": []}, {"text": "An example of a structured instrument is the CELF-4, which includes nineteen multi-item subtests with tasks such as object naming, word definition, reciting the days of the week, or repeating sentences (.", "labels": [], "entities": [{"text": "object naming", "start_pos": 116, "end_pos": 129, "type": "TASK", "confidence": 0.7130860537290573}, {"text": "word definition", "start_pos": 131, "end_pos": 146, "type": "TASK", "confidence": 0.7222184538841248}]}, {"text": "Over the past two decades, researchers have discussed the limitations of standardized tests and how well they tap into different language impairments.", "labels": [], "entities": []}, {"text": "Many have advocated the potential benefits of language sample analysis (LSA).", "labels": [], "entities": [{"text": "language sample analysis (LSA)", "start_pos": 46, "end_pos": 76, "type": "TASK", "confidence": 0.7632067600886027}]}, {"text": "The analysis of natural language samples maybe particularly beneficial for language assessment in ASD, where pragmatic and social communication issues are paramount yet maybe hard to assess in a conventional test format.", "labels": [], "entities": [{"text": "language assessment", "start_pos": 75, "end_pos": 94, "type": "TASK", "confidence": 0.7418752610683441}, {"text": "ASD", "start_pos": 98, "end_pos": 101, "type": "TASK", "confidence": 0.9683104753494263}]}, {"text": "At present, the expense of LSA prevents it from being more widely used., while arguing that LSA is not too time-consuming, estimates that each minute of spoken language takes five to manually transcribe and annotate.", "labels": [], "entities": []}, {"text": "At this rate, it is clearly impractical for clinicians to perform LSA on hours of speech.", "labels": [], "entities": []}, {"text": "Techniques from natural language processing could be used to build tools to automatically annotate transcripts, thus facilitating LSA.", "labels": [], "entities": [{"text": "LSA", "start_pos": 130, "end_pos": 133, "type": "TASK", "confidence": 0.9511825442314148}]}, {"text": "Here, we evaluate the utility of a set of annotated corpora for automating a key annotation in the de facto standard annotation schema for LSA: the Systematic Analysis of Language Transcripts (SALT)).", "labels": [], "entities": [{"text": "Systematic Analysis of Language Transcripts (SALT))", "start_pos": 148, "end_pos": 199, "type": "TASK", "confidence": 0.7168195247650146}]}, {"text": "SALT comprises a scheme for coding transcripts of recorded speech, together with software that tallies these codes, computes scores describing utterance length and error counts, among a range of other standard measures, and compares these scores with normative samples.", "labels": [], "entities": []}, {"text": "SALT codes indicate bound morphemes, several types of grammatical errors (for example using a pronoun of the wrong gender or case), and mazes, which are defined as \"filled pauses, false starts, and repetitions and revisions of words, morphemes and phrases\".", "labels": [], "entities": []}, {"text": "Mazes have sparked interest in the child language disorders literature for several reasons.", "labels": [], "entities": []}, {"text": "They are most often analyzed from a language processing perspective where the disruptions are viewed as a consequence of monitoring, detecting and repairing language, potentially including speech errors.", "labels": [], "entities": []}, {"text": "Several studies have found that as grammatical complexity and utterance length increase, the number of mazes increases in typically developing children and children with language impairments.", "labels": [], "entities": []}, {"text": "Mazes in narrative contexts have been shown to differ between typical children and children with specific language impairment), though others have not found reliable group differences ().", "labels": [], "entities": []}, {"text": "Furthermore, outside the potential usefulness of looking at mazes in themselves, mazes always have to be detected and excluded in order to calculate other standard LSA measures such as mean length of utterance and type or token counts.", "labels": [], "entities": []}, {"text": "Mazes also must be excluded when analyzing speech errors, since some mazes are in fact self-corrections of language or speech errors.", "labels": [], "entities": []}, {"text": "Thus, automatically delimiting mazes could be clinically useful in several ways.", "labels": [], "entities": []}, {"text": "First, if mazes can be automatically detected, standard measures such as token and type counts can be calculated with ease, as noted above.", "labels": [], "entities": []}, {"text": "Automatic maze detection could also be a first processing step for automatically identifying errors: error codes cannot appear in mazes, and certain grammatical errors maybe easier to identify once mazes have been excised.", "labels": [], "entities": [{"text": "maze detection", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.7033225744962692}]}, {"text": "Finally, after mazes have been identified, further analysis of the mazes themselves (e.g. the number of word in mazes, and the placement of mazes in the sentence) can provide supplementary information about language formulation abilities and word retrieval abilities.", "labels": [], "entities": [{"text": "language formulation", "start_pos": 207, "end_pos": 227, "type": "TASK", "confidence": 0.6955167055130005}, {"text": "word retrieval", "start_pos": 242, "end_pos": 256, "type": "TASK", "confidence": 0.744255393743515}]}, {"text": "We use the corpora included with the SALT software to train maze detectors.", "labels": [], "entities": [{"text": "maze detectors", "start_pos": 60, "end_pos": 74, "type": "TASK", "confidence": 0.7222016155719757}]}, {"text": "These are the corpora that the software uses to compute reference counts.", "labels": [], "entities": []}, {"text": "These corpora share several characteristics we expect to be typical of clinical data: they were collected under a diverse set of circumstances; they were annotated by different groups; the annotations ostensibly follow the same guidelines; and the annotations were not designed with automation in mind.", "labels": [], "entities": []}, {"text": "We will investigate whether we can extract usable generalizations from the available data, and explore how well the automated system performs, which will be of interest to clinicians looking to expedite LSA.", "labels": [], "entities": [{"text": "LSA", "start_pos": 203, "end_pos": 206, "type": "TASK", "confidence": 0.9467215538024902}]}], "datasetContent": [{"text": "We split each SALT corpus into training, development, and test partitions.", "labels": [], "entities": [{"text": "SALT corpus", "start_pos": 14, "end_pos": 25, "type": "DATASET", "confidence": 0.8785646557807922}]}, {"text": "Each training partition contains 80% of the utterances the corpus, while the development and test partitions each contain 10% of the utterances.", "labels": [], "entities": []}, {"text": "We use the development portion of each corpus to set the penalty matrix system to roughly balance precision and recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 98, "end_pos": 107, "type": "METRIC", "confidence": 0.9994266033172607}, {"text": "recall", "start_pos": 112, "end_pos": 118, "type": "METRIC", "confidence": 0.9982428550720215}]}, {"text": "We evaluate maze detection in terms of both tagging performance and bracketing performance, both of which are standard forms of evaluation for various tasks in the Natural Language Processing literature.", "labels": [], "entities": [{"text": "maze detection", "start_pos": 12, "end_pos": 26, "type": "TASK", "confidence": 0.8477447628974915}, {"text": "bracketing", "start_pos": 68, "end_pos": 78, "type": "TASK", "confidence": 0.9491678476333618}]}, {"text": "Tagging performance captures how effectively maze detection is done on a wordby-word basis, while bracketing performance describes how well each maze is identified in its entirety.", "labels": [], "entities": [{"text": "maze detection", "start_pos": 45, "end_pos": 59, "type": "TASK", "confidence": 0.8247080147266388}]}, {"text": "For both tagging and bracketing performance, we count the number of true and false positives and negatives, as illustrated in.", "labels": [], "entities": [{"text": "tagging", "start_pos": 9, "end_pos": 16, "type": "TASK", "confidence": 0.9742193222045898}, {"text": "bracketing", "start_pos": 21, "end_pos": 31, "type": "TASK", "confidence": 0.951094388961792}]}, {"text": "In tagging performance, each word gets counted once, while in bracketing performance we compare the predicted and observed maze spans.", "labels": [], "entities": []}, {"text": "We use these counts to compute the following metrics: 2P RP + R Note that partial words and punctuation are both ignored in evaluation.", "labels": [], "entities": []}, {"text": "We exclude punctuation because punctuation does not need to be included in mazes: it is not counted in summary statistics (e.g. MLU, word count, etc.), and punctuation errors are not captured by the SALT error codes.", "labels": [], "entities": [{"text": "MLU", "start_pos": 128, "end_pos": 131, "type": "METRIC", "confidence": 0.8710256814956665}]}, {"text": "We exclude partial words because they are always in mazes, and therefore can be detected trivially with a simple rule.", "labels": [], "entities": []}, {"text": "Furthermore, because partial words are excluded from evaluation, the performance metrics are comparable across corpora, even if they vary widely in the frequency of partial words.", "labels": [], "entities": []}, {"text": "For both space and clarity, we do not present the complete results of every experiment in this paper, although they are available online . Instead, we present the complete baseline results, and then report F1 scores that are significantly better than the baseline.", "labels": [], "entities": [{"text": "clarity", "start_pos": 19, "end_pos": 26, "type": "METRIC", "confidence": 0.990223228931427}, {"text": "F1 scores", "start_pos": 206, "end_pos": 215, "type": "METRIC", "confidence": 0.9778671264648438}]}, {"text": "We establish statistical significance by using a randomized paired-sample test (see) to compare the baseline system (system A) and the proposed system (system B).", "labels": [], "entities": []}, {"text": "First, we compute the difference din F1 score between systems A and B.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.9285971820354462}]}, {"text": "Then, we repeatedly construct a random set of predictions for each input item by choosing between the outputs of system A and B with equal probability.", "labels": [], "entities": []}, {"text": "We compute the F1 score of these random predictions, and if it exceeds the F1 score of the baseline system by at least d, we count the iteration as a success.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9861240386962891}, {"text": "F1 score", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.9859818518161774}]}, {"text": "The significance level is at most the number of successes divided by one more than the number of trials).", "labels": [], "entities": [{"text": "significance level", "start_pos": 4, "end_pos": 22, "type": "METRIC", "confidence": 0.9598710536956787}]}], "tableCaptions": [{"text": " Table 1: Description of SALT corpora", "labels": [], "entities": [{"text": "SALT corpora", "start_pos": 25, "end_pos": 37, "type": "TASK", "confidence": 0.7880741059780121}]}, {"text": " Table 3: Baseline maze detection performance on development sections of SALT corpora: corpus-specific models", "labels": [], "entities": [{"text": "Baseline maze detection", "start_pos": 10, "end_pos": 33, "type": "TASK", "confidence": 0.677440325419108}]}]}