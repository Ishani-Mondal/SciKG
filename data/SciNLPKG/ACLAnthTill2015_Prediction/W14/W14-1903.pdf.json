{"title": [{"text": "Dialogue Strategy Learning in Healthcare: A Systematic Approach for Learning Dialogue Models from Data", "labels": [], "entities": []}], "abstractContent": [{"text": "We aim to build dialogue agents that optimize the dialogue strategy, specifically through learning the dialogue model components from dialogue data.", "labels": [], "entities": []}, {"text": "In this paper, we describe our current research on automatically learning dialogue strategies in the healthcare domain.", "labels": [], "entities": []}, {"text": "We go through our systematic approach of learning dialogue model components from data, specifically user intents and the user model, as well as the agent reward function.", "labels": [], "entities": []}, {"text": "We demonstrate our experiments on healthcare data from which we learned the dialogue model components.", "labels": [], "entities": []}, {"text": "We conclude by describing our current research for automatically learning dialogue features that can be used in representing dialogue states and learning the reward function.", "labels": [], "entities": []}], "introductionContent": [{"text": "Cognitive assistive technologies provide support systems for the elderly, possibly with cognitive or physical disabilities, for instance people with dementia (such as Alzheimer's disease) ().", "labels": [], "entities": []}, {"text": "Such support systems can significantly reduce the costs of performing several tasks, currently done by family members or employed caregivers.", "labels": [], "entities": []}, {"text": "In this context, () are working on a computerized caregiver that assist individuals with Alzheimer's disease (AD) to complete daily tasks (e.g., preparing meals) using verbal communication.", "labels": [], "entities": []}, {"text": "Thus, an important component of such technologies is the dialogue agent.", "labels": [], "entities": []}, {"text": "(left) shows sample dialogues collected by SmartWheeler, an intelligent wheelchair for persons with disabilities ().", "labels": [], "entities": []}, {"text": "In particular, SmartWheeler aims to minimize the physical and cognitive load required in steering it.", "labels": [], "entities": []}, {"text": "SmartWheeler is equipped with a dialogue agent, thus the users can give their commands through the spoken language besides a joystick.", "labels": [], "entities": [{"text": "SmartWheeler", "start_pos": 0, "end_pos": 12, "type": "DATASET", "confidence": 0.9486591815948486}]}, {"text": "The first line denoted by u 1 shows the true user utterance, which is the one that has been extracted manually from user audio recordings.", "labels": [], "entities": []}, {"text": "The following line denoted by\u02dcuby\u02dc by\u02dcu 1 is the recognized user utterances by automatic speech recognition (ASR).", "labels": [], "entities": [{"text": "automatic speech recognition (ASR)", "start_pos": 79, "end_pos": 113, "type": "TASK", "confidence": 0.7806285818417867}]}, {"text": "Finally, the line denoted by a 1 shows the performed action in response to the ASR output at the time of collecting the dialogues.", "labels": [], "entities": [{"text": "ASR", "start_pos": 79, "end_pos": 82, "type": "TASK", "confidence": 0.871819794178009}]}, {"text": "First, the users may say a command in different ways.", "labels": [], "entities": []}, {"text": "For instance for turning right, the user may say turn right a little please, turn right, right a little, or right.", "labels": [], "entities": []}, {"text": "Furthermore, challenges can occur when ASR output is imperfect.", "labels": [], "entities": [{"text": "ASR", "start_pos": 39, "end_pos": 42, "type": "TASK", "confidence": 0.9937785267829895}]}, {"text": "That is, for each dialogue utterance recognized by ASR, the dialogue agent must estimate the human user intent and then to perform the best action that satisfies the user intent.", "labels": [], "entities": [{"text": "ASR", "start_pos": 51, "end_pos": 54, "type": "TASK", "confidence": 0.934890627861023}]}, {"text": "The recognized utterance by ASR is not reliable for decision making.", "labels": [], "entities": [{"text": "ASR", "start_pos": 28, "end_pos": 31, "type": "TASK", "confidence": 0.8631772994995117}, {"text": "decision making", "start_pos": 52, "end_pos": 67, "type": "TASK", "confidence": 0.8672310411930084}]}, {"text": "For instance, the first utterance, u 1 : \"turn right a little\", shows the true user utterance.", "labels": [], "entities": []}, {"text": "The ASR output for this utterance is, \u02dc u 1 : [10 writer little].", "labels": [], "entities": [{"text": "ASR output", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.717600554227829}]}, {"text": "As such, the action performed by SmartWheeler at this dialogue turn is, the general query action u 1 : PLEASE REPEAT YOUR COMMAND.", "labels": [], "entities": [{"text": "PLEASE REPEAT YOUR COMMAND", "start_pos": 103, "end_pos": 129, "type": "METRIC", "confidence": 0.7656566798686981}]}, {"text": "We are working on automatically learning the dialogue strategy by learning the dialogue model components such as user intents, dialogue features, and the agent's reward function from data.", "labels": [], "entities": []}, {"text": "Our data includes dialogues between human users and expert agents (such as caregivers).", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 3: The learned user intents from the SmartWheeler dialogues and their top words. Each percentage  shows the probability of each word given the intent.", "labels": [], "entities": []}]}