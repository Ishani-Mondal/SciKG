{"title": [{"text": "A Systematic Comparison of Smoothing Techniques for Sentence-Level BLEU", "labels": [], "entities": [{"text": "BLEU", "start_pos": 67, "end_pos": 71, "type": "METRIC", "confidence": 0.9098904728889465}]}], "abstractContent": [{"text": "BLEU is the de facto standard machine translation (MT) evaluation metric.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9818525910377502}, {"text": "machine translation (MT) evaluation", "start_pos": 30, "end_pos": 65, "type": "TASK", "confidence": 0.8571907579898834}]}, {"text": "However , because BLEU computes a geometric mean of n-gram precisions, it often correlates poorly with human judgment on the sentence-level.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 18, "end_pos": 22, "type": "METRIC", "confidence": 0.9953385591506958}]}, {"text": "Therefore , several smoothing techniques have been proposed.", "labels": [], "entities": []}, {"text": "This paper systematically compares 7 smoothing techniques for sentence-level BLEU.", "labels": [], "entities": []}, {"text": "Three of them are first proposed in this paper, and they correlate better with human judgments on the sentence-level than other smoothing techniques.", "labels": [], "entities": []}, {"text": "Moreover, we also compare the performance of using the 7 smoothing techniques in statistical machine translation tuning.", "labels": [], "entities": [{"text": "statistical machine translation tuning", "start_pos": 81, "end_pos": 119, "type": "TASK", "confidence": 0.750026747584343}]}], "introductionContent": [{"text": "Since its invention, BLEU () has been the most widely used metric for both machine translation (MT) evaluation and tuning.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 21, "end_pos": 25, "type": "METRIC", "confidence": 0.9989768266677856}, {"text": "machine translation (MT) evaluation", "start_pos": 75, "end_pos": 110, "type": "TASK", "confidence": 0.8625007967154185}]}, {"text": "Many other metrics correlate better with human judgments of translation quality than BLEU, as shown in recent WMT Evaluation Task reports).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 85, "end_pos": 89, "type": "METRIC", "confidence": 0.9980190992355347}, {"text": "WMT Evaluation Task", "start_pos": 110, "end_pos": 129, "type": "TASK", "confidence": 0.6981487274169922}]}, {"text": "However, BLEU remains the de facto standard evaluation and tuning metric.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 9, "end_pos": 13, "type": "METRIC", "confidence": 0.9983741044998169}]}, {"text": "This is probably due to the following facts: 1.", "labels": [], "entities": []}, {"text": "BLEU is language independent (except for word segmentation decisions).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9154773950576782}, {"text": "word segmentation", "start_pos": 41, "end_pos": 58, "type": "TASK", "confidence": 0.7388694733381271}]}, {"text": "2. BLEU can be computed quickly.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 3, "end_pos": 7, "type": "METRIC", "confidence": 0.9986629486083984}]}, {"text": "This is important when choosing a tuning metric.", "labels": [], "entities": []}, {"text": "3. BLEU seems to be the best tuning metric from a quality point of view -i.e., models trained using BLEU obtain the highest scores from humans and even from other metrics.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 3, "end_pos": 7, "type": "METRIC", "confidence": 0.998944103717804}, {"text": "BLEU", "start_pos": 100, "end_pos": 104, "type": "METRIC", "confidence": 0.9905201196670532}]}, {"text": "One of the main criticisms of BLEU is that it has a poor correlation with human judgments on the sentence-level.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 30, "end_pos": 34, "type": "METRIC", "confidence": 0.9891439080238342}]}, {"text": "Because it computes a geometric mean of n-gram precisions, if a higher order n-gram precision (eg. n = 4) of a sentence is 0, then the BLEU score of the entire sentence is 0, no matter how many 1-grams or 2-grams are matched.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 135, "end_pos": 145, "type": "METRIC", "confidence": 0.9866208434104919}]}, {"text": "Therefore, several smoothing techniques for sentence-level BLEU have been proposed (.", "labels": [], "entities": []}, {"text": "In this paper, we systematically compare 7 smoothing techniques for sentence-level BLEU.", "labels": [], "entities": []}, {"text": "Three of them are first proposed in this paper, and they correlate better with human judgments on the sentence-level than other smoothing techniques on the WMT metrics task.", "labels": [], "entities": [{"text": "WMT metrics task", "start_pos": 156, "end_pos": 172, "type": "TASK", "confidence": 0.5533397098382314}]}, {"text": "Moreover, we compare the performance of using the 7 smoothing techniques in statistical machine translation tuning on NIST Chinese-to-English and Arabic-to-English tasks.", "labels": [], "entities": [{"text": "statistical machine translation tuning", "start_pos": 76, "end_pos": 114, "type": "TASK", "confidence": 0.7421528995037079}, {"text": "NIST", "start_pos": 118, "end_pos": 122, "type": "DATASET", "confidence": 0.8977251648902893}]}, {"text": "We show that when tuning optimizes the expected sum of these sentence-level metrics (as advocated by and among others), all of these metrics perform similarly in terms of their ability to produce strong BLEU scores on a held-out test set.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 203, "end_pos": 207, "type": "METRIC", "confidence": 0.9970597624778748}]}], "datasetContent": [{"text": "We carried out two series of experiments.", "labels": [], "entities": []}, {"text": "the metric task as evaluation metrics, then they were compared as metrics for tuning SMT systems to maximize the sum of expected sentence-level BLEU scores.", "labels": [], "entities": [{"text": "SMT", "start_pos": 85, "end_pos": 88, "type": "TASK", "confidence": 0.951468288898468}, {"text": "BLEU", "start_pos": 144, "end_pos": 148, "type": "METRIC", "confidence": 0.9455457925796509}]}, {"text": "We first compare the correlations with human judgment for the 7 smoothing techniques on WMT data; the development set (dev) is the WMT 2008 all-to-English data; the test sets are the WMT 2012 and WMT 2013 all-to-English, and English-to-all submissions.", "labels": [], "entities": [{"text": "WMT data", "start_pos": 88, "end_pos": 96, "type": "DATASET", "confidence": 0.9570929110050201}, {"text": "WMT 2008 all-to-English data", "start_pos": 131, "end_pos": 159, "type": "DATASET", "confidence": 0.9378842562437057}, {"text": "WMT 2012 and WMT 2013", "start_pos": 183, "end_pos": 204, "type": "DATASET", "confidence": 0.8450416684150696}]}, {"text": "The languages \"all\" (\"xx\" in Table 1) include French, Spanish, German, Czech and Russian.", "labels": [], "entities": []}, {"text": "We extract all pairwise comparisons where one system's translation of a particular segment was judged to be better than the other system's translation, i.e., we removed all tied human judgments fora particular segment.", "labels": [], "entities": []}, {"text": "If two translations fora particular segment are assigned the same BLEU score, then the #concordant-pairs and #discordant-pairs both get a half count.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 66, "end_pos": 76, "type": "METRIC", "confidence": 0.9855846762657166}]}, {"text": "In this way, we can keep the number of total pairs consistent for all different smoothing techniques.", "labels": [], "entities": []}, {"text": "For the system-level, we used Spearman's rank correlation coefficient \u03c1 and Pearson's correlation coefficient \u03b3 to measure the correlation of the metric with human judgments of translation.", "labels": [], "entities": [{"text": "rank correlation coefficient \u03c1", "start_pos": 41, "end_pos": 71, "type": "METRIC", "confidence": 0.8029936477541924}, {"text": "Pearson's correlation coefficient \u03b3", "start_pos": 76, "end_pos": 111, "type": "METRIC", "confidence": 0.8843338847160339}]}, {"text": "If we compute document-level BLEU as usual, all 7 smoothing techniques actually get the same result, as document-level BLEU does not need smoothing.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 29, "end_pos": 33, "type": "METRIC", "confidence": 0.8008096814155579}]}, {"text": "We therefore compute the documentlevel BLEU as the weighted average of sentencelevel BLEU, with the weights being the reference  lengths: where BLEU i is the BLEU score of sentence i, and Dis the size of the document in sentences.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 39, "end_pos": 43, "type": "METRIC", "confidence": 0.89231938123703}, {"text": "BLEU", "start_pos": 85, "end_pos": 89, "type": "METRIC", "confidence": 0.7572897672653198}, {"text": "BLEU", "start_pos": 144, "end_pos": 148, "type": "METRIC", "confidence": 0.9956485629081726}, {"text": "BLEU score", "start_pos": 158, "end_pos": 168, "type": "METRIC", "confidence": 0.978510320186615}, {"text": "Dis", "start_pos": 188, "end_pos": 191, "type": "METRIC", "confidence": 0.9676881432533264}]}, {"text": "We first set the free parameters of each smoothing method by grid search to optimize the sentence-level score on the dev set.", "labels": [], "entities": []}, {"text": "We set \u01eb to 0.1 for Smoothing 1; K = 5 for Smoothing 4; \u03b1 = 5 for Smoothing 6.", "labels": [], "entities": [{"text": "\u01eb", "start_pos": 7, "end_pos": 8, "type": "METRIC", "confidence": 0.9807253479957581}]}, {"text": "report our results on the metrics task.", "labels": [], "entities": []}, {"text": "We compared the 7 smoothing techniques described in Section 2.2 to a baseline with no smoothing (Smoothing 0).", "labels": [], "entities": []}, {"text": "All scores match ngrams n = 1 to 4.", "labels": [], "entities": []}, {"text": "Smoothing 3 is implemented in the standard official NIST evaluation toolkit (mteval-v13a.pl).", "labels": [], "entities": [{"text": "Smoothing", "start_pos": 0, "end_pos": 9, "type": "TASK", "confidence": 0.9211633205413818}, {"text": "NIST evaluation toolkit", "start_pos": 52, "end_pos": 75, "type": "DATASET", "confidence": 0.8640420436859131}]}, {"text": "Results are averaged across the 4 test sets.", "labels": [], "entities": []}, {"text": "All smoothing techniques improved sentencelevel correlations (\u03c4 ) over no smoothing.", "labels": [], "entities": [{"text": "sentencelevel correlations (\u03c4 )", "start_pos": 34, "end_pos": 65, "type": "METRIC", "confidence": 0.8092016994953155}]}, {"text": "Smoothing method 7 got the best sentence-level results on both the Into-English and Out-of-English tasks.", "labels": [], "entities": []}, {"text": "On the system-level, our weighted average of sentence-level BLEU scores (see Equation 13) achieved a better correlation with human judgement than the original IBM corpus-level BLEU.", "labels": [], "entities": []}, {"text": "However, the choice of which smoothing technique is used in the average did not make a very big difference; in particular, the system-level rank correlation \u03c1 did not change for 13 out of 14 cases.", "labels": [], "entities": [{"text": "system-level rank correlation \u03c1", "start_pos": 127, "end_pos": 158, "type": "METRIC", "confidence": 0.6582021415233612}]}, {"text": "These methods help when comparing one hypothesis to another, but taken as apart of a larger average, all seven methods assign relatively low scores  to the cases that require smoothing, resulting in similar system-level rankings.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of the WMT dev and test sets.", "labels": [], "entities": [{"text": "WMT dev and test sets", "start_pos": 28, "end_pos": 49, "type": "DATASET", "confidence": 0.7837066888809204}]}, {"text": " Table 2: Correlations with human judgment on  WMT data for Into-English task. Results are av- eraged on 4 test sets. \"crp\" is the origianl IBM  corpus-level BLEU.", "labels": [], "entities": [{"text": "WMT data", "start_pos": 47, "end_pos": 55, "type": "DATASET", "confidence": 0.8200325965881348}, {"text": "BLEU", "start_pos": 158, "end_pos": 162, "type": "METRIC", "confidence": 0.594896674156189}]}, {"text": " Table 3: Correlations with human judgment on  WMT data for Out-of-English task. Results are  averaged on 4 test sets. \"crp\" is the origianl IBM  corpus-level BLEU.", "labels": [], "entities": [{"text": "WMT data", "start_pos": 47, "end_pos": 55, "type": "DATASET", "confidence": 0.8470529615879059}, {"text": "BLEU", "start_pos": 159, "end_pos": 163, "type": "METRIC", "confidence": 0.6991170644760132}]}, {"text": " Table 4: Statistics of the NIST Chinese-English  and Arabic-English data.", "labels": [], "entities": [{"text": "NIST Chinese-English  and Arabic-English data", "start_pos": 28, "end_pos": 73, "type": "DATASET", "confidence": 0.9194947838783264}]}, {"text": " Table 5: Chinese-to-English Results for the small  feature set tuning task. Results are averaged across  5 replications; std is the standard deviation.", "labels": [], "entities": [{"text": "small  feature set tuning task", "start_pos": 45, "end_pos": 75, "type": "TASK", "confidence": 0.6265201568603516}]}]}