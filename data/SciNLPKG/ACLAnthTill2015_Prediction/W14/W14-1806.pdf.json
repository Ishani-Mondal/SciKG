{"title": [{"text": "The pragmatics of margin comments: An empirical study", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper describes the design and rationale behind a classification scheme for En-glish margin comments.", "labels": [], "entities": []}, {"text": "The scheme's design was informed by pragmatics and pedagogy theory, and by observations made from a corpus of 24,387 margin comments from assessed university assignments.", "labels": [], "entities": []}, {"text": "The purpose of the scheme is to computation-ally explore content and form relationships between margin comments and the passages to which they point.", "labels": [], "entities": []}, {"text": "The process of designing the scheme resulted in the conclusion that margin comments require more work to understand than utterances do, and that they are more prone to being misunderstood.", "labels": [], "entities": []}], "introductionContent": [{"text": "We have a collection of 24,387 real margin comments, expressed in English, which we want to exploit through machine learning in order to inform the design of an automatic margin comments generator.", "labels": [], "entities": []}, {"text": "The corpus margin comments were added by humans to a corpus of real assessed university assignments.", "labels": [], "entities": []}, {"text": "The assignments were argumentative essays submitted towards a Master's degree in Education.", "labels": [], "entities": []}, {"text": "We have designed a margin comment classification scheme which classifies natural language (NL) margin comments without reference to the essay parts to which they point.", "labels": [], "entities": [{"text": "margin comment classification", "start_pos": 19, "end_pos": 48, "type": "TASK", "confidence": 0.6368916134039561}, {"text": "classifies natural language (NL) margin comments", "start_pos": 62, "end_pos": 110, "type": "TASK", "confidence": 0.6478886902332306}]}, {"text": "High interannotator agreement scores have been achieved for the scheme.", "labels": [], "entities": []}, {"text": "We plan to use the scheme to look for relationships between the corpus comments and the essay parts to which they point.", "labels": [], "entities": []}, {"text": "This paper is about the classification scheme's design, including what led to the design decisions, which were informed by examination of the margin comments, the assignments corpus, and consideration of key ideas in pragmatics and pedagogy.", "labels": [], "entities": []}, {"text": "A feature of margin comments that became clear during the design process, and that influenced the design, is that margin comments are harder to understand and are more prone to being misunderstood than conversational utterances.", "labels": [], "entities": []}], "datasetContent": [{"text": "We have demonstrated that the classification scheme can be deployed with high agreement levels between independent annotators.", "labels": [], "entities": []}, {"text": "Agreement by two annotators was calculated for 313 sample comments that were annotated by each annotator independently.", "labels": [], "entities": [{"text": "Agreement", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9064052104949951}]}, {"text": "Annotator A designed the scheme over several months.", "labels": [], "entities": [{"text": "Annotator", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.9089723825454712}]}, {"text": "Annotator B spent about 50 minutes learning the scheme (from no prior exposure to it).", "labels": [], "entities": []}, {"text": "Annotator B took a mean average of 1.1 minutes to fully annotate each comment in the sample.", "labels": [], "entities": []}, {"text": "Annotator A took a mean average of .49 minutes to fully annotate each comment.", "labels": [], "entities": []}, {"text": "The corpus comprised 1,408 essays submitted for 13 different assessed university Master's modules, the official word limits of which ranged from 500 to 4,000.", "labels": [], "entities": []}, {"text": "The essays had been marked by 20 different markers.", "labels": [], "entities": []}, {"text": "The number of essays marked by each marker varied.", "labels": [], "entities": []}, {"text": "The mean average number of comments per essay per marker ranged from 4.83 to 47.00.", "labels": [], "entities": []}, {"text": "To avoid potential bias towards the more prolific markers' styles, the same number of essays were randomly sampled for each marker (where possible), and approximately the same number of comments were randomly sampled from each of those essays.", "labels": [], "entities": []}, {"text": "Some tutors appear to prefer very short comments, some long.", "labels": [], "entities": []}, {"text": "For some (but not all) of the tutors who marked essays of different lengths, there was a correlation between essay length and the number of margin comments.", "labels": [], "entities": []}, {"text": "No analysis of linguistic style similarities across comments within individual essays was carried out for this paper.", "labels": [], "entities": []}, {"text": "Inter-annotator agreement was calculated using Cohen's Kappa for each of the three layers of the scheme independently.", "labels": [], "entities": []}, {"text": "95% confidence intervals (CI) for test statistics were generated through 10,000 statistical bootstrappings of the annotated comments.", "labels": [], "entities": [{"text": "confidence intervals (CI)", "start_pos": 4, "end_pos": 29, "type": "METRIC", "confidence": 0.9352924346923828}]}, {"text": "The agreement coefficient for the attitude layer was 0.874 (95% CI, 0.831-0.914), for the target layer was 0.791 (0.734-0.844), and for the linguistic act layer was 0.822 (0.770-0.869).", "labels": [], "entities": [{"text": "agreement coefficient", "start_pos": 4, "end_pos": 25, "type": "METRIC", "confidence": 0.9711761176586151}]}, {"text": "The percentage agreement across all three layers was 72.1% (67.0%-77.0%) (the percentage of comments for which both annotators were in agreement on all three layers).", "labels": [], "entities": [{"text": "agreement", "start_pos": 15, "end_pos": 24, "type": "METRIC", "confidence": 0.8099173307418823}]}, {"text": "There were no occurrences of comments which both annotators deemed unclassifiable.", "labels": [], "entities": []}, {"text": "One of the comments was deemed unclassifiable by one annotator.", "labels": [], "entities": []}, {"text": "The scheme has five attitude+target cross-layer dependencies (Engage+Author, Thank+Author, Refer+Context-Dependent, Exclaim+Context-Dependent, Dispute+Argument), and five target+act cross-layer dependencies (each of the same five pairs plus a linguistic act).", "labels": [], "entities": []}, {"text": "We acknowledge that these might argue fora more complex agreement calculation.", "labels": [], "entities": [{"text": "agreement calculation", "start_pos": 56, "end_pos": 77, "type": "TASK", "confidence": 0.6653038859367371}]}, {"text": "It is expected that some linguistic act categories are unlikely to combine with some attitude categories, though this requires empirical verification.", "labels": [], "entities": []}, {"text": "A conservative estimate of the number of possible combinations of attitude, target, and act that we believe might be found in the corpus is 155 combinations.", "labels": [], "entities": []}, {"text": "Additionally, some categories from a given layer appear to be more frequent than other categories from the same layer.", "labels": [], "entities": []}, {"text": "We acknowledge, therefore, that a weighted coefficient method maybe more suitable for calculating inter-annotator agreement.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Skills-related terms in comments corpus", "labels": [], "entities": []}]}