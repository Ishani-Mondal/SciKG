{"title": [{"text": "Detecting Health Related Discussions in Everyday Telephone Conversations for Studying Medical Events in the Lives of Older Adults", "labels": [], "entities": [{"text": "Detecting Health Related Discussions in Everyday Telephone Conversations", "start_pos": 0, "end_pos": 72, "type": "TASK", "confidence": 0.8654634356498718}]}], "abstractContent": [{"text": "We apply semi-supervised topic modeling techniques to detect health-related discussions in everyday telephone conversations, which has applications in large-scale epidemiological studies and for clinical interventions for older adults.", "labels": [], "entities": [{"text": "detect health-related discussions in everyday telephone conversations", "start_pos": 54, "end_pos": 123, "type": "TASK", "confidence": 0.6629381605557033}]}, {"text": "The privacy requirements associated with utilizing everyday telephone conversations preclude manual annotations; hence, we explore semi-supervised methods in this task.", "labels": [], "entities": []}, {"text": "We adopt a semi-supervised version of Latent Dirichlet Allocation (LDA) to guide the learning process.", "labels": [], "entities": [{"text": "Latent Dirichlet Allocation (LDA)", "start_pos": 38, "end_pos": 71, "type": "METRIC", "confidence": 0.8375111222267151}]}, {"text": "Within this framework, we investigate a strategy to discard irrelevant words in the topic distribution and demonstrate that this strategy improves the average F-score on the in-domain task and an out-of-domain task (Fisher corpus).", "labels": [], "entities": [{"text": "F-score", "start_pos": 159, "end_pos": 166, "type": "METRIC", "confidence": 0.9967616200447083}, {"text": "Fisher corpus)", "start_pos": 216, "end_pos": 230, "type": "DATASET", "confidence": 0.8696844379107157}]}, {"text": "Our results show that the increase in discussion of health related conversations is statistically associated with actual medical events obtained through weekly self-reports.", "labels": [], "entities": []}], "introductionContent": [{"text": "There has been considerable interest in understanding, promoting, and monitoring healthy lifestyles among older adults while minimizing the frequency of clinical visits.", "labels": [], "entities": []}, {"text": "Longitudinal studies on large cohorts are necessary, for example, to understand the association between social networks, depression, dementia, and general health.", "labels": [], "entities": []}, {"text": "In this context, detecting discussions of health are important as indicators of under-reported health events in daily lives as well as for studying healthy social support networks.", "labels": [], "entities": [{"text": "detecting discussions of health", "start_pos": 17, "end_pos": 48, "type": "TASK", "confidence": 0.9034742712974548}]}, {"text": "The detection of medical events such as higher levels of pain or discomfort may also be useful in providing timely clinical intervention for managing chronic illness and thus promoting healthy independent living among older adults.", "labels": [], "entities": []}, {"text": "Motivated by this larger goal, we develop and investigate techniques for identifying conversations containing any health related discussion.", "labels": [], "entities": [{"text": "identifying conversations containing any health related discussion", "start_pos": 73, "end_pos": 139, "type": "TASK", "confidence": 0.7742613468851361}]}, {"text": "We are interested in detecting discussions about medication with doctors, as well as conversations with others, where among all different topics being discussed, subjects may also be complaining about pain or changes in health status.", "labels": [], "entities": [{"text": "detecting discussions about medication", "start_pos": 21, "end_pos": 59, "type": "TASK", "confidence": 0.853016197681427}]}, {"text": "The privacy concerns of recording and analyzing everyday telephone conversation prevents us from manually transcribing and annotating conversations.", "labels": [], "entities": []}, {"text": "So, we automatically transcribe the conversations using an automatic speech recognition system and look-up the telephone number corresponding to each conversation as a heuristic means of deriving labels.", "labels": [], "entities": []}, {"text": "This technique is suitable for labeling a small subset of the conversations that are only sufficient for developing semisupervised algorithms and for evaluating the methods for analysis.", "labels": [], "entities": []}, {"text": "Before delving into our approach, we discuss a few relevant and related studies in Section 2 and describe our unique naturalistic corpus in Section 3.", "labels": [], "entities": []}, {"text": "Given the restrictive nature of our labeled in-domain data set, we are interested in a classifier that generalizes to the unlabeled data.", "labels": [], "entities": []}, {"text": "We evaluate the generalizability of the classifiers using an out-of-domain corpus.", "labels": [], "entities": []}, {"text": "We adopt a semisupervised topic modeling approach to address our task, and develop an iterative feature selection method to improve our classifier, as described in Section 4.", "labels": [], "entities": []}, {"text": "We evaluate the efficacy of our approach empirically, on the in-domain as well as an out-of-domain corpus, and report results in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "In all of our experiments, we trained SVM classifiers, with different features, to detect the conversations on health using the popular lib-SVM (Chang and Lin, 2011) implementation.", "labels": [], "entities": []}, {"text": "We chose the parameters of the SVM using a 30-fold cross-validated (CV) grid search over the training data.", "labels": [], "entities": [{"text": "SVM", "start_pos": 31, "end_pos": 34, "type": "TASK", "confidence": 0.9066761136054993}]}, {"text": "We also used a 4-fold cross validation over the labeled set of conversations to maximize the use of the relatively small labeled set.", "labels": [], "entities": []}, {"text": "That is, we trained the feature selection algorithm on 3-folds and tested the resulting SVM tested on the fourth.", "labels": [], "entities": [{"text": "feature selection", "start_pos": 24, "end_pos": 41, "type": "TASK", "confidence": 0.7342133224010468}]}, {"text": "In in-domain task we always report the average performance across the folds.", "labels": [], "entities": []}, {"text": "shows the results of our experiments using different input features.", "labels": [], "entities": []}, {"text": "We report on recall, precision and F-measure in in-domain and out-of-domain (Fisher) task as well as on average F-measure of the two.", "labels": [], "entities": [{"text": "recall", "start_pos": 13, "end_pos": 19, "type": "METRIC", "confidence": 0.9994567036628723}, {"text": "precision", "start_pos": 21, "end_pos": 30, "type": "METRIC", "confidence": 0.9996123909950256}, {"text": "F-measure", "start_pos": 35, "end_pos": 44, "type": "METRIC", "confidence": 0.9977076053619385}, {"text": "F-measure", "start_pos": 112, "end_pos": 121, "type": "METRIC", "confidence": 0.9947031140327454}]}, {"text": "The justification for considering the average F-measure is that we want our algorithm to work well on both in-domain corpus and Fisher corpus since we need to make sure that our classifier is generalizable (i.e. it works well on Fisher) and it works well on the private and natural telephone conversations (i.e. the ones similar: Expanding the set of seed words: in each iteration, the current seed words are forced into the topic of health to guide TLDA towards finding more health related words.", "labels": [], "entities": []}, {"text": "The candidate set consists of the 50 most probable words of the topic of health in TLDA.", "labels": [], "entities": []}, {"text": "We investigate the gain of adding each word of the candidate set to the seed words by temporarily adding it to the seed words and looking at the average of precision and recall on the training set fora classifier that classifies a conversation as health-related if and only if it contains at least one of the seed words.", "labels": [], "entities": [{"text": "precision", "start_pos": 156, "end_pos": 165, "type": "METRIC", "confidence": 0.9995211362838745}, {"text": "recall", "start_pos": 170, "end_pos": 176, "type": "METRIC", "confidence": 0.9982271790504456}]}, {"text": "We select the words that maximize this objective and add them to the seed words until no other words contributes to the average precision and recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 128, "end_pos": 137, "type": "METRIC", "confidence": 0.9994006156921387}, {"text": "recall", "start_pos": 142, "end_pos": 148, "type": "METRIC", "confidence": 0.9972512125968933}]}, {"text": "to the in-domain corpus) When using the full vocabulary, the in-domain performance (the performance on the everyday telephone conversations data) is relatively good with 75.1% recall and 83.5% precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 176, "end_pos": 182, "type": "METRIC", "confidence": 0.9995050430297852}, {"text": "precision", "start_pos": 193, "end_pos": 202, "type": "METRIC", "confidence": 0.998828113079071}]}, {"text": "But the out-of-domain recall (recall on the Fisher data set) is considerably low at 2.8%.", "labels": [], "entities": [{"text": "recall", "start_pos": 22, "end_pos": 28, "type": "METRIC", "confidence": 0.9886723160743713}, {"text": "recall", "start_pos": 30, "end_pos": 36, "type": "METRIC", "confidence": 0.9951457381248474}, {"text": "Fisher data set", "start_pos": 44, "end_pos": 59, "type": "DATASET", "confidence": 0.9529087344805399}]}, {"text": "Ideally, we want a classifier that performs well in both domains.", "labels": [], "entities": []}, {"text": "Rows 2 to 5 can be seen as steps to get to such a classifier.", "labels": [], "entities": []}, {"text": "The second row shows the performance of the other extreme end of feature selection: the features include the manually chosen words doctor, medicine, and pain only.", "labels": [], "entities": []}, {"text": "While this leads to very good out-of-domain performance, the in-domain recall has dropped considerably.", "labels": [], "entities": [{"text": "recall", "start_pos": 71, "end_pos": 77, "type": "METRIC", "confidence": 0.9905678629875183}]}, {"text": "We trained TLDA 30 times, and selected the 50 most probable words in the health topic.", "labels": [], "entities": []}, {"text": "The third row in shows the average performance of SVM when using the tf-idf of these sets of words as the feature vector on in-domain and out-of-domain tasks.", "labels": [], "entities": []}, {"text": "Using the 50 most probable words in health topic significantly improves average F-score (71%) across: Performance of SVM classifiers using different feature selection methods.", "labels": [], "entities": [{"text": "F-score", "start_pos": 80, "end_pos": 87, "type": "METRIC", "confidence": 0.9976640939712524}]}, {"text": "The In-Domain task involves the everyday telephone conversations corpus.", "labels": [], "entities": []}, {"text": "We call Fisher corpus out of domain, because no example of this corpus was used in training.", "labels": [], "entities": []}, {"text": "both tasks over using the full vocabulary (42.3%) but it is clear that this is only due to improvement in out-of-domain task.", "labels": [], "entities": []}, {"text": "shows one set of the 50 most probable words in health topic,the result of one run of TLDA.", "labels": [], "entities": [{"text": "TLDA", "start_pos": 85, "end_pos": 89, "type": "DATASET", "confidence": 0.725765585899353}]}, {"text": "Evidently, these words contain many irrelevant words.", "labels": [], "entities": []}, {"text": "This is the motivation for our iterative algorithm.", "labels": [], "entities": []}, {"text": "Next, we evaluate the performance of our iterative algorithm.", "labels": [], "entities": []}, {"text": "The fourth row in shows the average performance of SVM using expanded seed words that our algorithm suggested in 30 runs.", "labels": [], "entities": []}, {"text": "Our algorithm improves the average F-score by 3% comparing to the standard TLDA.", "labels": [], "entities": [{"text": "F-score", "start_pos": 35, "end_pos": 42, "type": "METRIC", "confidence": 0.9979956150054932}, {"text": "TLDA", "start_pos": 75, "end_pos": 79, "type": "DATASET", "confidence": 0.754768967628479}]}, {"text": "This is due to a 5% improvement in out-of-domain task as opposed to a 0.3% performance decrease in indomain task.", "labels": [], "entities": []}, {"text": "Since our algorithm has a probabilistic topic modeling component (i.e. TLDA), different runs lead to different sets of expanded seed words.", "labels": [], "entities": []}, {"text": "We extract a union of all the words chosen over 30 runs and evaluate the performance of SVM using this union set.", "labels": [], "entities": []}, {"text": "This improves the performance of our method further to achieve the best average Fscore of 78.3%, which is an 85% improvement over using the SVM with full vocabulary.", "labels": [], "entities": [{"text": "Fscore", "start_pos": 80, "end_pos": 86, "type": "METRIC", "confidence": 0.9944795370101929}]}, {"text": "It is important to notice that the in-domain performance is still lower than the full-vocabulary baseline by less than 3% while the out-of-domain performance is the best obtained.", "labels": [], "entities": []}, {"text": "Once again, we are more interested in the average F-measure because we need our algorithm to generalize well (work well on out-of-domain corpus) and to work well on natural private conversations (on the conversations similar to the on-domain corpus).", "labels": [], "entities": [{"text": "F-measure", "start_pos": 50, "end_pos": 59, "type": "METRIC", "confidence": 0.9003552198410034}]}, {"text": "The bold words are the ones are hand-picked.", "labels": [], "entities": []}, {"text": "As mentioned in Section 3, we have access to weekly self-reports on medical events for subjects' in everyday telephone conversations corpus.", "labels": [], "entities": []}, {"text": "We used our best classifier, the SVM with union of expanded seed words, to classify all the conversations in our corpus into healthcontaining and health-free conversations.", "labels": [], "entities": []}, {"text": "We then mark each conversation as temporally near a medical event if a reported medical event occurred within a 3-week time window.", "labels": [], "entities": []}, {"text": "We chose a 3-week window to allow for one report before and after the event.", "labels": [], "entities": []}, {"text": "shows the number of conversations in different categories.", "labels": [], "entities": []}, {"text": "At first glance it might seem like the number of false positives or false negatives is quite large but we should notice that being near a medical event is not the ground truth here.", "labels": [], "entities": []}, {"text": "We just want to see if there is any association between occurrence of health-related conversations and occurrence of an actual medical event in lives of our subjects.", "labels": [], "entities": []}, {"text": "We can see that 90.9% of the conversations are classified as health-related but this percentage is slightly different for conversations near medical events(91.5%) vs. for the other conversations (89.1).", "labels": [], "entities": []}, {"text": "This slight difference is significant according to \u03c7 2 test of independence (\u03c7 2 (df = 1, N = 47288) = 61.17, p < 0.001).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Performance of SVM classifiers using different feature selection methods. The In-Domain task  involves the everyday telephone conversations corpus. We call Fisher corpus out of domain, because no  example of this corpus was used in training.", "labels": [], "entities": [{"text": "SVM classifiers", "start_pos": 25, "end_pos": 40, "type": "TASK", "confidence": 0.8869948387145996}]}, {"text": " Table 3: Number of telephone conversations in  different categories. Each conversation is consid- ered near a medical even if and only if there is  at least one self-report in a window of 3 weeks  around its date. Being near a medical event does  not reveal the true nature of the conversation and  thus is not the ground truth. So, there are no false  positive, true positive, etc. in this table.", "labels": [], "entities": []}]}