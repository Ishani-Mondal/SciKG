{"title": [{"text": "Similarity-Based Non-Scorable Response Detection for Automated Speech Scoring", "labels": [], "entities": [{"text": "Similarity-Based Non-Scorable Response Detection", "start_pos": 0, "end_pos": 48, "type": "TASK", "confidence": 0.7841191738843918}, {"text": "Automated Speech Scoring", "start_pos": 53, "end_pos": 77, "type": "TASK", "confidence": 0.6310223539670309}]}], "abstractContent": [{"text": "This study provides a method that identifies problematic responses which make automated speech scoring difficult.", "labels": [], "entities": [{"text": "automated speech scoring", "start_pos": 78, "end_pos": 102, "type": "TASK", "confidence": 0.6192273795604706}]}, {"text": "When automated scoring is used in the context of a high stakes language proficiency assessment , for which the scores are used to make consequential decisions, some test takers may have an incentive to try to game the system in order to artificially inflate their scores.", "labels": [], "entities": []}, {"text": "Since many automated proficiency scoring systems use fluency features such as speaking rate as one of the important features, students may engage in strategies designed to manipulate their speaking rate as measured by the system.", "labels": [], "entities": []}, {"text": "In order to address this issue, we developed a method which filters out non-scorable responses based on text similarity measures.", "labels": [], "entities": []}, {"text": "Given a test response, the method generated a set of features which calculated the topic similarity with the prompt question or the sample responses including relevant content.", "labels": [], "entities": []}, {"text": "Next, an automated filter which identified these problematic responses was implemented using the similarity features.", "labels": [], "entities": []}, {"text": "This filter improved the performance of the baseline filter in identifying responses with topic problems.", "labels": [], "entities": []}], "introductionContent": [{"text": "In spoken language proficiency assessment, some responses may include sub-optimal characteristics which make it difficult for the automated scoring system to provide a valid score.", "labels": [], "entities": [{"text": "spoken language proficiency assessment", "start_pos": 3, "end_pos": 41, "type": "TASK", "confidence": 0.7288749665021896}]}, {"text": "For instance, some test takers may try to game the system by speaking in their native languages or by citing memorized responses for unrelated topics.", "labels": [], "entities": []}, {"text": "Others may repeat questions or part of questions with modifications instead of generating his/her own response.", "labels": [], "entities": []}, {"text": "Hereafter, we call these problematic responses non-scorable (NS) responses.", "labels": [], "entities": []}, {"text": "By using these strategies, test takers can generate fluent speech, and the automated proficiency scoring system, which utilizes fluency as one of the important factors, may assign a high score.", "labels": [], "entities": []}, {"text": "In order to address this issue, the automated proficiency scoring system in this study used a two-step approach: these problematic responses were filtered out by a \"filtering model,\" and only the remaining responses were scored using the automated scoring model.", "labels": [], "entities": []}, {"text": "By filtering out these responses, the robustness of the automated scoring system can be improved.", "labels": [], "entities": []}, {"text": "The proportion of NS responses, in the assessment of which the responses are scored by human raters, are likely to below.", "labels": [], "entities": []}, {"text": "For instance, the proportion of NS responses in the international English language assessment used in this study was 2%.", "labels": [], "entities": []}, {"text": "Despite this low proportion, it is a serious problem which has a strong impact on the validity of the test.", "labels": [], "entities": [{"text": "validity", "start_pos": 86, "end_pos": 94, "type": "METRIC", "confidence": 0.9963855743408203}]}, {"text": "In addition, the likelihood of students engaging in gaming strategies may increase with the use of automated scoring.", "labels": [], "entities": []}, {"text": "Therefore, an automated filtering model with a high accuracy is a necessary step to use the automated scoring system as a sole rater.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.9975470900535583}]}, {"text": "Both off-topic and copy responses have topicrelated problems, although they are at the two extremes in the degree of similarity.", "labels": [], "entities": []}, {"text": "Focusing on the intermediate levels of similarity, presented a hierarchy of five similarity levels: unrelated, on the general topic, on the specific topic, same facts, and copied.", "labels": [], "entities": []}, {"text": "In the automated scoring of spontaneous speech, responses that fell into unrelated can be considered as offtopic, while the ones that fell into copied can be considered as repetition or plagiarism.", "labels": [], "entities": []}, {"text": "Following this approach, we developed a non-scorable response identification method utilizing similar- ity measures.", "labels": [], "entities": [{"text": "response identification", "start_pos": 53, "end_pos": 76, "type": "TASK", "confidence": 0.7240627706050873}]}, {"text": "We will show that this similarity based method is highly efficient in identifying offtopic or repetition responses.", "labels": [], "entities": []}, {"text": "Furthermore, we will show that the method can effectively detect NS responses that are not directly related to the topicality issue (e.g, non-English responses).", "labels": [], "entities": []}, {"text": "shows the overall architecture of our method including the automated speech proficiency scoring system.", "labels": [], "entities": []}, {"text": "For a given spoken response, the system performs speech processing including speech recognition and generates a word hypotheses and time stamps.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 77, "end_pos": 95, "type": "TASK", "confidence": 0.7087065875530243}]}, {"text": "In addition, the system computes pitch and power; the system calculates descriptive statistics such as the mean and standard deviation of pitch and power at both the word level and response level.", "labels": [], "entities": []}, {"text": "Given the word hypotheses and descriptive features of pitch/power, it derives features for automated proficiency scoring.", "labels": [], "entities": [{"text": "proficiency scoring", "start_pos": 101, "end_pos": 120, "type": "TASK", "confidence": 0.7175971269607544}]}, {"text": "In addition, the similarity features are generated based on the word hypotheses and topic models.", "labels": [], "entities": []}, {"text": "Finally, given both sets of features, the filtering model filters out non-scorable responses, and the remainder of the responses are scored using a scoring model.", "labels": [], "entities": []}, {"text": "A detailed description of the system is available from.", "labels": [], "entities": []}, {"text": "In this study, we will only focus on the filtering model.", "labels": [], "entities": []}, {"text": "This paper will proceed as follows: we first review previous studies in section 2, then describe the data in section 3, and present the method and experiment set-up in sections 4 and 5.", "labels": [], "entities": []}, {"text": "The results and discussion are presented in section 6, and the conclusions are presented in section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "An HMM-based speech recognizer was trained using the ASR set.", "labels": [], "entities": [{"text": "HMM-based speech recognizer", "start_pos": 3, "end_pos": 30, "type": "TASK", "confidence": 0.6311246653397878}, {"text": "ASR set", "start_pos": 53, "end_pos": 60, "type": "DATASET", "confidence": 0.6902634799480438}]}, {"text": "A gender independent triphone acoustic model and a combination of bigram, tri-gram, and four-gram language models were used.", "labels": [], "entities": []}, {"text": "A word error rate (WER) of 27% on the held-out test dataset was observed.", "labels": [], "entities": [{"text": "word error rate (WER)", "start_pos": 2, "end_pos": 23, "type": "METRIC", "confidence": 0.9157211283842722}]}, {"text": "For each response in the FM set, the word hypotheses was generated using this recognizer.", "labels": [], "entities": []}, {"text": "From this ASR-based transcription, the six similarity features were generated.", "labels": [], "entities": [{"text": "ASR-based", "start_pos": 10, "end_pos": 19, "type": "TASK", "confidence": 0.9779189229011536}]}, {"text": "In addition, the 30 A/S features described in 4.3 were generated.", "labels": [], "entities": []}, {"text": "Using these two sets of features, filtering models were trained using the Support Vector Machine algorithm (SVM) with the RBF kernel of the WEKA machine-learning toolkit (.", "labels": [], "entities": [{"text": "WEKA machine-learning toolkit", "start_pos": 140, "end_pos": 169, "type": "DATASET", "confidence": 0.8310578664143881}]}, {"text": "A 10 fold cross-validation was conducted using the FM dataset.", "labels": [], "entities": [{"text": "FM dataset", "start_pos": 51, "end_pos": 61, "type": "DATASET", "confidence": 0.9719357788562775}]}], "tableCaptions": [{"text": " Table 1: Data size and score distribution", "labels": [], "entities": []}, {"text": " Table 4: Performance of filters in topic-related NS  detection", "labels": [], "entities": [{"text": "NS  detection", "start_pos": 50, "end_pos": 63, "type": "TASK", "confidence": 0.9184420704841614}]}, {"text": " Table 5: Comparison of chunk-based and  document-based similarity features", "labels": [], "entities": []}, {"text": " Table 6: Performance of filters in all types of NS  detection", "labels": [], "entities": [{"text": "NS  detection", "start_pos": 49, "end_pos": 62, "type": "TASK", "confidence": 0.9336318969726562}]}]}