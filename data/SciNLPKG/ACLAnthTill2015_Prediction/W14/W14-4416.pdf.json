{"title": [{"text": "Classifiers for data-driven deep sentence generation", "labels": [], "entities": [{"text": "data-driven deep sentence generation", "start_pos": 16, "end_pos": 52, "type": "TASK", "confidence": 0.7365166693925858}]}], "abstractContent": [{"text": "State-of-the-art statistical sentence generators deal with isomorphic structures only.", "labels": [], "entities": []}, {"text": "Therefore, given that semantic and syntactic structures tend to differ in their topol-ogy and number of nodes, i.e., are not iso-morphic, statistical generation saw so far itself confined to shallow, syntactic generation.", "labels": [], "entities": []}, {"text": "In this paper, we present a series of fine-grained classifiers that are essential for data-driven deep sentence generation in that they handle the problem of the projection of non-isomorphic structures.", "labels": [], "entities": [{"text": "data-driven deep sentence generation", "start_pos": 86, "end_pos": 122, "type": "TASK", "confidence": 0.7099713981151581}]}], "introductionContent": [{"text": "Deep data-driven (or stochastic) sentence generation needs to be able to map abstract semantic structures onto syntactic structures.", "labels": [], "entities": [{"text": "sentence generation", "start_pos": 33, "end_pos": 52, "type": "TASK", "confidence": 0.7201233357191086}]}, {"text": "This has been a problem so far since both types of structures differ in their topology and number of nodes (i.e., are non-isomorphic).", "labels": [], "entities": []}, {"text": "For instance, a truly semantic structure will not contain any functional nodes, 1 while a surface-syntactic structure or a chain of tokens in a linearized tree will contain all of them.", "labels": [], "entities": []}, {"text": "Some state-of-the-art proposals use a rule-based module to handle the projection between non-isomorphic semantic and syntactic structures/chains of tokens, e.g.,, and some adapt the semantic structures to be isomorphic with syntactic structures . In this paper, we present two alternative stochastic approaches to the projection between non-isomorphic structures, both based on a cascade of Support Vector Machine (SVM) classifiers.", "labels": [], "entities": []}, {"text": "The first approach addresses the projection as a generic non-isomorphic graph transduction problem in terms of four classifiers for 1.", "labels": [], "entities": []}, {"text": "identification of the (non-isomorphic) correspondences between fragments of the source and target structure, 2.", "labels": [], "entities": []}, {"text": "generation of the nodes of the target structure, 3.", "labels": [], "entities": []}, {"text": "generation of the dependencies between corresponding fragments of the source and target structure, and 4.", "labels": [], "entities": []}, {"text": "generation of the internal dependencies in all fragments of the target structure.", "labels": [], "entities": []}, {"text": "The second approach takes advantage of the linguistic knowledge about the projection of the individual linguistic token types.", "labels": [], "entities": []}, {"text": "It replaces each of the above four classifiers by a set of classifiers, with each single classifier dealing with only one individual linguistic token type (verb, noun, adverb, etc.) or with a configuration thereof.", "labels": [], "entities": []}, {"text": "As will be seen, the linguistic knowledge pays off: the second approach achieves considerably better results.", "labels": [], "entities": []}, {"text": "Since our goal is to address the challenge of the projection of non-isomorphic structures, we focus, in what follows, on this task.", "labels": [], "entities": []}, {"text": "That is, we do not build a complete generation pipeline until the surface.", "labels": [], "entities": []}, {"text": "This could be done, for instance, by feeding the output obtained from the projection of a semantic onto a syntactic structure to the surface realizer described in ).", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we present the performance of the two approaches to DSyntS-SSyntS projection on the DSyntS-and SSynt-layers of the AnCora-UPF treebank (: Results of the evaluation of the generic classifiers for the non-isomorphic transduction.", "labels": [], "entities": [{"text": "DSyntS-and SSynt-layers", "start_pos": 101, "end_pos": 124, "type": "DATASET", "confidence": 0.8581165075302124}, {"text": "AnCora-UPF treebank", "start_pos": 132, "end_pos": 151, "type": "DATASET", "confidence": 0.8991732001304626}]}, {"text": "The results show that for hypernode identification and inter-hypernode dependency generation, the results of both types of classifiers are comparable, be it on the development set or on the test set.", "labels": [], "entities": [{"text": "hypernode identification", "start_pos": 26, "end_pos": 50, "type": "TASK", "confidence": 0.8401525616645813}, {"text": "inter-hypernode dependency generation", "start_pos": 55, "end_pos": 92, "type": "TASK", "confidence": 0.6531449059645335}]}, {"text": "However, thanks to the micro classifiers, with the same features, the lemma generation model based on micro classifiers improves by 4 points and the intra-hypernode dependency generation by nearly: Results of the evaluation of the micro classifiers for the non-isomorphic transduction.", "labels": [], "entities": []}, {"text": "This means that the intra-hypernode dependency generation task is too sparse to be realized as a single classifier.", "labels": [], "entities": []}, {"text": "The micro classifiers are in this case binary, i.e., 2:1, or unary, i.e., 1:1 classifiers, which implies a tremendous reduction of the search space (and thus higher accuracy).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 165, "end_pos": 173, "type": "METRIC", "confidence": 0.9986745119094849}]}, {"text": "In contrast, the single classifier is a multi-class classifier that must decide among more than 60 possible classes.", "labels": [], "entities": []}, {"text": "Although most of these 60 classes are diferentiated by features, the differentiation is not perfect.", "labels": [], "entities": []}, {"text": "In the case of lemma generation, we observe a similar phenomenon.", "labels": [], "entities": [{"text": "lemma generation", "start_pos": 15, "end_pos": 31, "type": "TASK", "confidence": 0.8009595572948456}]}, {"text": "In this case, the micro-classifiers are multi-class classifiers that normally have to cope with 5 different classes (lemmas in this case), while the unique classifier has to cope with around 60 different classes (or lemmas).", "labels": [], "entities": []}, {"text": "Hypernode identification and interhypernode dependency generation are completely guided by the input; thus, it seems that they do not err in the same way.", "labels": [], "entities": [{"text": "Hypernode identification", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.7063440382480621}, {"text": "interhypernode dependency generation", "start_pos": 29, "end_pos": 65, "type": "TASK", "confidence": 0.6336883902549744}]}, {"text": "Although the micro classifier approach leads to significantly better results, we believe that it can still be improved.", "labels": [], "entities": []}, {"text": "First, the introduction of prepositions causes most errors in hypernode detection and lemma generation: when a preposition should be introduced or not and which preposition should be introduced depends exclusively on the sub-categorization frame of the governor of the deep node.", "labels": [], "entities": [{"text": "hypernode detection", "start_pos": 62, "end_pos": 81, "type": "TASK", "confidence": 0.7715094983577728}, {"text": "lemma generation", "start_pos": 86, "end_pos": 102, "type": "TASK", "confidence": 0.7502510845661163}]}, {"text": "A treebank of a limited size as used in our experiments simply does not contain subcategorization patterns of all predicative lexical items (especially of nouns)-which would be crucial.", "labels": [], "entities": []}, {"text": "Thus, in the test set evaluation, out of the 171 lemma errors 147 are prepositions and out of the 717 errors on hypernode identification, more than 500 are due to nouns and preposition.", "labels": [], "entities": [{"text": "hypernode identification", "start_pos": 112, "end_pos": 136, "type": "TASK", "confidence": 0.6804524958133698}]}, {"text": "The increase of the size of the treebank would therefore bean advantage.", "labels": [], "entities": []}, {"text": "Second, in the case of inter-hypernode dependency, errors are due to the labels of the dependencies more than to the attachements, and are quite distributed over the different types of configurations.", "labels": [], "entities": []}, {"text": "The generation of these dependencies suffers from the fact that the SSyntS tag-set is very fine-grained.", "labels": [], "entities": [{"text": "SSyntS tag-set", "start_pos": 68, "end_pos": 82, "type": "DATASET", "confidence": 0.8185619115829468}]}, {"text": "For instance, there are 9 different types of verbal objects in SSyntS, which capture very specific syntactic properties of Spanish, such as \"can the dependent can be replaced by a clitic pronoun?", "labels": [], "entities": []}, {"text": "Can the dependent be moved away from its governor?", "labels": [], "entities": []}, {"text": "This kind of information is not of a high relevance for generation of well-formed text.", "labels": [], "entities": []}, {"text": "Using a more reduced (more coarse-grained) SSyntS tag set would definitely improve the quality of the projection.", "labels": [], "entities": [{"text": "SSyntS tag set", "start_pos": 43, "end_pos": 57, "type": "DATASET", "confidence": 0.8104782303174337}]}], "tableCaptions": [{"text": " Table 1: Results of the evaluation of the generic  classifiers for the non-isomorphic transduction.", "labels": [], "entities": []}, {"text": " Table 2: Results of the evaluation of the micro  classifiers for the non-isomorphic transduction.", "labels": [], "entities": []}]}