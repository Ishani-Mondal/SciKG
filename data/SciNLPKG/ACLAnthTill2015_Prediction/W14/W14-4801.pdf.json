{"title": [{"text": "Generalising and normalising distributional contexts to reduce data sparsity: application to medical corpora", "labels": [], "entities": []}], "abstractContent": [{"text": "Vector space models implement the distributional hypothesis.", "labels": [], "entities": []}, {"text": "They are based on the repetition of information occurring in the contexts of words to associate.", "labels": [], "entities": []}, {"text": "However, these models suffer from a high number of dimensions and data sparsity in the matrix of contextual vectors.", "labels": [], "entities": []}, {"text": "This is a major issue with specialised corpora that are of much smaller size and with much lower context frequencies.", "labels": [], "entities": []}, {"text": "We tackle the problem of data sparsity on specialised texts and we propose a method that allows to make the matrix denser, by generalising and normalising distributional contexts.", "labels": [], "entities": []}, {"text": "Generalisation gives better results with the Jaccard index, narrow sliding windows and relations of lexical inclusion.", "labels": [], "entities": []}, {"text": "On the other hand, normalisation has no positive effect on the relation extraction, with any combination of distributional parameters.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 63, "end_pos": 82, "type": "TASK", "confidence": 0.8444785177707672}]}], "introductionContent": [{"text": "Distributional Analysis (DA) assumes that words occurring in a similar context tend to be semantically close.", "labels": [], "entities": [{"text": "Distributional Analysis (DA)", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8366175055503845}]}, {"text": "This hypothesis is usually applied through vector space models (VSM) where vectors represent the contextual information and distributional statistical data).", "labels": [], "entities": []}, {"text": "Each target word in a text is represented as a point defined according to its distributional properties in the text (.", "labels": [], "entities": []}, {"text": "Thus, the semantic similarity between two words is defined as a closeness in an n-dimension space, where each dimension corresponds to some potential shared contexts.", "labels": [], "entities": []}, {"text": "The VSMs easily quantify the semantic similarity between two words by measuring the distance between the two corresponding vectors within this space, or the cosine of their angle.", "labels": [], "entities": []}, {"text": "On the other hand, besides the high number of dimensions required (for example, uses VSMs with up to several millions of dimensions), VSMs also suffer from data sparseness within the matrix representing the vector space: many elements are equal to zero because only few contexts are associated to a target word.", "labels": [], "entities": []}, {"text": "This disadvantage is partly due to word distribution in corpora: whatever the corpus size, most words have low frequencies and a very limited set of contexts compared to the number of words in the corpora.", "labels": [], "entities": []}, {"text": "These last two elements make the similarity between two words hard to compute.", "labels": [], "entities": []}, {"text": "Hence, methods based on the distributional hypothesis show better results when much information is available and especially with general corpora, usually of great size.", "labels": [], "entities": []}, {"text": "But the reduction of data sparseness is still an important aspect with general corpora.", "labels": [], "entities": []}, {"text": "It is as well a major issue when working with specialised corpora.", "labels": [], "entities": []}, {"text": "Indeed, these corpora are characterised by smaller sizes, and with frequencies and a number of different contexts especially lower.", "labels": [], "entities": []}, {"text": "We focus hereon this last point.", "labels": [], "entities": []}, {"text": "We propose a rule-based method that aims at reducing context diversity by generalising contexts.", "labels": [], "entities": []}, {"text": "The frequency of the obtained distributional contexts is then increased and, consequently, data sparseness and the dimensions of the vector space model are reduced.", "labels": [], "entities": []}, {"text": "We present here a generalisation of the distributional contexts thanks to semantic relations acquired on corpora.", "labels": [], "entities": []}, {"text": "The parameters of the distributional method are tuned to specialised corpora, especially in integrating those generalised contexts.", "labels": [], "entities": []}, {"text": "We first present a state of the art on data sparsity reduction within distributional methods.", "labels": [], "entities": [{"text": "data sparsity reduction", "start_pos": 39, "end_pos": 62, "type": "TASK", "confidence": 0.6185875435670217}]}, {"text": "Then we describe the proposed context generalisation and normalisation method as well as the experiments 1 we performed to evaluate its impact on specialised corpora.", "labels": [], "entities": [{"text": "context generalisation", "start_pos": 30, "end_pos": 52, "type": "TASK", "confidence": 0.7577885389328003}, {"text": "normalisation", "start_pos": 57, "end_pos": 70, "type": "TASK", "confidence": 0.8803114891052246}]}, {"text": "Results are evaluated and analysed with precision, R-precision and MAP.", "labels": [], "entities": [{"text": "precision", "start_pos": 40, "end_pos": 49, "type": "METRIC", "confidence": 0.9997169375419617}, {"text": "R-precision", "start_pos": 51, "end_pos": 62, "type": "METRIC", "confidence": 0.9920118451118469}, {"text": "MAP", "start_pos": 67, "end_pos": 70, "type": "METRIC", "confidence": 0.9958298802375793}]}], "datasetContent": [{"text": "We performed several series of experiments on the Menelas corpus to evaluate the impact of both generalisation and normalisation rules on the quality of the acquired relations.", "labels": [], "entities": [{"text": "Menelas corpus", "start_pos": 50, "end_pos": 64, "type": "DATASET", "confidence": 0.924548476934433}]}, {"text": "Our baseline is the VSM without context substitution (VSMonly).", "labels": [], "entities": [{"text": "VSM without context substitution", "start_pos": 20, "end_pos": 52, "type": "TASK", "confidence": 0.6626028567552567}]}, {"text": "First, we automatically compute the thresholds on the distributional parameters from the baseline (see section 4.1).", "labels": [], "entities": []}, {"text": "The values of the thresholds are listed in table 1.", "labels": [], "entities": []}, {"text": "We experiment two sliding window sizes ; a small window allows to detect classical types of relations (synonymy, meronymy, hypernymy, etc.) but increases the data sparseness problem.", "labels": [], "entities": []}, {"text": "On the other hand, a large window provides more general relations, a contextual proximity.: Definition of the threshold values on distributional parameters and on the similarity score according to the window width (21 and 5 words) and similarity measures (Jaccard index and Cosine) We perform separately the experiments regarding generalisation and normalisation rules.", "labels": [], "entities": [{"text": "Cosine", "start_pos": 274, "end_pos": 280, "type": "METRIC", "confidence": 0.9740726947784424}]}, {"text": "With generalisation rules, in order to grasp the contribution of each linguistic method (see section 3), we define a set of experiments where context generalisation is performed using the hypernymy relations proposed individually by each method.", "labels": [], "entities": []}, {"text": "The context generalisation rules ctxt i (w) are then applied separately using the sets H LSP (ctxt i (w)) (VSM/LSP), H LI (ctxt i (w)) (VSM/LI) and HT V (ctxt i (w)) (VSM/TV).", "labels": [], "entities": [{"text": "VSM/TV)", "start_pos": 167, "end_pos": 174, "type": "DATASET", "confidence": 0.8398109823465347}]}, {"text": "Then, sequentially, we apply generalisation rules by using the sets of hypernymy relations proposed by two linguistic approaches ( ).", "labels": [], "entities": []}, {"text": "All contexts are generalised following the relations proposed by one of the sets.Likewise, we combine the three sets of relations (for instance, H LSP (ctxt i (w)) then H LI (ctxt i (w)) then HT V (ctxt i (w)) -VSM/LSP+LI+TV).", "labels": [], "entities": []}, {"text": "By combining the hypernymy relation sources in several ways, we evaluate the complementarity of the approaches used for context generalisation.", "labels": [], "entities": [{"text": "context generalisation", "start_pos": 120, "end_pos": 142, "type": "TASK", "confidence": 0.7673562467098236}]}, {"text": "We also study the impact of the order of these methods in the generalisation sequence.", "labels": [], "entities": [{"text": "generalisation", "start_pos": 62, "end_pos": 76, "type": "TASK", "confidence": 0.9717591404914856}]}, {"text": "All the hypernymy relations independently of the method used for their acquisition.", "labels": [], "entities": []}, {"text": "We consider the set . With context normalisation, we consider only one set of experiment, with normalisation of contexts (VSM/Syn).", "labels": [], "entities": []}, {"text": "All the experiments have been performed on both window sizes: 5 words (\u00b1 2 words, centered on the target) and 21 words (\u00b1 10 words, centered on the target).", "labels": [], "entities": []}, {"text": "Indeed the window size influences the number and quality, but also the type of the relations acquired with distributional analysis.", "labels": [], "entities": []}, {"text": "In general, a small window size (5 words) allows to have a highest number of relevant contexts fora given target word, but leads to more data sparsity than with a larger window.", "labels": [], "entities": []}, {"text": "Furthermore, the results obtained with small size windows are of greatest quality, especially for classical relations (synonymy, antonymy, hypernymy, meronymy, etc.), whereas larger windows are more adapted to the identification of domain specific relations).", "labels": [], "entities": []}, {"text": "As usual to evaluate distributional methods, the obtained relations are considered as semantic neighbour sets associated to target words, and the quality of the neighbour sets is measured by comparing them to semantic relations issued from existing resources.", "labels": [], "entities": []}, {"text": "Thus, we compare the semantic relations acquired by our approach with the 1,735,419 relations in the French part of the UMLS metathesaurus 1 . The resource contains hypernyms, synonyms, co-hyponyms, meronyms and domain relations.", "labels": [], "entities": [{"text": "UMLS metathesaurus 1", "start_pos": 120, "end_pos": 140, "type": "DATASET", "confidence": 0.9201239148775736}]}, {"text": "We used classical measures to evaluate the quality of our results: macro-precision), the mean of the average precisions (MAP)) and R-precision.", "labels": [], "entities": [{"text": "average precisions (MAP))", "start_pos": 101, "end_pos": 126, "type": "METRIC", "confidence": 0.7855994939804077}, {"text": "R-precision", "start_pos": 131, "end_pos": 142, "type": "METRIC", "confidence": 0.9739869832992554}]}, {"text": "Macro-precision equally considers all target words whatever the number of semantic neighbours and provides a comprehensive quality of the results by computing the mean of the precision of each neighbour set.", "labels": [], "entities": [{"text": "precision", "start_pos": 175, "end_pos": 184, "type": "METRIC", "confidence": 0.9803943634033203}]}, {"text": "We consider one size of neighbour set for each target word: precision after examining 1 (P@1) semantic neighbour, the neighbour ranked first by its similarity score.", "labels": [], "entities": [{"text": "precision", "start_pos": 60, "end_pos": 69, "type": "METRIC", "confidence": 0.9997168183326721}]}, {"text": "Alternatively, we use R-precision that individually defines the size of the neighbour sets to examine as the number of correct neighbours expected for the corresponding target word).", "labels": [], "entities": []}, {"text": "To compute R-precision, we compare our results not to all the relations from the French part of UMLS, but to reference sets built from this resource, for each experiment.", "labels": [], "entities": [{"text": "French part of UMLS", "start_pos": 81, "end_pos": 100, "type": "DATASET", "confidence": 0.7728720754384995}]}, {"text": "Thus, we have as many references as experiments.", "labels": [], "entities": []}, {"text": "The mean of average precisions (MAP) is obtained taking in consideration the not interpolated precision of the semantic neighbours given their rank.", "labels": [], "entities": [{"text": "mean of average precisions (MAP)", "start_pos": 4, "end_pos": 36, "type": "METRIC", "confidence": 0.8254849995885577}, {"text": "precision", "start_pos": 94, "end_pos": 103, "type": "METRIC", "confidence": 0.5888172388076782}]}, {"text": "It reflects the ranking quality and evaluates the relevance of the similarity measure used.", "labels": [], "entities": []}, {"text": "Thus, the MAP favours the similarity measure that ranks all the correct semantic neighbours on top of the list.", "labels": [], "entities": [{"text": "similarity measure", "start_pos": 26, "end_pos": 44, "type": "METRIC", "confidence": 0.9413560628890991}]}, {"text": "Reciprocally, adding noisy semantic neighbours at the end of the list does not discriminate against the method.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Definition of the threshold values on distributional parameters and on the similarity score ac- cording to the window width (21 and 5 words) and similarity measures (Jaccard index and Cosine)", "labels": [], "entities": [{"text": "similarity score ac- cording", "start_pos": 85, "end_pos": 113, "type": "METRIC", "confidence": 0.9062623500823974}, {"text": "similarity", "start_pos": 155, "end_pos": 165, "type": "METRIC", "confidence": 0.9485340118408203}, {"text": "Jaccard index", "start_pos": 176, "end_pos": 189, "type": "METRIC", "confidence": 0.7817275524139404}, {"text": "Cosine", "start_pos": 194, "end_pos": 200, "type": "METRIC", "confidence": 0.9579964876174927}]}, {"text": " Table 2: Results obtained with the Jaccard index and Cosine measure for a 21 word window", "labels": [], "entities": [{"text": "Cosine measure", "start_pos": 54, "end_pos": 68, "type": "METRIC", "confidence": 0.9613151550292969}]}, {"text": " Table 3: Results obtained for a 5 word window-with thresholds on the distributional parameters", "labels": [], "entities": []}]}