{"title": [{"text": "NaDiR: Naive Distributional Response Generation", "labels": [], "entities": [{"text": "Naive Distributional Response Generation", "start_pos": 7, "end_pos": 47, "type": "TASK", "confidence": 0.941956028342247}]}], "abstractContent": [{"text": "This paper describes NaDiR (Naive DIstributional Response generation), a corpus-based system that, from a set of word stimuli as an input, generates a response word relying on association strength and distributional similarity.", "labels": [], "entities": [{"text": "Naive DIstributional Response generation)", "start_pos": 28, "end_pos": 69, "type": "TASK", "confidence": 0.7064192235469818}]}, {"text": "NaDiR participated in the CogALex 2014 shared task on multiword associations (restricted systems track), operationalizing the task as a ranking problem: candidate words from a large vocabulary are ranked by their average association or similarity to a given set of stimuli.", "labels": [], "entities": [{"text": "CogALex 2014 shared task", "start_pos": 26, "end_pos": 50, "type": "TASK", "confidence": 0.5171715468168259}]}, {"text": "We also report on a number of experiments conducted on the shared task data, comparing first-order models (based on co-occurrence and statistical association) to second-order models (based on distributional similarity).", "labels": [], "entities": []}], "introductionContent": [{"text": "This paper describes NaDiR, a corpus-based system designed for the reverse association task.", "labels": [], "entities": []}, {"text": "NaDiR is an acronym for Naive Distributional Response generation.", "labels": [], "entities": [{"text": "NaDiR", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9123765230178833}, {"text": "Naive Distributional Response generation", "start_pos": 24, "end_pos": 64, "type": "TASK", "confidence": 0.8353982418775558}]}, {"text": "NaDiR is naive because it is based on a very simple algorithm that operationalizes the multiword association task as a ranking problem: candidate words from a large vocabulary are ranked by their average statistical association or distributional similarity to a given set of stimuli, then the highest-ranked candidate is selected as NaDiR's response.", "labels": [], "entities": [{"text": "NaDiR", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8707200288772583}]}, {"text": "We compare models based on collocations (first-order models, see for an overview) to models based on distributional similarity (second-order models; see,, and reference therein fora review).", "labels": [], "entities": []}, {"text": "Previous work on this task showed that co-occurrence models outperform distributional semantic models (henceforth, DSMs), and that using rank measures improves performance because it accounts for directionality of the association/similarity (e.g., the association from stimulus to response maybe larger than the association from response to stimulus).", "labels": [], "entities": []}, {"text": "Our results corroborate both claims.", "labels": [], "entities": []}, {"text": "The paper is structured as follows: section 2 provides an overview of the task and of the problems we encountered in its implementation; section 3 summarizes related work; section 4 describes NaDiR in detail; section 5 reports the results of our experiments on the shared task training and test data; section 6 describes ongoing and future work on NaDiR.", "labels": [], "entities": []}], "datasetContent": [{"text": "In our experiments, we compared first-order (collocations) and second-order (DSM) models; for each class of models, we evaluated the different parameter values described in section 4.2.", "labels": [], "entities": []}, {"text": "display the results of our experiments on the training data, separately for first-order (tables 2-4) and second-order models.", "labels": [], "entities": []}, {"text": "Parameter configurations are reported in the Parameter column 8 . The number of correct responses in the lemmatized version is reported in the column Lemma (showing how often our system predicted the correct lemma).", "labels": [], "entities": []}, {"text": "The column Wordform reports the number of correct responses for which, before inverting the lemmatization, the inflected form was already identical to the lemma.", "labels": [], "entities": [{"text": "Wordform", "start_pos": 11, "end_pos": 19, "type": "DATASET", "confidence": 0.926834225654602}]}, {"text": "As the task of predicting exactly one word is particularly difficult, we further characterize the performance of our evaluated models by reporting the number of cases in which the correct answer from the training set was among the first 10 (< 10), 50 (< 50), or 100 (< 100) ranked candidates.", "labels": [], "entities": [{"text": "predicting exactly one word", "start_pos": 15, "end_pos": 42, "type": "TASK", "confidence": 0.8575958460569382}]}, {"text": "In the last column, we report the average rank of the correct responses (Avg correct).", "labels": [], "entities": [{"text": "Avg correct)", "start_pos": 73, "end_pos": 85, "type": "METRIC", "confidence": 0.9826355377833048}]}, {"text": "The results reported in tables 2 to 5 allowed us to identify best parameter configurations for the firstorder (symmetric 2 words window, frequency, backward rank) and second-order models (2 words window, distance).", "labels": [], "entities": []}, {"text": "We evaluated these configurations on the test data (table 6).", "labels": [], "entities": []}, {"text": "compares the performance of the best first-order and the best second-order model on the training and test datasets, both for lemmatized response (Training-Lemma, Test-Lemma) and generation of the correct word form (Training-Inflected, Test-Inflected).", "labels": [], "entities": []}, {"text": "A considerable portion of the experiments reported in this paper were conducted after the submission deadline of the CogALex shared task.", "labels": [], "entities": [{"text": "CogALex shared task", "start_pos": 117, "end_pos": 136, "type": "DATASET", "confidence": 0.7125256856282552}]}, {"text": "As a consequence, our submitted results do not correspond to the best overall configuration found in the evaluation study.", "labels": [], "entities": []}, {"text": "The submission was based on a second order model, a 4-word window, and cosine distance as index of distributional similarity.", "labels": [], "entities": []}, {"text": "In this configuration, NaDiR generated 262 correct responses, corresponding to an accuracy of 13%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 82, "end_pos": 90, "type": "METRIC", "confidence": 0.9995126724243164}]}], "tableCaptions": [{"text": " Table 2: First Order Models -Symmetric Window: 2 words to the left/right of the node -Training Data", "labels": [], "entities": []}, {"text": " Table 3: First Order Models -Asymmetric Window: 3 words to the left of the node -Training Data", "labels": [], "entities": []}, {"text": " Table 4: First Order Models -Asymmetric Window: 3 words to the right of the node -Training Data", "labels": [], "entities": []}, {"text": " Table 5: Second order models -Training data", "labels": [], "entities": []}]}