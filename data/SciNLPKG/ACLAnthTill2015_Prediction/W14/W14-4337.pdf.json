{"title": [{"text": "The Second Dialog State Tracking Challenge", "labels": [], "entities": [{"text": "Dialog State Tracking", "start_pos": 11, "end_pos": 32, "type": "TASK", "confidence": 0.6450742582480112}]}], "abstractContent": [{"text": "A spoken dialog system, while communicating with a user, must keep track of what the user wants from the system at each step.", "labels": [], "entities": []}, {"text": "This process, termed dialog state tracking, is essential fora successful dialog system as it directly informs the system's actions.", "labels": [], "entities": [{"text": "dialog state tracking", "start_pos": 21, "end_pos": 42, "type": "TASK", "confidence": 0.8416658043861389}]}, {"text": "The first Dialog State Tracking Challenge allowed for evaluation of different dialog state tracking techniques , providing common testbeds and evaluation suites.", "labels": [], "entities": [{"text": "Dialog State Tracking Challenge", "start_pos": 10, "end_pos": 41, "type": "TASK", "confidence": 0.7844079434871674}, {"text": "dialog state tracking", "start_pos": 78, "end_pos": 99, "type": "TASK", "confidence": 0.683394213517507}]}, {"text": "This paper presents a second challenge, which continues this tradition and introduces some additional features-a new domain, changing user goals and a richer dialog state.", "labels": [], "entities": []}, {"text": "The challenge received 31 entries from 9 research groups.", "labels": [], "entities": []}, {"text": "The results suggest that while large improvements on a competitive base-line are possible, trackers are still prone to degradation in mismatched conditions.", "labels": [], "entities": []}, {"text": "An investigation into ensemble learning demonstrates the most accurate tracking can be achieved by combining multiple trackers.", "labels": [], "entities": []}], "introductionContent": [{"text": "Spoken language provides a medium of communication that is natural to users as well as hands-and eyes-free.", "labels": [], "entities": []}, {"text": "Voice-based computer systems, called spoken dialog systems, allow users to interact using speech to achieve a goal.", "labels": [], "entities": []}, {"text": "Efficient operation of a spoken dialog system requires a component that can track what has happened in a dialog, incorporating system outputs, user speech and context from previous turns.", "labels": [], "entities": []}, {"text": "The building and evaluation of these trackers is an important field of research since the performance of dialog state tracking is important for the final performance of a complete system.", "labels": [], "entities": [{"text": "dialog state tracking", "start_pos": 105, "end_pos": 126, "type": "TASK", "confidence": 0.691758910814921}]}, {"text": "Until recently, it was difficult to compare approaches to state tracking because of the wide variety of metrics and corpora used for evaluation.", "labels": [], "entities": [{"text": "state tracking", "start_pos": 58, "end_pos": 72, "type": "TASK", "confidence": 0.7893365025520325}]}, {"text": "The first dialog state tracking challenge (DSTC1) attempted to overcome this by defining a challenge task with standard test conditions, freely available corpora and open access (.", "labels": [], "entities": [{"text": "dialog state tracking challenge (DSTC1)", "start_pos": 10, "end_pos": 49, "type": "TASK", "confidence": 0.805473940713065}]}, {"text": "This paper presents the results of a second challenge, which continues in this tradition with the inclusion of additional features relevant to the research community.", "labels": [], "entities": []}, {"text": "Some key differences to the first challenge include: \u2022 The domain is restaurant search instead of bus timetable information.", "labels": [], "entities": []}, {"text": "This provides participants with a different category of interaction where there is a database of matching entities.", "labels": [], "entities": []}, {"text": "\u2022 Users' goals are permitted to change.", "labels": [], "entities": []}, {"text": "In the first challenge, the user was assumed to always want a specific bus journey.", "labels": [], "entities": []}, {"text": "In this challenge the user's goal can change.", "labels": [], "entities": []}, {"text": "For example, they may want a 'Chinese' restaurant at the start of the dialog but change to wanting 'Italian' food by the end.", "labels": [], "entities": []}, {"text": "\u2022 The dialog state uses a richer representation than in DSTC1, including not only the slot/value attributes of the user goal, but also their search method, and what information they wanted the system to readout.", "labels": [], "entities": []}, {"text": "As well as presenting the results of the different state trackers, this paper attempts to obtain some insights into research progress by analysing their performance.", "labels": [], "entities": []}, {"text": "This includes analyses of the predictive power of performance on the development set, the effects of tracking the dialog state using joint distributions, and the correlation between 1-best accuracy and overall quality of probability distributions output by trackers.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 189, "end_pos": 197, "type": "METRIC", "confidence": 0.8220726847648621}]}, {"text": "An evaluation of the effects of ensemble learning is also performed.", "labels": [], "entities": []}, {"text": "The paper begins with an overview of the chal-lenge in section 2.", "labels": [], "entities": []}, {"text": "The labelling scheme and metrics used for evaluation are discussed in section 3 followed by a summary of the results of the challenge in section 4.", "labels": [], "entities": []}, {"text": "An analysis of ensemble learning is presented in section 5.", "labels": [], "entities": []}, {"text": "Section 6 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "The output of each tracker is a distribution over dialog states for each turn, as explained in section 2.1.", "labels": [], "entities": []}, {"text": "To allow evaluation of the tracker output, the single correct dialog state at each turn is labelled.", "labels": [], "entities": []}, {"text": "Labelling of the dialog state is facilitated by first labelling each user utterance with its semantic representation, in the dialog act format described in (some example semantic representations are given in appendix B).", "labels": [], "entities": []}, {"text": "The semantic labelling was achieved by first crowdsourcing the transcription of the audio to text.", "labels": [], "entities": []}, {"text": "Next a semantic decoder was run over the transcriptions, and the authors corrected the decoder's results by hand.", "labels": [], "entities": []}, {"text": "Given the sequence of machine actions and user actions, both represented semantically, the true dialog state is computed deterministically using a simple set of rules.", "labels": [], "entities": []}, {"text": "Recall the dialog state is composed of multiple components; the goal constraint for each slot, the requested slots, and the method.", "labels": [], "entities": []}, {"text": "Each of these is evaluated separately, by comparing the tracker output to the correct label.", "labels": [], "entities": []}, {"text": "The joint over the goal constraints is evaluated in the same way, where the tracker may either explicitly enumerate and score its joint hypotheses, or let the joint be computed as the product of the distributions over the slots.", "labels": [], "entities": []}, {"text": "A bank of metrics which look at the tracker output and the correct labels are calculated in the evaluation.", "labels": [], "entities": []}, {"text": "These metrics area slightly expanded set of those calculated in DSTC1.", "labels": [], "entities": [{"text": "DSTC1", "start_pos": 64, "end_pos": 69, "type": "DATASET", "confidence": 0.9681884050369263}]}, {"text": "Denote an example probability distribution given by a tracker asp and the correct label to be i, so we have that the probability reported to the correct hypothesis is pi , and j p j = 1.", "labels": [], "entities": []}, {"text": "Accuracy measures the fraction of turns where the top hypothesis is correct, i.e. where i = arg max j p j . AvgP, average probability, measures the mean score of the correct hypothesis, pi . This gives some idea of the quality of the score given to the correct hypothesis, ignoring the rest of the distribution.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9907134771347046}, {"text": "AvgP", "start_pos": 108, "end_pos": 112, "type": "METRIC", "confidence": 0.9912142157554626}, {"text": "average probability", "start_pos": 114, "end_pos": 133, "type": "METRIC", "confidence": 0.9218620359897614}]}, {"text": "Neglogp is the mean negative logarithm of the score given to the correct hypothesis, \u2212 log pi . Sometimes called the negative log likelihood, this is a standard score in machine learning tasks.", "labels": [], "entities": [{"text": "negative log likelihood", "start_pos": 117, "end_pos": 140, "type": "METRIC", "confidence": 0.6898147066434225}]}, {"text": "MRR is the mean reciprocal rank of the top hypothesis, i.e. 1 1+k where j k = i and p j 0 \u2265 p j 1 \u2265 . .", "labels": [], "entities": [{"text": "MRR", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.5597058534622192}]}, {"text": ".. This metric measures the quality of the ranking, without necessarily treating the scores as probabilities.", "labels": [], "entities": []}, {"text": "L2 measures the square of the l 2 norm between the distribution and the correct label, indicating quality of the whole reported distribution.", "labels": [], "entities": []}, {"text": "It is calculated for one turn as (1 \u2212 pi ) 2 + j =i p 2 j . Two metrics, Update precision and Update accuracy measure the accuracy and precision of updates to the top scoring hypothesis from one turn to the next.", "labels": [], "entities": [{"text": "precision", "start_pos": 80, "end_pos": 89, "type": "METRIC", "confidence": 0.8951891660690308}, {"text": "accuracy", "start_pos": 101, "end_pos": 109, "type": "METRIC", "confidence": 0.7324310541152954}, {"text": "accuracy", "start_pos": 122, "end_pos": 130, "type": "METRIC", "confidence": 0.9983336329460144}, {"text": "precision", "start_pos": 135, "end_pos": 144, "type": "METRIC", "confidence": 0.9906755685806274}]}, {"text": "For more details, see, which finds these metrics to be highly correlated with dialog success in their data.", "labels": [], "entities": []}, {"text": "Finally there is a set of measures relating to the receiver operating characteristic (ROC) curves, which measure the discrimination of the scores for the highest-ranked hypotheses.", "labels": [], "entities": [{"text": "receiver operating characteristic (ROC) curves", "start_pos": 51, "end_pos": 97, "type": "METRIC", "confidence": 0.8061441736561912}]}, {"text": "Two versions of ROC are computed, V1 and V2.", "labels": [], "entities": [{"text": "ROC", "start_pos": 16, "end_pos": 19, "type": "TASK", "confidence": 0.5518209934234619}]}, {"text": "V1 computes correct-accepts (CA), false accepts (FA) and falserejects (FR) as fractions of all utterances.", "labels": [], "entities": [{"text": "false accepts (FA) and falserejects (FR)", "start_pos": 34, "end_pos": 74, "type": "METRIC", "confidence": 0.7124382495880127}]}, {"text": "The V2 metrics consider fractions of correctly classified utterances, meaning the values always reach 100% regardless of the accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 125, "end_pos": 133, "type": "METRIC", "confidence": 0.9962530136108398}]}, {"text": "V2 metrics measure discrimination independently of the accuracy, and are therefore only comparable between trackers with similar accuracies.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 55, "end_pos": 63, "type": "METRIC", "confidence": 0.9994283318519592}]}, {"text": "Several metrics are computed from the ROC statistics.", "labels": [], "entities": [{"text": "ROC statistics", "start_pos": 38, "end_pos": 52, "type": "DATASET", "confidence": 0.839516669511795}]}, {"text": "ROC V1 EER computes the false acceptance rate at the point where false-accepts are equal to false-rejects.", "labels": [], "entities": [{"text": "ROC V1 EER", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.5547883609930674}, {"text": "false acceptance rate", "start_pos": 24, "end_pos": 45, "type": "METRIC", "confidence": 0.7076625823974609}]}, {"text": "ROC V1 CA05, ROC V1 CA10, ROC V1 CA20 and ROC V2 CA05, ROC V2 CA10, ROC V2 CA20, compute the correct acceptance rates for both versions of ROC at falseacceptance rates 0.05, 0.10, and 0.20.", "labels": [], "entities": [{"text": "acceptance", "start_pos": 101, "end_pos": 111, "type": "METRIC", "confidence": 0.9349854588508606}]}, {"text": "Two schedules are used to decide which turns to include when computing each metric.", "labels": [], "entities": []}, {"text": "Schedule 1 includes every turn.", "labels": [], "entities": []}, {"text": "Schedule 2 only includes a turn if any SLU hypothesis up to and including the turn contains some information about the component of the dialog state in question, or if the correct label is not None.", "labels": [], "entities": []}, {"text": "E.g. fora goal constraint, this is whether the slot has appeared with a value in any SLU hypothesis, an affirm/negate act has appeared after a system confirmation of the slot, or the user has in fact informed the slot regardless of the SLU.", "labels": [], "entities": []}, {"text": "The data is labelled using two schemes.", "labels": [], "entities": []}, {"text": "The first, scheme A, is considered the standard labelling of the dialog state.", "labels": [], "entities": []}, {"text": "Under this scheme, each component of the state is defined as the most recently asserted value given by the user.", "labels": [], "entities": []}, {"text": "The None value is used to indicate that a value is yet to be given.", "labels": [], "entities": [{"text": "None", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.9938680529594421}]}, {"text": "Appendix B demonstrates labelling under scheme A.", "labels": [], "entities": []}, {"text": "A second labelling scheme, scheme B, is included in the evaluation, where labels are prop-agated backwards through the dialog.", "labels": [], "entities": []}, {"text": "This labelling scheme is designed to assess whether a tracker is able to predict a user's intention before it has been stated.", "labels": [], "entities": []}, {"text": "Under scheme B, the label at a current turn fora particular component of the dialog state is considered to be the next value which the user settles on, and is reset in the case of goal constraints if the slot value pair is given in a canthelp act by the system (i.e. the system has informed that this constraint is not satisfiable).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Percentage of dialogs which included a change in", "labels": [], "entities": []}, {"text": " Table 3: Word Error Rate on the top hypothesis, and F-score", "labels": [], "entities": [{"text": "Word Error Rate", "start_pos": 10, "end_pos": 25, "type": "METRIC", "confidence": 0.7907167474428812}, {"text": "F-score", "start_pos": 53, "end_pos": 60, "type": "METRIC", "confidence": 0.997524082660675}]}, {"text": " Table 4: Accuracy and L2 for Joint goal constraint, Method, and Requested slots for the single best tracker (by accuracy) in", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9992969036102295}, {"text": "L2", "start_pos": 23, "end_pos": 25, "type": "METRIC", "confidence": 0.9885185360908508}, {"text": "Method", "start_pos": 53, "end_pos": 59, "type": "METRIC", "confidence": 0.9916797280311584}, {"text": "Requested slots", "start_pos": 65, "end_pos": 80, "type": "METRIC", "confidence": 0.9514601230621338}, {"text": "accuracy", "start_pos": 113, "end_pos": 121, "type": "METRIC", "confidence": 0.9989376664161682}]}]}