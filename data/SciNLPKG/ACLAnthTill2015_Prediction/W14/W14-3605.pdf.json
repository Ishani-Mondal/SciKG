{"title": [{"text": "The First QALB Shared Task on Automatic Text Correction for Arabic", "labels": [], "entities": []}], "abstractContent": [{"text": "We present a summary of the first shared task on automatic text correction for Ara-bic text.", "labels": [], "entities": [{"text": "text correction", "start_pos": 59, "end_pos": 74, "type": "TASK", "confidence": 0.7002693712711334}]}, {"text": "The shared task received 18 systems submissions from nine teams in six countries and represented a diversity of approaches.", "labels": [], "entities": []}, {"text": "Our report includes an overview of the QALB corpus which was the source of the datasets used for training and evaluation , an overview of participating systems , results of the competition and an analysis of the results and systems.", "labels": [], "entities": [{"text": "QALB corpus", "start_pos": 39, "end_pos": 50, "type": "DATASET", "confidence": 0.9190842509269714}]}], "introductionContent": [{"text": "The task of text correction has recently gained a lot of attention in the Natural Language Processing (NLP) community.", "labels": [], "entities": [{"text": "text correction", "start_pos": 12, "end_pos": 27, "type": "TASK", "confidence": 0.8646121621131897}]}, {"text": "Most of the effort in this area concentrated on English, especially on errors made by learners of English as a Second Language.", "labels": [], "entities": []}, {"text": "Four competitions devoted to error correction for non-native English writers took place recently: HOO () and CoNLL (.", "labels": [], "entities": [{"text": "error correction for non-native English writers", "start_pos": 29, "end_pos": 76, "type": "TASK", "confidence": 0.6981327335039774}, {"text": "HOO", "start_pos": 98, "end_pos": 101, "type": "METRIC", "confidence": 0.6405050158500671}, {"text": "CoNLL", "start_pos": 109, "end_pos": 114, "type": "DATASET", "confidence": 0.7795225977897644}]}, {"text": "Shared tasks of this kind are extremely important, as they bring together researchers who focus on this problem and promote development and dissemination of key resources, such as benchmark datasets.", "labels": [], "entities": []}, {"text": "Recently, there have been several efforts aimed at creating data resources related to the correction of Arabic text.", "labels": [], "entities": [{"text": "correction of Arabic text", "start_pos": 90, "end_pos": 115, "type": "TASK", "confidence": 0.8808312714099884}]}, {"text": "Those include human annotated corpora (, spell-checking lexicon () and unannotated language learner corpora.", "labels": [], "entities": []}, {"text": "A natural extension to these resource production efforts is the creation of robust automatic systems for error correction.", "labels": [], "entities": [{"text": "error correction", "start_pos": 105, "end_pos": 121, "type": "TASK", "confidence": 0.6645919233560562}]}, {"text": "* These authors contributed equally to this In this paper, we present a summary of the QALB shared task on automatic text correction for Arabic.", "labels": [], "entities": [{"text": "automatic text correction", "start_pos": 107, "end_pos": 132, "type": "TASK", "confidence": 0.5998419026533762}]}, {"text": "The Qatar Arabic Language Bank (QALB) project 1 is one of the first large scale data and system development efforts for automatic correction of Arabic which has resulted in annotation of the QALB corpus.", "labels": [], "entities": [{"text": "Qatar Arabic Language Bank (QALB) project 1", "start_pos": 4, "end_pos": 47, "type": "DATASET", "confidence": 0.8793997698360019}, {"text": "automatic correction of Arabic", "start_pos": 120, "end_pos": 150, "type": "TASK", "confidence": 0.8055306971073151}, {"text": "QALB corpus", "start_pos": 191, "end_pos": 202, "type": "DATASET", "confidence": 0.8002447485923767}]}, {"text": "In conjunction with the EMNLP Arabic NLP workshop, the QALB shared task is the first community effort for construction and evaluation of automatic correction systems for Arabic.", "labels": [], "entities": [{"text": "EMNLP Arabic NLP workshop", "start_pos": 24, "end_pos": 49, "type": "DATASET", "confidence": 0.7344633936882019}]}, {"text": "The results of the competition indicate that the shared task attracted a lot of interest and generated a diverse set of approaches from the participating teams.", "labels": [], "entities": []}, {"text": "In the next section, we present the shared task framework.", "labels": [], "entities": []}, {"text": "This is followed by an overview of the QALB corpus (Section 3).", "labels": [], "entities": [{"text": "QALB corpus", "start_pos": 39, "end_pos": 50, "type": "DATASET", "confidence": 0.9185341000556946}]}, {"text": "Section 4 describes the shared task data, and Section 5 presents the approaches adopted by the participating teams.", "labels": [], "entities": []}, {"text": "Section 6 discusses the results of the competition.", "labels": [], "entities": []}, {"text": "Finally, in Section 7, we offer a brief analysis and present preliminary experiments on system combination.", "labels": [], "entities": [{"text": "system combination", "start_pos": 88, "end_pos": 106, "type": "TASK", "confidence": 0.7360879182815552}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 3: Statistics on the shared task data.", "labels": [], "entities": []}, {"text": " Table 4: Distribution of annotations by type in the shared task data. Error types denotes the action  required in order to correct the error.", "labels": [], "entities": [{"text": "Error", "start_pos": 71, "end_pos": 76, "type": "METRIC", "confidence": 0.991336464881897}]}, {"text": " Table 7: Official results on the test set. Column 1  shows the system rank according to the F 1 score.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 93, "end_pos": 102, "type": "METRIC", "confidence": 0.9640986323356628}]}, {"text": " Table 8: Results on the development set.  Columns 1 and 2 show the rank of the system ac- cording to F 1 score obtained on the test set shown  in", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 102, "end_pos": 111, "type": "METRIC", "confidence": 0.9693766236305237}]}, {"text": " Table 9: Results on the test set in different settings: with punctuation errors removed from evaluation;  normalization errors removed; and when both punctuation and normalization errors are removed. Only  the best output from each team is shown.", "labels": [], "entities": []}, {"text": " Table 10: Comparing the best performing system  with two experimental hybrid systems.", "labels": [], "entities": []}]}