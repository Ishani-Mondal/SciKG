{"title": [{"text": "Rule-based Syntactic Preprocessing for Syntax-based Machine Translation", "labels": [], "entities": [{"text": "Syntax-based Machine Translation", "start_pos": 39, "end_pos": 71, "type": "TASK", "confidence": 0.6760370035966238}]}], "abstractContent": [{"text": "Several preprocessing techniques using syntactic information and linguistically motivated rules have been proposed to improve the quality of phrase-based machine translation (PBMT) output.", "labels": [], "entities": [{"text": "phrase-based machine translation (PBMT) output", "start_pos": 141, "end_pos": 187, "type": "TASK", "confidence": 0.7924981330122266}]}, {"text": "On the other hand, there has been little work on similar techniques in the context of other translation formalisms such as syntax-based SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 136, "end_pos": 139, "type": "TASK", "confidence": 0.8077732920646667}]}, {"text": "In this paper, we examine whether the sort of rule-based syntactic preprocess-ing approaches that have proved beneficial for PBMT can contribute to syntax-based SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 161, "end_pos": 164, "type": "TASK", "confidence": 0.8239175081253052}]}, {"text": "Specifically, we tailor a highly successful preprocessing method for English-Japanese PBMT to syntax-based SMT, and find that while the gains achievable are smaller than those for PBMT, significant improvements inaccuracy can be realized.", "labels": [], "entities": [{"text": "SMT", "start_pos": 107, "end_pos": 110, "type": "TASK", "confidence": 0.7766515612602234}]}], "introductionContent": [{"text": "In the widely-studied framework of phrase-based machine translation (PBMT) (, translation probabilities between phrases consisting of multiple words are calculated, and translated phrases are rearranged by the reordering model in the appropriate target language order.", "labels": [], "entities": [{"text": "phrase-based machine translation (PBMT)", "start_pos": 35, "end_pos": 74, "type": "TASK", "confidence": 0.7708548953135809}]}, {"text": "While PBMT provides a light-weight framework to learn translation models and achieves high translation quality in many language pairs, it does not directly incorporate morphological or syntactic information.", "labels": [], "entities": []}, {"text": "Thus, many preprocessing methods for PBMT using these types of information have been proposed.", "labels": [], "entities": []}, {"text": "Methods include preprocessing to obtain accurate word alignments by the division of the prefix of verbs), preprocessing to reduce the errors in verb conjugation and noun case agreement, and many others.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 49, "end_pos": 64, "type": "TASK", "confidence": 0.6985778659582138}]}, {"text": "The effectiveness of the syntactic preprocessing for PBMT has been supported by these and various related works.", "labels": [], "entities": []}, {"text": "In particular, much attention has been paid to preordering), a class of preprocessing methods for PBMT.", "labels": [], "entities": []}, {"text": "PBMT has well-known problems with language pairs that have very different word order, due to the fact that the reordering model has difficulty estimating the probability of long distance reorderings.", "labels": [], "entities": [{"text": "PBMT", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8188562393188477}]}, {"text": "Therefore, preordering methods attempt to improve the translation quality of PBMT by rearranging source language sentences into an order closer to that of the target language.", "labels": [], "entities": []}, {"text": "It's often the case that preordering methods are based on rule-based approaches, and these methods have achieved great success in ameliorating the word ordering problems faced by PBMT (.", "labels": [], "entities": [{"text": "word ordering", "start_pos": 147, "end_pos": 160, "type": "TASK", "confidence": 0.7436036169528961}]}, {"text": "One particularly successful example of rulebased syntactic preprocessing is Head Finalization (), a method of syntactic preprocessing for English to Japanese translation that has significantly improved translation quality of English-Japanese PBMT using simple rules based on the syntactic structure of the two languages.", "labels": [], "entities": [{"text": "Head Finalization", "start_pos": 76, "end_pos": 93, "type": "TASK", "confidence": 0.7126909792423248}]}, {"text": "The most central part of the method, as indicated by its name, is a reordering rule that moves the English headword to the end of the corresponding syntactic constituents to match the head-final syntactic structure of Japanese sentences.", "labels": [], "entities": []}, {"text": "Head Finalization also contains some additional preprocessing steps such as determiner elimination, particle insertion and singularization to generate a sentence that is closer to Japanese grammatical structure.", "labels": [], "entities": [{"text": "Head Finalization", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7724460661411285}, {"text": "determiner elimination", "start_pos": 76, "end_pos": 98, "type": "TASK", "confidence": 0.8242273330688477}, {"text": "particle insertion", "start_pos": 100, "end_pos": 118, "type": "TASK", "confidence": 0.7539261281490326}]}, {"text": "In addition to PBMT, there has also recently been interest in syntax-based SMT), which translates using syntactic information.", "labels": [], "entities": [{"text": "SMT", "start_pos": 75, "end_pos": 78, "type": "TASK", "confidence": 0.8217900991439819}]}, {"text": "However, few attempts have been made at syntactic preprocessing for syntax-based SMT, as the syntactic information given by the parser is already incorporated directly in the translation model.", "labels": [], "entities": [{"text": "SMT", "start_pos": 81, "end_pos": 84, "type": "TASK", "confidence": 0.8905642628669739}]}, {"text": "Notable excep-tions include methods to perform tree transformations improving correspondence between the sentence structure and word alignment, methods for binarizing parse trees to match word alignments (), and methods for adjusting label sets to be more appropriate for syntax-based SMT (.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 128, "end_pos": 142, "type": "TASK", "confidence": 0.7207900732755661}, {"text": "SMT", "start_pos": 285, "end_pos": 288, "type": "TASK", "confidence": 0.8976360559463501}]}, {"text": "It should be noted that these methods of syntactic preprocessing for syntax-based SMT are all based on automatically learned rules, and there has been little investigation of the manually-created linguisticallymotivated rules that have proved useful in preprocessing for PBMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 82, "end_pos": 85, "type": "TASK", "confidence": 0.7221531867980957}]}, {"text": "In this paper, we examine whether rule-based syntactic preprocessing methods designed for PBMT can contribute anything to syntax-based machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 135, "end_pos": 154, "type": "TASK", "confidence": 0.6960927546024323}]}, {"text": "Specifically, we examine whether the reordering and lexical processing of Head Finalization contributes to the improvement of syntax-based machine translation as it did for PBMT.", "labels": [], "entities": [{"text": "Head Finalization", "start_pos": 74, "end_pos": 91, "type": "TASK", "confidence": 0.7763920724391937}, {"text": "syntax-based machine translation", "start_pos": 126, "end_pos": 158, "type": "TASK", "confidence": 0.6741840442021688}]}, {"text": "Additionally, we examine whether it is possible to incorporate the intuitions behind the Head Finalization reordering rules as soft constraints by incorporating them as a decoder feature.", "labels": [], "entities": [{"text": "Head Finalization reordering", "start_pos": 89, "end_pos": 117, "type": "TASK", "confidence": 0.7577065626780192}]}, {"text": "As a result of our experiments, we demonstrate that rule-based lexical processing can contribute to improvement of translation quality of syntax-based machine translation.", "labels": [], "entities": [{"text": "syntax-based machine translation", "start_pos": 138, "end_pos": 170, "type": "TASK", "confidence": 0.6575222114721934}]}], "datasetContent": [{"text": "In our experiment, we examined how much each of the preprocessing steps (Reordering, Lexical Processing) contribute to improve the translation quality of PBMT and T2S.", "labels": [], "entities": []}, {"text": "We also examined the improvement in translation quality of T2S by the introduction of the Head Finalization feature.", "labels": [], "entities": [{"text": "translation", "start_pos": 36, "end_pos": 47, "type": "TASK", "confidence": 0.9541041851043701}, {"text": "Head Finalization", "start_pos": 90, "end_pos": 107, "type": "TASK", "confidence": 0.7129715234041214}]}, {"text": "For our English to Japanese translation experiments, we used NTCIR7 PATENT-MT's Patent corpus (.", "labels": [], "entities": [{"text": "English to Japanese translation", "start_pos": 8, "end_pos": 39, "type": "TASK", "confidence": 0.6459530591964722}, {"text": "NTCIR7", "start_pos": 61, "end_pos": 67, "type": "DATASET", "confidence": 0.8459155559539795}, {"text": "PATENT-MT", "start_pos": 68, "end_pos": 77, "type": "METRIC", "confidence": 0.9031363725662231}, {"text": "Patent corpus", "start_pos": 80, "end_pos": 93, "type": "DATASET", "confidence": 0.8613312244415283}]}, {"text": "shows the details of training data (train), development data (dev), and test data (test).", "labels": [], "entities": []}, {"text": "As the PBMT and T2S engines, we used the Moses ( and Travatar translation toolkits with the default settings.", "labels": [], "entities": []}, {"text": "Enju () is used to parse English sentences and KyTea) is used as a Japanese tokenizer.", "labels": [], "entities": [{"text": "parse English sentences", "start_pos": 19, "end_pos": 42, "type": "TASK", "confidence": 0.8389235337575277}]}, {"text": "We generated word alignments using GIZA++ and trained a Kneser-Ney smoothed 5-gram LM using SRILM).", "labels": [], "entities": [{"text": "word alignments", "start_pos": 13, "end_pos": 28, "type": "TASK", "confidence": 0.7243901938199997}, {"text": "SRILM", "start_pos": 92, "end_pos": 97, "type": "METRIC", "confidence": 0.5888391733169556}]}, {"text": "Minimum Error Rate Training (MERT) is used for tuning to optimize BLEU.", "labels": [], "entities": [{"text": "Minimum Error Rate Training (MERT)", "start_pos": 0, "end_pos": 34, "type": "METRIC", "confidence": 0.8994328464780535}, {"text": "BLEU", "start_pos": 66, "end_pos": 70, "type": "METRIC", "confidence": 0.9747825264930725}]}, {"text": "MERT is replicated three times to provide performance stability on test set evaluation.", "labels": [], "entities": []}, {"text": "We used BLEU () and RIBES () as evaluation measures of translation quality.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 8, "end_pos": 12, "type": "METRIC", "confidence": 0.9986839890480042}, {"text": "RIBES", "start_pos": 20, "end_pos": 25, "type": "METRIC", "confidence": 0.9926101565361023}, {"text": "translation", "start_pos": 55, "end_pos": 66, "type": "TASK", "confidence": 0.9537378549575806}]}, {"text": "RIBES is an evaluation method that focuses on word reordering information, and is known to have high correlation with human judgement for language pairs that have very different word order such as EnglishJapanese.", "labels": [], "entities": [{"text": "RIBES", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.5184336304664612}, {"text": "word reordering information", "start_pos": 46, "end_pos": 73, "type": "TASK", "confidence": 0.8116414546966553}]}, {"text": "shows translation quality for each combination of HF-feature, Reordering, and Lexical Processing.", "labels": [], "entities": []}, {"text": "Scores in boldface indicate no significant difference in comparison with the condition that has highest translation quality using the bootstrap resampling method) (p < 0.05).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Translation quality by combination of HF-feature, Reordering, and Lexical Processing. Bold  indicates results that are not statistically significantly different from the best result (39.60 BLEU in line  4 and 79.47 RIBES in line 2).", "labels": [], "entities": [{"text": "Translation", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.9619664549827576}, {"text": "Reordering", "start_pos": 60, "end_pos": 70, "type": "METRIC", "confidence": 0.9562152028083801}, {"text": "BLEU", "start_pos": 199, "end_pos": 203, "type": "METRIC", "confidence": 0.9971655011177063}, {"text": "RIBES", "start_pos": 225, "end_pos": 230, "type": "METRIC", "confidence": 0.9878169894218445}]}, {"text": " Table 4: Improvement of translation results due to Lexical Processing", "labels": [], "entities": [{"text": "translation", "start_pos": 25, "end_pos": 36, "type": "TASK", "confidence": 0.962346076965332}, {"text": "Lexical Processing", "start_pos": 52, "end_pos": 70, "type": "TASK", "confidence": 0.7961231768131256}]}]}