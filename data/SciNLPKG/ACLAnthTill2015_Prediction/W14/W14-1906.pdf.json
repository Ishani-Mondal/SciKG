{"title": [{"text": "Preliminary Test of a Real-Time, Interactive Silent Speech Interface Based on Electromagnetic Articulograph", "labels": [], "entities": []}], "abstractContent": [{"text": "A silent speech interface (SSI) maps articula-tory movement data to speech output.", "labels": [], "entities": []}, {"text": "Although still in experimental stages, silent speech interfaces hold significant potential for facilitating oral communication in persons after laryngectomy or with other severe voice impairments.", "labels": [], "entities": []}, {"text": "Despite the recent efforts on silent speech recognition algorithm development using offline data analysis, online test of SSIs have rarely been conducted.", "labels": [], "entities": [{"text": "silent speech recognition algorithm development", "start_pos": 30, "end_pos": 77, "type": "TASK", "confidence": 0.7894902944564819}]}, {"text": "In this paper, we present a preliminary, online test of a real-time, interactive SSI based on electromagnetic motion tracking.", "labels": [], "entities": [{"text": "electromagnetic motion tracking", "start_pos": 94, "end_pos": 125, "type": "TASK", "confidence": 0.6492140889167786}]}, {"text": "The SSI played back synthesized speech sounds in response to the user's tongue and lip movements.", "labels": [], "entities": []}, {"text": "Three English talkers participated in this test, where they mouthed (silently articulated) phrases using the device to complete a phrase-reading task.", "labels": [], "entities": []}, {"text": "Among the three participants , 96.67% to 100% of the mouthed phrases were correctly recognized and corresponding synthesized sounds were played after a short delay.", "labels": [], "entities": []}, {"text": "Furthermore, one participant demonstrated the feasibility of using the SSI fora short conversation.", "labels": [], "entities": []}, {"text": "The experimental results demonstrated the feasibility and potential of silent speech interfaces based on electromagnetic articulograph for future clinical applications.", "labels": [], "entities": []}], "introductionContent": [{"text": "Daily communication is often a struggle for persons who have undergone a laryngectomy, a surgical removal of the larynx due to the treatment of cancer ().", "labels": [], "entities": []}, {"text": "In 2013, about 12,260 new cases of laryngeal cancer were estimated in the United States.", "labels": [], "entities": []}, {"text": "Currently, there are only limited treatment options for these individuals including (1) esophageal speech, which involves oscillation of the esophagus and is difficult to learn; (2) tracheo-esophageal speech, in which a voice prosthesis is placed in a tracheo-esophageal puncture; and (3) electrolarynx, an external device held on the neck during articulation, which produces a robotic voice quality (.", "labels": [], "entities": []}, {"text": "Perhaps the greatest disadvantage of these approaches is that they produce abnormal sounding speech with a fundamental frequency that is low and limited in range.", "labels": [], "entities": []}, {"text": "The abnormal voice quality output severely affects the social life of people after laryngectomy (.", "labels": [], "entities": []}, {"text": "In addition, the tracheo-esophageal option requires an additional surgery, which is not suitable for every patient ().", "labels": [], "entities": []}, {"text": "Although research is being conducted on improving the voice quality of esophageal or electrolarynx speech (, new assistive technologies based on non-audio information (e.g., visual or articulatory information) maybe a good alternative approach for providing natural sounding speech output for persons after laryngectomy.", "labels": [], "entities": []}, {"text": "Visual speech recognition (or automatic lip reading) typically uses an optical camera to obtain lip and/or facial features during speech (including lip contour, color, opening, movement, etc.) and then classify these features to speech units (.", "labels": [], "entities": [{"text": "Visual speech recognition (or automatic lip reading", "start_pos": 0, "end_pos": 51, "type": "TASK", "confidence": 0.6383387185633183}]}, {"text": "However, due to the lack of information from tongue, the primary articulator, visual speech recognition (i.e., using visual information only, without tongue and audio information) may obtain a low accuracy (e.g., 30% -40% for phoneme classification,.", "labels": [], "entities": [{"text": "visual speech recognition", "start_pos": 78, "end_pos": 103, "type": "TASK", "confidence": 0.6817063093185425}, {"text": "accuracy", "start_pos": 197, "end_pos": 205, "type": "METRIC", "confidence": 0.9982130527496338}, {"text": "phoneme classification", "start_pos": 226, "end_pos": 248, "type": "TASK", "confidence": 0.7639506161212921}]}, {"text": "Furthermore, have showed any single tongue sensor (from tongue tip to tongue body 38 back on the midsagittal line) encodes significantly more information in distinguishing phonemes than do lips.", "labels": [], "entities": []}, {"text": "However, visual speech recognition is well suited for applications with smallvocabulary (e.g., a lip-reading based commandand-control system for home appliance) or using visual information as an additional source for acoustic speech recognition, referred to as audiovisual speech recognition (, because such a system based on portable camera is convenient in practical use.", "labels": [], "entities": [{"text": "visual speech recognition", "start_pos": 9, "end_pos": 34, "type": "TASK", "confidence": 0.7002143462498983}, {"text": "acoustic speech recognition", "start_pos": 217, "end_pos": 244, "type": "TASK", "confidence": 0.7201695839564005}, {"text": "audiovisual speech recognition", "start_pos": 261, "end_pos": 291, "type": "TASK", "confidence": 0.6381027599175771}]}, {"text": "In contrast, SSIs, with tongue information, have potential to obtain a high level of silent speech recognition accuracy (without audio information).", "labels": [], "entities": [{"text": "silent speech recognition", "start_pos": 85, "end_pos": 110, "type": "TASK", "confidence": 0.6189672946929932}, {"text": "accuracy", "start_pos": 111, "end_pos": 119, "type": "METRIC", "confidence": 0.853094756603241}]}, {"text": "Currently, two major obstacles for SSI development are lack of (a) fast and accurate recognition algorithms and (b) portable tongue motion tracking devices for daily use.", "labels": [], "entities": [{"text": "SSI development", "start_pos": 35, "end_pos": 50, "type": "TASK", "confidence": 0.952718585729599}, {"text": "tongue motion tracking", "start_pos": 125, "end_pos": 147, "type": "TASK", "confidence": 0.6465000212192535}]}, {"text": "SSIs convert articulatory information into text that drives a text-to-speech synthesizer.", "labels": [], "entities": []}, {"text": "Although still in developmental stages (e.g., speakerdependent recognition, small-vocabulary), SSIs even have potential to provide speech output based on prerecorded samples of the patient's own voice.", "labels": [], "entities": [{"text": "speakerdependent recognition", "start_pos": 46, "end_pos": 74, "type": "TASK", "confidence": 0.8066180646419525}, {"text": "SSIs", "start_pos": 95, "end_pos": 99, "type": "TASK", "confidence": 0.9567326307296753}]}, {"text": "Potential articulatory data acquisition methods for SSIs include ultrasound, surface electromyography electrodes, and electromagnetic articulograph (EMA).", "labels": [], "entities": [{"text": "SSIs", "start_pos": 52, "end_pos": 56, "type": "TASK", "confidence": 0.9778507351875305}]}, {"text": "Despite the recent effort on silent speech interface research, online test of SSIs has rarely been studied.", "labels": [], "entities": []}, {"text": "So far, most of the published work on SSIs has focused on development of silent speech recognition algorithm through offline analysis (i.e., using prerecorded data)).", "labels": [], "entities": [{"text": "silent speech recognition", "start_pos": 73, "end_pos": 98, "type": "TASK", "confidence": 0.6445867121219635}]}, {"text": "Ultrasoundbased SSIs have been tested online with multiple subjects and encouraging results were obtained in a phrase reading task where the subjects were asked to silently articulate sixty phrases.", "labels": [], "entities": [{"text": "Ultrasoundbased SSIs", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.36157743632793427}, {"text": "phrase reading task", "start_pos": 111, "end_pos": 130, "type": "TASK", "confidence": 0.8535465399424235}]}, {"text": "SSI based on electromagnetic sensing has been only tested using offline analysis (using pre-recorded data) collected from single subjects), although some work simulated online testing using prerecorded data (.", "labels": [], "entities": []}, {"text": "Online tests of SSIs using electromagnetic articulograph with multiple subjects are needed to show the feasibility and potential of the SSIs for future clinical applications.", "labels": [], "entities": []}, {"text": "In this paper, we report a preliminary, online test of a newly-developed, real-time, and interactive SSI based on a commercial EMA.", "labels": [], "entities": []}, {"text": "EMA tracks articulatory motion by placing small sensors on the surface of tongue and other articulators (e.g., lips and jaw).", "labels": [], "entities": []}, {"text": "EMA is well suited for the early state of SSI development because it (1) is non-invasive, (2) has a high spatial resolution in motion tracking, (3) has a high sampling rate, and (4) is affordable.", "labels": [], "entities": [{"text": "SSI", "start_pos": 42, "end_pos": 45, "type": "TASK", "confidence": 0.9592668414115906}, {"text": "motion tracking", "start_pos": 127, "end_pos": 142, "type": "TASK", "confidence": 0.7610843479633331}]}, {"text": "In this experiment, participants used the real-time SSI to complete an online phrase-reading task and one of them had a short conversation with another person.", "labels": [], "entities": []}, {"text": "The results demonstrated the feasibility and potential of SSIs based on electromagnetic sensing for future clinical applications.", "labels": [], "entities": [{"text": "SSIs", "start_pos": 58, "end_pos": 62, "type": "TASK", "confidence": 0.9741405844688416}]}, {"text": "illustrates the three-component design of the SSI: (a) real-time articulatory motion tracking using a commercial EMA, (b) online silent speech recognition (converting articulation information to text), and (c) text-to-speech synthesis for speech output.", "labels": [], "entities": [{"text": "real-time articulatory motion tracking", "start_pos": 55, "end_pos": 93, "type": "TASK", "confidence": 0.628892682492733}, {"text": "online silent speech recognition", "start_pos": 122, "end_pos": 154, "type": "TASK", "confidence": 0.59938183426857}, {"text": "text-to-speech synthesis", "start_pos": 210, "end_pos": 234, "type": "TASK", "confidence": 0.7152812778949738}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1. Phrase classification accuracy and  latency for all three participants.", "labels": [], "entities": [{"text": "Phrase classification", "start_pos": 10, "end_pos": 31, "type": "TASK", "confidence": 0.9507734477519989}, {"text": "accuracy", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.9706225395202637}, {"text": "latency", "start_pos": 46, "end_pos": 53, "type": "METRIC", "confidence": 0.9694830179214478}]}]}