{"title": [{"text": "Parmesan: Meteor without Paraphrases with Paraphrased References", "labels": [], "entities": [{"text": "Parmesan: Meteor without Paraphrases", "start_pos": 0, "end_pos": 36, "type": "DATASET", "confidence": 0.8546974897384644}]}], "abstractContent": [{"text": "This paper describes Parmesan, our submission to the 2014 Workshop on Statistical Machine Translation (WMT) met-rics task for evaluation English-to-Czech translation.", "labels": [], "entities": [{"text": "Statistical Machine Translation (WMT) met-rics task", "start_pos": 70, "end_pos": 121, "type": "TASK", "confidence": 0.8036822564899921}, {"text": "evaluation English-to-Czech translation", "start_pos": 126, "end_pos": 165, "type": "TASK", "confidence": 0.6336895227432251}]}, {"text": "We show that the Czech Meteor Paraphrase tables are so noisy that they actually can harm the performance of the metric.", "labels": [], "entities": [{"text": "Czech Meteor Paraphrase tables", "start_pos": 17, "end_pos": 47, "type": "DATASET", "confidence": 0.9434508085250854}]}, {"text": "However, they can be very useful after extensive filtering in targeted paraphrasing of Czech reference sentences prior to the evaluation.", "labels": [], "entities": []}, {"text": "Parmesan first performs targeted paraphrasing of reference sentences, then it computes the Meteor score using only the exact match on these new reference sentences.", "labels": [], "entities": [{"text": "Parmesan", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9304644465446472}, {"text": "Meteor score", "start_pos": 91, "end_pos": 103, "type": "DATASET", "confidence": 0.6575124859809875}]}, {"text": "It shows significantly higher correlation with human judgment than Meteor on the WMT12 and WMT13 data.", "labels": [], "entities": [{"text": "WMT12", "start_pos": 81, "end_pos": 86, "type": "DATASET", "confidence": 0.9711899757385254}, {"text": "WMT13 data", "start_pos": 91, "end_pos": 101, "type": "DATASET", "confidence": 0.8881597518920898}]}], "introductionContent": [{"text": "The metric for automatic evaluation of machine translation (MT) Meteor) has shown high correlation with human judgment since its appearance.", "labels": [], "entities": [{"text": "machine translation (MT) Meteor", "start_pos": 39, "end_pos": 70, "type": "TASK", "confidence": 0.819422647356987}]}, {"text": "It outperforms traditional metrics like BLEU () or NIST) as it explicitly addresses their weaknesses -it takes into account recall, distinguishes between functional and content words, allows language-specific tuning of parameters and many others.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 40, "end_pos": 44, "type": "METRIC", "confidence": 0.9737758636474609}, {"text": "NIST", "start_pos": 51, "end_pos": 55, "type": "DATASET", "confidence": 0.8744188547134399}, {"text": "recall", "start_pos": 124, "end_pos": 130, "type": "METRIC", "confidence": 0.9965157508850098}]}, {"text": "Another important advantage of Meteor is that it supports not only exact word matches between a hypothesis and its corresponding reference sentence, but also matches on the level of stems, synonyms and paraphrases.", "labels": [], "entities": []}, {"text": "The Meteor Paraphrase tables were created automatically using the pivot method () for six languages.", "labels": [], "entities": []}, {"text": "The basic setting of Meteor for evaluation of Czech sentences offers two levels of matches -exact and paraphrase.", "labels": [], "entities": [{"text": "exact", "start_pos": 92, "end_pos": 97, "type": "METRIC", "confidence": 0.997944176197052}]}, {"text": "In this paper, we show the impact of the quality of paraphrases on the performance of Meteor.", "labels": [], "entities": [{"text": "Meteor", "start_pos": 86, "end_pos": 92, "type": "DATASET", "confidence": 0.8994133472442627}]}, {"text": "We demonstrate that the Czech Meteor Paraphrase tables are full of noise and their addition to the metric worsens its correlation with human judgment.", "labels": [], "entities": [{"text": "Czech Meteor Paraphrase tables", "start_pos": 24, "end_pos": 54, "type": "DATASET", "confidence": 0.9512603133916855}]}, {"text": "However, they can be very useful (after extensive filtering) in creating new reference sentences by targeted paraphrasing.", "labels": [], "entities": []}, {"text": "Parmesan 2 starts with a simple greedy algorithm for substitution of synonymous words from a hypothesis in its corresponding reference sentence.", "labels": [], "entities": []}, {"text": "Further, we apply Depfix ( to fix grammar errors that might arise by the substitutions.", "labels": [], "entities": []}, {"text": "Our method is independent of the evaluation metric used.", "labels": [], "entities": []}, {"text": "In this paper, we use Meteor for its consistently high correlation with human judgment and we attempt to tune it further by modifying its paraphrase tables.", "labels": [], "entities": [{"text": "Meteor", "start_pos": 22, "end_pos": 28, "type": "DATASET", "confidence": 0.884739339351654}]}, {"text": "We show that reducing the size of the Meteor Paraphrase tables is very beneficial.", "labels": [], "entities": [{"text": "Meteor Paraphrase tables", "start_pos": 38, "end_pos": 62, "type": "DATASET", "confidence": 0.8391418059666952}]}, {"text": "On the WMT12 and WMT13 data, the Meteor scores computed using only the exact match on our new references significantly outperform Meteor with both exact and paraphrase match on original references.", "labels": [], "entities": [{"text": "WMT12", "start_pos": 7, "end_pos": 12, "type": "DATASET", "confidence": 0.9747480750083923}, {"text": "WMT13 data", "start_pos": 17, "end_pos": 27, "type": "DATASET", "confidence": 0.9432668089866638}, {"text": "exact", "start_pos": 147, "end_pos": 152, "type": "METRIC", "confidence": 0.9727169871330261}]}, {"text": "However, this result was not confirmed by this year's data.", "labels": [], "entities": []}, {"text": "We perform our experiments on English-toCzech translations, but the method is largely language independent.", "labels": [], "entities": [{"text": "English-toCzech translations", "start_pos": 30, "end_pos": 58, "type": "TASK", "confidence": 0.47626253962516785}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Average number of one-word paraphrases  per sentence found in WordNet, filtered Meteor ta- bles and their union over all systems.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 72, "end_pos": 79, "type": "DATASET", "confidence": 0.9756079912185669}]}, {"text": " Table 2: Pearson's correlation of different metrics  with the silver standard.", "labels": [], "entities": []}, {"text": " Table 3: Different paraphrase tables for Meteor and their size (number of paraphrase pairs).", "labels": [], "entities": []}, {"text": " Table 4: Pearson's correlation of Meteor and the silver standard.", "labels": [], "entities": [{"text": "Pearson", "start_pos": 10, "end_pos": 17, "type": "DATASET", "confidence": 0.8246718049049377}, {"text": "Meteor", "start_pos": 35, "end_pos": 41, "type": "DATASET", "confidence": 0.6614132523536682}]}, {"text": " Table 5: Pearson's correlation of Meteor and the gold standard -Expected Wins (Bojar et al., 2013). The  results corresponds very well with the silver standard in Table 4.", "labels": [], "entities": [{"text": "Meteor", "start_pos": 35, "end_pos": 41, "type": "DATASET", "confidence": 0.6827990412712097}, {"text": "gold standard -Expected Wins", "start_pos": 50, "end_pos": 78, "type": "METRIC", "confidence": 0.7323937296867371}]}, {"text": " Table 6: The frequency column shows average  number of substitution per sentence using the orig- inal Meteor Paraphrase tables only. The rest shows  Pearson's correlation with the silver standard us- ing these paraphrases.", "labels": [], "entities": []}, {"text": " Table 7: Pearson's correlation of Meteor and the silver standard.", "labels": [], "entities": [{"text": "Pearson", "start_pos": 10, "end_pos": 17, "type": "DATASET", "confidence": 0.794728696346283}, {"text": "Meteor", "start_pos": 35, "end_pos": 41, "type": "DATASET", "confidence": 0.6913133263587952}]}, {"text": " Table 8: Pearson's correlation of Meteor and the gold standard -TrueSkill (Bojar et al., 2014). Note that  as opposed to official WMT14 results, the version 1.4 of Meteor is still used in this table.", "labels": [], "entities": [{"text": "Meteor", "start_pos": 35, "end_pos": 41, "type": "DATASET", "confidence": 0.7941271662712097}, {"text": "TrueSkill", "start_pos": 65, "end_pos": 74, "type": "DATASET", "confidence": 0.7256073355674744}, {"text": "WMT14", "start_pos": 131, "end_pos": 136, "type": "DATASET", "confidence": 0.8447371125221252}, {"text": "Meteor", "start_pos": 165, "end_pos": 171, "type": "DATASET", "confidence": 0.9234178066253662}]}, {"text": " Table 9: Pearson's correlation of Meteor and the silver standard on sentences originally Czech or English  only. In this case, the interpretation of human judgment was computed only on those sentences as well.", "labels": [], "entities": [{"text": "interpretation of human judgment", "start_pos": 132, "end_pos": 164, "type": "TASK", "confidence": 0.8342188000679016}]}]}