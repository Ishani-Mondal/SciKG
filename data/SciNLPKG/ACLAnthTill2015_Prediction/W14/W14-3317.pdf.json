{"title": [], "abstractContent": [{"text": "This paper describes the statistical machine translation (SMT) systems developed at RWTH Aachen University for the German\u2192English translation task of the ACL 2014 Eighth Workshop on Statistical Machine Translation (WMT 2014).", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 25, "end_pos": 62, "type": "TASK", "confidence": 0.8193747301896414}, {"text": "German\u2192English translation task of the ACL 2014 Eighth Workshop on Statistical Machine Translation (WMT 2014)", "start_pos": 115, "end_pos": 224, "type": "TASK", "confidence": 0.7913989079625983}]}, {"text": "Both hierarchical and phrase-based SMT systems are applied employing hierarchical phrase reordering and word class language models.", "labels": [], "entities": [{"text": "SMT", "start_pos": 35, "end_pos": 38, "type": "TASK", "confidence": 0.8509163856506348}]}, {"text": "For the phrase-based system , we run discriminative phrase training.", "labels": [], "entities": []}, {"text": "In addition, we describe our preprocessing pipeline for German\u2192English.", "labels": [], "entities": []}], "introductionContent": [{"text": "For the WMT 2014 shared translation task 1 RWTH utilized state-of-the-art phrase-based and hierarchical translation systems.", "labels": [], "entities": [{"text": "WMT 2014 shared translation task", "start_pos": 8, "end_pos": 40, "type": "TASK", "confidence": 0.7548773407936096}]}, {"text": "First, we describe our preprocessing pipeline for the language pair German\u2192English in Section 2.", "labels": [], "entities": []}, {"text": "Furthermore, we utilize morpho-syntactic analysis to preprocess the data (Section 2.3).", "labels": [], "entities": []}, {"text": "In Section 3, we give a survey of the employed systems and the basic methods they implement.", "labels": [], "entities": []}, {"text": "More details are given about the discriminative phrase training and the hierarchical reordering model for hierarchical machine translation (Section 3.5).", "labels": [], "entities": [{"text": "hierarchical machine translation", "start_pos": 106, "end_pos": 138, "type": "TASK", "confidence": 0.6583844621976217}]}, {"text": "Experimental results are discussed in Section 4.", "labels": [], "entities": []}], "datasetContent": [{"text": "The results of the phrase-based system (SCSS) as well as the hierarchical phrase-based system (HPBT) are summarized in.", "labels": [], "entities": []}, {"text": "The phrase-based baseline system, which includes the hierarchical reordering model by and is tuned on newstest2012, reaches a performance of 25.9% BLEU on newstest2013.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 147, "end_pos": 151, "type": "METRIC", "confidence": 0.9995162487030029}]}, {"text": "Adding the word class language model improves performance by 0.4% BLEU absolute and the first round of discriminative phrase training by 0.5% BLEU absolute.", "labels": [], "entities": [{"text": "BLEU absolute", "start_pos": 66, "end_pos": 79, "type": "METRIC", "confidence": 0.9581863284111023}, {"text": "BLEU absolute", "start_pos": 142, "end_pos": 155, "type": "METRIC", "confidence": 0.9705038070678711}]}, {"text": "Next, we switched to tuning on a concatenation of newstest2011 and newstest2012, which we expect to be more reliable with respect to unseen data.", "labels": [], "entities": [{"text": "newstest2011", "start_pos": 50, "end_pos": 62, "type": "DATASET", "confidence": 0.961728572845459}, {"text": "newstest2012", "start_pos": 67, "end_pos": 79, "type": "DATASET", "confidence": 0.913682758808136}]}, {"text": "Although the BLEU score does not improve and TER goes up slightly, we kept this tuning set in the subsequent setups, as it yielded longer translations, which in our experience will usually be preferred by human evaluators.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 13, "end_pos": 23, "type": "METRIC", "confidence": 0.9765353202819824}, {"text": "TER", "start_pos": 45, "end_pos": 48, "type": "METRIC", "confidence": 0.9996360540390015}]}, {"text": "Switching from the interpolated language model to the unpruned language model trained with KenLM on the full concatenated monolingual training data in a single pass gained us another 0.3% BLEU.", "labels": [], "entities": [{"text": "KenLM", "start_pos": 91, "end_pos": 96, "type": "DATASET", "confidence": 0.7858827114105225}, {"text": "BLEU", "start_pos": 188, "end_pos": 192, "type": "METRIC", "confidence": 0.999295711517334}]}, {"text": "For the final system, we ran a second round of discriminative training on different training data (cf. Section 3.4), which increased performance by 0.1% BLEU to the final score 27.2.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 153, "end_pos": 157, "type": "METRIC", "confidence": 0.9996230602264404}]}, {"text": "For the phrase-based system, we also experimented with weighted phrase extraction), but did not observe improvements.", "labels": [], "entities": [{"text": "phrase extraction", "start_pos": 64, "end_pos": 81, "type": "TASK", "confidence": 0.6959470212459564}]}, {"text": "The hierarchical phrase-based baseline without any additional model is on the same level as the phrase-based system including the word class language model, hierarchical reordering model and discriminative phrase training in terms of BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 234, "end_pos": 238, "type": "METRIC", "confidence": 0.9923384189605713}]}, {"text": "However, extending the system with a word class language model or the additional reordering models does not seem to help.", "labels": [], "entities": []}, {"text": "Even the combination of both models does not improve the translation quality.", "labels": [], "entities": [{"text": "translation", "start_pos": 57, "end_pos": 68, "type": "TASK", "confidence": 0.9691144227981567}]}, {"text": "Note, that the hierarchical system was tuned on the concatenation newstest2011 and newstest2012.", "labels": [], "entities": [{"text": "newstest2012", "start_pos": 83, "end_pos": 95, "type": "DATASET", "confidence": 0.9257960915565491}]}, {"text": "The final system employs both word class language model and hierarchical reordering model.", "labels": [], "entities": []}, {"text": "Both phrase-based and hierarchical phrasebased final systems are used in the EU-Bridge system combination).", "labels": [], "entities": [{"text": "EU-Bridge system", "start_pos": 77, "end_pos": 93, "type": "DATASET", "confidence": 0.9183616936206818}]}], "tableCaptions": [{"text": " Table 2: Corpus statistics after each filtering step and compound splitting.", "labels": [], "entities": [{"text": "compound splitting", "start_pos": 58, "end_pos": 76, "type": "TASK", "confidence": 0.7444264590740204}]}, {"text": " Table 2. The language model are  4-grams trained on the respective target side of  the bilingual data, 1  2 of the Shuffled News Crawl  corpus, 1  4 of the 10 9 French-English corpus and", "labels": [], "entities": [{"text": "Shuffled News Crawl  corpus", "start_pos": 116, "end_pos": 143, "type": "DATASET", "confidence": 0.7719689011573792}]}, {"text": " Table 3: Results (truecase) for the German\u2192English translation task. BLEU and TER are given in  percentage. All HPBT setups are tuned on the concatenation of newstest2012 and newstest2013. The  very first SCSS setups are optimized on newstest2012 only.", "labels": [], "entities": [{"text": "German\u2192English translation task", "start_pos": 37, "end_pos": 68, "type": "TASK", "confidence": 0.6436197578907012}, {"text": "BLEU", "start_pos": 70, "end_pos": 74, "type": "METRIC", "confidence": 0.9995916485786438}, {"text": "TER", "start_pos": 79, "end_pos": 82, "type": "METRIC", "confidence": 0.9963468909263611}, {"text": "newstest2013", "start_pos": 176, "end_pos": 188, "type": "DATASET", "confidence": 0.9100793600082397}]}]}