{"title": [{"text": "Extracting Socioeconomic Patterns from the News: Modelling Text and Outlet Importance Jointly", "labels": [], "entities": [{"text": "Extracting Socioeconomic Patterns from the News", "start_pos": 0, "end_pos": 47, "type": "TASK", "confidence": 0.8876710732777914}, {"text": "Modelling Text and Outlet Importance", "start_pos": 49, "end_pos": 85, "type": "TASK", "confidence": 0.7091849625110627}]}], "abstractContent": [{"text": "Information from news articles can be used to study correlations between textual discourse and socioeconomic patterns.", "labels": [], "entities": []}, {"text": "This work focuses on the task of understanding how words contained in the news as well as the news outlets themselves may relate to a set of indicators, such as economic sentiment or unemployment rates.", "labels": [], "entities": []}, {"text": "The bilinear nature of the applied regression model facilitates learning jointly word and outlet importance, supervised by these indicators.", "labels": [], "entities": []}, {"text": "By evaluating the predictive ability of the extracted features, we can also assess their relevance to the target socioeconomic phenomena.", "labels": [], "entities": []}, {"text": "Therefore, our approach can be formulated as a potential NLP tool, particularly suitable to the computational social science community, as it can be used to interpret connections between vast amounts of textual content and measurable society-driven factors.", "labels": [], "entities": []}], "introductionContent": [{"text": "Vast amounts of user-generated content on the Internet as well as digitised textual resources allow us to study text in connection to real world events across large intervals of time.", "labels": [], "entities": []}, {"text": "Over the last decade, there has been a shift in user news consumption starting with a move from offline to online sources (); in more recent years user-generated news have also become prominent.", "labels": [], "entities": []}, {"text": "However, traditional news outlets continue to be a central reference point as they still have the advantage of being professionally authored, alleviating the noisy nature of citizen journalism formats.", "labels": [], "entities": []}, {"text": "Here, we present a framework for analysing socioeconomic patterns in news articles.", "labels": [], "entities": []}, {"text": "In contrast to prior approaches, which primarily focus on the textual contents, our analysis shows how Machine Learning methods can be used to gain insights into the interplay between text in news articles, the news outlets and socioeconomic indicators.", "labels": [], "entities": []}, {"text": "Our experiments are performed on a set of EU-related news summaries spanning over 8 years, with the intention to study two basic economic factors: EU's unemployment rate and Economic Sentiment Index (ESI).", "labels": [], "entities": [{"text": "Economic Sentiment Index (ESI)", "start_pos": 174, "end_pos": 204, "type": "METRIC", "confidence": 0.7690628170967102}]}, {"text": "To determine connections between the news, the outlets and the indicators of interest, we formulate our learning task as bilinear text-based regression).", "labels": [], "entities": []}, {"text": "Approaches to learning the correlation of news, or text in general, with real world indicators have been performed in both unsupervised and supervised settings.", "labels": [], "entities": []}, {"text": "For example, uncover interesting patterns in EU's Mediasphere, whereas demonstrate that news articles can predict financial indicators.", "labels": [], "entities": [{"text": "EU's Mediasphere", "start_pos": 45, "end_pos": 61, "type": "DATASET", "confidence": 0.7067285577456156}]}, {"text": "Conversely, show that emotions in the textual content of books reflect back on inflation and unemployment rates during the 20th century.", "labels": [], "entities": []}, {"text": "Recently, Social Media text has been intensively studied as a quicker, unobtrusive and cheaper alternative to traditional surveys.", "labels": [], "entities": []}, {"text": "Application areas include politics, finance), health) or psychology.", "labels": [], "entities": []}, {"text": "In this paper, we apply a modified version of a bilinear regularised regression model (BEN) proposed for the task of voting intention inference from Twitter content ().", "labels": [], "entities": [{"text": "bilinear regularised regression model (BEN)", "start_pos": 48, "end_pos": 91, "type": "METRIC", "confidence": 0.5640766322612762}, {"text": "voting intention inference from Twitter content", "start_pos": 117, "end_pos": 164, "type": "TASK", "confidence": 0.7968090375264486}]}, {"text": "The main characteristic of BEN is the ability of modelling word frequencies as well as individual user importance in a joint optimisation task.", "labels": [], "entities": [{"text": "BEN", "start_pos": 27, "end_pos": 30, "type": "METRIC", "confidence": 0.6551144123077393}]}, {"text": "By applying it in the context of supervised news analysis, we are able to visualise relevant discourse to a particular socioeconomic factor, identifying relevant words together with important outlets.", "labels": [], "entities": []}], "datasetContent": [{"text": "Both models are applied to the news summaries data set with the aim to predict EU's ESI and rate of unemployment.", "labels": [], "entities": [{"text": "news summaries data set", "start_pos": 31, "end_pos": 54, "type": "DATASET", "confidence": 0.7943545281887054}, {"text": "ESI", "start_pos": 84, "end_pos": 87, "type": "METRIC", "confidence": 0.5729600191116333}]}, {"text": "The predictive capability of the derived models, assessed by their respective inference performance, is used as a metric for judging the degree of relevance between the learnt model parameters -word and outlet weights -and the response variable.", "labels": [], "entities": []}, {"text": "A strong predictive performance increases confidence on the soundness of those parameters.", "labels": [], "entities": []}, {"text": "To match input with the monthly temporal resolution of the response variables, we compute the mean monthly term frequencies for each outlet.", "labels": [], "entities": []}, {"text": "In this way, we emulate a scenario where we always train on past and predict future points.", "labels": [], "entities": []}, {"text": "Performance results for LEN and BEN are presented in; we show the average Root Mean Squared Error (RMSE) as well as an error rate (RMSE over \u00b5(y)) across folds to allow fora better interpretation.", "labels": [], "entities": [{"text": "BEN", "start_pos": 32, "end_pos": 35, "type": "METRIC", "confidence": 0.9960534572601318}, {"text": "Root Mean Squared Error (RMSE)", "start_pos": 74, "end_pos": 104, "type": "METRIC", "confidence": 0.8941753676959446}, {"text": "error rate", "start_pos": 119, "end_pos": 129, "type": "METRIC", "confidence": 0.9487765729427338}, {"text": "RMSE", "start_pos": 131, "end_pos": 135, "type": "METRIC", "confidence": 0.616487443447113}]}, {"text": "BEN outperforms LEN in both tasks, with a clearer improvement when predicting ESI.", "labels": [], "entities": [{"text": "BEN", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.991426944732666}, {"text": "LEN", "start_pos": 16, "end_pos": 19, "type": "METRIC", "confidence": 0.985446572303772}, {"text": "predicting ESI", "start_pos": 67, "end_pos": 81, "type": "TASK", "confidence": 0.5985461622476578}]}, {"text": "Predictions for all folds are depicted in together with the actual values.", "labels": [], "entities": []}, {"text": "Note that reformulating the problem into a multi-task learning scenario, where ESI and unemployment are modelled jointly did not improve inference performance.", "labels": [], "entities": [{"text": "ESI", "start_pos": 79, "end_pos": 82, "type": "METRIC", "confidence": 0.8751802444458008}]}, {"text": "The relatively small average error rates (< 8.8%) make meaningful a further analysis of the model's outputs.", "labels": [], "entities": []}, {"text": "Due to space limitations, we choose to focus on the most recent results, depicting the models derived in the 10th fold.", "labels": [], "entities": []}, {"text": "Following the example of, we use a word cloud visu-   alisation, where the font size is proportional to the derived weights by applying BEN, flipped terms denote negative weights and colours are determined by the frequency of use in the corpus.", "labels": [], "entities": [{"text": "BEN", "start_pos": 136, "end_pos": 139, "type": "METRIC", "confidence": 0.9981111288070679}]}, {"text": "Word clouds depict the top-60 positively and negatively weighted n-grams (120 in total) together with the top-30 outlets; bigrams are separated by ' '.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: 10-fold validation average RMSEs (and  error rates) for LEN and BEN on ESI and unem- ployment rates prediction.", "labels": [], "entities": [{"text": "BEN", "start_pos": 74, "end_pos": 77, "type": "METRIC", "confidence": 0.9939084053039551}]}]}