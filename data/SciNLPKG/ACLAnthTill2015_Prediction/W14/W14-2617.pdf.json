{"title": [{"text": "Improving Agreement and Disagreement Identification in Online Discussions with A Socially-Tuned Sentiment Lexicon", "labels": [], "entities": [{"text": "Improving Agreement and Disagreement Identification", "start_pos": 0, "end_pos": 51, "type": "TASK", "confidence": 0.9088099956512451}]}], "abstractContent": [{"text": "We study the problem of agreement and disagreement detection in online discussions.", "labels": [], "entities": [{"text": "agreement and disagreement detection", "start_pos": 24, "end_pos": 60, "type": "TASK", "confidence": 0.6128916069865227}]}, {"text": "An isotonic Conditional Random Fields (isotonic CRF) based sequential model is proposed to make predictions on sentence-or segment-level.", "labels": [], "entities": []}, {"text": "We automatically construct a socially-tuned lexicon that is bootstrapped from existing general-purpose sentiment lexicons to further improve the performance.", "labels": [], "entities": []}, {"text": "We evaluate our agreement and disagreement tagging model on two disparate online discussion corpora-Wikipedia Talk pages and online debates.", "labels": [], "entities": [{"text": "agreement and disagreement tagging", "start_pos": 16, "end_pos": 50, "type": "TASK", "confidence": 0.627166360616684}]}, {"text": "Our model is shown to outperform the state-of-the-art approaches in both datasets.", "labels": [], "entities": []}, {"text": "For example, the iso-tonic CRF model achieves F1 scores of 0.74 and 0.67 for agreement and disagreement detection, when a linear chain CRF obtains 0.58 and 0.56 for the discussions on Wikipedia Talk pages.", "labels": [], "entities": [{"text": "F1", "start_pos": 46, "end_pos": 48, "type": "METRIC", "confidence": 0.9996565580368042}, {"text": "agreement and disagreement detection", "start_pos": 77, "end_pos": 113, "type": "TASK", "confidence": 0.5619255676865578}]}], "introductionContent": [{"text": "We are in an era where people can easily voice and exchange their opinions on the internet through forums or social media.", "labels": [], "entities": []}, {"text": "Mining public opinion and the social interactions from online discussions is an important task, which has a wide range of applications.", "labels": [], "entities": [{"text": "Mining public opinion", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.8804628650347391}]}, {"text": "For example, by analyzing the users' attitude in forum posts on social and political problems, it is able to identify ideological stance) and user relations (, and thus further discover subgroups; Abu-Jbara et al., 2012) with similar ideological viewpoint.", "labels": [], "entities": []}, {"text": "Meanwhile, catching the sentiment in the conversation can help detect online disputes, reveal popular or controversial topics, and potentially disclose the public opinion formation process.", "labels": [], "entities": [{"text": "catching the sentiment in the conversation", "start_pos": 11, "end_pos": 53, "type": "TASK", "confidence": 0.8537546197573344}, {"text": "public opinion formation", "start_pos": 156, "end_pos": 180, "type": "TASK", "confidence": 0.7715661525726318}]}, {"text": "In this work, we study the problem of agreement and disagreement identification in online discussions.", "labels": [], "entities": [{"text": "agreement and disagreement identification in online discussions", "start_pos": 38, "end_pos": 101, "type": "TASK", "confidence": 0.6949301745210376}]}, {"text": "Sentence-level agreement and disagreement detection for this domain is challenging in its own right due to the dynamic nature of online conversations, and the less formal, and usually very emotional language used.", "labels": [], "entities": [{"text": "Sentence-level agreement and disagreement detection", "start_pos": 0, "end_pos": 51, "type": "TASK", "confidence": 0.7394659578800201}]}, {"text": "As an example, consider a snippet of discussion from Wikipedia Talk page for article \"Iraq War\" where editors argue on the correctness of the information in the opening paragraph (.", "labels": [], "entities": [{"text": "Wikipedia Talk page", "start_pos": 53, "end_pos": 72, "type": "DATASET", "confidence": 0.9468962550163269}]}, {"text": "\"So what?\" should presumably be tagged as a negative sentence as should the sentence \"If you're going to troll, do us all a favor and stick to the guidelines.\".", "labels": [], "entities": []}, {"text": "We hypothesize that these, and other, examples will be difficult for the tagger unless the context surrounding each sentence is considered and in the absence of a sentiment lexicon tuned for conversational text.", "labels": [], "entities": []}, {"text": "As a result, we investigate isotonic Conditional Random Fields (isotonic CRF)) for the sentiment tagging task since they preserve the advantages of the popular CRF sequential tagging models () while providing an efficient mechanism to encode domain knowledge -in our case, a sentiment lexicon -through isotonic constraints on the model parameters.", "labels": [], "entities": [{"text": "sentiment tagging", "start_pos": 87, "end_pos": 104, "type": "TASK", "confidence": 0.874261736869812}]}, {"text": "In particular, we bootstrap the construction of a sentiment lexicon from Wikipedia talk pages using the lexical items in existing general-purpose sentiment lexicons as seeds and in conjunction with an existing label propagation algorithm ().", "labels": [], "entities": [{"text": "label propagation", "start_pos": 210, "end_pos": 227, "type": "TASK", "confidence": 0.7651114165782928}]}, {"text": "To summarize, our chief contributions include: (1) We propose an agreement and disagreement identification model based on isotonic Conditional Random Fields ( to identify users' attitude in online discussion.", "labels": [], "entities": [{"text": "agreement and disagreement identification", "start_pos": 65, "end_pos": 106, "type": "TASK", "confidence": 0.6345323696732521}]}, {"text": "Our predictions that are made on the sentence-Zer0faults: So questions comments feedback welcome.", "labels": [], "entities": []}, {"text": "I just hope we can remove the assertations that WMD's were in fact the sole reason for the US invasion, considering that HJ Res 114 covers many many reasons.", "labels": [], "entities": [{"text": "HJ Res 114", "start_pos": 121, "end_pos": 131, "type": "DATASET", "confidence": 0.9379787445068359}]}, {"text": ">Mr. Tibbs: So basically what you want to do is remove all mention of the cassus belli of the Iraq War and try to create the false impression that this military action was as inevitable as the sunrise.", "labels": [], "entities": []}, {"text": "No. Just because things didn't turnout the way the Bush administration wanted doesn't give you license to rewrite history.", "labels": [], "entities": []}, {"text": ">>MONGO: Regardless, the article is an antiwar propaganda tool.", "labels": [], "entities": [{"text": "MONGO", "start_pos": 2, "end_pos": 7, "type": "METRIC", "confidence": 0.9723768830299377}]}, {"text": ">>>Mr. Tibbs: So what?", "labels": [], "entities": []}, {"text": "That wasn't the cassus belli and trying to give that impression After the Fact is Untrue.", "labels": [], "entities": [{"text": "After the Fact", "start_pos": 64, "end_pos": 78, "type": "TASK", "confidence": 0.8545426726341248}]}, {"text": "Hell, the reason it wasn't the cassus belli is because there are dictators in Africa that make Saddam look like a pussycat...", "labels": [], "entities": []}, {"text": ">>Haizum: Start using the proper format or it's over for your comments.", "labels": [], "entities": []}, {"text": "If you're going to troll, do us all a favor and stick to the guidelines.", "labels": [], "entities": [{"text": "troll", "start_pos": 19, "end_pos": 24, "type": "TASK", "confidence": 0.9484716057777405}]}, {"text": "Tmorton166: Hi, I wonder if, as an outsider to this debate I can put my word inhere.", "labels": [], "entities": [{"text": "Tmorton166", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.5674535632133484}]}, {"text": "I considered mediating this discussion however I'd prefer just to comment and leave it at that : >>Tmorton166: ...", "labels": [], "entities": [{"text": "Tmorton166", "start_pos": 99, "end_pos": 109, "type": "METRIC", "confidence": 0.484656423330307}]}, {"text": "To suggest in the opening paragraph that the ONLY reason for the war was WMD's is wrong -because it simply isn't.", "labels": [], "entities": [{"text": "ONLY", "start_pos": 45, "end_pos": 49, "type": "METRIC", "confidence": 0.9949157238006592}]}, {"text": "However I agree that the emphasis needs to be on the armaments crisis because it was the reason sold to the public and the major one used to justify the invasion but it needs to acknowledge that there was at least 12 reasons for the war as well.", "labels": [], "entities": []}, {"text": "...: Example discussion from wikipedia talk page for article \"Iraq War\", where editors discuss about the correctness of the information in the opening paragraph.", "labels": [], "entities": [{"text": "Iraq War\"", "start_pos": 62, "end_pos": 71, "type": "DATASET", "confidence": 0.9272833267847697}]}, {"text": "We only show some sentences that are relevant for demonstration.", "labels": [], "entities": []}, {"text": "Other sentences are omitted by ellipsis.", "labels": [], "entities": []}, {"text": "Names of editors are in bold.", "labels": [], "entities": []}, {"text": "\">\" is an indicator for the reply structure, where turns starting with > are response for most previous turn that with one less >.", "labels": [], "entities": []}, {"text": "We use \"NN\", \"N\", and \"PP\" to indicate \"strongly disagree\", \"disagree\", and \"strongly agree\".", "labels": [], "entities": []}, {"text": "Sentences in blue are examples whose sentiment is hard to detect by an existing lexicon. or segment-level, are able to discover fine-grained sentiment flow within each turn, which can be further applied in other applications, such as dispute detection or argumentation structure analysis.", "labels": [], "entities": [{"text": "dispute detection", "start_pos": 234, "end_pos": 251, "type": "TASK", "confidence": 0.902964860200882}, {"text": "argumentation structure analysis", "start_pos": 255, "end_pos": 287, "type": "TASK", "confidence": 0.8608504136403402}]}, {"text": "(2) Furthermore, we construct anew sentiment lexicon for online discussion.", "labels": [], "entities": []}, {"text": "We show that the learned lexicon significantly improves performance over systems that use existing generalpurpose lexicons (i.e. MPQA lexicon (), General Inquirer (, and SentiWordNet ().", "labels": [], "entities": [{"text": "MPQA lexicon", "start_pos": 129, "end_pos": 141, "type": "DATASET", "confidence": 0.9106623232364655}, {"text": "General Inquirer", "start_pos": 146, "end_pos": 162, "type": "DATASET", "confidence": 0.8154124021530151}]}, {"text": "Our lexicon is constructed from a very large-scale discussion corpus based on Wikipedia talk page, where previous work for constructing online discussion lexicon relies on human annotations derived from limited number of conversations.", "labels": [], "entities": []}, {"text": "In the remainder of the paper, we describe first the related work (Section 2).", "labels": [], "entities": []}, {"text": "Then we introduce the sentence-level agreement and disagreement identification model (Section 3) as well as the label propagation algorithm for lexicon construction (Section 4).", "labels": [], "entities": [{"text": "sentence-level agreement and disagreement identification", "start_pos": 22, "end_pos": 78, "type": "TASK", "confidence": 0.5476229608058929}, {"text": "label propagation", "start_pos": 112, "end_pos": 129, "type": "TASK", "confidence": 0.7083722949028015}, {"text": "lexicon construction", "start_pos": 144, "end_pos": 164, "type": "TASK", "confidence": 0.7253338098526001}]}, {"text": "After explain the experimental setup, we display the results and provide further analysis in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "The first dataset we use is Authority and Alignment in Wikipedia Discussions (AAWD) corpus.", "labels": [], "entities": [{"text": "Authority and Alignment in Wikipedia Discussions (AAWD)", "start_pos": 28, "end_pos": 83, "type": "TASK", "confidence": 0.7852731810675727}]}, {"text": "AAWD consists of 221 English Wikipedia discussions with agreement and disagreement annotations.", "labels": [], "entities": [{"text": "AAWD", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8794640898704529}]}, {"text": "The annotation of AAWD is made at utteranceor turn-level, where a turn is defined as continuous body of text uttered by the same participant.", "labels": [], "entities": []}, {"text": "Annotators either label each utterance as agreement, disagreement or neutral, and select the corresponding spans of text, or label the full turn.", "labels": [], "entities": []}, {"text": "Each turn is annotated by two or three people.", "labels": [], "entities": []}, {"text": "To induce an utterance-level label for instances that have only a turn-level label, we assume they have the same label as the turn.", "labels": [], "entities": []}, {"text": "To train our sentiment model, we further transform agreement and disagreement labels (i.e. 3-way) into the 5-way labels.", "labels": [], "entities": []}, {"text": "For utterances that are annotated as agreement and have the text span specified by at least two annotators, they are treated as \"strongly agree\" (PP).", "labels": [], "entities": []}, {"text": "If an utterance is only selected as agreement by one annotator or it gets the label by turn-level annotation, it is \"agree\" (P).", "labels": [], "entities": [{"text": "agree\" (P)", "start_pos": 117, "end_pos": 127, "type": "METRIC", "confidence": 0.8637549400329589}]}, {"text": "\"Strongly disagree\" (NN) and \"disagree\" (N) are collected in the same way from disagreement label.", "labels": [], "entities": []}, {"text": "All others are neutral (O).", "labels": [], "entities": []}, {"text": "In total, we have 16,501 utterances.", "labels": [], "entities": []}, {"text": "1,930 and 1,102 utterances are labeled as \"NN\" and \"N\".", "labels": [], "entities": []}, {"text": "532 and 99 of them are \"PP\" and \"P\".", "labels": [], "entities": []}, {"text": "All other 12,648 are neutral samples.", "labels": [], "entities": []}, {"text": "The second dataset is the Internet Argument Corpus (IAC) ( collected from an online debate forum.", "labels": [], "entities": [{"text": "Internet Argument Corpus (IAC)", "start_pos": 26, "end_pos": 56, "type": "DATASET", "confidence": 0.7355985194444656}]}, {"text": "Each discussion in IAC consists of multiple posts, where we treat each post as a turn.", "labels": [], "entities": []}, {"text": "Most posts (72.3%) contain quoted content from the posts they target at or other resources.", "labels": [], "entities": []}, {"text": "A post can have more than one quote, which naturally break the post into multiple segments.", "labels": [], "entities": []}, {"text": "1,806 discussions are annotated with agreement and disagreement on the segmentlevel from -5 to 5, with -5 as strongly disagree and 5 as strongly agree.", "labels": [], "entities": []}, {"text": "We first compute the average score for each segment among different annotators and transform the score into sentiment label in the following way.", "labels": [], "entities": []}, {"text": "In the test phase, utterances or segments predicted with NN or N are treated as disagreement; the ones predicted as PP or P are agreement; O is neutral.", "labels": [], "entities": []}, {"text": "Moreover, we evaluate the effectiveness of features by adding one type of features each time.", "labels": [], "entities": []}, {"text": "The results are listed in.", "labels": [], "entities": []}, {"text": "As it can be seen, the performance gets improved incrementally with every new set of features.", "labels": [], "entities": []}, {"text": "We also utilize \u03c7 2 -test to highlight some of the salient features on the two datasets.", "labels": [], "entities": []}, {"text": "We can see from that, for online debates (IAC), some features are highly topic related, such as \"the male\" or \"the scientist\".", "labels": [], "entities": [{"text": "online debates (IAC)", "start_pos": 26, "end_pos": 46, "type": "TASK", "confidence": 0.7248420715332031}]}, {"text": "This observation concurs with the conclusion in that features with topic information are indicative for agreement and disagreement detection.: Results on Wikipedia talk page (AAWD) (with soft F1 score) and online debate (IAC) with different feature sets (i.e Lexical, Syntacitc/Semantic, Discourse, Conversation, and Sentiment features) by using isotonic CRF.", "labels": [], "entities": [{"text": "disagreement detection.", "start_pos": 118, "end_pos": 141, "type": "TASK", "confidence": 0.7494020760059357}, {"text": "Wikipedia talk page (AAWD)", "start_pos": 154, "end_pos": 180, "type": "DATASET", "confidence": 0.9084423780441284}, {"text": "F1 score", "start_pos": 192, "end_pos": 200, "type": "METRIC", "confidence": 0.9593939483165741}]}, {"text": "The numbers in bold are statistically significantly higher than the numbers above it (paired-t test, p < 0.05).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Strict and soft F1 scores for agreement and disagreement detection on Wikipedia talk pages  (AAWD). All the numbers are multiplied by 100. In each column, bold entries (if any) are statistically  significantly higher than all the rest, and the italic entry has the highest absolute value. Our model based  on the isotonic CRF with the new lexicon produces significantly better results than all the other systems  for agreement and disagreement detection. Downsampling, however, is not always helpful.", "labels": [], "entities": [{"text": "F1", "start_pos": 26, "end_pos": 28, "type": "METRIC", "confidence": 0.7337601780891418}, {"text": "agreement and disagreement detection", "start_pos": 40, "end_pos": 76, "type": "TASK", "confidence": 0.5894446298480034}, {"text": "Wikipedia talk pages  (AAWD)", "start_pos": 80, "end_pos": 108, "type": "DATASET", "confidence": 0.7608732680479685}, {"text": "agreement and disagreement detection", "start_pos": 427, "end_pos": 463, "type": "TASK", "confidence": 0.6886217370629311}]}, {"text": " Table 5. As it can be seen,  the performance gets improved incrementally with  every new set of features.  We also utilize \u03c7 2 -test to highlight some of  the salient features on the two datasets. We can  see from", "labels": [], "entities": []}, {"text": " Table 5: Results on Wikipedia talk page  (AAWD) (with soft F1 score) and online de- bate (IAC) with different feature sets (i.e Lexical,  Syntacitc/Semantic, Discourse, Conversation, and  Sentiment features) by using isotonic CRF. The  numbers in bold are statistically significantly  higher than the numbers above it (paired-t test,  p < 0.05).", "labels": [], "entities": [{"text": "Wikipedia talk page  (AAWD)", "start_pos": 21, "end_pos": 48, "type": "DATASET", "confidence": 0.9055283566315969}, {"text": "F1 score", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.968483179807663}, {"text": "online de- bate (IAC", "start_pos": 74, "end_pos": 94, "type": "METRIC", "confidence": 0.6390054672956467}]}]}