{"title": [{"text": "Factored Statistical Machine Translation for Grammatical Error Correction", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 9, "end_pos": 40, "type": "TASK", "confidence": 0.6040673156579336}, {"text": "Grammatical Error Correction", "start_pos": 45, "end_pos": 73, "type": "TASK", "confidence": 0.7569897373517355}]}], "abstractContent": [{"text": "This paper describes our ongoing work on grammatical error correction (GEC).", "labels": [], "entities": [{"text": "grammatical error correction (GEC)", "start_pos": 41, "end_pos": 75, "type": "TASK", "confidence": 0.751563827196757}]}, {"text": "Focusing on all possible error types in a real-life environment, we propose a factored statistical machine translation (SMT) model for this task.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 87, "end_pos": 124, "type": "TASK", "confidence": 0.7765165815750757}]}, {"text": "We consider error correction as a series of language translation problems guided by various linguistic information, as factors that influence translation results.", "labels": [], "entities": [{"text": "error correction", "start_pos": 12, "end_pos": 28, "type": "TASK", "confidence": 0.6906915307044983}, {"text": "language translation", "start_pos": 44, "end_pos": 64, "type": "TASK", "confidence": 0.741958498954773}]}, {"text": "Factors included in our study are morphological information, i.e. word stem, prefix, suffix, and Part-of-Speech (PoS) information.", "labels": [], "entities": []}, {"text": "In addition, we also experimented with different combinations of translation models (TM), phrase-based and factor-based, trained on various datasets to boost the overall performance.", "labels": [], "entities": []}, {"text": "Empirical results show that the proposed model yields an improvement of 32.54% over a baseline phrase-based SMT model.", "labels": [], "entities": [{"text": "SMT", "start_pos": 108, "end_pos": 111, "type": "TASK", "confidence": 0.9033738970756531}]}, {"text": "The system participated in the CoNLL 2014 shared task and achieved the 7 th and 5 th F 0.5 scores 1 on the official test set among the thirteen participating teams.", "labels": [], "entities": [{"text": "CoNLL 2014 shared task", "start_pos": 31, "end_pos": 53, "type": "DATASET", "confidence": 0.8077180236577988}, {"text": "F 0.5 scores 1", "start_pos": 85, "end_pos": 99, "type": "METRIC", "confidence": 0.9764597564935684}]}], "introductionContent": [{"text": "The task of grammatical error detection and correction (GEC) is to make use of computational methods to fix the mistakes in a written text.", "labels": [], "entities": [{"text": "grammatical error detection and correction (GEC)", "start_pos": 12, "end_pos": 60, "type": "TASK", "confidence": 0.7876969948410988}]}, {"text": "It is useful in two aspects.", "labels": [], "entities": []}, {"text": "For a non-native English learner it may help to improve the grammatical quality of the written text.", "labels": [], "entities": []}, {"text": "For a native speaker the tool may help to remedy mistakes automatically.", "labels": [], "entities": []}, {"text": "Automatic These two rankings are based on gold-standard edits without and with alternative answers, respectively.", "labels": [], "entities": []}, {"text": "correction of grammatical errors is an active research topic, aiming at improving the writing process with the help of artificial intelligent techniques.", "labels": [], "entities": [{"text": "correction of grammatical errors", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.9126788973808289}]}, {"text": "Second language learning is a user group of particular interest.", "labels": [], "entities": [{"text": "Second language learning", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.728739599386851}]}, {"text": "Recently, Helping Our Own (HOO) and CoNLL held a number of shared tasks on this topic ().", "labels": [], "entities": [{"text": "CoNLL", "start_pos": 36, "end_pos": 41, "type": "DATASET", "confidence": 0.8970309495925903}]}, {"text": "Previous studies based on rules (, data-driven methods) and hybrid methods) have shown substantial gains for some frequent error types over baseline methods.", "labels": [], "entities": []}, {"text": "Most proposed methods share the commonality that a sub-model is built fora specific type of error, on top of which a strategy is applied to combine a number of these individual models.", "labels": [], "entities": []}, {"text": "Also, detection and correction are often split into two steps.", "labels": [], "entities": [{"text": "detection and correction", "start_pos": 6, "end_pos": 30, "type": "TASK", "confidence": 0.6660927335421244}]}, {"text": "For example, presented the UM-Checker for five error types in the CoNLL 2013 shared task.", "labels": [], "entities": [{"text": "CoNLL 2013 shared task", "start_pos": 66, "end_pos": 88, "type": "DATASET", "confidence": 0.8663219809532166}]}, {"text": "The system implements a cascade of five individual detection-and-correction models for different types of error.", "labels": [], "entities": []}, {"text": "Given an input sentence, errors are detected and corrected one-by-one by each submodel at the level of its corresponding error type.", "labels": [], "entities": []}, {"text": "The specifics of an error type are fully considered in each sub-model, which is easier to realize fora single error type than for multiple types in a single model.", "labels": [], "entities": []}, {"text": "In addition, dividing the error detection and correction into two steps alleviates the application of machine learning classifiers.", "labels": [], "entities": [{"text": "error detection and correction", "start_pos": 26, "end_pos": 56, "type": "TASK", "confidence": 0.745987556874752}]}, {"text": "However, an approach that considers error types individually may have negative effects: \uf0b7 This approach assumes independence between each error type.", "labels": [], "entities": []}, {"text": "It ignores the interaction of neighboring errors.", "labels": [], "entities": []}, {"text": "Results ( have shown that consecutive errors of multiple types tend to hinder solving these errors individually.", "labels": [], "entities": []}, {"text": "\uf0b7 As the number of error types increases, the complexities of analyzing, designing, and implementing the model increase, in particular when combinatorial errors are taken into account.", "labels": [], "entities": []}, {"text": "\uf0b7 Looking for an optimal model combination becomes complex.", "labels": [], "entities": []}, {"text": "A simple pipeline approach would result in interference and the generation of new errors, and hence to propagating those errors to the subsequent processes.", "labels": [], "entities": []}, {"text": "\uf0b7 Separating the detection and correction tasks may result in more errors.", "labels": [], "entities": [{"text": "Separating the detection and correction", "start_pos": 2, "end_pos": 41, "type": "TASK", "confidence": 0.6991858303546905}]}, {"text": "For instance, once a candidate is misidentified as an error, it would be further revised and turned into an error by the correction model.", "labels": [], "entities": []}, {"text": "In this scenario the model risks losing precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 40, "end_pos": 49, "type": "METRIC", "confidence": 0.9991323351860046}]}, {"text": "In the shared task of this year (Ng et la., 2014), two novelties are introduced: 1) all types of errors present in an essay are to be detected and corrected (i.e., there is no restriction on the five error types of the 2013 shared task); 2) the official evaluation metric of this year adopts F 0.5 , weighting precision twice as much as recall.", "labels": [], "entities": [{"text": "F 0.5", "start_pos": 292, "end_pos": 297, "type": "METRIC", "confidence": 0.9845176637172699}, {"text": "precision", "start_pos": 310, "end_pos": 319, "type": "METRIC", "confidence": 0.7808951139450073}, {"text": "recall", "start_pos": 337, "end_pos": 343, "type": "METRIC", "confidence": 0.9955807328224182}]}, {"text": "This requires us to explore an alternative universal joint model that can tackle various kinds of grammatical errors as well as join the detection and correction processes together.", "labels": [], "entities": [{"text": "detection and correction", "start_pos": 137, "end_pos": 161, "type": "TASK", "confidence": 0.6942121585210165}]}, {"text": "Regarding grammatical error correction as a process of translation has been shown to be effective.", "labels": [], "entities": [{"text": "grammatical error correction", "start_pos": 10, "end_pos": 38, "type": "TASK", "confidence": 0.6055727998415629}, {"text": "translation", "start_pos": 55, "end_pos": 66, "type": "TASK", "confidence": 0.964610755443573}]}, {"text": "We treat the problematic sentences and golden sentences as pairs of source and target sentences.", "labels": [], "entities": []}, {"text": "In SMT, a translation model is trained on a parallel corpus that consists of the source sentences (i.e. sentences that may contain grammatical errors) and the targeted translations (i.e. the grammatically well-formed sentences).", "labels": [], "entities": [{"text": "SMT", "start_pos": 3, "end_pos": 6, "type": "TASK", "confidence": 0.9921108484268188}]}, {"text": "The challenge is that we need a large amount of these parallel sentences for constructing such a data-driven SMT system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 109, "end_pos": 112, "type": "TASK", "confidence": 0.972451388835907}]}, {"text": "Some researches explore generating artificial errors to resolve this sparsity problem.", "labels": [], "entities": []}, {"text": "Other studies) focus on using syntactic information (such as PoS or tree structure) to enhance the SMT models.", "labels": [], "entities": [{"text": "SMT", "start_pos": 99, "end_pos": 102, "type": "TASK", "confidence": 0.9954989552497864}]}, {"text": "In this paper, we propose a factored SMT model by taking into account not only the surface information contained in the sentence, but also morphological and syntactic clues (i.e., word stem, prefix, suffix and finer PoS information).", "labels": [], "entities": [{"text": "SMT", "start_pos": 37, "end_pos": 40, "type": "TASK", "confidence": 0.9357724785804749}]}, {"text": "To counter the sparsity problem we do not use artificial or manual approaches to enrich the training data.", "labels": [], "entities": []}, {"text": "Instead we apply factored and transductive learning techniques to enhance the model on a small dataset.", "labels": [], "entities": []}, {"text": "In addition, we also experimented with different combinations of translation models (TM), phrase-and factorbased, that are trained on different datasets to boost the overall performance.", "labels": [], "entities": []}, {"text": "Empirical results show that the proposed model yields an improvement of 32.54% over a baseline phrasebased SMT model.", "labels": [], "entities": [{"text": "SMT", "start_pos": 107, "end_pos": 110, "type": "TASK", "confidence": 0.9017057418823242}]}, {"text": "The remainder of this paper is organized as follows: Section 2 describes our proposed methods.", "labels": [], "entities": []}, {"text": "Section 3 reports on the design of our experiments.", "labels": [], "entities": []}, {"text": "We discuss the result, including the official shared task results, in Section 4,.", "labels": [], "entities": []}, {"text": "We summarize our conclusions in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "We pre-process the NUCLE corpus () as described in Section 2 for training different translation models.", "labels": [], "entities": [{"text": "NUCLE corpus", "start_pos": 19, "end_pos": 31, "type": "DATASET", "confidence": 0.934793621301651}]}, {"text": "We use both the official golden sentences and additional WMT2014 English monolingual data to train an in-domain and a general-domain language model (LM), respectively.", "labels": [], "entities": [{"text": "WMT2014 English monolingual data", "start_pos": 57, "end_pos": 89, "type": "DATASET", "confidence": 0.8743846118450165}]}, {"text": "These language models are linearly interpolated in the decoding phase.", "labels": [], "entities": []}, {"text": "We also randomly select a number of sentence pairs from the parallel corpus as a development set and a test set, disjoint from the training data.", "labels": [], "entities": []}, {"text": "The experiments were carried outwith MOSES 1.0 4 (Philipp . The translation and the re-ordering model utilizes the \"grow-diag-final\" symmetrized word-to-word alignments created with GIZA++ 5 and the training scripts of MOSES.", "labels": [], "entities": [{"text": "Philipp", "start_pos": 50, "end_pos": 57, "type": "DATASET", "confidence": 0.8530998826026917}, {"text": "translation", "start_pos": 64, "end_pos": 75, "type": "TASK", "confidence": 0.9690554141998291}, {"text": "MOSES", "start_pos": 219, "end_pos": 224, "type": "DATASET", "confidence": 0.8918690085411072}]}, {"text": "A 5-gram LM was trained using the SRILM toolkit 6 (), exploiting the improved modified Kneser-Ney smoothing, and quantizing both probabilities and back-off weights.", "labels": [], "entities": [{"text": "SRILM toolkit 6", "start_pos": 34, "end_pos": 49, "type": "DATASET", "confidence": 0.91243843237559}]}, {"text": "For the log-linear model training, we take minimum-error-rate training (MERT) method as described in.", "labels": [], "entities": [{"text": "minimum-error-rate training (MERT)", "start_pos": 43, "end_pos": 77, "type": "METRIC", "confidence": 0.8092982053756714}]}, {"text": "The result is evaluated by M 2 Scorer (Dahlmeier and Ng, 2012) computing precision, recall and F 0.5 . In total, one baseline system, five individual systems, and four combination systems are evaluated in this study.", "labels": [], "entities": [{"text": "M 2 Scorer", "start_pos": 27, "end_pos": 37, "type": "METRIC", "confidence": 0.8705926140149435}, {"text": "precision", "start_pos": 73, "end_pos": 82, "type": "METRIC", "confidence": 0.99972003698349}, {"text": "recall", "start_pos": 84, "end_pos": 90, "type": "METRIC", "confidence": 0.9997300505638123}, {"text": "F 0.5", "start_pos": 95, "end_pos": 100, "type": "METRIC", "confidence": 0.9872090518474579}]}, {"text": "The baseline system (Baseline) is trained on the words-only corpus using a phrase-based translation model.", "labels": [], "entities": []}, {"text": "For the individual systems we adopt the factored translation model that are trained respectively on 1) surface and stem factors (Sys +stem ), 2) surface and suffix factors (Sys +suf ), 3) surface and prefix factors (Sys +pref ), 4) surface and PoS factors (Sys +PoS ), and 5) surface and modified-PoS factors (Sys +MPoS ).", "labels": [], "entities": []}, {"text": "The combination systems include: 1) the combination of \"factored + phrase-based\" and \"factored + factored\" for models cascading; and 2) the factors of surface, stem and modified-PoS (Sys +stem+MPoS ) are combined for constructing a correction system based on a multi-factor model.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Statistics of used corpora.", "labels": [], "entities": []}, {"text": " Table 3: Performance of various models.", "labels": [], "entities": []}, {"text": " Table 4: The capacity of different models in  handling six frequent error types.", "labels": [], "entities": []}, {"text": " Table 5: Evaluation results of combined models.", "labels": [], "entities": []}, {"text": " Table 6: The official correction results of our  submitted system.", "labels": [], "entities": []}, {"text": " Table 7: Detailed error information of evaluation  system (with alternative result).", "labels": [], "entities": [{"text": "Detailed error information", "start_pos": 10, "end_pos": 36, "type": "METRIC", "confidence": 0.8274265726407369}]}]}