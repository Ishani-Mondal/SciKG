{"title": [{"text": "Automated Disease Normalization with Low Rank Approximations", "labels": [], "entities": [{"text": "Automated Disease Normalization", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.7072323759396871}, {"text": "Low Rank Approximations", "start_pos": 37, "end_pos": 60, "type": "METRIC", "confidence": 0.5624450743198395}]}], "abstractContent": [{"text": "While machine learning methods for named entity recognition (mention-level detection) have become common, machine learning methods have rarely been applied to normalization (concept-level identification).", "labels": [], "entities": [{"text": "named entity recognition (mention-level detection)", "start_pos": 35, "end_pos": 85, "type": "TASK", "confidence": 0.7047447434493473}, {"text": "concept-level identification)", "start_pos": 174, "end_pos": 203, "type": "TASK", "confidence": 0.783483882745107}]}, {"text": "Recent research introduced a machine learning method for normalization based on pairwise learning to rank.", "labels": [], "entities": []}, {"text": "This method, DNorm, uses a linear model to score the similarity between mentions and concept names, and has several desirable properties, including learning term variation directly from training data.", "labels": [], "entities": []}, {"text": "In this manuscript we employ a dimensionality reduction technique based on low-rank matrix approximation , similar to latent semantic indexing.", "labels": [], "entities": [{"text": "dimensionality reduction", "start_pos": 31, "end_pos": 55, "type": "TASK", "confidence": 0.7235456854104996}, {"text": "latent semantic indexing", "start_pos": 118, "end_pos": 142, "type": "TASK", "confidence": 0.5937838753064474}]}, {"text": "We compare the performance of the low rank method to previous work, using disease name normalization in the NCBI Disease Corpus as the test case, and demonstrate increased performance as the matrix rank increases.", "labels": [], "entities": [{"text": "NCBI Disease Corpus", "start_pos": 108, "end_pos": 127, "type": "DATASET", "confidence": 0.9637789130210876}]}, {"text": "We further demonstrate a significant reduction in the number of parameters to be learned and discuss the implications of this result in the context of algorithm scalability.", "labels": [], "entities": []}], "introductionContent": [{"text": "The data necessary to answer a wide variety of biomedical research questions is locked away in narrative text.", "labels": [], "entities": []}, {"text": "Automating the location (named entity recognition) and identification (normalization) of key biomedical entities () such as diseases, proteins and chemicals in narrative text may reduce curation costs, enable significantly increased scale and ultimately accelerate biomedical discovery (.", "labels": [], "entities": [{"text": "named entity recognition)", "start_pos": 25, "end_pos": 50, "type": "TASK", "confidence": 0.7805638760328293}, {"text": "biomedical discovery", "start_pos": 265, "end_pos": 285, "type": "TASK", "confidence": 0.7911430597305298}]}, {"text": "Named entity recognition (NER) techniques have typically focused on machine learning methods such as conditional random fields (CRFs), which have provided high performance when coupled with a rich feature approach.", "labels": [], "entities": [{"text": "Named entity recognition (NER)", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.8123372793197632}]}, {"text": "The utility of NER for biomedical end users is limited, however, since many applications require each mention to be normalized, that is, identified within a specified controlled vocabulary.", "labels": [], "entities": []}, {"text": "The normalization task has been highlighted in the BioCreative challenges, where a variety of methods have been explored for normalizing gene names, including string matching, pattern matching, and heuristic rules.", "labels": [], "entities": [{"text": "normalization task", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.9027236104011536}, {"text": "BioCreative challenges", "start_pos": 51, "end_pos": 73, "type": "DATASET", "confidence": 0.7308819591999054}, {"text": "string matching", "start_pos": 159, "end_pos": 174, "type": "TASK", "confidence": 0.706851527094841}, {"text": "pattern matching", "start_pos": 176, "end_pos": 192, "type": "TASK", "confidence": 0.7340258806943893}]}, {"text": "Similar methods have been applied to disease names () and species names, and the MetaMap program is used to locate and identify concepts from the UMLS MetaThesaurus).", "labels": [], "entities": [{"text": "UMLS MetaThesaurus", "start_pos": 146, "end_pos": 164, "type": "DATASET", "confidence": 0.8617208302021027}]}, {"text": "Machine learning methods for NER have provided high performance, enhanced system adaptability to new entity types, and abstracted many details of specific rule patterns.", "labels": [], "entities": [{"text": "NER", "start_pos": 29, "end_pos": 32, "type": "TASK", "confidence": 0.9267998337745667}]}, {"text": "While machine learning methods for normalization have been explored (), these are far less common.", "labels": [], "entities": [{"text": "normalization", "start_pos": 35, "end_pos": 48, "type": "TASK", "confidence": 0.9631603956222534}]}, {"text": "This is partially due to the lack of appropriate training data, and also partially due to the need fora generalizable supporting framework.", "labels": [], "entities": []}, {"text": "Normalization is frequently decomposed into the sub-tasks of candidate generation and disambiguation (.", "labels": [], "entities": []}, {"text": "During candidate generation, the set of concept names is constrained to a set of possible matches using the text of the mention.", "labels": [], "entities": [{"text": "candidate generation", "start_pos": 7, "end_pos": 27, "type": "TASK", "confidence": 0.7233358323574066}]}, {"text": "The primary difficulty addressed in candidate generation is term variation: the need to identify terms which are semantically similar but textually distinct (e.g. \"nephropathy\" and \"kidney disease\").", "labels": [], "entities": [{"text": "candidate generation", "start_pos": 36, "end_pos": 56, "type": "TASK", "confidence": 0.7433806359767914}]}, {"text": "The disambiguation step then differentiates between the different candidates to remove false positives, typically using the context of the mention and the article metadata.", "labels": [], "entities": []}, {"text": "Recently, developed an algorithm (DNorm) that directly addresses the term variation problem with machine learning, and used diseases -an important biomedical entity -as the first case study.", "labels": [], "entities": []}, {"text": "The algorithm learns a similarity function between mentions and concept names directly from training data using a method based on pairwise learning to rank.", "labels": [], "entities": []}, {"text": "The method was shown to provide high performance on the NCBI Disease Corpus (, and was also applied to clinical notes in the ShARe / CLEF eHealth task, where it achieved the highest normalization performance out of 17 international teams ().", "labels": [], "entities": [{"text": "NCBI Disease Corpus", "start_pos": 56, "end_pos": 75, "type": "DATASET", "confidence": 0.9661872982978821}, {"text": "ShARe / CLEF eHealth task", "start_pos": 125, "end_pos": 150, "type": "DATASET", "confidence": 0.5636709272861481}]}, {"text": "The normalization step does not consider context, and therefore must be combined with a disambiguation method for tasks where disambiguation is important.", "labels": [], "entities": []}, {"text": "However, this method provides high performance when paired with a conditional random field system for NER, making the combination a step towards fully adaptable mention recognition and normalization systems.", "labels": [], "entities": [{"text": "NER", "start_pos": 102, "end_pos": 105, "type": "TASK", "confidence": 0.8600161671638489}, {"text": "mention recognition", "start_pos": 161, "end_pos": 180, "type": "TASK", "confidence": 0.7144440710544586}]}, {"text": "This manuscript adapts DNorm to use a dimensionality reduction technique based on low rank matrix approximation.", "labels": [], "entities": [{"text": "DNorm", "start_pos": 23, "end_pos": 28, "type": "DATASET", "confidence": 0.8947942852973938}, {"text": "dimensionality reduction", "start_pos": 38, "end_pos": 62, "type": "TASK", "confidence": 0.6735397279262543}]}, {"text": "This may provide several benefits.", "labels": [], "entities": []}, {"text": "First, it may increase the scalability of the method, since the number of parameters used by the original technique is proportional to the square of the number of unique tokens.", "labels": [], "entities": []}, {"text": "Second, reducing the number of parameters may, in turn, improve the stability of the method and improve its generalization due to the induction of a latent \"concept space,\" similar to latent semantic indexing (.", "labels": [], "entities": []}, {"text": "Finally, while the rich feature approach typically used with conditional random fields allows it to partially compensate for out-of-vocabulary effects, DNorm ignores unknown tokens.", "labels": [], "entities": []}, {"text": "This reduces the ability of the model to generalize, due to the zipfian distribution of text, and is especially problematic in text which contains many misspellings, such as consumer text.", "labels": [], "entities": []}, {"text": "Using a richer feature space with DNorm would not be feasible, however, unless the parameter scalability problem is resolved.", "labels": [], "entities": []}, {"text": "In this article we expand the DNorm method in a pilot study on feasibility of using low rank approximation methods for disease name normalization.", "labels": [], "entities": [{"text": "disease name normalization", "start_pos": 119, "end_pos": 145, "type": "TASK", "confidence": 0.7012620568275452}]}, {"text": "To make this work comparable to the previous work on DNorm, we again employed the NCBI Disease Corpus ().", "labels": [], "entities": [{"text": "DNorm", "start_pos": 53, "end_pos": 58, "type": "DATASET", "confidence": 0.8559713959693909}, {"text": "NCBI Disease Corpus", "start_pos": 82, "end_pos": 101, "type": "DATASET", "confidence": 0.959513247013092}]}, {"text": "This corpus contains nearly 800 abstracts, split into training, development, and test sets, as described in.", "labels": [], "entities": []}, {"text": "Each disease mention is annotated for span and concept, using the MEDIC vocabulary (, which combines MeSH\u00ae) and OMIM\u00ae", "labels": [], "entities": [{"text": "span", "start_pos": 38, "end_pos": 42, "type": "METRIC", "confidence": 0.9697190523147583}, {"text": "MEDIC vocabulary", "start_pos": 66, "end_pos": 82, "type": "DATASET", "confidence": 0.7293087542057037}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1. Descriptive statistics for the NCBI Disease  Corpus.", "labels": [], "entities": [{"text": "NCBI Disease  Corpus", "start_pos": 41, "end_pos": 61, "type": "DATASET", "confidence": 0.9366767207781473}]}, {"text": " Table 2. Performance measurements for each  model on the NCBI Disease Test set. Full corre- sponds with the full-rank matrix used in previous  work.", "labels": [], "entities": [{"text": "NCBI Disease Test set", "start_pos": 58, "end_pos": 79, "type": "DATASET", "confidence": 0.9739712625741959}]}, {"text": " Table 3. Number of model parameters for each  variant, showing the low rank methods using 1 to  2 orders of magnitude fewer parameters.", "labels": [], "entities": []}]}