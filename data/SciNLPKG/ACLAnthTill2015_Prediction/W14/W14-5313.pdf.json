{"title": [], "abstractContent": [{"text": "The paper presents work on improved sentence-level dialect classification of Egyptian Arabic (ARZ) vs. Modern Standard Arabic (MSA).", "labels": [], "entities": [{"text": "sentence-level dialect classification of Egyptian Arabic (ARZ)", "start_pos": 36, "end_pos": 98, "type": "TASK", "confidence": 0.6461245583163368}]}, {"text": "Our approach is based on binary feature functions that can be implemented with a minimal amount of task-specific knowledge.", "labels": [], "entities": []}, {"text": "We train a feature-rich linear classifier based on a linear support-vector machine (linear SVM) approach.", "labels": [], "entities": []}, {"text": "Our best system achieves an accuracy of 89.1 % on the Arabic Online Commentary (AOC) dataset (Zaidan and Callison-Burch, 2011) using 10-fold stratified cross validation: a 1.3 % absolute accuracy improvement over the results published by (Zaidan and Callison-Burch, 2014).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.9994576573371887}, {"text": "Arabic Online Commentary (AOC) dataset", "start_pos": 54, "end_pos": 92, "type": "DATASET", "confidence": 0.7180147681917463}, {"text": "accuracy", "start_pos": 187, "end_pos": 195, "type": "METRIC", "confidence": 0.9277502298355103}]}, {"text": "We also evaluate the classifier on dialect data from an additional data source.", "labels": [], "entities": []}, {"text": "Here, we find that features which measure the informalness of a sentence actually decrease classification accuracy significantly.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 106, "end_pos": 114, "type": "METRIC", "confidence": 0.9590458273887634}]}], "introductionContent": [{"text": "The standard form of written Arabic is Modern Standard Arabic (MSA) . It differs significantly from various spoken varieties of Arabic (.", "labels": [], "entities": [{"text": "Modern Standard Arabic (MSA)", "start_pos": 39, "end_pos": 67, "type": "DATASET", "confidence": 0.7725593050320944}]}, {"text": "Even though these dialects do not originally exist in written form, they are present in social media texts.", "labels": [], "entities": []}, {"text": "Recently a dataset of dialectal Arabic has been made available in the form of the Arabic Online Commentary (AOC) set).", "labels": [], "entities": [{"text": "Arabic Online Commentary (AOC) set", "start_pos": 82, "end_pos": 116, "type": "DATASET", "confidence": 0.6379479723317283}]}, {"text": "The data consists of reader commentary from the online versions of Arabic newspapers, which have a high degree of dialect content.", "labels": [], "entities": []}, {"text": "Data for the following dialects has been collected: Levantine, Gulf, and Egyptian.", "labels": [], "entities": []}, {"text": "The data had been obtained by a crowd-sourcing effort.", "labels": [], "entities": []}, {"text": "In the current paper, we present results fora binary classification task only, where we predict the dialect of Egyptian Arabic ARZ vs. MSA sentences from the Al-Youm Al-Sabe' newspaper online commentaries . Our ultimate goal is to use the dialect classifier for building a dialect-aware Arabic-English statistical machine translation (SMT) system.", "labels": [], "entities": [{"text": "dialect-aware Arabic-English statistical machine translation (SMT)", "start_pos": 273, "end_pos": 339, "type": "TASK", "confidence": 0.7593129128217697}]}, {"text": "Our Arabic-English training data contains a significant amount of Egyptian dialect data only, and we would like to adapt the components of our hierarchical phrase-based SMT system to that data.", "labels": [], "entities": [{"text": "SMT", "start_pos": 169, "end_pos": 172, "type": "TASK", "confidence": 0.8885735869407654}]}, {"text": "Similar to, we present a sentence-level classifier that is trained in a supervised manner.", "labels": [], "entities": []}, {"text": "Our approach is based on an Arabic tokenizer, but we do not use a range of specialized tokenizers or orthography normalizers.", "labels": [], "entities": []}, {"text": "In contrast to the language-model (LM) based classifier used by), we present a linear classifier approach that works best without the use of LMbased features.", "labels": [], "entities": []}, {"text": "Some improvements in terms of classification accuracy and 10-fold cross validation under the same data conditions as) are presented.", "labels": [], "entities": [{"text": "classification", "start_pos": 30, "end_pos": 44, "type": "TASK", "confidence": 0.9530566334724426}, {"text": "accuracy", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.9565820693969727}]}, {"text": "In general, we aim at a smaller amount of domain specific feature engineering than previous related approaches.", "labels": [], "entities": [{"text": "domain specific feature engineering", "start_pos": 42, "end_pos": 77, "type": "TASK", "confidence": 0.6403103768825531}]}, {"text": "The paper is structured as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we present related work on language and dialect identification.", "labels": [], "entities": [{"text": "language and dialect identification", "start_pos": 41, "end_pos": 76, "type": "TASK", "confidence": 0.614431582391262}]}, {"text": "In Section 3, we discuss the linear classification model used in this paper.", "labels": [], "entities": [{"text": "linear classification", "start_pos": 29, "end_pos": 50, "type": "TASK", "confidence": 0.676432654261589}]}, {"text": "In Section 4, we evaluate the classifier performance in terms of classification accuracy on two data sets and present some error analysis.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 80, "end_pos": 88, "type": "METRIC", "confidence": 0.9015013575553894}]}, {"text": "Finally, in Section 5, we discuss future work on improved dialect-level classification and its application to system adaptation for machine translation.", "labels": [], "entities": [{"text": "dialect-level classification", "start_pos": 58, "end_pos": 86, "type": "TASK", "confidence": 0.763348400592804}, {"text": "system adaptation", "start_pos": 110, "end_pos": 127, "type": "TASK", "confidence": 0.7266597151756287}, {"text": "machine translation", "start_pos": 132, "end_pos": 151, "type": "TASK", "confidence": 0.7828822731971741}]}], "datasetContent": [{"text": "In this section, we present experimental results.", "labels": [], "entities": []}, {"text": "Firstly, Section 4.1 demonstrates that our data is annotated consistently.", "labels": [], "entities": []}, {"text": "In Section 4.2, we present dialect prediction results in terms of accuracy and F-score on our two data sets.", "labels": [], "entities": [{"text": "dialect prediction", "start_pos": 27, "end_pos": 45, "type": "TASK", "confidence": 0.8614657521247864}, {"text": "accuracy", "start_pos": 66, "end_pos": 74, "type": "METRIC", "confidence": 0.999592125415802}, {"text": "F-score", "start_pos": 79, "end_pos": 86, "type": "METRIC", "confidence": 0.9989503026008606}]}, {"text": "In Section 4.3, we perform some qualitative error analysis for our classifier.", "labels": [], "entities": []}, {"text": "In Section 4.4, we present some preliminary effects on training a SMT system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 66, "end_pos": 69, "type": "TASK", "confidence": 0.994149923324585}]}, {"text": "Following previous work, we present dialect prediction results in terms of accuracy:.", "labels": [], "entities": [{"text": "dialect prediction", "start_pos": 36, "end_pos": 54, "type": "TASK", "confidence": 0.8893673419952393}, {"text": "accuracy", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.9995667338371277}]}, {"text": "An in-lab annotator's dialect prediction is compared against the AOC data gold-standard dialect labels.", "labels": [], "entities": [{"text": "AOC data gold-standard dialect labels", "start_pos": 65, "end_pos": 102, "type": "DATASET", "confidence": 0.9639588952064514}]}, {"text": "where '# sent' is the number of sentences.", "labels": [], "entities": []}, {"text": "In addition, we present dialect prediction results in terms of precision, recall, and F-score.", "labels": [], "entities": [{"text": "dialect prediction", "start_pos": 24, "end_pos": 42, "type": "TASK", "confidence": 0.8241746127605438}, {"text": "precision", "start_pos": 63, "end_pos": 72, "type": "METRIC", "confidence": 0.9996999502182007}, {"text": "recall", "start_pos": 74, "end_pos": 80, "type": "METRIC", "confidence": 0.9995383024215698}, {"text": "F-score", "start_pos": 86, "end_pos": 93, "type": "METRIC", "confidence": 0.9977843165397644}]}, {"text": "They are defined as follows: Prec = # sent correctly tagged as ARZ # sent tagged as ARZ Recall = # sent correctly tagged as ARZ # ref sent tagged as ARZ MSA prediction F-score is defined analogously.", "labels": [], "entities": [{"text": "Prec", "start_pos": 29, "end_pos": 33, "type": "METRIC", "confidence": 0.896159827709198}, {"text": "ARZ", "start_pos": 63, "end_pos": 66, "type": "DATASET", "confidence": 0.9614558219909668}, {"text": "ARZ", "start_pos": 84, "end_pos": 87, "type": "DATASET", "confidence": 0.7452175617218018}, {"text": "Recall", "start_pos": 88, "end_pos": 94, "type": "METRIC", "confidence": 0.40176922082901}, {"text": "ARZ MSA prediction", "start_pos": 149, "end_pos": 167, "type": "DATASET", "confidence": 0.7061371803283691}, {"text": "F-score", "start_pos": 168, "end_pos": 175, "type": "METRIC", "confidence": 0.4806244671344757}]}, {"text": "Experimental results are presented in, where we present results for different sets of feature types and the two test sets in.", "labels": [], "entities": []}, {"text": "In the top half of the table, results are presented in terms of 10-fold cross validation on the ARZ-MSA portion of the AOC data.", "labels": [], "entities": [{"text": "ARZ-MSA portion of the AOC data", "start_pos": 96, "end_pos": 127, "type": "DATASET", "confidence": 0.8714490135510763}]}, {"text": "In the bottom half, we present results on DEV12 tune set, where we use the entire dialect data in for training (about 26K sentences).", "labels": [], "entities": [{"text": "DEV12 tune set", "start_pos": 42, "end_pos": 56, "type": "DATASET", "confidence": 0.9624953866004944}]}, {"text": "As our baseline we have re-implemented the language-model-perplexity based approach reported in ().", "labels": [], "entities": []}, {"text": "We train language models on the dialect-labeled commentary training data for each of the dialect classes c \u2208 {MSA, ARZ}.", "labels": [], "entities": []}, {"text": "During testing, we compute the language model probability of a sentence s for each of the classes c.", "labels": [], "entities": []}, {"text": "We assign a sentence to the class c with the highest probability (or the lowest perplexity) . For the 10-fold cross validation experiments, 10 language models are built and perplexities are computed on 10 different test sets.", "labels": [], "entities": []}, {"text": "The resulting (averaged) accuracy is 83.3 % for cross-validation and 82.2 % on the DEV12 tune set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9951836466789246}, {"text": "DEV12 tune set", "start_pos": 83, "end_pos": 97, "type": "DATASET", "confidence": 0.9873462120691935}]}, {"text": "In comparison, reports an accuracy of 80.4 % as perplexity-based baseline.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9996352195739746}]}, {"text": "We have carried out additional experiments with a simple feature set that consists of only unigram token and bigram token features as defined in Eq.", "labels": [], "entities": [{"text": "Eq", "start_pos": 145, "end_pos": 147, "type": "DATASET", "confidence": 0.9160895347595215}]}, {"text": "3. Such a system performs surprisingly well under both testing conditions: we achieved an accuracy of 87.7 % on the AOC data and an accuracy of 83.4 % on the DEV12 test set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 90, "end_pos": 98, "type": "METRIC", "confidence": 0.9995740056037903}, {"text": "AOC data", "start_pos": 116, "end_pos": 124, "type": "DATASET", "confidence": 0.8878443837165833}, {"text": "accuracy", "start_pos": 132, "end_pos": 140, "type": "METRIC", "confidence": 0.9993863105773926}, {"text": "DEV12 test set", "start_pos": 158, "end_pos": 172, "type": "DATASET", "confidence": 0.9893316825230917}]}, {"text": "On the AOC set using 10-fold cross validation, we achieve only a small improvement from using the dictionary features defined in Eq.", "labels": [], "entities": [{"text": "AOC set", "start_pos": 7, "end_pos": 14, "type": "DATASET", "confidence": 0.8262714743614197}]}, {"text": "4. The accuracy is improved from 87.7 % to 88.0 %.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 7, "end_pos": 15, "type": "METRIC", "confidence": 0.9989168643951416}]}, {"text": "On the DEV12 set, we obtain a much larger improvement from using these features.", "labels": [], "entities": [{"text": "DEV12 set", "start_pos": 7, "end_pos": 16, "type": "DATASET", "confidence": 0.980312705039978}]}, {"text": "Furthermore, we have investigated the usefulness of the AIDA-based features.", "labels": [], "entities": []}, {"text": "The stand-alone sentence-level classification of the AIDA tool performs quite poorly.", "labels": [], "entities": [{"text": "sentence-level classification", "start_pos": 16, "end_pos": 45, "type": "TASK", "confidence": 0.6715198755264282}]}, {"text": "On the DEV12 set, it achieves an accuracy of just 77.9 %.", "labels": [], "entities": [{"text": "DEV12 set", "start_pos": 7, "end_pos": 16, "type": "DATASET", "confidence": 0.9853522181510925}, {"text": "accuracy", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.9996727705001831}]}, {"text": "But using the AIDA assigned sentence-level and token-level dialect labels based on the binary features defined in Eq.", "labels": [], "entities": []}, {"text": "5 improves accuracy significantly, e.g. from 85.3 % to 87.8 % on the DEV12 set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.9995488524436951}, {"text": "DEV12 set", "start_pos": 69, "end_pos": 78, "type": "DATASET", "confidence": 0.9775074422359467}]}, {"text": "In the current experiments, the so-called meta features which are computed at the sentence level do not improve classification accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 127, "end_pos": 135, "type": "METRIC", "confidence": 0.879213809967041}]}, {"text": "The meta features are only useful in classifying dialect data based on the in-formalness of the data, i.e. the ARZ news commentaries tend to exhibit more in-formalness than the MSA commentaries.", "labels": [], "entities": [{"text": "ARZ news commentaries", "start_pos": 111, "end_pos": 132, "type": "DATASET", "confidence": 0.944404145081838}]}, {"text": "Finally, the sentence-level perplexity feature defined in Eq.", "labels": [], "entities": [{"text": "Eq", "start_pos": 58, "end_pos": 60, "type": "DATASET", "confidence": 0.9265945553779602}]}, {"text": "6 did not improve accuracy as well (no results for this feature are presented in).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9995751976966858}]}], "tableCaptions": [{"text": " Table 1: We used the following dialect data: 1) the ARZ-MSA portion of the AOC data from commen- taries of the Egyptian newspaper Al-Youm Al-Sabe', and 2) the DEV12 tune set (1219 sentences) which  is the LDC2012E30 corpus BOLT Phase 1 dev-tune set. The DEV12 tune set was annotated by a native  speaker of Arabic.", "labels": [], "entities": [{"text": "ARZ-MSA", "start_pos": 53, "end_pos": 60, "type": "METRIC", "confidence": 0.7713769674301147}, {"text": "AOC data from commen- taries of the Egyptian newspaper Al-Youm Al-Sabe", "start_pos": 76, "end_pos": 146, "type": "DATASET", "confidence": 0.8438875377178192}, {"text": "DEV12 tune set", "start_pos": 160, "end_pos": 174, "type": "DATASET", "confidence": 0.6933005650838217}, {"text": "LDC2012E30 corpus BOLT Phase 1 dev-tune set", "start_pos": 206, "end_pos": 249, "type": "DATASET", "confidence": 0.8864181126867022}, {"text": "DEV12 tune set", "start_pos": 255, "end_pos": 269, "type": "DATASET", "confidence": 0.9328246315320333}]}, {"text": " Table 2: Inter annotator agreement on 250 randomly selected AOC sentences from the data in", "labels": [], "entities": []}, {"text": " Table 1.  An in-lab annotator's dialect prediction is compared against the AOC data gold-standard dialect labels.", "labels": [], "entities": [{"text": "AOC data gold-standard dialect labels", "start_pos": 76, "end_pos": 113, "type": "DATASET", "confidence": 0.9634428739547729}]}, {"text": " Table 3: Arabic Dialect Classification Results: predicting MSA vs. (ARZ) dialect in terms of 10-fold  cross-validation on the AOC data and on the DEV12 set using all the AOC data for training.", "labels": [], "entities": [{"text": "Arabic Dialect Classification", "start_pos": 10, "end_pos": 39, "type": "TASK", "confidence": 0.5858696897824606}, {"text": "AOC data", "start_pos": 127, "end_pos": 135, "type": "DATASET", "confidence": 0.9677899181842804}, {"text": "DEV12 set", "start_pos": 147, "end_pos": 156, "type": "DATASET", "confidence": 0.9473608732223511}, {"text": "AOC data", "start_pos": 171, "end_pos": 179, "type": "DATASET", "confidence": 0.9589205980300903}]}]}