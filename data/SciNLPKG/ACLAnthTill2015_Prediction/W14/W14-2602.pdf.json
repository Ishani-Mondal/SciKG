{"title": [{"text": "Robust Cross-Domain Sentiment Analysis for Low-Resource Languages", "labels": [], "entities": [{"text": "Robust Cross-Domain Sentiment Analysis", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.6865626946091652}]}], "abstractContent": [{"text": "While various approaches to domain adaptation exist, the majority of them requires knowledge of the target domain, and additional data, preferably labeled.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 28, "end_pos": 45, "type": "TASK", "confidence": 0.7346012890338898}]}, {"text": "For a language like English, it is often feasible to match most of those conditions, but in low-resource languages, it presents a problem.", "labels": [], "entities": []}, {"text": "We explore the situation when neither data nor other information about the target domain is available.", "labels": [], "entities": []}, {"text": "We use two samples of Danish, a low-resource language , from the consumer review domain (film vs. company reviews) in a sentiment analysis task.", "labels": [], "entities": [{"text": "sentiment analysis task", "start_pos": 120, "end_pos": 143, "type": "TASK", "confidence": 0.9430518746376038}]}, {"text": "We observe dramatic performance drops when moving from one domain to the other.", "labels": [], "entities": []}, {"text": "We then introduce a simple offline method that makes models more robust towards unseen domains, and observe relative improvements of more than 50%.", "labels": [], "entities": []}], "introductionContent": [{"text": "Sentiment analysis, the task of determining the polarity of a text, is a valuable tool for gathering information from the vast amount of opinionated text produced today.", "labels": [], "entities": [{"text": "Sentiment analysis", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.961418628692627}]}, {"text": "It is actively used in reputation management and consumer assessment.", "labels": [], "entities": [{"text": "reputation management", "start_pos": 23, "end_pos": 44, "type": "TASK", "confidence": 0.9178591966629028}, {"text": "consumer assessment", "start_pos": 49, "end_pos": 68, "type": "TASK", "confidence": 0.7675931751728058}]}, {"text": "While supervised approaches achieve reasonable performance (, they are typically highly domain-dependent.", "labels": [], "entities": []}, {"text": "In fact, moving from one (source) domain to a different (target) domain will often lead to severe performance drops (.", "labels": [], "entities": []}, {"text": "This is mainly due to the models overfitting the source (training) data, both in terms of its label and word distribution.", "labels": [], "entities": []}, {"text": "The task of overcoming this tendency is known as domain adaptation (DA).", "labels": [], "entities": [{"text": "domain adaptation (DA)", "start_pos": 49, "end_pos": 71, "type": "TASK", "confidence": 0.8150800585746765}]}, {"text": "There are three different approaches to DA: in Supervised DA, labeled training data for the target domain exists, in Unsupervised DA, data for the target domain exists, but it is unlabeled.", "labels": [], "entities": [{"text": "DA", "start_pos": 40, "end_pos": 42, "type": "TASK", "confidence": 0.9886577725410461}]}, {"text": "A third, less investigated scenario is Blind DA: the target domain is not known at all in advance.", "labels": [], "entities": [{"text": "Blind DA", "start_pos": 39, "end_pos": 47, "type": "TASK", "confidence": 0.5871471613645554}]}, {"text": "Supervised DA effectively counteracts domain-bias by including labeled data from the target domain during training, thus preventing overfitting to both the label and the word distribution of the source.", "labels": [], "entities": []}, {"text": "Unsupervised methods usually rely either on external data, in the form of gazetteers, dictionaries, or on unlabeled data from the target domain.", "labels": [], "entities": []}, {"text": "While they do not prevent overfitting to the source domain's label distribution, the additional data acts as a regularizer by introducing a larger vocabulary.", "labels": [], "entities": []}, {"text": "However, both cases presuppose that we already know the target domain and have data from it.", "labels": [], "entities": []}, {"text": "In many real-world settings, these conditions are not met, especially when dealing with low-resource languages.", "labels": [], "entities": []}, {"text": "We thus need to regularize our models independent of the possible target domains.", "labels": [], "entities": []}, {"text": "Effectively, this means that we need to prevent our models from memorizing the observed label distribution, and from putting too much weight on features that are predictive in the source domain, but might not even be present in the target domain.", "labels": [], "entities": []}, {"text": "In this paper, we investigate sentiment analysis for Danish, a low-resource language, and therefore approach it as a Blind DA problem.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 30, "end_pos": 48, "type": "TASK", "confidence": 0.9524779319763184}]}, {"text": "We perform experiments on two types of domains, namely reviews for movies and companies.", "labels": [], "entities": []}, {"text": "The challenge lies in the fact that the label distribution (positive, negative, neutral) changes dramatically when moving from one domain to the other, and many highly predictive words in the company domain (e.g., \"reliable\") are unlikely to carryover to the movie domain, and vice versa.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, this is the first study to perform sentiment analysis for Danish, a low-resource language where relevant resources like polarity dictionaries are hard to come by.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 65, "end_pos": 83, "type": "TASK", "confidence": 0.9315153062343597}]}, {"text": "We present a simple offline-learning version inspired by previous work on corruptions, which also addresses the sparsity of available training data.", "labels": [], "entities": []}, {"text": "Our method introduces a relative improvement on out-of-domain performance by up to 54%.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our experiments use Danish reviews from two domains: movies and companies.", "labels": [], "entities": []}, {"text": "The specifications of the data sets are listed in and.", "labels": [], "entities": []}, {"text": "The two data sets differ considerably in data size and label distribution.: Overview of data set and split sizes in number of reviews and number of words.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Overview of data set and split sizes in  number of reviews and number of words.", "labels": [], "entities": []}, {"text": " Table 3: Evaluation on development and test sets measured in accuracy (Acc.) and the average f-score  for positive and negative instances (AF).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.9995655417442322}, {"text": "Acc.)", "start_pos": 72, "end_pos": 77, "type": "METRIC", "confidence": 0.9155377745628357}, {"text": "AF)", "start_pos": 140, "end_pos": 143, "type": "METRIC", "confidence": 0.9513881206512451}]}]}