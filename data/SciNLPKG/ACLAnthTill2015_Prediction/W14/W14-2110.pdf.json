{"title": [{"text": "Applying Argumentation Schemes for Essay Scoring", "labels": [], "entities": [{"text": "Applying", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.8387281894683838}, {"text": "Essay Scoring", "start_pos": 35, "end_pos": 48, "type": "TASK", "confidence": 0.6580699980258942}]}], "abstractContent": [{"text": "Under the framework of the argumentation scheme theory (Walton, 1996), we developed annotation protocols for an argumentative writing task to support identification and classification of the arguments being made in essays.", "labels": [], "entities": [{"text": "identification and classification of the arguments being made in essays", "start_pos": 150, "end_pos": 221, "type": "TASK", "confidence": 0.760267260670662}]}, {"text": "Each annotation protocol defined ar-gumentation schemes (i.e., reasoning patterns) in a given writing prompt and listed questions to help evaluate an argument based on these schemes, to make the argument structure in a text explicit and classifiable.", "labels": [], "entities": []}, {"text": "We report findings based on an annotation of 600 essays.", "labels": [], "entities": []}, {"text": "Most annotation categories were applied reliably by human annotators, and some categories significantly contributed to essay score.", "labels": [], "entities": []}, {"text": "An NLP system to identify sentences containing scheme-relevant critical questions was developed based on the human annotations.", "labels": [], "entities": []}], "introductionContent": [{"text": "In this paper, we analyze the structure of arguments as a first step in analyzing their quality.", "labels": [], "entities": []}, {"text": "Argument structure plays a critical role in identifying relevant arguments based on their content, so it seems reasonable to focus first on identifying characteristic patterns of argumentation and the ways in which such arguments are typically developed when they are explicitly stated.", "labels": [], "entities": [{"text": "Argument structure", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8151364624500275}]}, {"text": "It is worthwhile to classify the arguments in a text and to identify their structure when they are extended to include whole text segments, but it is not clear how far human annotation can go in analyzing argument structure.", "labels": [], "entities": []}, {"text": "An analysis of the effectiveness and full complexity of argument structure is different than the identification of generic elements that might compose an argument, such as claims (e.g., a thesis sentence), main reasons (e.g., supporting topic sentences), evidence (e.g., elaborating segments), and other components, such as the introduction and conclusion.", "labels": [], "entities": [{"text": "identification of generic elements that might compose an argument, such as claims (e.g., a thesis sentence), main reasons (e.g., supporting topic sentences), evidence (e.g., elaborating segments), and other components, such as the introduction and conclusion", "start_pos": 97, "end_pos": 355, "type": "Description", "confidence": 0.7719904841208944}]}, {"text": "In contrast, here we focus on analyzing specific types of arguments, what the literature terms argumentation schemes.", "labels": [], "entities": []}, {"text": "Argumentation schemes include schematic content and take into account a pattern of possible argumentation moves in a larger persuasive dialog.", "labels": [], "entities": []}, {"text": "Understanding these argumentation schemes is important for understanding the logic behind an argument.", "labels": [], "entities": []}, {"text": "Critical questions associated with a particular argumentation scheme provide a normative standard that can be used to evaluate the relevance of an argument's justificatory structure.", "labels": [], "entities": []}, {"text": "We aimed to lay foundations for the automated analysis of argumentation schemes, such as the identification and classification of the arguments in an essay.", "labels": [], "entities": [{"text": "identification and classification of the arguments in an essay", "start_pos": 93, "end_pos": 155, "type": "TASK", "confidence": 0.8891003264321221}]}, {"text": "Specifically, we developed annotation protocols for writing prompts in an argument analysis task from a graduate school admissions test.", "labels": [], "entities": [{"text": "argument analysis task", "start_pos": 74, "end_pos": 96, "type": "TASK", "confidence": 0.7771478096644083}]}, {"text": "The task was designed to assess how well a student analyzes someone else's argument, which is provided by the prompt.", "labels": [], "entities": []}, {"text": "The student must critically evaluate the logical soundness of the given argument.", "labels": [], "entities": []}, {"text": "The annotation categories were designed to map student responses to the scheme-relevant critical questions.", "labels": [], "entities": []}, {"text": "We examined whether this approach provides a useful framework for describing argumentation and whether human annotators can apply it reliably and consistently.", "labels": [], "entities": []}, {"text": "Furthermore, we have begun work on automating the annotation process by developing a system to predict whether sentences contain scheme-relevant critical questions.", "labels": [], "entities": []}], "datasetContent": [{"text": "For these experiments, we used the training and testing sets described in Section 5.", "labels": [], "entities": []}, {"text": "We trained models on the training data for each prompt individually and on the combination of the training data for both prompts.", "labels": [], "entities": []}, {"text": "To measure generalization across prompts, we tested these models on the testing data for each prompt and on the combina-tion of the testing data for the two prompts.", "labels": [], "entities": []}, {"text": "We evaluated performance in terms of unweighted Cohen's kappa.", "labels": [], "entities": []}, {"text": "The model trained on data from both prompts performed relatively well compared to the other models.", "labels": [], "entities": []}, {"text": "For the testing data for prompt B, the combined model outperformed the model trained on just data from prompt B.", "labels": [], "entities": [{"text": "prompt B", "start_pos": 25, "end_pos": 33, "type": "TASK", "confidence": 0.8250469863414764}]}, {"text": "However, the prompt-specific model for prompt A slightly outperformed the combined model on the testing data for prompt A.", "labels": [], "entities": []}, {"text": "Although the performance of models trained with data from one prompt and tested with data from another prompt did not perform as well, there is evidence of some generalization across prompts.", "labels": [], "entities": []}, {"text": "The model trained on data from prompt B and tested on data from prompt A had kappa = 0.217; the model trained on data from prompt A and tested on data from prompt B had kappa = 0.285.", "labels": [], "entities": []}, {"text": "Of course, these human-machine agreement values were somewhat lower than humanhuman agreement values (0.572 and 0.604, respectively), leaving substantial room for improvement in future work.", "labels": [], "entities": []}, {"text": "We also examined the most strongly weighted features in the combined model.", "labels": [], "entities": []}, {"text": "We observed that multiple hedge words (e.g., \"perhaps\", \"may\") had positive weights, which associated with the \"generic\" class.", "labels": [], "entities": []}, {"text": "We also observed that words related to argumentation (e.g., \"conclusions\", \"questions\") had negative weights, which associated them with the nongeneric class, as one would expect.", "labels": [], "entities": []}, {"text": "One issue of concern is that some words related to the specific topics discussed in the prompts received high weights as well, which may limit generalizability.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Performance of essay scoring models  with and without argumentation features", "labels": [], "entities": []}]}