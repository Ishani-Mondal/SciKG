{"title": [{"text": "The Tel Aviv University System for the Code-Switching Workshop Shared Task", "labels": [], "entities": [{"text": "Tel Aviv University System", "start_pos": 4, "end_pos": 30, "type": "DATASET", "confidence": 0.7305644303560257}]}], "abstractContent": [{"text": "We describe our entry in the EMNLP 2014 code-switching shared task.", "labels": [], "entities": [{"text": "EMNLP 2014 code-switching shared task", "start_pos": 29, "end_pos": 66, "type": "DATASET", "confidence": 0.7527756333351135}]}, {"text": "Our system is based on a sequential classifier, trained on the shared training set using various character-and word-level features, some calculated using a large monolingual corpora.", "labels": [], "entities": []}, {"text": "We participated in the Twitter-genre Spanish-English track, obtaining an accuracy of 0.868 when measured on the tweet level and 0.858 on the word level.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 73, "end_pos": 81, "type": "METRIC", "confidence": 0.9993071556091309}]}], "introductionContent": [{"text": "Code switching is the act of changing language while speaking or writing, as often done by bilinguals.", "labels": [], "entities": [{"text": "Code switching", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.8327123820781708}]}, {"text": "Identifying the transition points is a necessary first step before applying other linguistic algorithms, which usually target a single language.", "labels": [], "entities": []}, {"text": "A switching point may occur between sentences, phrases, words, or even between certain morphological components.", "labels": [], "entities": []}, {"text": "Code switching happens frequently in informal ways of communication, such as verbal conversations, blogs and microblogs; however, there are many examples in which languages are switched informal settings.", "labels": [], "entities": [{"text": "Code switching", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.8095963597297668}]}, {"text": "For example, alternating between Colloquial Egyptian Arabic and Modern Standard Arabic in modern Egyptian prose is prevalent).", "labels": [], "entities": []}, {"text": "This shared task (), 1 the first of its kind, challenges participants with identifying those switching points in blogs as well as in microblog posts.", "labels": [], "entities": []}, {"text": "Given posts with a mix of a specific pair of languages, each participating system is required to identify the language of every word.", "labels": [], "entities": []}, {"text": "Four language-pair tracks were offered by the task organizers: Spanish-English, NepaliEnglish, Modern Standard Arabic and Colloquial Arabic, and Mandarin-English.", "labels": [], "entities": []}, {"text": "For each language pair, a training set of Twitter 2 statuses was provided, which was manually annotated with a label for every word, indicating its language.", "labels": [], "entities": []}, {"text": "In addition to the two language labels, a few additional labels were used.", "labels": [], "entities": []}, {"text": "Altogether there were six labels: (1) lang1-the first language; (2) lang2-the second language; (3) ne-named entity; (4) ambiguous-for ambiguous words belonging to both languages; (5) mixed-for words composed of morphemes in each language; and (6) other-for cases where it is impossible to determine the language.", "labels": [], "entities": []}, {"text": "For most of the language pairs, the organizers supplied three different evaluation sets.", "labels": [], "entities": []}, {"text": "The first set was composed of a set of unseen Twitter statuses, provided with no manual annotation.", "labels": [], "entities": []}, {"text": "The other two sets contained data from a \"surprise genre\", mainly composed of blog posts.", "labels": [], "entities": []}, {"text": "We took part only in the Spanish-English track.", "labels": [], "entities": []}, {"text": "Both English and Spanish are written in Latin script.", "labels": [], "entities": []}, {"text": "The Spanish alphabet contains some additional letters, such as those indicating stress (vowels with acute accents: \u00b4 a, \u00b4 e, \u00b4 \u0131, \u00b4 o, \u00b4 u), a u adorned with a diaeresis (\u00a8 u), the additional letter\u00f1letter\u02dcletter\u00f1 (e\u00f1e), and inverted question and exclamation punctuation marks \u00bf and \u00a1 (used at the beginning of questions and exclamatory phrases, respectively).", "labels": [], "entities": []}, {"text": "Although social-media users are not generally consistent in their use of accents, their appearance in a word may disclose its language.", "labels": [], "entities": []}, {"text": "By and large, algorithms for code switching have used the character-based k-mer feature, introduced by).", "labels": [], "entities": [{"text": "code switching", "start_pos": 29, "end_pos": 43, "type": "TASK", "confidence": 0.812689483165741}]}, {"text": "Our system is an implementation of a multiclass classifier that works on the word level, considering features that we calculate using large Spanish as well as English monolingual corpora.", "labels": [], "entities": []}, {"text": "Working with a sequential classifier, the predicted labels of the previous words are used as features in predicting the current word.", "labels": [], "entities": []}, {"text": "In Section 2, we describe our system and the features we use for classification.", "labels": [], "entities": []}, {"text": "Section 3 contains the evaluation results, as published by the organizers of this shared task.", "labels": [], "entities": []}, {"text": "We conclude with a brief discussion.", "labels": [], "entities": []}], "datasetContent": [{"text": "We report on the results obtained on the unseen task evaluation sets, which were provided by the workshop organizers.", "labels": [], "entities": []}, {"text": "There are three evaluation sets.", "labels": [], "entities": []}, {"text": "The first is composed of a set of unseen Twitter statuses and the other two contain data from a \"surprise genre\".", "labels": [], "entities": []}, {"text": "The results are available online at the time of writing only for the first and second sets.", "labels": [], "entities": []}, {"text": "The results of the third set will be published during the upcoming workshop meeting.", "labels": [], "entities": []}, {"text": "The training set contains 11,400 statuses, comprising 140,706 words.", "labels": [], "entities": []}, {"text": "shows the distribution of labels.", "labels": [], "entities": []}, {"text": "The first evaluation set contains 3,060 tweets.", "labels": [], "entities": []}, {"text": "However, we were asked to download the statuses directly from Twitter, and some of the statuses were missing.", "labels": [], "entities": []}, {"text": "Therefore, we ended up with only 1,661 available statuses, corresponding to 17,723: Results for the first evaluation set, measured on tweet level. words.", "labels": [], "entities": []}, {"text": "According to the organizers, the evaluation was performed only on the 1,626 tweets that were available for all the participating groups.", "labels": [], "entities": []}, {"text": "Out of the 1,626, there are 1,155 monolingual tweets and 471 code-switched tweets.", "labels": [], "entities": []}, {"text": "shows the evaluation results for the Tel Aviv University (TAU) system on the first set, reported on the tweet level.", "labels": [], "entities": [{"text": "Tel Aviv University (TAU) system", "start_pos": 37, "end_pos": 69, "type": "DATASET", "confidence": 0.6365072854927608}]}, {"text": "In addition, the organizers provide evaluation results, calculated on the word level.", "labels": [], "entities": []}, {"text": "shows the label distribution among the words in the first evaluation set, and shows the actual results.", "labels": [], "entities": []}, {"text": "The overall accuracy on the word level is 0.858.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9996001124382019}]}, {"text": "The second evaluation set contains 1,103 words of a \"surprise\" (unseen) genre, mainly blog posts.", "labels": [], "entities": []}, {"text": "Out of the 49 posts, 27 are monolingual and 22 are code-switched posts.", "labels": [], "entities": []}, {"text": "shows the results for the surprise set, calculated on the post level.", "labels": [], "entities": []}, {"text": "As for the first set, shows the distribution of the labels among the words in the surprise set, and in we present the results as measured on the word level.", "labels": [], "entities": []}, {"text": "The overall accuracy on the surprise set is 0.941.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9998002648353577}]}], "tableCaptions": [{"text": " Table 1: Label distribution in the training set.", "labels": [], "entities": [{"text": "Label distribution", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.6447913944721222}]}, {"text": " Table 3: Label distribution in the first evaluation  set.", "labels": [], "entities": []}, {"text": " Table 4: Results for the first evaluation set, mea- sured on word level.", "labels": [], "entities": [{"text": "mea- sured", "start_pos": 48, "end_pos": 58, "type": "METRIC", "confidence": 0.9094078540802002}]}, {"text": " Table 5: Results for the second, \"surprise\" evalua- tion set, measured on the post level.", "labels": [], "entities": []}, {"text": " Table 6: Label distribution in the \"surprise\" eval- uation set.", "labels": [], "entities": []}]}