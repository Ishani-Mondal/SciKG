{"title": [{"text": "SentiMerge: Combining Sentiment Lexicons in a Bayesian Framework", "labels": [], "entities": []}], "abstractContent": [{"text": "Many approaches to sentiment analysis rely on a lexicon that labels words with a prior polarity.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 19, "end_pos": 37, "type": "TASK", "confidence": 0.9564014077186584}]}, {"text": "This is particularly true for languages other than English, where labelled training data is not easily available.", "labels": [], "entities": []}, {"text": "Existing efforts to produce such lexicons exist, and to avoid duplicated effort, a principled way to combine multiple resources is required.", "labels": [], "entities": []}, {"text": "In this paper, we introduce a Bayesian probabilistic model, which can simultaneously combine polarity scores from several data sources and estimate the quality of each source.", "labels": [], "entities": []}, {"text": "We apply this algorithm to a set of four German sentiment lexicons, to produce the SentiMerge lexicon, which we make publically available.", "labels": [], "entities": [{"text": "German sentiment lexicons", "start_pos": 41, "end_pos": 66, "type": "DATASET", "confidence": 0.754283626874288}]}, {"text": "Ina simple classification task, we show that this lexicon outperforms each of the underlying resources, as well as a majority vote model.", "labels": [], "entities": []}], "introductionContent": [{"text": "Wiegand (2011) describes sentiment analysis as the task of identifying and classifying opinionated content in natural language text.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 25, "end_pos": 43, "type": "TASK", "confidence": 0.9455285966396332}, {"text": "classifying opinionated content in natural language text", "start_pos": 75, "end_pos": 131, "type": "TASK", "confidence": 0.6697002521583012}]}, {"text": "There area number of subtasks within this field, such as identifying the holder of the opinion, and the target of the opinion.", "labels": [], "entities": []}, {"text": "In this paper, however, we are concerned with the more specific task of identifying polar languagethat is, expressing either positive or negative opinions.", "labels": [], "entities": []}, {"text": "Throughout the rest of this paper, we will use the terms sentiment and polarity more or less interchangeably.", "labels": [], "entities": []}, {"text": "As explain, sentiment analysis has become a major area of research within natural language processing (NLP), with many established techniques, and a range of potential applications.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 12, "end_pos": 30, "type": "TASK", "confidence": 0.9744583964347839}, {"text": "natural language processing (NLP)", "start_pos": 74, "end_pos": 107, "type": "TASK", "confidence": 0.7781581083933512}]}, {"text": "Indeed, in recent years there has been increasing interest in sentiment analysis for commercial purposes.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 62, "end_pos": 80, "type": "TASK", "confidence": 0.9527372419834137}]}, {"text": "Despite the rapid growth of this area, there is alack of gold-standard corpora which can be used to train supervised models, particularly for languages other than English.", "labels": [], "entities": []}, {"text": "Consequently, many algorithms rely on sentiment lexicons, which provide prior knowledge about which lexical items might indicate opinionated language.", "labels": [], "entities": []}, {"text": "Such lexicons can be used directly to define features in a classifier, or can be combined with a bootstrapping approach.", "labels": [], "entities": []}, {"text": "However, when presented with a number of overlapping and potentially contradictory sentiment lexicons, many machine learning techniques breakdown, and we therefore require away to merge them into a single resource -or else a researcher must choose between resources, and we are left with a leaky pipeline between resource creation and application.", "labels": [], "entities": []}, {"text": "We review methods for combining sources of information in section 2, and then describe four German sentiment lexicons in section 3.", "labels": [], "entities": []}, {"text": "To merge these resources, we first want to make them match as closely as possible, and then deal with the differences that remain.", "labels": [], "entities": []}, {"text": "We deal with the first step in section 4, describing how to align the polarity scores in different lexicons so that they can be directly compared.", "labels": [], "entities": []}, {"text": "Then in section 5, we describe how to combine these scores together.", "labels": [], "entities": []}, {"text": "We report results in section 6, including evaluation against a small annotated corpus, where our merged resource outperforms both the original resources and also a majority vote baseline.", "labels": [], "entities": []}, {"text": "Finally, we discuss distribution of our resource in section 7, future work in section 8, and conclude in section 9.: Comparison of lexicon sizes", "labels": [], "entities": []}], "datasetContent": [{"text": "The MLSA data (see section 3.5) consists of discrete polarity judgements -a word is positive, negative, or neutral, but nothing in between.", "labels": [], "entities": [{"text": "MLSA data", "start_pos": 4, "end_pos": 13, "type": "DATASET", "confidence": 0.6802518367767334}]}, {"text": "8 To allow direct evaluation against such a resource, we need to discretise the continuous range of polarity values; i.e. if the polarity value is above some positive threshold, we judge it to be positive; if it is below a negative threshold, negative; and if it is between the two thresholds, neutral.", "labels": [], "entities": []}, {"text": "To choose this threshold before evaluation, we calculated a Gaussian kernel density estimate of the polarity values in the entire lexicon, as shown in.", "labels": [], "entities": []}, {"text": "There is a large density near 0, reflecting that the bulk of the vocabulary is not strongly polar; indeed, so that the density of polar items is clearly visible, we have chosen a scale that forces this bulk to go off the top of the chart.", "labels": [], "entities": []}, {"text": "The high density stops at around \u00b10.23, and we have accordingly set this as our threshold.", "labels": [], "entities": []}, {"text": "We compared the merged resource to each of the original lexicons, as well as a \"majority vote\" baseline which represents an alternative method to combine lexicons.", "labels": [], "entities": []}, {"text": "This baseline involves considering the polarity judgements of each lexicon (positive, negative, or neutral), and taking the most common answer.", "labels": [], "entities": []}, {"text": "To break ties, we took the first answer when consulting the lexicons in the following order, reflecting their reliability: C&K, PolarityClues, SentiWS, SentiSpin.", "labels": [], "entities": []}, {"text": "For the automatically derived resources, we can introduce a threshold as we did for SentiMerge.", "labels": [], "entities": []}, {"text": "However, to make these baselines as competitive as possible, we optimised them on the test data, rather than choosing them in advance.", "labels": [], "entities": []}, {"text": "They were chosen to maximise the macro-averaged f-score.", "labels": [], "entities": []}, {"text": "For SentiWS, the threshold was 0, and for SentiSpin, 0.02.", "labels": [], "entities": []}, {"text": "Note that a perfect score would be impossible to achieve, since 31 lemmas were annotated with more than polarity type.", "labels": [], "entities": [{"text": "perfect score", "start_pos": 12, "end_pos": 25, "type": "METRIC", "confidence": 0.9643691182136536}]}, {"text": "These cases generally involve polysemous words which could be interpreted with different polarities depending on the context.", "labels": [], "entities": []}, {"text": "Indeed, two words appeared with all three labels: Spannung (tension) and Widerstand (resistance).", "labels": [], "entities": [{"text": "Widerstand (resistance", "start_pos": 73, "end_pos": 95, "type": "METRIC", "confidence": 0.9133630196253458}]}, {"text": "Ina political context, interpreting Widerstand as positive or: Performance on MLSA, macro-averaged negative depends very much on whose side you support.", "labels": [], "entities": [{"text": "MLSA", "start_pos": 78, "end_pos": 82, "type": "DATASET", "confidence": 0.8620412349700928}]}, {"text": "In such cases, a greater context is necessary to decide on polarity, and a lexicon simply cannot suffice.", "labels": [], "entities": []}, {"text": "We calculated precision, recall, and f-score (the harmonic mean of precision and recall) for both positive and negative polarity.", "labels": [], "entities": [{"text": "precision", "start_pos": 14, "end_pos": 23, "type": "METRIC", "confidence": 0.9995361566543579}, {"text": "recall", "start_pos": 25, "end_pos": 31, "type": "METRIC", "confidence": 0.9984585046768188}, {"text": "f-score", "start_pos": 37, "end_pos": 44, "type": "METRIC", "confidence": 0.9899813532829285}, {"text": "precision", "start_pos": 67, "end_pos": 76, "type": "METRIC", "confidence": 0.997713565826416}, {"text": "recall", "start_pos": 81, "end_pos": 87, "type": "METRIC", "confidence": 0.9854169487953186}]}, {"text": "We report the average of these two scores in 3.", "labels": [], "entities": []}, {"text": "We can see that in terms of fscore, SentiMerge outperforms all four data sources, as well as the majority vote.", "labels": [], "entities": []}, {"text": "In applications where either precision or recall is deemed to be more important, it would be possible to adjust the threshold accordingly.", "labels": [], "entities": [{"text": "precision", "start_pos": 29, "end_pos": 38, "type": "METRIC", "confidence": 0.9995176792144775}, {"text": "recall", "start_pos": 42, "end_pos": 48, "type": "METRIC", "confidence": 0.9988679885864258}]}, {"text": "Indeed, by dropping the threshold to zero, we achieve recall of 0.894, competitive with the majority vote method; and by increasing the threshold to 0.4, we achieve precision of 0.755, competitive with the C&K lexicon.", "labels": [], "entities": [{"text": "recall", "start_pos": 54, "end_pos": 60, "type": "METRIC", "confidence": 0.9996966123580933}, {"text": "precision", "start_pos": 165, "end_pos": 174, "type": "METRIC", "confidence": 0.9994024038314819}, {"text": "C&K lexicon", "start_pos": 206, "end_pos": 217, "type": "DATASET", "confidence": 0.9330224990844727}]}, {"text": "Furthermore, in this latter case, the f-score also increases to 0.760.", "labels": [], "entities": [{"text": "f-score", "start_pos": 38, "end_pos": 45, "type": "METRIC", "confidence": 0.9854857921600342}]}, {"text": "We do not report this figure in the table above because it would not be possible to predict such a judicious choice of threshold without peeking at the test data.", "labels": [], "entities": []}, {"text": "Nonetheless, this demonstrates that our method is robust to changes in parameter settings.", "labels": [], "entities": []}, {"text": "The majority vote method performs considerably worse than SentiMerge, at least in terms of f-score.", "labels": [], "entities": []}, {"text": "Indeed, it actually performs worse than the C&K lexicon, with noticeably lower precision.", "labels": [], "entities": [{"text": "C&K lexicon", "start_pos": 44, "end_pos": 55, "type": "DATASET", "confidence": 0.7919086068868637}, {"text": "precision", "start_pos": 79, "end_pos": 88, "type": "METRIC", "confidence": 0.9965421557426453}]}, {"text": "This finding is consistent with the results of, who argue against using majority voting, and who also find that it performs poorly.", "labels": [], "entities": []}, {"text": "The C&K lexicon achieves almost the same level of performance as SentiMerge, so it is reasonable to ask if there is any point in building a merged lexicon at all.", "labels": [], "entities": []}, {"text": "We believe there are two good reasons for doing this.", "labels": [], "entities": []}, {"text": "Firstly, although the C&K lexicon maybe the most accurate, it is also small, especially compared to SentiSpin.", "labels": [], "entities": [{"text": "C&K lexicon", "start_pos": 22, "end_pos": 33, "type": "DATASET", "confidence": 0.7461910247802734}]}, {"text": "SentiMerge thus manages to exploit the complementary nature of the different lexicons, achieving the broad coverage of SentiSpin, but maintaining the precision of the C&K lexicon for the most important lexical items.", "labels": [], "entities": [{"text": "precision", "start_pos": 150, "end_pos": 159, "type": "METRIC", "confidence": 0.9988327622413635}]}, {"text": "Secondly, SentiMerge can provide much more accurate values for polarity strength than any humanannotated resource can.", "labels": [], "entities": []}, {"text": "As show, inter-annotator agreement for polarity strength is low, even when agreement for polarity direction is high.", "labels": [], "entities": [{"text": "agreement", "start_pos": 25, "end_pos": 34, "type": "METRIC", "confidence": 0.8888335227966309}]}, {"text": "Nonetheless, some notion of polarity strength can still be helpful in computational applications.", "labels": [], "entities": []}, {"text": "To demonstrate this, we calculated the precision, recall, and f-scores again, but weighting each answer as a function of the distance from the estimated polarity strength to the threshold.", "labels": [], "entities": [{"text": "precision", "start_pos": 39, "end_pos": 48, "type": "METRIC", "confidence": 0.9997010827064514}, {"text": "recall", "start_pos": 50, "end_pos": 56, "type": "METRIC", "confidence": 0.9975371360778809}, {"text": "f-scores", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.9715373516082764}]}, {"text": "With this weighted approach, we get a macro-averaged fscore of 0.852.", "labels": [], "entities": [{"text": "fscore", "start_pos": 53, "end_pos": 59, "type": "METRIC", "confidence": 0.747309148311615}]}, {"text": "This is considerably higher than the results given in table 3, which demonstrates that the polarity scores in SentiMerge are useful as a measure of classification certainty.", "labels": [], "entities": [{"text": "classification", "start_pos": 148, "end_pos": 162, "type": "TASK", "confidence": 0.942454993724823}, {"text": "certainty", "start_pos": 163, "end_pos": 172, "type": "METRIC", "confidence": 0.477504163980484}]}], "tableCaptions": [{"text": " Table 1: Comparison of lexicon sizes", "labels": [], "entities": []}, {"text": " Table 2: An example lemma, labelled with polarity strengths from each data source", "labels": [], "entities": []}, {"text": " Table 3: Performance on MLSA, macro-averaged", "labels": [], "entities": [{"text": "MLSA", "start_pos": 25, "end_pos": 29, "type": "DATASET", "confidence": 0.8745378255844116}]}]}