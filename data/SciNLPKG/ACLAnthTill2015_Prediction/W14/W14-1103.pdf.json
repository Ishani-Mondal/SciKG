{"title": [{"text": "The impact of near domain transfer on biomedical named entity recognition", "labels": [], "entities": [{"text": "near domain transfer", "start_pos": 14, "end_pos": 34, "type": "TASK", "confidence": 0.6743770837783813}]}], "abstractContent": [{"text": "Current research in fully supervised biomedical named entity recognition (bioNER) is often conducted in a setting of low sample sizes.", "labels": [], "entities": [{"text": "fully supervised biomedical named entity recognition (bioNER)", "start_pos": 20, "end_pos": 81, "type": "TASK", "confidence": 0.7219824658499824}]}, {"text": "Whilst experimental results show strong performance in-domain it has been recognised that quality suffers when models are applied to heterogeneous text collections.", "labels": [], "entities": []}, {"text": "However the causal factors have until now been uncertain.", "labels": [], "entities": []}, {"text": "In this paper we describe a controlled experiment into near domain bias for two Medline corpora on hereditary diseases.", "labels": [], "entities": []}, {"text": "Five strategies are employed for mitigating the impact of near domain transference including simple transfer-ence, pooling, stacking, class re-labeling and feature augmentation.", "labels": [], "entities": [{"text": "near domain transference", "start_pos": 58, "end_pos": 82, "type": "TASK", "confidence": 0.6498941977818807}, {"text": "stacking", "start_pos": 124, "end_pos": 132, "type": "TASK", "confidence": 0.9556308388710022}]}, {"text": "We measure their effect on f-score performance against an in domain baseline.", "labels": [], "entities": []}, {"text": "Stacking and feature augmentation mitigate f-score loss but do not necessarily result in superior performance except for selected classes.", "labels": [], "entities": [{"text": "f-score loss", "start_pos": 43, "end_pos": 55, "type": "METRIC", "confidence": 0.9335773885250092}]}, {"text": "Simple pooling of data across domains failed to exploit size effects for most classes.", "labels": [], "entities": []}, {"text": "We conclude that we can expect lower performance and higher annotation costs if we do not adequately compensate for the distributional dissimilarities of domains during learning.", "labels": [], "entities": []}], "introductionContent": [{"text": "Model and feature selection are important experimental tasks in supervised machine learning for suggesting approaches that will generalise well on real world data.", "labels": [], "entities": [{"text": "Model and feature selection", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.6169693917036057}]}, {"text": "Research in biomedical named entity recognition (bioNER) often displays two features: (1) small samples of labeled data, and (2) an implicit assumption that the future data will be * collier@ebi.ac.uk drawn from a similar distribution to the labeled data and hence that minimising expected prediction error on held out data will minimise actual future loss.", "labels": [], "entities": [{"text": "biomedical named entity recognition (bioNER)", "start_pos": 12, "end_pos": 56, "type": "TASK", "confidence": 0.7549654543399811}]}, {"text": "Since expert labeling is time consuming and expensive, labeled data sets tend to be relatively small, e.g. (, in the region of a few hundred or thousand Medline abstracts.", "labels": [], "entities": [{"text": "expert labeling", "start_pos": 6, "end_pos": 21, "type": "TASK", "confidence": 0.690296858549118}]}, {"text": "Despite the danger of intrinsic idiosyncracies such corpora are often used to demonstrate putative prediction error across the heterogeneous collection of 22 million Medline abstracts.", "labels": [], "entities": [{"text": "Medline abstracts", "start_pos": 166, "end_pos": 183, "type": "DATASET", "confidence": 0.9432765245437622}]}, {"text": "Once this assumption is made explicit it is of interest to both researchers and users that the implications and limitations of such experimental settings are explored.", "labels": [], "entities": []}, {"text": "Cross domain studies have indicated an advantage for mechanisms that compensate for domain bias.", "labels": [], "entities": []}, {"text": "For fully supervised learning, which is the scenario we explore here, recent methods include: feature augmentation, instance weighting, schema harmonisation ( and semi-supervised/lightly supervised approaches ().", "labels": [], "entities": [{"text": "instance weighting", "start_pos": 116, "end_pos": 134, "type": "TASK", "confidence": 0.6571626663208008}]}, {"text": "More generally there is a wide body of work in transfer learning (also known as domain adaptation) that tries to handle discrepancies between training and testing distributions.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 47, "end_pos": 64, "type": "TASK", "confidence": 0.9232039451599121}, {"text": "domain adaptation)", "start_pos": 80, "end_pos": 98, "type": "TASK", "confidence": 0.749038835366567}]}, {"text": "As an illustration of near domain bias consider the list of high frequency named entities in Table 1 drawn from two sub-domains in the research literature of hereditary diseases.", "labels": [], "entities": []}, {"text": "A domain expert in hereditary diseases would have no difficulty in dividing them into two non-overlapping sets corresponding to the two near domains with one term t 5 patients shared by both: {t 1 ,t 6 ,t 8 ,t 9 } and {t 2 ,t 3 ,t 4 ,t 7 ,t 10 }.", "labels": [], "entities": []}, {"text": "Previous studies have shown what happens when you radically change the domain and/or the t 1 rheumatoid t human leukocyte arthritis antigen t 2 lupus t 7 coronary heart erythematosus disease t 3 leopard syndrome t 8 type 1 diabetes t 4 Omapatrilat t 9 T1D t 5 patients t 10 hypertension: High frequency entities in the hereditory disease literature for auto-immune and cardiovascular diseases.", "labels": [], "entities": []}, {"text": "annotation schema, e.g. from newswire to Medline or Web pages.", "labels": [], "entities": [{"text": "Medline", "start_pos": 41, "end_pos": 48, "type": "DATASET", "confidence": 0.899836003780365}]}, {"text": "But what happens when the annotation schema, the annotator and the primary domain stay the same?", "labels": [], "entities": []}, {"text": "Although the notion of domain is difficult to formalise in the context of research literature, this study explores the condition where the variable factor is a shift to a near domain of literature as defined by biocurators and illustrated in the previous example.", "labels": [], "entities": []}, {"text": "Our contribution to biomedical named entity recognition (bioNER) is in five areas: 1.", "labels": [], "entities": [{"text": "biomedical named entity recognition (bioNER)", "start_pos": 20, "end_pos": 64, "type": "TASK", "confidence": 0.7432613628251212}]}, {"text": "We compare four data combination strategies for mitigating the impact of near domain transference and measure their effect on fscore performance against an in domain baseline.", "labels": [], "entities": [{"text": "near domain transference", "start_pos": 73, "end_pos": 97, "type": "TASK", "confidence": 0.6519276897112528}]}, {"text": "2. We provide additional evidence for the effectiveness of)'s frustratingly simple strategy which provides both general and domain-specific features; in effect a joint learning model.", "labels": [], "entities": []}, {"text": "3. Expectedly, but not trivially, we show that a general loss of f-score occurs on bioNER when transfering to near domains.", "labels": [], "entities": [{"text": "f-score", "start_pos": 65, "end_pos": 72, "type": "METRIC", "confidence": 0.9858376383781433}]}, {"text": "This loss is not uniform across all classes.", "labels": [], "entities": []}, {"text": "We provide class-by-class drill down analysis to the underlying causal factors which make some entities more robust to near domain transference in biomedicine than others.", "labels": [], "entities": []}, {"text": "4. Our results challenge the notion that pooling small corpora, even when guideline differences are reconciled, leads to improved f-score performance ().", "labels": [], "entities": []}, {"text": "5. In addition to the usual biomedical entity types we introduce the class of phenotypes which are valued as indicators of genetic malfunction and characteristic of diseases.", "labels": [], "entities": []}, {"text": "The phenotype class incorporates a complex dependency between classes, notably anatomical entities and genes.", "labels": [], "entities": []}, {"text": "This paper is organised as follows: Section 2 describes related work in cross domain transfer for biomedical NER, Section 3 discusses our approach including the two data sets used in our experiments, CRF model, feature choices and evaluation framework.", "labels": [], "entities": [{"text": "cross domain transfer", "start_pos": 72, "end_pos": 93, "type": "TASK", "confidence": 0.6487492918968201}, {"text": "biomedical NER", "start_pos": 98, "end_pos": 112, "type": "TASK", "confidence": 0.5096001625061035}]}, {"text": "In Section 4 we outline our experimental design.", "labels": [], "entities": []}, {"text": "Finally in Section 5 we compare the performance of six data selection strategies that try to maximise f-score performance on domain entity classes in the target corpus.", "labels": [], "entities": []}], "datasetContent": [{"text": "Traditional re-sampling using k-fold cross validation (k-CV) divides then labelled documents into k disjoint subsets of approximately equal size designated as Di for i = 1, .., k.", "labels": [], "entities": []}, {"text": "The NER learner is trained successively on k \u2212 1 folds from D and tested on a held out fold over k iterations.", "labels": [], "entities": [{"text": "NER", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.9180293679237366}]}, {"text": "In order to preserve independence between contexts in training and held out data we assume here that the unit of division is the document, i.e. a single Medline abstract.", "labels": [], "entities": []}, {"text": "Estimated prediction error is calculated based on the learner's labels on the k held out folds.", "labels": [], "entities": [{"text": "Estimated prediction error", "start_pos": 0, "end_pos": 26, "type": "METRIC", "confidence": 0.7702639400959015}]}, {"text": "Whilst k-CV is known to be nearly unbiased it is a highly variable estimator.", "labels": [], "entities": []}, {"text": "Several studies have looked at k-CV for small sample sets.", "labels": [], "entities": []}, {"text": "For example, (Braga-Neto and Dougherty, 2004) found on classifier experiments for small microarray samples (20 <= n <= 120) that whilst k-CV showed low bias they suffered from excessive variance compared to bootstrap or resubstitution estimators.", "labels": [], "entities": []}, {"text": "One cause of variance has been identified as within-block and between-block training errors arising from the disproportionate effects of a single abstract appearing in the training set of many folds.", "labels": [], "entities": []}, {"text": "In order to reduce this effect Monte Carlo cross validation was used (also called CV with repetition).", "labels": [], "entities": [{"text": "repetition", "start_pos": 90, "end_pos": 100, "type": "METRIC", "confidence": 0.9525918960571289}]}, {"text": "100 iterations were used to randomly reorder the documents in the corpora before 10-fold CV sampling was run (cv10r100).", "labels": [], "entities": []}, {"text": "Sampling of documents is done without replacement so that the independence between training and testing sets are maintained.", "labels": [], "entities": [{"text": "Sampling of documents", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.8845224579175314}]}, {"text": "Micro averaged f-scores for labeling accuracy were calculated based on the 1000 test folds for each model.", "labels": [], "entities": [{"text": "labeling", "start_pos": 28, "end_pos": 36, "type": "TASK", "confidence": 0.9641156196594238}, {"text": "accuracy", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.8873603940010071}]}, {"text": "Evaluation was done in both directions (training and testing) for each corpus C1 and C2 to show any asymmetrical effects.", "labels": [], "entities": []}, {"text": "To minimse the time taken for each experiment a cluster computer was used with 48 nodes.", "labels": [], "entities": []}, {"text": "The matching criteria we employ is the exact match -i.e. the span of the system labeling and the held out data labels should be exactly the same.", "labels": [], "entities": [{"text": "exact match", "start_pos": 39, "end_pos": 50, "type": "METRIC", "confidence": 0.918241411447525}]}, {"text": "Although this is not a necessary criteria for some applications such as database curation we used it here as it is widely applied in shared evaluations and shows the clearest effects of modeling choice.", "labels": [], "entities": []}, {"text": "We evaluate using the named entity precision, recall and F-score calculated using the CoNLL 2003 Perl script.", "labels": [], "entities": [{"text": "precision", "start_pos": 35, "end_pos": 44, "type": "METRIC", "confidence": 0.9575175046920776}, {"text": "recall", "start_pos": 46, "end_pos": 52, "type": "METRIC", "confidence": 0.9997465014457703}, {"text": "F-score", "start_pos": 57, "end_pos": 64, "type": "METRIC", "confidence": 0.9992351531982422}, {"text": "CoNLL 2003 Perl script", "start_pos": 86, "end_pos": 108, "type": "DATASET", "confidence": 0.9616849720478058}]}, {"text": "This was calculated as, where, and, A true positive (TP) is a gold standard NE tagged by the system as an NE.", "labels": [], "entities": [{"text": "true positive (TP)", "start_pos": 38, "end_pos": 56, "type": "METRIC", "confidence": 0.7165113151073456}]}, {"text": "A true negative (TN) is a gold standard none-NE tagged by the system as a none-NE.", "labels": [], "entities": []}, {"text": "A false positive (FP) is a gold standard none-NE tagged by the system as an NE.", "labels": [], "entities": [{"text": "false positive (FP)", "start_pos": 2, "end_pos": 21, "type": "METRIC", "confidence": 0.8178003787994385}]}, {"text": "Evaluation is based on correctly marked whole entities rather than tokens.", "labels": [], "entities": []}, {"text": "In this section we present the experimental conditions we used, starting with a description of the models which we designate M1 to M6 and describe below.", "labels": [], "entities": []}, {"text": "All methods made use of 100 iterations of Monte Carlo 10-fold cross validation.", "labels": [], "entities": []}, {"text": "In we show f-score performance from near biomedical domains with our six strategies.", "labels": [], "entities": []}, {"text": "This section now tries to draw together an interpretation for the performance trends that we see and to drill down to some of the causal factors.", "labels": [], "entities": []}, {"text": "Held out tests performed in-domain (M1) on both corpora C1 and C2 indicate a relatively high level of performance, conservatively inline with state-of-the-art estimates.", "labels": [], "entities": []}, {"text": "The broad trend in performance is for entity classes with more instances to outperform others with lower numbers.", "labels": [], "entities": []}, {"text": "The class which most obviously breaks this trend is the complex entity type of PHE.", "labels": [], "entities": []}, {"text": "To understand this consider that PHE is defined as an observable property on an organism and as such tends to be formed from a quality such as malformed that describes a structural entity such as valve.", "labels": [], "entities": []}, {"text": "To see closer what is happening we looked at the confusion matrices for M1 on both corpora.", "labels": [], "entities": [{"text": "M1", "start_pos": 72, "end_pos": 74, "type": "METRIC", "confidence": 0.6576996445655823}]}, {"text": "For both C1 and C2 we observed that a substantial proportion of words inside PHE sequences were confused with GGP, DIS or ANA entities.", "labels": [], "entities": []}, {"text": "Similarly a high proportion of words inside ANA sequences were confused with PHE entities.", "labels": [], "entities": []}, {"text": "This indicates that dependencies within complex biomedical entities like PHE might better be modeled explicitly using tree-structures in a manner similar to events rather than using n-gram relations.", "labels": [], "entities": []}, {"text": "In the M2 out of domain experiments we see a generally severe loss of f-score performance across most classes.", "labels": [], "entities": [{"text": "f-score", "start_pos": 70, "end_pos": 77, "type": "METRIC", "confidence": 0.9444257616996765}]}, {"text": "Training on C2 and testing on C1 results in a 19.1% loss (F1 69.9 to 50.8) and training on C1 and testing on C2 results in a 11.9% loss overall (F1 58.5 to 46.6).", "labels": [], "entities": [{"text": "F1", "start_pos": 58, "end_pos": 60, "type": "METRIC", "confidence": 0.9988138675689697}, {"text": "F1", "start_pos": 145, "end_pos": 147, "type": "METRIC", "confidence": 0.9963275790214539}]}, {"text": "The results agree with Wang et al.'s experience on heterogeneous Medline corpora and extend the upper limit on all-class loss due to domain transferrence to 19%.", "labels": [], "entities": []}, {"text": "The only NE class where we see asymmetric benefit from pooling entities in M3 is for ORG (F1 68.4 to 72.2, F1 73.2 to 77.4).", "labels": [], "entities": [{"text": "ORG", "start_pos": 85, "end_pos": 88, "type": "METRIC", "confidence": 0.9886838793754578}, {"text": "F1", "start_pos": 90, "end_pos": 92, "type": "METRIC", "confidence": 0.9634280204772949}, {"text": "F1", "start_pos": 107, "end_pos": 109, "type": "METRIC", "confidence": 0.8925836086273193}]}, {"text": "Intriguingly the data from hint at a correlation between the success of M3 pooling for ORG and broad cross-domain compatibility on the vocabulary (over 50% of ORG vocabulary is shared across corpora).", "labels": [], "entities": []}, {"text": "However this is not supported in the low sharing case for CHE where we see increased performance from pooling (F1 31.3 to 38.7) when the target is C2 but decreased performance when the target is C1 (F1 29.5 to 20.0).", "labels": [], "entities": [{"text": "F1", "start_pos": 111, "end_pos": 113, "type": "METRIC", "confidence": 0.9920274615287781}, {"text": "F1", "start_pos": 199, "end_pos": 201, "type": "METRIC", "confidence": 0.9301562905311584}]}, {"text": "When we look at the pooling method (M3) and compare to the in-domain method (M1) no obvious size effect occurs for the number of entities in each class.", "labels": [], "entities": []}, {"text": "To see this we can examine entity classes with an imbalanced number of instances in C1 and C2 such as CHE, GGP and PHE.", "labels": [], "entities": []}, {"text": "If simply pooling more entities was important to improved f-score we would expect to see a clearer pattern of improvement but we do not.", "labels": [], "entities": []}, {"text": "bounds observed by ( and () for their pooling of heterogeneous Medline corpora.", "labels": [], "entities": []}, {"text": "Except for the ORG class which we higlighted above, we might cautiously quantify the loss of pooled entity mentions as being in the range up to 9.5% for CHE but more typically below 4%.", "labels": [], "entities": [{"text": "ORG", "start_pos": 15, "end_pos": 18, "type": "METRIC", "confidence": 0.99382483959198}]}, {"text": "The majority of the differences they observed -which are not present in our data -are most likely due to concept definition differences and annotation conventions.", "labels": [], "entities": []}, {"text": "In contrast to our expectations the M4 experiments showed very mild benefits for stacking and these were mixed across entity types.", "labels": [], "entities": [{"text": "stacking", "start_pos": 81, "end_pos": 89, "type": "TASK", "confidence": 0.9823404550552368}]}, {"text": "M4 tests on C2 showed no general improvement but some improvement in CHE and ORG.", "labels": [], "entities": [{"text": "CHE", "start_pos": 69, "end_pos": 72, "type": "METRIC", "confidence": 0.9151857495307922}, {"text": "ORG", "start_pos": 77, "end_pos": 80, "type": "METRIC", "confidence": 0.9941931366920471}]}, {"text": "M4 tests on C1 resulted again in no overall improvement except for some gain for ORG, supporting our hypothesis that there is greater compatibility in ORG across domains.", "labels": [], "entities": [{"text": "ORG", "start_pos": 81, "end_pos": 84, "type": "METRIC", "confidence": 0.9268293380737305}]}, {"text": "The M5 approach of splitting the PHE labels for the two corpora resulted in a noticable improvement over M3 on the C1 test but unfortunately this was not sustained when testing on C2.", "labels": [], "entities": []}, {"text": "It is striking that in the M6 experiments the feature augmentation method only just meets the indomain f-score on C1 and mildly exceeds it on C2.", "labels": [], "entities": []}, {"text": "One explanation is that the corpora are so small that a richer feature set has only marginal effects on performance.", "labels": [], "entities": []}, {"text": "certainly indicates that many of the features have low predictive capacity (gain ratio values below 0.1) in an intra-corpus setting but this is not the case for others such as GENIA NE tags or HPO gazzetteer terms.", "labels": [], "entities": [{"text": "GENIA NE tags", "start_pos": 176, "end_pos": 189, "type": "DATASET", "confidence": 0.8658202091852824}]}, {"text": "Overall when we average the f-scores across models for C1 and C2 we see that there is a marginal benefit to the M1, M4 and M6 strategies over M3 and M5 with M2 suffering the greatest loss in performance.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Characteristics of the C1 auto-immune  and C2 cardiovascular corpora: number of ab- stracts, number of tokens, average sentence length,  frequency of each entity type. Figures in parenthe- ses represent counts after removing duplication. a:", "labels": [], "entities": []}, {"text": " Table 4: Named entity recognition f-scores using Methods 1 to 6. All methods were tested using 100  iterations of Monte Carlo 10-fold cross validation. Figures in bold show best in class scores. Figures in  italics show scores above the M1 baseline.", "labels": [], "entities": [{"text": "Named entity recognition f-scores", "start_pos": 10, "end_pos": 43, "type": "TASK", "confidence": 0.6683456897735596}, {"text": "M1", "start_pos": 238, "end_pos": 240, "type": "METRIC", "confidence": 0.8259484767913818}]}]}