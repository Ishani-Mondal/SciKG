{"title": [{"text": "Investigating the Contribution of Distributional Semantic Information for Dialogue Act Classification", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper presents a series of experiments in applying compositional distributional semantic models to dialogue act classification.", "labels": [], "entities": [{"text": "dialogue act classification", "start_pos": 104, "end_pos": 131, "type": "TASK", "confidence": 0.8413944244384766}]}, {"text": "In contrast to the widely used bag-of-words approach, we build the meaning of an utterance from its parts by composing the distributional word vectors using vector addition and multiplication.", "labels": [], "entities": []}, {"text": "We investigate the contribution of word sequence, dialogue act sequence, and distributional information to the performance, and compare with the current state of the art approaches.", "labels": [], "entities": []}, {"text": "Our experiment suggests that that distributional information is useful for dialogue act tagging but that simple models of compositionality fail to capture crucial information from word and utterance sequence; more advanced approaches (e.g. sequence-or grammar-driven, such as categorical , word vector composition) are required .", "labels": [], "entities": [{"text": "dialogue act tagging", "start_pos": 75, "end_pos": 95, "type": "TASK", "confidence": 0.7931517163912455}, {"text": "word vector composition", "start_pos": 290, "end_pos": 313, "type": "TASK", "confidence": 0.6721536914507548}]}], "introductionContent": [{"text": "One of the fundamental tasks in automatic dialogue processing is dialogue act tagging: labelling each utterance with a tag relating to its function in the dialogue and effect on the emerging context: greeting, query, statement etc (see e.g.).", "labels": [], "entities": [{"text": "automatic dialogue processing", "start_pos": 32, "end_pos": 61, "type": "TASK", "confidence": 0.6292803486188253}, {"text": "dialogue act tagging", "start_pos": 65, "end_pos": 85, "type": "TASK", "confidence": 0.6282017032305399}]}, {"text": "Although factors such as intonation also play a role (see e.g. (), one of the most important sources of information in this task is the semantic meaning of an utterance, and this is reflected in the fact that people use similar words when they perform similar utterance acts.", "labels": [], "entities": []}, {"text": "For example, utterances which state opinion (tagged sv in the standard DAMSL schema, see below) often include words such as \"I think\", \"I believe\", \"I guess\" etc.", "labels": [], "entities": []}, {"text": "Hence, a similarity-based model of meaning -for instance, a distributional semantic model -should provide benefits over a purely word-based model for dialogue act tagging.", "labels": [], "entities": [{"text": "dialogue act tagging", "start_pos": 150, "end_pos": 170, "type": "TASK", "confidence": 0.697080671787262}]}, {"text": "However, since utterances generally consist of more than one word, one has to be able to extend such similarity-based models from single words to sentences and/or complete utterances.", "labels": [], "entities": []}, {"text": "Hence, we consider here the application of compositional distributional semantics for this task.", "labels": [], "entities": []}, {"text": "Here, we extend bag-of-word models common in previous approaches ( with simple compositional distributional operations ( and examine the improvements gained.", "labels": [], "entities": []}, {"text": "These improvements suggest that distributional information does improve performance, but that more sophisticated compositional operations such as matrix multiplication () should provide further benefits.", "labels": [], "entities": []}, {"text": "The state of the art is a supervised method based on Recurrent Convolutional Neural Networks.", "labels": [], "entities": []}, {"text": "This method learns both the sentence model and the discourse model from the same training corpus, making it hard to understand how much of the contribution comes from the inclusion of distributional word meaning, and how much from learning patterns specific to the corpus at hand.", "labels": [], "entities": []}, {"text": "Here, in contrast, we use an external unlabeled resource to obtain a model of word meaning, composing words to obtain representations for utterances, and rely on training data only for discourse learning for the tagging task itself.", "labels": [], "entities": []}, {"text": "First, we discuss related work by introducing distributional semantics and describe common approaches for dialogue act tagging in Section 2.", "labels": [], "entities": [{"text": "dialogue act tagging", "start_pos": 106, "end_pos": 126, "type": "TASK", "confidence": 0.7285819252332052}]}, {"text": "Section 3 proposes several models for utterance representation based on the bag of words approach and word vector composition.", "labels": [], "entities": [{"text": "utterance representation", "start_pos": 38, "end_pos": 62, "type": "TASK", "confidence": 0.9318190515041351}, {"text": "word vector composition", "start_pos": 102, "end_pos": 125, "type": "TASK", "confidence": 0.657187839349111}]}, {"text": "We describe the experiment and discuss the result in Section 4.", "labels": [], "entities": []}, {"text": "Finally, Section 5 concludes the work.", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate these possible models we follow).", "labels": [], "entities": []}, {"text": "Once we have applied a model to extract features from utterances and build a vector space, the dimensionality of the vector space is reduced using SVD to 50 dimensions.", "labels": [], "entities": []}, {"text": "Then a k-nearest neighbours (KNN) classifier is trained and used for utterance tag prediction.", "labels": [], "entities": [{"text": "utterance tag prediction", "start_pos": 69, "end_pos": 93, "type": "TASK", "confidence": 0.8490198850631714}]}, {"text": "In contrast to (, we use Euclidean distance as a distance metric and choose the most Method Accuracy ( 0.739 ( 0.719 ( 0.710 ( 0.", "labels": [], "entities": [{"text": "Method Accuracy", "start_pos": 85, "end_pos": 100, "type": "METRIC", "confidence": 0.8737891614437103}]}, {"text": "Baseline In our experiments, the bag of unigrams model accuracy of 0.602 is lower than the accuracy of 0.654 reported in (, see.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 55, "end_pos": 63, "type": "METRIC", "confidence": 0.972745418548584}, {"text": "accuracy", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.9980785846710205}]}, {"text": "The lower performance maybe due to the differences between Switchboard and CallHome37 corpora, in particular the tag distribution.", "labels": [], "entities": []}, {"text": "In CallHome37, 42.7% of utterances are labeled with the most frequent dialogue act, while the figure in Switchboard is 31.5%; the more even distribution in Switchboard is likely to make overall average accuracy levels lower.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 202, "end_pos": 210, "type": "METRIC", "confidence": 0.9968462586402893}]}, {"text": "Word order As shows, the bag of bigrams model improves over unigrams.", "labels": [], "entities": []}, {"text": "This confirms that word order provides important information for predicting dialogue act tags.", "labels": [], "entities": [{"text": "predicting dialogue act tags", "start_pos": 65, "end_pos": 93, "type": "TASK", "confidence": 0.9159853309392929}]}, {"text": "Distributional models Performance of compositional distributional models depends both on compositional operator and weighting.", "labels": [], "entities": []}, {"text": "demonstrates accuracy of the models.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.999518871307373}]}, {"text": "We instantiate 3 vector spaces from Google Ngrams: one space with raw co-occurrence frequencies, a tf-idf weighted space and a reduced space using NMF.", "labels": [], "entities": []}, {"text": "Addition outperforms multiplication in our experiments, although for other tasks multiplication has been shown to perform better (.", "labels": [], "entities": []}, {"text": "Lower multiplication performance here might be The CallHome37 corpus is not currently available to us.", "labels": [], "entities": [{"text": "CallHome37 corpus", "start_pos": 51, "end_pos": 68, "type": "DATASET", "confidence": 0.9684523046016693}]}], "tableCaptions": [{"text": " Table 1: Comparison with previous work. Note  that (Serafin et al., 2003) do not use Switchboard  and therefore their results are not directly compa- rable to others.", "labels": [], "entities": []}]}