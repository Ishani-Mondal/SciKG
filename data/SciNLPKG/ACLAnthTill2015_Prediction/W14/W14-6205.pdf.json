{"title": [{"text": "Semi-supervised Sequence Labeling for Named Entity Extraction based on Tri-Training: Case Study on Chinese Person Name Extraction", "labels": [], "entities": [{"text": "Sequence Labeling", "start_pos": 16, "end_pos": 33, "type": "TASK", "confidence": 0.9225704669952393}, {"text": "Named Entity Extraction", "start_pos": 38, "end_pos": 61, "type": "TASK", "confidence": 0.6332927246888479}, {"text": "Chinese Person Name Extraction", "start_pos": 99, "end_pos": 129, "type": "TASK", "confidence": 0.6072743237018585}]}], "abstractContent": [{"text": "Named entity extraction is a fundamental task for many knowledge engineering applications.", "labels": [], "entities": [{"text": "Named entity extraction", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.6849481264750162}]}, {"text": "Existing studies rely on annotated training data, which is quite expensive when used to obtain large data sets, limiting the effectiveness of recognition.", "labels": [], "entities": []}, {"text": "In this research, we propose an automatic labeling procedure to prepare training data from structured resources which contain known named entities.", "labels": [], "entities": []}, {"text": "While this automatically labeled training data may contain noise, a self-testing procedure maybe used as a follow-up to remove low-confidence annotation and increase the extraction performance with less training data.", "labels": [], "entities": []}, {"text": "In addition to the preparation of labeled training data, we also employed semi-supervised learning to utilize large unlabeled training data.", "labels": [], "entities": []}, {"text": "By modifying tri-training for sequence labeling and deriving the proper initialization, we can further improve entity extraction.", "labels": [], "entities": [{"text": "entity extraction", "start_pos": 111, "end_pos": 128, "type": "TASK", "confidence": 0.799761950969696}]}, {"text": "In the task of Chinese personal name extraction with 364,685 sentences (8,672 news articles) and 54,449 (11,856 distinct) person names, an F-measure of 90.4% can be achieved.", "labels": [], "entities": [{"text": "Chinese personal name extraction", "start_pos": 15, "end_pos": 47, "type": "TASK", "confidence": 0.5942781940102577}, {"text": "F-measure", "start_pos": 139, "end_pos": 148, "type": "METRIC", "confidence": 0.9996923208236694}]}], "introductionContent": [{"text": "Detecting named entities in documents is one of the most important tasks for message understanding.", "labels": [], "entities": [{"text": "Detecting named entities in documents", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.9156938791275024}, {"text": "message understanding", "start_pos": 77, "end_pos": 98, "type": "TASK", "confidence": 0.8836776316165924}]}, {"text": "For example, the #Microposts 2014 Workshop hosted an \"Entity Extraction and Linking Challenge\", which aimed to automatically extract entities from English microposts and link them to the corresponding English DBpedia v3.9 resources (if a linkage existed).", "labels": [], "entities": []}, {"text": "Like many other types of research, this task relies on annotated training examples that require large amounts of manual labeling, leading to a limited number of training examples (e.g. 2.3K tweets).", "labels": [], "entities": []}, {"text": "While human-labelled training examples (\u00ed \u00b5\u00ed\u00b0\u00bf) have high quality, their cost is very high.", "labels": [], "entities": []}, {"text": "Thus the major concern in this paper is how to prepare training data for entity extraction learning on the Web.", "labels": [], "entities": [{"text": "entity extraction learning", "start_pos": 73, "end_pos": 99, "type": "TASK", "confidence": 0.8615173896153768}]}, {"text": "In practice, sometimes there are existing structured databases of known entities that are valuable to improve extraction accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 121, "end_pos": 129, "type": "METRIC", "confidence": 0.8607908487319946}]}, {"text": "For examples, personal names, school names, and company names can be obtained from a Who's Who website, and accessible government data for registered schools and businesses, respectively.", "labels": [], "entities": []}, {"text": "Meanwhile, there are many unlabeled training examples that can be used for many information extraction tasks.", "labels": [], "entities": [{"text": "information extraction tasks", "start_pos": 80, "end_pos": 108, "type": "TASK", "confidence": 0.8852927883466085}]}, {"text": "If we can automatically label known entities in the unlabeled training examples, we can obtain large labeled training set.", "labels": [], "entities": []}, {"text": "While such training data may contain errors, self-testing can be applied to filter unreliable labeling with less confidence.", "labels": [], "entities": []}, {"text": "On the other hand, the use of unlabeled training examples (\u00ed \u00b5\u00ed\u00b1\u0088) has also been proved to be a promising technique for classification.", "labels": [], "entities": []}, {"text": "For example, co-training) and tri-training ( ) are two successful techniques that use examples with high-confidence as predicted by the other classifier or examples with consensus answers from the other two classifiers in order to prepare new labeled training data for learning.", "labels": [], "entities": []}, {"text": "By estimating the error rate of each learned classifier, we can calculate the maximum number of new consensus answers for learning to ensure the error rates are reduced.", "labels": [], "entities": [{"text": "error rate", "start_pos": 18, "end_pos": 28, "type": "METRIC", "confidence": 0.9495662748813629}]}, {"text": "In this paper, we explore the possibility of extending semi-supervised learning to sequence labeling via tri-training so that unlabeled training examples can also be used in the learning phase.", "labels": [], "entities": []}, {"text": "The challenge here is to obtain a common label sequence as a consensus answer from multiple models.", "labels": [], "entities": []}, {"text": "As enumerating all possible label sequences will be too time-consuming, we employ a confidence level to control the co-labeling answer such that a label sequence with the largest probability is selected.", "labels": [], "entities": []}, {"text": "Comparing with a common label sequence from multiple models, the most probable label sequence has larger chance to obtain a consensus answer for training and testing.", "labels": [], "entities": []}, {"text": "In addition to the extension of tri-training algorithm to sequence labeling, another key issue with tritraining is the assumption of the initial error rate (0.5), leading to a limited number of co-labeling examples for training and early termination for large set training.", "labels": [], "entities": [{"text": "sequence labeling", "start_pos": 58, "end_pos": 75, "type": "TASK", "confidence": 0.6135870963335037}, {"text": "initial error rate", "start_pos": 137, "end_pos": 155, "type": "METRIC", "confidence": 0.8925818999608358}]}, {"text": "Therefore, anew estimation method is devised for the estimation of initial error rate to alleviate the problem and improve the overall performance.", "labels": [], "entities": [{"text": "initial error rate", "start_pos": 67, "end_pos": 85, "type": "METRIC", "confidence": 0.7166033387184143}]}, {"text": "To validate the proposed method, we conduct experiments on Chinese personal name extraction using 7,000 known Chinese celebrity names (abbreviated as CCN).", "labels": [], "entities": [{"text": "Chinese personal name extraction", "start_pos": 59, "end_pos": 91, "type": "TASK", "confidence": 0.6039742305874825}]}, {"text": "We collect news articles containing these personal names from Google's search engine (using these names as keywords) and automatically label these articles containing CCN and known reporters' names.", "labels": [], "entities": []}, {"text": "Ina test set of 8,672 news articles (364,685 sentences) containing 54,449 personal names (11,856 distinct names), the basic model built on CRF (conditional random field) has a performance of 76.8% F-measure when using 500 celebrity names for preparing training data, and is improved to 86.4% F-measure when 7,000 celebrity names are used.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 197, "end_pos": 206, "type": "METRIC", "confidence": 0.9990311861038208}, {"text": "F-measure", "start_pos": 292, "end_pos": 301, "type": "METRIC", "confidence": 0.998868465423584}]}, {"text": "With self-testing, the performance is improved to 88.9%.", "labels": [], "entities": []}, {"text": "Finally, tri-training can further improve the performance through unlabeled data to 90.4%.", "labels": [], "entities": []}], "datasetContent": [{"text": "We apply our proposed approach on Chinese personal name extraction.", "labels": [], "entities": [{"text": "Chinese personal name extraction", "start_pos": 34, "end_pos": 66, "type": "TASK", "confidence": 0.5836945101618767}]}, {"text": "We use known celebrity names to query search engines for news articles from four websites (including Liberty Times, Apple Daily, China Times, and United Daily News) and collect the top 10 search results for sentences that contain the query keyword and uses these query keyword as extraction target via automatic labeling.", "labels": [], "entities": [{"text": "Liberty Times", "start_pos": 101, "end_pos": 114, "type": "DATASET", "confidence": 0.9786336123943329}, {"text": "China Times", "start_pos": 129, "end_pos": 140, "type": "DATASET", "confidence": 0.8753949999809265}, {"text": "United Daily News", "start_pos": 146, "end_pos": 163, "type": "DATASET", "confidence": 0.9643386403719584}]}, {"text": "Given different numbers of personal names, we prepare six datasets by automatically labeling as mentioned in the beginning of Section 3 and consider them as labeled training examples.", "labels": [], "entities": []}, {"text": "We also crawl these four news websites from 2013/01/01 to 2013/03/31 and obtain 20,974 articles for unlabeled and testing data.", "labels": [], "entities": [{"text": "2013/01/01 to 2013/03/31", "start_pos": 44, "end_pos": 68, "type": "DATASET", "confidence": 0.8300587914206765}]}, {"text": "To increase the possibility of containing person names, we select sentences that include some common The relationship among Eq.,, and (9).", "labels": [], "entities": []}, {"text": "surname followed by some common first name to obtain 240,994 as unlabeled data (\u00ed \u00b5\u00ed\u00b1\u0088)).", "labels": [], "entities": []}, {"text": "For testing, we manually labeled 8,672 news articles, yielding a total of 364,685 sentences with 54,449 person names (11,856 distinct person names).", "labels": [], "entities": []}, {"text": "For the tagging scheme, we used BIEOS to mark the named entities to be extracted.", "labels": [], "entities": [{"text": "tagging", "start_pos": 8, "end_pos": 15, "type": "TASK", "confidence": 0.9639085531234741}, {"text": "BIEOS", "start_pos": 32, "end_pos": 37, "type": "METRIC", "confidence": 0.992230236530304}]}, {"text": "Fourteen features were used in the experiment including, common surnames, first names, job titles, numeric tokens, alphabet tokens, punctuation symbol, and common characters in front or behind personal names.", "labels": [], "entities": []}, {"text": "The predefined dictionaries contain 486 job titles, 224 surnames, 38,261 first names, and 107 symbols as well as 223 common words in front of and behind person name.", "labels": [], "entities": []}, {"text": "We use CRF++) for the following experiment.", "labels": [], "entities": []}, {"text": "With a template involving unigram macros and the previous three tokens and behind, a total of 195 features are produced.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1 Labeled dataset (\u00ed \u00b5\u00ed\u00b0\u00bf) and unlabeled dataset (\u00ed \u00b5\u00ed\u00b1\u0088) for Chinese person name extraction", "labels": [], "entities": []}]}