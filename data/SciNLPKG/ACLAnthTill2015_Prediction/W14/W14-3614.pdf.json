{"title": [{"text": "A Pipeline Approach to Supervised Error Correction for the QALB-2014 Shared Task", "labels": [], "entities": [{"text": "QALB-2014 Shared Task", "start_pos": 59, "end_pos": 80, "type": "DATASET", "confidence": 0.8206484913825989}]}], "abstractContent": [{"text": "This paper describes our submission to the ANLP-2014 shared task on automatic Arabic error correction.", "labels": [], "entities": [{"text": "ANLP-2014 shared task", "start_pos": 43, "end_pos": 64, "type": "TASK", "confidence": 0.5708046356836954}, {"text": "automatic Arabic error correction", "start_pos": 68, "end_pos": 101, "type": "TASK", "confidence": 0.5383372977375984}]}, {"text": "We present a pipeline approach integrating an error detection model, a combination of character-and word-level translation models , a reranking model and a punctuation insertion model.", "labels": [], "entities": [{"text": "error detection", "start_pos": 46, "end_pos": 61, "type": "TASK", "confidence": 0.696428045630455}, {"text": "word-level translation", "start_pos": 100, "end_pos": 122, "type": "TASK", "confidence": 0.6448248624801636}]}, {"text": "We achieve an F 1 score of 62.8% on the development set of the QALB corpus, and 58.6% on the official test set.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 14, "end_pos": 23, "type": "METRIC", "confidence": 0.9930473367373148}, {"text": "QALB corpus", "start_pos": 63, "end_pos": 74, "type": "DATASET", "confidence": 0.964430570602417}, {"text": "official test set", "start_pos": 93, "end_pos": 110, "type": "DATASET", "confidence": 0.7582232554753622}]}], "introductionContent": [{"text": "Devising algorithms for automatic error correction generated considerable interest in the community since the early 1960s for at least two reasons.", "labels": [], "entities": [{"text": "automatic error correction", "start_pos": 24, "end_pos": 50, "type": "TASK", "confidence": 0.6335885723431905}]}, {"text": "First, typical NLP tools lack in robustness against errors in their input.", "labels": [], "entities": []}, {"text": "This sensitivity jeopardizes their usefulness especially for unedited text, which is prevalent on the web.", "labels": [], "entities": []}, {"text": "Second, automated spell and grammar checkers facilitate text editing and can be of great help to nonnative speakers of a language.", "labels": [], "entities": [{"text": "text editing", "start_pos": 56, "end_pos": 68, "type": "TASK", "confidence": 0.7912605404853821}]}, {"text": "Several resources and shared tasks appeared recently, including the HOO task and the CoNLL task on grammatical error correction (.", "labels": [], "entities": [{"text": "grammatical error correction", "start_pos": 99, "end_pos": 127, "type": "TASK", "confidence": 0.5705964465936025}]}, {"text": "In this paper we describe our participation to the first shared task on automatic error correction for Arabic ( ).", "labels": [], "entities": [{"text": "automatic error correction for Arabic", "start_pos": 72, "end_pos": 109, "type": "TASK", "confidence": 0.6848182380199432}]}, {"text": "While non-word errors are relatively easy to handle, the task is more challenging for grammatical and semantic errors.", "labels": [], "entities": []}, {"text": "Detecting and correcting such errors require context-sensitive approaches in order to capture the dependencies between the words of a text at various lexical and semantic levels.", "labels": [], "entities": [{"text": "Detecting", "start_pos": 0, "end_pos": 9, "type": "TASK", "confidence": 0.9591243267059326}]}, {"text": "All the more so for Arabic which brings dependence down to the morphological level.", "labels": [], "entities": []}, {"text": "A particularity interesting approach to error correction relies on statistical machine translation (SMT)), due to its contextsensitivity and data-driven aspect.", "labels": [], "entities": [{"text": "error correction", "start_pos": 40, "end_pos": 56, "type": "TASK", "confidence": 0.7680764496326447}, {"text": "statistical machine translation (SMT))", "start_pos": 67, "end_pos": 105, "type": "TASK", "confidence": 0.7853303700685501}]}, {"text": "Therefore, the pipeline system which we describe in Section 2 has as its core a phrase-based SMT component (PBSMT) (Section 2.3).", "labels": [], "entities": [{"text": "SMT", "start_pos": 93, "end_pos": 96, "type": "TASK", "confidence": 0.8262777328491211}]}, {"text": "Nevertheless, several factors may hinder the success of this approach, such as data sparsity, discrepancies between translation and error correction tasks, and the difficulty of incorporating context-sensitive features into the SMT decoder.", "labels": [], "entities": [{"text": "translation and error correction", "start_pos": 116, "end_pos": 148, "type": "TASK", "confidence": 0.8111731261014938}, {"text": "SMT decoder", "start_pos": 228, "end_pos": 239, "type": "TASK", "confidence": 0.9195317625999451}]}, {"text": "We address all these issues in our system which achieves a better correction quality than a simple word-level PBSMT baseline on the QALB corpus ( ) as we show in our experiments in Section 3.", "labels": [], "entities": [{"text": "QALB corpus", "start_pos": 132, "end_pos": 143, "type": "DATASET", "confidence": 0.9274974763393402}]}], "datasetContent": [{"text": "All the models we use in our pipeline are trained in a supervised way using the training part of the QALB corpus ( ), while we reserve the development part of the corpus for testing.", "labels": [], "entities": [{"text": "QALB corpus", "start_pos": 101, "end_pos": 112, "type": "DATASET", "confidence": 0.894921064376831}]}], "tableCaptions": [{"text": " Table 1: Pipeline precision, recall and F 1 scores.  ED: error detection, PI: punctuation insertion.", "labels": [], "entities": [{"text": "precision", "start_pos": 19, "end_pos": 28, "type": "METRIC", "confidence": 0.9440329670906067}, {"text": "recall", "start_pos": 30, "end_pos": 36, "type": "METRIC", "confidence": 0.99937903881073}, {"text": "F 1 scores", "start_pos": 41, "end_pos": 51, "type": "METRIC", "confidence": 0.9850906133651733}, {"text": "ED", "start_pos": 54, "end_pos": 56, "type": "METRIC", "confidence": 0.990263819694519}, {"text": "error detection", "start_pos": 58, "end_pos": 73, "type": "TASK", "confidence": 0.8218487501144409}, {"text": "PI", "start_pos": 75, "end_pos": 77, "type": "METRIC", "confidence": 0.9733204245567322}, {"text": "punctuation insertion", "start_pos": 79, "end_pos": 100, "type": "TASK", "confidence": 0.539418175816536}]}]}