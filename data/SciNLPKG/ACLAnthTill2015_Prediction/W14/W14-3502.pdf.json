{"title": [{"text": "An analysis of a French as a Foreign language corpus for readability assessment", "labels": [], "entities": []}], "abstractContent": [{"text": "Readability aims to assess the difficulty of texts based on various linguistic predictors (the lexicon used, the complexity of sentences, the coherence of the text, etc.).", "labels": [], "entities": []}, {"text": "It is an active field that has applications in a large number of NLP domains, among which machine translation, text simplification, text summarisation, or CALL (Computer-Assisted Language Learning).", "labels": [], "entities": [{"text": "machine translation", "start_pos": 90, "end_pos": 109, "type": "TASK", "confidence": 0.8146314024925232}, {"text": "text simplification", "start_pos": 111, "end_pos": 130, "type": "TASK", "confidence": 0.80375075340271}, {"text": "text summarisation", "start_pos": 132, "end_pos": 150, "type": "TASK", "confidence": 0.7759377062320709}]}, {"text": "For CALL, readability tools could be used to help the retrieval of educational materials or to make CALL platforms more adaptive.", "labels": [], "entities": []}, {"text": "However, developing a readability formula is a costly process that requires a large amount of texts annotated in terms of difficulty.", "labels": [], "entities": []}, {"text": "The current mainstream method to gather such a large corpus of annotated texts is to get them from educational resources such as textbooks or simplified readers.", "labels": [], "entities": []}, {"text": "In this paper, we describe the collection process of an annotated corpus of French as a foreign language texts with the purpose of training a readability model.", "labels": [], "entities": []}, {"text": "We follow the mainstream approach, getting the texts from textbooks, but we are concerned with the limitations of such \"annotation\" approach, in particular, as regards the homogeneity of the difficulty annotations across textbook series.", "labels": [], "entities": []}, {"text": "Their reliability is assessed using both a qualitative and a quantitative analysis.", "labels": [], "entities": [{"text": "reliability", "start_pos": 6, "end_pos": 17, "type": "METRIC", "confidence": 0.9897058010101318}]}, {"text": "It appears that, for some educational levels, the hypothesis of the annotation ho-mogeneity must be rejected.", "labels": [], "entities": []}, {"text": "Various reasons for such findings are discussed and the paper concludes with recommandations for future similar attempts.", "labels": [], "entities": []}], "introductionContent": [{"text": "Today, the market for foreign language learning is actively growing as a result of various factors, such as the E.U. enlargement and the increase in the number of languages represented in the Union, but also a greater mobility of its citizens.", "labels": [], "entities": []}, {"text": "Faced with this increased interest in foreign language learning, teaching institutions are struggling to keep up with demand.", "labels": [], "entities": [{"text": "foreign language learning", "start_pos": 38, "end_pos": 63, "type": "TASK", "confidence": 0.662109782298406}]}, {"text": "In this context, the domains of CALL (Computer-Assisted Language Learning) and iCALL (Intelligent CALL) have a role to play.", "labels": [], "entities": []}, {"text": "Various CALL and iCALL applications have been designed to enhance classroom practices or replace it, but they still lack some flexibility as regards the input and the feedback offered to the user ().", "labels": [], "entities": []}, {"text": "For instance, some adaptive programs are able to select, in an exercise database, an item tailored to the learner's level).", "labels": [], "entities": []}, {"text": "However, it requires the pre-annotation of all the items in terms of difficulty, which restricts the versatility of the user module.", "labels": [], "entities": []}, {"text": "Being able to generate suitable exercises on the fly from a corpus appears as a better way to adapt to specific learner difficulties.", "labels": [], "entities": []}, {"text": "The automatic generation of exercises has already been researched, mostly for English), but also for French ().", "labels": [], "entities": []}, {"text": "However, the majority of these systems either use excerpts whose difficulty has been manually annotated or excerpts extracted from a large corpus and thus lacking any difficulty annotations.", "labels": [], "entities": []}, {"text": "In the first case, the system is able to adapt to the user's needs only within the limits of the available materials.", "labels": [], "entities": []}, {"text": "In the second case, any type of exercise can be generated on the fly, but because there is no control of the difficulty of excerpts, the contextual complexity is likely to hinder the user's comprehension and his/her ability to perform the exercise.", "labels": [], "entities": []}, {"text": "Faced with this challenge, one solution is to use readability metrics in order to pre-select a subset of excerpts matching the user's proficiency level, as it is done in the L\u00e4rka platform (.", "labels": [], "entities": []}, {"text": "Readability is afield that aims to assess the difficulty of texts in a reproducible way -which can therefore be automatized -based on various linguistic dimensions of the texts (e.g. lexicon, syntax, text structure, etc.).", "labels": [], "entities": []}, {"text": "The first studies in the field date back to the 1920's ( and have traditionally been carried out by psychologists.", "labels": [], "entities": []}, {"text": "However, readability has undergone recent developments.", "labels": [], "entities": []}, {"text": "They result from the contact with two other fields: natural language processing (NLP) is used to extract more complex linguistic predictors, whereas artificial intelligence (AI) provides complex statistical algorithms to better exploit the regularities existing between text difficulty and the linguistic predictors.", "labels": [], "entities": []}, {"text": "Recent work has been carried out mostly on English as a first language (L1)) or English as a second or foreign language (L2)), but also on other languages such as Swedish (), French (), or Arabic (Al-Khalifa and Al-Ajlan, 2010), among others.", "labels": [], "entities": []}, {"text": "Although the field is quite lively, there is only limited work specifically dedicated to the readability of L2 languages.", "labels": [], "entities": []}, {"text": "Furthermore, attempts to integrate such L2 readability models within an automatic exercise generation system are even more scarce.", "labels": [], "entities": []}, {"text": "In our view, this can be explained by the high cost needed to create a readability model, especially in terms of the corpus collection process.", "labels": [], "entities": [{"text": "corpus collection process", "start_pos": 117, "end_pos": 142, "type": "TASK", "confidence": 0.7569954097270966}]}, {"text": "Moreover, a convenient readability model should be able to output predictions that are useful for users.", "labels": [], "entities": []}, {"text": "In Europe, this means to be able to assess text complexity in terms of the Common European Framework of Reference for Languages (CEFR)).", "labels": [], "entities": [{"text": "Common European Framework of Reference for Languages (CEFR))", "start_pos": 75, "end_pos": 135, "type": "DATASET", "confidence": 0.7417175740003585}]}, {"text": "This scale has now become the reference for foreign language education within Europe.", "labels": [], "entities": []}, {"text": "To our knowledge, only two research teams have currently designed a readability model compliant with the CEFR scale ().", "labels": [], "entities": [{"text": "CEFR scale", "start_pos": 105, "end_pos": 115, "type": "DATASET", "confidence": 0.7791354060173035}]}, {"text": "We suspect that this is partly due to the efforts needed to collect the training corpus required to develop such readability formula.", "labels": [], "entities": []}, {"text": "In this paper, we detail the collection process of a readability-intended corpus that has been carried out for French as a foreign language (FFL), using FFL textbooks as a source for the labelled texts.", "labels": [], "entities": [{"text": "French as a foreign language (FFL)", "start_pos": 111, "end_pos": 145, "type": "TASK", "confidence": 0.6627021431922913}, {"text": "FFL textbooks", "start_pos": 153, "end_pos": 166, "type": "DATASET", "confidence": 0.9067112505435944}]}, {"text": "We describe the various issues encountered during this collection, focusing mostly on the issue of the reliability of the difficulty annotations.", "labels": [], "entities": [{"text": "reliability", "start_pos": 103, "end_pos": 114, "type": "METRIC", "confidence": 0.9872570633888245}]}, {"text": "In section 2, we first expose the various type of criteria that have been used in readability studies to get data annotated in terms of difficulty and we discuss the advantages and shortcomings of each of them.", "labels": [], "entities": []}, {"text": "The section 3 then details our collection process and describes the resulting corpus.", "labels": [], "entities": []}, {"text": "Finally, Section 4 investigates the quality of the collected data, using both a qualitative analysis and a quantitative analysis based on statistical tests to assess the homogeneity of the annotations across textbooks.", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to determine whether having an unbalanced dataset would impact subsequent learning on that corpus, we applied the following methodology.", "labels": [], "entities": []}, {"text": "We sampled two different datasets from the whole corpus.", "labels": [], "entities": []}, {"text": "For the first (Corpus6Apriori), we simply applied a stratified sampling that respects the a priori probability of each class.", "labels": [], "entities": [{"text": "Corpus6Apriori", "start_pos": 15, "end_pos": 29, "type": "DATASET", "confidence": 0.9582515954971313}]}, {"text": "This amounts to 66 texts for A1, 72 for A2, 99 for B1, 29 for B2, 27 for C1 and 7 for C2.", "labels": [], "entities": [{"text": "A2", "start_pos": 40, "end_pos": 42, "type": "METRIC", "confidence": 0.9474558234214783}]}, {"text": "For the second dataset (Corpus6Equi), we also applied a stratified sampling, but selected a fixed amount of texts in each class-about 50, which corresponds to the size of the least populated class (C2).", "labels": [], "entities": [{"text": "Corpus6Equi", "start_pos": 24, "end_pos": 35, "type": "DATASET", "confidence": 0.9308391809463501}]}, {"text": "Finally, we sampled 120 texts (20 per level) in the remaining texts 2 to be used as the test set.", "labels": [], "entities": []}, {"text": "Concerning the readability model, since the aim was not to reach the highest performance possible, we selected two simple and broadly-used linguistic features as predictors: the mean number of letter per words (NLM) and the mean number of words per sentence (NWS).", "labels": [], "entities": [{"text": "mean number of letter per words (NLM)", "start_pos": 178, "end_pos": 215, "type": "METRIC", "confidence": 0.6991180380185446}, {"text": "mean number of words per sentence (NWS)", "start_pos": 224, "end_pos": 263, "type": "METRIC", "confidence": 0.6467183530330658}]}, {"text": "They were combined with a proportional-odds model, also known as ordinal logistic regression.", "labels": [], "entities": []}, {"text": "Their performance were assessed with the multiple correlation coefficient (R 2 ), estimated on the training set, the test set and using a bootstrap .632 procedure 3 . The results are detailed in: R 2 estimated, for both datasets, on the training set, on the test set or with the bootstrap .632 procedure.", "labels": [], "entities": [{"text": "multiple correlation coefficient (R 2 )", "start_pos": 41, "end_pos": 80, "type": "METRIC", "confidence": 0.9239375591278076}]}, {"text": "Surprisingly, the Corpus6Apriori model performs better in all of the three conditions (training, test and bootstrap).", "labels": [], "entities": [{"text": "Corpus6Apriori", "start_pos": 18, "end_pos": 32, "type": "DATASET", "confidence": 0.932855486869812}, {"text": "bootstrap", "start_pos": 106, "end_pos": 115, "type": "METRIC", "confidence": 0.938062310218811}]}, {"text": "However, this apparent superiority must be qualified when we look more closely at the confusion matrix.", "labels": [], "entities": []}, {"text": "show the confusion matrix for both models on the test set.", "labels": [], "entities": [{"text": "confusion", "start_pos": 9, "end_pos": 18, "type": "METRIC", "confidence": 0.9749040603637695}]}, {"text": "It clearly appears that the high number of B1 texts in the Corpus6Apriori condition distorts the regression space (about 50% of the texts are predicted as B1).", "labels": [], "entities": [{"text": "Corpus6Apriori condition", "start_pos": 59, "end_pos": 83, "type": "DATASET", "confidence": 0.9502445459365845}]}, {"text": "The model trained on Corpus6Equi presents a more balanced distribution that slightly favours the extreme classes (A1 and C2) . Moreover, the Corpus6Apriori model is notable to classify any text as B2, which is a very critical flaw fora tool aiming to be used in real contexts by L2 learners or teachers.", "labels": [], "entities": [{"text": "Corpus6Equi", "start_pos": 21, "end_pos": 32, "type": "DATASET", "confidence": 0.9707324504852295}]}, {"text": "We conclude from this first experiment that a readability corpus should have, as much as possible, a balanced number of observations per class.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Number of texts per level, by textbooks series", "labels": [], "entities": []}, {"text": " Table 3: R 2 estimated, for both datasets, on the training set, on the test set or with the bootstrap  .632 procedure.", "labels": [], "entities": []}, {"text": " Table 4: Confusion matrix for the model trained on Corpus6Equi.", "labels": [], "entities": [{"text": "Corpus6Equi", "start_pos": 52, "end_pos": 63, "type": "DATASET", "confidence": 0.9792940020561218}]}, {"text": " Table 5: Confusion matrix for the model trained on Corpus6Apriori.", "labels": [], "entities": [{"text": "Corpus6Apriori", "start_pos": 52, "end_pos": 66, "type": "DATASET", "confidence": 0.9796004891395569}]}, {"text": " Table 6: Mean number of letters per word for each textbook and per CEFR level. Textbooks  with a possible problem of consistency are highlighted in bold. Numbers in parentheses refer  to the textbook volume within the series. Some levels indeed have texts extracted from two  different textbooks in the same series.", "labels": [], "entities": [{"text": "CEFR", "start_pos": 68, "end_pos": 72, "type": "METRIC", "confidence": 0.5598192811012268}, {"text": "consistency", "start_pos": 118, "end_pos": 129, "type": "METRIC", "confidence": 0.9678471684455872}]}, {"text": " Table 7: Mean number of words per sentence for each textbook and per CEFR level. Textbooks  with a possible problem of consistency are highlighted in bold.", "labels": [], "entities": [{"text": "Mean", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9856165647506714}]}, {"text": " Table 8: Mean number of ideas per text for each textbook and per CEFR level. Textbooks with a  possible problem of consistency are highlighted in bold.", "labels": [], "entities": [{"text": "CEFR", "start_pos": 66, "end_pos": 70, "type": "DATASET", "confidence": 0.750749409198761}]}, {"text": " Table 9: P-value for each ANOVA tests. A value inferior to 0.05 means that the homogeneity  hypothesis has been rejected for this level. Significance level are noted as follows: p < 0.001:   *   *   * ; p < 0.01:  *  *  et p < 0.05:  * .", "labels": [], "entities": [{"text": "P-value", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.979674756526947}, {"text": "ANOVA", "start_pos": 27, "end_pos": 32, "type": "TASK", "confidence": 0.598865270614624}]}]}