{"title": [{"text": "Automatic Assessment of the Speech of Young English Learners", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper introduces some of the research behind automatic scoring of the speaking part of the Arizona English Language Learner Assessment, a large-scale test now operational for students in Arizona.", "labels": [], "entities": [{"text": "Arizona English Language Learner Assessment", "start_pos": 96, "end_pos": 139, "type": "TASK", "confidence": 0.6012304544448852}]}, {"text": "Approximately 70% of the students tested are in the range 4-11 years old.", "labels": [], "entities": []}, {"text": "We cover the methods used to assess spoken responses automatically, considering both what the student says and the way in which the student speaks.", "labels": [], "entities": []}, {"text": "We also provide evidence for the validity of machine scores.", "labels": [], "entities": [{"text": "validity", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.9907213449478149}]}, {"text": "The assessments include 10 open-ended item types.", "labels": [], "entities": []}, {"text": "For 9 of the 10 open item types, machine scoring performed at a similar level or better than human scoring at the item-type level.", "labels": [], "entities": []}, {"text": "At the participant level, correlation coefficients between machine overall scores and average human overall scores were: Kindergarten: 0.88; Grades 1-2: 0.90; Grades 3-5: 0.94; Grades 6-8: 0.95; Grades 9-12: 0.93.", "labels": [], "entities": []}, {"text": "The average correlation coefficient was 0.92.", "labels": [], "entities": [{"text": "correlation", "start_pos": 12, "end_pos": 23, "type": "METRIC", "confidence": 0.9890432357788086}]}, {"text": "We include a note on implementing a detector to catch problematic test performances.", "labels": [], "entities": []}], "introductionContent": [{"text": "Arizona English Language Learner Assessment (AZELLA)) is a test administered in the state of Arizona to all students from kindergarten up to grade 12 (K-12) who had been previously identified as English learners (ELs).", "labels": [], "entities": [{"text": "Arizona English Language Learner Assessment (AZELLA))", "start_pos": 0, "end_pos": 53, "type": "TASK", "confidence": 0.6666590832173824}]}, {"text": "AZELLA is used to place EL students into an appropriate level of instructional and to reassess EL students on an annual basis to monitor their progress.", "labels": [], "entities": [{"text": "AZELLA", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.8742936253547668}]}, {"text": "AZELLA was originally a fully human-delivered paper-pencil test covering four domains: listening, speaking, reading and writing.", "labels": [], "entities": [{"text": "AZELLA", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.7880134582519531}]}, {"text": "The Arizona Department of Education chose to automate the delivery and scoring of the speaking parts of the test, and further decided that test delivery via speakerphone would be the most efficient and universally accessible mode of administration.", "labels": [], "entities": []}, {"text": "During the first field test (Nov. 7 -Dec. 2, 2011) over 31,000 tests were administered to 1st to 12th graders on speakerphones in Arizona schools.", "labels": [], "entities": []}, {"text": "A second field test in April 2012 delivered over 13,000 AZELLA tests to kindergarten students.", "labels": [], "entities": [{"text": "AZELLA", "start_pos": 56, "end_pos": 62, "type": "METRIC", "confidence": 0.9452560544013977}]}, {"text": "This paper reports research results based on analysis of data sets from the 44,000 students tested in these two administrations.", "labels": [], "entities": []}], "datasetContent": [{"text": "All results presented in this section used the validation data sets, while the recognition and scoring models were built from completely separate material.", "labels": [], "entities": []}, {"text": "The participant-level speaking scores were designed not to consider the scores from Readby-Syllables and Read-Three-Words.", "labels": [], "entities": [{"text": "Readby-Syllables", "start_pos": 84, "end_pos": 100, "type": "DATASET", "confidence": 0.9743524789810181}]}, {"text": "For each test, the system produced holistic scores for Repeat items and for non-Repeat items.", "labels": [], "entities": []}, {"text": "For every Repeat item, the machine generated pronunciation, fluency and accuracy scores mapped into the 0 to 4 score-point range.", "labels": [], "entities": [{"text": "pronunciation", "start_pos": 45, "end_pos": 58, "type": "METRIC", "confidence": 0.8933885097503662}, {"text": "accuracy scores", "start_pos": 72, "end_pos": 87, "type": "METRIC", "confidence": 0.9744018912315369}]}, {"text": "Both human and machine holistic scores fora Repeat response are equal to: 50% \u00b7 Accuracy + 25% \u00b7 P ronunciation + 25% \u00b7 F luency.", "labels": [], "entities": [{"text": "Repeat", "start_pos": 44, "end_pos": 50, "type": "TASK", "confidence": 0.8074228763580322}, {"text": "Accuracy", "start_pos": 80, "end_pos": 88, "type": "METRIC", "confidence": 0.9979718327522278}, {"text": "F luency", "start_pos": 120, "end_pos": 128, "type": "METRIC", "confidence": 0.9626009166240692}]}, {"text": "Accuracy scores were scaled as percent_correct times four.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9969878792762756}, {"text": "percent_correct", "start_pos": 31, "end_pos": 46, "type": "METRIC", "confidence": 0.7199413776397705}]}, {"text": "Human accuracy scores were based on human transcriptions instead of ASR transcriptions.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 6, "end_pos": 14, "type": "METRIC", "confidence": 0.9985841512680054}]}, {"text": "Holistic scores for Repeat items at the participant level were the simple average of the corresponding item-level scores.", "labels": [], "entities": [{"text": "Holistic", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9442561268806458}, {"text": "Repeat items", "start_pos": 20, "end_pos": 32, "type": "TASK", "confidence": 0.8832609355449677}]}, {"text": "For every non-Repeat item, we generated one holistic score that considered pronunciation, fluency and content together.", "labels": [], "entities": []}, {"text": "The non-Repeat holistic scores at the participant level were the simple average of the corresponding item level scores after normalizing them to the same scale.", "labels": [], "entities": []}, {"text": "The final generated holistic scores for Repeats were scaled to a 0 \u2212 4 range and non-Repeat holistic scores were scaled to a 0 \u2212 10 range to satisfy an AZELLA design requirement that Repeat items count for 4 points and non-Repeats count for 10 points.", "labels": [], "entities": [{"text": "Repeats", "start_pos": 40, "end_pos": 47, "type": "TASK", "confidence": 0.8420064449310303}, {"text": "AZELLA", "start_pos": 152, "end_pos": 158, "type": "DATASET", "confidence": 0.6445748209953308}]}, {"text": "The overall participant level scores are the sum of the Repeat holistic scores and the non-Repeat holistic scores (maximum 14).", "labels": [], "entities": []}, {"text": "All machine-generated scores are continuous values.", "labels": [], "entities": []}, {"text": "In the following tables, H-H r stands for the human-human correlation and M-H r stands for the correlation between machine-generated scores and average human scores.", "labels": [], "entities": [{"text": "M-H r", "start_pos": 74, "end_pos": 79, "type": "METRIC", "confidence": 0.876565009355545}]}, {"text": "We summarize the psychometric properties of different item types that contribute to the final scores in.", "labels": [], "entities": []}, {"text": "For each item-type and each stage, the third column in presents the mean and standard deviation of the words-perresponse produced by students, showing that older students generally produce more spoken material.", "labels": [], "entities": []}, {"text": "We found that the number of words spoken is a better measure than speech signal duration to represent the amount of material produced, because young English learners often emit long silences while speaking.", "labels": [], "entities": []}, {"text": "The difference between the two measures in columns 4 and 5 is statistically significant (two-tailed, p < 0.05) for item types Naming (Stage I), Ask Qs about a Thing (Stage II), Questions on Image (Stage III), and Repeat Sentence (all Stages), in which machine scoring does not match human; and for item types Give Directions from Map (Stage III, IV), in which machine is better than a single human score.", "labels": [], "entities": [{"text": "Naming", "start_pos": 126, "end_pos": 132, "type": "TASK", "confidence": 0.8933355212211609}]}, {"text": "For almost all open-ended items, machine scoring is similar to or better than human scoring.", "labels": [], "entities": []}, {"text": "We noticed that machine scoring of one open-ended item type, Ask Qs about a Thing used in Stage II test forms, was significantly worse than human scoring, leading us to identify problems specific to the item type itself, both in the human rating rubric and in the machine grading approach.", "labels": [], "entities": []}, {"text": "Arizona is not using this item type in operational tests., 4, 5 present scatter plots of overall scores at the participant level comparing human and machine scores for test in each AZELLA stage.", "labels": [], "entities": []}, {"text": "shows the averaged human holistic score distribution for participants in the validation set for Stage V. The human holistic score distributions for participants in other AZELLA stages are similar to those in, except the means shift somewhat.", "labels": [], "entities": []}, {"text": "We identified several participants for whom the difference between human and machine scores is bigger than 4 in, 5.", "labels": [], "entities": []}, {"text": "Listening to the recordings of these tests, we concluded that the most important factor was low Signal-toNoise Ratio (SNR).", "labels": [], "entities": [{"text": "Signal-toNoise Ratio (SNR)", "start_pos": 96, "end_pos": 122, "type": "METRIC", "confidence": 0.9179053664207458}]}, {"text": "Either the background noise was very high (in 6 of 1,362 tests in the validation set), or speech volume was low (in 3 of 1,362 tests in the validation set).", "labels": [], "entities": []}, {"text": "Either condition can make recognition difficult.", "labels": [], "entities": [{"text": "recognition", "start_pos": 26, "end_pos": 37, "type": "TASK", "confidence": 0.97480708360672}]}, {"text": "With very low voice amplitude and high background noise levels, the SNR of some outlier response recordings is so low that human raters refuse to affirm that they understand the      content of the response or rate its pronunciation.", "labels": [], "entities": [{"text": "SNR", "start_pos": 68, "end_pos": 71, "type": "METRIC", "confidence": 0.5055477023124695}]}, {"text": "Since many young children in kindergarten and early elementary school speak softly, the youngest children's speech is substantially harder to recognize ().", "labels": [], "entities": []}, {"text": "This probably contributes to the lower reliabilities in Stage I and II.", "labels": [], "entities": [{"text": "reliabilities", "start_pos": 39, "end_pos": 52, "type": "METRIC", "confidence": 0.9558693170547485}]}, {"text": "When setting the total rejection rate at 6%, our unscorable test detector identifies only 7 of the 13 outlier tests.", "labels": [], "entities": [{"text": "rejection rate", "start_pos": 23, "end_pos": 37, "type": "METRIC", "confidence": 0.9476660490036011}]}, {"text": "summarizes the reliabilities of the tests in different stages.", "labels": [], "entities": []}, {"text": "At the participant level, the average inter-rater reliability coefficient across the five stages was 0.96, suggesting that the welltrained human raters agree with each other with high consistency when ratings are combined overall the material in all the responses in a whole test; the average correlation coefficient between machine-generated overall scores and average human overall scores was 0.92.", "labels": [], "entities": [{"text": "inter-rater reliability coefficient", "start_pos": 38, "end_pos": 73, "type": "METRIC", "confidence": 0.6903857588768005}]}, {"text": "This suggests that the machine grading maybe sufficiently reliable for most purposes.", "labels": [], "entities": []}, {"text": "summarizes the reliabilities of test scores in the different stages considering the nonRepeat holistic scores and Repeat holistic scores separately to check the effect of adding the Repeat items.", "labels": [], "entities": []}, {"text": "Repeat items improve the machine re-liability in Stage I significantly, but not so much for other stages.", "labels": [], "entities": []}, {"text": "This difference may relate to the difficulty in eliciting sufficient speech samples in non-Repeat items from the young EL students in Stage I. Eliciting spoken materials in Repeat items is more straightforward.", "labels": [], "entities": []}, {"text": "Consideration of suggests that using only open-ended item-types can also achieve sufficiently reliable results.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Stages, grades, and number of field tests", "labels": [], "entities": []}, {"text": " Table 2: Stage III (Grades 3-5) items.", "labels": [], "entities": []}, {"text": " Table 3: Item types used in AZELLA speaking  field tests.  Description (restriction)  Score-Points  Naming (Stage I)  0-1  Short Response (Stage I)  0-2  Open Question (Stage I)  0-2  Read-by-Syllables  0-1  Read-Three-Words  0-1 or 0-3  Repeat Sentence  0-4  Questions on Image  0-4  Similarities & Differences (III)  0-4  Give Directions from Map  0-4  Ask Qs about a Thing (II)  0-2  Ask Qs about a Statement (III)  0-4  Give Instructions  0-4  Open Questions on Topic  0-4  Detailed Response to Topic  0-4", "labels": [], "entities": [{"text": "AZELLA speaking  field tests", "start_pos": 29, "end_pos": 57, "type": "DATASET", "confidence": 0.7078479528427124}, {"text": "Repeat Sentence  0-4  Questions on Image  0-4  Similarities & Differences (III)  0-4  Give Directions from Map  0-4  Ask Qs about a Thing (II)  0-2  Ask Qs about a Statement (III)  0-4  Give Instructions  0-4  Open Questions on Topic  0-4  Detailed Response to Topic  0-4", "start_pos": 239, "end_pos": 510, "type": "TASK", "confidence": 0.7995685350894928}]}, {"text": " Table 5: Human rating reliabilities and Machine- human correlations by item type. Third column  gives mean and standard deviation of words per  response.", "labels": [], "entities": []}, {"text": " Table 6: Reliability of human scores and Human- Machine correlations of overall test scores by  stage.", "labels": [], "entities": []}, {"text": " Table 7: Test reliability by stage, separating non- Repeat holistic scores and Repeat holistic scores.", "labels": [], "entities": [{"text": "reliability", "start_pos": 15, "end_pos": 26, "type": "METRIC", "confidence": 0.9498430490493774}]}]}