{"title": [], "abstractContent": [{"text": "Writers usually need iterations of revisions and edits during their writings.", "labels": [], "entities": []}, {"text": "To better understand the process of rewriting, we need to know what has changed between the revisions.", "labels": [], "entities": []}, {"text": "Prior work mainly fo-cuses on detecting corrections within sentences , which is at the level of words or phrases.", "labels": [], "entities": [{"text": "detecting corrections within sentences", "start_pos": 30, "end_pos": 68, "type": "TASK", "confidence": 0.9112783223390579}]}, {"text": "This paper proposes to detect revision changes at the sentence level.", "labels": [], "entities": []}, {"text": "Looking at revisions at a higher level allows us to have a different understanding of the revision process.", "labels": [], "entities": []}, {"text": "This paper also proposes an approach to automatically detect sentence revision changes.", "labels": [], "entities": [{"text": "detect sentence revision changes", "start_pos": 54, "end_pos": 86, "type": "TASK", "confidence": 0.666580818593502}]}, {"text": "The proposed approach shows high accuracy in an evaluation using first and final draft essays from an undergraduate writing class.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.9991945624351501}]}], "introductionContent": [{"text": "Rewriting is considered to bean important process during writing.", "labels": [], "entities": []}, {"text": "However, conducting successful rewriting is not an easy task, especially for novice writers.", "labels": [], "entities": []}, {"text": "Instructors work hard on providing suggestions for rewriting), but usually such advice is quite general.", "labels": [], "entities": []}, {"text": "We need to understand the changes between revisions better to provide more specific and helpful advice.", "labels": [], "entities": []}, {"text": "There has already been work on detecting corrections in sentence revisions.", "labels": [], "entities": [{"text": "detecting corrections in sentence revisions", "start_pos": 31, "end_pos": 74, "type": "TASK", "confidence": 0.8573794364929199}]}, {"text": "However, these works mainly focus on detecting changes at the level of words or phrases.", "labels": [], "entities": []}, {"text": "According to Faigley's definition of revision change, these works could help the identification of Surface Changes (changes that do not add or remove information to the original text).", "labels": [], "entities": []}, {"text": "However, Text Changes (changes that add or remove information) will be more difficult to identify if we only look at revisions within sentences.", "labels": [], "entities": [{"text": "Text Changes (changes that add or remove information)", "start_pos": 9, "end_pos": 62, "type": "TASK", "confidence": 0.7659135580062866}]}, {"text": "According to, when instructors were presented a comparison of differences between papers derived from words, they felt the information regarding changes between revisions was overwhelming.", "labels": [], "entities": []}, {"text": "This paper proposes to look at the changes between revisions at the level of sentences.", "labels": [], "entities": []}, {"text": "Comparing to detecting changes at the word level, detecting changes at the sentence level contains less information, but still keeps enough information to understand the authors' intention behind their modifications to the text.", "labels": [], "entities": []}, {"text": "The sentence level edits could then be grouped and classified into different types of changes.", "labels": [], "entities": []}, {"text": "The long-term goal of this project is to allow us to be able to identify both Text Changes and Surface Changes automatically.", "labels": [], "entities": []}, {"text": "Students, teachers, and researchers could then perform analysis on the different types of changes and have a better understanding of the rewriting process.", "labels": [], "entities": []}, {"text": "As a preliminary work, this paper explores steps toward this goal: First, automatically generate the description of changes based on four primitives: Add, Delete, Modify, Keep; Second, merge the primitives that come from the same purpose.", "labels": [], "entities": []}], "datasetContent": [{"text": "Sentence alignment We use accuracy as the evaluation metric.", "labels": [], "entities": [{"text": "Sentence alignment", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.924053430557251}, {"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9992541670799255}]}, {"text": "For each pair of drafts, we count the number of sentences in the final draft N 1 . For each sentence in the final draft, we count the number of sentences that get the correct alignment as N 2 . The accuracy of the sentence alignment is N 2 N 1 . We use Hashemi's approach as the baseline.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 198, "end_pos": 206, "type": "METRIC", "confidence": 0.9994404911994934}]}, {"text": "Compare Suite colors the differences out, as shown in.", "labels": [], "entities": []}, {"text": "We treat the green sentences as Modify and aligned to the original sentence.", "labels": [], "entities": []}, {"text": "For our method, we tried four groups of settings.", "labels": [], "entities": []}, {"text": "Group 1 and group 2 perform leave-one-out cross validation on C1 and C2 (test on one pair of paper drafts and train on the others).", "labels": [], "entities": []}, {"text": "Group 3 and group 4 train on one corpus and test on the other.: Accuracy of our approach vs. baseline shows that all our methods beat the baseline . Among the three similarity metrics, TF*IDF is the most predictive.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.9955229759216309}, {"text": "TF*IDF", "start_pos": 185, "end_pos": 191, "type": "METRIC", "confidence": 0.8679336508115133}]}, {"text": "Edit sequence generation We use WER (Word Error Rate) from speech recognition for evaluating the generated sequence by comparing the generated sequence to the gold standard.", "labels": [], "entities": [{"text": "Edit sequence generation", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.6347990135351816}, {"text": "WER (Word Error Rate)", "start_pos": 32, "end_pos": 53, "type": "METRIC", "confidence": 0.7938026736179987}, {"text": "speech recognition", "start_pos": 59, "end_pos": 77, "type": "TASK", "confidence": 0.7160504460334778}]}, {"text": "WER is calculated based on edit distances between sequences.", "labels": [], "entities": [{"text": "WER", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.9686253070831299}]}, {"text": "The ratio is calculated as: W ER = S+D+I N , where S means the number of modifications, D means the number of deletes, I means the number of inserts.", "labels": [], "entities": [{"text": "ER", "start_pos": 30, "end_pos": 32, "type": "METRIC", "confidence": 0.6539747714996338}]}, {"text": "We apply our method on the gold standard of sentence alignment.", "labels": [], "entities": [{"text": "sentence alignment", "start_pos": 44, "end_pos": 62, "type": "TASK", "confidence": 0.7458969354629517}]}, {"text": "The generated edit sequence is then compared with the gold standard edit sequence to calculate WER.", "labels": [], "entities": [{"text": "WER", "start_pos": 95, "end_pos": 98, "type": "METRIC", "confidence": 0.9948847889900208}]}, {"text": "Hashemi's approach is chosen as the baseline.", "labels": [], "entities": []}, {"text": "The WER of our method is 0.035 on C1 and 0.017 on C2, comparing to 0.091 on C1 and 0.153 on C2 for the baseline, which shows that our rule-based method has promise.", "labels": [], "entities": [{"text": "WER", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.9997190833091736}]}, {"text": "Notice that we have the case that one sentence is aligned to two sentences (i.e. Consolidation, as sentence 66 in).", "labels": [], "entities": []}, {"text": "In our evaluation, an alignment is considered to be correct only if the alignment covers all the sentences that should be covered.", "labels": [], "entities": []}, {"text": "For example, if Sentence 66 in is aligned to Sentence 55 in the first draft, it is counted as an error.", "labels": [], "entities": []}, {"text": "For Groups 1 and 2, we calculate the accuracy of Hashemi's approach under a leave-one-out setting, each time remove one pair of document and calculate the accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.9995444416999817}, {"text": "accuracy", "start_pos": 155, "end_pos": 163, "type": "METRIC", "confidence": 0.9989379048347473}]}, {"text": "A significance testis also conducted, the worst metric LD in Group 1 and WO in Group 2 both beat the baseline significantly ( p1 = 0.025,p2 = 0.017) in two-tailed T-test.", "labels": [], "entities": [{"text": "WO", "start_pos": 73, "end_pos": 75, "type": "METRIC", "confidence": 0.9903466105461121}]}, {"text": "Applying our method on the predicted alignment on the first step gets 0.067 on C1 and 0.025 on C2, which although degraded still beats the baseline.", "labels": [], "entities": [{"text": "alignment", "start_pos": 37, "end_pos": 46, "type": "METRIC", "confidence": 0.9235467314720154}]}, {"text": "Edit sequence merging There are only a limited number of Consolidation and Distribution examples in our corpus.", "labels": [], "entities": [{"text": "Edit sequence merging", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.6230128010114034}]}, {"text": "Together there are 9 Consolidation and 5 Distribution operations.", "labels": [], "entities": [{"text": "Distribution", "start_pos": 41, "end_pos": 53, "type": "TASK", "confidence": 0.9707236886024475}]}, {"text": "In our current data, the number of sentences involved in these operations is always 2.", "labels": [], "entities": []}, {"text": "Our rule-based method achieved 100% accuracy in the identification of these operations.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.999582827091217}]}, {"text": "It needs further work to see if this method would perform equally well in more complicated corpora.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: Accuracy of our approach vs. baseline", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9983676075935364}]}]}