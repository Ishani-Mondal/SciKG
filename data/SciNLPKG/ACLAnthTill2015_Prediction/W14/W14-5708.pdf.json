{"title": [{"text": "Multiword noun compound bracketing using Wikipedia", "labels": [], "entities": [{"text": "Multiword noun compound bracketing", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.7824340462684631}]}], "abstractContent": [{"text": "This research suggests two contributions in relation to the multiword noun compound bracketing problem: first, demonstrate the usefulness of Wikipedia for the task, and second, present a novel bracketing method relying on a word association model.", "labels": [], "entities": [{"text": "multiword noun compound bracketing", "start_pos": 60, "end_pos": 94, "type": "TASK", "confidence": 0.6616455912590027}]}, {"text": "The intent of the association model is to represent combined evidence about the possibly lexical, relational or coordinate nature of links between all pairs of words within a compound.", "labels": [], "entities": []}, {"text": "As for Wikipedia, it is promoted for its encyclopedic nature, meaning it describes terms and named entities, as well as for its size, large enough for corpus-based statistical analysis.", "labels": [], "entities": []}, {"text": "Both types of information will be used in measuring evidence about lexical units, noun relations and noun coordinates in order to feed the association model in the bracketing algorithm.", "labels": [], "entities": []}, {"text": "Using a gold standard of around 4800 multiword noun compounds, we show performances of 73% in a strict match evaluation, comparing favourably to results reported in the literature using unsupervised approaches.", "labels": [], "entities": []}], "introductionContent": [{"text": "The noun compound bracketing task consists in determining related subgroups of nouns within a larger compound.", "labels": [], "entities": [{"text": "noun compound bracketing", "start_pos": 4, "end_pos": 28, "type": "TASK", "confidence": 0.6724454661210378}]}, {"text": "For example), (woman (aid worker)) requires a right-bracketing interpretation, contrarily to ((copper alloy) rod) requiring a left-bracketing interpretation.", "labels": [], "entities": []}, {"text": "When only three words are used, n1 n2 n3, bracketing is defined as a binary decision between grouping (n1,n2) or grouping (n2,n3).", "labels": [], "entities": []}, {"text": "Two models, described in early work by, are commonly used to inform such decision: the adjacency model and the dependency model.", "labels": [], "entities": []}, {"text": "The former compares probabilities (or more loosely, strength of association) of two alternative adjacent noun compounds, that of n1 n2 and of n2 n3.", "labels": [], "entities": []}, {"text": "The latter compares probabilities of two alternative dependencies, either between n1 and n3 or between n2 and n3.", "labels": [], "entities": []}, {"text": "Most compound bracketing research has focused on three-noun compounds as described above.", "labels": [], "entities": [{"text": "compound bracketing", "start_pos": 5, "end_pos": 24, "type": "TASK", "confidence": 0.7208186686038971}]}, {"text": "Some recent work (,) looks at larger compounds, experimenting with a dataset created by which we also use in our research.", "labels": [], "entities": []}, {"text": "For larger noun compounds, the adjacency model alone will not allow longer range dependencies to betaken into account.", "labels": [], "entities": []}, {"text": "This had been noted much earlier in Barker (1998) using examples such as (wooden (((French (onion soup)) bowl) handle)) to show a long-range dependency between wooden and handle.", "labels": [], "entities": []}, {"text": "To allow for such long-range dependencies, our bracketing algorithm looks at all possible word associations within the full expression to make its decisions.", "labels": [], "entities": []}, {"text": "The word associations are captured within an association model which goes beyond the adjacency and dependency models.", "labels": [], "entities": []}, {"text": "The association model represents combined evidence about the possibly lexical, relational or coordinate nature of the links between all word pairs.", "labels": [], "entities": []}, {"text": "In its current implementation, our association model relies on Wikipedia as a resource for obtaining all three types of evidence.", "labels": [], "entities": []}, {"text": "Wikipedia is used in two forms: first as a list of terms and named entities (Wikipedia page titles), and second, as a large corpus obtained from the merging of all its pages.", "labels": [], "entities": []}, {"text": "The resulting corpus is large enough to be used for statistical measures.", "labels": [], "entities": []}, {"text": "The most current version contains 14,466,099 pages in English for an uncompressed file size of 47 gigabytes (including some metadata).", "labels": [], "entities": []}, {"text": "To the best of our knowledge, no previous research has used Wikipedia for the noun bracketing task, and this research will explore its usefulness.", "labels": [], "entities": [{"text": "noun bracketing task", "start_pos": 78, "end_pos": 98, "type": "TASK", "confidence": 0.8338800271352133}]}, {"text": "The reminder of this article will unfold as follows.", "labels": [], "entities": []}, {"text": "Section 2 presents a brief literature review.", "labels": [], "entities": []}, {"text": "Section 3 describes the dataset used in our experiments.", "labels": [], "entities": []}, {"text": "Section 4 presents the bracketing algorithm, and Section 5 the implementation of a word association model using Wikipedia.", "labels": [], "entities": [{"text": "bracketing", "start_pos": 23, "end_pos": 33, "type": "TASK", "confidence": 0.983371913433075}, {"text": "Wikipedia", "start_pos": 112, "end_pos": 121, "type": "DATASET", "confidence": 0.9434762597084045}]}, {"text": "Section 6 describes our evaluation approach, while results are presented and analysed in Section 7.", "labels": [], "entities": []}, {"text": "Section 8 concludes and suggests future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "Vadas and Curran (2007a) manually went through the Penn Treebank ( to further annotate large NPs.", "labels": [], "entities": [{"text": "Penn Treebank (", "start_pos": 51, "end_pos": 66, "type": "DATASET", "confidence": 0.989965577920278}]}, {"text": "They openly published a diff file of the Penn Treebank to show their annotations which differ from the original.", "labels": [], "entities": [{"text": "diff file of the Penn Treebank", "start_pos": 24, "end_pos": 54, "type": "DATASET", "confidence": 0.7611919244130453}]}, {"text": "From this available file, we constructed our gold-standard dataset by extracting large NPs (three or more words) which only include relevant items (common and proper nouns, adverbs and adjectives), removing determiners, numbers, punctuations and conjunctions.", "labels": [], "entities": []}, {"text": "The expressions were then verified for completeness, so that the opening bracket should be closed within the length of text defined in the differential file.", "labels": [], "entities": []}, {"text": "Finally, tags and single words enclosing parentheses were removed to produce simplified versions of the bracketed expressions (e.g (NML (NNP Nesbitt) (NNP Thomson) (NNP Deacon) ) becomes (Nesbitt (Thomson Deacon)) ).", "labels": [], "entities": [{"text": "NML (NNP Nesbitt) (NNP Thomson) (NNP Deacon)", "start_pos": 132, "end_pos": 176, "type": "DATASET", "confidence": 0.8574969539275537}]}, {"text": "Vadas and Curran (2007a) used a Named Entity annotator to suggest bracketing to the human annotators (who could accept or reject them).", "labels": [], "entities": []}, {"text": "The entity types used were the ones defined by Weischedel and Ada (e.g. Person, Facility, Organization, Nationality, Product, Event, etc).", "labels": [], "entities": []}, {"text": "Named entities could be kept as-is by the annotators or could be bracketed if deemed compositional.", "labels": [], "entities": []}, {"text": "Annotators were also instructed to use a default right-bracketing (implicit in Penn Treebank) for difficult decision.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 79, "end_pos": 92, "type": "DATASET", "confidence": 0.9943503439426422}]}, {"text": "In our dataset, we transformed the ones left as-is into right-bracketed in order to have all expressions fully bracketed.", "labels": [], "entities": []}, {"text": "This process might seem controversial, as it assumes compositionality of all named entities, which for sure, is a wrong hypothesis.", "labels": [], "entities": []}, {"text": "The alternative, though, would require the bracketing algorithm to recognize named entities, which we consider outside the scope of this research.", "labels": [], "entities": [{"text": "bracketing", "start_pos": 43, "end_pos": 53, "type": "TASK", "confidence": 0.9598684310913086}]}, {"text": "Furthermore, it would also be wrong to assume all named entities are non-compositional.", "labels": [], "entities": []}, {"text": "For example New York Stock Exchange is clearly compositional, and a Named Entity Tagger based on Wikipedia would easily identify it as a named entity (although the use of Wikipedia as a source of named entities is also debatable).", "labels": [], "entities": [{"text": "New York Stock Exchange", "start_pos": 12, "end_pos": 35, "type": "DATASET", "confidence": 0.8295549899339676}]}, {"text": "Clearly, no solution is satisfying.", "labels": [], "entities": []}, {"text": "We opted for the approximation which provided a fully bracketed gold standard to which our results could be compared.", "labels": [], "entities": []}, {"text": "We are aware that this will have a negative impact, in some cases, on our results.", "labels": [], "entities": []}, {"text": "The extraction produced a total 6,600 examples from which we removed duplicate expressions, yielding a corpus of 4,749 unique expressions.", "labels": [], "entities": []}, {"text": "Among those unique expressions, 2,889 (60.95%) were three words long (e.g. Mary Washington College), 1,270 (26.79%) had four words (e.g. standardized achievement tests scores), 413 (8.71%) with five words (e.g. annual gross domestic product growth) and the remaining longer expressions (up to nine words) covered around 3.5% of the dataset 1 .  Three methods are used to evaluate performances: strict, lenient binary tree and lenient sub-expression.", "labels": [], "entities": []}, {"text": "The strict evaluation verifies that all bracketed groups of the gold-standard expression are exactly the same as those found in the evaluated expression, providing a score of 1 or 0.", "labels": [], "entities": []}, {"text": "The two lenient evaluations compute the ratio between the number of matching groups from a gold expression with those found in the evaluated expression.", "labels": [], "entities": []}, {"text": "In other words, lenient is the recall score based on the gold elements.", "labels": [], "entities": [{"text": "recall score", "start_pos": 31, "end_pos": 43, "type": "METRIC", "confidence": 0.9872794449329376}]}, {"text": "In lenient binary tree, each fully bracketed expression is parsed as a binary tree.", "labels": [], "entities": []}, {"text": "From that tree, each modifier/head pair becomes a basic evaluation element.", "labels": [], "entities": []}, {"text": "For example, in (A (B C)), two elements A-C and B-C are used for the evaluation process.", "labels": [], "entities": []}, {"text": "This method boosts the performance level on most expressions, but especially those composed of three words, for which a minimum 50% is always obtained.", "labels": [], "entities": []}, {"text": "In lenient sub-expression, evaluation elements are rather sub-expressions to provide a more balanced score.", "labels": [], "entities": []}, {"text": "The method extracts each bracketed group except the top-level group and removes all internal parentheses from each one.", "labels": [], "entities": []}, {"text": "Thus, from the expression (((A B) C) D), the method extracts (A B) and (A B C).", "labels": [], "entities": []}, {"text": "The two resulting sub-expressions become gold elements for comparison with those obtained from the evaluated expression.", "labels": [], "entities": []}, {"text": "shows five examples illustrating score variations using the different methods on expressions of different length.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Comparing basic association scores in Wikipedia and Google Web.", "labels": [], "entities": []}, {"text": " Table 3: Impact of corpus-based statistics (lexical, relational, coordinate association)", "labels": [], "entities": []}]}