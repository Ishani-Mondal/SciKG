{"title": [{"text": "Bilingual Markov Reordering Labels for Hierarchical SMT", "labels": [], "entities": [{"text": "Bilingual Markov Reordering Labels", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.7417943030595779}, {"text": "SMT", "start_pos": 52, "end_pos": 55, "type": "TASK", "confidence": 0.6640952229499817}]}], "abstractContent": [{"text": "Earlier work on labeling Hiero grammars with monolingual syntax reports improved performance, suggesting that such labeling may impact phrase reordering as well as lexical selection.", "labels": [], "entities": [{"text": "labeling Hiero grammars", "start_pos": 16, "end_pos": 39, "type": "TASK", "confidence": 0.80462117989858}]}, {"text": "In this paper we explore the idea of inducing bilingual labels for Hiero grammars without using any additional resources other than original Hiero itself does.", "labels": [], "entities": []}, {"text": "Our bilingual labels aim at capturing salient patterns of phrase reordering in the training parallel corpus.", "labels": [], "entities": [{"text": "phrase reordering", "start_pos": 58, "end_pos": 75, "type": "TASK", "confidence": 0.7077636569738388}]}, {"text": "These bilingual labels originate from hierarchical factorizations of the word alignments in Hiero's own training data.", "labels": [], "entities": [{"text": "Hiero's own training data", "start_pos": 92, "end_pos": 117, "type": "DATASET", "confidence": 0.6324177384376526}]}, {"text": "In this paper we take a Markovian view on synchronous top-down derivations over these factorizations which allows us to extract 0 th-and 1 st-order bilingual reordering labels.", "labels": [], "entities": []}, {"text": "Using exactly the same training data as Hiero we show that the Marko-vian interpretation of word alignment fac-torization offers major benefits over the unlabeled version.", "labels": [], "entities": [{"text": "word alignment fac-torization", "start_pos": 92, "end_pos": 121, "type": "TASK", "confidence": 0.7931465109189352}]}, {"text": "We report extensive experiments with strict and soft bilingual labeled Hiero showing improved performance up to 1 BLEU points for Chinese-English and about 0.1 BLEU points for German-English.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 114, "end_pos": 118, "type": "METRIC", "confidence": 0.9989798665046692}, {"text": "BLEU", "start_pos": 160, "end_pos": 164, "type": "METRIC", "confidence": 0.9983238577842712}]}], "introductionContent": [], "datasetContent": [{"text": "We evaluate our method on two language pairs: using German/Chinese as source and English as target.", "labels": [], "entities": []}, {"text": "In all experiments we decode with a 4-gram language model smoothed with modified Knesser-Ney discounting).", "labels": [], "entities": []}, {"text": "The data used for training the language models differs per language pair, details are given in the next paragraphs.", "labels": [], "entities": []}, {"text": "All data is lowercased as a last pre-processing step.", "labels": [], "entities": []}, {"text": "In all experiments we use our own grammar extractor for the generation of all grammars, including the baseline Hiero grammars.", "labels": [], "entities": []}, {"text": "This enables us to use the same features (as far as applicable given the grammar formalism) and assure true comparability of the grammars under comparison.", "labels": [], "entities": []}, {"text": "In our experiments we explore the influence of three dimensions of bilingual reordering labels on translation accuracy.", "labels": [], "entities": [{"text": "translation", "start_pos": 98, "end_pos": 109, "type": "TASK", "confidence": 0.9391862750053406}, {"text": "accuracy", "start_pos": 110, "end_pos": 118, "type": "METRIC", "confidence": 0.8447416424751282}]}, {"text": "These dimensions are: \u2022 label granularity : granularity of the labeling {Coarse,Fine} \u2022 label order : the type/order of the labeling {0 th , 1 st } \u2022 matching type : the type of label matching performed during decoding {Strict,Soft} Combining these dimensions gives 8 different reordering labeled systems per language pair.", "labels": [], "entities": []}, {"text": "On top of that we use two baseline systems, namely Hiero and Syntax Augmented Machine Translation (SAMT) to measure these systems against.", "labels": [], "entities": [{"text": "Hiero", "start_pos": 51, "end_pos": 56, "type": "DATASET", "confidence": 0.8195732235908508}, {"text": "Syntax Augmented Machine Translation (SAMT)", "start_pos": 61, "end_pos": 104, "type": "TASK", "confidence": 0.7015846584524427}]}, {"text": "An overview of the naming of our reordering labeled systems is given in.", "labels": [], "entities": []}, {"text": "Training and decoding details Our experiments use Joshua () with Viterbi best derivation.", "labels": [], "entities": []}, {"text": "Baseline experiments use normal decoding whereas soft labeling experiments use soft constraint decoding.", "labels": [], "entities": []}, {"text": "For training we use standard Hiero grammar extraction constraints) (phrase pairs with source spans up to 10 words; abstract rules are forbidden).", "labels": [], "entities": [{"text": "Hiero grammar extraction", "start_pos": 29, "end_pos": 53, "type": "TASK", "confidence": 0.6571818788846334}]}, {"text": "During decoding maximum span 10 on the source side is maintained.", "labels": [], "entities": []}, {"text": "Following common practice, we use relative frequency estimates for phrase probabilities, lexical probabilities and generative rule probability.", "labels": [], "entities": []}, {"text": "We train our systems using (batch-kbest) Mira as borrowed by Joshua from the Moses codebase, allowing up to 30 tuning iterations.", "labels": [], "entities": []}, {"text": "Following standard practice, we tune on BLEU, and after tuning we use the configuration with the highest scores on the dev set with actual (corpus level) BLEU evaluation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 40, "end_pos": 44, "type": "METRIC", "confidence": 0.972824215888977}, {"text": "BLEU", "start_pos": 154, "end_pos": 158, "type": "METRIC", "confidence": 0.9550551176071167}]}, {"text": "We report lowercase BLEU (), METEOR (Denkowski and Lavie, 2011) and TER () scores for the tuned test set and also for the tuned dev set, the latter mainly to observe any possible overfitting.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 20, "end_pos": 24, "type": "METRIC", "confidence": 0.9972737431526184}, {"text": "METEOR", "start_pos": 29, "end_pos": 35, "type": "METRIC", "confidence": 0.995375394821167}, {"text": "TER", "start_pos": 68, "end_pos": 71, "type": "METRIC", "confidence": 0.9976398944854736}]}, {"text": "We use Multeval version 0.5.1. 12 for computing these metrics.", "labels": [], "entities": [{"text": "Multeval version 0.5.1. 12", "start_pos": 7, "end_pos": 33, "type": "DATASET", "confidence": 0.8975900113582611}]}, {"text": "We also use MultEval's implementation of statistical significance testing between systems, which is based on multiple optimizer runs and approximate randomization.) randomly swaps outputs between systems and estimates the probability that the observed score difference arose by chance.", "labels": [], "entities": []}, {"text": "Differences that are statistically significant and correspond to improvement/worsening with respect to the baseline are marked with / at the p \u2264 .05 level and / at the p \u2264 .01 level.", "labels": [], "entities": []}, {"text": "We also report the Kendall Reordering Score (KRS), which is the reordering-only variant of the LRscore (Birch and Osborne, 2010) (without the optional interpolation with BLEU) and which is a sentence-level score.", "labels": [], "entities": [{"text": "Kendall Reordering Score (KRS)", "start_pos": 19, "end_pos": 49, "type": "METRIC", "confidence": 0.8643518288930258}, {"text": "BLEU", "start_pos": 170, "end_pos": 174, "type": "METRIC", "confidence": 0.9976031184196472}]}, {"text": "For the computation of statistical significance of this metric we use our own implementation of the sign test, as also described in.", "labels": [], "entities": []}, {"text": "In our experiments we repeated each experiment three times to counter unreliable conclusions due to optimizer variance.", "labels": [], "entities": []}, {"text": "Scores are averages over three runs of tuning plus testing.", "labels": [], "entities": []}, {"text": "Scores marked with are significantly better than the baseline, those marked with are significantly worse; according to the resampling test of Multeval).", "labels": [], "entities": [{"text": "resampling", "start_pos": 123, "end_pos": 133, "type": "METRIC", "confidence": 0.9574398398399353}, {"text": "Multeval", "start_pos": 142, "end_pos": 150, "type": "DATASET", "confidence": 0.7736935019493103}]}, {"text": "Initial experiments concerned 0 th -order reordering labels in a strict matching approach (no soft constraints).", "labels": [], "entities": []}, {"text": "The results are shown in  To make optimal usage of the 3 runs we computed equally weighted improvement/worsening counts for all possible 3 \u00d7 3 baseline output / system output pairs and use those weighted counts in the sign test.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Experiment names legend", "labels": [], "entities": []}, {"text": " Table 2: Mean results bilingual labels with strict matching. 4", "labels": [], "entities": [{"text": "Mean", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9679909944534302}]}, {"text": " Table 3: Mean results bilingual labels with soft matching. 4", "labels": [], "entities": [{"text": "Mean", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9690231680870056}]}]}