{"title": [{"text": "Leveraging Rich Linguistic Features for Cross-domain Chinese Segmentation", "labels": [], "entities": [{"text": "Cross-domain Chinese Segmentation", "start_pos": 40, "end_pos": 73, "type": "TASK", "confidence": 0.6945880055427551}]}], "abstractContent": [{"text": "This paper describes the system that we use for Chinese segmentation task in the 3rd CIPS-SIGHAN bakeoff.", "labels": [], "entities": [{"text": "Chinese segmentation task", "start_pos": 48, "end_pos": 73, "type": "TASK", "confidence": 0.7830795844395956}, {"text": "CIPS-SIGHAN bakeoff", "start_pos": 85, "end_pos": 104, "type": "DATASET", "confidence": 0.8819594383239746}]}, {"text": "We use character sequence labeling method for seg-mentation, and in order to improve seg-mentation accuracy over multi-domain, we present a CRF-based Chinese segmen-tation system integrating supervised, un-supervised and lexical features.", "labels": [], "entities": [{"text": "character sequence labeling", "start_pos": 7, "end_pos": 34, "type": "TASK", "confidence": 0.5977452496687571}, {"text": "accuracy", "start_pos": 99, "end_pos": 107, "type": "METRIC", "confidence": 0.9833797812461853}]}, {"text": "We firstly preliminarily segment the target data using CRF model trained over three types of features mentioned above, from the result of which new words are detected and absorbed into the lexicon.", "labels": [], "entities": []}, {"text": "To generalize across different domains, we then execute the second segment with the updated lexicon.", "labels": [], "entities": []}, {"text": "The OOV recognition is further promoted with refined post processing.", "labels": [], "entities": [{"text": "OOV recognition", "start_pos": 4, "end_pos": 19, "type": "TASK", "confidence": 0.7320877313613892}]}, {"text": "All the features we used share a unified feature template trained by CRF.", "labels": [], "entities": [{"text": "CRF", "start_pos": 69, "end_pos": 72, "type": "DATASET", "confidence": 0.9190230369567871}]}, {"text": "Our system achieves a competitive F score of 0.9730 for this bakeoff.", "labels": [], "entities": [{"text": "F score", "start_pos": 34, "end_pos": 41, "type": "METRIC", "confidence": 0.9933441281318665}]}], "introductionContent": [{"text": "Word is the fundamental unit in natural language understanding.", "labels": [], "entities": [{"text": "natural language understanding", "start_pos": 32, "end_pos": 62, "type": "TASK", "confidence": 0.6931613683700562}]}, {"text": "Since people do not retain the boundary information between words in practical use, Chinese Word Segmentation (CWS) is the very first step in Chinese information processing.", "labels": [], "entities": [{"text": "Chinese Word Segmentation (CWS)", "start_pos": 84, "end_pos": 115, "type": "TASK", "confidence": 0.7293029626210531}, {"text": "Chinese information processing", "start_pos": 142, "end_pos": 172, "type": "TASK", "confidence": 0.6807861924171448}]}, {"text": "A considerable amount of research has shown that using character sequence labeling is a simple but effective formulation of Chinese word segmentation task), among which the method using sequence labeling based on CRF () is widely used with attractive performance.", "labels": [], "entities": [{"text": "character sequence labeling", "start_pos": 55, "end_pos": 82, "type": "TASK", "confidence": 0.6156637867291769}, {"text": "Chinese word segmentation task", "start_pos": 124, "end_pos": 154, "type": "TASK", "confidence": 0.6496568843722343}]}, {"text": "However, most of the existing segmentation systems greatly rely on data that the model was trained over.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 30, "end_pos": 42, "type": "TASK", "confidence": 0.9700727462768555}]}, {"text": "The segmentation performance tends to would reduce significantly when the test data differs greatly from the training data in phraseology and vocabulary.", "labels": [], "entities": []}, {"text": "Exploiting corpora in multi-domain for model learning can solve the problem above directly, whereas labeling corpora manually costs a lot, so that it is unrealistic to label mass corpora.", "labels": [], "entities": []}, {"text": "So far there are two ways to improve the performance of cross-domain word segmentation system.", "labels": [], "entities": [{"text": "cross-domain word segmentation", "start_pos": 56, "end_pos": 86, "type": "TASK", "confidence": 0.6355133354663849}]}, {"text": "The first way is proposed in (, in which they put forward a unified framework that integrated supervised and unsupervised segmentation together, where they could take full advantage of unsupervised segmentation to discover new word from untagged corpora and obtain the ability of supervised segmentation to recognize the known words at the same time.", "labels": [], "entities": []}, {"text": "The segmentation system is generalized to some extent.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 4, "end_pos": 16, "type": "TASK", "confidence": 0.9767510890960693}]}, {"text": "The second way is to build a segmentation system with multi-layers.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 29, "end_pos": 41, "type": "TASK", "confidence": 0.969123363494873}]}, {"text": "The first layer is a set of distinctive word segmentation subsystems, who might has an outstanding performance on specific domain.", "labels": [], "entities": [{"text": "word segmentation subsystems", "start_pos": 40, "end_pos": 68, "type": "TASK", "confidence": 0.8050424655278524}]}, {"text": "And the second layer combines all the outputs of these subsystems, determining the most possible segmentation boundaries on test dataset.", "labels": [], "entities": []}, {"text": "used this method achieved top performance in three test domains out of the four during.", "labels": [], "entities": []}, {"text": "In this paper we follow the first method to improve the performance of cross-domain segmentation, meanwhile add some of the effective features that mentioned in method two.", "labels": [], "entities": [{"text": "cross-domain segmentation", "start_pos": 71, "end_pos": 96, "type": "TASK", "confidence": 0.7657704651355743}]}, {"text": "And the performance of handling OOV is improved by adding lexical feature and new words discovery.", "labels": [], "entities": [{"text": "words discovery", "start_pos": 82, "end_pos": 97, "type": "TASK", "confidence": 0.6947543323040009}]}, {"text": "In Section 2, we describe the features we adopted in our system.", "labels": [], "entities": []}, {"text": "Section 3 represents how we discover new words from preliminary segmentation results and how we expand the lexicon to update lexical feature before we segment test data again to improve the segmentation performance.", "labels": [], "entities": []}, {"text": "BB 2 E 4 BB 2 B 3 E 5 BB 2 B 3 ME \u2265 6 BB 2 B 3 M\u00b7 \u00b7 \u00b7 ME: Illustration of character tagging", "labels": [], "entities": [{"text": "character tagging", "start_pos": 74, "end_pos": 91, "type": "TASK", "confidence": 0.7155594527721405}]}], "datasetContent": [{"text": "In order to prove the performance of our method, we considered four kinds of feature combination demonstrated in, in which Closed means closed test, Open means open test in which we used a cross-domain lexicon -Webdict 3 . Refined represents that we added new words' process proposed in Section 3 on the basis of Open.", "labels": [], "entities": []}, {"text": "For Refined, we needed corpora to create statistical Bigram information and a lexicon for training.", "labels": [], "entities": []}, {"text": "Because of the limited scale of labeled data and we have merely sufficient simplified Chinese training data and lexicon, we didn't process both the AS and CityU of Bakeoff-2005 for Refined.", "labels": [], "entities": [{"text": "AS", "start_pos": 148, "end_pos": 150, "type": "METRIC", "confidence": 0.8884183764457703}, {"text": "CityU of Bakeoff-2005", "start_pos": 155, "end_pos": 176, "type": "DATASET", "confidence": 0.9068643649419149}]}, {"text": "All the experiments in this section were linked to post-processing mentioned in Section 2.5.", "labels": [], "entities": []}, {"text": "We tested our system on Bakeoff-2005 and Bakeoff-2010 dataset with major measure index F score.", "labels": [], "entities": [{"text": "Bakeoff-2005", "start_pos": 24, "end_pos": 36, "type": "DATASET", "confidence": 0.9524528980255127}, {"text": "Bakeoff-2010 dataset", "start_pos": 41, "end_pos": 61, "type": "DATASET", "confidence": 0.9594567120075226}, {"text": "major measure index F score", "start_pos": 67, "end_pos": 94, "type": "METRIC", "confidence": 0.8625384330749511}]}, {"text": "shows the experiment result on.", "labels": [], "entities": []}, {"text": "When computing conditional entropy feature and AV feature, corresponding test corpus and training corpus should be mixed together, wiping off of the segmentation boundaries before the feature extraction.", "labels": [], "entities": []}, {"text": "\"Best closed\" indicates the best result on closed test of and \"Best open\" stands for the best open test of official outcome.", "labels": [], "entities": []}, {"text": "Our closed test outcome fully exceeded the \"Best closed\", and open test outcome exist a slight achieves a slightly lower F scores compared with \"Best open\" only on PKU test set, which might due to the deficiency of corpora and might be im- shows the test result on Bakeoff-2010 simplified Chinese dataset.", "labels": [], "entities": [{"text": "F", "start_pos": 121, "end_pos": 122, "type": "METRIC", "confidence": 0.9988372921943665}, {"text": "PKU test set", "start_pos": 164, "end_pos": 176, "type": "DATASET", "confidence": 0.9800116022427877}, {"text": "Bakeoff-2010 simplified Chinese dataset", "start_pos": 265, "end_pos": 304, "type": "DATASET", "confidence": 0.9322161227464676}]}, {"text": "When computing conditional entropy feature and AV feature, we needed to combine all of the simplified Chinese corpus together without segmentation boundaries of Bakeoff-2010 corpora to create the statistical feature values.", "labels": [], "entities": []}, {"text": "\"Best closed\" and \"Best open\" shows the best result on official closed test and open test.", "labels": [], "entities": []}, {"text": "Our closed test result on test set A differs greatly from \"Best closed\", yet the result is closer to \"Best closed\" on other test sets.", "labels": [], "entities": []}, {"text": "The performance on Closed improves a lot comparing to the baseline.", "labels": [], "entities": []}, {"text": "In addition, our method exceeded \"Best open\" on dataset C, D in open test, while slightly poorer results than the best on dataset A and B but the differences are not significant.", "labels": [], "entities": []}, {"text": "From the Refined results of both and Table 7, we can observe that our strategy on detecting new words provide improvements overall the R OOV compared to all the Open system in general.", "labels": [], "entities": [{"text": "R", "start_pos": 135, "end_pos": 136, "type": "METRIC", "confidence": 0.9832735061645508}, {"text": "OOV", "start_pos": 137, "end_pos": 140, "type": "METRIC", "confidence": 0.68171626329422}]}, {"text": "Meanwhile, our Refined model provide more balanced F scores among all the dataset.", "labels": [], "entities": [{"text": "F scores", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.9831277132034302}]}, {"text": "It is proved on two Bakeoff datasets that our Open feature combination and Refined feature combination are effective.", "labels": [], "entities": [{"text": "Bakeoff datasets", "start_pos": 20, "end_pos": 36, "type": "DATASET", "confidence": 0.9162421822547913}]}, {"text": "On account of lacking training corpus of this Bakeoff, Open data testis required.", "labels": [], "entities": []}, {"text": "Hence we used Open and Refined feature combination in: Test result on Bakeoff-2010 dataset news, microblog and novel.", "labels": [], "entities": [{"text": "Bakeoff-2010 dataset news", "start_pos": 70, "end_pos": 95, "type": "DATASET", "confidence": 0.9774244427680969}]}, {"text": "The data we used is explained as followed: \u2022 PKU-Corpus: labeled People's Daily corpus in year 1998 and 2000.", "labels": [], "entities": [{"text": "PKU-Corpus", "start_pos": 45, "end_pos": 55, "type": "DATASET", "confidence": 0.9145852327346802}, {"text": "People's Daily corpus", "start_pos": 65, "end_pos": 86, "type": "DATASET", "confidence": 0.8744641542434692}]}, {"text": "\u2022 PKU-Raw: PKU-Corpus without segmentation boundaries.", "labels": [], "entities": []}, {"text": "\u2022 Web-Corpus: combines all the unlabeled corpora from web crawler.", "labels": [], "entities": []}, {"text": "\u2022 Sample-Corpus: randomly select 15% from Web-Corpus.", "labels": [], "entities": []}, {"text": "\u2022 Entropy-Corpus: PKU-Raw together with Web-Corpus.", "labels": [], "entities": []}, {"text": "\u2022 AV-Corpus: PKU-Raw together with Sample-Corpus.", "labels": [], "entities": [{"text": "PKU-Raw", "start_pos": 13, "end_pos": 20, "type": "DATASET", "confidence": 0.8578245639801025}]}, {"text": "Finally we used PKU-Corpus as training data, and extracted from Entropy-Corpus to extract conditional entropy feature while making use of AVCorpus to extract AV features, together with character feature and character type feature to train CRF word segmentation model.", "labels": [], "entities": [{"text": "PKU-Corpus", "start_pos": 16, "end_pos": 26, "type": "DATASET", "confidence": 0.9231580495834351}, {"text": "CRF word segmentation", "start_pos": 239, "end_pos": 260, "type": "TASK", "confidence": 0.7996732791264852}]}, {"text": "Our results on this bakeoff are showed in, which achieves a competitive F score of 0.9730.", "labels": [], "entities": [{"text": "F score", "start_pos": 72, "end_pos": 79, "type": "METRIC", "confidence": 0.9918136894702911}]}, {"text": "From this table, we can catch that Refined feature combination outperforms Open, which further confirms that the new word detection is critical for cross-domain Chinese segmentation.", "labels": [], "entities": [{"text": "word detection", "start_pos": 117, "end_pos": 131, "type": "TASK", "confidence": 0.7490483820438385}, {"text": "cross-domain Chinese segmentation", "start_pos": 148, "end_pos": 181, "type": "TASK", "confidence": 0.6837664941946665}]}], "tableCaptions": [{"text": " Table 1: Illustration of character tagging", "labels": [], "entities": [{"text": "character tagging", "start_pos": 26, "end_pos": 43, "type": "TASK", "confidence": 0.7594470679759979}]}, {"text": " Table 3: Comparion experiment on AV feature, n- gram feature and character type feature were used  for each experiment", "labels": [], "entities": []}, {"text": " Table 6: Test result on Bakeoff-2005 dataset", "labels": [], "entities": [{"text": "Bakeoff-2005 dataset", "start_pos": 25, "end_pos": 45, "type": "DATASET", "confidence": 0.9464691877365112}]}, {"text": " Table 7: Test result on Bakeoff-2010 dataset", "labels": [], "entities": [{"text": "Bakeoff-2010 dataset", "start_pos": 25, "end_pos": 45, "type": "DATASET", "confidence": 0.9563672542572021}]}, {"text": " Table 8: Results on Bakeoff-2014 dataset", "labels": [], "entities": [{"text": "Bakeoff-2014 dataset", "start_pos": 21, "end_pos": 41, "type": "DATASET", "confidence": 0.9183816909790039}]}]}