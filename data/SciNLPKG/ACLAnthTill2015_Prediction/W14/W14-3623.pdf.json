{"title": [{"text": "A Large Scale Arabic Sentiment Lexicon for Arabic Opinion Mining", "labels": [], "entities": []}], "abstractContent": [{"text": "Most opinion mining methods in English rely successfully on sentiment lexicons, such as English SentiWordnet (ESWN).", "labels": [], "entities": [{"text": "opinion mining", "start_pos": 5, "end_pos": 19, "type": "TASK", "confidence": 0.7916525900363922}]}, {"text": "While there have been efforts towards building Arabic sentiment lexicons, they suffer from many deficiencies: limited size, unclear usability plan given Ara-bic's rich morphology, or non-availability publicly.", "labels": [], "entities": []}, {"text": "In this paper, we address all of these issues and produce the first publicly available large scale Standard Arabic sentiment lexicon (Ar-SenL) using a combination of existing resources: ESWN, Arabic WordNet, and the Standard Arabic Morphological Ana-lyzer (SAMA).", "labels": [], "entities": [{"text": "Standard Arabic sentiment lexicon", "start_pos": 99, "end_pos": 132, "type": "TASK", "confidence": 0.6289809793233871}, {"text": "Ar-SenL", "start_pos": 134, "end_pos": 141, "type": "METRIC", "confidence": 0.5966411828994751}, {"text": "ESWN", "start_pos": 186, "end_pos": 190, "type": "DATASET", "confidence": 0.8814085721969604}]}, {"text": "We compare and combine two methods of constructing this lexicon with an eye on insights for Ara-bic dialects and other low resource languages.", "labels": [], "entities": []}, {"text": "We also present an extrinsic evaluation in terms of subjectivity and sentiment analysis.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 69, "end_pos": 87, "type": "TASK", "confidence": 0.9368723034858704}]}], "introductionContent": [{"text": "Opinion mining refers to the extraction of subjectivity and polarity from text (Pang and).", "labels": [], "entities": [{"text": "Opinion mining", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.8049938380718231}]}, {"text": "With the growing availability and popularity of opinion rich resources such as online review sites and personal blogs, opinion mining is capturing the interest of many researchers due to its significant role in helping people make their decisions).", "labels": [], "entities": [{"text": "opinion mining", "start_pos": 119, "end_pos": 133, "type": "TASK", "confidence": 0.857276052236557}]}, {"text": "Some opinion mining methods in English rely on the English lexicon SentiWordnet (ESWN) ( for extracting word-level sentiment polarity.", "labels": [], "entities": [{"text": "opinion mining", "start_pos": 5, "end_pos": 19, "type": "TASK", "confidence": 0.8141608238220215}, {"text": "extracting word-level sentiment polarity", "start_pos": 93, "end_pos": 133, "type": "TASK", "confidence": 0.7708722501993179}]}, {"text": "Some researchers used the stored positive or negative connotation of the words to combine them and derive the polarity of the text).", "labels": [], "entities": []}, {"text": "Recently, special interest has been given to mining opinion from Arabic texts, and as a result, there has also been interest in developing an Arabic Lexicon for word-level sentiment evaluation.", "labels": [], "entities": [{"text": "word-level sentiment evaluation", "start_pos": 161, "end_pos": 192, "type": "TASK", "confidence": 0.778522272904714}]}, {"text": "The availability of a large scale Arabic based SWN is still limited (.", "labels": [], "entities": []}, {"text": "In fact, there is no publicly available large scale Arabic sentiment lexicon similar to ESWN.", "labels": [], "entities": [{"text": "ESWN", "start_pos": 88, "end_pos": 92, "type": "DATASET", "confidence": 0.8753331303596497}]}, {"text": "Additionally there are limitations with existing Arabic lexicons including deficiency in covering the correct number and type of lemmas.", "labels": [], "entities": []}, {"text": "In this paper, we propose to address these challenges, and create a large-scale sentiment lexicon benefiting from available Arabic lexica.", "labels": [], "entities": []}, {"text": "We compare two methods with an eye towards creating such resources for other Arabic dialects and low resource languages.", "labels": [], "entities": []}, {"text": "One lexicon is created by matching Arabic WordNet (AWN)) to ESWN.", "labels": [], "entities": [{"text": "ESWN", "start_pos": 60, "end_pos": 64, "type": "DATASET", "confidence": 0.647024929523468}]}, {"text": "This path relies on the existence of a wordnet, a rather expensive resource; while the second lexicon is developed by matching lemmas in the SAMA () lexicon to ESWN directly.", "labels": [], "entities": [{"text": "ESWN", "start_pos": 160, "end_pos": 164, "type": "DATASET", "confidence": 0.5957781076431274}]}, {"text": "This path relies on the existence of a mere dictionary, still expensive but more likely available than a wordnet.", "labels": [], "entities": []}, {"text": "Finally, the combination of the two lexicons is used to create the proposed large-scale Arabic Sentiment Lexicon (ArSenL).", "labels": [], "entities": [{"text": "large-scale Arabic Sentiment Lexicon (ArSenL)", "start_pos": 76, "end_pos": 121, "type": "TASK", "confidence": 0.4943688767296927}]}, {"text": "Each lemma entry in the lexicon has three scores associated with the level of matching for each of the three sentiment labels: positive, negative, and objective.", "labels": [], "entities": []}, {"text": "The paper is organized as follows.", "labels": [], "entities": []}, {"text": "A literature review presented in section 2 is conducted on work that involved developing multilingual lexi-cal resources.", "labels": [], "entities": []}, {"text": "In section 3, the steps followed to create ArSenL are detailed.", "labels": [], "entities": [{"text": "ArSenL", "start_pos": 43, "end_pos": 49, "type": "METRIC", "confidence": 0.7411530017852783}]}, {"text": "Extrinsic evaluation of ArSenL is discussed in section 4.", "labels": [], "entities": [{"text": "ArSenL", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.9470773935317993}]}, {"text": "In section 5, we conclude our work and outline possible extensions.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conduct an extrinsic evaluation to compare the different versions of ArSenL on the task of subjectivity and sentiment analysis (SSA).", "labels": [], "entities": [{"text": "ArSenL", "start_pos": 72, "end_pos": 78, "type": "METRIC", "confidence": 0.720085859298706}, {"text": "subjectivity and sentiment analysis (SSA)", "start_pos": 94, "end_pos": 135, "type": "TASK", "confidence": 0.7371304631233215}]}, {"text": "We also compare the performance of the SIFAAT lexicon (Abdul-Mageed et al., 2011) discussed in Section 2.", "labels": [], "entities": [{"text": "SIFAAT", "start_pos": 39, "end_pos": 45, "type": "TASK", "confidence": 0.8838931918144226}]}, {"text": "Experimental Settings We perform our experiments on the same corpus used by.", "labels": [], "entities": []}, {"text": "The corpus consists of 400 documents form the Penn Arabic Treebank (part 1 version 3) that are gold segmented and lemmatized.", "labels": [], "entities": [{"text": "Penn Arabic Treebank (part 1 version", "start_pos": 46, "end_pos": 82, "type": "DATASET", "confidence": 0.9494519574301583}]}, {"text": "The sentences are tagged as objective, subjective-positive, subjective-negative and subjective-neutral.", "labels": [], "entities": []}, {"text": "We use nonlinear SVM implementation in MATLAB, with the radial basis function (RBF) kernel, to evaluate the different lexicons in the context of SSA.", "labels": [], "entities": []}, {"text": "The classification model is developed in two steps.", "labels": [], "entities": []}, {"text": "In the first step, the kernel parameters (kernel's width \u00ed \u00b5\u00ed\u00bb\u00be and regularization parameter \u00ed \u00b5\u00ed\u00b0 \u00b6) are selected, and in the second step the classification model is developed and evalu-ated based on the selected parameters.", "labels": [], "entities": [{"text": "regularization parameter \u00ed \u00b5\u00ed\u00b0 \u00b6)", "start_pos": 68, "end_pos": 101, "type": "METRIC", "confidence": 0.9152584572633108}]}, {"text": "To decide on the choice of RBF kernel parameters, we use the first 80% of the dataset to tune the kernel parameters to the values that produce the best F1-score using 5-fold cross-validation.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 152, "end_pos": 160, "type": "METRIC", "confidence": 0.9976515173912048}]}, {"text": "The resulting parameters are then used to develop and evaluate the SVM model using 5-fold cross-validation on the whole dataset.", "labels": [], "entities": []}, {"text": "Two experiments were conducted to evaluate the impact of the different lexicons on opinion mining.", "labels": [], "entities": [{"text": "opinion mining", "start_pos": 83, "end_pos": 97, "type": "TASK", "confidence": 0.8427599966526031}]}, {"text": "The first experiment considers subjectivity classification where sentences are classified as either subjective or objective.", "labels": [], "entities": [{"text": "subjectivity classification", "start_pos": 31, "end_pos": 58, "type": "TASK", "confidence": 0.7314209491014481}]}, {"text": "In this experiment, the SVM kernel parameters were tuned to maximize the F1-score for predicting subjective sentences.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 73, "end_pos": 81, "type": "METRIC", "confidence": 0.9990467429161072}, {"text": "predicting subjective sentences", "start_pos": 86, "end_pos": 117, "type": "TASK", "confidence": 0.9014711777369181}]}, {"text": "The second experiment considers sentiment classification, where only subjective sentences are classified as either positive or negative.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 32, "end_pos": 56, "type": "TASK", "confidence": 0.9706437885761261}]}, {"text": "In this experiment, the classifier's parameters are tuned to maximize the average F1-score of positive and negative labels.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 82, "end_pos": 90, "type": "METRIC", "confidence": 0.9983298182487488}]}, {"text": "We report the performance measures of the individual classes, as well as their average.", "labels": [], "entities": []}, {"text": "For baseline comparison, the majority class is chosen in each of the experiments, where all sentences are assigned to the majority class.", "labels": [], "entities": []}, {"text": "For subjective versus objective baseline classification, all sentences were classified as subjective since the majority (55.1%) of the sentences were subjective.", "labels": [], "entities": []}, {"text": "To further emphasize the importance of detecting subjectivity, we chose the F1-score for subjective as baseline.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 76, "end_pos": 84, "type": "METRIC", "confidence": 0.999057948589325}]}, {"text": "For positive versus negative baseline classification, all sentences were classified as negative since the majority (58.4%) of the dataset was annotated as negative.", "labels": [], "entities": []}, {"text": "The resulting baseline performance measures are captured in, and serve as basis for comparison with our developed models.", "labels": [], "entities": []}, {"text": "For the subjective versus objective the baseline F1-score is 71.1%, and for positive versus negative, the baseline F1-score is averaged as 36.9%.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.9149860143661499}, {"text": "F1-score", "start_pos": 115, "end_pos": 123, "type": "METRIC", "confidence": 0.9455214738845825}]}, {"text": "Features We train the SVM classifier using sentence vectors consisting of three numerical features that reflect the sentiments expressed in each sentence, namely positivity, negativity and objectivity.", "labels": [], "entities": [{"text": "SVM classifier", "start_pos": 22, "end_pos": 36, "type": "TASK", "confidence": 0.8817570209503174}]}, {"text": "The value of each feature is calculated by matching the lemmas in each sentence to each of the lexicons separately: ArSenL-AWN, ArSenL-Eng, ArSenL-Union and SIFAAT.", "labels": [], "entities": [{"text": "ArSenL-AWN", "start_pos": 116, "end_pos": 126, "type": "METRIC", "confidence": 0.8492947220802307}, {"text": "ArSenL-Eng", "start_pos": 128, "end_pos": 138, "type": "METRIC", "confidence": 0.8014808297157288}, {"text": "ArSenL-Union", "start_pos": 140, "end_pos": 152, "type": "METRIC", "confidence": 0.6398723125457764}]}, {"text": "The corresponding scores are then accumulated and normalized by the length of the sentence.", "labels": [], "entities": []}, {"text": "We remove all stop words in the process.", "labels": [], "entities": []}, {"text": "For words that occur in the lexicon multiple times, the average sentiment score is used.", "labels": [], "entities": []}, {"text": "It is worth noting that the choice of aggregation for the different scores and the choice of nonlinear SVM was concluded after a set of experiments, but not reported in the paper.", "labels": [], "entities": []}, {"text": "In this regards, we conducted a suite of experiments to evaluate the impact of using: (a) linear versus Gaussian nonlinear SVM kernels, (b) normalization based on sentence length, (c) normalization using z-score versus not, and (d) using the confidence score from the lexicons.", "labels": [], "entities": []}, {"text": "Our best results across the different configurations reflected the best results with the nonlinear Gaussian RBF kernels, with sentence length-based normalization and without confidence weighting.", "labels": [], "entities": []}, {"text": "Results Three evaluations were conducted to compare the performances of the developed sentiment lexicons.", "labels": [], "entities": []}, {"text": "The results of the experiments are shown in.", "labels": [], "entities": []}, {"text": "First, we evaluate the coverage of the different lexicons.", "labels": [], "entities": []}, {"text": "We define coverage as the percentage of lemmas (excluding stop words) covered by each lexicon.", "labels": [], "entities": [{"text": "coverage", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9710817337036133}]}, {"text": "ArSenL-AWN and SIFAAT have lower coverage than the ArSenL-Eng lexicon.", "labels": [], "entities": []}, {"text": "The union lexicon has the highest coverage.", "labels": [], "entities": [{"text": "coverage", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9670168161392212}]}, {"text": "This is normally due to the larger number of lemmas included in the English and union lexicons, as shown in.", "labels": [], "entities": []}, {"text": "In subjectivity classification, ArSenL lexicons perform better than the majority baseline and outperform SIFAAT in terms of F1-score.", "labels": [], "entities": [{"text": "subjectivity classification", "start_pos": 3, "end_pos": 30, "type": "TASK", "confidence": 0.7606795132160187}, {"text": "ArSenL", "start_pos": 32, "end_pos": 38, "type": "METRIC", "confidence": 0.9740331172943115}, {"text": "F1-score", "start_pos": 124, "end_pos": 132, "type": "METRIC", "confidence": 0.9977171421051025}]}, {"text": "Overall, the developed ArSenL-Union gives the best performance among all lexicons.", "labels": [], "entities": [{"text": "ArSenL-Union", "start_pos": 23, "end_pos": 35, "type": "METRIC", "confidence": 0.7529123425483704}]}, {"text": "The only exception of better performance for SIFAAT for subjectivity is in terms of precision, which is associated with a much lower recall resulting in an F1-score that is lower than that of ArSenL's.", "labels": [], "entities": [{"text": "precision", "start_pos": 84, "end_pos": 93, "type": "METRIC", "confidence": 0.9996495246887207}, {"text": "recall", "start_pos": 133, "end_pos": 139, "type": "METRIC", "confidence": 0.9993775486946106}, {"text": "F1-score", "start_pos": 156, "end_pos": 164, "type": "METRIC", "confidence": 0.9991335272789001}, {"text": "ArSenL", "start_pos": 192, "end_pos": 198, "type": "METRIC", "confidence": 0.8416376709938049}]}, {"text": "Similarly, sentiment classification experiment reveals that ArSenL lexicons produce results that are consistently better than SIFAAT and the majority baseline.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 11, "end_pos": 35, "type": "TASK", "confidence": 0.9575389325618744}, {"text": "ArSenL", "start_pos": 60, "end_pos": 66, "type": "METRIC", "confidence": 0.9172400832176208}, {"text": "SIFAAT", "start_pos": 126, "end_pos": 132, "type": "METRIC", "confidence": 0.6140561699867249}]}, {"text": "The ArSenL-Union lexicon outperforms all lexicons in all measures without exceptions.", "labels": [], "entities": [{"text": "ArSenL-Union", "start_pos": 4, "end_pos": 16, "type": "METRIC", "confidence": 0.7738242745399475}]}, {"text": "In summary, it can be observed that the Englishbased lexicon produces results that are superior to the AWN-based lexicon.", "labels": [], "entities": [{"text": "AWN-based lexicon", "start_pos": 103, "end_pos": 120, "type": "DATASET", "confidence": 0.9083275496959686}]}, {"text": "Combining both resources, through the union, allows further improvement in SSA performance.", "labels": [], "entities": [{"text": "SSA", "start_pos": 75, "end_pos": 78, "type": "TASK", "confidence": 0.9870167374610901}]}, {"text": "It is also worth noting that the English and union lexicons consistently outperform SIFAAT despite the fact that the latter was manually derived from the same corpus we are using for evaluation.", "labels": [], "entities": [{"text": "SIFAAT", "start_pos": 84, "end_pos": 90, "type": "DATASET", "confidence": 0.6379175782203674}]}, {"text": "We close by showing examples of ArSenL in.", "labels": [], "entities": [{"text": "ArSenL", "start_pos": 32, "end_pos": 38, "type": "METRIC", "confidence": 0.9535267949104309}]}, {"text": "The lemmas are in their Buckwalter (2004) format for easier integration in any NLP task.", "labels": [], "entities": []}, {"text": "The word NA stands for Not Applicable.", "labels": [], "entities": [{"text": "Applicable", "start_pos": 27, "end_pos": 37, "type": "METRIC", "confidence": 0.7385166883468628}]}, {"text": "In the case where AWN Offset is NA and AWN lemma is NA, this means that the entry is retrieved from ArSenL-Eng.", "labels": [], "entities": []}, {"text": "Otherwise, the entries are from ArSenL-AWN.", "labels": [], "entities": [{"text": "ArSenL-AWN", "start_pos": 32, "end_pos": 42, "type": "DATASET", "confidence": 0.8274258375167847}]}, {"text": "The additions to the lemmas such as \"_v1AR\" , \"_n1AR\", \"_1\" or \"_2\" can be dropped when data processing is performed.", "labels": [], "entities": []}, {"text": "They were kept for easier retrieval in the original sources (AWN and SAMA).", "labels": [], "entities": [{"text": "AWN", "start_pos": 61, "end_pos": 64, "type": "DATASET", "confidence": 0.9425215721130371}, {"text": "SAMA", "start_pos": 69, "end_pos": 73, "type": "DATASET", "confidence": 0.7085590362548828}]}, {"text": "We added the \"English Gloss\" field for easier understanding of the Arabic word in the table.", "labels": [], "entities": [{"text": "English", "start_pos": 14, "end_pos": 21, "type": "DATASET", "confidence": 0.7689707279205322}, {"text": "Gloss", "start_pos": 22, "end_pos": 27, "type": "METRIC", "confidence": 0.5426708459854126}]}, {"text": "Moreover, it can be seen that only positive and negative scores are reported in the lexicon since the objective score can be easily derived by subtracting the sum of positive and negative scores from 1.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. The different resources used to build ArSenL.", "labels": [], "entities": [{"text": "ArSenL", "start_pos": 48, "end_pos": 54, "type": "METRIC", "confidence": 0.4462597966194153}]}, {"text": " Table 3. Examples of entries that were mapped incorrectly from AWN to SAMA", "labels": [], "entities": [{"text": "AWN", "start_pos": 64, "end_pos": 67, "type": "DATASET", "confidence": 0.9316864013671875}, {"text": "SAMA", "start_pos": 71, "end_pos": 75, "type": "DATASET", "confidence": 0.6204290390014648}]}, {"text": " Table 5. Sizes of the created sentiment lexica.", "labels": [], "entities": []}, {"text": " Table 6. Results of extrinsic evaluation. Numbers that  are highlighted reflect the best performances obtained  by the lexicons, without considering the baseline", "labels": [], "entities": []}]}