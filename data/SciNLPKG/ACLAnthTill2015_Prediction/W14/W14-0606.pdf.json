{"title": [{"text": "Enhancing the possibilities of corpus-based investigations: Word sense disambiguation on query results of large text corpora", "labels": [], "entities": [{"text": "Word sense disambiguation", "start_pos": 60, "end_pos": 85, "type": "TASK", "confidence": 0.7228960990905762}]}], "abstractContent": [{"text": "Common large digital text corpora do not distinguish between different meanings of word forms, intense manual effort has to be done for disambiguation tasks when querying for homonyms or polysemes.", "labels": [], "entities": []}, {"text": "To improve this situation, we ran experiments with automatic word sense disambiguation methods operating directly on the output of the corpus query.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 61, "end_pos": 86, "type": "TASK", "confidence": 0.672282079855601}]}, {"text": "In this paper, we present experiments with topic models to cluster search result snippets in order to separate occurrences of homonymous or polysemous queried words by their meanings .", "labels": [], "entities": []}], "introductionContent": [{"text": "Large digital text corpora contain text documents from different sources, genres and periods of time as well as often structural and linguistic markups.", "labels": [], "entities": []}, {"text": "Nowadays, they provide novel and enhanced possibilities of exploring research questions at the basis of authentic language usage not only in the field of linguistics, but for humanities and social sciences in general.", "labels": [], "entities": []}, {"text": "But even though tools for query and analysis are getting more and more flexible and sophisticated (not least thanks to the efforts been done in infrastructure projects like CLARIN), automatically obtained data have to be reviewed manually inmost cases because of false positives.", "labels": [], "entities": [{"text": "query and analysis", "start_pos": 26, "end_pos": 44, "type": "TASK", "confidence": 0.6936457852522532}]}, {"text": "Depending on the amount of data, intense manual effort has to be done for cleaning, classification or disambiguation tasks.", "labels": [], "entities": [{"text": "classification or disambiguation", "start_pos": 84, "end_pos": 116, "type": "TASK", "confidence": 0.6525713205337524}]}, {"text": "Hence, many research questions cannot be addressed because of time constraints.", "labels": [], "entities": []}, {"text": "A project funded by the German BMBF (Bundesministerium f\u00fcr Bildung und Forschung, \"Federal Ministry of Education and Research\"), therefore, is investigating benefits and issues of using machine learning technology in order to perform these tasks automatically.", "labels": [], "entities": []}, {"text": "In this paper, we focus on the disambiguation task, which is an issue known fora longtime in the field of corpus-based lexicography), but has not been satisfactorily solved, yet, and is still highly relevant also to social scientists or historians.", "labels": [], "entities": [{"text": "disambiguation task", "start_pos": 31, "end_pos": 50, "type": "TASK", "confidence": 0.8827269375324249}]}, {"text": "In the humanities, researchers usually are not examining word forms, but terms representing relations of word forms and their meanings.", "labels": [], "entities": []}, {"text": "While the common large corpora do not distinguish between different meanings of word forms, the disambiguation task has to be carried out manually most of the times.", "labels": [], "entities": []}, {"text": "To improve this situation, we ran experiments with word sense disambiguation methods operating directly on the output of the corpus queries, i.e. search result lists containing small snippets with the occurrences of the search keyword, each in a context of about only three sentences.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 51, "end_pos": 76, "type": "TASK", "confidence": 0.689408540725708}]}, {"text": "In particular, we used topic modelling to automatically detect clusters of keyword occurrences with similar contexts, that we consider corresponding to a certain meaning of the keyword.", "labels": [], "entities": []}, {"text": "In the following, we report our findings from experiments with the German terms Leiter and zeitnah, both supposed to provide interesting insights into processes of language change.", "labels": [], "entities": []}, {"text": "Der Leiter \"chief\", \"director\" and die Leiter \"ladder\" are homonyms with possible further senses Energieleiter \"conducting medium\" and Tonleiter \"scale\" (in music), whereby der Leiter competes against borrowings like Boss or Chef.", "labels": [], "entities": []}, {"text": "Zeitnah, a polyseme meaning zeitgenssisch \"contemporary\", zeitkritisch \"critical of the times\" as well as unverzglich \"prompt\", seems to have acquired the latter meaning as anew sense not until the second half of the last century.", "labels": [], "entities": []}, {"text": "The basis of our experiments are search result lists derived from the DWDS Kernkorpus core corpus of the 20th century (for Leiter) and, in addition, from the ZEIT corpus (for zeitnah).", "labels": [], "entities": [{"text": "DWDS Kernkorpus core corpus", "start_pos": 70, "end_pos": 97, "type": "DATASET", "confidence": 0.9396464228630066}, {"text": "ZEIT corpus", "start_pos": 158, "end_pos": 169, "type": "DATASET", "confidence": 0.8596301078796387}]}, {"text": "The DWDS Kernkorpus, constructed at the Berlin-Brandenburg Academy of Sciences (BBAW), contains approximately 100 million running words, balanced chronologically (over the decades of the 20th century) and by text genre (over the genres journalism, literary texts, scientific literature and other nonfiction;).", "labels": [], "entities": [{"text": "DWDS Kernkorpus", "start_pos": 4, "end_pos": 19, "type": "DATASET", "confidence": 0.8775003850460052}]}, {"text": "The ZEIT corpus covers all the issues of the German weekly newspaper Die Zeit from 1946 to 2009, approximately 460 million running words (http://www.dwds.de/ressourcen/korpora).", "labels": [], "entities": [{"text": "ZEIT corpus", "start_pos": 4, "end_pos": 15, "type": "DATASET", "confidence": 0.7457186877727509}, {"text": "German weekly newspaper Die Zeit from 1946", "start_pos": 45, "end_pos": 87, "type": "DATASET", "confidence": 0.728094973734447}]}], "datasetContent": [{"text": "We performed experiments on two data sets that consist of short snippets retrieved by corpus queries for the words Leiter and zeitnah in the DWDS Kernkorpus www.dwds.de and the ZEIT corpus (see Section 1).", "labels": [], "entities": [{"text": "DWDS Kernkorpus www.dwds.de", "start_pos": 141, "end_pos": 168, "type": "DATASET", "confidence": 0.9577476183573405}, {"text": "ZEIT corpus", "start_pos": 177, "end_pos": 188, "type": "DATASET", "confidence": 0.912637323141098}]}, {"text": "Each snippet consists of the three sentences, whereby the second sentence contains the search keyword (the word to disambiguate) in each case.", "labels": [], "entities": []}, {"text": "The snippets belong to the different text categories covered by the mentioned corpora: journalism, literary texts, scientific literature and other nonfiction (see Section 1).", "labels": [], "entities": []}, {"text": "For each snippet, we have information to which category it belongs to.", "labels": [], "entities": []}, {"text": "This information is used only for validation, not for the topic extraction.", "labels": [], "entities": [{"text": "validation", "start_pos": 34, "end_pos": 44, "type": "TASK", "confidence": 0.9636399149894714}, {"text": "topic extraction", "start_pos": 58, "end_pos": 74, "type": "TASK", "confidence": 0.840821236371994}]}, {"text": "For each data set, 30 percent of snippets were disambiguated manually by two independent annotators, whereby doubtful cases were clarified by a third person.", "labels": [], "entities": []}, {"text": "The annotations are not used for disambiguation, but for the validation of the method.", "labels": [], "entities": []}, {"text": "For each snippet we generate bag-of-words vectors using contexts of 10, 40, 80 or all words around the word of interest.", "labels": [], "entities": []}, {"text": "Hence, for context size 10 we use the ten words before the token, the token itself and the ten following tokens, as representation of the snippet.", "labels": [], "entities": []}, {"text": "For further experiments we used the Stanford Constituent Parser ( to get only the words that syntactically depend on the words of interest.", "labels": [], "entities": [{"text": "Stanford Constituent Parser", "start_pos": 36, "end_pos": 63, "type": "DATASET", "confidence": 0.8097129066785177}]}, {"text": "For the extraction of the topics and distribution over the text categories we used the Gibbs sampler for LDA and the author topic model from the Matlab library Topictoolbox ( ).", "labels": [], "entities": []}, {"text": "Based on the annotation mentioned above we can estimate the Normalized Mutual Information (NMI) as score for the goodness of the method.", "labels": [], "entities": [{"text": "Mutual Information (NMI)", "start_pos": 71, "end_pos": 95, "type": "METRIC", "confidence": 0.858234453201294}]}, {"text": "NMI measures how many snippets that are annotated as being from different topics are placed into the same topic based on the extracted topics from LDA.", "labels": [], "entities": [{"text": "NMI", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.5956976413726807}, {"text": "LDA", "start_pos": 147, "end_pos": 150, "type": "DATASET", "confidence": 0.8781057000160217}]}, {"text": "It is defined as the fraction of the sum of the entropies of the distributions of the annotations and the disambiguation results, and the entropy of the joint distribution of annotations and results.", "labels": [], "entities": []}, {"text": "Further, we use one of the standard measures to estimate the goodness of a word sense disambiguation result, the F1 score.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 75, "end_pos": 100, "type": "TASK", "confidence": 0.5992691119511923}, {"text": "F1 score", "start_pos": 113, "end_pos": 121, "type": "METRIC", "confidence": 0.9837474524974823}]}, {"text": "The F1 score is the weighted average of the precision and recall of the disambiguation results for the given annotations.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9829896092414856}, {"text": "precision", "start_pos": 44, "end_pos": 53, "type": "METRIC", "confidence": 0.9994413256645203}, {"text": "recall", "start_pos": 58, "end_pos": 64, "type": "METRIC", "confidence": 0.9980574250221252}]}, {"text": "This and further evaluation methods are described in).", "labels": [], "entities": []}, {"text": "In the we show the NMI and F1 score for the extracted topics, respectively senses, by LDA.", "labels": [], "entities": [{"text": "NMI", "start_pos": 19, "end_pos": 22, "type": "METRIC", "confidence": 0.9554072022438049}, {"text": "F1 score", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9819997251033783}, {"text": "LDA", "start_pos": 86, "end_pos": 89, "type": "METRIC", "confidence": 0.6109226942062378}]}, {"text": "We tested different context sizes from 10 to 80 words around the word of interest.", "labels": [], "entities": []}, {"text": "Compared to the results when we use the whole snippets, we see that a context size of 40 results in the: Translation of the most frequent words for each of the extracted senses for the word zeitnah.", "labels": [], "entities": []}, {"text": "Less context decrease the performance and the filtering by constituencies give the worst results.", "labels": [], "entities": []}, {"text": "The experiments show that a windowing approach is well suited to represent documents fora word sense disambiguation task.", "labels": [], "entities": [{"text": "word sense disambiguation task", "start_pos": 90, "end_pos": 120, "type": "TASK", "confidence": 0.7252761721611023}]}, {"text": "The size of the window seems to be crucial and must be chosen a priori.", "labels": [], "entities": []}, {"text": "Optimal window size could be found by cross validation techniques using annotated snippets.", "labels": [], "entities": []}, {"text": "Next, we investigated the distribution of the topics over the text categories.", "labels": [], "entities": []}, {"text": "We used the author topic model as described above to estimate how the categories distribute over the sense.", "labels": [], "entities": []}, {"text": "show the most likely words to appear in the corresponding senses translated into English for four extracted topics.", "labels": [], "entities": []}, {"text": "In the distribution of the senses over the given categories are presented.", "labels": [], "entities": []}, {"text": "Based on the posterior distribution of the categories, we simulated the process of assigning topics to categories for each word in the snippets.", "labels": [], "entities": []}, {"text": "In the tables we present the number of times we assign sense i to category c.", "labels": [], "entities": []}, {"text": "For the word Leiter in, we see that in each category always one certain sense for the word is prominent.", "labels": [], "entities": []}, {"text": "For instance sense 2, here   Leiter appears in the context of a ladder.", "labels": [], "entities": []}, {"text": "In this context, the word is more likely to appear in a fictional text than in the other categories.", "labels": [], "entities": []}, {"text": "For zeitnah in the results are not very clear.", "labels": [], "entities": []}, {"text": "First, the word is most likely to appear in news papers rather than in literature or science articles.", "labels": [], "entities": []}, {"text": "This is due to the fact that we have much more snippets from news papers.", "labels": [], "entities": []}, {"text": "Only in sense 3, the word is also likely to appear in other categories.", "labels": [], "entities": []}, {"text": "This context seems to be German films.", "labels": [], "entities": []}, {"text": "In contrast, we see sense 2 that is about social questions appears only in news papers.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: NMI of the extracted senses with respect  to the given annotations of the text snippets.", "labels": [], "entities": [{"text": "NMI", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9668191075325012}]}, {"text": " Table 2: F1 score of the extracted senses with re- spect to the given annotations of the text snippets.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9764664173126221}]}, {"text": " Table 5: The distribution of the senses among the  text categories during the simulation for the word  Leiter.", "labels": [], "entities": []}, {"text": " Table 6: The distribution of the senses among the  text categories during the simulation for the word  zeitnah.", "labels": [], "entities": []}]}