{"title": [{"text": "Unsupervised adaptation of supervised part-of-speech taggers for closely related languages", "labels": [], "entities": [{"text": "part-of-speech taggers", "start_pos": 38, "end_pos": 60, "type": "TASK", "confidence": 0.7107808291912079}]}], "abstractContent": [{"text": "When developing NLP tools for low-resource languages, one is often confronted with the lack of annotated data.", "labels": [], "entities": []}, {"text": "We propose to circumvent this bottleneck by training a supervised HMM tagger on a closely related language for which annotated data are available, and translating the words in the tagger parameter files into the low-resource language.", "labels": [], "entities": [{"text": "HMM tagger", "start_pos": 66, "end_pos": 76, "type": "TASK", "confidence": 0.7815762162208557}]}, {"text": "The translation dictionaries are created with unsupervised lexicon induction techniques that rely only on raw textual data.", "labels": [], "entities": [{"text": "translation dictionaries", "start_pos": 4, "end_pos": 28, "type": "TASK", "confidence": 0.8986448347568512}]}, {"text": "We obtain a tagging accuracy of up to 89.08% using a Spanish tagger adapted to Catalan, which is 30.66% above the performance of an unadapted Spanish tagger, and 8.88% below the performance of a supervised tagger trained on annotated Catalan data.", "labels": [], "entities": [{"text": "tagging", "start_pos": 12, "end_pos": 19, "type": "TASK", "confidence": 0.9378655552864075}, {"text": "accuracy", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9578567743301392}]}, {"text": "Furthermore, we evaluate our model on several Romance, Germanic and Slavic languages and obtain tagging accuracies of up to 92%.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 104, "end_pos": 114, "type": "METRIC", "confidence": 0.8188117742538452}]}], "introductionContent": [{"text": "Recently, a lot of research has dealt with the task of creating part-of-speech taggers for languages which lack manually annotated training corpora.", "labels": [], "entities": [{"text": "part-of-speech taggers", "start_pos": 64, "end_pos": 86, "type": "TASK", "confidence": 0.7062433212995529}]}, {"text": "This is usually done through some type of annotation projection from a language for which a tagger or an annotated corpus exists (henceforth called RL for resourced language) towards another language that lacks such data (NRL for non-resourced language).", "labels": [], "entities": []}, {"text": "One possibility is to use word-aligned parallel corpora and transfer the tags from the RL to the NRL along alignment links.", "labels": [], "entities": []}, {"text": "Another possibility is to adapt the parameters of the RL tagger using bilingual dictionaries or manually built transformation rules.", "labels": [], "entities": [{"text": "RL tagger", "start_pos": 54, "end_pos": 63, "type": "TASK", "confidence": 0.9071545600891113}]}, {"text": "In this paper, we argue that neither parallel corpora nor hand-written resources are required if the RL and the NRL are closely related.", "labels": [], "entities": []}, {"text": "We propose a generic method for tagger adaptation that relies on three assumptions which generally hold for closely related language varieties.", "labels": [], "entities": [{"text": "tagger adaptation", "start_pos": 32, "end_pos": 49, "type": "TASK", "confidence": 0.9915265142917633}]}, {"text": "First, we assume that the two languages share a lot of cognates, i.e., word pairs that are formally similar and that are translations of each other.", "labels": [], "entities": []}, {"text": "Second, we suppose that the word order of both languages is similar.", "labels": [], "entities": []}, {"text": "Third, we assume that the set of POS tags is identical.", "labels": [], "entities": []}, {"text": "Under these assumptions, we can avoid the requirements of parallel data and of manual annotation.", "labels": [], "entities": []}, {"text": "Following, the reasoning behind our method is that a Hidden Markov Model (HMM) tagger trained in a supervised way on RL data can be adapted to the NRL by translating the RL words in its parameter files to the NRL.", "labels": [], "entities": []}, {"text": "This requires a bilingual dictionary between RL words and NRL words.", "labels": [], "entities": []}, {"text": "In this paper, we create different HMM taggers using the bilingual dictionaries obtained with the unsupervised lexicon induction methods presented in our earlier work.", "labels": [], "entities": [{"text": "HMM taggers", "start_pos": 35, "end_pos": 46, "type": "TASK", "confidence": 0.8703419268131256}]}, {"text": "The paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we present related work on tagger adaptation and lexicon induction.", "labels": [], "entities": [{"text": "tagger adaptation", "start_pos": 41, "end_pos": 58, "type": "TASK", "confidence": 0.9799737632274628}, {"text": "lexicon induction", "start_pos": 63, "end_pos": 80, "type": "TASK", "confidence": 0.7108216285705566}]}, {"text": "In Section 3, we review Hidden Markov Models and their relevance for tagging and for our method of tagger adaptation.", "labels": [], "entities": [{"text": "tagging", "start_pos": 69, "end_pos": 76, "type": "TASK", "confidence": 0.9701789617538452}, {"text": "tagger adaptation", "start_pos": 99, "end_pos": 116, "type": "TASK", "confidence": 0.9565345346927643}]}, {"text": "Section 4 presents a set of different taggers in some detail and evaluates them on Catalan, using Spanish as RL.", "labels": [], "entities": []}, {"text": "In Section 5, we demonstrate the validity of the proposed approach by performing small-scale evaluations on a number of Romance, Germanic and Slavic languages: we transfer part-of-speech tags from Spanish to Aragonese, from Czech to Slovak and Sorbian, from Standard German to Dutch and Palatine German.", "labels": [], "entities": []}, {"text": "We conclude in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "In addition to the Spanish-Catalan experiment, we have induced taggers for several closely related languages from Romance, Germanic and Slavic language families and tested them on the multilingual data set used by.", "labels": [], "entities": []}, {"text": "Although the results of these additional experiments are less reliable than the Spanish-Catalan data due to the small test corpus sizes, they allow us to generalize our findings to other languages and language families.", "labels": [], "entities": []}, {"text": "The experiments are setup as follows: \u2022 The Aragonese taggers were adapted from a Spanish tagger trained on AnCora.", "labels": [], "entities": [{"text": "Aragonese taggers", "start_pos": 44, "end_pos": 61, "type": "TASK", "confidence": 0.7119229137897491}, {"text": "AnCora", "start_pos": 108, "end_pos": 114, "type": "DATASET", "confidence": 0.9580473303794861}]}, {"text": "They are tested on a Wikipedia excerpt of 100 sentences that was manually annotated with the simplified AnCora labels of Section 4.1.", "labels": [], "entities": []}, {"text": "The Wikipedia corpora used for lexicon induction contained 5.4M words for Aragonese, and 431M words for Spanish.", "labels": [], "entities": [{"text": "lexicon induction", "start_pos": 31, "end_pos": 48, "type": "TASK", "confidence": 0.6805286556482315}]}, {"text": "\u2022 The Dutch and Palatine German taggers were adapted from a Standard German tagger trained on the TIGER treebank (900 000 tokens; 55 tags;).", "labels": [], "entities": [{"text": "TIGER treebank", "start_pos": 98, "end_pos": 112, "type": "DATASET", "confidence": 0.9224835634231567}]}, {"text": "The gold standard corpora are Wikipedia excerpts of 100 sentences each, manually annotated with TIGER labels.", "labels": [], "entities": []}, {"text": "The Wikipedia corpora used for lexicon induction contained 0.5M words for Dutch, 0.3M words for Palatine German, and 612M words for Standard German.", "labels": [], "entities": []}, {"text": "\u2022 The Upper Sorbian, Slovak and Polish taggers were adapted from a Czech Tagger trained on the Prague Dependency Treebank 2.5 (2M tokens; 57 simplified tags).", "labels": [], "entities": [{"text": "Prague Dependency Treebank 2.5", "start_pos": 95, "end_pos": 125, "type": "DATASET", "confidence": 0.9592806845903397}]}, {"text": "The gold standard corpora are Wikipedia excerpts of 30 sentences each, manually annotated with simplified PDT labels.", "labels": [], "entities": []}, {"text": "The Wikipedia corpora used for lexicon induction contained 0.9M words for Upper Sorbian, 30M words for Slovak, 206M words for Polish, and 85M words for Czech.", "labels": [], "entities": [{"text": "lexicon induction", "start_pos": 31, "end_pos": 48, "type": "TASK", "confidence": 0.6702511310577393}]}, {"text": "The tagging accuracies are reported in the left part of.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 12, "end_pos": 22, "type": "METRIC", "confidence": 0.7876023650169373}]}, {"text": "The accuracy values vary widely across languages, with baseline performances ranging from 24% to 81%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9994847774505615}]}, {"text": "This variation essentially reflects the linguistic distance between the RL and the NRL: German and Dutch seem to be particularly distant, while Czech and Slovak are particularly closely related.", "labels": [], "entities": [{"text": "RL", "start_pos": 72, "end_pos": 74, "type": "TASK", "confidence": 0.5603646636009216}]}, {"text": "In contrast, the overall tendency of the tagging models is the same for all languages: there are consistent gradual improvements from the baseline tagger to Tagger 3.", "labels": [], "entities": []}, {"text": "These findings are inline with the Catalan experiments.", "labels": [], "entities": []}, {"text": "The differences between Tagger 3 and Tagger 4 are not significant for any language, whereas the Catalan experiment showed a slight but significant improvement.", "labels": [], "entities": []}, {"text": "Finally, Taggers 3 and 4 slightly outperform the unigram tagger of) on most languages, although the difference is less marked than for Catalan.", "labels": [], "entities": []}, {"text": "The right half of shows what percentage of the emission files could be translated at each step, analogously to the figures reported for Catalan in.", "labels": [], "entities": []}, {"text": "The variation observed here mainly depends on the language proximity and on the size of the corpora used for lexicon induction.", "labels": [], "entities": [{"text": "lexicon induction", "start_pos": 109, "end_pos": 126, "type": "TASK", "confidence": 0.7257139682769775}]}, {"text": "Globally, the Germanic languages obtain the lowest accuracy scores.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.9992589354515076}]}, {"text": "This is due to a combination of factors.", "labels": [], "entities": []}, {"text": "First, as stated above, the baseline performance is already lower than in the other language families, which essentially results from a lower number of identical NRL-RL word pairs than in other language families.", "labels": [], "entities": []}, {"text": "Second, the lexicon induction corpora are much smaller than for the other language families.", "labels": [], "entities": []}, {"text": "Third, Germanic languages tend to have longer words due to compounding, so that the BI-SIM threshold is more difficult to satisfy.", "labels": [], "entities": [{"text": "BI-SIM threshold", "start_pos": 84, "end_pos": 100, "type": "METRIC", "confidence": 0.9836742877960205}]}, {"text": "This obviously reduces the potential for accuracy gains in Tagger 1, but it also hampers the training of the CSMT system at the origin of Tagger 2.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.9989988207817078}]}, {"text": "However, one should note that good tagging results can be achieved even with relatively low translation coverage, as shown by the Upper Sorbian experiment.", "labels": [], "entities": [{"text": "tagging", "start_pos": 35, "end_pos": 42, "type": "TASK", "confidence": 0.9663358926773071}]}], "tableCaptions": [{"text": " Table 1: Results of the Catalan tagging experiments. The first line reports tagging accuracies of the  different taggers. The second line shows -where applicable -how many words of the emission files  could be translated.", "labels": [], "entities": [{"text": "Catalan tagging", "start_pos": 25, "end_pos": 40, "type": "TASK", "confidence": 0.6027956455945969}]}]}