{"title": [], "abstractContent": [{"text": "Powerful tools could help users explore and maintain domain specific documentations, provided that documents have been semantically annotated.", "labels": [], "entities": []}, {"text": "For that, the annotations must be sufficiently specialized and rich, relying on some explicit semantic model, usually an ontology, that represents the semantics of the target domain.", "labels": [], "entities": []}, {"text": "In this paper, we learn to annotate biomedical scientific publications with respect to a Gene Regulation Ontology.", "labels": [], "entities": [{"text": "Gene Regulation Ontology", "start_pos": 89, "end_pos": 113, "type": "TASK", "confidence": 0.7583019336064657}]}, {"text": "We devise a two-step approach to annotate semantic events and relations.", "labels": [], "entities": []}, {"text": "The first step is recast as a text segmentation and labeling problem and solved using machine translation tools and a CRF, the second as multi-class classification.", "labels": [], "entities": [{"text": "text segmentation and labeling", "start_pos": 30, "end_pos": 60, "type": "TASK", "confidence": 0.8034854158759117}, {"text": "machine translation", "start_pos": 86, "end_pos": 105, "type": "TASK", "confidence": 0.7163769006729126}, {"text": "multi-class classification", "start_pos": 137, "end_pos": 163, "type": "TASK", "confidence": 0.6571791470050812}]}, {"text": "We evaluate the approach on the BioNLP-GRO benchmark, achieving an average 61% F-measure on the event detection by itself and 50% F-measure on biological relation annotation.", "labels": [], "entities": [{"text": "BioNLP-GRO benchmark", "start_pos": 32, "end_pos": 52, "type": "DATASET", "confidence": 0.7964671552181244}, {"text": "F-measure", "start_pos": 79, "end_pos": 88, "type": "METRIC", "confidence": 0.999197781085968}, {"text": "F-measure", "start_pos": 130, "end_pos": 139, "type": "METRIC", "confidence": 0.9982869029045105}]}, {"text": "This suggests that human annotators can be supported in domain specific semantic annotation tasks.", "labels": [], "entities": []}, {"text": "Under different experimental settings, we also conclude some interesting observations: (1) For event detection and compared to classical time-consuming sequence labeling approach, the newly proposed machine translation based method performed equally well but with much less computation resource required.", "labels": [], "entities": [{"text": "event detection", "start_pos": 95, "end_pos": 110, "type": "TASK", "confidence": 0.8745265901088715}]}, {"text": "(2) A highly domain specific part of the task, namely proteins and transcription factors detection, is best performed by domain aware tools, which can be used separately as an initial step of the pipeline.", "labels": [], "entities": [{"text": "transcription factors detection", "start_pos": 67, "end_pos": 98, "type": "TASK", "confidence": 0.70307989915212}]}], "introductionContent": [{"text": "As is mostly the case with technical documents, biomedical documents, a critical resource for many applications, are usually rich with domain knowledge.", "labels": [], "entities": []}, {"text": "Efforts in formalizing biomedical information have resulted in many interesting biomedical ontologies, such as Gene Ontology and SNOMED CT.", "labels": [], "entities": [{"text": "SNOMED CT", "start_pos": 129, "end_pos": 138, "type": "TASK", "confidence": 0.5198046118021011}]}, {"text": "Ontology-based semantic annotation for biomedical documents is necessary to grasp important semantic information, to enhance interoperability among systems, and to allow for semantic search instead of plain text search).", "labels": [], "entities": [{"text": "Ontology-based semantic annotation", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.6419725716114044}]}, {"text": "Furthermore, it provides a platform for consistency checking, decisions support, etc.", "labels": [], "entities": [{"text": "consistency checking", "start_pos": 40, "end_pos": 60, "type": "TASK", "confidence": 0.7054598480463028}]}, {"text": "Ideal annotation should be accurate, thus requiring intensive knowledge and context awareness, and it should be automatic at the same time, since expert work is time consuming.", "labels": [], "entities": [{"text": "Ideal annotation", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8581898510456085}]}, {"text": "Many efforts have been made in this field, from named entity recognition (NER) to information extraction (, both in open domain ( and particular domains).", "labels": [], "entities": [{"text": "named entity recognition (NER)", "start_pos": 48, "end_pos": 78, "type": "TASK", "confidence": 0.7712627748648325}, {"text": "information extraction", "start_pos": 82, "end_pos": 104, "type": "TASK", "confidence": 0.8940585553646088}]}, {"text": "Most cases of NER or information extraction focus on a small set of categories to be annotated, such as Person, Location, Organization, Misc, etc.", "labels": [], "entities": [{"text": "NER or information extraction", "start_pos": 14, "end_pos": 43, "type": "TASK", "confidence": 0.6477288827300072}]}, {"text": "Such a scenario often requires a special vocabulary, and generally benefits much from a limited set of linguistic templates for names or verbs.", "labels": [], "entities": []}, {"text": "These restrictions can be widened by linguistic efforts in recognizing relevant forms, but they are the condition of accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 117, "end_pos": 125, "type": "METRIC", "confidence": 0.9936462640762329}]}, {"text": "With the increasing importance of ontologies in general or in specific domains 1 , annotating a text regarding to a rich ontology has become necessary.", "labels": [], "entities": []}, {"text": "For example, the BioNLP ST'11 GENIA challenge This work is licensed under a Creative Commons Attribution 4.0 International Licence.", "labels": [], "entities": [{"text": "BioNLP ST'11 GENIA challenge", "start_pos": 17, "end_pos": 45, "type": "DATASET", "confidence": 0.6098264530301094}]}, {"text": "Page numbers and proceedings footer are added by the organisers.", "labels": [], "entities": []}, {"text": "Licence details: http://creativecommons.org/licenses/by/4.0/task involved merely 10 concepts and 6 relations, but BioNLP ST'13 GRO task concerns more than 200 concepts and 10 relations.", "labels": [], "entities": [{"text": "BioNLP ST'13 GRO task", "start_pos": 114, "end_pos": 135, "type": "DATASET", "confidence": 0.7063882052898407}]}, {"text": "Some ontology-based annotating systems exist and include SemTag (,), Wiki Machine (.", "labels": [], "entities": []}, {"text": "However, each of them is devoted to a particular ontology, for instance, Stanford TAP entity catalog) for SemTag and DBpedia Lexicalization Dataset 2 for DBpediaSpotlight.", "labels": [], "entities": [{"text": "Stanford TAP entity catalog", "start_pos": 73, "end_pos": 100, "type": "DATASET", "confidence": 0.8411666452884674}, {"text": "DBpedia Lexicalization Dataset 2", "start_pos": 117, "end_pos": 149, "type": "DATASET", "confidence": 0.8391813933849335}]}, {"text": "Hence, these existing systems cannot be directly used to reliably annotate biomedical domain, which is the case of the present work.", "labels": [], "entities": []}, {"text": "To this end, the challenge that we focus on is semantic annotation of texts in a particular technical domain with regards to a rather large ontology (a large set of categories), which comes with its technical language and involves uses of concepts or relations that are not named entities.", "labels": [], "entities": []}, {"text": "In this kind of use cases, one can get some manual expert annotations, but generally not in large quantity.", "labels": [], "entities": []}, {"text": "And one has to learn from them in order to annotate more.", "labels": [], "entities": []}, {"text": "This paper experiments on a set of biological texts provided for the BioNLP GRO task 3 . Since our approach is solely data-driven, it can be directly applied to obtain helpful annotation on legal texts governing a particular activity, formalization of specifications and requirement engineering, conformance of permanent services to their defining contracts, etc.", "labels": [], "entities": [{"text": "BioNLP GRO task 3", "start_pos": 69, "end_pos": 86, "type": "DATASET", "confidence": 0.7972266227006912}]}, {"text": "The task at hand is described in section 2, together with the main features of the GRO ontology used in the experiments.", "labels": [], "entities": [{"text": "GRO", "start_pos": 83, "end_pos": 86, "type": "DATASET", "confidence": 0.7793225049972534}]}, {"text": "We consider here a classical pipeline architecture.", "labels": [], "entities": []}, {"text": "The subtasks are recast as machine translation and sequence labeling problems, and standard tools are used to solve them.", "labels": [], "entities": [{"text": "machine translation and sequence labeling", "start_pos": 27, "end_pos": 68, "type": "TASK", "confidence": 0.7262311279773712}]}, {"text": "The first layer is based on domain lexicons and is not our work.", "labels": [], "entities": []}, {"text": "Our tools are applied to the detection of relations and events 4 . Section 3 presents experiments, results and comparisons on the annotation of event terms.", "labels": [], "entities": []}, {"text": "Section 4 presents experiments in detecting relations and completing event terms with their arguments.", "labels": [], "entities": []}], "datasetContent": [{"text": "The reference result has much more 'No' than 'Yes', and labeling randomly while respecting the proportion would give a good score for the No. So in the evaluation the numbers of true positives, false positives and false negatives only account for 'Yes' answers.", "labels": [], "entities": []}, {"text": "The criterion is an exact match (label and position) at each end of the link.", "labels": [], "entities": []}, {"text": "gives the results for the relations appearing in our test set.", "labels": [], "entities": []}, {"text": "The number of occurrences of each relation in the reference is pointed out.", "labels": [], "entities": []}, {"text": "Except for the sparse 'hasFunction', the precision is at least 57% and higher for relations which have the greatest number of occurrences.", "labels": [], "entities": [{"text": "precision", "start_pos": 41, "end_pos": 50, "type": "METRIC", "confidence": 0.9996724128723145}]}, {"text": "For recall, however, only 'fromSpecies' relation has an important recall.The mean precision is 80% and the mean recall is 37%, which yields a F-measure of 50%.", "labels": [], "entities": [{"text": "recall", "start_pos": 4, "end_pos": 10, "type": "METRIC", "confidence": 0.9828613996505737}, {"text": "recall.The mean precision", "start_pos": 66, "end_pos": 91, "type": "METRIC", "confidence": 0.8245754440625509}, {"text": "recall", "start_pos": 112, "end_pos": 118, "type": "METRIC", "confidence": 0.9294037818908691}, {"text": "F-measure", "start_pos": 142, "end_pos": 151, "type": "METRIC", "confidence": 0.999238133430481}]}, {"text": "The annotation of events presents seemingly more difficulties than relations: the precision is at best 60% fora much higher number of occurrences.", "labels": [], "entities": [{"text": "precision", "start_pos": 82, "end_pos": 91, "type": "METRIC", "confidence": 0.9995906949043274}]}, {"text": "The recall has the same order of magnitude for the agent role, and is better for the patient role which has twice more occurrences.", "labels": [], "entities": [{"text": "recall", "start_pos": 4, "end_pos": 10, "type": "METRIC", "confidence": 0.999629020690918}]}, {"text": "The mean precision is 58%, and the mean recall is 36%.", "labels": [], "entities": [{"text": "precision", "start_pos": 9, "end_pos": 18, "type": "METRIC", "confidence": 0.8893321752548218}, {"text": "recall", "start_pos": 40, "end_pos": 46, "type": "METRIC", "confidence": 0.9735659956932068}]}, {"text": "In the pipeline evaluation presented in the next section, errors due to event recognition will accumulate with errors proper to relation annotation.", "labels": [], "entities": [{"text": "event recognition", "start_pos": 72, "end_pos": 89, "type": "TASK", "confidence": 0.6960982382297516}]}, {"text": "The pipeline evaluation compares the relations and events obtained at the end of the pipeline to the reference.", "labels": [], "entities": []}, {"text": "We have implemented the algorithm defined in the task description, and applied it to one unused half of the development data.", "labels": [], "entities": []}, {"text": "In this evaluation, the data consist in 175 documents for training (of which 25 are reserved for Moses for tuning) and 25 for testing.: Pipeline precision, recall and F-measure using strict matching for the NER4SA and SMT4SA approaches for event detection, and for their combination.", "labels": [], "entities": [{"text": "precision", "start_pos": 145, "end_pos": 154, "type": "METRIC", "confidence": 0.9589128494262695}, {"text": "recall", "start_pos": 156, "end_pos": 162, "type": "METRIC", "confidence": 0.9996153116226196}, {"text": "F-measure", "start_pos": 167, "end_pos": 176, "type": "METRIC", "confidence": 0.998907208442688}, {"text": "NER4SA", "start_pos": 207, "end_pos": 213, "type": "DATASET", "confidence": 0.9237484931945801}, {"text": "event detection", "start_pos": 240, "end_pos": 255, "type": "TASK", "confidence": 0.7322793304920197}]}], "tableCaptions": [{"text": " Table 1: Event detection as NER results. TP is for true positive, FP for false positive, and FN for false  negative.", "labels": [], "entities": [{"text": "Event detection", "start_pos": 10, "end_pos": 25, "type": "TASK", "confidence": 0.7870160341262817}, {"text": "NER", "start_pos": 29, "end_pos": 32, "type": "TASK", "confidence": 0.8642050623893738}, {"text": "TP", "start_pos": 42, "end_pos": 44, "type": "METRIC", "confidence": 0.9974714517593384}, {"text": "FP", "start_pos": 67, "end_pos": 69, "type": "METRIC", "confidence": 0.9861053824424744}, {"text": "FN", "start_pos": 94, "end_pos": 96, "type": "METRIC", "confidence": 0.9955604076385498}]}, {"text": " Table 3: The results of experiments on event detection as phrase-based SMT.", "labels": [], "entities": [{"text": "event detection", "start_pos": 40, "end_pos": 55, "type": "TASK", "confidence": 0.7269994914531708}, {"text": "SMT", "start_pos": 72, "end_pos": 75, "type": "TASK", "confidence": 0.6961774230003357}]}, {"text": " Table 4: Detection of relations", "labels": [], "entities": [{"text": "Detection of relations", "start_pos": 10, "end_pos": 32, "type": "TASK", "confidence": 0.8916589220364889}]}, {"text": " Table 5: Detecting arguments of events", "labels": [], "entities": [{"text": "Detecting arguments of events", "start_pos": 10, "end_pos": 39, "type": "TASK", "confidence": 0.9026011377573013}]}, {"text": " Table 6: Pipeline precision, recall and F-measure using strict matching for the NER4SA and SMT4SA  approaches for event detection, and for their combination.", "labels": [], "entities": [{"text": "precision", "start_pos": 19, "end_pos": 28, "type": "METRIC", "confidence": 0.961600124835968}, {"text": "recall", "start_pos": 30, "end_pos": 36, "type": "METRIC", "confidence": 0.9996293783187866}, {"text": "F-measure", "start_pos": 41, "end_pos": 50, "type": "METRIC", "confidence": 0.9991863369941711}, {"text": "NER4SA", "start_pos": 81, "end_pos": 87, "type": "DATASET", "confidence": 0.9431253671646118}, {"text": "event detection", "start_pos": 115, "end_pos": 130, "type": "TASK", "confidence": 0.7457405626773834}]}]}