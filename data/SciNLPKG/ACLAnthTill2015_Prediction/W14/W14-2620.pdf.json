{"title": [{"text": "Evaluating Sentiment Analysis Evaluation: A Case Study in Securities Trading", "labels": [], "entities": [{"text": "Evaluating Sentiment Analysis Evaluation", "start_pos": 0, "end_pos": 40, "type": "TASK", "confidence": 0.8333955258131027}, {"text": "Securities Trading", "start_pos": 58, "end_pos": 76, "type": "TASK", "confidence": 0.669097438454628}]}], "abstractContent": [{"text": "There are numerous studies suggesting that published news stories have an important effect on the direction of the stock market, its volatility, the volume of trades, and the value of individual stocks mentioned in the news.", "labels": [], "entities": []}, {"text": "There is even some published research suggesting that automated sentiment analysis of news documents , quarterly reports, blogs and/or Twitter data can be productively used as part of a trading strategy.", "labels": [], "entities": [{"text": "sentiment analysis of news documents", "start_pos": 64, "end_pos": 100, "type": "TASK", "confidence": 0.8940688967704773}]}, {"text": "This paper presents just such a family of trading strategies, and then uses this application to reexamine some of the tacit assumptions behind how sentiment analyzers are generally evaluated, in spite of the contexts of their application.", "labels": [], "entities": [{"text": "sentiment analyzers", "start_pos": 147, "end_pos": 166, "type": "TASK", "confidence": 0.8460899889469147}]}, {"text": "This discrepancy comes at a cost.", "labels": [], "entities": []}], "introductionContent": [{"text": "Amidst the vast amount of user-generated and professionally-produced textual data, analysts from different fields are turning to the natural language processing community to sift through these large corpora and make sense of them.", "labels": [], "entities": []}, {"text": "International collaborative projects such as the Digging into Data Challenge (2012) or the Big Data Conference sponsored by the Marketing Science Institute (2012) are some recent examples of these initiatives.", "labels": [], "entities": [{"text": "Digging into Data Challenge (2012)", "start_pos": 49, "end_pos": 83, "type": "TASK", "confidence": 0.8973727822303772}]}, {"text": "The proliferation of opinion-rich text on the World Wide Web, which includes anything from product reviews to political blog posts, led to the growth of sentiment analysis as a research field more than a decade ago.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 153, "end_pos": 171, "type": "TASK", "confidence": 0.94435915350914}]}, {"text": "The market need to quantify opinions expressed in social media and the blogosphere has provided a great opportunity for sentiment analysis technology to make an impact in many sectors, including the financial industry, in which interest in automatically detecting news sentiment in order to inform trading strategies extends back at least 10 years.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 120, "end_pos": 138, "type": "TASK", "confidence": 0.7953004240989685}]}, {"text": "In this case, sentiment takes on a slightly different meaning; positive sentiment is not the emotional and subjective use of laudatory language.", "labels": [], "entities": []}, {"text": "Rather, a news article that contains positive sentiment is optimistic about the future financial prospects of a company.", "labels": [], "entities": []}, {"text": "have shown that news sentiment can effectively inform simple market neutral trading algorithms, producing a maximum yearly return of around 30%, and even more when using sentiment from blogs and Twitter data.", "labels": [], "entities": []}, {"text": "They did so, however, without an appropriate baseline, making it very difficult to appreciate the significance of this number.", "labels": [], "entities": []}, {"text": "Using a very standard sentiment analyzer, we are able to garner annualized returns over twice that percentage (70.1%), and in a manner that highlights some of the better design decisions that made, viz., their decision to use raw SVM scores rather than discrete positive or negative sentiment classes, and their decision to go long (resp., short) in then best-(worst-) ranking securities rather than to treat all positive (negative) securities equally.", "labels": [], "entities": []}, {"text": "We trade based upon the raw SVM score itself, rather than its relative rank within a basket of other securities, and tune a threshold for that score that determines whether to go long, neutral or short.", "labels": [], "entities": []}, {"text": "We sample our stocks for both training and evaluation with and without survivor bias, the tendency for long positions in stocks that are publicly traded as of the date of the experiment to pay better using historical trading data than long positions in random stocks sampled on the trading days themselves.", "labels": [], "entities": [{"text": "survivor", "start_pos": 71, "end_pos": 79, "type": "METRIC", "confidence": 0.9867951273918152}]}, {"text": "Most of the evaluations of sentiment-based trading either unwittingly adopt this bias, or do not need to address it because their returns are computed over historical periods so brief.", "labels": [], "entities": []}, {"text": "We also provide appropriate trading baselines as well as Sharpe ratios to attempt to quan-tify the relative risk inherent to our experimental strategies.", "labels": [], "entities": [{"text": "Sharpe ratios", "start_pos": 57, "end_pos": 70, "type": "METRIC", "confidence": 0.9792673587799072}]}, {"text": "As tacitly assumed by most of the work on this subject, our trading strategy is not portfolio-limited, and our returns are calculated on a percentage basis with theoretical, commissionfree trades.", "labels": [], "entities": []}, {"text": "Our motivation for undertaking this study has been to reappraise the evaluation standards for sentiment analyzers.", "labels": [], "entities": [{"text": "sentiment analyzers", "start_pos": 94, "end_pos": 113, "type": "TASK", "confidence": 0.932131290435791}]}, {"text": "It is not at all uncommon within the sentiment analysis community to evaluate a sentiment analyzer with a variety of classification accuracy or hypothesis testing scores such as F-measures, kappas or Krippendorff alphas derived from human-subject annotations, even when more extensional measures are available.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 37, "end_pos": 55, "type": "TASK", "confidence": 0.9321710169315338}, {"text": "accuracy", "start_pos": 132, "end_pos": 140, "type": "METRIC", "confidence": 0.5292853713035583}, {"text": "F-measures", "start_pos": 178, "end_pos": 188, "type": "METRIC", "confidence": 0.9837215542793274}]}, {"text": "In securities trading, this would of course include actual market returns from historical data.", "labels": [], "entities": [{"text": "securities trading", "start_pos": 3, "end_pos": 21, "type": "TASK", "confidence": 0.8323297202587128}]}, {"text": "With Hollywood films, another popular domain for automatic sentiment analysis, one might refer to box-office returns or the number of award nominations that a film receives rather than to its star-rankings on review websites where pile-on and confirmation biases are widely known to be rampant.", "labels": [], "entities": [{"text": "automatic sentiment analysis", "start_pos": 49, "end_pos": 77, "type": "TASK", "confidence": 0.6826028525829315}]}, {"text": "Are the opinions of human judges, paid or unpaid, a sufficient proxy for the business cases that actually drive the demand for sentiment analyzers?", "labels": [], "entities": [{"text": "sentiment analyzers", "start_pos": 127, "end_pos": 146, "type": "TASK", "confidence": 0.9028778672218323}]}, {"text": "We regret to report that they are not.", "labels": [], "entities": []}, {"text": "We have even found a particular modification to our standard financial sentiment analyzer that, when evaluated against an evaluation test set sampled from the same pool of human-subject annotations as the analyzer's training data, returns significantly poorer performance, but when evaluated against actual market returns, yields significantly better performance.", "labels": [], "entities": []}, {"text": "This should worry researchers who rely on classification accuracies and hypothesis tests relative to human-subject data, because the improvements that they report, whether based on better feature selection or different pattern recognition algorithms, may in fact not be improvements at all.", "labels": [], "entities": []}, {"text": "The good news, however, is that, based upon our experience within this particular domain, training on human-subject annotations and then tuning on more extensional data, in cases where the latter are less abundant, seems to suffice for bringing the evaluation back to reality.", "labels": [], "entities": []}, {"text": "A likely machinelearning explanation for this is that whenever two unbiased estimators are pitted against each other, they often result in an improved combined performance because each acts as a regularizer against the other.", "labels": [], "entities": []}, {"text": "If true, this merely attests to the relative independence of task-based and human-annotated knowledge sources.", "labels": [], "entities": []}, {"text": "A more HCI-oriented view would argue that direct human-subject annotations are highly problematic unless the annotations have been elicited in manner that is ecologically valid.", "labels": [], "entities": []}, {"text": "When human subjects are paid to annotate quarterly reports or business news, they are paid regardless of the quality of their annotations, the quality of their training, or even their degree of comprehension of what they are supposed to be doing.", "labels": [], "entities": []}, {"text": "When human subjects post film reviews on web-sites, they are participating in a cultural activity in which the quality of the film under consideration is only one factor.", "labels": [], "entities": []}, {"text": "These sources of annotation have not been properly controlled.", "labels": [], "entities": []}], "datasetContent": [{"text": "For each selected document, we first filter out all punctuation characters and the most common 429 stop words.", "labels": [], "entities": []}, {"text": "Our sentiment analyzer is a supportvector machine with a linear kernel function implemented using SVM light).", "labels": [], "entities": [{"text": "sentiment analyzer", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.8741570711135864}]}, {"text": "We have experimented with raw term frequencies, binary term-presence features, and term frequencies weighted by the BM25 scheme, which had the most resilience in the study of informationretrieval weighting schemes for sentiment analysis by.", "labels": [], "entities": [{"text": "BM25", "start_pos": 116, "end_pos": 120, "type": "DATASET", "confidence": 0.7421427965164185}, {"text": "sentiment analysis", "start_pos": 218, "end_pos": 236, "type": "TASK", "confidence": 0.9175722301006317}]}, {"text": "We performed 10 fold cross-validation on the training data, constructing our folds so that each contains an approximately equal number of negative and positive examples.", "labels": [], "entities": []}, {"text": "This ensures that we do not accidentally bias a fold.", "labels": [], "entities": []}, {"text": "use word presence features with no stop list, instead excluding all words with frequencies of 3 or less.", "labels": [], "entities": []}, {"text": "normalize their word presence feature vectors, rather than term weighting with an IR-based scheme like BM25, which also involves a normalization step.", "labels": [], "entities": [{"text": "BM25", "start_pos": 103, "end_pos": 107, "type": "DATASET", "confidence": 0.8040364384651184}]}, {"text": "also use an SVM with a linear kernel on their features, but they train and compute sentiment values on film reviews rather than financial texts, and their human judges also classified the training films on a scale from 1 to 5, whereas ours used a scale that can be viewed as being from -1 to 1, with specific qualitative interpretations assigned to each number.", "labels": [], "entities": []}, {"text": "use SVMs with a polynomial kernel (of unstated degree) to train on word frequencies relative to a three-valued classification, but they only count frequencies for the 1000 words with the highest mutual information scores relative to the classification labels.", "labels": [], "entities": []}, {"text": "also use an SVM trained upon a very different set of features, and with a polynomial kernel of degree 3.", "labels": [], "entities": []}, {"text": "As a sanity check, we measured the accuracy of our sentiment analyzer on film reviews by training and evaluating on Pang and Lee's () film reviews dataset, which contains 1000 positively and 1000 negatively labelled reviews.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9994186162948608}, {"text": "Pang and Lee's () film reviews dataset", "start_pos": 116, "end_pos": 154, "type": "DATASET", "confidence": 0.7816081196069717}]}, {"text": "Pang and Lee conveniently labelled the folds that they used when they ran their experiments.", "labels": [], "entities": []}, {"text": "Using these same folds, we obtain an average accuracy of 86.85%, which is comparable to Pang and Lee's 86.4% score for subjectivity extraction.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.9985650181770325}, {"text": "subjectivity extraction", "start_pos": 119, "end_pos": 142, "type": "TASK", "confidence": 0.8394540548324585}]}, {"text": "shows the performance of SVM with BM25 weighting on our Reuters evaluation set versus several baselines.", "labels": [], "entities": [{"text": "SVM", "start_pos": 25, "end_pos": 28, "type": "TASK", "confidence": 0.8098514080047607}, {"text": "BM25", "start_pos": 34, "end_pos": 38, "type": "METRIC", "confidence": 0.9664091467857361}, {"text": "Reuters evaluation set", "start_pos": 56, "end_pos": 78, "type": "DATASET", "confidence": 0.9694045384724935}]}, {"text": "All baselines are identical except for the term weighting schemes used, and whether stop words were removed.", "labels": [], "entities": []}, {"text": "As can be observed, SVM-BM25 has the highest sentiment classification accuracy: 80.164% on average over the 10 folds.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 45, "end_pos": 69, "type": "TASK", "confidence": 0.8613431453704834}, {"text": "accuracy", "start_pos": 70, "end_pos": 78, "type": "METRIC", "confidence": 0.935771107673645}]}, {"text": "This compares favourably with previous reports of 70.3% average accuracy over 10 folds on financial news documents ().", "labels": [], "entities": [{"text": "accuracy", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.9522486925125122}]}, {"text": "We will nevertheless adhere to normalized term presence for now, in order to stay close to Pang and Lee's () implementation.", "labels": [], "entities": [{"text": "Pang and Lee", "start_pos": 91, "end_pos": 103, "type": "DATASET", "confidence": 0.9011367758115133}]}, {"text": "In our second evaluation protocol, we evaluate the accuracy of the sentiment analyzer by embedding the analyzer inside a simple trading strategy, and then trading with it.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.998927652835846}, {"text": "sentiment analyzer", "start_pos": 67, "end_pos": 85, "type": "TASK", "confidence": 0.8173925578594208}]}, {"text": "Our trading strategy is simple: going long when the classifier reports positive sentiment in a news article about a company, and short when the classifier reports negative sentiment.", "labels": [], "entities": []}, {"text": "In section 4.1, we use the discrete polarity returned by the classifier to decide whether go long/abstain/short a stock.", "labels": [], "entities": []}, {"text": "In section 4.2 we instead use the raw SVM score that reports the distance of the current document from the classifier's decision boundary.", "labels": [], "entities": []}, {"text": "In section 4.3, we hold the trading strategy constant, and instead vary the document representation features in the underlying sentiment analyzer.", "labels": [], "entities": [{"text": "sentiment analyzer", "start_pos": 127, "end_pos": 145, "type": "TASK", "confidence": 0.7106965184211731}]}, {"text": "Here, we measure both market return and classifier accuracy to determine whether they agree.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.9554368257522583}]}, {"text": "In all three experiments, we compare the perposition returns of trading strategies with the following four standards, where the number of days for which a position is held remains constant: 1.", "labels": [], "entities": []}, {"text": "The momentum strategy computes the price of the stock h days ago, where h is the holding period.", "labels": [], "entities": []}, {"text": "Then, it goes long for h days if the previous price is lower than the current price.", "labels": [], "entities": []}, {"text": "2. The S&P strategy simply goes long on the S&P 500 for the holding period.", "labels": [], "entities": [{"text": "S&P", "start_pos": 7, "end_pos": 10, "type": "DATASET", "confidence": 0.6687830289204916}, {"text": "S&P 500", "start_pos": 44, "end_pos": 51, "type": "DATASET", "confidence": 0.9382797181606293}]}, {"text": "This strategy completely ignores the stock in question and the news about it.", "labels": [], "entities": []}, {"text": "3. The oracle S&P strategy computes the value of the S&P 500 index h days into the future.", "labels": [], "entities": [{"text": "S&P 500 index h", "start_pos": 53, "end_pos": 68, "type": "DATASET", "confidence": 0.8324361344178518}]}, {"text": "If the future value is greater than the current day's value, then it goes long on the S&P 500 index.", "labels": [], "entities": [{"text": "S&P 500 index", "start_pos": 86, "end_pos": 99, "type": "DATASET", "confidence": 0.907174801826477}]}, {"text": "Otherwise, it goes short.", "labels": [], "entities": []}, {"text": "4. The oracle strategy computes the value of the stock h days into the future.", "labels": [], "entities": []}, {"text": "If the future value is greater than the current day's value, then it goes long on the stock.", "labels": [], "entities": []}, {"text": "Otherwise, it goes short.", "labels": [], "entities": []}, {"text": "The oracle and oracle S&P strategies are included as toplines to determine how close the experimental strategies come to ones with perfect knowledge of the future.", "labels": [], "entities": []}, {"text": "\"Market-trained\" is the same as \"experimental\" attest time, but trains the sentiment analyzer on the market return of the stock in question for h days following a training article's publication, rather than the article's annotation.", "labels": [], "entities": []}, {"text": "Given a news document fora publicly traded company, the trading agent first computes the sentiment class of the document.", "labels": [], "entities": []}, {"text": "If the sentiment is positive, the agent goes long on the stock on the date the news is released.", "labels": [], "entities": []}, {"text": "If the sentiment is negative, it goes short.", "labels": [], "entities": []}, {"text": "All trades are made based on the adjusted closing price on this date.", "labels": [], "entities": []}, {"text": "We evaluate the performance of this strategy using four different holding periods: 30, 5, 3, and 1 day(s).", "labels": [], "entities": []}, {"text": "The returns and Sharpe ratios are presented in for the four different holding periods and the five different trading strategies.", "labels": [], "entities": [{"text": "returns", "start_pos": 4, "end_pos": 11, "type": "METRIC", "confidence": 0.9720699787139893}, {"text": "Sharpe ratios", "start_pos": 16, "end_pos": 29, "type": "METRIC", "confidence": 0.8725929260253906}]}, {"text": "The Sharpe ratio can be viewed as a return to risk ratio.", "labels": [], "entities": [{"text": "Sharpe ratio", "start_pos": 4, "end_pos": 16, "type": "METRIC", "confidence": 0.9849023818969727}]}, {"text": "A high Sharpe ratio indicates good return for relatively low risk.", "labels": [], "entities": [{"text": "Sharpe ratio", "start_pos": 7, "end_pos": 19, "type": "METRIC", "confidence": 0.989069014787674}]}, {"text": "The Sharpe ratio is calculated as follows: where Ra is the return of a single asset and Rb is the return of a risk-free asset, such as a 10-year U.S. Treasury note.", "labels": [], "entities": [{"text": "Sharpe ratio", "start_pos": 4, "end_pos": 16, "type": "METRIC", "confidence": 0.9838127195835114}, {"text": "Ra", "start_pos": 49, "end_pos": 51, "type": "METRIC", "confidence": 0.9648576974868774}, {"text": "Rb", "start_pos": 88, "end_pos": 90, "type": "METRIC", "confidence": 0.9433924555778503}]}, {"text": "The returns from this experimental trading system are fairly low, although they do beat the baselines.", "labels": [], "entities": []}, {"text": "A one-way ANOVA test between the experimental strategy, momentum strategy, and S&P strategy using the percent returns from the individual trades yields p values of 0.06493, 0.08162, 0.1792, and 0.4164, respectively, thus failing to reject the null hypothesis that the returns are not significantly higher.", "labels": [], "entities": [{"text": "ANOVA", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.49351173639297485}]}, {"text": "Furthermore, the means and medians of all three trading strategies are approximately the same and centred around 0.", "labels": [], "entities": []}, {"text": "The standard deviations of the experimental strategy and the momentum strategy are nearly identical, differing only in the thousandths digit.", "labels": [], "entities": []}, {"text": "The standard deviations for the S&P strategy differ from the other two strategies due to the fact that the strategy buys and sells the entire S&P 500 index and not the individual stocks described in the news articles.", "labels": [], "entities": [{"text": "S&P strategy", "start_pos": 32, "end_pos": 44, "type": "DATASET", "confidence": 0.7382613644003868}, {"text": "S&P 500 index", "start_pos": 142, "end_pos": 155, "type": "DATASET", "confidence": 0.8450603723526001}]}, {"text": "There is, in fact, no convincing evidence that discrete sentiment class leads to an improved trading strategy from this or any other study with which we are familiar, based on the details that they publish.", "labels": [], "entities": []}, {"text": "One may note, however, that the returns from the experimental strategy have slightly higher Sharpe ratios than either of the baselines.", "labels": [], "entities": [{"text": "Sharpe", "start_pos": 92, "end_pos": 98, "type": "METRIC", "confidence": 0.9914761185646057}]}, {"text": "One may also note that using a sentiment analyzer mostly beats training directly on market data, which to an extent vindicates the use of sentiment annotation as a separate component.", "labels": [], "entities": [{"text": "sentiment analyzer", "start_pos": 31, "end_pos": 49, "type": "TASK", "confidence": 0.7702201008796692}]}, {"text": "shows the market capitalizations of the companies for each individual trade plotted against the percent return for the 1 day holding period.", "labels": [], "entities": []}, {"text": "The correlation between the two variables is not significant.", "labels": [], "entities": []}, {"text": "The graphs for the other holding periods are similar.", "labels": [], "entities": []}, {"text": "shows the percent change in share value plotted against the raw SVM score for the different holding periods.", "labels": [], "entities": []}, {"text": "We can see a weak correlation between the two.", "labels": [], "entities": []}, {"text": "For the 30 days, 5 days, 3 days, and 1 day holding periods, the correlations are 0.017, 0.16, 0.16, and 0.16, respectively.", "labels": [], "entities": []}, {"text": "The line of best fit is shown.", "labels": [], "entities": [{"text": "fit", "start_pos": 17, "end_pos": 20, "type": "METRIC", "confidence": 0.9399973750114441}]}, {"text": "This prompts us to conduct our next experiment.", "labels": [], "entities": []}, {"text": "Let us now hold the trading strategy fixed (at the final one, with safety zones) and turn to the underlying sentiment analyzer.", "labels": [], "entities": [{"text": "sentiment analyzer", "start_pos": 108, "end_pos": 126, "type": "TASK", "confidence": 0.7833795845508575}]}, {"text": "With a good trading    strategy in place, it is clearly possible to vary some aspect of the sentiment analyzer in order to determine its best setting in this context.", "labels": [], "entities": [{"text": "sentiment analyzer", "start_pos": 92, "end_pos": 110, "type": "TASK", "confidence": 0.8330114483833313}]}, {"text": "Is classifier accuracy a suitable proxy for this?", "labels": [], "entities": []}, {"text": "Indeed, we may hope that classifier accuracy will be more portable to other possible tasks, but then it must at least correlate well with task-based performance.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.9588392972946167}]}, {"text": "We tried another feature representation for documents.", "labels": [], "entities": []}, {"text": "In addition to evaluating those attempted earlier, we now hypothesize that the passive voice maybe useful to emphasize in our representations, as the existential passive can be used to evade responsibility.", "labels": [], "entities": []}, {"text": "So we add to the BM25 weighted vector the counts of word tokens ending in \"n\" or \"d\" as well as the total count of every conjugated form of the copular verb: \"be\", \"is\", \"am\", \"are\", \"were\", \"was\", and \"been\".", "labels": [], "entities": [{"text": "BM25", "start_pos": 17, "end_pos": 21, "type": "DATASET", "confidence": 0.8747203946113586}]}, {"text": "These three features are superficial indicators of the passive voice.", "labels": [], "entities": []}, {"text": "presents the returns obtained from these 6 feature representations.", "labels": [], "entities": []}, {"text": "The feature set with BM25-weighted term frequencies plus the number of copulars and tokens ending in \"n\", \"d\" (bm25 freq d n copular) yields higher returns than any other representation attempted on the 5, 3, and 1 day holding periods, and the secondhighest on the 30 days holding period, But it has the worst classification accuracy by far: a full 18 percentage points below term presence.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 325, "end_pos": 333, "type": "METRIC", "confidence": 0.9875116348266602}]}, {"text": "This is a very compelling illustration of how misleading an intrinsic evaluation can be.", "labels": [], "entities": []}, {"text": "Other agreement measures likewise point in the opposite direction.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Average 10-fold cross validation ac- curacy of the sentiment classifier using different  term-frequency weighting schemes. The same  folds were used in all feature sets.", "labels": [], "entities": []}, {"text": " Table 2: Returns and Sharpe ratios for the Experi- mental, baseline and topline trading strategies over  30, 5, 3, and 1 day(s) holding periods.", "labels": [], "entities": [{"text": "Returns and Sharpe ratios", "start_pos": 10, "end_pos": 35, "type": "METRIC", "confidence": 0.7733169868588448}]}, {"text": " Table 3: Sentiment classification accuracy (average 10-fold cross-validation), Scott's \u03c0, Krippendorff's  \u03b1, Cohen's \u03ba and trade returns of different feature sets and term frequency weighting schemes in Exp. 3.  The same folds were used for the different representations. The non-annualized returns are presented in  columns 3-6.", "labels": [], "entities": [{"text": "Sentiment classification", "start_pos": 10, "end_pos": 34, "type": "TASK", "confidence": 0.9160322844982147}, {"text": "accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9261950850486755}, {"text": "trade returns", "start_pos": 124, "end_pos": 137, "type": "METRIC", "confidence": 0.965784341096878}, {"text": "Exp", "start_pos": 204, "end_pos": 207, "type": "DATASET", "confidence": 0.9050731658935547}]}]}