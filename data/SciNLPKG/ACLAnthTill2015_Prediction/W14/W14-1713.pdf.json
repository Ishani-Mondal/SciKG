{"title": [{"text": "A Unified Framework for Grammar Error Correction", "labels": [], "entities": [{"text": "Grammar Error Correction", "start_pos": 24, "end_pos": 48, "type": "TASK", "confidence": 0.7972575624783834}]}], "abstractContent": [{"text": "In this paper we describe the PKU system for the CoNLL-2014 grammar error correction shared task.", "labels": [], "entities": [{"text": "CoNLL-2014 grammar error correction shared task", "start_pos": 49, "end_pos": 96, "type": "TASK", "confidence": 0.8324786921342214}]}, {"text": "We propose a unified framework for correcting all types of errors.", "labels": [], "entities": [{"text": "correcting", "start_pos": 35, "end_pos": 45, "type": "TASK", "confidence": 0.9586432576179504}]}, {"text": "We use unlabeled news texts instead of large amount of human annotated texts as training data.", "labels": [], "entities": []}, {"text": "Based on these data, a tri-gram language model is used to correct the replacement errors while two extra classification models are trained to correct errors related to determiners and prepositions.", "labels": [], "entities": []}, {"text": "Our system achieves 25.32% inf 0.5 on the original test data and 29.10% on the revised test data.", "labels": [], "entities": []}], "introductionContent": [{"text": "The task of grammar error correction is difficult yet important.", "labels": [], "entities": [{"text": "grammar error correction", "start_pos": 12, "end_pos": 36, "type": "TASK", "confidence": 0.5718939105669657}]}, {"text": "An automatic grammar error correction system can help second language(L2) learners improve the quality of their writing.", "labels": [], "entities": []}, {"text": "Previous shared tasks for grammar error correction, such as the HOO shared task of 2012 ( and the CoNLL-2013 shared task , focus on limited types of errors.", "labels": [], "entities": [{"text": "grammar error correction", "start_pos": 26, "end_pos": 50, "type": "TASK", "confidence": 0.7222740054130554}, {"text": "HOO shared task of 2012", "start_pos": 64, "end_pos": 87, "type": "DATASET", "confidence": 0.6193534314632416}]}, {"text": "For example, HOO-2012 only considers errors related to determiners and prepositions.", "labels": [], "entities": [{"text": "HOO-2012", "start_pos": 13, "end_pos": 21, "type": "DATASET", "confidence": 0.6981245279312134}]}, {"text": "CoNLL-2013 further considers errors that are related to noun number, verb form and subject-object agreement.", "labels": [], "entities": [{"text": "CoNLL-2013", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.9495521783828735}]}, {"text": "In the CoNLL-2014 shared task, all systems should consider all the 28 kinds of errors, including errors such as spelling errors which cannot be corrected using a single classifier.", "labels": [], "entities": []}, {"text": "Most of the top-ranked systems in the CoNLL-2013 shared task() train individual classifiers or language models for each kind of errors independently.", "labels": [], "entities": []}, {"text": "Although later systems such as;  use Integer Linear Programming (ILP) to decode a global optimized result, the input scores for ILP still come from the individual classification confidence of each kind of errors.", "labels": [], "entities": []}, {"text": "It is hard to adapt these methods directly into the CoNLL-2014 shared task.", "labels": [], "entities": [{"text": "CoNLL-2014 shared task", "start_pos": 52, "end_pos": 74, "type": "DATASET", "confidence": 0.7878444790840149}]}, {"text": "It will be both time-consuming and impossible to train individual classifiers for all the 28 kinds of errors.", "labels": [], "entities": []}, {"text": "Besides the classifier and language model based methods, some systems() also use the machine translation approach.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 85, "end_pos": 104, "type": "TASK", "confidence": 0.7487543523311615}]}, {"text": "Because there area limited amount of training data, this kind of approaches often need to use other corpora of L2 learners, such as the Cambridge Learner Corpus.", "labels": [], "entities": [{"text": "Cambridge Learner Corpus", "start_pos": 136, "end_pos": 160, "type": "DATASET", "confidence": 0.9738142093022665}]}, {"text": "Because these corpora use different annotation criteria, the correction systems should figure out ways to map the error types from one corpus to another.", "labels": [], "entities": []}, {"text": "Even with these additions and transformations, there are still too few training data available to train a good translation model.", "labels": [], "entities": [{"text": "translation", "start_pos": 111, "end_pos": 122, "type": "TASK", "confidence": 0.978611171245575}]}, {"text": "In contrast, we think the grammar error correction system should 1) correct most kinds of errors in a unified framework and 2) use as much unlabeled data as possible instead of using large amount of human annotated data.", "labels": [], "entities": [{"text": "grammar error correction", "start_pos": 26, "end_pos": 50, "type": "TASK", "confidence": 0.5944560170173645}]}, {"text": "To be specific, our system do not need to train individual classifiers for each kind of errors, nor do we need to use manually corrected texts.", "labels": [], "entities": []}, {"text": "Following the observation that a correction can either replace a wrong word or delete/insert a word, our system is divided into two parts.", "labels": [], "entities": []}, {"text": "Firstly, we use a Language Model(LM) to correct errors with respect to the wrongly used words.", "labels": [], "entities": []}, {"text": "The LM only uses the statistics from a large corpus.", "labels": [], "entities": []}, {"text": "All errors related to wrongly used words can be examined in this unified model instead of designing individual systems for each kind of errors.", "labels": [], "entities": []}, {"text": "Secondly, we train extra classifiers for determiner errors and preposition errors.", "labels": [], "entities": []}, {"text": "We further consider these two kinds of errors because many of the deletion and insertion errors belongs to determiner or preposition errors.", "labels": [], "entities": [{"text": "insertion", "start_pos": 79, "end_pos": 88, "type": "METRIC", "confidence": 0.9854597449302673}]}, {"text": "The training data of the two classification models also come from a large unlabeled news corpus therefore no human annotation is needed.", "labels": [], "entities": []}, {"text": "Although we try to use a unified framework to get better performance in the grammar error correction task, there are still a small portion of errors we do not consider.", "labels": [], "entities": [{"text": "grammar error correction task", "start_pos": 76, "end_pos": 105, "type": "TASK", "confidence": 0.6950065121054649}]}, {"text": "The insertion and deletion of words are not considered if the word is neither a determiner nor a preposition.", "labels": [], "entities": []}, {"text": "Our system is also incapable of replacing a word sequence into another word sequence.", "labels": [], "entities": []}, {"text": "We do not consider these kinds of errors because we find some of them are hard to generate correction candidates without further understanding of the context, and are not easy to be corrected even by human beings.", "labels": [], "entities": []}, {"text": "The paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 1 gives the introduction.", "labels": [], "entities": []}, {"text": "In section 2 we describe the task.", "labels": [], "entities": []}, {"text": "In section 3 we describe our algorithm.", "labels": [], "entities": []}, {"text": "Experiments are described in section 4.", "labels": [], "entities": []}, {"text": "We also give a detailed analysis of the results in section 4.", "labels": [], "entities": []}, {"text": "In section 5 related works are introduced, and the paper is concluded in the last section.", "labels": [], "entities": []}], "datasetContent": [{"text": "We experiment on the CoNLL-2014 test data.", "labels": [], "entities": [{"text": "CoNLL-2014 test data", "start_pos": 21, "end_pos": 41, "type": "DATASET", "confidence": 0.9750259717305502}]}, {"text": "We evaluate our system based on the M2 scorer which is provided by the organizers.", "labels": [], "entities": [{"text": "M2 scorer", "start_pos": 36, "end_pos": 45, "type": "METRIC", "confidence": 0.6568284332752228}]}, {"text": "Details of the M2 scorer can be found in.", "labels": [], "entities": [{"text": "M2 scorer", "start_pos": 15, "end_pos": 24, "type": "DATASET", "confidence": 0.8339651525020599}]}, {"text": "We tune the additional parameters like all the thresholds on the CoNLL-2014 official training data.", "labels": [], "entities": [{"text": "CoNLL-2014 official training data", "start_pos": 65, "end_pos": 98, "type": "DATASET", "confidence": 0.975240170955658}]}, {"text": "We use all the text in the Gigaword corpus to train the language model.", "labels": [], "entities": [{"text": "Gigaword corpus", "start_pos": 27, "end_pos": 42, "type": "DATASET", "confidence": 0.9387036561965942}]}, {"text": "We use 2.5 million sentences in the Gigaword corpus to train the extra two classifier.", "labels": [], "entities": [{"text": "Gigaword corpus", "start_pos": 36, "end_pos": 51, "type": "DATASET", "confidence": 0.9292995035648346}]}, {"text": "Results of our system are shown in table 2.", "labels": [], "entities": []}, {"text": "LM refers to using language model alone.", "labels": [], "entities": []}, {"text": "LM+det refers to using a determiner classifier after using a language model.", "labels": [], "entities": []}, {"text": "LM+prep refers to using a preposition classifier after using a language model.", "labels": [], "entities": []}, {"text": "LM+det+preposition refers to using a preposition classifier after LM+det, which is the method used in our final system.: The experimental results of our system in the CoNLL-2014 shared task on the revised annotations.", "labels": [], "entities": [{"text": "CoNLL-2014", "start_pos": 167, "end_pos": 177, "type": "DATASET", "confidence": 0.8960331082344055}]}, {"text": "The threshold for determiner model and preposition model is 0.99 and 0.99.", "labels": [], "entities": []}, {"text": "Parameters are tuned on the CoNLL-2014 training data.", "labels": [], "entities": [{"text": "Parameters", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9839093089103699}, {"text": "CoNLL-2014 training data", "start_pos": 28, "end_pos": 52, "type": "DATASET", "confidence": 0.9716590642929077}]}, {"text": "From the results we can see that the main contribution comes from the LM model and determiner model.", "labels": [], "entities": []}, {"text": "The preposition model can correct part of the errors while introduce new errors.", "labels": [], "entities": []}, {"text": "The preposition model may harm the overall performance.", "labels": [], "entities": []}, {"text": "But considering the fact that the grammar error correction systems are always used for recommending errors, we still keep the preposition model in real applications and suggest the errors predicted by the preposition model.", "labels": [], "entities": [{"text": "grammar error correction", "start_pos": 34, "end_pos": 58, "type": "TASK", "confidence": 0.6405985653400421}]}, {"text": "One limitation of our system is that we only use a tri-gram based language model as well as up to 4-gram features for limited instances.", "labels": [], "entities": []}, {"text": "Previous works( have shown that other resources like the Google 5-gram statistics can help improve performance.", "labels": [], "entities": [{"text": "Google 5-gram statistics", "start_pos": 57, "end_pos": 81, "type": "DATASET", "confidence": 0.797103484471639}]}, {"text": "For the determiner and preposition models, we experiment on different size of training data, from near zero to the upper bound of our server's memory limit (about 72GB).", "labels": [], "entities": []}, {"text": "We find that under this limitation, the performance is still improving when adding more training instances.", "labels": [], "entities": []}, {"text": "We believe the performance can be further improved.", "labels": [], "entities": []}, {"text": "Scores based on the revised annotations is shown in table 3.", "labels": [], "entities": []}, {"text": "For the convenience of future meaningful comparison, we report the result of our system on the CoNLL-2013 data set in table 4.", "labels": [], "entities": [{"text": "CoNLL-2013 data set", "start_pos": 95, "end_pos": 114, "type": "DATASET", "confidence": 0.9828424453735352}]}, {"text": "We tune the additional parameters like all the thresholds on the CoNLL-2013 official training data.", "labels": [], "entities": [{"text": "CoNLL-2013 official training data", "start_pos": 65, "end_pos": 98, "type": "DATASET", "confidence": 0.9750606715679169}]}, {"text": "Note that in CoNLL-2013 the scorer considers F1 score in-: The experimental results of our system on the CoNLL-2013 shared task data.", "labels": [], "entities": [{"text": "CoNLL-2013", "start_pos": 13, "end_pos": 23, "type": "DATASET", "confidence": 0.9646961688995361}, {"text": "F1 score", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.98406121134758}, {"text": "CoNLL-2013 shared task data", "start_pos": 105, "end_pos": 132, "type": "DATASET", "confidence": 0.9024780094623566}]}, {"text": "The threshold for determiner model and preposition model is 0.75 and 0.99.", "labels": [], "entities": []}, {"text": "Parameters are tuned on the CoNLL-2013 training data.", "labels": [], "entities": [{"text": "Parameters", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9837623238563538}, {"text": "CoNLL-2013 training data", "start_pos": 28, "end_pos": 52, "type": "DATASET", "confidence": 0.9720124999682108}]}, {"text": "CoNLL13 1st is and the 2nd is stead of F0.5.", "labels": [], "entities": [{"text": "CoNLL13", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9193766713142395}, {"text": "F0.5", "start_pos": 39, "end_pos": 43, "type": "METRIC", "confidence": 0.8514243960380554}]}, {"text": "Therefore some of the thresholds are different with the ones in the CoNLL-2014 system.", "labels": [], "entities": [{"text": "CoNLL-2014 system", "start_pos": 68, "end_pos": 85, "type": "DATASET", "confidence": 0.9283947646617889}]}, {"text": "Because the CoNLL-2013 shared task only considers 5 types of errors, it will be much easier to design components specially for each kind of errors.", "labels": [], "entities": []}, {"text": "Therefore our system is a bit less accurate than the best system.", "labels": [], "entities": [{"text": "accurate", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9647262096405029}]}, {"text": "In this system, we restrict the candidates to be either noun or verb, and omit the spell checking model.", "labels": [], "entities": []}, {"text": "We also omit some postprocessings like deciding whether a word should be split into two words, because these kinds of errors are not included.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: The experimental results of our system in  the CoNLL-2014 shared task. The threshold for  determiner model and preposition model is 0.99  and 0.99. Parameters are tuned on the CoNLL- 2014 training data.", "labels": [], "entities": [{"text": "CoNLL-2014 shared task", "start_pos": 57, "end_pos": 79, "type": "DATASET", "confidence": 0.8488213618596395}, {"text": "Parameters", "start_pos": 158, "end_pos": 168, "type": "METRIC", "confidence": 0.9733721613883972}, {"text": "CoNLL- 2014 training data", "start_pos": 186, "end_pos": 211, "type": "DATASET", "confidence": 0.9607186913490295}]}, {"text": " Table 3: The experimental results of our system  in the CoNLL-2014 shared task on the revised an- notations. The threshold for determiner model and  preposition model is 0.99 and 0.99. Parameters are  tuned on the CoNLL-2014 training data.", "labels": [], "entities": [{"text": "CoNLL-2014", "start_pos": 57, "end_pos": 67, "type": "DATASET", "confidence": 0.9478003978729248}, {"text": "Parameters", "start_pos": 186, "end_pos": 196, "type": "METRIC", "confidence": 0.9724521040916443}, {"text": "CoNLL-2014 training data", "start_pos": 215, "end_pos": 239, "type": "DATASET", "confidence": 0.9490079482396444}]}, {"text": " Table 4: The experimental results of our system  on the CoNLL-2013 shared task data. The thresh- old for determiner model and preposition model  is 0.75 and 0.99. Parameters are tuned on the  CoNLL-2013 training data. CoNLL13 1st is", "labels": [], "entities": [{"text": "CoNLL-2013 shared task data", "start_pos": 57, "end_pos": 84, "type": "DATASET", "confidence": 0.9016666412353516}, {"text": "thresh- old for determiner model", "start_pos": 90, "end_pos": 122, "type": "METRIC", "confidence": 0.7377831935882568}, {"text": "CoNLL-2013 training data", "start_pos": 193, "end_pos": 217, "type": "DATASET", "confidence": 0.9357912540435791}, {"text": "CoNLL13", "start_pos": 219, "end_pos": 226, "type": "DATASET", "confidence": 0.8528193831443787}]}]}