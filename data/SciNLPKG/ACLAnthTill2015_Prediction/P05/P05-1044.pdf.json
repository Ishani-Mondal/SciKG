{"title": [{"text": "Contrastive Estimation: Training Log-Linear Models on Unlabeled Data *", "labels": [], "entities": [{"text": "Contrastive Estimation", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8840828835964203}]}], "abstractContent": [{"text": "Conditional random fields (Lafferty et al., 2001) are quite effective at sequence labeling tasks like shallow parsing (Sha and Pereira, 2003) and named-entity extraction (McCallum and Li, 2003).", "labels": [], "entities": [{"text": "sequence labeling", "start_pos": 73, "end_pos": 90, "type": "TASK", "confidence": 0.5950596928596497}, {"text": "shallow parsing", "start_pos": 102, "end_pos": 117, "type": "TASK", "confidence": 0.6084065735340118}, {"text": "named-entity extraction", "start_pos": 146, "end_pos": 169, "type": "TASK", "confidence": 0.7649845778942108}]}, {"text": "CRFs are log-linear, allowing the incorporation of arbitrary features into the model.", "labels": [], "entities": []}, {"text": "To train on unlabeled data, we require unsupervised estimation methods for log-linear models; few exist.", "labels": [], "entities": []}, {"text": "We describe a novel approach, contrastive estimation.", "labels": [], "entities": []}, {"text": "We show that the new technique can be intuitively understood as exploiting implicit negative evidence and is computa-tionally efficient.", "labels": [], "entities": []}, {"text": "Applied to a sequence labeling problem-POS tagging given a tagging dictionary and unlabeled text-contrastive estimation outper-forms EM (with the same feature set), is more robust to degradations of the dictionary, and can largely recover by modeling additional features.", "labels": [], "entities": [{"text": "sequence labeling problem-POS tagging", "start_pos": 13, "end_pos": 50, "type": "TASK", "confidence": 0.7997059375047684}, {"text": "EM", "start_pos": 133, "end_pos": 135, "type": "METRIC", "confidence": 0.9098898768424988}]}], "introductionContent": [{"text": "Finding linguistic structure in raw text is not easy.", "labels": [], "entities": []}, {"text": "The classical forward-backward and inside-outside algorithms try to guide probabilistic models to discover structure in text, but they tend to get stuck in local maxima.", "labels": [], "entities": []}, {"text": "Even when they avoid local maxima (e.g., through clever initialization) they typically deviate from human ideas of what the \"right\" structure is.", "labels": [], "entities": []}, {"text": "One strategy is to incorporate domain knowledge into the model's structure.", "labels": [], "entities": []}, {"text": "Instead of blind HMMs or PCFGs, one could use models whose features * This work was supported by a Fannie and John Hertz Foundation fellowship to the first author and NSF ITR grant IIS-0313193 to the second author.", "labels": [], "entities": [{"text": "NSF ITR grant IIS-0313193", "start_pos": 167, "end_pos": 192, "type": "DATASET", "confidence": 0.5164988785982132}]}, {"text": "The views expressed are not necessarily endorsed by the sponsors.", "labels": [], "entities": []}, {"text": "The authors also thank three anonymous ACL reviewers for helpful comments, colleagues at JHU CLSP (especially David Smith and Roy Tromble) and Miles Osborne for insightful feedback, and Eric Goldlust and Markus Dreyer for Dyna language support. are crafted to pay attention to a range of domainspecific linguistic cues.", "labels": [], "entities": [{"text": "JHU CLSP", "start_pos": 89, "end_pos": 97, "type": "DATASET", "confidence": 0.9457407295703888}]}, {"text": "Log-linear models can be so crafted and have already achieved excellent performance when trained on annotated data, where they are known as \"maximum entropy\" models.", "labels": [], "entities": []}, {"text": "Our goal is to learn log-linear models from unannotated data.", "labels": [], "entities": []}, {"text": "Since the forward-backward and inside-outside algorithms are instances of Expectation-Maximization (EM), a natural approach is to construct EM algorithms that handle log-linear models.", "labels": [], "entities": []}, {"text": "did so, then resorted to an approximation because the true objective function was hard to normalize.", "labels": [], "entities": []}, {"text": "Stepping back from EM, we may generally envision parameter estimation for probabilistic modeling as pushing probability mass toward the training examples.", "labels": [], "entities": []}, {"text": "We must consider not only where the learner pushes the mass, but also from where the mass is taken.", "labels": [], "entities": []}, {"text": "In this paper, we describe an alternative to EM: contrastive estimation (CE), which (unlike EM) explicitly states the source of the probability mass that is to be given to an example.", "labels": [], "entities": [{"text": "contrastive estimation (CE)", "start_pos": 49, "end_pos": 76, "type": "METRIC", "confidence": 0.7340780854225158}]}, {"text": "One reason is to make normalization efficient.", "labels": [], "entities": [{"text": "normalization", "start_pos": 22, "end_pos": 35, "type": "TASK", "confidence": 0.9717375636100769}]}, {"text": "Indeed, CE generalizes EM and other practical techniques used to train log-linear models, including conditional estimation (for the supervised case) and Riezler's approximation (for the unsupervised case).", "labels": [], "entities": [{"text": "CE", "start_pos": 8, "end_pos": 10, "type": "METRIC", "confidence": 0.7847057580947876}]}, {"text": "The other reason to use CE is to improve accuracy.", "labels": [], "entities": [{"text": "CE", "start_pos": 24, "end_pos": 26, "type": "METRIC", "confidence": 0.9592314958572388}, {"text": "accuracy", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.9984912872314453}]}, {"text": "CE offers an additional way to inject domain knowledge into unsupervised learning).", "labels": [], "entities": [{"text": "CE", "start_pos": 0, "end_pos": 2, "type": "DATASET", "confidence": 0.8526018261909485}]}, {"text": "CE hypothesizes that each positive example in training implies a domain-specific set of examples which are (for the most part) degraded ( \u00a72).", "labels": [], "entities": []}, {"text": "This class of implicit negative evidence provides the source of probability mass for the observed example.", "labels": [], "entities": []}, {"text": "We discuss the application of CE to loglinear models in \u00a73.", "labels": [], "entities": [{"text": "CE", "start_pos": 30, "end_pos": 32, "type": "METRIC", "confidence": 0.9278953075408936}]}, {"text": "We are particularly interested in log-linear models over sequences, like the conditional random fields (CRFs) of and weighted CFGs ().", "labels": [], "entities": []}, {"text": "For a given sequence, implicit negative evidence can be represented as a lattice derived by finite-state operations ( \u00a74).", "labels": [], "entities": []}, {"text": "Effectiveness of the approach on POS tagging using unlabeled data is demonstrated ( \u00a75).", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 33, "end_pos": 44, "type": "TASK", "confidence": 0.8921506404876709}]}, {"text": "We discuss future work ( \u00a76) and conclude ( \u00a77).", "labels": [], "entities": []}], "datasetContent": [{"text": "We compare CE (using neighborhoods from \u00a74) with EM on POS tagging using unlabeled data.", "labels": [], "entities": [{"text": "CE", "start_pos": 11, "end_pos": 13, "type": "METRIC", "confidence": 0.8811060190200806}, {"text": "EM", "start_pos": 49, "end_pos": 51, "type": "METRIC", "confidence": 0.8098917603492737}, {"text": "POS tagging", "start_pos": 55, "end_pos": 66, "type": "TASK", "confidence": 0.7032682150602341}]}], "tableCaptions": [{"text": " Table 3: Percent of all words correctly tagged in the 24K dataset, as the tagging dictionary is diluted. Unsupervised model selection  (\"sel.\") and oracle model selection (\"oracle\") across smoothing parameters are shown. Note that we evaluated on all words (unlike", "labels": [], "entities": [{"text": "24K dataset", "start_pos": 55, "end_pos": 66, "type": "DATASET", "confidence": 0.7832934856414795}]}]}