{"title": [{"text": "Domain Kernels for Word Sense Disambiguation", "labels": [], "entities": [{"text": "Word Sense Disambiguation", "start_pos": 19, "end_pos": 44, "type": "TASK", "confidence": 0.673975924650828}]}], "abstractContent": [{"text": "In this paper we present a supervised Word Sense Disambiguation methodology , that exploits kernel methods to model sense distinctions.", "labels": [], "entities": [{"text": "Word Sense Disambiguation", "start_pos": 38, "end_pos": 63, "type": "TASK", "confidence": 0.6517453591028849}]}, {"text": "In particular a combination of kernel functions is adopted to estimate independently both syntagmatic and domain similarity.", "labels": [], "entities": []}, {"text": "We defined a kernel function, namely the Domain Kernel, that allowed us to plug \"external knowl-edge\" into the supervised learning process.", "labels": [], "entities": []}, {"text": "External knowledge is acquired from unlabeled data in a totally unsupervised way, and it is represented by means of Domain Models.", "labels": [], "entities": []}, {"text": "We evaluated our methodology on several lexical sample tasks in different languages, outperforming significantly the state-of-the-art for each of them, while reducing the amount of labeled training data required for learning.", "labels": [], "entities": []}], "introductionContent": [{"text": "The main limitation of many supervised approaches for Natural Language Processing (NLP) is the lack of available annotated training data.", "labels": [], "entities": [{"text": "Natural Language Processing (NLP)", "start_pos": 54, "end_pos": 87, "type": "TASK", "confidence": 0.7891809046268463}]}, {"text": "This problem is known as the Knowledge Acquisition Bottleneck.", "labels": [], "entities": [{"text": "Knowledge Acquisition Bottleneck", "start_pos": 29, "end_pos": 61, "type": "TASK", "confidence": 0.6142792701721191}]}, {"text": "To reach high accuracy, state-of-the-art systems for Word Sense Disambiguation (WSD) are designed according to a supervised learning framework, in which the disambiguation of each word in the lexicon is performed by constructing a different classifier.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9868685007095337}, {"text": "Word Sense Disambiguation (WSD)", "start_pos": 53, "end_pos": 84, "type": "TASK", "confidence": 0.8074374745289484}]}, {"text": "A large set of sense tagged examples is then required to train each classifier.", "labels": [], "entities": []}, {"text": "This methodology is called word expert approach).", "labels": [], "entities": []}, {"text": "However this is clearly unfeasible for all-words WSD tasks, in which all the words of an open text should be disambiguated.", "labels": [], "entities": [{"text": "WSD tasks", "start_pos": 49, "end_pos": 58, "type": "TASK", "confidence": 0.9146859645843506}]}, {"text": "On the other hand, the word expert approach works very well for lexical sample WSD tasks (i.e. tasks in which it is required to disambiguate only those words for which enough training data is provided).", "labels": [], "entities": [{"text": "WSD tasks", "start_pos": 79, "end_pos": 88, "type": "TASK", "confidence": 0.860209584236145}]}, {"text": "As the original rationale of the lexical sample tasks was to define a clear experimental settings to enhance the comprehension of WSD, they should be considered as preceding exercises to all-words tasks.", "labels": [], "entities": [{"text": "WSD", "start_pos": 130, "end_pos": 133, "type": "TASK", "confidence": 0.857414722442627}]}, {"text": "However this is not the actual case.", "labels": [], "entities": []}, {"text": "Algorithms designed for lexical sample WSD are often based on pure supervision and hence \"data hungry\".", "labels": [], "entities": []}, {"text": "We think that lexical sample WSD should regain its original explorative role and possibly use a minimal amount of training data, exploiting instead external knowledge acquired in an unsupervised way to reach the actual state-of-the-art performance.", "labels": [], "entities": [{"text": "WSD", "start_pos": 29, "end_pos": 32, "type": "TASK", "confidence": 0.7620387673377991}]}, {"text": "By the way, minimal supervision is the basis of state-of-the-art systems for all-words tasks (e.g. (), that are trained on small sense tagged corpora (e.g. SemCor), in which few examples fora subset of the ambiguous words in the lexicon can be found.", "labels": [], "entities": []}, {"text": "Thus improving the performance of WSD systems with few learning examples is a fundamental step towards the direction of designing a WSD system that works well on real texts.", "labels": [], "entities": [{"text": "WSD", "start_pos": 34, "end_pos": 37, "type": "TASK", "confidence": 0.9319635629653931}]}, {"text": "In addition, it is a common opinion that the performance of state-of-the-art WSD systems is not satisfactory from an applicative point of view yet.", "labels": [], "entities": [{"text": "WSD", "start_pos": 77, "end_pos": 80, "type": "TASK", "confidence": 0.9211652278900146}]}, {"text": "To achieve these goals we identified two promising research directions: 1.", "labels": [], "entities": []}, {"text": "Modeling independently domain and syntagmatic aspects of sense distinction, to improve the feature representation of sense tagged examples ( ).", "labels": [], "entities": [{"text": "sense distinction", "start_pos": 57, "end_pos": 74, "type": "TASK", "confidence": 0.7191694229841232}]}, {"text": "2. Leveraging external knowledge acquired from unlabeled corpora.", "labels": [], "entities": [{"text": "Leveraging external knowledge acquired from unlabeled corpora", "start_pos": 3, "end_pos": 64, "type": "TASK", "confidence": 0.8122814553124564}]}, {"text": "The first direction is motivated by the linguistic assumption that syntagmatic and domain (associative) relations are both crucial to represent sense distictions, while they are basically originated by very different phenomena.", "labels": [], "entities": []}, {"text": "Syntagmatic relations hold among words that are typically located close to each other in the same sentence in a given temporal order, while domain relations hold among words that are typically used in the same semantic domain (i.e. in texts having similar topics ( ).", "labels": [], "entities": []}, {"text": "Their different nature suggests to adopt different learning strategies to detect them.", "labels": [], "entities": []}, {"text": "Regarding the second direction, external knowledge would be required to help WSD algorithms to better generalize over the data available for training.", "labels": [], "entities": [{"text": "WSD", "start_pos": 77, "end_pos": 80, "type": "TASK", "confidence": 0.9643813967704773}]}, {"text": "On the other hand, most of the state-of-the-art supervised approaches to WSD are still completely based on \"internal\" information only (i.e. the only information available to the training algorithm is the set of manually annotated examples).", "labels": [], "entities": [{"text": "WSD", "start_pos": 73, "end_pos": 76, "type": "TASK", "confidence": 0.9803144931793213}]}, {"text": "For example, in the Senseval-3 evaluation exercise) many lexical sample tasks were provided, beyond the usual labeled training data, with a large set of unlabeled data.", "labels": [], "entities": [{"text": "Senseval-3 evaluation exercise", "start_pos": 20, "end_pos": 50, "type": "TASK", "confidence": 0.749518613020579}]}, {"text": "However, at our knowledge, none of the participants exploited this unlabeled material.", "labels": [], "entities": []}, {"text": "Exploring this direction is the main focus of this paper.", "labels": [], "entities": []}, {"text": "In particular we acquire a Domain Model (DM) for the lexicon (i.e. a lexical resource representing domain associations among terms), and we exploit this information inside our supervised WSD algorithm.", "labels": [], "entities": []}, {"text": "DMs can be automatically induced from unlabeled corpora, allowing the portability of the methodology among languages.", "labels": [], "entities": []}, {"text": "We identified kernel methods as a viable framework in which to implement the assumptions above ( ).", "labels": [], "entities": []}, {"text": "Exploiting the properties of kernels, we have defined independently a set of domain and syntagmatic kernels and we combined them in order to define a complete kernel for WSD.", "labels": [], "entities": [{"text": "WSD", "start_pos": 170, "end_pos": 173, "type": "DATASET", "confidence": 0.6226791143417358}]}, {"text": "The domain kernels estimate the (domain) similarity) among contexts, while the syntagmatic kernels evaluate the similarity among collocations.", "labels": [], "entities": []}, {"text": "We will demonstrate that using DMs induced from unlabeled corpora is a feasible strategy to increase the generalization capability of the WSD algorithm.", "labels": [], "entities": [{"text": "WSD", "start_pos": 138, "end_pos": 141, "type": "TASK", "confidence": 0.8081727027893066}]}, {"text": "Our system far outperforms the state-ofthe-art systems in all the tasks in which it has been tested.", "labels": [], "entities": []}, {"text": "Moreover, a comparative analysis of the learning curves shows that the use of DMs allows us to remarkably reduce the amount of sense-tagged examples, opening new scenarios to develop systems for all-words tasks with minimal supervision.", "labels": [], "entities": []}, {"text": "The paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 introduces the notion of Domain Model.", "labels": [], "entities": []}, {"text": "In particular an automatic acquisition technique based on Latent Semantic Analysis (LSA) is described.", "labels": [], "entities": [{"text": "automatic acquisition", "start_pos": 17, "end_pos": 38, "type": "TASK", "confidence": 0.5992379486560822}]}, {"text": "In Section 3 we present a WSD system based on a combination of kernels.", "labels": [], "entities": [{"text": "WSD", "start_pos": 26, "end_pos": 29, "type": "TASK", "confidence": 0.9437782764434814}]}, {"text": "In particular we define a Domain Kernel (see Section 3.1) and a Syntagmatic Kernel (see Section 3.2), to model separately syntagmatic and domain aspects.", "labels": [], "entities": []}, {"text": "In Section 4 our WSD system is evaluated in the Senseval-3 English, Italian, Spanish and Catalan lexical sample tasks.", "labels": [], "entities": [{"text": "WSD", "start_pos": 17, "end_pos": 20, "type": "TASK", "confidence": 0.8533670902252197}]}], "datasetContent": [{"text": "In this section we present the performance of our kernel-based algorithms for WSD.", "labels": [], "entities": [{"text": "WSD", "start_pos": 78, "end_pos": 81, "type": "TASK", "confidence": 0.8427323698997498}]}, {"text": "The objectives of these experiments are: \u2022 to study the combination of different kernels, \u2022 to understand the benefits of plugging external information using domain models, \u2022 to verify the portability of our methodology among different languages.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3. The results  show that combining kernels significantly improves  the performance of the system.", "labels": [], "entities": []}, {"text": " Table 4: Comparative evaluation on the lexical sample tasks. Columns report: the Most Frequent baseline,  the inter annotator agreement, the F1 of the best system at Senseval-3, the F1 of K wsd , the F1 of K \ud97b\udf59  wsd ,  DM+ (the improvement due to DM, i.e. K \ud97b\udf59  wsd \u2212 K wsd ).", "labels": [], "entities": [{"text": "Frequent", "start_pos": 87, "end_pos": 95, "type": "METRIC", "confidence": 0.9393407702445984}, {"text": "F1", "start_pos": 142, "end_pos": 144, "type": "METRIC", "confidence": 0.9983465671539307}, {"text": "F1", "start_pos": 183, "end_pos": 185, "type": "METRIC", "confidence": 0.9980143308639526}, {"text": "F1", "start_pos": 201, "end_pos": 203, "type": "METRIC", "confidence": 0.9977128505706787}]}]}