{"title": [{"text": "Discriminative Syntactic Language Modeling for Speech Recognition", "labels": [], "entities": [{"text": "Discriminative Syntactic Language Modeling", "start_pos": 0, "end_pos": 42, "type": "TASK", "confidence": 0.685871385037899}, {"text": "Speech Recognition", "start_pos": 47, "end_pos": 65, "type": "TASK", "confidence": 0.7369575053453445}]}], "abstractContent": [{"text": "We describe a method for discriminative training of a language model that makes use of syntactic features.", "labels": [], "entities": []}, {"text": "We follow a reranking approach, where a baseline recogniser is used to produce 1000-best output for each acoustic input, and a second \"reranking\" model is then used to choose an utterance from these 1000-best lists.", "labels": [], "entities": []}, {"text": "The reranking model makes use of syntactic features together with a parameter estimation method that is based on the perceptron algorithm.", "labels": [], "entities": []}, {"text": "We describe experiments on the Switchboard speech recognition task.", "labels": [], "entities": [{"text": "Switchboard speech recognition task", "start_pos": 31, "end_pos": 66, "type": "TASK", "confidence": 0.8975226134061813}]}, {"text": "The syntactic features provide an additional 0.3% reduction in test-set error rate beyond the model of (Roark et al., 2004a; Roark et al., 2004b) (signifi-cant at p < 0.001), which makes use of a discriminatively trained n-gram model, giving a total reduction of 1.2% over the baseline Switchboard system.", "labels": [], "entities": [{"text": "error rate", "start_pos": 72, "end_pos": 82, "type": "METRIC", "confidence": 0.8810587525367737}]}], "introductionContent": [{"text": "The predominant approach within language modeling for speech recognition has been to use an ngram language model, within the \"source-channel\" or \"noisy-channel\" paradigm.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 54, "end_pos": 72, "type": "TASK", "confidence": 0.7336868643760681}]}, {"text": "The language model assigns a probability Pl (w) to each string win the language; the acoustic model assigns a conditional probability Pa (a|w) to each pair (a, w) where a is a sequence of acoustic vectors, and w is a string.", "labels": [], "entities": []}, {"text": "For a given acoustic input a, the highest scoring string under the model is w * = arg max w (\u03b2 log Pl (w) + log Pa (a|w)) where \u03b2 > 0 is some value that reflects the relative importance of the language model; \u03b2 is typically chosen by optimization on held-out data.", "labels": [], "entities": []}, {"text": "In an n-gram language model, a Markov assumption is made, namely that each word depends only on the previous (n \u2212 1) words.", "labels": [], "entities": []}, {"text": "The parameters of the language model are usually estimated from a large quantity of text data.", "labels": [], "entities": []}, {"text": "See) for an overview of estimation techniques for n-gram models.", "labels": [], "entities": []}, {"text": "This paper describes a method for incorporating syntactic features into the language model, using discriminative parameter estimation techniques.", "labels": [], "entities": []}, {"text": "We build on the work in, which was summarized and extended in.", "labels": [], "entities": []}, {"text": "These papers used discriminative methods for n-gram language models.", "labels": [], "entities": []}, {"text": "Our approach reranks the 1000-best output from the Switchboard recognizer of.", "labels": [], "entities": [{"text": "Switchboard recognizer", "start_pos": 51, "end_pos": 73, "type": "TASK", "confidence": 0.6511382311582565}]}, {"text": "Each candidate string w is parsed using the statistical parser of to give a parse tree T (w).", "labels": [], "entities": []}, {"text": "Information from the parse tree is incorporated in the model using a feature-vector approach: we define \u03a6(a, w) to be a d-dimensional feature vector which in principle could track arbitrary features of the string w together with the acoustic input a.", "labels": [], "entities": []}, {"text": "In this paper we restrict \u03a6(a, w) to only consider the string w and/or the parse tree T (w) for w.", "labels": [], "entities": []}, {"text": "For example, \u03a6(a, w) might track counts of context-free rule productions in T (w), or bigram lexical dependencies within T (w).", "labels": [], "entities": []}, {"text": "The optimal string under our new model is defined as w * = arg max w (\u03b2 log Pl (w) + \ud97b\udf59 \u00af \u03b1, \u03a6(a, w)\ud97b\udf59+ log Pa(a|w)) where the arg max is taken overall strings in the 1000-best list, and where \u00af \u03b1 \u2208 Rd is a parameter vector specifying the \"weight\" for each feature in \u03a6 (note that we define \ud97b\udf59x, y\ud97b\udf59 to be the inner, or dot product, between vectors x and y).", "labels": [], "entities": []}, {"text": "For this paper, we train the parameter vector \u00af \u03b1 using the perceptron algorithm).", "labels": [], "entities": []}, {"text": "The perceptron algorithm is a very fast training method, in practice requiring only a few passes over the training set, allowing fora detailed comparison of a wide variety of feature sets.", "labels": [], "entities": []}, {"text": "A number of researchers have described work that incorporates syntactic language models into a speech recognizer.", "labels": [], "entities": [{"text": "speech recognizer", "start_pos": 95, "end_pos": 112, "type": "TASK", "confidence": 0.691390335559845}]}, {"text": "These methods have almost exclusively worked within the noisy channel paradigm, where the syntactic language model has the task of modeling a distribution over strings in the language, in a very similar way to traditional n-gram language models.", "labels": [], "entities": []}, {"text": "The Structured Language Model () makes use of an incremental shift-reduce parser to enable the probability of words to be conditioned on k previous c-commanding lexical heads, rather than simply on the previous k words.", "labels": [], "entities": []}, {"text": "Incremental topdown and left-corner parsing) and head-driven parsing) approaches have directly used generative PCFG models as language models.", "labels": [], "entities": [{"text": "head-driven parsing", "start_pos": 49, "end_pos": 68, "type": "TASK", "confidence": 0.6536613404750824}]}, {"text": "In the work of Wen Wang and Mary Harper (), a constraint dependency grammar and a finite-state tagging model derived from that grammar were used to exploit syntactic dependencies.", "labels": [], "entities": []}, {"text": "Our approach differs from previous work in a couple of important respects.", "labels": [], "entities": []}, {"text": "First, through the featurevector representations \u03a6(a, w) we can essentially incorporate arbitrary sources of information from the string or parse tree into the model.", "labels": [], "entities": []}, {"text": "We would argue that our method allows considerably more flexibility in terms of the choice of features in the model; in previous work features were incorporated in the model through modification of the underlying generative parsing or tagging model, and modifying a generative model is a rather indirect way of changing the features used by a model.", "labels": [], "entities": [{"text": "generative parsing or tagging", "start_pos": 213, "end_pos": 242, "type": "TASK", "confidence": 0.8364887088537216}]}, {"text": "In this respect, our approach is similar to that advocated in, which used Maximum Entropy modeling to allow for the use of shallow syntactic features for language modeling.", "labels": [], "entities": []}, {"text": "A second contrast between our work and previous work, including that of, is in the use of discriminative parameter estimation techniques.", "labels": [], "entities": [{"text": "discriminative parameter estimation", "start_pos": 90, "end_pos": 125, "type": "TASK", "confidence": 0.6642138560612997}]}, {"text": "The criterion we use to optimize the parameter vector \u00af \u03b1 is closely related to the end goal in speech recognition, i.e., word error rate.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 96, "end_pos": 114, "type": "TASK", "confidence": 0.7492838203907013}]}, {"text": "Previous work has shown that discriminative methods within an ngram approach can lead to significant reductions in WER, in spite of the features being of the same type as the original language model.", "labels": [], "entities": [{"text": "WER", "start_pos": 115, "end_pos": 118, "type": "METRIC", "confidence": 0.5732256174087524}]}, {"text": "In this paper we extend this approach, by including syntactic features that were not in the baseline speech recognizer.", "labels": [], "entities": []}, {"text": "This paper describe experiments using a variety of syntactic features within this approach.", "labels": [], "entities": []}, {"text": "We tested the model on the Switchboard (SWB) domain, using the recognizer of.", "labels": [], "entities": []}, {"text": "The discriminative approach for n-gram modeling gave a 0.9% reduction in WER on this domain; the syntactic features we describe give a further 0.3% reduction.", "labels": [], "entities": [{"text": "WER", "start_pos": 73, "end_pos": 76, "type": "METRIC", "confidence": 0.9949139356613159}]}, {"text": "In the remainder of this paper, section 2 describes previous work, including the parameter estimation methods we use, and section 3 describes the featurevector representations of parse trees that we used in our experiments.", "labels": [], "entities": []}, {"text": "Section 4 describes experiments using the approach.", "labels": [], "entities": []}], "datasetContent": [{"text": "The experimental set-up we use is very similar to that of, and the extensions to that work in.", "labels": [], "entities": []}, {"text": "We make use of the Rich Transcription 2002 evaluation test set (rt02) as our development set, and use the Rich Transcription 2003 Spring evaluation CTS test set (rt03) as test set.", "labels": [], "entities": [{"text": "Rich Transcription 2002 evaluation test set (rt02)", "start_pos": 19, "end_pos": 69, "type": "DATASET", "confidence": 0.9591478837860955}, {"text": "Rich Transcription 2003 Spring evaluation CTS test set (rt03", "start_pos": 106, "end_pos": 166, "type": "DATASET", "confidence": 0.9548743903636933}]}, {"text": "The rt02 set consists of 6081 sentences (63804 words) and has three subsets: Switchboard 1, Switchboard 2, Switchboard Cellular.", "labels": [], "entities": [{"text": "rt02 set", "start_pos": 4, "end_pos": 12, "type": "DATASET", "confidence": 0.777354508638382}]}, {"text": "The rt03 set consists of 9050 sentences (76083 words) and has two subsets: Switchboard and Fisher.", "labels": [], "entities": [{"text": "rt03 set", "start_pos": 4, "end_pos": 12, "type": "DATASET", "confidence": 0.7733074724674225}]}, {"text": "The training set consists of 297580 transcribed utterances (3297579 words) . For each utterance, a weighted word-lattice was produced, representing alternative transcriptions, from the ASR system.", "labels": [], "entities": []}, {"text": "The baseline ASR system that we are comparing against then performed a rescoring pass on these first pass lattices, allowing for better silence modeling, and replaces the trigram language model score with a 6-gram model.", "labels": [], "entities": []}, {"text": "1000-best lists were then extracted from these lattices.", "labels": [], "entities": []}, {"text": "For each candidate in the 1000-best lists, we identified the number of edits (insertions, deletions or substitutions) for that candidate, relative to the \"target\" transcribed utterance.", "labels": [], "entities": []}, {"text": "The oracle score for the 1000-best lists was 16.7%.", "labels": [], "entities": []}, {"text": "To produce the word-lattices, each training utterance was processed by the baseline ASR system.", "labels": [], "entities": []}, {"text": "Ina naive approach, we would simply train the baseline system (i.e., an acoustic model and language model) on the entire training set, and then decode the training utterances with this system to produce lattices.", "labels": [], "entities": []}, {"text": "We would then use these lattices with the perceptron algorithm.", "labels": [], "entities": []}, {"text": "Unfortunately, this approach is likely to produce a set of training lattices that are very different from test lattices, in that they will have very low word-error rates, given that the lattice for each utterance was produced by a model that was trained on that utterance.", "labels": [], "entities": []}, {"text": "To somewhat control for this, the training set was partitioned into 28 sets, and baseline Katz backoff trigram models were built for each set by including only transcripts from the other 27 sets.", "labels": [], "entities": []}, {"text": "Lattices for each utterance were produced with an acoustic model that had been trained on the entire training set, but with a language model that was trained on the 27 data portions that did not include the current utterance.", "labels": [], "entities": []}, {"text": "Since language models are generally far more prone to overtraining than standard acoustic models, this goes along way toward making the training conditions similar to testing conditions.", "labels": [], "entities": []}, {"text": "Similar procedures were used to train the parsing and tagging models for the training set, since the Switchboard treebank overlaps extensively with the ASR training utterances.", "labels": [], "entities": [{"text": "parsing and tagging", "start_pos": 42, "end_pos": 61, "type": "TASK", "confidence": 0.743492841720581}, {"text": "Switchboard treebank", "start_pos": 101, "end_pos": 121, "type": "DATASET", "confidence": 0.7534313201904297}, {"text": "ASR training utterances", "start_pos": 152, "end_pos": 175, "type": "TASK", "confidence": 0.7814120848973592}]}, {"text": "presents the word-error rates on rt02 and rt03 of the baseline ASR system, 1000-best perceptron and GCLM results from under this condition, and our 1000-best perceptron results.", "labels": [], "entities": [{"text": "ASR", "start_pos": 63, "end_pos": 66, "type": "TASK", "confidence": 0.8764869570732117}, {"text": "GCLM", "start_pos": 100, "end_pos": 104, "type": "METRIC", "confidence": 0.9472660422325134}]}, {"text": "Note that our n-best result, using just ngram features, improves upon the perceptron result of: Use of POS-tag sequence derived features condition.", "labels": [], "entities": []}, {"text": "(Note that the perceptron-trained n-gram features were trigrams (i.e., n = 3).)", "labels": [], "entities": []}, {"text": "This is due to a larger training set being used in our experiments; we have added data that was used as held-out data in) to the training set that we use.", "labels": [], "entities": []}, {"text": "The first additional features that we experimented with were POS-tag sequence derived features.", "labels": [], "entities": []}, {"text": "Let ti and w i be the POS tag and word at position i, respectively.", "labels": [], "entities": []}, {"text": "We experimented with the following three feature definitions: summarizes the results of these trials on the held outset.", "labels": [], "entities": []}, {"text": "Using the simple features (number 1 above) yielded an improvement beyond just n-grams, but additional, more complicated features failed to yield additional improvements.", "labels": [], "entities": []}, {"text": "Next, we considered features derived from shallow parsing sequences.", "labels": [], "entities": []}, {"text": "Given the results from the POS-tag sequence derived features, for any given sequence, we simply use n-tag and tag/word features (number 1 above).", "labels": [], "entities": []}, {"text": "The first sequence type from which we extracted features was the shallow parse tag sequence (S1), as shown in.", "labels": [], "entities": []}, {"text": "Next, we tried the composite shallow/POS tag sequence (S2), as in.", "labels": [], "entities": []}, {"text": "Finally, we tried extracting features from the shallow constituent sequence (S3), as shown in.", "labels": [], "entities": []}, {"text": "When EDITED and  INTJ nodes are ignored, we refer to this condition as S3-E.", "labels": [], "entities": []}, {"text": "For full-parse feature extraction, we tried context-free rule features (CF) and head-to-head features (H2H), of the kind shown in table 1.", "labels": [], "entities": [{"text": "full-parse feature extraction", "start_pos": 4, "end_pos": 33, "type": "TASK", "confidence": 0.6814001897970835}]}, {"text": "shows the results of these trials on rt02.", "labels": [], "entities": []}, {"text": "Although the single digit precision in the table does not show it, the H2H trial, using features extracted from the full parses along with n-grams and POS-tag sequence features, was the best performing model on the held out data, so we selected it for application to the rt03 test data.", "labels": [], "entities": [{"text": "precision", "start_pos": 26, "end_pos": 35, "type": "METRIC", "confidence": 0.962011992931366}, {"text": "rt03 test data", "start_pos": 271, "end_pos": 285, "type": "DATASET", "confidence": 0.9345201452573141}]}, {"text": "This yielded 35.2% WER, a reduction of 0.3% absolute over what was achieved with just n-grams, which is significant at p < 0.001, 5 reaching a total reduction of 1.2% over the baseline recognizer.", "labels": [], "entities": [{"text": "WER", "start_pos": 19, "end_pos": 22, "type": "METRIC", "confidence": 0.998822033405304}]}], "tableCaptions": [{"text": " Table 2: Baseline word-error rates versus Roark et al. (2005)", "labels": [], "entities": []}, {"text": " Table 3: Use of POS-tag sequence derived features", "labels": [], "entities": []}, {"text": " Table 4: Use of shallow parse sequence and full parse derived", "labels": [], "entities": []}]}