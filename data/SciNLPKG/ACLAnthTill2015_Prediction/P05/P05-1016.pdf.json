{"title": [], "abstractContent": [{"text": "In this paper, we present an unsupervised methodology for propagating lexical co-occurrence vectors into an ontology such as WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 125, "end_pos": 132, "type": "DATASET", "confidence": 0.9675163626670837}]}, {"text": "We evaluate the framework on the task of automatically attaching new concepts into the ontology.", "labels": [], "entities": []}, {"text": "Experimental results show 73.9% attachment accuracy in the first position and 81.3% accuracy in the top-5 positions.", "labels": [], "entities": [{"text": "attachment", "start_pos": 32, "end_pos": 42, "type": "METRIC", "confidence": 0.7290577292442322}, {"text": "accuracy", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.8987452983856201}, {"text": "accuracy", "start_pos": 84, "end_pos": 92, "type": "METRIC", "confidence": 0.9992715716362}]}, {"text": "This framework could potentially serve as a foundation for on-tologizing lexical-semantic resources and assist the development of other large-scale and internally consistent collections of semantic information.", "labels": [], "entities": []}], "introductionContent": [{"text": "Despite considerable effort, there is still today no commonly accepted semantic corpus, semantic framework, notation, or even agreement on precisely which aspects of semantics are most useful (if at all).", "labels": [], "entities": []}, {"text": "We believe that one important reason for this rather startling fact is the absence of truly wide-coverage semantic resources.", "labels": [], "entities": []}, {"text": "Recognizing this, some recent work on wide coverage term banks, like WordNet and CYC, and annotated corpora, like FrameNet (), and Nombank (), seeks to address the problem.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 69, "end_pos": 76, "type": "DATASET", "confidence": 0.9685210585594177}, {"text": "CYC", "start_pos": 81, "end_pos": 84, "type": "DATASET", "confidence": 0.5570148229598999}]}, {"text": "But manual efforts such as these suffer from two drawbacks: they are difficult to tailor to new domains, and they have internal inconsistencies that can make automating the acquisition process difficult.", "labels": [], "entities": []}, {"text": "In this work, we introduce a general framework for inducing co-occurrence feature vectors for nodes in a WordNet-like ontology.", "labels": [], "entities": []}, {"text": "We believe that this framework will be useful fora variety of applications, including adding additional semantic information to existing semantic term banks by disambiguating lexical-semantic resources.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we provide a quantitative and qualitative evaluation of our framework.", "labels": [], "entities": []}, {"text": "We used Minipar (Lin 1994), abroad coverage parser, to parse two 3GB corpora.", "labels": [], "entities": [{"text": "Minipar (Lin 1994)", "start_pos": 8, "end_pos": 26, "type": "DATASET", "confidence": 0.8266383647918701}, {"text": "abroad coverage parser", "start_pos": 28, "end_pos": 50, "type": "TASK", "confidence": 0.48262648781140643}]}, {"text": "We collected the frequency counts of the grammatical relations (contexts) output by Minipar and used these to construct the lexical feature vectors as described in Section 3.", "labels": [], "entities": []}, {"text": "WordNet 2.0 served as our testing ontology.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9635737538337708}]}, {"text": "Using the algorithm presented in Section 4, we induced ontological feature vectors for the noun nodes in WordNet using the lexical co-occurrence features from the TREC-2002 corpus.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 105, "end_pos": 112, "type": "DATASET", "confidence": 0.9568849205970764}, {"text": "TREC-2002 corpus", "start_pos": 163, "end_pos": 179, "type": "DATASET", "confidence": 0.9477827250957489}]}, {"text": "Due to memory limitations, we were only able to propagate features to one quarter of the ontology.", "labels": [], "entities": []}, {"text": "We experimented with both the Shared and Committee propagation models described in Section 4.1.", "labels": [], "entities": [{"text": "Shared and Committee propagation", "start_pos": 30, "end_pos": 62, "type": "TASK", "confidence": 0.5916766300797462}]}, {"text": "To evaluate the resulting ontological feature vectors, we considered the task of attaching new nodes into the ontology.", "labels": [], "entities": []}, {"text": "To automatically evaluate this, we randomly extracted a set of 1000 noun leaf nodes from the ontology and accumulated lexical feature vectors for them using the TREC-9 corpus (a separate corpus than the one used to propagate features, but of the same genre).", "labels": [], "entities": [{"text": "TREC-9 corpus", "start_pos": 161, "end_pos": 174, "type": "DATASET", "confidence": 0.9132468402385712}]}, {"text": "We experimented with two test sets: \u2022 Full: The 424 of the 1000 random nodes that existed in the TREC-9 corpus \u2022 Subset: Subset of Full where only nodes that do not have concept siblings are kept (380 nodes).", "labels": [], "entities": [{"text": "TREC-9 corpus", "start_pos": 97, "end_pos": 110, "type": "DATASET", "confidence": 0.9128389358520508}]}, {"text": "For each random node, we computed the similarity of the node with each concept node in the ontology by computing the cosine of the angle between the lexical feature vector of the random node e i and the ontological feature vector of the concept nodes e j : We only kept those similar nodes that had a similarity above a threshold \u03c3.", "labels": [], "entities": []}, {"text": "We experimentally set \u03c3 = 0.1.", "labels": [], "entities": []}], "tableCaptions": []}