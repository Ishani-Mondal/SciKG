{"title": [{"text": "Scaling Conditional Random Fields Using Error-Correcting Codes", "labels": [], "entities": [{"text": "Scaling Conditional Random Fields", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.8484751135110855}]}], "abstractContent": [{"text": "Conditional Random Fields (CRFs) have been applied with considerable success to a number of natural language processing tasks.", "labels": [], "entities": []}, {"text": "However, these tasks have mostly involved very small label sets.", "labels": [], "entities": []}, {"text": "When deployed on tasks with larger label sets, the requirements for computational resources mean that training becomes intractable.", "labels": [], "entities": []}, {"text": "This paper describes a method for training CRFs on such tasks, using error correcting output codes (ECOC).", "labels": [], "entities": []}, {"text": "A number of CRFs are independently trained on the separate binary labelling tasks of distinguishing between a subset of the labels and its complement.", "labels": [], "entities": []}, {"text": "During decoding, these models are combined to produce a predicted label sequence which is resilient to errors by individual models.", "labels": [], "entities": []}, {"text": "Error-correcting CRF training is much less resource intensive and has a much faster training time than a standardly formulated CRF, while decoding performance remains quite comparable.", "labels": [], "entities": []}, {"text": "This allows us to scale CRFs to previously impossible tasks, as demonstrated by our experiments with large label sets.", "labels": [], "entities": []}], "introductionContent": [{"text": "Conditional random fields (CRFs) () are probabilistic models for labelling sequential data.", "labels": [], "entities": []}, {"text": "CRFs are undirected graphical models that define a conditional distribution over label sequences given an observation sequence.", "labels": [], "entities": []}, {"text": "They allow the use of arbitrary, overlapping, non-independent features as a result of their global conditioning.", "labels": [], "entities": []}, {"text": "This allows us to avoid making unwarranted independence assumptions over the observation sequence, such as those required by typical generative models.", "labels": [], "entities": []}, {"text": "Efficient inference and training methods exist when the graphical structure of the model forms a chain, where each position in a sequence is connected to its adjacent positions.", "labels": [], "entities": []}, {"text": "CRFs have been applied with impressive empirical results to the tasks of named entity recognition, simplified part-of-speech (POS) tagging (), noun phrase chunking) and extraction of tabular data (), among other tasks.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 73, "end_pos": 97, "type": "TASK", "confidence": 0.6351000269254049}, {"text": "simplified part-of-speech (POS) tagging", "start_pos": 99, "end_pos": 138, "type": "TASK", "confidence": 0.5948311140139898}, {"text": "noun phrase chunking", "start_pos": 143, "end_pos": 163, "type": "TASK", "confidence": 0.7330858111381531}, {"text": "extraction of tabular data", "start_pos": 169, "end_pos": 195, "type": "TASK", "confidence": 0.8476060926914215}]}, {"text": "CRFs are usually estimated using gradient-based methods such as limited memory variable metric (LMVM).", "labels": [], "entities": []}, {"text": "However, even with these efficient methods, training can be slow.", "labels": [], "entities": []}, {"text": "Consequently, most of the tasks to which CRFs have been applied are relatively small scale, having only a small number of training examples and small label sets.", "labels": [], "entities": [{"text": "CRFs", "start_pos": 41, "end_pos": 45, "type": "TASK", "confidence": 0.9658660888671875}]}, {"text": "For much larger tasks, with hundreds of labels and millions of examples, current training methods prove intractable.", "labels": [], "entities": []}, {"text": "Although training can potentially be parallelised and thus run more quickly on large clusters of computers, this in itself is not a solution to the problem: tasks can reasonably be expected to increase in size and complexity much faster than any increase in computing power.", "labels": [], "entities": []}, {"text": "In order to provide scalability, the factors which most affect the resource usage and runtime of the training method must be addressed directly -ideally the dependence on the number of labels should be reduced.", "labels": [], "entities": []}, {"text": "This paper presents an approach which enables CRFs to be used on larger tasks, with a significant reduction in the time and resources needed for training.", "labels": [], "entities": [{"text": "CRFs", "start_pos": 46, "end_pos": 50, "type": "TASK", "confidence": 0.9406009912490845}]}, {"text": "This reduction does not come at the cost of performance -the results obtained on benchmark natural language problems compare favourably, and sometimes exceed, the results produced from regular CRF training.", "labels": [], "entities": []}, {"text": "Error correcting output codes (ECOC) are used to train a community of CRFs on binary tasks, with each discriminating between a subset of the labels and its complement.", "labels": [], "entities": [{"text": "Error correcting output codes (ECOC)", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.7688495303903308}]}, {"text": "Inference is performed by applying these 'weak' models to an unknown example, with each component model removing some ambiguity when predicting the label sequence.", "labels": [], "entities": []}, {"text": "Given a sufficient number of binary models predicting suitably diverse label subsets, the label sequence can be inferred while being robust to a number of individual errors from the weak models.", "labels": [], "entities": []}, {"text": "As each of these weak models are binary, individually they can be efficiently trained, even on large problems.", "labels": [], "entities": []}, {"text": "The number of weak learners required to achieve good performance is shown to be relatively small on practical tasks, such that the overall complexity of error-correcting CRF training is found to be much less than that of regular CRF training methods.", "labels": [], "entities": []}, {"text": "We have evaluated the error-correcting CRF on the CoNLL 2003 named entity recognition (NER) task, where we show that the method yields similar generalisation performance to standardly formulated CRFs, while requiring only a fraction of the resources, and no increase in training time.", "labels": [], "entities": [{"text": "CoNLL 2003 named entity recognition (NER) task", "start_pos": 50, "end_pos": 96, "type": "TASK", "confidence": 0.8890069060855441}]}, {"text": "We have also shown how the errorcorrecting CRF scales when applied to the larger task of POS tagging the Penn Treebank and also the even larger task of simultaneously noun phrase chunking (NPC) and POS tagging using the CoNLL 2000 data-set).", "labels": [], "entities": [{"text": "errorcorrecting CRF", "start_pos": 27, "end_pos": 46, "type": "METRIC", "confidence": 0.8452984094619751}, {"text": "POS tagging", "start_pos": 89, "end_pos": 100, "type": "TASK", "confidence": 0.784037709236145}, {"text": "Penn Treebank", "start_pos": 105, "end_pos": 118, "type": "DATASET", "confidence": 0.9854187667369843}, {"text": "noun phrase chunking (NPC)", "start_pos": 167, "end_pos": 193, "type": "TASK", "confidence": 0.7607015470663706}, {"text": "POS tagging", "start_pos": 198, "end_pos": 209, "type": "TASK", "confidence": 0.7786339819431305}, {"text": "CoNLL 2000 data-set", "start_pos": 220, "end_pos": 239, "type": "DATASET", "confidence": 0.9705145359039307}]}], "datasetContent": [{"text": "Our experiments show that error-correcting CRFs are highly accurate on benchmark problems with small label sets, as well as on larger problems with many more labels, which would be otherwise prove intractable for traditional CRFs.", "labels": [], "entities": []}, {"text": "Moreover, with a good code, the time and resources required for training and decoding can be much less than that of the standardly formulated CRF.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: F 1 scores on NER task.", "labels": [], "entities": [{"text": "F 1", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9800726175308228}, {"text": "NER task", "start_pos": 24, "end_pos": 32, "type": "TASK", "confidence": 0.6021475046873093}]}, {"text": " Table 2: POS tagging accuracy.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.7687589228153229}, {"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9871765971183777}]}]}