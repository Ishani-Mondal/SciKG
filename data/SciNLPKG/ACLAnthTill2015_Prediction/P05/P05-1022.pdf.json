{"title": [{"text": "Coarse-to-fine n-best parsing and MaxEnt discriminative reranking", "labels": [], "entities": [{"text": "Coarse-to-fine n-best parsing", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.4241291383902232}, {"text": "MaxEnt discriminative reranking", "start_pos": 34, "end_pos": 65, "type": "METRIC", "confidence": 0.8154292503992716}]}], "abstractContent": [{"text": "Discriminative reranking is one method for constructing high-performance statistical parsers (Collins, 2000).", "labels": [], "entities": [{"text": "Collins, 2000)", "start_pos": 94, "end_pos": 108, "type": "DATASET", "confidence": 0.9196394085884094}]}, {"text": "A discrim-inative reranker requires a source of candidate parses for each sentence.", "labels": [], "entities": []}, {"text": "This paper describes a simple yet novel method for constructing sets of 50-best parses based on a coarse-to-fine generative parser (Charniak, 2000).", "labels": [], "entities": []}, {"text": "This method generates 50-best lists that are of substantially higher quality than previously obtainable.", "labels": [], "entities": []}, {"text": "We used these parses as the input to a MaxEnt reranker (Johnson et al., 1999; Riezler et al., 2002) that selects the best parse from the set of parses for each sentence , obtaining an f-score of 91.0% on sentences of length 100 or less.", "labels": [], "entities": [{"text": "f-score", "start_pos": 184, "end_pos": 191, "type": "METRIC", "confidence": 0.9686180353164673}]}], "introductionContent": [{"text": "We describe a reranking parser which uses a regularized MaxEnt reranker to select the best parse from the 50-best parses returned by a generative parsing model.", "labels": [], "entities": []}, {"text": "The 50-best parser is a probabilistic parser that on its own produces high quality parses; the maximum probability parse trees (according to the parser's model) have an f -score of 0.897 on section 23 of the Penn Treebank, which is still state-of-the-art.", "labels": [], "entities": [{"text": "f -score", "start_pos": 169, "end_pos": 177, "type": "METRIC", "confidence": 0.9589742620786031}, {"text": "Penn Treebank", "start_pos": 208, "end_pos": 221, "type": "DATASET", "confidence": 0.9930236637592316}]}, {"text": "However, the 50 best (i.e., the 50 highest probability) parses of a sentence often contain considerably better parses (in terms off -score); this paper describes a 50-best parsing algorithm with an oracle f -score of 96.8 on the same data.", "labels": [], "entities": []}, {"text": "The reranker attempts to select the best parse fora sentence from the 50-best list of possible parses for the sentence.", "labels": [], "entities": []}, {"text": "Because the reranker only has to consider a relatively small number of parses per sentences, it is not necessary to use dynamic programming, which permits the features to be essentially arbitrary functions of the parse trees.", "labels": [], "entities": []}, {"text": "While our reranker does not achieve anything like the oracle f -score, the parses it selects do have an f -score of 91.0, which is considerably better than the maximum probability parses of the n-best parser.", "labels": [], "entities": [{"text": "f -score", "start_pos": 104, "end_pos": 112, "type": "METRIC", "confidence": 0.9490412274996439}]}, {"text": "In more detail, for each string s the n-best parsing algorithm described in section 2 returns then highest probability parses Y(s) = {y 1 (s), . .", "labels": [], "entities": []}, {"text": ", y n (s)} together with the probability p(y) of each parse y according to the parser's probability model.", "labels": [], "entities": []}, {"text": "The number n of parses was set to 50 for the experiments described here, but some simple sentences actually received fewer than 50 parses (so n is actually a function of s).", "labels": [], "entities": []}, {"text": "Each yield or terminal string in the training, development and test data sets is mapped to such an n-best list of parse/probability pairs; the cross-validation scheme described in was used to avoid training the n-best parser on the sentence it was being used to parse.", "labels": [], "entities": []}, {"text": "A feature extractor, described in section 3, is a vector of m functions f = (f 1 , . .", "labels": [], "entities": []}, {"text": ", f m ), where each f j maps a parse y to areal number f j (y), which is the value of the jth feature on y.", "labels": [], "entities": []}, {"text": "So a feature extractor maps each y to a vector of feature values f (y) = (f 1 (y), . .", "labels": [], "entities": []}, {"text": ", f m (y)).", "labels": [], "entities": []}, {"text": "Our reranking parser associates a parse with a score v \u03b8 (y), which is a linear function of the feature values f (y).", "labels": [], "entities": []}, {"text": "That is, each feature f j is associated with a weight \u03b8 j , and the feature values and weights define the score v \u03b8 (y) of each parse y as follows: Given a string s, the reranking parser's output\u02c6youtput\u02c6 output\u02c6y(s) on string sis the highest scoring parse in the n-best parses Y(s) for s, i.e., The feature weight vector \u03b8 is estimated from the labelled training corpus as described in section 4.", "labels": [], "entities": []}, {"text": "Because we use labelled training data we know the correct parse y (s) for each sentence sin the training data.", "labels": [], "entities": []}, {"text": "The correct parse y (s) is not always a member of the n-best parser's output Y(s), but we can identify the parses Y + (s) in Y(s) with the highest f -scores.", "labels": [], "entities": []}, {"text": "Informally, the estimation procedure finds a weight vector \u03b8 that maximizes the score v \u03b8 (y) of the parses y \u2208 Y + (s) relative to the scores of the other parses in Y(s), for each sin the training data.", "labels": [], "entities": []}, {"text": "2 Recovering the n-best parses using coarse-to-fine parsing The major difficulty in n-best parsing, compared to 1-best parsing, is dynamic programming.", "labels": [], "entities": []}, {"text": "For example, n-best parsing is straight-forward in best-first search or beam search approaches that do not use dynamic programming: to generate more than one parse, one simply allows the search mechanism to create successive versions to one's heart's content.", "labels": [], "entities": []}, {"text": "A good example of this is the Roark parser) which works left-to right through the sentence, and abjures dynamic programming in favor of abeam search, keeping some large number of possibilities to extend by adding the next word, and then re-pruning.", "labels": [], "entities": []}, {"text": "At the end one has a beam-width's number of best parses).", "labels": [], "entities": []}, {"text": "The Collins parser) does use dynamic programming in its search.", "labels": [], "entities": [{"text": "Collins", "start_pos": 4, "end_pos": 11, "type": "DATASET", "confidence": 0.9827802181243896}]}, {"text": "That is, whenever a constituent with the same history is generated a second time, it is discarded if its probability is lower than the original version.", "labels": [], "entities": []}, {"text": "If the opposite is true, then the original is discarded.", "labels": [], "entities": []}, {"text": "This is fine if one only wants the first-best, but obviously it does not directly enumerate the n-best parses.", "labels": [], "entities": []}, {"text": "However, Collins has created an nbest version of his parser by turning off dynamic programming (see the user's guide to Bikel's re-implementation of Collins' parser, http://www.cis.upenn.edu/ dbikel/software.html#stat-parser).", "labels": [], "entities": []}, {"text": "As with Roark's parser, it is necessary to add a beam-width constraint to make the search tractable.", "labels": [], "entities": []}, {"text": "With abeam width of 1000 the parser returns something like a 50-best list (Collins, personal communication), but the actual number of parses returned for each sentences varies.", "labels": [], "entities": []}, {"text": "However, turning off dynamic programming results in a loss in efficiency.", "labels": [], "entities": []}, {"text": "Indeed, Collins's n-best list of parses for section 24 of the Penn tree-bank has some sentences with only a single parse, because the n-best parser could not find any parses.", "labels": [], "entities": [{"text": "Collins's n-best list of parses for section 24 of the Penn tree-bank", "start_pos": 8, "end_pos": 76, "type": "DATASET", "confidence": 0.8719093341093797}]}, {"text": "Now there are two known ways to produce n-best parses while retaining the use of dynamic programming: the obvious way and the clever way.", "labels": [], "entities": []}, {"text": "The clever way is based upon an algorithm developed by.", "labels": [], "entities": []}, {"text": "Recall the key insight in the Viterbi algorithm: in the optimal parse the parsing decisions at each of the choice points that determine a parse must be optimal, since otherwise one could find a better parse.", "labels": [], "entities": []}, {"text": "This insight extends to n-best parsing as follows.", "labels": [], "entities": []}, {"text": "Consider the secondbest parse: if it is to differ from the best parse, then at least one of its parsing decisions must be suboptimal.", "labels": [], "entities": []}, {"text": "In fact, all but one of the parsing decisions in second-best parse must be optimal, and the one suboptimal decision must be the second-best choice at that choice point.", "labels": [], "entities": []}, {"text": "Further, the nth-best parse can only involve at most n suboptimal parsing decisions, and all but one of these must be involved in one of the second through then \u2212 1th-best parses.", "labels": [], "entities": []}, {"text": "Thus the basic idea behind this approach to n-best parsing is to first find the best parse, then find the second-best parse, then the third-best, and soon.", "labels": [], "entities": []}, {"text": "The algorithm was originally described for hidden Markov models.", "labels": [], "entities": []}, {"text": "Since this first draft of this paper we have become aware of two PCFG implementations of this algorithm).", "labels": [], "entities": []}, {"text": "The first was tried on relatively small grammars, while the second was implemented on top of the Bikel re-implementation of the Collins parser) and achieved oracle results for 50-best parses similar to those we report below.", "labels": [], "entities": []}, {"text": "Here, however, we describe how to find n-best parses in a more straight-forward fashion.", "labels": [], "entities": []}, {"text": "Rather than storing a single best parse of each edge, one stores n of them.", "labels": [], "entities": []}, {"text": "That is, when using dynamic programming, rather than throwing away a candidate if it scores less than the best, one keeps it if it is one of the top n analyses for this edge discovered so far.", "labels": [], "entities": []}, {"text": "This is really very straight-forward.", "labels": [], "entities": []}, {"text": "Dynamic programming parsing algorithms for PCFGs require O(m 2 ) dynamic programming states, where m is the length of the sentence, so an n-best parsing algorithm requires O(nm 2 ).", "labels": [], "entities": []}, {"text": "However things get much worse when the grammar is bilexicalized.", "labels": [], "entities": []}, {"text": "As shown by) the dynamic programming algorithms for bilexicalized PCFGs require O(m 3 ) states, so a n-best parser would require O(nm 3 ) states.", "labels": [], "entities": []}, {"text": "Things become worse still in a parser like the one described in because it conditions on (and hence splits the dynamic programming states according to) features of the grandparent node in addition to the parent, thus multiplying the number of possible dynamic programming states even more.", "labels": [], "entities": []}, {"text": "Thus nobody has implemented this version.", "labels": [], "entities": []}, {"text": "There is, however, one particular feature of the Charniak parser that mitigates the space problem: it is a \"coarse-to-fine\" parser.", "labels": [], "entities": []}, {"text": "By \"coarse-to-fine\" we mean that it first produces a crude version of the parse using coarse-grained dynamic programming states, and then builds fine-grained analyses by splitting the most promising of coarse-grained states.", "labels": [], "entities": []}, {"text": "A prime example of this idea is from Goodman (1997), who describes a method for producing a simple but crude approximate grammar of a standard context-free grammar.", "labels": [], "entities": []}, {"text": "He parses a sentence using the approximate grammar, and the results are used to constrain the search fora parse with the full CFG.", "labels": [], "entities": [{"text": "CFG", "start_pos": 126, "end_pos": 129, "type": "DATASET", "confidence": 0.9776861667633057}]}, {"text": "He finds that total parsing time is greatly reduced.", "labels": [], "entities": [{"text": "parsing", "start_pos": 20, "end_pos": 27, "type": "TASK", "confidence": 0.9028570055961609}]}, {"text": "A somewhat different take on this paradigm is seen in the parser we use in this paper.", "labels": [], "entities": []}, {"text": "Here the parser first creates a parse forest based upon a much less complex version of the complete grammar.", "labels": [], "entities": []}, {"text": "In particular, it only looks at standard CFG features, the parent and neighbor labels.", "labels": [], "entities": []}, {"text": "Because this grammar encodes relatively little state information, its dynamic programming states are relatively coarse and hence there are comparatively few of them, so it can be efficiently parsed using a standard dynamic programming bottom-up CFG parser.", "labels": [], "entities": []}, {"text": "However, precisely because this first stage uses a grammar that ignores many important contextual features, the best parse it finds will not, in general, be the best parse according to the finer-grained second-stage grammar, so clearly we do not want to perform best-first parsing with this grammar.", "labels": [], "entities": []}, {"text": "Instead, the output of the first stage is a polynomial-sized packed parse forest which records the left and right string positions for each local tree in the parses generated by this grammar.", "labels": [], "entities": []}, {"text": "The edges in the packed parse forest are then pruned, to focus attention on the coarsegrained states that are likely to correspond to highprobability fine-grained states.", "labels": [], "entities": []}, {"text": "The edges are then pruned according to their marginal probability conditioned on the string s being parsed as follows: Here n i j,k is a constituent of type i spanning the words from j to k, \u03b1(n i j,k ) is the outside probability of this constituent, and \u03b2(n i j,k ) is its inside probability.", "labels": [], "entities": []}, {"text": "From parse forest both \u03b1 and \u03b2 can be computed in time proportional to the size of the compact forest.", "labels": [], "entities": []}, {"text": "The parser then removes all constituents n i j,k whose probability falls below some preset threshold.", "labels": [], "entities": []}, {"text": "In the version of this parser available on the web, this threshold is on the order of 10 \u22124 . The unpruned edges are then exhaustively evaluated according to the fine-grained probabilistic model; in effect, each coarse-grained dynamic programming state is split into one or more fine-grained dynamic programming states.", "labels": [], "entities": []}, {"text": "As noted above, the fine-grained model conditions on information that is not available in the coarse-grained model.", "labels": [], "entities": []}, {"text": "This includes the lexical head of one's parents, the part of speech of this head, the parent's and grandparent's category labels, etc.", "labels": [], "entities": []}, {"text": "The fine-grained states investigated by the parser are constrained to be refinements of the coarse-grained states, which drastically reduces the number of fine-grained states that need to be investigated.", "labels": [], "entities": []}, {"text": "It is certainly possible to do dynamic programming parsing directly with the fine-grained grammar, but precisely because the fine-grained grammar conditions on a wide variety of non-local contextual information there would be a very large number of different dynamic programming states, so direct dynamic programming parsing with the fine-grained grammar would be very expensive in terms of time and memory.", "labels": [], "entities": [{"text": "dynamic programming parsing", "start_pos": 31, "end_pos": 58, "type": "TASK", "confidence": 0.7387048403422037}, {"text": "dynamic programming parsing", "start_pos": 297, "end_pos": 324, "type": "TASK", "confidence": 0.6316583851973215}]}, {"text": "As the second stage parse evaluates all the remaining constituents in all of the contexts in which they appear (e.g., what are the possible grand-parent labels) it keeps track of the most probable expansion of the constituent in that context, and at the end is able to start at the root and piece together the overall best parse.", "labels": [], "entities": []}, {"text": "Now comes the easy part.", "labels": [], "entities": []}, {"text": "To create a 50-best parser we simply change the fine-grained version of 1-best algorithm in accordance with the \"obvious\" scheme outlined earlier in this section.", "labels": [], "entities": []}, {"text": "The first, coarse-grained, pass is not changed, but the second, fine-grained, pass keeps the n-best possibilities at each dynamic programming state, rather than keeping just first best.", "labels": [], "entities": []}, {"text": "When combining two constituents to form a larger constituent, we keep the best 50 of the 2500 possibilities they offer.", "labels": [], "entities": []}, {"text": "Naturally, if we keep each 50-best list sorted, we do nothing like 2500 operations.", "labels": [], "entities": []}, {"text": "The experimental question is whether, in practice, the coarse-to-fine architecture keeps the number of dynamic programming states sufficiently low that space considerations do not defeat us.", "labels": [], "entities": []}, {"text": "The answer seems to be yes.", "labels": [], "entities": []}, {"text": "We ran the algorithm on section 24 of the Penn WSJ tree-bank using the default pruning settings mentioned above.", "labels": [], "entities": [{"text": "Penn WSJ tree-bank", "start_pos": 42, "end_pos": 60, "type": "DATASET", "confidence": 0.9343781272570292}]}, {"text": "shows how the number of fine-grained dynamic programming states increases as a function of sentence length for the sentences in section 24 of the Treebank.", "labels": [], "entities": []}, {"text": "There are no sentences of length greater than 69 in this section.", "labels": [], "entities": []}, {"text": "Columns two to four show the number of sentences in each bucket, their average length, and the average number of fine-grained dynamic programming structures per sentence.", "labels": [], "entities": []}, {"text": "The final column gives the value of the function 100 * L 1.5 where L is the average length of sentences in the bucket.", "labels": [], "entities": []}, {"text": "Except for bucket 6, which is abnormally low, it seems that this add-hoc function tracks the number of structures quite well.", "labels": [], "entities": []}, {"text": "Thus the number of dynamic programming states does not grow as L 2 , much less as L 3 . To put the number of these structures per sen-: Oracle f -score as a function of number n of n-best parses tence in perspective, consider the size of such structures.", "labels": [], "entities": [{"text": "Oracle f -score", "start_pos": 136, "end_pos": 151, "type": "METRIC", "confidence": 0.5735807716846466}]}, {"text": "Each one must contain a probability, the nonterminal label of the structure, and a vector of pointers to it's children (an average parent has slightly more than two children).", "labels": [], "entities": []}, {"text": "If one were concerned about every byte this could be made quite small.", "labels": [], "entities": []}, {"text": "In our implementation probably the biggest factor is the STL overhead on vectors.", "labels": [], "entities": [{"text": "STL overhead", "start_pos": 57, "end_pos": 69, "type": "METRIC", "confidence": 0.6795631498098373}]}, {"text": "If we figure we are using, say, 25 bytes per structure, the total space required is only 1.25Mb even for 50,000 dynamic programming states, so it is clearly not worth worrying about the memory required.", "labels": [], "entities": []}, {"text": "The resulting n-bests are quite good, as shown in.", "labels": [], "entities": []}, {"text": "(The results are for all sentences of section 23 of the WSJ tree-bank of length \u2264 100.)", "labels": [], "entities": [{"text": "WSJ tree-bank", "start_pos": 56, "end_pos": 69, "type": "DATASET", "confidence": 0.9714075326919556}]}, {"text": "From the 1-best result we see that the base accuracy of the parser is 89.7%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 44, "end_pos": 52, "type": "METRIC", "confidence": 0.7768200039863586}]}, {"text": "1 2-best and 10-best show dramatic oracle-rate improvements.", "labels": [], "entities": []}, {"text": "After that things start to slowdown, and we achieve an oracle rate of 0.968 at 50-best.", "labels": [], "entities": []}, {"text": "To put this in perspective, Roark) reports oracle results of 0.941 (with the same experimental setup) using his parser to return a variable number of parses.", "labels": [], "entities": []}, {"text": "For the case cited his parser returns, on average, 70 parses per sentence.", "labels": [], "entities": []}, {"text": "Finally, we note that 50-best parsing is only a fac-tor of two or three slower than 1-best.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated the performance of our reranking parser using the standard PARSEVAL metrics.", "labels": [], "entities": []}, {"text": "We n-best trees f -score New 0.9102 Collins 0.9037: Results on new n-best trees and Collins nbest trees, with weights estimated from sections 2-21 and the regularizer constant c adjusted for optimal f -score on section 24 and evaluated on sentences of length less than 100 in section 23.", "labels": [], "entities": [{"text": "Collins 0.9037", "start_pos": 36, "end_pos": 50, "type": "DATASET", "confidence": 0.7353702485561371}]}, {"text": "trained the n-best parser on sections 2-21 of the Penn Treebank, and used section 24 as development data to tune the mixing parameters of the smoothing model.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 50, "end_pos": 63, "type": "DATASET", "confidence": 0.9943565130233765}]}, {"text": "Similarly, we trained the feature weights \u03b8 with the MaxEnt reranker on sections 2-21, and adjusted the regularizer constant c to maximize the f -score on section 24 of the treebank.", "labels": [], "entities": [{"text": "f -score", "start_pos": 143, "end_pos": 151, "type": "METRIC", "confidence": 0.9138530095418295}]}, {"text": "We did this both on the trees supplied to us by Michael Collins, and on the output of the n-best parser described in this paper.", "labels": [], "entities": []}, {"text": "The results are presented in.", "labels": [], "entities": []}, {"text": "The n-best parser's most probable parses are already of state-of-the-art quality, but the reranker further improves the f -score.", "labels": [], "entities": [{"text": "f -score", "start_pos": 120, "end_pos": 128, "type": "METRIC", "confidence": 0.9457423488299052}]}], "tableCaptions": [{"text": " Table 1: Number of structures created as a function  of sentence length", "labels": [], "entities": []}, {"text": " Table 2. (The results are for all sentences of sec- tion 23 of the WSJ tree-bank of length \u2264 100.) From  the 1-best result we see that the base accuracy of the  parser is 89.7%. 1 2-best and 10-best show dramatic  oracle-rate improvements. After that things start to  slow down, and we achieve an oracle rate of 0.968  at 50-best. To put this in perspective, Roark", "labels": [], "entities": [{"text": "WSJ tree-bank", "start_pos": 68, "end_pos": 81, "type": "DATASET", "confidence": 0.9825846552848816}, {"text": "accuracy", "start_pos": 145, "end_pos": 153, "type": "METRIC", "confidence": 0.9186168909072876}]}]}