{"title": [], "abstractContent": [{"text": "We present a Czech-English statistical machine translation system which performs tree-to-tree translation of dependency structures.", "labels": [], "entities": [{"text": "Czech-English statistical machine translation", "start_pos": 13, "end_pos": 58, "type": "TASK", "confidence": 0.5137599855661392}]}, {"text": "The only bilingual resource required is a sentence-aligned parallel corpus.", "labels": [], "entities": []}, {"text": "All other resources are monolingual.", "labels": [], "entities": []}, {"text": "We also refer to an evaluation method and plan to compare our sys-tem's output with a benchmark system.", "labels": [], "entities": []}], "introductionContent": [{"text": "The goal of statistical machine translation (SMT) is to develop mathematical models of the translation process whose parameters can be automatically estimated from a parallel corpus.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 12, "end_pos": 49, "type": "TASK", "confidence": 0.8143511712551117}]}, {"text": "Given a string of foreign words F, we seek to find the English string E which is a \"correct\" translation of the foreign string.", "labels": [], "entities": []}, {"text": "The first work on SMT done at IBM (, used a noisy-channel model, resulting in what call \"the Fundamental Equation of Machine Translation\": In this equation we see that the translation problem is factored into two subproblems.", "labels": [], "entities": [{"text": "SMT", "start_pos": 18, "end_pos": 21, "type": "TASK", "confidence": 0.9958640336990356}, {"text": "Machine Translation", "start_pos": 117, "end_pos": 136, "type": "TASK", "confidence": 0.7488838136196136}]}, {"text": "P (E) is the language model and P (F | E) is the translation model.", "labels": [], "entities": []}, {"text": "The work described here focuses on developing improvements to the translation model.", "labels": [], "entities": [{"text": "translation", "start_pos": 66, "end_pos": 77, "type": "TASK", "confidence": 0.982475996017456}]}, {"text": "While the IBM work was groundbreaking, it was also deficient in several ways.", "labels": [], "entities": []}, {"text": "Their model translates words in isolation, and the component which accounts for word order differences between languages is based on linear position in the sentence.", "labels": [], "entities": []}, {"text": "Conspicuously absent is all but the most elementary use of syntactic information.", "labels": [], "entities": []}, {"text": "Several researchers have subsequently formulated models which incorporate the intuition that syntactically close constituents tend to stay close across languages.", "labels": [], "entities": []}, {"text": "Below are descriptions of some of these different methods of integrating syntax.", "labels": [], "entities": []}, {"text": "\u2022 Stochastic Inversion Transduction Grammars (: This formalism uses a grammar for English and from it derives a possible grammar for the foreign language.", "labels": [], "entities": [{"text": "Stochastic Inversion Transduction Grammars", "start_pos": 2, "end_pos": 44, "type": "TASK", "confidence": 0.7474847584962845}]}, {"text": "This derivation includes adding productions where the order of the RHS is reversed from the ordering of the English.", "labels": [], "entities": [{"text": "RHS", "start_pos": 67, "end_pos": 70, "type": "DATASET", "confidence": 0.6517686247825623}]}, {"text": "\u2022 Syntax-based Statistical Translation ( ): This model extends the above by allowing all possible permutations of the RHS of the English rules.", "labels": [], "entities": [{"text": "Statistical Translation", "start_pos": 15, "end_pos": 38, "type": "TASK", "confidence": 0.8163858950138092}]}, {"text": "\u2022 Statistical Phrase-based Translation (: Here \"phrase-based\" means \"subsequence-based\", as there is no guarantee that the phrases learned by the model will have any relation to what we would think of as syntactic phrases.", "labels": [], "entities": [{"text": "Statistical Phrase-based Translation", "start_pos": 2, "end_pos": 38, "type": "TASK", "confidence": 0.8012826840082804}]}, {"text": "\u2022 Dependency-based Translation: This model assumes a dependency parser for the foreign language.", "labels": [], "entities": [{"text": "Dependency-based Translation", "start_pos": 2, "end_pos": 30, "type": "TASK", "confidence": 0.7020889520645142}]}, {"text": "The syntactic structure and labels are preserved during translation.", "labels": [], "entities": []}, {"text": "A generator builds an English sentence out of the structure, labels, and translated words.", "labels": [], "entities": []}], "datasetContent": [{"text": "We propose to evaluate system performance with version 0.9 of the NIST automated scorer, which is a modification of the BLEU system ().", "labels": [], "entities": [{"text": "NIST automated scorer", "start_pos": 66, "end_pos": 87, "type": "DATASET", "confidence": 0.7201283176740011}, {"text": "BLEU", "start_pos": 120, "end_pos": 124, "type": "METRIC", "confidence": 0.9870302081108093}]}, {"text": "BLEU calculates a score based on a weighted sum of the counts of matching n-grams, along with a penalty fora significant difference in length between the system output and the reference translation closest in length.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9621998071670532}]}, {"text": "Experiments have shown a high degree of correlation between BLEU score and the translation quality judgments of humans.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 60, "end_pos": 70, "type": "METRIC", "confidence": 0.9829753637313843}]}, {"text": "The most interesting difference in the NIST scorer is that it weights n-grams based on a notion of informativeness.", "labels": [], "entities": [{"text": "NIST scorer", "start_pos": 39, "end_pos": 50, "type": "DATASET", "confidence": 0.8548920452594757}]}, {"text": "Details of the scorer can be found in their paper.", "labels": [], "entities": []}, {"text": "For our experiments, we propose to use the data from the PDT, which has already been segmented into training, held out (or development test), and evaluation sets.", "labels": [], "entities": [{"text": "PDT", "start_pos": 57, "end_pos": 60, "type": "TASK", "confidence": 0.6043552756309509}]}, {"text": "As a baseline, we will run the GIZA++ implementation of IBM's Model 4 translation algorithm under the same training conditions as our own system).", "labels": [], "entities": [{"text": "IBM's Model 4 translation algorithm", "start_pos": 56, "end_pos": 91, "type": "TASK", "confidence": 0.6444137195746104}]}], "tableCaptions": []}