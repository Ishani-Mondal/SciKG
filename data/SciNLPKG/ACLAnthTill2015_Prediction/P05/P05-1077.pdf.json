{"title": [{"text": "Randomized Algorithms and NLP: Using Locality Sensitive Hash Function for High Speed Noun Clustering", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper, we explore the power of randomized algorithm to address the challenge of working with very large amounts of data.", "labels": [], "entities": []}, {"text": "We apply these algorithms to generate noun similarity lists from 70 million pages.", "labels": [], "entities": []}, {"text": "We reduce the running time from quadratic to practically linear in the number of elements to be computed.", "labels": [], "entities": []}], "introductionContent": [{"text": "In the last decade, the field of Natural Language Processing (NLP), has seen a surge in the use of corpus motivated techniques.", "labels": [], "entities": [{"text": "Natural Language Processing (NLP)", "start_pos": 33, "end_pos": 66, "type": "TASK", "confidence": 0.7968830168247223}]}, {"text": "Several NLP systems are modeled based on empirical data and have had varying degrees of success.", "labels": [], "entities": []}, {"text": "Of late, however, corpusbased techniques seem to have reached a plateau in performance.", "labels": [], "entities": []}, {"text": "Three possible areas for future research investigation to overcoming this plateau include: 1.", "labels": [], "entities": []}, {"text": "Working with large amounts of data () 2.", "labels": [], "entities": []}, {"text": "Improving semi-supervised and unsupervised algorithms.", "labels": [], "entities": []}, {"text": "3. Using more sophisticated feature functions.", "labels": [], "entities": []}, {"text": "The above listing may not be exhaustive, but it is probably not a bad bet to work in one of the above directions.", "labels": [], "entities": []}, {"text": "In this paper, we investigate the first two avenues.", "labels": [], "entities": []}, {"text": "Handling terabytes of data requires more efficient algorithms than are currently used in NLP.", "labels": [], "entities": []}, {"text": "We propose a web scalable solution to clustering nouns, which employs randomized algorithms.", "labels": [], "entities": [{"text": "clustering nouns", "start_pos": 38, "end_pos": 54, "type": "TASK", "confidence": 0.8974815905094147}]}, {"text": "In doing so, we are going to explore the literature and techniques of randomized algorithms.", "labels": [], "entities": []}, {"text": "All clustering algorithms make use of some distance similarity (e.g., cosine similarity) to measure pairwise distance between sets of vectors.", "labels": [], "entities": []}, {"text": "Assume that we are given n points to cluster with a maximum of k features.", "labels": [], "entities": []}, {"text": "Calculating the full similarity matrix would take time complexity n 2 k.", "labels": [], "entities": []}, {"text": "With large amounts of data, say n in the order of millions or even billions, having an n 2 k algorithm would be very infeasible.", "labels": [], "entities": []}, {"text": "To be scalable, we ideally want our algorithm to be proportional to nk.", "labels": [], "entities": []}, {"text": "Fortunately, we can borrow some ideas from the Math and Theoretical Computer Science community to tackle this problem.", "labels": [], "entities": [{"text": "Math and Theoretical Computer Science", "start_pos": 47, "end_pos": 84, "type": "TASK", "confidence": 0.6328307211399078}]}, {"text": "The crux of our solution lies in defining Locality Sensitive Hash (LSH) functions.", "labels": [], "entities": []}, {"text": "LSH functions involve the creation of short signatures (fingerprints) for each vector in space such that those vectors that are closer to each other are more likely to have similar fingerprints.", "labels": [], "entities": []}, {"text": "LSH functions are generally based on randomized algorithms and are probabilistic.", "labels": [], "entities": []}, {"text": "We present LSH algorithms that can help reduce the time complexity of calculating our distance similarity atrix to nk. proposed the use of hash functions from random irreducible polynomials to create short fingerprint representations for very large strings.", "labels": [], "entities": []}, {"text": "These hash function had the nice property that the fingerprint of two identical strings had the same fingerprints, while dissimilar strings had different fingerprints with a very small probability of collision.", "labels": [], "entities": []}, {"text": "Broder (1997) first introduced LSH.", "labels": [], "entities": []}, {"text": "He proposed the use of Min-wise independent functions to create fingerprints that preserved the Jaccard sim-ilarity between every pair of vectors.", "labels": [], "entities": []}, {"text": "These techniques are used today, for example, to eliminate duplicate web pages.", "labels": [], "entities": [{"text": "eliminate duplicate web pages", "start_pos": 49, "end_pos": 78, "type": "TASK", "confidence": 0.7456354200839996}]}, {"text": "proposed the use of random hyperplanes to generate an LSH function that preserves the cosine similarity between every pair of vectors.", "labels": [], "entities": []}, {"text": "Interestingly, cosine similarity is widely used in NLP for various applications such as clustering.", "labels": [], "entities": []}, {"text": "In this paper, we perform high speed similarity list creation for nouns collected from a huge web corpus.", "labels": [], "entities": [{"text": "similarity list creation", "start_pos": 37, "end_pos": 61, "type": "TASK", "confidence": 0.6402046183745066}]}, {"text": "We linearize this step by using the LSH proposed by.", "labels": [], "entities": []}, {"text": "This reduction in complexity of similarity computation makes it possible to address vastly larger datasets, at the cost, as shown in Section 5, of only little reduction inaccuracy.", "labels": [], "entities": []}, {"text": "In our experiments, we generate a similarity list for each noun extracted from 70 million page web corpus.", "labels": [], "entities": []}, {"text": "Although the NLP community has begun experimenting with the web, we know of no work in published literature that has applied complex language analysis beyond IR and simple surface-level pattern matching.", "labels": [], "entities": [{"text": "surface-level pattern matching", "start_pos": 172, "end_pos": 202, "type": "TASK", "confidence": 0.6708075801531473}]}], "datasetContent": [{"text": "Evaluating clustering systems is generally considered to be quite difficult.", "labels": [], "entities": [{"text": "Evaluating clustering", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.8691320717334747}]}, {"text": "However, we are mainly concerned with evaluating the quality and speed of our high speed randomized algorithm.", "labels": [], "entities": []}, {"text": "The web corpus is used to show that our framework is webscalable, while the newspaper corpus is used to compare the output of our system with the similarity lists output by an existing system, which are calculated using the traditional formula as given in equation 1.", "labels": [], "entities": [{"text": "newspaper corpus", "start_pos": 76, "end_pos": 92, "type": "DATASET", "confidence": 0.9542974233627319}]}, {"text": "For this base comparison system we use the one built by.", "labels": [], "entities": []}, {"text": "We perform 3 kinds of evaluation: 1.", "labels": [], "entities": []}, {"text": "Performance of Locality Sensitive Hash Function; 2.", "labels": [], "entities": []}, {"text": "Performance of fast Hamming distance search algorithm; 3.", "labels": [], "entities": [{"text": "Hamming distance search", "start_pos": 20, "end_pos": 43, "type": "TASK", "confidence": 0.890012522538503}]}, {"text": "Quality of final similarity lists.", "labels": [], "entities": []}, {"text": "To perform this evaluation, we randomly choose 100 nouns (vectors) from the web collection.", "labels": [], "entities": []}, {"text": "For each noun, we calculate the cosine distance using the traditional slow method (as given by equation 1), with all other nouns in the collection.", "labels": [], "entities": []}, {"text": "This process creates similarity lists for each of the 100 vectors.", "labels": [], "entities": []}, {"text": "These similarity lists are cutoff at a threshold of 0.15.", "labels": [], "entities": [{"text": "similarity", "start_pos": 6, "end_pos": 16, "type": "METRIC", "confidence": 0.9604740142822266}]}, {"text": "These lists are considered to be the gold standard test set for our evaluation.", "labels": [], "entities": []}, {"text": "For the above 100 chosen vectors, we also calculate the cosine similarity using the randomized approach as given by equation 4 and calculate the mean squared error with the gold standard test set using the following formula: where CS real,i and CS calc,i are the cosine similarity scores calculated using the traditional (equation 1) and randomized (equation 4) technique re- spectively.", "labels": [], "entities": [{"text": "mean squared error", "start_pos": 145, "end_pos": 163, "type": "METRIC", "confidence": 0.8184130589167277}, {"text": "gold standard test set", "start_pos": 173, "end_pos": 195, "type": "DATASET", "confidence": 0.7854307591915131}]}, {"text": "i is the index overall pairs of elements that have CS real,i >= 0.15 We calculate the error (error av ) for various values of d, the total number of unit random vectors r used in the process.", "labels": [], "entities": [{"text": "error (error av )", "start_pos": 86, "end_pos": 103, "type": "METRIC", "confidence": 0.9708815693855286}]}, {"text": "The results are reported in 6 . As we generate more random vectors, the error rate decreases.", "labels": [], "entities": [{"text": "error rate", "start_pos": 72, "end_pos": 82, "type": "METRIC", "confidence": 0.9825985133647919}]}, {"text": "For example, generating 10 random vectors gives us a cosine error of 0.4432 (which is a large number since cosine similarity ranges from 0 to 1.)", "labels": [], "entities": []}, {"text": "However, generation of more random vectors leads to reduction in error rate as seen by the values for 1000 (0.0493) and 10000 (0.0156).", "labels": [], "entities": [{"text": "error rate", "start_pos": 65, "end_pos": 75, "type": "METRIC", "confidence": 0.9830402135848999}]}, {"text": "But as we generate more random vectors the time taken by the algorithm also increases.", "labels": [], "entities": []}, {"text": "We choose d = 3000 random vectors as our optimal (time-accuracy) cutoff.", "labels": [], "entities": []}, {"text": "It is also very interesting to note that by using only 3000 bits for each of the 655,495 nouns, we are able to measure cosine similarity between every pair of them to within an average error margin of 0.027.", "labels": [], "entities": []}, {"text": "This algorithm is also highly memory efficient since we can represent every vector by only a few thousand bits.", "labels": [], "entities": []}, {"text": "Also the randomization process makes the the algorithm easily parallelizable since each processor can independently contribute a few bits for every vector.", "labels": [], "entities": []}, {"text": "We initially obtain a list of bit streams for all the vectors (nouns) from our web corpus using the randomized algorithm described in Section 3 (Steps 1 to 3).", "labels": [], "entities": []}, {"text": "The next step involves the calculation of hamming distance.", "labels": [], "entities": [{"text": "hamming distance", "start_pos": 42, "end_pos": 58, "type": "METRIC", "confidence": 0.6798598766326904}]}, {"text": "To evaluate the quality of this search algorithm we again randomly choose 100 vectors (nouns) from our collection.", "labels": [], "entities": []}, {"text": "For each of these 100 vectors we manually calculate the hamming distance The time is calculated for running the algorithm on a single Pentium IV processor with 4GB of memory with all other vectors in the collection.", "labels": [], "entities": [{"text": "hamming distance", "start_pos": 56, "end_pos": 72, "type": "METRIC", "confidence": 0.9204365611076355}]}, {"text": "We only retain those pairs of vectors whose cosine distance (as manually calculated) is above 0.15.", "labels": [], "entities": []}, {"text": "This similarity list is used as the gold standard test set for evaluating our fast hamming search.", "labels": [], "entities": []}, {"text": "We then apply the fast hamming distance search algorithm as described in Section 3.", "labels": [], "entities": [{"text": "hamming distance search", "start_pos": 23, "end_pos": 46, "type": "TASK", "confidence": 0.8291260798772176}]}, {"text": "In particular, it involves steps 3 to 6 of the algorithm.", "labels": [], "entities": []}, {"text": "We evaluate the hamming distance with respect to two criteria: 1.", "labels": [], "entities": [{"text": "hamming distance", "start_pos": 16, "end_pos": 32, "type": "METRIC", "confidence": 0.7495341897010803}]}, {"text": "Number of bit index random permutations functions q; 2.", "labels": [], "entities": []}, {"text": "For each vector in the test collection, we take the top N elements from the gold standard similarity list and calculate how many of these elements are actually discovered by the fast hamming distance algorithm.", "labels": [], "entities": []}, {"text": "We report the results in and with beam parameters of (B = 25) and (B = 100) respectively.", "labels": [], "entities": [{"text": "B", "start_pos": 54, "end_pos": 55, "type": "METRIC", "confidence": 0.9824388027191162}, {"text": "B", "start_pos": 67, "end_pos": 68, "type": "METRIC", "confidence": 0.9502466320991516}]}, {"text": "For each beam, we experiment with various values for q, the number of random permutation function used.", "labels": [], "entities": []}, {"text": "In general, by increasing the value for beam B and number of random permutation q , the accuracy of the search algorithm increases.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 88, "end_pos": 96, "type": "METRIC", "confidence": 0.9996635913848877}]}, {"text": "For example in by using abeam B = 100 and using 1000 random bit permutations, we are able to discover 72.8% of the elements of the Top 100 list.", "labels": [], "entities": [{"text": "abeam B", "start_pos": 24, "end_pos": 31, "type": "METRIC", "confidence": 0.7644445896148682}, {"text": "Top 100 list", "start_pos": 131, "end_pos": 143, "type": "DATASET", "confidence": 0.8235764304796854}]}, {"text": "However, increasing the values of q and B also increases search time.", "labels": [], "entities": []}, {"text": "With abeam (B) of 100 and the number of random permutations equal to 100 (i.e., q = 1000) it takes 570 hours of processing time on a single Pentium IV machine, whereas with B = 25 and q = 1000, reduces processing time by more than 50% to 240 hours.", "labels": [], "entities": [{"text": "abeam (B)", "start_pos": 5, "end_pos": 14, "type": "METRIC", "confidence": 0.9307763278484344}]}, {"text": "We could not calculate the total time taken to build noun similarity list using the traditional technique on the entire corpus.", "labels": [], "entities": []}, {"text": "However, we estimate that its time taken would beat least 50,000 hours (and perhaps even more) with a few of Terabytes of disk space needed.", "labels": [], "entities": []}, {"text": "This is a very rough estimate.", "labels": [], "entities": []}, {"text": "This estimate assumes the widely used reverse indexing technique, wherein one compares only those vector pairs that have at least one feature in common.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Error in cosine similarity", "labels": [], "entities": [{"text": "Error", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9962450861930847}]}, {"text": " Table 3: Hamming search accuracy (Beam B = 25)", "labels": [], "entities": [{"text": "Hamming search", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.8869864344596863}, {"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.946026623249054}, {"text": "Beam B", "start_pos": 35, "end_pos": 41, "type": "METRIC", "confidence": 0.9649582505226135}]}, {"text": " Table 4: Hamming search accuracy (Beam B = 100)", "labels": [], "entities": [{"text": "Hamming search", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.8946161866188049}, {"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9463335275650024}, {"text": "Beam B", "start_pos": 35, "end_pos": 41, "type": "METRIC", "confidence": 0.9732679426670074}]}, {"text": " Table 5: Final Quality of Similarity Lists", "labels": [], "entities": []}]}