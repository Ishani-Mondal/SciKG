{"title": [{"text": "A Hierarchical Phrase-Based Model for Statistical Machine Translation", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 38, "end_pos": 69, "type": "TASK", "confidence": 0.8572032252947489}]}], "abstractContent": [{"text": "We present a statistical phrase-based translation model that uses hierarchical phrases-phrases that contain subphrases.", "labels": [], "entities": [{"text": "statistical phrase-based translation", "start_pos": 13, "end_pos": 49, "type": "TASK", "confidence": 0.5718532502651215}]}, {"text": "The model is formally asynchronous context-free grammar but is learned from a bitext without any syntactic information.", "labels": [], "entities": []}, {"text": "Thus it can be seen as a shift to the formal machinery of syntax-based translation systems without any linguistic commitment.", "labels": [], "entities": []}, {"text": "In our experiments using BLEU as a metric, the hierarchical phrase-based model achieves a relative improvement of 7.5% over Pharaoh, a state-of-the-art phrase-based system.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 25, "end_pos": 29, "type": "METRIC", "confidence": 0.9964272379875183}]}], "introductionContent": [{"text": "The alignment template translation model () and related phrase-based models advanced the previous state of the art by moving from words to phrases as the basic unit of translation.", "labels": [], "entities": [{"text": "alignment template translation", "start_pos": 4, "end_pos": 34, "type": "TASK", "confidence": 0.6490284899870554}]}, {"text": "Phrases, which can be any substring and not necessarily phrases in any syntactic theory, allow these models to learn local reorderings, translation of short idioms, or insertions and deletions that are sensitive to local context.", "labels": [], "entities": []}, {"text": "They are thus a simple and powerful mechanism for machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 50, "end_pos": 69, "type": "TASK", "confidence": 0.8036779463291168}]}, {"text": "The basic phrase-based model is an instance of the noisy-channel approach, in which the translation of a French sentence f into an Throughout this paper, we follow the convention of Brown et al. of designating the source and target languages as \"French\" and \"English,\" respectively.", "labels": [], "entities": []}, {"text": "The variables f and e stand for source and target sentences; f j i stands for the substring off from position i to position j inclusive, and similarly fore j i . English sentence e is modeled as: arg max The translation model P( f | e) \"encodes\" e into f by the following steps: 1.", "labels": [], "entities": []}, {"text": "segment e into phrases \u00af e 1 \u00b7 \u00b7 \u00b7 \u00af e I , typically with a uniform distribution over segmentations; 2.", "labels": [], "entities": []}, {"text": "reorder the \u00af e i according to some distortion model; 3.", "labels": [], "entities": []}, {"text": "translate each of the \u00af e i into French phrases according to a model P( \u00af f | \u00af e) estimated from the training data.", "labels": [], "entities": []}, {"text": "Other phrase-based models model the joint distribution P(e, f ) () or made P(e) and P( f | e) into features of a log-linear model).", "labels": [], "entities": []}, {"text": "But the basic architecture of phrase segmentation (or generation), phrase reordering, and phrase translation remains the same.", "labels": [], "entities": [{"text": "phrase segmentation (or generation)", "start_pos": 30, "end_pos": 65, "type": "TASK", "confidence": 0.8385192354520162}, {"text": "phrase reordering", "start_pos": 67, "end_pos": 84, "type": "TASK", "confidence": 0.8325971364974976}, {"text": "phrase translation", "start_pos": 90, "end_pos": 108, "type": "TASK", "confidence": 0.8355617821216583}]}, {"text": "Phrase-based models can robustly perform translations that are localized to substrings that are common enough to have been observed in training.", "labels": [], "entities": []}, {"text": "But  find that phrases longer than three words improve performance little, suggesting that data sparseness takes over for longer phrases.", "labels": [], "entities": []}, {"text": "Above the phrase level, these models typically have a simple distortion model that reorders phrases independently of their content (, or not at all ().", "labels": [], "entities": []}, {"text": "But it is often desirable to capture translations whose scope is larger than a few consecutive words.", "labels": [], "entities": []}, {"text": "If we count zhiyi, lit.", "labels": [], "entities": []}, {"text": "'of-one,' as a single token, then translating this sentence correctly into English requires reversing a sequence of five elements.", "labels": [], "entities": []}, {"text": "When we run a phrase-based system, Pharaoh (, on this sentence (using the experimental setup described below), we get the following phrases with translations: where we have used subscripts to indicate the reordering of phrases.", "labels": [], "entities": []}, {"text": "The phrase-based model is able to order \"diplomatic.", "labels": [], "entities": []}, {"text": ".Korea\" correctly (using phrase reordering) and \"one.", "labels": [], "entities": []}, {"text": ".countries\" correctly (using a phrase translation), but does not accomplish the necessary inversion of those two groups.", "labels": [], "entities": []}, {"text": "A lexicalized phrase-reordering model like that in use in ISI's system ) might be able to learn a better reordering, but simpler distortion models will probably not.", "labels": [], "entities": []}, {"text": "We propose a solution to these problems that does not interfere with the strengths of the phrasebased approach, but rather capitalizes on them: since phrases are good for learning reorderings of words, we can use them to learn reorderings of phrases as well.", "labels": [], "entities": []}, {"text": "In order to do this we need hierarchical phrases that consist of both words and subphrases.", "labels": [], "entities": []}, {"text": "For example, a hierarchical phrase pair that might help with the above example is: (5) yu 1 you 2 , have 2 with 1 where 1 and 2 are placeholders for subphrases.", "labels": [], "entities": []}, {"text": "This would capture the fact that Chinese PPs almost always modify VP on the left, whereas English PPs usually modify VP on the right.", "labels": [], "entities": []}, {"text": "Because it generalizes over possible prepositional objects and direct objects, it acts both as a discontinuous phrase pair and as a phrase-reordering rule.", "labels": [], "entities": []}, {"text": "Thus it is considerably more powerful than a conventional phrase pair.", "labels": [], "entities": []}, {"text": "Similarly, The system we describe below uses rules like this, and in fact is able to learn them automatically from a bitext without syntactic annotation.", "labels": [], "entities": []}, {"text": "It translates the above example almost exactly as we have shown, the only error being that it omits the word 'that' from (6) and therefore (8).", "labels": [], "entities": []}, {"text": "These hierarchical phrase pairs are formally productions of asynchronous context-free grammar (defined below).", "labels": [], "entities": []}, {"text": "A move to synchronous CFG can be seen as a move towards syntax-based MT; however, we make a distinction here between formally syntax-based and linguistically syntax-based MT.", "labels": [], "entities": [{"text": "MT", "start_pos": 69, "end_pos": 71, "type": "TASK", "confidence": 0.9222038388252258}]}, {"text": "A system like that of is both formally and linguistically syntax-based: formally because it uses synchronous CFG, linguistically because the structures it is defined over are (on the English side) informed by syntactic theory (via the Penn Treebank).", "labels": [], "entities": [{"text": "Penn Treebank)", "start_pos": 235, "end_pos": 249, "type": "DATASET", "confidence": 0.9804378747940063}]}, {"text": "Our system is formally syntaxbased in that it uses synchronous CFG, but not necessarily linguistically syntax-based, because it induces a grammar from a parallel text without relying on any linguistic annotations or assumptions; the result sometimes resembles a syntactician's grammar but often does not.", "labels": [], "entities": []}, {"text": "In this respect it resembles Wu's bilingual bracketer (Wu, 1997), but ours uses a different extraction method that allows more than one lexical item in a rule, in keeping with the phrasebased philosophy.", "labels": [], "entities": []}, {"text": "Our extraction method is basically the same as that of Block, except we allow more than one nonterminal symbol in a rule, and use a more sophisticated probability model.", "labels": [], "entities": []}, {"text": "In this paper we describe the design and implementation of our hierarchical phrase-based model, and report on experiments that demonstrate that hierarchical phrases indeed improve translation.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our experiments were on Mandarin-to-English translation.", "labels": [], "entities": [{"text": "Mandarin-to-English translation", "start_pos": 24, "end_pos": 55, "type": "TASK", "confidence": 0.6732728779315948}]}, {"text": "We compared a baseline system, the state-of-the-art phrase-based system Pharaoh (), against our system.", "labels": [], "entities": []}, {"text": "For all three systems we trained the translation model on the FBIS corpus (7.2M+9.2M words); for the language model, we used the SRI Language Modeling Toolkit to train a trigram model with modified Kneser-Ney smoothing) on 155M words of English newswire text, mostly from the Xinhua portion of the Gigaword corpus.", "labels": [], "entities": [{"text": "FBIS corpus", "start_pos": 62, "end_pos": 73, "type": "DATASET", "confidence": 0.9468477666378021}, {"text": "Gigaword corpus", "start_pos": 298, "end_pos": 313, "type": "DATASET", "confidence": 0.8054200708866119}]}, {"text": "We used the 2002 NIST MT evaluation test set as our development set, and the 2003 test set as our test set.", "labels": [], "entities": [{"text": "NIST MT evaluation test set", "start_pos": 17, "end_pos": 44, "type": "DATASET", "confidence": 0.9005568504333497}, {"text": "2003 test set", "start_pos": 77, "end_pos": 90, "type": "DATASET", "confidence": 0.8045564293861389}]}, {"text": "Our evaluation metric was BLEU), as calculated by the NIST script (version 11a) with its default settings, which is to perform case-insensitive matching of n-grams up ton = 4, and to use the shortest (as opposed to nearest) reference sentence for the brevity penalty.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 26, "end_pos": 30, "type": "METRIC", "confidence": 0.9990764856338501}, {"text": "NIST script", "start_pos": 54, "end_pos": 65, "type": "DATASET", "confidence": 0.9518835544586182}]}, {"text": "The results of the experiments are summarized in.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results on baseline system and hierarchical system, with and without constituent feature.", "labels": [], "entities": []}, {"text": " Table 2: Feature weights obtained by minimum-error-rate training (normalized so that absolute values sum  to one). Word = word penalty; Phr = phrase penalty. Note that we have inverted the sense of Pharaoh's  phrase penalty so that a positive weight indicates a penalty.", "labels": [], "entities": []}]}