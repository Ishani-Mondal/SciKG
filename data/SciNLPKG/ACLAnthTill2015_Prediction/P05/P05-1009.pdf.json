{"title": [{"text": "Towards Developing Generation Algorithms for Text-to-Text Applications", "labels": [], "entities": []}], "abstractContent": [{"text": "We describe anew sentence realization framework for text-to-text applications.", "labels": [], "entities": [{"text": "sentence realization", "start_pos": 17, "end_pos": 37, "type": "TASK", "confidence": 0.7312705814838409}]}, {"text": "This framework uses IDL-expressions as a representation formalism, and a generation mechanism based on algorithms for intersecting IDL-expressions with proba-bilistic language models.", "labels": [], "entities": []}, {"text": "We present both theoretical and empirical results concerning the correctness and efficiency of these algorithms.", "labels": [], "entities": []}], "introductionContent": [{"text": "Many of today's most popular natural language applications -Machine Translation, Summarization, Question Answering -are text-to-text applications.", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 60, "end_pos": 79, "type": "TASK", "confidence": 0.753310889005661}, {"text": "Summarization", "start_pos": 81, "end_pos": 94, "type": "TASK", "confidence": 0.9601284861564636}, {"text": "Question Answering", "start_pos": 96, "end_pos": 114, "type": "TASK", "confidence": 0.8417099714279175}]}, {"text": "That is, they produce textual outputs from inputs that are also textual.", "labels": [], "entities": []}, {"text": "Because these applications need to produce well-formed text, it would appear natural that they are the favorite testbed for generic generation components developed within the Natural Language Generation (NLG) community.", "labels": [], "entities": []}, {"text": "Over the years, several proposals of generic NLG systems have been made: Penman, FUF,, Fergus), HALogen), Amalgam), etc.", "labels": [], "entities": [{"text": "Penman", "start_pos": 73, "end_pos": 79, "type": "DATASET", "confidence": 0.9641559720039368}, {"text": "FUF", "start_pos": 81, "end_pos": 84, "type": "METRIC", "confidence": 0.36459410190582275}, {"text": "Amalgam", "start_pos": 106, "end_pos": 113, "type": "DATASET", "confidence": 0.8229948878288269}]}, {"text": "Instead of relying on such generic NLG systems, however, most of the current text-to-text applications use other means to address the generation need.", "labels": [], "entities": []}, {"text": "In Machine Translation, for example, sentences are produced using application-specific \"decoders\", inspired by work on speech recognition, whereas in Summarization, summaries are produced as either extracts or using task-specific strategies.", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 3, "end_pos": 22, "type": "TASK", "confidence": 0.8080183267593384}, {"text": "speech recognition", "start_pos": 119, "end_pos": 137, "type": "TASK", "confidence": 0.7478041350841522}, {"text": "Summarization", "start_pos": 150, "end_pos": 163, "type": "TASK", "confidence": 0.9652498364448547}]}, {"text": "The main reason for which text-to-text applications do not usually involve generic NLG systems is that such applications do not have access to the kind of information that the input representation formalisms of current NLG systems require.", "labels": [], "entities": []}, {"text": "A machine translation or summarization system does not usually have access to deep subject-verb or verb-object relations (such as ACTOR, AGENT, PATIENT, POSSESSOR, etc.) as needed by Penman or FUF, or even shallower syntactic relations (such as subject, object, premod, etc.) as needed by HALogen.", "labels": [], "entities": [{"text": "machine translation or summarization", "start_pos": 2, "end_pos": 38, "type": "TASK", "confidence": 0.7159256786108017}, {"text": "ACTOR", "start_pos": 130, "end_pos": 135, "type": "METRIC", "confidence": 0.9920280575752258}, {"text": "AGENT", "start_pos": 137, "end_pos": 142, "type": "METRIC", "confidence": 0.891575276851654}, {"text": "PATIENT", "start_pos": 144, "end_pos": 151, "type": "METRIC", "confidence": 0.8237584829330444}, {"text": "FUF", "start_pos": 193, "end_pos": 196, "type": "DATASET", "confidence": 0.828637957572937}]}, {"text": "In this paper, following the recent proposal made by, we argue for the use of IDL-expressions as an applicationindependent, information-slim representation language for text-to-text natural language generation.", "labels": [], "entities": [{"text": "text-to-text natural language generation", "start_pos": 169, "end_pos": 209, "type": "TASK", "confidence": 0.6470992863178253}]}, {"text": "IDL-expressions are created from strings using four operators: concatenation (\u00a2 ), interleave ( \u00a3 ), disjunction (\u00a4 ), and lock ( \u00a5 ).", "labels": [], "entities": []}, {"text": "We claim that the IDL formalism is appropriate for text-to-text generation, as it encodes meaning only via words and phrases, combined using a set of formally defined operators.", "labels": [], "entities": [{"text": "text-to-text generation", "start_pos": 51, "end_pos": 74, "type": "TASK", "confidence": 0.7270791977643967}]}, {"text": "Appropriate words and phrases can be, and usually are, produced by the applications mentioned above.", "labels": [], "entities": []}, {"text": "The IDL operators have been specifically designed to handle natural constraints such as word choice and precedence, constructions such as phrasal combination, and underspecifications such as free word order.: Comparison of the present proposal with current NLG systems.", "labels": [], "entities": []}, {"text": "In, we present a summary of the representation and generation characteristics of current NLG systems.", "labels": [], "entities": []}, {"text": "We mark by characteristics that are needed/desirable in a generation component for textto-text applications, and by \u00a1 characteristics that make the proposal inapplicable or problematic.", "labels": [], "entities": []}, {"text": "For instance, as already argued, the representation formalism of all previous proposals except for IDL is problematic (\u00a1 ) for text-to-text applications.", "labels": [], "entities": []}, {"text": "The IDL formalism, while applicable to text-to-text applications, has the additional desirable property that it is a compact representation, while formalisms such as word-lattices and non-recursive CFGs can have exponential size in the number of words available for generation (.", "labels": [], "entities": []}, {"text": "While the IDL representational properties are all desirable, the generation mechanism proposed for IDL by is problematic (\u00a1 ), because it does not allow for scoring and ranking of candidate realizations.", "labels": [], "entities": []}, {"text": "Their generation mechanism, while computationally efficient, involves intersection with context free grammars, and therefore works by excluding all realizations that are not accepted by a CFG and including (without ranking) all realizations that are accepted.", "labels": [], "entities": [{"text": "CFG", "start_pos": 188, "end_pos": 191, "type": "DATASET", "confidence": 0.8934817314147949}]}, {"text": "The approach to generation taken in this paper is presented in the last row in, and can be summarized as a tiling of generation characteristics of previous proposals (see the shaded area in).", "labels": [], "entities": []}, {"text": "Our goal is to provide an optimal generation framework for text-to-text applications, in which the representation formalism, the generation mechanism, and the computational properties are all needed and desirable ( ).", "labels": [], "entities": []}, {"text": "Toward this goal, we present anew generation mechanism that intersects IDL-expressions with probabilistic language models.", "labels": [], "entities": []}, {"text": "The generation mechanism implements new algorithms, which cover a wide spectrum of run-time behaviors (from linear to exponential), depending on the complexity of the input.", "labels": [], "entities": []}, {"text": "We also present theoretical results concerning the correctness and the efficiency input IDL-expression) of our algorithms.", "labels": [], "entities": []}, {"text": "We evaluate these algorithms by performing experiments on a challenging word-ordering task.", "labels": [], "entities": []}, {"text": "These experiments are carried out under a highcomplexity generation scenario: find the most probable sentence realization under an n-gram language model for IDL-expressions encoding bags-of-words of size up to 25 (up to 10 \u00a2 \u00a4 \u00a3 possible realizations!).", "labels": [], "entities": []}, {"text": "Our evaluation shows that the proposed algorithms are able to cope well with such orders of complexity, while maintaining high levels of accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 137, "end_pos": 145, "type": "METRIC", "confidence": 0.9975614547729492}]}, {"text": "The concatenation (\u00a2 ) operator takes two arguments, and uses the strings encoded by its argument expressions to obtain concatenated strings that respect the order of the arguments; e.g., encodes the set\u00a8\u00a9 The concatenation (\u00a2 ) operator captures precedence constraints, such as the fact that a determiner like the appears before the noun it determines.", "labels": [], "entities": []}, {"text": "The lock ( \u00a5 ) operator enforces phrase-encoding constraints, such as the fact that the captives is a phrase which should be used as a whole.", "labels": [], "entities": []}, {"text": "The disjunction (\u00a4 ) operator allows for multiple word/phrase choice (e.g., the prisoners versus the captives), and the interleave ( \u00a3 ) operator allows for word-order freedom, i.e., word order underspecification at meaning representation level.", "labels": [], "entities": [{"text": "interleave ( \u00a3 ) operator", "start_pos": 120, "end_pos": 145, "type": "METRIC", "confidence": 0.9258412599563599}]}, {"text": "Among the strings encoded by IDLexpression 1 are the following: finally the prisoners were released the captives finally were released the prisoners were finally released The following strings, however, are not part of the language defined by IDL-expression 1: the finally captives were released the prisoners were released finally the captives released were The first string is disallowed because the \u00a5 operator locks the phrase the captives.", "labels": [], "entities": []}, {"text": "The second string is not allowed because the \u00a3 operator requires all its arguments to be represented.", "labels": [], "entities": []}, {"text": "The last string violates the order imposed by the precedence operator between were and released.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we present results concerning the performance of our algorithms on a wordordering task.", "labels": [], "entities": []}, {"text": "This task can be easily defined as follows: from a bag of words originating from some sentence, reconstruct the original sentence as faithfully as possible.", "labels": [], "entities": []}, {"text": "In our case, from an original sentence such as \"the gifts are donated by american companies\", we create the IDL-expression 3 , from which some algorithm realizes a sentence such as \"donated by the american companies are gifts\".", "labels": [], "entities": []}, {"text": "Note the natural way we represent in an IDL-expression beginning and end of sentence constraints, using the \u00a2 operator.", "labels": [], "entities": []}, {"text": "Since this is generation from bag-of-words, the task is known to beat the high-complexity extreme of the run-time behavior of our algorithms.", "labels": [], "entities": []}, {"text": "As such, we consider it a good test for the ability of our algorithms to scale up to increasingly complex inputs.", "labels": [], "entities": []}, {"text": "We use a state-of-the-art, publicly available toolkit 2 to train a trigram language model using Kneser-Ney smoothing, on 10 million sentences (170 million words) from the Wall Street Journal (WSJ), lowercase and no final punctuation.", "labels": [], "entities": [{"text": "Wall Street Journal (WSJ)", "start_pos": 171, "end_pos": 196, "type": "DATASET", "confidence": 0.8877824147542318}]}, {"text": "The test data is also lowercase (such that upper-case words cannot be hypothesized as first words), with final punctuation removed (such that periods cannot be hypothesized as final words), and consists of 2000 unseen WSJ sentences of length 3-7, and 2000 unseen WSJ sentences of length 10-25.", "labels": [], "entities": []}, {"text": "The algorithms we tested in this experiments were the ones presented in Section 3.2, plus two baseline algorithms.", "labels": [], "entities": []}, {"text": "The first baseline algorithm, L, uses an inverse-lexicographic order for the bag items as its output, in order to get the word the on sentence initial position.", "labels": [], "entities": []}, {"text": "The second baseline algorithm, G, is a greedy algorithm that realizes sentences by maximizing the probability of joining any two word sequences until only one sequence is left.", "labels": [], "entities": []}, {"text": "For the A0 algorithm, an admissible cost is computed for each state \u00a1 in a weighted finite-state automaton, as the sum (over all unused words) of the minimum language model cost (i.e., maximum probability) of each unused word when conditioning overall sequences of two words available at that particular state for future conditioning (see Equation 2, with ).", "labels": [], "entities": []}, {"text": "These estimates are also used by the beam algorithm for deciding which IDL-graph nodes are not unfolded.", "labels": [], "entities": []}, {"text": "We also test a greedy version of the A0 algorithm, denoted A0 , which considers for unfolding only the nodes extracted from the priority queue which already unfolded a path of length greater than or equal to the maximum length already unfolded minus (in this notation, the A0 algorithm would be denoted A0 \u00a1 ).", "labels": [], "entities": []}, {"text": "For the beam algorithms, we use the notation B\u00a2 to specify a probabilistic beam of size \u00a2 , i.e., an algorithm that beams out the states reachable with probability less than the current maximum probability times \u00a2 . Our first batch of experiments concerns bags-ofwords of size 3-7, for which exhaustive search is possible.", "labels": [], "entities": []}, {"text": "In, we present the results on the word-ordering task achieved by various algorithms.", "labels": [], "entities": []}, {"text": "We evaluate accuracy performance using two automatic metrics: an identity metric, ID, which measures the percent of sentences recreated exactly, and BLEU (), which gives the geometric average of the number of uni-, bi-, tri-, and four-grams recreated exactly.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9747562408447266}, {"text": "ID", "start_pos": 82, "end_pos": 84, "type": "METRIC", "confidence": 0.9873864054679871}, {"text": "BLEU", "start_pos": 149, "end_pos": 153, "type": "METRIC", "confidence": 0.9993687272071838}]}, {"text": "We evaluate the search performance by the percent of Search Errors made by our algorithms, as well as a percent figure of Estimated Search Errors, computed as the percent of searches that result in a string with a lower probability than the probability of the original sentence.", "labels": [], "entities": []}, {"text": "To measure the impact of using IDL-expressions for this task, we also measure the percent of unfolding of an IDL graph with respect to a full unfolding.", "labels": [], "entities": []}, {"text": "We report speed results as the average number of seconds per bag-of-words, when using a 3.0GHz CPU machine under a Linux OS.", "labels": [], "entities": []}, {"text": "The first notable result in   Our second batch of experiments concerns bagof-words of size 10-25, for which exhaustive search is no longer possible.", "labels": [], "entities": []}, {"text": "Not only exhaustive search, but also full A0 search is too expensive in terms of memory (we were limited to 2GiB of RAM for our experiments) and speed.", "labels": [], "entities": [{"text": "A0 search", "start_pos": 42, "end_pos": 51, "type": "TASK", "confidence": 0.6407986581325531}]}, {"text": "Only the greedy versions A0\u00e4nd A0\u00a8A0\u00e4nd A0 \u00a2 , and the beam search using tight probability beams (0.2-0.1) scale up to these bag sizes.", "labels": [], "entities": [{"text": "A0\u00e4nd A0\u00a8A0\u00e4nd A0", "start_pos": 25, "end_pos": 42, "type": "METRIC", "confidence": 0.8222489356994629}]}, {"text": "Because we no longer have access to the string of maximum probability, we report only the percent of Estimated Search Errors.", "labels": [], "entities": [{"text": "Estimated Search Errors", "start_pos": 101, "end_pos": 124, "type": "METRIC", "confidence": 0.8675538102785746}]}, {"text": "Note that, in terms of accuracy, we get around 20% Estimated Search Errors for the best performing algorithms (A0 \u00a2 and B\u00a5 \u00a4 \u00a3\u00a8) \u00a3\u00a8), which means that 80% of the time the algorithms are able to find sentences of equal or better probability than the original sentences.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9994397759437561}, {"text": "Estimated Search Errors", "start_pos": 51, "end_pos": 74, "type": "METRIC", "confidence": 0.9339912533760071}]}], "tableCaptions": [{"text": " Table 2: Bags-of-words of size 3-7: accuracy (ID,  BLEU), Search Errors (and Estimated Search Errors), space  savings (Unfold), and speed results.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.9996064305305481}, {"text": "BLEU)", "start_pos": 52, "end_pos": 57, "type": "METRIC", "confidence": 0.9679635465145111}, {"text": "Search Errors (and Estimated Search Errors)", "start_pos": 59, "end_pos": 102, "type": "METRIC", "confidence": 0.731335423886776}, {"text": "speed", "start_pos": 133, "end_pos": 138, "type": "METRIC", "confidence": 0.98479163646698}]}, {"text": " Table 3: Bags-of-words of size 10-25: accuracy (ID,  BLEU), Estimated Search Errors, and speed results.", "labels": [], "entities": [{"text": "Bags-of-words", "start_pos": 10, "end_pos": 23, "type": "METRIC", "confidence": 0.7563235759735107}, {"text": "accuracy", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.9997254014015198}, {"text": "ID", "start_pos": 49, "end_pos": 51, "type": "METRIC", "confidence": 0.8682710528373718}, {"text": "BLEU)", "start_pos": 54, "end_pos": 59, "type": "METRIC", "confidence": 0.97519251704216}, {"text": "Estimated Search Errors", "start_pos": 61, "end_pos": 84, "type": "METRIC", "confidence": 0.9219188292821249}, {"text": "speed", "start_pos": 90, "end_pos": 95, "type": "METRIC", "confidence": 0.992552638053894}]}]}