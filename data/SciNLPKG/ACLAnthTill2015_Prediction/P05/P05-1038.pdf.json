{"title": [{"text": "Lexicalization in Crosslinguistic Probabilistic Parsing: The Case of French", "labels": [], "entities": [{"text": "Crosslinguistic Probabilistic Parsing", "start_pos": 18, "end_pos": 55, "type": "TASK", "confidence": 0.5971048971017202}]}], "abstractContent": [{"text": "This paper presents the first probabilistic parsing results for French, using the recently released French Treebank.", "labels": [], "entities": [{"text": "French Treebank", "start_pos": 100, "end_pos": 115, "type": "DATASET", "confidence": 0.9860797226428986}]}, {"text": "We start with an unlexicalized PCFG as a base-line model, which is enriched to the level of Collins' Model 2 by adding lexical-ization and subcategorization.", "labels": [], "entities": [{"text": "PCFG", "start_pos": 31, "end_pos": 35, "type": "DATASET", "confidence": 0.8636166453361511}, {"text": "Collins' Model 2", "start_pos": 92, "end_pos": 108, "type": "DATASET", "confidence": 0.9304503003756205}]}, {"text": "The lexi-calized sister-head model and a bigram model are also tested, to deal with the flatness of the French Treebank.", "labels": [], "entities": [{"text": "French Treebank", "start_pos": 104, "end_pos": 119, "type": "DATASET", "confidence": 0.9906478822231293}]}, {"text": "The bigram model achieves the best performance: 81% constituency F-score and 84% dependency accuracy.", "labels": [], "entities": [{"text": "constituency", "start_pos": 52, "end_pos": 64, "type": "METRIC", "confidence": 0.9667040109634399}, {"text": "F-score", "start_pos": 65, "end_pos": 72, "type": "METRIC", "confidence": 0.732759416103363}, {"text": "accuracy", "start_pos": 92, "end_pos": 100, "type": "METRIC", "confidence": 0.9128401279449463}]}, {"text": "All lexicalized models outperform the unlexicalized baseline, consistent with probabilistic parsing results for English, but contrary to results for German, where lexicalization has only a limited effect on parsing performance.", "labels": [], "entities": []}], "introductionContent": [{"text": "This paper brings together two strands of research that have recently emerged in the field of probabilistic parsing: crosslinguistic parsing and lexicalized parsing.", "labels": [], "entities": [{"text": "probabilistic parsing", "start_pos": 94, "end_pos": 115, "type": "TASK", "confidence": 0.6135993301868439}, {"text": "crosslinguistic parsing", "start_pos": 117, "end_pos": 140, "type": "TASK", "confidence": 0.7241669595241547}]}, {"text": "Interest in parsing models for languages other than English has been growing, starting with work on Czech () and Chinese.", "labels": [], "entities": []}, {"text": "Probabilistic parsing for German has also been explored by a range of authors ().", "labels": [], "entities": []}, {"text": "In general, these authors have found that existing lexicalized parsing models for English (e.g., Collins 1997) do not straightforwardly generalize to new languages; this typically manifests itself in a severe reduction in parsing performance compared to the results for English.", "labels": [], "entities": []}, {"text": "A second recent strand in parsing research has dealt with the role of lexicalization.", "labels": [], "entities": [{"text": "parsing", "start_pos": 26, "end_pos": 33, "type": "TASK", "confidence": 0.979073166847229}]}, {"text": "The conventional wisdom since Magerman has been that lexicalization substantially improves performance compared to an unlexicalized baseline model (e.g., a probabilistic context-free grammar, PCFG).", "labels": [], "entities": []}, {"text": "However, this has been challenged by, who demonstrate that an unlexicalized model can achieve a performance close to the state of the art for lexicalized models.", "labels": [], "entities": []}, {"text": "provides evidence that lexical information (in the form of bi-lexical dependencies) only makes a small contribution to the performance of parsing models such as.", "labels": [], "entities": [{"text": "parsing", "start_pos": 138, "end_pos": 145, "type": "TASK", "confidence": 0.9634228944778442}]}, {"text": "The only previous authors that have directly addressed the role of lexicalization in crosslinguistic parsing are.", "labels": [], "entities": [{"text": "crosslinguistic parsing", "start_pos": 85, "end_pos": 108, "type": "TASK", "confidence": 0.8065869808197021}]}, {"text": "They show that standard lexicalized models fail to outperform an unlexicalized baseline (a vanilla PCFG) on Negra, a German treebank ().", "labels": [], "entities": [{"text": "Negra, a German treebank", "start_pos": 108, "end_pos": 132, "type": "DATASET", "confidence": 0.9307215809822083}]}, {"text": "They attribute this result to two facts: (a) The Negra annotation assumes very flat trees, which means that Collins-style head-lexicalization fails to pickup the relevant information from non-head nodes.", "labels": [], "entities": []}, {"text": "(b) German allows flexible word order, which means that standard parsing models based on context free grammars perform poorly, as they fail to generalize over different positions of the same constituent.", "labels": [], "entities": []}, {"text": "As it stands, work does not tell us whether treebank flatness or word order flexibility is responsible for their results: for English, the annotation scheme is non-flat, and the word order is non-flexible; lexicalization improves performance.", "labels": [], "entities": []}, {"text": "For German, the annotation scheme is flat and the word order is flexible; lexicalization fails to improve performance.", "labels": [], "entities": []}, {"text": "The present paper provides the missing piece of evidence by applying probabilistic parsing models to French, a language with non-flexible word order (like English), but with a treebank with a flat annotation scheme (like German).", "labels": [], "entities": []}, {"text": "Our results show that French patterns with English: a large increase of parsing performance can be obtained by using a lexicalized model.", "labels": [], "entities": [{"text": "parsing", "start_pos": 72, "end_pos": 79, "type": "TASK", "confidence": 0.9565737843513489}]}, {"text": "We conclude that the failure to find a sizable effect of lexicalization in German can be attributed to the word order flexibility of that language, rather than to the flatness of the annotation in the German treebank.", "labels": [], "entities": [{"text": "German treebank", "start_pos": 201, "end_pos": 216, "type": "DATASET", "confidence": 0.796466201543808}]}, {"text": "The paper is organized as follows: In Section 2, we give an overview of the French Treebank we use for our experiments.", "labels": [], "entities": [{"text": "French Treebank", "start_pos": 76, "end_pos": 91, "type": "DATASET", "confidence": 0.9895358383655548}]}, {"text": "Section 3 discusses its annotation scheme and introduces a set of tree transformations that we apply.", "labels": [], "entities": []}, {"text": "Section 4 describes the pars-<NP> <w lemma=\"eux\" ei=\"PROmp\" ee=\"PRO-3mp\" cat=\"PRO\" subcat=\"3mp\">eux</w> </NP>: Word-level annotation in the French Treebank: eux 'they' (cat: POS tag, subcat: subcategory, ei, ee: inflection) ing models, followed by the results for the unlexicalized baseline model in Section 6 and fora range of lexicalized models in Section 5.", "labels": [], "entities": [{"text": "French Treebank", "start_pos": 140, "end_pos": 155, "type": "DATASET", "confidence": 0.939950555562973}]}, {"text": "Finally, Section 7 provides a crosslinguistic comparison involving data sets of the same size extracted from the French, English, and German treebanks.", "labels": [], "entities": []}], "datasetContent": [{"text": "The lexicalized models were tested on the Cont+CR data set, i.e., compounds were contracted and coordination was raised (this is the configuration that gave the best performance in Experiment 1).", "labels": [], "entities": [{"text": "Cont+CR data set", "start_pos": 42, "end_pos": 58, "type": "DATASET", "confidence": 0.7661890387535095}, {"text": "coordination", "start_pos": 96, "end_pos": 108, "type": "METRIC", "confidence": 0.9655182361602783}]}, {"text": "shows that all lexicalized models achieve a performance of around 80% recall and precision, i.e., they outperform the best unlexicalized model by at least 14% (see).", "labels": [], "entities": [{"text": "recall", "start_pos": 70, "end_pos": 76, "type": "METRIC", "confidence": 0.9995417594909668}, {"text": "precision", "start_pos": 81, "end_pos": 90, "type": "METRIC", "confidence": 0.9960417747497559}]}, {"text": "This is consistent with what has been reported for English on the PTB.", "labels": [], "entities": [{"text": "the PTB", "start_pos": 62, "end_pos": 69, "type": "DATASET", "confidence": 0.6813746690750122}]}, {"text": "Collins' Model 2, which adds the complement/adjunct distinction and subcategorization frames achieved only a very small improvement over Collins' Model 1, which was not statistically significant using a \u03c7 2 test.", "labels": [], "entities": []}, {"text": "It might well be that the annotation scheme of the FTB does not lend itself particularly well to the demands of Model 2.", "labels": [], "entities": [{"text": "FTB", "start_pos": 51, "end_pos": 54, "type": "DATASET", "confidence": 0.9601082801818848}]}, {"text": "Moreover, as mentions, some of the benefits of Model 2 are already captured by inclusion of the distance measure.", "labels": [], "entities": []}, {"text": "A further small improvement was achieved using sister-head model; however, again the difference did not reach statistical significance.", "labels": [], "entities": []}, {"text": "The bigram model, however, yielded a statistically significant improvement over Collins' Model 1 (recall \u03c7 2 = 3.91, df = 1, p \u2264 .048; precision \u03c7 2 = 3.97, df = 1, p \u2264 .046).", "labels": [], "entities": [{"text": "recall \u03c7 2", "start_pos": 98, "end_pos": 108, "type": "METRIC", "confidence": 0.9437755346298218}, {"text": "precision", "start_pos": 135, "end_pos": 144, "type": "METRIC", "confidence": 0.9940946698188782}]}, {"text": "This is consistent with the findings of for Czech, where the bigram model upped dependency accuracy by about 0.9%, as well as for English where reports an increase in F-score of approximately 0.3%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.9568613171577454}, {"text": "F-score", "start_pos": 167, "end_pos": 174, "type": "METRIC", "confidence": 0.9989125728607178}]}, {"text": "The BigramFlat model, which applies the bigram model to only those labels which have a high degree of flatness, performs: Results for lexicalized and unlexicalized models (sentences \u226440 words) with correct POS tags supplied; all lexicalized models used the Cont+CR data set at roughly the same level as Model 1.", "labels": [], "entities": [{"text": "Cont+CR data set", "start_pos": 257, "end_pos": 273, "type": "DATASET", "confidence": 0.6616745173931122}]}, {"text": "The models in implemented their own POS tagging.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 36, "end_pos": 47, "type": "TASK", "confidence": 0.6646024435758591}]}, {"text": "Tagging accuracy was 91-93% for BitPar (unlexicalized models) and around 96% for the word-feature enhanced tagging model of the Bikel parser (lexicalized models).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 8, "end_pos": 16, "type": "METRIC", "confidence": 0.9792807698249817}]}, {"text": "POS tags are an important cue for parsing.", "labels": [], "entities": [{"text": "parsing", "start_pos": 34, "end_pos": 41, "type": "TASK", "confidence": 0.9795794486999512}]}, {"text": "To gain an upper bound on the performance of the parsing models, we reran the experiments by providing the correct POS tag for the words in the test set.", "labels": [], "entities": [{"text": "POS", "start_pos": 115, "end_pos": 118, "type": "METRIC", "confidence": 0.9456562995910645}]}, {"text": "While BitPar always uses the tags provided, the Bikel parser only uses them for words whose frequency is less than the unknown word threshold.", "labels": [], "entities": []}, {"text": "As shows, perfect tagging increased parsing performance in the lexicalized models by around 3%.", "labels": [], "entities": [{"text": "parsing", "start_pos": 36, "end_pos": 43, "type": "TASK", "confidence": 0.9624584913253784}]}, {"text": "This shows that the poor POS tagging performed by BitPar is one of the reasons of the poor performance of the lexicalized models.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 25, "end_pos": 36, "type": "TASK", "confidence": 0.7667575478553772}]}, {"text": "The impact of perfect tagging is less drastic on the lexicalized models (around 1% increase).", "labels": [], "entities": []}, {"text": "However, our main finding, viz., that lexicalized models outperform unlexicalized models considerable on the FTB, remains valid, even with perfect tagging.", "labels": [], "entities": [{"text": "FTB", "start_pos": 109, "end_pos": 112, "type": "DATASET", "confidence": 0.9721001386642456}]}, {"text": "3 Dependency Evaluation We also evaluated our models using dependency measures, which have been argued to be more annotation-neutral than Parseval.", "labels": [], "entities": []}, {"text": "notes that labeled bracketing scores are more susceptible to cascading errors, where one incorrect attachment decision causes the scoring algorithm to count more than one error.", "labels": [], "entities": []}, {"text": "The gold standard and parsed trees were converted into dependency trees using the algorithm described by.", "labels": [], "entities": []}, {"text": "Dependency accuracy is defined as the ratio of correct dependencies over the total number of dependencies in a sentence.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.9030424952507019}]}, {"text": "(Note that this is an unlabeled dependency measure.)", "labels": [], "entities": []}, {"text": "Dependency accuracy and constituency F-score are shown for the most relevant FTB models.", "labels": [], "entities": [{"text": "Dependency", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.821327269077301}, {"text": "accuracy", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.7679408192634583}, {"text": "F-score", "start_pos": 37, "end_pos": 44, "type": "METRIC", "confidence": 0.7379486560821533}, {"text": "FTB", "start_pos": 77, "end_pos": 80, "type": "DATASET", "confidence": 0.7128998637199402}]}, {"text": "(Fscore is computed as the geometric mean of labeled recall and precision.)", "labels": [], "entities": [{"text": "Fscore", "start_pos": 1, "end_pos": 7, "type": "METRIC", "confidence": 0.9993411898612976}, {"text": "recall", "start_pos": 53, "end_pos": 59, "type": "METRIC", "confidence": 0.9802369475364685}, {"text": "precision", "start_pos": 64, "end_pos": 73, "type": "METRIC", "confidence": 0.9977637529373169}]}, {"text": "Numerically, dependency accuracies are higher than constituency F-scores across the board.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 24, "end_pos": 34, "type": "METRIC", "confidence": 0.5545483231544495}, {"text": "F-scores", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.7585067749023438}]}, {"text": "However, the effect of lexicalization is the same on both measures: for the FTB, again of 11% in dependency accuracy is observed for the lexicalized model.", "labels": [], "entities": [{"text": "FTB", "start_pos": 76, "end_pos": 79, "type": "DATASET", "confidence": 0.7909907698631287}, {"text": "accuracy", "start_pos": 108, "end_pos": 116, "type": "METRIC", "confidence": 0.9010778665542603}]}, {"text": "The results reported in Experiments 1 and 2 shed some light on the role of lexicalization for parsing French, but they are not strictly comparable to the results that have been reported for other languages.", "labels": [], "entities": [{"text": "parsing French", "start_pos": 94, "end_pos": 108, "type": "TASK", "confidence": 0.9161984622478485}]}, {"text": "This is because the treebanks available for different languages typically vary considerably in size: our FTB training set was about 8,500 sentences large, while the standard training set for the PTB is about 40,000 sentences in size, and the Negra training set used by comprises about 18,600 sentences.", "labels": [], "entities": [{"text": "FTB training set", "start_pos": 105, "end_pos": 121, "type": "DATASET", "confidence": 0.9164533813794454}, {"text": "PTB", "start_pos": 195, "end_pos": 198, "type": "DATASET", "confidence": 0.9397296905517578}, {"text": "Negra training set", "start_pos": 242, "end_pos": 260, "type": "DATASET", "confidence": 0.8811438083648682}]}, {"text": "This means that the differences in the effect of lexicalization that we observe could be simply due to the size of the training set: lexicalized models are more susceptible to data sparseness than unlexicalized ones.", "labels": [], "entities": []}, {"text": "We therefore conducted another experiment in which we applied Collins' Model 2 to subsets of the PTB that were comparable in size to our FTB data sets.", "labels": [], "entities": [{"text": "Collins' Model 2", "start_pos": 62, "end_pos": 78, "type": "DATASET", "confidence": 0.9256773591041565}, {"text": "PTB", "start_pos": 97, "end_pos": 100, "type": "DATASET", "confidence": 0.9610155820846558}, {"text": "FTB data sets", "start_pos": 137, "end_pos": 150, "type": "DATASET", "confidence": 0.9782333970069885}]}, {"text": "We combined sections 02-05 and 08 of the PTB (8,345 sentences in total) to form the training set, and the first 1,000 sentences of section 23 to form our test set.", "labels": [], "entities": [{"text": "PTB", "start_pos": 41, "end_pos": 44, "type": "DATASET", "confidence": 0.8316872715950012}]}, {"text": "As a baseline model, we also run an unlexicalized PCFG on the same data sets.", "labels": [], "entities": []}, {"text": "For comparison with Negra, we also include the results of: they report the performance of Collins' Model 1 on a data set of 9,301 sentences and a test set of 1,000 sentences, which are comparable in size to our FTB data sets.", "labels": [], "entities": [{"text": "Negra", "start_pos": 20, "end_pos": 25, "type": "DATASET", "confidence": 0.9194818735122681}, {"text": "Collins' Model 1", "start_pos": 90, "end_pos": 106, "type": "DATASET", "confidence": 0.927336593468984}, {"text": "FTB data sets", "start_pos": 211, "end_pos": 224, "type": "DATASET", "confidence": 0.9858746727307638}]}, {"text": "The results of the crosslinguistic comparison are shown in.", "labels": [], "entities": []}, {"text": "We conclude that the effect of report only F-scores for the reduced data set (see their); the other scores were provided by Amit Dubey.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.9953708052635193}]}, {"text": "No results for Model 2 are available.", "labels": [], "entities": []}, {"text": "For this experiments, the same POS tagging model was applied to the PTB and the FTB data, which is why the FTB fig-: The effect of lexicalization on different corpora for training sets of comparable size (sentences \u226440 words) lexicalization is stable even if the size of the training set is held constant across languages: For the FTB we find that lexicalization increases F-score by around 13%.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 31, "end_pos": 42, "type": "TASK", "confidence": 0.6801346093416214}, {"text": "PTB", "start_pos": 68, "end_pos": 71, "type": "DATASET", "confidence": 0.9598667025566101}, {"text": "FTB data", "start_pos": 80, "end_pos": 88, "type": "DATASET", "confidence": 0.9826075434684753}, {"text": "FTB fig", "start_pos": 107, "end_pos": 114, "type": "DATASET", "confidence": 0.7975369095802307}, {"text": "FTB", "start_pos": 331, "end_pos": 334, "type": "DATASET", "confidence": 0.9476021528244019}, {"text": "F-score", "start_pos": 373, "end_pos": 380, "type": "METRIC", "confidence": 0.9815391898155212}]}, {"text": "Also for the PTB, we find an effect of lexicalization of about 14%.", "labels": [], "entities": [{"text": "PTB", "start_pos": 13, "end_pos": 16, "type": "DATASET", "confidence": 0.7343358993530273}]}, {"text": "For the German Negra treebank, however, the performance of the lexicalized and the unlexicalized model are almost indistinguishable.", "labels": [], "entities": [{"text": "German Negra treebank", "start_pos": 8, "end_pos": 29, "type": "DATASET", "confidence": 0.7836611270904541}]}, {"text": "(This is true for Collins' Model 1; note that do report a small improvement for the lexicalized sister-head model.)", "labels": [], "entities": [{"text": "Collins' Model 1", "start_pos": 18, "end_pos": 34, "type": "DATASET", "confidence": 0.9268075029055277}]}], "tableCaptions": [{"text": " Table 1: Results for unlexicalized models (sentences  \u226440 words); each model performed its own POS  tagging.", "labels": [], "entities": [{"text": "POS  tagging", "start_pos": 96, "end_pos": 108, "type": "TASK", "confidence": 0.7252914309501648}]}, {"text": " Table 2: Average number of daughter nodes per con- stituents in three treebanks", "labels": [], "entities": []}, {"text": " Table 3: Results for lexicalized models (sentences  \u226440 words); each model performed its own POS  tagging; all lexicalized models used the Cont+CR  data set", "labels": [], "entities": [{"text": "POS  tagging", "start_pos": 94, "end_pos": 106, "type": "TASK", "confidence": 0.6758096218109131}, {"text": "Cont+CR  data set", "start_pos": 140, "end_pos": 157, "type": "DATASET", "confidence": 0.6751937985420227}]}, {"text": " Table 4: Results for lexicalized and unlexical- ized models (sentences \u226440 words) with correct  POS tags supplied; all lexicalized models used the  Cont+CR data set", "labels": [], "entities": [{"text": "Cont+CR data set", "start_pos": 149, "end_pos": 165, "type": "DATASET", "confidence": 0.6646591365337372}]}, {"text": " Table 5: Dependency vs. constituency scores for lex- icalized and unlexicalized models", "labels": [], "entities": []}, {"text": " Table 6: The effect of lexicalization on different cor- pora for training sets of comparable size (sentences  \u226440 words)", "labels": [], "entities": []}]}