{"title": [{"text": "Machine Translation Using Probabilistic Synchronous Dependency Insertion Grammars", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.756372720003128}, {"text": "Synchronous Dependency Insertion Grammars", "start_pos": 40, "end_pos": 81, "type": "TASK", "confidence": 0.7035006284713745}]}], "abstractContent": [{"text": "Syntax-based statistical machine translation (MT) aims at applying statistical models to structured data.", "labels": [], "entities": [{"text": "Syntax-based statistical machine translation (MT)", "start_pos": 0, "end_pos": 49, "type": "TASK", "confidence": 0.7410822297845568}]}, {"text": "In this paper, we present a syntax-based statistical machine translation system based on a prob-abilistic synchronous dependency insertion grammar.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 41, "end_pos": 72, "type": "TASK", "confidence": 0.6089624067147573}, {"text": "prob-abilistic synchronous dependency insertion grammar", "start_pos": 91, "end_pos": 146, "type": "TASK", "confidence": 0.7229957222938538}]}, {"text": "Synchronous dependency insertion grammars area version of synchronous grammars defined on dependency trees.", "labels": [], "entities": [{"text": "dependency insertion grammars", "start_pos": 12, "end_pos": 41, "type": "TASK", "confidence": 0.7676962415377299}]}, {"text": "We first introduce our approach to inducing such a grammar from parallel corpora.", "labels": [], "entities": []}, {"text": "Second, we describe the graphical model for the machine translation task, which can also be viewed as a stochastic tree-to-tree transducer.", "labels": [], "entities": [{"text": "machine translation task", "start_pos": 48, "end_pos": 72, "type": "TASK", "confidence": 0.828929622968038}]}, {"text": "We introduce a polynomial time decoding algorithm for the model.", "labels": [], "entities": []}, {"text": "We evaluate the outputs of our MT system using the NIST and Bleu automatic MT evaluation software.", "labels": [], "entities": [{"text": "MT", "start_pos": 31, "end_pos": 33, "type": "TASK", "confidence": 0.9907546639442444}, {"text": "NIST", "start_pos": 51, "end_pos": 55, "type": "DATASET", "confidence": 0.970734715461731}, {"text": "MT evaluation", "start_pos": 75, "end_pos": 88, "type": "TASK", "confidence": 0.8233189880847931}]}, {"text": "The result shows that our system outperforms the baseline system based on the IBM models in both translation speed and quality.", "labels": [], "entities": [{"text": "translation", "start_pos": 97, "end_pos": 108, "type": "TASK", "confidence": 0.949537992477417}]}], "introductionContent": [{"text": "Statistical approaches to machine translation, pioneered by, achieved impressive performance by leveraging large amounts of parallel corpora.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 26, "end_pos": 45, "type": "TASK", "confidence": 0.8006519079208374}]}, {"text": "Such approaches, which are essentially stochastic string-to-string transducers, do not explicitly model natural language syntax or semantics.", "labels": [], "entities": []}, {"text": "In reality, pure statistical systems sometimes suffer from ungrammatical outputs, which are understandable at the phrasal level but sometimes hard to comprehend as a coherent sentence.", "labels": [], "entities": []}, {"text": "In recent years, syntax-based statistical machine translation, which aims at applying statistical models to structural data, has begun to emerge.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 30, "end_pos": 61, "type": "TASK", "confidence": 0.6159386932849884}]}, {"text": "With the research advances in natural language parsing, especially the broad-coverage parsers trained from treebanks, for example, the utilization of structural analysis of different languages has been made possible.", "labels": [], "entities": [{"text": "natural language parsing", "start_pos": 30, "end_pos": 54, "type": "TASK", "confidence": 0.6676328778266907}]}, {"text": "Ideally, by combining the natural language syntax and machine learning methods, a broad-coverage and linguistically wellmotivated statistical MT system can be constructed.", "labels": [], "entities": [{"text": "MT", "start_pos": 142, "end_pos": 144, "type": "TASK", "confidence": 0.9303721189498901}]}, {"text": "However, structural divergences between languages\uff0cwhich are due to either systematic differences between languages or loose translations in real corpora\uff0cpose a major challenge to syntax-based statistical MT.", "labels": [], "entities": [{"text": "MT", "start_pos": 204, "end_pos": 206, "type": "TASK", "confidence": 0.8833889961242676}]}, {"text": "As a result, the syntax based MT systems have to transduce between non-isomorphic tree structures.", "labels": [], "entities": [{"text": "MT", "start_pos": 30, "end_pos": 32, "type": "TASK", "confidence": 0.9726415276527405}]}, {"text": "() introduced a polynomial-time solution for the alignment problem based on synchronous binary trees.", "labels": [], "entities": [{"text": "alignment problem", "start_pos": 49, "end_pos": 66, "type": "TASK", "confidence": 0.9344721436500549}]}, {"text": "() represents each production in parallel dependency trees as a finite-state transducer.", "labels": [], "entities": []}, {"text": "Both approaches learn the tree representations directly from parallel sentences, and do not make allowances for nonisomorphic structures.", "labels": [], "entities": []}, {"text": "() modeled translation as a sequence of tree operations transforming a syntactic tree into a string of the target language.", "labels": [], "entities": []}, {"text": "When researchers try to use syntax trees in both languages, the problem of non-isomorphism must be addressed.", "labels": [], "entities": []}, {"text": "In theory, stochastic tree transducers and some versions of synchronous grammars provide solutions for the non-isomorphic tree based transduction problem and hence possible solutions for MT.", "labels": [], "entities": [{"text": "MT", "start_pos": 187, "end_pos": 189, "type": "TASK", "confidence": 0.9905656576156616}]}, {"text": "Synchronous Tree Adjoining Grammars, proposed by), were introduced primarily for semantics but were later also proposed for translation.", "labels": [], "entities": [{"text": "translation", "start_pos": 124, "end_pos": 135, "type": "TASK", "confidence": 0.9686183929443359}]}, {"text": "proposed viewing the MT problem as a probabilistic synchronous tree substitution grammar parsing problem.) formalized the MT problem as synchronous parsing based on multitext grammars.", "labels": [], "entities": [{"text": "MT problem", "start_pos": 21, "end_pos": 31, "type": "TASK", "confidence": 0.9085127711296082}, {"text": "synchronous tree substitution grammar parsing", "start_pos": 51, "end_pos": 96, "type": "TASK", "confidence": 0.7267455458641052}, {"text": "MT problem", "start_pos": 122, "end_pos": 132, "type": "TASK", "confidence": 0.9194734692573547}]}, {"text": "defined training and decoding algorithms for both generalized tree-to-tree and tree-to-string transducers.", "labels": [], "entities": []}, {"text": "All these approaches, though different in formalism, model the two languages using tree-based transduction rules or asynchronous grammar, possibly probabilistic, and using multi-lemma elementary structures as atomic units.", "labels": [], "entities": []}, {"text": "The machine translation is done either as a stochastic tree-to-tree transduction or asynchronous parsing process.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.6626946777105331}]}, {"text": "However, few of the above mentioned formalisms have large scale implementations.", "labels": [], "entities": []}, {"text": "And to the best of our knowledge, the advantages of syntax based statistical MT systems over pure statistical MT systems have yet to be empirically verified.", "labels": [], "entities": []}, {"text": "We believe difficulties in inducing asynchronous grammar or a set of tree transduction rules from large scale parallel corpora are caused by: 1.", "labels": [], "entities": []}, {"text": "The abilities of synchronous grammars and tree transducers to handle non-isomorphism are limited.", "labels": [], "entities": []}, {"text": "At some level, asynchronous derivation process must exist between the source and target language sentences.", "labels": [], "entities": []}, {"text": "2. The training and/or induction of asynchronous grammar or a set of transduction rules are usually computationally expensive if all the possible operations and elementary structures are allowed.", "labels": [], "entities": []}, {"text": "The exhaustive search for all the possible sub-sentential structures in a syntax tree of a sentence is NP-complete.", "labels": [], "entities": []}, {"text": "3. The problem is aggravated by the non-perfect training corpora.", "labels": [], "entities": []}, {"text": "Loose translations are less of a problem for string based approaches than for approaches that require syntactic analysis.", "labels": [], "entities": [{"text": "Loose translations", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8065645694732666}]}, {"text": "limited non-isomorphism by n-to-m matching of nodes in the two trees.", "labels": [], "entities": []}, {"text": "However, even after extending this model by allowing cloning operations on subtrees, found that parallel trees over-constrained the alignment problem, and achieved better results with a tree-to-string model than with a tree-to-tree model using two trees.", "labels": [], "entities": []}, {"text": "Ina different approach, aligned the parallel sentences using phrase based statistical MT models and then projected the alignments back to the parse trees.", "labels": [], "entities": []}, {"text": "This motivated us to look fora more efficient and effective way to induce asynchronous grammar from parallel corpora and to build an MT system that performs competitively with the pure statistical MT systems.", "labels": [], "entities": [{"text": "MT", "start_pos": 133, "end_pos": 135, "type": "TASK", "confidence": 0.9692226052284241}]}, {"text": "We chose to build the synchronous grammar on the parallel dependency structures of the sentences.", "labels": [], "entities": []}, {"text": "The synchronous grammar is induced by hierarchical tree partitioning operations.", "labels": [], "entities": []}, {"text": "The rest of this paper describes the system details as follows: Sections 2 and 3 describe the motivation behind the usage of dependency structures and how aversion of synchronous dependency grammar is learned.", "labels": [], "entities": []}, {"text": "This grammar is used as the primary translation knowledge source for our system.", "labels": [], "entities": []}, {"text": "Section 4 defines the tree-to-tree transducer and the graphical model for the stochastic tree-to-tree transduction process and introduces a polynomial time decoding algorithm for the transducer.", "labels": [], "entities": []}, {"text": "We evaluate our system in section 5 with the NIST/Bleu automatic MT evaluation software and the results are discussed in Section 6.", "labels": [], "entities": [{"text": "NIST/Bleu automatic", "start_pos": 45, "end_pos": 64, "type": "DATASET", "confidence": 0.8969003558158875}, {"text": "MT evaluation", "start_pos": 65, "end_pos": 78, "type": "TASK", "confidence": 0.6071861088275909}]}], "datasetContent": [{"text": "We implemented the above approach fora Chinese-English machine translation system.", "labels": [], "entities": [{"text": "Chinese-English machine translation", "start_pos": 39, "end_pos": 74, "type": "TASK", "confidence": 0.5992629925409952}]}, {"text": "We used an automatic syntactic parser) to produce the parallel parse trees.", "labels": [], "entities": []}, {"text": "The parser was trained using the Penn English/Chinese Treebanks.", "labels": [], "entities": [{"text": "Penn English/Chinese Treebanks", "start_pos": 33, "end_pos": 63, "type": "DATASET", "confidence": 0.9728078126907349}]}, {"text": "We then used the algorithm in (Xia 2001) to convert the phrasal structure trees to dependency trees to acquire the parallel dependency trees.", "labels": [], "entities": []}, {"text": "The statistics of the datasets we used are shown as follows:.", "labels": [], "entities": []}, {"text": "Evaluation data details The training set consists of Xinhua newswire data from LDC and the FBIS data (mostly news), both filtered to ensure parallel sentence pair quality.", "labels": [], "entities": [{"text": "Xinhua newswire data from LDC", "start_pos": 53, "end_pos": 82, "type": "DATASET", "confidence": 0.8558686971664429}, {"text": "FBIS data", "start_pos": 91, "end_pos": 100, "type": "DATASET", "confidence": 0.9029258191585541}]}, {"text": "We used the development test data from the 2001 NIST MT evaluation workshop as our test data for the MT system performance.", "labels": [], "entities": [{"text": "development test data from the 2001 NIST MT evaluation workshop", "start_pos": 12, "end_pos": 75, "type": "DATASET", "confidence": 0.6516679018735886}, {"text": "MT", "start_pos": 101, "end_pos": 103, "type": "TASK", "confidence": 0.992108166217804}]}, {"text": "In the testing data, each input Chinese sentence has 4 English translations as references.", "labels": [], "entities": []}, {"text": "Our MT system was evaluated using the n-gram based Bleu () and NIST machine translation evaluation software.", "labels": [], "entities": [{"text": "MT", "start_pos": 4, "end_pos": 6, "type": "TASK", "confidence": 0.9834852814674377}, {"text": "Bleu", "start_pos": 51, "end_pos": 55, "type": "METRIC", "confidence": 0.7143701910972595}, {"text": "NIST machine translation evaluation", "start_pos": 63, "end_pos": 98, "type": "TASK", "confidence": 0.8313528597354889}]}, {"text": "We used the NIST software package \"mteval\" version 11a, configured as case-insensitive.", "labels": [], "entities": [{"text": "NIST software package \"mteval\" version 11a", "start_pos": 12, "end_pos": 54, "type": "DATASET", "confidence": 0.8998471200466156}]}, {"text": "In comparison, we deployed the GIZA++ MT modeling toolkit, which is an implementation of the IBM Models 1 to 4 (.", "labels": [], "entities": [{"text": "GIZA++ MT modeling", "start_pos": 31, "end_pos": 49, "type": "TASK", "confidence": 0.6965895146131516}]}, {"text": "The IBM models were trained on the same training data as our system.", "labels": [], "entities": []}, {"text": "We used the ISI Rewrite decoder () to decode the IBM models.", "labels": [], "entities": [{"text": "ISI Rewrite decoder", "start_pos": 12, "end_pos": 31, "type": "DATASET", "confidence": 0.8547675609588623}, {"text": "IBM models", "start_pos": 49, "end_pos": 59, "type": "DATASET", "confidence": 0.9362320005893707}]}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "The score types \"I\" and \"C\" stand for individual and cumulative n-gram scores.", "labels": [], "entities": []}, {"text": "The final NIST and Bleu scores are marked with bold fonts.", "labels": [], "entities": [{"text": "NIST and Bleu scores", "start_pos": 10, "end_pos": 30, "type": "DATASET", "confidence": 0.7910262048244476}]}, {"text": "The evaluation results show that the NIST score achieved a 97.3% increase, while the Bleu score increased by 21.1%.", "labels": [], "entities": [{"text": "NIST score", "start_pos": 37, "end_pos": 47, "type": "DATASET", "confidence": 0.8397437930107117}, {"text": "Bleu score", "start_pos": 85, "end_pos": 95, "type": "METRIC", "confidence": 0.9840252697467804}]}, {"text": "In terms of decoding speed, the Rewrite decoder took 8102 seconds to decode the test sentences on a Xeon 1.2GHz 2GB memory machine.", "labels": [], "entities": [{"text": "Rewrite decoder", "start_pos": 32, "end_pos": 47, "type": "DATASET", "confidence": 0.8587608635425568}]}, {"text": "On the same machine, the SDIG decoder took 3 seconds to decode, excluding the parsing time.", "labels": [], "entities": [{"text": "SDIG decoder", "start_pos": 25, "end_pos": 37, "type": "DATASET", "confidence": 0.9199904501438141}, {"text": "parsing", "start_pos": 78, "end_pos": 85, "type": "TASK", "confidence": 0.9679310321807861}]}, {"text": "The recent advances in parsing have achieved parsers with 3 ( ) O n time complexity without the grammar constant ().", "labels": [], "entities": [{"text": "parsing", "start_pos": 23, "end_pos": 30, "type": "TASK", "confidence": 0.9696951508522034}]}, {"text": "It can be expected that the total decoding time for SDIG can be as short as 0.1 second per sentence.", "labels": [], "entities": []}, {"text": "Neither of the two systems has any specific translation components, which are usually present in real world systems (E.g. components that translate numbers, dates, names, etc.)", "labels": [], "entities": []}, {"text": "It is reasonable to expect that the performance of SDIG can be further improved with such specific optimizations.", "labels": [], "entities": []}], "tableCaptions": []}