{"title": [{"text": "Using bilingual dependencies to align words in Enlish/French parallel corpora", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper describes a word and phrase alignment approach based on a dependency analysis of French/English parallel corpora, referred to as alignment by \"syn-tax-based propagation.\"", "labels": [], "entities": [{"text": "word and phrase alignment", "start_pos": 23, "end_pos": 48, "type": "TASK", "confidence": 0.5906598642468452}, {"text": "syn-tax-based propagation", "start_pos": 154, "end_pos": 179, "type": "TASK", "confidence": 0.7115364223718643}]}, {"text": "Both corpora are analysed with a deep and robust dependency parser.", "labels": [], "entities": []}, {"text": "Starting with an anchor pair consisting of two words that are translations of one another within aligned sentences , the alignment link is propagated to syntactically connected words.", "labels": [], "entities": []}], "introductionContent": [{"text": "It is now an acknowledged fact that alignment of parallel corpora at the word and phrase level plays a major role in bilingual linguistic resource extraction and machine translation.", "labels": [], "entities": [{"text": "bilingual linguistic resource extraction", "start_pos": 117, "end_pos": 157, "type": "TASK", "confidence": 0.6267511621117592}, {"text": "machine translation", "start_pos": 162, "end_pos": 181, "type": "TASK", "confidence": 0.7830946445465088}]}, {"text": "There are basically two kinds of systems working at these segmentation levels: the most widespread rely on statistical models, in particular the IBM ones; others combine simpler association measures with different kinds of linguistic information (.", "labels": [], "entities": []}, {"text": "Mainly dedicated to machine translation, purely statistical systems have gradually been enriched with syntactic knowledge.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 20, "end_pos": 39, "type": "TASK", "confidence": 0.8003811240196228}]}, {"text": "As pointed out in these studies, the introduction of linguistic knowledge leads to a significant improvement in alignment quality.", "labels": [], "entities": []}, {"text": "In the method described hereafter, syntactic information is the kernel of the alignment process.", "labels": [], "entities": []}, {"text": "Indeed, syntactic dependencies identified on both sides of English/French bitexts with a parser are used to discover correspondences between words.", "labels": [], "entities": []}, {"text": "This approach has been chosen in order to capture frequent alignments as well as sparse and/or corpus-specific ones.", "labels": [], "entities": []}, {"text": "Moreover, as stressed in previous research, using syntactic dependencies seems to be particularly well suited to coping with the problem of linguistic variation across languages).", "labels": [], "entities": []}, {"text": "The implemented procedure is referred to as \"syntax-based propagation\".", "labels": [], "entities": [{"text": "syntax-based propagation", "start_pos": 45, "end_pos": 69, "type": "TASK", "confidence": 0.7216526120901108}]}], "datasetContent": [{"text": "The results achieved using the syntax-based alignment (sba) are compared to those obtained with the baseline provided by the IBM models implemented in the giza++ package) and).", "labels": [], "entities": []}, {"text": "More precisely, we used the intersection of IBM-4 Viterbi alignments for both translation directions.", "labels": [], "entities": [{"text": "IBM-4 Viterbi alignments", "start_pos": 44, "end_pos": 68, "type": "DATASET", "confidence": 0.8966541290283203}]}, {"text": "shows the precision assessed against a reference set of 1000 alignments manually annotated in the INRA and the JOC corpus respectively.", "labels": [], "entities": [{"text": "precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9994934797286987}, {"text": "INRA", "start_pos": 98, "end_pos": 102, "type": "DATASET", "confidence": 0.7599149942398071}, {"text": "JOC corpus", "start_pos": 111, "end_pos": 121, "type": "DATASET", "confidence": 0.9593655169010162}]}, {"text": "It can be observed that the syntax-based alignment offers good accuracy, similar to that of the baseline..", "labels": [], "entities": [{"text": "accuracy", "start_pos": 63, "end_pos": 71, "type": "METRIC", "confidence": 0.9993230104446411}]}, {"text": "They have been obtained using reference data from an evaluation of word alignment systems.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 67, "end_pos": 81, "type": "TASK", "confidence": 0.7699830532073975}]}, {"text": "It should be noted that the figures concerning the syntax-based alignment were assessed in respect to the annotations that do not involve empty words, since up to now we focused only on content words.", "labels": [], "entities": []}, {"text": "Whereas the baseline precision for the HLT corpus is comparable to the one reported in, the syntax-based alignment score decreases.", "labels": [], "entities": [{"text": "precision", "start_pos": 21, "end_pos": 30, "type": "METRIC", "confidence": 0.845469057559967}, {"text": "HLT corpus", "start_pos": 39, "end_pos": 49, "type": "DATASET", "confidence": 0.7525314092636108}, {"text": "syntax-based alignment score", "start_pos": 92, "end_pos": 120, "type": "METRIC", "confidence": 0.690286268790563}]}, {"text": "Moreover, the difference between the two approaches is considerable with regard to the recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 87, "end_pos": 93, "type": "METRIC", "confidence": 0.9954878687858582}]}, {"text": "This maybe due to the fact that our syntaxbased alignment approach basically relies on isomorphic syntactic structures, i.e. in which the two following conditions are met: i) the relation under consideration is identical in both languages and ii) the words involved in the syntactic propagation have the same POS.", "labels": [], "entities": [{"text": "syntaxbased alignment", "start_pos": 36, "end_pos": 57, "type": "TASK", "confidence": 0.7129919677972794}, {"text": "syntactic propagation", "start_pos": 273, "end_pos": 294, "type": "TASK", "confidence": 0.7328237891197205}, {"text": "POS", "start_pos": 309, "end_pos": 312, "type": "METRIC", "confidence": 0.9724342823028564}]}, {"text": "Most of the cases of nonisomorphism, apart from the ones presented section 5.1, are not taken into account.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1 gives some characteristics of the corpora.", "labels": [], "entities": []}, {"text": " Table 2. sba ~ giza++: INRA & JOC", "labels": [], "entities": [{"text": "INRA", "start_pos": 24, "end_pos": 28, "type": "METRIC", "confidence": 0.9884948134422302}, {"text": "JOC", "start_pos": 31, "end_pos": 34, "type": "DATASET", "confidence": 0.6399903893470764}]}, {"text": " Table 3. They have been  obtained using reference data from an evaluation  of word alignment systems", "labels": [], "entities": [{"text": "word alignment", "start_pos": 79, "end_pos": 93, "type": "TASK", "confidence": 0.737504705786705}]}, {"text": " Table 3. sba ~ giza++: HLT", "labels": [], "entities": [{"text": "HLT", "start_pos": 24, "end_pos": 27, "type": "DATASET", "confidence": 0.54803067445755}]}]}