{"title": [{"text": "An Unsupervised System for Identifying English Inclusions in German Text", "labels": [], "entities": [{"text": "Identifying English Inclusions in German", "start_pos": 27, "end_pos": 67, "type": "TASK", "confidence": 0.9110359787940979}]}], "abstractContent": [{"text": "We present an unsupervised system that exploits linguistic knowledge resources, namely English and German lexical databases and the World Wide Web, to identify English inclusions in German text.", "labels": [], "entities": []}, {"text": "We describe experiments with this system and the corpus which was developed for this task.", "labels": [], "entities": []}, {"text": "We report the classification results of our system and compare them to the performance of a trained machine learner in a series of in-and cross-domain experiments.", "labels": [], "entities": []}], "introductionContent": [{"text": "The recognition of foreign words and foreign named entities (NEs) in otherwise mono-lingual text is beyond the capability of many existing approaches and is only starting to be addressed.", "labels": [], "entities": [{"text": "recognition of foreign words and foreign named entities (NEs) in otherwise mono-lingual text", "start_pos": 4, "end_pos": 96, "type": "TASK", "confidence": 0.8729459166526794}]}, {"text": "This language mixing phenomenon is prevalent in German where the number of anglicisms has increased considerably.", "labels": [], "entities": []}, {"text": "We have developed an unsupervised and highly efficient system that identifies English inclusions in German text by means of a computationally inexpensive lookup procedure.", "labels": [], "entities": [{"text": "identifies English inclusions in German text", "start_pos": 67, "end_pos": 111, "type": "TASK", "confidence": 0.8096799949804941}]}, {"text": "By unsupervised we mean that the system does not require any annotated training data and only relies on lexicons and the Web.", "labels": [], "entities": []}, {"text": "Our system allows linguists and lexicographers to observe language changes overtime, and to investigate the use and frequency of foreign words in a given language and domain.", "labels": [], "entities": []}, {"text": "The output also represents valuable information fora number of applications, including polyglot text-to-speech (TTS) synthesis and machine translation (MT).", "labels": [], "entities": [{"text": "polyglot text-to-speech (TTS) synthesis", "start_pos": 87, "end_pos": 126, "type": "TASK", "confidence": 0.6738307376702627}, {"text": "machine translation (MT)", "start_pos": 131, "end_pos": 155, "type": "TASK", "confidence": 0.8290260791778564}]}, {"text": "We will first explain the issue of foreign inclusions in German text in greater detail with examples in Section 2.", "labels": [], "entities": [{"text": "foreign inclusions in German text", "start_pos": 35, "end_pos": 68, "type": "TASK", "confidence": 0.7662253379821777}]}, {"text": "Sections 3 and 4 describe the data we used and the architecture of our system.", "labels": [], "entities": []}, {"text": "In Section 5, we provide an evaluation of the system output and compare the results with those of a series of in-and cross-domain machine learning experiments outlined in Section 6.", "labels": [], "entities": []}, {"text": "We conclude and outline future work in Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated the system's performance for all tokens against the gold standard.", "labels": [], "entities": []}, {"text": "While the accuracies in represent the percentage of all correctly tagged tokens, the F-scores refer to the English tokens and are calculated giving equal weight to precision (P) and recall (R) as . The system yields relatively high F-scores of 72.4 and 73.1 for the internet and space travel data but only a low F-score of 38.6 for the EU data.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 85, "end_pos": 93, "type": "METRIC", "confidence": 0.9943423271179199}, {"text": "precision (P)", "start_pos": 164, "end_pos": 177, "type": "METRIC", "confidence": 0.942004069685936}, {"text": "recall (R)", "start_pos": 182, "end_pos": 192, "type": "METRIC", "confidence": 0.9546458125114441}, {"text": "F-scores", "start_pos": 232, "end_pos": 240, "type": "METRIC", "confidence": 0.9951295852661133}, {"text": "F-score", "start_pos": 312, "end_pos": 319, "type": "METRIC", "confidence": 0.998666524887085}, {"text": "EU data", "start_pos": 336, "end_pos": 343, "type": "DATASET", "confidence": 0.8895387649536133}]}, {"text": "The latter is due to the sparseness of English inclusions in that domain.", "labels": [], "entities": []}, {"text": "Although recall for this data is comparable to that of the other two domains, the number of false positives is high, causing low precision and F-score.", "labels": [], "entities": [{"text": "recall", "start_pos": 9, "end_pos": 15, "type": "METRIC", "confidence": 0.9994468092918396}, {"text": "precision", "start_pos": 129, "end_pos": 138, "type": "METRIC", "confidence": 0.9994458556175232}, {"text": "F-score", "start_pos": 143, "end_pos": 150, "type": "METRIC", "confidence": 0.998930037021637}]}, {"text": "As the system does not lookup one-character tokens, we implemented further postprocessing to classify individual characters as English if followed by a hyphen and an English inclusion.", "labels": [], "entities": []}, {"text": "This improves the F-score by 4.8 for the internet data to 77.2 and by 0.6 for the space travel data to 73.7 as both data sets contain words like E-Mail or E-Business.", "labels": [], "entities": [{"text": "F-score", "start_pos": 18, "end_pos": 25, "type": "METRIC", "confidence": 0.9995644688606262}]}, {"text": "Post-processing does not decrease the EU score.", "labels": [], "entities": [{"text": "EU score", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.96369868516922}]}, {"text": "This indicates that domain-specific postprocessing can improve performance.", "labels": [], "entities": []}, {"text": "Baseline accuracies when assuming that all tokens are German are also listed in.", "labels": [], "entities": []}, {"text": "As Fscores are calculated based on the English tokens in the gold standard, we cannot report comparable baseline F-scores.", "labels": [], "entities": [{"text": "Fscores", "start_pos": 3, "end_pos": 10, "type": "METRIC", "confidence": 0.9936564564704895}, {"text": "English tokens in the gold standard", "start_pos": 39, "end_pos": 74, "type": "DATASET", "confidence": 0.7905546824137369}, {"text": "F-scores", "start_pos": 113, "end_pos": 121, "type": "METRIC", "confidence": 0.9395194053649902}]}, {"text": "Unsurprisingly, the baseline accuracies are relatively high as most tokens in a German text are German and the amount of foreign material is relatively small.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 29, "end_pos": 39, "type": "METRIC", "confidence": 0.7232319712638855}]}, {"text": "The added classification of English inclusions yielded highly statistical significant improvements (p\" 0.001) over the baseline of 3.5% for the internet data and 1.5% for the space travel data.", "labels": [], "entities": [{"text": "internet data", "start_pos": 144, "end_pos": 157, "type": "DATASET", "confidence": 0.7829715013504028}]}, {"text": "When classifying English inclusions in the EU data, accuracy decreased slightly by 0.3%.", "labels": [], "entities": [{"text": "classifying English inclusions", "start_pos": 5, "end_pos": 35, "type": "TASK", "confidence": 0.8658500512441}, {"text": "EU data", "start_pos": 43, "end_pos": 50, "type": "DATASET", "confidence": 0.7600391209125519}, {"text": "accuracy", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.9996809959411621}]}, {"text": "also shows the performance of TextCat, an n-gram-based text categorisation algorithm of.", "labels": [], "entities": []}, {"text": "While this language idenfication tool requires no lexicons, its F-scores are low for all 3 domains and very poor for the EU data.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.9968984127044678}, {"text": "EU data", "start_pos": 121, "end_pos": 128, "type": "DATASET", "confidence": 0.8126245141029358}]}, {"text": "This confirms that the identification of English inclusions is more difficult for this domain, coinciding with the result of the lookup system.", "labels": [], "entities": [{"text": "identification of English inclusions", "start_pos": 23, "end_pos": 59, "type": "TASK", "confidence": 0.8656782805919647}]}, {"text": "The low scores also prove that such language identification is unsuitable for token-based language classification.", "labels": [], "entities": [{"text": "language identification", "start_pos": 36, "end_pos": 59, "type": "TASK", "confidence": 0.7640112042427063}, {"text": "token-based language classification", "start_pos": 78, "end_pos": 113, "type": "TASK", "confidence": 0.7417746782302856}]}, {"text": "The recognition of foreign inclusions bears great similarity to classification tasks such as named entity recognition (NER), for which various machine learning techniques have proved successful.", "labels": [], "entities": [{"text": "recognition of foreign inclusions", "start_pos": 4, "end_pos": 37, "type": "TASK", "confidence": 0.831671953201294}, {"text": "named entity recognition (NER)", "start_pos": 93, "end_pos": 123, "type": "TASK", "confidence": 0.7667000989119211}]}, {"text": "We were therefore interested in determining the performance of a trained classifier for our task.", "labels": [], "entities": []}, {"text": "We experimented with a conditional Markov model tagger that performed well on language-independent NER () and the identification of gene and protein names ().", "labels": [], "entities": [{"text": "identification of gene and protein names", "start_pos": 114, "end_pos": 154, "type": "TASK", "confidence": 0.8277117709318796}]}, {"text": "We performed several 10-fold cross-validation experiments with different feature sets.", "labels": [], "entities": []}, {"text": "They are referred to as in-domain (ID) experiments as the tagger is trained and tested on data from the same domain).", "labels": [], "entities": []}, {"text": "In the first experiment (ID1), we use the tagger's standard feature set including words, character sub-strings, word shapes, POS-tags, abbreviations and NE tags ().", "labels": [], "entities": []}, {"text": "The resulting F-scores are high for the internet and space travel data (84.3 and 91.4) but are extremely low for the EU data (13.3) due to the sparseness of English inclusions in that data set.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9983464479446411}, {"text": "EU data", "start_pos": 117, "end_pos": 124, "type": "DATASET", "confidence": 0.7104619145393372}]}, {"text": "ID2 involves the same setup as ID1 but eliminating all features relying on the POS-tags.", "labels": [], "entities": [{"text": "ID2", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8576335310935974}]}, {"text": "The tagger performs similarly well for the internet and space travel data but improves by 8 points to an F-score of 21.3 for the EU data.", "labels": [], "entities": [{"text": "F-score", "start_pos": 105, "end_pos": 112, "type": "METRIC", "confidence": 0.9994816184043884}, {"text": "EU data", "start_pos": 129, "end_pos": 136, "type": "DATASET", "confidence": 0.8363089561462402}]}, {"text": "This can be attributed to the fact that the POS-tagger does not perform with perfect accuracy particularly on data containing foreign inclusions.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 85, "end_pos": 93, "type": "METRIC", "confidence": 0.9976822137832642}]}, {"text": "Providing the tagger with this information is therefore not necessarily useful for this task, especially when the data is sparse.", "labels": [], "entities": []}, {"text": "Nevertheless, there is a big discrepancy between the F-score for the EU data and those of the other two data sets.", "labels": [], "entities": [{"text": "F-score", "start_pos": 53, "end_pos": 60, "type": "METRIC", "confidence": 0.9987393021583557}, {"text": "EU data", "start_pos": 69, "end_pos": 76, "type": "DATASET", "confidence": 0.9189884066581726}]}, {"text": "ID3 and ID4 are setup as ID1 and ID2 but incorporating the output of the lookup system as a gazetteer feature.", "labels": [], "entities": []}, {"text": "The tagger benefits considerably from this lookup feature and yields better F-scores for all three domains in ID3 (internet: 90.6, space travel: 93.7, EU: 44.4).", "labels": [], "entities": [{"text": "F-scores", "start_pos": 76, "end_pos": 84, "type": "METRIC", "confidence": 0.9986181259155273}, {"text": "ID3", "start_pos": 110, "end_pos": 113, "type": "DATASET", "confidence": 0.9295264482498169}]}, {"text": "also compares the best F-scores produced with the tagger's own feature set (ID2) to the best results of the lookup system and the baseline.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9888651371002197}]}, {"text": "While the tagger performs much better for the internet and the space travel data, it requires hand-annotated training data.", "labels": [], "entities": []}, {"text": "The lookup system, on the other hand, is essentially unsupervised and therefore much more portable to new domains.", "labels": [], "entities": []}, {"text": "Given the necessary lexicons, it can easily be run over new text and text in a different language or domain without further cost.", "labels": [], "entities": []}, {"text": "The tagger achieved surprisingly high F-scores for the internet and space travel data, considering the small training data set of around 700 sentences used for each ID experiment described above.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.9989476799964905}]}, {"text": "Although both domains contain a large number of English inclusions, their type-token ratio amounts to 0.29 in the internet data and 0.15 in the space travel data (), signalling that English inclusions are frequently repeated in both domains.", "labels": [], "entities": [{"text": "internet data", "start_pos": 114, "end_pos": 127, "type": "DATASET", "confidence": 0.7829437255859375}]}, {"text": "As a result, the likelihood of the tagger encountering an unknown inclusion in the test data is relatively small.", "labels": [], "entities": []}, {"text": "To examine the tagger's performance on anew domain containing more unknown inclusions, we ran two cross-domain (CD) experiments: CD1, training on the internet and testing on the space travel data, and CD2, training on the space travel and testing on the internet data.", "labels": [], "entities": []}, {"text": "We chose these two domain pairs to ensure that both the training and test data contain a relatively large number of English inclusions.", "labels": [], "entities": []}, {"text": "shows that the F-scores for both CD experiments are much lower than those obtained when training and testing the tagger on documents from the same domain.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9985998272895813}]}, {"text": "In experiment CD1, the Fscore only amounts to 54.2 while the percentage of: Accuracies, F-scores and percentages of unknown target types (UTT) for cross-domain experiments compared to best lookup and baseline unknown target types in the space travel test data is 81.9%.", "labels": [], "entities": [{"text": "Fscore", "start_pos": 23, "end_pos": 29, "type": "METRIC", "confidence": 0.9990496039390564}, {"text": "Accuracies", "start_pos": 76, "end_pos": 86, "type": "METRIC", "confidence": 0.9990761280059814}, {"text": "F-scores and percentages of unknown target types (UTT)", "start_pos": 88, "end_pos": 142, "type": "METRIC", "confidence": 0.7498458981513977}, {"text": "space travel test data", "start_pos": 237, "end_pos": 259, "type": "DATASET", "confidence": 0.7046202421188354}]}, {"text": "The F-score is even lower in the second experiment at 22.2 which can be attributed to the fact that the percentage of unknown target types in the internet test data is higher still at 93.9%.", "labels": [], "entities": [{"text": "F-score", "start_pos": 4, "end_pos": 11, "type": "METRIC", "confidence": 0.9996054768562317}, {"text": "internet test data", "start_pos": 146, "end_pos": 164, "type": "DATASET", "confidence": 0.7788759271303812}]}, {"text": "These results indicate that the tagger's high performance in the ID experiments is largely due to the fact that the English inclusions in the test data are known, i.e. the tagger learns a lexicon.", "labels": [], "entities": []}, {"text": "It is therefore more complex to train a machine learning classifier to perform well on new data with more and more new anglicisms entering German overtime.", "labels": [], "entities": []}, {"text": "The amount of unknown tokens will increase constantly unless new annotated training data is added.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: English token and type statistics and type- token-ratios (TTR) in the gold standard", "labels": [], "entities": [{"text": "type- token-ratios (TTR)", "start_pos": 48, "end_pos": 72, "type": "METRIC", "confidence": 0.6514397412538528}]}, {"text": " Table 2: Raw counts (in million) and normalised  counts of two Google lookup examples", "labels": [], "entities": []}, {"text": " Table 3: Lookup results (with and without post- processing) compared to TextCat and baseline", "labels": [], "entities": [{"text": "TextCat", "start_pos": 73, "end_pos": 80, "type": "DATASET", "confidence": 0.9390640258789062}]}, {"text": " Table 4: Accuracies and F-scores for ID experiments", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9992601275444031}, {"text": "F-scores", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9977306723594666}]}, {"text": " Table 5: Accuracies, F-scores and percentages of  unknown target types (UTT) for cross-domain ex- periments compared to best lookup and baseline", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9991911053657532}, {"text": "F-scores", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9955527186393738}, {"text": "UTT)", "start_pos": 73, "end_pos": 77, "type": "METRIC", "confidence": 0.898188978433609}]}]}