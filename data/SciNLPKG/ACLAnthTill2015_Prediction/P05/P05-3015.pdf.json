{"title": [], "abstractContent": [{"text": "We report an empirical study on the role of syntactic features in building a semi-supervised named entity (NE) tagger.", "labels": [], "entities": []}, {"text": "Our study addresses two questions: What types of syntactic features are suitable for extracting potential NEs to train a classi-fier in a semi-supervised setting?", "labels": [], "entities": []}, {"text": "How good is the resulting NE classifier on testing instances dissimilar from its training data?", "labels": [], "entities": []}, {"text": "Our study shows that constituency and dependency parsing constraints are both suitable features to extract NEs and train the classifier.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 38, "end_pos": 56, "type": "TASK", "confidence": 0.7122671008110046}]}, {"text": "Moreover, the classi-fier showed significant accuracy improvement when constituency features are combined with new dependency feature.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.9993268251419067}]}, {"text": "Furthermore, the degradation inaccuracy on unfamiliar test cases is low, suggesting that the trained classifier generalizes well.", "labels": [], "entities": []}], "introductionContent": [{"text": "Named entity (NE) tagging is the task of recognizing and classifying phrases into one of many semantic classes such as persons, organizations and locations.", "labels": [], "entities": [{"text": "Named entity (NE) tagging is the task of recognizing and classifying phrases into one of many semantic classes such as persons, organizations and locations", "start_pos": 0, "end_pos": 155, "type": "Description", "confidence": 0.7294468736207044}]}, {"text": "Many successful NE tagging systems rely on a supervised learning framework where systems use large annotated training resources).", "labels": [], "entities": [{"text": "NE tagging", "start_pos": 16, "end_pos": 26, "type": "TASK", "confidence": 0.9512989819049835}]}, {"text": "These resources may not always be available for non-English domains.", "labels": [], "entities": []}, {"text": "This paper examines the practicality of developing a syntax-based semi-supervised NE tagger.", "labels": [], "entities": [{"text": "NE tagger", "start_pos": 82, "end_pos": 91, "type": "TASK", "confidence": 0.7661514282226562}]}, {"text": "In our study we compared the effects of two types of syntactic rules (constituency and dependency) in extracting and classifying potential named entities.", "labels": [], "entities": [{"text": "extracting and classifying potential named entities", "start_pos": 102, "end_pos": 153, "type": "TASK", "confidence": 0.7320147057374319}]}, {"text": "We train a Naive Bayes classification model on a combination of labeled and unlabeled examples with the Expectation Maximization (EM) algorithm.", "labels": [], "entities": [{"text": "Naive Bayes classification", "start_pos": 11, "end_pos": 37, "type": "TASK", "confidence": 0.6391516327857971}, {"text": "Expectation Maximization (EM)", "start_pos": 104, "end_pos": 133, "type": "TASK", "confidence": 0.6353632867336273}]}, {"text": "We find that a significant improvement in classification accuracy can be achieved when we combine both dependency and constituency extraction methods.", "labels": [], "entities": [{"text": "classification", "start_pos": 42, "end_pos": 56, "type": "TASK", "confidence": 0.9652925133705139}, {"text": "accuracy", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.9630570411682129}, {"text": "constituency extraction", "start_pos": 118, "end_pos": 141, "type": "TASK", "confidence": 0.804091066122055}]}, {"text": "In our experiments, we evaluate the generalization (coverage) of this bootstrapping approach under three testing schemas.", "labels": [], "entities": []}, {"text": "Each of these schemas represented a certain level of test data coverage (recall).", "labels": [], "entities": [{"text": "recall", "start_pos": 73, "end_pos": 79, "type": "METRIC", "confidence": 0.9287151098251343}]}, {"text": "Although the system performs best on (unseen) test data that is extracted by the syntactic rules (i.e., similar syntactic structures as the training examples), the performance degradation is not high when the system is tested on more general test cases.", "labels": [], "entities": []}, {"text": "Our experimental results suggest that a semi-supervised NE tagger can be successfully developed using syntax-rich features.", "labels": [], "entities": [{"text": "NE tagger", "start_pos": 56, "end_pos": 65, "type": "TASK", "confidence": 0.8332060277462006}]}], "datasetContent": [{"text": "In order to evaluate the effects of each group of syntactic features, we experimented with three different training strategies (using constituency rules, dependency rules or combinations of both).", "labels": [], "entities": []}, {"text": "We conducted the comparison study with three types of test data that represent three levels of coverage (recall) for the system: 1.", "labels": [], "entities": [{"text": "coverage", "start_pos": 95, "end_pos": 103, "type": "METRIC", "confidence": 0.9695578813552856}, {"text": "recall", "start_pos": 105, "end_pos": 111, "type": "METRIC", "confidence": 0.8371641039848328}]}, {"text": "Gold Standard NEs: This test set contains instances taken directly from the ACE data, and are therefore independent of the syntactic rules.", "labels": [], "entities": [{"text": "Gold Standard NEs", "start_pos": 0, "end_pos": 17, "type": "DATASET", "confidence": 0.8685178955396017}, {"text": "ACE data", "start_pos": 76, "end_pos": 84, "type": "DATASET", "confidence": 0.956777960062027}]}, {"text": "2. Any single or series of proper nouns in the text: This is a heuristic for locating potential NEs so as to have the broadest coverage.", "labels": [], "entities": [{"text": "locating potential NEs", "start_pos": 77, "end_pos": 99, "type": "TASK", "confidence": 0.75955730676651}]}, {"text": "3. NEs extracted from text by the syntactic rules.", "labels": [], "entities": []}, {"text": "This evaluation approach is similar to that of Collins and Singer.", "labels": [], "entities": []}, {"text": "The main difference is that we have to match the extracted expressions to a pre-labeled gold standard from ACE rather than performing manual annotations ourselves.", "labels": [], "entities": [{"text": "ACE", "start_pos": 107, "end_pos": 110, "type": "DATASET", "confidence": 0.9279757142066956}]}, {"text": "All tests have been performed under a 5-fold cross validation training-testing setup.", "labels": [], "entities": []}, {"text": "presents the accuracy of the NE classification and the size of labeled data in the different training-testing configurations.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.9996333122253418}, {"text": "NE classification", "start_pos": 29, "end_pos": 46, "type": "TASK", "confidence": 0.9074922800064087}]}, {"text": "The second line of each cell shows the size of labeled training data and the third line shows the size of testing data.", "labels": [], "entities": []}, {"text": "Each column presents the result for one type of the syntactic features that were used to extract NEs.", "labels": [], "entities": []}, {"text": "Each row of the table presents one of the three testing schema.", "labels": [], "entities": []}, {"text": "We tested the statistical significance of each of the cross-row accuracy improvements against an alpha value of 0.1 and observed significant improvement in all of the testing schemas.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.9769580364227295}]}, {"text": "Our results suggest that dependency parsing features are reasonable extraction patterns, as their accuracy rates are competitive against the model based solely on constituency rules.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 25, "end_pos": 43, "type": "TASK", "confidence": 0.8400316536426544}, {"text": "accuracy", "start_pos": 98, "end_pos": 106, "type": "METRIC", "confidence": 0.9988692402839661}]}, {"text": "Moreover, they make a good complement to the constituency rules proposed by Collins and Singer, since the accuracy rates of the union is higher than either model alone.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 106, "end_pos": 114, "type": "METRIC", "confidence": 0.9995114803314209}]}, {"text": "As expected, all methods perform the best when the test data are extracted in the same manner as the training examples.", "labels": [], "entities": []}, {"text": "However, if the systems were given a well-formed named entity, the performance degradation is reasonably small, about 2% absolute difference for all training methods.", "labels": [], "entities": []}, {"text": "The performance is somewhat lower when classifying very general test cases of all proper nouns.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Classification Accuracy, labeled training &  testing data size", "labels": [], "entities": [{"text": "Classification", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.9025319218635559}, {"text": "Accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.8818467855453491}]}]}