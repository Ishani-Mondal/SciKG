{"title": [{"text": "Learning Strategies for Open-Domain Natural Language Question Answering", "labels": [], "entities": [{"text": "Open-Domain Natural Language Question Answering", "start_pos": 24, "end_pos": 71, "type": "TASK", "confidence": 0.5855713248252868}]}], "abstractContent": [{"text": "This work presents a model for learning inference procedures for story comprehension through inductive generalization and reinforcement learning, based on classified examples.", "labels": [], "entities": []}, {"text": "The learned inference procedures (or strategies) are represented as of sequences of transformation rules.", "labels": [], "entities": []}, {"text": "The approach is compared to three prior systems, and experimental results are presented demonstrating the efficacy of the model.", "labels": [], "entities": []}], "introductionContent": [{"text": "This paper presents an approach to automatically learning strategies for natural language question answering from examples composed of textual sources, questions, and answers.", "labels": [], "entities": [{"text": "natural language question answering from examples composed of textual sources, questions", "start_pos": 73, "end_pos": 161, "type": "TASK", "confidence": 0.7667947659889857}]}, {"text": "Our approach is focused on one specific type of text-based question answering known as story comprehension.", "labels": [], "entities": [{"text": "text-based question answering", "start_pos": 48, "end_pos": 77, "type": "TASK", "confidence": 0.6840276718139648}]}, {"text": "Most TREC-style QA systems are designed to extract an answer from a document contained in a fairly large general collection.", "labels": [], "entities": [{"text": "TREC-style QA", "start_pos": 5, "end_pos": 18, "type": "TASK", "confidence": 0.759681224822998}]}, {"text": "They tend to follow a generic architecture, such as the one suggested by, that includes components for document preprocessing and analysis, candidate passage selection, answer extraction, and response generation.", "labels": [], "entities": [{"text": "document preprocessing and analysis", "start_pos": 103, "end_pos": 138, "type": "TASK", "confidence": 0.6759428828954697}, {"text": "candidate passage selection", "start_pos": 140, "end_pos": 167, "type": "TASK", "confidence": 0.7770701845486959}, {"text": "answer extraction", "start_pos": 169, "end_pos": 186, "type": "TASK", "confidence": 0.9135323166847229}, {"text": "response generation", "start_pos": 192, "end_pos": 211, "type": "TASK", "confidence": 0.8074629306793213}]}, {"text": "Story comprehension requires a similar approach, but involves answering questions from a single narrative document.", "labels": [], "entities": [{"text": "Story comprehension", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8108915984630585}]}, {"text": "An important challenge in text-based question answering in general is posed by the syntactic and semantic variability of question and answer forms, which makes it difficult to establish a match between the question and answer candidate.", "labels": [], "entities": [{"text": "text-based question answering", "start_pos": 26, "end_pos": 55, "type": "TASK", "confidence": 0.6179124514261881}]}, {"text": "This problem is particularly acute in the case of story comprehension due to the rarity of information restatement in the single document.", "labels": [], "entities": [{"text": "story comprehension", "start_pos": 50, "end_pos": 69, "type": "TASK", "confidence": 0.7717825174331665}]}, {"text": "Several recent systems have specifically addressed the task of story comprehension.", "labels": [], "entities": [{"text": "story comprehension", "start_pos": 63, "end_pos": 82, "type": "TASK", "confidence": 0.7907130122184753}]}, {"text": "The Deep Read reading comprehension system) uses a statistical bag-ofwords approach, matching the question with the lexically most similar sentence in the story.", "labels": [], "entities": []}, {"text": "Quarc) utilizes manually generated rules that selects a sentence deemed to contain the answer based on a combination of syntactic similarity and semantic correspondence (i.e., semantic categories of nouns).", "labels": [], "entities": []}, {"text": "The Brown University statistical language processing class project systems () combine the use of manually generated rules with statistical techniques such as bag-of-words and bag-of-verb matching, as well as deeper semantic analysis of nouns.", "labels": [], "entities": [{"text": "Brown University statistical language processing class project", "start_pos": 4, "end_pos": 66, "type": "DATASET", "confidence": 0.7835652317319598}]}, {"text": "As a rule, these three systems are effective at identifying the sentence containing the correct answer as long as the answer is explicit and contained entirely in that sentence.", "labels": [], "entities": []}, {"text": "They find it difficult, however, to deal with semantic alternations of even moderate complexity.", "labels": [], "entities": []}, {"text": "They also do not address situations where answers are split across multiple sentences, or those requiring complex inference.", "labels": [], "entities": []}, {"text": "Our framework, called QABLe (QuestionAnswering Behavior Learner), draws on prior work in learning action and problem-solving strategies.", "labels": [], "entities": []}, {"text": "We represent textual sources as sets of features in a sparse domain, and treat the QA task as behavior in a stochastic, partially observable world.", "labels": [], "entities": []}, {"text": "QA strategies are learned as sequences of transformation rules capable of deriving certain types of answers from particular text-question combinations.", "labels": [], "entities": []}, {"text": "The transformation rules are generated by instantiating primitive domain operators in specific feature contexts.", "labels": [], "entities": []}, {"text": "A process of reinforcement learning () is used to select and promote effective transformation rules.", "labels": [], "entities": []}, {"text": "We rely on recent work in attribute-efficient relational learning () to acquire natural representations of the underlying domain features.", "labels": [], "entities": []}, {"text": "These representations are learned in the course of interacting with the domain, and encode the features at the levels of abstraction that are found to be conducive to successful behavior.", "labels": [], "entities": []}, {"text": "This selection effect is achieved through a combination of inductive generalization and reinforcement learning elements.", "labels": [], "entities": []}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 presents the details of the QABLe framework.", "labels": [], "entities": [{"text": "QABLe framework", "start_pos": 38, "end_pos": 53, "type": "DATASET", "confidence": 0.8963164687156677}]}, {"text": "In section 3 we describe preliminary experimental results which indicate promise for our approach.", "labels": [], "entities": []}, {"text": "In section 4 we summarize and draw conclusions.", "labels": [], "entities": []}, {"text": "This framework is used both for learning to answer questions and for the actual QA task.", "labels": [], "entities": [{"text": "QA task", "start_pos": 80, "end_pos": 87, "type": "TASK", "confidence": 0.8730381727218628}]}, {"text": "While learning, the system is provided with a set of training instances, each consisting of a textual narrative, a question, and a corresponding answer.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our approach to open-domain natural language question answering on the Remedia corpus.", "labels": [], "entities": [{"text": "open-domain natural language question answering", "start_pos": 28, "end_pos": 75, "type": "TASK", "confidence": 0.590426230430603}, {"text": "Remedia corpus", "start_pos": 83, "end_pos": 97, "type": "DATASET", "confidence": 0.9094693064689636}]}, {"text": "This is a collection of 115 children's stories provided by Remedia Publications for reading comprehension.", "labels": [], "entities": []}, {"text": "The comprehension of each story is tested by answering five who, what, where, and why questions.", "labels": [], "entities": []}, {"text": "The Remedia Corpus was initially used to evaluate the Deep Read reading comprehension system, and later also other systems, including Quarc and the Brown University statistical language processing class project.", "labels": [], "entities": [{"text": "Brown University statistical language processing class project", "start_pos": 148, "end_pos": 210, "type": "DATASET", "confidence": 0.7380587118012565}]}, {"text": "The corpus includes two answer keys.", "labels": [], "entities": []}, {"text": "The first answer key contains annotations indicating the story sentence that is lexically closest to the answer found in the published answer key (AutSent).", "labels": [], "entities": []}, {"text": "The second answer key contains sentences that a human judged to best answer each question (HumSent).", "labels": [], "entities": [{"text": "HumSent)", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.8413315415382385}]}, {"text": "Examination of the two keys shows the latter to be more reliable.", "labels": [], "entities": []}, {"text": "We trained and tested using the HumSent answers.", "labels": [], "entities": [{"text": "HumSent answers", "start_pos": 32, "end_pos": 47, "type": "DATASET", "confidence": 0.9773587286472321}]}, {"text": "We also compare our results to the HumSent results of prior systems.", "labels": [], "entities": [{"text": "HumSent", "start_pos": 35, "end_pos": 42, "type": "DATASET", "confidence": 0.8491570949554443}]}, {"text": "In the Remedia corpus, approximately 10% of the questions lack an answer.", "labels": [], "entities": [{"text": "Remedia corpus", "start_pos": 7, "end_pos": 21, "type": "DATASET", "confidence": 0.8438831865787506}]}, {"text": "Following prior work, only questions with annotated answers were considered.", "labels": [], "entities": []}, {"text": "We divided the Remedia corpus into a set of 55 tests used for development, and 60 tests used to evaluate our model, employing the same partition scheme as followed by the prior work mentioned above.", "labels": [], "entities": [{"text": "Remedia corpus", "start_pos": 15, "end_pos": 29, "type": "DATASET", "confidence": 0.7549685835838318}]}, {"text": "With five questions being supplied with each test, this breakdown provided 275 example instances for training, and 300 example instances to test with.", "labels": [], "entities": []}, {"text": "However, due to the heavy reliance of our model on learning, many more training examples were necessary.", "labels": [], "entities": []}, {"text": "We widened the training set by adding story-question-answer sets obtained from several online sources.", "labels": [], "entities": []}, {"text": "With the extended corpus, QABLe was trained on 262 stories with 3-5 questions each, corresponding to 1000 example instances 48%.", "labels": [], "entities": [{"text": "QABLe", "start_pos": 26, "end_pos": 31, "type": "DATASET", "confidence": 0.8526822924613953}]}, {"text": "Analysis of transformation rule learning and use.", "labels": [], "entities": [{"text": "transformation rule learning", "start_pos": 12, "end_pos": 40, "type": "TASK", "confidence": 0.7875862717628479}]}, {"text": "compares the performance of different versions of QABLe with those reported by the three systems described above.", "labels": [], "entities": [{"text": "QABLe", "start_pos": 50, "end_pos": 55, "type": "DATASET", "confidence": 0.8805253505706787}]}, {"text": "We wish to discern the particular contribution of transformation rule learning in the QABLe model, as well as the value of expanding the training set.", "labels": [], "entities": [{"text": "QABLe", "start_pos": 86, "end_pos": 91, "type": "DATASET", "confidence": 0.8559203147888184}]}, {"text": "Thus, the QABLe-N/L results indicate the accuracy of answers returned by the QA matching and extraction algorithm described in section 2.6 only.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.9990837574005127}, {"text": "QA matching and extraction", "start_pos": 77, "end_pos": 103, "type": "TASK", "confidence": 0.7929774969816208}]}, {"text": "This algorithm is similar to prior answer extraction techniques, and provides a baseline for our experiments.", "labels": [], "entities": [{"text": "prior answer extraction", "start_pos": 29, "end_pos": 52, "type": "TASK", "confidence": 0.6218777596950531}]}, {"text": "The QABLe-L results include answers returned by the full QABLe framework, including the utilization of learned transformation rules, but trained only on the limited training portion of the Remedia corpus.", "labels": [], "entities": [{"text": "Remedia corpus", "start_pos": 189, "end_pos": 203, "type": "DATASET", "confidence": 0.8816646635532379}]}, {"text": "The QABLe-L+ results are for the version trained on the expanded training set.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2. Comparison of QA accuracy by question type.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.8811916708946228}]}, {"text": " Table 3. Analysis of transformation rule learning and use.", "labels": [], "entities": [{"text": "transformation rule learning", "start_pos": 22, "end_pos": 50, "type": "TASK", "confidence": 0.7709503173828125}]}]}