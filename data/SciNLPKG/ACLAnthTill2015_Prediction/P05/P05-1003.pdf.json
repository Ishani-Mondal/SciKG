{"title": [{"text": "Logarithmic Opinion Pools for Conditional Random Fields", "labels": [], "entities": []}], "abstractContent": [{"text": "Recent work on Conditional Random Fields (CRFs) has demonstrated the need for regularisation to counter the tendency of these models to overfit.", "labels": [], "entities": [{"text": "Conditional Random Fields (CRFs)", "start_pos": 15, "end_pos": 47, "type": "TASK", "confidence": 0.6599239905675253}]}, {"text": "The standard approach to regularising CRFs involves a prior distribution over the model parameters , typically requiring search over a hy-perparameter space.", "labels": [], "entities": [{"text": "regularising CRFs", "start_pos": 25, "end_pos": 42, "type": "TASK", "confidence": 0.8738373816013336}]}, {"text": "In this paper we address the overfitting problem from a different perspective, by factoring the CRF distribution into a weighted product of individual \"expert\" CRF distributions.", "labels": [], "entities": []}, {"text": "We call this model a logarithmic opinion pool (LOP) of CRFs (LOP-CRFs).", "labels": [], "entities": []}, {"text": "We apply the LOP-CRF to two sequencing tasks.", "labels": [], "entities": [{"text": "LOP-CRF", "start_pos": 13, "end_pos": 20, "type": "METRIC", "confidence": 0.8636282086372375}]}, {"text": "Our results show that unregularised expert CRFs with an unregularised CRF under a LOP can outperform the unregularised CRF, and attain a performance level close to the regularised CRF.", "labels": [], "entities": []}, {"text": "LOP-CRFs therefore provide a viable alternative to CRF regularisation without the need for hyper-parameter search.", "labels": [], "entities": [{"text": "CRF regularisation", "start_pos": 51, "end_pos": 69, "type": "TASK", "confidence": 0.844323068857193}]}], "introductionContent": [{"text": "In recent years, conditional random fields (CRFs) () have shown success on a number of natural language processing (NLP) tasks, including shallow parsing, named entity recognition and information extraction from research papers ().", "labels": [], "entities": [{"text": "shallow parsing", "start_pos": 138, "end_pos": 153, "type": "TASK", "confidence": 0.615588515996933}, {"text": "named entity recognition", "start_pos": 155, "end_pos": 179, "type": "TASK", "confidence": 0.643246998389562}, {"text": "information extraction from research papers", "start_pos": 184, "end_pos": 227, "type": "TASK", "confidence": 0.8503775238990784}]}, {"text": "In general, this work has demonstrated the susceptibility of CRFs to overfit the training data during parameter estimation.", "labels": [], "entities": []}, {"text": "As a consequence, it is now standard to use some form of overfitting reduction in CRF training.", "labels": [], "entities": [{"text": "CRF training", "start_pos": 82, "end_pos": 94, "type": "TASK", "confidence": 0.8974441289901733}]}, {"text": "Recently, there have been a number of sophisticated approaches to reducing overfitting in CRFs, including automatic feature induction and a full Bayesian approach to training and inference).", "labels": [], "entities": []}, {"text": "These advanced methods tend to be difficult to implement and are often computationally expensive.", "labels": [], "entities": []}, {"text": "Consequently, due to its ease of implementation, the current standard approach to reducing overfitting in CRFs is the use of a prior distribution over the model parameters, typically a Gaussian.", "labels": [], "entities": []}, {"text": "The disadvantage with this method, however, is that it requires adjusting the value of one or more of the distribution's hyperparameters.", "labels": [], "entities": []}, {"text": "This usually involves manual or automatic tuning on a development set, and can bean expensive process as the CRF must be retrained many times for different hyperparameter values.", "labels": [], "entities": []}, {"text": "In this paper we address the overfitting problem in CRFs from a different perspective.", "labels": [], "entities": []}, {"text": "We factor the CRF distribution into a weighted product of individual expert CRF distributions, each focusing on a particular subset of the distribution.", "labels": [], "entities": []}, {"text": "We call this model a logarithmic opinion pool (LOP) of CRFs (LOP-CRFs), and provide a procedure for learning the weight of each expert in the product.", "labels": [], "entities": []}, {"text": "The LOP-CRF framework is \"parameter-free\" in the sense that it does not involve the requirement to adjust hyperparameter values.", "labels": [], "entities": []}, {"text": "LOP-CRFs are theoretically advantageous in that their Kullback-Leibler divergence with a given distribution can be explicitly represented as a function of the KL-divergence with each of their expert distributions.", "labels": [], "entities": []}, {"text": "This provides a well-founded framework for designing new overfitting reduction schemes: look to factorise a CRF distribution as a set of diverse experts.", "labels": [], "entities": []}, {"text": "We apply LOP-CRFs to two sequencing tasks in NLP: named entity recognition and part-of-speech tagging.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 50, "end_pos": 74, "type": "TASK", "confidence": 0.682699720064799}, {"text": "part-of-speech tagging", "start_pos": 79, "end_pos": 101, "type": "TASK", "confidence": 0.7466316521167755}]}, {"text": "Our results show that combination of unregularised expert CRFs with an unregularised standard CRF under a LOP can outperform the unregularised standard CRF, and attain a performance level that rivals that of the regularised standard CRF.", "labels": [], "entities": []}, {"text": "LOP-CRFs therefore provide a viable alternative to CRF regularisation without the need for hyperparameter search.", "labels": [], "entities": [{"text": "CRF regularisation", "start_pos": 51, "end_pos": 69, "type": "TASK", "confidence": 0.8309620916843414}]}], "datasetContent": [{"text": "To compare the performance of LOP-CRFs trained using the procedure we described previously to that of a standard CRF regularised with a Gaussian prior, we do the following for both NER and POS tagging: \u2022 Train a monolithic CRF with regularisation using a Gaussian prior.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 189, "end_pos": 200, "type": "TASK", "confidence": 0.5903546214103699}]}, {"text": "We use the development set to optimise the value of the variance hyperparameter.", "labels": [], "entities": []}, {"text": "\u2022 Train every expert CRF in each expert set without regularisation (each expert set includes the monolithic CRF, which clearly need only be trained once).", "labels": [], "entities": []}, {"text": "\u2022 For each expert set, create a LOP-CRF from the expert CRFs and train the weights of the LOP-CRF without regularisation.", "labels": [], "entities": []}, {"text": "We compare its performance to that of the unregularised and regularised monolithic CRFs.", "labels": [], "entities": []}, {"text": "\u2022 To investigate whether training the LOP-CRF weights contributes significantly to the LOP-CRF's performance, for each expert set we create a LOP-CRF with uniform weights and compare its performance to that of the LOP-CRF with trained weights.", "labels": [], "entities": []}, {"text": "\u2022 To investigate whether unregularised training of the LOP-CRF weights leads to overfitting, for each expert set we train the weights of the LOP-CRF with regularisation using a Dirichlet prior.", "labels": [], "entities": []}, {"text": "We optimise the hyperparameter in the Dirichlet distribution on the development set.", "labels": [], "entities": []}, {"text": "We then compare the performance of the LOP-CRF with regularised weights to that of the LOP-CRF with unregularised weights.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Development set F scores for NER experts", "labels": [], "entities": [{"text": "F", "start_pos": 26, "end_pos": 27, "type": "METRIC", "confidence": 0.917456328868866}, {"text": "NER", "start_pos": 39, "end_pos": 42, "type": "TASK", "confidence": 0.8689110279083252}]}, {"text": " Table 2: F scores for NER unregularised LOP-CRFs", "labels": [], "entities": [{"text": "F scores", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.976027101278305}, {"text": "NER unregularised LOP-CRFs", "start_pos": 23, "end_pos": 49, "type": "TASK", "confidence": 0.705521821975708}]}, {"text": " Table 3: Accuracies for POS tagging unregularised  LOP-CRFs", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.996250331401825}, {"text": "POS tagging unregularised  LOP-CRFs", "start_pos": 25, "end_pos": 60, "type": "TASK", "confidence": 0.8494847267866135}]}]}