{"title": [{"text": "Multi-Field Information Extraction and Cross-Document Fusion", "labels": [], "entities": [{"text": "Multi-Field Information Extraction", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.6696181297302246}]}], "abstractContent": [{"text": "In this paper, we examine the task of extracting a set of biographic facts about target individuals from a collection of Web pages.", "labels": [], "entities": []}, {"text": "We automatically annotate training text with positive and negative examples of fact extractions and train Rote, Na\u00a8\u0131veNa\u00a8\u0131ve Bayes, and Conditional Random Field extraction models for fact extraction from individual Web pages.", "labels": [], "entities": [{"text": "fact extractions", "start_pos": 79, "end_pos": 95, "type": "TASK", "confidence": 0.7185208052396774}, {"text": "Rote", "start_pos": 106, "end_pos": 110, "type": "METRIC", "confidence": 0.9388367533683777}, {"text": "Conditional Random Field extraction", "start_pos": 136, "end_pos": 171, "type": "TASK", "confidence": 0.5396704152226448}, {"text": "fact extraction from individual Web pages", "start_pos": 183, "end_pos": 224, "type": "TASK", "confidence": 0.8215932448705038}]}, {"text": "We then propose and evaluate methods for fusing the extracted information across documents to return a consensus answer.", "labels": [], "entities": []}, {"text": "A novel cross-field bootstrapping method leverages data interdependencies to yield improved performance.", "labels": [], "entities": []}], "introductionContent": [{"text": "Much recent statistical information extraction research has applied graphical models to extract information from one particular document after training on a large corpus of annotated data).", "labels": [], "entities": [{"text": "statistical information extraction", "start_pos": 12, "end_pos": 46, "type": "TASK", "confidence": 0.7405837178230286}]}, {"text": "1 Such systems are widely applicable, yet there remain many information extraction tasks that are not readily amenable to these methods.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 60, "end_pos": 82, "type": "TASK", "confidence": 0.7947458922863007}]}, {"text": "Annotated data required for training statistical extraction systems is sometimes unavailable, while there are examples of the desired information.", "labels": [], "entities": [{"text": "statistical extraction", "start_pos": 37, "end_pos": 59, "type": "TASK", "confidence": 0.7629498839378357}]}, {"text": "Further, the goal maybe to find a few interrelated pieces of information that are stated multiple times in a set of documents.", "labels": [], "entities": []}, {"text": "Here, we investigate one task that meets the above criteria.", "labels": [], "entities": []}, {"text": "Given the name of a celebrity such as \"Frank Zappa\", our goal is to extract a set of biographic facts (e.g., birthdate, birthplace and occupation) about that person from documents on the Web.", "labels": [], "entities": []}, {"text": "First, we describe a general method of automatic annotation for training from positive and negative examples and use the method to train Rote, Na\u00a8\u0131veNa\u00a8\u0131ve Bayes, and Conditional Random Field models (Section 2).", "labels": [], "entities": []}, {"text": "We then examine how multiple extractions can be combined to form one consensus answer (Section 3).", "labels": [], "entities": []}, {"text": "We compare fusion methods and show that frequency voting outperforms the single highest confidence answer by an average of 11% across the various extractors.", "labels": [], "entities": []}, {"text": "Increasing the number of retrieved documents boosts the overall system accuracy as additional documents which mention the individual in question lead to higher recall.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 71, "end_pos": 79, "type": "METRIC", "confidence": 0.9973418116569519}, {"text": "recall", "start_pos": 160, "end_pos": 166, "type": "METRIC", "confidence": 0.9985425472259521}]}, {"text": "This improved recall more than compensates fora loss in per-extraction precision from these additional documents.", "labels": [], "entities": [{"text": "recall", "start_pos": 14, "end_pos": 20, "type": "METRIC", "confidence": 0.9995205402374268}, {"text": "precision", "start_pos": 71, "end_pos": 80, "type": "METRIC", "confidence": 0.8710035085678101}]}, {"text": "Next, we present a method for cross-field bootstrapping (Section 4) which improves per-field accuracy by 7%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 93, "end_pos": 101, "type": "METRIC", "confidence": 0.9857028126716614}]}, {"text": "We demonstrate that a small training set with only the most relevant documents can be as effective as a larger training set with additional, less relevant documents (Section 5).", "labels": [], "entities": []}], "datasetContent": [{"text": "To test the performance of the different extractors, we collected a set of 152 semistructured mini-biographies from an online site (www.infoplease.com), and used simple rules to extract a biographic fact database of birthday and month (henceforth birthday), birth year, occupation, birthplace, and year of death (when applicable).", "labels": [], "entities": []}, {"text": "An example of the data can be found in.", "labels": [], "entities": []}, {"text": "In our system, we normalized birthdays, and performed capitalization normalization for the remaining fields.", "labels": [], "entities": [{"text": "normalized birthdays", "start_pos": 18, "end_pos": 38, "type": "TASK", "confidence": 0.810134619474411}]}, {"text": "We did no further normalization, such as normalizing state names to their two letter acronyms (e.g., California \u2192 CA).", "labels": [], "entities": []}, {"text": "Fifteen names were set aside as training data, and the rest were used for testing.", "labels": [], "entities": []}, {"text": "For each name, 150 documents were downloaded from Google to serve as the hook corpus for either training or testing.", "labels": [], "entities": []}, {"text": "In training, we automatically annotated documents using people in the training set as hooks, and in testing, tried to get targets that exactly matched what was present in the database.", "labels": [], "entities": []}, {"text": "This is a very strict method of evaluation for three reasons.", "labels": [], "entities": []}, {"text": "First, since the facts were automatically collected, they contain errors and thus the system is tested against wrong answers.", "labels": [], "entities": []}, {"text": "10 Second, the extractors might have retrieved information that was simply not present in the database but nevertheless correct (e.g., someone's occupation might be listed as writer and the retrieved occupation might be novelist).", "labels": [], "entities": []}, {"text": "Third, since the retrieved targets were not normalized, there system may have retrieved targets that were correct but were not recognized (e.g., the database birthplace is New York, and the system retrieves NY).", "labels": [], "entities": []}, {"text": "In testing, we rejected candidate targets that were not present in our target set models Er . In some cases, this resulted in the system being unable to find the correct target fora particular relationship, since it was not in the target set.", "labels": [], "entities": []}, {"text": "Before fusion (Section 3), we gathered all the facts extracted by the system and graded them in isolation.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Two of 152 entries in the Biographic Database. Each  entry contains incomplete information about various celebrities.  Here, Aaron Neville's birth state is missing, and Frank Zappa  could be equally well described as a guitarist or rock-star.", "labels": [], "entities": [{"text": "Biographic Database", "start_pos": 36, "end_pos": 55, "type": "DATASET", "confidence": 0.762649804353714}]}, {"text": " Table 2: Pre-Fusion Precision of extracted facts for various extraction systems, trained on 15 people each with 150 documents, and  tested on 137 people each with 150 documents.", "labels": [], "entities": []}, {"text": " Table 3: Pre-Fusion Pseudo-Recall of extract facts with the identical training/testing set-up as above.", "labels": [], "entities": []}, {"text": " Table 4: Average Accuracy of the Highest Confidence (Best)  and Most Frequent (Vote) across five extraction fields.", "labels": [], "entities": [{"text": "Average", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.960487961769104}, {"text": "Accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.7657822966575623}, {"text": "Frequent (Vote)", "start_pos": 70, "end_pos": 85, "type": "METRIC", "confidence": 0.9166703373193741}]}, {"text": " Table 5: Voting for information fusion, evaluated per person.  CRF+E has best average performance (67.8%).", "labels": [], "entities": [{"text": "information fusion", "start_pos": 21, "end_pos": 39, "type": "TASK", "confidence": 0.8790429830551147}, {"text": "CRF+E", "start_pos": 64, "end_pos": 69, "type": "METRIC", "confidence": 0.5256170133749644}]}, {"text": " Table 6: Performance of Cross-Field Bootstrapping Models.  (f) indicates that the best fused result was taken. birth year(f)  means birth years were annotated using the system that discov- ered the most accurate birth years.", "labels": [], "entities": [{"text": "birth year(f)", "start_pos": 112, "end_pos": 125, "type": "METRIC", "confidence": 0.9185826063156128}]}]}