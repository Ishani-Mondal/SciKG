{"title": [], "abstractContent": [{"text": "This paper describes a novel framework for interactive question-answering (Q/A) based on predictive questioning.", "labels": [], "entities": [{"text": "question-answering (Q/A)", "start_pos": 55, "end_pos": 79, "type": "TASK", "confidence": 0.6335153132677078}]}, {"text": "Generated off-line from topic representations of complex scenarios, predictive questions represent requests for information that capture the most salient (and diverse) aspects of a topic.", "labels": [], "entities": [{"text": "predictive questions represent requests for information that capture the most salient (and diverse) aspects of a topic", "start_pos": 68, "end_pos": 186, "type": "Description", "confidence": 0.7078534505869213}]}, {"text": "We present experimental results from large user studies (featur-ing a fully-implemented interactive Q/A system named FERRET) that demonstrates that surprising performance is achieved by integrating predictive questions into the context of a Q/A dialogue.", "labels": [], "entities": [{"text": "FERRET", "start_pos": 117, "end_pos": 123, "type": "METRIC", "confidence": 0.9623036980628967}]}], "introductionContent": [{"text": "In this paper, we propose anew architecture for interactive question-answering based on predictive questioning.", "labels": [], "entities": []}, {"text": "We present experimental results from a currently-implemented interactive Q/A system, named FERRET, that demonstrates that surprising performance is achieved by integrating sources of topic information into the context of a Q/A dialogue.", "labels": [], "entities": [{"text": "FERRET", "start_pos": 91, "end_pos": 97, "type": "METRIC", "confidence": 0.9689217805862427}]}, {"text": "In interactive Q/A, professional users engage in extended dialogues with automatic Q/A systems in order to obtain information relevant to a complex scenario.", "labels": [], "entities": [{"text": "Q/A", "start_pos": 15, "end_pos": 18, "type": "TASK", "confidence": 0.7397890090942383}]}, {"text": "Unlike Q/A in isolation, where the performance of a system is evaluated in terms of how well answers returned by a system meet the specific information requirements of a single question, the performance of interactive Q/A systems have traditionally been evaluated by analyzing aspects of the dialogue as a whole.", "labels": [], "entities": [{"text": "Q/A", "start_pos": 7, "end_pos": 10, "type": "TASK", "confidence": 0.7184591293334961}]}, {"text": "Q/A dialogues have been evaluated in terms of (1) efficiency, defined as the number of questions that the user must pose to find particular information, (2) effectiveness, defined by the relevance of the answers returned, (3) user satisfaction.", "labels": [], "entities": []}, {"text": "In order to maximize performance in these three areas, interactive Q/A systems need a predictive dialogue architecture that enables them to propose related questions about the relevant information that could be returned to a user, given a domain of interest.", "labels": [], "entities": []}, {"text": "We argue that interactive Q/A systems depend on three factors: (1) the effective representation of the topic of a dialogue, (2) the dynamic recognition of the structure of the dialogue, and (3) the ability to return relevant answers to a particular question.", "labels": [], "entities": []}, {"text": "In this paper, we describe results from experiments we conducted with our own interactive Q/A system, FERRET, under the auspices of the ARDA AQUAINT 1 program, involving 8 different dialogue scenarios and more than 30 users.", "labels": [], "entities": [{"text": "FERRET", "start_pos": 102, "end_pos": 108, "type": "METRIC", "confidence": 0.9625887870788574}]}, {"text": "The results presented here illustrate the role of predictive questioning in enhancing the performance of Q/A interactions.", "labels": [], "entities": []}, {"text": "In the remainder of this paper, we describe anew architecture for interactive Q/A.", "labels": [], "entities": []}, {"text": "Section 2 presents the functionality of several of FERRET's modules and describes the NLP techniques it relies upon.", "labels": [], "entities": [{"text": "FERRET", "start_pos": 51, "end_pos": 57, "type": "DATASET", "confidence": 0.8299890756607056}]}, {"text": "In Section 3, we present one of the dialogue scenarios and the topic representations we have employed.", "labels": [], "entities": []}, {"text": "Section 4 highlights the management of the interaction between the user and FERRET, while Section 5 presents the results of evaluating our proposed model, and Section 6 summarizes the conclusions.", "labels": [], "entities": [{"text": "FERRET", "start_pos": 76, "end_pos": 82, "type": "METRIC", "confidence": 0.6813826560974121}]}], "datasetContent": [{"text": "To date, we have used FERRET to produce over 90 Q/A dialogues with human users.", "labels": [], "entities": []}, {"text": "illustrates three turns from areal dialogue from a human user investigating Iran's chemical weapons prorgram.", "labels": [], "entities": []}, {"text": "As it can be seen coherence can be established between the user's questions and the system's answers (e.g. Q3 is related to both A1 and A3) as well as between the QUABs and the user's follow-up questions (e.g. QUAB (1b) is more related to Q2 than either Q1 or A1).", "labels": [], "entities": []}, {"text": "Coherence alone is not sufficient to analyze the quality of interactions, however.", "labels": [], "entities": []}, {"text": "In order to better understand interactive Q/A dialogues, we have conducted three sets of experiments with human users of FERRET.", "labels": [], "entities": [{"text": "FERRET", "start_pos": 121, "end_pos": 127, "type": "DATASET", "confidence": 0.8645171523094177}]}, {"text": "In these experiments, users were allotted two hours to interact with Ferret to gather information requested by a dialogue scenario similar to the one presented in.", "labels": [], "entities": []}, {"text": "In Experiment 1 (E1), 8 U.S. Navy Reserve (USNR) intelligence analysts used FERRET to research 8 different scenarios related to chemical and biological weapons.", "labels": [], "entities": [{"text": "FERRET", "start_pos": 76, "end_pos": 82, "type": "METRIC", "confidence": 0.9678277373313904}]}, {"text": "Experiment 2 and Experiment 3 considered several of the same scenarios addressed in E1: E2 included 24 mixed teams of analysts and novice users working with 2 scenarios, while E3 featured 4 USNR analysts working with 6 of the original 8 scenarios.", "labels": [], "entities": []}, {"text": "(Details for each experiment are provided in.)", "labels": [], "entities": []}, {"text": "Users were also given a task to focus their research; in E1 and E3, users prepared a short report detailing their findings; in E2, users were given a list of \"challenge\" questions to answer.", "labels": [], "entities": []}, {"text": "In E1 and E2, users had access to a total of 3210 QUAB questions that had been hand-created by developers for each the 8 dialogue scenarios.", "labels": [], "entities": []}, {"text": "provides totals for each scenario.)", "labels": [], "entities": []}, {"text": "In E3, users performed research with aversion of FERRET that included no QUABs at all.", "labels": [], "entities": [{"text": "FERRET", "start_pos": 49, "end_pos": 55, "type": "METRIC", "confidence": 0.8851609826087952}]}], "tableCaptions": [{"text": " Table 3: QUAB distribution over scenarios", "labels": [], "entities": []}, {"text": " Table 4: Efficiency of Dialogues in Experiment 1", "labels": [], "entities": [{"text": "Efficiency", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9057309031486511}]}, {"text": " Table 5: Efficiency of Dialogues in Experiment 2", "labels": [], "entities": [{"text": "Efficiency", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9238439202308655}]}, {"text": " Table 6: Effectiveness of dialogs", "labels": [], "entities": []}, {"text": " Table 7: Quality of QUABs acquired automatically", "labels": [], "entities": [{"text": "Quality", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9909775257110596}]}, {"text": " Table 8: User Satisfaction Survey Results", "labels": [], "entities": [{"text": "User Satisfaction", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.83900186419487}]}]}