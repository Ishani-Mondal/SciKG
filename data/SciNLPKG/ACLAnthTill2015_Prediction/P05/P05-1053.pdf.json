{"title": [{"text": "Exploring Various Knowledge in Relation Extraction", "labels": [], "entities": [{"text": "Exploring Various Knowledge in Relation Extraction", "start_pos": 0, "end_pos": 50, "type": "TASK", "confidence": 0.6303708205620447}]}], "abstractContent": [{"text": "Extracting semantic relationships between entities is challenging.", "labels": [], "entities": [{"text": "Extracting semantic relationships between entities", "start_pos": 0, "end_pos": 50, "type": "TASK", "confidence": 0.9071443676948547}]}, {"text": "This paper investigates the incorporation of diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using SVM.", "labels": [], "entities": [{"text": "feature-based relation extraction", "start_pos": 98, "end_pos": 131, "type": "TASK", "confidence": 0.6320145726203918}]}, {"text": "Our study illustrates that the base phrase chunking information is very effective for relation extraction and contributes to most of the performance improvement from syntactic aspect while additional information from full parsing gives limited further enhancement.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 86, "end_pos": 105, "type": "TASK", "confidence": 0.9365299046039581}]}, {"text": "This suggests that most of useful information in full parse trees for relation extraction is shallow and can be captured by chunking.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 70, "end_pos": 89, "type": "TASK", "confidence": 0.8787476420402527}]}, {"text": "We also demonstrate how semantic information such as WordNet and Name List, can be used in feature based relation extraction to further improve the performance.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 53, "end_pos": 60, "type": "DATASET", "confidence": 0.9380417466163635}, {"text": "relation extraction", "start_pos": 105, "end_pos": 124, "type": "TASK", "confidence": 0.69953852891922}]}, {"text": "Evaluation on the ACE corpus shows that effective incorporation of diverse features enables our system outper-form previously best-reported systems on the 24 ACE relation subtypes and significantly outperforms tree kernel-based systems by over 20 in F-measure on the 5 ACE relation types.", "labels": [], "entities": [{"text": "ACE corpus", "start_pos": 18, "end_pos": 28, "type": "DATASET", "confidence": 0.8884064853191376}, {"text": "F-measure", "start_pos": 250, "end_pos": 259, "type": "METRIC", "confidence": 0.9960013031959534}]}], "introductionContent": [{"text": "With the dramatic increase in the amount of textual information available in digital archives and the WWW, there has been growing interest in techniques for automatically extracting information from text.", "labels": [], "entities": [{"text": "WWW", "start_pos": 102, "end_pos": 105, "type": "DATASET", "confidence": 0.9112971425056458}, {"text": "automatically extracting information from text", "start_pos": 157, "end_pos": 203, "type": "TASK", "confidence": 0.7776825189590454}]}, {"text": "Information Extraction (IE) systems are expected to identify relevant information (usually of pre-defined types) from text documents in a certain domain and put them in a structured format.", "labels": [], "entities": [{"text": "Information Extraction (IE)", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.8345041155815125}]}, {"text": "According to the scope of the NIST Automatic Content Extraction (ACE) program, current research in IE has three main objectives: Entity Detection and Tracking (EDT), Relation Detection and Characterization (RDC), and Event Detection and Characterization (EDC).", "labels": [], "entities": [{"text": "NIST Automatic Content Extraction (ACE)", "start_pos": 30, "end_pos": 69, "type": "TASK", "confidence": 0.7928545985903058}, {"text": "IE", "start_pos": 99, "end_pos": 101, "type": "TASK", "confidence": 0.9729883074760437}, {"text": "Entity Detection and Tracking (EDT)", "start_pos": 129, "end_pos": 164, "type": "TASK", "confidence": 0.8002919895308358}, {"text": "Relation Detection and Characterization (RDC)", "start_pos": 166, "end_pos": 211, "type": "TASK", "confidence": 0.802595181124551}, {"text": "Event Detection and Characterization (EDC)", "start_pos": 217, "end_pos": 259, "type": "TASK", "confidence": 0.804011766399656}]}, {"text": "The EDT task entails the detection of entity mentions and chaining them together by identifying their coreference.", "labels": [], "entities": []}, {"text": "In ACE vocabulary, entities are objects, mentions are references to them, and relations are semantic relationships between entities.", "labels": [], "entities": []}, {"text": "Entities can be of five types: persons, organizations, locations, facilities and geo-political entities (GPE: geographically defined regions that indicate apolitical boundary, e.g. countries, states, cities, etc.).", "labels": [], "entities": []}, {"text": "Mentions have three levels: names, nomial expressions or pronouns.", "labels": [], "entities": []}, {"text": "The RDC task detects and classifies implicit and explicit relations 1 between entities identified by the EDT task.", "labels": [], "entities": []}, {"text": "For example, we want to determine whether a person is at a location, based on the evidence in the context.", "labels": [], "entities": []}, {"text": "Extraction of semantic relationships between entities can be very useful for applications such as question answering, e.g. to answer the query \"Who is the president of the United States?\".", "labels": [], "entities": [{"text": "Extraction of semantic relationships between entities", "start_pos": 0, "end_pos": 53, "type": "TASK", "confidence": 0.8667237261931101}, {"text": "question answering", "start_pos": 98, "end_pos": 116, "type": "TASK", "confidence": 0.8801880776882172}, {"text": "answer the query \"Who is the president of the United States?\"", "start_pos": 126, "end_pos": 187, "type": "TASK", "confidence": 0.6377213574372805}]}, {"text": "This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).", "labels": [], "entities": [{"text": "ACE RDC task", "start_pos": 26, "end_pos": 38, "type": "TASK", "confidence": 0.7985445658365885}, {"text": "relation extraction", "start_pos": 118, "end_pos": 137, "type": "TASK", "confidence": 0.6951021403074265}]}, {"text": "Our study illustrates that the base phrase chunking information contributes to most of the performance inprovement from syntactic aspect while additional full parsing information does not contribute much, largely due to the fact that most of relations defined in ACE corpus are within a very short distance.", "labels": [], "entities": [{"text": "ACE corpus", "start_pos": 263, "end_pos": 273, "type": "DATASET", "confidence": 0.8125199973583221}]}, {"text": "We also demonstrate how semantic information such as WordNet) and Name List can be used in the feature-based framework.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 53, "end_pos": 60, "type": "DATASET", "confidence": 0.9172537326812744}]}, {"text": "Evaluation shows that the incorporation of diverse features enables our system achieve best reported performance.", "labels": [], "entities": []}, {"text": "It also shows that our fea-ture-based approach outperforms tree kernel-based approaches by 11 F-measure in relation detection and more than 20 F-measure in relation detection and classification on the 5 ACE relation types.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 94, "end_pos": 103, "type": "METRIC", "confidence": 0.9822636246681213}, {"text": "relation detection", "start_pos": 107, "end_pos": 125, "type": "TASK", "confidence": 0.803426593542099}, {"text": "F-measure", "start_pos": 143, "end_pos": 152, "type": "METRIC", "confidence": 0.9773338437080383}, {"text": "relation detection and classification", "start_pos": 156, "end_pos": 193, "type": "TASK", "confidence": 0.7541447281837463}]}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 presents related work.", "labels": [], "entities": []}, {"text": "Section 3 and Section 4 describe our approach and various features employed respectively.", "labels": [], "entities": []}, {"text": "Finally, we present experimental setting and results in Section 5 and conclude with some general observations in relation extraction in Section 6.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 113, "end_pos": 132, "type": "TASK", "confidence": 0.8668503761291504}]}], "datasetContent": [{"text": "This paper uses the ACE corpus provided by LDC to train and evaluate our feature-based relation extraction system.", "labels": [], "entities": [{"text": "ACE corpus", "start_pos": 20, "end_pos": 30, "type": "DATASET", "confidence": 0.9130854308605194}, {"text": "LDC", "start_pos": 43, "end_pos": 46, "type": "DATASET", "confidence": 0.769988477230072}, {"text": "relation extraction", "start_pos": 87, "end_pos": 106, "type": "TASK", "confidence": 0.6871743053197861}]}, {"text": "The ACE corpus is gathered from various newspapers, newswire and broadcasts.", "labels": [], "entities": [{"text": "ACE corpus", "start_pos": 4, "end_pos": 14, "type": "DATASET", "confidence": 0.8278524279594421}]}, {"text": "In this paper, we only model explicit relations because of poor inter-annotator agreement in the annotation of implicit relations and their limited number.", "labels": [], "entities": []}, {"text": "We use the official ACE corpus from LDC.", "labels": [], "entities": [{"text": "ACE corpus from LDC", "start_pos": 20, "end_pos": 39, "type": "DATASET", "confidence": 0.8765053600072861}]}, {"text": "The training set consists of 674 annotated text documents (~300k words) and 9683 instances of relations.", "labels": [], "entities": []}, {"text": "During development, 155 of 674 documents in the training set are set aside for fine-tuning the system.", "labels": [], "entities": []}, {"text": "The testing set is held out only for final evaluation.", "labels": [], "entities": []}, {"text": "It consists of 97 documents (~50k words) and 1386 instances of relations.", "labels": [], "entities": []}, {"text": "lists the types and subtypes of relations for the ACE Relation Detection and Characterization (RDC) task, along with their frequency of occurrence in the ACE training set.", "labels": [], "entities": [{"text": "ACE Relation Detection and Characterization (RDC) task", "start_pos": 50, "end_pos": 104, "type": "TASK", "confidence": 0.8613240917523702}, {"text": "ACE training set", "start_pos": 154, "end_pos": 170, "type": "DATASET", "confidence": 0.844170610109965}]}, {"text": "It shows that the ACE corpus suffers from a small amount of annotated data fora few subtypes such as the subtype \"Founder\" under the type \"ROLE\".", "labels": [], "entities": [{"text": "ACE corpus", "start_pos": 18, "end_pos": 28, "type": "DATASET", "confidence": 0.936122864484787}, {"text": "Founder", "start_pos": 114, "end_pos": 121, "type": "DATASET", "confidence": 0.910784125328064}, {"text": "ROLE", "start_pos": 139, "end_pos": 143, "type": "METRIC", "confidence": 0.9332378506660461}]}, {"text": "It also shows that the ACE RDC task defines some difficult subtypes such as the subtypes \"Based-In\", \"Located\" and \"Residence\" under the type \"AT\", which are difficult even for human experts to differentiate.", "labels": [], "entities": [{"text": "ACE RDC task", "start_pos": 23, "end_pos": 35, "type": "TASK", "confidence": 0.6752957900365194}, {"text": "AT", "start_pos": 143, "end_pos": 145, "type": "METRIC", "confidence": 0.9248583912849426}]}, {"text": "In this paper, we explicitly model the argument order of the two mentions involved.", "labels": [], "entities": []}, {"text": "For example, when comparing mentions m1 and m2, we distinguish between m1-ROLE.Citizen-Of-m2 and m2-ROLE.Citizen-Of-m1.", "labels": [], "entities": []}, {"text": "Note that only 6 of these 24 relation subtypes are symmetric: \"RelativeLocation\", \"Associate\", \"Other-Relative\", \"OtherProfessional\", \"Sibling\", and \"Spouse\".", "labels": [], "entities": []}, {"text": "In this way, we model relation extraction as a multi-class classification problem with 43 classes, two for each relation subtype (except the above 6 symmetric subtypes) and a \"NONE\" class for the case where the two mentions are not related.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 22, "end_pos": 41, "type": "TASK", "confidence": 0.8563716113567352}, {"text": "NONE", "start_pos": 176, "end_pos": 180, "type": "METRIC", "confidence": 0.9156992435455322}]}, {"text": "In this paper, we only measure the performance of relation extraction on \"true\" mentions with \"true\" chaining of coreference (i.e. as annotated by the corpus annotators) in the ACE corpus.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 50, "end_pos": 69, "type": "TASK", "confidence": 0.8443194329738617}, {"text": "ACE corpus", "start_pos": 177, "end_pos": 187, "type": "DATASET", "confidence": 0.9760195910930634}]}, {"text": "measures the performance of our relation extrac-tion system over the 43 ACE relation subtypes on the testing set.", "labels": [], "entities": []}, {"text": "It shows that our system achieves best performance of 63.1%/49.5%/ 55.5 in precision/recall/F-measure when combining diverse lexical, syntactic and semantic features.", "labels": [], "entities": [{"text": "precision", "start_pos": 75, "end_pos": 84, "type": "METRIC", "confidence": 0.9985994696617126}, {"text": "recall", "start_pos": 85, "end_pos": 91, "type": "METRIC", "confidence": 0.6049817204475403}, {"text": "F-measure", "start_pos": 92, "end_pos": 101, "type": "METRIC", "confidence": 0.6015239357948303}]}, {"text": "\u2022 Entity type features are very useful and improve the F-measure by 8.1 largely due to the recall increase.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 55, "end_pos": 64, "type": "METRIC", "confidence": 0.9974363446235657}, {"text": "recall", "start_pos": 91, "end_pos": 97, "type": "METRIC", "confidence": 0.9993784427642822}]}, {"text": "\u2022 The usefulness of mention level features is quite limited.", "labels": [], "entities": []}, {"text": "It only improves the F-measure by 0.8 due to the recall increase.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 21, "end_pos": 30, "type": "METRIC", "confidence": 0.9935439825057983}, {"text": "recall", "start_pos": 49, "end_pos": 55, "type": "METRIC", "confidence": 0.9994140863418579}]}, {"text": "\u2022 Incorporating the overlap features gives some balance between precision and recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 64, "end_pos": 73, "type": "METRIC", "confidence": 0.99959796667099}, {"text": "recall", "start_pos": 78, "end_pos": 84, "type": "METRIC", "confidence": 0.9975787997245789}]}, {"text": "It increases the F-measure by 3.6 with a big precision decrease and a big recall increase.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 17, "end_pos": 26, "type": "METRIC", "confidence": 0.9954196214675903}, {"text": "precision decrease", "start_pos": 45, "end_pos": 63, "type": "METRIC", "confidence": 0.982681155204773}, {"text": "recall", "start_pos": 74, "end_pos": 80, "type": "METRIC", "confidence": 0.999679684638977}]}, {"text": "\u2022 Chunking features are very useful.", "labels": [], "entities": []}, {"text": "It increases the precision/recall/F-measure by 4.1%/5.6%/ 5.2 respectively.", "labels": [], "entities": [{"text": "precision", "start_pos": 17, "end_pos": 26, "type": "METRIC", "confidence": 0.9992781281471252}, {"text": "recall", "start_pos": 27, "end_pos": 33, "type": "METRIC", "confidence": 0.9618890285491943}, {"text": "F-measure", "start_pos": 34, "end_pos": 43, "type": "METRIC", "confidence": 0.9100416302680969}]}, {"text": "\u2022 To our surprise, incorporating the dependency tree and parse tree features only improve the Fmeasure by 0.6 and 0.4 respectively.", "labels": [], "entities": [{"text": "Fmeasure", "start_pos": 94, "end_pos": 102, "type": "METRIC", "confidence": 0.7267816662788391}]}, {"text": "This maybe due to the fact that most of relations in the ACE corpus are quite local.", "labels": [], "entities": [{"text": "ACE corpus", "start_pos": 57, "end_pos": 67, "type": "DATASET", "confidence": 0.8859280943870544}]}, {"text": "shows that about 70% of relations exist where two mentions are embedded in each other or separated by at most one word.", "labels": [], "entities": []}, {"text": "While short-distance relations dominate and can be resolved by above simple features, the dependency tree and parse tree features can only take effect in the remaining much less long-distance relations.", "labels": [], "entities": []}, {"text": "However, full parsing is always prone to long distance errors although the Collins' parser used in our system represents the state-of-the-art in full parsing.", "labels": [], "entities": [{"text": "full parsing", "start_pos": 9, "end_pos": 21, "type": "TASK", "confidence": 0.614684671163559}]}, {"text": "\u2022 Incorporating semantic resources such as the country name list and the personal relative trigger word list further increases the F-measure by 1.5 largely due to the differentiation of the relation subtype \"ROLE.Citizen-Of\" from \"ROLE.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 131, "end_pos": 140, "type": "METRIC", "confidence": 0.9965165853500366}, {"text": "ROLE", "start_pos": 231, "end_pos": 235, "type": "METRIC", "confidence": 0.8033818602561951}]}, {"text": "Residence\" by distinguishing country GPEs from other GPEs.", "labels": [], "entities": []}, {"text": "The effect of personal relative trigger words is very limited due to the limited number of testing instances over personal social relation subtypes.", "labels": [], "entities": []}, {"text": "separately measures the performance of different relation types and major subtypes.", "labels": [], "entities": []}, {"text": "It also indicates the number of testing instances, the number of correctly classified instances and the number of wrongly classified instances for each type or subtype.", "labels": [], "entities": []}, {"text": "It is not surprising that the performance on the relation type \"NEAR\" is low because it occurs rarely in both the training and testing data.", "labels": [], "entities": []}, {"text": "Others like \"PART.Subsidary\" and \"SOCIAL.", "labels": [], "entities": [{"text": "PART.Subsidary", "start_pos": 13, "end_pos": 27, "type": "METRIC", "confidence": 0.5784128904342651}]}, {"text": "Other-Professional\" also suffer from their low occurrences.", "labels": [], "entities": [{"text": "occurrences", "start_pos": 47, "end_pos": 58, "type": "METRIC", "confidence": 0.9888994097709656}]}, {"text": "It also shows that our system performs best on the subtype \"SOCIAL.Parent\" and \"ROLE.", "labels": [], "entities": [{"text": "ROLE", "start_pos": 80, "end_pos": 84, "type": "METRIC", "confidence": 0.9801427721977234}]}, {"text": "This is largely due to incorporation of two semantic resources, i.e. the country name list and the personal relative trigger word list.", "labels": [], "entities": []}, {"text": "also indicates the low performance on the relation type \"AT\" although it frequently occurs in both the training and testing data.", "labels": [], "entities": [{"text": "AT", "start_pos": 57, "end_pos": 59, "type": "METRIC", "confidence": 0.9103736281394958}]}, {"text": "This suggests the difficulty of detecting and classifying the relation type \"AT\" and its subtypes.", "labels": [], "entities": []}, {"text": "separates the performance of relation detection from overall performance on the testing set.", "labels": [], "entities": [{"text": "relation detection", "start_pos": 29, "end_pos": 47, "type": "TASK", "confidence": 0.8986534178256989}]}, {"text": "It shows that our system achieves the performance of 84.8%/66.7%/74.7 in precision/recall/Fmeasure on relation detection.", "labels": [], "entities": [{"text": "precision", "start_pos": 73, "end_pos": 82, "type": "METRIC", "confidence": 0.9968668818473816}, {"text": "recall", "start_pos": 83, "end_pos": 89, "type": "METRIC", "confidence": 0.7310483455657959}, {"text": "Fmeasure", "start_pos": 90, "end_pos": 98, "type": "METRIC", "confidence": 0.8775418996810913}, {"text": "relation detection", "start_pos": 102, "end_pos": 120, "type": "TASK", "confidence": 0.8377490341663361}]}, {"text": "It also shows that our system achieves overall performance of 77.2%/60.7%/68.0 and 63.1%/49.5%/55.5 in precision/recall/F-measure on the 5 ACE relation types and the best-reported systems on the ACE corpus.", "labels": [], "entities": [{"text": "precision", "start_pos": 103, "end_pos": 112, "type": "METRIC", "confidence": 0.9983189702033997}, {"text": "recall", "start_pos": 113, "end_pos": 119, "type": "METRIC", "confidence": 0.6218199133872986}, {"text": "F-measure", "start_pos": 120, "end_pos": 129, "type": "METRIC", "confidence": 0.6446452736854553}, {"text": "ACE corpus", "start_pos": 195, "end_pos": 205, "type": "DATASET", "confidence": 0.9807141423225403}]}, {"text": "It shows that our system achieves better performance by ~3 F-measure largely due to its gain in recall.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 59, "end_pos": 68, "type": "METRIC", "confidence": 0.9983717799186707}, {"text": "recall", "start_pos": 96, "end_pos": 102, "type": "METRIC", "confidence": 0.9982519745826721}]}, {"text": "It also shows that feature-based methods dramatically outperform kernel methods.", "labels": [], "entities": []}, {"text": "This suggests that feature-based methods can effectively combine different features from a variety of sources (e.g. WordNet and gazetteers) that can be brought to bear on relation extraction.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 171, "end_pos": 190, "type": "TASK", "confidence": 0.877971738576889}]}, {"text": "The tree kernels developed in) are yet to be effective on the ACE RDC task.", "labels": [], "entities": [{"text": "ACE RDC task", "start_pos": 62, "end_pos": 74, "type": "TASK", "confidence": 0.6801469326019287}]}, {"text": "Finally, shows the distributions of errors.", "labels": [], "entities": []}, {"text": "It shows that 73% (627/864) of errors results from relation detection and 27% (237/864) of errors results from relation characterization, among which 17.8% (154/864) of errors are from misclassification across relation types and 9.6% (83/864) of errors are from misclassification of relation subtypes inside the same relation types.", "labels": [], "entities": [{"text": "relation detection", "start_pos": 51, "end_pos": 69, "type": "TASK", "confidence": 0.8689098656177521}, {"text": "relation characterization", "start_pos": 111, "end_pos": 136, "type": "TASK", "confidence": 0.8414731025695801}]}, {"text": "This suggests that relation detection is critical for relation extraction.", "labels": [], "entities": [{"text": "relation detection", "start_pos": 19, "end_pos": 37, "type": "TASK", "confidence": 0.9321250915527344}, {"text": "relation extraction", "start_pos": 54, "end_pos": 73, "type": "TASK", "confidence": 0.9199867844581604}]}], "tableCaptions": [{"text": " Table 1: Relation types and subtypes in the ACE  training data", "labels": [], "entities": [{"text": "ACE  training data", "start_pos": 45, "end_pos": 63, "type": "DATASET", "confidence": 0.8879391749699911}]}, {"text": " Table 2: Contribution of different features over 43  relation subtypes in the test data", "labels": [], "entities": []}, {"text": " Table 3: Distribution of relations over #words and #other mentions in between in the training data", "labels": [], "entities": []}, {"text": " Table 4: Performance of different relation types and major subtypes in the test data", "labels": [], "entities": []}, {"text": " Table 5: Comparison of our system with other best-reported systems on the ACE corpus", "labels": [], "entities": [{"text": "ACE", "start_pos": 75, "end_pos": 78, "type": "DATASET", "confidence": 0.8401899933815002}]}, {"text": " Table 6: Distribution of errors", "labels": [], "entities": [{"text": "Distribution of errors", "start_pos": 10, "end_pos": 32, "type": "METRIC", "confidence": 0.4854300320148468}]}]}