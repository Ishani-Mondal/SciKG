{"title": [{"text": "Learning Information Structure in The Prague Treebank", "labels": [], "entities": [{"text": "Prague Treebank", "start_pos": 38, "end_pos": 53, "type": "DATASET", "confidence": 0.9455727934837341}]}], "abstractContent": [{"text": "This paper investigates the automatic identification of aspects of Information Structure (IS) in texts.", "labels": [], "entities": [{"text": "automatic identification of aspects of Information Structure (IS) in texts", "start_pos": 28, "end_pos": 102, "type": "TASK", "confidence": 0.8238595401247343}]}, {"text": "The experiments use the Prague Dependency Treebank which is annotated with IS following the Praguian approach of Topic Focus Artic-ulation.", "labels": [], "entities": [{"text": "Prague Dependency Treebank", "start_pos": 24, "end_pos": 50, "type": "DATASET", "confidence": 0.9413650830586752}]}, {"text": "We automatically detect t(opic) and f(ocus), using node attributes from the treebank as basic features and derived features inspired by the annotation guidelines.", "labels": [], "entities": []}, {"text": "We show the performance of C4.5, Bagging, and Ripper classifiers on several classes of instances such as nouns and pronouns, only nouns, only pronouns.", "labels": [], "entities": [{"text": "Bagging, and Ripper classifiers", "start_pos": 33, "end_pos": 64, "type": "TASK", "confidence": 0.5159013390541076}]}, {"text": "A baseline system assigning always f(ocus) has an F-score of 42.5%.", "labels": [], "entities": [{"text": "F-score", "start_pos": 50, "end_pos": 57, "type": "METRIC", "confidence": 0.9996999502182007}]}, {"text": "Our best system obtains 82.04%.", "labels": [], "entities": []}], "introductionContent": [{"text": "Information Structure (IS) is a partitioning of the content of a sentence according to its relation to the discourse context.", "labels": [], "entities": [{"text": "Information Structure (IS)", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.795382022857666}]}, {"text": "There are numerous theoretical approaches describing IS and its semantics) and the terminology used is diverse (see for an overview).", "labels": [], "entities": [{"text": "IS", "start_pos": 53, "end_pos": 55, "type": "TASK", "confidence": 0.9711860418319702}]}, {"text": "However, all theories consider at least one of the following two distinctions: (i) a topic/focus 1 distinction that divides the linguistic meaning of the sentence into parts that link the content to the context, and others that advance the discourse, i.e. add or modify information; and (ii) \uf731 We use the Praguian terminology for this distinction.", "labels": [], "entities": [{"text": "Praguian terminology", "start_pos": 305, "end_pos": 325, "type": "DATASET", "confidence": 0.9333035349845886}]}, {"text": "a background/kontrast 2 distinction between parts of the utterance which contribute to distinguishing its actual content from alternatives the context makes available.", "labels": [], "entities": []}, {"text": "Existing theories, however, state their principles using carefully selected illustrative examples.", "labels": [], "entities": []}, {"text": "Because of this, they fail to adequately explain what possibly different linguistic dimensions cooperate to realize IS and how they do it.", "labels": [], "entities": []}, {"text": "In this paper we report the results of an experiment aimed to automatically identify aspects of IS.", "labels": [], "entities": [{"text": "IS", "start_pos": 96, "end_pos": 98, "type": "TASK", "confidence": 0.8529676795005798}]}, {"text": "This effort is part of a larger investigation aimed to get a more realistic view on the realization of IS in naturally occurring texts.", "labels": [], "entities": [{"text": "realization of IS in naturally occurring texts", "start_pos": 88, "end_pos": 134, "type": "TASK", "confidence": 0.7910455763339996}]}, {"text": "For such an investigation, the existence of a corpus annotated with some kind of 'informativity status' is of great importance.", "labels": [], "entities": []}, {"text": "Fully manual annotation of such a corpus is tedious and time-consuming.", "labels": [], "entities": []}, {"text": "Our plan is to initially annotate a small amount of data and then to build models to automatically detect IS in order to apply bootstrapping techniques to create a larger corpus.", "labels": [], "entities": []}, {"text": "This paper describes the results of a pilot study; its aim is to check if the idea of learning IS works by trying it on an already existing corpus.", "labels": [], "entities": []}, {"text": "For our experiments, we have used the Prague Dependency Treebank (PDT), as it is the only corpus annotated with IS (following the theory of Topic-Focus Articulation).", "labels": [], "entities": [{"text": "Prague Dependency Treebank (PDT)", "start_pos": 38, "end_pos": 70, "type": "DATASET", "confidence": 0.9262607793013254}]}, {"text": "We trained three different classifiers, C4.5, Bagging and Ripper, using basic features from the treebank and derived features inspired by the annotation guidelines.", "labels": [], "entities": [{"text": "Ripper", "start_pos": 58, "end_pos": 64, "type": "METRIC", "confidence": 0.8392429947853088}]}, {"text": "We have evaluated the performance of the classifiers against a baseline that simulates the preprocessing procedure that preceded the manual annotation of PDT, and \uf732 The notion 'kontrast' with a 'k' has been introduced in) to replace what Steedman calls 'focus', and to avoid confusion with other definitions of focus.", "labels": [], "entities": []}, {"text": "against a rule-based system which we implemented following the annotation instructions.", "labels": [], "entities": []}, {"text": "The organization of the paper is as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes the Prague Dependency Treebank, Section 3 presents the Praguian approach of TopicFocus Articulation, from two perspectives: of the theoretical definition and of the annotation guidelines that have been followed to annotate the PDT.", "labels": [], "entities": [{"text": "Prague Dependency Treebank", "start_pos": 24, "end_pos": 50, "type": "DATASET", "confidence": 0.9614769419034322}]}, {"text": "Section 4 presents the experimental setting, evaluation metric and results.", "labels": [], "entities": []}, {"text": "The paper closes with conclusions and issues for future research (Section 5).", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to perform the evaluation, we randomly selected 101,054 instances (1/3 of the data) from all the instances, which represents our test set; the remaining 2/3 of the data we used as a training set.", "labels": [], "entities": []}, {"text": "The same test set is used by all three classifiers.", "labels": [], "entities": []}, {"text": "In our experiments we have not tweaked the features and thus we have not set aside a development set.", "labels": [], "entities": []}, {"text": "In the test set 87% of the instances are nouns and 13% are pronouns.", "labels": [], "entities": []}, {"text": "The t/f distribution in the test set is as follows: 58% of the instances are t, and 42% instances are f.", "labels": [], "entities": []}, {"text": "We have built models using decision trees (C4.5), bagging and rule-induction (Ripper) machine learning techniques to predict the Information Structure.", "labels": [], "entities": []}, {"text": "We have also implemented a deterministic, rulebased system that assigns tor f according to the annotation guidelines presented in.", "labels": [], "entities": []}, {"text": "The rulebased system does not have access to what intonation center (IC) is.", "labels": [], "entities": []}, {"text": "The baseline simulates the preprocessing procedure used before the manual annotation of TFA attribute in the PDT, i.e., assigns always the class that has the most instances.", "labels": [], "entities": []}, {"text": "Our machine learning models are compared against the baseline and the rule-based system.", "labels": [], "entities": []}, {"text": "As a metric we have used the Weighted Averaged F-score which is computed as follows: The reason why we have chosen this metric (instead of Correctly Classified, for example) is that it gives a more realistic evaluation of the system, considering also the distribution oft and f items 6 . \uf736 Consider, for example, the casein which the test set consists of 70% f items and 30% t items.", "labels": [], "entities": [{"text": "Weighted Averaged F-score", "start_pos": 29, "end_pos": 54, "type": "METRIC", "confidence": 0.6347587505976359}]}], "tableCaptions": [{"text": " Table 2: Overall results: Weighted Averaged F-score as percentage", "labels": [], "entities": [{"text": "Weighted Averaged", "start_pos": 27, "end_pos": 44, "type": "METRIC", "confidence": 0.832308828830719}, {"text": "F-score", "start_pos": 45, "end_pos": 52, "type": "METRIC", "confidence": 0.6320241093635559}]}, {"text": " Table 3: Contribution of different features. F-score  given as a percentage.", "labels": [], "entities": [{"text": "F-score", "start_pos": 46, "end_pos": 53, "type": "METRIC", "confidence": 0.9993828535079956}]}]}