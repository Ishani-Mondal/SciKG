{"title": [{"text": "Supervised and Unsupervised Learning for Sentence Compression", "labels": [], "entities": [{"text": "Sentence Compression", "start_pos": 41, "end_pos": 61, "type": "TASK", "confidence": 0.8749366700649261}]}], "abstractContent": [{"text": "In Statistics-Based Summarization-Step One: Sentence Compression, Knight and Marcu (Knight and Marcu, 2000) (K&M) present a noisy-channel model for sentence compression.", "labels": [], "entities": [{"text": "Statistics-Based Summarization-Step", "start_pos": 3, "end_pos": 38, "type": "TASK", "confidence": 0.5944850742816925}, {"text": "sentence compression", "start_pos": 148, "end_pos": 168, "type": "TASK", "confidence": 0.7586187124252319}]}, {"text": "The main difficulty in using this method is the lack of data; Knight and Marcu use a corpus of 1035 training sentences.", "labels": [], "entities": []}, {"text": "More data is not easily available, so in addition to improving the original K&M noisy-channel model, we create unsupervised and semi-supervised models of the task.", "labels": [], "entities": []}, {"text": "Finally, we point out problems with modeling the task in this way.", "labels": [], "entities": []}, {"text": "They suggest areas for future research .", "labels": [], "entities": []}], "introductionContent": [{"text": "Summarization in general, and sentence compression in particular, are popular topics.", "labels": [], "entities": [{"text": "Summarization", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.9707485437393188}, {"text": "sentence compression", "start_pos": 30, "end_pos": 50, "type": "TASK", "confidence": 0.7915339171886444}]}, {"text": "Knight and Marcu (henceforth K&M) introduce the task of statistical sentence compression in Statistics-Based Summarization -Step One: Sentence Compression).", "labels": [], "entities": [{"text": "statistical sentence compression", "start_pos": 56, "end_pos": 88, "type": "TASK", "confidence": 0.6503742337226868}, {"text": "Statistics-Based Summarization", "start_pos": 92, "end_pos": 122, "type": "TASK", "confidence": 0.6130779683589935}]}, {"text": "The appeal of this problem is that it produces summarizations on a small scale.", "labels": [], "entities": []}, {"text": "It simplifies general compression problems, such as text-to-abstract conversion, by eliminating the need for coherency between sentences.", "labels": [], "entities": [{"text": "text-to-abstract conversion", "start_pos": 52, "end_pos": 79, "type": "TASK", "confidence": 0.7262775599956512}]}, {"text": "The model is further simplified by being constrained to word deletion: no rearranging of words takes place.", "labels": [], "entities": []}, {"text": "Others have performed the sentence compression task using syntactic approaches to this problem () (), but we focus exclusively on the K&M formulation.", "labels": [], "entities": [{"text": "sentence compression task", "start_pos": 26, "end_pos": 51, "type": "TASK", "confidence": 0.8476846019426981}, {"text": "K&M formulation", "start_pos": 134, "end_pos": 149, "type": "TASK", "confidence": 0.5244624763727188}]}, {"text": "Though the problem is simpler, it is still pertinent to current needs; generation of captions for television and audio scanning services for the blind, as well as compressing chosen sentences for headline generation) are examples of uses for sentence compression.", "labels": [], "entities": [{"text": "headline generation", "start_pos": 196, "end_pos": 215, "type": "TASK", "confidence": 0.7967427670955658}, {"text": "sentence compression", "start_pos": 242, "end_pos": 262, "type": "TASK", "confidence": 0.7770774066448212}]}, {"text": "In addition to simplifying the task, K&M's noisy-channel formulation is also appealing.", "labels": [], "entities": []}, {"text": "In the following sections, we discuss the K&M noisy-channel model.", "labels": [], "entities": []}, {"text": "We then present our cleaned up, and slightly improved noisy-channel model.", "labels": [], "entities": []}, {"text": "We also develop unsupervised and semi-supervised (our term fora combination of supervised and unsupervised) methods of sentence compression with inspiration from the K&M model, and create additional constraints to improve the compressions.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 119, "end_pos": 139, "type": "TASK", "confidence": 0.7443997412919998}]}, {"text": "We conclude with the problems inherent in both models.", "labels": [], "entities": []}], "datasetContent": [{"text": "As with original work, we use the same 32 sentence pairs as our Test Corpus, leaving us with 1035 training pairs.", "labels": [], "entities": [{"text": "Test Corpus", "start_pos": 64, "end_pos": 75, "type": "DATASET", "confidence": 0.9709683656692505}]}, {"text": "After adjusting the supervised weighting parameter, we fold the development setback into the training data.", "labels": [], "entities": []}, {"text": "We presented four judges with nine compressed versions of each of the 32 long sentences: A humangenerated short version, the K&M version, our first supervised version, our supervised version with our special rules, our supervised version with special rules and additional constraints, our unsupervised version, our supervised version with additional constraints, our semi-supervised version, and our semisupervised version with additional constraints.", "labels": [], "entities": [{"text": "K&M version", "start_pos": 125, "end_pos": 136, "type": "DATASET", "confidence": 0.6764808222651482}]}, {"text": "The judges were asked to rate the sentences in two ways: the grammaticality of the short sentences on a scale from 1 to 5, and the importance of the short sentence, or how well the compressed version retained the important words from the original, also on a scale from 1 to 5.", "labels": [], "entities": []}, {"text": "The short sentences were randomly shuffled across test cases.", "labels": [], "entities": []}, {"text": "The results in show compression rates, as well as average grammar and importance scores across judges.", "labels": [], "entities": [{"text": "compression", "start_pos": 20, "end_pos": 31, "type": "METRIC", "confidence": 0.9921179413795471}, {"text": "grammar and importance scores", "start_pos": 58, "end_pos": 87, "type": "METRIC", "confidence": 0.8095565587282181}]}, {"text": "There are two main ideas to takeaway from these results.", "labels": [], "entities": []}, {"text": "First, we can get good compressions without paired training data.", "labels": [], "entities": []}, {"text": "Second, we achieved a good boost by adding our additional constraints in two of the three versions.", "labels": [], "entities": []}, {"text": "Note that importance is a somewhat arbitrary distinction, since according to our judges, all of the computer-generated versions do as well in importance as the human-generated versions.", "labels": [], "entities": []}], "tableCaptions": []}