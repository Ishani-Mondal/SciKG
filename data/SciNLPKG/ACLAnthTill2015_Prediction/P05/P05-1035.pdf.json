{"title": [{"text": "QARLA:A Framework for the Evaluation of Text Summarization Systems", "labels": [], "entities": [{"text": "Evaluation of Text Summarization", "start_pos": 26, "end_pos": 58, "type": "TASK", "confidence": 0.588334709405899}]}], "abstractContent": [{"text": "This paper presents a probabilistic framework, QARLA, for the evaluation of text summarisation systems.", "labels": [], "entities": [{"text": "QARLA", "start_pos": 47, "end_pos": 52, "type": "METRIC", "confidence": 0.6748834848403931}, {"text": "text summarisation", "start_pos": 76, "end_pos": 94, "type": "TASK", "confidence": 0.6840350776910782}]}, {"text": "The input of the framework is a set of manual (reference) summaries, a set of base-line (automatic) summaries and a set of similarity metrics between summaries.", "labels": [], "entities": []}, {"text": "It provides i) a measure to evaluate the quality of any set of similarity metrics, ii) a measure to evaluate the quality of a summary using an optimal set of similarity metrics, and iii) a measure to evaluate whether the set of baseline summaries is reliable or may produce biased results.", "labels": [], "entities": []}, {"text": "Compared to previous approaches, our framework is able to combine different metrics and evaluate the quality of a set of metrics without any a-priori weight-ing of their relative importance.", "labels": [], "entities": []}, {"text": "We provide quantitative evidence about the effectiveness of the approach to improve the automatic evaluation of text sum-marisation systems by combining several similarity metrics.", "labels": [], "entities": []}], "introductionContent": [{"text": "The quality of an automatic summary can be established mainly with two approaches: Human assessments: The output of a number of summarisation systems is compared by human judges, using some set of evaluation guidelines.", "labels": [], "entities": []}, {"text": "Proximity to a gold standard: The best automatic summary is the one that is closest to some reference summary made by humans.", "labels": [], "entities": []}, {"text": "Using human assessments has some clear advantages: the results of the evaluation are interpretable, and we can trace what a system is doing well, and what is doing poorly.", "labels": [], "entities": []}, {"text": "But it also has a couple of serious drawbacks: i) different human assessors reach different conclusions, and ii) the outcome of a comparative evaluation exercise is not directly reusable for new techniques, i.e., a summarisation strategy developed after the comparative exercise cannot be evaluated without additional human assessments made from scratch.", "labels": [], "entities": []}, {"text": "Proximity to a gold standard, on the other hand, is a criterion that can be automated (see Section 6), with the advantages of i) being objective, and ii) once gold standard summaries are built fora comparative evaluation of systems, the resulting testbed can iteratively be used to refine text summarisation techniques and re-evaluate them automatically.", "labels": [], "entities": [{"text": "text summarisation", "start_pos": 289, "end_pos": 307, "type": "TASK", "confidence": 0.6406138390302658}]}, {"text": "This second approach, however, requires solving a number of non-trivial issues.", "labels": [], "entities": []}, {"text": "For instance, (i) How can we know whether an evaluation metric is good enough for automatic evaluation?, (ii) different users produce different summaries, all of them equally good as gold standards, (iii) if we have several metrics which test different features of a summary, how can we combine them into an optimal test?, (iv) how do we know if our test bed is reliable, or the evaluation outcome may change by adding, for instance, additional gold standards?", "labels": [], "entities": []}, {"text": "In this paper, we introduce a probabilistic framework, QARLA, that addresses such issues.", "labels": [], "entities": []}, {"text": "Given a set of manual summaries and another set of baseline summaries per task, together with a set of similarity metrics, QARLA provides quantitative measures to (i) select and combine the best (independent) metrics (KING measure), (ii) apply the best set of metrics to evaluate automatic summaries (QUEEN measure), and (iii) test whether evaluating with that test-bed is reliable (JACK measure).", "labels": [], "entities": [{"text": "KING measure)", "start_pos": 218, "end_pos": 231, "type": "METRIC", "confidence": 0.9171241521835327}, {"text": "QUEEN measure)", "start_pos": 301, "end_pos": 315, "type": "METRIC", "confidence": 0.8217761715253195}, {"text": "JACK measure)", "start_pos": 383, "end_pos": 396, "type": "METRIC", "confidence": 0.9574967424074808}]}], "datasetContent": [{"text": "We are looking fora framework to evaluate automatic summarisation systems objectively using similarity metrics to compare summaries.", "labels": [], "entities": [{"text": "summarisation", "start_pos": 52, "end_pos": 65, "type": "TASK", "confidence": 0.9231886267662048}]}, {"text": "The input of the framework is: \u2022 A summarisation task (e.g. topic oriented, informative multi-document summarisation on a given domain/corpus).", "labels": [], "entities": []}, {"text": "\u2022 A set T of test cases (e.g. topic/document set pairs for the example above) \u2022 A set of summaries M produced by humans (models), and a set of automatic summaries A (peers), for every test case.", "labels": [], "entities": []}, {"text": "\u2022 A set X of similarity metrics to compare summaries.", "labels": [], "entities": []}, {"text": "An evaluation framework should include, at least: the quality of an automatic summary a, using the similarity metrics in X to compare the summary with the models in M . With Q, we can compare the quality of automatic summaries.", "labels": [], "entities": []}, {"text": "\u2022 A measure K M,A (X) \u2208 [0, 1] that estimates the suitability of a set of similarity metrics X for our evaluation purposes.", "labels": [], "entities": []}, {"text": "With K, we can choose the best similarity metrics.", "labels": [], "entities": []}, {"text": "Our main assumption is that all manual summaries are equally optimal and, while they are likely to be different, the best similarity metric is the one that identifies and uses the features that are common to all manual summaries, grouping and separating them from the automatic summaries.", "labels": [], "entities": []}, {"text": "With these assumption in mind, it is useful to think of some formal restrictions that any evaluation framework Q, K must hold.", "labels": [], "entities": []}, {"text": "We will consider the following ones (see illustrations in): (1) Given two automatic summaries a, a and a similarity measure x, if a is more distant to all manual summaries than a , then a cannot be better than a . Formally: \u2200m \u2208 M.x(a, m) < x(a , m) \u2192 QM,x(a) \u2264 QM,x(a ) (2) A similarity metric x is better when it is able to group manual summaries more closely, while keeping them more distant from automatic summaries: If x is a perfect similarity metric, the quality of a manual summary cannot be zero: KM,A(x) = 1 \u2192 \u2200m \u2208 M.QM,x(m) > 0 (4) The quality of a similarity metric or a summary should not be dependent on scale issues.", "labels": [], "entities": []}, {"text": "In general, if x = f (x) with f being a growing monotonic function, then The quality of a similarity metric should not be sensitive to repeated elements in A, i.e.   The question of how to know which similarity metric is best to evaluate automatic summaries/translations has been addressed by \u2022 comparing the quality of automatic items with the quality of manual references (Culy and.", "labels": [], "entities": [{"text": "summaries/translations", "start_pos": 248, "end_pos": 270, "type": "TASK", "confidence": 0.8764632145563761}]}, {"text": "If the metric does not identify that the manual references are better, then it is not good enough for evaluation purposes.", "labels": [], "entities": []}, {"text": "\u2022 measuring the correlation between the values given by different metrics).", "labels": [], "entities": []}, {"text": "\u2022 measuring the correlation between the rankings generated by each metric and rankings generated by human assessors.", "labels": [], "entities": []}, {"text": "The methodology which is closest to our framework is ORANGE), which evaluates a similarity metric using the average ranks obtained by reference items within a baseline set.", "labels": [], "entities": [{"text": "ORANGE", "start_pos": 53, "end_pos": 59, "type": "METRIC", "confidence": 0.9787757992744446}]}, {"text": "As in our framework, ORANGE performs an automatic meta-evaluation, there is no need for human assessments, and it does not depend on the scale properties of the metric being evaluated (because changes of scale preserve rankings).", "labels": [], "entities": [{"text": "ORANGE", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.8009891510009766}]}, {"text": "The OR-ANGE approach is, indeed, closely related to the original QARLA measure introduced in ().", "labels": [], "entities": [{"text": "OR-ANGE", "start_pos": 4, "end_pos": 11, "type": "METRIC", "confidence": 0.955008327960968}, {"text": "QARLA measure", "start_pos": 65, "end_pos": 78, "type": "METRIC", "confidence": 0.8706235289573669}]}, {"text": "Our KING, QUEEN, JACK framework, however, has a number of advantages over ORANGE: \u2022 It is able to combine different metrics, and evaluate the quality of metric sets, without any a-priori weighting of their relative importance.", "labels": [], "entities": [{"text": "KING", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.9532923102378845}, {"text": "QUEEN", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9386508464813232}, {"text": "JACK", "start_pos": 17, "end_pos": 21, "type": "METRIC", "confidence": 0.925665020942688}, {"text": "ORANGE", "start_pos": 74, "end_pos": 80, "type": "METRIC", "confidence": 0.8789126873016357}]}, {"text": "\u2022 It is not sensitive to repeated (or very similar) baseline elements.", "labels": [], "entities": []}, {"text": "\u2022 It provides a mechanism, JACK, to check whether a set X, M, A of metrics, manual and baseline items is reliable enough to produce a stable evaluation of automatic summarisation systems.", "labels": [], "entities": [{"text": "JACK", "start_pos": 27, "end_pos": 31, "type": "METRIC", "confidence": 0.960793137550354}]}, {"text": "Probably the most significant improvement over ORANGE is the ability of KING, QUEEN, JACK to combine automatically the information of different metrics.", "labels": [], "entities": [{"text": "ORANGE", "start_pos": 47, "end_pos": 53, "type": "METRIC", "confidence": 0.9583114385604858}, {"text": "KING", "start_pos": 72, "end_pos": 76, "type": "METRIC", "confidence": 0.991179883480072}, {"text": "QUEEN", "start_pos": 78, "end_pos": 83, "type": "METRIC", "confidence": 0.9858617782592773}, {"text": "JACK", "start_pos": 85, "end_pos": 89, "type": "METRIC", "confidence": 0.9838756918907166}]}, {"text": "We believe that a comprehensive automatic evaluation of a summary must necessarily capture different aspects of the problem with different metrics, and that the results of every individual metric should not be combined in any prescribed algebraic way (such as a linear weighted combination).", "labels": [], "entities": []}, {"text": "Our framework satisfies this condition.", "labels": [], "entities": []}, {"text": "An advantage of ORAN GE, however, is that it does not require a large number of gold standards to reach stability, as in the case of QARLA.", "labels": [], "entities": [{"text": "ORAN GE", "start_pos": 16, "end_pos": 23, "type": "METRIC", "confidence": 0.7858643531799316}, {"text": "QARLA", "start_pos": 133, "end_pos": 138, "type": "DATASET", "confidence": 0.9072104692459106}]}, {"text": "Finally, it is interesting to compare the rankings produced by QARLA with the output of human assessments, even if the philosophy of QARLA is not considering human assessments as the gold standard for evaluation.", "labels": [], "entities": [{"text": "QARLA", "start_pos": 63, "end_pos": 68, "type": "DATASET", "confidence": 0.8338227868080139}, {"text": "QARLA", "start_pos": 133, "end_pos": 138, "type": "DATASET", "confidence": 0.8721492290496826}]}], "tableCaptions": [{"text": " Table 1: Results of the test of identifying the manual summariser", "labels": [], "entities": [{"text": "summariser", "start_pos": 56, "end_pos": 66, "type": "TASK", "confidence": 0.7459388375282288}]}]}