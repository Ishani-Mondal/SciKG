{"title": [{"text": "Multi-Engine Machine Translation Guided by Explicit Word Matching", "labels": [], "entities": [{"text": "Multi-Engine Machine Translation", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.6077832678953806}, {"text": "Explicit Word Matching", "start_pos": 43, "end_pos": 65, "type": "TASK", "confidence": 0.5365961988766988}]}], "abstractContent": [{"text": "We describe anew approach for synthetically combining the output of several different Machine Translation (MT) engines operating on the same input.", "labels": [], "entities": [{"text": "Machine Translation (MT)", "start_pos": 86, "end_pos": 110, "type": "TASK", "confidence": 0.852734637260437}]}, {"text": "The goal is to produce a synthetic combination that surpasses all of the original systems in translation quality.", "labels": [], "entities": []}, {"text": "Our approach uses the individual MT engines as \"black boxes\" and does not require any explicit cooperation from the original MT systems.", "labels": [], "entities": []}, {"text": "A decoding algorithm uses explicit word matches, in conjunction with confidence estimates for the various engines and a tri-gram language model in order to score and rank a collection of sentence hypotheses that are synthetic combinations of words from the various original engines.", "labels": [], "entities": []}, {"text": "The highest scoring sentence hypothesis is selected as the final output of our system.", "labels": [], "entities": []}, {"text": "Experiments, using several Arabic-to-English systems of similar quality, show a substantial improvement in the quality of the translation output.", "labels": [], "entities": []}], "introductionContent": [{"text": "A variety of different paradigms for machine translation (MT) have been developed over the years, ranging from statistical systems that learn mappings between words and phrases in the source language and their corresponding translations in the target language, to Interlingua-based systems that perform deep semantic analysis.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 37, "end_pos": 61, "type": "TASK", "confidence": 0.8757305800914764}, {"text": "deep semantic analysis", "start_pos": 303, "end_pos": 325, "type": "TASK", "confidence": 0.6429150899251302}]}, {"text": "Each approach and system has different advantages and disadvantages.", "labels": [], "entities": []}, {"text": "While statistical systems provide broad coverage with little manpower, the quality of the corpus based systems rarely reaches the quality of knowledge based systems.", "labels": [], "entities": []}, {"text": "With such a wide range of approaches to machine translation, it would be beneficial to have an effective framework for combining these systems into an MT system that carries many of the advantages of the individual systems and suffers from few of their disadvantages.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 40, "end_pos": 59, "type": "TASK", "confidence": 0.7849417328834534}, {"text": "MT", "start_pos": 151, "end_pos": 153, "type": "TASK", "confidence": 0.974108099937439}]}, {"text": "Attempts at combining the output of different systems have proved useful in other areas of language technologies, such as the ROVER approach for speech recognition.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 145, "end_pos": 163, "type": "TASK", "confidence": 0.815654993057251}]}, {"text": "Several approaches to multi-engine machine translation systems have been proposed over the past decade.", "labels": [], "entities": [{"text": "multi-engine machine translation", "start_pos": 22, "end_pos": 54, "type": "TASK", "confidence": 0.6278331577777863}]}, {"text": "The Pangloss system and work by several other researchers attempted to combine lattices from many different MT systems).", "labels": [], "entities": [{"text": "Pangloss system", "start_pos": 4, "end_pos": 19, "type": "DATASET", "confidence": 0.9643230736255646}, {"text": "MT", "start_pos": 108, "end_pos": 110, "type": "TASK", "confidence": 0.9387529492378235}]}, {"text": "These systems suffer from requiring cooperation from all the systems to produce compatible lattices as well as the hard research problem of standardizing confidence scores that come from the individual engines.", "labels": [], "entities": []}, {"text": "In 2001, Bangalore et al used string alignments between the different translations to train a finite state machine to produce a consensus translation.", "labels": [], "entities": []}, {"text": "The alignment algorithm described in that work, which only allows insertions, deletions and substitutions, does not accurately capture long range phrase movement.", "labels": [], "entities": []}, {"text": "In this paper, we propose anew way of combining the translations of multiple MT systems based on a more versatile word alignment algorithm.", "labels": [], "entities": [{"text": "MT", "start_pos": 77, "end_pos": 79, "type": "TASK", "confidence": 0.9499748945236206}, {"text": "word alignment", "start_pos": 114, "end_pos": 128, "type": "TASK", "confidence": 0.7583445310592651}]}, {"text": "A \"decoding\" algorithm then uses these alignments, in conjunction with confidence estimates for the various engines and a trigram language model, in order to score and rank a collection of sentence hypotheses that are synthetic combinations of words from the various original engines.", "labels": [], "entities": []}, {"text": "The highest scoring sentence hypothesis is selected as the final output of our system.", "labels": [], "entities": []}, {"text": "We experimentally tested the new approach by combining translations obtained from combining three Arabic-to-English translation systems.", "labels": [], "entities": []}, {"text": "Translation quality is scored using the METEOR MT evaluation metric (Lavie,.", "labels": [], "entities": [{"text": "Translation", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.9458386301994324}, {"text": "METEOR MT evaluation metric", "start_pos": 40, "end_pos": 67, "type": "METRIC", "confidence": 0.72722227871418}]}, {"text": "Our experiments demonstrate that our new MEMT system achieves a substantial improvement overall of the original systems, and also outperforms an \"oracle\" capable of selecting the best of the original systems on a sentence-by-sentence basis.", "labels": [], "entities": []}, {"text": "The remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "In section 2 we describe the algorithm for generating multi-engine synthetic translations.", "labels": [], "entities": [{"text": "generating multi-engine synthetic translations", "start_pos": 43, "end_pos": 89, "type": "TASK", "confidence": 0.5616106986999512}]}, {"text": "Section 3 describes the experimental setup used to evaluate our approach, and section 4 presents the results of the evaluation.", "labels": [], "entities": []}, {"text": "Our conclusions and directions for future work are presented in section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "We combined outputs of three Arabic-to-English machine translation systems on the 2003 TIDES Arabic test set.", "labels": [], "entities": [{"text": "Arabic-to-English machine translation", "start_pos": 29, "end_pos": 66, "type": "TASK", "confidence": 0.6956767837206522}, {"text": "2003 TIDES Arabic test set", "start_pos": 82, "end_pos": 108, "type": "DATASET", "confidence": 0.8125970482826232}]}, {"text": "The systems were AppTek's rule based system, CMU's EBMT system, and Systran's web-based translation system.", "labels": [], "entities": []}, {"text": "We compare the results of MEMT to the individual online machine translation systems.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 56, "end_pos": 75, "type": "TASK", "confidence": 0.7427431643009186}]}, {"text": "We also compare the performance of MEMT to the score of an \"oracle system\" that chooses the best scoring of the individual systems for each sentence.", "labels": [], "entities": []}, {"text": "Note that this oracle is not a realistic system, since areal system cannot determine at runtime which of the original systems is best on a sentence-by-sentence basis.", "labels": [], "entities": []}, {"text": "One goal of the evaluation was to see how rich the space of synthetic translations produced by our hypothesis generator is.", "labels": [], "entities": []}, {"text": "To this end, we also compare the output selected by our current MEMT system to an \"oracle system\" that chooses the best synthetic translation that was generated by the decoder for each sentence.", "labels": [], "entities": []}, {"text": "This too is not a realistic system, but it allows us to see how well our hypothesis scoring currently performs.", "labels": [], "entities": []}, {"text": "This also provides away of estimating a performance ceiling of the MEMT approach, since our MEMT can only produce words that are provided by the original systems.", "labels": [], "entities": []}, {"text": "Due to the computational complexity of running the oracle system, several practical restrictions were imposed.", "labels": [], "entities": []}, {"text": "First, the oracle system only had access to the top 1000 translation hypotheses produced by MEMT for each sentence.", "labels": [], "entities": [{"text": "MEMT", "start_pos": 92, "end_pos": 96, "type": "DATASET", "confidence": 0.8043394088745117}]}, {"text": "While this does not guarantee finding the best translation that the decoder can produce, this method provides a good approximation.", "labels": [], "entities": []}, {"text": "We also ran the oracle experiment only on the first 140 sentences of the test sets due to time constraints.", "labels": [], "entities": []}, {"text": "All the system performances are measured using the METEOR evaluation metric).", "labels": [], "entities": [{"text": "METEOR evaluation metric", "start_pos": 51, "end_pos": 75, "type": "METRIC", "confidence": 0.8680242697397867}]}, {"text": "METEOR was chosen since, unlike the more commonly used BLEU metric (), it provides reasonably reliable scores for individual sentences.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.8924840092658997}, {"text": "BLEU metric", "start_pos": 55, "end_pos": 66, "type": "METRIC", "confidence": 0.9782834947109222}]}, {"text": "This property is essential in order to run our oracle experiments.", "labels": [], "entities": []}, {"text": "METEOR produces scores in the range of, based on a combination of unigram precision, unigram recall and an explicit penalty related to the average length of matched segments between the evaluated translation and its reference.", "labels": [], "entities": [{"text": "precision", "start_pos": 74, "end_pos": 83, "type": "METRIC", "confidence": 0.6713840365409851}, {"text": "recall", "start_pos": 93, "end_pos": 99, "type": "METRIC", "confidence": 0.8692587614059448}]}, {"text": "On the 2003 TIDES data, the three original systems had similar METEOR scores.", "labels": [], "entities": [{"text": "2003 TIDES data", "start_pos": 7, "end_pos": 22, "type": "DATASET", "confidence": 0.7089920441309611}, {"text": "METEOR", "start_pos": 63, "end_pos": 69, "type": "METRIC", "confidence": 0.9829522371292114}]}, {"text": "shows the scores of the three systems, with their names obscured to protect their privacy.", "labels": [], "entities": []}, {"text": "Also shown are the score of MEMT's output and the score of the oracle system that chooses the best original translation on a sentence-by-sentence basis.", "labels": [], "entities": []}, {"text": "The score of the MEMT system is significantly better than any of the original systems, and the sentence oracle.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: METEOR Scores on TIDES 2003 Dataset", "labels": [], "entities": [{"text": "METEOR Scores", "start_pos": 10, "end_pos": 23, "type": "METRIC", "confidence": 0.9606505036354065}, {"text": "TIDES 2003 Dataset", "start_pos": 27, "end_pos": 45, "type": "DATASET", "confidence": 0.8347394267717997}]}]}