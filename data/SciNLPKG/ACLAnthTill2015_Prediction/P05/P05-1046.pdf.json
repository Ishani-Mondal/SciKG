{"title": [{"text": "Unsupervised Learning of Field Segmentation Models for Information Extraction", "labels": [], "entities": [{"text": "Unsupervised Learning of Field Segmentation", "start_pos": 0, "end_pos": 43, "type": "TASK", "confidence": 0.5725293517112732}, {"text": "Information Extraction", "start_pos": 55, "end_pos": 77, "type": "TASK", "confidence": 0.7093840837478638}]}], "abstractContent": [{"text": "The applicability of many current information extraction techniques is severely limited by the need for supervised training data.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 34, "end_pos": 56, "type": "TASK", "confidence": 0.8349452316761017}]}, {"text": "We demonstrate that for certain field structured extraction tasks, such as classified advertisements and bibliographic citations , small amounts of prior knowledge can be used to learn effective models in a primarily unsu-pervised fashion.", "labels": [], "entities": [{"text": "field structured extraction tasks", "start_pos": 32, "end_pos": 65, "type": "TASK", "confidence": 0.7883405685424805}]}, {"text": "Although hidden Markov models (HMMs) provide a suitable generative model for field structured text, general unsupervised HMM learning fails to learn useful structure in either of our domains.", "labels": [], "entities": []}, {"text": "However, one can dramatically improve the quality of the learned structure by exploiting simple prior knowledge of the desired solutions.", "labels": [], "entities": []}, {"text": "In both domains, we found that unsuper-vised methods can attain accuracies with 400 un-labeled examples comparable to those attained by supervised methods on 50 labeled examples, and that semi-supervised methods can make good use of small amounts of labeled data.", "labels": [], "entities": []}], "introductionContent": [{"text": "Information extraction is potentially one of the most useful applications enabled by current natural language processing technology.", "labels": [], "entities": [{"text": "Information extraction", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8799588084220886}]}, {"text": "However, unlike general tools like parsers or taggers, which generalize reasonably beyond their training domains, extraction systems must be entirely retrained for each application.", "labels": [], "entities": []}, {"text": "As an example, consider the task of turning a set of diverse classified advertisements into a queryable database; each type of ad would require tailored training data fora supervised system.", "labels": [], "entities": []}, {"text": "Approaches which required little or no training data would therefore provide substantial resource savings and extend the practicality of extraction systems.", "labels": [], "entities": []}, {"text": "The term information extraction was introduced in the MUC evaluations for the task of finding short pieces of relevant information within a broader text that is mainly irrelevant, and returning it in a structured form.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 9, "end_pos": 31, "type": "TASK", "confidence": 0.8304294943809509}, {"text": "MUC evaluations", "start_pos": 54, "end_pos": 69, "type": "DATASET", "confidence": 0.7438790500164032}]}, {"text": "For such \"nugget extraction\" tasks, the use of unsupervised learning methods is difficult and unlikely to be fully successful, in part because the nuggets of interest are determined only extrinsically by the needs of the user or task.", "labels": [], "entities": [{"text": "nugget extraction\" tasks", "start_pos": 10, "end_pos": 34, "type": "TASK", "confidence": 0.8374377340078354}]}, {"text": "However, the term information extraction was in time generalized to a related task that we distinguish as field segmentation.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 18, "end_pos": 40, "type": "TASK", "confidence": 0.867928147315979}]}, {"text": "In this task, a document is regarded as a sequence of pertinent fields, and the goal is to segment the document into fields, and to label the fields.", "labels": [], "entities": []}, {"text": "For example, bibliographic citations, such as the one in(a), exhibit clear field structure, with fields such as author, title, and date.", "labels": [], "entities": []}, {"text": "Classified advertisements, such as the one in(b), also exhibit field structure, if less rigidly: an ad consists of descriptions of attributes of an item or offer, and a set of ads for similar items share the same attributes.", "labels": [], "entities": []}, {"text": "In these cases, the fields present a salient, intrinsic form of linguistic structure, and it is reasonable to hope that field segmentation models could be learned in an unsupervised fashion.", "labels": [], "entities": []}, {"text": "In this paper, we investigate unsupervised learning of field segmentation models in two domains: bibliographic citations and classified advertisements for apartment rentals.", "labels": [], "entities": []}, {"text": "General, unconstrained induction of HMMs using the EM algorithm fails to detect useful field structure in either domain.", "labels": [], "entities": []}, {"text": "However, we demonstrate that small amounts of prior knowledge can be used to greatly improve the learned model.", "labels": [], "entities": []}, {"text": "In both domains, we found that unsupervised methods can attain accuracies with 400 unlabeled examples comparable to those attained by supervised methods on 50 labeled examples, and that semi-supervised methods can make good use of small amounts of labeled data.", "labels": [], "entities": []}, {"text": ".: Examples of three domains for HMM learning: the bibliographic citation fields in (a) and classified advertisements for apartment rentals shown in (b) exhibit field structure.", "labels": [], "entities": [{"text": "HMM learning", "start_pos": 33, "end_pos": 45, "type": "TASK", "confidence": 0.9400193691253662}]}, {"text": "Contrast these to part-of-speech tagging in (c) which does not.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 18, "end_pos": 40, "type": "TASK", "confidence": 0.7411324977874756}]}], "datasetContent": [{"text": "The bibliographic citations data is described in, and is distributed at http://www.cs.umass.edu/~mccallum/.", "labels": [], "entities": []}, {"text": "It consists of 500 hand-annotated citations, each taken from the reference section of a different computer science research paper.", "labels": [], "entities": []}, {"text": "The citations are annotated with 13 fields, including author, title, date, journal, and soon.", "labels": [], "entities": []}, {"text": "The average citation has 35 tokens in 5.5 fields.", "labels": [], "entities": [{"text": "citation", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9729838967323303}]}, {"text": "We split this data, using its natural order, into a 300-document training set, a 100-document development set, and a 100-document test set.", "labels": [], "entities": []}, {"text": "The classified advertisements data set is novel, and consists of 8,767 classified advertisements for apartment rentals in the San Francisco Bay Area downloaded in June 2004 from the Craigslist website.", "labels": [], "entities": [{"text": "classified advertisements data set", "start_pos": 4, "end_pos": 38, "type": "DATASET", "confidence": 0.7909466028213501}]}, {"text": "It is distributed at http://www.stanford.edu/~grenager/.", "labels": [], "entities": []}, {"text": "302 of the ads have been labeled with 12 fields, including size, rent, neighborhood, features, and soon.", "labels": [], "entities": [{"text": "soon", "start_pos": 99, "end_pos": 103, "type": "METRIC", "confidence": 0.9823211431503296}]}, {"text": "The average ad has 119 tokens in 8.7 fields.", "labels": [], "entities": []}, {"text": "The annotated data is divided into a 102-document training set, a 100-document development set, and a 100-document test set.", "labels": [], "entities": []}, {"text": "The remaining 8465 documents form an unannotated training set.", "labels": [], "entities": []}, {"text": "In both cases, all system development and parameter tuning was performed on the development set,: Matrix representations of the target transition structure in two field structured domains: (a) classified advertisements (b) bibliographic citations.", "labels": [], "entities": [{"text": "parameter tuning", "start_pos": 42, "end_pos": 58, "type": "TASK", "confidence": 0.7431756258010864}]}, {"text": "Columns and rows are indexed by the same sequence of fields.", "labels": [], "entities": []}, {"text": "Also shown is (c) a submatrix of the transition structure fora part-of-speech tagging task.", "labels": [], "entities": [{"text": "part-of-speech tagging task", "start_pos": 63, "end_pos": 90, "type": "TASK", "confidence": 0.7512624263763428}]}, {"text": "In all cases the column labels are the same as the row labels. and the test set was only used once, for running final experiments.", "labels": [], "entities": []}, {"text": "Supervised learning experiments train on documents selected randomly from the annotated training set and test on the complete test set.", "labels": [], "entities": []}, {"text": "Unsupervised learning experiments also test on the complete test set, but create a training set by first adding documents from the test set (without annotation), then adding documents from the annotated training set (without annotation), and finally adding documents from the unannotated training set.", "labels": [], "entities": []}, {"text": "Thus if an unsupervised training set is larger than the test set, it fully contains the test set.", "labels": [], "entities": []}, {"text": "To evaluate our models, we first learn a set of model parameters, and then use the parameterized model to label the sequence of tokens in the test data with the model's hidden states.", "labels": [], "entities": []}, {"text": "We then compare the similarity of the guessed sequence to the humanannotated sequence of gold labels, and compute accuracy on a per-token basis.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 114, "end_pos": 122, "type": "METRIC", "confidence": 0.9994117021560669}]}, {"text": "In evaluation of supervised methods, the model states and gold labels are the same.", "labels": [], "entities": []}, {"text": "For models learned in a fully unsupervised fashion, we map each model state in a greedy fashion to the gold label to which it most often corresponds in the gold data.", "labels": [], "entities": []}, {"text": "There is a worry with this kind of greedy mapping: it increasingly inflates the results as the number of hidden states grows.", "labels": [], "entities": []}, {"text": "To keep the accuracies meaningful, all of our models have exactly the same number of hidden states as gold labels, and so the comparison is valid.", "labels": [], "entities": []}, {"text": "search procedure by carefully choosing the starting point; indeed smart initialization has been critical to success in many previous unsupervised learning experiments.", "labels": [], "entities": [{"text": "smart initialization", "start_pos": 66, "end_pos": 86, "type": "TASK", "confidence": 0.6494917571544647}]}, {"text": "The central idea of this paper is that we can instead restrict the entire search domain by constraining the model class to reflect the desired structure in the data, thereby directing the search toward models of interest.", "labels": [], "entities": []}, {"text": "We do this in several ways, which are described in the following sections.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Summary of results. For each experiment, we report  percentage accuracy on the test set. Supervised experiments  use 100 training documents, and unsupervised experiments use  400 training documents. Because unsupervised techniques are  stochastic, those results are averaged over 50 runs, and differ- ences greater than 1.0% are significant at p=0.05% or better ac- cording to the t-test. The last 6 rows are not cumulative.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 73, "end_pos": 81, "type": "METRIC", "confidence": 0.9895517230033875}, {"text": "ac- cording", "start_pos": 372, "end_pos": 383, "type": "METRIC", "confidence": 0.9485291043917338}]}]}