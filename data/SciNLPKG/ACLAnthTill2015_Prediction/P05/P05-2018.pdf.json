{"title": [{"text": "Centrality Measures in Text Mining: Prediction of Noun Phrases that Appear in Abstracts", "labels": [], "entities": [{"text": "Centrality Measures", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7292639911174774}, {"text": "Text Mining", "start_pos": 23, "end_pos": 34, "type": "TASK", "confidence": 0.6651181131601334}]}], "abstractContent": [{"text": "In this paper, we study different centrality measures being used in predicting noun phrases appearing in the abstracts of scientific articles.", "labels": [], "entities": [{"text": "predicting noun phrases appearing in the abstracts of scientific articles", "start_pos": 68, "end_pos": 141, "type": "TASK", "confidence": 0.8573704242706299}]}, {"text": "Our experimental results show that centrality measures improve the accuracy of the prediction in terms of both precision and recall.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 67, "end_pos": 75, "type": "METRIC", "confidence": 0.9992926120758057}, {"text": "precision", "start_pos": 111, "end_pos": 120, "type": "METRIC", "confidence": 0.9995583891868591}, {"text": "recall", "start_pos": 125, "end_pos": 131, "type": "METRIC", "confidence": 0.9981797933578491}]}, {"text": "We also found that the method of constructing Noun Phrase Network significantly influences the accuracy when using the centrality heuristics itself, but is negligible when it is used together with other text features in decision trees.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 95, "end_pos": 103, "type": "METRIC", "confidence": 0.9994868040084839}]}], "introductionContent": [{"text": "Research on text summarization, information retrieval, and information extraction often faces the question of how to determine which words are more significant than others in text.", "labels": [], "entities": [{"text": "text summarization", "start_pos": 12, "end_pos": 30, "type": "TASK", "confidence": 0.7183060646057129}, {"text": "information retrieval", "start_pos": 32, "end_pos": 53, "type": "TASK", "confidence": 0.8009233474731445}, {"text": "information extraction", "start_pos": 59, "end_pos": 81, "type": "TASK", "confidence": 0.8014492392539978}]}, {"text": "Normally we only consider content words, i.e., the open class words.", "labels": [], "entities": []}, {"text": "Non-content words or stop words, which are called function words in natural language processing, do not convey semantics so that they are excluded although they sometimes appear more frequently than content words.", "labels": [], "entities": []}, {"text": "A content word is usually defined as a term, although a term can also be a phrase.", "labels": [], "entities": []}, {"text": "Its significance is often indicated by Term Frequency (TF) and Inverse Document Frequency (IDF).", "labels": [], "entities": [{"text": "Inverse Document Frequency (IDF)", "start_pos": 63, "end_pos": 95, "type": "METRIC", "confidence": 0.6731260418891907}]}, {"text": "The usage of TF comes from \"the simple notion that terms which occur frequently in a document may reflect its meaning more strongly than terms that occur less frequently\").", "labels": [], "entities": []}, {"text": "On the contrary, IDF assigns smaller weights to terms which are contained in more documents.", "labels": [], "entities": [{"text": "IDF", "start_pos": 17, "end_pos": 20, "type": "DATASET", "confidence": 0.5175140500068665}]}, {"text": "That is simply because \"the more documents having the term, the less useful the term is in discriminating those documents having it from those not having it\" (.", "labels": [], "entities": []}, {"text": "TF and IDF also find their usage in automatic text summarization.", "labels": [], "entities": [{"text": "TF", "start_pos": 0, "end_pos": 2, "type": "DATASET", "confidence": 0.8306095004081726}, {"text": "IDF", "start_pos": 7, "end_pos": 10, "type": "DATASET", "confidence": 0.4950350224971771}, {"text": "text summarization", "start_pos": 46, "end_pos": 64, "type": "TASK", "confidence": 0.6288151293992996}]}, {"text": "In this circumstance, TF is used individually more often than together with IDF, since the term is not used to distinguish a document from another.", "labels": [], "entities": [{"text": "IDF", "start_pos": 76, "end_pos": 79, "type": "METRIC", "confidence": 0.7037886381149292}]}, {"text": "Automatic text summarization seeks away of producing a text which is much shorter than the document(s) to be summarized, and can serve as a surrogate for full-text.", "labels": [], "entities": [{"text": "text summarization", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.6434306353330612}]}, {"text": "Thus, for extractive summaries, i.e., summaries composed of original sentences from the text to be summarized, we try to find those terms which are more likely to be included in the summary.", "labels": [], "entities": [{"text": "summaries composed of original sentences from the text", "start_pos": 38, "end_pos": 92, "type": "TASK", "confidence": 0.8170486390590668}]}, {"text": "The overall goal of our research is to build a machine learning framework for automatic text summarization.", "labels": [], "entities": [{"text": "automatic text summarization", "start_pos": 78, "end_pos": 106, "type": "TASK", "confidence": 0.5973424712816874}]}, {"text": "This framework will learn the relationship between text documents and their corresponding abstracts written by human.", "labels": [], "entities": []}, {"text": "At the current stage the framework tries to generate a sentence ranking function and use it to produce extractive summaries.", "labels": [], "entities": []}, {"text": "It is important to find a set of features which represent most information in a sentence and hence the machine learning mechanism can work on it to produce a ranking function.", "labels": [], "entities": []}, {"text": "The next stage in our research will be to use the framework to generate abstractive summaries, i.e. summaries which do not use sentences from the input text verbatim.", "labels": [], "entities": []}, {"text": "Therefore, it is important to know what terms should be included in the summary.", "labels": [], "entities": []}, {"text": "In this paper we present the approach of using social network analysis technique to find terms, specifically noun phrases (NPs) in our experiments, which occur in the human-written abstracts.", "labels": [], "entities": []}, {"text": "We show that centrality measures increase the prediction accuracy.", "labels": [], "entities": [{"text": "prediction", "start_pos": 46, "end_pos": 56, "type": "TASK", "confidence": 0.9065982699394226}, {"text": "accuracy", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.9758828282356262}]}, {"text": "Two ways of constructing noun phrase network are compared.", "labels": [], "entities": []}, {"text": "Conclusions and future work are discussed at the end.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1. Results for Using 6 Feature Sets with YaDT", "labels": [], "entities": [{"text": "YaDT", "start_pos": 48, "end_pos": 52, "type": "DATASET", "confidence": 0.6532805562019348}]}]}