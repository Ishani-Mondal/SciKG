{"title": [{"text": "A Phonotactic Language Model for Spoken Language Identification", "labels": [], "entities": [{"text": "Spoken Language Identification", "start_pos": 33, "end_pos": 63, "type": "TASK", "confidence": 0.8851964076360067}]}], "abstractContent": [{"text": "We have established a phonotactic language model as the solution to spoken language identification (LID).", "labels": [], "entities": [{"text": "spoken language identification (LID)", "start_pos": 68, "end_pos": 104, "type": "TASK", "confidence": 0.8205054899056753}]}, {"text": "In this framework, we define a single set of acoustic tokens to represent the acoustic activities in the world's spoken languages.", "labels": [], "entities": []}, {"text": "A voice tokenizer converts a spoken document into a text-like document of acoustic tokens.", "labels": [], "entities": []}, {"text": "Thus a spoken document can be represented by a count vector of acoustic tokens and token n-grams in the vector space.", "labels": [], "entities": []}, {"text": "We apply latent semantic analysis to the vectors, in the same way that it is applied in information retrieval, in order to capture salient phonotactics present in spoken documents.", "labels": [], "entities": []}, {"text": "The vector space modeling of spoken utterances constitutes a paradigm shift in LID technology and has proven to be very successful.", "labels": [], "entities": [{"text": "vector space modeling of spoken utterances", "start_pos": 4, "end_pos": 46, "type": "TASK", "confidence": 0.7453805804252625}]}, {"text": "It presents a 12.4% error rate reduction over one of the best reported results on the 1996 NIST Language Recognition Evaluation database.", "labels": [], "entities": [{"text": "error rate reduction", "start_pos": 20, "end_pos": 40, "type": "METRIC", "confidence": 0.9744040171305338}, {"text": "NIST Language Recognition Evaluation", "start_pos": 91, "end_pos": 127, "type": "TASK", "confidence": 0.7438696920871735}]}], "introductionContent": [{"text": "Spoken language and written language are similar in many ways.", "labels": [], "entities": []}, {"text": "Therefore, much of the research in spoken language identification, LID, has been inspired by text-categorization methodology.", "labels": [], "entities": [{"text": "spoken language identification", "start_pos": 35, "end_pos": 65, "type": "TASK", "confidence": 0.6893841624259949}]}, {"text": "Both text and voice are generated from language dependent vocabulary.", "labels": [], "entities": []}, {"text": "For example, both can be seen as stochastic time-sequences corrupted by a channel noise.", "labels": [], "entities": []}, {"text": "The n-gram language model has achieved equal amounts of success in both tasks, e.g. n-character slice for text categorization by language) and Phone Recognition followed by n-gram Language Modeling, or PRLM . Orthographic forms of language, ranging from Latin alphabet to Cyrillic script to Chinese characters, are far more unique to the language than their phonetic counterparts.", "labels": [], "entities": [{"text": "Phone Recognition", "start_pos": 143, "end_pos": 160, "type": "TASK", "confidence": 0.6957785189151764}]}, {"text": "From the speech production point of view, thousands of spoken languages from allover the world are phonetically articulated using only a few hundred distinctive sounds or phonemes.", "labels": [], "entities": []}, {"text": "In other words, common sounds are shared considerably across different spoken languages.", "labels": [], "entities": []}, {"text": "In addition, spoken documents 1 , in the form of digitized wave files, are far less structured than written documents and need to be treated with techniques that go beyond the bounds of written language.", "labels": [], "entities": []}, {"text": "All of this makes the identification of spoken language based on phonetic units much more challenging than the identification of written language.", "labels": [], "entities": [{"text": "identification of spoken language based on phonetic units", "start_pos": 22, "end_pos": 79, "type": "TASK", "confidence": 0.793781079351902}, {"text": "identification of written language", "start_pos": 111, "end_pos": 145, "type": "TASK", "confidence": 0.8361336439847946}]}, {"text": "In fact, the challenge of LID is inter-disciplinary, involving digital signal processing, speech recognition and natural language processing.", "labels": [], "entities": [{"text": "LID", "start_pos": 26, "end_pos": 29, "type": "TASK", "confidence": 0.9600231051445007}, {"text": "speech recognition", "start_pos": 90, "end_pos": 108, "type": "TASK", "confidence": 0.7542890310287476}]}, {"text": "In general, a LID system usually has three fundamental components as follows: 1) A voice tokenizer which segments incoming voice feature frames and associates the segments with acoustic or phonetic labels, called tokens; 2) A statistical language model which captures language dependent phonetic and phonotactic information from the sequences of tokens; 3) A language classifier which identifies the language based on discriminatory characteristics of acoustic score from the voice tokenizer and phonotactic score from the language model.", "labels": [], "entities": []}, {"text": "In this paper, we present a novel solution to the three problems, focusing on the second and third problems from a computational linguistic perspective.", "labels": [], "entities": []}, {"text": "The paper is organized as follows: In Section 2, we summarize relevant existing approaches to the LID task.", "labels": [], "entities": [{"text": "LID task", "start_pos": 98, "end_pos": 106, "type": "TASK", "confidence": 0.9206327199935913}]}, {"text": "We highlight the shortcomings of existing approaches and our attempts to address the issues.", "labels": [], "entities": []}, {"text": "In Section 3 we propose the bag-of-sounds paradigm to turn the LID task into atypical text categorization problem.", "labels": [], "entities": []}, {"text": "In Section 4, we study the effects of different settings in experiments on the 1996 NIST Language Recognition Evaluation (LRE) database 2 . In Section 5, we conclude our study and discuss future work.", "labels": [], "entities": [{"text": "1996 NIST Language Recognition Evaluation (LRE) database", "start_pos": 79, "end_pos": 135, "type": "DATASET", "confidence": 0.7053938210010529}]}], "datasetContent": [{"text": "This section will experimentally analyze the performance of the proposed bag-of-sounds framework using the 1996 NIST Language Recognition Evaluation (LRE) data.", "labels": [], "entities": [{"text": "NIST Language Recognition Evaluation (LRE) data", "start_pos": 112, "end_pos": 159, "type": "DATASET", "confidence": 0.6735261306166649}]}, {"text": "The database was intended to establish a baseline of performance capability for language recognition of conversational telephone speech.", "labels": [], "entities": [{"text": "language recognition of conversational telephone speech", "start_pos": 80, "end_pos": 135, "type": "TASK", "confidence": 0.8348972151676813}]}, {"text": "The database contains recorded speech of 12 languages: Arabic, English, Farsi, French, German, Hindi, Japanese, Korean, Mandarin, Spanish, Tamil and Vietnamese.", "labels": [], "entities": []}, {"text": "We use the training set and development set from LDC CallFriend corpus 3 as the training data.", "labels": [], "entities": [{"text": "LDC CallFriend corpus 3", "start_pos": 49, "end_pos": 72, "type": "DATASET", "confidence": 0.9356880486011505}]}, {"text": "Each conversation is segmented into overlapping sessions of about 30 seconds each, resulting in about 12,000 sessions for each language.", "labels": [], "entities": []}, {"text": "The evaluation set consists of 1,492 30-sec sessions, each distributed among the various languages of interest.", "labels": [], "entities": []}, {"text": "We treat a 30-sec session as a spoken document in both training and testing.", "labels": [], "entities": []}, {"text": "We report error rates (ER) of the 1,492 test trials.", "labels": [], "entities": [{"text": "error rates (ER)", "start_pos": 10, "end_pos": 26, "type": "METRIC", "confidence": 0.9606008529663086}]}], "tableCaptions": [{"text": " Table 1. Effect of acoustic vocabulary (KNC)", "labels": [], "entities": []}, {"text": " Table 2. Effect of number of centroids (KNC)", "labels": [], "entities": []}, {"text": " Table 3. Effect of number of mixtures (MMC)", "labels": [], "entities": []}]}