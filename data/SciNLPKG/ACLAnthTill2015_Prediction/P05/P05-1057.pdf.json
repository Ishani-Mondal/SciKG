{"title": [], "abstractContent": [{"text": "We present a framework for word alignment based on log-linear models.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 27, "end_pos": 41, "type": "TASK", "confidence": 0.8239738941192627}]}, {"text": "All knowledge sources are treated as feature functions, which depend on the source langauge sentence, the target language sentence and possible additional variables.", "labels": [], "entities": []}, {"text": "Log-linear models allow statistical alignment models to be easily extended by incorporating syntactic information.", "labels": [], "entities": [{"text": "statistical alignment", "start_pos": 24, "end_pos": 45, "type": "TASK", "confidence": 0.742127537727356}]}, {"text": "In this paper, we use IBM Model 3 alignment probabilities, POS correspondence , and bilingual dictionary coverage as features.", "labels": [], "entities": []}, {"text": "Our experiments show that log-linear models significantly out-perform IBM translation models.", "labels": [], "entities": []}], "introductionContent": [{"text": "Word alignment, which can be defined as an object for indicating the corresponding words in a parallel text, was first introduced as an intermediate result of statistical translation models (.", "labels": [], "entities": [{"text": "Word alignment", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7222802340984344}]}, {"text": "In statistical machine translation, word alignment plays a crucial role as word-aligned corpora have been found to bean excellent source of translation-related knowledge.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 3, "end_pos": 34, "type": "TASK", "confidence": 0.6681014994780222}, {"text": "word alignment", "start_pos": 36, "end_pos": 50, "type": "TASK", "confidence": 0.7945041060447693}]}, {"text": "Various methods have been proposed for finding word alignments between parallel texts.", "labels": [], "entities": [{"text": "finding word alignments between parallel texts", "start_pos": 39, "end_pos": 85, "type": "TASK", "confidence": 0.7806285818417867}]}, {"text": "There are generally two categories of alignment approaches: statistical approaches and heuristic approaches.", "labels": [], "entities": []}, {"text": "Statistical approaches, which depend on a set of unknown parameters that are learned from training data, try to describe the relationship between a bilingual sentence pair (.", "labels": [], "entities": []}, {"text": "Heuristic approaches obtain word alignments by using various similarity functions between the types of the two languages ().", "labels": [], "entities": [{"text": "word alignments", "start_pos": 28, "end_pos": 43, "type": "TASK", "confidence": 0.7486840188503265}]}, {"text": "The central distinction between statistical and heuristic approaches is that statistical approaches are based on well-founded probabilistic models while heuristic ones are not.", "labels": [], "entities": []}, {"text": "Studies reveal that statistical alignment models outperform the simple Dice coefficient.", "labels": [], "entities": [{"text": "statistical alignment", "start_pos": 20, "end_pos": 41, "type": "TASK", "confidence": 0.665797159075737}]}, {"text": "Finding word alignments between parallel texts, however, is still far from a trivial work due to the diversity of natural languages.", "labels": [], "entities": [{"text": "word alignments between parallel texts", "start_pos": 8, "end_pos": 46, "type": "TASK", "confidence": 0.836990213394165}]}, {"text": "For example, the alignment of words within idiomatic expressions, free translations, and missing content or function words is problematic.", "labels": [], "entities": [{"text": "alignment of words within idiomatic expressions", "start_pos": 17, "end_pos": 64, "type": "TASK", "confidence": 0.8378199736277262}]}, {"text": "When two languages widely differ in word order, finding word alignments is especially hard.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 56, "end_pos": 71, "type": "TASK", "confidence": 0.6869453340768814}]}, {"text": "Therefore, it is necessary to incorporate all useful linguistic information to alleviate these problems.", "labels": [], "entities": []}, {"text": "introduced a word alignment approach based on combination of association clues.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 13, "end_pos": 27, "type": "TASK", "confidence": 0.7947889864444733}]}, {"text": "Clues combination is done by disjunction of single clues, which are defined as probabilities of associations.", "labels": [], "entities": [{"text": "Clues combination", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7193359732627869}]}, {"text": "The crucial assumption of clue combination that clues are independent of each other, however, is not always true.", "labels": [], "entities": [{"text": "clue combination", "start_pos": 26, "end_pos": 42, "type": "TASK", "confidence": 0.6970631629228592}]}, {"text": "proposed Model 6, a log-linear combination of IBM translation models and HMM model.", "labels": [], "entities": []}, {"text": "Although Model 6 yields better results than naive IBM models, it fails to include dependencies other than IBM models and HMM model.", "labels": [], "entities": []}, {"text": "developed a statistical model to find word alignments, which allow easy integration of context-specific features.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 38, "end_pos": 53, "type": "TASK", "confidence": 0.7191455364227295}]}, {"text": "Log-linear models, which are very suitable to incorporate additional dependencies, have been successfully applied to statistical machine translation.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 117, "end_pos": 148, "type": "TASK", "confidence": 0.7265945871671041}]}, {"text": "In this paper, we present a framework for word alignment based on log-linear models, allowing statistical models to be easily extended by incorporating additional syntactic dependencies.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 42, "end_pos": 56, "type": "TASK", "confidence": 0.8088590800762177}]}, {"text": "We use IBM Model 3 alignment probabilities, POS correspondence, and bilingual dictionary coverage as features.", "labels": [], "entities": []}, {"text": "Our experiments show that log-linear models significantly outperform IBM translation models.", "labels": [], "entities": []}, {"text": "We begin by describing log-linear models for word alignment.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 45, "end_pos": 59, "type": "TASK", "confidence": 0.7796485126018524}]}, {"text": "The design of feature functions is discussed then.", "labels": [], "entities": []}, {"text": "Next, we present the training method and the search algorithm for log-linear models.", "labels": [], "entities": []}, {"text": "We will follow with our experimental results and conclusion and close with a discussion of possible future directions.", "labels": [], "entities": []}], "datasetContent": [{"text": "We present in this section results of experiments on a parallel corpus of Chinese-English texts.", "labels": [], "entities": []}, {"text": "Statistics for the corpus are shown in.", "labels": [], "entities": []}, {"text": "We use a training corpus, which is used to train IBM translation models, a bilingual dictionary, a development corpus, and a test corpus.", "labels": [], "entities": []}, {"text": "The Chinese sentences in both the development and test corpus are segmented and POS tagged by ICTCLAS.", "labels": [], "entities": [{"text": "ICTCLAS", "start_pos": 94, "end_pos": 101, "type": "DATASET", "confidence": 0.8919615149497986}]}, {"text": "The English sentences are tokenized by a simple tokenizer of ours and POS tagged by a rule-based tagger written by Eric Brill.", "labels": [], "entities": []}, {"text": "We manually aligned 935 sentences, in which we selected 500 sentences as test corpus.", "labels": [], "entities": []}, {"text": "The remaining 435 sentences are used as development corpus to train POS tags transition probabilities and to optimize the model parameters and gain threshold.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. We use a  training corpus, which is used to train IBM transla- tion models, a bilingual dictionary, a development  corpus, and a test corpus.", "labels": [], "entities": []}, {"text": " Table 1. Statistics of training corpus (Train), bilin- gual dictionary (Dict), development corpus (Dev),  and test corpus (Test).", "labels": [], "entities": []}, {"text": " Table 2. Comparison of AER for results of using IBM Model 3 (GIZA++) and log-linear models.", "labels": [], "entities": [{"text": "AER", "start_pos": 24, "end_pos": 27, "type": "METRIC", "confidence": 0.9940314888954163}, {"text": "IBM Model 3 (GIZA++)", "start_pos": 49, "end_pos": 69, "type": "DATASET", "confidence": 0.845757782459259}]}, {"text": " Table 3. Comparison of AER for results of using IBM Model 5 (GIZA++) and log-linear models.", "labels": [], "entities": [{"text": "AER", "start_pos": 24, "end_pos": 27, "type": "METRIC", "confidence": 0.9938754439353943}, {"text": "IBM Model 5 (GIZA++)", "start_pos": 49, "end_pos": 69, "type": "DATASET", "confidence": 0.8676553269227346}]}, {"text": " Table 4. Resulting model scaling factors: \u03bb 1 : Model  3 E \u2192 C (MEC); \u03bb 2 : Model 3 C \u2192 E (MCE); \u03bb 3 :  POS E \u2192 C (PEC); \u03bb 4 : POS C \u2192 E (PCE); \u03bb 5 : Dict  (normalized such that", "labels": [], "entities": [{"text": "Dict", "start_pos": 151, "end_pos": 155, "type": "METRIC", "confidence": 0.9804540276527405}]}]}