{"title": [{"text": "Extracting Semantic Orientations of Words using Spin Model", "labels": [], "entities": [{"text": "Extracting Semantic Orientations of Words", "start_pos": 0, "end_pos": 41, "type": "TASK", "confidence": 0.8971979141235351}]}], "abstractContent": [{"text": "We propose a method for extracting semantic orientations of words: desirable or undesirable.", "labels": [], "entities": [{"text": "extracting semantic orientations of words", "start_pos": 24, "end_pos": 65, "type": "TASK", "confidence": 0.8839422821998596}]}, {"text": "Regarding semantic ori-entations as spins of electrons, we use the mean field approximation to compute the approximate probability function of the system instead of the intractable actual probability function.", "labels": [], "entities": []}, {"text": "We also propose a criterion for parameter selection on the basis of magnetization.", "labels": [], "entities": []}, {"text": "Given only a small number of seed words, the proposed method extracts semantic orienta-tions with high accuracy in the experiments on English lexicon.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 103, "end_pos": 111, "type": "METRIC", "confidence": 0.9970934391021729}]}, {"text": "The result is comparable to the best value ever reported .", "labels": [], "entities": []}], "introductionContent": [{"text": "Identification of emotions (including opinions and attitudes) in text is an important task which has a variety of possible applications.", "labels": [], "entities": [{"text": "Identification of emotions (including opinions and attitudes) in text", "start_pos": 0, "end_pos": 69, "type": "TASK", "confidence": 0.8934092738411643}]}, {"text": "For example, we can efficiently collect opinions on anew product from the internet, if opinions in bulletin boards are automatically identified.", "labels": [], "entities": []}, {"text": "We will also be able to grasp people's attitudes in questionnaire, without actually reading all the responds.", "labels": [], "entities": []}, {"text": "An important resource in realizing such identification tasks is a list of words with semantic orientation: positive or negative (desirable or undesirable).", "labels": [], "entities": []}, {"text": "Frequent appearance of positive words in a document implies that the writer of the document would have a positive attitude on the topic.", "labels": [], "entities": []}, {"text": "The goal of this paper is to propose a method for automatically creating such a word list from glosses (i.e., definition or explanation sentences ) in a dictionary, as well as from a thesaurus and a corpus.", "labels": [], "entities": []}, {"text": "For this purpose, we use spin model, which is a model fora set of electrons with spins.", "labels": [], "entities": []}, {"text": "Just as each electron has a direction of spin (up or down), each word has a semantic orientation (positive or negative).", "labels": [], "entities": []}, {"text": "We therefore regard words as a set of electrons and apply the mean field approximation to compute the average orientation of each word.", "labels": [], "entities": []}, {"text": "We also propose a criterion for parameter selection on the basis of magnetization, a notion in statistical physics.", "labels": [], "entities": [{"text": "parameter selection", "start_pos": 32, "end_pos": 51, "type": "TASK", "confidence": 0.740671843290329}]}, {"text": "Magnetization indicates the global tendency of polarization.", "labels": [], "entities": []}, {"text": "We empirically show that the proposed method works well even with a small number of seed words.", "labels": [], "entities": []}, {"text": "proposed two algorithms for extraction of semantic orientations of words.", "labels": [], "entities": [{"text": "extraction of semantic orientations of words", "start_pos": 28, "end_pos": 72, "type": "TASK", "confidence": 0.871017595132192}]}, {"text": "To calculate the association strength of a word with positive (negative) seed words, they used the number of hits returned by a search engine, with a query consisting of the word and one of seed words (e.g., \"word NEAR good\", \"word NEAR bad\").", "labels": [], "entities": []}, {"text": "They regarded the difference of two association strengths as a measure of semantic orientation.", "labels": [], "entities": [{"text": "semantic orientation", "start_pos": 74, "end_pos": 94, "type": "TASK", "confidence": 0.7520137429237366}]}, {"text": "They also proposed to use Latent Semantic Analysis to compute the association strength with seed words.", "labels": [], "entities": []}, {"text": "An empirical evaluation was conducted on 3596 words extracted from General Inquirer (.", "labels": [], "entities": [{"text": "General Inquirer (.", "start_pos": 67, "end_pos": 86, "type": "DATASET", "confidence": 0.9557682871818542}]}, {"text": "focused on conjunctive expressions such as \"simple and well-received\" and \"simplistic but well-received\", where the former pair of words tend to have the same semantic orientation, and the latter tend to have the opposite orientation.", "labels": [], "entities": []}, {"text": "They first classify each conjunctive expression into the same-orientation class or the different-orientation class.", "labels": [], "entities": []}, {"text": "They then use the classified expressions to cluster words into the positive class and the negative class.", "labels": [], "entities": []}, {"text": "The experiments were conducted with the dataset that they created on their own.", "labels": [], "entities": []}, {"text": "Evaluation was limited to adjectives.", "labels": [], "entities": []}, {"text": "proposed a method for extracting semantic orientations of words with bootstrapping.", "labels": [], "entities": [{"text": "extracting semantic orientations of words", "start_pos": 22, "end_pos": 63, "type": "TASK", "confidence": 0.8720200657844543}]}, {"text": "The semantic orientation of a word is determined on the basis of its gloss, if any of their 52 hand-crafted rules is applicable to the sentence.", "labels": [], "entities": []}, {"text": "Rules are applied iteratively in the bootstrapping framework.", "labels": [], "entities": []}, {"text": "Althoughs work provided an accurate investigation on this task and inspired our work, it has drawbacks: low recall and language dependency.", "labels": [], "entities": [{"text": "recall", "start_pos": 108, "end_pos": 114, "type": "METRIC", "confidence": 0.998857855796814}]}, {"text": "They reported that the semantic orientations of only 113 words are extracted with precision 84.1% (the low recall is due partly to their large set of seed words (1187 words)).", "labels": [], "entities": [{"text": "precision", "start_pos": 82, "end_pos": 91, "type": "METRIC", "confidence": 0.9981542229652405}, {"text": "recall", "start_pos": 107, "end_pos": 113, "type": "METRIC", "confidence": 0.9989064931869507}]}, {"text": "The handcrafted rules are only for Japanese.", "labels": [], "entities": []}, {"text": "constructed a network by connecting each pair of synonymous words provided by WordNet, and then used the shortest paths to two seed words \"good\" and \"bad\" to obtain the semantic orientation of a word.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 78, "end_pos": 85, "type": "DATASET", "confidence": 0.9585662484169006}]}, {"text": "Limitations of their method are that a synonymy dictionary is required, that antonym relations cannot be incorporated into the model.", "labels": [], "entities": []}, {"text": "Their evaluation is restricted to adjectives.", "labels": [], "entities": []}, {"text": "The method proposed by is quite similar to the shortest-path method.", "labels": [], "entities": []}, {"text": "Hu and Liu's method iteratively determines the semantic orientations of the words neighboring any of the seed words and enlarges the seed word set in a bootstrapping manner.", "labels": [], "entities": []}], "datasetContent": [{"text": "We used glosses, synonyms, antonyms and hypernyms of WordNet to construct an English lexical network.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 53, "end_pos": 60, "type": "DATASET", "confidence": 0.9527721405029297}]}, {"text": "For part-of-speech tagging and lemmatization of glosses, we used TreeTagger (.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 4, "end_pos": 26, "type": "TASK", "confidence": 0.7702328860759735}]}, {"text": "35 stopwords (quite frequent words such as \"be\" and \"have\") are removed from the lexical network.", "labels": [], "entities": []}, {"text": "Negation words include 33 words.", "labels": [], "entities": []}, {"text": "In addition to usual negation words such as \"not\" and \"never\", we include words and phrases which mean negation in a general sense, such as \"free from\" and \"lack of\".", "labels": [], "entities": []}, {"text": "The whole network consists of approximately 88,000 words.", "labels": [], "entities": []}, {"text": "We collected 804 conjunctive expressions from Wall Street Journal and Brown corpus as described in Section 4.2.", "labels": [], "entities": [{"text": "Wall Street Journal", "start_pos": 46, "end_pos": 65, "type": "DATASET", "confidence": 0.9763630827267965}, {"text": "Brown corpus", "start_pos": 70, "end_pos": 82, "type": "DATASET", "confidence": 0.908575028181076}]}, {"text": "The labeled dataset used as a gold standard is General Inquirer lexicon () as in the work by.", "labels": [], "entities": [{"text": "General Inquirer lexicon", "start_pos": 47, "end_pos": 71, "type": "DATASET", "confidence": 0.7670631408691406}]}, {"text": "We extracted the words tagged with \"Positiv\" or \"Negativ\", and reduced multiple-entry words to single entries.", "labels": [], "entities": []}, {"text": "As a result, we obtained 3596 words (1616 positive words and 1980 negative words) . In the computation of: Classification accuracy (%) with various networks and four different sets of seed words.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 122, "end_pos": 130, "type": "METRIC", "confidence": 0.8680151104927063}]}, {"text": "In the parentheses, the predicted value of \u03b2 is written.", "labels": [], "entities": []}, {"text": "accuracy, seed words are eliminated from these 3596 words.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9906429648399353}]}, {"text": "We conducted experiments with different values of \u03b2 from 0.1 to 2.0, with the interval 0.1, and predicted the best value as explained in Section 4.3.", "labels": [], "entities": []}, {"text": "The threshold of the magnetization for hyper-parameter estimation is set to 1.0 \u00d7 10 \u22125 . That is, the predicted optimal value of \u03b2 is the largest \u03b2 whose corresponding magnetization does not exceeds the threshold value.", "labels": [], "entities": []}, {"text": "We performed 10-fold cross validation as well as experiments with fixed seed words.", "labels": [], "entities": [{"text": "cross validation", "start_pos": 21, "end_pos": 37, "type": "TASK", "confidence": 0.5495869368314743}]}, {"text": "The fixed seed words are the ones used by Turney and Littman: 14 seed words {good, nice, excellent, positive, fortunate, correct, superior, bad, nasty, poor, negative, unfortunate, wrong, inferior}; 4 seed words {good, superior, bad, inferior}; 2 seed words {good, bad}.", "labels": [], "entities": []}, {"text": "shows the accuracy values of semantic orientation classification for four different sets of seed words and various networks.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9995260238647461}, {"text": "semantic orientation classification", "start_pos": 29, "end_pos": 64, "type": "TASK", "confidence": 0.8170957167943319}]}, {"text": "In the table, cv corresponds to the result of 10-fold cross validation, in which case we use the pseudo leave-one-out error for hyper-parameter estimation, while in other cases we use magnetization.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Classification accuracy (%) with various  networks and four different sets of seed words. In  the parentheses, the predicted value of \u03b2 is written.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.868933379650116}]}, {"text": " Table 2: Actual best classification accuracy (%)  with various networks and four different sets of seed  words. In the parenthesis, the actual best value of \u03b2  is written, except for cv.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.7850042581558228}]}, {"text": " Table 3: Precision (%) for selected adjectives.  Comparison between the proposed method and the  shortest-path method.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9977656602859497}]}, {"text": " Table 4: Precision (%) for adjectives. Comparison  between the proposed method and the bootstrapping  method.  seeds proposed bootstrap  14  83.6 (0.8)  72.8  4  82.3 (0.9)  73.2  2  83.5 (0.7)  71.1", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9939839243888855}]}]}