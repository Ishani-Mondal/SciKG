{"title": [{"text": "Towards Finding and Fixing Fragments: Using ML to Identify Non-Sentential Utterances and their Antecedents in Multi-Party Dialogue", "labels": [], "entities": [{"text": "Identify Non-Sentential Utterances and their Antecedents", "start_pos": 50, "end_pos": 106, "type": "TASK", "confidence": 0.6313444922367731}]}], "abstractContent": [{"text": "Non-sentential utterances (e.g., short-answers as in \"Who came to the party?\"-\"Peter.\") are pervasive in dialogue.", "labels": [], "entities": []}, {"text": "As with other forms of ellipsis, the elided material is typically present in the context (e.g., the question that a short answer answers).", "labels": [], "entities": []}, {"text": "We present a machine learning approach to the novel task of identifying fragments and their antecedents in multi-party dialogue.", "labels": [], "entities": []}, {"text": "We compare the performance of several learning algorithms, using a mixture of structural and lexical features , and show that the task of identifying antecedents given a fragment can be learnt successfully (f (0.5) = .76); we discuss why the task of identifying fragments is harder (f (0.5) = .41) and finally report on a combined task (f (0.5) = .38).", "labels": [], "entities": []}], "introductionContent": [{"text": "Non-sentential utterances (NSUs) as in (1) are pervasive in dialogue: recent studies put the proportion of such utterances at around 10% across different types of dialogue (;.", "labels": [], "entities": [{"text": "Non-sentential utterances (NSUs)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.631554126739502}]}, {"text": "Such utterances pose an obvious problem for natural language processing applications, namely that the intended information (in (1-a)-B a proposition) has to be recovered from the uttered information (here, an NP meaning) with the help of information from the context.", "labels": [], "entities": []}, {"text": "While some systems that automatically resolve such fragments have recently been developed (, they have the drawback that they require \"deep\" linguistic processing (full parses, and also information about discourse structure) and hence are not very robust.", "labels": [], "entities": []}, {"text": "We have defined a well-defined subtask of this problem, namely identifying fragments (certain kinds of NSUs, see below) and their antecedents (in multi-party dialogue, in our case), and present a novel machine learning approach to it, which we hypothesise will be useful for tasks such as automatic meeting summarisation.", "labels": [], "entities": [{"text": "automatic meeting summarisation", "start_pos": 289, "end_pos": 320, "type": "TASK", "confidence": 0.6769992411136627}]}, {"text": "The remainder of this paper is structured as follows.", "labels": [], "entities": []}, {"text": "In the next section we further specify the task and different possible approaches to it.", "labels": [], "entities": []}, {"text": "We then describe the corpus we used, some of its characteristics with respect to fragments, and the features we extracted from it for machine learning.", "labels": [], "entities": []}, {"text": "Section 4 describes our experimental settings and reports the results.", "labels": [], "entities": []}, {"text": "After a comparison to related work in Section 5, we close with a conclusion and some further work that is planned.", "labels": [], "entities": []}], "datasetContent": [{"text": "For the learning experiments, we used three classifiers on all data-sets for the the three tasks: \u2022 SLIPPER (Simple Learner with Iterative Pruning to Produce Error Reduction),), which is a rule learner which combines the separate-and-conquer approach with confidencerated boosting.", "labels": [], "entities": [{"text": "SLIPPER", "start_pos": 100, "end_pos": 107, "type": "METRIC", "confidence": 0.9128592014312744}]}, {"text": "It is unique among the classifiers that we have used in that it can make use of \"set-valued\" features, e.g. strings; we have run this learner both with only the features listed above and with the utterances (and POS-tags) as an additional feature.", "labels": [], "entities": []}, {"text": "\u2022 TIMBL (Tilburg Memory-Based Learner), (, which implements a memory-based learning algorithm (IB1) which predicts the class of a test data point by looking at its distance to all examples from the training data, using some distance metric.", "labels": [], "entities": [{"text": "TIMBL", "start_pos": 2, "end_pos": 7, "type": "METRIC", "confidence": 0.8136385679244995}]}, {"text": "In our experiments, we have used the weighted-overlap method, which assigns weights to all features.", "labels": [], "entities": []}, {"text": "\u2022 MAXENT, Zhang Le's C++ implementation 8 of maximum entropy modelling (.", "labels": [], "entities": [{"text": "MAXENT", "start_pos": 2, "end_pos": 8, "type": "METRIC", "confidence": 0.8001064658164978}]}, {"text": "In our experiments, we used L-BFGS parameter estimation.", "labels": [], "entities": [{"text": "L-BFGS parameter estimation", "start_pos": 28, "end_pos": 55, "type": "METRIC", "confidence": 0.863440215587616}]}, {"text": "We also implemented a na\u00a8\u0131vena\u00a8\u0131ve bayes classifier and ran it on the fragment-task, with a data-set consisting only of the strings and POS-tags.", "labels": [], "entities": []}, {"text": "To determine the contribution of all features, we used an iterative process similar to the one described in: we start with training a model using a baseline set of features, and then add each remaining feature individually, recording the gain (w.r.t. the fmeasure (f (0.5), to be precise)), and choosing the best-performing feature, incrementally until no further gain is recorded.", "labels": [], "entities": []}, {"text": "All individual training-and evaluation-steps are performed using 8-fold crossvalidation (given the small number of positive instances, more folds would have made the number of instances in the test set set too small).", "labels": [], "entities": []}, {"text": "The baselines were as follows: for the fragmenttask, we used bvb and lbe as baseline, i.e. we let the classifier know the length of the candidate and whether the candidate contains a verb or not.", "labels": [], "entities": []}, {"text": "For the antecedent-task we tested a very simple baseline, containing only of one feature, the distance between \u03b1 and \u03b2 (dis).", "labels": [], "entities": []}, {"text": "The baseline for the combinedtask, finally, was a combination of those two baselines, i.e. bvb+lbe+dis.", "labels": [], "entities": []}, {"text": "The full feature-set for the fragment-task was lbe, bvb, bpr, nrb, bft, bds (since for this task there was no \u03b1 to compute features of), for the two other tasks it was the complete set shown in.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Results for the fragment task. (Cl. = classifier used, where s = slipper, s.s = slipper + set-valued  features, t = timbl, m = maxent, b = naive bayes; UB/B = (un)balanced training data.)", "labels": [], "entities": [{"text": "UB/B", "start_pos": 162, "end_pos": 166, "type": "METRIC", "confidence": 0.598168283700943}]}, {"text": " Table 4: Results for the antecedent task.", "labels": [], "entities": []}, {"text": " Table 5: Results for the combined task.", "labels": [], "entities": []}]}