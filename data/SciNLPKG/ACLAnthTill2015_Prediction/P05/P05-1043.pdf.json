{"title": [{"text": "Learning Stochastic OT Grammars: A Bayesian approach using Data Augmentation and Gibbs Sampling", "labels": [], "entities": [{"text": "OT Grammars", "start_pos": 20, "end_pos": 31, "type": "TASK", "confidence": 0.7488068342208862}]}], "abstractContent": [{"text": "Stochastic Optimality Theory (Boersma, 1997) is a widely-used model in linguistics that did not have a theoretically sound learning method previously.", "labels": [], "entities": [{"text": "Stochastic Optimality Theory (Boersma, 1997)", "start_pos": 0, "end_pos": 44, "type": "TASK", "confidence": 0.8573445379734039}]}, {"text": "In this paper , a Markov chain Monte-Carlo method is proposed for learning Stochastic OT Grammars.", "labels": [], "entities": []}, {"text": "Following a Bayesian framework , the goal is finding the posterior distribution of the grammar given the relative frequencies of input-output pairs.", "labels": [], "entities": []}, {"text": "The Data Augmentation algorithm allows one to simulate a joint posterior distribution by iterating two conditional sampling steps.", "labels": [], "entities": [{"text": "Data Augmentation", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.657677486538887}]}, {"text": "This Gibbs sampler constructs a Markov chain that converges to the joint distribution , and the target posterior can be derived as its marginal distribution.", "labels": [], "entities": []}], "introductionContent": [{"text": "Optimality Theory () is a linguistic theory that dominates the field of phonology, and some areas of morphology and syntax.", "labels": [], "entities": [{"text": "Optimality Theory", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8299940526485443}]}, {"text": "The standard version of OT contains the following assumptions: \u2022 A grammar is a set of ordered constraints ({C i : i = 1, \u00b7 \u00b7 \u00b7 , N }, >); \u2022 Each constraint Ci is a function: \u03a3 * \u2192 {0, 1, \u00b7 \u00b7 \u00b7 }, where \u03a3 * is the set of strings in the language; \u2022 Each underlying form u corresponds to a set of candidates GEN (u).", "labels": [], "entities": [{"text": "OT", "start_pos": 24, "end_pos": 26, "type": "TASK", "confidence": 0.8931189179420471}]}, {"text": "To obtain the unique surface form, the candidate set is successively filtered according to the order of constraints, so that only the most harmonic candidates remain after each filtering.", "labels": [], "entities": []}, {"text": "If only 1 candidate is left in the candidate set, it is chosen as the optimal output.", "labels": [], "entities": []}, {"text": "The popularity of OT is partly due to learning algorithms that induce constraint ranking from data.", "labels": [], "entities": [{"text": "OT", "start_pos": 18, "end_pos": 20, "type": "TASK", "confidence": 0.9307528138160706}]}, {"text": "However, most of such algorithms cannot be applied to noisy learning data.", "labels": [], "entities": []}, {"text": "Stochastic Optimality Theory) is a variant of Optimality Theory that tries to quantitatively predict linguistic variation.", "labels": [], "entities": [{"text": "Stochastic Optimality Theory", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.7066834767659506}]}, {"text": "As a popular model among linguists that are more engaged with empirical data than with formalisms, Stochastic OT has been used in a large body of linguistics literature.", "labels": [], "entities": [{"text": "Stochastic OT", "start_pos": 99, "end_pos": 112, "type": "TASK", "confidence": 0.6843985021114349}]}, {"text": "In Stochastic OT, constraints are regarded as independent normal distributions with unknown means and fixed variance.", "labels": [], "entities": [{"text": "Stochastic OT", "start_pos": 3, "end_pos": 16, "type": "TASK", "confidence": 0.6791399717330933}]}, {"text": "As a result, the stochastic constraint hierarchy generates systematic linguistic variation.", "labels": [], "entities": []}, {"text": "For example, consider a grammar with 3 constraints, C 1 \u223c N (\u00b5 1 , \u03c3 2 ), C 2 \u223c N (\u00b5 2 , \u03c3 2 ), C 3 \u223c N (\u00b5 3 , \u03c3 2 ), and 2 competing candidates fora given input x: p(.)", "labels": [], "entities": []}, {"text": "C 1 C 2 C 3 x \u223c y 1 .77 0 0 1 x \u223c y 2 .23 1 1 0 The probabilities p(.) are obtained by repeatedly sampling the 3 normal distributions, generating the winning candidate according to the ordering of constraints, and counting the relative frequencies in the outcome.", "labels": [], "entities": []}, {"text": "As a result, the grammar will assign nonzero probabilities to a given set of outputs, as shown above.", "labels": [], "entities": []}, {"text": "The learning problem of Stochastic OT involves fitting a grammar G \u2208 RN to a set of candidates with frequency counts in a corpus.", "labels": [], "entities": [{"text": "Stochastic OT", "start_pos": 24, "end_pos": 37, "type": "TASK", "confidence": 0.7327153980731964}]}, {"text": "For example, if the learning data is the above table, we need to find an estimate of G = (\u00b5 1 , \u00b5 2 , \u00b5 3 ) 1 so that the following ordering relations hold with certain probabilities: with probability .77 max{C 1 , C 2 } < C 3 ; with probability .23 (1) The current method for fitting Stochastic OT models, used by many linguists, is the Gradual Learning Algorithm (GLA)).", "labels": [], "entities": []}, {"text": "GLA looks for the correct ranking values by using the following heuristic, which resembles gradient descent.", "labels": [], "entities": []}, {"text": "First, an input-output pair is sampled from the data; second, an ordering of the constraints is sampled from the grammar and used to generate an output; and finally, the means of the constraints are updated so as to minimize the error.", "labels": [], "entities": []}, {"text": "The updating is done by adding or subtracting a \"plasticity\" value that goes to zero overtime.", "labels": [], "entities": []}, {"text": "The intuition behind GLA is that it does \"frequency matching\", i.e. looking fora better match between the output frequencies of the grammar and those in the data.", "labels": [], "entities": [{"text": "GLA", "start_pos": 21, "end_pos": 24, "type": "TASK", "confidence": 0.8053078651428223}]}, {"text": "As it turns out, GLA does notwork in all cases 2 , and its lack of formal foundations has been questioned by a number of researchers (.", "labels": [], "entities": [{"text": "GLA", "start_pos": 17, "end_pos": 20, "type": "TASK", "confidence": 0.8678624033927917}]}, {"text": "However, considering the broad range of linguistic data that has been analyzed with Stochastic OT, it seems unadvisable to reject this model because of the absence of theoretically sound learning methods.", "labels": [], "entities": []}, {"text": "Rather, a general solution is needed to evaluate Stochastic OT as a model for linguistic variation.", "labels": [], "entities": [{"text": "Stochastic OT", "start_pos": 49, "end_pos": 62, "type": "TASK", "confidence": 0.5198084115982056}]}, {"text": "In this paper, I introduce an algorithm for learning Stochastic OT grammars using Markov chain Monte-Carlo methods.", "labels": [], "entities": []}, {"text": "Within a Bayesian frame-work, the learning problem is formalized as finding the posterior distribution of ranking values (G) given the information on constraint interaction based on input-output pairs (D).", "labels": [], "entities": []}, {"text": "The posterior contains all the information needed for linguists' use: for example, if there is a grammar that will generate the exact frequencies as in the data, such a grammar will appear as a mode of the posterior.", "labels": [], "entities": []}, {"text": "In computation, the posterior distribution is simulated with MCMC methods because the likelihood function has a complex form, thus making a maximum-likelihood approach hard to perform.", "labels": [], "entities": []}, {"text": "Such problems are avoided by using the Data Augmentation algorithm to make computation feasible: to simulate the posterior distribution G \u223c p(G|D), we augment the parameter space and simulate a joint distribution . It turns out that by setting Y as the value of constraints that observe the desired ordering, simulating from p(G, Y |D) can be achieved with a Gibbs sampler, which constructs a Markov chain that converges to the joint posterior distribution).", "labels": [], "entities": [{"text": "Data Augmentation", "start_pos": 39, "end_pos": 56, "type": "TASK", "confidence": 0.6530836075544357}]}, {"text": "I will also discuss some issues related to efficiency in implementation.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 4: Conditional sampling steps for", "labels": [], "entities": [{"text": "Conditional sampling", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.9376094043254852}]}, {"text": " Table 5: Data for Ilokano reduplication.", "labels": [], "entities": [{"text": "Ilokano reduplication", "start_pos": 19, "end_pos": 40, "type": "TASK", "confidence": 0.7474471032619476}]}, {"text": " Table 6: Data for Spanish diminutive suffixation.", "labels": [], "entities": [{"text": "Spanish diminutive suffixation", "start_pos": 19, "end_pos": 49, "type": "TASK", "confidence": 0.5868041117986044}]}, {"text": " Table 8: Comparison of Max-Ent and Stochastic OT models", "labels": [], "entities": []}]}