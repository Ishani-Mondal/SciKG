{"title": [{"text": "Extracting Relations with Integrated Information Using Kernel Methods", "labels": [], "entities": [{"text": "Extracting Relations", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8815993368625641}]}], "abstractContent": [{"text": "Entity relation detection is a form of information extraction that finds predefined relations between pairs of entities in text.", "labels": [], "entities": [{"text": "Entity relation detection", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.7756664156913757}, {"text": "information extraction", "start_pos": 39, "end_pos": 61, "type": "TASK", "confidence": 0.7274715155363083}]}, {"text": "This paper describes a relation detection approach that combines clues from different levels of syntactic processing using kernel methods.", "labels": [], "entities": [{"text": "relation detection", "start_pos": 23, "end_pos": 41, "type": "TASK", "confidence": 0.9434786438941956}]}, {"text": "Information from three different levels of processing is considered: tokenization, sentence parsing and deep dependency analysis.", "labels": [], "entities": [{"text": "tokenization", "start_pos": 69, "end_pos": 81, "type": "TASK", "confidence": 0.9790241718292236}, {"text": "sentence parsing", "start_pos": 83, "end_pos": 99, "type": "TASK", "confidence": 0.779515266418457}, {"text": "deep dependency analysis", "start_pos": 104, "end_pos": 128, "type": "TASK", "confidence": 0.5990875363349915}]}, {"text": "Each source of information is represented by kernel functions.", "labels": [], "entities": []}, {"text": "Then composite kernels are developed to integrate and extend individual kernels so that processing errors occurring atone level can be overcome by information from other levels.", "labels": [], "entities": []}, {"text": "We present an evaluation of these methods on the 2004 ACE relation detection task, using Support Vector Machines, and show that each level of syntactic processing contributes useful information for this task.", "labels": [], "entities": [{"text": "ACE relation detection task", "start_pos": 54, "end_pos": 81, "type": "TASK", "confidence": 0.7324445098638535}]}, {"text": "When evaluated on the official test data, our approach produced very competitive ACE value scores.", "labels": [], "entities": [{"text": "official test data", "start_pos": 22, "end_pos": 40, "type": "DATASET", "confidence": 0.7361376682917277}, {"text": "ACE", "start_pos": 81, "end_pos": 84, "type": "METRIC", "confidence": 0.8531516790390015}]}, {"text": "We also compare the SVM with KNN on different kernels.", "labels": [], "entities": []}], "introductionContent": [{"text": "Information extraction subsumes abroad range of tasks, including the extraction of entities, relations and events from various text sources, such as newswire documents and broadcast transcripts.", "labels": [], "entities": [{"text": "Information extraction", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.7990294992923737}]}, {"text": "One such task, relation detection, finds instances of predefined relations between pairs of entities, such as a Located-In relation between the entities Centre College and Danville, KY in the phrase Centre College in Danville, KY.", "labels": [], "entities": [{"text": "relation detection", "start_pos": 15, "end_pos": 33, "type": "TASK", "confidence": 0.9028494656085968}, {"text": "Centre College and Danville, KY in the phrase Centre College in Danville, KY", "start_pos": 153, "end_pos": 229, "type": "DATASET", "confidence": 0.8213911732037862}]}, {"text": "The 'entities' are the individuals of selected semantic types (such as people, organizations, countries, \u2026) which are referred to in the text.", "labels": [], "entities": []}, {"text": "Prior approaches to this task () have relied on partial or full syntactic analysis.", "labels": [], "entities": []}, {"text": "Syntactic analysis can find relations not readily identified based on sequences of tokens alone.", "labels": [], "entities": []}, {"text": "Even 'deeper' representations, such as logical syntactic relations or predicate-argument structure, can in principle capture additional generalizations and thus lead to the identification of additional instances of relations.", "labels": [], "entities": []}, {"text": "However, a general problem in Natural Language Processing is that as the processing gets deeper, it becomes less accurate.", "labels": [], "entities": [{"text": "Natural Language Processing", "start_pos": 30, "end_pos": 57, "type": "TASK", "confidence": 0.6182277997334799}]}, {"text": "For instance, the current accuracy of tokenization, chunking and sentence parsing for English is about 99%, 92%, and 90% respectively.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9996335506439209}, {"text": "tokenization", "start_pos": 38, "end_pos": 50, "type": "TASK", "confidence": 0.9690584540367126}, {"text": "sentence parsing", "start_pos": 65, "end_pos": 81, "type": "TASK", "confidence": 0.6993937492370605}]}, {"text": "Algorithms based solely on deeper representations inevitably suffer from the errors in computing these representations.", "labels": [], "entities": []}, {"text": "On the other hand, low level processing such as tokenization will be more accurate, and may also contain useful information missed by deep processing of text.", "labels": [], "entities": [{"text": "tokenization", "start_pos": 48, "end_pos": 60, "type": "TASK", "confidence": 0.9679534435272217}]}, {"text": "Systems based on a single level of representation are forced to choose between shallower representations, which will have fewer errors, and deeper representations, which maybe more general.", "labels": [], "entities": []}, {"text": "Based on these observations, proposed a discriminative model to combine information from different syntactic sources using a kernel SVM (Support Vector Machine).", "labels": [], "entities": []}, {"text": "We showed that adding sentence level word trigrams as global information to local dependency context boosted the performance of finding slot fillers for management succession events.", "labels": [], "entities": []}, {"text": "This paper describes an extension of this approach to the identification of entity relations, in which syntactic information from sentence tokenization, parsing and deep dependency analysis is combined using kernel methods.", "labels": [], "entities": [{"text": "identification of entity relations", "start_pos": 58, "end_pos": 92, "type": "TASK", "confidence": 0.8406434804201126}]}, {"text": "At each level, kernel functions (or kernels) are developed to represent the syntactic information.", "labels": [], "entities": []}, {"text": "Five kernels have been developed for this task, including two at the surface level, one at the parsing level and two at the deep dependency level.", "labels": [], "entities": [{"text": "parsing", "start_pos": 95, "end_pos": 102, "type": "TASK", "confidence": 0.9591763019561768}]}, {"text": "Our experiments show that each level of processing may contribute useful clues for this task, including surface information like word bigrams.", "labels": [], "entities": []}, {"text": "Adding kernels one by one continuously improves performance.", "labels": [], "entities": []}, {"text": "The experiments were carried out on the ACE RDR (Relation Detection and Recognition) task with annotated entities.", "labels": [], "entities": [{"text": "ACE RDR (Relation Detection and Recognition) task", "start_pos": 40, "end_pos": 89, "type": "TASK", "confidence": 0.7236173384719424}]}, {"text": "Using SVM as a classifier along with the full composite kernel produced the best performance on this task.", "labels": [], "entities": []}, {"text": "This paper will also show a comparison of SVM and KNN (k-Nearest-Neighbors) under different kernel setups.", "labels": [], "entities": []}], "datasetContent": [{"text": "Experiments were carried out on the ACE RDR (Relation Detection and Recognition) task using hand-annotated entities, provided as part of the ACE evaluation.", "labels": [], "entities": [{"text": "ACE RDR (Relation Detection and Recognition) task", "start_pos": 36, "end_pos": 85, "type": "TASK", "confidence": 0.7519638339678446}, {"text": "ACE evaluation", "start_pos": 141, "end_pos": 155, "type": "DATASET", "confidence": 0.8716484606266022}]}, {"text": "The ACE corpora contain documents from two sources: newswire (nwire) documents and broadcast news transcripts (bnews).", "labels": [], "entities": [{"text": "ACE corpora", "start_pos": 4, "end_pos": 15, "type": "DATASET", "confidence": 0.8486586213111877}]}, {"text": "In this section we will compare performance of different kernel setups trained with SVM, as well as different classifiers, KNN and SVM, with the same kernel setup.", "labels": [], "entities": []}, {"text": "The SVM package we used is SVM light . The training parameters were chosen using cross-validation.", "labels": [], "entities": []}, {"text": "One-against-all classification was applied to each pair of entities in a sentence.", "labels": [], "entities": []}, {"text": "When SVM predictions conflict on a relation example, the one with larger margin will be selected as the final answer.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2. SVM performance on incremental kernel  setups. Each setup adds one level of kernels to the  previous one except setup F. Evaluated on the  ACE training data with 5-fold cross-validation. F- scores marked by * are significantly better than the  previous setup (at 95% confidence level).", "labels": [], "entities": [{"text": "ACE training data", "start_pos": 149, "end_pos": 166, "type": "DATASET", "confidence": 0.9344699581464132}, {"text": "F- scores", "start_pos": 197, "end_pos": 206, "type": "METRIC", "confidence": 0.9720744291941324}]}, {"text": " Table 3. Performance of SVM and KNN (k=3) on  different kernel setups. Types are ordered in de- creasing order of frequency of occurrence in the  ACE corpus. In SVM training, the same  parameters were used for all 7 types.", "labels": [], "entities": [{"text": "ACE corpus", "start_pos": 147, "end_pos": 157, "type": "DATASET", "confidence": 0.9588507413864136}]}]}