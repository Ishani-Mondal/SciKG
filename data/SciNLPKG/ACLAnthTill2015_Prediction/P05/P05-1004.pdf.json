{"title": [{"text": "Supersense Tagging of Unknown Nouns using Semantic Similarity", "labels": [], "entities": [{"text": "Supersense Tagging of Unknown Nouns", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.9176089882850647}]}], "abstractContent": [{"text": "The limited coverage of lexical-semantic resources is a significant problem for NLP systems which can be alleviated by automatically classifying the unknown words.", "labels": [], "entities": []}, {"text": "Su-persense tagging assigns unknown nouns one of 26 broad semantic categories used by lexicographers to organise their manual insertion into WORDNET.", "labels": [], "entities": [{"text": "Su-persense tagging", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7315409481525421}, {"text": "organise their manual insertion into WORDNET", "start_pos": 104, "end_pos": 148, "type": "TASK", "confidence": 0.6749335726102194}]}, {"text": "Ciaramita and Johnson (2003) present a tagger which uses synonym set glosses as annotated training examples.", "labels": [], "entities": []}, {"text": "We describe an unsupervised approach, based on vector-space similarity, which does not require annotated examples but significantly outper-forms their tagger.", "labels": [], "entities": []}, {"text": "We also demonstrate the use of an extremely large shallow-parsed corpus for calculating vector-space semantic similarity.", "labels": [], "entities": [{"text": "calculating vector-space semantic similarity", "start_pos": 76, "end_pos": 120, "type": "TASK", "confidence": 0.5785287991166115}]}], "introductionContent": [{"text": "Lexical-semantic resources have been applied successful to a wide range of Natural Language Processing (NLP) problems ranging from collocation extraction) and class-based smoothing), to text classification ( and question answering).", "labels": [], "entities": [{"text": "text classification", "start_pos": 186, "end_pos": 205, "type": "TASK", "confidence": 0.8328245282173157}, {"text": "question answering", "start_pos": 212, "end_pos": 230, "type": "TASK", "confidence": 0.8404035568237305}]}, {"text": "In particular, WORDNET) has significantly influenced research in NLP.", "labels": [], "entities": []}, {"text": "Unfortunately, these resource are extremely timeconsuming and labour-intensive to manually develop and maintain, requiring considerable linguistic and domain expertise.", "labels": [], "entities": []}, {"text": "Lexicographers cannot possibly keep pace with language evolution: sense distinctions are continually made and merged, words are coined or become obsolete, and technical terms migrate into the vernacular.", "labels": [], "entities": []}, {"text": "Technical domains, such as medicine, require separate treatment since common words often take on special meanings, and a significant proportion of their vocabulary does not overlap with everyday vocabulary.", "labels": [], "entities": []}, {"text": "compared an alignment of WORDNET with the UMLS medical resource and found only a very small degree of overlap.", "labels": [], "entities": [{"text": "WORDNET", "start_pos": 25, "end_pos": 32, "type": "DATASET", "confidence": 0.8490826487541199}, {"text": "UMLS medical resource", "start_pos": 42, "end_pos": 63, "type": "DATASET", "confidence": 0.9645127654075623}]}, {"text": "Also, lexicalsemantic resources suffer from: bias towards concepts and senses from particular topics.", "labels": [], "entities": []}, {"text": "Some specialist topics are better covered in WORD-NET than others, e.g. dog has finer-grained distinctions than cat and worm although this does not reflect finer distinctions in reality; limited coverage of infrequent words and senses.", "labels": [], "entities": []}, {"text": "Ciaramita and Johnson found that common nouns missing from WORDNET 1.6 occurred every 8 sentences in the BLLIP corpus.", "labels": [], "entities": [{"text": "BLLIP corpus", "start_pos": 105, "end_pos": 117, "type": "DATASET", "confidence": 0.8949662744998932}]}, {"text": "By WORDNET 2.0, coverage has improved but the problem of keeping up with language evolution remains difficult.", "labels": [], "entities": [{"text": "coverage", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.944561243057251}]}, {"text": "consistency when classifying similar words into categories.", "labels": [], "entities": []}, {"text": "For instance, the WORDNET lexicographer file for ionosphere (location) is different to exosphere and stratosphere (object), two other layers of the earth's atmosphere.", "labels": [], "entities": [{"text": "WORDNET lexicographer file", "start_pos": 18, "end_pos": 44, "type": "DATASET", "confidence": 0.8850840131441752}]}, {"text": "These problems demonstrate the need for automatic or semi-automatic methods for the creation and maintenance of lexical-semantic resources.", "labels": [], "entities": []}, {"text": "Broad semantic classification is currently used by lexicographers to organise the manual insertion of words into WORDNET, and is an experimental precursor to automatically inserting words directly into the WORDNET hierarchy.", "labels": [], "entities": [{"text": "Broad semantic classification", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.7565365632375082}]}, {"text": "Ciaramita and Johnson call this supersense tagging and describe a multi-class perceptron tagger, which uses WORDNET's hierarchical structure to create many annotated training instances from the synset glosses.", "labels": [], "entities": []}, {"text": "This paper describes an unsupervised approach to supersense tagging that does not require annotated sentences.", "labels": [], "entities": [{"text": "supersense tagging", "start_pos": 49, "end_pos": 67, "type": "TASK", "confidence": 0.7984563112258911}]}, {"text": "Instead, we use vector-space similarity to retrieve a number of synonyms for each unknown common noun.", "labels": [], "entities": []}, {"text": "The supersenses of these synonyms are then combined to determine the supersense.", "labels": [], "entities": []}, {"text": "This approach significantly outperforms the multi-class perceptron on the same dataset based on WORDNET 1.6 and 1.7.1.", "labels": [], "entities": []}], "datasetContent": [{"text": "Ciaramita and Johnson propose a very natural evaluation for supersense tagging: inserting the extra common nouns that have been added to anew version of WORDNET.", "labels": [], "entities": [{"text": "supersense tagging", "start_pos": 60, "end_pos": 78, "type": "TASK", "confidence": 0.7781145572662354}, {"text": "WORDNET", "start_pos": 153, "end_pos": 160, "type": "DATASET", "confidence": 0.911382794380188}]}, {"text": "They use the common nouns that have been added to WORDNET 1.7.1 since WORDNET 1.6 and compare this evaluation with a standard cross-validation approach that uses a small percentage of the words from their WORDNET 1.6 training set for evaluation.", "labels": [], "entities": [{"text": "WORDNET 1.7.1", "start_pos": 50, "end_pos": 63, "type": "DATASET", "confidence": 0.9203224778175354}, {"text": "WORDNET 1.6", "start_pos": 70, "end_pos": 81, "type": "DATASET", "confidence": 0.8894689977169037}, {"text": "WORDNET 1.6 training set", "start_pos": 205, "end_pos": 229, "type": "DATASET", "confidence": 0.9282010048627853}]}, {"text": "Their results suggest that the WORDNET 1.7.1 test set is significantly harder because of the large number of abstract category nouns, e.g. communication and cognition, that appear in the 1.7.1 data, which are difficult to classify.", "labels": [], "entities": [{"text": "WORDNET 1.7.1 test set", "start_pos": 31, "end_pos": 53, "type": "DATASET", "confidence": 0.8056003600358963}]}, {"text": "Our evaluation will use exactly the same test sets as . The WORDNET 1.7.1 test set consists of 744 previously unseen nouns, the majority of which (over 90%) have only one sense.", "labels": [], "entities": [{"text": "WORDNET 1.7.1 test set", "start_pos": 60, "end_pos": 82, "type": "DATASET", "confidence": 0.8727862536907196}]}, {"text": "The WORD-NET 1.6 test set consists of several cross-validation sets of 755 nouns randomly selected from the BLLIP training set used by . They have kindly supplied us with the WORDNET 1.7.1 test set and one cross-validation run of the WORDNET 1.6 test set.", "labels": [], "entities": [{"text": "WORD-NET 1.6 test set", "start_pos": 4, "end_pos": 25, "type": "DATASET", "confidence": 0.9211752712726593}, {"text": "BLLIP training set", "start_pos": 108, "end_pos": 126, "type": "DATASET", "confidence": 0.7976762255032858}, {"text": "WORDNET 1.7.1 test set", "start_pos": 175, "end_pos": 197, "type": "DATASET", "confidence": 0.917058065533638}, {"text": "WORDNET 1.6 test set", "start_pos": 234, "end_pos": 254, "type": "DATASET", "confidence": 0.9616943150758743}]}, {"text": "Our development experiments are performed on the WORDNET 1.6 test set with one final run on the WORD-NET 1.7.1 test set.", "labels": [], "entities": [{"text": "WORDNET 1.6 test set", "start_pos": 49, "end_pos": 69, "type": "DATASET", "confidence": 0.9723508059978485}, {"text": "WORD-NET 1.7.1 test set", "start_pos": 96, "end_pos": 119, "type": "DATASET", "confidence": 0.9722926467657089}]}, {"text": "Some examples from the test sets are given in with their supersenses.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Example nouns and their supersenses", "labels": [], "entities": []}, {"text": " Table 3: 2 billion word corpus statistics", "labels": [], "entities": []}, {"text": " Table 6: Summary of supersense tagging accuracies", "labels": [], "entities": [{"text": "supersense tagging accuracies", "start_pos": 21, "end_pos": 50, "type": "TASK", "confidence": 0.6397631466388702}]}, {"text": " Table 6. The accuracy  of the best-performing configurations was 68% on the", "labels": [], "entities": [{"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9991112351417542}]}, {"text": " Table 7: Breakdown of results by supersense", "labels": [], "entities": [{"text": "Breakdown", "start_pos": 10, "end_pos": 19, "type": "TASK", "confidence": 0.6197252869606018}]}]}