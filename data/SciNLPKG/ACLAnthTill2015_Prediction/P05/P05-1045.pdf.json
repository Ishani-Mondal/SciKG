{"title": [{"text": "Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling", "labels": [], "entities": []}], "abstractContent": [{"text": "Most current statistical natural language processing models use only local features so as to permit dynamic programming in inference, but this makes them unable to fully account for the long distance structure that is prevalent in language use.", "labels": [], "entities": []}, {"text": "We show how to solve this dilemma with Gibbs sampling , a simple Monte Carlo method used to perform approximate inference in factored probabilis-tic models.", "labels": [], "entities": []}, {"text": "By using simulated annealing in place of Viterbi decoding in sequence models such as HMMs, CMMs, and CRFs, it is possible to incorporate non-local structure while preserving tractable inference.", "labels": [], "entities": []}, {"text": "We use this technique to augment an existing CRF-based information extraction system with long-distance dependency models, enforcing label consistency and extraction template consistency constraints.", "labels": [], "entities": [{"text": "CRF-based information extraction", "start_pos": 45, "end_pos": 77, "type": "TASK", "confidence": 0.6284728348255157}]}, {"text": "This technique results in an error reduction of up to 9% over state-of-the-art systems on two established information extraction tasks.", "labels": [], "entities": [{"text": "error reduction", "start_pos": 29, "end_pos": 44, "type": "METRIC", "confidence": 0.9593420028686523}, {"text": "information extraction", "start_pos": 106, "end_pos": 128, "type": "TASK", "confidence": 0.7675713896751404}]}], "introductionContent": [{"text": "Most statistical models currently used in natural language processing represent only local structure.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 42, "end_pos": 69, "type": "TASK", "confidence": 0.6928340395291647}]}, {"text": "Although this constraint is critical in enabling tractable model inference, it is a key limitation in many tasks, since natural language contains a great deal of nonlocal structure.", "labels": [], "entities": []}, {"text": "A general method for solving this problem is to relax the requirement of exact inference, substituting approximate inference algorithms instead, thereby permitting tractable inference in models with non-local structure.", "labels": [], "entities": []}, {"text": "One such algorithm is Gibbs sampling, a simple Monte Carlo algorithm that is appropriate for inference in any factored probabilistic model, including sequence models and probabilistic context free grammars.", "labels": [], "entities": []}, {"text": "Although Gibbs sampling is widely used elsewhere, there has been extremely little use of it in natural language processing.", "labels": [], "entities": [{"text": "Gibbs sampling", "start_pos": 9, "end_pos": 23, "type": "TASK", "confidence": 0.8276993036270142}]}, {"text": "Here, we use it to add non-local dependencies to sequence models for information extraction.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 69, "end_pos": 91, "type": "TASK", "confidence": 0.8645478785037994}]}, {"text": "Statistical hidden state sequence models, such as Hidden Markov Models (HMMs)), Conditional Markov Models (CMMs), and Conditional Random Fields (CRFs) () area prominent recent approach to information extraction tasks.", "labels": [], "entities": [{"text": "information extraction tasks", "start_pos": 188, "end_pos": 216, "type": "TASK", "confidence": 0.8761275808016459}]}, {"text": "These models all encode the Markov property: decisions about the state at a particular position in the sequence can depend only on a small local window.", "labels": [], "entities": []}, {"text": "It is this property which allows tractable computation: the Viterbi, Forward Backward, and Clique Calibration algorithms all become intractable without it.", "labels": [], "entities": []}, {"text": "However, information extraction tasks can benefit from modeling non-local structure.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 9, "end_pos": 31, "type": "TASK", "confidence": 0.8366900682449341}]}, {"text": "As an example, several authors (see Section 8) mention the value of enforcing label consistency in named entity recognition (NER) tasks.", "labels": [], "entities": [{"text": "named entity recognition (NER) tasks", "start_pos": 99, "end_pos": 135, "type": "TASK", "confidence": 0.8343501091003418}]}, {"text": "In the example given in, the second occurrence of the token Tanjug is mislabeled by our CRF-based statistical NER system, because by looking only at local evidence it is unclear whether it is a person or organization.", "labels": [], "entities": [{"text": "CRF-based statistical NER system", "start_pos": 88, "end_pos": 120, "type": "DATASET", "confidence": 0.7964199036359787}]}, {"text": "The first occurrence of Tanjug provides ample evidence that it is an organization, however, and by enforcing label consistency the system should be able to get it right.", "labels": [], "entities": []}, {"text": "We show how to incorporate constraints of this form into a CRF model by using Gibbs sampling instead of the Viterbi algorithm as our inference procedure, and demonstrate that this technique yields significant improvements on two established IE tasks.", "labels": [], "entities": [{"text": "IE tasks", "start_pos": 241, "end_pos": 249, "type": "TASK", "confidence": 0.9175131320953369}]}], "datasetContent": [{"text": "We test the effectiveness of our technique on two established datasets: the CoNLL 2003 English named entity recognition dataset, and the CMU Seminar Announcements information extraction dataset.", "labels": [], "entities": [{"text": "CoNLL 2003 English named entity recognition dataset", "start_pos": 76, "end_pos": 127, "type": "DATASET", "confidence": 0.8938723802566528}, {"text": "CMU Seminar Announcements information extraction dataset", "start_pos": 137, "end_pos": 193, "type": "DATASET", "confidence": 0.8875379165013632}]}], "tableCaptions": [{"text": " Table 1: An illustration of the effectiveness of Gibbs sampling,  compared to Viterbi inference, for the two tasks addressed in  this paper: the CoNLL named entity recognition task, and the  CMU Seminar Announcements information extraction task. We  show 10 runs of Gibbs sampling in the same CRF model that  was used for Viterbi. For each run the sampler was initialized  to a random sequence, and used a linear annealing schedule that  sampled the complete sequence 1000 times. CoNLL perfor- mance is measured as per-entity F1, and CMU Seminar An- nouncements performance is measured as per-token F1.", "labels": [], "entities": [{"text": "CoNLL named entity recognition task", "start_pos": 146, "end_pos": 181, "type": "TASK", "confidence": 0.6411839663982392}, {"text": "CMU Seminar Announcements information extraction", "start_pos": 192, "end_pos": 240, "type": "TASK", "confidence": 0.6791651964187622}]}, {"text": " Table 3: Counts of the number of times multiple occurrences of  a token sequence is labeled as different entity types in the same  document. Taken from the CoNLL training set.", "labels": [], "entities": [{"text": "CoNLL training set", "start_pos": 157, "end_pos": 175, "type": "DATASET", "confidence": 0.9632837176322937}]}, {"text": " Table 4: Counts of the number of times an entity sequence is  labeled differently from an occurrence of a subsequence of it  elsewhere in the document. Rows correspond to sequences, and  columns to subsequences. Taken from the CoNLL training set.", "labels": [], "entities": [{"text": "CoNLL training set", "start_pos": 228, "end_pos": 246, "type": "DATASET", "confidence": 0.9688045382499695}]}, {"text": " Table 5: F1 scores of the local CRF and non-local models on the  CoNLL 2003 named entity recognition dataset. We also provide  the results from Bunescu and Mooney (2004) for comparison.", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9995099306106567}, {"text": "CoNLL 2003 named entity recognition dataset", "start_pos": 66, "end_pos": 109, "type": "DATASET", "confidence": 0.8813843329747518}]}, {"text": " Table 6: F1 scores of the local CRF and non-local models on  the CMU Seminar Announcements dataset. We also provide  the results from", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.999459445476532}, {"text": "CMU Seminar Announcements dataset", "start_pos": 66, "end_pos": 99, "type": "DATASET", "confidence": 0.9650003015995026}]}]}