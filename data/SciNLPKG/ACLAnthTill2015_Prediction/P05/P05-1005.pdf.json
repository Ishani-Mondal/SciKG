{"title": [{"text": "Learning Semantic Classes for Word Sense Disambiguation", "labels": [], "entities": [{"text": "Word Sense Disambiguation", "start_pos": 30, "end_pos": 55, "type": "TASK", "confidence": 0.7573283612728119}]}], "abstractContent": [{"text": "Word Sense Disambiguation suffers from a long-standing problem of knowledge acquisition bottleneck.", "labels": [], "entities": [{"text": "Word Sense Disambiguation", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.7207823793093363}]}, {"text": "Although state of the art supervised systems report good accuracies for selected words, they have not been shown to be promising in terms of scalability.", "labels": [], "entities": []}, {"text": "In this paper, we present an approach for learning coarser and more general set of concepts from a sense tagged corpus, in order to alleviate the knowledge acquisition bottleneck.", "labels": [], "entities": [{"text": "knowledge acquisition", "start_pos": 146, "end_pos": 167, "type": "TASK", "confidence": 0.735296905040741}]}, {"text": "We show that these general concepts can be transformed to fine grained word senses using simple heuristics, and applying the technique for recent SENSEVAL data sets shows that our approach can yield state of the art performance .", "labels": [], "entities": [{"text": "SENSEVAL data sets", "start_pos": 146, "end_pos": 164, "type": "DATASET", "confidence": 0.8349625865618387}]}], "introductionContent": [{"text": "Word Sense Disambiguation (WSD) is the task of determining the meaning of a word in a given context.", "labels": [], "entities": [{"text": "Word Sense Disambiguation (WSD)", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.7876077592372894}, {"text": "determining the meaning of a word in a given context", "start_pos": 47, "end_pos": 99, "type": "TASK", "confidence": 0.5053814679384232}]}, {"text": "This task has along history in natural language processing, and is considered to bean intermediate task, success of which is considered to be important for other tasks such as Machine Translation, Language Understanding, and Information Retrieval.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 31, "end_pos": 58, "type": "TASK", "confidence": 0.6631154616673788}, {"text": "Machine Translation", "start_pos": 176, "end_pos": 195, "type": "TASK", "confidence": 0.8391087651252747}, {"text": "Language Understanding", "start_pos": 197, "end_pos": 219, "type": "TASK", "confidence": 0.7653496265411377}, {"text": "Information Retrieval", "start_pos": 225, "end_pos": 246, "type": "TASK", "confidence": 0.823712557554245}]}, {"text": "Despite along history of attempts to solve WSD problem by empirical means, there is not any clear consensus on what it takes to build a high performance implementation of WSD.", "labels": [], "entities": [{"text": "WSD problem", "start_pos": 43, "end_pos": 54, "type": "TASK", "confidence": 0.9082737863063812}]}, {"text": "Algorithms based on Supervised Learning, in general, show better performance compared to unsupervised systems.", "labels": [], "entities": []}, {"text": "But they suffer from a serious drawback: the difficulty of acquiring considerable amounts of training data, also known as knowledge acquisition bottleneck.", "labels": [], "entities": [{"text": "knowledge acquisition", "start_pos": 122, "end_pos": 143, "type": "TASK", "confidence": 0.7851490676403046}]}, {"text": "In the typical setting, supervised learning needs training data created for each and every polysemous word; estimates an effort of 16 personyears for acquiring training data for 3,200 significant words in English.", "labels": [], "entities": []}, {"text": "provide a similar estimate of an 80 person-year effort for creating manually labelled training data for about 20,000 words in a common English dictionary.", "labels": [], "entities": []}, {"text": "Two basic approaches have been tried as solutions to the lack of training data, namely unsupervised systems and semi-supervised bootstrapping techniques.", "labels": [], "entities": []}, {"text": "Unsupervised systems mostly work on knowledge-based techniques, exploiting sense knowledge encoded in machine-readable dictionary entries, taxonomical hierarchies such as WORD-NET, and soon.", "labels": [], "entities": []}, {"text": "Most of the bootstrapping techniques start from a few 'seed' labelled examples, classify some unlabelled instances using this knowledge, and iteratively expand their knowledge using information available within newly labelled data.", "labels": [], "entities": []}, {"text": "Some others employ hierarchical relatives such as hypernyms and hyponyms.", "labels": [], "entities": []}, {"text": "In this work, we present another practical alternative: we reduce the WSD problem to a one of finding generic semantic class of a given word instance.", "labels": [], "entities": [{"text": "WSD", "start_pos": 70, "end_pos": 73, "type": "TASK", "confidence": 0.9685121178627014}]}, {"text": "We show that learning such classes can help relieve the problem of knowledge acquisition bottleneck.", "labels": [], "entities": [{"text": "knowledge acquisition bottleneck", "start_pos": 67, "end_pos": 99, "type": "TASK", "confidence": 0.8392092188199362}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Results of baseline, individual, and com- bined classifiers: recall measures for nouns and  verbs combined.", "labels": [], "entities": [{"text": "recall", "start_pos": 71, "end_pos": 77, "type": "METRIC", "confidence": 0.9965580105781555}]}, {"text": " Table 3: Effect of different similarity schemes on  recall, combined results for nouns and verbs", "labels": [], "entities": [{"text": "recall", "start_pos": 53, "end_pos": 59, "type": "METRIC", "confidence": 0.9892438650131226}]}, {"text": " Table 4: Improvement of performance with classifier  weighting. Combined results for nouns and verbs  with voting schemes Simple Majority (SM), Global  classifier weights (GW) and local weights (LW).", "labels": [], "entities": []}]}