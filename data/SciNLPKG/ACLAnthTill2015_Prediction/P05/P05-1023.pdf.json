{"title": [{"text": "Data-Defined Kernels for Parse Reranking Derived from Probabilistic Models", "labels": [], "entities": [{"text": "Parse Reranking Derived", "start_pos": 25, "end_pos": 48, "type": "TASK", "confidence": 0.9090099136034647}]}], "abstractContent": [{"text": "Previous research applying kernel methods to natural language parsing have focussed on proposing kernels over parse trees, which are hand-crafted based on domain knowledge and computational considerations.", "labels": [], "entities": [{"text": "natural language parsing", "start_pos": 45, "end_pos": 69, "type": "TASK", "confidence": 0.6762411594390869}]}, {"text": "In this paper we propose a method for defining kernels in terms of a probabilistic model of parsing.", "labels": [], "entities": []}, {"text": "This model is then trained, so that the parameters of the probabilistic model reflect the generalizations in the training data.", "labels": [], "entities": []}, {"text": "The method we propose then uses these trained parameters to define a kernel for rerank-ing parse trees.", "labels": [], "entities": []}, {"text": "In experiments, we use a neural network based statistical parser as the probabilistic model, and use the resulting kernel with the Voted Percep-tron algorithm to rerank the top 20 parses from the probabilistic model.", "labels": [], "entities": []}, {"text": "This method achieves a significant improvement over the accuracy of the probabilistic model.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.9994252920150757}]}], "introductionContent": [{"text": "Kernel methods have been shown to be very effective in many machine learning problems.", "labels": [], "entities": []}, {"text": "They have the advantage that learning can try to optimize measures related directly to expected testing performance (i.e. \"large margin\" methods), rather than the probabilistic measures used in statistical models, which are only indirectly related to expected testing performance.", "labels": [], "entities": []}, {"text": "Work on kernel methods in natural language has focussed on the definition of appropriate kernels for natural language tasks.", "labels": [], "entities": []}, {"text": "In particular, most of the work on parsing with kernel methods has focussed on kernels over parse trees).", "labels": [], "entities": [{"text": "parsing", "start_pos": 35, "end_pos": 42, "type": "TASK", "confidence": 0.9695594310760498}]}, {"text": "These kernels have all been hand-crafted to try reflect properties of parse trees which are relevant to discriminating correct parse trees from incorrect ones, while at the same time maintaining the tractability of learning.", "labels": [], "entities": []}, {"text": "Some work in machine learning has taken an alternative approach to defining kernels, where the kernel is derived from a probabilistic model of the task ().", "labels": [], "entities": []}, {"text": "This way of defining kernels has two advantages.", "labels": [], "entities": []}, {"text": "First, linguistic knowledge about parsing is reflected in the design of the probabilistic model, not directly in the kernel.", "labels": [], "entities": []}, {"text": "Designing probabilistic models to reflect linguistic knowledge is a process which is currently well understood, both in terms of reflecting generalizations and controlling computational cost.", "labels": [], "entities": []}, {"text": "Because many NLP problems are unbounded in size and complexity, it is hard to specify all possible relevant kernel features without having so many features that the computations become intractable and/or the data becomes too sparse.", "labels": [], "entities": []}, {"text": "Second, the kernel is defined using the trained parameters of the probabilistic model.", "labels": [], "entities": []}, {"text": "Thus the kernel is in part determined by the training data, and is automatically tailored to reflect properties of parse trees which are relevant to parsing.", "labels": [], "entities": []}, {"text": "In this paper, we propose anew method for deriving a kernel from a probabilistic model which is specifically tailored to reranking tasks, and we apply this method to natural language parsing.", "labels": [], "entities": [{"text": "natural language parsing", "start_pos": 166, "end_pos": 190, "type": "TASK", "confidence": 0.6014276643594106}]}, {"text": "For the probabilistic model, we use a state-of-the-art neural network based statistical parser).", "labels": [], "entities": []}, {"text": "The resulting kernel is then used with the Voted Perceptron algorithm to reranking the top 20 parses from the probabilistic model.", "labels": [], "entities": []}, {"text": "This method achieves a significant improvement over the accuracy of the probabilistic model alone.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.9993008375167847}]}], "datasetContent": [{"text": "We used the Penn Treebank WSJ corpus) to perform empirical experiments on the proposed parsing models.", "labels": [], "entities": [{"text": "Penn Treebank WSJ corpus", "start_pos": 12, "end_pos": 36, "type": "DATASET", "confidence": 0.982908770442009}]}, {"text": "In each case the input to the network is a sequence of tag-word pairs.", "labels": [], "entities": []}, {"text": "We report results for two different vocabulary sizes, varying in the frequency with which tag-word pairs must occur in the training set in order to be included explicitly in the vocabulary.", "labels": [], "entities": []}, {"text": "A frequency threshold of 200 resulted in a vocabulary of 508 tag-word pairs (including tag-unknown word pairs) and a threshold of 20 resulted in 4215 tag-word pairs.", "labels": [], "entities": []}, {"text": "We denote the probabilistic model trained with the vocabulary of 508 by the SSN-Freq\u2265200, the model trained with the vocabulary of 4215 by the SSN-Freq\u226520.", "labels": [], "entities": []}, {"text": "Testing the probabilistic parser requires using abeam search through the space of possible parses.", "labels": [], "entities": []}, {"text": "We used a form of beam search which prunes the search after the prediction of each word.", "labels": [], "entities": []}, {"text": "We set the width of this post-word beam to 40 for both testing of the probabilistic model and generating the candidate list for reranking.", "labels": [], "entities": []}, {"text": "For training and testing of the kernel models, we provided a candidate list consisting of the top 20 parses found by the generative probabilistic model.", "labels": [], "entities": []}, {"text": "When using the Fisher kernel, we added the log-probability of the tree given by the probabilistic model as the feature.", "labels": [], "entities": []}, {"text": "This was not necessary for the TOP kernels because they already contain a feature corresponding to the probability estimated by the probabilistic model (see section 2.3).", "labels": [], "entities": []}, {"text": "We trained the VP model with all three kernels using the 508 word vocabulary (Fisher-Freq\u2265200, TOP-Freq\u2265200, TOP-Eff-Freq\u2265200) but only the efficient TOP reranking kernel model was trained with the vocabulary of 4215 words (TOP-Eff-Freq\u226520).", "labels": [], "entities": []}, {"text": "The non-sparsity of the feature vectors for other kernels led to the excessive memory requirements and larger testing time.", "labels": [], "entities": []}, {"text": "In each case, the VP model was run for only one epoch.", "labels": [], "entities": []}, {"text": "We would expect some improvement if running it for more epochs, as has been empirically demonstrated in other domains.", "labels": [], "entities": []}, {"text": "To avoid repeated testing on the standard testing set, we first compare the different models with their performance on the validation set.", "labels": [], "entities": []}, {"text": "Note that the validation set wasn't used during learning of the kernel models or for adjustment of any parameters.: Percentage labeled constituent recall (LR), precision (LP), and a combination of both (F \u03b2=1 ) on validation set sentences of length at most 100.", "labels": [], "entities": [{"text": "constituent recall (LR)", "start_pos": 135, "end_pos": 158, "type": "METRIC", "confidence": 0.7979066371917725}, {"text": "precision (LP)", "start_pos": 160, "end_pos": 174, "type": "METRIC", "confidence": 0.9673179984092712}]}, {"text": "model, but only the improvement of the TOP kernels is statistically significant.", "labels": [], "entities": []}, {"text": "For the TOP kernel, the improvement over baseline is about the same with both vocabulary sizes.", "labels": [], "entities": []}, {"text": "Also note that the performance of the efficient TOP reranking kernel is the same as that of the original TOP reranking kernel, for the smaller vocabulary.", "labels": [], "entities": []}, {"text": "For comparison to previous results, table 2 lists the results on the testing set for our best model (TOP-Efficient-Freq\u226520) and several other statistical parsers.", "labels": [], "entities": []}, {"text": "First note that the parser based on the TOP efficient kernel has better accuracy than, which used the same parsing method as our baseline model, although the trained network parameters were not the same.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 72, "end_pos": 80, "type": "METRIC", "confidence": 0.9994262456893921}]}, {"text": "When compared to other kernel methods, our approach performs better than those based on the Tree kernel (, and is only 0.2% worse than the best results achieved by a kernel method for parsing).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Percentage labeled constituent recall (LR),  precision (LP), and a combination of both (F \u03b2=1 ) on  the entire testing set.", "labels": [], "entities": [{"text": "recall (LR)", "start_pos": 41, "end_pos": 52, "type": "METRIC", "confidence": 0.926786333322525}, {"text": "precision (LP)", "start_pos": 55, "end_pos": 69, "type": "METRIC", "confidence": 0.970926970243454}, {"text": "F \u03b2=1 )", "start_pos": 98, "end_pos": 105, "type": "METRIC", "confidence": 0.9531853199005127}]}]}