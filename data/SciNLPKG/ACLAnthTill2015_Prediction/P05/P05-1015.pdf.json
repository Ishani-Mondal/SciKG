{"title": [{"text": "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales", "labels": [], "entities": [{"text": "sentiment categorization", "start_pos": 49, "end_pos": 73, "type": "TASK", "confidence": 0.9511544108390808}]}], "abstractContent": [{"text": "We address the rating-inference problem, wherein rather than simply decide whether a review is \"thumbs up\" or \"thumbs down\", as in previous sentiment analysis work, one must determine an author's evaluation with respect to a multi-point scale (e.g., one to five \"stars\").", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 140, "end_pos": 158, "type": "TASK", "confidence": 0.8270459175109863}]}, {"text": "This task represents an interesting twist on standard multi-class text categorization because there are several different degrees of similarity between class labels; for example , \"three stars\" is intuitively closer to \"four stars\" than to \"one star\".", "labels": [], "entities": [{"text": "multi-class text categorization", "start_pos": 54, "end_pos": 85, "type": "TASK", "confidence": 0.6179988185564677}]}, {"text": "We first evaluate human performance at the task.", "labels": [], "entities": []}, {"text": "Then, we apply a meta-algorithm, based on a metric labeling formulation of the problem, that alters a given \u00a6-ary classifier's output in an explicit attempt to ensure that similar items receive similar labels.", "labels": [], "entities": []}, {"text": "We show that the meta-algorithm can provide significant improvements over both multi-class and regression versions of SVMs when we employ a novel similarity measure appropriate to the problem.", "labels": [], "entities": []}], "introductionContent": [{"text": "There has recently been a dramatic surge of interest in sentiment analysis, as more and more people become aware of the scientific challenges posed and the scope of new applications enabled by the processing of subjective language.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 56, "end_pos": 74, "type": "TASK", "confidence": 0.971747636795044}]}, {"text": "(The papers collected by form a representative sample of research in the area.)", "labels": [], "entities": []}, {"text": "Most prior work on the specific problem of categorizing expressly opinionated text has focused on the binary distinction of positive vs. negative.", "labels": [], "entities": [{"text": "categorizing expressly opinionated text", "start_pos": 43, "end_pos": 82, "type": "TASK", "confidence": 0.8492791801691055}]}, {"text": "But it is often helpful to have more information than this binary distinction provides, especially if one is ranking items by recommendation or comparing several reviewers' opinions: example applications include collaborative filtering and deciding which conference submissions to accept.", "labels": [], "entities": []}, {"text": "Therefore, in this paper we consider generalizing to finer-grained scales: rather than just determine whether a review is \"thumbs up\" or not, we attempt to infer the author's implied numerical rating, such as \"three stars\" or \"four stars\".", "labels": [], "entities": []}, {"text": "Note that this differs from identifying opinion strength): rants and raves have the same strength but represent opposite evaluations, and referee forms often allow one to indicate that one is very confident (high strength) that a conference submission is mediocre (middling rating).", "labels": [], "entities": []}, {"text": "Also, our task differs from ranking not only because one can be given a single item to classify (as opposed to a set of items to be ordered relative to one another), but because there are settings in which classification is harder than ranking, and vice versa.", "labels": [], "entities": []}, {"text": "One can apply standard \u00a6 -ary classifiers or regression to this rating-inference problem; independent work by considers such methods.", "labels": [], "entities": []}, {"text": "But an alternative approach that explicitly incorporates information about item similarities together with label similarity information (for instance, \"one star\" is closer to \"two stars\" than to \"four stars\") is to think of the task as one of metric labeling), where label relations are encoded via a distance metric.", "labels": [], "entities": []}, {"text": "This observation yields a meta-algorithm, applicable to both semi-supervised (via graph-theoretic techniques) and supervised settings, that alters a given \u00a6 -ary classifier's output so that similar items tend to be assigned similar labels.", "labels": [], "entities": []}, {"text": "In what follows, we first demonstrate that humans can discern relatively small differences in (hidden) evaluation scores, indicating that rating inference is indeed a meaningful task.", "labels": [], "entities": []}, {"text": "We then present three types of algorithms -one-vs-all, regression, and metric labeling -that can be distinguished by how explicitly they attempt to leverage similarity between items and between labels.", "labels": [], "entities": []}, {"text": "Next, we consider what item similarity measure to apply, proposing one based on the positive-sentence percentage.", "labels": [], "entities": []}, {"text": "Incorporating this new measure within the metriclabeling framework is shown to often provide significant improvements over the other algorithms.", "labels": [], "entities": []}, {"text": "We hope that some of the insights derived here might apply to other scales for text classifcation that have been considered, such as clause-level opinion strength); affect types like disgust; reading level; and urgency or criticality).", "labels": [], "entities": [{"text": "text classifcation", "start_pos": 79, "end_pos": 97, "type": "TASK", "confidence": 0.7344542741775513}]}], "datasetContent": [{"text": "This section compares the accuracies of the approaches outlined in Section 3 on the four corpora comprising our scale dataset.", "labels": [], "entities": []}, {"text": "(Results using } error were qualitatively similar.)", "labels": [], "entities": []}, {"text": "Throughout, when we refer to something as \"significant\", we mean statistically so  to those values yielding the best performance, we then re-train A (but with SVM parameters fixed, as described above) on the whole training set.", "labels": [], "entities": []}, {"text": "At test time, the nearest neighbors of each item are also taken from the full training set.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Average over authors and class pairs of  between-class vocabulary overlap as the class labels  of the pair grow farther apart.", "labels": [], "entities": [{"text": "Average", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9856160879135132}]}]}