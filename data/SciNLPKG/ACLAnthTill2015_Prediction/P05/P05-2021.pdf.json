{"title": [{"text": "Speech Recognition of Czech -Inclusion of Rare Words Helps", "labels": [], "entities": [{"text": "Speech Recognition of Czech -Inclusion of Rare Words", "start_pos": 0, "end_pos": 52, "type": "TASK", "confidence": 0.8661658830112882}]}], "abstractContent": [{"text": "Large vocabulary continuous speech recognition of inflective languages, such as Czech, Russian or Serbo-Croatian, is heavily deteriorated by excessive out of vocabulary rate.", "labels": [], "entities": []}, {"text": "In this paper, we tackle the problem of vocabulary selection, language modeling and pruning for inflective languages.", "labels": [], "entities": [{"text": "vocabulary selection", "start_pos": 40, "end_pos": 60, "type": "TASK", "confidence": 0.7809575200080872}, {"text": "language modeling", "start_pos": 62, "end_pos": 79, "type": "TASK", "confidence": 0.7505002617835999}]}, {"text": "We show that by explicit reduction of out of vocabulary rate we can achieve significant improvements in recognition accuracy while almost preserving the model size.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 116, "end_pos": 124, "type": "METRIC", "confidence": 0.9658258557319641}]}, {"text": "Reported results are on Czech speech corpora.", "labels": [], "entities": []}], "introductionContent": [{"text": "Large vocabulary continuous speech recognition of inflective languages is a challenging task for mainly two reasons.", "labels": [], "entities": [{"text": "Large vocabulary continuous speech recognition", "start_pos": 0, "end_pos": 46, "type": "TASK", "confidence": 0.5952131628990174}]}, {"text": "Rich morphology generates huge number of forms which are not captured by limited-size dictionaries, and therefore leads to worse recognition results.", "labels": [], "entities": []}, {"text": "Relatively free word order admits enormous number of word sequences and thus impoverishes \u00a2 -gram language models.", "labels": [], "entities": []}, {"text": "In this paper we are concerned with the former issue.", "labels": [], "entities": []}, {"text": "Previous work which deals with excessive vocabulary growth goes mainly in two lines.", "labels": [], "entities": []}, {"text": "Authors have either decided to break words into sub-word units or to adapt dictionaries in a multi-pass scenario.", "labels": [], "entities": []}, {"text": "On Czech data,) suggest to use linguistically motivated recognition units.", "labels": [], "entities": [{"text": "Czech data", "start_pos": 3, "end_pos": 13, "type": "DATASET", "confidence": 0.8803440034389496}]}, {"text": "Words are broken down to stems and endings and used as the recognition units in the first recognition phase.", "labels": [], "entities": []}, {"text": "In the second phase, stems and endings are concatenated.", "labels": [], "entities": []}, {"text": "On Serbo-Croatian,) also tested morphemes as the recognition units.", "labels": [], "entities": []}, {"text": "Both groups of authors agreed that this approach is not beneficial for speech recognition of inflective languages.", "labels": [], "entities": [{"text": "speech recognition of inflective languages", "start_pos": 71, "end_pos": 113, "type": "TASK", "confidence": 0.7994620680809021}]}, {"text": "Vocabulary adaptation, however, brought considerable improvement.", "labels": [], "entities": [{"text": "Vocabulary adaptation", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.9308880269527435}]}, {"text": "Both) on Czech and () on SerboCroatian reported substantial reduction of word error rate.", "labels": [], "entities": [{"text": "Czech", "start_pos": 9, "end_pos": 14, "type": "DATASET", "confidence": 0.9836269617080688}, {"text": "SerboCroatian", "start_pos": 25, "end_pos": 38, "type": "DATASET", "confidence": 0.9155458211898804}, {"text": "word error rate", "start_pos": 73, "end_pos": 88, "type": "METRIC", "confidence": 0.724275271097819}]}, {"text": "Both authors followed the same procedure.", "labels": [], "entities": []}, {"text": "In the first pass, they used a dictionary composed of the most frequent words.", "labels": [], "entities": []}, {"text": "Generated lattices were then processed to get a list of all words which appeared in them.", "labels": [], "entities": []}, {"text": "This list served as a basis fora new adapted dictionary into which morphological variants were added.", "labels": [], "entities": []}, {"text": "It can be concluded that large corpora contain a host of words which are ignored during estimation of language models used in first pass, despite the fact that these rare words can bring substantial improvement.", "labels": [], "entities": []}, {"text": "Therefore, it is desirable to explore how to incorporate rare or even unseen words into a language model which can be used in a first pass.", "labels": [], "entities": []}], "datasetContent": [{"text": "We have evaluated our approach on two corpora, Czech Broadcast News and the Czech portion of MALACH data.", "labels": [], "entities": [{"text": "Czech Broadcast News", "start_pos": 47, "end_pos": 67, "type": "DATASET", "confidence": 0.9416523178418478}, {"text": "Czech portion of MALACH data", "start_pos": 76, "end_pos": 104, "type": "DATASET", "confidence": 0.8366366386413574}]}], "tableCaptions": [{"text": " Table 4: Evaluation on 2500 test sentences. OWER  stands for the oracle error rate.", "labels": [], "entities": [{"text": "OWER", "start_pos": 45, "end_pos": 49, "type": "METRIC", "confidence": 0.9951974749565125}]}, {"text": " Table 7: Word error rate and oracle WER for base- line and injected models. Uniform Morph refers  to the constant uniform loop of the morphology- generated words. Inj denotes the loop of the rest  of words of the CNC corpus and the morphology- generated words.", "labels": [], "entities": [{"text": "Word error rate", "start_pos": 10, "end_pos": 25, "type": "METRIC", "confidence": 0.6255605717500051}, {"text": "WER", "start_pos": 37, "end_pos": 40, "type": "METRIC", "confidence": 0.9679815173149109}, {"text": "CNC corpus", "start_pos": 214, "end_pos": 224, "type": "DATASET", "confidence": 0.8084913194179535}]}]}