{"title": [{"text": "Jointly Labeling Multiple Sequences: A Factorial HMM Approach", "labels": [], "entities": [{"text": "Jointly Labeling Multiple Sequences", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.6785788089036942}, {"text": "HMM Approach", "start_pos": 49, "end_pos": 61, "type": "TASK", "confidence": 0.5339489728212357}]}], "abstractContent": [{"text": "We present new statistical models for jointly labeling multiple sequences and apply them to the combined task of part-of-speech tagging and noun phrase chunk-ing.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 113, "end_pos": 135, "type": "TASK", "confidence": 0.7106857746839523}, {"text": "noun phrase chunk-ing", "start_pos": 140, "end_pos": 161, "type": "TASK", "confidence": 0.7148186564445496}]}, {"text": "The model is based on the Factorial Hidden Markov Model (FHMM) with distributed hidden states representing part-of-speech and noun phrase sequences.", "labels": [], "entities": []}, {"text": "We demonstrate that this joint labeling approach , by enabling information sharing between tagging/chunking subtasks, out-performs the traditional method of tagging and chunking in succession.", "labels": [], "entities": []}, {"text": "Further , we extend this into a novel model, Switching FHMM, to allow for explicit modeling of cross-sequence dependencies based on linguistic knowledge.", "labels": [], "entities": [{"text": "FHMM", "start_pos": 55, "end_pos": 59, "type": "METRIC", "confidence": 0.4646316170692444}]}, {"text": "We report tagging/chunking accuracies for varying dataset sizes and show that our approach is relatively robust to data sparsity.", "labels": [], "entities": [{"text": "tagging/chunking", "start_pos": 10, "end_pos": 26, "type": "TASK", "confidence": 0.7536811431248983}]}], "introductionContent": [{"text": "Traditionally, various sequence labeling problems in natural language processing are solved by the cascading of well-defined subtasks, each extracting specific knowledge.", "labels": [], "entities": [{"text": "sequence labeling", "start_pos": 23, "end_pos": 40, "type": "TASK", "confidence": 0.6906282901763916}]}, {"text": "For instance, the problem of information extraction from sentences maybe broken into several stages: First, part-of-speech (POS) tagging is performed on the sequence of word tokens.", "labels": [], "entities": [{"text": "information extraction from sentences", "start_pos": 29, "end_pos": 66, "type": "TASK", "confidence": 0.8257344141602516}, {"text": "part-of-speech (POS) tagging", "start_pos": 108, "end_pos": 136, "type": "TASK", "confidence": 0.6152990698814392}]}, {"text": "This result is then utilized in noun-phrase and verbphrase chunking.", "labels": [], "entities": [{"text": "verbphrase chunking", "start_pos": 48, "end_pos": 67, "type": "TASK", "confidence": 0.6861629635095596}]}, {"text": "Finally, a higher-level analyzer extracts relevant information based on knowledge gleaned in previous subtasks.", "labels": [], "entities": []}, {"text": "The decomposition of problems into well-defined subtasks is useful but sometimes leads to unnecessary errors.", "labels": [], "entities": []}, {"text": "The problem is that errors in earlier subtasks will propagate to downstream subtasks, ultimately deteriorating overall performance.", "labels": [], "entities": []}, {"text": "Therefore, a method that allows the joint labeling of subtasks is desired.", "labels": [], "entities": []}, {"text": "Two major advantages arise from simultaneous labeling: First, there is more robustness against error propagation.", "labels": [], "entities": [{"text": "error propagation", "start_pos": 95, "end_pos": 112, "type": "TASK", "confidence": 0.6806070953607559}]}, {"text": "This is especially relevant if we use probabilities in our models.", "labels": [], "entities": []}, {"text": "Cascading subtasks inherently \"throws away\" the probability at each stage; joint labeling preserves the uncertainty.", "labels": [], "entities": []}, {"text": "Second, information between simultaneous subtasks can be shared to further improve accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 83, "end_pos": 91, "type": "METRIC", "confidence": 0.9962448477745056}]}, {"text": "For instance, it is possible that knowing a certain noun phrase chunk may help the model infer POS tags more accurately, and vice versa.", "labels": [], "entities": []}, {"text": "In this paper, we propose a solution to the joint labeling problem by representing multiple sequences in a single Factorial Hidden Markov Model (FHMM)).", "labels": [], "entities": []}, {"text": "The FHMM generalizes hidden Markov models (HMM) by allowing separate hidden state sequences.", "labels": [], "entities": [{"text": "FHMM", "start_pos": 4, "end_pos": 8, "type": "DATASET", "confidence": 0.8922826647758484}]}, {"text": "In our case, these hidden state sequences represent the POS tags and phrase chunk labels.", "labels": [], "entities": []}, {"text": "The links between the two hidden sequences model dependencies between tags and chunks.", "labels": [], "entities": []}, {"text": "Together the hidden sequences generate an observed word sequence, and the task of the tagger/chunker is to invert this process and infer the original tags and chunks.", "labels": [], "entities": []}, {"text": "Previous work on joint tagging/chunking has shown promising results.", "labels": [], "entities": [{"text": "joint tagging/chunking", "start_pos": 17, "end_pos": 39, "type": "TASK", "confidence": 0.7639186531305313}]}, {"text": "For example, Xun et: Baseline FHMM.", "labels": [], "entities": [{"text": "Baseline FHMM", "start_pos": 21, "end_pos": 34, "type": "DATASET", "confidence": 0.7127566635608673}]}, {"text": "The two hidden sequences y 1:t and z 1:t can represent tags and chunks, respectively.", "labels": [], "entities": []}, {"text": "Together they generate x 1:t , the observed word sequence. al.", "labels": [], "entities": []}, {"text": "(2000) uses a POS tagger to output an N-best list of tags, then a Viterbi search to find the chunk sequence that maximizes the joint tag/chunk probability.", "labels": [], "entities": []}, {"text": "extends transformationbased learning tagger to a joint tagger/chunker by modifying the objective function such that a transformation rule is evaluated on the classification of all simultaneous subtasks.", "labels": [], "entities": []}, {"text": "Our work is most similar in spirit to Dynamic Conditional Random Fields (DCRF) (), which also models tagging and chunking in a factorial framework.", "labels": [], "entities": []}, {"text": "Some main differences between our model and DCRF maybe described as 1) directed graphical model vs. undirected graphical model, and 2) generative model vs. conditional model.", "labels": [], "entities": []}, {"text": "The main advantage of FHMM over DCRF is that FHMM requires considerably less computation and exact inference is easily achievable for FHMM and its variants.", "labels": [], "entities": [{"text": "FHMM", "start_pos": 22, "end_pos": 26, "type": "DATASET", "confidence": 0.8326227068901062}, {"text": "FHMM", "start_pos": 45, "end_pos": 49, "type": "DATASET", "confidence": 0.8103822469711304}, {"text": "FHMM", "start_pos": 134, "end_pos": 138, "type": "DATASET", "confidence": 0.8652114868164062}]}, {"text": "The paper is structured as follows: Section 2 describes in detail the FHMM.", "labels": [], "entities": [{"text": "FHMM", "start_pos": 70, "end_pos": 74, "type": "DATASET", "confidence": 0.7004862427711487}]}, {"text": "Section 3 presents anew model, the Switching FHMM, which represents cross-sequence dependencies more effectively than FHMMs.", "labels": [], "entities": [{"text": "Switching", "start_pos": 35, "end_pos": 44, "type": "TASK", "confidence": 0.8802993893623352}, {"text": "FHMM", "start_pos": 45, "end_pos": 49, "type": "DATASET", "confidence": 0.7274807691574097}]}, {"text": "Section 4 discusses the task and data and Section 5 presents various experimental results Section 6 discusses future work and concludes.", "labels": [], "entities": []}], "datasetContent": [{"text": "We report two sets of experiments.", "labels": [], "entities": []}, {"text": "Experiment 1 compares several FHMMs with cascaded HMMs and demonstrates the benefit of joint labeling.", "labels": [], "entities": [{"text": "FHMMs", "start_pos": 30, "end_pos": 35, "type": "DATASET", "confidence": 0.7960796356201172}]}, {"text": "Experiment 2 evaluates the Switching FHMM for various training dataset sizes and shows its robustness against data sparsity.", "labels": [], "entities": [{"text": "Switching FHMM", "start_pos": 27, "end_pos": 41, "type": "TASK", "confidence": 0.7005268037319183}]}, {"text": "All models are implemented using the Graphical Models Toolkit (GMTK) ().", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: POS and NP Accuracy for Cascaded HMM  and FHMM Models.", "labels": [], "entities": [{"text": "POS", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9527693390846252}, {"text": "Accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.8407776951789856}, {"text": "FHMM", "start_pos": 52, "end_pos": 56, "type": "DATASET", "confidence": 0.8604388236999512}]}]}