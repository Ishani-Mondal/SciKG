{"title": [{"text": "A Practical Solution to the Problem of Automatic Part-of-Speech Induction from Text", "labels": [], "entities": [{"text": "Automatic Part-of-Speech Induction from Text", "start_pos": 39, "end_pos": 83, "type": "TASK", "confidence": 0.7566856265068054}]}], "abstractContent": [{"text": "The problem of part-of-speech induction from text involves two aspects: Firstly, a set of word classes is to be derived automatically.", "labels": [], "entities": [{"text": "part-of-speech induction from text", "start_pos": 15, "end_pos": 49, "type": "TASK", "confidence": 0.8076150417327881}]}, {"text": "Secondly, each word of a vocabulary is to be assigned to one or several of these word classes.", "labels": [], "entities": []}, {"text": "In this paper we present a method that solves both problems with good accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 70, "end_pos": 78, "type": "METRIC", "confidence": 0.9975224137306213}]}, {"text": "Our approach adopts a mixture of statistical methods that have been successfully applied in word sense induction.", "labels": [], "entities": [{"text": "word sense induction", "start_pos": 92, "end_pos": 112, "type": "TASK", "confidence": 0.7895151376724243}]}, {"text": "Its main advantage over previous attempts is that it reduces the syntactic space to only the most important dimensions, thereby almost eliminating the otherwise omnipresent problem of data sparseness.", "labels": [], "entities": []}], "introductionContent": [{"text": "Whereas most previous statistical work concerning parts of speech has been on tagging, this paper deals with part-of-speech induction.", "labels": [], "entities": [{"text": "part-of-speech induction", "start_pos": 109, "end_pos": 133, "type": "TASK", "confidence": 0.7486138343811035}]}, {"text": "In part-ofspeech induction two phases can be distinguished: In the first phase a set of word classes is to be derived automatically on the basis of the distribution of the words in a text corpus.", "labels": [], "entities": []}, {"text": "These classes should be in accordance with human intuitions, i.e. common distinctions such as nouns, verbs and adjectives are desirable.", "labels": [], "entities": []}, {"text": "In the second phase, based on its observed usage each word is assigned to one or several of the previously defined classes.", "labels": [], "entities": []}, {"text": "The main reason why part-of-speech induction has received far less attention than part-of-speech tagging is probably that there seemed no urgent need for it as linguists have always considered classifying words as one of their core tasks, and as a consequence accurate lexicons providing such information are readily available for many languages.", "labels": [], "entities": [{"text": "part-of-speech induction", "start_pos": 20, "end_pos": 44, "type": "TASK", "confidence": 0.8035714030265808}, {"text": "part-of-speech tagging", "start_pos": 82, "end_pos": 104, "type": "TASK", "confidence": 0.7152754664421082}]}, {"text": "Nevertheless, deriving word classes automatically is an interesting intellectual challenge with relevance to cognitive science.", "labels": [], "entities": []}, {"text": "Also, advantages of the automatic systems are that they should be more objective and can provide precise information on the likelihood distribution for each of a word's parts of speech, an aspect that is useful for statistical machine translation.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 215, "end_pos": 246, "type": "TASK", "confidence": 0.7308910389741262}]}, {"text": "The pioneering work on class based n-gram models by was motivated by such considerations.", "labels": [], "entities": []}, {"text": "In contrast, by applying a neural network approach put the emphasis on the cognitive side.", "labels": [], "entities": []}, {"text": "More recent work includes who combines distributional and morphological information, and who uses a hidden Marcov model in combination with co-clustering.", "labels": [], "entities": []}, {"text": "Most studies use abstract statistical measures such as perplexity or the F-measure for evaluation.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 73, "end_pos": 82, "type": "METRIC", "confidence": 0.9728118181228638}]}, {"text": "This is good for quantitative comparisons, but makes it difficult to check if the results agree with human intuitions.", "labels": [], "entities": []}, {"text": "In this paper we use a straightforward approach for evaluation.", "labels": [], "entities": []}, {"text": "It involves checking if the automatically generated word classes agree with the word classes known from grammar books, and whether the class assignments for each word are correct.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our results are presented as dendrograms which in contrast to 2-dimensional dot-plots have the advantage of being able to correctly show the true distances between clusters.", "labels": [], "entities": []}, {"text": "The two dendrograms in where both computed by applying the procedure as described in the previous section, with the only difference that in generating the upper dendrogram the SVD-step has been omitted, whereas in generating the lower dendrogram it has been conducted.", "labels": [], "entities": []}, {"text": "Without SVD the expected clusters of verbs, nouns and adjectives are not clearly separated, and the adjectives widely and rural are placed outside the adjective cluster.", "labels": [], "entities": []}, {"text": "With SVD, all 50 words are in their appropriate clusters and the three discovered clusters are much more salient.", "labels": [], "entities": []}, {"text": "Also, widely and rural are well within the adjective cluster.", "labels": [], "entities": []}, {"text": "The comparison of the two dendrograms indicates that the SVD was capable of making appropriate generalizations.", "labels": [], "entities": []}, {"text": "Also, when we look inside each cluster we can see that ambiguous words like suit, drop or brief are somewhat closer to their secondary class than unambiguous words.", "labels": [], "entities": []}, {"text": "Having obtained the three expected clusters, the next investigation concerns the assignment of the ambiguous words to additional clusters.", "labels": [], "entities": []}, {"text": "As described previously, this is done by computing differential vectors, and by assigning these to the most similar other cluster.", "labels": [], "entities": []}, {"text": "Hereby for the cosine similarity we set a threshold of 0.8.", "labels": [], "entities": []}, {"text": "That is, only if the similarity between the differential vector and its closest centroid was higher than 0.8 we assigned the word to this cluster and continued to compute differential vectors.", "labels": [], "entities": []}, {"text": "Otherwise we assumed that the differential vector was caused by sampling errors and aborted the process of searching for additional class assignments.", "labels": [], "entities": []}, {"text": "The results from this procedure are shown in table 2 where for each of the 50 words all computed classes are given in the order as they were obtained by the algorithm, i.e. the dominant assignments are listed first.", "labels": [], "entities": []}, {"text": "Although our algorithm does not name the classes, for simplicity we interpret them in the obvious way, i.e. as nouns, verbs and adjectives.", "labels": [], "entities": []}, {"text": "A comparison with WordNet 2.0 choices is given in brackets.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 18, "end_pos": 25, "type": "DATASET", "confidence": 0.9240404367446899}]}, {"text": "For example, +N means that WordNet lists the additional assignment noun, and -A indicates that the assignment adjective found by the algorithm is not listed in WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 27, "end_pos": 34, "type": "DATASET", "confidence": 0.9243425130844116}, {"text": "WordNet", "start_pos": 160, "end_pos": 167, "type": "DATASET", "confidence": 0.9674094915390015}]}, {"text": "According to this comparison, for all 50 words the first reading is correct.", "labels": [], "entities": []}, {"text": "For 16 words an additional second reading was computed which is correct in 11 cases.", "labels": [], "entities": []}, {"text": "16 of the WordNet assignments are missing, among them the verb readings for reform, suit, and rain and the noun reading for serve.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 10, "end_pos": 17, "type": "DATASET", "confidence": 0.9271286129951477}]}, {"text": "However, as many of the WordNet assignments seem rare, it is not clear in how far the omissions can be attributed to shortcomings of the algorithm.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. Co-occurrence matrix of adjacent words.", "labels": [], "entities": []}]}