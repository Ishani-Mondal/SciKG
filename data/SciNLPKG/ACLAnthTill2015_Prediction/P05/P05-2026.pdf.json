{"title": [], "abstractContent": [{"text": "We present a search-based approach to automatic surface realization given a corpus of domain sentences.", "labels": [], "entities": [{"text": "automatic surface realization", "start_pos": 38, "end_pos": 67, "type": "TASK", "confidence": 0.679173469543457}]}, {"text": "Using heuris-tic search based on a statistical language model and a structure we introduce called an inheritance table we overgenerate a set of complete syntactic-semantic trees that are consistent with the given semantic structure and have high likelihood relative to the language model.", "labels": [], "entities": []}, {"text": "These trees are then lexicalized, linearized, scored, and ranked.", "labels": [], "entities": []}, {"text": "This model is being developed to generate real-time navigation instructions.", "labels": [], "entities": []}], "introductionContent": [{"text": "The target application for this work is real-time, interactive navigation instructions.", "labels": [], "entities": []}, {"text": "Good directiongivers respond actively to a driver's actions and questions, and express instructions relative to a large variety of landmarks, times, and distances.", "labels": [], "entities": []}, {"text": "These traits require robust, real-time natural language generation.", "labels": [], "entities": []}, {"text": "This can be broken into three steps: (1) generating a route plan, (2) reasoning about the route and the user to produce an abstract representation of individual instructions, and (3) realizing these instructions as sentences in natural language (in our case, English).", "labels": [], "entities": []}, {"text": "We focus on the last of these steps: given a structure that represents the semantic content of a sentence, we want to produce an English sentence that expresses this content.", "labels": [], "entities": []}, {"text": "According to the traditional division of content determination, sentence planning, and surface realization, our work is primarily concerned with surface realization, but also includes aspects of sentence planning.", "labels": [], "entities": [{"text": "content determination", "start_pos": 41, "end_pos": 62, "type": "TASK", "confidence": 0.7139749079942703}, {"text": "sentence planning", "start_pos": 64, "end_pos": 81, "type": "TASK", "confidence": 0.7117854952812195}, {"text": "surface realization", "start_pos": 87, "end_pos": 106, "type": "TASK", "confidence": 0.798760175704956}, {"text": "sentence planning", "start_pos": 195, "end_pos": 212, "type": "TASK", "confidence": 0.7140148431062698}]}, {"text": "Our application requires robust flexibility within a restricted domain that is not well represented in the traditional corpora or tools.", "labels": [], "entities": []}, {"text": "These requirements suggest using trainable stochastic generation.", "labels": [], "entities": []}, {"text": "A number of statistical surface realizers have been described, notably the FERGUS) and HALogen systems), as well as experiments in).", "labels": [], "entities": [{"text": "statistical surface realizers", "start_pos": 12, "end_pos": 41, "type": "TASK", "confidence": 0.6215982437133789}, {"text": "FERGUS", "start_pos": 75, "end_pos": 81, "type": "METRIC", "confidence": 0.9617788195610046}]}, {"text": "FERGUS (Flexible Empiricist/Rationalist Generation Using Syntax) takes as input a dependency tree whose nodes are marked with lexemes only.", "labels": [], "entities": [{"text": "FERGUS", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9577642679214478}]}, {"text": "The generator automatically \"supertags\" each input node with a TAG tree, then produces a lattice of all possible linearizations consistent with the supertagged dependency tree.", "labels": [], "entities": []}, {"text": "Finally it selects the most likely traversal of this lattice, conditioned on a domain-trained language model.", "labels": [], "entities": []}, {"text": "The HALogen system is a broad-coverage generator that uses a combination of statistical and symbolic techniques.", "labels": [], "entities": []}, {"text": "The input, a structure of featurevalue pairs (see Section 3.1), is symbolically transformed into a forest of possible expressions, which are then ranked using a corpus-trained statistical language model.", "labels": [], "entities": []}, {"text": "Ratnaparkhi also uses an overgeneration approach, using search to generate candidate sentences which are then scored and ranked.", "labels": [], "entities": []}, {"text": "His paper outlines experiments with an n-gram model, a trained dependency grammar, and finally a handbuilt grammar including content-driven conditions for applying rules.", "labels": [], "entities": []}, {"text": "The last of these systems outperformed the n-gram and trained grammar in testing based on human judgments.", "labels": [], "entities": []}, {"text": "The basic idea of our system fits in the overgenerate-and-rank paradigm.", "labels": [], "entities": []}, {"text": "Our approach is partly motivated by the idea of 'softening' Ratnaparkhi's third system, replacing the hand-built grammar rules with a combination of a trained statistical language model and a structure called an inheritance table, which captures long-run dependency information.", "labels": [], "entities": []}, {"text": "This allows us to overgenerate based on rules that are sensitive to structured content without incurring the cost of designing such rules by hand.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}