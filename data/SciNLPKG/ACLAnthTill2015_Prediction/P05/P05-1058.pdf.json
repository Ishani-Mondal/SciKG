{"title": [{"text": "Alignment Model Adaptation for Domain-Specific Word Alignment", "labels": [], "entities": [{"text": "Alignment Model Adaptation", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.847140351931254}, {"text": "Domain-Specific Word Alignment", "start_pos": 31, "end_pos": 61, "type": "TASK", "confidence": 0.6327458719412485}]}], "abstractContent": [{"text": "This paper proposes an alignment adaptation approach to improve domain-specific (in-domain) word alignment.", "labels": [], "entities": [{"text": "alignment adaptation", "start_pos": 23, "end_pos": 43, "type": "TASK", "confidence": 0.925255686044693}, {"text": "domain-specific (in-domain) word alignment", "start_pos": 64, "end_pos": 106, "type": "TASK", "confidence": 0.655574748913447}]}, {"text": "The basic idea of alignment adaptation is to use out-of-domain corpus to improve in-domain word alignment results.", "labels": [], "entities": [{"text": "alignment adaptation", "start_pos": 18, "end_pos": 38, "type": "TASK", "confidence": 0.9697410762310028}]}, {"text": "In this paper, we first train two statistical word alignment models with the large-scale out-of-domain corpus and the small-scale in-domain corpus respectively, and then interpolate these two models to improve the domain-specific word alignment.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 46, "end_pos": 60, "type": "TASK", "confidence": 0.6793297529220581}, {"text": "domain-specific word alignment", "start_pos": 214, "end_pos": 244, "type": "TASK", "confidence": 0.6199385126431783}]}, {"text": "Experimental results show that our approach improves domain-specific word alignment in terms of both precision and recall, achieving a relative error rate reduction of 6.56% as compared with the state-of-the-art technologies.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 69, "end_pos": 83, "type": "TASK", "confidence": 0.686419889330864}, {"text": "precision", "start_pos": 101, "end_pos": 110, "type": "METRIC", "confidence": 0.9987174272537231}, {"text": "recall", "start_pos": 115, "end_pos": 121, "type": "METRIC", "confidence": 0.9977497458457947}, {"text": "error rate reduction", "start_pos": 144, "end_pos": 164, "type": "METRIC", "confidence": 0.9451264142990112}]}], "introductionContent": [{"text": "Word alignment was first proposed as an intermediate result of statistical machine translation (.", "labels": [], "entities": [{"text": "Word alignment", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7669537961483002}, {"text": "statistical machine translation", "start_pos": 63, "end_pos": 94, "type": "TASK", "confidence": 0.6187534133593241}]}, {"text": "In recent years, many researchers have employed statistical models ( or association measures () to build alignment links.", "labels": [], "entities": []}, {"text": "In order to achieve satisfactory results, all of these methods require a large-scale bilingual corpus for training.", "labels": [], "entities": []}, {"text": "When the large-scale bilingual corpus is not available, some researchers use existing dictionaries to improve word alignment.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 110, "end_pos": 124, "type": "TASK", "confidence": 0.7640053331851959}]}, {"text": "However, only a few studies () directly address the problem of domain-specific word alignment when neither the large-scale domain-specific bilingual corpus nor the domain-specific translation dictionary is available.", "labels": [], "entities": [{"text": "domain-specific word alignment", "start_pos": 63, "end_pos": 93, "type": "TASK", "confidence": 0.6387997368971506}]}, {"text": "In this paper, we address the problem of word alignment in a specific domain, in which only a small-scale corpus is available.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 41, "end_pos": 55, "type": "TASK", "confidence": 0.7926374077796936}]}, {"text": "In the domain-specific (in-domain) corpus, there are two kinds of words: general words, which also frequently occur in the out-of-domain corpus, and domain-specific words, which only occur in the specific domain.", "labels": [], "entities": []}, {"text": "Thus, we can use the out-of-domain bilingual corpus to improve the alignment for general words and use the in-domain bilingual corpus for domain-specific words.", "labels": [], "entities": []}, {"text": "We implement this by using alignment model adaptation.", "labels": [], "entities": [{"text": "alignment model adaptation", "start_pos": 27, "end_pos": 53, "type": "TASK", "confidence": 0.8449791471163431}]}, {"text": "Although the adaptation technology is widely used for other tasks such as language modeling (, only a few studies, to the best of our knowledge, directly address word alignment adaptation.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 74, "end_pos": 91, "type": "TASK", "confidence": 0.7544557750225067}, {"text": "word alignment adaptation", "start_pos": 162, "end_pos": 187, "type": "TASK", "confidence": 0.8459988633791605}]}, {"text": "adapted the alignment results obtained with the out-of-domain corpus to the results obtained with the in-domain corpus.", "labels": [], "entities": []}, {"text": "This method first trained two models and two translation dictionaries with the in-domain corpus and the out-of-domain corpus, respectively.", "labels": [], "entities": []}, {"text": "Then these two models were applied to the in-domain corpus to get different results.", "labels": [], "entities": []}, {"text": "The trained translation dictionaries were used to select alignment links from these different results.", "labels": [], "entities": []}, {"text": "Thus, this method performed adaptation through result combination.", "labels": [], "entities": []}, {"text": "The experimental results showed a significant error rate reduction as compared with the method directly combining the two corpora as training data.", "labels": [], "entities": [{"text": "error rate", "start_pos": 46, "end_pos": 56, "type": "METRIC", "confidence": 0.977619081735611}]}, {"text": "In this paper, we improve domain-specific word alignment through statistical alignment model adaptation instead of result adaptation.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 42, "end_pos": 56, "type": "TASK", "confidence": 0.6746535748243332}, {"text": "statistical alignment model adaptation", "start_pos": 65, "end_pos": 103, "type": "TASK", "confidence": 0.5849289521574974}]}, {"text": "Our method includes the following steps: (1) two word alignment models are trained using a small-scale in-domain bilingual corpus and a large-scale out-of-domain bilingual corpus, respectively.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 49, "end_pos": 63, "type": "TASK", "confidence": 0.7058553844690323}]}, {"text": "(2) A new alignment model is built by interpolating the two trained models.", "labels": [], "entities": []}, {"text": "(3) A translation dictionary is also built by interpolating the two dictionaries that are trained from the two training corpora.", "labels": [], "entities": []}, {"text": "(4) The new alignment model and the translation dictionary are employed to improve domain-specific word alignment results.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 99, "end_pos": 113, "type": "TASK", "confidence": 0.6867535561323166}]}, {"text": "Experimental results show that our approach improves domain-specific word alignment in terms of both precision and recall, achieving a relative error rate reduction of 6.56% as compared with the state-of-the-art technologies.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 69, "end_pos": 83, "type": "TASK", "confidence": 0.6864199489355087}, {"text": "precision", "start_pos": 101, "end_pos": 110, "type": "METRIC", "confidence": 0.9987174272537231}, {"text": "recall", "start_pos": 115, "end_pos": 121, "type": "METRIC", "confidence": 0.9977497458457947}, {"text": "error rate reduction", "start_pos": 144, "end_pos": 164, "type": "METRIC", "confidence": 0.9451264142990112}]}, {"text": "The remainder of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 introduces the statistical word alignment model.", "labels": [], "entities": [{"text": "statistical word alignment", "start_pos": 25, "end_pos": 51, "type": "TASK", "confidence": 0.6653483112653097}]}, {"text": "Section 3 describes our alignment model adaptation method.", "labels": [], "entities": [{"text": "alignment model adaptation", "start_pos": 24, "end_pos": 50, "type": "TASK", "confidence": 0.8748611013094584}]}, {"text": "Section 4 describes the method used to build the translation dictionary.", "labels": [], "entities": []}, {"text": "Section 5 describes the model adaptation algorithm.", "labels": [], "entities": [{"text": "model adaptation", "start_pos": 24, "end_pos": 40, "type": "TASK", "confidence": 0.7208047956228256}]}, {"text": "Section 6 presents the evaluation results.", "labels": [], "entities": []}, {"text": "The last section concludes our approach.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use the same evaluation metrics as described in ().", "labels": [], "entities": []}, {"text": "If we use to represent the set of alignment links identified by the proposed methods and to denote the reference alignment set, the methods to calculate the precision, recall, f-measure, and alignment error rate (AER) are shown in Equation,,, and (16).", "labels": [], "entities": [{"text": "precision", "start_pos": 157, "end_pos": 166, "type": "METRIC", "confidence": 0.9996358156204224}, {"text": "recall", "start_pos": 168, "end_pos": 174, "type": "METRIC", "confidence": 0.9962779879570007}, {"text": "f-measure", "start_pos": 176, "end_pos": 185, "type": "METRIC", "confidence": 0.9049447774887085}, {"text": "alignment error rate (AER)", "start_pos": 191, "end_pos": 217, "type": "METRIC", "confidence": 0.9292434453964233}, {"text": "Equation", "start_pos": 231, "end_pos": 239, "type": "METRIC", "confidence": 0.9298930764198303}]}, {"text": "It can be seen that the higher the f-measure is, the lower the alignment error rate is.", "labels": [], "entities": [{"text": "alignment error rate", "start_pos": 63, "end_pos": 83, "type": "METRIC", "confidence": 0.892730712890625}]}, {"text": "Thus, we will only show precision, recall and AER scores in the evaluation results.", "labels": [], "entities": [{"text": "precision", "start_pos": 24, "end_pos": 33, "type": "METRIC", "confidence": 0.9997468590736389}, {"text": "recall", "start_pos": 35, "end_pos": 41, "type": "METRIC", "confidence": 0.9992197751998901}, {"text": "AER", "start_pos": 46, "end_pos": 49, "type": "METRIC", "confidence": 0.9994314312934875}]}, {"text": "We use the held-out set described in section 6.1 to set the interpolation weights.", "labels": [], "entities": []}, {"text": "The coefficient \u03b1 in Equation is set to 0.8, the interpolation weight in Equation is set to 0.1, the interpolation weight in model 3 in Equation is set to 0.1, and the interpolation weight in model 4 is set to 1.", "labels": [], "entities": [{"text": "Equation", "start_pos": 21, "end_pos": 29, "type": "DATASET", "confidence": 0.8650649785995483}, {"text": "Equation", "start_pos": 73, "end_pos": 81, "type": "DATASET", "confidence": 0.8656765222549438}]}, {"text": "In addition, log-likelihood ratio score thresholds are set to and . With these parameters, we get the lowest alignment error rate on the held-out set.", "labels": [], "entities": [{"text": "alignment error rate", "start_pos": 109, "end_pos": 129, "type": "METRIC", "confidence": 0.8632980187733968}]}, {"text": "Using these parameters, we build two adaptation models and a translation dictionary on the training data, which are applied to the testing set.", "labels": [], "entities": []}, {"text": "The evaluation results on our testing set are shown in.", "labels": [], "entities": []}, {"text": "From the results, it can be seen that our approach performs the best among all of the methods, achieving the lowest alignment error rate.", "labels": [], "entities": [{"text": "alignment error rate", "start_pos": 116, "end_pos": 136, "type": "METRIC", "confidence": 0.7802318334579468}]}, {"text": "Compared with the method \"ResAdapt\", our method achieves a higher precision without loss of recall, resulting in an error rate reduction of 6.56%.", "labels": [], "entities": [{"text": "precision", "start_pos": 66, "end_pos": 75, "type": "METRIC", "confidence": 0.9991827607154846}, {"text": "recall", "start_pos": 92, "end_pos": 98, "type": "METRIC", "confidence": 0.9974900484085083}, {"text": "error rate reduction", "start_pos": 116, "end_pos": 136, "type": "METRIC", "confidence": 0.9829545219739279}]}, {"text": "Compared with the method \"Gen+Spec\", our method gets a higher recall, resulting in an error rate reduction of 17.43%.", "labels": [], "entities": [{"text": "recall", "start_pos": 62, "end_pos": 68, "type": "METRIC", "confidence": 0.9997090697288513}, {"text": "error rate reduction", "start_pos": 86, "end_pos": 106, "type": "METRIC", "confidence": 0.9865753054618835}]}, {"text": "This indicates that our model adaptation method is very effective to alleviate the data-sparseness problem of domain-specific word alignment.", "labels": [], "entities": [{"text": "domain-specific word alignment", "start_pos": 110, "end_pos": 140, "type": "TASK", "confidence": 0.6322866380214691}]}, {"text": "The method that only uses the large-scale out-of-domain corpus as training data does not produce good result.", "labels": [], "entities": []}, {"text": "The alignment error rate is almost the same as that of the method only using the in-domain corpus.", "labels": [], "entities": [{"text": "alignment error rate", "start_pos": 4, "end_pos": 24, "type": "METRIC", "confidence": 0.8011317451794943}]}, {"text": "In order to further analyze the result, we classify the alignment links into two classes: single word alignment links (SWA) and multiword alignment links (MWA).", "labels": [], "entities": []}, {"text": "Single word alignment links only include one-to-one alignments.", "labels": [], "entities": []}, {"text": "The multiword alignment links include those links in which there are multiword units in the source language or/and the target language.", "labels": [], "entities": [{"text": "multiword alignment", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.7556412518024445}]}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "From the results, it can be seen that the method \"Spec\" produces better results for multiword alignment while the method \"Gen\" produces better results for single word alignment.", "labels": [], "entities": [{"text": "multiword alignment", "start_pos": 84, "end_pos": 103, "type": "TASK", "confidence": 0.7896056771278381}, {"text": "single word alignment", "start_pos": 155, "end_pos": 176, "type": "TASK", "confidence": 0.6955711642901102}]}, {"text": "This indicates that the multiword alignment links mainly include the domain-specific words.", "labels": [], "entities": []}, {"text": "Among the 504 multiword alignment links, about 60% of the links include domain-specific words.", "labels": [], "entities": []}, {"text": "In, we also present the results of our method.", "labels": [], "entities": []}, {"text": "Our method achieves the lowest error rate results on both single word alignment and multiword alignment.", "labels": [], "entities": [{"text": "error rate", "start_pos": 31, "end_pos": 41, "type": "METRIC", "confidence": 0.9608972072601318}, {"text": "single word alignment", "start_pos": 58, "end_pos": 79, "type": "TASK", "confidence": 0.6538108189900717}, {"text": "multiword alignment", "start_pos": 84, "end_pos": 103, "type": "TASK", "confidence": 0.7429182529449463}]}], "tableCaptions": [{"text": " Table 1. From the results, it can be seen  that our approach performs the best among all of  the methods, achieving the lowest alignment error  rate. Compared with the method \"ResAdapt\", our  method achieves a higher precision without loss of  recall, resulting in an error rate reduction of 6.56%.  Compared with the method \"Gen+Spec\", our  method gets a higher recall, resulting in an error  rate reduction of 17.43%. This indicates that our  model adaptation method is very effective to  alleviate the data-sparseness problem of  domain-specific word alignment.", "labels": [], "entities": [{"text": "alignment error  rate", "start_pos": 128, "end_pos": 149, "type": "METRIC", "confidence": 0.8207559982935587}, {"text": "precision", "start_pos": 218, "end_pos": 227, "type": "METRIC", "confidence": 0.9938620328903198}, {"text": "recall", "start_pos": 245, "end_pos": 251, "type": "METRIC", "confidence": 0.9860532879829407}, {"text": "error rate reduction", "start_pos": 269, "end_pos": 289, "type": "METRIC", "confidence": 0.8902745445569357}, {"text": "recall", "start_pos": 364, "end_pos": 370, "type": "METRIC", "confidence": 0.998621940612793}, {"text": "word alignment", "start_pos": 550, "end_pos": 564, "type": "TASK", "confidence": 0.6933800429105759}]}, {"text": " Table 1. Word Alignment Adaptation Results", "labels": [], "entities": [{"text": "Word Alignment Adaptation", "start_pos": 10, "end_pos": 35, "type": "TASK", "confidence": 0.8170469502607981}]}, {"text": " Table 2. From the results,  it can be seen that the method \"Spec\" produces  better results for multiword alignment while the  method \"Gen\" produces better results for single  word alignment. This indicates that the multiword  alignment links mainly include the domain-specific  words. Among the 504 multiword alignment links,  about 60% of the links include domain-specific  words. In", "labels": [], "entities": [{"text": "multiword alignment", "start_pos": 96, "end_pos": 115, "type": "TASK", "confidence": 0.8491961359977722}]}, {"text": " Table 2. Single Word and Multiword Alignment  Results", "labels": [], "entities": [{"text": "Multiword Alignment", "start_pos": 26, "end_pos": 45, "type": "TASK", "confidence": 0.67313551902771}]}, {"text": " Table 3. Alignment Adaptation Results Using a  Smaller In-Domain Corpus", "labels": [], "entities": [{"text": "Alignment Adaptation", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.981976330280304}]}, {"text": " Table 4. Alignment Adaptation Results Using  In-Domain Corpora of Different Sizes", "labels": [], "entities": [{"text": "Alignment Adaptation", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.9866372346878052}]}, {"text": " Table 6. Adaptation Alignment Results Using  Out-of-Domain Corpora of Different Sizes", "labels": [], "entities": [{"text": "Adaptation Alignment", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.9664260745048523}]}]}