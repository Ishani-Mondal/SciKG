{"title": [{"text": "The Distributional Inclusion Hypotheses and Lexical Entailment", "labels": [], "entities": [{"text": "Distributional Inclusion Hypotheses", "start_pos": 4, "end_pos": 39, "type": "TASK", "confidence": 0.9587355057398478}, {"text": "Lexical Entailment", "start_pos": 44, "end_pos": 62, "type": "TASK", "confidence": 0.8477780222892761}]}], "abstractContent": [{"text": "This paper suggests refinements for the Distributional Similarity Hypothesis.", "labels": [], "entities": [{"text": "Distributional Similarity Hypothesis", "start_pos": 40, "end_pos": 76, "type": "TASK", "confidence": 0.8115036686261495}]}, {"text": "Our proposed hypotheses relate the distribu-tional behavior of pairs of words to lexical entailment-a tighter notion of semantic similarity that is required by many NLP applications.", "labels": [], "entities": []}, {"text": "To automatically explore the validity of the defined hypotheses we developed an inclusion testing algorithm for characteristic features of two words, which incorporates corpus and web-based feature sampling to overcome data sparseness.", "labels": [], "entities": []}, {"text": "The degree of hypotheses validity was then empirically tested and manually analyzed with respect to the word sense level.", "labels": [], "entities": [{"text": "validity", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9538077116012573}]}, {"text": "In addition, the above testing algorithm was exploited to improve lexical entailment acquisition.", "labels": [], "entities": [{"text": "lexical entailment acquisition", "start_pos": 66, "end_pos": 96, "type": "TASK", "confidence": 0.7133242587248484}]}], "introductionContent": [{"text": "Distributional Similarity between words has been an active research area for more than a decade.", "labels": [], "entities": [{"text": "Distributional Similarity between words", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.8175293654203415}]}, {"text": "It is based on the general idea of Harris' Distributional Hypothesis, suggesting that words that occur within similar contexts are semantically similar.", "labels": [], "entities": []}, {"text": "Concrete similarity measures compare a pair of weighted context feature vectors that characterize two words.", "labels": [], "entities": []}, {"text": "As it turns out, distributional similarity captures a somewhat loose notion of semantic similarity (see).", "labels": [], "entities": []}, {"text": "It does not ensure that the meaning of one word is preserved when replacing it with the other one in some context.", "labels": [], "entities": []}, {"text": "However, many semantic information-oriented applications like Question Answering, Information Extraction and Paraphrase Acquisition require a tighter similarity criterion, as was also demonstrated by papers at the recent PASCAL Challenge on Recognizing Textual Entailment ().", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 62, "end_pos": 80, "type": "TASK", "confidence": 0.8553138077259064}, {"text": "Information Extraction", "start_pos": 82, "end_pos": 104, "type": "TASK", "confidence": 0.7442719340324402}, {"text": "Paraphrase Acquisition", "start_pos": 109, "end_pos": 131, "type": "TASK", "confidence": 0.8612801730632782}, {"text": "PASCAL Challenge on Recognizing Textual Entailment", "start_pos": 221, "end_pos": 271, "type": "TASK", "confidence": 0.6000983367363611}]}, {"text": "In particular, all these applications need to know when the meaning of one word can be inferred (entailed) from another word, so that one word could substitute the other in some contexts.", "labels": [], "entities": []}, {"text": "This relation corresponds to several lexical semantic relations, such as synonymy, hyponymy and some cases of meronymy.", "labels": [], "entities": []}, {"text": "For example, in Question Answering, the word company in a question can be substituted in the text by firm (synonym), automaker (hyponym) or division (meronym).", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 16, "end_pos": 34, "type": "TASK", "confidence": 0.7413607835769653}]}, {"text": "Unfortunately, existing manually constructed resources of lexical semantic relations, such as WordNet, are not exhaustive and comprehensive enough fora variety of domains and thus are not sufficient as a sole resource for application needs . Most works that attempt to learn such concrete lexical semantic relations employ a co-occurrence pattern-based approach).", "labels": [], "entities": [{"text": "WordNet", "start_pos": 94, "end_pos": 101, "type": "DATASET", "confidence": 0.9570904970169067}]}, {"text": "Typically, they use a set of predefined lexicosyntactic patterns that characterize specific semantic relations.", "labels": [], "entities": []}, {"text": "If a candidate word pair (like company-automaker) co-occurs within the same sentence satisfying a concrete pattern (like \" \u2026companies, such as automakers\"), then it is expected that the corresponding semantic relation holds between these words (hypernym-hyponym in this example).", "labels": [], "entities": []}, {"text": "In recent work) we explored the correspondence between the distributional characterization of two words (which may hardly co-occur, as is usually the case for syno-nyms) and the kind of tight semantic relationship that might hold between them.", "labels": [], "entities": []}, {"text": "We formulated a lexical entailment relation that corresponds to the above mentioned substitutability criterion, and is termed meaning entailing substitutability (which we term here for brevity as lexical entailment).", "labels": [], "entities": []}, {"text": "Given a pair of words, this relation holds if there are some contexts in which one of the words can be substituted by the other, such that the meaning of the original word can be inferred from the new one.", "labels": [], "entities": []}, {"text": "We then proposed anew feature weighting function (RFF) that yields more accurate distributional similarity lists, which better approximate the lexical entailment relation.", "labels": [], "entities": []}, {"text": "Yet, this method still applies a standard measure for distributional vector similarity (over vectors with the improved feature weights), and thus produces many loose similarities that do not correspond to entailment.", "labels": [], "entities": []}, {"text": "This paper explores more deeply the relationship between distributional characterization of words and lexical entailment, proposing two new hypotheses as a refinement of the distributional similarity hypothesis.", "labels": [], "entities": [{"text": "distributional characterization of words", "start_pos": 57, "end_pos": 97, "type": "TASK", "confidence": 0.8087801188230515}]}, {"text": "The main idea is that if one word entails the other then we would expect that virtually all the characteristic context features of the entailing word will actually occur also with the entailed word.", "labels": [], "entities": []}, {"text": "To test this idea we developed an automatic method for testing feature inclusion between a pair of words.", "labels": [], "entities": [{"text": "testing feature inclusion between a pair of words", "start_pos": 55, "end_pos": 104, "type": "TASK", "confidence": 0.7881117537617683}]}, {"text": "This algorithm combines corpus statistics with a web-based feature sampling technique.", "labels": [], "entities": []}, {"text": "The web is utilized to overcome the data sparseness problem, so that features which are not found with one of the two words can be considered as truly distinguishing evidence.", "labels": [], "entities": []}, {"text": "Using the above algorithm we first tested the empirical validity of the hypotheses.", "labels": [], "entities": []}, {"text": "Then, we demonstrated how the hypotheses can be leveraged in practice to improve the precision of automatic acquisition of the entailment relation.", "labels": [], "entities": [{"text": "precision", "start_pos": 85, "end_pos": 94, "type": "METRIC", "confidence": 0.9990154504776001}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Distribution of 400 entailing/non- entailing ordered pairs that hold/do not hold  feature inclusion at the word level.", "labels": [], "entities": []}]}