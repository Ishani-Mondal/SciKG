{"title": [{"text": "Joint Learning Improves Semantic Role Labeling", "labels": [], "entities": [{"text": "Joint Learning Improves Semantic Role Labeling", "start_pos": 0, "end_pos": 46, "type": "TASK", "confidence": 0.7846881548563639}]}], "abstractContent": [{"text": "Despite much recent progress on accurate semantic role labeling, previous work has largely used independent classifiers, possibly combined with separate label sequence models via Viterbi decoding.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 41, "end_pos": 63, "type": "TASK", "confidence": 0.6029405295848846}]}, {"text": "This stands in stark contrast to the linguistic observation that a core argument frame is a joint structure, with strong dependencies between arguments.", "labels": [], "entities": []}, {"text": "We show how to build a joint model of argument frames, incorporating novel features that model these interactions into discriminative log-linear models.", "labels": [], "entities": []}, {"text": "This system achieves an error reduction of 22% on all arguments and 32% on core arguments over a state-of-the art independent classifier for gold-standard parse trees on PropBank.", "labels": [], "entities": [{"text": "error reduction", "start_pos": 24, "end_pos": 39, "type": "METRIC", "confidence": 0.9701530933380127}, {"text": "PropBank", "start_pos": 170, "end_pos": 178, "type": "DATASET", "confidence": 0.968985378742218}]}], "introductionContent": [{"text": "The release of semantically annotated corpora such as FrameNet () and PropBank () has made it possible to develop high-accuracy statistical models for automated semantic role labeling ().", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 161, "end_pos": 183, "type": "TASK", "confidence": 0.6086960633595785}]}, {"text": "Such systems have identified several linguistically motivated features for discriminating arguments and their labels (see).", "labels": [], "entities": []}, {"text": "These features usually characterize aspects of individual arguments and the predicate.", "labels": [], "entities": []}, {"text": "It is evident that the labels and the features of arguments are highly correlated.", "labels": [], "entities": []}, {"text": "For example, there are hard constraints -that arguments cannot overlap with each other or the predicate, and also soft constraints -for example, is it unlikely that a predicate will have two or more AGENT arguments, or that a predicate used in the active voice will have a THEME argument prior to an AGENT argument.", "labels": [], "entities": [{"text": "THEME", "start_pos": 273, "end_pos": 278, "type": "METRIC", "confidence": 0.9246973991394043}]}, {"text": "Several systems have incorporated such dependencies, for example, ( and several systems submitted in the CoNLL-2004 shared task).", "labels": [], "entities": []}, {"text": "However, we show that there are greater gains to be had by modeling joint information about a verb's argument structure.", "labels": [], "entities": []}, {"text": "We propose a discriminative log-linear joint model for semantic role labeling, which incorporates more global features and achieves superior performance in comparison to state-of-the-art models.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 55, "end_pos": 77, "type": "TASK", "confidence": 0.8144696950912476}]}, {"text": "To deal with the computational complexity of the task, we employ dynamic programming and reranking approaches.", "labels": [], "entities": []}, {"text": "We present performance results on the February 2004 version of PropBank on gold-standard parse trees as well as results on automatic parses generated by Charniak's parser).", "labels": [], "entities": [{"text": "PropBank", "start_pos": 63, "end_pos": 71, "type": "DATASET", "confidence": 0.7025192975997925}]}], "datasetContent": [{"text": "For our experiments we used the February 2004 release of PropBank.", "labels": [], "entities": [{"text": "PropBank", "start_pos": 57, "end_pos": 65, "type": "DATASET", "confidence": 0.7867376804351807}]}, {"text": "3 As is standard, we used the annotations from sections 02-21 for training, 24 for development, and 23 for testing.", "labels": [], "entities": []}, {"text": "As is done in some previous work on semantic role labeling, we discard the relatively infrequent discontinuous arguments from both the training and test sets.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 36, "end_pos": 58, "type": "TASK", "confidence": 0.7537692189216614}]}, {"text": "In addition to reporting the standard results on individual argument F-Measure, we also report Frame Accuracy (Acc.), the fraction of sentences for which we successfully label all nodes.", "labels": [], "entities": [{"text": "Frame Accuracy (Acc.)", "start_pos": 95, "end_pos": 116, "type": "METRIC", "confidence": 0.9205094099044799}]}, {"text": "There are reasons to prefer Frame Accuracy as a measure of performance over individual-argument statistics.", "labels": [], "entities": [{"text": "Frame Accuracy", "start_pos": 28, "end_pos": 42, "type": "METRIC", "confidence": 0.5365588665008545}]}, {"text": "Foremost, potential applications of role labeling may require correct labeling of all (or at least the core) arguments in a sentence in order to be effective, and partially correct labelings may not be very useful.", "labels": [], "entities": [{"text": "role labeling", "start_pos": 36, "end_pos": 49, "type": "TASK", "confidence": 0.7368643879890442}]}, {"text": "We report results for two variations of the semantic role labeling task.", "labels": [], "entities": [{"text": "semantic role labeling task", "start_pos": 44, "end_pos": 71, "type": "TASK", "confidence": 0.7157260030508041}]}, {"text": "For CORE, we identify and label only core arguments.", "labels": [], "entities": [{"text": "CORE", "start_pos": 4, "end_pos": 8, "type": "TASK", "confidence": 0.625784158706665}]}, {"text": "For ARGM, we identify and label core as well as modifier arguments.", "labels": [], "entities": []}, {"text": "We report results for local and joint models on argument identification, argument classification, and the complete identification and classification pipeline.", "labels": [], "entities": [{"text": "argument identification", "start_pos": 48, "end_pos": 71, "type": "TASK", "confidence": 0.7292162179946899}, {"text": "argument classification", "start_pos": 73, "end_pos": 96, "type": "TASK", "confidence": 0.7435064613819122}, {"text": "complete identification and classification", "start_pos": 106, "end_pos": 148, "type": "TASK", "confidence": 0.7293858528137207}]}, {"text": "Our local models use the features listed in and the technique for enforcing the non-overlapping constraint discussed in Section 3.1.", "labels": [], "entities": []}, {"text": "The labeling of the tree in is a specific example of the kind of errors fixed by the joint models.", "labels": [], "entities": []}, {"text": "The local classifier labeled the first argument in the tree as ARG0 instead of ARG1, probably because an ARG0 label is more likely for the subject position.", "labels": [], "entities": [{"text": "ARG0", "start_pos": 63, "end_pos": 67, "type": "DATASET", "confidence": 0.7765617370605469}]}, {"text": "All joint models for these experiments used the whole sequence and frame features.", "labels": [], "entities": []}, {"text": "As can be seen from, our joint models achieve error reductions of 32% and 22% over our local models in FMeasure on CORE and ARGM respectively.", "labels": [], "entities": [{"text": "error reductions", "start_pos": 46, "end_pos": 62, "type": "METRIC", "confidence": 0.9757132530212402}, {"text": "CORE", "start_pos": 115, "end_pos": 119, "type": "DATASET", "confidence": 0.8310690522193909}, {"text": "ARGM", "start_pos": 124, "end_pos": 128, "type": "DATASET", "confidence": 0.7016895413398743}]}, {"text": "With respect to the Frame Accuracy metric, the joint error reduction is 38% and 26% for CORE and ARGM respectively.", "labels": [], "entities": [{"text": "Frame Accuracy metric", "start_pos": 20, "end_pos": 41, "type": "METRIC", "confidence": 0.65337602297465}, {"text": "joint error reduction", "start_pos": 47, "end_pos": 68, "type": "METRIC", "confidence": 0.9224902987480164}, {"text": "CORE", "start_pos": 88, "end_pos": 92, "type": "METRIC", "confidence": 0.4718419015407562}, {"text": "ARGM", "start_pos": 97, "end_pos": 101, "type": "METRIC", "confidence": 0.774921178817749}]}, {"text": "We also report results on automatic parses (see).", "labels": [], "entities": []}, {"text": "We trained and tested on automatic parse trees from Charniak's parser.", "labels": [], "entities": []}, {"text": "For approximately 5.6% of the argument constituents in the test set, we could not find exact matches in the automatic parses.", "labels": [], "entities": []}, {"text": "Instead of discarding these arguments, we took the largest constituent in the automatic parse having the same head-word as the gold-standard argument constituent.", "labels": [], "entities": []}, {"text": "Also, 19 of the propositions in the test set were discarded because Charniak's parser altered the tokenization of the input sentence and tokens could not be aligned.", "labels": [], "entities": []}, {"text": "As our results show, the error reduction of our joint model with respect to the local model is more modest in this setting.", "labels": [], "entities": [{"text": "error reduction", "start_pos": 25, "end_pos": 40, "type": "METRIC", "confidence": 0.9659483134746552}]}, {"text": "One reason for this is the lower upper bound, due largely to the the much poorer performance of the identification model on automatic parses.", "labels": [], "entities": []}, {"text": "For ARGM, the local identification model achieves 85.9 F-Measure and 59.4 Frame Accuracy; the local classification model achieves 92.3 F-Measure and 83.1 Frame Accuracy.", "labels": [], "entities": [{"text": "ARGM", "start_pos": 4, "end_pos": 8, "type": "DATASET", "confidence": 0.699498176574707}, {"text": "F-Measure", "start_pos": 55, "end_pos": 64, "type": "METRIC", "confidence": 0.9927905797958374}, {"text": "Accuracy", "start_pos": 80, "end_pos": 88, "type": "METRIC", "confidence": 0.5525462031364441}, {"text": "F-Measure", "start_pos": 135, "end_pos": 144, "type": "METRIC", "confidence": 0.9767252802848816}, {"text": "Accuracy", "start_pos": 160, "end_pos": 168, "type": "METRIC", "confidence": 0.7299541234970093}]}, {"text": "It seems that the largest boost would come from features that can identify arguments in the presence of parser errors, rather than the features of our joint model, which ensure global coherence of the argument frame.", "labels": [], "entities": []}, {"text": "We still achieve 10.7% and 18.5% error reduction for CORE arguments in F-Measure and Frame Accuracy respectively.: Performance of local and joint models on identification+classification on section 23, using Charniak automatically generated parse trees.", "labels": [], "entities": [{"text": "error reduction", "start_pos": 33, "end_pos": 48, "type": "METRIC", "confidence": 0.9634044468402863}, {"text": "F-Measure", "start_pos": 71, "end_pos": 80, "type": "METRIC", "confidence": 0.9494582414627075}, {"text": "identification+classification", "start_pos": 156, "end_pos": 185, "type": "TASK", "confidence": 0.8886471192042033}]}], "tableCaptions": [{"text": " Table 2: Performance of local classifiers on identification, classification, and identification+classification on  section 23, using gold-standard parse trees.", "labels": [], "entities": [{"text": "identification, classification", "start_pos": 46, "end_pos": 76, "type": "TASK", "confidence": 0.7586356997489929}]}, {"text": " Table 3: Oracle upper bounds for performance on the complete identification+classification task, using  varying numbers of top N joint labelings according to local classifiers.", "labels": [], "entities": [{"text": "complete identification+classification task", "start_pos": 53, "end_pos": 96, "type": "TASK", "confidence": 0.8162690043449402}]}, {"text": " Table 4: Performance of local and joint models on identification+classification on section 23, using gold- standard parse trees.", "labels": [], "entities": [{"text": "identification", "start_pos": 51, "end_pos": 65, "type": "TASK", "confidence": 0.9801280498504639}]}, {"text": " Table 5: Performance of local and joint models on identification+classification on section 23, using Charniak  automatically generated parse trees.", "labels": [], "entities": [{"text": "identification+classification", "start_pos": 51, "end_pos": 80, "type": "TASK", "confidence": 0.8359500567118326}]}]}