{"title": [{"text": "Stochastic Lexicalized Inversion Transduction Grammar for Alignment", "labels": [], "entities": [{"text": "Stochastic Lexicalized Inversion Transduction Grammar", "start_pos": 0, "end_pos": 53, "type": "TASK", "confidence": 0.717134666442871}, {"text": "Alignment", "start_pos": 58, "end_pos": 67, "type": "TASK", "confidence": 0.7861664295196533}]}], "abstractContent": [{"text": "We present aversion of Inversion Trans-duction Grammar where rule probabilities are lexicalized throughout the synchronous parse tree, along with pruning techniques for efficient training.", "labels": [], "entities": []}, {"text": "Alignment results improve over unlexicalized ITG on short sentences for which full EM is feasible, but pruning seems to have a negative impact on longer sentences.", "labels": [], "entities": [{"text": "Alignment", "start_pos": 0, "end_pos": 9, "type": "TASK", "confidence": 0.9326696395874023}]}], "introductionContent": [{"text": "The Inversion Transduction Grammar (ITG) of Wu (1997) is a syntactically motivated algorithm for producing word-level alignments of pairs of translationally equivalent sentences in two languages.", "labels": [], "entities": [{"text": "Inversion Transduction Grammar (ITG)", "start_pos": 4, "end_pos": 40, "type": "TASK", "confidence": 0.7828292548656464}, {"text": "word-level alignments of pairs of translationally equivalent sentences", "start_pos": 107, "end_pos": 177, "type": "TASK", "confidence": 0.7119278311729431}]}, {"text": "The algorithm builds asynchronous parse tree for both sentences, and assumes that the trees have the same underlying structure but that the ordering of constituents may differ in the two languages.", "labels": [], "entities": []}, {"text": "This probabilistic, syntax-based approach has inspired much subsequent reasearch.", "labels": [], "entities": []}, {"text": "In the tree-to-string model of, a parse tree for one sentence of a translation pair is projected onto the other string.", "labels": [], "entities": []}, {"text": "presents algorithms for synchronous parsing with more complex grammars, discussing how to parse grammars with greater than binary branching and lexicalization of synchronous grammars.", "labels": [], "entities": [{"text": "synchronous parsing", "start_pos": 24, "end_pos": 43, "type": "TASK", "confidence": 0.58540940284729}]}, {"text": "Despite being one of the earliest probabilistic syntax-based translation models, ITG remains stateof-the art.", "labels": [], "entities": []}, {"text": "found that the constraints of ITG were a better match to the decoding task than the heuristics used in the IBM decoder of. found ITG to outperform the tree-to-string model for word-level alignment, as measured against human gold-standard alignments.", "labels": [], "entities": [{"text": "word-level alignment", "start_pos": 176, "end_pos": 196, "type": "TASK", "confidence": 0.7489749491214752}]}, {"text": "One explanation for this result is that, while a tree representation is helpful for modeling translation, the trees assigned by the traditional monolingual parsers (and the treebanks on which they are trained) may not be optimal for translation of a specific language pair.", "labels": [], "entities": []}, {"text": "ITG has the advantage of being entirely data-driven -the trees are derived from an expectation maximization procedure given only the original strings as input.", "labels": [], "entities": [{"text": "ITG", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9122095704078674}]}, {"text": "In this paper, we extend ITG to condition the grammar production probabilities on lexical information throughout the tree.", "labels": [], "entities": []}, {"text": "This model is reminiscent of lexicalization as used in modern statistical parsers, in that a unique headword is chosen for each constituent in the tree.", "labels": [], "entities": []}, {"text": "It differs in that the head words are chosen through EM rather than deterministic rules.", "labels": [], "entities": []}, {"text": "This approach is designed to retain the purely data-driven character of ITG, while giving the model more information to work with.", "labels": [], "entities": []}, {"text": "By conditioning on lexical information, we expect the model to be able capture the same systematic differences in languages' grammars that motive the tree-to-string model, for example, SVO vs. SOV word order or prepositions vs. postpositions, but to be able to do so in a more fine-grained manner.", "labels": [], "entities": []}, {"text": "The interaction between lexical information and word order also explains the higher performance of IBM model 4 over IBM model 3 for alignment.", "labels": [], "entities": [{"text": "alignment", "start_pos": 132, "end_pos": 141, "type": "TASK", "confidence": 0.9602558612823486}]}, {"text": "We begin by presenting the probability model in the following section, detailing how we address issues of pruning and smoothing that lexicalization introduces.", "labels": [], "entities": []}, {"text": "We present alignment results on a parallel Chinese-English corpus in Section 3.", "labels": [], "entities": []}], "datasetContent": [{"text": "We trained both the unlexicalized and the lexicalized ITGs on a parallel corpus of Chinese-English newswire text.", "labels": [], "entities": []}, {"text": "The Chinese data were automatically segmented into tokens, and English capitalization was retained.", "labels": [], "entities": []}, {"text": "We replaced words occurring only once with an unknown word token, resulting in a Chinese vocabulary of 23,783 words and an English vocabulary of 27,075 words.", "labels": [], "entities": []}, {"text": "In the first experiment, we restricted ourselves to sentences of no more than 15 words in either language, resulting in a training corpus of 6,984 sentence pairs with a total of 66,681 Chinese words and 74,651 English words.", "labels": [], "entities": []}, {"text": "In this experiment, we didn't apply the pruning techniques for the lexicalized ITG.", "labels": [], "entities": []}, {"text": "In the second experiment, we enabled the pruning techniques for the LITG with the beam ratio for the tic-tac-toe pruning as 10 \u22125 and the number k for the top-k pruning as 25.", "labels": [], "entities": [{"text": "LITG", "start_pos": 68, "end_pos": 72, "type": "DATASET", "confidence": 0.8570903539657593}, {"text": "beam ratio", "start_pos": 82, "end_pos": 92, "type": "METRIC", "confidence": 0.9814786612987518}]}, {"text": "We ran the experiments on sentences up to 25 words long in both languages.", "labels": [], "entities": []}, {"text": "The resulting training corpus had 18,773 sentence pairs with a total of 276,113 Chinese words and 315,415 English words.", "labels": [], "entities": []}, {"text": "We evaluate our translation models in terms of agreement with human-annotated word-level alignments between the sentence pairs.", "labels": [], "entities": []}, {"text": "For scoring the Viterbi alignments of each system against goldstandard annotated alignments, we use the alignment error rate (AER) of, which measures agreement at the level of pairs of words: where A is the set of word pairs aligned by the automatic system, G S is the set marked in the gold standard as \"sure\", and GP is the set marked as \"possible\" (including the \"sure\" pairs).", "labels": [], "entities": [{"text": "alignment error rate (AER)", "start_pos": 104, "end_pos": 130, "type": "METRIC", "confidence": 0.9112168947855631}, {"text": "GP", "start_pos": 316, "end_pos": 318, "type": "METRIC", "confidence": 0.968920111656189}]}, {"text": "In our Chinese-English data, only one type of alignment was marked, meaning that GP = G S . In our hand-aligned data, 20 sentence pairs are less than or equal to 15 words in both languages, and were used as the test set for the first experiment, and 47 sentence pairs are no longer than 25 words in either language and were used to evaluate the pruned  A separate development set of hand-aligned sentence pairs was used to control overfitting.", "labels": [], "entities": [{"text": "GP", "start_pos": 81, "end_pos": 83, "type": "METRIC", "confidence": 0.9375556707382202}]}, {"text": "The subset of up to 15 words in both languages was used for cross-validating in the first experiment.", "labels": [], "entities": []}, {"text": "The subset of up to 25 words in both languages was used for the same purpose in the second experiment.", "labels": [], "entities": []}, {"text": "compares results using the full (unpruned) model of unlexicalized ITG with the full model of lexicalized ITG.", "labels": [], "entities": []}, {"text": "The two models were initialized from uniform distributions for all rules and were trained until AER began to rise on our held-out cross-validation data, which turned out to be 4 iterations for ITG and 3 iterations for LITG.", "labels": [], "entities": [{"text": "AER", "start_pos": 96, "end_pos": 99, "type": "METRIC", "confidence": 0.9967532753944397}, {"text": "ITG", "start_pos": 193, "end_pos": 196, "type": "DATASET", "confidence": 0.9114631414413452}]}, {"text": "The results from the second experiment are shown in.", "labels": [], "entities": []}, {"text": "The performance of the full model of unlexicalized ITG is compared with the pruned model of lexicalized ITG using more training data and evaluation data.", "labels": [], "entities": []}, {"text": "Under the same check condition, we trained ITG for 3 iterations and the pruned LITG for 1 iteration.", "labels": [], "entities": [{"text": "ITG", "start_pos": 43, "end_pos": 46, "type": "DATASET", "confidence": 0.7955475449562073}, {"text": "LITG", "start_pos": 79, "end_pos": 83, "type": "METRIC", "confidence": 0.8845929503440857}]}, {"text": "For comparison, we also included the results from IBM Model 1 and Model 4.", "labels": [], "entities": [{"text": "IBM Model 1", "start_pos": 50, "end_pos": 61, "type": "DATASET", "confidence": 0.8919176260630289}]}, {"text": "The numbers of iterations for the training of the IBM models were chosen to be the turning points of AER changing on the cross-validation data.", "labels": [], "entities": [{"text": "AER", "start_pos": 101, "end_pos": 104, "type": "METRIC", "confidence": 0.8519499897956848}]}], "tableCaptions": [{"text": " Table 1: Alignment results on Chinese-English corpus (\u2264 15 words on both sides). Full ITG vs. Full LITG", "labels": [], "entities": []}, {"text": " Table 2: Alignment results on Chinese-English corpus (\u2264 25 words on both sides). Full ITG vs. Pruned  LITG", "labels": [], "entities": []}]}