{"title": [{"text": "Semi-Supervised Chinese Word Segmentation Using Partial-Label Learning With Conditional Random Fields", "labels": [], "entities": [{"text": "Semi-Supervised Chinese Word Segmentation", "start_pos": 0, "end_pos": 41, "type": "TASK", "confidence": 0.5282281041145325}]}], "abstractContent": [{"text": "There is rich knowledge encoded in on-line web data.", "labels": [], "entities": []}, {"text": "For example, punctuation and entity tags in Wikipedia data define some word boundaries in a sentence.", "labels": [], "entities": []}, {"text": "In this paper we adopt partial-label learning with conditional random fields to make use of this valuable knowledge for semi-supervised Chinese word segmenta-tion.", "labels": [], "entities": []}, {"text": "The basic idea of partial-label learning is to optimize a cost function that marginalizes the probability mass in the constrained space that encodes this knowledge.", "labels": [], "entities": [{"text": "partial-label learning", "start_pos": 18, "end_pos": 40, "type": "TASK", "confidence": 0.7325744032859802}]}, {"text": "By integrating some domain adaptation techniques, such as EasyAdapt, our result reaches an F-measure of 95.98% on the CTB-6 corpus, a significant improvement from both the supervised baseline and a previous proposed approach, namely constrained decode.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 91, "end_pos": 100, "type": "METRIC", "confidence": 0.9994603991508484}, {"text": "CTB-6 corpus", "start_pos": 118, "end_pos": 130, "type": "DATASET", "confidence": 0.9762709140777588}]}], "introductionContent": [{"text": "A general approach for supervised Chinese word segmentation is to formulate it as a character sequence labeling problem, to label each character with its location in a word.", "labels": [], "entities": [{"text": "supervised Chinese word segmentation", "start_pos": 23, "end_pos": 59, "type": "TASK", "confidence": 0.6077723279595375}, {"text": "character sequence labeling", "start_pos": 84, "end_pos": 111, "type": "TASK", "confidence": 0.698665976524353}]}, {"text": "For example, proposes a four-label scheme based on some linguistic intuitions: 'B' for the beginning character of a word, 'I' for the internal characters, 'E' for the ending character, and 'S' for singlecharacter word.", "labels": [], "entities": []}, {"text": "Thus the word sequence \"\u6d3d\u8c08\u4f1a \u5f88 \u6210\u529f\" can be turned into a character sequence with labels as \u6d3d\\B \u8c08\\I \u4f1a\\E \u5f88\\S \u6210\\B \u529f\\E.", "labels": [], "entities": []}, {"text": "A machine learning algorithm for sequence labeling, such as conditional random fields (CRF), can be applied to the labelled training data to learn a model.", "labels": [], "entities": []}, {"text": "Labelled data for supervised learning of Chinese word segmentation, however, is usually expensive and tends to be of a limited amount.", "labels": [], "entities": [{"text": "supervised learning of Chinese word segmentation", "start_pos": 18, "end_pos": 66, "type": "TASK", "confidence": 0.5654822637637457}]}, {"text": "Researchers are thus interested in semi-supervised learning, which is to make use of unlabelled data to further improve the performance of supervised learning.", "labels": [], "entities": []}, {"text": "There is a large amount of unlabelled data available, for example, the Gigaword corpus in the LDC catalog or the Chinese Wikipedia on the web.", "labels": [], "entities": [{"text": "Gigaword corpus", "start_pos": 71, "end_pos": 86, "type": "DATASET", "confidence": 0.928339034318924}, {"text": "LDC catalog", "start_pos": 94, "end_pos": 105, "type": "DATASET", "confidence": 0.9002760648727417}]}, {"text": "Faced with the large amount of unlabelled data, an intuitive idea is to use self-training or EM, by first training a baseline model (from the supervised data) and then iteratively decoding the unlabelled data and updating the baseline model. and further propose to minimize the entropy of the predicted label distribution on unlabeled data and use it as a regularization term in CRF (i.e. entropy regularization).", "labels": [], "entities": []}, {"text": "Beyond these ideas, and Sun and Xu (2011) experiment with deriving a large set of statistical features such as mutual information and accessor variety from unlabelled data, and add them to supervised discriminative training.", "labels": [], "entities": []}, {"text": "experiment with graph propagation to extract information from unlabelled data to regularize the CRF training.,, and experiment with co-training for semi-supervised Chinese word segmentation.", "labels": [], "entities": [{"text": "graph propagation", "start_pos": 16, "end_pos": 33, "type": "TASK", "confidence": 0.7731095850467682}, {"text": "Chinese word segmentation", "start_pos": 164, "end_pos": 189, "type": "TASK", "confidence": 0.6425860226154327}]}, {"text": "All these approaches only leverage the distribution of the unlabelled data, yet do not make use of the knowledge that the unlabelled data might have integrated in.", "labels": [], "entities": []}, {"text": "There could be valuable information encoded within the unlabelled data that researchers can take advantage of.", "labels": [], "entities": []}, {"text": "For example, punctuation creates natural word boundaries (: the character before a comma can only be labelled as either 'S' or 'E', while the character after a comma can only be labelled as 'S' or 'B'.", "labels": [], "entities": []}, {"text": "Furthermore, entity tags (HTML tags or Wikipedia tags) on the web, such as emphasis and cross reference, also provide rich information for word segmentation: they might define a word or at least give word boundary information similar to punctuation.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 139, "end_pos": 156, "type": "TASK", "confidence": 0.7151730060577393}]}, {"text": "refer to such structural information on the web as natural annotations, and propose that they encode knowledge for NLP.", "labels": [], "entities": []}, {"text": "For Chinese word segmentation, natural annotations and punctuation create a sausage 1 constraint for the possible labels, as illustrated in.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 4, "end_pos": 29, "type": "TASK", "confidence": 0.6021945377190908}]}, {"text": "In the sentence \"\u8fd1\u5e74\u6765\uff0c\u4eba\u5de5\u667a\u80fd\u548c\u673a\u5668\u5b66\u4e60\u8fc5 \u731b\u53d1\u5c55\u3002\", the first character \u8fd1 can only be labelled with 'S' or 'B'; and the characters \u6765 before the comma and \u5c55 before the Chinese period can only be labelled as 'S' or 'E'.", "labels": [], "entities": []}, {"text": "\"\u4eba\u5de5\u667a\u80fd\" and \"\u673a \u5668\u5b66\u4e60\" are two Wikipedia entities, and so they define the word boundaries before the first character and after the last character of the entities as well.", "labels": [], "entities": []}, {"text": "The single character \u548c between these two entities has only one label 'S'.", "labels": [], "entities": []}, {"text": "This sausage constraint thus encodes rich information for word segmentation.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 58, "end_pos": 75, "type": "TASK", "confidence": 0.7618793249130249}]}, {"text": "To make use of the knowledge encoded in the sausage constraint, adopt a constrained decode approach.", "labels": [], "entities": []}, {"text": "They first train a baseline model with labelled data, and then run constrained decode on the unlabelled data by binding the search space with the sausage; and so the decoded labels are consistent with the sausage constraint.", "labels": [], "entities": []}, {"text": "The unlabelled data, together with the labels from constrained decode, are then selectively added to the labelled data for training the final model.", "labels": [], "entities": []}, {"text": "This approach, using constrained decode as a middle step, provides an indirect way of leaning the knowledge.", "labels": [], "entities": []}, {"text": "However, the middle step, constrained decode, has the risk of reinforcing the errors in the baseline model: the decoded labels added to the training data for building the final model might contain errors introduced from the baseline model.", "labels": [], "entities": []}, {"text": "The knowledge encoded in 1 Also referred to as confusion network.", "labels": [], "entities": []}, {"text": "the data carrying the information from punctuation and natural annotations is thus polluted by the errorful re-decoded labels.", "labels": [], "entities": []}, {"text": "A sentence where each character has exactly one label is fully-labelled; and a sentence where each character receives all possible labels is zerolabelled.", "labels": [], "entities": []}, {"text": "A sentence with sausage-constrained labels can be viewed as partially-labelled.", "labels": [], "entities": []}, {"text": "These partial labels carry valuable information that researchers would like to learn in a model, yet the normal CRF training typically uses fully-labelled sentences.", "labels": [], "entities": []}, {"text": "Recently, propose an approach to train a CRF model directly from partial labels.", "labels": [], "entities": []}, {"text": "The basic idea is to marginalize the probability mass of the constrained sausage in the cost function.", "labels": [], "entities": []}, {"text": "The normal CRF training using fully-labelled sentences is a special case where the sausage constraint is a linear line; while on the other hand a zero-labelled sentence, where the sausage constraint is the full lattice, makes no contribution in the learning since the sum of probabilities is deemed to be one.", "labels": [], "entities": []}, {"text": "This new approach, without the need of using constrained re-decoding as a middle step, provides a direct means to learn the knowledge in the partial labels.", "labels": [], "entities": []}, {"text": "In this research we explore using the partiallabel learning for semi-supervised Chinese word segmentation.", "labels": [], "entities": [{"text": "semi-supervised Chinese word segmentation", "start_pos": 64, "end_pos": 105, "type": "TASK", "confidence": 0.6126710176467896}]}, {"text": "We use the CTB-6 corpus as the labelled training, development and test data, and use the Chinese Wikipedia as the unlabelled data.", "labels": [], "entities": [{"text": "CTB-6 corpus", "start_pos": 11, "end_pos": 23, "type": "DATASET", "confidence": 0.9749245941638947}, {"text": "Chinese Wikipedia", "start_pos": 89, "end_pos": 106, "type": "DATASET", "confidence": 0.8620142042636871}]}, {"text": "We first train a baseline model with labelled data only, and then selectively add Wikipedia data with partial labels to build a second model.", "labels": [], "entities": []}, {"text": "Because the Wikipedia data is out of domain and has distribution bias, we also experiment with two domain adaptation techniques: model interpolation and EasyAdapt.", "labels": [], "entities": [{"text": "Wikipedia data", "start_pos": 12, "end_pos": 26, "type": "DATASET", "confidence": 0.8796444833278656}]}, {"text": "Our result reaches an F-measure of 95.98%, an absolute improvement of 0.72% over the very strong base-line (corresponding to 15.19% relative error reduction), and 0.33% over the constrained decode approach (corresponding to 7.59% relative error reduction).", "labels": [], "entities": [{"text": "F-measure", "start_pos": 22, "end_pos": 31, "type": "METRIC", "confidence": 0.9996984004974365}, {"text": "relative error reduction", "start_pos": 132, "end_pos": 156, "type": "METRIC", "confidence": 0.7214681108792623}]}, {"text": "We conduct a detailed error analysis, illustrating how partial-label learning excels constrained decode in learning the knowledge encoded in the Wikipedia data.", "labels": [], "entities": [{"text": "Wikipedia data", "start_pos": 145, "end_pos": 159, "type": "DATASET", "confidence": 0.8545386791229248}]}, {"text": "As a note, our result also out-performs () and (Sun and Xu, 2011).", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we describe the basic setup for our experiments of semi-supervised Chinese word segmentation.", "labels": [], "entities": [{"text": "semi-supervised Chinese word segmentation", "start_pos": 67, "end_pos": 108, "type": "TASK", "confidence": 0.5836555510759354}]}, {"text": "In order to determine how well the models learn the encoded knowledge (i.e. partial labels) from the Wikipedia data, we first evaluate the models against the Wikipedia test set.", "labels": [], "entities": [{"text": "Wikipedia data", "start_pos": 101, "end_pos": 115, "type": "DATASET", "confidence": 0.8581655025482178}, {"text": "Wikipedia test set", "start_pos": 158, "end_pos": 176, "type": "DATASET", "confidence": 0.9536497592926025}]}, {"text": "The Wikipedia test set, however, is only partially-labelled.", "labels": [], "entities": [{"text": "Wikipedia test set", "start_pos": 4, "end_pos": 22, "type": "DATASET", "confidence": 0.9704154928525289}]}, {"text": "Thus the metric we use here is consistent label accuracy, similar to how we rank the sentences in Section 3.3, defined as whether a predicted label fora character is an element in the constrained labels.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.6254762411117554}]}, {"text": "Because partial labels are only sparsely distributed in the test data, a lot of characters receive all four labels in the constrained sausage.", "labels": [], "entities": []}, {"text": "Evaluating against characters with all four labels do not really represent the models' difference as it is deemed to be consistent.", "labels": [], "entities": []}, {"text": "Thus beyond evaluating against all characters in the Wikipedia test set (referred to as Full measurement), we also evaluate against characters that are only constrained with less than four labels (referred to as Label measurement).", "labels": [], "entities": [{"text": "Wikipedia test set", "start_pos": 53, "end_pos": 71, "type": "DATASET", "confidence": 0.9462156097094218}]}, {"text": "The Label measurement focuses on en-coded knowledge in the test set and so can better represent the model's capability of learning from the partial labels.", "labels": [], "entities": []}, {"text": "Results are shown in with the Full measurement and in with the Label measurement.", "labels": [], "entities": [{"text": "Label measurement", "start_pos": 63, "end_pos": 80, "type": "METRIC", "confidence": 0.8252712786197662}]}, {"text": "The x axes are the size of Wikipedia training data, as explained in Section 3.3.", "labels": [], "entities": [{"text": "Wikipedia training data", "start_pos": 27, "end_pos": 50, "type": "DATASET", "confidence": 0.7962196469306946}]}, {"text": "As can be seen, both constrained decode and partiallabel learning perform much better than the baseline supervised model that is trained from CTB-6 data only, indicating that both of them are learning the encoded knowledge from the Wikipedia training data.", "labels": [], "entities": [{"text": "CTB-6 data", "start_pos": 142, "end_pos": 152, "type": "DATASET", "confidence": 0.9598364233970642}, {"text": "Wikipedia training data", "start_pos": 232, "end_pos": 255, "type": "DATASET", "confidence": 0.7714697519938151}]}, {"text": "Also we seethe trend that the performance improves with more data in training, also suggesting the learning of encoded knowledge.", "labels": [], "entities": []}, {"text": "Most importantly, we see that partial-label learning consistently out-performs constrained decode in all data points.", "labels": [], "entities": []}, {"text": "With the Label measurement, partial-label learning gives 1.7% or higher absolute improvement over constrained decode across all data points.", "labels": [], "entities": [{"text": "absolute", "start_pos": 72, "end_pos": 80, "type": "METRIC", "confidence": 0.9695110321044922}]}, {"text": "At the data point of 600k, constrained decode gives an accuracy of 97.14%, while partial-label learning gives 98.93% (baseline model gives 87.08%).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 55, "end_pos": 63, "type": "METRIC", "confidence": 0.9994229078292847}]}, {"text": "The relative gain (from learning the knowledge) of partial-label learning over constrained decode is thus 18% ((98.93 \u2212 97.14)/(97.14 \u2212 87.08)).", "labels": [], "entities": []}, {"text": "These results suggest that partial-label learning is more effective in learning the encoded knowledge in the Wikipedia data than constrained decode.", "labels": [], "entities": [{"text": "Wikipedia data", "start_pos": 109, "end_pos": 123, "type": "DATASET", "confidence": 0.8870671093463898}]}, {"text": "Our ultimate goal, however, is to determine whether we can leverage the encoded knowledge in the Wikipedia data to improve the word segmentation in CTB-6.", "labels": [], "entities": [{"text": "Wikipedia data", "start_pos": 97, "end_pos": 111, "type": "DATASET", "confidence": 0.9403195083141327}, {"text": "word segmentation", "start_pos": 127, "end_pos": 144, "type": "TASK", "confidence": 0.7141702324151993}, {"text": "CTB-6", "start_pos": 148, "end_pos": 153, "type": "DATASET", "confidence": 0.8508653044700623}]}, {"text": "We run our models against the CTB-6 test set, with results shown in.", "labels": [], "entities": [{"text": "CTB-6 test set", "start_pos": 30, "end_pos": 44, "type": "DATASET", "confidence": 0.9869189858436584}]}, {"text": "Because we have fully-labelled sentences in the CTB-6 data, we adopt the F-measure as our evaluation metric here.", "labels": [], "entities": [{"text": "CTB-6 data", "start_pos": 48, "end_pos": 58, "type": "DATASET", "confidence": 0.9775393009185791}, {"text": "F-measure", "start_pos": 73, "end_pos": 82, "type": "METRIC", "confidence": 0.9896893501281738}]}, {"text": "The baseline model achieves 95.26% in F-measure, providing a stateof-the-art supervised performance.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 38, "end_pos": 47, "type": "METRIC", "confidence": 0.9984123706817627}]}, {"text": "Constrained decode is able to improve on this already very strong baseline performance, and we seethe nice trend of higher performance with more unlabeled data for training, indicating that constrained decode is making use of the encoded knowledge in the Wikipedia data to help CTB-6 segmentation.", "labels": [], "entities": [{"text": "Wikipedia data", "start_pos": 255, "end_pos": 269, "type": "DATASET", "confidence": 0.8243911266326904}, {"text": "CTB-6 segmentation", "start_pos": 278, "end_pos": 296, "type": "TASK", "confidence": 0.7680620849132538}]}, {"text": "When we look at the partial-label model, however, the results tell a totally different story.", "labels": [], "entities": []}, {"text": "First, it actually performs worse than the baseline model, and the more data added to training, the worse the performance is.", "labels": [], "entities": []}, {"text": "In the previous section we show that partial-label learning is more effective in learning the encoded knowledge in Wikipedia data than constrained decode.", "labels": [], "entities": []}, {"text": "So, what goes wrong?", "labels": [], "entities": []}, {"text": "We hypothesize that there is an out-of-domain distribution bias in the partial labels, and so the more data we add, the worse the in-domain performance is.", "labels": [], "entities": []}, {"text": "Constrained decode actually helps to smooth out the out-of-domain distribution bias by using the re-decoded labels with the in-domain supervised baseline model.", "labels": [], "entities": []}, {"text": "For example, both the baseline model and constrained decode correctly give the segmentation \"\u63d0\u4f9b/\u4e86/\u8fd0\u8f93/\u548c/\u7ed9 \u7ed9 \u7ed9\u6392 \u6392 \u6392\u6c34 \u6c34 \u6c34/\u4e4b/\u4fbf\", while partiallabel learning gives incorrect segmentation \"\u63d0 \u4f9b/\u4e86/\u8fd0 \u8f93/\u548c/\u7ed9 \u7ed9 \u7ed9/\u6392 \u6392 \u6392 \u6c34 \u6c34 \u6c34/\u4e4b/\u4fbf\".", "labels": [], "entities": []}, {"text": "Looking at the Wikipedia training data, \u6392\u6c34 is tagged as an entity 13 times; and \u7ed9 \u6392 \u6c34, although occurs 13 times in the data, is never tagged as an entity.", "labels": [], "entities": [{"text": "Wikipedia training data", "start_pos": 15, "end_pos": 38, "type": "DATASET", "confidence": 0.9047460357348124}]}, {"text": "Partial-label learning, which focuses on the tagged entities, thus overrules the segmentation of \u7ed9\u6392 \u6c34.", "labels": [], "entities": [{"text": "Partial-label learning", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.801819235086441}]}, {"text": "Constrained decode, on the other hand, by using the correctly re-decoded labels from the baseline model, observes enough evidence to correctly segment \u7ed9\u6392\u6c34 as a word.", "labels": [], "entities": []}, {"text": "To smooth out the out-of-domain distribution bias, we experiment with two approaches: model interpolation and EasyAdapt.", "labels": [], "entities": []}], "tableCaptions": []}