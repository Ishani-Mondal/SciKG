{"title": [{"text": "Joint Emotion Analysis via Multi-task Gaussian Processes", "labels": [], "entities": [{"text": "Joint Emotion Analysis", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.700825164715449}]}], "abstractContent": [{"text": "We propose a model for jointly predicting multiple emotions in natural language sentences.", "labels": [], "entities": [{"text": "predicting multiple emotions in natural language sentences", "start_pos": 31, "end_pos": 89, "type": "TASK", "confidence": 0.7791252136230469}]}, {"text": "Our model is based on a low-rank coregionalisation approach, which combines a vector-valued Gaussian Process with a rich parameterisation scheme.", "labels": [], "entities": []}, {"text": "We show that our approach is able to learn correlations and anti-correlations between emotions on a news headlines dataset.", "labels": [], "entities": [{"text": "news headlines dataset", "start_pos": 100, "end_pos": 122, "type": "DATASET", "confidence": 0.6727044880390167}]}, {"text": "The proposed model outperforms both single-task baselines and other multi-task approaches .", "labels": [], "entities": []}], "introductionContent": [{"text": "Multi-task learning) has been widely used in Natural Language Processing.", "labels": [], "entities": [{"text": "Natural Language Processing", "start_pos": 45, "end_pos": 72, "type": "TASK", "confidence": 0.6562251051266988}]}, {"text": "Most of these learning methods are aimed for Domain Adaptation, where we hypothesize that we can learn from multiple domains by assuming similarities between them.", "labels": [], "entities": [{"text": "Domain Adaptation", "start_pos": 45, "end_pos": 62, "type": "TASK", "confidence": 0.7396739423274994}]}, {"text": "A more recent use of multi-task learning is to model annotator bias and noise for datasets labelled by multiple annotators . The settings mentioned above have one aspect in common: they assume some degree of positive correlation between tasks.", "labels": [], "entities": []}, {"text": "In Domain Adaptation, we assume that some \"general\", domainindependent knowledge exists in the data.", "labels": [], "entities": []}, {"text": "For annotator noise modelling, we assume that a \"ground truth\" exists and that annotations are some noisy deviations from this truth.", "labels": [], "entities": [{"text": "annotator noise modelling", "start_pos": 4, "end_pos": 29, "type": "TASK", "confidence": 0.6951973040898641}]}, {"text": "However, for some settings these assumptions do not necessarily hold and often tasks can be anti-correlated.", "labels": [], "entities": []}, {"text": "For these cases, we need to employ multi-task methods that are able to learn these relations from data and correctly employ them when making predictions, avoiding negative knowledge transfer.", "labels": [], "entities": []}, {"text": "An example of a problem that shows this behaviour is Emotion Analysis, where the goal is to automatically detect emotions in a text).", "labels": [], "entities": [{"text": "Emotion Analysis", "start_pos": 53, "end_pos": 69, "type": "TASK", "confidence": 0.9398990273475647}]}, {"text": "This problem is closely related to Opinion Mining (, with similar applications, but it is usually done at a more fine-grained level and involves the prediction of a set of labels (one for each emotion) instead of a single label.", "labels": [], "entities": [{"text": "Opinion Mining", "start_pos": 35, "end_pos": 49, "type": "TASK", "confidence": 0.8390488922595978}]}, {"text": "While we expect some emotions to have some degree of correlation, this is usually not the case for all possible emotions.", "labels": [], "entities": []}, {"text": "For instance, we expect sadness and joy to be anti-correlated.", "labels": [], "entities": []}, {"text": "We propose a multi-task setting for Emotion Analysis based on a vector-valued Gaussian Process (GP) approach known as coregionalisation ( \u00b4).", "labels": [], "entities": [{"text": "Emotion Analysis", "start_pos": 36, "end_pos": 52, "type": "TASK", "confidence": 0.9746935069561005}]}, {"text": "The idea is to combine a GP with a low-rank matrix which encodes task correlations.", "labels": [], "entities": []}, {"text": "Our motivation to employ this model is three-fold: \u2022 Datasets for this task are scarce and small so we hypothesize that a multi-task approach will results in better models by allowing a task to borrow statistical strength from other tasks; \u2022 The annotation scheme is subjective and very fine-grained, and is therefore heavily prone to bias and noise, both which can be modelled easily using GPs; \u2022 Finally, we also have the goal to learn a model that shows sound and interpretable correlations between emotions.", "labels": [], "entities": []}], "datasetContent": [{"text": "To address the feasibility of our approach, we propose a set of experiments with three goals in mind: \u2022 To find our whether the ICM is able to learn sensible emotion correlations; \u2022 To check if these correlations are able to improve predictions for unseen texts; \u2022 To investigate the behaviour of the ICM model as we increase the training set size.", "labels": [], "entities": []}, {"text": "Dataset We use the dataset provided by the \"Affective Text\" shared task in, which is composed of 1000 news headlines annotated in terms of six emotions: Anger, Disgust, Fear, Joy, Sadness and Surprise.", "labels": [], "entities": [{"text": "Joy", "start_pos": 175, "end_pos": 178, "type": "METRIC", "confidence": 0.94990473985672}]}, {"text": "For each emotion, a score between 0 and 100 is given, 0 meaning total lack of emotion and 100 maximum emotional load.", "labels": [], "entities": []}, {"text": "We use 100 sentences for training and the remaining 900 for testing.", "labels": [], "entities": []}, {"text": "Model For all experiments, we use a Radial Basis Function (RBF) data kernel over a bag-ofwords feature representation.", "labels": [], "entities": []}, {"text": "Words were downcased and lemmatized using the WordNet lemmatizer in the NLTK 2 toolkit ().", "labels": [], "entities": [{"text": "WordNet lemmatizer", "start_pos": 46, "end_pos": 64, "type": "DATASET", "confidence": 0.9248685240745544}]}, {"text": "We then use the GPy toolkit 3 to combine this kernel with a coregionalisation model over the six emotions, comparing a number of low-rank approximations.", "labels": [], "entities": []}, {"text": "Baselines and Evaluation We compare prediction results with a set of single-task baselines: a Support Vector Machine (SVM) using an RBF kernel with hyperparameters optimised via crossvalidation and a single-task GP, optimised via likelihood maximisation.", "labels": [], "entities": []}, {"text": "The SVM models were trained using the Scikit-learn toolkit).", "labels": [], "entities": []}, {"text": "We also compare our results against the ones obtained by employing the \"Combined\" and \"Combined+\" models proposed by . Following previous work in this area, we use Pearson's correlation coefficient as evaluation metric.", "labels": [], "entities": [{"text": "Pearson's correlation coefficient", "start_pos": 164, "end_pos": 197, "type": "METRIC", "confidence": 0.6638326346874237}]}, {"text": "shows the learned coregionalisation matrix setting the initial rank as 1, reordering the emotions to emphasize the learned structure.", "labels": [], "entities": []}, {"text": "We can see that the matrix follows a block structure, clustering some of the emotions.", "labels": [], "entities": []}, {"text": "This picture shows two interesting behaviours:", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Prediction results in terms of Pearson's correlation coefficient (higher is better). Boldface values", "labels": [], "entities": [{"text": "Prediction", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9834701418876648}, {"text": "Pearson's correlation coefficient", "start_pos": 41, "end_pos": 74, "type": "METRIC", "confidence": 0.9326707124710083}]}]}