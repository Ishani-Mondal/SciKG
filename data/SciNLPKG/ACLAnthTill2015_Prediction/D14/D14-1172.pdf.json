{"title": [{"text": "Assessing the Impact of Translation Errors on Machine Translation Quality with Mixed-effects Models", "labels": [], "entities": [{"text": "Assessing the Impact of Translation Errors", "start_pos": 0, "end_pos": 42, "type": "TASK", "confidence": 0.7822387119134268}, {"text": "Machine Translation Quality", "start_pos": 46, "end_pos": 73, "type": "TASK", "confidence": 0.8391724824905396}]}], "abstractContent": [{"text": "Learning from errors is a crucial aspect of improving expertise.", "labels": [], "entities": []}, {"text": "Based on this notion , we discuss a robust statistical framework for analysing the impact of different error types on machine translation (MT) output quality.", "labels": [], "entities": [{"text": "machine translation (MT) output", "start_pos": 118, "end_pos": 149, "type": "TASK", "confidence": 0.819770892461141}]}, {"text": "Our approach is based on linear mixed-effects models, which allow the analysis of error-annotated MT output taking into account the variability inherent to the specific experimental setting from which the empirical observations are drawn.", "labels": [], "entities": [{"text": "MT", "start_pos": 98, "end_pos": 100, "type": "TASK", "confidence": 0.9446017146110535}]}, {"text": "Our experiments are carried out on different language pairs involving Chi-nese, Arabic and Russian as target languages.", "labels": [], "entities": []}, {"text": "Interesting findings are reported, concerning the impact of different error types both at the level of human perception of quality and with respect to performance results measured with automatic metrics.", "labels": [], "entities": []}], "introductionContent": [{"text": "The dominant statistical approach to machine translation (MT) is based on learning from large amounts of parallel data and tuning the resulting models on reference-based metrics that can be computed automatically, such as BLEU (), METEOR (Banerjee and), TER (), GTM (.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 37, "end_pos": 61, "type": "TASK", "confidence": 0.8772789597511291}, {"text": "BLEU", "start_pos": 222, "end_pos": 226, "type": "METRIC", "confidence": 0.9982526898384094}, {"text": "METEOR", "start_pos": 231, "end_pos": 237, "type": "METRIC", "confidence": 0.9268821477890015}, {"text": "TER", "start_pos": 254, "end_pos": 257, "type": "METRIC", "confidence": 0.9235373735427856}]}, {"text": "Despite the steady progress in the last two decades, especially for few well resourced translation directions having English as target language, this way to approach the problem is quickly reaching a performance plateau.", "labels": [], "entities": []}, {"text": "One reason is that parallel data area source of reliable information but, alone, limit systems knowledge to observed positive examples (i.e. how a sentence should be translated) without explicitly modelling any notion of error (i.e. how a sentence should not be translated).", "labels": [], "entities": []}, {"text": "Another reason is that, as a development and evaluation criterion, automatic metrics provide a holistic view of systems' behaviour without identifying the specific issues of a translation.", "labels": [], "entities": []}, {"text": "Indeed, the global scores returned by MT evaluation metrics depend on comparisons between translation hypotheses and reference translations, where the causes and the nature of the differences between them are not identified.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 38, "end_pos": 51, "type": "TASK", "confidence": 0.8928751051425934}]}, {"text": "To cope with these issues and define system improvement priorities, the focus of MT evaluation research is gradually shifting towards profiling systems' behaviour with respect to various typologies of errors (.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 81, "end_pos": 94, "type": "TASK", "confidence": 0.9479345977306366}]}, {"text": "This shift has enriched the traditional MT evaluation framework with anew element, that is the actual errors done by a system.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 40, "end_pos": 53, "type": "TASK", "confidence": 0.935884565114975}]}, {"text": "Until now, most of the research has focused on the relationship (i.e. the correlation) between two elements of the framework: humans and automatic evaluation metrics.", "labels": [], "entities": []}, {"text": "As anew element of the framework, which becomes a sort of \"evaluation triangle\", the analysis of error annotations opens interesting research problems related to the relationships between: i) error types and human perception of MT quality and ii) error types and the sensitivity of automatic metrics.", "labels": [], "entities": [{"text": "MT", "start_pos": 228, "end_pos": 230, "type": "TASK", "confidence": 0.955794632434845}]}, {"text": "Besides motivating further investigation on metrics featuring high correlation with human judgements (a well-established MT research sub-field, which is out of the scope of this paper), connecting the vertices of this triangle raises new challenging questions such as: (1) Which types of MT errors have the highest impact on human perception of translation quality?", "labels": [], "entities": [{"text": "MT research", "start_pos": 121, "end_pos": 132, "type": "TASK", "confidence": 0.897830069065094}, {"text": "MT", "start_pos": 288, "end_pos": 290, "type": "TASK", "confidence": 0.9651550054550171}]}, {"text": "Surprisingly, little prior work focused on this side of the triangle.", "labels": [], "entities": []}, {"text": "Error annotations have been considered to highlight strengths and weaknesses of MT engines or to investigate the influence of different error types on post-editors' work.", "labels": [], "entities": [{"text": "MT engines", "start_pos": 80, "end_pos": 90, "type": "TASK", "confidence": 0.9119941294193268}]}, {"text": "However, the direct connection between er-rors and users' preferences has been only partially understood, mainly from a descriptive standpoint and through rudimentary techniques unsuitable to draw clear-cut conclusions or reliable inferences.", "labels": [], "entities": []}, {"text": "(2) To which types of errors are different MT evaluation metrics more sensitive?", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 43, "end_pos": 56, "type": "TASK", "confidence": 0.8905326426029205}]}, {"text": "This side of the triangle has been even less explored.", "labels": [], "entities": []}, {"text": "For instance, little has been done to understand which automatic metric is more suitable to assess system improvements with respect to a specific issue (e.g. word order or morphology) or to shed light on the joint impact of different error types on performance results calculated with different metrics.", "labels": [], "entities": []}, {"text": "To answer these questions, we propose a robust statistical framework to analyse the impact of different error types, alone and in combination, both on human perception of quality and on MT evaluation metrics' results.", "labels": [], "entities": [{"text": "MT evaluation metrics'", "start_pos": 186, "end_pos": 208, "type": "TASK", "confidence": 0.8869055906931559}]}, {"text": "Our analysis is carried out by employing linear mixed-effects models, a generalization of linear regression models suited to model responses with fixed and random effects.", "labels": [], "entities": []}, {"text": "Experiments are performed on data covering three translation directions (English to Chinese, Arabic and Russian).", "labels": [], "entities": []}, {"text": "For each direction, two automatic translations were collected for around 400 sentences and were manually evaluated by expert translators through absolute quality judgements and error annotation.", "labels": [], "entities": []}, {"text": "Building on the advantages offered by linear mixed-effects models, our main contributions include: \u2022 A rigorous method, novel to MT error analysis research, to relate MT issues to human preferences and MT metrics' results; \u2022 The application of such method to three translation directions having English as source and different languages as target; \u2022 A number of findings, specific to each language direction, which are out of the reach of the few simpler methods proposed so far.", "labels": [], "entities": [{"text": "MT error analysis", "start_pos": 129, "end_pos": 146, "type": "TASK", "confidence": 0.911729633808136}, {"text": "MT issues", "start_pos": 167, "end_pos": 176, "type": "TASK", "confidence": 0.9153081178665161}, {"text": "MT metrics'", "start_pos": 202, "end_pos": 213, "type": "TASK", "confidence": 0.8812214732170105}]}, {"text": "Overall, our study has clear practical implications for MT systems' development and evaluation.", "labels": [], "entities": [{"text": "MT", "start_pos": 56, "end_pos": 58, "type": "TASK", "confidence": 0.9948692321777344}]}, {"text": "Indeed, the proposed statistical analysis framework represents an ideal instrument to: i) identify translation issues having the highest impact on human perception of quality and ii) choose the most appropriate evaluation metric to measure progress towards their solution.", "labels": [], "entities": []}], "datasetContent": [{"text": "For our analysis we used a dataset that covers three translation directions, corresponding to English to Chinese, Arabic, and Russian.", "labels": [], "entities": []}, {"text": "An international organization provided us a set of English sentences together with their translation produced by two anonymous MT systems.", "labels": [], "entities": [{"text": "MT", "start_pos": 127, "end_pos": 129, "type": "TASK", "confidence": 0.8841509222984314}]}, {"text": "For each evaluation item (source sentence and two MT outputs) three experts were asked to assign quality scores to the MT outputs, and a fourth expert was asked to annotate translation errors.", "labels": [], "entities": []}, {"text": "The four experts, who were all professional translators native in the examined target languages, were carefully trained to get acquainted with the evaluation guidelines and the annotation tool specifically developed for these evaluation tasks ().", "labels": [], "entities": []}, {"text": "The annotation process was carried out in parallel by all annotators over one week, resulting in a final dataset composed of 312 evaluation items for the ENZH direction, 393 for ENAR, and 437 for ENRU.", "labels": [], "entities": [{"text": "ENZH direction", "start_pos": 154, "end_pos": 168, "type": "DATASET", "confidence": 0.9437866806983948}, {"text": "ENAR", "start_pos": 178, "end_pos": 182, "type": "DATASET", "confidence": 0.9424068927764893}, {"text": "ENRU", "start_pos": 196, "end_pos": 200, "type": "DATASET", "confidence": 0.9813041687011719}]}, {"text": "To assess the impact of translation errors on MT quality we perform two sets of experiments.", "labels": [], "entities": [{"text": "MT", "start_pos": 46, "end_pos": 48, "type": "TASK", "confidence": 0.9961656332015991}]}, {"text": "The first set ( \u00a75.1) addresses the relation between errors and human quality judgements.", "labels": [], "entities": []}, {"text": "The second set ( \u00a75.2) focuses on the relation between errors and automatic metrics.", "labels": [], "entities": []}, {"text": "In both cases, before measuring the impact of different errors on the response variable (respectively quality judgements and metrics), we validate the effectiveness of mixed linear models by comparing their prediction capability with other methods.", "labels": [], "entities": []}, {"text": "In all experiments, error counts of each category were normalized into percentages with respect to the sentence length and mapped in a logarithmic scale.", "labels": [], "entities": []}, {"text": "In this way, we basically assume that the impact of errors tends to saturate above a given threshold, hypothesis that also results in better fits by our models.", "labels": [], "entities": []}, {"text": "Notice that while the chosen log- In other words, we assume that human sensitivity to er-10 base is easy to interpret, linear models can implicitly adjust it.", "labels": [], "entities": []}, {"text": "Our analysis makes use of mixed linear models incorporating, as fixed effects, the four types of errors (lex, miss, morph and reo) and their pairwise interactions (the product of the single error log counts), while their random structure depends on each specific experiment.", "labels": [], "entities": []}, {"text": "For the experiments we rely on the R language (R Core Team, 2013) implementation of linear mixed model in the lme4 library ().", "labels": [], "entities": [{"text": "R Core Team, 2013)", "start_pos": 47, "end_pos": 65, "type": "DATASET", "confidence": 0.9025343954563141}]}, {"text": "We assess the quality of our mixed linear models (MLM) by comparing their prediction capability with a sequence of simpler linear models including only fixed effects.", "labels": [], "entities": []}, {"text": "In particular, we built five univariate models and two multivariate models.", "labels": [], "entities": []}, {"text": "The univariate models use as covariates, respectively, the sum of all error types (baseline), and each of the four types of errors (lex, miss, morph and reo).", "labels": [], "entities": []}, {"text": "The two multivariate models include all the four error types, considering them without interactions (FLM w/o Interact.) and with interactions (FLM).", "labels": [], "entities": [{"text": "FLM", "start_pos": 143, "end_pos": 146, "type": "METRIC", "confidence": 0.5660837888717651}]}, {"text": "Prediction performance is computed in terms of Mean Absolute Error (MAE), which we estimate by averaging over 1,000 random splits of the data in 90% training and 10% test.", "labels": [], "entities": [{"text": "Prediction", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9405571222305298}, {"text": "Mean Absolute Error (MAE)", "start_pos": 47, "end_pos": 72, "type": "METRIC", "confidence": 0.9296386241912842}]}, {"text": "In particular, for the human quality classes we pick the integer between 1-5 that is closest to the predicted value.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Average quality scores per annotator and  per system.", "labels": [], "entities": [{"text": "Average quality scores", "start_pos": 10, "end_pos": 32, "type": "METRIC", "confidence": 0.8466814756393433}]}, {"text": " Table 2. Differences in systems'  performance can be observed for all language  pairs; as we will observe in  \u00a75.2 such variability  explains the effectiveness of considering the MT  systems as a random effect.", "labels": [], "entities": [{"text": "MT", "start_pos": 180, "end_pos": 182, "type": "TASK", "confidence": 0.9566765427589417}]}, {"text": " Table 2: Overall automatic scores per system.", "labels": [], "entities": []}, {"text": " Table 3: Prediction capability of human judge- ments (MAE).", "labels": [], "entities": [{"text": "Prediction", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.8864185810089111}]}, {"text": " Table 5: Prediction capability of BLEU score, TER and GTM (MAE).", "labels": [], "entities": [{"text": "Prediction", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9875199794769287}, {"text": "BLEU score", "start_pos": 35, "end_pos": 45, "type": "METRIC", "confidence": 0.9674280881881714}, {"text": "TER", "start_pos": 47, "end_pos": 50, "type": "METRIC", "confidence": 0.9966278672218323}, {"text": "GTM", "start_pos": 55, "end_pos": 58, "type": "METRIC", "confidence": 0.6691152453422546}, {"text": "MAE", "start_pos": 60, "end_pos": 63, "type": "METRIC", "confidence": 0.5086947679519653}]}, {"text": " Table 6: Effect of translation errors on BLEU score, TER and GTM on all judged sentences and correla- tion with their corresponding effects on human quality scores (from", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 42, "end_pos": 52, "type": "METRIC", "confidence": 0.9829369485378265}, {"text": "TER", "start_pos": 54, "end_pos": 57, "type": "METRIC", "confidence": 0.99671870470047}, {"text": "GTM", "start_pos": 62, "end_pos": 65, "type": "METRIC", "confidence": 0.7346363067626953}]}]}