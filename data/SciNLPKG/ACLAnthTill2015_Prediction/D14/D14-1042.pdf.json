{"title": [{"text": "Werdy: Recognition and Disambiguation of Verbs and Verb Phrases with Syntactic and Semantic Pruning", "labels": [], "entities": [{"text": "Recognition and Disambiguation of Verbs and Verb Phrases", "start_pos": 7, "end_pos": 63, "type": "TASK", "confidence": 0.8476776331663132}]}], "abstractContent": [{"text": "Word-sense recognition and disambigua-tion (WERD) is the task of identifying word phrases and their senses in natural language text.", "labels": [], "entities": [{"text": "Word-sense recognition and disambigua-tion (WERD)", "start_pos": 0, "end_pos": 49, "type": "TASK", "confidence": 0.7045622595718929}, {"text": "identifying word phrases and their senses in natural language text", "start_pos": 65, "end_pos": 131, "type": "TASK", "confidence": 0.4915612518787384}]}, {"text": "Though it is well understood how to disambiguate noun phrases, this task is much less studied for verbs and verbal phrases.", "labels": [], "entities": []}, {"text": "We present Werdy, a framework for WERD with particular focus on verbs and verbal phrases.", "labels": [], "entities": [{"text": "Werdy", "start_pos": 11, "end_pos": 16, "type": "DATASET", "confidence": 0.5318634510040283}, {"text": "WERD", "start_pos": 34, "end_pos": 38, "type": "DATASET", "confidence": 0.5138553977012634}]}, {"text": "Our framework first identifies multi-word expressions based on the syntactic structure of the sentence; this allows us to recognize both contiguous and non-contiguous phrases.", "labels": [], "entities": []}, {"text": "We then generate a list of candidate senses for each word or phrase, using novel syntactic and semantic pruning techniques.", "labels": [], "entities": []}, {"text": "We also construct and leverage anew resource of pairs of senses for verbs and their object arguments.", "labels": [], "entities": []}, {"text": "Finally, we feed the so-obtained candidate senses into standard word-sense disambiguation (WSD) methods, and boost their precision and recall.", "labels": [], "entities": [{"text": "word-sense disambiguation (WSD)", "start_pos": 64, "end_pos": 95, "type": "TASK", "confidence": 0.8096575319766999}, {"text": "precision", "start_pos": 121, "end_pos": 130, "type": "METRIC", "confidence": 0.9992105960845947}, {"text": "recall", "start_pos": 135, "end_pos": 141, "type": "METRIC", "confidence": 0.9971919655799866}]}, {"text": "Our experiments indicate that Werdy significantly increases the performance of existing WSD methods.", "labels": [], "entities": [{"text": "Werdy", "start_pos": 30, "end_pos": 35, "type": "METRIC", "confidence": 0.48908495903015137}, {"text": "WSD", "start_pos": 88, "end_pos": 91, "type": "TASK", "confidence": 0.9756386876106262}]}], "introductionContent": [{"text": "Understanding the semantics of words and multiword expressions in natural language text is an important task for automatic knowledge acquisition.", "labels": [], "entities": [{"text": "Understanding the semantics of words and multiword expressions in natural language text", "start_pos": 0, "end_pos": 87, "type": "TASK", "confidence": 0.802815705537796}, {"text": "automatic knowledge acquisition", "start_pos": 113, "end_pos": 144, "type": "TASK", "confidence": 0.6361550589402517}]}, {"text": "It serves as a fundamental building block in a wide area of applications, including semantic parsing, question answering, paraphrasing, knowledge base construction, etc.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 84, "end_pos": 100, "type": "TASK", "confidence": 0.7496466934680939}, {"text": "question answering", "start_pos": 102, "end_pos": 120, "type": "TASK", "confidence": 0.8832531571388245}, {"text": "knowledge base construction", "start_pos": 136, "end_pos": 163, "type": "TASK", "confidence": 0.6124477386474609}]}, {"text": "In this paper, we study the task of word-sense recognition and disambiguation (WERD) with a focus on verbs and verbal phrases.", "labels": [], "entities": [{"text": "word-sense recognition and disambiguation (WERD)", "start_pos": 36, "end_pos": 84, "type": "TASK", "confidence": 0.7902184469359261}]}, {"text": "Verbs are the central element in a sentence, and the key to understand the relations between sets of entities expressed in a sentence.", "labels": [], "entities": []}, {"text": "We propose Werdy, a method to (i) automatically recognize in natural language text both single words and multi-word phrases that match entries in a lexical knowledge base (KB) like WordNet, and (ii) disambiguate these words or phrases by identifying their senses in the KB.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 181, "end_pos": 188, "type": "DATASET", "confidence": 0.9529173970222473}]}, {"text": "WordNet is a comprehensive lexical resource for word-sense disambiguation (WSD), covering nouns, verbs, adjectives, adverbs, and many multiword expressions.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9663304090499878}, {"text": "word-sense disambiguation (WSD)", "start_pos": 48, "end_pos": 79, "type": "TASK", "confidence": 0.8419456124305725}]}, {"text": "In the following, the notion of an entry refers to a word or phrase in the KB, whereas a sense denotes the lexical synset of the entry's meaning in the given sentence.", "labels": [], "entities": []}, {"text": "A key challenge for recognizing KB entries in natural language text is that entries often consist of multiple words.", "labels": [], "entities": [{"text": "recognizing KB entries in natural language text", "start_pos": 20, "end_pos": 67, "type": "TASK", "confidence": 0.818593008177621}]}, {"text": "In WordNet-3.0 more than 40% of the entries are multi-word.", "labels": [], "entities": [{"text": "WordNet-3.0", "start_pos": 3, "end_pos": 14, "type": "DATASET", "confidence": 0.958508312702179}]}, {"text": "Such entries are challenging to recognize accurately for two main reasons: First, the components of multi-word entries in the KB (such as fiscal year) often consist of components that are themselves KB entries (fiscal and year).", "labels": [], "entities": []}, {"text": "Second, multi-word entries (such as take a breath) may not appear consecutively in a sentence (\"He takes a deep breath.\").", "labels": [], "entities": []}, {"text": "Werdy addresses the latter problem by (conceptually) matching the syntactic structure of the KB entries to the syntactic structure of the input sentence.", "labels": [], "entities": []}, {"text": "To address the former problem, Werdy identifies all possible entries in a sentence and passes them to the disambiguation phase (take, breath, take a breath, . .", "labels": [], "entities": []}, {"text": "); the disambiguation phase provides more information about which multi-word entries to keep.", "labels": [], "entities": []}, {"text": "Thus, our method solves the recognition and the disambiguation tasks jointly.", "labels": [], "entities": []}, {"text": "Once KB entries have been identified, Werdy disambiguates each entry against its possible senses.", "labels": [], "entities": [{"text": "Werdy disambiguates", "start_pos": 38, "end_pos": 57, "type": "TASK", "confidence": 0.6874906122684479}]}, {"text": "State-of-the-art methods for WSD) work fairly well for nouns and noun phrases.", "labels": [], "entities": [{"text": "WSD)", "start_pos": 29, "end_pos": 33, "type": "TASK", "confidence": 0.984591543674469}]}, {"text": "However, the disambiguation of verbs and verbal phrases has received much less attention in the literature.", "labels": [], "entities": [{"text": "disambiguation of verbs and verbal phrases", "start_pos": 13, "end_pos": 55, "type": "TASK", "confidence": 0.8291859825452169}]}, {"text": "WSD methods can be roughly categorized into (i) methods that are based on supervised training over sense-annotated corpora (e.g.,), and (ii) methods that harness KB's to assess the semantic relatedness among word senses for mapping entries to senses (e.g.,).", "labels": [], "entities": [{"text": "WSD", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.9086349010467529}]}, {"text": "For these methods, mapping verbs to senses is a difficult task since verbs tend to have more senses than nouns.", "labels": [], "entities": []}, {"text": "In WordNet, including monosemous words, there are on average 1.24 senses per noun and 2.17 per verb.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 3, "end_pos": 10, "type": "DATASET", "confidence": 0.9446119070053101}]}, {"text": "To disambiguate verbs and verbal phrases, Werdy proceeds in multiple steps.", "labels": [], "entities": []}, {"text": "First, Werdy obtains the set of candidate senses for each recognized entry from the KB.", "labels": [], "entities": []}, {"text": "Second, it reduces the set of candidate entries using novel syntactic and semantic pruning techniques.", "labels": [], "entities": []}, {"text": "The key insight behind our syntactic pruning is that each verb sense tends to occur in a only limited number of syntactic patterns.", "labels": [], "entities": []}, {"text": "For example, the sentence \"Albert Einstein remained in Princeton\" has a subject (\"Albert Einstein\"), a verb (\"remained\") and an adverbial (\"in Princeton\"), it follows an SVA clause pattern.", "labels": [], "entities": []}, {"text": "We can thus safely prune verb senses that do not match the syntactic structure of the sentence.", "labels": [], "entities": []}, {"text": "Moreover, each verb sense is compatible with only a limited number of semantic argument types (such as location, river, person, musician, etc); this phenomena is called selectional preference or selectional restriction.", "labels": [], "entities": []}, {"text": "Senses that are compatible only with argument types not present in the sentence can be pruned.", "labels": [], "entities": []}, {"text": "Our pruning steps are based on the idea that a verb selects the categories of its arguments both syntactically (c-selection) and semantically (s-selection).", "labels": [], "entities": []}, {"text": "In the final step, Werdy employs a state-of-the-art general WSD method to select the most suitable sense from the remaining candidates.", "labels": [], "entities": [{"text": "WSD", "start_pos": 60, "end_pos": 63, "type": "TASK", "confidence": 0.9024262428283691}]}, {"text": "Since incorrect senses have already been greatly pruned, this step significantly gains accuracy and efficiency over standard WSD.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 87, "end_pos": 95, "type": "METRIC", "confidence": 0.9995284080505371}]}, {"text": "Our semantic pruning technique builds on a newly created resource of pairs of senses for verbs and their object arguments.", "labels": [], "entities": []}, {"text": "For example, the WordNet verb sense play-1 (i.e., the 1st sense of the verb entry \"play\") selects as direct object the noun sense sport-1.", "labels": [], "entities": [{"text": "WordNet verb sense play-1", "start_pos": 17, "end_pos": 42, "type": "DATASET", "confidence": 0.8174870610237122}]}, {"text": "We refer to this novel resource as the VO Sense Repository, or VOS repository for short.", "labels": [], "entities": [{"text": "VO Sense Repository", "start_pos": 39, "end_pos": 58, "type": "DATASET", "confidence": 0.7954190969467163}, {"text": "VOS repository", "start_pos": 63, "end_pos": 77, "type": "DATASET", "confidence": 0.932460606098175}]}, {"text": "1 It is constructed from the WordNet gloss-tags corpus, the SemCor dataset, and a small set of manually created VO sense pairs.", "labels": [], "entities": [{"text": "WordNet gloss-tags corpus", "start_pos": 29, "end_pos": 54, "type": "DATASET", "confidence": 0.9409183263778687}, {"text": "SemCor dataset", "start_pos": 60, "end_pos": 74, "type": "DATASET", "confidence": 0.9154568016529083}]}, {"text": "We evaluated Werdy on the SemEval-2007 coarse-grained WSD task (, both with and without automatic recognition of entries.", "labels": [], "entities": [{"text": "SemEval-2007 coarse-grained WSD task", "start_pos": 26, "end_pos": 62, "type": "TASK", "confidence": 0.49002353847026825}]}, {"text": "We found that our techniques boost state-ofthe-art WSD methods and obtain high-quality results.", "labels": [], "entities": [{"text": "WSD", "start_pos": 51, "end_pos": 54, "type": "TASK", "confidence": 0.9841872453689575}]}, {"text": "Werdy significantly increases the precision and recall of the best performing baselines.", "labels": [], "entities": [{"text": "Werdy", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.7745131850242615}, {"text": "precision", "start_pos": 34, "end_pos": 43, "type": "METRIC", "confidence": 0.9995670914649963}, {"text": "recall", "start_pos": 48, "end_pos": 54, "type": "METRIC", "confidence": 0.9985985159873962}]}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 gives an overview of Werdy components.", "labels": [], "entities": [{"text": "Werdy", "start_pos": 31, "end_pos": 36, "type": "DATASET", "confidence": 0.5469817519187927}]}, {"text": "Section 3 presents the entry recognition, and Sections 4 and 5 discuss our novel syntactic and semantic pruning techniques.", "labels": [], "entities": [{"text": "entry recognition", "start_pos": 23, "end_pos": 40, "type": "TASK", "confidence": 0.9836374223232269}]}, {"text": "Section 6 presents the Semantic VO Repository and how we constructed it.", "labels": [], "entities": []}, {"text": "Section 7 gives the results of our evaluation.", "labels": [], "entities": []}, {"text": "Section 8 discusses related work.", "labels": [], "entities": []}], "datasetContent": [{"text": "We tested Werdy on the SemEval-2007 coarse-grained dataset.", "labels": [], "entities": [{"text": "SemEval-2007 coarse-grained dataset", "start_pos": 23, "end_pos": 58, "type": "DATASET", "confidence": 0.777486244837443}]}, {"text": "It consists of five senseannotated documents; the sense annotations refer to a coarse-grained version of WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 105, "end_pos": 112, "type": "DATASET", "confidence": 0.9648330807685852}]}, {"text": "In addition to sense annotations, the corpus also provides the corresponding KB entries (henceforth termed \"gold entries\") as well as a POS tag.", "labels": [], "entities": []}, {"text": "We restrict our evaluation to verbs that act as clause heads.", "labels": [], "entities": []}, {"text": "In total, 461 such verbs were recognized by ClausIE and the Stanford Parser (.", "labels": [], "entities": [{"text": "ClausIE", "start_pos": 44, "end_pos": 51, "type": "DATASET", "confidence": 0.9540451765060425}, {"text": "Stanford Parser (", "start_pos": 60, "end_pos": 77, "type": "DATASET", "confidence": 0.909196158250173}]}, {"text": "For the final step of Werdy, we used the KB-based WSD algorithms of and It-MakesSense (, a state-of-the-art supervised system that was the best performer in SemEval-2007.", "labels": [], "entities": []}, {"text": "Each method only labels entries for which it is sufficiently confident.", "labels": [], "entities": []}, {"text": "Simplified Extended Lesk (SimpleExtLesk).", "labels": [], "entities": []}, {"text": "Each entry is assigned the sense with highest term overlap between the entry's context (words in the sentence) and both the sense's gloss) as well as the glosses of its neighbors.", "labels": [], "entities": []}, {"text": "A sense is output only if the overlap exceeds some threshold; we used thresholds in the range of 1-20 in our experiments.", "labels": [], "entities": []}, {"text": "There are many subtleties and details in the implementation of SimpleExtLesk so we used two different libraries: a Java implementation of WordNet::Similarity (), which we modified to accept a context string, and DKPro-WSD () version 1.1.0, with lemmatization, removal of stop words, paired overlap enabled and normalization disabled.", "labels": [], "entities": []}, {"text": "The method collects all paths connecting each candidate sense of an entry to the set of candidate senses of the words the entry's context.", "labels": [], "entities": []}, {"text": "The candidate sense with the highest degree in the resulting subgraph is selected.", "labels": [], "entities": [{"text": "degree", "start_pos": 37, "end_pos": 43, "type": "METRIC", "confidence": 0.9503719210624695}]}, {"text": "We implemented this algorithm using the Neo4j library.", "labels": [], "entities": [{"text": "Neo4j library", "start_pos": 40, "end_pos": 53, "type": "DATASET", "confidence": 0.9733881056308746}]}, {"text": "We used a fixed threshold of 1 and vary the search depth in range 1-20.", "labels": [], "entities": []}, {"text": "We used the candidate senses of all nouns and verbs in a sentence as context.", "labels": [], "entities": []}, {"text": "A state-of-the-art, publicly available supervised system () and a refined version of, which ranked first in the SemEval-2007 coarse grained task.", "labels": [], "entities": [{"text": "SemEval-2007 coarse grained task", "start_pos": 112, "end_pos": 144, "type": "TASK", "confidence": 0.5505042672157288}]}, {"text": "We modified the code to accept KB entries and their candidate senses.", "labels": [], "entities": []}, {"text": "We tested both in WordNet-2.1 and 3.0; for the later we mapped Werdy's set of candidates to WordNet-2.1.", "labels": [], "entities": [{"text": "WordNet-2.1", "start_pos": 18, "end_pos": 29, "type": "DATASET", "confidence": 0.9752848148345947}, {"text": "Werdy's set", "start_pos": 63, "end_pos": 74, "type": "DATASET", "confidence": 0.7674828171730042}, {"text": "WordNet-2.1", "start_pos": 92, "end_pos": 103, "type": "DATASET", "confidence": 0.9724493622779846}]}, {"text": "Most Frequent Sense (MFS).", "labels": [], "entities": [{"text": "Most Frequent Sense (MFS)", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.5965402772029241}]}, {"text": "Selects the most frequent sense (according to WordNet frequencies) among the set of candidate senses of an entry.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 46, "end_pos": 53, "type": "DATASET", "confidence": 0.8996096253395081}]}, {"text": "If there is a tie, we do not label.", "labels": [], "entities": []}, {"text": "Note that this procedure differs slightly from the standard of picking the entry with the smallest sense id.", "labels": [], "entities": []}, {"text": "We do not follow this approach since it cannot handle well overlapping entries.", "labels": [], "entities": []}, {"text": "When one of the above methods fails to provide a sense label (or provides more than one), we used the MFS method above with a threshold of 1.", "labels": [], "entities": []}, {"text": "This procedure increased the performance in all cases.", "labels": [], "entities": []}, {"text": "The disambiguation was performed with respect to coarse-grained sense clusters.", "labels": [], "entities": []}, {"text": "The score of a cluster is the sum of the individual scores of its senses (except for IMS which provides only one answer per word); the cluster with the highest score was selected.", "labels": [], "entities": []}, {"text": "Our source code and the results of our evaluation are publicly available . The SemEval-2007 task was not designed for automatic entry recognition, for each word or multi-word expression it provides the WordNet entry and the POS tag.", "labels": [], "entities": [{"text": "automatic entry recognition", "start_pos": 118, "end_pos": 145, "type": "TASK", "confidence": 0.6146737436453501}]}, {"text": "We proceeded as follows to handle multi-word entries.", "labels": [], "entities": []}, {"text": "In the WSD step, we considered the candidate senses of all recognized entries that overlap with the gold entry.", "labels": [], "entities": [{"text": "WSD", "start_pos": 7, "end_pos": 10, "type": "TASK", "confidence": 0.8662005066871643}]}, {"text": "For example, we considered the candidate senses of entries take, breath, and take a breath for gold entry take a breath.", "labels": [], "entities": []}, {"text": "The SemEval-2007 task uses WordNet-2.1 but Werdy uses WordNet-3.0.", "labels": [], "entities": [{"text": "WordNet-2.1", "start_pos": 27, "end_pos": 38, "type": "DATASET", "confidence": 0.9685816764831543}, {"text": "WordNet-3.0", "start_pos": 54, "end_pos": 65, "type": "DATASET", "confidence": 0.9717854857444763}]}, {"text": "We mapped both the sense keys and clusters from WordNet-2.1 to WordNet-3.0.", "labels": [], "entities": [{"text": "WordNet-2.1", "start_pos": 48, "end_pos": 59, "type": "DATASET", "confidence": 0.9803932309150696}, {"text": "WordNet-3.0", "start_pos": 63, "end_pos": 74, "type": "DATASET", "confidence": 0.9501285552978516}]}, {"text": "All senses in WordNet-3.0 that could not be mapped to any cluster were consider to belong each of them to a single sense cluster.", "labels": [], "entities": [{"text": "WordNet-3.0", "start_pos": 14, "end_pos": 25, "type": "DATASET", "confidence": 0.9746257662773132}]}, {"text": "Note that this procedure is fair: for such senses  the disambiguation is equivalent to a fine-grained disambiguation, which is harder.", "labels": [], "entities": []}, {"text": "Our results are displayed in.", "labels": [], "entities": []}, {"text": "We ran each algorithm with the gold KB entries provided by in the dataset (+ in column \"gold entry) as well as the entries obtained by our method of Sec.", "labels": [], "entities": []}, {"text": "We also enabled (+) and disabled (-) the pruning steps as well as the MFS back-off strategy.", "labels": [], "entities": []}, {"text": "The highest F1 score was achieved by SimpleExtLesk (DKPro) with pruning and MFS back-off: 81.18 with gold entries and 78.52 with automatic entry recognition.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9761718511581421}, {"text": "SimpleExtLesk (DKPro", "start_pos": 37, "end_pos": 57, "type": "DATASET", "confidence": 0.6662737727165222}, {"text": "pruning", "start_pos": 64, "end_pos": 71, "type": "METRIC", "confidence": 0.987789511680603}, {"text": "MFS back-off", "start_pos": 76, "end_pos": 88, "type": "METRIC", "confidence": 0.8442258536815643}]}, {"text": "In all cases, our syntactic and semantic pruning strategy increased performance (up to +10.85 F1 points).", "labels": [], "entities": [{"text": "F1", "start_pos": 94, "end_pos": 96, "type": "METRIC", "confidence": 0.9945439100265503}]}, {"text": "We next discuss the impact of the various steps of Werdy in detail.", "labels": [], "entities": [{"text": "Werdy", "start_pos": 51, "end_pos": 56, "type": "DATASET", "confidence": 0.5897019505500793}]}, {"text": "When no gold entries were provided, performance dropped due to the increase of sense candidates for multi-word expressions, which include the possible senses of the expression itself as well as the senses of the entry's parts that are them-  Step-by-step results selves WordNet entries.", "labels": [], "entities": []}, {"text": "Our entry recognizer tends to do a good job since it managed to correctly identify all the relevant entries except in two cases (i.e. \"take up\" and \"get rolling\"), in which the dependency parse was incorrect.", "labels": [], "entities": [{"text": "entry recognizer", "start_pos": 4, "end_pos": 20, "type": "TASK", "confidence": 0.6998145580291748}]}, {"text": "The drop in F1 for our automatic entry recognition was mainly due to incorrect selection of the correct entry of a set of alternative, overlapping entries.", "labels": [], "entities": [{"text": "F1", "start_pos": 12, "end_pos": 14, "type": "METRIC", "confidence": 0.9921013712882996}, {"text": "automatic entry recognition", "start_pos": 23, "end_pos": 50, "type": "TASK", "confidence": 0.6082292596499125}]}, {"text": "Syntactic pruning did not prune the correct sense inmost cases.", "labels": [], "entities": []}, {"text": "In 16 cases (with gold entries), however, the correct sense was pruned.", "labels": [], "entities": []}, {"text": "Five of these senses were pruned due to incorrect dependency parses, which led to incorrect frame identification.", "labels": [], "entities": [{"text": "frame identification", "start_pos": 92, "end_pos": 112, "type": "TASK", "confidence": 0.7118715345859528}]}, {"text": "In two cases, the sense was not annotated with the recognized frame in WordNet, although it seemed adequate.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 71, "end_pos": 78, "type": "DATASET", "confidence": 0.9777632355690002}]}, {"text": "In the remaining cases, a general frame from WordNet was incorrectly omitted.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 45, "end_pos": 52, "type": "DATASET", "confidence": 0.9702592492103577}]}, {"text": "Improvements to WordNet's frame annotations may thus make syntactic pruning even more effective.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 16, "end_pos": 23, "type": "DATASET", "confidence": 0.9314523935317993}]}, {"text": "Semantic pruning also improves performance.", "labels": [], "entities": []}, {"text": "Here the correct sense was pruned for 11 verbs, mainly due to the noisiness and incompleteness of our VOS repository.", "labels": [], "entities": [{"text": "VOS repository", "start_pos": 102, "end_pos": 116, "type": "DATASET", "confidence": 0.9628719091415405}]}, {"text": "Without using gold entries, we found in total 237 semantic matches between possible verbs senses and possible object senses (200 with gold entries).", "labels": [], "entities": []}, {"text": "We also found that our manual annotations in the VOS repository (see Sec. 6) did not affect our experiments.", "labels": [], "entities": [{"text": "VOS repository", "start_pos": 49, "end_pos": 63, "type": "DATASET", "confidence": 0.9763190746307373}]}, {"text": "The results show that syntactic and semantic pruning are beneficial for verb sense disambiguation, but also stress the necessity to improve existing resources.", "labels": [], "entities": [{"text": "verb sense disambiguation", "start_pos": 72, "end_pos": 97, "type": "TASK", "confidence": 0.7507969737052917}]}, {"text": "Ideally, each verb sense would be annotated with both the possible clause types or syntactic patterns in which it can occur as well as the possible senses of its objects.", "labels": [], "entities": []}, {"text": "Annotations for subjects and adverbial arguments may also be beneficial.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Results on SemEval-2007 coarse-grained (verbs as clause heads)", "labels": [], "entities": [{"text": "SemEval-2007 coarse-grained (verbs as clause heads", "start_pos": 21, "end_pos": 71, "type": "TASK", "confidence": 0.6869567675249917}]}]}