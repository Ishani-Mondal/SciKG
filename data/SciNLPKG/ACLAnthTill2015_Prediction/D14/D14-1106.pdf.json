{"title": [{"text": "Data Driven Grammatical Error Detection in Transcripts of Children's Speech", "labels": [], "entities": [{"text": "Data Driven Grammatical Error Detection in Transcripts of Children's Speech", "start_pos": 0, "end_pos": 75, "type": "TASK", "confidence": 0.6548191227696158}]}], "abstractContent": [{"text": "We investigate grammatical error detection in spoken language, and present a data-driven method to train a dependency parser to automatically identify and label grammatical errors.", "labels": [], "entities": [{"text": "grammatical error detection in spoken language", "start_pos": 15, "end_pos": 61, "type": "TASK", "confidence": 0.7904786566893259}]}, {"text": "This method is agnostic to the label set used, and the only manual annotations needed for training are grammatical error labels.", "labels": [], "entities": []}, {"text": "We find that the proposed system is robust to disfluencies, so that a separate stage to elide disfluen-cies is not required.", "labels": [], "entities": []}, {"text": "The proposed system outperforms two baseline systems on two different corpora that use different sets of error tags.", "labels": [], "entities": []}, {"text": "It is able to identify utterances with grammatical errors with an F1-score as high as 0.623, as compared to a baseline F1 of 0.350 on the same data.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 66, "end_pos": 74, "type": "METRIC", "confidence": 0.9995512366294861}, {"text": "F1", "start_pos": 119, "end_pos": 121, "type": "METRIC", "confidence": 0.9587909579277039}]}], "introductionContent": [{"text": "Research into automatic grammatical error detection has primarily been motivated by the task of providing feedback to writers, whether they be native speakers of a language or second language learners.", "labels": [], "entities": [{"text": "automatic grammatical error detection", "start_pos": 14, "end_pos": 51, "type": "TASK", "confidence": 0.6635744124650955}]}, {"text": "Grammatical error detection, however, is also useful in the clinical domain, for example, to assess a child's ability to produce grammatical language.", "labels": [], "entities": [{"text": "Grammatical error detection", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.8902492125829061}]}, {"text": "At present, clinicians and researchers into child language must manually identify and classify particular kinds of grammatical errors in transcripts of children's speech if they wish to assess particular aspects of the child's linguistic ability from a sample of spoken language.", "labels": [], "entities": []}, {"text": "Such manual annotation, which is called language sample analysis in the clinical field, is expensive, hindering its widespread adoption.", "labels": [], "entities": [{"text": "language sample analysis", "start_pos": 40, "end_pos": 64, "type": "TASK", "confidence": 0.6304352780183157}]}, {"text": "Manual annotations may also be inconsistent, particularly between different research groups, which maybe investigating different phenomena.", "labels": [], "entities": []}, {"text": "Automated grammatical error detection has the potential to address both of these issues, being both cheap and consistent.", "labels": [], "entities": [{"text": "grammatical error detection", "start_pos": 10, "end_pos": 37, "type": "TASK", "confidence": 0.6987425287564596}]}, {"text": "Aside from performance, there are at least two key requirements fora grammatical error detector to be useful in a clinical setting: 1) it must be able to handle spoken language, and 2) it must be trainable.", "labels": [], "entities": [{"text": "grammatical error detector", "start_pos": 69, "end_pos": 95, "type": "TASK", "confidence": 0.6232718626658121}]}, {"text": "Clinical data typically consists of transcripts of spoken language, rather than formal written language.", "labels": [], "entities": []}, {"text": "As a result, a system must be prepared to handle disfluencies, utterance fragments, and other phenomena that are entirely grammatical in speech, but not in writing.", "labels": [], "entities": []}, {"text": "On the other hand, a system designed for transcripts of speech does not need to identify errors specific to written language such as punctuation or spelling mistakes.", "labels": [], "entities": []}, {"text": "Furthermore, a system designed for clinical data must be able to handle language produced by children who may have atypical language due to a developmental disorder, and therefore may produce grammatical errors that would be unexpected in written language.", "labels": [], "entities": []}, {"text": "A grammatical error detector appropriate fora clinical setting must also be trainable because different groups of clinicians may wish to investigate different phenomena, and will therefore prefer different annotation standards.", "labels": [], "entities": []}, {"text": "This is quite different from grammatical error detectors for written language, which may have models for different domains, but which are not typically designed to enable the detection of novel error sets.", "labels": [], "entities": []}, {"text": "We examine two baseline techniques for grammatical error detection, then present a simple datadriven technique to turn a dependency parser into a grammatical error detector.", "labels": [], "entities": [{"text": "grammatical error detection", "start_pos": 39, "end_pos": 66, "type": "TASK", "confidence": 0.7712463537851969}]}, {"text": "Interestingly, we find that the dependency parser-based approach massively outperforms the baseline systems in terms of identifying ungrammatical utterances.", "labels": [], "entities": [{"text": "dependency parser-based", "start_pos": 32, "end_pos": 55, "type": "TASK", "confidence": 0.774044394493103}]}, {"text": "Furthermore, the proposed system is able to identify specific error codes, which the baseline systems cannot do.", "labels": [], "entities": []}, {"text": "We find that disfluencies do not degrade performance of the proposed detector, obviating the need (for this task) for explicit disfluency detection.", "labels": [], "entities": [{"text": "explicit disfluency detection", "start_pos": 118, "end_pos": 147, "type": "TASK", "confidence": 0.6509888271490732}]}, {"text": "We also analyze the output of our system to see which errors it finds, and which it misses.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our investigation into using a dependency parser to identify and label grammatical errors requires training data with two types of annotations: dependency labels, and grammatical error labels.", "labels": [], "entities": []}, {"text": "We are not aware of any corpora of speech with both of these annotations.", "labels": [], "entities": []}, {"text": "Therefore, we use two different sets of training data: the Switchboard corpus, which contains syntactic parses; and SALT annotated corpora, which have grammatical error annotations.", "labels": [], "entities": [{"text": "Switchboard corpus", "start_pos": 59, "end_pos": 77, "type": "DATASET", "confidence": 0.7993187606334686}]}, {"text": "Evaluating system performance in tagging tasks on manually annotated data is typically straight-  forward: we simply compare system output to the gold standard.", "labels": [], "entities": [{"text": "tagging tasks", "start_pos": 33, "end_pos": 46, "type": "TASK", "confidence": 0.9164360761642456}]}, {"text": "Such evaluation assumes that the best system is the one that most faithfully reproduces the gold standard.", "labels": [], "entities": []}, {"text": "This is not necessarily the case with applying SALT error codes for three reasons, and each of these reasons suggests a different form of evaluation.", "labels": [], "entities": []}, {"text": "First, automatically detecting SALT error codes is an important task because it can aid clinical investigations.", "labels": [], "entities": [{"text": "automatically detecting SALT error codes", "start_pos": 7, "end_pos": 47, "type": "TASK", "confidence": 0.7255950570106506}]}, {"text": "As illustrated, even extremely coarse features derived from SALT annotations, for example a binary feature for each utterance indicating the presence of any error codes, can be of immense utility for identifying language impairments.", "labels": [], "entities": []}, {"text": "Therefore, we will evaluate our system as a binary tagger: each utterance, both in the manually annotated data and system output either contains an error code, or it does not.", "labels": [], "entities": []}, {"text": "We will label this form of evaluation as UTTERANCE level.", "labels": [], "entities": [{"text": "UTTERANCE level", "start_pos": 41, "end_pos": 56, "type": "METRIC", "confidence": 0.9694932997226715}]}, {"text": "Second, clinicians are not only interested in how many utterances have an error, but also which particular errors appear in which utterances.", "labels": [], "entities": []}, {"text": "To address this issue, we will compute precision, recall, and F1 score from the counts of each error code in each utterance.", "labels": [], "entities": [{"text": "precision", "start_pos": 39, "end_pos": 48, "type": "METRIC", "confidence": 0.9994664788246155}, {"text": "recall", "start_pos": 50, "end_pos": 56, "type": "METRIC", "confidence": 0.9992133378982544}, {"text": "F1 score", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.9834079444408417}]}, {"text": "We will label this form of evaluation as ERROR level.", "labels": [], "entities": [{"text": "ERROR level", "start_pos": 41, "end_pos": 52, "type": "METRIC", "confidence": 0.968948632478714}]}, {"text": "illustrates both UTTERANCE and ERROR level evaluation.", "labels": [], "entities": [{"text": "UTTERANCE", "start_pos": 17, "end_pos": 26, "type": "METRIC", "confidence": 0.8464469313621521}, {"text": "ERROR", "start_pos": 31, "end_pos": 36, "type": "METRIC", "confidence": 0.9619714021682739}]}, {"text": "Note that the utterance level error code is only allowed to appear once per utterance.", "labels": [], "entities": []}, {"text": "As a result, we will ignore any predicted codes beyond the first.", "labels": [], "entities": []}, {"text": "Third, the quality of the SALT annotations themselves is unknown, and therefore evaluation in which we treat the manually annotated data as a gold standard may not yield informative metrics.", "labels": [], "entities": []}, {"text": "found that there are likely inconsistencies in maze annotations both within and across corpora.", "labels": [], "entities": []}, {"text": "In light of that finding, it is possible that error code annotations are somewhat inconsistent as well.", "labels": [], "entities": []}, {"text": "Furthermore, our approach has a critical difference from manual annotation: we perform classification one utterance at a time, while manual annotators have access to the context of an utterance.", "labels": [], "entities": []}, {"text": "Therefore certain types of errors, for example using a pronoun of the wrong gender, or responding ungrammatically to a question (ex.", "labels": [], "entities": []}, {"text": "'What are you doing?'", "labels": [], "entities": []}, {"text": "will appear grammatical to our system, but not to a human annotator.", "labels": [], "entities": []}, {"text": "We address both of these issues with an indepth analysis of the output of one of our systems, which includes manually re-coding utterances out of context.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Baseline and current paper systems' performance on ENNI. Evaluation is at the UTTERANCE  (UTT) level except for the current paper's system, which also presents evaluation at the ERROR (ERR)  level.", "labels": [], "entities": [{"text": "ENNI", "start_pos": 61, "end_pos": 65, "type": "DATASET", "confidence": 0.843599796295166}, {"text": "UTTERANCE  (UTT) level", "start_pos": 88, "end_pos": 110, "type": "METRIC", "confidence": 0.9330974340438842}, {"text": "ERROR (ERR)  level", "start_pos": 188, "end_pos": 206, "type": "METRIC", "confidence": 0.9063640236854553}]}, {"text": " Table 4: ERROR level detection performance for  each code (system trained on ENNI; 30% error  utterances; ZHANG feature set; with mazes)", "labels": [], "entities": [{"text": "ERROR level detection", "start_pos": 10, "end_pos": 31, "type": "TASK", "confidence": 0.809592088063558}, {"text": "ENNI", "start_pos": 78, "end_pos": 82, "type": "DATASET", "confidence": 0.7552414536476135}]}, {"text": " Table 5: System performance using ERROR level evaluation on 200 utterances selected from ENNI-dev  using original and revised annotations as gold standard", "labels": [], "entities": [{"text": "ENNI-dev", "start_pos": 90, "end_pos": 98, "type": "DATASET", "confidence": 0.9446818828582764}]}, {"text": " Table 6: Error detection performance on NSR-development, mazes included", "labels": [], "entities": [{"text": "Error detection", "start_pos": 10, "end_pos": 25, "type": "TASK", "confidence": 0.8825122117996216}]}]}