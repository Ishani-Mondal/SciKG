{"title": [{"text": "Non-linear Mapping for Improved Identification of 1300+ Languages", "labels": [], "entities": [{"text": "Improved Identification of 1300+ Languages", "start_pos": 23, "end_pos": 65, "type": "TASK", "confidence": 0.8277973930040995}]}], "abstractContent": [{"text": "Non-linear mappings of the form P (ngram) \u03b3 and log(1+\u03c4 P (ngram)) log(1+\u03c4) are applied to the n-gram probabilities in five trainable open-source language identifiers.", "labels": [], "entities": []}, {"text": "The first mapping reduces classification errors by 4.0% to 83.9% over a test set of more than one million 65-character strings in 1366 languages, and by 2.6% to 76.7% over a subset of 781 languages.", "labels": [], "entities": [{"text": "errors", "start_pos": 41, "end_pos": 47, "type": "METRIC", "confidence": 0.7469077706336975}]}, {"text": "The second mapping improves four of the five identifiers by 10.6% to 83.8% on the larger corpus and 14.4% to 76.7% on the smaller corpus.", "labels": [], "entities": []}, {"text": "The subset corpus and the modified programs are made freely available for download at", "labels": [], "entities": []}], "introductionContent": [{"text": "Language identification, particularly of short strings, is a task which is becoming quite important as a preliminary step in much automated processing of online data streams such as microblogs (e.g. Twitter).", "labels": [], "entities": [{"text": "Language identification", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.6924043893814087}]}, {"text": "In addition, an increasing number of languages are represented online, so it is desireable that performance remain high as more languages are added to the identifier.", "labels": [], "entities": []}, {"text": "In this paper, we stress-test five open-source n-gram-based language identifiers by presenting them with 65-character strings (about one printed line of text in a book) in up to 1366 languages.", "labels": [], "entities": []}, {"text": "We then apply a simple modification to their scoring algorithms which improves the classification accuracy of all five of them, three quite dramatically.", "labels": [], "entities": [{"text": "classification", "start_pos": 83, "end_pos": 97, "type": "TASK", "confidence": 0.9247880578041077}, {"text": "accuracy", "start_pos": 98, "end_pos": 106, "type": "METRIC", "confidence": 0.9546443819999695}]}], "datasetContent": [{"text": "Using the data sets described in the previous section, we ran a sweep of different gamma and tau values for each language identifier to determine their optimal values on both development and test strings.", "labels": [], "entities": []}, {"text": "Step sizes for \u03b3 were generally 0.1, while those for \u03c4 were 1.0, with smaller steps near the minima.", "labels": [], "entities": []}, {"text": "Since it does not provide explicit control over model sizes, LangDetect was trained on a maximum of 1,000,000 bytes per model, as reported optimal in.", "labels": [], "entities": []}, {"text": "The other programs were trained on a maximum of 2,500,000 bytes per model; libtextcat and whatlang used default model sizes of 400 and 3500, respectively, while mguesser was set to the previouslyreported 1500 n-grams per model.", "labels": [], "entities": []}, {"text": "After some experimentation, YALI was set to use 5-grams, with 3500 n-grams per model to match whatlang.", "labels": [], "entities": [{"text": "YALI", "start_pos": 28, "end_pos": 32, "type": "METRIC", "confidence": 0.3717470169067383}]}, {"text": "show the absolute performance and relative percentage change in classification errors for the five programs using the two mapping functions, as well as the values of \u03b3 and \u03c4 at which the fewest errors were made on the development set.", "labels": [], "entities": []}, {"text": "Overall, the smaller corpus performed worse due to the greater percentage of Wikipedia texts, which are polluted with words and phrases in other languages.", "labels": [], "entities": []}, {"text": "In the test set, this occasionally causes a correct identification as another language to be scored as an error.", "labels": [], "entities": []}, {"text": "graph the classification error rates (number of incorrectly-labeled strings divided by total number of strings in the test set) in percent for different values of \u03b3.", "labels": [], "entities": [{"text": "classification error rates", "start_pos": 10, "end_pos": 36, "type": "METRIC", "confidence": 0.7812861402829488}]}, {"text": "A gamma of 1.0 is the baseline condition.", "labels": [], "entities": []}, {"text": "The dramatic improvements in mguesser, whatlang and YALI are quite evident, while the smaller but non-trivial im-: Language-identification accuracy on the 781-language corpus.", "labels": [], "entities": [{"text": "YALI", "start_pos": 52, "end_pos": 56, "type": "METRIC", "confidence": 0.9428375959396362}, {"text": "accuracy", "start_pos": 139, "end_pos": 147, "type": "METRIC", "confidence": 0.9919658899307251}, {"text": "781-language corpus", "start_pos": 155, "end_pos": 174, "type": "DATASET", "confidence": 0.7761180102825165}]}, {"text": "\u03b3 and \u03c4 were tuned on the 119-language development set.", "labels": [], "entities": []}, {"text": "libtextcat did not improve with the loglike mapping (see text).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Language-identification accuracy on the 1366-language corpus. \u03b3 and \u03c4 were tuned on the  220-language development set; only marginally better results can be achieved by tuning on the test set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9902253746986389}, {"text": "1366-language corpus", "start_pos": 50, "end_pos": 70, "type": "DATASET", "confidence": 0.8299367725849152}]}, {"text": " Table 2: Language-identification accuracy on the 781-language corpus. \u03b3 and \u03c4 were tuned on the 119- language development set. libtextcat did not improve with the loglike mapping (see text).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9847927093505859}, {"text": "781-language corpus", "start_pos": 50, "end_pos": 69, "type": "DATASET", "confidence": 0.8925259113311768}]}]}