{"title": [{"text": "Aligning English Strings with Abstract Meaning Representation Graphs", "labels": [], "entities": [{"text": "Aligning English Strings", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8997831543286642}]}], "abstractContent": [{"text": "We align pairs of English sentences and corresponding Abstract Meaning Representations (AMR), at the token level.", "labels": [], "entities": [{"text": "Abstract Meaning Representations (AMR)", "start_pos": 54, "end_pos": 92, "type": "TASK", "confidence": 0.7140558063983917}]}, {"text": "Such alignments will be useful for downstream extraction of semantic interpretation and generation rules.", "labels": [], "entities": [{"text": "downstream extraction of semantic interpretation", "start_pos": 35, "end_pos": 83, "type": "TASK", "confidence": 0.6719197630882263}]}, {"text": "Our method involves linearizing AMR structures and performing symmetrized EM training.", "labels": [], "entities": [{"text": "AMR structures", "start_pos": 32, "end_pos": 46, "type": "TASK", "confidence": 0.8202382922172546}]}, {"text": "We obtain 86.5% and 83.1% alignment F score on development and test sets.", "labels": [], "entities": [{"text": "alignment F score", "start_pos": 26, "end_pos": 43, "type": "METRIC", "confidence": 0.8878050645192465}]}], "introductionContent": [{"text": ", so there are no manually-annotated alignment links between English words and AMR concepts.", "labels": [], "entities": []}, {"text": "This paper studies how to build such links automatically, using co-occurrence and other information.", "labels": [], "entities": []}, {"text": "Automatic alignments maybe useful for downstream extraction of semantic interpretation and generation rules.", "labels": [], "entities": [{"text": "downstream extraction of semantic interpretation", "start_pos": 38, "end_pos": 86, "type": "TASK", "confidence": 0.7168494939804078}]}, {"text": "AMRs are directed, acyclic graphs with labeled edges, e.g., the sentence The boy wants to go is represented as: We have hand-aligned a subset of the 13,050 available AMR/English pairs.", "labels": [], "entities": []}, {"text": "We evaluate our automatic alignments against this gold standard.", "labels": [], "entities": []}, {"text": "A sample hand-aligned AMR is here (\"\u02dcn\" specifies a link to the nth English word): the boy wants to go (w / want-01\u02dc3 :arg0 (b / boy\u02dc2) :arg1 (g / go-01\u02dc5 :arg0 b)) This alignment problem resembles that of statistical machine translation (SMT).", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 206, "end_pos": 243, "type": "TASK", "confidence": 0.7714632451534271}]}, {"text": "It is easier in some ways, because AMR and English are highly cognate.", "labels": [], "entities": []}, {"text": "It is harder in other ways, as AMR is graphstructured, and children of an AMR node are unordered.", "labels": [], "entities": []}, {"text": "There are also fewer available training pairs than in SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 54, "end_pos": 57, "type": "TASK", "confidence": 0.981410026550293}]}, {"text": "One approach is to define a generative model from AMR graphs to strings.", "labels": [], "entities": []}, {"text": "We can then use EM to uncover hidden derivations, which alignments weakly reflect.", "labels": [], "entities": []}, {"text": "This approach is used in string/string SMT (.", "labels": [], "entities": [{"text": "string/string SMT", "start_pos": 25, "end_pos": 42, "type": "TASK", "confidence": 0.5173946246504784}]}, {"text": "However, we do not yet have such a generative graphto-string model, and even if we did, there might not bean efficient EM solution.", "labels": [], "entities": []}, {"text": "For example, in syntax-based SMT systems (), the generative tree/string transduction story is clear, but in the absence of alignment constraints, there are too many derivations and rules for EM to efficiently consider.", "labels": [], "entities": [{"text": "SMT", "start_pos": 29, "end_pos": 32, "type": "TASK", "confidence": 0.9079713225364685}, {"text": "generative tree/string transduction", "start_pos": 49, "end_pos": 84, "type": "TASK", "confidence": 0.7811845421791077}]}, {"text": "We therefore follow syntax-based SMT custom and use string/string alignment models in aligning our graph/string pairs.", "labels": [], "entities": [{"text": "SMT", "start_pos": 33, "end_pos": 36, "type": "TASK", "confidence": 0.9471076130867004}]}, {"text": "However, while it is straightforward to convert syntax trees into strings data (by taking yields), it is not obvious how to do this for unordered AMR graph elements.", "labels": [], "entities": []}, {"text": "The example above also shows that gold alignment links reach into the internal nodes of AMR.", "labels": [], "entities": [{"text": "AMR", "start_pos": 88, "end_pos": 91, "type": "DATASET", "confidence": 0.7475154399871826}]}, {"text": "Prior SMT work () describes alignment of semantic graphs and strings, though their experiments are limited to the GeoQuery domain, and their methods are not described in detail.", "labels": [], "entities": [{"text": "SMT", "start_pos": 6, "end_pos": 9, "type": "TASK", "confidence": 0.9895049333572388}]}, {"text": "describe a heuristic AMR/English aligner.", "labels": [], "entities": []}, {"text": "While heuristic aligners can achieve good accuracy, they will not automatically improve as more AMR/English data comes online.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.9970594048500061}, {"text": "AMR/English data", "start_pos": 96, "end_pos": 112, "type": "DATASET", "confidence": 0.6056794673204422}]}, {"text": "The contributions of this paper are: \u2022 A set of gold, manually-aligned AMR/English pairs.", "labels": [], "entities": [{"text": "AMR/English", "start_pos": 71, "end_pos": 82, "type": "TASK", "confidence": 0.5187393228212992}]}, {"text": "\u2022 An algorithm for automatically aligning AMR/English pairs.", "labels": [], "entities": [{"text": "AMR/English pairs", "start_pos": 42, "end_pos": 59, "type": "TASK", "confidence": 0.6507589444518089}]}, {"text": "\u2022 An empirical study establishing alignment accuracy of 86.5% and 83.1% F score for development and test sets respectively.", "labels": [], "entities": [{"text": "alignment", "start_pos": 34, "end_pos": 43, "type": "METRIC", "confidence": 0.5007774829864502}, {"text": "accuracy", "start_pos": 44, "end_pos": 52, "type": "METRIC", "confidence": 0.8984582424163818}, {"text": "F score", "start_pos": 72, "end_pos": 79, "type": "METRIC", "confidence": 0.9862834513187408}]}], "datasetContent": [{"text": "We use MGIZA++ ( as the implementation of the IBM models.", "labels": [], "entities": [{"text": "MGIZA++", "start_pos": 7, "end_pos": 14, "type": "DATASET", "confidence": 0.8864577412605286}]}, {"text": "We run Model 1 and HMM for 5 iterations each, then run our training algorithm on Model 4 for 4 iterations, at which point the alignments become stable.", "labels": [], "entities": []}, {"text": "As alignments are usually many to one from AMR to English, we compute the alignments from AMR to English in the final step.", "labels": [], "entities": []}, {"text": "shows the alignment accuracy for Model 1, HMM, Model 4, and Model 4 plus the modification described in section 2.2 (Model 4+).", "labels": [], "entities": [{"text": "alignment", "start_pos": 10, "end_pos": 19, "type": "TASK", "confidence": 0.6184138059616089}, {"text": "accuracy", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.911367654800415}]}, {"text": "The alignment accuracy on the test set is lower than the development set mainly because it is intrinsically a harder set, as we only made small modifications to the system based on the development set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.7684739232063293}]}, {"text": "Recall error due to stop words is one difference.", "labels": [], "entities": [{"text": "Recall error", "start_pos": 0, "end_pos": 12, "type": "METRIC", "confidence": 0.9311924576759338}]}, {"text": "While the alignment method works very well on non-role tokens, it works poorly on the role tokens.", "labels": [], "entities": []}, {"text": "Role tokens are sometimes matched with a word or part of a word in the English sentence.", "labels": [], "entities": []}, {"text": "For example :polarity is matched with the un part of the word unpopular, :manner is matched with most adverbs, or even in the pair: thanks (t / thank-01 :arg0 (i / i) :arg1 (y / you)) all AMR tokens including :arg0 and :arg1 are matched to the only English word thanks.", "labels": [], "entities": [{"text": "manner", "start_pos": 74, "end_pos": 80, "type": "METRIC", "confidence": 0.9536062479019165}]}, {"text": "Inconsistency in aligning role tokens has made this a hard problem even for human experts.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Results on different models. Our training  method (Model 4+) increases the F score by 1.7  and 3.1 points on dev and test sets respectively.", "labels": [], "entities": [{"text": "F score", "start_pos": 85, "end_pos": 92, "type": "METRIC", "confidence": 0.9924197196960449}]}, {"text": " Table 3: Results breakdown into role and non- role AMR tokens. The numbers in the parentheses  show the percent of recall errors caused by remov- ing aligned tokens as stop words.", "labels": [], "entities": [{"text": "recall errors", "start_pos": 116, "end_pos": 129, "type": "METRIC", "confidence": 0.9679199159145355}]}]}