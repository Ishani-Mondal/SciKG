{"title": [{"text": "Balanced Korean Word Spacing with Structural SVM", "labels": [], "entities": [{"text": "Balanced Korean Word Spacing", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8438200652599335}]}], "abstractContent": [{"text": "Most studies on statistical Korean word spacing do not utilize the information provided by the input sentence and assume that it was completely concatenated.", "labels": [], "entities": [{"text": "statistical Korean word spacing", "start_pos": 16, "end_pos": 47, "type": "TASK", "confidence": 0.5314277112483978}]}, {"text": "This makes the word spacer ignore the correct spaced parts of the input sentence and erroneously alter them.", "labels": [], "entities": []}, {"text": "To overcome such limit, this paper proposes a structural SVM-based Korean word spacing method that can utilize the space information of the input sentence.", "labels": [], "entities": [{"text": "SVM-based Korean word spacing", "start_pos": 57, "end_pos": 86, "type": "TASK", "confidence": 0.6956077367067337}]}, {"text": "The experiment on sentences with 10% spacing errors showed that our method achieved 96.81% F-score, while the basic structural SVM method only achieved 92.53% F-score.", "labels": [], "entities": [{"text": "F-score", "start_pos": 91, "end_pos": 98, "type": "METRIC", "confidence": 0.9976801872253418}, {"text": "F-score", "start_pos": 159, "end_pos": 166, "type": "METRIC", "confidence": 0.979973554611206}]}, {"text": "The more the input sentence was correctly spaced, the more accurately our method performed.", "labels": [], "entities": []}], "introductionContent": [{"text": "Automatic word spacing is a task to decide boundaries between words, which is frequently used for correcting spacing errors of text messages, Tweets, or Internet comments before using them in information retrieval applications.", "labels": [], "entities": [{"text": "Automatic word spacing", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.6258648534615835}, {"text": "correcting spacing errors of text messages, Tweets, or Internet comments", "start_pos": 98, "end_pos": 170, "type": "TASK", "confidence": 0.8649337391058604}, {"text": "information retrieval", "start_pos": 192, "end_pos": 213, "type": "TASK", "confidence": 0.7128100693225861}]}, {"text": "It is also often used in postprocessing optical character recognition (OCR) or voice recognition ( . Except for some Asian languages such as Chinese, Japanese and Thai, most languages have explicit word spacing that improves the readability of the text and helps readers better understand the meaning of it.", "labels": [], "entities": [{"text": "postprocessing optical character recognition (OCR)", "start_pos": 25, "end_pos": 75, "type": "TASK", "confidence": 0.7508063656943185}, {"text": "voice recognition", "start_pos": 79, "end_pos": 96, "type": "TASK", "confidence": 0.7316154986619949}]}, {"text": "Korean especially has a tricky word spacing system and users often make mistakes, which makes automatic word spacing an interesting and essential task.", "labels": [], "entities": [{"text": "word spacing", "start_pos": 31, "end_pos": 43, "type": "TASK", "confidence": 0.708186149597168}, {"text": "word spacing", "start_pos": 104, "end_pos": 116, "type": "TASK", "confidence": 0.7192713916301727}]}, {"text": "In order to easily acquire the training data, most studies on statistical Korean word spacing assume that well-spaced raw text (e.g. newspaper articles) is perfectly spaced and use it for training (.", "labels": [], "entities": [{"text": "statistical Korean word spacing", "start_pos": 62, "end_pos": 93, "type": "TASK", "confidence": 0.549012154340744}]}, {"text": "This approach, however, cannot observe incorrect spacing since the assumption makes the training data devoid of negative example.", "labels": [], "entities": []}, {"text": "Consequently, word spacers cannot use the spacing information given by the user, and erroneously alter the correctly spaced parts of the sentence.", "labels": [], "entities": []}, {"text": "To utilize the user-given spacing information, a corpus of input sentences and their correctly spaced version is necessary.", "labels": [], "entities": []}, {"text": "Constructing such corpus, however, requires much time and resource.", "labels": [], "entities": []}, {"text": "In this paper, to resolve such issue, we propose a structural SVM-based Korean word spacing model that can utilize the word spacing information given by the user.", "labels": [], "entities": [{"text": "SVM-based Korean word spacing", "start_pos": 62, "end_pos": 91, "type": "TASK", "confidence": 0.6364473924040794}]}, {"text": "We name the proposed model \"Balanced Word Spacing Model (BWSM)\".", "labels": [], "entities": [{"text": "Balanced Word Spacing Model (BWSM)\"", "start_pos": 28, "end_pos": 63, "type": "TASK", "confidence": 0.694164684840611}]}, {"text": "Our approach trains a basic structural SVM-based Korean word spacing model as in, and tries to obtain the sentence which achieves the maximum score for the basic model while minimally altering the input sentence.", "labels": [], "entities": [{"text": "SVM-based Korean word spacing", "start_pos": 39, "end_pos": 68, "type": "TASK", "confidence": 0.6243564635515213}]}, {"text": "In the following section, we discuss related studies.", "labels": [], "entities": []}, {"text": "In Section 3, the proposed method and its relation to Karush-Kuhn-Tucker (KKT) condition are explained.", "labels": [], "entities": []}, {"text": "The experiment and discussion is presented in Section 4.", "labels": [], "entities": []}, {"text": "Finally, in Section 5, the conclusion and future work for this study is given.", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to compare the performance of BWSM with HMM-based Korean word spacing and structural SVM-based Korean word spacing, we use Sejong raw corpus () as train data and ETRI POS tagging corpus as test data . Pegasos-struct algorithm from () was used to train the basic structural SVM-based model.", "labels": [], "entities": [{"text": "BWSM", "start_pos": 39, "end_pos": 43, "type": "DATASET", "confidence": 0.7323608994483948}, {"text": "Sejong raw corpus", "start_pos": 132, "end_pos": 149, "type": "DATASET", "confidence": 0.904694934686025}, {"text": "ETRI POS tagging corpus", "start_pos": 171, "end_pos": 194, "type": "DATASET", "confidence": 0.699314534664154}]}, {"text": "The optimal value for the tradeoff variable C of structural SVM was found after conducting several experiments . The rate of word spacing error varies depending on the corpus.", "labels": [], "entities": [{"text": "word spacing", "start_pos": 125, "end_pos": 137, "type": "TASK", "confidence": 0.6517897844314575}]}, {"text": "Newspaper articles rarely have word spacing errors but text messages or Tweets frequently contain word spacing errors.", "labels": [], "entities": []}, {"text": "To reflect such variety, we randomly insert spacing errors into the test set to produce various test sets with spacing error rate 0%, 10%, 20%, 35%, 50%, 60%, and 70% 5 . Figure 2: Word-based F-score of N-best reranking approach.", "labels": [], "entities": [{"text": "F-score", "start_pos": 192, "end_pos": 199, "type": "METRIC", "confidence": 0.8640637993812561}]}, {"text": "shows the relation between \u00ed \u00b5\u00ed\u00bb\u00bc(x-axis) and word-based F-score 6 (y-axis) of N-best re-ranking approach using test sets with different spacing error rate.", "labels": [], "entities": [{"text": "F-score", "start_pos": 57, "end_pos": 64, "type": "METRIC", "confidence": 0.8715800046920776}]}, {"text": "When \u00ed \u00b5\u00ed\u00bb\u00bc = 0 , BWSM becomes a normal structural SVM-based model.", "labels": [], "entities": [{"text": "BWSM", "start_pos": 18, "end_pos": 22, "type": "METRIC", "confidence": 0.9800388216972351}]}, {"text": "As \u00ed \u00b5\u00ed\u00bb\u00bc increases, F-score also increases fora while but decreases afterward.", "labels": [], "entities": [{"text": "F-score", "start_pos": 21, "end_pos": 28, "type": "METRIC", "confidence": 0.9995304346084595}]}, {"text": "And F-score increases more when using test sets with low error rate.", "labels": [], "entities": [{"text": "F-score", "start_pos": 4, "end_pos": 11, "type": "METRIC", "confidence": 0.9993621706962585}]}, {"text": "It is worth noticing that when using the test set with 0% error rate, as \u00ed \u00b5\u00ed\u00bb\u00bc increases, F-score converges to 98%.", "labels": [], "entities": [{"text": "error rate", "start_pos": 58, "end_pos": 68, "type": "METRIC", "confidence": 0.9742120206356049}, {"text": "F-score", "start_pos": 91, "end_pos": 98, "type": "METRIC", "confidence": 0.9994080066680908}]}, {"text": "The reason it does not reach 100% is that the correct label sequence is sometimes not included in the N-best sequences.", "labels": [], "entities": []}, {"text": "shows the relation between \u00ed \u00b5\u00ed\u00bb\u00bc(x-axis) and word-based F-score(y-axis) of modified Viterbi search approach using test sets with different spacing error rate.", "labels": [], "entities": [{"text": "F-score", "start_pos": 57, "end_pos": 64, "type": "METRIC", "confidence": 0.710553765296936}]}, {"text": "The graphs are similar to, but F-score reaches higher values compared to N-best re-ranking approach.", "labels": [], "entities": [{"text": "F-score", "start_pos": 31, "end_pos": 38, "type": "METRIC", "confidence": 0.9982330799102783}]}, {"text": "Notice that, when using the test set with 0% error rate, F-score becomes 100% as \u00ed \u00b5\u00ed\u00bb\u00bc surpasses 3.", "labels": [], "entities": [{"text": "error rate", "start_pos": 45, "end_pos": 55, "type": "METRIC", "confidence": 0.9650576114654541}, {"text": "F-score", "start_pos": 57, "end_pos": 64, "type": "METRIC", "confidence": 0.9987209439277649}]}, {"text": "This is because, unlike N-best re-ranking approach, modified Viterbi search approach considers all possible sequences as candidates.", "labels": [], "entities": []}, {"text": "From and 3, it can be seen that BWSM, which takes into consideration the spacing information provided by the user, can improve performance significantly.", "labels": [], "entities": [{"text": "BWSM", "start_pos": 32, "end_pos": 36, "type": "METRIC", "confidence": 0.9740471243858337}]}, {"text": "It is also apparent that modified Viterbi search approach outperforms N-best re-ranking approach.", "labels": [], "entities": []}, {"text": "The optimal value for \u00ed \u00b5\u00ed\u00bb\u00bc varies as test sets with different error rate are used.", "labels": [], "entities": [{"text": "\u00ed \u00b5\u00ed", "start_pos": 22, "end_pos": 26, "type": "METRIC", "confidence": 0.6677438914775848}]}, {"text": "It is natural that, for test sets with low error rate, the optimal value of \u00ed \u00b5\u00ed\u00bb\u00bc increases, thus forcing the model to more utilize the usergiven spacing information.", "labels": [], "entities": []}, {"text": "It is difficult to automatically obtain the optimal \u00ed \u00b5\u00ed\u00bb\u00bc for an arbitrary input sentence.", "labels": [], "entities": []}, {"text": "Therefore we set \u00ed \u00b5\u00ed\u00bb\u00bc to 1, which, according to, is more or less the optimal value for most of the test sets.", "labels": [], "entities": [{"text": "\u00ed \u00b5\u00ed", "start_pos": 17, "end_pos": 21, "type": "METRIC", "confidence": 0.61265829205513}]}, {"text": "Recall word = (# of correctly spaced words) / (the total number of words in the test data): Precision of BWSM and previous studies", "labels": [], "entities": [{"text": "Precision", "start_pos": 92, "end_pos": 101, "type": "METRIC", "confidence": 0.9897463321685791}, {"text": "BWSM", "start_pos": 105, "end_pos": 109, "type": "DATASET", "confidence": 0.6129162907600403}]}], "tableCaptions": [{"text": " Table 1: Precision of BWSM and previous  studies", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.986021101474762}, {"text": "BWSM", "start_pos": 23, "end_pos": 27, "type": "METRIC", "confidence": 0.6076452136039734}]}]}