{"title": [{"text": "A Fast and Accurate Dependency Parser using Neural Networks", "labels": [], "entities": [{"text": "Accurate", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.991051435470581}]}], "abstractContent": [{"text": "Almost all current dependency parsers classify based on millions of sparse indicator features.", "labels": [], "entities": [{"text": "dependency parsers classify", "start_pos": 19, "end_pos": 46, "type": "TASK", "confidence": 0.718528538942337}]}, {"text": "Not only do these features generalize poorly, but the cost of feature computation restricts parsing speed significantly.", "labels": [], "entities": [{"text": "parsing", "start_pos": 92, "end_pos": 99, "type": "TASK", "confidence": 0.9752827882766724}]}, {"text": "In this work, we propose a novel way of learning a neural network classifier for use in a greedy, transition-based dependency parser.", "labels": [], "entities": []}, {"text": "Because this classifier learns and uses just a small number of dense features , it can work very fast, while achieving an about 2% improvement in unla-beled and labeled attachment scores on both English and Chinese datasets.", "labels": [], "entities": []}, {"text": "Concretely , our parser is able to parse more than 1000 sentences per second at 92.2% unlabeled attachment score on the English Penn Treebank.", "labels": [], "entities": [{"text": "unlabeled attachment score", "start_pos": 86, "end_pos": 112, "type": "METRIC", "confidence": 0.665479044119517}, {"text": "English Penn Treebank", "start_pos": 120, "end_pos": 141, "type": "DATASET", "confidence": 0.9335174163182577}]}], "introductionContent": [{"text": "In recent years, enormous parsing success has been achieved by the use of feature-based discriminative dependency parsers).", "labels": [], "entities": [{"text": "parsing", "start_pos": 26, "end_pos": 33, "type": "TASK", "confidence": 0.9771115183830261}]}, {"text": "In particular, for practical applications, the speed of the subclass of transition-based dependency parsers has been very appealing.", "labels": [], "entities": []}, {"text": "However, these parsers are not perfect.", "labels": [], "entities": []}, {"text": "First, from a statistical perspective, these parsers suffer from the use of millions of mainly poorly estimated feature weights.", "labels": [], "entities": []}, {"text": "While in aggregate both lexicalized features and higher-order interaction term features are very important in improving the performance of these systems, nevertheless, there is insufficient data to correctly weight most such features.", "labels": [], "entities": []}, {"text": "For this reason, techniques for introducing higher-support features such as word class features have also been very successful in improving parsing performance (.", "labels": [], "entities": [{"text": "parsing", "start_pos": 140, "end_pos": 147, "type": "TASK", "confidence": 0.9677407145500183}]}, {"text": "Second, almost all existing parsers rely on a manually designed set of feature templates, which require a lot of expertise and are usually incomplete.", "labels": [], "entities": []}, {"text": "Third, the use of many feature templates cause a less studied problem: in modern dependency parsers, most of the runtime is consumed not by the core parsing algorithm but in the feature extraction step).", "labels": [], "entities": [{"text": "dependency parsers", "start_pos": 81, "end_pos": 99, "type": "TASK", "confidence": 0.7499742805957794}, {"text": "feature extraction", "start_pos": 178, "end_pos": 196, "type": "TASK", "confidence": 0.7243024408817291}]}, {"text": "For instance, reports that his baseline parser spends 99% of its time doing feature extraction, despite that being done in standard efficient ways.", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 76, "end_pos": 94, "type": "TASK", "confidence": 0.7338423728942871}]}, {"text": "In this work, we address all of these problems by using dense features in place of the sparse indicator features.", "labels": [], "entities": []}, {"text": "This is inspired by the recent success of distributed word representations in many NLP tasks, e.g., POS tagging ), machine translation (, and constituency parsing.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 100, "end_pos": 111, "type": "TASK", "confidence": 0.847942054271698}, {"text": "machine translation", "start_pos": 115, "end_pos": 134, "type": "TASK", "confidence": 0.8311893343925476}, {"text": "constituency parsing", "start_pos": 142, "end_pos": 162, "type": "TASK", "confidence": 0.8620524704456329}]}, {"text": "Low-dimensional, dense word embeddings can effectively alleviate sparsity by sharing statistical strength between similar words, and can provide us a good starting point to construct features of words and their interactions.", "labels": [], "entities": []}, {"text": "Nevertheless, there remain challenging problems of how to encode all the available information from the configuration and how to model higher-order features based on the dense representations.", "labels": [], "entities": []}, {"text": "In this paper, we train a neural network classifier to make parsing decisions within a transition-based dependency parser.", "labels": [], "entities": []}, {"text": "The neural network learns compact dense vector representations of words, part-of-speech (POS) tags, and dependency labels.", "labels": [], "entities": []}, {"text": "This results in a fast, compact classifier, which uses only 200 learned dense features while yielding good gains in parsing accuracy and speed on two languages (English and Chinese) and two different dependency representations (CoNLL and Stanford dependencies).", "labels": [], "entities": [{"text": "parsing", "start_pos": 116, "end_pos": 123, "type": "TASK", "confidence": 0.9577153921127319}, {"text": "accuracy", "start_pos": 124, "end_pos": 132, "type": "METRIC", "confidence": 0.9359610676765442}, {"text": "CoNLL", "start_pos": 228, "end_pos": 233, "type": "DATASET", "confidence": 0.9205336570739746}]}, {"text": "The main contributions of this work are: (i) showing the usefulness of dense representations that are learned within the parsing task, (ii) developing a neural network architecture that gives good accuracy and speed, and (iii) introducing a novel acti-vation function for the neural network that better captures higher-order interaction features.", "labels": [], "entities": [{"text": "parsing task", "start_pos": 121, "end_pos": 133, "type": "TASK", "confidence": 0.9119322299957275}, {"text": "accuracy", "start_pos": 197, "end_pos": 205, "type": "METRIC", "confidence": 0.9984651803970337}]}], "datasetContent": [{"text": "We conduct our experiments on the English Penn Treebank (PTB) and the Chinese Penn Treebank (CTB) datasets.", "labels": [], "entities": [{"text": "English Penn Treebank (PTB)", "start_pos": 34, "end_pos": 61, "type": "DATASET", "confidence": 0.950803260008494}, {"text": "Chinese Penn Treebank (CTB) datasets", "start_pos": 70, "end_pos": 106, "type": "DATASET", "confidence": 0.9430065665926252}]}, {"text": "For English, we follow the standard splits of PTB3, using sections 2-21 for training, section   For Chinese, we adopt the same split of CTB5 as described in.", "labels": [], "entities": [{"text": "PTB3", "start_pos": 46, "end_pos": 50, "type": "DATASET", "confidence": 0.9647125601768494}, {"text": "CTB5", "start_pos": 136, "end_pos": 140, "type": "DATASET", "confidence": 0.9312502145767212}]}, {"text": "Dependencies are converted using the Penn2Malt tool with the head-finding rules of (.", "labels": [], "entities": [{"text": "Penn2Malt", "start_pos": 37, "end_pos": 46, "type": "DATASET", "confidence": 0.9736364483833313}]}, {"text": "And following, we use gold segmentation and POS tags for the input.", "labels": [], "entities": []}, {"text": "gives statistics of the three datasets.", "labels": [], "entities": []}, {"text": "In particular, over 99% of the trees are projective in all datasets.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Data Statistics. \"Projective\" is the percentage of projective trees on the training set.", "labels": [], "entities": []}, {"text": " Table 4: Accuracy and parsing speed on PTB +  CoNLL dependencies.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9974328875541687}, {"text": "parsing", "start_pos": 23, "end_pos": 30, "type": "TASK", "confidence": 0.9636813402175903}, {"text": "PTB +  CoNLL dependencies", "start_pos": 40, "end_pos": 65, "type": "DATASET", "confidence": 0.8348386883735657}]}, {"text": " Table 5: Accuracy and parsing speed on PTB +  Stanford dependencies.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9969359636306763}, {"text": "parsing", "start_pos": 23, "end_pos": 30, "type": "TASK", "confidence": 0.972192645072937}, {"text": "PTB +  Stanford dependencies", "start_pos": 40, "end_pos": 68, "type": "DATASET", "confidence": 0.9314413666725159}]}, {"text": " Table 6: Accuracy and parsing speed on CTB.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9983364939689636}, {"text": "parsing", "start_pos": 23, "end_pos": 30, "type": "TASK", "confidence": 0.9652714133262634}, {"text": "CTB", "start_pos": 40, "end_pos": 43, "type": "DATASET", "confidence": 0.9195315837860107}]}]}