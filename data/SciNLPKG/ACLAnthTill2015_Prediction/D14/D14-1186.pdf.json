{"title": [{"text": "Joint Learning of Chinese Words, Terms and Keywords", "labels": [], "entities": []}], "abstractContent": [{"text": "Previous work often used a pipelined framework where Chinese word segmen-tation is followed by term extraction and keyword extraction.", "labels": [], "entities": [{"text": "term extraction", "start_pos": 95, "end_pos": 110, "type": "TASK", "confidence": 0.6870654970407486}, {"text": "keyword extraction", "start_pos": 115, "end_pos": 133, "type": "TASK", "confidence": 0.6976607888936996}]}, {"text": "Such framework suffers from error propagation and is unable to leverage information in later modules for prior components.", "labels": [], "entities": [{"text": "error propagation", "start_pos": 28, "end_pos": 45, "type": "TASK", "confidence": 0.6685736328363419}]}, {"text": "In this paper, we propose a four-level Dirichlet Process based model (DP-4) to jointly learn the word distributions from the corpus, domain and document levels simultaneously.", "labels": [], "entities": []}, {"text": "Based on the DP-4 model, a sentence-wise Gibbs sampler is adopted to obtain proper segmentation results.", "labels": [], "entities": []}, {"text": "Meanwhile, terms and keywords are acquired in the sampling process.", "labels": [], "entities": []}, {"text": "Experimental results have shown the effectiveness of our method.", "labels": [], "entities": []}], "introductionContent": [{"text": "For Chinese language which does not contain explicitly marked word boundaries, word segmentation (WS) is usually the first important step for many Natural Language Processing (NLP) tasks including term extraction (TE) and keyword extraction (KE).", "labels": [], "entities": [{"text": "word segmentation (WS)", "start_pos": 79, "end_pos": 101, "type": "TASK", "confidence": 0.8180167078971863}, {"text": "term extraction (TE)", "start_pos": 197, "end_pos": 217, "type": "TASK", "confidence": 0.8261870861053466}, {"text": "keyword extraction (KE)", "start_pos": 222, "end_pos": 245, "type": "TASK", "confidence": 0.8139242053031921}]}, {"text": "Generally, Chinese terms and keywords can be regarded as words which are representative of one domain or one document respectively.", "labels": [], "entities": []}, {"text": "Previous work of TE and KE normally used the pipelined approaches which first conducted WS and then extracted important word sequences as terms or keywords.", "labels": [], "entities": [{"text": "WS", "start_pos": 88, "end_pos": 90, "type": "TASK", "confidence": 0.9119726419448853}]}, {"text": "It is obvious that the pipelined approaches are prone to suffer from error propagation and fail to leverage information for word segmentation from later stages.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 124, "end_pos": 141, "type": "TASK", "confidence": 0.7292123287916183}]}, {"text": "Here, we provide one example in the disease domain, to demonstrate the common problems in current pipelined approaches and propose the basic idea of our joint learning of words, terms and keywords.", "labels": [], "entities": []}, {"text": "This is a correctly segmented Chinese sentence.", "labels": [], "entities": []}, {"text": "The document containing the example sentence mainly talks about the property of \"{ \u2022 (heparinoid)\" which can be regarded as one keyword of the document.", "labels": [], "entities": []}, {"text": "At the same time, the word @\u2022\u00cf\u00c7(thrombocytopenia) appears frequently in the disease domain and can be treated as a domain-specific term.", "labels": [], "entities": []}, {"text": "However, for such a simple sentence, current segmentation tools perform poorly.", "labels": [], "entities": []}, {"text": "The segmentation result with the state-of-the-art Conditional Random Fields (CRFs) approach () is as follows: where @\u2022\u00cf\u00c7 is segmented into three common Chinese words and {\u2022 is mixed with its neighbors.", "labels": [], "entities": []}, {"text": "Ina text processing pipeline of WS, TE and KE, it is obvious that imprecise WS results will make the overall system performance unsatisfying.", "labels": [], "entities": []}, {"text": "At the same time, we can hardly make use of domain-level and document-level information collected in TE and KE to promote the performance of WS.", "labels": [], "entities": [{"text": "WS", "start_pos": 141, "end_pos": 143, "type": "TASK", "confidence": 0.9305763840675354}]}, {"text": "Thus, one question comes to our minds: can words, terms and keywords be jointly learned with consideration of all the information from the corpus, domain, and document levels?", "labels": [], "entities": []}, {"text": "Recently, the hierarchical Dirichlet process (HDP) model has been used as a smoothed bigram model to conduct word segmentation).", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 109, "end_pos": 126, "type": "TASK", "confidence": 0.7557767033576965}]}, {"text": "Meanwhile, one strong point of the HDP based models is that they can model the diversity and commonality in multiple correlated corpora).", "labels": [], "entities": []}, {"text": "Inspired by such existing work, we propose a four-level DP based model,  2 DP-4 Model) applied the HDP model on the word segmentation task.", "labels": [], "entities": [{"text": "word segmentation task", "start_pos": 116, "end_pos": 138, "type": "TASK", "confidence": 0.8332149386405945}]}, {"text": "In essence, Goldwater's model can be viewed as a bigram language model with a unigram back-off.", "labels": [], "entities": []}, {"text": "With the language model, word segmentation is implemented by a character-based Gibbs sampler which repeatedly samples the possible word boundary positions between two neighboring words, conditioned on the current values of all other words.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 25, "end_pos": 42, "type": "TASK", "confidence": 0.7541272044181824}]}, {"text": "However, Goldwater's model can be deemed as modeling the whole corpus only, and does not distinguish between domains and documents.", "labels": [], "entities": []}, {"text": "To jointly learn the word information from the corpus, domain and document levels, we extend Goldwater's model by adding two levels (domain level and document level) of DPs, as illustrated in.", "labels": [], "entities": []}], "datasetContent": [{"text": "The following baselines are implemented for comparison of segmentation results: (1) Forward maximum matching (FMM) algorithm with a vocabulary compiled from the PKU corpus; (2) Reverse maximum matching (RMM) algorithm with the compiled vocabulary; (3) Conditional Random Fields (CRFs) 3 based supervised algorithm trained from the PKU corpus; (4) HDP based semisupervised algorithm) us-ing the PKU corpus.", "labels": [], "entities": [{"text": "Forward maximum matching (FMM)", "start_pos": 84, "end_pos": 114, "type": "METRIC", "confidence": 0.8295493026574453}, {"text": "PKU corpus", "start_pos": 161, "end_pos": 171, "type": "DATASET", "confidence": 0.8410445749759674}, {"text": "Reverse maximum matching (RMM)", "start_pos": 177, "end_pos": 207, "type": "METRIC", "confidence": 0.8957055409749349}, {"text": "PKU corpus", "start_pos": 331, "end_pos": 341, "type": "DATASET", "confidence": 0.8009365797042847}, {"text": "PKU corpus", "start_pos": 394, "end_pos": 404, "type": "DATASET", "confidence": 0.9103560745716095}]}, {"text": "The strength of's NPYLM based segmentation model is its speed due to the sentence-wise sampling technique, and its performance is similar to's model.", "labels": [], "entities": [{"text": "NPYLM based segmentation", "start_pos": 18, "end_pos": 42, "type": "TASK", "confidence": 0.524635891119639}]}, {"text": "Thus, we do not consider the NPYLM based model for comparison here.", "labels": [], "entities": []}, {"text": "Then, the segmentation results of FMM, RMM, CRF, and HDP methods are used respectively for further extracting terms and keywords.", "labels": [], "entities": []}, {"text": "We use the mutual information to identify the candidate terms or keywords composed of more than two segmented words.", "labels": [], "entities": []}, {"text": "As for DP-4, this recognition process has been done implicitly during sampling.", "labels": [], "entities": []}, {"text": "To measure the candidate terms or keywords, we refer to the metric in Nazar (2011) to calculate their importance in some specific domain or document.", "labels": [], "entities": []}, {"text": "The metrics of F 1 and the out-of-vocabulary Recall (OOV-R) are used to evaluate the segmentation results, referring to the gold standard results.", "labels": [], "entities": [{"text": "F 1", "start_pos": 15, "end_pos": 18, "type": "METRIC", "confidence": 0.9931085705757141}, {"text": "Recall (OOV-R)", "start_pos": 45, "end_pos": 59, "type": "METRIC", "confidence": 0.9009342789649963}, {"text": "segmentation", "start_pos": 85, "end_pos": 97, "type": "TASK", "confidence": 0.9620362520217896}]}, {"text": "The second and third columns of show the F 1 and OOV-R scores averaged on the 10 domains for all the compared methods.", "labels": [], "entities": [{"text": "F 1", "start_pos": 41, "end_pos": 44, "type": "METRIC", "confidence": 0.99398273229599}, {"text": "OOV-R", "start_pos": 49, "end_pos": 54, "type": "METRIC", "confidence": 0.9729917049407959}]}, {"text": "Our method significantly outperforms FMM, RMM and HDP according to t-test (p-value \u2264 0.05).", "labels": [], "entities": [{"text": "FMM", "start_pos": 37, "end_pos": 40, "type": "DATASET", "confidence": 0.6701083183288574}]}, {"text": "From the segmentation results, we can see that the FMM and RMM methods are highly dependent on the compiled vocabulary and their identified OOV words are mainly the ones composed of a single Chinese character.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 9, "end_pos": 21, "type": "TASK", "confidence": 0.9625372886657715}, {"text": "FMM", "start_pos": 51, "end_pos": 54, "type": "DATASET", "confidence": 0.7867804169654846}]}, {"text": "The HDP method is heavily influenced by the segmented text, but it also exhibits the ability of learning new words.", "labels": [], "entities": []}, {"text": "Our method only shows a slight advantage over the CRF approach.", "labels": [], "entities": []}, {"text": "We check our segmentation results and find that the performance of the DP-4 model is depressed by the identified terms and keywords which maybe composed of more than two words in the gold standard results, because the DP-4 model always treats the term or keyword as a single word.", "labels": [], "entities": []}, {"text": "For example, in the gold standard, \"-W \u2021((Lingnan Culture)\" is segmented into two words \"-W\" and \" \u2021 \", \"p n \u00a5 \u00e3(data interface)\" is segmented into \"pn\" and \"\u00a5\u00e3\" and soon.", "labels": [], "entities": [{"text": "Lingnan Culture)\"", "start_pos": 42, "end_pos": 59, "type": "DATASET", "confidence": 0.917476216952006}]}, {"text": "In fact, our segmentation results correctly treat \"-W \u2021\" and \"p n\u00a5\u00e3\" as words.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 13, "end_pos": 25, "type": "TASK", "confidence": 0.9602755308151245}]}, {"text": "To evaluate the TE and KE performance, the top 50 (TE-50) and 100 (TE-100) accuracy are measured for the identified terms of one domain, while the top 5 (KE-5) and 10 (KE-10) accuracy for the keywords in one document, are shown in the right four columns of.", "labels": [], "entities": [{"text": "TE-100) accuracy", "start_pos": 67, "end_pos": 83, "type": "METRIC", "confidence": 0.8131520549456278}, {"text": "accuracy", "start_pos": 175, "end_pos": 183, "type": "METRIC", "confidence": 0.6958834528923035}]}, {"text": "We can see that DP-4 performs significantly better than all the other methods in TE and KE results.", "labels": [], "entities": [{"text": "TE", "start_pos": 81, "end_pos": 83, "type": "METRIC", "confidence": 0.7502864599227905}]}, {"text": "As for the ten domains, we find our approach behaves much better than the other approaches on the following three domains: Disease, Physics and Computer.", "labels": [], "entities": []}, {"text": "It is because the language of these three domains is much different from that of the general domain (PKU corpus), while the rest domains are more similar to the general domain.", "labels": [], "entities": [{"text": "PKU corpus)", "start_pos": 101, "end_pos": 112, "type": "DATASET", "confidence": 0.881442646185557}]}], "tableCaptions": [{"text": " Table 1: Comparison of WS, TE and KE Perfor- mance (averaged on the 10 domains).", "labels": [], "entities": [{"text": "TE", "start_pos": 28, "end_pos": 30, "type": "METRIC", "confidence": 0.8582213521003723}, {"text": "KE Perfor- mance", "start_pos": 35, "end_pos": 51, "type": "METRIC", "confidence": 0.8832599520683289}]}]}