{"title": [{"text": "Finding Good Enough: A Task-Based Evaluation of Query Biased Summarization for Cross Language Information Retrieval", "labels": [], "entities": [{"text": "Query Biased Summarization", "start_pos": 48, "end_pos": 74, "type": "TASK", "confidence": 0.6598873535792033}, {"text": "Cross Language Information Retrieval", "start_pos": 79, "end_pos": 115, "type": "TASK", "confidence": 0.7264226973056793}]}], "abstractContent": [{"text": "In this paper we present our task-based evaluation of query biased summarization for cross-language information retrieval (CLIR) using relevance prediction.", "labels": [], "entities": [{"text": "cross-language information retrieval (CLIR)", "start_pos": 85, "end_pos": 128, "type": "TASK", "confidence": 0.7537005891402563}, {"text": "relevance prediction", "start_pos": 135, "end_pos": 155, "type": "TASK", "confidence": 0.7562168538570404}]}, {"text": "We describe our 13 summarization methods each from one of four summarization strategies.", "labels": [], "entities": [{"text": "summarization", "start_pos": 19, "end_pos": 32, "type": "TASK", "confidence": 0.9773706197738647}]}, {"text": "We show how well our methods perform using Farsi text from the CLEF 2008 shared-task, which we translated to English automtatically.", "labels": [], "entities": [{"text": "CLEF 2008 shared-task", "start_pos": 63, "end_pos": 84, "type": "DATASET", "confidence": 0.84180215994517}]}, {"text": "We report preci-sion/recall/F1, accuracy and time-on-task.", "labels": [], "entities": [{"text": "preci-sion", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9928263425827026}, {"text": "recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.8305550217628479}, {"text": "F1", "start_pos": 28, "end_pos": 30, "type": "METRIC", "confidence": 0.8528074622154236}, {"text": "accuracy", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.9997316002845764}]}, {"text": "We found that different summarization methods perform optimally for different evaluation metrics, but overall query biased word clouds are the best summariza-tion strategy.", "labels": [], "entities": [{"text": "summarization", "start_pos": 24, "end_pos": 37, "type": "TASK", "confidence": 0.9653899073600769}]}, {"text": "In our analysis, we demonstrate that using the ROUGE metric on our sentence-based summaries cannot make the same kinds of distinctions as our evaluation framework does.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 47, "end_pos": 52, "type": "METRIC", "confidence": 0.9570688605308533}]}, {"text": "Finally, we present our recommendations for creating much-needed evaluation standards and datasets.", "labels": [], "entities": []}], "introductionContent": [{"text": "Despite many recent advances in query biased summarization for cross-language information retrieval (CLIR), there are no existing evaluation standards or datasets to make comparisons among different methods, and across different languages.", "labels": [], "entities": [{"text": "query biased summarization", "start_pos": 32, "end_pos": 58, "type": "TASK", "confidence": 0.5276893178621928}, {"text": "cross-language information retrieval (CLIR)", "start_pos": 63, "end_pos": 106, "type": "TASK", "confidence": 0.7590664774179459}]}, {"text": "Consider that creating this kind of summary requires familiarity with techniques from machine translation (MT), summarization, and information retrieval (IR).", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 86, "end_pos": 110, "type": "TASK", "confidence": 0.8475387334823609}, {"text": "summarization", "start_pos": 112, "end_pos": 125, "type": "TASK", "confidence": 0.9815672039985657}, {"text": "information retrieval (IR)", "start_pos": 131, "end_pos": 157, "type": "TASK", "confidence": 0.80719496011734}]}, {"text": "In this This work was sponsored by the Federal Bureau of Investigation under Air Force Contract FA8721-05-C-0002.", "labels": [], "entities": [{"text": "Air Force Contract FA8721-05-C-0002", "start_pos": 77, "end_pos": 112, "type": "DATASET", "confidence": 0.8221175968647003}]}, {"text": "Opinions, interpretations, conclusions, and recommendations are those of the authors and are not necessarily endorsed by the United States Government.", "labels": [], "entities": []}, {"text": "paper, we arrive at the intersection of each of these research areas.", "labels": [], "entities": []}, {"text": "Query biased summarization (also known as query-focused, query-relevant, and query-dependent) involves automatically capturing relevant ideas and content from a document with respect to a given query, and presenting it as a condensed version of the original document.", "labels": [], "entities": [{"text": "Query biased summarization", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.6891561349232992}]}, {"text": "This kind of summarization is mostly used in search engines because when search results are tailored to a user's information need, the user can find texts that they are looking for more quickly and more accurately ().", "labels": [], "entities": [{"text": "summarization", "start_pos": 13, "end_pos": 26, "type": "TASK", "confidence": 0.9723231196403503}]}, {"text": "Query biased summarization is a valuable research area in natural language processing (NLP), especially for CLIR.", "labels": [], "entities": [{"text": "Query biased summarization", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.6057294408480326}, {"text": "natural language processing (NLP)", "start_pos": 58, "end_pos": 91, "type": "TASK", "confidence": 0.8042623996734619}]}, {"text": "Users of CLIR systems meet their information needs by submitting their queries in L 1 to search through documents that have been composed in L 2 , even though they may not be familiar with L 2 (.", "labels": [], "entities": []}, {"text": "There are no standards for objectively evaluating summaries for CLIR -a research gap that we begin to address in this paper.", "labels": [], "entities": []}, {"text": "The problem we explore is two-fold: what kinds of summaries are well-suited for CLIR applications, and how should the summaries be evaluated.", "labels": [], "entities": []}, {"text": "Our evaluation is extrinsic, that is to say we are interested in how summarization affects performance on a different task ().", "labels": [], "entities": [{"text": "summarization", "start_pos": 69, "end_pos": 82, "type": "TASK", "confidence": 0.9667889475822449}]}, {"text": "We use relevance prediction as our extrinsic task: a human must decide if a summary fora given document is relevant to a particular information need, or not.", "labels": [], "entities": [{"text": "relevance prediction", "start_pos": 7, "end_pos": 27, "type": "TASK", "confidence": 0.7792166471481323}]}, {"text": "Relevance prediction is known to be useful as it correlates with some automatic intrinsic methods as well.", "labels": [], "entities": [{"text": "Relevance prediction", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8100504875183105}]}, {"text": "To the best of our knowledge, we are the first to apply this evaluation framework to cross language query biased summarization.", "labels": [], "entities": [{"text": "cross language query biased summarization", "start_pos": 85, "end_pos": 126, "type": "TASK", "confidence": 0.6897625148296356}]}, {"text": "Each one of the summarization methods that we present in this paper belongs to one of the following strategies: (1) unbiased full machine translated text, (2) unbiased word clouds, (3) query biased word clouds, and (4) query biased sentence summaries.", "labels": [], "entities": [{"text": "summarization", "start_pos": 16, "end_pos": 29, "type": "TASK", "confidence": 0.9798765182495117}, {"text": "query biased sentence summaries", "start_pos": 219, "end_pos": 250, "type": "TASK", "confidence": 0.5840160474181175}]}, {"text": "The methods and strategies that we present are fast, cheap, and language-independent.", "labels": [], "entities": []}, {"text": "All of these strategies are extractive, meaning that we used existing parts of a document to create the condensed version, or summary.", "labels": [], "entities": []}, {"text": "We approach our task as an engineering problem: the goal is to decide if summaries are good enough to help CLIR system users find what they are looking for.", "labels": [], "entities": []}, {"text": "We have simplified the task by assuming that a set of documents has already been retrieved from a search engine, as CLIR techniques are outside the scope of this paper.", "labels": [], "entities": []}, {"text": "We predict that showing the full MT English text as a summarization strategy would not be particularly helpful in our relevance prediction task because the words in the text could be mixed-up, or sentences could be nonsensical, resulting in poor readability.", "labels": [], "entities": [{"text": "MT English text", "start_pos": 33, "end_pos": 48, "type": "TASK", "confidence": 0.7898269891738892}, {"text": "summarization", "start_pos": 54, "end_pos": 67, "type": "TASK", "confidence": 0.9664554595947266}, {"text": "relevance prediction", "start_pos": 118, "end_pos": 138, "type": "TASK", "confidence": 0.8732261955738068}]}, {"text": "For the same reasons, we expect that showing the full MT English text would take longer to arrive at a relevance decision.", "labels": [], "entities": [{"text": "MT English text", "start_pos": 54, "end_pos": 69, "type": "DATASET", "confidence": 0.6793161034584045}]}, {"text": "Finally, we predict that query biased summaries will result in faster, more accurate decisions from the participants (.", "labels": [], "entities": [{"text": "query biased summaries", "start_pos": 25, "end_pos": 47, "type": "TASK", "confidence": 0.601492702960968}]}, {"text": "We treat the actual CLIR search engine as if it were a black box so that we can focus on evaluating if the summaries themselves are useful.", "labels": [], "entities": []}, {"text": "As a starting point, we begin with some principles that we expect to hold true when we evaluate.", "labels": [], "entities": []}, {"text": "These principles provide us with the kind of framework that we need fora productive and judicious discussion about how well a summarization method works.", "labels": [], "entities": [{"text": "summarization", "start_pos": 126, "end_pos": 139, "type": "TASK", "confidence": 0.9741544723510742}]}, {"text": "We encourage the NLP community to consider the following concepts when developing evaluation standards for this problem: \u2022 End-user intelligiblity \u2022 Query-salience \u2022 Retrieval-relevance Summaries should be presented to the end-user in away that is both concise and intelligible, even if the machine translated text is difficult to understand.", "labels": [], "entities": []}, {"text": "Our notions of query-salience and retrievalrelevance capture the expectation that good summaries will be efficient enough to help end-users fulfill their information needs.", "labels": [], "entities": []}, {"text": "For query-salience, we want users to positively identify relevant documents.", "labels": [], "entities": []}, {"text": "Similarly, for retrieval-relevance we want users to be able to find as many relevant documents as possible.", "labels": [], "entities": []}, {"text": "This paper is structured as follows: Section 2 presents related work; Section 3 describes our data and pre-processing; Section 4 details our summarization methods and strategies; Section 5 describes our experiments; Section 6 shows our results and analysis; and in Section 7, we conclude and discuss some future directions for the NLP community.", "labels": [], "entities": []}], "datasetContent": [{"text": "There has been a lot of work towards developing metrics for understanding what makes a summary good.", "labels": [], "entities": []}, {"text": "Evaluation metrics are either intrinsic or extrinsic.", "labels": [], "entities": []}, {"text": "Intrinsic metrics, such as ROUGE, measure the quality of a summary with respect to gold human-generated summaries.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 27, "end_pos": 32, "type": "METRIC", "confidence": 0.9936503767967224}]}, {"text": "Generating gold standard summaries is expensive and time-consuming, a problem that persists with cross-language query biased summarization because those summaries must be query biased as well as in a different language from the source documents.", "labels": [], "entities": [{"text": "Generating gold standard summaries", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.5968702509999275}]}, {"text": "On the other hand, extrinsic metrics measure the quality of summaries at the system level, by looking at overall system performance on downstream tasks (.", "labels": [], "entities": []}, {"text": "One of the most important findings for query biased summarization comes from Tombros and Sanderson (1998).", "labels": [], "entities": [{"text": "query biased summarization", "start_pos": 39, "end_pos": 65, "type": "TASK", "confidence": 0.739007850488027}]}, {"text": "In their monolingual taskbased evaluation, they measured user speed and accuracy at identifying relevant documents.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 72, "end_pos": 80, "type": "METRIC", "confidence": 0.9988515377044678}]}, {"text": "They found that query biased summarization improved the user speed and accuracy when the user was asked to make relevance judgements for IR tasks.", "labels": [], "entities": [{"text": "query biased summarization", "start_pos": 16, "end_pos": 42, "type": "TASK", "confidence": 0.5909767746925354}, {"text": "speed", "start_pos": 61, "end_pos": 66, "type": "METRIC", "confidence": 0.9623791575431824}, {"text": "accuracy", "start_pos": 71, "end_pos": 79, "type": "METRIC", "confidence": 0.9978259205818176}, {"text": "IR tasks", "start_pos": 137, "end_pos": 145, "type": "TASK", "confidence": 0.9185454547405243}]}, {"text": "We also expect that our evaluation will demonstrate that user speed and accuracy is better when summaries are query biased.", "labels": [], "entities": [{"text": "speed", "start_pos": 62, "end_pos": 67, "type": "METRIC", "confidence": 0.9698494672775269}, {"text": "accuracy", "start_pos": 72, "end_pos": 80, "type": "METRIC", "confidence": 0.998410701751709}]}, {"text": "We tested each of our summarization methods and overall strategies in a task-based evaluation framework using relevance prediction.", "labels": [], "entities": []}, {"text": "We used Mechanical Turk for our experiments since it has been shown to be useful for evaluating NLP systems).", "labels": [], "entities": []}, {"text": "We obtained human judgments for whether or not a document was considered relevant to a query, or information need.", "labels": [], "entities": []}, {"text": "We measured the relevance judgements by precision/recall/F1, accuracy, and also time-on-task based on the average response time per Human Intelligence Task (HIT).", "labels": [], "entities": [{"text": "precision", "start_pos": 40, "end_pos": 49, "type": "METRIC", "confidence": 0.9995751976966858}, {"text": "recall", "start_pos": 50, "end_pos": 56, "type": "METRIC", "confidence": 0.8672272562980652}, {"text": "F1", "start_pos": 57, "end_pos": 59, "type": "METRIC", "confidence": 0.9778789281845093}, {"text": "accuracy", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.9997374415397644}]}], "tableCaptions": [{"text": " Table 1: Individual method results: precision/recall/F1, time-on-task, and accuracy. Note that results for  time-on-task and accuracy scores are distinguished for relevant (R) and non-relevant (NR) documents.", "labels": [], "entities": [{"text": "precision", "start_pos": 37, "end_pos": 46, "type": "METRIC", "confidence": 0.9994171857833862}, {"text": "recall", "start_pos": 47, "end_pos": 53, "type": "METRIC", "confidence": 0.7799829244613647}, {"text": "F1", "start_pos": 54, "end_pos": 56, "type": "METRIC", "confidence": 0.9184907078742981}, {"text": "accuracy", "start_pos": 76, "end_pos": 84, "type": "METRIC", "confidence": 0.9995564818382263}, {"text": "accuracy", "start_pos": 126, "end_pos": 134, "type": "METRIC", "confidence": 0.9978911280632019}]}, {"text": " Table 2: Comparison of peer systems on DUC  2005 shared-task for monolingual question-biased  summarization, f-scores from ROUGE-2 and  ROUGE-SU4.  Peer ID ROUGE-2 ROUGE-SU4  17  0.07170  0.12970  8  0.06960  0.12790  4  0.06850  0.12770  Tel-Eng-Sum  0.06048  0.12058  LQ  0.05124  0.09343  REL  0.04914  0.09081", "labels": [], "entities": [{"text": "DUC  2005", "start_pos": 40, "end_pos": 49, "type": "DATASET", "confidence": 0.9458440542221069}, {"text": "Peer ID ROUGE-2 ROUGE-SU4  17  0.07170  0.12970  8  0.06960  0.12790  4  0.06850  0.12770  Tel-Eng-Sum  0.06048  0.12058  LQ  0.05124  0.09343  REL  0.04914  0.09081", "start_pos": 149, "end_pos": 314, "type": "DATASET", "confidence": 0.7989096695726569}]}, {"text": " Table 3: Top 3 system precision scores for  ROUGE-2 and ROUGE-SU4.  Peer ID ROUGE-2 ROUGE-SU4  LQ  0.08272  0.15197  REL  0.0809  0.15049  15  0.07249  0.13129", "labels": [], "entities": [{"text": "precision", "start_pos": 23, "end_pos": 32, "type": "METRIC", "confidence": 0.9796229004859924}, {"text": "Peer ID ROUGE-2 ROUGE-SU4  LQ  0.08272  0.15197  REL  0.0809  0.15049  15  0.07249  0.13129", "start_pos": 69, "end_pos": 160, "type": "METRIC", "confidence": 0.7064066987771255}]}]}