{"title": [{"text": "Recursive Deep Models for Discourse Parsing", "labels": [], "entities": [{"text": "Discourse Parsing", "start_pos": 26, "end_pos": 43, "type": "TASK", "confidence": 0.67096808552742}]}], "abstractContent": [{"text": "Text-level discourse parsing remains a challenge: most approaches employ features that fail to capture the intentional, semantic , and syntactic aspects that govern discourse coherence.", "labels": [], "entities": [{"text": "Text-level discourse parsing", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.5675882895787557}]}, {"text": "In this paper, we propose a recursive model for discourse parsing that jointly models distributed representations for clauses, sentences, and entire discourses.", "labels": [], "entities": [{"text": "discourse parsing", "start_pos": 48, "end_pos": 65, "type": "TASK", "confidence": 0.7282956540584564}]}, {"text": "The learned representations canto some extent learn the semantic and intentional import of words and larger discourse units automatically,.", "labels": [], "entities": []}, {"text": "The proposed framework obtains comparable performance regarding standard discoursing parsing evaluations when compared against current state-of-art systems.", "labels": [], "entities": []}], "introductionContent": [{"text": "Ina coherent text, units (clauses, sentences, and larger multi-clause groupings) are tightly connected semantically, syntactically, and logically.", "labels": [], "entities": []}, {"text": "define a text to be coherent when it is possible to describe clearly the role that each discourse unit (at any level of grouping) plays with respect to the whole.", "labels": [], "entities": []}, {"text": "Ina coherent text, no unit is completely isolated.", "labels": [], "entities": []}, {"text": "Discourse parsing tries to identify how the units are connected with each other and thereby uncover the hierarchical structure of the text, from which multiple NLP tasks can benefit, including text summarization (, sentence compression) or questionanswering (.", "labels": [], "entities": [{"text": "Discourse parsing", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8355423808097839}, {"text": "text summarization", "start_pos": 193, "end_pos": 211, "type": "TASK", "confidence": 0.6929213255643845}, {"text": "sentence compression", "start_pos": 215, "end_pos": 235, "type": "TASK", "confidence": 0.7205105721950531}]}, {"text": "Despite recent progress in automatic discourse segmentation and sentence-level parsing (e.g.,, document-level discourse parsing remains a significant challenge.", "labels": [], "entities": [{"text": "automatic discourse segmentation", "start_pos": 27, "end_pos": 59, "type": "TASK", "confidence": 0.6558288832505544}, {"text": "sentence-level parsing", "start_pos": 64, "end_pos": 86, "type": "TASK", "confidence": 0.6967084258794785}, {"text": "document-level discourse parsing", "start_pos": 95, "end_pos": 127, "type": "TASK", "confidence": 0.619597464799881}]}, {"text": "Recent attempts (e.g.,) are still considerably inferior when compared to human goldstandard discourse analysis.", "labels": [], "entities": []}, {"text": "The challenge stems from the fact that compared with sentence-level dependency parsing, the set of relations between discourse units is less straightforward to define.", "labels": [], "entities": [{"text": "sentence-level dependency parsing", "start_pos": 53, "end_pos": 86, "type": "TASK", "confidence": 0.6453936795393626}]}, {"text": "Because there are no clause-level 'parts of discourse' analogous to word-level parts of speech, there is no discourse-level grammar analogous to sentence-level grammar.", "labels": [], "entities": []}, {"text": "To understand how discourse units are connected, one has to understand the communicative function of each unit, and the role it plays within the context that encapsulates it, taken recursively all the way up for the entire text.", "labels": [], "entities": []}, {"text": "Manually developed features relating to words and other syntax-related cues, used inmost of the recent prevailing approaches (e.g.,), are insufficient for capturing such nested intentionality.", "labels": [], "entities": []}, {"text": "Recently, deep learning architectures have been applied to various natural language processing tasks (for details see Section 2) and have shown the advantages to capture the relevant semantic and syntactic aspects of units in context.", "labels": [], "entities": []}, {"text": "As word distributions are composed to form the meanings of clauses, the goal is to extend distributed clauselevel representations to the single-and multisentence (discourse) levels, and produce the hierarchical structure of entire texts.", "labels": [], "entities": []}, {"text": "Inspired by this idea, we introduce in this paper a deep learning approach for discourse parsing.", "labels": [], "entities": [{"text": "discourse parsing", "start_pos": 79, "end_pos": 96, "type": "TASK", "confidence": 0.7329011559486389}]}, {"text": "The proposed parsing algorithm relies on a recursive neural network to decide (1) whether two discourse units are connected and if so by what relation they are connected.", "labels": [], "entities": [{"text": "parsing", "start_pos": 13, "end_pos": 20, "type": "TASK", "confidence": 0.9735850095748901}]}, {"text": "Concretely, the parsing algorithm takes as input a document of any length, and first obtains the distributed representation for each of its sentences using recursive convolution based on the sentence parse tree.", "labels": [], "entities": []}, {"text": "It then proceeds bottom-up, applying a binary classifier to determine the probability of two adjacent discourse units being merged to form anew subtree followed by a multi-class classifier to select the appropriate discourse relation label, and calculates the distributed representation for the subtree so formed, gradually unifying subtrees until a single overall tree spans the entire sentence.", "labels": [], "entities": []}, {"text": "The compositional distributed representation enables the parser to make accurate parsing decisions and capture relations between different sentences and units.", "labels": [], "entities": []}, {"text": "The binary and multi-class classifiers, along with parameters involved in convolution, are jointly trained from a collection of gold-standard discourse structures.", "labels": [], "entities": []}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "We present related work in Section 2 and describe the RST Discourse Treebank in Section 3.", "labels": [], "entities": [{"text": "RST Discourse Treebank", "start_pos": 54, "end_pos": 76, "type": "DATASET", "confidence": 0.7020506064097086}]}, {"text": "The sentence convolution approach is illustrated in Section 4 and the discourse parser model in Section 5.", "labels": [], "entities": []}, {"text": "We report experimental results in Section 6 and conclude in Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "A measure of the performance of the system is realized by comparing the structure and labeling of the RS-tree produced by our algorithm to goldstandard annotations.", "labels": [], "entities": []}, {"text": "Standard evaluation of discourse parsing output computes the ratio of the number of identical tree constituents shared in the generated RS-trees and the gold-standard trees against the total number of constituents in the generated discourse trees 2 , which is further divided to three matrices: Span (on the blank tree structure), nuclearity (on the tree structure with nuclearity indication), and relation (on the tree structure with rhetorical relation indication but no nuclearity indication).", "labels": [], "entities": [{"text": "discourse parsing output", "start_pos": 23, "end_pos": 47, "type": "TASK", "confidence": 0.7679150303204855}, {"text": "Span", "start_pos": 295, "end_pos": 299, "type": "METRIC", "confidence": 0.9754865765571594}, {"text": "relation", "start_pos": 398, "end_pos": 406, "type": "METRIC", "confidence": 0.9567261934280396}]}, {"text": "The nuclearity and relation decisions are made based on the multi-class output labels from the deep learning framework.", "labels": [], "entities": [{"text": "nuclearity", "start_pos": 4, "end_pos": 14, "type": "TASK", "confidence": 0.9841827750205994}]}, {"text": "As we do not consider nuclearity when classifying different discourse relations, the two labels attribute[S] and attribute[N] made by multi-class classifier will be treated as the same relation label ATTRIBUTE.: Performances for different approaches.", "labels": [], "entities": [{"text": "ATTRIBUTE.", "start_pos": 200, "end_pos": 210, "type": "METRIC", "confidence": 0.9712833166122437}]}, {"text": "Performances for baselines are reprinted from).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Performances for different approaches.  Performances for baselines are reprinted from", "labels": [], "entities": []}]}