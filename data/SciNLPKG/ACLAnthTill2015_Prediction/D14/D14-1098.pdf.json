{"title": [{"text": "Language Modeling with Functional Head Constraint for Code Switching Speech Recognition", "labels": [], "entities": [{"text": "Language Modeling", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.6722979694604874}, {"text": "Code Switching Speech Recognition", "start_pos": 54, "end_pos": 87, "type": "TASK", "confidence": 0.8026825189590454}]}], "abstractContent": [{"text": "In this paper, we propose novel struc-tured language modeling methods for code mixing speech recognition by incorporating a well-known syntactic constraint for switching code, namely the Functional Head Constraint (FHC).", "labels": [], "entities": [{"text": "code mixing speech recognition", "start_pos": 74, "end_pos": 104, "type": "TASK", "confidence": 0.770832359790802}]}, {"text": "Code mixing data is not abundantly available for training language models.", "labels": [], "entities": [{"text": "Code mixing", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.8047656118869781}]}, {"text": "Our proposed methods successfully alleviate this core problem for code mixing speech recognition by using bilingual data to train a struc-tured language model with syntactic constraint.", "labels": [], "entities": [{"text": "code mixing speech recognition", "start_pos": 66, "end_pos": 96, "type": "TASK", "confidence": 0.856056734919548}]}, {"text": "Linguists and bilingual speakers found that code switch do not happen between the functional head and its complements.", "labels": [], "entities": []}, {"text": "We propose to learn the code mixing language model from bilingual data with this constraint in a weighted finite state transducer (WFST) framework.", "labels": [], "entities": []}, {"text": "The constrained code switch language model is obtained by first expanding the search network with a translation model, and then using parsing to restrict paths to those permissible under the constraint.", "labels": [], "entities": []}, {"text": "We implement and compare two approaches-lattice parsing enables a sequential coupling whereas partial parsing enables a tight coupling between parsing and filtering.", "labels": [], "entities": []}, {"text": "We tested our system on a lecture speech dataset with 16% embedded second language, and on a lunch conversation dataset with 20% embedded language.", "labels": [], "entities": []}, {"text": "Our language models with lattice parsing and partial parsing reduce word error rates from a baseline mixed language model by 3.8% and 3.9% in terms of word error rate relatively on the average on the first and second tasks respectively.", "labels": [], "entities": [{"text": "lattice parsing", "start_pos": 25, "end_pos": 40, "type": "TASK", "confidence": 0.7051578164100647}, {"text": "partial parsing", "start_pos": 45, "end_pos": 60, "type": "TASK", "confidence": 0.6957330107688904}, {"text": "word error rates", "start_pos": 68, "end_pos": 84, "type": "METRIC", "confidence": 0.6782599488894144}, {"text": "word error rate", "start_pos": 151, "end_pos": 166, "type": "METRIC", "confidence": 0.7529133160909017}]}, {"text": "It outperforms the interpolated language model by 3.7% and 5.6% in terms of word error rate relatively, and outperforms the adapted language model by 2.6% and 4.6% relatively.", "labels": [], "entities": [{"text": "word error rate", "start_pos": 76, "end_pos": 91, "type": "METRIC", "confidence": 0.7594595154126486}]}, {"text": "Our proposed approach avoids making early decisions on code-switch boundaries and is therefore more robust.", "labels": [], "entities": []}, {"text": "We address the code switch data scarcity challenge by using bilingual data with syntactic structure.", "labels": [], "entities": []}], "introductionContent": [{"text": "In multilingual communities, it is common for people to mix two or more languages in their speech.", "labels": [], "entities": []}, {"text": "A single sentence spoken by bilingual speakers often contains the main, matrix language and an embedded second language.", "labels": [], "entities": []}, {"text": "This type of linguistic phenomenon is called \"code switching\" by linguists.", "labels": [], "entities": [{"text": "code switching\"", "start_pos": 46, "end_pos": 61, "type": "TASK", "confidence": 0.7828765908877054}]}, {"text": "It is increasingly important for automatic speech recognition (ASR) systems to recognize code switching speech as they exist in scenarios such as meeting and interview speech, lecture speech, and conversational speech.", "labels": [], "entities": [{"text": "automatic speech recognition (ASR)", "start_pos": 33, "end_pos": 67, "type": "TASK", "confidence": 0.8085112969080607}]}, {"text": "Code switching is common among bilingual speakers of Spanish-English, Hindi-English, Chinese-English, and Arabic-English, among others.", "labels": [], "entities": [{"text": "Code switching", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7913390100002289}]}, {"text": "In China, lectures, meetings and conversations with technical contents are frequently peppered with English terms even though the general population is not considered bilingual in Chinese and English.", "labels": [], "entities": []}, {"text": "Unlike the thousands and tens of thousands of hours of monolingual data available to train, for example, voice search engines, transcribed code switch data necessary for training language models is hard to come by.", "labels": [], "entities": []}, {"text": "Code switch language modeling is therefore an even harder problem than acoustic modeling.", "labels": [], "entities": [{"text": "Code switch language modeling", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.7880364954471588}, {"text": "acoustic modeling", "start_pos": 71, "end_pos": 88, "type": "TASK", "confidence": 0.7167539447546005}]}, {"text": "One approach for code switch speech recognition is to explicitly recognizing the code switch points by language identification first using phonetic or acoustic information, before applying speech recognizers for the matrix and embedded languages.", "labels": [], "entities": [{"text": "code switch speech recognition", "start_pos": 17, "end_pos": 47, "type": "TASK", "confidence": 0.7513350248336792}, {"text": "language identification", "start_pos": 103, "end_pos": 126, "type": "TASK", "confidence": 0.6933769732713699}]}, {"text": "This approach is extremely error-prone as language identification at each frame of the speech is necessary and any error will be propagated in the second speech recognition stage leading to fatal and irrecoverable errors.", "labels": [], "entities": [{"text": "language identification", "start_pos": 42, "end_pos": 65, "type": "TASK", "confidence": 0.7211699783802032}, {"text": "speech recognition", "start_pos": 154, "end_pos": 172, "type": "TASK", "confidence": 0.7375481128692627}]}, {"text": "Meanwhile, there are two general approaches to solve the problem of lack of training data for language modeling.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 94, "end_pos": 111, "type": "TASK", "confidence": 0.7540944516658783}]}, {"text": "Ina first approach, two language models are trained from both the matrix and embedded language separately and then interpolated together).", "labels": [], "entities": []}, {"text": "However, an interpolated language model effectively allows code switch at all word boundaries without much of a constraint.", "labels": [], "entities": []}, {"text": "Another approach is to adapt the matrix language language model with a small amount of code switch data).", "labels": [], "entities": []}, {"text": "The effectiveness of adaptation is also limited as positions of code switching points are not generalizable from the limited data.", "labels": [], "entities": []}, {"text": "Significant progress in speech recognition has been made by using deep neural networks for acoustic modeling and language model.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 24, "end_pos": 42, "type": "TASK", "confidence": 0.8662330508232117}]}, {"text": "However, improvement thus gained on code switch speech recognition remains very small.", "labels": [], "entities": [{"text": "code switch speech recognition", "start_pos": 36, "end_pos": 66, "type": "TASK", "confidence": 0.7258712202310562}]}, {"text": "Again, we propose that syntactic constraints of the code switching phenomenon can help improve performance and model accuracy.", "labels": [], "entities": [{"text": "code switching", "start_pos": 52, "end_pos": 66, "type": "TASK", "confidence": 0.7723415195941925}, {"text": "accuracy", "start_pos": 117, "end_pos": 125, "type": "METRIC", "confidence": 0.9891026020050049}]}, {"text": "Previous work of using partof-speech tags and our previous work using syntactic constraints () have made progress in this area.", "labels": [], "entities": []}, {"text": "Part-of-speech is relatively weak in predicting code switching points.", "labels": [], "entities": [{"text": "predicting code switching", "start_pos": 37, "end_pos": 62, "type": "TASK", "confidence": 0.9035266240437826}]}, {"text": "It is generally accepted by linguists that code switching follows the so-called Functional Head Constraint, where words on the nodes of a syntactic sub tree must follow the language of that of the headword.", "labels": [], "entities": [{"text": "code switching", "start_pos": 43, "end_pos": 57, "type": "TASK", "confidence": 0.8474354147911072}, {"text": "Functional Head Constraint", "start_pos": 80, "end_pos": 106, "type": "TASK", "confidence": 0.5873640278975168}]}, {"text": "If the headword is in the matrix language then none of its complements can switch to the embedded language.", "labels": [], "entities": []}, {"text": "In this work, we propose two ways to incorporate the Functional Head Constraint into speech recognition and compare them.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 85, "end_pos": 103, "type": "TASK", "confidence": 0.7356180995702744}]}, {"text": "We suggest two approaches of introducing syntactic constraints into the speech recognition system.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 72, "end_pos": 90, "type": "TASK", "confidence": 0.7536232173442841}]}, {"text": "One is to apply the knowledge sources in a sequential order.", "labels": [], "entities": []}, {"text": "The acoustic model and a monolingual language model are used first to produce an intermediate lattice, then a second pass choose the best result using the syntactic constraints.", "labels": [], "entities": []}, {"text": "Another approach uses tight coupling.", "labels": [], "entities": []}, {"text": "We propose using structured language model) to build the syntactic structure incrementally.", "labels": [], "entities": []}, {"text": "Following our previous work, we suggest incorporating the acoustic model, the monolingual language model and a translation model into a WFST framework.", "labels": [], "entities": [{"text": "WFST framework", "start_pos": 136, "end_pos": 150, "type": "DATASET", "confidence": 0.7987709641456604}]}, {"text": "Using a translation model allows us to learn what happens when a language switches to another with context information.", "labels": [], "entities": []}, {"text": "We will motivate and describe this WFST framework for code switching speech recognition in the next section.", "labels": [], "entities": [{"text": "code switching speech recognition", "start_pos": 54, "end_pos": 87, "type": "TASK", "confidence": 0.823477566242218}]}, {"text": "The Functional Head Constraint is described in Section 3.", "labels": [], "entities": [{"text": "Functional Head Constraint", "start_pos": 4, "end_pos": 30, "type": "TASK", "confidence": 0.672407070795695}]}, {"text": "The proposed code switch language models and speech recognition coupling is described in Section 4.", "labels": [], "entities": [{"text": "speech recognition coupling", "start_pos": 45, "end_pos": 72, "type": "TASK", "confidence": 0.8614902098973592}]}, {"text": "Experimental setup and results are presented in Section 5.", "labels": [], "entities": []}, {"text": "Finally we conclude in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "The bilingual acoustic model used for our mixed language ASR is trained from 160 hours of speech from GALE Phase 1 Chinese broadcast conversation, 40 hours of speech from GALE Phase 1 English broadcast conversation, and 3 hours of in-house nonnative English data.", "labels": [], "entities": [{"text": "mixed language ASR", "start_pos": 42, "end_pos": 60, "type": "TASK", "confidence": 0.5040503144264221}, {"text": "GALE Phase 1 Chinese broadcast conversation", "start_pos": 102, "end_pos": 145, "type": "DATASET", "confidence": 0.9321004649003347}, {"text": "GALE Phase 1 English broadcast conversation", "start_pos": 171, "end_pos": 214, "type": "DATASET", "confidence": 0.9368011752764384}]}, {"text": "The acoustic features used in our experiments consist of 39 components (13MFCC, 13MFCC, 13 MFCC using cepstral mean normalization), which are analyzed at a 10msec frame rate with a 25msec window size.", "labels": [], "entities": []}, {"text": "The acoustic models used throughout our paper are state-clustered crossword tri-phone HMMs with 16 Gaussian mixture output densities per state.", "labels": [], "entities": []}, {"text": "We use the phone set consists of 21 Mandarin standard initials, 37 Mandarin finals, 6 zero initials and 6 extended English phones.", "labels": [], "entities": []}, {"text": "The pronunciation dictionary is obtained by modifying Mandarin and English dictionaries using the phone set.", "labels": [], "entities": []}, {"text": "The acoustic models are reconstructed reports precision, recall and F-measure of code switching point in the recognition results of the baseline and our proposed language models.", "labels": [], "entities": [{"text": "precision", "start_pos": 46, "end_pos": 55, "type": "METRIC", "confidence": 0.9996230602264404}, {"text": "recall", "start_pos": 57, "end_pos": 63, "type": "METRIC", "confidence": 0.9995579123497009}, {"text": "F-measure", "start_pos": 68, "end_pos": 77, "type": "METRIC", "confidence": 0.9993578791618347}]}, {"text": "Our proposed code switching language models with functional head constraint improve both precision and recall of the code switching point detection on the code switching lecture speech and lunch conversation 4.48%.", "labels": [], "entities": [{"text": "precision", "start_pos": 89, "end_pos": 98, "type": "METRIC", "confidence": 0.9996047616004944}, {"text": "recall", "start_pos": 103, "end_pos": 109, "type": "METRIC", "confidence": 0.9989997744560242}, {"text": "code switching point detection", "start_pos": 117, "end_pos": 147, "type": "TASK", "confidence": 0.6683169454336166}, {"text": "code switching lecture speech and lunch conversation", "start_pos": 155, "end_pos": 207, "type": "TASK", "confidence": 0.7861158081463405}]}, {"text": "Our method by tightcoupling increases the F-measure by 9.38% relatively on the lecture speech and by 6.90% relatively on the lunch conversation compared to the baseline adapted language model.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 42, "end_pos": 51, "type": "METRIC", "confidence": 0.998282790184021}]}, {"text": "The shows the word error rates (WERs) of experiments on the code switching lecture speech and shows the WERs on the code switching lunch conversations.", "labels": [], "entities": [{"text": "word error rates (WERs)", "start_pos": 14, "end_pos": 37, "type": "METRIC", "confidence": 0.8810271620750427}, {"text": "WERs", "start_pos": 104, "end_pos": 108, "type": "METRIC", "confidence": 0.9870994687080383}]}, {"text": "Our proposed code switching language model with Functional Head Constraints by sequential-coupling reduces the WERs in the baseline mixed language model by 3.72% relative on Test 1, and 5.85% on Test 2.", "labels": [], "entities": [{"text": "WERs", "start_pos": 111, "end_pos": 115, "type": "METRIC", "confidence": 0.9892463088035583}]}, {"text": "Our method by tight-coupling also reduces WER by 2.51% relative compared to the baseline language model on Test 1, and by 4.57% on Test 2.", "labels": [], "entities": [{"text": "WER", "start_pos": 42, "end_pos": 45, "type": "METRIC", "confidence": 0.9630470871925354}]}, {"text": "We use the speech recognition scoring toolkit (SCTK) developed by the National Institute of Standards and Technology to compute the significance levels, which is based on two-proportion z-test comparing the difference between the recognition results of our proposed approach and the baseline.", "labels": [], "entities": [{"text": "speech recognition scoring", "start_pos": 11, "end_pos": 37, "type": "TASK", "confidence": 0.7768239974975586}]}, {"text": "All the WER reductions are statistically significant.", "labels": [], "entities": [{"text": "WER reductions", "start_pos": 8, "end_pos": 22, "type": "METRIC", "confidence": 0.861732006072998}]}, {"text": "For our reference, we also compare the performance of using Functional Head Constraint to that of using inversion constraint in ( and found that the present model reduces WER by 0.85% on Test 2 but gives no improvement on Test 1.", "labels": [], "entities": [{"text": "WER", "start_pos": 171, "end_pos": 174, "type": "METRIC", "confidence": 0.997550904750824}]}, {"text": "We hypothesize that since  Test 1 has mostly Chinese words, the proposed method is not as advantageous compared to our previous work.", "labels": [], "entities": []}, {"text": "Another future direction is for us to improve the lattice parser as we believe it will lead to further improvement on the final result of our proposed method.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Our proposed system outperforms the baselines in terms of WER on the lecture speech  Matrix Embedded Overall  MixedLM  34.41%  39.16%  35.17%  InterpolatedLM  34.11%  40.28%  35.10%  AdaptedLM  35.11%  38.41%  34.73%  Sequential coupling 33.17%  36.84%  33.76%  Tight coupling  33.14%  36.65%  33.70%", "labels": [], "entities": [{"text": "WER", "start_pos": 68, "end_pos": 71, "type": "METRIC", "confidence": 0.9933453798294067}]}, {"text": " Table 3: Our proposed system outperforms the baselines in terms of WER on the lunch conversation  Matrix Embedded Overall  MixedLM  46.4%  48.55%  46.83%  InterpolatedLM  46.04%  49.04%  46.64%  AdaptedLM  46.64%  48.39%  46.20%  Sequential coupling 43.24%  46.27%  43.89%  Tight coupling  42.97%  46.03%  43.58%", "labels": [], "entities": [{"text": "WER", "start_pos": 68, "end_pos": 71, "type": "METRIC", "confidence": 0.9875311255455017}]}]}