{"title": [{"text": "Morpho-syntactic Lexical Generalization for CCG Semantic Parsing", "labels": [], "entities": [{"text": "Morpho-syntactic Lexical Generalization", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.5803103049596151}]}], "abstractContent": [{"text": "In this paper, we demonstrate that significant performance gains can be achieved in CCG semantic parsing by introducing a linguistically motivated grammar induction scheme.", "labels": [], "entities": [{"text": "CCG semantic parsing", "start_pos": 84, "end_pos": 104, "type": "TASK", "confidence": 0.780613382657369}]}, {"text": "We present anew morpho-syntactic fac-tored lexicon that models systematic variations in morphology, syntax, and semantics across word classes.", "labels": [], "entities": []}, {"text": "The grammar uses domain-independent facts about the English language to restrict the number of incorrect parses that must be considered, thereby enabling effective learning from less data.", "labels": [], "entities": []}, {"text": "Experiments in benchmark domains match previous models with one quarter of the data and provide new state-of-the-art results with all available data, including up to 45% relative test-error reduction.", "labels": [], "entities": []}], "introductionContent": [{"text": "Semantic parsers map sentences to formal representations of their meaning.", "labels": [], "entities": []}, {"text": "One common approach is to induce a probabilistic CCG grammar, which defines the meanings of individual words and phrases and how to best combine them to analyze complete sentences.", "labels": [], "entities": []}, {"text": "There has been recent work developing learning algorithms for CCG semantic parsers () and using them for applications ranging from question answering) to robot control (.", "labels": [], "entities": [{"text": "CCG semantic parsers", "start_pos": 62, "end_pos": 82, "type": "TASK", "confidence": 0.6452934940656027}, {"text": "question answering", "start_pos": 131, "end_pos": 149, "type": "TASK", "confidence": 0.8218347728252411}]}, {"text": "One key learning challenge for this style of learning is to induce the CCG lexicon, which lists possible meanings for each phrase and defines a set of possible parses for each sentence.", "labels": [], "entities": []}, {"text": "Previous approaches have either hand-engineered a small set of lexical templates or automatically learned such templates ().", "labels": [], "entities": []}, {"text": "These methods are designed to learn grammars that overgenerate; they produce spurious parses that can complicate parameter estimation.", "labels": [], "entities": []}, {"text": "In this paper, we demonstrate that significant gains can instead be achieved by using a more constrained, linguistically motivated grammar induction scheme.", "labels": [], "entities": []}, {"text": "The grammar is restricted by introducing detailed syntactic modeling of a wider range of constructions than considered in previous work, for example introducing explicit treatments of relational nouns, Davidsonian events, and verb tense.", "labels": [], "entities": []}, {"text": "We also introduce anew lexical generalization model that abstracts over systematic morphological, syntactic, and semantic alternations within word classes.", "labels": [], "entities": []}, {"text": "This includes, for example, the facts that verbs in relative clauses and nominal constructions (e.g., \"flights departing NYC\" and \"departing flights\") should be infinitival while verbs in phrases (e.g., \"What flights depart at noon?\") should have tense.", "labels": [], "entities": []}, {"text": "These grammar modeling techniques use universal, domain-independent facts about the English language to restrict word usage to appropriate syntactic contexts, and as such are potentially applicable to any semantic parsing application.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 205, "end_pos": 221, "type": "TASK", "confidence": 0.7721130549907684}]}, {"text": "More specifically, we introduce anew morpho-syntactic, factored CCG lexicon that imposes our grammar restrictions during learning.", "labels": [], "entities": []}, {"text": "Each lexical entry has (1) a word stem, automatically constructed from Wiktionary, with part-of-speech and morphological attributes, (2) a lexeme that is learned and pairs the stem with semantic content that is invariant to syntactic usage, and (3) a lexical template that specifies the remaining syntactic and semantic content.", "labels": [], "entities": []}, {"text": "The full set of templates is defined in terms of a small set of base templates and template transformations that model morphological variants such as passivization and nominalization of verbs.", "labels": [], "entities": []}, {"text": "This approach allows us to efficiently encode a general grammar for semantic parsing while also eliminating large classes of incorrect analyses considered by previous work.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 68, "end_pos": 84, "type": "TASK", "confidence": 0.7971688508987427}]}, {"text": "We perform experiments in two benchmark semantic parsing datasets: GeoQuery (Zelle and Mooney, 1996) and ATIS (.", "labels": [], "entities": [{"text": "benchmark semantic parsing", "start_pos": 30, "end_pos": 56, "type": "TASK", "confidence": 0.696325679620107}, {"text": "ATIS", "start_pos": 105, "end_pos": 109, "type": "DATASET", "confidence": 0.8482105135917664}]}, {"text": "In both cases, our approach achieves state-of-the-art performance, including a nearly 45% relative error reduction on the ATIS test set.", "labels": [], "entities": [{"text": "relative error reduction", "start_pos": 90, "end_pos": 114, "type": "METRIC", "confidence": 0.8225058515866598}, {"text": "ATIS test set", "start_pos": 122, "end_pos": 135, "type": "DATASET", "confidence": 0.9126407106717428}]}, {"text": "We also show that the gains increase with less data, including matching previous model's performance with less than 25% of the training data.", "labels": [], "entities": []}, {"text": "Such gains are particularly practical for semantic parsers; they can greatly reduce the amount of data that is needed for each new application domain.", "labels": [], "entities": []}], "datasetContent": [{"text": "Data and Metrics We evaluate performance on two benchmark semantic parsing datasets, Geo880 and ATIS.", "labels": [], "entities": [{"text": "Geo880", "start_pos": 85, "end_pos": 91, "type": "DATASET", "confidence": 0.8897998929023743}, {"text": "ATIS", "start_pos": 96, "end_pos": 100, "type": "DATASET", "confidence": 0.8925038576126099}]}, {"text": "We use the standard data splits, including 600/280 train/test for Geo880 and 4460/480/450 train/develop/test for ATIS.", "labels": [], "entities": [{"text": "Geo880", "start_pos": 66, "end_pos": 72, "type": "DATASET", "confidence": 0.9531510472297668}, {"text": "ATIS", "start_pos": 113, "end_pos": 117, "type": "DATASET", "confidence": 0.9796589612960815}]}, {"text": "To support the new representations in Section 5, we systematically convert annotations with existential quantifiers, temporal events and relational nouns to new logical forms with equivalent meanings.", "labels": [], "entities": []}, {"text": "All systems are evaluated with exact match accuracy, the percentage of fully correct logical forms.", "labels": [], "entities": [{"text": "exact match accuracy", "start_pos": 31, "end_pos": 51, "type": "METRIC", "confidence": 0.7422791322072347}]}, {"text": "Initialization We assign positive initial weights to the indicator features for entries in the initial lexicon, as defined in Section 7, to encourage their use.", "labels": [], "entities": [{"text": "Initialization", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.9567607045173645}]}, {"text": "The elliptical template and metonymy template features are initialized with negative weights to initially discourage word skipping.", "labels": [], "entities": [{"text": "word skipping", "start_pos": 117, "end_pos": 130, "type": "TASK", "confidence": 0.7160938084125519}]}, {"text": "Comparison Systems We compare performance with all recent CCG grammar induction algorithms that work with our datasets.", "labels": [], "entities": [{"text": "CCG grammar induction", "start_pos": 58, "end_pos": 79, "type": "TASK", "confidence": 0.5940612852573395}]}, {"text": "This includes methods that used a limited set of hand-engineered templates for inducing the lexicon, ZC05) and ZC07, and those that learned grammar structure by automatically splitting the labeled log-", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 7: Exact-match Geo880 test accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9070919156074524}]}, {"text": " Table 8: Exact-match accuracy on the ATIS  development and test sets.", "labels": [], "entities": [{"text": "Exact-match", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9355724453926086}, {"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9488039016723633}, {"text": "ATIS  development and test sets", "start_pos": 38, "end_pos": 69, "type": "DATASET", "confidence": 0.9219377517700196}]}]}