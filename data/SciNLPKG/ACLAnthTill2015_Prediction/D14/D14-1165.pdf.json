{"title": [{"text": "Typed Tensor Decomposition of Knowledge Bases for Relation Extraction", "labels": [], "entities": [{"text": "Typed Tensor Decomposition of Knowledge Bases", "start_pos": 0, "end_pos": 45, "type": "TASK", "confidence": 0.7997502783934275}, {"text": "Relation Extraction", "start_pos": 50, "end_pos": 69, "type": "TASK", "confidence": 0.956542581319809}]}], "abstractContent": [{"text": "While relation extraction has traditionally been viewed as a task relying solely on textual data, recent work has shown that by taking as input existing facts in the form of entity-relation triples from both knowledge bases and textual data, the performance of relation extraction can be improved significantly.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 6, "end_pos": 25, "type": "TASK", "confidence": 0.8848633170127869}, {"text": "relation extraction", "start_pos": 261, "end_pos": 280, "type": "TASK", "confidence": 0.8073734641075134}]}, {"text": "Following this new paradigm, we propose a tensor decomposition approach for knowledge base embedding that is highly scalable, and is especially suitable for relation extraction.", "labels": [], "entities": [{"text": "tensor decomposition", "start_pos": 42, "end_pos": 62, "type": "TASK", "confidence": 0.7829451262950897}, {"text": "relation extraction", "start_pos": 157, "end_pos": 176, "type": "TASK", "confidence": 0.9489513337612152}]}, {"text": "By leveraging relational domain knowledge about entity type information, our learning algorithm is significantly faster than previous approaches and is better able to discover new relations missing from the database.", "labels": [], "entities": []}, {"text": "In addition, when applied to a relation extraction task, our approach alone is comparable to several existing systems, and improves the weighted mean average precision of a state-of-the-art method by 10 points when used as a subcomponent.", "labels": [], "entities": [{"text": "relation extraction task", "start_pos": 31, "end_pos": 55, "type": "TASK", "confidence": 0.8860251506169637}, {"text": "weighted mean average precision", "start_pos": 136, "end_pos": 167, "type": "METRIC", "confidence": 0.6726364642381668}]}], "introductionContent": [{"text": "Identifying the relationship between entities from free text, relation extraction is a key task for acquiring new facts to increase the coverage of a structured knowledge base.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 62, "end_pos": 81, "type": "TASK", "confidence": 0.853306770324707}]}, {"text": "Given a pre-defined database schema, traditional relation extraction approaches focus on learning a classifier using textual data alone, such as patterns between the occurrences of two entities in documents, to determine whether the entities have a particular relation.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 49, "end_pos": 68, "type": "TASK", "confidence": 0.787654846906662}]}, {"text": "Other than using the existing known facts to label the text corpora in a distant supervision setting ( Work conducted while interning at Microsoft Research.", "labels": [], "entities": []}, {"text": "2009;, an existing knowledge base is typically not involved in the process of relation extraction.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 78, "end_pos": 97, "type": "TASK", "confidence": 0.9043154716491699}]}, {"text": "However, this paradigm has started to shift recently, as researchers showed that by taking existing facts of a knowledge base as an integral part of relation extraction, the model can leverage richer information and thus yields better performance.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 149, "end_pos": 168, "type": "TASK", "confidence": 0.8860894441604614}]}, {"text": "For instance, borrowed the idea of collective filtering and constructed a matrix where each row is a pair of entities and each column is a particular relation.", "labels": [], "entities": [{"text": "collective filtering", "start_pos": 35, "end_pos": 55, "type": "TASK", "confidence": 0.7463362216949463}]}, {"text": "For a true entityrelation triple (e 1 , r, e 2 ), either from the text corpus or from the knowledge base, the corresponding entry in the matrix is 1.", "labels": [], "entities": []}, {"text": "A previously unknown fact (i.e., triple) can be discovered through matrix decomposition.", "labels": [], "entities": []}, {"text": "This approach can be viewed as creating vector representations of each relation and candidate pair of entities.", "labels": [], "entities": []}, {"text": "Because each entity does not have its own representation, relationships of any unpaired entities cannot be discovered.", "labels": [], "entities": []}, {"text": "Alternatively, created two types of embedding -one based on textual similarity and the other based on knowledge base, where the latter maps each entity and relation to the same ddimensional vector space using a model proposed by.", "labels": [], "entities": []}, {"text": "They also showed that combining these two models results in a significant improvement over the model trained using only textual data.", "labels": [], "entities": []}, {"text": "To make such an integrated strategy work, it is important to capture all existing entities and relations, as well as the known facts, from both textual data and large databases.", "labels": [], "entities": []}, {"text": "In this paper, we propose anew knowledge base embedding model, TRESCAL, that is highly efficient and scalable, with relation extraction as our target application.", "labels": [], "entities": [{"text": "TRESCAL", "start_pos": 63, "end_pos": 70, "type": "METRIC", "confidence": 0.9107298851013184}, {"text": "relation extraction", "start_pos": 116, "end_pos": 135, "type": "TASK", "confidence": 0.7887172102928162}]}, {"text": "Our work is built on top of RESCAL (), which is a tensor decomposition method that has proven its scalability by factoring YAGO () with 3 million entities and 41 million triples ().", "labels": [], "entities": [{"text": "RESCAL", "start_pos": 28, "end_pos": 34, "type": "METRIC", "confidence": 0.9752306342124939}, {"text": "tensor decomposition", "start_pos": 50, "end_pos": 70, "type": "TASK", "confidence": 0.722678929567337}, {"text": "YAGO", "start_pos": 123, "end_pos": 127, "type": "METRIC", "confidence": 0.7623409628868103}]}, {"text": "We improve the tensor decomposition model with two technical innovations.", "labels": [], "entities": [{"text": "tensor decomposition", "start_pos": 15, "end_pos": 35, "type": "TASK", "confidence": 0.8207681477069855}]}, {"text": "First, we exclude the triples that do not satisfy the relational constraints (e.g., both arguments of the relation spouse-of need to be person entities) from the loss, which is done by selecting sub-matrices of each slice of the tensor during training.", "labels": [], "entities": []}, {"text": "Second, we introduce a mathematical technique that significantly reduces the computational complexity in both time and space when the loss function contains a regularization term.", "labels": [], "entities": []}, {"text": "As a consequence, our method is more than four times faster than RESCAL, and is also more accurate in discovering unseen triples.", "labels": [], "entities": [{"text": "RESCAL", "start_pos": 65, "end_pos": 71, "type": "METRIC", "confidence": 0.9277411699295044}]}, {"text": "First, compared to other knowledge base embedding methods developed more recently, it is much more efficient to train our model.", "labels": [], "entities": []}, {"text": "As will be seen in Sec.", "labels": [], "entities": []}, {"text": "5, when applied to a large knowledge base created using NELL) that has 1.8M entity-relation triples, our method finishes training in 4 to 5 hours, while an alternative method (Bordes et al., 2013a) needs almost 3 days.", "labels": [], "entities": []}, {"text": "Moreover, the prediction accuracy of our model is competitive to others, if not higher.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9756394028663635}]}, {"text": "Second, to validate its value to relation extraction, we apply TRESCAL to extracting relations from a free text corpus along with a knowledge base, using the data provided in ().", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 33, "end_pos": 52, "type": "TASK", "confidence": 0.8687728345394135}, {"text": "TRESCAL", "start_pos": 63, "end_pos": 70, "type": "METRIC", "confidence": 0.9745372533798218}]}, {"text": "We show that TRESCAL is complementary to existing systems and significantly improves their performance when using it as a subcomponent.", "labels": [], "entities": [{"text": "TRESCAL", "start_pos": 13, "end_pos": 20, "type": "METRIC", "confidence": 0.4799119234085083}]}, {"text": "For instance, this strategy improves the weighted mean average precision of the best approach in () by 10 points (47% to 57%).", "labels": [], "entities": [{"text": "weighted mean average precision", "start_pos": 41, "end_pos": 72, "type": "METRIC", "confidence": 0.6941412389278412}]}, {"text": "The remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "We survey most related work in Sec.", "labels": [], "entities": [{"text": "Sec.", "start_pos": 31, "end_pos": 35, "type": "TASK", "confidence": 0.7761723101139069}]}, {"text": "2 and provide the technical background of our approach in Sec.", "labels": [], "entities": []}, {"text": "3. Our approach is detailed in Sec.", "labels": [], "entities": []}, {"text": "4, followed by the experimental validation in Sec.", "labels": [], "entities": [{"text": "Sec.", "start_pos": 46, "end_pos": 50, "type": "TASK", "confidence": 0.7164149880409241}]}], "datasetContent": [{"text": "We conduct two sets of experiments.", "labels": [], "entities": []}, {"text": "The first evaluates the proposed TRESCAL algorithm on inferring unknown facts using existing relationentity triples, while the second demonstrates its application to relation extraction when a text corpus is available.", "labels": [], "entities": [{"text": "TRESCAL", "start_pos": 33, "end_pos": 40, "type": "METRIC", "confidence": 0.6742121577262878}, {"text": "relation extraction", "start_pos": 166, "end_pos": 185, "type": "TASK", "confidence": 0.8358263969421387}]}], "tableCaptions": [{"text": " Table 1: Data statistics of the training set from  NELL in our experiments.", "labels": [], "entities": [{"text": "NELL", "start_pos": 52, "end_pos": 56, "type": "TASK", "confidence": 0.6783279180526733}]}, {"text": " Table 2: Model performance in mean average precision (MAP) on entity retrieval and relation retrieval.   \u2020 and  \u2021 indicate the comparison to TRESCAL in the same setting is statistically significant using a paired- t test on average precision of each query, with p < 0.01 and p < 0.05, respectively. Enforcing type  constraints during test time improves entity retrieval substantially, but does not help in relation retrieval.", "labels": [], "entities": [{"text": "mean average precision (MAP)", "start_pos": 31, "end_pos": 59, "type": "METRIC", "confidence": 0.8711369633674622}, {"text": "entity retrieval", "start_pos": 63, "end_pos": 79, "type": "TASK", "confidence": 0.6997283399105072}, {"text": "relation retrieval", "start_pos": 84, "end_pos": 102, "type": "TASK", "confidence": 0.7340096086263657}, {"text": "entity retrieval", "start_pos": 354, "end_pos": 370, "type": "TASK", "confidence": 0.729753702878952}, {"text": "relation retrieval", "start_pos": 407, "end_pos": 425, "type": "TASK", "confidence": 0.7925259470939636}]}, {"text": " Table 3: Weighted Mean Average Precisions. The # column shows the number of true facts in the pool.  Bold faced are winners per relation, italics indicate ties based on a sign test.", "labels": [], "entities": [{"text": "Weighted Mean Average Precisions", "start_pos": 10, "end_pos": 42, "type": "METRIC", "confidence": 0.8108412027359009}]}]}