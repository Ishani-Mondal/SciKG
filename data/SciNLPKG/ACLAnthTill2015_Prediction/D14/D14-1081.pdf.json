{"title": [{"text": "The Inside-Outside Recursive Neural Network model for Dependency Parsing", "labels": [], "entities": [{"text": "Dependency Parsing", "start_pos": 54, "end_pos": 72, "type": "TASK", "confidence": 0.8375776410102844}]}], "abstractContent": [{"text": "We propose the first implementation of an infinite-order generative dependency model.", "labels": [], "entities": []}, {"text": "The model is based on anew recursive neural network architecture, the Inside-Outside Recursive Neural Network.", "labels": [], "entities": []}, {"text": "This architecture allows information to flow not only bottom-up, as in traditional recursive neural networks, but also top-down.", "labels": [], "entities": []}, {"text": "This is achieved by computing content as well as context representations for any constituent, and letting these representations interact.", "labels": [], "entities": []}, {"text": "Experimental results on the English section of the Universal Dependency Treebank show that the infinite-order model achieves a per-plexity seven times lower than the traditional third-order model using counting, and tends to choose more accurate parses in k-best lists.", "labels": [], "entities": [{"text": "English section of the Universal Dependency Treebank", "start_pos": 28, "end_pos": 80, "type": "DATASET", "confidence": 0.9246041945048741}]}, {"text": "In addition, reranking with this model achieves state-of-the-art unla-belled attachment scores and unlabelled exact match scores.", "labels": [], "entities": [{"text": "exact match scores", "start_pos": 110, "end_pos": 128, "type": "METRIC", "confidence": 0.8796331485112509}]}], "introductionContent": [{"text": "Estimating probability distributions is the core issue in modern, data-driven natural language processing methods.", "labels": [], "entities": [{"text": "Estimating probability distributions", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.8676752845446268}]}, {"text": "Because of the traditional definition of discrete probability P r(A) \u2261 the number of times A occurs the size of event space counting has become a standard method to tackle the problem.", "labels": [], "entities": []}, {"text": "When data are sparse, smoothing techniques are needed to adjust counts for nonobserved or rare events.", "labels": [], "entities": []}, {"text": "However, successful use of those techniques has turned out to bean art.", "labels": [], "entities": []}, {"text": "For instance, much skill and expertise is required to create reasonable reduction lists for back-off, and to avoid impractically large count tables, which store events and their counts.", "labels": [], "entities": []}, {"text": "An alternative to counting for estimating probability distributions is to use neural networks.", "labels": [], "entities": []}, {"text": "Thanks to recent advances in deep learning, this approach has recently started to look very promising again, with state-of-the-art results in sentiment analysis, language modelling (, and other tasks.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 142, "end_pos": 160, "type": "TASK", "confidence": 0.9741975963115692}, {"text": "language modelling", "start_pos": 162, "end_pos": 180, "type": "TASK", "confidence": 0.7882452011108398}]}, {"text": "The work, in particular, demonstrates the advantage of neural-network-based approaches over counting-based approaches in language modelling: it shows that recurrent neural networks are capable of capturing long histories efficiently and surpass standard n-gram techniques (e.g., Kneser-Ney smoothed 5-gram).", "labels": [], "entities": [{"text": "language modelling", "start_pos": 121, "end_pos": 139, "type": "TASK", "confidence": 0.7807543873786926}]}, {"text": "In this paper, keeping in mind the success of these models, we compare the two approaches.", "labels": [], "entities": []}, {"text": "Complementing recent work that focused on such a comparison for the case of finding appropriate word vectors (), we focus hereon models that involve more complex, hierarchical structures.", "labels": [], "entities": []}, {"text": "Starting with existing generative models that use counting to estimate probability distributions over constituency and dependency parses (e.g.,,), we develop an alternative based on recursive neural networks.", "labels": [], "entities": []}, {"text": "This is a non-trivial task because, to our knowledge, no existing neural network architecture can be used in this way.", "labels": [], "entities": []}, {"text": "For instance, classic recurrent neural networks unfold to leftbranching trees, and are notable to process arbitrarily shaped parse trees that the counting approaches are applied to.", "labels": [], "entities": []}, {"text": "Recursive neural networks () and extensions), on the other hand, do work with trees of arbitrary shape, but process them in a bottom-up manner.", "labels": [], "entities": []}, {"text": "The probabilities we need to estimate are, in contrast, defined by top-down generative models, or by models that require information flows in both directions (e.g., the probability of generating anode depends on the whole fragment rooted at its just-generated sis- ter).", "labels": [], "entities": []}, {"text": "To tackle this problem, we propose anew architecture: the Inside-Outside Recursive Neural Network (IORNN) in which information can flow not only bottom-up but also top-down, inward and outward.", "labels": [], "entities": []}, {"text": "The crucial innovation in our architecture is that every node in a hierarchical structure is associated with two vectors: one vector, the inner representation, representing the content under that node, and another vector, the outer representation, representing its context (see).", "labels": [], "entities": []}, {"text": "Inner representations can be computed bottom-up; outer representations, in turn, can be computed top-down.", "labels": [], "entities": []}, {"text": "This allows information to flow in any direction, depending on the application, and makes the IORNN a natural tool for estimating probabilities in tree-based generative models.", "labels": [], "entities": []}, {"text": "We demonstrate the use of the IORNN by applying it to an \u221e-order generative dependency model which is impractical for counting due to the problem of data sparsity.", "labels": [], "entities": [{"text": "IORNN", "start_pos": 30, "end_pos": 35, "type": "METRIC", "confidence": 0.5117865204811096}]}, {"text": "Counting, instead, is used to estimate a third-order generative model as in and.", "labels": [], "entities": []}, {"text": "Our experimental results show that our new model not only achieves a seven times lower perplexity than the third-order model, but also tends to choose more accurate candidates in k-best lists.", "labels": [], "entities": []}, {"text": "In addition, reranking with this model achieves stateof-the-art scores on the task of supervised dependency parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 97, "end_pos": 115, "type": "TASK", "confidence": 0.6748367995023727}]}, {"text": "The outline of the paper is following.", "labels": [], "entities": []}, {"text": "Firstly, we give an introduction to Eisner's generative model in Section 2.", "labels": [], "entities": []}, {"text": "Then, we present the third-order model using counting in Section 3, and propose the IORNN in Section 4.", "labels": [], "entities": [{"text": "IORNN", "start_pos": 84, "end_pos": 89, "type": "METRIC", "confidence": 0.8127853274345398}]}, {"text": "Finally, in Section 5 we show our experimental results.", "labels": [], "entities": []}], "datasetContent": [{"text": "In our experiments, we convert the Penn Treebank to dependencies using the Universal dependency annotation ; this yields a dependency tree corpus we label PTB-U.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 35, "end_pos": 48, "type": "DATASET", "confidence": 0.9930136501789093}, {"text": "PTB-U", "start_pos": 155, "end_pos": 160, "type": "DATASET", "confidence": 0.9359928965568542}]}, {"text": "In order to compare with other systems, we also experiment with an alternative conversion using the head rules of ; this yields a dependency tree corpus we label PTB-YM.", "labels": [], "entities": [{"text": "PTB-YM", "start_pos": 162, "end_pos": 168, "type": "DATASET", "confidence": 0.9264516234397888}]}, {"text": "Sections 2-21 are used for training, section 22 for development, and section 23 for testing.", "labels": [], "entities": []}, {"text": "For the PTB-U, the gold POS-tags are used.", "labels": [], "entities": [{"text": "PTB-U", "start_pos": 8, "end_pos": 13, "type": "DATASET", "confidence": 0.9073984026908875}]}, {"text": "For the PTB-YM, the development and test sets are tagged by the Stanford POS-tagger 4 trained on the whole Perplexity 3rd-order model 1736.73 \u221e-order model 236.58: Perplexities of the two models on PTB-U-22.", "labels": [], "entities": [{"text": "PTB-YM", "start_pos": 8, "end_pos": 14, "type": "DATASET", "confidence": 0.9686230421066284}, {"text": "PTB-U-22", "start_pos": 198, "end_pos": 206, "type": "DATASET", "confidence": 0.9793821573257446}]}, {"text": "training data, whereas 10-way jackknifing is used to generate tags for the training set.", "labels": [], "entities": []}, {"text": "The vocabulary for both models, the third-order model and the \u221e-order model, is taken as a list of words occurring more than two times in the training data.", "labels": [], "entities": []}, {"text": "All other words are labelled 'UN-KNOWN' and every digit is replaced by '0'.", "labels": [], "entities": [{"text": "UN-KNOWN", "start_pos": 30, "end_pos": 38, "type": "DATASET", "confidence": 0.5453431606292725}]}, {"text": "For the IORNN used by the \u221e-order model, we set n = 200, and define f as the tanh activation function.", "labels": [], "entities": [{"text": "IORNN", "start_pos": 8, "end_pos": 13, "type": "DATASET", "confidence": 0.7906251549720764}]}, {"text": "We initialise it with the 50-dim word embeddings from and train it with the learning rate 0.1, \u03bb W = 10 \u22124 , \u03bb L = 10 \u221210 .", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Percentages of events extracted from  PTB-U-22 appearing more than twice in the train- ing set. Events are grouped according to the reduc- tion lists in Equation 2. d, t, w, c stand for depen- dency relation, POS-tag, word, and capitalisation  feature.", "labels": [], "entities": [{"text": "PTB-U-22", "start_pos": 48, "end_pos": 56, "type": "DATASET", "confidence": 0.9537951350212097}]}, {"text": " Table 3: Comparison based on reranking on PTB- U-23. The numbers in the brackets are improve- ments over the MSTParser.", "labels": [], "entities": [{"text": "PTB- U-23", "start_pos": 43, "end_pos": 52, "type": "DATASET", "confidence": 0.960761288801829}, {"text": "MSTParser", "start_pos": 110, "end_pos": 119, "type": "DATASET", "confidence": 0.9804466962814331}]}]}