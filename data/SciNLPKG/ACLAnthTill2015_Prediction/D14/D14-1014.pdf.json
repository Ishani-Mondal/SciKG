{"title": [{"text": "Submodularity for Data Selection in Statistical Machine Translation", "labels": [], "entities": [{"text": "Data Selection", "start_pos": 18, "end_pos": 32, "type": "TASK", "confidence": 0.7219076901674271}, {"text": "Statistical Machine Translation", "start_pos": 36, "end_pos": 67, "type": "TASK", "confidence": 0.7834300597508749}]}], "abstractContent": [{"text": "We introduce submodular optimization to the problem of training data subset selection for statistical machine translation (SMT).", "labels": [], "entities": [{"text": "training data subset selection", "start_pos": 55, "end_pos": 85, "type": "TASK", "confidence": 0.6360593065619469}, {"text": "statistical machine translation (SMT)", "start_pos": 90, "end_pos": 127, "type": "TASK", "confidence": 0.7933796495199203}]}, {"text": "By explicitly formulating data selection as a submodular program, we obtain fast scalable selection algorithms with mathematical performance guarantees, resulting in a unified framework that clarifies existing approaches and also makes both new and many previous approaches easily accessible.", "labels": [], "entities": []}, {"text": "We present anew class of submodular functions designed specifically for SMT and evaluate them on two different translation tasks.", "labels": [], "entities": [{"text": "SMT", "start_pos": 72, "end_pos": 75, "type": "TASK", "confidence": 0.9943364262580872}]}, {"text": "Our results show that our best submodular method significantly outperforms several baseline methods, including the widely-used cross-entropy based data selection method.", "labels": [], "entities": []}, {"text": "In addition, our approach easily scales to large data sets and is applicable to other data selection problems in natural language processing.", "labels": [], "entities": []}], "introductionContent": [{"text": "SMT has made significant progress over the last decade, not least due to the availability of increasingly larger data sets.", "labels": [], "entities": [{"text": "SMT", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.98208087682724}]}, {"text": "Large-scale SMT systems are now routinely trained on millions of sentences of parallel data, and billions of words of monolingual data for language modeling.", "labels": [], "entities": [{"text": "SMT", "start_pos": 12, "end_pos": 15, "type": "TASK", "confidence": 0.9913167953491211}, {"text": "language modeling", "start_pos": 139, "end_pos": 156, "type": "TASK", "confidence": 0.6829971373081207}]}, {"text": "Large data sets are often beneficial, but they do create certain other problems.", "labels": [], "entities": []}, {"text": "First, they place higher demands on computational resources (storage and compute).", "labels": [], "entities": []}, {"text": "Hence, existing software infrastructure may need to be adapted and optimized to handle such large data sets.", "labels": [], "entities": []}, {"text": "Second, experimental turn-around time is increased as well, making it more difficult to quickly train, fine-tune, and evaluate novel modeling approaches.", "labels": [], "entities": []}, {"text": "Most importantly, however, SMT performance does not increase linearly with the training data size but levels off after a certain point.", "labels": [], "entities": [{"text": "SMT", "start_pos": 27, "end_pos": 30, "type": "TASK", "confidence": 0.9948244094848633}]}, {"text": "This is because the additional training data maybe noisy, irrelevant to the task at hand, or inherently redundant.", "labels": [], "entities": []}, {"text": "Thus, a linear increase in the amount of training data typically leads to a sublinear increase in performance, an effect known as diminishing returns.", "labels": [], "entities": []}, {"text": "Several recent papers) have amply demonstrated this effect.", "labels": [], "entities": []}, {"text": "A way to counteract this is to perform data subset selection, i.e., choose a subset of the available training data to optimize a particular quality criterion.", "labels": [], "entities": []}, {"text": "One scheme is to select a subset that expresses as much of the information in the original data set as possible -i.e., the data set should be \"summarized\" by excluding redundant information.", "labels": [], "entities": []}, {"text": "Another scheme, popular in the context of SMT, is to subselect the original training set to match the properties of a particular test set.", "labels": [], "entities": [{"text": "SMT", "start_pos": 42, "end_pos": 45, "type": "TASK", "confidence": 0.9967323541641235}]}, {"text": "In this paper, we introduce submodularity for subselecting SMT training data, a methodology that follows both of the above schemes.", "labels": [], "entities": [{"text": "SMT training", "start_pos": 59, "end_pos": 71, "type": "TASK", "confidence": 0.8966361582279205}]}, {"text": "1 Submodular functions) area class of discrete set functions having the property of diminishing returns.", "labels": [], "entities": []}, {"text": "They occur naturally in a wide range of problems in a diverse set of fields including economics, game theory, operations research, circuit theory, and more recently machine learning.", "labels": [], "entities": [{"text": "game theory", "start_pos": 97, "end_pos": 108, "type": "TASK", "confidence": 0.7174937278032303}, {"text": "operations research", "start_pos": 110, "end_pos": 129, "type": "TASK", "confidence": 0.8487198054790497}, {"text": "circuit theory", "start_pos": 131, "end_pos": 145, "type": "TASK", "confidence": 0.8613811731338501}]}, {"text": "Submodular functions share certain properties with convexity (e.g., naturalness and mathematical tractability) although submodularity is still quite distinct from convexity.", "labels": [], "entities": []}, {"text": "We present a novel class of submodular functions particularly suited for SMT subselection and evaluate it against state-of-the-art baseline methods on two different translation tasks, showing that our method outperforms them significantly inmost cases.", "labels": [], "entities": [{"text": "SMT subselection", "start_pos": 73, "end_pos": 89, "type": "TASK", "confidence": 0.947079062461853}]}, {"text": "While many approaches to SMT data selection have been developed previously (a detailed overview is provided in Section 3), many of them are heuristic and do not offer performance guarantees.", "labels": [], "entities": [{"text": "SMT data selection", "start_pos": 25, "end_pos": 43, "type": "TASK", "confidence": 0.8734442591667175}]}, {"text": "Certain previous approaches, however, have inadvertently made use of submodular methods.", "labels": [], "entities": []}, {"text": "This, in addition to our own positive results, provides strong evidence that submodularity is a natural and practical framework for data subset selection in SMT and related fields.", "labels": [], "entities": [{"text": "data subset selection", "start_pos": 132, "end_pos": 153, "type": "TASK", "confidence": 0.6834659179051717}, {"text": "SMT", "start_pos": 157, "end_pos": 160, "type": "TASK", "confidence": 0.9753791689872742}]}, {"text": "An additional advantage of this framework is that many submodular programs (e.g., the greedy procedure reviewed in Section 2) are fast and scalable to large data sets.", "labels": [], "entities": []}, {"text": "By contrast, trying to solve a submodular problem using, say, an integer-linear programming (ILP) procedure, would lead to impenetrable scalability problems.", "labels": [], "entities": []}, {"text": "Initial value f(X) = 2 colors in urn.", "labels": [], "entities": []}, {"text": "Updated value f(X\u222a{v}) = 3 with added blue ball.", "labels": [], "entities": []}, {"text": "Initial value f(Y) = 3 colors in urn.", "labels": [], "entities": [{"text": "Initial value f(Y)", "start_pos": 0, "end_pos": 18, "type": "METRIC", "confidence": 0.830754429101944}]}, {"text": "Updated value f(Y\u222a{v}) = 3 with added blue ball.", "labels": [], "entities": []}, {"text": "X Y v v This paper makes several contributions: First, we present a brief overview of submodular functions (Section 2) and their potential application to natural language processing (NLP).", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 154, "end_pos": 187, "type": "TASK", "confidence": 0.7712444067001343}]}, {"text": "Next we review previous approaches to MT data selection and analyze them with respect to their submodular properties.", "labels": [], "entities": [{"text": "MT data selection", "start_pos": 38, "end_pos": 55, "type": "TASK", "confidence": 0.8531084458033243}]}, {"text": "We find that some previous approaches are submodular in nature although this connection was not heretofore made explicit.", "labels": [], "entities": []}, {"text": "Section 4 details our new approach.", "labels": [], "entities": []}, {"text": "We discuss desirable properties of an SMT data selection objective and present anew class of submodular functions tailored towards this problem.", "labels": [], "entities": [{"text": "SMT data selection objective", "start_pos": 38, "end_pos": 66, "type": "TASK", "confidence": 0.8709413409233093}]}, {"text": "Section 5 presents the data and systems used for the experiments, and results are reported in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "Function parameters: Different instantiations of the general submodular function in Eq.", "labels": [], "entities": [{"text": "Function", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.8329402804374695}, {"text": "Eq", "start_pos": 84, "end_pos": 86, "type": "DATASET", "confidence": 0.8825704455375671}]}, {"text": "4 (\u03b2 = 1.5 in all cases).", "labels": [], "entities": []}, {"text": "We first trained a baseline system on 100% of the training data.", "labels": [], "entities": []}, {"text": "Different data selection methods were then used to select subsets of 10%, 20%, 30% and 40% of the data.", "labels": [], "entities": []}, {"text": "While not reported in the tables, above 40%, the performance slowly drops to the 100% performance.", "labels": [], "entities": []}, {"text": "The first baseline selection method utilizes random data selection, for which 3 different data sets of the specified size were drawn randomly from the training data.", "labels": [], "entities": []}, {"text": "Individual systems were trained on all random subsets of the same size, and their scores were averaged.", "labels": [], "entities": []}, {"text": "The second baseline is the cross-entropy method by.", "labels": [], "entities": []}, {"text": "In-domain language models were trained on the combined development and test data, and outof-domain models were trained on an equivalent amount of data drawn randomly from the training set.", "labels": [], "entities": []}, {"text": "Sentences were ranked by the function in Eq.", "labels": [], "entities": [{"text": "Eq", "start_pos": 41, "end_pos": 43, "type": "DATASET", "confidence": 0.8757076859474182}]}, {"text": "5, and the top k percent were chosen.", "labels": [], "entities": []}, {"text": "The order of the n-gram models was optimized on the development set and was found to be 3.", "labels": [], "entities": []}, {"text": "Larger model orders resulted in worse performance, possibly due to the limited size of the data used for their training.", "labels": [], "entities": []}, {"text": "Since this method also involves random data selection, we report the average BLEU score over 5 different trials.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 77, "end_pos": 87, "type": "METRIC", "confidence": 0.9801377058029175}]}, {"text": "For the submodular selection method, shows the different values that were tested for the four components listed in Section 4.", "labels": [], "entities": []}, {"text": "The combination was optimized on the development set.", "labels": [], "entities": []}, {"text": "The selection algorithm (Alg. 1) runs within a few minutes on our complete training set of 189M words.", "labels": [], "entities": []}, {"text": "Results on the NIST 2009 test set are shown in.", "labels": [], "entities": [{"text": "NIST 2009 test set", "start_pos": 15, "end_pos": 33, "type": "DATASET", "confidence": 0.986693486571312}]}, {"text": "The scores for the submodular systems are averages over 3 different runs of MERT tuning.", "labels": [], "entities": []}, {"text": "Random data subset selection (Row 1) falls short of the baseline system using 100% of the training data.", "labels": [], "entities": [{"text": "Random data subset selection", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.5574208498001099}]}, {"text": "The cross-entropy method (Row 2) surpasses the performance of the baseline system at about 20% of the data, demonstrating that data subset selection is a suitable technique for such mixeddomain translation tasks.", "labels": [], "entities": [{"text": "mixeddomain translation tasks", "start_pos": 182, "end_pos": 211, "type": "TASK", "confidence": 0.7752110362052917}]}, {"text": "The following rows show results for the various submodular functions shown in.", "labels": [], "entities": []}, {"text": "Out of these, SM-5 corresponds to the best approach in ().", "labels": [], "entities": []}, {"text": "SM-6 is our own best-performing function, beating the cross-entropy method by a statistically significant margin (p \u2264 0.05) under all conditions.", "labels": [], "entities": []}, {"text": "5 SM-6 is also significantly better than SM-5 in two cases.", "labels": [], "entities": []}, {"text": "Finally, it surpasses the performance of the all-data system at only 10% of the training data; possibly, even smaller training data sets could be used but this option was not investigated.", "labels": [], "entities": []}, {"text": "While the bilingual submodular functions SM-2 and SM-7) yield an improvement of up to 0.015 BLEU points on the dev set (not shown in the table), they do not consistently outperform the monolingual functions on the test set.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 92, "end_pos": 96, "type": "METRIC", "confidence": 0.9986603260040283}]}, {"text": "Since test set target features cannot be used in our scenario, bilingual features are only helpful to the extent that the development set closely matches the test set.", "labels": [], "entities": []}, {"text": "However, target features should be quite helpful when selecting data from an out-of-domain set to match an in-domain training set (as in e.g.).", "labels": [], "entities": []}, {"text": "We found no gain from the length reward \u03b2 |u| . The Europarl results show a similar pattern.", "labels": [], "entities": [{"text": "Europarl", "start_pos": 52, "end_pos": 60, "type": "DATASET", "confidence": 0.9761924147605896}]}, {"text": "Although the differences in BLEU scores are smaller overall (as expected on an in-domain translation task), data subset selection improves over the all-data baseline system in this case as well.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 28, "end_pos": 32, "type": "METRIC", "confidence": 0.9973273277282715}]}, {"text": "The cross-entropy method again outperforms random data selection.", "labels": [], "entities": []}, {"text": "On this task we only tested our submodular function that worked best on the Statistical significance was measured using the paired bootstrap resampling test of), applied to the systems with the median BLEU scores.", "labels": [], "entities": [{"text": "Statistical significance", "start_pos": 76, "end_pos": 100, "type": "METRIC", "confidence": 0.8413910865783691}, {"text": "paired bootstrap resampling test", "start_pos": 124, "end_pos": 156, "type": "METRIC", "confidence": 0.6535846069455147}, {"text": "BLEU", "start_pos": 201, "end_pos": 205, "type": "METRIC", "confidence": 0.9976887702941895}]}, {"text": "NIST task; again we find that it outperforms the cross-entropy method.", "labels": [], "entities": [{"text": "NIST", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9285112619400024}]}, {"text": "In two conditions (10% and 30%) these differences are statistically significant.", "labels": [], "entities": []}, {"text": "10% of the training data suffices to outperform the all-data system, and up to a full BLEU point can be gained on this task using 20-30% of the data and a submodular data selection method.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 86, "end_pos": 90, "type": "METRIC", "confidence": 0.9993112087249756}]}], "tableCaptions": [{"text": " Table 2: BLEU scores (standard deviations) on the NIST 2009 (Ara-En) test set for random (Rand),  cross-entropy (Xent), and submodular (SM) data selection methods defined in", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9993934631347656}, {"text": "NIST 2009 (Ara-En) test set", "start_pos": 51, "end_pos": 78, "type": "DATASET", "confidence": 0.9684861132076809}]}, {"text": " Table 4. 100% = system  using all of the training data. Boldface numbers indicate a statistically significant improvement (p \u2264 0.05)  over the median Xent system. Starred scores are also significantly better than SM-5.", "labels": [], "entities": []}, {"text": " Table 3: BLEU scores (standard deviation) on the Europarl translation task for random (Rand), cross- entropy (Xent), and submodular (SM) data selection methods. 100% = system using all of the training  data. Boldface numbers indicate a statistically significant improvement (p \u2264 0.05) over the median Xent  system. Starred scores are significantly better than SM-5.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9993547797203064}, {"text": "Europarl translation task", "start_pos": 50, "end_pos": 75, "type": "TASK", "confidence": 0.6970959305763245}]}]}