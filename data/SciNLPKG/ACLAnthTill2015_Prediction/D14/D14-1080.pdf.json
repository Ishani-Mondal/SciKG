{"title": [{"text": "Opinion Mining with Deep Recurrent Neural Networks", "labels": [], "entities": [{"text": "Opinion Mining", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.8048321008682251}]}], "abstractContent": [{"text": "Recurrent neural networks (RNNs) are con-nectionist models of sequential data that are naturally applicable to the analysis of natural language.", "labels": [], "entities": []}, {"text": "Recently, \"depth in space\"-as an orthogonal notion to \"depth in time\"-in RNNs has been investigated by stacking multiple layers of RNNs and shown empirically to bring a temporal hierarchy to the architecture.", "labels": [], "entities": []}, {"text": "In this work we apply these deep RNNs to the task of opinion expression extraction formulated as a token-level sequence-labeling task.", "labels": [], "entities": [{"text": "opinion expression extraction", "start_pos": 53, "end_pos": 82, "type": "TASK", "confidence": 0.7573867440223694}]}, {"text": "Experimental results show that deep, narrow RNNs outperform traditional shallow, wide RNNs with the same number of parameters.", "labels": [], "entities": []}, {"text": "Furthermore, our approach outperforms previous CRF-based baselines, including the state-of-the-art semi-Markov CRF model, and does so without access to the powerful opinion lexicons and syntactic features relied upon by the semi-CRF, as well as without the standard layer-by-layer pre-training typically required of RNN architectures.", "labels": [], "entities": []}], "introductionContent": [{"text": "Fine-grained opinion analysis aims to detect the subjective expressions in a text (e.g. \"hate\") and to characterize their intensity (e.g. strong) and sentiment (e.g. negative) as well as to identify the opinion holder (the entity expressing the opinion) and the target, or topic, of the opinion (i.e. what the opinion is about) ( ).", "labels": [], "entities": []}, {"text": "Fine-grained opinion analysis is important fora variety of NLP tasks including opinion-oriented question answering and opinion summarization.", "labels": [], "entities": [{"text": "opinion analysis", "start_pos": 13, "end_pos": 29, "type": "TASK", "confidence": 0.6995076686143875}, {"text": "opinion-oriented question answering", "start_pos": 79, "end_pos": 114, "type": "TASK", "confidence": 0.6170948644479116}, {"text": "opinion summarization", "start_pos": 119, "end_pos": 140, "type": "TASK", "confidence": 0.7401581406593323}]}, {"text": "As a result, it has been studied extensively in recent years.", "labels": [], "entities": []}, {"text": "In this work, we focus on the detection of opinion expressions -both direct subjective expressions (DSEs) and expressive subjective expressions (ESEs) as defined in . DSEs consist of explicit mentions of private states or speech events expressing private states; and ESEs consist of expressions that indicate sentiment, emotion, etc., without explicitly conveying them.", "labels": [], "entities": []}, {"text": "An example sentence shown in in which the DSE \"has refused to make any statements\" explicitly expresses an opinion holder's attitude and the: An example sentence with labels ESE \"as usual\" indirectly expresses the attitude of the writer.", "labels": [], "entities": [{"text": "DSE", "start_pos": 42, "end_pos": 45, "type": "DATASET", "confidence": 0.9100437760353088}, {"text": "ESE", "start_pos": 174, "end_pos": 177, "type": "METRIC", "confidence": 0.8554387092590332}]}, {"text": "Opinion extraction has often been tackled as a sequence labeling problem in previous work (e.g.).", "labels": [], "entities": [{"text": "Opinion extraction", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8951146304607391}, {"text": "sequence labeling", "start_pos": 47, "end_pos": 64, "type": "TASK", "confidence": 0.6527952402830124}]}, {"text": "This approach views a sentence as a sequence of tokens labeled using the conventional BIO tagging scheme: B indicates the beginning of an opinion-related expression, I is used for tokens inside the opinion-related expression, and O indicates tokens outside any opinion-related class.", "labels": [], "entities": [{"text": "BIO tagging", "start_pos": 86, "end_pos": 97, "type": "TASK", "confidence": 0.6269888579845428}]}, {"text": "The example sentence in shows the appropriate tags in the BIO scheme.", "labels": [], "entities": [{"text": "BIO scheme", "start_pos": 58, "end_pos": 68, "type": "DATASET", "confidence": 0.7881620824337006}]}, {"text": "For instance, the ESE \"as usual\" results in the tags B ESE for \"as\" and I ESE for \"usual\".", "labels": [], "entities": [{"text": "ESE", "start_pos": 18, "end_pos": 21, "type": "METRIC", "confidence": 0.9532527327537537}, {"text": "ESE", "start_pos": 55, "end_pos": 58, "type": "METRIC", "confidence": 0.5053455233573914}, {"text": "I ESE", "start_pos": 72, "end_pos": 77, "type": "METRIC", "confidence": 0.7669507563114166}]}, {"text": "Variants of conditional random field (CRF) approaches have been successfully applied to opinion expression extraction using this token-based view: the state-of-the-art approach is the semiCRF, which relaxes the Markovian assumption inherent to CRFs and operates at the phrase level rather than the token level, allowing the incorporation of phrase-level features.", "labels": [], "entities": [{"text": "opinion expression extraction", "start_pos": 88, "end_pos": 117, "type": "TASK", "confidence": 0.7890608708063761}]}, {"text": "The success of the CRF-and semiCRF-based approaches, however, hinges critically on access to an appropriate feature set, typically based on constituent and dependency parse trees, manually crafted opinion lexicons, named entity taggers and other preprocessing components (see for an up-todate list).", "labels": [], "entities": []}, {"text": "Distributed representation learners provide a different approach to learning in which latent features are modeled as distributed dense vectors of hidden layers.", "labels": [], "entities": []}, {"text": "A recurrent neural network (RNN) is one such learner that can operate on sequential data of variable length, which means it can also be applied as a sequence labeler.", "labels": [], "entities": []}, {"text": "Moreover, bidirectional RNNs incorporate information from preceding as well as following tokens) while recent advances in word embedding induction) have enabled more effective training of RNNs by allowing a lower dimensional dense input representation and hence, more compact networks.", "labels": [], "entities": [{"text": "word embedding induction", "start_pos": 122, "end_pos": 146, "type": "TASK", "confidence": 0.6586858431498209}]}, {"text": "Finally, deep recurrent networks, a type of RNN with multiple stacked hidden layers, are shown to naturally employ a temporal hierarchy with multiple layers operating at different time scales: lower levels capture short term interactions among words; higher layers reflect interpretations aggregated over longer spans of text.", "labels": [], "entities": []}, {"text": "When applied to natural language sentences, such hierarchies might better model the multi-scale language effects that are emblematic of natural languages, as suggested by previous results.", "labels": [], "entities": []}, {"text": "Motivated by the recent success of deep architectures in general and deep recurrent networks in particular, we explore an application of deep bidirectional RNNshenceforth deep RNNs -to the task of opinion expression extraction.", "labels": [], "entities": [{"text": "opinion expression extraction", "start_pos": 197, "end_pos": 226, "type": "TASK", "confidence": 0.7579358617464701}]}, {"text": "For both DSE and ESE detection, we show that such models outperform conventional, shallow (uni-and bidirectional) RNNs as well as previous CRF-based state-of-the-art baselines, including the semiCRF model.", "labels": [], "entities": [{"text": "ESE detection", "start_pos": 17, "end_pos": 30, "type": "TASK", "confidence": 0.7545068562030792}]}, {"text": "In the rest of the paper we discuss related work (Section 2) and describe the architecture and training methods for recurrent neural networks (RNNs), bidirectional RNNs, and deep (bidirectional) RNNs (Section 3).", "labels": [], "entities": []}, {"text": "We present experiments using a standard corpus for fine-grained opinion extraction in Section 4.", "labels": [], "entities": [{"text": "opinion extraction", "start_pos": 64, "end_pos": 82, "type": "TASK", "confidence": 0.712045893073082}]}], "datasetContent": [{"text": "We employ the standard softmax activation for the output layer: g(x) = e xi / j e xj . For the hidden layers we use the rectifier linear activation: f (x) = max{0, x}.", "labels": [], "entities": []}, {"text": "Experimentally, rectifier activation gives better performance, faster convergence, and sparse representations.", "labels": [], "entities": []}, {"text": "Previous work also reported good results when training deep neural networks using rectifiers, without a pretraining step (, and Proportional Overlap imparts a partial correctness, proportional to the overlapping amount, to each match (.", "labels": [], "entities": []}, {"text": "All statistical comparisons are done using a two-sided paired t-test with a confidence level of \u03b1 = .05.", "labels": [], "entities": []}, {"text": "Baselines (CRF and SEMICRF).", "labels": [], "entities": []}, {"text": "As baselines, we use the CRF-based method of and the SEMICRF-based method of, which is the state-of-the-art in opinion expression extraction.", "labels": [], "entities": [{"text": "SEMICRF-based", "start_pos": 53, "end_pos": 66, "type": "METRIC", "confidence": 0.9173316359519958}, {"text": "opinion expression extraction", "start_pos": 111, "end_pos": 140, "type": "TASK", "confidence": 0.8235530455907186}]}, {"text": "Features that the baselines use are words, part-of-speech tags and membership in a manually constructed opinion lexicon (within a [-1, +1] context window).", "labels": [], "entities": []}, {"text": "Since SEMICRF relaxes the Markovian assumption and operates at the segment-level instead of the token-level, it also has access to parse trees of sentences to generate candidate segments.", "labels": [], "entities": []}, {"text": "Word Vectors (+VEC).", "labels": [], "entities": []}, {"text": "We also include versions of the baselines that have access to pre-trained word vectors.", "labels": [], "entities": []}, {"text": "In particular, CRF+VEC employs word vectors as continuous features per every token.", "labels": [], "entities": []}, {"text": "Since SEMI-CRF has phrase-level rather than word-level features, we simply take the mean of every word vector fora phrase-level vector representation for SEMICRF+VEC as suggested in.", "labels": [], "entities": []}, {"text": "In all of our experiments, we keep the word vectors fixed (i.e. do not finetune) to reduce the degree of freedom of our models.", "labels": [], "entities": []}, {"text": "We use the publicly available 300-dimensional word vectors of, trained on part of the Google News dataset (\u223c100B words).", "labels": [], "entities": [{"text": "Google News dataset", "start_pos": 86, "end_pos": 105, "type": "DATASET", "confidence": 0.9307988286018372}]}, {"text": "Preliminary experiments with other word vector representations such as Collobert-Weston (2008) embeddings or HLBL provided poorer results (\u223c \u22123% difference in proportional and binary F1).", "labels": [], "entities": [{"text": "F1", "start_pos": 183, "end_pos": 185, "type": "METRIC", "confidence": 0.7771929502487183}]}, {"text": "We do not employ any regularization for smaller networks (\u223c24,000 parameters) because we have not observed strong overfitting (i.e. the difference between training and test performance is small).", "labels": [], "entities": []}, {"text": "Larger networks are regularized with the recently proposed dropout technique ( : we randomly set entries of hidden representations to 0 with a probability called the dropout rate, which is tuned over the development set.", "labels": [], "entities": []}, {"text": "Dropout prevents learned  Network Training.", "labels": [], "entities": []}, {"text": "We use the standard multiclass cross-entropy as the objective function when training the neural networks.", "labels": [], "entities": []}, {"text": "We use stochastic gradient descent with momentum with a fixed learning rate (.005) and a fixed momentum rate (.7).", "labels": [], "entities": []}, {"text": "We update weights after minibatches of 80 sentences.", "labels": [], "entities": []}, {"text": "We run 200 epochs for training.", "labels": [], "entities": []}, {"text": "Weights are initialized from small random uniform noise.", "labels": [], "entities": []}, {"text": "We experiment with networks of various sizes, however we have the same number of hidden units across multiple forward and backward hidden layers of a single RNN.", "labels": [], "entities": []}, {"text": "We do not employ a pre-training step; deep architectures are trained with the supervised error signal, even though the output layer is connected to only the final hidden layer.", "labels": [], "entities": []}, {"text": "With these configurations, every architecture successfully converges without any oscillatory behavior.", "labels": [], "entities": []}, {"text": "Additionally, we employ early stopping for the neural networks: out of all iterations, the model with the best development set performance (Proportional F1) is selected as the final model to be evaluated.", "labels": [], "entities": [{"text": "Proportional F1)", "start_pos": 140, "end_pos": 156, "type": "METRIC", "confidence": 0.820823609828949}]}], "tableCaptions": [{"text": " Table 2: Experimental evaluation of RNNs for DSE extraction", "labels": [], "entities": [{"text": "DSE", "start_pos": 46, "end_pos": 49, "type": "TASK", "confidence": 0.9352591633796692}]}, {"text": " Table 3: Experimental evaluation of RNNs for ESE extraction", "labels": [], "entities": [{"text": "ESE", "start_pos": 46, "end_pos": 49, "type": "TASK", "confidence": 0.9229089021682739}]}, {"text": " Table 4: Comparison of Deep RNNs to state-of-the-art (semi)CRF baselines for DSE and ESE detection", "labels": [], "entities": [{"text": "ESE detection", "start_pos": 86, "end_pos": 99, "type": "TASK", "confidence": 0.8161876201629639}]}]}