{"title": [{"text": "Detecting Disagreement in Conversations using Pseudo-Monologic Rhetorical Structure", "labels": [], "entities": [{"text": "Detecting Disagreement in Conversations", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.9223943501710892}]}], "abstractContent": [{"text": "Casual online forums such as Reddit, Slashdot and Digg, are continuing to increase in popularity as a means of communication.", "labels": [], "entities": [{"text": "Digg", "start_pos": 50, "end_pos": 54, "type": "DATASET", "confidence": 0.9030337929725647}]}, {"text": "Detecting disagreement in this domain is a considerable challenge.", "labels": [], "entities": [{"text": "Detecting disagreement", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8938935101032257}]}, {"text": "Many topics are unique to the conversation on the forum, and the appearance of disagreement maybe much more subtle than on political blogs or social media sites such as twitter.", "labels": [], "entities": []}, {"text": "In this analysis we present a crowd-sourced annotated corpus for topic level disagreement detection in Slashdot, showing that disagreement detection in this domain is difficult even for humans.", "labels": [], "entities": [{"text": "topic level disagreement detection", "start_pos": 65, "end_pos": 99, "type": "TASK", "confidence": 0.6796494871377945}, {"text": "disagreement detection", "start_pos": 126, "end_pos": 148, "type": "TASK", "confidence": 0.7442314028739929}]}, {"text": "We then proceed to show that anew set of features determined from the rhetorical structure of the conversation significantly improves the performance on disagreement detection over a baseline consisting of unigram/bigram features, discourse markers, structural features and meta-post features.", "labels": [], "entities": [{"text": "disagreement detection", "start_pos": 153, "end_pos": 175, "type": "TASK", "confidence": 0.7409028857946396}]}], "introductionContent": [{"text": "How does disagreement arise in conversation?", "labels": [], "entities": []}, {"text": "Being able to detect agreement and disagreement has a range of applications.", "labels": [], "entities": []}, {"text": "For an online educator, dissent over a newly introduced topic may alert the teacher to fundamental misconceptions about the material.", "labels": [], "entities": []}, {"text": "For a business, understanding disputes over features of a product maybe helpful in future design iterations.", "labels": [], "entities": []}, {"text": "By better understanding how debate arises and propagates in a conversation, we may also gain insight into how authors' opinions on a topic can be influenced overtime.", "labels": [], "entities": []}, {"text": "The long term goal of our research is to lay the foundations for understanding argumentative structure in conversations, which could be applied to NLP tasks such as summarization, information retrieval, and text visualization.", "labels": [], "entities": [{"text": "summarization", "start_pos": 165, "end_pos": 178, "type": "TASK", "confidence": 0.9782916307449341}, {"text": "information retrieval", "start_pos": 180, "end_pos": 201, "type": "TASK", "confidence": 0.7496239244937897}]}, {"text": "Argumentative structure theory has been thoroughly studied in both psychology and rhetoric, with negation and discourse markers, as well as hedging and dispreferred responses, being known to be indicative of argument.", "labels": [], "entities": [{"text": "Argumentative structure theory", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.9093887209892273}]}, {"text": "As a starting point, in this paper we focus on the detection of disagreement in casual conversations between users.", "labels": [], "entities": [{"text": "detection of disagreement in casual conversations between users", "start_pos": 51, "end_pos": 114, "type": "TASK", "confidence": 0.8481930419802666}]}, {"text": "This requires a generalized approach that can accurately identify disagreement in topics ranging from something as mundane as whether GPS stands for galactic positioning system or global positioning system, to more ideological debates about distrust in science.", "labels": [], "entities": []}, {"text": "Motivated by the widespread consensus in both computational and theoretical linguistics on the utility of discourse markers for signalling pragmatic functions such as disagreement and personal opinions), we introduce anew set of features based on the Discourse Tree (DT) of a conversational text.", "labels": [], "entities": []}, {"text": "Discourse Trees were formalized by as part of their Rhetorical Structure Theory (RST) to represent the structure of discourse.", "labels": [], "entities": [{"text": "Rhetorical Structure Theory (RST)", "start_pos": 52, "end_pos": 85, "type": "TASK", "confidence": 0.6880788405736288}]}, {"text": "Although this theory is for monologic discourse, we propose to treat conversational dialogue as a collection of linked monologues, and subsequently build a relation graph describing both rhetorical connections within user posts, as well as between different users.", "labels": [], "entities": []}, {"text": "Features obtained from this graph offer significant improvements on disagreement detection over a baseline consisting of meta-post features, lexical features, discourse markers and conversational features.", "labels": [], "entities": [{"text": "disagreement detection", "start_pos": 68, "end_pos": 90, "type": "TASK", "confidence": 0.7663437128067017}]}, {"text": "Not only do these features improve disagreement detection, but the discovered importance of relations known to be theoretically relevant to disagreement detection, such as COMPAR-ISON, suggest that this approach maybe capturing the essential aspects of the conversational argumentative structure.", "labels": [], "entities": [{"text": "disagreement detection", "start_pos": 35, "end_pos": 57, "type": "TASK", "confidence": 0.8647040128707886}, {"text": "disagreement detection", "start_pos": 140, "end_pos": 162, "type": "TASK", "confidence": 0.7869874536991119}]}, {"text": "As a second contribution of this work, we provide anew dataset consisting of 95 topics annotated for disagreement.", "labels": [], "entities": []}, {"text": "Unlike the publicly available ARGUE corpus based on the online debate forum 4forums.com), our corpus is based on Slashdot, which is a general purpose forum not targeted to debates.", "labels": [], "entities": [{"text": "ARGUE corpus", "start_pos": 30, "end_pos": 42, "type": "DATASET", "confidence": 0.8514947891235352}]}, {"text": "Therefore, we expect that detecting disagreement maybe a more difficult task in our new corpus, ascertain topics (like discussing GPS systems) maybe targeted towards objective information sharing without any participants expressing opinions or stances.", "labels": [], "entities": []}, {"text": "Because of this, our corpus represents an excellent testbed to examine methods for more subtle disagreement detection, as well as the major differences between news-style and argument-style dialogue.", "labels": [], "entities": [{"text": "disagreement detection", "start_pos": 95, "end_pos": 117, "type": "TASK", "confidence": 0.7345724403858185}]}], "datasetContent": [{"text": "Experiments were all performed using the Weka machine learning toolkit.", "labels": [], "entities": [{"text": "Weka machine learning toolkit", "start_pos": 41, "end_pos": 70, "type": "DATASET", "confidence": 0.9012621492147446}]}, {"text": "Two different types of experiments were conducted -one using all annotated topics in a binary classification of containing disagreement or not, and one using only those topics with confidence scores greater than 0.75 (corresponding to the more certain cases).", "labels": [], "entities": []}, {"text": "All results were obtained by performing 10 fold crossvalidation on a balanced test set.", "labels": [], "entities": []}, {"text": "Additionally, infold cross-validation was performed to determine the optimal number of features to use for each feature set.", "labels": [], "entities": []}, {"text": "Since this is done in-fold, a paired t-test is still a valid comparison of different feature sets to determine significant differences in F-score and accuracy.", "labels": [], "entities": [{"text": "F-score", "start_pos": 138, "end_pos": 145, "type": "METRIC", "confidence": 0.9911026954650879}, {"text": "accuracy", "start_pos": 150, "end_pos": 158, "type": "METRIC", "confidence": 0.9953849911689758}]}], "tableCaptions": [{"text": " Table 1: Characteristics of the four categories determined from the crowd-sourced annotation. All values except for the number  of topics in the category are given as the average score per topic across all topics in that category. Key: C and NC: Confident  (score \u2265 0.75) and Not confident (score < 0.75), Num: Number of topics in category, P/A: Posts per author, S/P: Sentences  per post, W/P: Words per post, Num Authors: Number of authors, W/S: Words per sentence, TBP: Time between posts  (minutes), TT: Total time in minutes, TP: Total posts, and Length: Length of topic in sentences", "labels": [], "entities": [{"text": "TBP", "start_pos": 469, "end_pos": 472, "type": "METRIC", "confidence": 0.986268937587738}, {"text": "TT", "start_pos": 505, "end_pos": 507, "type": "METRIC", "confidence": 0.9476589560508728}, {"text": "Length", "start_pos": 553, "end_pos": 559, "type": "METRIC", "confidence": 0.9931880235671997}]}, {"text": " Table 2: Basic: Meta-post, all structural, bias words, discourse markers, referrals, punctuation RhetAll: Structural and sen- timent based rhetorical features All: Basic, all rhetorical, sentiment and FQG features. The N-gram models include unigrams  and bi-grams. All feature sets in the bottom part of the table include rhetorical reatures.", "labels": [], "entities": [{"text": "RhetAll", "start_pos": 98, "end_pos": 105, "type": "METRIC", "confidence": 0.9328481554985046}]}, {"text": " Table 3: Precision, recall, F1, accuracy and ROC-AUC scores for the simpler task of identifying the cases deemed \"high  confidence\" in the crowd-sourcing task.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9990874528884888}, {"text": "recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9994465708732605}, {"text": "F1", "start_pos": 29, "end_pos": 31, "type": "METRIC", "confidence": 0.9996321201324463}, {"text": "accuracy", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.9997701048851013}, {"text": "ROC-AUC", "start_pos": 46, "end_pos": 53, "type": "METRIC", "confidence": 0.9987732768058777}]}, {"text": " Table 5: Joty et al. document-level parser accuracy of the  parser used in this paper. The parser was originally tested  on two corpora: RST-DT and Instructional. HILDA was the  state-of-the-art parser at that time. Span and Nuclearity met- rics assess the quality of the structure of the resulting tree,  while the Relation metric assesses the quality of the relation  labels.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 44, "end_pos": 52, "type": "METRIC", "confidence": 0.993699312210083}, {"text": "Relation metric", "start_pos": 317, "end_pos": 332, "type": "METRIC", "confidence": 0.8702438175678253}]}]}