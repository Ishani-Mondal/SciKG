{"title": [{"text": "Syntactic SMT Using a Discriminative Text Generation Model", "labels": [], "entities": [{"text": "Syntactic SMT", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.8795101046562195}, {"text": "Discriminative Text Generation", "start_pos": 22, "end_pos": 52, "type": "TASK", "confidence": 0.6068018972873688}]}], "abstractContent": [{"text": "We study a novel architecture for syntactic SMT.", "labels": [], "entities": [{"text": "syntactic SMT", "start_pos": 34, "end_pos": 47, "type": "TASK", "confidence": 0.5105894207954407}]}, {"text": "In contrast to the dominant approach in the literature, the system does not rely on translation rules, but treat translation as an unconstrained target sentence generation task, using soft features to capture lexical and syntactic correspondences between the source and target languages.", "labels": [], "entities": []}, {"text": "Target syntax features and bilingual translation features are trained consistently in a discriminative model.", "labels": [], "entities": []}, {"text": "Experiments using the IWSLT 2010 dataset show that the system achieves BLEU comparable to the state-of-the-art syntactic SMT systems.", "labels": [], "entities": [{"text": "IWSLT 2010 dataset", "start_pos": 22, "end_pos": 40, "type": "DATASET", "confidence": 0.9637213548024496}, {"text": "BLEU", "start_pos": 71, "end_pos": 75, "type": "METRIC", "confidence": 0.9997013211250305}, {"text": "SMT", "start_pos": 121, "end_pos": 124, "type": "TASK", "confidence": 0.8339083194732666}]}], "introductionContent": [{"text": "Translation rules have been central to hierarchical phrase-based and syntactic statistical machine translation (SMT) ().", "labels": [], "entities": [{"text": "syntactic statistical machine translation (SMT)", "start_pos": 69, "end_pos": 116, "type": "TASK", "confidence": 0.7706700393131801}]}, {"text": "They are attractive by capturing the recursiveness of languages and syntactic correspondences between them.", "labels": [], "entities": []}, {"text": "One important advantage of translation rules is that they allow efficient decoding by treating MT as a statistical parsing task, transforming a source sentence to its translation via recursive rule application.", "labels": [], "entities": [{"text": "MT", "start_pos": 95, "end_pos": 97, "type": "TASK", "confidence": 0.9828282594680786}, {"text": "statistical parsing task", "start_pos": 103, "end_pos": 127, "type": "TASK", "confidence": 0.693450927734375}]}, {"text": "The efficiency takes root in the fact that target word orders are encoded in translation rules.", "labels": [], "entities": []}, {"text": "This fact, however, also leads to rule explosion, noise and coverage problems (, which can hurt translation quality.", "labels": [], "entities": [{"text": "coverage", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.9375238418579102}]}, {"text": "Flexibility of function word usage, rich morphology and paraphrasing all add to the difficulty of rule extraction.", "labels": [], "entities": [{"text": "rule extraction", "start_pos": 98, "end_pos": 113, "type": "TASK", "confidence": 0.8154408931732178}]}, {"text": "In addition, restricting target word orders by hard translation rules can also hurt output fluency.", "labels": [], "entities": []}, {"text": "* * Work done while visiting Singapore University of Figure 1: Overall system architecture.", "labels": [], "entities": [{"text": "Singapore University of Figure 1", "start_pos": 29, "end_pos": 61, "type": "DATASET", "confidence": 0.9248138308525086}]}, {"text": "A potential solution to the problems above is to treat translation as a generation task, representing syntactic correspondences using soft features.", "labels": [], "entities": [{"text": "translation", "start_pos": 55, "end_pos": 66, "type": "TASK", "confidence": 0.9671090245246887}]}, {"text": "Both adequacy and fluency can potentially be improved by giving full flexibility to target synthesis, and leaving all options to the statistical model.", "labels": [], "entities": []}, {"text": "The main challenge to this method is a significant increase in the search space.", "labels": [], "entities": []}, {"text": "To this end, recent advances in tackling complex search tasks for text generation offer some solutions In this short paper, we present a preliminary investigation on the possibility of building a syntactic SMT system that does not use hard translation rules, by utilizing recent advances in statistical natural language generation (NLG).", "labels": [], "entities": [{"text": "text generation", "start_pos": 66, "end_pos": 81, "type": "TASK", "confidence": 0.7523855566978455}, {"text": "SMT", "start_pos": 206, "end_pos": 209, "type": "TASK", "confidence": 0.7956324219703674}, {"text": "statistical natural language generation (NLG)", "start_pos": 291, "end_pos": 336, "type": "TASK", "confidence": 0.733473824603217}]}, {"text": "The overall architecture is shown in.", "labels": [], "entities": []}, {"text": "Translation is performed by first parsing the source sentence, then transferring source words and phrases to their target equivalences, and finally synthesizing the target output.", "labels": [], "entities": [{"text": "Translation", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.9568836092948914}]}, {"text": "We choose dependency grammar for both the source and the target syntax, and adapt the syntactic text synthesis system of Zhang (2013), which performs dependency-based linearization.", "labels": [], "entities": []}, {"text": "The linearization task for MT is different from the monolingual task in that not all translation options are used to build the output, and that bilingual correspondences need to betaken into account dur-ing synthesis.", "labels": [], "entities": [{"text": "MT", "start_pos": 27, "end_pos": 29, "type": "TASK", "confidence": 0.9897790551185608}]}, {"text": "The algorithms of are modified to perform word selection as well as ordering, using two sets of features to control translation adequacy and fluency, respectively.", "labels": [], "entities": [{"text": "word selection", "start_pos": 42, "end_pos": 56, "type": "TASK", "confidence": 0.776088297367096}]}, {"text": "Preliminary experiments on the IWSLT 1 2010 data show that the system gives BLEU comparable to traditional tree-to-string and string-to-tree translation systems.", "labels": [], "entities": [{"text": "IWSLT 1 2010 data", "start_pos": 31, "end_pos": 48, "type": "DATASET", "confidence": 0.9577494263648987}, {"text": "BLEU", "start_pos": 76, "end_pos": 80, "type": "METRIC", "confidence": 0.9981776475906372}]}, {"text": "It demonstrates the feasibility of leveraging statistical NLG techniques for SMT, and the possibility of building a statistical transferbased MT system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 77, "end_pos": 80, "type": "TASK", "confidence": 0.9944460391998291}, {"text": "MT", "start_pos": 142, "end_pos": 144, "type": "TASK", "confidence": 0.813234269618988}]}], "datasetContent": [{"text": "We perform experiments on the IWSLT 2010 Chinese-English dataset, which consists of training sentence pairs from the dialog task (dialog) and Basic Travel and Expression Corpus (BTEC).", "labels": [], "entities": [{"text": "IWSLT 2010 Chinese-English dataset", "start_pos": 30, "end_pos": 64, "type": "DATASET", "confidence": 0.941138967871666}, {"text": "Basic Travel and Expression Corpus (BTEC)", "start_pos": 142, "end_pos": 183, "type": "DATASET", "confidence": 0.496714536100626}]}, {"text": "The union of dialog and BTEC are taken as our training set, which contains 30,033 sentence pairs.", "labels": [], "entities": [{"text": "BTEC", "start_pos": 24, "end_pos": 28, "type": "DATASET", "confidence": 0.8591281771659851}]}, {"text": "For system tuning, we use the IWSLT 2004 test set (also released as the second development test set of IWSLT 2010), which contains 500 sentences.", "labels": [], "entities": [{"text": "system tuning", "start_pos": 4, "end_pos": 17, "type": "TASK", "confidence": 0.794179379940033}, {"text": "IWSLT 2004 test set", "start_pos": 30, "end_pos": 49, "type": "DATASET", "confidence": 0.9786364883184433}, {"text": "IWSLT 2010)", "start_pos": 103, "end_pos": 114, "type": "DATASET", "confidence": 0.8736741344134012}]}, {"text": "For final test, we use the IWSLT 2003 test set (also released as the first development test set of IWSLT 2010), which contains 506 sentences.", "labels": [], "entities": [{"text": "IWSLT 2003 test set", "start_pos": 27, "end_pos": 46, "type": "DATASET", "confidence": 0.9694715887308121}, {"text": "IWSLT 2010)", "start_pos": 99, "end_pos": 110, "type": "DATASET", "confidence": 0.878443976243337}]}, {"text": "The Chinese sentences in the datasets are segmented using NiuTrans), while POS-tagging of both English and Chinese is performed using ZPar 4 version 0.5 ().", "labels": [], "entities": [{"text": "NiuTrans", "start_pos": 58, "end_pos": 66, "type": "DATASET", "confidence": 0.9426724314689636}]}, {"text": "We train the English POS-tagger using the WSJ sections of the Penn Treebank (, turned into lower-case.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 42, "end_pos": 45, "type": "DATASET", "confidence": 0.9516525864601135}, {"text": "Penn Treebank", "start_pos": 62, "end_pos": 75, "type": "DATASET", "confidence": 0.9443224370479584}]}, {"text": "For syntactic parsing of both English and Chinese, we use the default models of ZPar 0.5.", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.7535683214664459}, {"text": "ZPar 0.5", "start_pos": 80, "end_pos": 88, "type": "DATASET", "confidence": 0.8967138230800629}]}, {"text": "We choose three baseline systems: a string-totree (S2T) system, a tree-to-string (T2S) system and a tree-to-tree (T2T) system.", "labels": [], "entities": []}, {"text": "The Moses release 1.0 implementations of all three systems are used, with default parameter settings.", "labels": [], "entities": []}, {"text": "IRSTLM 5 release 5.80.03    over the English training data, which is applied to the baseline systems and our system.", "labels": [], "entities": [{"text": "IRSTLM 5", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9677094221115112}, {"text": "English training data", "start_pos": 37, "end_pos": 58, "type": "DATASET", "confidence": 0.6776522497336069}]}, {"text": "Kneser-Ney smoothing is used to train the language model.", "labels": [], "entities": []}, {"text": "We use the tuning set to determine the optimal number of training iterations.", "labels": [], "entities": []}, {"text": "The translation option filter \u03bb is set to 0.1; the phrase size limit sis set to 5 in order to verify the effectiveness of synthesis; the number of expanded nodes L is set to 200; the chart factor k is set to 16 fora balance between efficiency and accuracy; the goal parameter \u03b2 is set to 0.8.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 247, "end_pos": 255, "type": "METRIC", "confidence": 0.9982202649116516}]}, {"text": "The final scores of our system and the baselines are shown in.", "labels": [], "entities": []}, {"text": "Our system gives a BLEU of 34.24, which is comparable to the baseline systems.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 19, "end_pos": 23, "type": "METRIC", "confidence": 0.999750554561615}]}, {"text": "Some example outputs are shown in.", "labels": [], "entities": []}, {"text": "Manual comparison does not show significant differences in overall translation adequacy or fluency between the outputs of the four systems.", "labels": [], "entities": []}, {"text": "However, an observation is that, while our system can produce more fluent outputs, the choice of translation options can be more frequently incorrect.", "labels": [], "entities": []}, {"text": "This suggests that while the target synthesis component is effective under the bilingual setting, a stronger lexical selection component maybe necessary for better translation quality.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: Sample output sentences.", "labels": [], "entities": []}]}