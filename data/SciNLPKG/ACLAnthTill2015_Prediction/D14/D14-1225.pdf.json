{"title": [{"text": "Prune-and-Score: Learning for Greedy Coreference Resolution", "labels": [], "entities": [{"text": "Coreference Resolution", "start_pos": 37, "end_pos": 59, "type": "TASK", "confidence": 0.8006856739521027}]}], "abstractContent": [{"text": "We propose a novel search-based approach for greedy coreference resolution, where the mentions are processed in order and added to previous coreference clusters.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 52, "end_pos": 74, "type": "TASK", "confidence": 0.8901309967041016}]}, {"text": "Our method is distinguished by the use of two functions to make each corefer-ence decision: a pruning function that prunes bad coreference decisions from further consideration, and a scoring function that then selects the best among the remaining decisions.", "labels": [], "entities": []}, {"text": "Our framework reduces learning of these functions to rank learning, which helps leverage powerful off-the-shelf rank-learners.", "labels": [], "entities": []}, {"text": "We show that our Prune-and-Score approach is superior to using a single scoring function to make both decisions and outperforms several state-of-the-art approaches on multiple benchmark corpora including OntoNotes.", "labels": [], "entities": [{"text": "Prune-and-Score", "start_pos": 17, "end_pos": 32, "type": "METRIC", "confidence": 0.9047986268997192}, {"text": "OntoNotes", "start_pos": 204, "end_pos": 213, "type": "DATASET", "confidence": 0.8938243985176086}]}], "introductionContent": [{"text": "Coreference resolution is the task of clustering a set of mentions in the text such that all mentions in the same cluster refer to the same entity.", "labels": [], "entities": [{"text": "Coreference resolution", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9318108260631561}]}, {"text": "It is one of the first stages in deep language understanding and has a big potential impact on the rest of the stages.", "labels": [], "entities": [{"text": "deep language understanding", "start_pos": 33, "end_pos": 60, "type": "TASK", "confidence": 0.6680112381776174}]}, {"text": "Several of the state-of-the-art approaches learn a scoring function defined over mention pair, cluster-mention or cluster-cluster pair to guide the coreference decision-making process;.", "labels": [], "entities": []}, {"text": "One common and persistent problem with these approaches is that the scoring function has to make all the coreference decisions, which leads to a highly non-realizable learning problem.", "labels": [], "entities": []}, {"text": "Inspired by the recent success of the HC-Search Framework () for studying a variety of structured prediction problems (), we study a novel approach for search-based coreference resolution called Prune-and-Score.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 165, "end_pos": 187, "type": "TASK", "confidence": 0.812832772731781}]}, {"text": "HC-Search is a divideand-conquer solution that learns multiple components with pre-defined roles, and each of them contribute towards the overall goal by making the role of the other components easier.", "labels": [], "entities": [{"text": "HC-Search", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.898298442363739}]}, {"text": "The HCSearch framework operates in the space of complete outputs, and relies on the loss function which is only defined on the complete outputs to drive its learning.", "labels": [], "entities": []}, {"text": "Unfortunately, this method does notwork for incremental coreference resolution since the search space for coreference resolution consists of partial outputs, i.e., a set of mentions only some of which have been clustered so far.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 56, "end_pos": 78, "type": "TASK", "confidence": 0.9094006419181824}, {"text": "coreference resolution", "start_pos": 106, "end_pos": 128, "type": "TASK", "confidence": 0.9017660915851593}]}, {"text": "We develop an alternative framework to HCSearch that allows us to effectively learn from partial output spaces and apply it to greedy coreference resolution.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 134, "end_pos": 156, "type": "TASK", "confidence": 0.8292403817176819}]}, {"text": "The key idea of our work is to address the problem of non-realizability of the scoring function by learning two different functions: 1) a pruning function to prune most of the bad decisions, and 2) a scoring function to pick the best decision among those that are remaining.", "labels": [], "entities": []}, {"text": "Our Prune-and-Score approach is a particular instantiation of the general idea of learning nearly-sound constraints for pruning, and leveraging the learned constraints to learn improved heuristic functions for guiding the search.", "labels": [], "entities": []}, {"text": "The pruning constraints can take different forms (e.g., classifiers, decisionlist, or ranking functions) depending on the search architecture.", "labels": [], "entities": []}, {"text": "Therefore, other coreference resolution systems () can also benefit from this idea.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 17, "end_pos": 39, "type": "TASK", "confidence": 0.9250223338603973}]}, {"text": "While our basic idea of twolevel selection might appear similar to the coarseto-fine inference architectures, the details differ significantly.", "labels": [], "entities": []}, {"text": "Importantly, our pruning and scoring functions operate sequentially at each greedy search step, whereas in the cascades approach, the second level function makes its prediction only when the first level decision-making is done.", "labels": [], "entities": []}, {"text": "The main contributions of our work are as follows.", "labels": [], "entities": []}, {"text": "First, we motivate and introduce the Prune-and-Score approach to search-based coreference resolution.", "labels": [], "entities": [{"text": "Prune-and-Score", "start_pos": 37, "end_pos": 52, "type": "METRIC", "confidence": 0.9624373912811279}, {"text": "coreference resolution", "start_pos": 78, "end_pos": 100, "type": "TASK", "confidence": 0.9017871618270874}]}, {"text": "Second, we identify a decomposition of the overall loss of the Prune-and-Score approach into the pruning loss and the scoring loss, and reduce the problem of learning these two functions to rank learning, which allows us to leverage powerful and efficient off-the-shelf rank learners.", "labels": [], "entities": []}, {"text": "Third, we evaluate our approach on OntoNotes, ACE, and MUC data, and show that it compares favorably to several state-of-the-art approaches as well as a greedy search-based approach that uses a single scoring function.", "labels": [], "entities": [{"text": "MUC data", "start_pos": 55, "end_pos": 63, "type": "DATASET", "confidence": 0.8326035141944885}]}, {"text": "The remainder of the paper proceeds as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we dicuss the related work.", "labels": [], "entities": []}, {"text": "We introduce our problem setup in Section 3 and then describe our Prune-and-Score approach in Section 4.", "labels": [], "entities": [{"text": "Prune-and-Score", "start_pos": 66, "end_pos": 81, "type": "METRIC", "confidence": 0.9879620671272278}]}, {"text": "We explain our approaches for learning the pruning and scoring functions in Section 5.", "labels": [], "entities": []}, {"text": "Section 6 presents our experimental results followed by the conclusions in Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we evaluate our greedy Pruneand-Score approach on three benchmark corpora -OntoNotes 5.0 (Pradhan et al., 2012), ACE 2004, and MUC6 (MUC6, 1995) -and compare it against the state-of-the-art approaches for coreference resolution.", "labels": [], "entities": [{"text": "ACE 2004", "start_pos": 130, "end_pos": 138, "type": "DATASET", "confidence": 0.8802728950977325}, {"text": "MUC6 (MUC6, 1995)", "start_pos": 144, "end_pos": 161, "type": "DATASET", "confidence": 0.8292748431364695}, {"text": "coreference resolution", "start_pos": 222, "end_pos": 244, "type": "TASK", "confidence": 0.9731210470199585}]}, {"text": "For OntoNotes data, we report the results on both gold mentions and predicted mentions.", "labels": [], "entities": [{"text": "OntoNotes data", "start_pos": 4, "end_pos": 18, "type": "DATASET", "confidence": 0.8203871846199036}]}, {"text": "We also report the results on gold mentions for ACE 2004 and MUC6 data.", "labels": [], "entities": [{"text": "ACE 2004", "start_pos": 48, "end_pos": 56, "type": "DATASET", "confidence": 0.9298567175865173}, {"text": "MUC6 data", "start_pos": 61, "end_pos": 70, "type": "DATASET", "confidence": 0.8217121660709381}]}, {"text": "For OntoNotes corpus, we employ the official split for training, validation, and testing.", "labels": [], "entities": [{"text": "OntoNotes corpus", "start_pos": 4, "end_pos": 20, "type": "DATASET", "confidence": 0.7782720029354095}]}, {"text": "There are 2802 documents in the training set; 343 documents in the validation set; and 345 documents in the testing set.", "labels": [], "entities": []}, {"text": "The ACE 2004 corpus contains 443 documents.", "labels": [], "entities": [{"text": "ACE 2004 corpus", "start_pos": 4, "end_pos": 19, "type": "DATASET", "confidence": 0.9826265970865885}]}, {"text": "We follow the) split in our experiments by employing 268 documents for training, 68 documents for validation, and 107 documents (ACE2004-CULOTTA-TEST) for testing.", "labels": [], "entities": [{"text": "ACE2004-CULOTTA-TEST", "start_pos": 129, "end_pos": 149, "type": "DATASET", "confidence": 0.7823015451431274}]}, {"text": "We also evaluate our system on the 128 newswire documents in ACE 2004 corpus fora fair comparison with the state-of-the-art.", "labels": [], "entities": [{"text": "newswire documents in ACE 2004 corpus", "start_pos": 39, "end_pos": 76, "type": "DATASET", "confidence": 0.7333648502826691}]}, {"text": "The MUC6 corpus containts 255 documents.", "labels": [], "entities": [{"text": "MUC6 corpus containts 255 documents", "start_pos": 4, "end_pos": 39, "type": "DATASET", "confidence": 0.9615204334259033}]}, {"text": "We employ the official test set of 30 documents (MUC6-TEST) for testing purposes.", "labels": [], "entities": [{"text": "MUC6-TEST", "start_pos": 49, "end_pos": 58, "type": "DATASET", "confidence": 0.6403860449790955}]}, {"text": "From the remaining 225 documents, which includes 195 official training documents and 30 dry-run test documents, we randomly pick 30 documents for validation, and use the remaining ones for training.", "labels": [], "entities": []}, {"text": "We compute three most popular performance metrics for coreference resolution: MUC (, B-Cubed (, and Entity-based CEAF (CEAF \u03c64 ) (.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 54, "end_pos": 76, "type": "TASK", "confidence": 0.975064754486084}]}, {"text": "As it is commonly done in CoNLL shared tasks), we employ the average F1 score (CoNLL F1) of these three metrics for comparison purposes.", "labels": [], "entities": [{"text": "F1 score (CoNLL F1)", "start_pos": 69, "end_pos": 88, "type": "METRIC", "confidence": 0.8819852670033773}]}, {"text": "We evaluate all the results using the updated version 1 (7.0) of the coreference scorer.", "labels": [], "entities": []}, {"text": "We built 2 our coreference resolver based on the Easy-first coreference system, which is derived from the Reconcile system (.", "labels": [], "entities": [{"text": "coreference resolver", "start_pos": 15, "end_pos": 35, "type": "TASK", "confidence": 0.8927549123764038}]}, {"text": "We essentially employ the same features as in the Easyfirst system.", "labels": [], "entities": []}, {"text": "However, we provide some highlevel details that are necessary for subsequent discussion.", "labels": [], "entities": []}, {"text": "Recall that our features \u03c6(s, a) for both pruning and scoring functions are defined over state-action pairs, where each state s consists of a set of clusters and an action a corresponds to merging an unprocessed mention m with a cluster C instate s or create one for itself.", "labels": [], "entities": []}, {"text": "Therefore, \u03c6(s, a) defines features over cluster-mention pairs (C, m).", "labels": [], "entities": []}, {"text": "Our feature vector consists of three parts: a) mention pair features; b) entity pair features; and c) a single indicator feature to represent NEW action (i.e., mention m starts its own cluster).", "labels": [], "entities": []}, {"text": "For mention pair features, we average the pair-wise features overall links between m and every mention m c in cluster C (often referred to as averagelink).", "labels": [], "entities": []}, {"text": "Note that, we cannot employ the best-link feature representation because we perform offline training and do not have weights for scoring the links.", "labels": [], "entities": []}, {"text": "For entity pair features, we treat mention m as a singleton entity and compute features by pairing it with the entity represented by cluster C (exactly as in the Easy-first system).", "labels": [], "entities": []}, {"text": "The indicator feature will be 1 for the NEW action and 0 for all other actions.We have a total of 140 features: 90 mention pair features; 49 entity pair features; and one NEW indicator feature.", "labels": [], "entities": []}, {"text": "We believe that our approach can benefit from employing features of the mention for the NEW action.", "labels": [], "entities": [{"text": "NEW action", "start_pos": 88, "end_pos": 98, "type": "DATASET", "confidence": 0.9044724106788635}]}, {"text": "However, we were constrained by the Reconcile system and could not leverage these features for the NEW action.", "labels": [], "entities": []}, {"text": "Our pruning and scoring function learning algorithms need abase ranklearner.", "labels": [], "entities": []}, {"text": "We employ LambdaMART, a state-of-the art rank learner from the RankLib 3 library.", "labels": [], "entities": [{"text": "RankLib 3 library", "start_pos": 63, "end_pos": 80, "type": "DATASET", "confidence": 0.9100488622983297}]}, {"text": "LambdaMART is a variant of boosted regression trees.", "labels": [], "entities": []}, {"text": "We use a learning rate of 0.1, specify the maximum number of boosting iterations (or trees) as 1000 noting that its actual value is automatically decided based on the validation set, and tune the number of leaves per tree based on the validation data.", "labels": [], "entities": []}, {"text": "Once we fix the hyper-parameters of LambdaMART, we train the final model on all of the training data.", "labels": [], "entities": []}, {"text": "LambdaMART uses an internal train/validation split of the input ranking examples to decide when to stop the boosting iterations.", "labels": [], "entities": []}, {"text": "We fixed this ratio to 0.8 noting that the performance is not sensitive to this parameter.", "labels": [], "entities": []}, {"text": "For scoring function learning, we used 5 folds for the cross-validation training.", "labels": [], "entities": [{"text": "scoring function learning", "start_pos": 4, "end_pos": 29, "type": "TASK", "confidence": 0.7896365722020467}]}, {"text": "The hyper-parameter b controls the amount of pruning in our Prune-andScore approach.", "labels": [], "entities": []}, {"text": "We perform experiments with different values of band pick the best value based on the performance on the validation set.", "labels": [], "entities": []}, {"text": "Singleton Mention Filter for OntoNotes Corpus.", "labels": [], "entities": [{"text": "Singleton Mention Filter", "start_pos": 0, "end_pos": 24, "type": "DATASET", "confidence": 0.8754737377166748}, {"text": "OntoNotes Corpus", "start_pos": 29, "end_pos": 45, "type": "DATASET", "confidence": 0.7902122437953949}]}, {"text": "We employ the Illinois-Coref system (Chang et al., 2012) to extract system mentions for our OntoNotes experiments, and observe that the num-ber of predicted mentions is thrice the number of gold mentions.", "labels": [], "entities": []}, {"text": "Since the training data provides the clustering supervision for only gold mentions, it is not clear how to train with the system mentions that are not part of gold mentions.", "labels": [], "entities": []}, {"text": "A common way of dealing with this problem is to treat all the extra system mentions as singleton clusters.", "labels": [], "entities": []}, {"text": "However, this solution most likely will notwork with our current feature representation (i.e., NEW action is represented as a single indicator feature).", "labels": [], "entities": []}, {"text": "Recall that to predict these extra system mentions as singleton clusters with our incremental clustering approach, the learned model should first predict a NEW action while processing these mentions to form a temporary singleton cluster, and then refrain from merging any of the subsequent mentions with that cluster so that it becomes a singleton cluster in the final clustering output.", "labels": [], "entities": []}, {"text": "However, in OntoNotes corpus, the training data does not include singleton clusters for the gold mentions.", "labels": [], "entities": [{"text": "OntoNotes corpus", "start_pos": 12, "end_pos": 28, "type": "DATASET", "confidence": 0.9176554977893829}]}, {"text": "Therefore, only the large number (57%) of system mentions that are not part of gold mentions will constitute the set of singleton clusters.", "labels": [], "entities": []}, {"text": "This leads to a highly imbalanced learning problem because our model needs to learn (the weight of the single indicator feature) to predict NEW as the best action fora large set of mentions, which will bias our model to predict large number of NEW actions during testing.", "labels": [], "entities": []}, {"text": "As a result, we will generate many singleton clusters, which will hurt the recall of the mention detection after post-processing.", "labels": [], "entities": [{"text": "recall", "start_pos": 75, "end_pos": 81, "type": "METRIC", "confidence": 0.9985033273696899}, {"text": "mention detection", "start_pos": 89, "end_pos": 106, "type": "TASK", "confidence": 0.7269826382398605}]}, {"text": "Therefore, we aim to learn a singleton mention filter that will be used as a pre-processor before training and testing to overcome this problem.", "labels": [], "entities": []}, {"text": "We would like to point out that our filter is complementary to other solutions (e.g., employing features that can discriminate a given mention to be anaphoric or not in place of our single indicator feature, or using a customized loss to weight our ranking examples for cost-sensitive training) . Filter Learning.", "labels": [], "entities": []}, {"text": "The singleton mention filter is a classifier that will label a given mention as \"singleton\" or not.", "labels": [], "entities": []}, {"text": "We represent each mention min a document by averaging the mention-pair features \u03c6(m, m ) of the k-most similar mentions (obtained by ranking all other mentions min the document with a learned ranking function R given m) and then learn a decision-tree classifier by optimizing the F1 loss.", "labels": [], "entities": [{"text": "F1", "start_pos": 280, "end_pos": 282, "type": "METRIC", "confidence": 0.9976724982261658}]}, {"text": "We learn the mention-ranking function R by optimizing the recall of positive pairs fora given k, and employ LambdaMART as our base ranker.", "labels": [], "entities": [{"text": "recall", "start_pos": 58, "end_pos": 64, "type": "METRIC", "confidence": 0.9971739053726196}]}, {"text": "The hyper-parameters are tuned based on the performance on the validation set.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Performance of the singleton mention filter on  the OntoNotes 5.0 development set. The numerators of  the fractions in the brackets show the exact numbers of  mentions that are matched with the gold mentions.", "labels": [], "entities": [{"text": "OntoNotes 5.0 development set", "start_pos": 62, "end_pos": 91, "type": "DATASET", "confidence": 0.9623025953769684}]}, {"text": " Table 2: Performance of Prune-and-Score approach  with and without the singleton mention filter, and Only- Score approach without the filter.", "labels": [], "entities": [{"text": "Prune-and-Score", "start_pos": 25, "end_pos": 40, "type": "METRIC", "confidence": 0.9393579959869385}]}, {"text": " Table 4: Comparison of Prune-and-Score with state-of-the-art approaches. Metric values reflect version 7 of  CoNLL scorer.", "labels": [], "entities": [{"text": "Prune-and-Score", "start_pos": 24, "end_pos": 39, "type": "METRIC", "confidence": 0.9727447032928467}, {"text": "Metric", "start_pos": 74, "end_pos": 80, "type": "METRIC", "confidence": 0.965116560459137}, {"text": "CoNLL scorer", "start_pos": 110, "end_pos": 122, "type": "DATASET", "confidence": 0.7224032580852509}]}, {"text": " Table 3: Performance of Prune-and-Score approach  with different values of the pruning parameter b. For  b = \u221e, Prune-and-Score becomes an Only-Scoring al- gorithm.", "labels": [], "entities": []}]}