{"title": [{"text": "Modeling Biological Processes for Reading Comprehension", "labels": [], "entities": [{"text": "Reading Comprehension", "start_pos": 34, "end_pos": 55, "type": "TASK", "confidence": 0.7717058658599854}]}], "abstractContent": [{"text": "Machine reading calls for programs that read and understand text, but most current work only attempts to extract facts from redundant web-scale corpora.", "labels": [], "entities": [{"text": "Machine reading", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.794748842716217}]}, {"text": "In this paper , we focus on anew reading comprehension task that requires complex reasoning over a single document.", "labels": [], "entities": []}, {"text": "The input is a paragraph describing a biological process , and the goal is to answer questions that require an understanding of the relations between entities and events in the process.", "labels": [], "entities": []}, {"text": "To answer the questions, we first predict a rich structure representing the process in the paragraph.", "labels": [], "entities": []}, {"text": "Then, we map the question to a formal query, which is executed against the predicted structure.", "labels": [], "entities": []}, {"text": "We demonstrate that answering questions via predicted structures substantially improves accuracy over baselines that use shallower representations.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 88, "end_pos": 96, "type": "METRIC", "confidence": 0.9970699548721313}]}], "introductionContent": [{"text": "The goal of machine reading is to develop programs that read text to learn about the world and make decisions based on accumulated knowledge.", "labels": [], "entities": [{"text": "machine reading", "start_pos": 12, "end_pos": 27, "type": "TASK", "confidence": 0.7801488935947418}]}, {"text": "Work in this field has focused mostly on macro-reading, i.e., processing large text collections and extracting knowledge bases of facts).", "labels": [], "entities": []}, {"text": "Such methods rely on redundancy, and are thus suitable for answering common factoid questions which have ample evidence in text.", "labels": [], "entities": []}, {"text": "However, reading a single document (micro-reading) to answer comprehension questions that require deep reasoning is currently beyond the scope of state-of-the-art systems.", "labels": [], "entities": []}, {"text": "In this paper, we introduce a task where given a paragraph describing a process, the goal is to * Both authors equally contributed to the paper.", "labels": [], "entities": []}, {"text": "answer reading comprehension questions that test understanding of the underlying structure.", "labels": [], "entities": []}, {"text": "In particular, we consider processes in biology textbooks such as this excerpt and the question that follows: This excerpt describes a process in which a complex set of events and entities are related to one another.", "labels": [], "entities": []}, {"text": "A system trying to answer this question must extract a rich structure spanning multiple sentences and reason that water splitting combined with light absorption leads to transfer of ions.", "labels": [], "entities": []}, {"text": "Note that shallow methods, which rely on lexical overlap or text proximity, will fail.", "labels": [], "entities": []}, {"text": "Indeed, both answers are covered by the paragraph and the wrong answer is closer in the text to the question.", "labels": [], "entities": []}, {"text": "We propose a novel method that tackles this challenging problem (see).", "labels": [], "entities": []}, {"text": "First, we train a supervised structure predictor that learns to extract entities, events and their relations describing the biological process.", "labels": [], "entities": []}, {"text": "This is a difficult problem because events have complex interactions that span multiple sentences.", "labels": [], "entities": []}, {"text": "Then, treating this structure as a small knowledge-base, we map questions to formal queries that are executed against the structure to provide the answer.", "labels": [], "entities": []}, {"text": "Micro-reading is an important aspect of natural language understanding (.", "labels": [], "entities": [{"text": "natural language understanding", "start_pos": 40, "end_pos": 70, "type": "TASK", "confidence": 0.6565258304278055}]}, {"text": "In this work, we focus specifically on modeling processes, where events and entities relate to one another through complex interactions.", "labels": [], "entities": []}, {"text": "While we work in the biology \".", "labels": [], "entities": []}, {"text": "Water is split, providing a source of electrons and protons (hydrogen ions, H + ) and giving off O2 as a by-product.", "labels": [], "entities": []}, {"text": "Light absorbed by chlorophyll drives a transfer of the electrons and hydrogen ions from water to an acceptor called NADP+ . .", "labels": [], "entities": []}, {"text": "\" Q What can the splitting of water lead to?", "labels": [], "entities": [{"text": "Q", "start_pos": 2, "end_pos": 3, "type": "METRIC", "confidence": 0.9613325595855713}]}, {"text": "a Light absorption Step 1 Step 2 Step 3: Answer = b Figure 1: An overview of our reading comprehension system.", "labels": [], "entities": [{"text": "Answer", "start_pos": 41, "end_pos": 47, "type": "METRIC", "confidence": 0.9766679406166077}]}, {"text": "First, we predict a structure from the input paragraph (the top right portion shows a partial structure skipping some arguments for brevity).", "labels": [], "entities": []}, {"text": "Circles denote events, squares denote arguments, solid arrows represent event-event relations, and dashed arrows represent event-argument relations.", "labels": [], "entities": []}, {"text": "Second, we map the question paired with each answer into a query that will be answered using the structure.", "labels": [], "entities": []}, {"text": "The bottom right shows the query representation.", "labels": [], "entities": []}, {"text": "Last, the two queries are executed against the structure, and a final answer is returned.", "labels": [], "entities": []}, {"text": "domain, processes are abundant in domains such as chemistry, economics, manufacturing, and even everyday events like shopping or cooking, and our model can be applied to these domains as well.", "labels": [], "entities": []}, {"text": "The contributions of this paper are: 1.", "labels": [], "entities": []}, {"text": "We propose a reading comprehension task which requires deep reasoning over structures that represent complex relations between multiple events and entities.", "labels": [], "entities": []}, {"text": "2. We present PROCESSBANK, anew dataset consisting of descriptions of biological processes, fully-annotated with rich process structures, and accompanied by multiplechoice questions.", "labels": [], "entities": []}, {"text": "3. We present a novel method for answering questions, by predicting process structures and mapping questions to queries.", "labels": [], "entities": []}, {"text": "We demonstrate that by predicting structures we can improve reading comprehension accuracy over baselines that do not exploit the underlying structure.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 82, "end_pos": 90, "type": "METRIC", "confidence": 0.9647546410560608}]}, {"text": "The data and code for this paper are available at http://www-nlp.stanford.edu/ software/bioprocess.", "labels": [], "entities": []}], "datasetContent": [{"text": "We used 150 processes (435 questions) for training and 50 processes (150 questions) as the test set.", "labels": [], "entities": []}, {"text": "For development, we randomly split the training set 10 times (80%/20%), and tuned hyperparameters by maximizing average accuracy on question answering.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 120, "end_pos": 128, "type": "METRIC", "confidence": 0.9959569573402405}, {"text": "question answering", "start_pos": 132, "end_pos": 150, "type": "TASK", "confidence": 0.7015688121318817}]}, {"text": "We preprocessed the paragraphs with the Stanford CoreNLP pipeline version 3.4) and Illinois SRL ().", "labels": [], "entities": [{"text": "Stanford CoreNLP pipeline version 3.4", "start_pos": 40, "end_pos": 77, "type": "DATASET", "confidence": 0.9493277788162231}, {"text": "Illinois SRL", "start_pos": 83, "end_pos": 95, "type": "DATASET", "confidence": 0.8792448341846466}]}, {"text": "We used the Gurobi optimization package 3 for inference.", "labels": [], "entities": []}, {"text": "We compare our system PROREAD to baselines that do not have access to the process structure: 1.", "labels": [], "entities": []}, {"text": "BOW: For each answer, we compute the proportion of content word lemmas covered by the paragraph and choose the one with higher coverage.", "labels": [], "entities": [{"text": "BOW", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.9877047538757324}]}, {"text": "For true-false questions, we compute the coverage of the question statement, and answer \"True\" if it is higher than a threshold tuned on the development set.", "labels": [], "entities": []}, {"text": "2. TEXTPROX: For dependency questions, we align content word lemmas in both the question and answer against the text and select the answer whose aligned tokens are closer to the aligned tokens of the question.", "labels": [], "entities": [{"text": "TEXTPROX", "start_pos": 3, "end_pos": 11, "type": "METRIC", "confidence": 0.9323697686195374}]}, {"text": "For temporal questions, we return the answer for which the order of events is identical to their order in the paragraph.", "labels": [], "entities": []}, {"text": "For true-false questions, we return \"True\" if the number of bigrams from the question covered in the text is higher than a threshold tuned on the development set.", "labels": [], "entities": []}, {"text": "3. SYNTPROX: For dependency questions, we use proximity as in TEXTPROX, except that distance is measured using dependency tree edges.", "labels": [], "entities": []}, {"text": "To support multiple sentences we connect roots of adjacent sentences with bidirectional edges.", "labels": [], "entities": []}, {"text": "For temporal questions this baseline is identical to TEXTPROX.", "labels": [], "entities": [{"text": "TEXTPROX", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.6870602965354919}]}, {"text": "For truefalse questions, we compute the number of dependency tree edges in the question statement covered by edges in the paragraph (an edge has a source lemma, relation, and target lemma), and answer \"True\" if the coverage is  higher than a threshold tuned on the training set.", "labels": [], "entities": []}, {"text": "To separate the contribution of process structures from the performance of our structure predictor, we also run our QA system given manually annotated gold standard structures (GOLD).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: Reading comprehension test set accuracy. The All", "labels": [], "entities": [{"text": "accuracy", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.9754628539085388}]}, {"text": " Table 5: Structured prediction test set results.", "labels": [], "entities": [{"text": "Structured prediction test", "start_pos": 10, "end_pos": 36, "type": "TASK", "confidence": 0.8232146501541138}]}, {"text": " Table 6: Error analysis results. An explanation of the vari-", "labels": [], "entities": [{"text": "Error", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9177488088607788}]}]}