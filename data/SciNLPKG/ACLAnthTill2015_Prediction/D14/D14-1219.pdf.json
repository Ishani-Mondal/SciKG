{"title": [{"text": "Discriminative Reranking of Discourse Parses Using Tree Kernels", "labels": [], "entities": [{"text": "Discriminative Reranking of Discourse Parses", "start_pos": 0, "end_pos": 44, "type": "TASK", "confidence": 0.7959834098815918}]}], "abstractContent": [{"text": "In this paper, we present a discrimina-tive approach for reranking discourse trees generated by an existing probabilistic discourse parser.", "labels": [], "entities": []}, {"text": "The reranker relies on tree kernels (TKs) to capture the global dependencies between discourse units in a tree.", "labels": [], "entities": []}, {"text": "In particular, we design new computational structures of discourse trees, which combined with standard TKs, originate novel discourse TKs.", "labels": [], "entities": []}, {"text": "The empirical evaluation shows that our reranker can improve the state-of-the-art sentence-level parsing accuracy from 79.77% to 82.15%, a relative error reduction of 11.8%, which in turn pushes the state-of-the-art document-level accuracy from 55.8% to 57.3%.", "labels": [], "entities": [{"text": "sentence-level parsing", "start_pos": 82, "end_pos": 104, "type": "TASK", "confidence": 0.5749480426311493}, {"text": "accuracy", "start_pos": 105, "end_pos": 113, "type": "METRIC", "confidence": 0.8895728588104248}, {"text": "accuracy", "start_pos": 231, "end_pos": 239, "type": "METRIC", "confidence": 0.9614311456680298}]}], "introductionContent": [{"text": "Clauses and sentences in a well-written text are interrelated and exhibit a coherence structure.", "labels": [], "entities": []}, {"text": "Rhetorical Structure Theory (RST) represents the coherence structure of a text by a labeled tree, called discourse tree (DT) as shown in.", "labels": [], "entities": [{"text": "Rhetorical Structure Theory (RST)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.8009516845146815}]}, {"text": "The leaves correspond to contiguous clause-like units called elementary discourse units (EDUs).", "labels": [], "entities": []}, {"text": "Adjacent EDUs and larger discourse units are hierarchically connected by coherence relations (e.g., ELABORA-TION, CAUSE).", "labels": [], "entities": []}, {"text": "Discourse units connected by a relation are further distinguished depending on their relative importance: nuclei are the core parts of the relation while satellites are the supportive ones.", "labels": [], "entities": []}, {"text": "Conventionally, discourse analysis in RST involves two subtasks: (i) discourse segmentation: breaking the text into a sequence of EDUs, and (ii) discourse parsing: linking the discourse units to form a labeled tree.", "labels": [], "entities": [{"text": "discourse analysis", "start_pos": 16, "end_pos": 34, "type": "TASK", "confidence": 0.7284208834171295}, {"text": "RST", "start_pos": 38, "end_pos": 41, "type": "TASK", "confidence": 0.9140398502349854}, {"text": "discourse segmentation", "start_pos": 69, "end_pos": 91, "type": "TASK", "confidence": 0.7274368703365326}, {"text": "discourse parsing", "start_pos": 145, "end_pos": 162, "type": "TASK", "confidence": 0.7123663127422333}]}, {"text": "Despite the fact that discourse analysis is central to many NLP applications, the state-of-the-art document-level discourse parser () has an f -score of only 55.83% using manual discourse segmentation on the RST Discourse Treebank (RST-DT).", "labels": [], "entities": [{"text": "discourse analysis", "start_pos": 22, "end_pos": 40, "type": "TASK", "confidence": 0.7135571986436844}, {"text": "f -score", "start_pos": 141, "end_pos": 149, "type": "METRIC", "confidence": 0.9839388728141785}, {"text": "RST Discourse Treebank (RST-DT)", "start_pos": 208, "end_pos": 239, "type": "DATASET", "confidence": 0.8476138810316721}]}, {"text": "Although recent work has proposed rich linguistic features) and powerful parsing models (, discourse parsing remains a hard task, partly because these approaches do not consider global features and long range structural dependencies between DT constituents.", "labels": [], "entities": [{"text": "discourse parsing", "start_pos": 91, "end_pos": 108, "type": "TASK", "confidence": 0.7205866724252701}]}, {"text": "For example, consider the humanannotated DT () and the DT generated by the discourse parser of for the same text.", "labels": [], "entities": []}, {"text": "The parser makes a mistake in finding the right structure: it considers only e 3 as the text to be attributed toe 2 , where all the text spans from e 3 toe 6 (linked by CAUSE and ELAB-ORATION) compose the statement to be attributed.", "labels": [], "entities": [{"text": "CAUSE", "start_pos": 169, "end_pos": 174, "type": "METRIC", "confidence": 0.754349946975708}]}, {"text": "Such errors occur because existing systems do not encode long range dependencies between DT constituents such as those between e 3 and e 4\u22126 . Reranking models can make the global structural information available to the system as follows: first, abase parser produces several DT hypotheses; and then a classifier exploits the entire information in each hypothesis, e.g., the complete DT with its dependencies, for selecting the best DT.", "labels": [], "entities": []}, {"text": "Designing features capturing such global properties is however not trivial as it requires the selection of important DT fragments.", "labels": [], "entities": []}, {"text": "This means selecting subtree patterns from an exponential feature space.", "labels": [], "entities": []}, {"text": "An alternative approach is to implicitly generate the whole feature space using tree kernels (TKs) ().", "labels": [], "entities": []}, {"text": "In this paper, we present reranking models for discourse parsing based on Support Vector Machines (SVMs) and TKs.", "labels": [], "entities": [{"text": "discourse parsing", "start_pos": 47, "end_pos": 64, "type": "TASK", "confidence": 0.7072126567363739}]}, {"text": "The latter allows us to represent structured data using the substructure space thus capturing structural dependencies between DT constituents, which is essential for effective discourse parsing.", "labels": [], "entities": [{"text": "discourse parsing", "start_pos": 176, "end_pos": 193, "type": "TASK", "confidence": 0.6872773766517639}]}, {"text": "Specifically, we made the following contributions.", "labels": [], "entities": []}, {"text": "First, we extend the  existing discourse parser 1 () to produce a list of k most probable parses for each input text, with associated probabilities that define the initial ranking.", "labels": [], "entities": []}, {"text": "Second, we define a set of discourse tree kernels (DISCTK) based on the functional composition of standard TKs with structures representing the properties of DTs.", "labels": [], "entities": []}, {"text": "DISCTK can be used for any classification task involving discourse trees.", "labels": [], "entities": [{"text": "DISCTK", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.8115291595458984}]}, {"text": "Third, we use DISCTK to define kernels for reranking and use them in SVMs.", "labels": [], "entities": []}, {"text": "Our rerankers can exploit the complete DT structure using TKs.", "labels": [], "entities": []}, {"text": "They can ascertain if portions of a DT are compatible, incompatible or simply not likely to coexist, since each substructure is an exploitable feature.", "labels": [], "entities": []}, {"text": "In other words, problematic DTs are expected to be ranked lower by our reranker.", "labels": [], "entities": []}, {"text": "Finally, we investigate the potential of our approach by computing the oracle f -scores for both document-and sentence-level discourse parsing.", "labels": [], "entities": [{"text": "document-and sentence-level discourse parsing", "start_pos": 97, "end_pos": 142, "type": "TASK", "confidence": 0.5490805506706238}]}, {"text": "However, as demonstrated later in Section 6, for document-level parsing, the top k parses often miss the best parse.", "labels": [], "entities": [{"text": "document-level parsing", "start_pos": 49, "end_pos": 71, "type": "TASK", "confidence": 0.587149366736412}]}, {"text": "For example, the oracle fscores for 5-and 20-best document-level parsing are only 56.91% and 57.65%, respectively.", "labels": [], "entities": [{"text": "document-level parsing", "start_pos": 50, "end_pos": 72, "type": "TASK", "confidence": 0.55730240046978}]}, {"text": "Thus the scope of improvement for the reranker is rather narrow at the document level.", "labels": [], "entities": []}, {"text": "On the other hand, the oracle f -score for 5-best sentence-level discourse parsing is 88.09%, where the base parser (i.e., 1-best) has an oracle f -score of 79.77%.", "labels": [], "entities": [{"text": "oracle f -score", "start_pos": 23, "end_pos": 38, "type": "METRIC", "confidence": 0.6791848540306091}, {"text": "sentence-level discourse parsing", "start_pos": 50, "end_pos": 82, "type": "TASK", "confidence": 0.6183357437451681}]}, {"text": "Therefore, in this paper we address the following two questions: (i) how far can a reranker improve the parsing accuracy at the sentence level? and (ii) how far can this improvement, if at all, push the (combined) document-level parsing accuracy?", "labels": [], "entities": [{"text": "parsing", "start_pos": 104, "end_pos": 111, "type": "TASK", "confidence": 0.952570378780365}, {"text": "accuracy", "start_pos": 112, "end_pos": 120, "type": "METRIC", "confidence": 0.7180225253105164}, {"text": "document-level parsing", "start_pos": 214, "end_pos": 236, "type": "TASK", "confidence": 0.5652136355638504}, {"text": "accuracy", "start_pos": 237, "end_pos": 245, "type": "METRIC", "confidence": 0.4632011950016022}]}, {"text": "To this end, our comparative experiments on 1 Available from http://alt.qcri.org/tools/ RST-DT show that the sentence-level reranker can improve the f -score of the state-of-the-art from 79.77% to 82.15%, corresponding to a relative error reduction of 11.8%, which in turn pushes the state-of-the-art document-level f -score from 55.8% to 57.3%, an error reduction of 3.4%.", "labels": [], "entities": [{"text": "f -score", "start_pos": 149, "end_pos": 157, "type": "METRIC", "confidence": 0.9490884939829508}]}, {"text": "In the rest of the paper, after introducing the TK technology in Section 2, we illustrate our novel structures, and how they lead to the design of novel DISCTKs in Section 3.", "labels": [], "entities": []}, {"text": "We present the kbest discourse parser in Section 4.", "labels": [], "entities": []}, {"text": "In Section 5, we describe our reranking approach using DISCTKs.", "labels": [], "entities": []}, {"text": "We report our experiments in Section 6.", "labels": [], "entities": [{"text": "Section 6", "start_pos": 29, "end_pos": 38, "type": "DATASET", "confidence": 0.9018997251987457}]}, {"text": "We briefly overview the related work in Section 7, and finally, we summarize our contributions in Section 8.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our experiments aim to show that reranking of discourse parses is a promising research direction, which can improve the state-of-the-art.", "labels": [], "entities": [{"text": "reranking of discourse parses", "start_pos": 33, "end_pos": 62, "type": "TASK", "confidence": 0.8244931250810623}]}, {"text": "To achieve this, we (i) compute the oracle accuracy of the kbest parser, (ii) test different kernels for reranking discourse parses by applying standard kernels to our new structures, (iii) show the reranking performance using the best kernel for different number of hypotheses, and (iv) show the relative importance of features coming from different sources.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.9602062106132507}, {"text": "reranking discourse parses", "start_pos": 105, "end_pos": 131, "type": "TASK", "confidence": 0.7003865242004395}]}, {"text": "We use the standard RST-DT corpus), which comes with discourse annotations for 385 articles (347 for training and 38 for testing) from the Wall Street Journal.", "labels": [], "entities": [{"text": "RST-DT corpus", "start_pos": 20, "end_pos": 33, "type": "DATASET", "confidence": 0.7704228162765503}, {"text": "Wall Street Journal", "start_pos": 139, "end_pos": 158, "type": "DATASET", "confidence": 0.9613338311513265}]}, {"text": "We extracted sentence-level DTs from a document-level DT by finding the subtrees that exactly span over the sentences.", "labels": [], "entities": []}, {"text": "This gives 7321 and 951 sentences in the training and test sets, respectively.", "labels": [], "entities": []}, {"text": "Following previous work, we use the same 18 coarser relations defined by.", "labels": [], "entities": [{"text": "coarser", "start_pos": 44, "end_pos": 51, "type": "METRIC", "confidence": 0.9540401697158813}]}, {"text": "We create the training data for the reranker in a 5-fold cross-validation fashion.", "labels": [], "entities": []}, {"text": "8 Specifically, we split the training set into 5 equal-sized folds, and train the parsing model on 4 folds and apply to the rest to produce k most probable DTs for each text.", "labels": [], "entities": []}, {"text": "Then we generate and label the pairs (by comparing with the gold) from the k most probable trees as described in Section 5.1.", "labels": [], "entities": []}, {"text": "Finally, we merge the 5 labeled folds to create the full training data.", "labels": [], "entities": []}, {"text": "We use SVM-light-TK to train our reranking models, 9 which enables the use of tree kernels) in SVM-light).", "labels": [], "entities": [{"text": "SVM-light-TK", "start_pos": 7, "end_pos": 19, "type": "DATASET", "confidence": 0.8896222710609436}]}, {"text": "We build our new kernels for reranking exploiting the standard built-in TK functions, such as STK, STK band PTK.", "labels": [], "entities": [{"text": "STK", "start_pos": 94, "end_pos": 97, "type": "DATASET", "confidence": 0.8538327813148499}, {"text": "STK band PTK", "start_pos": 99, "end_pos": 111, "type": "DATASET", "confidence": 0.8342620531717936}]}, {"text": "We applied a linear kernel to standard feature vectors as it showed to be the best on our development set.", "labels": [], "entities": []}, {"text": "The standard procedure to evaluate discourse parsing performance is to compute Precision, Recall and f -score of the unlabeled and labeled metrics proposed by.", "labels": [], "entities": [{"text": "discourse parsing", "start_pos": 35, "end_pos": 52, "type": "TASK", "confidence": 0.6806750744581223}, {"text": "Precision", "start_pos": 79, "end_pos": 88, "type": "METRIC", "confidence": 0.9992552399635315}, {"text": "Recall", "start_pos": 90, "end_pos": 96, "type": "METRIC", "confidence": 0.9924390316009521}, {"text": "f -score", "start_pos": 101, "end_pos": 109, "type": "METRIC", "confidence": 0.988069494565328}]}, {"text": "Specifically, the unlabeled metric Span measures how accurate the parser is in finding the right structure (i.e., skeleton) of the DT, while the labeled metrics Nuclearity and Relation measure the parser's ability to find the right labels (nuclearity and relation) in addition to the right structure.", "labels": [], "entities": []}, {"text": "Optimization of the Relation metric is considered to be the hardest and the most desirable goal in discourse parsing since it gives aggregated evaluation on tree structure and relation labels.", "labels": [], "entities": [{"text": "Relation", "start_pos": 20, "end_pos": 28, "type": "TASK", "confidence": 0.8684117197990417}, {"text": "discourse parsing", "start_pos": 99, "end_pos": 116, "type": "TASK", "confidence": 0.7197160422801971}]}, {"text": "Therefore, we measure the oracle accuracy of the k-best discourse parser based on the f -scores of the Relation metric, and our reranking framework aims to optimize the Relation metric.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.9810047745704651}]}, {"text": "11 Specifically, the oracle accuracy for k-best parsing is measured as fol- , where N is the total number of texts (sentences or documents) evaluated, g i is the gold DT annotation for text i, h j i is the j th parse hypothesis generated by the k-best parser for text i, and f -score r (g i , h j i ) is the f -score accuracy of hypothesis h j ion the Relation metric.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.9443503022193909}, {"text": "fol-", "start_pos": 71, "end_pos": 75, "type": "METRIC", "confidence": 0.9669883847236633}, {"text": "f -score r", "start_pos": 275, "end_pos": 285, "type": "METRIC", "confidence": 0.9503726810216904}, {"text": "f -score accuracy", "start_pos": 308, "end_pos": 325, "type": "METRIC", "confidence": 0.8102753907442093}, {"text": "Relation metric", "start_pos": 352, "end_pos": 367, "type": "DATASET", "confidence": 0.7451618909835815}]}, {"text": "In all our experiments we report the f -scores of the Relation metric.", "labels": [], "entities": [{"text": "f -scores", "start_pos": 37, "end_pos": 46, "type": "METRIC", "confidence": 0.9586454629898071}, {"text": "Relation", "start_pos": 54, "end_pos": 62, "type": "TASK", "confidence": 0.5169029235839844}]}, {"text": "presents the oracle scores of the kbest intra-sentential parser PAR-S on the standard RST-DT test set.", "labels": [], "entities": [{"text": "RST-DT test set", "start_pos": 86, "end_pos": 101, "type": "DATASET", "confidence": 0.8327740629514059}]}, {"text": "The 1-best result corresponds to the accuracy of the base parser (i.e., 79.77%).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.9996191263198853}]}, {"text": "The 2-best shows dramatic oracle-rate improvement (i.e., 4.65% absolute), suggesting that the base parser often generates the best tree in its top 2 outputs.", "labels": [], "entities": []}, {"text": "5-best increases the oracle score to 88.09%.", "labels": [], "entities": []}, {"text": "Afterwards, the increase inaccuracy slows down, achieving, e.g., 90.37% and 92.57% at 10-best and 20-best, respectively.", "labels": [], "entities": [{"text": "inaccuracy", "start_pos": 25, "end_pos": 35, "type": "METRIC", "confidence": 0.9767084717750549}]}], "tableCaptions": [{"text": " Table 1: Oracle scores as a function of k of k-best sentence-", "labels": [], "entities": [{"text": "Oracle scores", "start_pos": 10, "end_pos": 23, "type": "METRIC", "confidence": 0.8669536411762238}]}, {"text": " Table 2: Oracle scores as a function of k of k-best", "labels": [], "entities": []}, {"text": " Table 3: Reranking performance of different discourse tree", "labels": [], "entities": [{"text": "Reranking", "start_pos": 10, "end_pos": 19, "type": "TASK", "confidence": 0.7400510311126709}]}, {"text": " Table 5: Comparison of features from different sources for", "labels": [], "entities": []}, {"text": " Table 4: Reranking performance (RR) in comparison with oracle (OR) accuracy for different values of k on the standard", "labels": [], "entities": [{"text": "Reranking performance (RR)", "start_pos": 10, "end_pos": 36, "type": "METRIC", "confidence": 0.9541339159011841}, {"text": "OR) accuracy", "start_pos": 64, "end_pos": 76, "type": "METRIC", "confidence": 0.7483594020207723}]}]}