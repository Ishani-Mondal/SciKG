{"title": [{"text": "Efficient Non-parametric Estimation of Multiple Embeddings per Word in Vector Space", "labels": [], "entities": []}], "abstractContent": [{"text": "There is rising interest in vector-space word embeddings and their use in NLP, especially given recent methods for their fast estimation at very large scale.", "labels": [], "entities": [{"text": "NLP", "start_pos": 74, "end_pos": 77, "type": "TASK", "confidence": 0.9240930676460266}]}, {"text": "Nearly all this work, however, assumes a single vector per word type-ignoring poly-semy and thus jeopardizing their usefulness for downstream tasks.", "labels": [], "entities": []}, {"text": "We present an extension to the Skip-gram model that efficiently learns multiple embeddings per word type.", "labels": [], "entities": []}, {"text": "It differs from recent related work by jointly performing word sense discrimination and embedding learning, by non-parametrically estimating the number of senses per word type, and by its efficiency and scalability.", "labels": [], "entities": [{"text": "word sense discrimination", "start_pos": 58, "end_pos": 83, "type": "TASK", "confidence": 0.6839595437049866}]}, {"text": "We present new state-of-the-art results in the word similarity in context task and demonstrate its scal-ability by training with one machine on a corpus of nearly 1 billion tokens in less than 6 hours.", "labels": [], "entities": [{"text": "word similarity in context task", "start_pos": 47, "end_pos": 78, "type": "TASK", "confidence": 0.7226277589797974}]}], "introductionContent": [{"text": "Representing words by dense, real-valued vector embeddings, also commonly called \"distributed representations,\" helps address the curse of dimensionality and improve generalization because they can place near each other words having similar semantic and syntactic roles.", "labels": [], "entities": []}, {"text": "This has been shown dramatically in state-of-the-art results on language modeling ( as well as improvements in other natural language processing tasks).", "labels": [], "entities": [{"text": "language modeling", "start_pos": 64, "end_pos": 81, "type": "TASK", "confidence": 0.7488031685352325}]}, {"text": "Substantial benefit arises when embeddings can be trained on large volumes of data.", "labels": [], "entities": []}, {"text": "Hence the recent considerable interest in the CBOW and Skip-gram models The first two authors contributed equally to this paper. of Mikolov et al (2013a); Mikolov et al (2013b)-relatively simple log-linear models that can be trained to produce high-quality word embeddings on the entirety of English Wikipedia text in less than half a day on one machine.", "labels": [], "entities": [{"text": "CBOW", "start_pos": 46, "end_pos": 50, "type": "DATASET", "confidence": 0.8713377118110657}]}, {"text": "There is rising enthusiasm for applying these models to improve accuracy in natural language processing, much like Brown clusters) have become common input features for many tasks, such as named entity extraction () and parsing ().", "labels": [], "entities": [{"text": "accuracy", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.9931550025939941}, {"text": "named entity extraction", "start_pos": 189, "end_pos": 212, "type": "TASK", "confidence": 0.6120054920514425}, {"text": "parsing", "start_pos": 220, "end_pos": 227, "type": "TASK", "confidence": 0.9602822661399841}]}, {"text": "In comparison to Brown clusters, the vector embeddings have the advantages of substantially better scalability in their training, and intriguing potential for their continuous and multi-dimensional interrelations.", "labels": [], "entities": []}, {"text": "In fact, present new state-of-the-art results in CoNLL 2003 named entity extraction by directly inputting continuous vector embeddings obtained by aversion of Skipgram that injects supervision with lexicons.", "labels": [], "entities": [{"text": "CoNLL 2003", "start_pos": 49, "end_pos": 59, "type": "DATASET", "confidence": 0.9412162005901337}, {"text": "entity extraction", "start_pos": 66, "end_pos": 83, "type": "TASK", "confidence": 0.750091552734375}, {"text": "Skipgram", "start_pos": 159, "end_pos": 167, "type": "DATASET", "confidence": 0.9463409781455994}]}, {"text": "Similarly show results in dependency parsing using Skip-gram embeddings.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 26, "end_pos": 44, "type": "TASK", "confidence": 0.8773817718029022}]}, {"text": "They have also recently been applied to machine translation (.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 40, "end_pos": 59, "type": "TASK", "confidence": 0.8315883278846741}]}, {"text": "A notable deficiency in this prior work is that each word type (e.g. the word string plant) has only one vector representation-polysemy and hononymy are ignored.", "labels": [], "entities": []}, {"text": "This results in the word plant having an embedding that is approximately the average of its different contextual semantics relating to biology, placement, manufacturing and power generation.", "labels": [], "entities": []}, {"text": "In moderately highdimensional spaces a vector can be relatively \"close\" to multiple regions at a time, but this does not negate the unfortunate influence of the triangle inequality 2 here: words that are not synonyms but are synonymous with different senses of the same word will be pulled together.", "labels": [], "entities": []}, {"text": "For example, pollen and refinery will be inappropriately pulled to a dis-tance not more than the sum of the distances plantpollen and plant-refinery.", "labels": [], "entities": []}, {"text": "Fitting the constraints of legitimate continuous gradations of semantics are challenge enough without the additional encumbrance of these illegitimate triangle inequalities.", "labels": [], "entities": []}, {"text": "Discovering embeddings for multiple senses per word type is the focus of work by and.", "labels": [], "entities": []}, {"text": "They both pre-cluster the contexts of a word type's tokens into discriminated senses, use the clusters to re-label the corpus' tokens according to sense, and then learn embeddings for these re-labeled words.", "labels": [], "entities": []}, {"text": "The second paper improves upon the first by employing an earlier pass of non-discriminated embedding learning to obtain vectors used to represent the contexts.", "labels": [], "entities": []}, {"text": "Note that by pre-clustering, these methods lose the opportunity to jointly learn the sense-discriminated vectors and the clustering.", "labels": [], "entities": []}, {"text": "Other weaknesses include their fixed number of sense per word type, and the computational expense of the two-step process-the Huang et al (2012) method took one week of computation to learn multiple embeddings fora 6,000 subset of the 30,000 vocabulary on a corpus containing close to billion tokens.", "labels": [], "entities": []}, {"text": "This paper presents anew method for learning vector-space embeddings for multiple senses per word type, designed to provide several advantages over previous approaches.", "labels": [], "entities": []}, {"text": "(1) Sensediscriminated vectors are learned jointly with the assignment of token contexts to senses; thus we can use the emerging sense representation to more accurately perform the clustering.", "labels": [], "entities": []}, {"text": "(2) A nonparametric variant of our method automatically discovers a varying number of senses per word type.", "labels": [], "entities": []}, {"text": "(3) Efficient online joint training makes it fast and scalable.", "labels": [], "entities": []}, {"text": "We refer to our method as Multiple-sense Skip-gram, or MSSG, and its nonparametric counterpart as NP-MSSG.", "labels": [], "entities": []}, {"text": "Our method builds on the Skip-gram model (), but maintains multiple vectors per word type.", "labels": [], "entities": []}, {"text": "During online training with a particular token, we use the average of its context words' vectors to select the token's sense that is closest, and perform a gradient update on that sense.", "labels": [], "entities": []}, {"text": "In the non-parametric version of our method, we build on facility location: anew cluster is created with probability proportional to the distance from the context to the Personal communication with authors Eric H. nearest sense.", "labels": [], "entities": []}, {"text": "We present experimental results demonstrating the benefits of our approach.", "labels": [], "entities": []}, {"text": "We show qualitative improvements over single-sense, comparing against word neighbors from our parametric and non-parametric methods.", "labels": [], "entities": []}, {"text": "We present quantitative results in three tasks.", "labels": [], "entities": []}, {"text": "On both the SCWS and WordSim353 data sets our methods surpass the previous state-ofthe-art.", "labels": [], "entities": [{"text": "SCWS", "start_pos": 12, "end_pos": 16, "type": "DATASET", "confidence": 0.9504094123840332}, {"text": "WordSim353 data sets", "start_pos": 21, "end_pos": 41, "type": "DATASET", "confidence": 0.965207576751709}]}, {"text": "The Google Analogy task is not especially well-suited for word-sense evaluation since its lack of context makes selecting the sense difficult; however our method dramatically outperforms Huang et al (2012) on this task.", "labels": [], "entities": [{"text": "word-sense evaluation", "start_pos": 58, "end_pos": 79, "type": "TASK", "confidence": 0.7449889183044434}]}, {"text": "Finally we also demonstrate scalabilty, learning multiple senses, training on nearly a billion tokens in less than 6 hours-a 27x improvement on Huang et al.", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate our algorithms we train embeddings using the same corpus and vocabulary as used in Huang et al, which is the April 2010 snapshot of the Wikipedia corpus).", "labels": [], "entities": [{"text": "Wikipedia corpus", "start_pos": 148, "end_pos": 164, "type": "DATASET", "confidence": 0.82120081782341}]}, {"text": "It contains approximately 2 million articles and 990 million tokens.", "labels": [], "entities": []}, {"text": "In all our experiments we remove all the words with less than 20 occurrences and use a maximum context window (N ) of length 5 (5 words before and after the word occurrence).", "labels": [], "entities": []}, {"text": "We fix the number of senses (K) to be 3 for the MSSG model unless otherwise specified.", "labels": [], "entities": [{"text": "MSSG", "start_pos": 48, "end_pos": 52, "type": "DATASET", "confidence": 0.8306357860565186}]}, {"text": "Our hyperparameter values were selected by a small amount of manual exploration on a validation set.", "labels": [], "entities": []}, {"text": "In NP-MSSG we set \u03bb to -0.5.", "labels": [], "entities": []}, {"text": "The Skip-gram model, MSSG and NP-MSSG models sample one noisy context word (S) for each of the observed context words.", "labels": [], "entities": []}, {"text": "We train our models using AdaGrad stochastic gradient decent () with initial learning rate set to 0.025.", "labels": [], "entities": []}, {"text": "Similarly to Huang et al, we don't use a regularization penalty.", "labels": [], "entities": []}, {"text": "Below we describe qualitative results, displaying the embeddings and the nearest neighbors of each word sense, and quantitative experiments in two benchmark word similarity tasks.", "labels": [], "entities": []}, {"text": "shows time to train our models, compared with other models from previous work.", "labels": [], "entities": []}, {"text": "All these times are from single-machine implementations running on similar-sized corpora.", "labels": [], "entities": []}, {"text": "We see that our model shows significant improvement in the training time over the model in Huang et al, being within well within an order-ofmagnitude of the training time for Skip-gram models.: Nearest neighbors of each sense of each word, by cosine similarity, for different algorithms.", "labels": [], "entities": []}, {"text": "Note that the different senses closely correspond to intuitions regarding the senses of the given word types.", "labels": [], "entities": []}, {"text": "shows qualitatively the results of discovering multiple senses by presenting the nearest neighbors associated with various embeddings.", "labels": [], "entities": []}, {"text": "The nearest neighbors of a word are computed by comparing the cosine similarity between the embedding for each sense of the word and the context embeddings of all other words in the vocabulary.", "labels": [], "entities": []}, {"text": "Note that each of the discovered senses are indeed semantically coherent, and that a reasonable number of senses are created by the non-parametric method.", "labels": [], "entities": []}, {"text": "shows the nearest neighbors of the word plant for Skip-gram, MSSG , NP-MSSG and Haung's model).: Nearest Neighbors of the word plant for different models.", "labels": [], "entities": [{"text": "Skip-gram", "start_pos": 50, "end_pos": 59, "type": "DATASET", "confidence": 0.9319804906845093}]}, {"text": "We see that the discovered senses in both our models are more semantically coherent than Huang et al (2012) and NP-MSSG is able to learn reasonable number of senses.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: Experimental results in the SCWS task. The numbers are Spearmans correlation \u03c1 \u00d7 100  between each model's similarity judgments and the human judgments, in context. First three models  learn only a single embedding per model and hence, avgSim, avgSimC and localSim are not reported  for these models, as they'd be identical to globalSim. Both our parametric and non-parametric models  outperform the baseline models, and our best model achieves a score of 69.3 in this task. NP-MSSG  achieves the best results when globalSim, avgSim and localSim similarity measures are used. The best  results according to each metric are in bold face.", "labels": [], "entities": [{"text": "SCWS task", "start_pos": 38, "end_pos": 47, "type": "TASK", "confidence": 0.924324244260788}]}, {"text": " Table 5: Results on the WordSim-353 dataset.  The table shows the Spearmans correlation \u03c1 be- tween the model's similarities and human judg- ments. G indicates the globalSim similarity mea- sure and M indicates avgSim measure.The best  results among models that learn low-dimensional  and dense representations are in bold face. Pruned  TF-IDF (Reisinger and Mooney, 2010a), ESA  (Gabrilovich and Markovitch, 2007) and Tiered  TF-IDF (Reisinger and Mooney, 2010b) construct  spare, high-dimensional representations.", "labels": [], "entities": [{"text": "WordSim-353 dataset", "start_pos": 25, "end_pos": 44, "type": "DATASET", "confidence": 0.9839887320995331}, {"text": "globalSim similarity mea- sure", "start_pos": 165, "end_pos": 195, "type": "METRIC", "confidence": 0.7418156027793884}]}, {"text": " Table 6: Experiment results on WordSim-353 and  SCWS Task. Multiple Embeddings are learned for  top 30,000 most frequent words in the vocabulary.  The embedding dimension size is 300 for all the  models for this task. The number of senses for  MSSG model is 3.", "labels": [], "entities": [{"text": "WordSim-353", "start_pos": 32, "end_pos": 43, "type": "DATASET", "confidence": 0.9655624628067017}]}]}