{"title": [{"text": "Large-scale Reordering Model for Statistical Machine Translation using Dual Multinomial Logistic Regression", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 33, "end_pos": 64, "type": "TASK", "confidence": 0.8283340533574423}]}], "abstractContent": [{"text": "Phrase reordering is a challenge for statistical machine translation systems.", "labels": [], "entities": [{"text": "Phrase reordering", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9660422205924988}, {"text": "statistical machine translation", "start_pos": 37, "end_pos": 68, "type": "TASK", "confidence": 0.6597849229971567}]}, {"text": "Posing phrase movements as a prediction problem using contextual features modeled by maximum entropy-based classifier is superior to the commonly used lexicalized reordering model.", "labels": [], "entities": []}, {"text": "However, Training this discriminative model using large-scale parallel corpus might be computationally expensive.", "labels": [], "entities": []}, {"text": "In this paper, we explore recent advancements in solving large-scale classification problems.", "labels": [], "entities": [{"text": "solving large-scale classification problems", "start_pos": 49, "end_pos": 92, "type": "TASK", "confidence": 0.6580430865287781}]}, {"text": "Using the dual problem to multinomial logistic regression, we managed to shrink the training data while iterating and produce significant saving in computation and memory while preserving the accuracy.", "labels": [], "entities": [{"text": "memory", "start_pos": 164, "end_pos": 170, "type": "METRIC", "confidence": 0.9660771489143372}, {"text": "accuracy", "start_pos": 192, "end_pos": 200, "type": "METRIC", "confidence": 0.9981993436813354}]}], "introductionContent": [{"text": "Phrase reordering is a common problem when translating between two grammatically different languages.", "labels": [], "entities": [{"text": "Phrase reordering", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9704747796058655}]}, {"text": "Analogous to speech recognition systems, statistical machine translation (SMT) systems relied on language models to produce more fluent output.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 13, "end_pos": 31, "type": "TASK", "confidence": 0.7359814345836639}, {"text": "statistical machine translation (SMT)", "start_pos": 41, "end_pos": 78, "type": "TASK", "confidence": 0.7950304547945658}]}, {"text": "While early work penalized phrase movements without considering reorderings arising from vastly differing grammatical structures across language pairs like Arabic-English), many researchers considered lexicalized reordering models that attempted to learn orientation based on the training corpus).", "labels": [], "entities": [{"text": "phrase movements", "start_pos": 27, "end_pos": 43, "type": "TASK", "confidence": 0.723340705037117}]}, {"text": "Building on this, some researchers have borrowed powerful ideas from the machine learning literature, to pose the phrase movement problem as a prediction problem using contextual input features whose importance is modeled as weights of a linear classifier trained by entropic criteria.", "labels": [], "entities": [{"text": "phrase movement problem", "start_pos": 114, "end_pos": 137, "type": "TASK", "confidence": 0.7915128469467163}]}, {"text": "The approach (so called maximum entropy classifier or simply MaxEnt) is a popular choice ().", "labels": [], "entities": []}, {"text": "Max-margin structure classifiers were also proposed ().", "labels": [], "entities": []}, {"text": "Alternatively, proposed recently using sparse features optimize the translation quality with the decoder instead of training a classifier independently.", "labels": [], "entities": []}, {"text": "While large-scale parallel corpus is advantageous for improving such reordering model, this improvement comes at a price of computational complexity.", "labels": [], "entities": []}, {"text": "This issue is particularly pronounced when discriminative models are considered such as maximum entropy-based model due to the required iterative learning.", "labels": [], "entities": []}, {"text": "Advancements in solving large-scale classification problems have been shown to be effective such as dual coordinate descent method for linear support vector machines (.", "labels": [], "entities": [{"text": "dual coordinate descent", "start_pos": 100, "end_pos": 123, "type": "TASK", "confidence": 0.6126193404197693}]}, {"text": "Similarly, proposed a two-level dual coordinate descent method for maximum entropy classifier.", "labels": [], "entities": []}, {"text": "In this work we explore the dual problem to multinomial logistic regression for building largescale reordering model (section 3).", "labels": [], "entities": []}, {"text": "One of the main advantages of solving the dual problem is providing a mechanism to shrink the training data which is a serious issue in building such largescale system.", "labels": [], "entities": []}, {"text": "We present empirical results comparing between the primal and the dual problems (section 4).", "labels": [], "entities": []}, {"text": "Our approach is shown to be fast and memory-efficient.", "labels": [], "entities": []}], "datasetContent": [{"text": "We used MultiUN which is a large-scale parallel corpus extracted from the United Nations website.", "labels": [], "entities": [{"text": "United Nations website", "start_pos": 74, "end_pos": 96, "type": "DATASET", "confidence": 0.9004096786181132}]}, {"text": "We have used Arabic and English portion of MultiUN where the English side is about 300 million words.", "labels": [], "entities": [{"text": "MultiUN", "start_pos": 43, "end_pos": 50, "type": "DATASET", "confidence": 0.8769416213035583}]}, {"text": "We simplify the problem by classifying phrase movements into three categories (monotone, swap, discontinuous).", "labels": [], "entities": []}, {"text": "To train the reordering models, we used GIZA++ to produce word alignments).", "labels": [], "entities": [{"text": "word alignments", "start_pos": 58, "end_pos": 73, "type": "TASK", "confidence": 0.691653698682785}]}, {"text": "Then, we used the extract tool that comes with the Moses toolkit () in order to extract phrase pairs along with their orientation classes.", "labels": [], "entities": []}, {"text": "As shown in, each extracted phrase pair is represented by linguistic features as follows: \u2022 Aligned source and target words in a phrase pair.", "labels": [], "entities": []}, {"text": "Each word alignment is a feature.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 5, "end_pos": 19, "type": "TASK", "confidence": 0.650780662894249}]}, {"text": "\u2022 Words within a window around the source phrase to capture the context.", "labels": [], "entities": []}, {"text": "We choose adjacent words of the phrase boundary.", "labels": [], "entities": []}, {"text": "The extracted phrase pairs after filtering are 47,227,789.", "labels": [], "entities": []}, {"text": "The features that occur more than 10 times are 670,154.", "labels": [], "entities": []}, {"text": "Sentence pair: f : Extracted phrase pairs ( \u00af f , \u00af e) : All linguistic features: 1.", "labels": [], "entities": []}, {"text": "f 1 &e 1 2.", "labels": [], "entities": []}, {"text": "f 2 &e 1 3.", "labels": [], "entities": []}, {"text": "f 3 &e 5 5.", "labels": [], "entities": []}, {"text": "f 5 &e 4 6.", "labels": [], "entities": []}, {"text": "f 6 &e 2 9.", "labels": [], "entities": []}, {"text": "f 6 &e 3 10.", "labels": [], "entities": []}, {"text": "f 5 Bag-of-words representation: a phrase pair is represented as a vector where each feature is a discrete number (0=not exist).: A generic example of the process of phrase pair extraction and representation.", "labels": [], "entities": [{"text": "Bag-of-words representation", "start_pos": 4, "end_pos": 31, "type": "TASK", "confidence": 0.9215404391288757}, {"text": "phrase pair extraction", "start_pos": 166, "end_pos": 188, "type": "TASK", "confidence": 0.6670725146929423}]}], "tableCaptions": [{"text": " Table 3: BLEU scores for Arabic-English transla- tion systems with different reordering models (*:  better than the lexicalized model with at least 95%  statistical significance).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.999029278755188}]}]}