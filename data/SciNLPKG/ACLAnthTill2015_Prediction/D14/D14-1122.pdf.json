{"title": [{"text": "Dependency Parsing for Weibo: An Efficient Probabilistic Logic Programming Approach", "labels": [], "entities": [{"text": "Dependency Parsing", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.862534910440445}]}], "abstractContent": [{"text": "Dependency parsing is a core task in NLP, and it is widely used by many applications such as information extraction, question answering, and machine translation.", "labels": [], "entities": [{"text": "Dependency parsing", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8966303169727325}, {"text": "information extraction", "start_pos": 93, "end_pos": 115, "type": "TASK", "confidence": 0.8389754295349121}, {"text": "question answering", "start_pos": 117, "end_pos": 135, "type": "TASK", "confidence": 0.9284426867961884}, {"text": "machine translation", "start_pos": 141, "end_pos": 160, "type": "TASK", "confidence": 0.8144122362136841}]}, {"text": "In the era of social media, a big challenge is that parsers trained on traditional newswire corpora typically suffer from the domain mismatch issue, and thus perform poorly on social media data.", "labels": [], "entities": []}, {"text": "We present anew GFL/FUDG-annotated Chinese tree-bank with more than 18K tokens from Sina Weibo (the Chinese equivalent of Twit-ter).", "labels": [], "entities": [{"text": "FUDG-annotated Chinese tree-bank", "start_pos": 20, "end_pos": 52, "type": "DATASET", "confidence": 0.7850980361302694}, {"text": "Sina Weibo", "start_pos": 84, "end_pos": 94, "type": "DATASET", "confidence": 0.8496626019477844}]}, {"text": "We formulate the dependency parsing problem as many small and paralleliz-able arc prediction tasks: for each task, we use a programmable probabilistic first-order logic to infer the dependency arc of a token in the sentence.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 17, "end_pos": 35, "type": "TASK", "confidence": 0.8140817582607269}, {"text": "paralleliz-able arc prediction", "start_pos": 62, "end_pos": 92, "type": "TASK", "confidence": 0.6758289734522501}]}, {"text": "In experiments, we show that the proposed model outperforms an off-the-shelf Stanford Chinese parser, as well as a strong MaltParser baseline that is trained on the same in-domain data.", "labels": [], "entities": [{"text": "MaltParser baseline", "start_pos": 122, "end_pos": 141, "type": "DATASET", "confidence": 0.9276259541511536}]}], "introductionContent": [{"text": "Weibo, in particular Sina Weibo 1 , has attracted more than 30% of Internet users (), making it one of the most popular social media services in the world.", "labels": [], "entities": [{"text": "Sina Weibo 1", "start_pos": 21, "end_pos": 33, "type": "DATASET", "confidence": 0.8090797861417135}]}, {"text": "While Weibo posts are abundantly available, NLP techniques for analyzing Weibo posts have not been well-studied in the past.", "labels": [], "entities": []}, {"text": "Syntactic analysis of Weibo is made difficult for three reasons: first, in the last few decades, Computational Linguistics researchers have primarily focused on building resources and tools using standard English newswire corpora 2 , and thus, there are fewer resources in other languages in general.", "labels": [], "entities": [{"text": "Syntactic analysis of Weibo", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.8824828267097473}]}, {"text": "Second, microblog posts are typically short, noisy, and can be considered as a \"dialect\", which is very different from news data.", "labels": [], "entities": []}, {"text": "Due to the differences in genre, part-of-speech taggers and parsers trained on newswire corpora typically fail on social media texts.", "labels": [], "entities": [{"text": "part-of-speech taggers", "start_pos": 33, "end_pos": 55, "type": "TASK", "confidence": 0.7140432447195053}]}, {"text": "Third, most existing parsers use languageindependent standard features (), and these features may not be optimal for Chinese.", "labels": [], "entities": []}, {"text": "To most of the application developers, the parser is more like a blackbox, which is not directly programmable.", "labels": [], "entities": []}, {"text": "Therefore, it is non-trivial to adapt these generic parsers to language-specific social media text.", "labels": [], "entities": []}, {"text": "In this paper, we present anew probabilistic dependency parsing approach for Weibo, with the following contributions: \u2022 We present a freely available Chinese Weibo dependency treebank 3 , manually annotated with more than 18,000 tokens; \u2022 We introduce a novel probabilistic logic programming approach for dependency arc prediction, making the parser directly programmable for theory engineering; \u2022 We show that the proposed approach outperforms an off-the-shelf dependency parser, as well as a strong baseline trained on the same in-domain data.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 45, "end_pos": 63, "type": "TASK", "confidence": 0.7312025129795074}, {"text": "Chinese Weibo dependency treebank 3", "start_pos": 150, "end_pos": 185, "type": "DATASET", "confidence": 0.7942319989204407}, {"text": "dependency arc prediction", "start_pos": 305, "end_pos": 330, "type": "TASK", "confidence": 0.822135845820109}]}, {"text": "In the next section, we describe existing work on dependency parsing for Chinese.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 50, "end_pos": 68, "type": "TASK", "confidence": 0.8575277030467987}]}, {"text": "In Section 3, we present the new Chinese Weibo Treebank to the research community.", "labels": [], "entities": [{"text": "Chinese Weibo Treebank", "start_pos": 33, "end_pos": 55, "type": "DATASET", "confidence": 0.887061874071757}]}, {"text": "In Section 4, we introduce the proposed efficient probabilistic programming approach for parsing Weibo.", "labels": [], "entities": [{"text": "parsing Weibo", "start_pos": 89, "end_pos": 102, "type": "TASK", "confidence": 0.7480107247829437}]}, {"text": "We show the experimental results in Section 5, and conclude in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this experiment, we compare the proposed parser with two well-known baselines.", "labels": [], "entities": []}, {"text": "First, we compare with an off-the-shelf Stanford Chinese Parser (.", "labels": [], "entities": [{"text": "Stanford Chinese Parser", "start_pos": 40, "end_pos": 63, "type": "DATASET", "confidence": 0.907372236251831}]}, {"text": "Second, we compare with the MaltParser () that is trained on the same in-domain Weibo dataset.", "labels": [], "entities": [{"text": "Weibo dataset", "start_pos": 80, "end_pos": 93, "type": "DATASET", "confidence": 0.9772332608699799}]}, {"text": "The train, development, and test splits are described in Section 3.", "labels": [], "entities": []}, {"text": "We tune the regularization hyperparameters of the models on the dev.", "labels": [], "entities": []}, {"text": "set, and report Unlabeled Attachment Score (UAS) results for both the dev.", "labels": [], "entities": [{"text": "Unlabeled Attachment Score (UAS)", "start_pos": 16, "end_pos": 48, "type": "METRIC", "confidence": 0.8392946322758993}]}, {"text": "set and the hold-out test set.", "labels": [], "entities": []}, {"text": "We experiment with the bilexical and bi-POS firstorder logic theory separately, as well as a combined full model with directional and distance features.", "labels": [], "entities": []}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "We see that both of the two attached pre-trained models from the Stanford parser do not perform very well on this Weibo dataset, probably because of the mismatched training and test data.", "labels": [], "entities": [{"text": "Weibo dataset", "start_pos": 114, "end_pos": 127, "type": "DATASET", "confidence": 0.9767098128795624}]}, {"text": "MaltParser is widely considered as one of the most popular dependency parsers, not only because of its speed, but also the acclaimed accuracy.", "labels": [], "entities": [{"text": "MaltParser", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.9379990696907043}, {"text": "accuracy", "start_pos": 133, "end_pos": 141, "type": "METRIC", "confidence": 0.999186098575592}]}, {"text": "We see that when using the full model, the UAS results between our methods and MaltParser are very similar on the development set, but both of our approaches outperform the Maltparser in the holdout test set.", "labels": [], "entities": [{"text": "UAS", "start_pos": 43, "end_pos": 46, "type": "METRIC", "confidence": 0.6998339891433716}]}, {"text": "The truncated tanh variant of ProPPR obtains the best UAS score of 0.675.", "labels": [], "entities": [{"text": "ProPPR", "start_pos": 30, "end_pos": 36, "type": "DATASET", "confidence": 0.8476665019989014}, {"text": "UAS", "start_pos": 54, "end_pos": 57, "type": "METRIC", "confidence": 0.9009605050086975}]}], "tableCaptions": [{"text": " Table 1: Comparing our Weibo parser to other  baselines (UAS). The off-the-shelf Stanford parser  uses its attached Xinhua and Chinese factored  models, which are trained on external Chinese  treebank of newswire data. MaltParser was trained  on the same in-domain data as our proposed ap- proach. * indicates p < .001 comparing to the  MaltParser.", "labels": [], "entities": []}]}