{"title": [{"text": "Automatic Generation of Related Work Sections in Scientific Papers: An Optimization Approach", "labels": [], "entities": [{"text": "Automatic Generation of Related Work Sections", "start_pos": 0, "end_pos": 45, "type": "TASK", "confidence": 0.8260254164536794}]}], "abstractContent": [{"text": "In this paper, we investigate a challenging task of automatic related work generation.", "labels": [], "entities": [{"text": "automatic related work generation", "start_pos": 52, "end_pos": 85, "type": "TASK", "confidence": 0.6185878366231918}]}, {"text": "Given multiple reference papers as input, the task aims to generate a related work section fora target paper.", "labels": [], "entities": []}, {"text": "The generated related work section can be used as a draft for the author to complete his or her final related work section.", "labels": [], "entities": []}, {"text": "We propose our Automatic Related Work Generation system called ARWG to address this task.", "labels": [], "entities": [{"text": "Automatic Related Work Generation", "start_pos": 15, "end_pos": 48, "type": "TASK", "confidence": 0.6008656695485115}, {"text": "ARWG", "start_pos": 63, "end_pos": 67, "type": "DATASET", "confidence": 0.7872702479362488}]}, {"text": "It first exploits a PLSA model to split the sentence set of the given papers into different topic-biased parts, and then applies regression models to learn the importance of the sentences.", "labels": [], "entities": []}, {"text": "At last it employs an optimization framework to generate the related work section.", "labels": [], "entities": []}, {"text": "Our evaluation results on a test set of 150 target papers along with their reference papers show that our proposed ARWG system can generate related work sections with better quality.", "labels": [], "entities": []}, {"text": "A user study is also performed to show ARWG can achieve an improvement over generic multi-document summarization baselines.", "labels": [], "entities": []}], "introductionContent": [{"text": "The related work section is an important part of a paper.", "labels": [], "entities": []}, {"text": "An author often needs to help readers to understand the context of his or her research problem and compare his or her current work with previous works.", "labels": [], "entities": []}, {"text": "A related work section is often used for this purpose to show the differences and advantages of his or her work, compared with related research works.", "labels": [], "entities": []}, {"text": "In this study, we attempt to automatically generate a related work section fora target academic paper with its reference papers.", "labels": [], "entities": []}, {"text": "This kind of related work sections can be used as a basis to reduce the author's time and effort when he or she wants to complete his or her final related work section.", "labels": [], "entities": []}, {"text": "Automatic related work section generation is a very challenging task.", "labels": [], "entities": [{"text": "Automatic related work section generation", "start_pos": 0, "end_pos": 41, "type": "TASK", "confidence": 0.5812567114830017}]}, {"text": "It can be considered a topic-biased, multiple-document summarization problem.", "labels": [], "entities": [{"text": "multiple-document summarization", "start_pos": 37, "end_pos": 68, "type": "TASK", "confidence": 0.5932170152664185}]}, {"text": "The input is a target academic paper, which has no related work section, along with its reference papers.", "labels": [], "entities": []}, {"text": "The goal is to create a related work section that describes the related works and addresses the relationship between the target paper and the reference papers.", "labels": [], "entities": []}, {"text": "Here we assume that the set of reference papers has been given as part of the input.", "labels": [], "entities": []}, {"text": "Existing works in the NLP and recommendation systems communities have already focused on the task of finding reference papers.", "labels": [], "entities": []}, {"text": "For example, citation prediction aims at finding individual paper citation patterns.", "labels": [], "entities": [{"text": "citation prediction", "start_pos": 13, "end_pos": 32, "type": "TASK", "confidence": 0.9789279401302338}]}, {"text": "Generally speaking, automatic related work section generation is a strikingly different problem and it is much more difficult in comparison with general multi-document summarization tasks.", "labels": [], "entities": [{"text": "automatic related work section generation", "start_pos": 20, "end_pos": 61, "type": "TASK", "confidence": 0.5681865632534027}, {"text": "multi-document summarization", "start_pos": 153, "end_pos": 181, "type": "TASK", "confidence": 0.5488241612911224}]}, {"text": "For example, multi-document summarization of news articles aims at synthesizing contents of similar news and removing the redundant information contained by the different news articles.", "labels": [], "entities": [{"text": "multi-document summarization of news articles", "start_pos": 13, "end_pos": 58, "type": "TASK", "confidence": 0.8205323934555053}]}, {"text": "However, each scientific paper has much specific content to state its own work and contribution.", "labels": [], "entities": []}, {"text": "Even for the papers that investigate the same research topic, their contributions and contents can be totally different.", "labels": [], "entities": []}, {"text": "The related work section generation task needs to find the specific contributions of individual papers and arrange them into one or several paragraphs.", "labels": [], "entities": [{"text": "work section generation", "start_pos": 12, "end_pos": 35, "type": "TASK", "confidence": 0.7522132992744446}]}, {"text": "In this study, we focus on the problem of automatic related work section generation and propose a novel system called ARWG to address the problem.", "labels": [], "entities": [{"text": "automatic related work section generation", "start_pos": 42, "end_pos": 83, "type": "TASK", "confidence": 0.5991757452487946}, {"text": "ARWG", "start_pos": 118, "end_pos": 122, "type": "DATASET", "confidence": 0.6167721152305603}]}, {"text": "For the target paper, we assume that the abstract and introduction sections have already been written by the author and they can be used to help generate the related work section.", "labels": [], "entities": []}, {"text": "For the reference papers, we only consider and extract the abstract, introduction, related work and conclusion sections, because other sections like the method and evaluation sections always describe the extreme details of the specific work and they are not suitable for this task.", "labels": [], "entities": []}, {"text": "Then we generate the related work section using both sentence sets which are extracted from the target paper and reference papers, respectively.", "labels": [], "entities": []}, {"text": "Firstly, we use a PLSA model to group both sentence sets of the target paper and its reference papers into different topic-biased clusters.", "labels": [], "entities": []}, {"text": "Secondly, the importance of each sentence in the target paper and the reference papers is learned by using two different Support Vector Regression (SVR) models.", "labels": [], "entities": []}, {"text": "At last, a global optimization framework is proposed to generate the related work section by selecting sentences from both the target paper and the reference papers.", "labels": [], "entities": []}, {"text": "Meanwhile, the framework selects sentences from different topic-biased clusters globally.", "labels": [], "entities": []}, {"text": "Experimental results on a test set of 150 target papers show our method can generate related work sections with better quality than those of several baseline methods.", "labels": [], "entities": []}, {"text": "With the ROUGE toolkit, the results indicate the related work sections generated by our system can get higher ROUGE scores.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 110, "end_pos": 115, "type": "METRIC", "confidence": 0.9614732265472412}]}, {"text": "Moreover, our related work sections can get higher rating scores based on a user study.", "labels": [], "entities": []}, {"text": "Therefore, our related work sections can be much more suitable for the authors to prepare their final related work sections.", "labels": [], "entities": []}], "datasetContent": [{"text": "To setup our experiments, we divide our dataset which contains 1050 target papers and their reference papers into two parts: 700 target papers for training, 150 papers for test and the other 200 papers for validation.", "labels": [], "entities": []}, {"text": "The PLSA topic model is applied to the whole dataset.", "labels": [], "entities": []}, {"text": "We train two SVR regression models based on the own work part and the previous work part of the training data and apply the models to the test data.", "labels": [], "entities": []}, {"text": "The global optimization framework is used to generate the related work sections.", "labels": [], "entities": []}, {"text": "We set the maximum word count of the generated related work section to be equal to that of the gold related work section.", "labels": [], "entities": []}, {"text": "The parameter values of \u00ed \u00b5\u00ed\u00bc\u0086 1 , \u00ed \u00b5\u00ed\u00bc\u0086 2 and \u00ed \u00b5\u00ed\u00bc\u0086 3 are set to 0.3, 0.1 and 0.6, respectively.", "labels": [], "entities": []}, {"text": "The parameter values are tuned on the validation data.", "labels": [], "entities": []}, {"text": "We compare our system with five baseline systems: MEAD-WT, LexRank-WT, ARWG-WT, MEAD and LexRank.", "labels": [], "entities": [{"text": "MEAD-WT", "start_pos": 50, "end_pos": 57, "type": "METRIC", "confidence": 0.5924195647239685}, {"text": "LexRank-WT", "start_pos": 59, "end_pos": 69, "type": "DATASET", "confidence": 0.9504006505012512}, {"text": "ARWG-WT", "start_pos": 71, "end_pos": 78, "type": "DATASET", "confidence": 0.7380094528198242}, {"text": "MEAD", "start_pos": 80, "end_pos": 84, "type": "DATASET", "confidence": 0.6993514895439148}, {"text": "LexRank", "start_pos": 89, "end_pos": 96, "type": "DATASET", "confidence": 0.9790126085281372}]}, {"text": "MEAD 6 ( ) is an open-source extractive multidocument summarizer.", "labels": [], "entities": [{"text": "MEAD 6", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.8240674138069153}]}, {"text": "LexRank 7 () is a multi-document summarization system which is based on a random walk on the similarity graph of sentences.", "labels": [], "entities": [{"text": "LexRank 7", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.9458497166633606}]}, {"text": "We also implement the MEAD, LexRank baselines and our method with only the reference papers (i.e. the target paper's content is not considered).", "labels": [], "entities": [{"text": "MEAD", "start_pos": 22, "end_pos": 26, "type": "METRIC", "confidence": 0.6529446244239807}, {"text": "LexRank baselines", "start_pos": 28, "end_pos": 45, "type": "DATASET", "confidence": 0.9385536909103394}]}, {"text": "Those methods are signed by \"-WT\".", "labels": [], "entities": [{"text": "WT", "start_pos": 30, "end_pos": 32, "type": "METRIC", "confidence": 0.7285920977592468}]}, {"text": "To evaluate the effectiveness of the SVR models we employ, we implement a baseline system RWGOF that uses the random walk scores as the important scores of the sentences and take the scores as inputs for the same global optimization framework as our system to generate the related work section.", "labels": [], "entities": []}, {"text": "The random walk scores are computed for the sentences in the reference papers and the target paper, respectively.", "labels": [], "entities": []}, {"text": "We use the ROUGE toolkit to evaluate the content quality of the generated related work sections.", "labels": [], "entities": []}, {"text": "ROUGE) is a widely used automatic summarization evaluation method based on n-gram comparison.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.8815950751304626}, {"text": "summarization evaluation", "start_pos": 34, "end_pos": 58, "type": "TASK", "confidence": 0.8943520188331604}]}, {"text": "Here, we use the FMeasure scores of ROUGE-1, ROUGE-2 and ROUGE-SU4.", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 36, "end_pos": 43, "type": "METRIC", "confidence": 0.8998661637306213}, {"text": "ROUGE-2", "start_pos": 45, "end_pos": 52, "type": "METRIC", "confidence": 0.8592879772186279}, {"text": "ROUGE-SU4", "start_pos": 57, "end_pos": 66, "type": "METRIC", "confidence": 0.6495581269264221}]}, {"text": "The model texts are set as the gold related work sections extracted from the target papers, and word stemming is utilized.", "labels": [], "entities": [{"text": "word stemming", "start_pos": 96, "end_pos": 109, "type": "TASK", "confidence": 0.7345258146524429}]}, {"text": "ROUGE-N is an n-gram based measure between a candidate text and a reference text.", "labels": [], "entities": [{"text": "ROUGE-N", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.9625195264816284}]}, {"text": "The recall oriented score, the precision oriented score and the F-measure score for ROUGE-N are computed as follows: where n stands for the length of the n-gram \u00ed \u00b5\u00ed\u00b1\u0094\u00ed \u00b5\u00ed\u00b1\u009f\u00ed \u00b5\u00ed\u00b1\u008e\u00ed \u00b5\u00ed\u00b1\u009a \u00ed \u00b5\u00ed\u00b1\u009b , and \u00ed \u00b5\u00ed\u00b0 \u00b6\u00ed \u00b5\u00ed\u00b1\u009c\u00ed \u00b5\u00ed\u00b1\u00a2\u00ed \u00b5\u00ed\u00b1\u009b\u00ed \u00b5\u00ed\u00b1\u00a1 \u00ed \u00b5\u00ed\u00b1\u009a\u00ed \u00b5\u00ed\u00b1\u008e\u00ed \u00b5\u00ed\u00b1\u00a1\u00ed \u00b5\u00ed\u00b1\u0090\u210e (\u00ed \u00b5\u00ed\u00b1\u0094\u00ed \u00b5\u00ed\u00b1\u009f\u00ed \u00b5\u00ed\u00b1\u008e\u00ed \u00b5\u00ed\u00b1\u009a \u00ed \u00b5\u00ed\u00b1\u009b ) is the maximum number of n-grams co-occurring in a candidate text and a reference text.", "labels": [], "entities": [{"text": "recall oriented score", "start_pos": 4, "end_pos": 25, "type": "METRIC", "confidence": 0.9693451921145121}, {"text": "precision oriented score", "start_pos": 31, "end_pos": 55, "type": "METRIC", "confidence": 0.9569916923840841}, {"text": "F-measure score", "start_pos": 64, "end_pos": 79, "type": "METRIC", "confidence": 0.9704551100730896}, {"text": "ROUGE-N", "start_pos": 84, "end_pos": 91, "type": "DATASET", "confidence": 0.4647354483604431}]}, {"text": "In addition, we conducted a user study to subjectively evaluate the related work sections to get more evidences.", "labels": [], "entities": []}, {"text": "We selected the related work sections generated by different methods for 15 random target papers in the test set.", "labels": [], "entities": []}, {"text": "We asked three human judges to follow an evaluation guideline we design and evaluate these related work sections.", "labels": [], "entities": []}, {"text": "The human judges are graduate students in the computer science field and they did not know the identities of the evaluated related work sections.", "labels": [], "entities": []}, {"text": "They were asked to give a rating on a scale of 1 (very poor) to 5 (very good) for the correctness, readability and usefulness of the related work sections, respectively: 1) Correctness: Is the related work section actually related to the target paper?", "labels": [], "entities": []}, {"text": "2) Readability: Is the related work section easy for the readers to read and grasp the key content?", "labels": [], "entities": []}, {"text": "3) Usefulness: Is the related work section useful for the author to prepare their final related work section?", "labels": [], "entities": []}, {"text": "Paired T-Tests are applied to both the ROUGE scores and rating scores for comparing ARWG and baselines and comparing the systems with WT and without WT.", "labels": [], "entities": [{"text": "Paired T-Tests", "start_pos": 0, "end_pos": 14, "type": "METRIC", "confidence": 0.9035716652870178}, {"text": "ROUGE", "start_pos": 39, "end_pos": 44, "type": "METRIC", "confidence": 0.9856591820716858}, {"text": "ARWG", "start_pos": 84, "end_pos": 88, "type": "DATASET", "confidence": 0.7302437424659729}, {"text": "WT", "start_pos": 134, "end_pos": 136, "type": "DATASET", "confidence": 0.7385159730911255}]}, {"text": "The evaluation results over ROUGE metrics are presented in.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 28, "end_pos": 33, "type": "METRIC", "confidence": 0.8104424476623535}]}, {"text": "It shows that our proposed system can get higher ROUGE scores, i.e., better content quality.", "labels": [], "entities": [{"text": "ROUGE scores", "start_pos": 49, "end_pos": 61, "type": "METRIC", "confidence": 0.9748111963272095}]}, {"text": "In our system, we split the sentence set into different topic-biased parts, and the importance scores of sentences in the target paper and reference papers are learned differently.", "labels": [], "entities": []}, {"text": "So the obtained importance scores of the sentences are more reliable.", "labels": [], "entities": []}, {"text": "The global optimization framework considers the extraction of both the previous work part and the own work part.", "labels": [], "entities": [{"text": "global optimization", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.7222048342227936}]}, {"text": "We can seethe importance of the own work part by comparing the results of the methods with or without considering the own work part.", "labels": [], "entities": []}, {"text": "MEAD, LexRank and our method all get a significant improvement after considering the own work part by extracting sentences from the target paper.", "labels": [], "entities": [{"text": "MEAD", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.5905694961547852}, {"text": "LexRank", "start_pos": 6, "end_pos": 13, "type": "DATASET", "confidence": 0.9498933553695679}]}, {"text": "The results also prove our assumption about the related work section structure.", "labels": [], "entities": []}, {"text": "presents the fluctuation of ROUGE scores when tuning the parameters \u03bb 1 , \u03bb 2 and \u03bb 3 . We can see our method generally performs better than the baselines.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 28, "end_pos": 33, "type": "METRIC", "confidence": 0.9858479499816895}]}, {"text": "All the three parts in the objective function are useful to generate related work sections with good quality.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: ROUGE F-measure comparison results", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9792180061340332}, {"text": "F-measure", "start_pos": 16, "end_pos": 25, "type": "METRIC", "confidence": 0.5621688365936279}]}, {"text": " Table 5: Average rating scores of judges", "labels": [], "entities": [{"text": "Average rating scores", "start_pos": 10, "end_pos": 31, "type": "METRIC", "confidence": 0.9498112996419271}]}, {"text": " Table 6: ROUGE F-measure comparison of dif- ferent sentence importance scores", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9864726066589355}, {"text": "F-measure", "start_pos": 16, "end_pos": 25, "type": "METRIC", "confidence": 0.8233515024185181}]}]}