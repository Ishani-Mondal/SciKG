{"title": [{"text": "Combining Distant and Partial Supervision for Relation Extraction", "labels": [], "entities": [{"text": "Combining Distant and Partial Supervision", "start_pos": 0, "end_pos": 41, "type": "TASK", "confidence": 0.6192554354667663}, {"text": "Relation Extraction", "start_pos": 46, "end_pos": 65, "type": "TASK", "confidence": 0.9731827974319458}]}], "abstractContent": [{"text": "Broad-coverage relation extraction either requires expensive supervised training data, or suffers from drawbacks inherent to distant supervision.", "labels": [], "entities": [{"text": "Broad-coverage relation extraction", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.7450294097264608}]}, {"text": "We present an approach for providing partial supervision to a distantly supervised relation extrac-tor using a small number of carefully selected examples.", "labels": [], "entities": []}, {"text": "We compare against established active learning criteria and propose a novel criterion to sample examples which are both uncertain and representative.", "labels": [], "entities": []}, {"text": "In this way, we combine the benefits of fine-grained supervision for difficult examples with the coverage of a large distantly supervised corpus.", "labels": [], "entities": []}, {"text": "Our approach gives a substantial increase of 3.9% end-to-end F 1 on the 2013 KBP Slot Filling evaluation, yielding a net F 1 of 37.7%.", "labels": [], "entities": [{"text": "end-to-end", "start_pos": 50, "end_pos": 60, "type": "METRIC", "confidence": 0.9795476198196411}, {"text": "F 1", "start_pos": 61, "end_pos": 64, "type": "METRIC", "confidence": 0.8845541179180145}, {"text": "2013 KBP Slot Filling evaluation", "start_pos": 72, "end_pos": 104, "type": "DATASET", "confidence": 0.75926833152771}, {"text": "F 1", "start_pos": 121, "end_pos": 124, "type": "METRIC", "confidence": 0.9974262714385986}]}], "introductionContent": [{"text": "Fully supervised relation extractors are limited to relatively small training sets.", "labels": [], "entities": [{"text": "relation extractors", "start_pos": 17, "end_pos": 36, "type": "TASK", "confidence": 0.7245004326105118}]}, {"text": "While able to make use of much more data, distantly supervised approaches either make dubious assumptions in order to simulate fully supervised data, or make use of latent-variable methods which get stuck in local optima easily.", "labels": [], "entities": []}, {"text": "We hope to combine the benefits of supervised and distantly supervised methods by annotating a small subset of the available data using selection criteria inspired by active learning.", "labels": [], "entities": []}, {"text": "To illustrate, our training corpus contains 1 208 524 relation mentions; annotating all of these mentions fora fully supervised classifier, at an average of $0.13 per annotation, would cost approximately $160 000.", "labels": [], "entities": []}, {"text": "Distant supervision allows us to make use of this large corpus without requiring costly annotation.", "labels": [], "entities": []}, {"text": "The traditional approach is based on the assumption that every mention of an entity pair (e.g., Obama and USA) participates in the known relation between the two (i.e., born in).", "labels": [], "entities": []}, {"text": "However, this introduces noise, as not every mention expresses the relation we are assigning to it.", "labels": [], "entities": []}, {"text": "We show that by providing annotations for only 10 000 informative examples, combined with a large corpus of distantly labeled data, we can yield notable improvements in performance over the distantly supervised data alone.", "labels": [], "entities": []}, {"text": "We report results on three criteria for selecting examples to annotate: a baseline of sampling examples uniformly at random, an established active learning criterion, and anew metric incorporating both the uncertainty and the representativeness of an example.", "labels": [], "entities": []}, {"text": "We show that the choice of metric is important -yielding as much as a 3% F 1 difference -and that our new proposed criterion outperforms the standard method in many cases.", "labels": [], "entities": [{"text": "F 1 difference", "start_pos": 73, "end_pos": 87, "type": "METRIC", "confidence": 0.976507306098938}]}, {"text": "Lastly, we train a supervised classifier on these collected examples, and report performance comparable to distantly supervised methods.", "labels": [], "entities": []}, {"text": "Furthermore, we notice that initializing the distantly supervised model using this supervised classifier is critical for obtaining performance improvements.", "labels": [], "entities": []}, {"text": "This work makes a number of concrete contributions.", "labels": [], "entities": []}, {"text": "We propose a novel application of active learning techniques to distantly supervised relation extraction.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 85, "end_pos": 104, "type": "TASK", "confidence": 0.739584431052208}]}, {"text": "To the best of the authors knowledge, we are the first to apply active learning to the class of latent-variable distantly supervised models presented in this paper.", "labels": [], "entities": []}, {"text": "We show that annotating a proportionally small number of examples yields improvements in end-to-end accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 100, "end_pos": 108, "type": "METRIC", "confidence": 0.8808355331420898}]}, {"text": "We compare various selection criteria, and show that this decision has a notable impact on the gain in performance.", "labels": [], "entities": []}, {"text": "In many ways this reconciles our results with the negative results of, who show limited gains from na\u00a8\u0131velyna\u00a8\u0131vely annotating examples.", "labels": [], "entities": []}, {"text": "Lastly, we make our annotations available to the research community.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate the three high-level research contributions of this work: we show that we improve the accuracy of MIML-RE, we validate the effectiveness of our selection criteria, and we provide a corpus of annotated examples, evaluating a supervised classifier trained on this corpus.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 98, "end_pos": 106, "type": "METRIC", "confidence": 0.9990239143371582}, {"text": "MIML-RE", "start_pos": 110, "end_pos": 117, "type": "TASK", "confidence": 0.6205698847770691}]}, {"text": "The training and testing methodology for evaluating these contributions is given in Sections 6.1 and 6.2; experiments are given in Section 6.3.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: A summary of results on the end-to-end KBP 2013 evaluation for various experiments. The  first column denotes the algorithm used: either traditional distant supervision (Mintz++), MIML-RE, or  a supervised classifier. In the case of MIML-RE, the model may be initialized either using Mintz++, or  the corresponding supervised classifier (the \"Not Used\" column is initialized with the \"All\" supervised  classifier). One of five active learning scenarios are evaluated: no annotated examples provided, the three  active learning criteria, and all available examples used. The entry in blue denotes the basic MIML-RE  model; entries in gray perform worse than this model. The bold items denote the best performance  among selection criteria.", "labels": [], "entities": [{"text": "KBP 2013 evaluation", "start_pos": 49, "end_pos": 68, "type": "DATASET", "confidence": 0.8480539321899414}]}, {"text": " Table 2: A summary of improvements to MIML- RE on the end-to-end slotfilling task, copied from", "labels": [], "entities": [{"text": "MIML- RE", "start_pos": 39, "end_pos": 47, "type": "TASK", "confidence": 0.6239074468612671}]}, {"text": " Table 1. Mintz++ is the traditional distantly su- pervised model. The second row corresponds to  the unmodified MIML-RE model. The third row  corresponds to MIML-RE initialized with a su- pervised classifier (trained on all examples). The  fourth row is MIML-RE with annotated exam- ples incorporated during training (but not initial- ization). The last row shows the best results ob- tained by our model.", "labels": [], "entities": []}, {"text": " Table 3: A summary of the performance of each  example selection criterion. In each case, the  model was initialized with a supervised classifier.  The first row corresponds to the MIML-RE model  initialized with a supervised classifier. The middle  three rows show performance for the three selec- tion criteria, used both for initialization and during  training. The last row shows results if all available  annotations are used, independent of their source.", "labels": [], "entities": []}, {"text": " Table 4: A comparison of the best performing su- pervised classifier with other systems. The top  section compares the supervised classifier with  prior work. The lower section highlights the im- provements gained from initializing MIML-RE  with a supervised classifier.", "labels": [], "entities": []}]}