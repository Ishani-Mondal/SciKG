{"title": [{"text": "Learning Abstract Concept Embeddings from Multi-Modal Data: Since You Probably Can't See What I Mean", "labels": [], "entities": []}], "abstractContent": [{"text": "Models that acquire semantic representations from both linguistic and perceptual input are of interest to researchers in NLP because of the obvious parallels with human language learning.", "labels": [], "entities": []}, {"text": "Performance advantages of the multi-modal approach over language-only models have been clearly established when models are required to learn concrete noun concepts.", "labels": [], "entities": []}, {"text": "However, such concepts are comparatively rare in everyday language.", "labels": [], "entities": []}, {"text": "In this work, we present anew means of extending the scope of multi-modal models to more commonly-occurring abstract lexical concepts via an approach that learns multi-modal embeddings.", "labels": [], "entities": []}, {"text": "Our architecture out-performs previous approaches in combining input from distinct modalities, and propagates perceptual information on concrete concepts to abstract concepts more effectively than alternatives.", "labels": [], "entities": []}, {"text": "We discuss the implications of our results both for optimizing the performance of multi-modal models and for theories of abstract conceptual representation.", "labels": [], "entities": []}], "introductionContent": [{"text": "Multi-modal models that learn semantic representations from both language and information about the perceptible properties of concepts were originally motivated by parallels with human word learning () and evidence that many concepts are grounded in perception).", "labels": [], "entities": []}, {"text": "The perceptual information in such models is generally mined directly from images) or from data collected in psychological studies.", "labels": [], "entities": []}, {"text": "By exploiting the additional information encoded in perceptual input, multi-modal models can outperform language-only models on a range of semantic NLP tasks, including modelling similarity () and free association, predicting compositionality and concept categorization.", "labels": [], "entities": [{"text": "predicting compositionality", "start_pos": 215, "end_pos": 242, "type": "TASK", "confidence": 0.9430229067802429}]}, {"text": "However, to date, these previous approaches to multi-modal concept learning focus on concrete words such as cat or dog, rather than abstract concepts, such as curiosity or loyalty.", "labels": [], "entities": [{"text": "multi-modal concept learning", "start_pos": 47, "end_pos": 75, "type": "TASK", "confidence": 0.6137135326862335}]}, {"text": "However, differences between abstract and concrete processing and representation suggest that conclusions about concrete concept learning may not necessarily hold in the general case.", "labels": [], "entities": []}, {"text": "In this paper, we therefore focus on multi-modal models for learning both abstract and concrete concepts.", "labels": [], "entities": []}, {"text": "Although concrete concepts might seem more basic or fundamental, the vast majority of openclass, meaning-bearing words in everyday language are in fact abstract.", "labels": [], "entities": []}, {"text": "72% of the noun or verb tokens in the British National Corpus ( are rated by human judges 1 as more abstract than the noun war, for instance, a concept many would already consider to be quite abstract.", "labels": [], "entities": [{"text": "British National Corpus", "start_pos": 38, "end_pos": 61, "type": "DATASET", "confidence": 0.9409961501757304}]}, {"text": "Moreover, abstract concepts by definition encode higher-level (more general) principles than concrete concepts, which typically reside naturally in a single semantic category or domain (Crutch and).", "labels": [], "entities": []}, {"text": "It is therefore likely that abstract representations may prove highly applicable for multi-task, multi-domain or transfer learning models, which aim to acquire 'general-purpose' conceptual knowledge without reference to a specific objective or task.", "labels": [], "entities": []}, {"text": "Ina recent paper,  investigate whether the multi-modal models cited above are effective for learning concepts other than concrete nouns.", "labels": [], "entities": []}, {"text": "They observe that representations of certain abstract concepts can indeed be enhanced in multi-modal models by combining perceptual and linguistic input with an information propagation step.", "labels": [], "entities": []}, {"text": "propose ridge regression as an alternative to the nearest-neighbour averaging proposed by for such propagation, and show that it is more robust to changes in the type of concept to be learned.", "labels": [], "entities": [{"text": "ridge regression", "start_pos": 8, "end_pos": 24, "type": "TASK", "confidence": 0.8573614656925201}]}, {"text": "However, both methods are somewhat inelegant, in that they learn separate linguistic and 'pseudo-perceptual' representations, which must be combined via a separate information combination step.", "labels": [], "entities": []}, {"text": "Moreover, for the majority of abstract concepts, the best performing multi-modal model employing these techniques remains less effective than conventional text-only representation learning model.", "labels": [], "entities": []}, {"text": "Motivated by these observations, we introduce an architecture for learning both abstract and concrete representations that generalizes the skipgram model of from text-based to multi-modal learning.", "labels": [], "entities": []}, {"text": "Aspects of the model design are influenced by considering the process of human language learning.", "labels": [], "entities": []}, {"text": "The model moderates the training input to include more perceptual information about commonly-occurring concrete concepts and less information about rarer concepts.", "labels": [], "entities": []}, {"text": "Moreover, it integrates the processes of combining perceptual and linguistic input and propagating information from concrete to abstract concepts into a single representation update process based on back-propagation.", "labels": [], "entities": []}, {"text": "We train our model on running-text language and two sources of perceptual descriptors for concrete nouns: the ESPGame dataset of annotated images) and the CSLB set of concept property norms).", "labels": [], "entities": [{"text": "ESPGame dataset", "start_pos": 110, "end_pos": 125, "type": "DATASET", "confidence": 0.8651204705238342}, {"text": "CSLB set of concept property norms", "start_pos": 155, "end_pos": 189, "type": "DATASET", "confidence": 0.8866323530673981}]}, {"text": "We find that our model combines information from the different modalities more effectively than previous methods, resulting in an improved ability to model the USF free association gold standard () for concrete nouns.", "labels": [], "entities": [{"text": "USF free association gold standard", "start_pos": 160, "end_pos": 194, "type": "DATASET", "confidence": 0.7696710824966431}]}, {"text": "In addition, the architecture propagates the extra-linguistic input for concrete nouns to improve representations of abstract concepts more effectively than alternative methods.", "labels": [], "entities": []}, {"text": "While this propagation can effectively extend the advantage of the multi-modal approach to many more concepts than simple concrete nouns, we observe that the benefit of adding perceptual input appears to decrease as target concepts become more abstract.", "labels": [], "entities": []}, {"text": "Indeed, for the most abstract concepts of all, language-only models still provide the most effective learning mechanism.", "labels": [], "entities": []}, {"text": "Finally, we investigate the optimum quantity and type of perceptual input for such models.", "labels": [], "entities": []}, {"text": "Between the most concrete concepts, which can be effectively represented directly in the perceptual modality, and the most abstract concepts, which cannot, we identify a set of concepts that cannot be represented effectively directly in the perceptual modality, but still benefit from perceptual input propagated in the model via concrete concepts.", "labels": [], "entities": []}, {"text": "The motivation in designing our model and experiments is both practical and theoretical.", "labels": [], "entities": []}, {"text": "Taken together, the empirical observations we present are potentially important for optimizing the learning of representations of concrete and abstract concepts in multi-modal models.", "labels": [], "entities": []}, {"text": "In addition, they offer a degree of insight into the poorly understood issue of how abstract concepts maybe encoded inhuman memory.", "labels": [], "entities": []}], "datasetContent": [{"text": "The ESP-Game dataset (ESP)) consists of 100,000 images, each annotated with a list of lexical concepts that appear in that image.", "labels": [], "entities": [{"text": "ESP-Game dataset (ESP))", "start_pos": 4, "end_pos": 27, "type": "DATASET", "confidence": 0.8477239847183228}]}, {"text": "For any concept w identified in an ESP image, we construct a corresponding bag of features b(w).", "labels": [], "entities": []}, {"text": "For each ESP image I that contains w, we append the other concept tokens identified in Ito b(w).", "labels": [], "entities": []}, {"text": "Thus, the more frequently a concept cooccurs with win images, the more its corresponding lexical token occurs in b(w).", "labels": [], "entities": []}, {"text": "The array P ESP in this case then consists of the (w, b(w)) pairs.", "labels": [], "entities": []}, {"text": "CSLB Property Norms The Centre for Speech, Language and the Brain norms (CSLB)) is a recently-released dataset containing semantic properties for 638 concrete concepts produced by human annotators.", "labels": [], "entities": [{"text": "CSLB Property Norms The Centre for Speech, Language and the Brain norms (CSLB))", "start_pos": 0, "end_pos": 79, "type": "DATASET", "confidence": 0.7882054224610329}]}, {"text": "The CSLB dataset was compiled in the same way as the property norms used widely in multimodal models; we use CSLB because it contains more concepts.", "labels": [], "entities": [{"text": "CSLB dataset", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.8894923627376556}]}, {"text": "For each concept, the proportion of the 30 annotators that produced a given feature can also be employed as a measure of the strength of that feature.", "labels": [], "entities": []}, {"text": "When encoding the CSLB data in P, we first map properties to lexical forms (e.g. is green becomes green).", "labels": [], "entities": [{"text": "CSLB data", "start_pos": 18, "end_pos": 27, "type": "DATASET", "confidence": 0.8059049844741821}]}, {"text": "By directly identifying perceptual features and linguistic forms in this way, we treat features observed in the perceptual data as (sub)concepts to be acquired via the same multi-modal input streams and stored in the same domain-general memory as the evaluation concepts.", "labels": [], "entities": []}, {"text": "This design decision in fact corresponds to a view of cognition that is sometimes disputed.", "labels": [], "entities": []}, {"text": "In future studies we hope to compare the present approach to architectures with domain-specific conceptual memories.", "labels": [], "entities": []}, {"text": "For each concept win CSLB, we then construct a feature bag b(w) by appending lexical forms to b(w) such that the count of each feature word is equal to the strength of that feature for w.", "labels": [], "entities": []}, {"text": "Thus, when features are sampled from b(w) to create pseudo-sentences (as detailed previously) the probability of a feature word occurring in a sentence reflects feature strength.", "labels": [], "entities": []}, {"text": "The array P CSLB then consists of all (w, b(w)) pairs.: Example concept pairs (with mean concreteness rating) and free-association scores from the USF dataset.", "labels": [], "entities": [{"text": "USF dataset", "start_pos": 147, "end_pos": 158, "type": "DATASET", "confidence": 0.9899383187294006}]}, {"text": "We evaluate the quality of representations by how well they reflect free association scores, an empirical measure of cognitive conceptual proximity.", "labels": [], "entities": []}, {"text": "on the intuition that information propagation may occur differently from noun to noun or from noun to verb (because of their distinct structural relationships in sentences).", "labels": [], "entities": [{"text": "information propagation", "start_pos": 22, "end_pos": 45, "type": "TASK", "confidence": 0.7216185927391052}]}, {"text": "POS-tags are not assigned as part of the USF data, so we draw the noun/verb distinction based on the majority POS-tag of USF concepts in the lemmatized British National Corpus (.", "labels": [], "entities": [{"text": "USF data", "start_pos": 41, "end_pos": 49, "type": "DATASET", "confidence": 0.9448523223400116}, {"text": "British National Corpus", "start_pos": 152, "end_pos": 175, "type": "DATASET", "confidence": 0.8446847200393677}]}, {"text": "The abstract/concrete and noun/verb dichotomies yield four distinct concept lists.", "labels": [], "entities": []}, {"text": "For consistency, the concrete noun list is filtered so that each concrete noun concept w has a perceptual representation b(w) in both P ESP and P CSLB . For the four resulting concept lists C (concrete/abstract, noun/verb), a corresponding set of evaluation pairs {(w 1 , w 2 ) \u2208 U SF : w 1 , w 2 \u2208 C} is extracted (see for details).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Concepts identified in images in the ESP  Game (left) and features produced for concepts by  human annotators in the CSLB dataset (with fea- ture strength, max=30).", "labels": [], "entities": [{"text": "ESP  Game", "start_pos": 47, "end_pos": 56, "type": "DATASET", "confidence": 0.8771977424621582}, {"text": "CSLB dataset", "start_pos": 127, "end_pos": 139, "type": "DATASET", "confidence": 0.9852015972137451}, {"text": "fea- ture strength", "start_pos": 146, "end_pos": 164, "type": "METRIC", "confidence": 0.9275077283382416}]}, {"text": " Table 2: Example concept pairs (with mean con- creteness rating) and free-association scores from  the USF dataset.", "labels": [], "entities": [{"text": "USF dataset", "start_pos": 104, "end_pos": 115, "type": "DATASET", "confidence": 0.989706426858902}]}]}