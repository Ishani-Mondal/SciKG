{"title": [{"text": "Neural Network Based Bilingual Language Model Growing for Statistical Machine Translation", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 58, "end_pos": 89, "type": "TASK", "confidence": 0.8282236655553182}]}], "abstractContent": [{"text": "Since larger n-gram Language Model (LM) usually performs better in Statistical Machine Translation (SMT), how to construct efficient large LM is an important topic in SMT.", "labels": [], "entities": [{"text": "Statistical Machine Translation (SMT)", "start_pos": 67, "end_pos": 104, "type": "TASK", "confidence": 0.8109422028064728}, {"text": "SMT", "start_pos": 167, "end_pos": 170, "type": "TASK", "confidence": 0.9934772253036499}]}, {"text": "However, most of the existing LM growing methods need an extra monolingual corpus, where additional LM adaption technology is necessary.", "labels": [], "entities": [{"text": "LM growing", "start_pos": 30, "end_pos": 40, "type": "TASK", "confidence": 0.9272074699401855}]}, {"text": "In this paper, we propose a novel neural network based bilingual LM growing method, only using the bilingual parallel corpus in SMT.", "labels": [], "entities": [{"text": "LM growing", "start_pos": 65, "end_pos": 75, "type": "TASK", "confidence": 0.8332730531692505}, {"text": "SMT", "start_pos": 128, "end_pos": 131, "type": "TASK", "confidence": 0.9568938612937927}]}, {"text": "The results show that our method can improve both the perplexity score for LM evaluation and BLEU score for SMT, and significantly outperforms the existing LM growing methods without extra corpus.", "labels": [], "entities": [{"text": "LM evaluation", "start_pos": 75, "end_pos": 88, "type": "TASK", "confidence": 0.8902463912963867}, {"text": "BLEU score", "start_pos": 93, "end_pos": 103, "type": "METRIC", "confidence": 0.9829080700874329}, {"text": "SMT", "start_pos": 108, "end_pos": 111, "type": "TASK", "confidence": 0.9921405911445618}]}], "introductionContent": [{"text": "'Language Model (LM) Growing' refers to adding n-grams outside the corpus together with their probabilities into the original LM.", "labels": [], "entities": []}, {"text": "This operation is useful as it can make LM perform better through letting it become larger and larger, by only using a small training corpus.", "labels": [], "entities": [{"text": "LM", "start_pos": 40, "end_pos": 42, "type": "TASK", "confidence": 0.9497725963592529}]}, {"text": "There are various methods for adding n-grams selected by different criteria from a monolingual corpus (.", "labels": [], "entities": []}, {"text": "However, all of these approaches need additional corpora.", "labels": [], "entities": []}, {"text": "Meanwhile the extra corpora from different domains will not result in better LMs Part of this work was done as Rui).", "labels": [], "entities": []}, {"text": "In addition, it is very difficult or even impossible to collect an extra large corpus for some special domains such as the TED corpus () or for some rare languages.", "labels": [], "entities": [{"text": "TED corpus", "start_pos": 123, "end_pos": 133, "type": "DATASET", "confidence": 0.8912368714809418}]}, {"text": "Therefore, to improve the performance of LMs, without assistance of extra corpus, is one of important research topics in SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 121, "end_pos": 124, "type": "TASK", "confidence": 0.9935176372528076}]}, {"text": "Recently, Continues Space Language Model (CSLM), especially Neural Network based Language Model (NNLM) (, is being actively used in SMT (.", "labels": [], "entities": [{"text": "SMT", "start_pos": 132, "end_pos": 135, "type": "TASK", "confidence": 0.9911762475967407}]}, {"text": "One of the main advantages of CSLM is that it can more accurately predict the probabilities of the n-grams, which are not in the training corpus.", "labels": [], "entities": []}, {"text": "However, in practice, CSLMs have not been widely used in the current SMT systems, due to their too high computational cost.", "labels": [], "entities": [{"text": "SMT", "start_pos": 69, "end_pos": 72, "type": "TASK", "confidence": 0.9921582937240601}]}, {"text": "Vaswani and colleagues propose a method for reducing the training cost of CSLM and apply it to SMT decoder.", "labels": [], "entities": [{"text": "SMT decoder", "start_pos": 95, "end_pos": 106, "type": "TASK", "confidence": 0.9237445294857025}]}, {"text": "However, they do not show their improvement for decoding speed, and their method is still slower than the n-gram LM.", "labels": [], "entities": []}, {"text": "There are several other methods for attempting to implement neural network based LM or translation model for SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 109, "end_pos": 112, "type": "TASK", "confidence": 0.9913195967674255}]}, {"text": "However, the decoding speed using n-gram LM is still state-ofthe-art one.", "labels": [], "entities": []}, {"text": "Some approaches calculate the probabilities of the n-grams n-grams before decoding, and store them in the n-gram format ().", "labels": [], "entities": []}, {"text": "The 'converted CSLM' can be directly used in SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 45, "end_pos": 48, "type": "TASK", "confidence": 0.9900013208389282}]}, {"text": "Though more n-grams which are not in the train-ing corpus can be generated by using some of these 'converting' methods, these methods only consider the monolingual information, and do not take the bilingual information into account.", "labels": [], "entities": []}, {"text": "We observe that the translation output of a phrase-based SMT system is concatenation of phrases from the phrase table, whose probabilities can be calculated by CSLM.", "labels": [], "entities": [{"text": "SMT", "start_pos": 57, "end_pos": 60, "type": "TASK", "confidence": 0.9014928936958313}]}, {"text": "Based on this observation, a novel neural network based bilingual LM growing method is proposed using the 'connecting phrases'.", "labels": [], "entities": [{"text": "LM growing", "start_pos": 66, "end_pos": 76, "type": "TASK", "confidence": 0.831494152545929}]}, {"text": "The remainder of this paper is organized as follows: In Section 2, we will review the existing CSLM converting methods.", "labels": [], "entities": [{"text": "CSLM converting", "start_pos": 95, "end_pos": 110, "type": "TASK", "confidence": 0.8448358774185181}]}, {"text": "The new neural network based bilingual LM growing method will be proposed in Section 3.", "labels": [], "entities": [{"text": "LM growing", "start_pos": 39, "end_pos": 49, "type": "TASK", "confidence": 0.9066846072673798}]}, {"text": "In Section 4, the experiments will be conducted and the results will be analyzed.", "labels": [], "entities": []}, {"text": "We will conclude our work in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "The same setting up of the NTCIR-9 Chinese to English translation baseline system (Goto et al., 2011) was followed, only with various LMs to compare them.", "labels": [], "entities": [{"text": "NTCIR-9 Chinese to English translation baseline system (Goto et al., 2011)", "start_pos": 27, "end_pos": 101, "type": "DATASET", "confidence": 0.8405293353966304}]}, {"text": "The Moses phrase-based SMT system was applied ( , together with GIZA++ () for alignment and MERT for tuning on the development data.", "labels": [], "entities": [{"text": "SMT", "start_pos": 23, "end_pos": 26, "type": "TASK", "confidence": 0.9443937540054321}, {"text": "alignment", "start_pos": 78, "end_pos": 87, "type": "TASK", "confidence": 0.962956428527832}, {"text": "MERT", "start_pos": 92, "end_pos": 96, "type": "METRIC", "confidence": 0.9977017045021057}]}, {"text": "Fourteen standard SMT features were used: five translation model scores, one word penalty score, seven distortion scores, and one LM score.", "labels": [], "entities": [{"text": "SMT", "start_pos": 18, "end_pos": 21, "type": "TASK", "confidence": 0.989340603351593}, {"text": "distortion scores", "start_pos": 103, "end_pos": 120, "type": "METRIC", "confidence": 0.9648135900497437}]}, {"text": "The translation performance was measured by the case-insensitive BLEU on the tokenized test data.", "labels": [], "entities": [{"text": "translation", "start_pos": 4, "end_pos": 15, "type": "TASK", "confidence": 0.9556319713592529}, {"text": "BLEU", "start_pos": 65, "end_pos": 69, "type": "METRIC", "confidence": 0.9867689609527588}]}, {"text": "We used the patent data for the Chinese to English patent translation subtask from the NTCIR-9 patent translation task).", "labels": [], "entities": [{"text": "Chinese to English patent translation subtask", "start_pos": 32, "end_pos": 77, "type": "TASK", "confidence": 0.6650165865818659}, {"text": "NTCIR-9 patent translation task", "start_pos": 87, "end_pos": 118, "type": "TASK", "confidence": 0.8318456411361694}]}, {"text": "The parallel training, development, and test data sets consist of 1 million (M), 2,000, and 2,000 sentences, respectively.", "labels": [], "entities": []}, {"text": "Using SRILM), we trained a 5-gram LM with the interpolated Kneser-Ney smoothing method using the 1M English training sentences containing 42M words without cutoff.", "labels": [], "entities": [{"text": "SRILM", "start_pos": 6, "end_pos": 11, "type": "METRIC", "confidence": 0.6058883666992188}]}, {"text": "The 2,3,4,5-CSLMs were trained on the same 1M training sentences using CSLM toolkit.", "labels": [], "entities": []}, {"text": "The settings for CSLMs were: input layer of the same dimension as vocabulary size (456K), projection layer of dimension 256 for each word, hidden layer of dimension 384 and output layer (short-list) of dimension 8192, which were recommended in the CSLM toolkit and () . Arsoy used around 55 M words as the corpus, including  The TED corpus is in special domain as discussed in the introduction, where large extra monolingual corpora are hard to find.", "labels": [], "entities": [{"text": "TED corpus", "start_pos": 329, "end_pos": 339, "type": "DATASET", "confidence": 0.7211690694093704}]}, {"text": "In this subsection, we conducted the SMT experiments on TED corpora using our proposed LM growing method, to evaluate whether our method was adaptable to some special domains.", "labels": [], "entities": [{"text": "SMT", "start_pos": 37, "end_pos": 40, "type": "TASK", "confidence": 0.9953304529190063}, {"text": "LM growing", "start_pos": 87, "end_pos": 97, "type": "TASK", "confidence": 0.745729386806488}]}, {"text": "We mainly followed the baselines of the IWSLT 2014 evaluation campaign , only with a few modifications such as the LM toolkits and n-gram order for constructing LMs.", "labels": [], "entities": [{"text": "IWSLT 2014 evaluation campaign", "start_pos": 40, "end_pos": 70, "type": "DATASET", "confidence": 0.8329981565475464}]}, {"text": "The Chinese (CN) to English (EN) language pair was chosen, using dev2010 as development data and test2010 as evaluation data.", "labels": [], "entities": []}, {"text": "The same LM growing method was ap-7 https://wit3.fbk.eu/ plied on TED corpora as on NTCIR corpora.", "labels": [], "entities": [{"text": "LM growing", "start_pos": 9, "end_pos": 19, "type": "TASK", "confidence": 0.8247629404067993}, {"text": "NTCIR corpora", "start_pos": 84, "end_pos": 97, "type": "DATASET", "confidence": 0.9093473255634308}]}, {"text": "The results were shown in. indicated that our proposed LM growing method improved both PPL and BLEU in comparison with both BNLM and our previous CSLM converting method, so it was suitable for domain adaptation, which is one of focuses of the current SMT research.", "labels": [], "entities": [{"text": "LM growing", "start_pos": 55, "end_pos": 65, "type": "TASK", "confidence": 0.8697443306446075}, {"text": "BLEU", "start_pos": 95, "end_pos": 99, "type": "METRIC", "confidence": 0.9982901215553284}, {"text": "BNLM", "start_pos": 124, "end_pos": 128, "type": "DATASET", "confidence": 0.7507712841033936}, {"text": "CSLM converting", "start_pos": 146, "end_pos": 161, "type": "TASK", "confidence": 0.7450900375843048}, {"text": "domain adaptation", "start_pos": 193, "end_pos": 210, "type": "TASK", "confidence": 0.8141438663005829}, {"text": "SMT", "start_pos": 251, "end_pos": 254, "type": "TASK", "confidence": 0.9940150380134583}]}], "tableCaptions": [{"text": " Table 1: Performance of the Grown LMs", "labels": [], "entities": []}, {"text": " Table 2: CN-EN TED Experiments", "labels": [], "entities": [{"text": "CN-EN TED Experiments", "start_pos": 10, "end_pos": 31, "type": "TASK", "confidence": 0.542816698551178}]}]}