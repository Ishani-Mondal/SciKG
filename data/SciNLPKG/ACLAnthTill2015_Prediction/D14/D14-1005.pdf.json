{"title": [{"text": "Learning Image Embeddings using Convolutional Neural Networks for Improved Multi-Modal Semantics", "labels": [], "entities": []}], "abstractContent": [{"text": "We construct multi-modal concept representations by concatenating a skip-gram linguistic representation vector with a visual concept representation vector computed using the feature extraction layers of a deep convolutional neural network (CNN) trained on a large labeled object recognition dataset.", "labels": [], "entities": []}, {"text": "This transfer learning approach brings a clear performance gain over features based on the traditional bag-of-visual-word approach.", "labels": [], "entities": []}, {"text": "Experimental results are reported on the WordSim353 and MEN semantic relatedness evaluation tasks.", "labels": [], "entities": [{"text": "WordSim353", "start_pos": 41, "end_pos": 51, "type": "DATASET", "confidence": 0.93658846616745}, {"text": "MEN semantic relatedness evaluation", "start_pos": 56, "end_pos": 91, "type": "TASK", "confidence": 0.7090603709220886}]}, {"text": "We use visual features computed using either ImageNet or ESP Game images.", "labels": [], "entities": [{"text": "ESP Game images", "start_pos": 57, "end_pos": 72, "type": "DATASET", "confidence": 0.8028700153032938}]}], "introductionContent": [{"text": "Recent works have shown that multi-modal semantic representation models outperform unimodal linguistic models on a variety of tasks, including modeling semantic relatedness and predicting compositionality.", "labels": [], "entities": [{"text": "predicting compositionality", "start_pos": 177, "end_pos": 204, "type": "TASK", "confidence": 0.9478698074817657}]}, {"text": "These results were obtained by combining linguistic feature representations with robust visual features extracted from a set of images associated with the concept in question.", "labels": [], "entities": []}, {"text": "This extraction of visual features usually follows the popular computer vision approach consisting of computing local features, such as SIFT features, and aggregating them as bags of visual words.", "labels": [], "entities": []}, {"text": "Meanwhile, deep transfer learning techniques have gained considerable attention in the computer vision community.", "labels": [], "entities": [{"text": "deep transfer learning", "start_pos": 11, "end_pos": 33, "type": "TASK", "confidence": 0.8401185870170593}]}, {"text": "First, a deep convolutional neural network (CNN) is trained on a large * This work was carried out while Douwe Kiela was an intern at Microsoft Research, New York.", "labels": [], "entities": []}, {"text": "The convolutional layers are then used as mid-level feature extractors on a variety of computer vision tasks (.", "labels": [], "entities": []}, {"text": "Although transferring convolutional network features is not anew idea, the simultaneous availability of large datasets and cheap GPU co-processors has contributed to the achievement of considerable performance gains on a variety computer vision benchmarks: \"SIFT and HOG descriptors produced big performance gains a decade ago, and now deep convolutional features are providing a similar breakthrough\".", "labels": [], "entities": []}, {"text": "This work reports on results obtained by using CNN-extracted features in multi-modal semantic representation models.", "labels": [], "entities": []}, {"text": "These results are interesting in several respects.", "labels": [], "entities": []}, {"text": "First, these superior features provide the opportunity to increase the performance gap achieved by augmenting linguistic features with multi-modal features.", "labels": [], "entities": []}, {"text": "Second, this increased performance confirms that the multimodal performance improvement results from the information contained in the images and not the information used to select which images to use to represent a concept.", "labels": [], "entities": []}, {"text": "Third, our evaluation reveals an intriguing property of the CNN-extracted features.", "labels": [], "entities": []}, {"text": "Finally, since we use the skip-gram approach of  to generate our linguistic features, we believe that this work represents the first approach to multimodal distributional semantics that exclusively relies on deep learning for both its linguistic and visual components.", "labels": [], "entities": []}], "datasetContent": [{"text": "We carried out experiments using visual representations computed using two canonical image datasets.", "labels": [], "entities": []}, {"text": "The resulting multi-modal concept representations were evaluated using two well-known semantic relatedness datasets.", "labels": [], "entities": []}, {"text": "We evaluate our multi-modal word representations using two semantic relatedness datasets widely used in distributional semantics ().", "labels": [], "entities": []}, {"text": "WordSim353 () is a selection of 353 concept pairs with a similarity rating provided by human annotators.", "labels": [], "entities": [{"text": "WordSim353", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.957115888595581}]}, {"text": "Since this is probably the most widely used evaluation dataset for distributional semantics, we include it for comparison with other approaches.", "labels": [], "entities": []}, {"text": "WordSim353 has some known idiosyncracies: it includes named entities, such as OPEC, Arafat, and Maradona, as well as abstract words, such as antecedent and credibility, for which it maybe hard to find corresponding images.", "labels": [], "entities": [{"text": "WordSim353", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.959418773651123}]}, {"text": "Multi-modal representations are often evaluated on an unspecified subset of), making it impossible to compare the reported scores.", "labels": [], "entities": []}, {"text": "In this work, we report scores on the full WordSim353 dataset (W353) by setting the visual vector v vis to zero for concepts without images.", "labels": [], "entities": [{"text": "WordSim353 dataset (W353)", "start_pos": 43, "end_pos": 68, "type": "DATASET", "confidence": 0.950568950176239}]}, {"text": "We also report scores on the subset (W353-Relevant) of pairs for which both concepts have both ImageNet and ESP Game images using the aforementioned selection procedure.", "labels": [], "entities": [{"text": "W353-Relevant", "start_pos": 37, "end_pos": 50, "type": "DATASET", "confidence": 0.8628710508346558}]}, {"text": "MEN () was in part designed to alleviate the WordSim353 problems.", "labels": [], "entities": [{"text": "WordSim353", "start_pos": 45, "end_pos": 55, "type": "DATASET", "confidence": 0.8981086015701294}]}, {"text": "It was constructed in such away that only frequent words with at least 50 images in the ESP Game dataset were included in the evaluation pairs.", "labels": [], "entities": [{"text": "ESP Game dataset", "start_pos": 88, "end_pos": 104, "type": "DATASET", "confidence": 0.914171576499939}]}, {"text": "The MEN dataset has been found to mirror the aggregate score over a variety of tasks and similarity datasets . It is also much larger, with 3000 words pairs consisting of 751 individual words.", "labels": [], "entities": [{"text": "MEN dataset", "start_pos": 4, "end_pos": 15, "type": "DATASET", "confidence": 0.9173462390899658}]}, {"text": "Although MEN was constructed so as to have at least a minimum amount of images available in the ESP Game dataset for each concept, this is not the case for ImageNet.", "labels": [], "entities": [{"text": "ESP Game dataset", "start_pos": 96, "end_pos": 112, "type": "DATASET", "confidence": 0.940114955107371}]}, {"text": "Hence, similarly to WordSim353, we also evaluate on a subset (MEN-Relevant) for which images are available in both datasets.", "labels": [], "entities": [{"text": "WordSim353", "start_pos": 20, "end_pos": 30, "type": "DATASET", "confidence": 0.9648668169975281}]}, {"text": "We evaluate the models in terms of their Spearman \u03c1 correlation with the human relatedness ratings.", "labels": [], "entities": [{"text": "Spearman \u03c1 correlation", "start_pos": 41, "end_pos": 63, "type": "METRIC", "confidence": 0.9815243482589722}]}, {"text": "The similarity between the representations associated with a pair of words is calculated using the cosine similarity:  It is important to ask whether the source image dataset has a large impact on performance.", "labels": [], "entities": []}, {"text": "Although the scores for the visual representation in some cases differ, performance of multimodal representations remains close for both image datasets.", "labels": [], "entities": []}, {"text": "This implies that our method is robust over different datasets.", "labels": [], "entities": []}, {"text": "It also suggests that it is beneficial to train on high-quality datasets like ImageNet and to subsequently generate embeddings for other sets of images like the ESP Game dataset that are more noisy but have better coverage.", "labels": [], "entities": [{"text": "ImageNet", "start_pos": 78, "end_pos": 86, "type": "DATASET", "confidence": 0.9527128338813782}, {"text": "ESP Game dataset", "start_pos": 161, "end_pos": 177, "type": "DATASET", "confidence": 0.9468725721041361}]}, {"text": "The results show the benefit of transfering convolutional network features, corroborating recent results in computer vision.", "labels": [], "entities": []}, {"text": "There is an interesting discrepancy between the two types of network with respect to dataset performance: CNN-Mean multi-modal models tend to perform best on MEN and MEN-Relevant, while CNN-Max multi-modal models perform better on W353 and W353-Relevant.", "labels": [], "entities": [{"text": "MEN", "start_pos": 158, "end_pos": 161, "type": "DATASET", "confidence": 0.9188455939292908}, {"text": "W353", "start_pos": 231, "end_pos": 235, "type": "DATASET", "confidence": 0.9326421618461609}]}, {"text": "There also appears to be some interplay between the source corpus, the evaluation dataset and the best performing CNN: the performance leap on W353- Relevant for CNN-Max is much larger using ESP Game images than with ImageNet images.", "labels": [], "entities": []}, {"text": "We speculate that this is because CNN-Max performs better than CNN-Mean on a somewhat different type of similarity.", "labels": [], "entities": []}, {"text": "It has been noted) that WordSim353 captures both similarity (as in tiger-cat, with a score of 7.35) as well as relatedness (as in Maradona-football, with a score of 8.62).", "labels": [], "entities": [{"text": "WordSim353", "start_pos": 24, "end_pos": 34, "type": "DATASET", "confidence": 0.846771240234375}, {"text": "similarity", "start_pos": 49, "end_pos": 59, "type": "METRIC", "confidence": 0.99679034948349}]}, {"text": "MEN, however, is explicitly designed to capture semantic relatedness only (.", "labels": [], "entities": [{"text": "MEN", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.819313645362854}]}, {"text": "CNN-Max using sparse feature vectors means that we treat the dominant components as definitive of the concept class, which is more suited to similarity.", "labels": [], "entities": [{"text": "CNN-Max", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.8653872609138489}]}, {"text": "CNN-Mean averages overall the feature components, and as such might be more suited to relatedness.", "labels": [], "entities": [{"text": "CNN-Mean", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.8302343487739563}]}, {"text": "We conjecture that the performance increase on WordSim353 is due to increased performance on the similarity subset of that dataset.", "labels": [], "entities": [{"text": "WordSim353", "start_pos": 47, "end_pos": 57, "type": "DATASET", "confidence": 0.9555155634880066}]}], "tableCaptions": [{"text": " Table 1: Results (see sections 4 and 5).", "labels": [], "entities": []}, {"text": " Table 2: The top 5 best and top 5 worst scoring pairs with respect to the gold standard.", "labels": [], "entities": []}]}