{"title": [{"text": "A Neural Network for Factoid Question Answering over Paragraphs", "labels": [], "entities": [{"text": "Factoid Question Answering over Paragraphs", "start_pos": 21, "end_pos": 63, "type": "TASK", "confidence": 0.7564537465572357}]}], "abstractContent": [{"text": "Text classification methods for tasks like factoid question answering typically use manually defined string matching rules or bag of words representations.", "labels": [], "entities": [{"text": "Text classification", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7325966507196426}, {"text": "factoid question answering", "start_pos": 43, "end_pos": 69, "type": "TASK", "confidence": 0.7627009550730387}]}, {"text": "These methods are ineffective when question text contains very few individual words (e.g., named entities) that are indicative of the answer.", "labels": [], "entities": []}, {"text": "We introduce a recursive neural network (rnn) model that can reason over such input by modeling textual composition-ality.", "labels": [], "entities": []}, {"text": "We apply our model, qanta, to a dataset of questions from a trivia competition called quiz bowl.", "labels": [], "entities": []}, {"text": "Unlike previous rnn models, qanta learns word and phrase-level representations that combine across sentences to reason about entities.", "labels": [], "entities": []}, {"text": "The model outperforms multiple baselines and, when combined with information retrieval methods, rivals the best human players.", "labels": [], "entities": []}], "introductionContent": [{"text": "Deep neural networks have seen widespread use in natural language processing tasks such as parsing, language modeling, and sentiment analysis ().", "labels": [], "entities": [{"text": "parsing", "start_pos": 91, "end_pos": 98, "type": "TASK", "confidence": 0.9254672527313232}, {"text": "language modeling", "start_pos": 100, "end_pos": 117, "type": "TASK", "confidence": 0.6904047280550003}, {"text": "sentiment analysis", "start_pos": 123, "end_pos": 141, "type": "TASK", "confidence": 0.9432304799556732}]}, {"text": "The vector spaces learned by these models cluster words and phrases together based on similarity.", "labels": [], "entities": []}, {"text": "For example, a neural network trained fora sentiment analysis task such as restaurant review classification might learn that \"tasty\" and \"delicious\" should have similar representations since they are synonymous adjectives.", "labels": [], "entities": [{"text": "sentiment analysis task", "start_pos": 43, "end_pos": 66, "type": "TASK", "confidence": 0.9023687044779459}, {"text": "restaurant review classification", "start_pos": 75, "end_pos": 107, "type": "TASK", "confidence": 0.7628362278143564}]}, {"text": "These models have so far only seen success in a limited range of text-based prediction tasks, Later in its existence, this polity's leader was chosen by a group that included three bishops and six laymen, up from the seven who traditionally made the decision.", "labels": [], "entities": [{"text": "text-based prediction tasks", "start_pos": 65, "end_pos": 92, "type": "TASK", "confidence": 0.705380400021871}]}, {"text": "Free imperial cities in this polity included Basel and Speyer.", "labels": [], "entities": [{"text": "Basel", "start_pos": 45, "end_pos": 50, "type": "DATASET", "confidence": 0.9187690615653992}]}, {"text": "Dissolved in 1806, its key events included the Investiture Controversy and the Golden Bull of 1356.", "labels": [], "entities": [{"text": "Golden Bull of 1356", "start_pos": 79, "end_pos": 98, "type": "DATASET", "confidence": 0.9580945819616318}]}, {"text": "Led by Charles V, Frederick Barbarossa, and Otto I, for 10 points, name this polity, which ruled most of what is now Germany through the Middle Ages and rarely ruled its titular city.", "labels": [], "entities": []}, {"text": "where inputs are typically a single sentence and outputs are either continuous or a limited discrete set.", "labels": [], "entities": []}, {"text": "Neural networks have not yet shown to be useful for tasks that require mapping paragraph-length inputs to rich output spaces.", "labels": [], "entities": []}, {"text": "Consider factoid question answering: given a description of an entity, identify the person, place, or thing discussed.", "labels": [], "entities": [{"text": "factoid question answering", "start_pos": 9, "end_pos": 35, "type": "TASK", "confidence": 0.7794488867123922}]}, {"text": "We describe a task with high-quality mappings from natural language text to entities in Section 2.", "labels": [], "entities": []}, {"text": "This task-quiz bowl-is a challenging natural language problem with large amounts of diverse and compositional data.", "labels": [], "entities": []}, {"text": "To answer quiz bowl questions, we develop a dependency tree recursive neural network in Section 3 and extend it to combine predictions across sentences to produce a question answering neural network with trans-sentential averaging (qanta).", "labels": [], "entities": []}, {"text": "We evaluate our model against strong computer and human baselines in Section 4 and conclude by examining the latent space and model mistakes.", "labels": [], "entities": []}], "datasetContent": [{"text": "We compare the performance of qanta against multiple strong baselines on two datasets.", "labels": [], "entities": []}, {"text": "qanta outperforms all baselines trained only on question text and improves an information retrieval model trained on all of Wikipedia.", "labels": [], "entities": []}, {"text": "qanta requires that an input sentence describes an entity without mentioning that entity, a constraint that is not followed by Wikipedia sentences.", "labels": [], "entities": []}, {"text": "While ir methods can operate over Wikipedia text with no issues, we show that the representations learned by qanta over just a dataset of question-answer pairs can significantly improve the performance of ir systems.", "labels": [], "entities": []}, {"text": "We evaluate our algorithms on a corpus of over 100,000 question/answer pairs from two different sources.", "labels": [], "entities": []}, {"text": "First, we expand the dataset used in Boyd-Graber et al. with publicallyavailable questions from quiz bowl tournaments held after that work was published.", "labels": [], "entities": []}, {"text": "This gives us 46,842 questions in fourteen different categories.", "labels": [], "entities": []}, {"text": "To this dataset we add 65,212 questions from naqt, an organization that runs quiz bowl tournaments and generously shared with us all of their questions from 1998-2013.", "labels": [], "entities": []}, {"text": "Because some categories contain substantially fewer questions than others (e.g., astronomy has only 331 questions), we consider only literature and history questions, as these two categories account for more than 40% of the corpus.", "labels": [], "entities": []}, {"text": "This leaves us with 21,041 history questions and 22,956 literature questions.", "labels": [], "entities": []}, {"text": "The disparity between ir-qb and ir-wiki indicates that the information retrieval models need lots of external data to work well at all sentence positions.", "labels": [], "entities": []}, {"text": "ir-wiki performs better than other models because Wikipedia contains many more sentences that partially match specific words or phrases found in early clues than the question training set.", "labels": [], "entities": []}, {"text": "In particular, it is impossible for all other models to answer clues in the test set that have no semantically similar or equivalent analogues in the training question data.", "labels": [], "entities": []}, {"text": "With that said, ir methods can also operate over data that does not follow the special constraints of quiz bowl questions (e.g., every sentence uniquely identifies the answer, answers don't appear in their corresponding questions), which qanta cannot handle.", "labels": [], "entities": []}, {"text": "By combining qanta and ir-wiki, we are able to leverage access to huge knowledge bases along with deep compositional representations, giving us the best of both worlds.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Accuracy for history and literature at the first two sentence positions of each question  and the full question. The top half of the table compares models trained on questions only, while  the IR models in the bottom half have access to Wikipedia. qanta outperforms all baselines  that are restricted to just the question data, and it substantially improves an IR model with  access to Wikipedia despite being trained on much less data.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9960307478904724}]}]}