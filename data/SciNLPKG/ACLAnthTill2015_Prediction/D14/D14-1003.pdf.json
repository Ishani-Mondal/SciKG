{"title": [{"text": "Translation Modeling with Bidirectional Recurrent Neural Networks", "labels": [], "entities": [{"text": "Translation Modeling", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.9648483693599701}]}], "abstractContent": [{"text": "This work presents two different translation models using recurrent neural networks.", "labels": [], "entities": []}, {"text": "The first one is a word-based approach using word alignments.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 45, "end_pos": 60, "type": "TASK", "confidence": 0.752997100353241}]}, {"text": "Second, we present phrase-based translation models that are more consistent with phrase-based decoding.", "labels": [], "entities": [{"text": "phrase-based translation", "start_pos": 19, "end_pos": 43, "type": "TASK", "confidence": 0.7239028811454773}]}, {"text": "Moreover, we introduce bidirectional recurrent neural models to the problem of machine translation, allowing us to use the full source sentence in our models, which is also of theoretical interest.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 79, "end_pos": 98, "type": "TASK", "confidence": 0.8133899867534637}]}, {"text": "We demonstrate that our translation models are capable of improving strong baselines already including recurrent neu-ral language models on three tasks: IWSLT 2013 German\u2192English, BOLT Arabic\u2192English and Chinese\u2192English.", "labels": [], "entities": [{"text": "IWSLT 2013 German\u2192English", "start_pos": 153, "end_pos": 178, "type": "DATASET", "confidence": 0.8929016709327697}, {"text": "BOLT", "start_pos": 180, "end_pos": 184, "type": "METRIC", "confidence": 0.9540868997573853}]}, {"text": "We obtain gains up to 1.6% BLEU and 1.7% TER by rescoring 1000-best lists.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 27, "end_pos": 31, "type": "METRIC", "confidence": 0.9995326995849609}, {"text": "TER", "start_pos": 41, "end_pos": 44, "type": "METRIC", "confidence": 0.9993763566017151}]}], "introductionContent": [{"text": "Neural network models have recently experienced unprecedented attention in research on statistical machine translation (SMT).", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 87, "end_pos": 124, "type": "TASK", "confidence": 0.7934208263953527}]}, {"text": "Several groups have reported strong improvements over state-of-the-art baselines using feedforward neural network-based language models (, as well as translation models ().", "labels": [], "entities": []}, {"text": "Different from the feedforward design, recurrent neural networks (RNNs) have the advantage of being able to take into account an unbounded history of previous observations.", "labels": [], "entities": []}, {"text": "In theory, this enables them to model long-distance dependencies of arbitrary length.", "labels": [], "entities": []}, {"text": "However, while previous work on translation modeling with recurrent neural networks shows its effectiveness on standard baselines, so far no notable gains have been presented on top of recurrent language models (.", "labels": [], "entities": [{"text": "translation modeling", "start_pos": 32, "end_pos": 52, "type": "TASK", "confidence": 0.9732781946659088}]}, {"text": "In this work, we present two novel approaches to recurrent neural translation modeling: wordbased and phrase-based.", "labels": [], "entities": [{"text": "recurrent neural translation modeling", "start_pos": 49, "end_pos": 86, "type": "TASK", "confidence": 0.7805882096290588}]}, {"text": "The word-based approach assumes one-to-one aligned source and target sentences.", "labels": [], "entities": []}, {"text": "We evaluate different ways of resolving alignment ambiguities to obtain such alignments.", "labels": [], "entities": []}, {"text": "The phrase-based RNN approach is more closely tied to the underlying translation paradigm.", "labels": [], "entities": []}, {"text": "It models actual phrasal translation probabilities while avoiding sparsity issues by using single words as input and output units.", "labels": [], "entities": [{"text": "phrasal translation probabilities", "start_pos": 17, "end_pos": 50, "type": "TASK", "confidence": 0.7621092995007833}]}, {"text": "Furthermore, in addition to the unidirectional formulation, we are the first to propose a bidirectional architecture which can take the full source sentence into account for all predictions.", "labels": [], "entities": []}, {"text": "Our experiments show that these models can improve state-of-the-art baselines containing a recurrent language model on three tasks.", "labels": [], "entities": []}, {"text": "For our competitive IWSLT 2013 German\u2192English system, we observe gains of up to 1.6% BLEU and 1.7% TER.", "labels": [], "entities": [{"text": "IWSLT 2013 German\u2192English system", "start_pos": 20, "end_pos": 52, "type": "DATASET", "confidence": 0.8513830502827963}, {"text": "BLEU", "start_pos": 85, "end_pos": 89, "type": "METRIC", "confidence": 0.999530553817749}, {"text": "TER", "start_pos": 99, "end_pos": 102, "type": "METRIC", "confidence": 0.9989645481109619}]}, {"text": "Improvements are also demonstrated on top of our evaluation systems for BOLT Arabic\u2192English and Chinese\u2192English, which also include recurrent neural language models.", "labels": [], "entities": [{"text": "BOLT Arabic\u2192English", "start_pos": 72, "end_pos": 91, "type": "TASK", "confidence": 0.5286411643028259}]}, {"text": "The rest of this paper is structured as follows.", "labels": [], "entities": []}, {"text": "In Section 2 we review related work and in Section 3 an overview of long short-term memory (LSTM) neural networks, a special type of recurrent neural networks we make use of in this work, is given.", "labels": [], "entities": []}, {"text": "Section 4 describes our novel translation models.", "labels": [], "entities": []}, {"text": "Finally, experiments are presented in Section 5 and we conclude with Section 6.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Comparison of including different sets  of tokens into the one-to-one alignment on the  IWSLT 2013 German\u2192English task using the uni- directional RNN translation model.", "labels": [], "entities": [{"text": "IWSLT 2013 German\u2192English task", "start_pos": 98, "end_pos": 128, "type": "DATASET", "confidence": 0.8911987543106079}, {"text": "RNN translation", "start_pos": 156, "end_pos": 171, "type": "TASK", "confidence": 0.6676111221313477}]}, {"text": " Table 2:  Results for the IWSLT 2013  German\u2192English task with different RNN  models. T: translation, J: joint, B: bidirectional,  P: phrase-based.", "labels": [], "entities": [{"text": "IWSLT 2013  German\u2192English task", "start_pos": 27, "end_pos": 58, "type": "TASK", "confidence": 0.6696077883243561}]}, {"text": " Table 3: Results for the IWSLT 2013 German\u2192English task with different RNN models. All results  include a recurrent language model. T: translation, J: joint, B: bidirectional, P: phrase-based.", "labels": [], "entities": [{"text": "IWSLT 2013 German\u2192English task", "start_pos": 26, "end_pos": 56, "type": "DATASET", "confidence": 0.7308751940727234}]}, {"text": " Table 4: Results for the BOLT Arabic\u2192English  task with different RNN models. The \"+\" sign in  the last two rows indicates that either of the corre- sponding deep models (BTM and BJM) are added  to the baseline including the recurrent language  model (i.e. they are not applied at the same time).  T: translation, J: joint, B: bidirectional.", "labels": [], "entities": [{"text": "BOLT Arabic\u2192English  task", "start_pos": 26, "end_pos": 51, "type": "TASK", "confidence": 0.6951376855373382}]}, {"text": " Table 5: Results for the BOLT Chinese\u2192English  task with different RNN models. The \"+\" sign in  the last two rows indicates that either of the corre- sponding deep models (BTM and BJM) are added  to the baseline including the recurrent language  model (i.e. they are not applied at the same time).  T: translation, B: bidirectional.", "labels": [], "entities": [{"text": "BOLT Chinese\u2192English  task", "start_pos": 26, "end_pos": 52, "type": "TASK", "confidence": 0.6622477293014526}, {"text": "translation", "start_pos": 303, "end_pos": 314, "type": "TASK", "confidence": 0.9446093440055847}]}]}