{"title": [{"text": "Revisiting Embedding Features for Simple Semi-supervised Learning", "labels": [], "entities": [{"text": "Revisiting Embedding", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.85931196808815}]}], "abstractContent": [{"text": "Recent work has shown success in using continuous word embeddings learned from unlabeled data as features to improve supervised NLP systems, which is regarded as a simple semi-supervised learning mechanism.", "labels": [], "entities": []}, {"text": "However, fundamental problems on effectively incorporating the word embedding features within the framework of linear models remain.", "labels": [], "entities": []}, {"text": "In this study, we investigate and analyze three different approaches, including anew proposed distributional prototype approach, for utilizing the embedding features.", "labels": [], "entities": []}, {"text": "The presented approaches can be integrated into most of the classical linear models in NLP.", "labels": [], "entities": []}, {"text": "Experiments on the task of named entity recognition show that each of the proposed approaches can better utilize the word embedding features, among which the distributional prototype approach performs the best.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 27, "end_pos": 51, "type": "TASK", "confidence": 0.6274294853210449}]}, {"text": "Moreover, the combination of the approaches provides additive improvements , outperforming the dense and continuous embedding features by nearly 2 points of F1 score.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 157, "end_pos": 165, "type": "METRIC", "confidence": 0.9790804088115692}]}], "introductionContent": [{"text": "Learning generalized representation of words is an effective way of handling data sparsity caused by high-dimensional lexical features in NLP systems, such as named entity recognition (NER) and dependency parsing.", "labels": [], "entities": [{"text": "Learning generalized representation of words", "start_pos": 0, "end_pos": 44, "type": "TASK", "confidence": 0.693031644821167}, {"text": "named entity recognition (NER)", "start_pos": 159, "end_pos": 189, "type": "TASK", "confidence": 0.7899324893951416}, {"text": "dependency parsing", "start_pos": 194, "end_pos": 212, "type": "TASK", "confidence": 0.7742299139499664}]}, {"text": "As atypical lowdimensional and generalized word representation, Brown clustering of words has been studied fora longtime.", "labels": [], "entities": []}, {"text": "For example, and used the Brown cluster features for semi-supervised learning of various NLP tasks and achieved significant improvements.", "labels": [], "entities": []}, {"text": "Recent research has focused on a special family of word representations, named \"word embeddings\".", "labels": [], "entities": []}, {"text": "Word embeddings are conventionally defined as dense, continuous, and low-dimensional vector representations of words.", "labels": [], "entities": []}, {"text": "Word embeddings can be learned from large-scale unlabeled texts through context-predicting models (e.g., neural network language models) or spectral methods (e.g., canonical correlation analysis) in an unsupervised setting.", "labels": [], "entities": [{"text": "canonical correlation analysis", "start_pos": 164, "end_pos": 194, "type": "TASK", "confidence": 0.6552502512931824}]}, {"text": "Compared with the so-called one-hot representation where each word is represented as a sparse vector of the same size of the vocabulary and only one dimension is on, word embedding preserves rich linguistic regularities of words with each dimension hopefully representing a latent feature.", "labels": [], "entities": []}, {"text": "Similar words are expected to be distributed close to one another in the embedding space.", "labels": [], "entities": []}, {"text": "Consequently, word embeddings can be beneficial fora variety of NLP applications in different ways, among which the most simple and general way is to be fed as features to enhance existing supervised NLP systems.", "labels": [], "entities": []}, {"text": "Previous work has demonstrated effectiveness of the continuous word embedding features in several tasks such as chunking and NER using generalized linear models (.", "labels": [], "entities": [{"text": "chunking", "start_pos": 112, "end_pos": 120, "type": "TASK", "confidence": 0.9692687392234802}, {"text": "NER", "start_pos": 125, "end_pos": 128, "type": "TASK", "confidence": 0.9028396010398865}]}, {"text": "However, there still remain two fundamental problems that should be addressed: \u2022 Are the continuous embedding features fit for the generalized linear models that are most widely adopted in NLP?", "labels": [], "entities": []}, {"text": "\u2022 How can the generalized linear models better utilize the embedding features?", "labels": [], "entities": []}, {"text": "According to the results provided by, the embedding features brought significantly less improvement than Brown clustering features.", "labels": [], "entities": []}, {"text": "This result is actually not reasonable because the expressing power of word embeddings is theoretically stronger than clustering-based representations which can be regarded as a kind of one-hot representation but over a low-dimensional vocabulary ().", "labels": [], "entities": []}, {"text": "showed that linear architectures perform better in high-dimensional discrete feature space than non-linear ones, whereas non-linear architectures are more effective in low-dimensional and continuous feature space.", "labels": [], "entities": []}, {"text": "Hence, the previous method that directly uses the continuous word embeddings as features in linear models (CRF) is inappropriate.", "labels": [], "entities": []}, {"text": "Word embeddings maybe better utilized in the linear modeling framework by smartly transforming the embeddings to some relatively higher dimensional and discrete representations.", "labels": [], "entities": []}, {"text": "Driven by this motivation, we present three different approaches: binarization (Section 3.2), clustering (Section 3.3) and anew proposed distributional prototype method (Section 3.4) for better incorporating the embeddings features.", "labels": [], "entities": []}, {"text": "In the binarization approach, we directly binarize the continuous word embeddings by dimension.", "labels": [], "entities": []}, {"text": "In the clustering approach, we cluster words based on their embeddings and use the resulting word cluster features instead.", "labels": [], "entities": []}, {"text": "In the distributional prototype approach, we derive task-specific features from word embeddings by utilizing a set of automatically extracted prototypes for each target label.", "labels": [], "entities": []}, {"text": "We carefully compare and analyze these approaches in the task of NER.", "labels": [], "entities": [{"text": "NER", "start_pos": 65, "end_pos": 68, "type": "TASK", "confidence": 0.9641321301460266}]}, {"text": "With each of the three approaches, we achieve higher performance than directly using the continuous embedding features, among which the distributional prototype approach performs the best.", "labels": [], "entities": []}, {"text": "Furthermore, by putting the most effective two of these features together, we finally outperform the continuous embedding features by nearly 2 points of F1 Score (86.21% vs. 88.11%).", "labels": [], "entities": [{"text": "F1 Score", "start_pos": 153, "end_pos": 161, "type": "METRIC", "confidence": 0.9401816427707672}]}, {"text": "The major contribution of this paper is twofold.", "labels": [], "entities": []}, {"text": "(1) We investigate various approaches that can better utilize word embeddings for semi-supervised learning.", "labels": [], "entities": []}, {"text": "(2) We propose a novel distributional prototype approach that shows the great potential of word embedding features.", "labels": [], "entities": []}, {"text": "All the presented approaches can be easily integrated into most of the classical linear NLP models.", "labels": [], "entities": []}], "datasetContent": [{"text": "Various tasks can be considered to compare and analyze the effectiveness of the above three approaches.", "labels": [], "entities": []}, {"text": "In this study, we partly follow Turian et al. and , and take NER as the supervised evaluation task.", "labels": [], "entities": [{"text": "NER", "start_pos": 61, "end_pos": 64, "type": "TASK", "confidence": 0.9061570167541504}]}, {"text": "NER identifies and classifies the named entities such as the names of persons, locations, and organizations in text.", "labels": [], "entities": [{"text": "NER identifies and classifies the named entities such as the names of persons, locations, and organizations in text", "start_pos": 0, "end_pos": 115, "type": "Description", "confidence": 0.7326104357838631}]}, {"text": "The state-of-the-art systems typically treat NER as a sequence labeling problem, where each word is tagged either as a BIO-style NE or a non-NE category.", "labels": [], "entities": []}, {"text": "Here, we use the linear chain CRF model, which is most widely used for sequence modeling in the field of NLP.", "labels": [], "entities": [{"text": "sequence modeling", "start_pos": 71, "end_pos": 88, "type": "TASK", "confidence": 0.7573360502719879}]}, {"text": "The CoNLL-2003 shared task dataset from the Reuters, which was used by and , was chosen as our evaluation dataset.", "labels": [], "entities": [{"text": "CoNLL-2003 shared task dataset from the Reuters", "start_pos": 4, "end_pos": 51, "type": "DATASET", "confidence": 0.8773835386548724}]}, {"text": "The training set contains 14,987 sentences, the development set contains 3,466 sentences and is used for parameter tuning, and the test set contains 3,684 sentences.", "labels": [], "entities": []}, {"text": "The baseline features are shown in.", "labels": [], "entities": []}, {"text": "We take the English Wikipedia until August 2012 as our unlabeled data to train the word embeddings.", "labels": [], "entities": [{"text": "English Wikipedia", "start_pos": 12, "end_pos": 29, "type": "DATASET", "confidence": 0.8719101548194885}]}, {"text": "Little pre-processing is conducted for the training of word embeddings.", "labels": [], "entities": []}, {"text": "We remove paragraphs that contain non-roman characters and all MediaWiki markups.", "labels": [], "entities": []}, {"text": "The resulting text is tokenized using the Stanford tokenizer, and every word is converted to lowercase.", "labels": [], "entities": []}, {"text": "The final dataset contains about 30 million sentences and 1.52 billion words.", "labels": [], "entities": []}, {"text": "We use a dictionary that contains 212,779 most common words (frequency \u2265 80) in the dataset.", "labels": [], "entities": []}, {"text": "An efficient open-source implementation of the Skip-gram model is adopted.", "labels": [], "entities": []}, {"text": "We apply the negative sampling 7 method for optimization, and the asynchronous stochastic gradient descent algorithm (Asynchronous SGD) for parallel weight updating.", "labels": [], "entities": []}, {"text": "In this study, we set the dimension of the word embeddings to 50.", "labels": [], "entities": []}, {"text": "Higher dimension is supposed to bring more improvements in semi-supervised learning, but its comparison is beyond the scope of this paper.", "labels": [], "entities": []}, {"text": "For the cluster features, we tune the number of clusters n from 500 to 3000 on the development set, and finally use the combination of n = 500, 1000, 1500, 2000, 3000, which achieves the best results.", "labels": [], "entities": []}, {"text": "For the distributional prototype features, we use a fixed number of prototype words (m) for each target label.", "labels": [], "entities": []}, {"text": "m is tuned on the development set and is finally set to 40.", "labels": [], "entities": [{"text": "development set", "start_pos": 18, "end_pos": 33, "type": "DATASET", "confidence": 0.7564436495304108}]}, {"text": "We induce 1,000 brown clusters of words, the setting in prior work (.", "labels": [], "entities": []}, {"text": "The training data of brown clustering is the same with that of training word embeddings.", "labels": [], "entities": []}, {"text": "shows the performances of NER on the test dataset.", "labels": [], "entities": []}, {"text": "Our baseline is slightly lower than that of, because they use the BILOU encoding of NE types which outperforms BIO encoding).", "labels": [], "entities": [{"text": "BILOU", "start_pos": 66, "end_pos": 71, "type": "METRIC", "confidence": 0.9934982061386108}]}, {"text": "8 Nonetheless, our conclusions hold.", "labels": [], "entities": []}, {"text": "As we can see, all of the three approaches we investigate in this study achieve better performance than the direct use of the dense continuous embedding features.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: The performance of semi-supervised  NER on the CoNLL-2003 test data, using vari- ous embedding features.  \u2020 DenseEmb refers to the  method used by Turian et al. (2010), i.e., the direct  use of the dense and continuous embeddings.", "labels": [], "entities": [{"text": "NER", "start_pos": 46, "end_pos": 49, "type": "TASK", "confidence": 0.9111568331718445}, {"text": "CoNLL-2003 test data", "start_pos": 57, "end_pos": 77, "type": "DATASET", "confidence": 0.9800231059392294}]}, {"text": " Table 4: Running time of different features on a  Intel(R) Xeon(R) E5620 2.40GHz machine.", "labels": [], "entities": []}, {"text": " Table 5: Performance of the NE/non-NE classi- fication on the CoNLL-2003 development dataset  using different embedding features.", "labels": [], "entities": [{"text": "CoNLL-2003 development dataset", "start_pos": 63, "end_pos": 93, "type": "DATASET", "confidence": 0.9529329737027487}]}]}