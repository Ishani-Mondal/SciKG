{"title": [{"text": "Clustering Aspect-related Phrases by Leveraging Sentiment Distribution Consistency", "labels": [], "entities": [{"text": "Sentiment Distribution Consistency", "start_pos": 48, "end_pos": 82, "type": "TASK", "confidence": 0.8244321544965109}]}], "abstractContent": [{"text": "Clustering aspect-related phrases in terms of product's property is a precursor process to aspect-level sentiment analysis which is a central task in sentiment analysis.", "labels": [], "entities": [{"text": "aspect-level sentiment analysis", "start_pos": 91, "end_pos": 122, "type": "TASK", "confidence": 0.7087716360886892}, {"text": "sentiment analysis", "start_pos": 150, "end_pos": 168, "type": "TASK", "confidence": 0.9607614278793335}]}, {"text": "Most of existing methods for addressing this problem are context-based models which assume that domain synonymous phrases share similar co-occurrence contexts.", "labels": [], "entities": []}, {"text": "In this paper, we explore a novel idea, sentiment distribution consistency, which states that different phrases (e.g. \"price\", \"money\", \"worth\", and \"cost\") of the same aspect tend to have consistent sentiment distribution.", "labels": [], "entities": []}, {"text": "Through formalizing sentiment distribution consistency as soft constraint, we propose a novel unsu-pervised model in the framework of Posterior Regularization (PR) to cluster aspect-related phrases.", "labels": [], "entities": [{"text": "Posterior Regularization (PR)", "start_pos": 134, "end_pos": 163, "type": "TASK", "confidence": 0.6560319602489472}]}, {"text": "Experiments demonstrate that our approach outperforms baselines remarkably.", "labels": [], "entities": []}], "introductionContent": [{"text": "Aspect-level sentiment analysis has become a central task in sentiment analysis because it can aggregate various opinions according to a product's properties, and provide much detailed, complete, and in-depth summaries of a large number of reviews.", "labels": [], "entities": [{"text": "Aspect-level sentiment analysis", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.919707198937734}, {"text": "sentiment analysis", "start_pos": 61, "end_pos": 79, "type": "TASK", "confidence": 0.9674233198165894}]}, {"text": "Aspect finding and clustering, a precursor process of aspect-level sentiment analysis, has attracted more and more attentions.", "labels": [], "entities": [{"text": "Aspect finding and clustering", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.7712651342153549}, {"text": "aspect-level sentiment analysis", "start_pos": 54, "end_pos": 85, "type": "TASK", "confidence": 0.6846430401007334}]}, {"text": "Aspect finding and clustering has never been a trivial task.", "labels": [], "entities": [{"text": "Aspect finding", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.9889481961727142}, {"text": "clustering", "start_pos": 19, "end_pos": 29, "type": "TASK", "confidence": 0.780798614025116}]}, {"text": "People often use different words or phrases to refer to the same product property (also called product aspect or feature in the literature).", "labels": [], "entities": []}, {"text": "Some terms are lexically dissimilar while semantically close, which makes the task more challenging.", "labels": [], "entities": []}, {"text": "For example, \"price\", \"money\" , \"worth\" and \"cost\" all refer to the aspect \"price\" in reviews.", "labels": [], "entities": []}, {"text": "In order to present aspect-specific summaries of opinions, we first of all, have to cluster different aspect-related phrases.", "labels": [], "entities": []}, {"text": "It is expensive and timeconsuming to manually group hundreds of aspectrelated phrases.", "labels": [], "entities": []}, {"text": "In this paper, we assume that the aspect phrases have been extracted in advance and we keep focused on clustering domain synonymous aspect-related phrases.", "labels": [], "entities": [{"text": "clustering domain synonymous aspect-related phrases", "start_pos": 103, "end_pos": 154, "type": "TASK", "confidence": 0.797119927406311}]}, {"text": "Existing studies addressing this problem are mainly based on the assumption that different phrases of the same aspect should have similar cooccurrence contexts.", "labels": [], "entities": []}, {"text": "In addition to the traditional assumption, we develop anew angle to address the problem, which is based on sentiment distribution consistency assumption that different phrases of the same aspect should have consistent sentiment distribution, which will be detailed soon later.", "labels": [], "entities": []}, {"text": "This new angle is inspired by this simple observation (as illustrated in): two phrases within the same cluster are not likely to be simultaneously placed in Pros and Cons of the same review.", "labels": [], "entities": []}, {"text": "A straightforward way to use this information is to formulate cannot-link knowledge in clustering algorithms).", "labels": [], "entities": []}, {"text": "However, we have a particularly different manner to leverage the knowledge.", "labels": [], "entities": []}, {"text": "Due to the availability of large-scale semistructured customer reviews (as exemplified in) that are supported by many web sites, we can easily get the estimation of sentiment distribution for each aspect phrase by simply counting how many times a phrase appears in Pros and Cons respectively.", "labels": [], "entities": []}, {"text": "As illustrated in, we can see that the estimated sentiment distribution of a phrase is close to that of its aspect.", "labels": [], "entities": []}, {"text": "The above observation suggests the sentiment distribution consistency assumption: different phrases of the same aspect tend to have the same sentiment distribution, or to have statistically close distributions.", "labels": [], "entities": []}, {"text": "This assumption is also verified by our data: for most (above 91.3%) phrase with relatively reliable estimation (whose occurrence \u226550), the KL-divergence between the sentiment distribution of a phrase and that of its corresponding aspect is less than 0.05.", "labels": [], "entities": [{"text": "KL-divergence", "start_pos": 140, "end_pos": 153, "type": "METRIC", "confidence": 0.9798951745033264}]}, {"text": "It is worth noting that, the sentiment distribution of a phrase can be estimated accurately only when we obtain a sufficient number of reviews.", "labels": [], "entities": []}, {"text": "When the number of reviews is limited, however, the estimated sentiment distribution for each phrase is unreliable (as shown in.", "labels": [], "entities": []}, {"text": "A key issue, arisen here, is how to formulate this assumption in a statistically robust manner.", "labels": [], "entities": []}, {"text": "The proposed model should be robust when only a limited number of reviews are available.: The sentiment distribution of aspect \"battery\" and its related-phrases on nokia 3110c with a small mumber of reviews.", "labels": [], "entities": [{"text": "nokia 3110c", "start_pos": 164, "end_pos": 175, "type": "DATASET", "confidence": 0.9381150007247925}]}, {"text": "To deal with this issue, we model sentiment distribution consistency as soft constraint, integrated into a probabilistic model that maximizes the data likelihood.", "labels": [], "entities": []}, {"text": "We design the constraint to work in the following way: when we have sufficient observations, the constraint becomes tighter, which plays a more important role in the learning process; when we have limited observations, the constraint becomes very loose so that it will have less effect on the model.", "labels": [], "entities": []}, {"text": "In this paper, we propose a novel unsupervised model, Sentiment Distribution Consistency Regularized Multinomial Naive Bayes (SDC-MNB).", "labels": [], "entities": [{"text": "Sentiment Distribution Consistency Regularized Multinomial Naive Bayes", "start_pos": 54, "end_pos": 124, "type": "TASK", "confidence": 0.7378479668072292}]}, {"text": "The context part is modeled by Multinomial Naive Bayes in which aspect is treated as latent variable, and Sentiment distribution consistency is encoded as soft constraint within the framework of Posterior Regularization (PR) ().", "labels": [], "entities": []}, {"text": "The main contributions of this paper are summarized as follows: \u2022 We study the problem of clustering phrases by integrating both context information and sentiment distribution of aspect-related phrases.", "labels": [], "entities": []}, {"text": "\u2022 We explore a novel concept, sentiment distribution consistency(SDC), and model it as soft constraint to guide the clustering process.", "labels": [], "entities": [{"text": "sentiment distribution consistency(SDC)", "start_pos": 30, "end_pos": 69, "type": "TASK", "confidence": 0.6373563806215922}]}, {"text": "\u2022 Experiments show that our model outperforms the state-of-art approaches for aspect clustering.", "labels": [], "entities": [{"text": "aspect clustering", "start_pos": 78, "end_pos": 95, "type": "TASK", "confidence": 0.8173530995845795}]}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "We introduce the SDC-MNB model in Section 2.", "labels": [], "entities": []}, {"text": "We present experiment results in Section 3.", "labels": [], "entities": []}, {"text": "In Section 4, we survey related work.", "labels": [], "entities": []}, {"text": "We summarize the work in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "We adapt three measures Purity, Entropy, and Rand Index for performance evaluation.", "labels": [], "entities": [{"text": "Purity", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.9904912114143372}, {"text": "Entropy", "start_pos": 32, "end_pos": 39, "type": "METRIC", "confidence": 0.9772560000419617}, {"text": "Rand Index", "start_pos": 45, "end_pos": 55, "type": "METRIC", "confidence": 0.987642377614975}]}, {"text": "These measures have been commonly used to evaluate clustering algorithms.", "labels": [], "entities": []}, {"text": "Given a data set DS, suppose its gold-standard partition is G = {g 1 , ..., g j , ..., g k }, where k is the number of clusters.", "labels": [], "entities": []}, {"text": "A clustering algorithm partitions DS into k disjoint subsets, say DS 1 , DS 2 , ..., DS k . Entropy: For each resulting cluster, we can measure its entropy using Eq.", "labels": [], "entities": []}, {"text": "17, where P i (g j ) is the proportion of data points of class g j in DS i . The entropy of the entire clustering result is calculated by Eq.", "labels": [], "entities": []}, {"text": "18. Purity: Purity measures the extent that a cluster contains only data from one gold-standard partition.", "labels": [], "entities": []}, {"text": "The cluster purity is computed with Eq.", "labels": [], "entities": [{"text": "cluster purity", "start_pos": 4, "end_pos": 18, "type": "METRIC", "confidence": 0.6539632678031921}, {"text": "Eq", "start_pos": 36, "end_pos": 38, "type": "METRIC", "confidence": 0.9226820468902588}]}, {"text": "19. The total purity of the whole clustering result (all clusters) is computed with Eq.", "labels": [], "entities": [{"text": "purity", "start_pos": 14, "end_pos": 20, "type": "METRIC", "confidence": 0.9483116865158081}, {"text": "Eq", "start_pos": 84, "end_pos": 86, "type": "METRIC", "confidence": 0.9485484957695007}]}, {"text": "20.  Positive) denotes number of pairs of elements in S that are in the same set in DS and in different sets in G.", "labels": [], "entities": []}, {"text": "FN (False Negative) denotes number of pairs of elements that are in different sets in DS and in the same set in G.", "labels": [], "entities": [{"text": "FN (False Negative)", "start_pos": 0, "end_pos": 19, "type": "METRIC", "confidence": 0.813207459449768}]}, {"text": "The Rand Index(RI) is computed with Eq.", "labels": [], "entities": [{"text": "Rand Index(RI)", "start_pos": 4, "end_pos": 18, "type": "METRIC", "confidence": 0.9636819362640381}, {"text": "Eq", "start_pos": 36, "end_pos": 38, "type": "METRIC", "confidence": 0.9585992097854614}]}], "tableCaptions": [{"text": " Table 2: Statistics of the review corpus. # denotes  the size.", "labels": [], "entities": []}, {"text": " Table 3: Comparison to unsupervised baselines. (P is short for purity, E for entropy, and RI for random  index.)", "labels": [], "entities": [{"text": "purity", "start_pos": 64, "end_pos": 70, "type": "METRIC", "confidence": 0.9197878837585449}, {"text": "RI", "start_pos": 91, "end_pos": 93, "type": "METRIC", "confidence": 0.9653459191322327}]}, {"text": " Table 4: Comparison to supervised baselines.  MNB-5% means MNB with 5% labeled data.", "labels": [], "entities": []}, {"text": " Table 5. We can see that the  cellphone domain has the most informative and  largest constraint set, that may explain why SDC- MNB achieves the largest purity gain(over L-EM)  in cellphone domain.", "labels": [], "entities": [{"text": "purity gain", "start_pos": 153, "end_pos": 164, "type": "METRIC", "confidence": 0.9631901681423187}]}, {"text": " Table 5: Constraint statistics on different domains.", "labels": [], "entities": []}]}