{"title": [{"text": "Question Answering over Linked Data Using First-order Logic *", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8073123395442963}]}], "abstractContent": [{"text": "Question Answering over Linked Data (QALD) aims to evaluate a question answering system over structured data, the key objective of which is to translate questions posed using natural language into structured queries.", "labels": [], "entities": [{"text": "Question Answering over Linked Data (QALD)", "start_pos": 0, "end_pos": 42, "type": "TASK", "confidence": 0.8208065703511238}, {"text": "question answering", "start_pos": 62, "end_pos": 80, "type": "TASK", "confidence": 0.7131707817316055}]}, {"text": "This technique can help common users to directly access open-structured knowledge on the Web and, accordingly, has attracted much attention.", "labels": [], "entities": []}, {"text": "To this end, we propose a novel method using first-order logic.", "labels": [], "entities": []}, {"text": "We formulate the knowledge for resolving the ambiguities in the main three steps of QALD (phrase detection, phrase-to-semantic-item mapping and semantic item grouping) as first-order logic clauses in a Markov Logic Network.", "labels": [], "entities": [{"text": "phrase detection", "start_pos": 90, "end_pos": 106, "type": "TASK", "confidence": 0.7851244807243347}, {"text": "phrase-to-semantic-item mapping", "start_pos": 108, "end_pos": 139, "type": "TASK", "confidence": 0.7274700254201889}, {"text": "semantic item grouping)", "start_pos": 144, "end_pos": 167, "type": "TASK", "confidence": 0.7114636600017548}]}, {"text": "All clauses can then produce interacted effects in a unified framework and can jointly resolve all ambiguities.", "labels": [], "entities": []}, {"text": "Moreover, our method adopts a pattern-learning strategy for semantic item grouping.", "labels": [], "entities": [{"text": "semantic item grouping", "start_pos": 60, "end_pos": 82, "type": "TASK", "confidence": 0.7477519909540812}]}, {"text": "In this way, our method can cover more text expressions and answer more questions than previous methods using manually designed patterns.", "labels": [], "entities": []}, {"text": "The experimental results using open benchmarks demonstrate the effectiveness of the proposed method.", "labels": [], "entities": []}], "introductionContent": [{"text": "With the rapid development of the Web of Data, many RDF datasets have been published as Linked), such as DBpedia (, and).", "labels": [], "entities": [{"text": "DBpedia", "start_pos": 105, "end_pos": 112, "type": "DATASET", "confidence": 0.9375063180923462}]}, {"text": "The growing amount of Linked Data contains a wealth of knowledge, including entities, classes and relations.", "labels": [], "entities": []}, {"text": "Moreover, these linked data usually have complex structures and are highly heterogeneous.", "labels": [], "entities": []}, {"text": "As a result, there are gaps for users regarding access.", "labels": [], "entities": []}, {"text": "Although a few experts can write queries using structured languages (such as SPARQL) based on their needs, this skill cannot be easily utilized by common users.", "labels": [], "entities": []}, {"text": "Thus, providing user-friendly, simple interfaces to access these linked data becomes increasingly more urgent.", "labels": [], "entities": []}, {"text": "Because of this, question answering over linked data (QALD) () has recently received much interest, and most studies on this topic have focused on translating natural language questions into structured queries).", "labels": [], "entities": [{"text": "question answering over linked data (QALD)", "start_pos": 17, "end_pos": 59, "type": "TASK", "confidence": 0.8347251787781715}]}, {"text": "For example, with respect to the question \"Which software has been developed by organizations founded in California, USA?\", the aim is to automatically convert this utterance into an SPARQL query that contains the following subject-property-object (SPO) triple format: ?url rdf:type dbo:Software, ?url dbo:developer ?x1, ?x1 rdf:type dbo:Company, ?x1 dbo:foundationPlace dbr:California 1 . To fulfill this objective, existing systems () usually adopt a pipeline framework that contains four major steps: 1) decomposing the question and detecting phrases, 2) mapping the detected phrases into semantic items of Linked Data, 3) grouping the mapped semantic items into semantic triples, and 4) generating the correct SPARQL query.", "labels": [], "entities": []}, {"text": "However, completing these four steps and constructing such a structured query is not easy.", "labels": [], "entities": []}, {"text": "The first three steps mentioned above are subject to the problem of ambiguity, which is the major challenge in QALD.", "labels": [], "entities": [{"text": "QALD", "start_pos": 111, "end_pos": 115, "type": "TASK", "confidence": 0.48957887291908264}]}, {"text": "Using the question mentioned above as an example, we can choose California or California, USA when detecting phrases, the phrase California can be mapped to the entity California State or California Film, and the class Software (mapped from the phrase software) can be matched with the first argument of the relation producer or developer (these two relations can be mapped from the phrase developed).", "labels": [], "entities": []}, {"text": "Previous methods (;) have usually performed disambiguation at each step only, and the subsequent step was performed based on the disambiguation results in the previous step(s).", "labels": [], "entities": []}, {"text": "However, we argue that the three steps mentioned above have mutual effects.", "labels": [], "entities": []}, {"text": "In the previous example, the phrase founded in (verb) can be mapped to the entities (Founding of Rome and Founder (company)), classes (Company and Department) or relations (foundedBy and foundationPlace).", "labels": [], "entities": []}, {"text": "If we know that the phrase California can refer to the entity California State, and which can be the second argument of the relation foundationPlace, together with a verb phrase being more likely to be mapped to Relation, we should map the phrase founded in to foundationPlace in this question.", "labels": [], "entities": []}, {"text": "Thus, we aim to determine if joint disambiguation is better than individual disambiguation.", "labels": [], "entities": []}, {"text": "In addition, previous systems usually employed manually designed patterns to extract predicateargument structures that are used to guide the disambiguation process in the three steps mentioned above ().", "labels": [], "entities": []}, {"text": "For example, () used only three dependency patterns to group the mapped semantic items into semantic triples.", "labels": [], "entities": []}, {"text": "Nevertheless, these three manually designed patterns miss many cases because of the diversity of the question expressions.", "labels": [], "entities": []}, {"text": "We gathered statistics on 144 questions and found that the macro-average F1 and micro-average F1 of the three patterns 2 used in () are only 62.8 and 66.2%, respectively.", "labels": [], "entities": [{"text": "F1", "start_pos": 73, "end_pos": 75, "type": "METRIC", "confidence": 0.8391463756561279}, {"text": "F1", "start_pos": 94, "end_pos": 96, "type": "METRIC", "confidence": 0.6654672622680664}]}, {"text": "Furthermore, these specially designed patterns may not be valid with variations in domains or languages.", "labels": [], "entities": []}, {"text": "Therefore, another important question arises: can we automatically learn rules or patterns to achieve the same ob-", "labels": [], "entities": []}], "datasetContent": [{"text": "We use the following three collections of questions from the QALD 13 task for question answering over linked data: QALD-1, QALD-3 and QALD-4.", "labels": [], "entities": [{"text": "QALD 13 task", "start_pos": 61, "end_pos": 73, "type": "DATASET", "confidence": 0.708704670270284}, {"text": "question answering", "start_pos": 78, "end_pos": 96, "type": "TASK", "confidence": 0.8321194350719452}, {"text": "QALD-1", "start_pos": 115, "end_pos": 121, "type": "DATASET", "confidence": 0.6944456100463867}, {"text": "QALD-3", "start_pos": 123, "end_pos": 129, "type": "DATASET", "confidence": 0.7240175604820251}, {"text": "QALD-4", "start_pos": 134, "end_pos": 140, "type": "DATASET", "confidence": 0.8445420265197754}]}, {"text": "The generated SPARQL queries are evaluated on Linked Data from DBpedia and YAGO using a Virtuoso engine . A typical example question from the QALD benchmark is \"Which books written by Kerouac were published by Viking Press?\".", "labels": [], "entities": [{"text": "YAGO", "start_pos": 75, "end_pos": 79, "type": "METRIC", "confidence": 0.660041868686676}]}, {"text": "As mentioned in Section 2.2, our system is not designed to answer questions that contain numbers, date comparisons and aggregation operations such as group by or order by.", "labels": [], "entities": []}, {"text": "Therefore, we remove these types of questions and retain 110 questions from the QALD-4 training set for generating the specific formulas and for training their weights in MLN.", "labels": [], "entities": [{"text": "QALD-4 training set", "start_pos": 80, "end_pos": 99, "type": "DATASET", "confidence": 0.9185733993848165}, {"text": "MLN", "start_pos": 171, "end_pos": 174, "type": "DATASET", "confidence": 0.808415412902832}]}, {"text": "We test our system using 37, 75 and 26 questions from the training set of QALD-1 15 , and the testing set of QALD-3 and QALD-4 respectively.", "labels": [], "entities": [{"text": "QALD-3", "start_pos": 109, "end_pos": 115, "type": "DATASET", "confidence": 0.9117519855499268}, {"text": "QALD-4", "start_pos": 120, "end_pos": 126, "type": "DATASET", "confidence": 0.8279969692230225}]}, {"text": "We use #T, #Q and #A to indicate the total number of questions in the testing set, the number of questions we could address and the number of questions answered correct, respectively.", "labels": [], "entities": []}, {"text": "We select Precision (P = #A #Q ), Recall (R = #A #T ), and F1-score (F 1 = 2\u00b7P \u00b7R P +R ) as the evaluation metrics.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9965691566467285}, {"text": "Recall (R = #A #T )", "start_pos": 34, "end_pos": 53, "type": "METRIC", "confidence": 0.8486550649007162}, {"text": "F1-score", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.9992400407791138}]}, {"text": "To assess the effectiveness of the disambiguation process in the MLN, we computed the overall quality measures by precision and recall with the manually obtained results.", "labels": [], "entities": [{"text": "MLN", "start_pos": 65, "end_pos": 68, "type": "TASK", "confidence": 0.5822045207023621}, {"text": "precision", "start_pos": 114, "end_pos": 123, "type": "METRIC", "confidence": 0.9994975328445435}, {"text": "recall", "start_pos": 128, "end_pos": 134, "type": "METRIC", "confidence": 0.9994831085205078}]}, {"text": "The Stanford dependency parser) is used for extracting features from the dependency parse trees.", "labels": [], "entities": []}, {"text": "We use the toolkit thebeast to learn the weights of the formulas and to perform the MAP inference.", "labels": [], "entities": []}, {"text": "The inference algorithm uses a cutting plane approach.", "labels": [], "entities": []}, {"text": "In addition, for the parameter learning, we set all initial weights to zero and use an online learning algorithm with MIRA update rules to update the weights of the formulas.", "labels": [], "entities": [{"text": "MIRA update", "start_pos": 118, "end_pos": 129, "type": "METRIC", "confidence": 0.9132146239280701}]}, {"text": "The number of iterations for the training and testing are set to 10 and 200, respectively.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Examples of the argument match types.", "labels": [], "entities": []}, {"text": " Table 6: Comparisons with DEANNA using the  QALD-1 test questions.", "labels": [], "entities": [{"text": "DEANNA", "start_pos": 27, "end_pos": 33, "type": "METRIC", "confidence": 0.5544042587280273}, {"text": "QALD-1 test questions", "start_pos": 45, "end_pos": 66, "type": "DATASET", "confidence": 0.8913985888163248}]}, {"text": " Table 5: The performance of joint learning on three benchmark datasets.", "labels": [], "entities": []}, {"text": " Table 8: Comparisons with state-of-the-art sys- tems using the QALD benchmark.", "labels": [], "entities": [{"text": "QALD benchmark", "start_pos": 64, "end_pos": 78, "type": "DATASET", "confidence": 0.8068695068359375}]}, {"text": " Table 9: Performance comparisons of different weighted formulas evaluated using the QALD-3 question  set.", "labels": [], "entities": [{"text": "QALD-3 question  set", "start_pos": 85, "end_pos": 105, "type": "DATASET", "confidence": 0.9327682058016459}]}]}