{"title": [{"text": "Learning Compact Lexicons for CCG Semantic Parsing", "labels": [], "entities": []}], "abstractContent": [{"text": "We present methods to control the lexicon size when learning a Combinatory Categorial Grammar semantic parser.", "labels": [], "entities": [{"text": "Combinatory Categorial Grammar semantic parser", "start_pos": 63, "end_pos": 109, "type": "TASK", "confidence": 0.6285260677337646}]}, {"text": "Existing methods incrementally expand the lexicon by greedily adding entries, considering a single training datapoint at a time.", "labels": [], "entities": []}, {"text": "We propose using corpus-level statistics for lexicon learning decisions.", "labels": [], "entities": []}, {"text": "We introduce voting to globally consider adding entries to the lexicon, and pruning to remove entries no longer required to explain the training data.", "labels": [], "entities": []}, {"text": "Our methods result in state-of-the-art performance on the task of executing sequences of natural language instructions, achieving up to 25% error reduction, with lexicons that are up to 70% smaller and are qualitatively less noisy.", "labels": [], "entities": []}], "introductionContent": [{"text": "Combinatory Categorial Grammar, CCG, henceforth) is a commonly used formalism for semantic parsing -the task of mapping natural language sentences to formal meaning representations (.", "labels": [], "entities": [{"text": "Combinatory Categorial Grammar, CCG", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.7062653601169586}, {"text": "semantic parsing", "start_pos": 82, "end_pos": 98, "type": "TASK", "confidence": 0.7337264865636826}]}, {"text": "Recently, CCG semantic parsers have been used for numerous language understanding tasks, including querying databases), referring to physical objects), information extraction, executing instructions (), generating regular expressions), question-answering and textual entailment (.", "labels": [], "entities": [{"text": "CCG semantic parsers", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.5719016492366791}, {"text": "language understanding tasks", "start_pos": 59, "end_pos": 87, "type": "TASK", "confidence": 0.7786269783973694}, {"text": "information extraction", "start_pos": 152, "end_pos": 174, "type": "TASK", "confidence": 0.8172740042209625}, {"text": "textual entailment", "start_pos": 259, "end_pos": 277, "type": "TASK", "confidence": 0.7080536633729935}]}, {"text": "In CCG, a lexicon is used to map words to formal representations of their meaning, which are then combined using bottom-up operations.", "labels": [], "entities": []}, {"text": "In this paper we present learning techniques * This research was carried out at Google.", "labels": [], "entities": []}, {"text": "Figure 1: Lexical entries for the word chair as learned with no corpus-level statistics.", "labels": [], "entities": []}, {"text": "Our approach is able to correctly learn only the top two bolded entries.", "labels": [], "entities": []}, {"text": "to explicitly control the size of the CCG lexicon, and show that this results in improved task performance and more compact models.", "labels": [], "entities": []}, {"text": "In most approaches for inducing CCGs for semantic parsing, lexicon learning and parameter estimation are performed jointly in an online algorithm, as introduced by.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 41, "end_pos": 57, "type": "TASK", "confidence": 0.7632999122142792}, {"text": "parameter estimation", "start_pos": 80, "end_pos": 100, "type": "TASK", "confidence": 0.6814972162246704}]}, {"text": "To induce the lexicon, words extracted from the training data are paired with CCG categories one sample at a time (for an overview of CCG, see \u00a72).", "labels": [], "entities": []}, {"text": "Joint approaches have the potential advantage that only entries participating in successful parses are added to the lexicon.", "labels": [], "entities": []}, {"text": "However, new entries are added greedily and these decisions are never revisited at later stages.", "labels": [], "entities": []}, {"text": "In practice, this often results in a large and noisy lexicon.", "labels": [], "entities": []}, {"text": "lists a sample of CCG lexical entries learned for the word chair with a greedy joint algorithm).", "labels": [], "entities": []}, {"text": "In the studied navigation domain, the word chair is often used to refer to chairs and sofas, as captured by the first two entries.", "labels": [], "entities": []}, {"text": "However, the system also learns several spurious meanings: the third shows an erroneous usage of chair as an adverbial phrase describing action length, while the fourth treats it as a noun phrase and the fifth as an adjective.", "labels": [], "entities": []}, {"text": "In contrast, our approach is able to correctly learn only the top two lexical entries.", "labels": [], "entities": []}, {"text": "We present a batch algorithm focused on controlling the size of the lexicon when learning CCG semantic parsers ( \u00a73).", "labels": [], "entities": []}, {"text": "Because we make updates only after processing the entire training set, we can take corpus-wide statistics into account before each lexicon update.", "labels": [], "entities": []}, {"text": "To explicitly control the size of the lexicon, we adopt two complementary strategies: voting and pruning.", "labels": [], "entities": []}, {"text": "First, we consider the lexical evidence each sample provides as a vote towards potential entries.", "labels": [], "entities": []}, {"text": "We describe two voting strategies for deciding which entries to add to the model lexicon ( \u00a74).", "labels": [], "entities": []}, {"text": "Second, even though we use voting to only conservatively add new lexicon entries, we also prune existing entries if they are no longer necessary for parsing the training data.", "labels": [], "entities": []}, {"text": "These steps are incorporated into the learning framework, allowing us to apply stricter criteria for lexicon expansion while maintaining a single learning algorithm.", "labels": [], "entities": []}, {"text": "We evaluate our approach on the robot navigation semantic parsing task (.", "labels": [], "entities": [{"text": "robot navigation semantic parsing task", "start_pos": 32, "end_pos": 70, "type": "TASK", "confidence": 0.891654658317566}]}, {"text": "Our experimental results show that we outperform previous state of the art on executing sequences of instructions, while learning significantly more compact lexicons ( \u00a76 and).", "labels": [], "entities": []}], "datasetContent": [{"text": "To isolate the effect of our lexicon learning techniques we closely follow the experimental setup of previous work and use its publicly available code.", "labels": [], "entities": []}, {"text": "This includes the provided beam-search CKY parser, two-pass parsing for testing, beam search for executing sequences of instructions and the same seed lexicon, weight initialization and features.", "labels": [], "entities": []}, {"text": "Finally, except the optimization parameters specified below, we use the same parameter settings.", "labels": [], "entities": []}, {"text": "Data For evaluation we use two related corpora: SAIL (Chen and Mooney, 2011) and ORA-CLE ().", "labels": [], "entities": [{"text": "SAIL", "start_pos": 48, "end_pos": 52, "type": "METRIC", "confidence": 0.8656786680221558}, {"text": "ORA-CLE", "start_pos": 81, "end_pos": 88, "type": "METRIC", "confidence": 0.9966186285018921}]}, {"text": "Due to how the original data was collected (), SAIL includes many wrong executions and about 30% of all instruction sequences are infeasible (e.g., instructing the agent to walk into a wall).", "labels": [], "entities": [{"text": "SAIL", "start_pos": 47, "end_pos": 51, "type": "TASK", "confidence": 0.8593981862068176}]}, {"text": "To better understand system performance and the effect of noise, ORACLE was created with the subset of valid instructions from SAIL paired with their gold executions.", "labels": [], "entities": [{"text": "ORACLE", "start_pos": 65, "end_pos": 71, "type": "METRIC", "confidence": 0.9792563915252686}]}, {"text": "Following previous work, we use a held-out set for the ORACLE corpus and cross-validation for the SAIL corpus.", "labels": [], "entities": [{"text": "ORACLE corpus", "start_pos": 55, "end_pos": 68, "type": "DATASET", "confidence": 0.782054990530014}, {"text": "SAIL corpus", "start_pos": 98, "end_pos": 109, "type": "DATASET", "confidence": 0.9116230607032776}]}, {"text": "Systems We report two baselines.", "labels": [], "entities": []}, {"text": "Our batch baseline uses the same regularized algorithm, but updates the lexicon by adding all entries without voting and skips pruning.", "labels": [], "entities": []}, {"text": "Additionally, we added post-hoc pruning to the algorithm of Artzi and Zettlemoyer (2013b) by discarding all learned entries that are not participating in max-scoring correct parses at the end of training.", "labels": [], "entities": []}, {"text": "For ablation, we study the influence of the two voting strategies and pruning, while keeping the same regularization setting.", "labels": [], "entities": []}, {"text": "Finally, we compare our approach to previous published results on both corpora.", "labels": [], "entities": []}, {"text": "Optimization Parameters We optimized the learning parameters using cross validation on the training data to maximize recall of complete sequence execution and minimize lexicon size.", "labels": [], "entities": [{"text": "recall", "start_pos": 117, "end_pos": 123, "type": "METRIC", "confidence": 0.9916664958000183}]}, {"text": "We use 10 training iterations and the learning rate \u00b5 = 0.1.", "labels": [], "entities": [{"text": "learning rate \u00b5", "start_pos": 38, "end_pos": 53, "type": "METRIC", "confidence": 0.9373934467633566}]}, {"text": "For SAIL we set the regularization parameter \u03b3 = 1.0 and for ORACLE \u03b3 = 0.5.", "labels": [], "entities": [{"text": "ORACLE \u03b3", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.9725788235664368}]}, {"text": "Full Sequence Inference To execute sequences of instructions we use the beam search procedure of Artzi and Zettlemoyer (2013b) with an identical beam size of 10.", "labels": [], "entities": []}, {"text": "The beam stores states, and is initialized with the starting state.", "labels": [], "entities": []}, {"text": "Instructions are executed in order, each is attempted from all states currently in the beam, the beam is then updated and pruned to keep the 10-best states.", "labels": [], "entities": []}, {"text": "At the end, the best scoring state in the beam is returned.", "labels": [], "entities": []}, {"text": "Evaluation Metrics We evaluate the end-to-end task of executing complete sequences of instructions against an oracle final state.", "labels": [], "entities": []}, {"text": "In addition, to better understand the results, we also measure task completion for single instructions.", "labels": [], "entities": []}, {"text": "We repeated: Ablation study using cross-validation on the ORACLE corpus training data.", "labels": [], "entities": [{"text": "ORACLE corpus training data", "start_pos": 58, "end_pos": 85, "type": "DATASET", "confidence": 0.9375610947608948}]}, {"text": "We report mean precision (P), recall (R) and harmonic mean (F1) of execution accuracy on single sentences and sequences of instructions and mean lexicon sizes.", "labels": [], "entities": [{"text": "mean precision (P)", "start_pos": 10, "end_pos": 28, "type": "METRIC", "confidence": 0.9222295880317688}, {"text": "recall (R)", "start_pos": 30, "end_pos": 40, "type": "METRIC", "confidence": 0.9562867432832718}, {"text": "harmonic mean (F1)", "start_pos": 45, "end_pos": 63, "type": "METRIC", "confidence": 0.9188531279563904}, {"text": "accuracy", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.5167022347450256}]}, {"text": "Bold numbers represent the best performing method on a given metric.", "labels": [], "entities": []}, {"text": "Final  Our final results compared to previous work on the SAIL and ORACLE corpora.", "labels": [], "entities": [{"text": "SAIL and ORACLE corpora", "start_pos": 58, "end_pos": 81, "type": "DATASET", "confidence": 0.6759669110178947}]}, {"text": "We report mean precision (P), recall (R), harmonic mean (F1) and lexicon size results and standard deviation between runs (in parenthesis) when appropriate.", "labels": [], "entities": [{"text": "mean precision (P)", "start_pos": 10, "end_pos": 28, "type": "METRIC", "confidence": 0.8876281261444092}, {"text": "recall (R)", "start_pos": 30, "end_pos": 40, "type": "METRIC", "confidence": 0.9610304534435272}, {"text": "harmonic mean (F1)", "start_pos": 42, "end_pos": 60, "type": "METRIC", "confidence": 0.8970865130424499}]}, {"text": "Our Approach stands for batch learning with a consensus voting and pruning.", "labels": [], "entities": [{"text": "Approach", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9838017821311951}]}, {"text": "Bold numbers represent the best performing method on a given metric.", "labels": [], "entities": []}, {"text": "each experiment five times and report mean precision, recall, 5 harmonic mean (F1) and lexicon size.", "labels": [], "entities": [{"text": "precision", "start_pos": 43, "end_pos": 52, "type": "METRIC", "confidence": 0.8439270853996277}, {"text": "recall", "start_pos": 54, "end_pos": 60, "type": "METRIC", "confidence": 0.9995009899139404}, {"text": "5 harmonic mean (F1)", "start_pos": 62, "end_pos": 82, "type": "METRIC", "confidence": 0.7567445337772369}]}, {"text": "For held-out test results we also report standard deviation.", "labels": [], "entities": [{"text": "standard", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.9892527461051941}]}, {"text": "For the baseline online experiments we shuffled the training data between runs.", "labels": [], "entities": []}, {"text": "shows ablation results for 5-fold crossvalidation on the ORACLE training data.", "labels": [], "entities": [{"text": "ablation", "start_pos": 6, "end_pos": 14, "type": "METRIC", "confidence": 0.9929179549217224}, {"text": "ORACLE training data", "start_pos": 57, "end_pos": 77, "type": "DATASET", "confidence": 0.8514581521352133}]}, {"text": "We evaluate against the online learning algorithm of, an extension of it to include post-hoc pruning and a batch baseline.", "labels": [], "entities": []}, {"text": "Our best sequence execution development result is obtained with CONSENSUSVOTE and pruning.", "labels": [], "entities": [{"text": "CONSENSUSVOTE", "start_pos": 64, "end_pos": 77, "type": "METRIC", "confidence": 0.9440332055091858}]}], "tableCaptions": [{"text": " Table 1: Ablation study using cross-validation on the ORACLE corpus training data. We report mean precision  (P), recall (R) and harmonic mean (F1) of execution accuracy on single sentences and sequences of instructions  and mean lexicon sizes. Bold numbers represent the best performing method on a given metric.", "labels": [], "entities": [{"text": "ORACLE corpus training data", "start_pos": 55, "end_pos": 82, "type": "DATASET", "confidence": 0.8869007080793381}, {"text": "mean precision  (P)", "start_pos": 94, "end_pos": 113, "type": "METRIC", "confidence": 0.8795518755912781}, {"text": "recall (R)", "start_pos": 115, "end_pos": 125, "type": "METRIC", "confidence": 0.955088809132576}, {"text": "harmonic mean (F1) of execution accuracy", "start_pos": 130, "end_pos": 170, "type": "METRIC", "confidence": 0.8069733083248138}]}, {"text": " Table 3: Example entries from a learned ORACLE corpus lexicon using batch learning. For each string we  report the number of lexical entries without voting (CONSENSUSVOTE) and pruning and with, and provide a few  examples. Struck entries were successfully avoided when using voting and pruning.", "labels": [], "entities": [{"text": "CONSENSUSVOTE", "start_pos": 158, "end_pos": 171, "type": "METRIC", "confidence": 0.9247967004776001}]}]}