{"title": [{"text": "Resolving Referring Expressions in Conversational Dialogs for Natural User Interfaces", "labels": [], "entities": [{"text": "Resolving Referring Expressions in Conversational Dialogs", "start_pos": 0, "end_pos": 57, "type": "TASK", "confidence": 0.9223278760910034}]}], "abstractContent": [{"text": "Unlike traditional over-the-phone spoken dialog systems (SDSs), modern dialog systems tend to have visual rendering on the device screen as an additional modality to communicate the system's response to the user.", "labels": [], "entities": []}, {"text": "Visual display of the system's response not only changes human behavior when interacting with devices, but also creates new research areas in SDSs.", "labels": [], "entities": [{"text": "SDSs", "start_pos": 142, "end_pos": 146, "type": "TASK", "confidence": 0.959323525428772}]}, {"text": "On-screen item identification and resolution in utterances is one critical problem to achieve a natural and accurate human-machine communication.", "labels": [], "entities": [{"text": "On-screen item identification and resolution in utterances", "start_pos": 0, "end_pos": 58, "type": "TASK", "confidence": 0.7065904693944114}]}, {"text": "We pose the problem as a classification task to correctly identify intended on-screen item(s) from user utterances.", "labels": [], "entities": []}, {"text": "Using syntactic, semantic as well as context features from the display screen, our model can resolve different types of referring expressions with up to 90% accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 157, "end_pos": 165, "type": "METRIC", "confidence": 0.9977982044219971}]}, {"text": "In the experiments we also show that the proposed model is robust to domain and screen layout changes.", "labels": [], "entities": []}], "introductionContent": [{"text": "Todays natural user interfaces (NUI) for applications running on smart devices, e.g, phones (SIRI, Cortana, GoogleNow), consoles (Amazon FireTV, XBOX), tablet, etc., can handle not only simple spoken commands, but also natural conversational utterances.", "labels": [], "entities": []}, {"text": "Unlike traditional over-the-phone spoken dialog systems (SDSs), user hears and sees the system's response displayed on the screen as an additional modality.", "labels": [], "entities": []}, {"text": "Having visual access to the system's response and results changes human behavior when interacting with the machine, creating new and challenging problems in SDS.: How can i help you today ?: Find non-fiction books by Chomsky.", "labels": [], "entities": [{"text": "SDS.", "start_pos": 157, "end_pos": 161, "type": "TASK", "confidence": 0.9593715667724609}]}, {"text": "[System]: (Fetches the following books from database): \"show details for the oldest production\" or \"details for the syntax book\" or \"open the last one\" or \"i want to seethe one on linguistics\" or \"bring me Jurafsky's text book\" Consider a sample dialog in between a user and a NUI in the books domain.", "labels": [], "entities": []}, {"text": "After the system displays results on the screen, the user may choose one or more of the on-screen items with natural language utterances as shown in.", "labels": [], "entities": []}, {"text": "Note that, there are multiple ways of referring to the same item, (e.g. the last book) . To achieve a natural and accurate human to machine conversation, it is crucial to accurately identify and resolve referring expressions in utterances.", "labels": [], "entities": []}, {"text": "As important as interpreting referring expressions (REs) is for modern NUI designs, relatively few studies have investigated withing the SDSs.", "labels": [], "entities": [{"text": "interpreting referring expressions (REs)", "start_pos": 16, "end_pos": 56, "type": "TASK", "confidence": 0.902265707651774}]}, {"text": "Those that do focus on the impact of the input from multimodal interfaces such as gesture for understanding), touch for ASR error correction, or cues from the screen ().", "labels": [], "entities": [{"text": "ASR error correction", "start_pos": 120, "end_pos": 140, "type": "TASK", "confidence": 0.8184955219427744}]}, {"text": "Most of these systems are engineered fora specific task, making it harder to generalize for different domains or SDSs.", "labels": [], "entities": []}, {"text": "In this paper, we investigate a rather generic contextual model for resolving natural language REs for on-screen item selection to improve conversational understanding.", "labels": [], "entities": [{"text": "conversational understanding", "start_pos": 139, "end_pos": 167, "type": "TASK", "confidence": 0.7715551555156708}]}, {"text": "Our model, which we call FIS (Flexible Item Selection), is able to (1) detect if the user is referring to any item(s) on the screen, and (2) resolve REs to identify which items are referred to and score each item.", "labels": [], "entities": [{"text": "FIS", "start_pos": 25, "end_pos": 28, "type": "METRIC", "confidence": 0.9908111095428467}]}, {"text": "FIS is a learning based system that uses information from pair of user utterance and candidate items on the screen to model association between them.", "labels": [], "entities": [{"text": "FIS", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8493852019309998}]}, {"text": "We cast the task as a classification problem to determine whether there is a relation between the utterance and the item, representing each instance in the training dataset as relational features.", "labels": [], "entities": []}, {"text": "Ina typical SDS, the spoken language understanding (SLU) engine maps user utterances into meaning representation by identifying user's intent and token level semantic slots via a semantic parser).", "labels": [], "entities": [{"text": "spoken language understanding (SLU) engine maps user utterances into meaning representation", "start_pos": 21, "end_pos": 112, "type": "TASK", "confidence": 0.7034088808756608}]}, {"text": "The dialog manager uses the SLU components to decide on the correct system action.", "labels": [], "entities": []}, {"text": "For on-screen item selection SLU alone may not be sufficient.", "labels": [], "entities": [{"text": "item selection SLU", "start_pos": 14, "end_pos": 32, "type": "TASK", "confidence": 0.5571592350800832}]}, {"text": "To correctly associate the user's utterance with any of the onscreen items one would need to resolve the relational information between the utterance and the items.", "labels": [], "entities": []}, {"text": "For instance, consider the dialog in Table 1.", "labels": [], "entities": []}, {"text": "SLU engine can provide signals to the dialog model about the selected item, e.g., that \"linguistics\" is a book-genre or content, but may not be enough to indicate which book the user is referring.", "labels": [], "entities": []}, {"text": "FIS module provides additional information for the dialog manager by augmenting SLU components.", "labels": [], "entities": [{"text": "FIS", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8923706412315369}]}, {"text": "In \u00a73, we provide details about our data as well as data collection and annotation steps.", "labels": [], "entities": [{"text": "data collection", "start_pos": 52, "end_pos": 67, "type": "TASK", "confidence": 0.761650413274765}]}, {"text": "In \u00a74, we present various syntactic and semantic features to resolve different REs in utterances.", "labels": [], "entities": []}, {"text": "In the experiments ( \u00a76), we evaluate the individual impact of each feature on the FIS model.", "labels": [], "entities": [{"text": "FIS model", "start_pos": 83, "end_pos": 92, "type": "DATASET", "confidence": 0.822166234254837}]}, {"text": "We analyze the performance of the FIS model per each type of REs.", "labels": [], "entities": [{"text": "FIS", "start_pos": 34, "end_pos": 37, "type": "DATASET", "confidence": 0.5167773365974426}]}, {"text": "Finally, we empirically investigate the robustness of the FIS model to domain and display screen changes.", "labels": [], "entities": [{"text": "FIS model", "start_pos": 58, "end_pos": 67, "type": "DATASET", "confidence": 0.8042668104171753}]}, {"text": "When tested on a domain that is unseen to the training data or on a device that has a different NUI design, the performance only slightly degrades proving its robustness to domain and design changes.", "labels": [], "entities": []}], "datasetContent": [{"text": "We investigate several aspects of the SISI model including its robustness in resolving REs for domain or device variability.", "labels": [], "entities": [{"text": "SISI", "start_pos": 38, "end_pos": 42, "type": "TASK", "confidence": 0.9473512768745422}]}, {"text": "We start with the details of the data and model parameters.", "labels": [], "entities": []}, {"text": "We collect around 16K utterances in the media domains (movies, music, tv, and books) and around 10K utterances in places (businesses and   locations) domain.", "labels": [], "entities": []}, {"text": "We also construct additional negative instances from utterance-item pairs using first turn non-selection queries, which mainly indicate anew search or starting over.", "labels": [], "entities": []}, {"text": "In total we compile around 250K utterance-item pairs for media domains and 150K utterance-item pairs for the places domain.", "labels": [], "entities": []}, {"text": "We randomly split each collection into 60%-20%-20% parts to construct the train/dev/test datasets.", "labels": [], "entities": []}, {"text": "We use the dev set to tune the regularization parameter for the GBDT and SVM using LIBSVM (Chang and Lin, 2011) with linear kernel.", "labels": [], "entities": [{"text": "GBDT", "start_pos": 64, "end_pos": 68, "type": "DATASET", "confidence": 0.9662964344024658}]}, {"text": "We use the training dataset to build the SLU intent and slot models for each domain.", "labels": [], "entities": [{"text": "slot", "start_pos": 56, "end_pos": 60, "type": "METRIC", "confidence": 0.849301278591156}]}, {"text": "For the intent model, we use the GBDT classifier with ngram and lexicon features.", "labels": [], "entities": [{"text": "GBDT", "start_pos": 33, "end_pos": 37, "type": "DATASET", "confidence": 0.8231711387634277}]}, {"text": "The lexicon entries are obtained from Freebase and are used as indicator variables, e.g., whether the utterance contains an instance which exists in the lexicon.", "labels": [], "entities": []}, {"text": "Similarly, we train a semantic slot tagging model using CRF method.", "labels": [], "entities": [{"text": "semantic slot tagging", "start_pos": 22, "end_pos": 43, "type": "TASK", "confidence": 0.6247367163499197}]}, {"text": "We use n-gram features with up to fivegram window, and lexicon features similar to the intent models.", "labels": [], "entities": []}, {"text": "shows the accuracy and Fscore values of SLU models on the test data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9997583031654358}, {"text": "Fscore", "start_pos": 23, "end_pos": 29, "type": "METRIC", "confidence": 0.9997630715370178}]}, {"text": "The slot and intent performance is consistent accroess domains.", "labels": [], "entities": [{"text": "slot", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.8679795861244202}]}, {"text": "The books domain has only two intents and hence we observe much better intent performance compared to other domains.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 5: Performance of the FIS models on test data using different features. Acc:Accuracy,. SIM: sim- ilarity features; SLU:Spoken Language Understanding features (intent and slot features); SLL:Semantic  Locational Labeler features; Gold: using true intent and slot values, Pred.: using predicted intent and  slot values from the SLU models.", "labels": [], "entities": [{"text": "Acc", "start_pos": 79, "end_pos": 82, "type": "METRIC", "confidence": 0.9962890148162842}, {"text": "Accuracy", "start_pos": 83, "end_pos": 91, "type": "METRIC", "confidence": 0.989639163017273}, {"text": "Pred.", "start_pos": 277, "end_pos": 282, "type": "METRIC", "confidence": 0.9961246848106384}]}, {"text": " Table 4: The performance of the SLU Engine's  intent detection models in accuracy (Acc.) and slot  tagging models in F-Score on the test dataset.", "labels": [], "entities": [{"text": "intent detection", "start_pos": 47, "end_pos": 63, "type": "TASK", "confidence": 0.676554799079895}, {"text": "accuracy (Acc.)", "start_pos": 74, "end_pos": 89, "type": "METRIC", "confidence": 0.8393498063087463}, {"text": "slot  tagging", "start_pos": 94, "end_pos": 107, "type": "TASK", "confidence": 0.7137663662433624}, {"text": "F-Score", "start_pos": 118, "end_pos": 125, "type": "METRIC", "confidence": 0.9713872671127319}]}, {"text": " Table 6: Distribution of referring expressions  (RE) in the media (large screen like tv) and places  (handheld device like phone) corpus and the FIS  accuracies per RE type.", "labels": [], "entities": [{"text": "Distribution of referring expressions  (RE)", "start_pos": 10, "end_pos": 53, "type": "TASK", "confidence": 0.6309918080057416}, {"text": "FIS", "start_pos": 146, "end_pos": 149, "type": "DATASET", "confidence": 0.702948808670044}]}, {"text": " Table 8: Accuracy of FIS models tested on domains that", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.997998058795929}]}, {"text": " Table 9: Sample of utterances collected from media and", "labels": [], "entities": [{"text": "Sample of utterances", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.8670871456464132}]}, {"text": " Table 10: Accuracy of FIS models tested on two separate", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.9990148544311523}, {"text": "FIS", "start_pos": 23, "end_pos": 26, "type": "METRIC", "confidence": 0.4441632330417633}]}]}