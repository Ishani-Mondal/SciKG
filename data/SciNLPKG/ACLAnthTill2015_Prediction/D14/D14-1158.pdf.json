{"title": [{"text": "Language Modeling with Power Low Rank Ensembles", "labels": [], "entities": [{"text": "Language Modeling", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.6964922100305557}]}], "abstractContent": [{"text": "We present power low rank ensembles (PLRE), a flexible framework for n-gram language modeling where ensembles of low rank matrices and tensors are used to obtain smoothed probability estimates of words in context.", "labels": [], "entities": []}, {"text": "Our method can be understood as a generalization of n-gram modeling to non-integer n, and includes standard techniques such as absolute discounting and Kneser-Ney smoothing as special cases.", "labels": [], "entities": []}, {"text": "PLRE training is efficient and our approach outperforms state-of-the-art modified Kneser Ney baselines in terms of perplexity on large corpora as well as on BLEU score in a downstream machine translation task.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 157, "end_pos": 167, "type": "METRIC", "confidence": 0.9777321219444275}, {"text": "machine translation task", "start_pos": 184, "end_pos": 208, "type": "TASK", "confidence": 0.7449583311875662}]}], "introductionContent": [{"text": "Language modeling is the task of estimating the probability of sequences of words in a language and is an important component in, among other applications, automatic speech recognition and machine translation.", "labels": [], "entities": [{"text": "Language modeling", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7158577591180801}, {"text": "estimating the probability of sequences of words in a language", "start_pos": 33, "end_pos": 95, "type": "TASK", "confidence": 0.5822426855564118}, {"text": "automatic speech recognition", "start_pos": 156, "end_pos": 184, "type": "TASK", "confidence": 0.656350831190745}, {"text": "machine translation", "start_pos": 189, "end_pos": 208, "type": "TASK", "confidence": 0.8062519729137421}]}, {"text": "The predominant approach to language modeling is the n-gram model, wherein the probability of a word sequence P (w 1 , . .", "labels": [], "entities": [{"text": "language modeling", "start_pos": 28, "end_pos": 45, "type": "TASK", "confidence": 0.7897639274597168}]}, {"text": ", w ) is decomposed using the chain rule, and then a Markov assumption is made: P (w 1 , . .", "labels": [], "entities": []}, {"text": ", w ) \u2248 i=1 P (w i |w i\u22121 i\u2212n+1 ).", "labels": [], "entities": []}, {"text": "While this assumption substantially reduces the modeling complexity, parameter estimation remains a major challenge.", "labels": [], "entities": [{"text": "parameter estimation", "start_pos": 69, "end_pos": 89, "type": "TASK", "confidence": 0.7586139142513275}]}, {"text": "Due to the power-law nature of language, the maximum likelihood estimator massively overestimates the probability of rare events and assigns zero probability to legitimate word sequences that happen not to have been observed in the training data).", "labels": [], "entities": []}, {"text": "Many smoothing techniques have been proposed to address the estimation challenge.", "labels": [], "entities": [{"text": "estimation", "start_pos": 60, "end_pos": 70, "type": "TASK", "confidence": 0.9770077466964722}]}, {"text": "These reassign probability mass (generally from overestimated events) to unseen word sequences, whose probabilities are estimated by interpolating with or backing off to lower order n-gram models.", "labels": [], "entities": []}, {"text": "Somewhat surprisingly, these widely used smoothing techniques differ substantially from techniques for coping with data sparsity in other domains, such as collaborative filtering) or matrix completion.", "labels": [], "entities": [{"text": "matrix completion", "start_pos": 183, "end_pos": 200, "type": "TASK", "confidence": 0.8239727318286896}]}, {"text": "In these areas, low rank approaches based on matrix factorization play a central role ().", "labels": [], "entities": []}, {"text": "For example, in recommender systems, a key challenge is dealing with the sparsity of ratings from a single user, since typical users will have rated only a few items.", "labels": [], "entities": []}, {"text": "By projecting the low rank representation of a user's (sparse) preferences into the original space, an estimate of ratings for new items is obtained.", "labels": [], "entities": []}, {"text": "These methods are attractive due to their computational efficiency and mathematical well-foundedness.", "labels": [], "entities": []}, {"text": "In this paper, we introduce power low rank ensembles (PLRE), in which low rank tensors are used to produce smoothed estimates for n-gram probabilities.", "labels": [], "entities": []}, {"text": "Ideally, we would like the low rank structures to discover semantic and syntactic relatedness among words and n-grams, which are used to produce smoothed estimates for word sequence probabilities.", "labels": [], "entities": []}, {"text": "In contrast to the few previous low rank language modeling approaches, PLRE is not orthogonal to n-gram models, but rather a general framework where existing n-gram smoothing methods such as Kneser-Ney smoothing are special cases.", "labels": [], "entities": []}, {"text": "A key insight is that PLRE does not compute low rank approximations of the original joint count matrices (in the case of bigrams) or tensors i.e. multi-way arrays (in the case of 3-grams and above), but instead altered quantities of these counts based on an element-wise power operation, similar to how some smoothing methods modify their lower order distributions.", "labels": [], "entities": []}, {"text": "Moreover, PLRE has two key aspects that lead to easy scalability for large corpora and vocabularies.", "labels": [], "entities": []}, {"text": "First, since it utilizes the original n-grams, the ranks required for the low rank matrices and tensors tend to be remain tractable (e.g. around 100 fora vocabulary size V \u2248 1 \u00d7 10 6 ) leading to fast training times.", "labels": [], "entities": []}, {"text": "This differentiates our approach over other methods that leverage an underlying latent space such as neural networks ( or soft-class models where the underlying dimension is required to be quite large to obtain good performance.", "labels": [], "entities": []}, {"text": "Moreover, attest time, the probability of a sequence can be queried in time O(\u03ba max ) where \u03ba max is the maximum rank of the low rank matrices/tensors used.", "labels": [], "entities": []}, {"text": "While this is larger than Kneser Ney's virtually constant query time, it is substantially faster than conditional exponential family models) and neural networks which require O(V ) for exact computation of the normalization constant.", "labels": [], "entities": [{"text": "O", "start_pos": 175, "end_pos": 176, "type": "METRIC", "confidence": 0.9687883853912354}]}, {"text": "See Section 7 fora more detailed discussion of related work.", "labels": [], "entities": []}, {"text": "Outline: We first review existing n-gram smoothing methods ( \u00a72) and then present the intuition behind the key components of our technique: rank ( \u00a73.1) and power ( \u00a73.2).", "labels": [], "entities": []}, {"text": "We then show how these can be interpolated into an ensemble ( \u00a74).", "labels": [], "entities": []}, {"text": "In the experimental evaluation on English and Russian corpora ( \u00a75), we find that PLRE outperforms Kneser-Ney smoothing and all its variants, as well as class-based language models.", "labels": [], "entities": []}, {"text": "We also include a comparison to the log-bilinear neural language model and evaluate performance on a downstream machine translation task ( \u00a76) where our method achieves consistent improvements in BLEU.", "labels": [], "entities": [{"text": "machine translation task", "start_pos": 112, "end_pos": 136, "type": "TASK", "confidence": 0.7893146077791849}, {"text": "BLEU", "start_pos": 196, "end_pos": 200, "type": "METRIC", "confidence": 0.9952221512794495}]}], "datasetContent": [{"text": "To evaluate PLRE, we compared its performance on English and Russian corpora with several vari-ants of KN smoothing, class-based models, and the log-bilinear neural language model.", "labels": [], "entities": [{"text": "PLRE", "start_pos": 12, "end_pos": 16, "type": "TASK", "confidence": 0.9102703928947449}]}, {"text": "We evaluated with perplexity inmost of our experiments, but also provide results evaluated with BLEU () on a downstream machine translation (MT) task.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 96, "end_pos": 100, "type": "METRIC", "confidence": 0.9993222951889038}, {"text": "machine translation (MT) task", "start_pos": 120, "end_pos": 149, "type": "TASK", "confidence": 0.8388023674488068}]}, {"text": "We have made the code for our approach publicly available . To build the hard class-based LMs, we utilized mkcls 4 , a tool to train word classes that uses the maximum likelihood criterion for classing.", "labels": [], "entities": []}, {"text": "We subsequently trained trigram class language models on these classes (corresponding to 2 nd -order HMMs) using SRILM, with KN-smoothing for the class transition probabilities.", "labels": [], "entities": [{"text": "SRILM", "start_pos": 113, "end_pos": 118, "type": "DATASET", "confidence": 0.6635326743125916}]}, {"text": "SRILM was also used for the baseline KN-smoothed models.", "labels": [], "entities": [{"text": "SRILM", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.7356083989143372}]}, {"text": "For our MT evaluation, we built a hierarchical phrase translation (Chiang, 2007) system using cdec ().", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 8, "end_pos": 21, "type": "TASK", "confidence": 0.9214149415493011}, {"text": "phrase translation", "start_pos": 47, "end_pos": 65, "type": "TASK", "confidence": 0.7296076416969299}]}, {"text": "The KN-smoothed models in the MT experiments were compiled using KenLM (Heafield, 2011).", "labels": [], "entities": [{"text": "MT", "start_pos": 30, "end_pos": 32, "type": "TASK", "confidence": 0.9535927772521973}, {"text": "KenLM", "start_pos": 65, "end_pos": 70, "type": "METRIC", "confidence": 0.7827001810073853}]}, {"text": "For the perplexity experiments, we evaluated our proposed approach on 4 datasets, 2 in English and 2 in Russian.", "labels": [], "entities": []}, {"text": "In all cases, the singletons were replaced with \"<unk>\" tokens in the training corpus, and any word not in the vocabulary was replaced with this token during evaluation.", "labels": [], "entities": []}, {"text": "There is a general dearth of evaluation on large-scale corpora in morphologically rich languages such as Russian, and thus we have made the processed Large-Russian corpus available for comparison 3 . \u2022 Small-English: APNews corpus (: Train -14 million words, Dev -963,000, Test -963,000.", "labels": [], "entities": [{"text": "APNews corpus", "start_pos": 217, "end_pos": 230, "type": "DATASET", "confidence": 0.89512038230896}]}, {"text": "\u2022 Small-Russian: Subset of Russian news commentary data from 2013 WMT translation task 5 : Train-3.5 million words, Dev -400,000 Test -400,000.", "labels": [], "entities": [{"text": "WMT translation task", "start_pos": 66, "end_pos": 86, "type": "TASK", "confidence": 0.500526616970698}]}, {"text": "\u2022 Large-English: English Gigaword, Training -837 million words, Dev -8.7 million, Test -8.7 million.", "labels": [], "entities": [{"text": "Test", "start_pos": 82, "end_pos": 86, "type": "METRIC", "confidence": 0.8698058724403381}]}, {"text": "\u2022 Large-Russian: Monolingual data from WMT 2013 task.", "labels": [], "entities": [{"text": "WMT 2013 task", "start_pos": 39, "end_pos": 52, "type": "DATASET", "confidence": 0.7116560240586599}]}, {"text": "Training -521 million words, Validation -50,000, Test -50,000.", "labels": [], "entities": [{"text": "Validation", "start_pos": 29, "end_pos": 39, "type": "METRIC", "confidence": 0.7787512540817261}, {"text": "Test", "start_pos": 49, "end_pos": 53, "type": "METRIC", "confidence": 0.9687787890434265}]}, {"text": "For the MT evaluation, we used the parallel data from the WMT 2013 shared task, excluding the Common Crawl corpus data.", "labels": [], "entities": [{"text": "MT", "start_pos": 8, "end_pos": 10, "type": "TASK", "confidence": 0.9905643463134766}, {"text": "WMT 2013 shared task", "start_pos": 58, "end_pos": 78, "type": "DATASET", "confidence": 0.9137486815452576}, {"text": "Common Crawl corpus data", "start_pos": 94, "end_pos": 118, "type": "DATASET", "confidence": 0.8661826997995377}]}, {"text": "The newstest2012 and newstest2013 evaluation sets were used as the development and test sets respectively.", "labels": [], "entities": [{"text": "newstest2013 evaluation sets", "start_pos": 21, "end_pos": 49, "type": "DATASET", "confidence": 0.8595027923583984}]}], "tableCaptions": [{"text": " Table 1: Perplexity results on small corpora for all methods.", "labels": [], "entities": []}, {"text": " Table 2: Mean perplexity results on large corpora,  with standard deviation.", "labels": [], "entities": [{"text": "Mean", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.994076132774353}]}]}