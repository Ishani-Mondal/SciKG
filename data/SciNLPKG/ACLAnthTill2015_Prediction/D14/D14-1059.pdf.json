{"title": [{"text": "NaturalLI: Natural Logic Inference for Common Sense Reasoning", "labels": [], "entities": [{"text": "Common Sense Reasoning", "start_pos": 39, "end_pos": 61, "type": "TASK", "confidence": 0.6936148603757223}]}], "abstractContent": [{"text": "Common-sense reasoning is important for AI applications, both in NLP and many vision and robotics tasks.", "labels": [], "entities": [{"text": "Common-sense reasoning", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.7635261118412018}]}, {"text": "We propose NaturalLI: a Natural Logic inference system for inferring commonsense facts-for instance, that cats have tails or tomatoes are round-from a very large database of known facts.", "labels": [], "entities": []}, {"text": "In addition to being able to provide strictly valid derivations, the system is also able to produce derivations which are only likely valid, accompanied by an associated confidence.", "labels": [], "entities": []}, {"text": "We both show that our system is able to capture strict Natural Logic inferences on the Fra-CaS test suite, and demonstrate its ability to predict commonsense facts with 49% recall and 91% precision.", "labels": [], "entities": [{"text": "Fra-CaS test suite", "start_pos": 87, "end_pos": 105, "type": "DATASET", "confidence": 0.927348792552948}, {"text": "recall", "start_pos": 173, "end_pos": 179, "type": "METRIC", "confidence": 0.9991970658302307}, {"text": "precision", "start_pos": 188, "end_pos": 197, "type": "METRIC", "confidence": 0.9954130053520203}]}], "introductionContent": [{"text": "We approach the task of database completion: given a database of true facts, we would like to predict whether an unseen fact is true and should belong in the database.", "labels": [], "entities": [{"text": "database completion", "start_pos": 24, "end_pos": 43, "type": "TASK", "confidence": 0.8127590715885162}]}, {"text": "This is intuitively cast as an inference problem from a collection of candidate premises to the truth of the query.", "labels": [], "entities": []}, {"text": "For example, we would like to infer that no carnivores eat animals is false given a database containing the cat ate a mouse (see).", "labels": [], "entities": []}, {"text": "These inferences are difficult to capture in a principled way while maintaining high recall, particularly for large scale open-domain tasks.", "labels": [], "entities": [{"text": "recall", "start_pos": 85, "end_pos": 91, "type": "METRIC", "confidence": 0.9991108775138855}]}, {"text": "Learned inference rules are difficult to generalize to arbitrary relations, and standard IR methods easily miss small but semantically important lexical differences.", "labels": [], "entities": []}, {"text": "Furthermore, many methods require explicitly modeling either the database, the query, or both in a formal meaning representation (e.g., Freebase tuples).", "labels": [], "entities": []}, {"text": "Although projects like the Abstract Meaning Representation (  The path to the boxed premise the cat ate a mouse disproves the query no carnivores eat animals, as it passes through the negation relation ().", "labels": [], "entities": [{"text": "Abstract Meaning Representation", "start_pos": 27, "end_pos": 58, "type": "TASK", "confidence": 0.682290275891622}]}, {"text": "This path is one of many candidates taken; the premise is one of many known facts in the database.", "labels": [], "entities": []}, {"text": "The edge labels denote Natural Logic inference steps.", "labels": [], "entities": []}, {"text": "headway in providing broad-coverage meaning representations, it remains appealing to use human language as the vessel for inference.", "labels": [], "entities": []}, {"text": "Furthermore, OpenIE and similar projects have been very successful at collecting databases of natural language snippets from an ever-increasing corpus of unstructured text.", "labels": [], "entities": []}, {"text": "These factors motivate our use of Natural Logic -a proof system built on the syntax of human language -for broad coverage database completion.", "labels": [], "entities": [{"text": "broad coverage database completion", "start_pos": 107, "end_pos": 141, "type": "TASK", "confidence": 0.7537510097026825}]}, {"text": "Prior work on Natural Logic has focused on inferences from a single relevant premise, making use of only formally valid inferences.", "labels": [], "entities": []}, {"text": "We improve upon computational Natural Logic in three ways: (i) our approach operates over a very large set of candidate premises simultaneously; (ii) we do not require explicit alignment between a premise and the query; and (iii) we allow imprecise inferences at an associated cost learned from data.", "labels": [], "entities": [{"text": "computational Natural Logic", "start_pos": 16, "end_pos": 43, "type": "TASK", "confidence": 0.6581971148649851}]}, {"text": "Our approach casts inference as a single unified search problem from a query to any valid supporting premise.", "labels": [], "entities": []}, {"text": "Each transition along the search denotes a (reverse) inference step in Natural Logic, and incurs a cost reflecting the system's confidence in the validity of that step.", "labels": [], "entities": []}, {"text": "This approach offers two contributions over prior work in database completion: (i) it allows for unstructured text as the input database without any assumptions about the schema or domain of the text, and (ii) it proposes Natural Logic for inference, rather than translating to a formal logic syntax.", "labels": [], "entities": [{"text": "database completion", "start_pos": 58, "end_pos": 77, "type": "TASK", "confidence": 0.7866490185260773}]}, {"text": "Moreover, the entire pipeline is implemented in a single elegant search framework, which scales easily to large databases.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our system on two tasks: the FraCaS test suite, used by, evaluates the system's ability to capture Natural Logic inferences even without the explicit alignments of these previous systems.", "labels": [], "entities": [{"text": "FraCaS test suite", "start_pos": 41, "end_pos": 58, "type": "DATASET", "confidence": 0.9152064124743143}]}, {"text": "In addition, we evaluate the system's ability to predict common-sense facts from a large corpus of OpenIE extractions.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 5: Accuracy inferring common-sense facts  on a balanced test set. Lookup queries the lem- matized lower-case fact directly in the 270M fact  database. NaturalLI Only disallows such lookups,  and infers every query from only distinct premises  in the database. NaturalLI + Lookup takes the  union of the two systems.", "labels": [], "entities": [{"text": "270M fact  database", "start_pos": 137, "end_pos": 156, "type": "DATASET", "confidence": 0.8872960607210795}]}]}