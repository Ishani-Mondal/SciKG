{"title": [{"text": "A Regularized Competition Model for Question Difficulty Estimation in Community Question Answering Services", "labels": [], "entities": [{"text": "Question Difficulty Estimation", "start_pos": 36, "end_pos": 66, "type": "TASK", "confidence": 0.8596823215484619}, {"text": "Question Answering", "start_pos": 80, "end_pos": 98, "type": "TASK", "confidence": 0.6754571199417114}]}], "abstractContent": [{"text": "Estimating questions' difficulty levels is an important task in community question answering (CQA) services.", "labels": [], "entities": [{"text": "Estimating questions' difficulty", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.807043711344401}, {"text": "community question answering (CQA)", "start_pos": 64, "end_pos": 98, "type": "TASK", "confidence": 0.7784412105878195}]}, {"text": "Previous studies propose to solve this problem based on the question-user comparisons extracted from the question answering threads.", "labels": [], "entities": []}, {"text": "However, they suffer from data sparseness problem as each question only gets a limited number of comparisons.", "labels": [], "entities": []}, {"text": "Moreover, they cannot handle newly posted questions which get no comparisons.", "labels": [], "entities": []}, {"text": "In this paper , we propose a novel question difficulty estimation approach called Regularized Competition Model (RCM), which naturally combines question-user comparisons and questions' textual descriptions into a unified framework.", "labels": [], "entities": [{"text": "question difficulty estimation", "start_pos": 35, "end_pos": 65, "type": "TASK", "confidence": 0.6999125878016154}]}, {"text": "By incorporating tex-tual information, RCM can effectively deal with data sparseness problem.", "labels": [], "entities": []}, {"text": "We further employ a K-Nearest Neighbor approach to estimate difficulty levels of newly posted questions, again by leveraging textu-al similarities.", "labels": [], "entities": []}, {"text": "Experiments on two publicly available data sets show that for both well-resolved and newly-posted questions , RCM performs the estimation task significantly better than existing methods, demonstrating the advantage of incorporating textual information.", "labels": [], "entities": []}, {"text": "More interestingly , we observe that RCM might provide an automatic way to quantitatively measure the knowledge levels of words.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recent years have seen rapid growth in community question answering (CQA) services.", "labels": [], "entities": [{"text": "community question answering (CQA)", "start_pos": 39, "end_pos": 73, "type": "TASK", "confidence": 0.78465702633063}]}, {"text": "They have been widely used in various scenarios, including general information seeking on the web 1 , knowl- An important research problem in CQA is how to automatically estimate the difficulty levels of questions, i.e., question difficulty estimation (QDE).", "labels": [], "entities": [{"text": "general information seeking", "start_pos": 59, "end_pos": 86, "type": "TASK", "confidence": 0.6498181025187174}, {"text": "question difficulty estimation (QDE)", "start_pos": 221, "end_pos": 257, "type": "METRIC", "confidence": 0.6353375564018885}]}, {"text": "QDE can benefit many applications.", "labels": [], "entities": []}, {"text": "Examples include 1) Question routing.", "labels": [], "entities": [{"text": "Question routing", "start_pos": 20, "end_pos": 36, "type": "TASK", "confidence": 0.8849576413631439}]}, {"text": "Routing questions to appropriate answerers can help obtain quick and high-quality answers.", "labels": [], "entities": []}, {"text": "have demonstrated that routing questions by matching question difficulty level with answerer expertise level will make better use of answerers' time and expertise.", "labels": [], "entities": [{"text": "routing questions", "start_pos": 23, "end_pos": 40, "type": "TASK", "confidence": 0.9118476212024689}]}, {"text": "This is even more important for enterprise question answering and MOOCs question answering, where human resources are expensive.", "labels": [], "entities": [{"text": "question answering", "start_pos": 43, "end_pos": 61, "type": "TASK", "confidence": 0.7637992799282074}, {"text": "MOOCs question answering", "start_pos": 66, "end_pos": 90, "type": "TASK", "confidence": 0.8781612118085226}]}, {"text": "2) Incentive mechanism design.", "labels": [], "entities": [{"text": "Incentive mechanism design", "start_pos": 3, "end_pos": 29, "type": "TASK", "confidence": 0.9020894964536031}]}, {"text": "have found that winning point awards offered by reputation systems is a driving factor for user participation in C-QA services.", "labels": [], "entities": []}, {"text": "Assigning higher point awards to more difficult questions will significantly improve user participation and satisfaction.", "labels": [], "entities": []}, {"text": "Researchers in computational linguistics are always interested in investigating the correlation between language and knowledge, to see how the language reflects one's knowledge.", "labels": [], "entities": []}, {"text": "As we will show in Section 5.4, QDE provides an automatic way to quantitatively measure the knowledge levels of words.", "labels": [], "entities": []}, {"text": "have done the pioneer work on QDE, by leveraging question-user comparisons extracted from the question answering threads.", "labels": [], "entities": [{"text": "question answering threads", "start_pos": 94, "end_pos": 120, "type": "TASK", "confidence": 0.7554886440436045}]}, {"text": "Specifically, they assumed that the difficulty level of a question is higher than the expertise level of the asker (i.e. the user who asked the question), but lower than that of the best answerer (i.e. the user who provided the best answer).", "labels": [], "entities": []}, {"text": "A TrueSkill al-gorithm) was further adopted to estimate question difficulty levels as well as user expertise levels from the pairwise comparisons among them.", "labels": [], "entities": []}, {"text": "To our knowledge, it is the only existing work on QDE.", "labels": [], "entities": [{"text": "QDE", "start_pos": 50, "end_pos": 53, "type": "DATASET", "confidence": 0.5954373478889465}]}, {"text": "have proposed a similar idea, but their work focuses on a different task, i.e., estimating difficulty levels of tasks in crowdsourcing contest services.", "labels": [], "entities": []}, {"text": "There are two major drawbacks of previous methods: 1) data sparseness problem and 2) coldstart problem.", "labels": [], "entities": []}, {"text": "By the former, we mean that under the framework of previous work, each question is compared only twice with the users (once with the asker and the other with the best answerer), which might not provide enough information and contaminate the estimation accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 252, "end_pos": 260, "type": "METRIC", "confidence": 0.774856686592102}]}, {"text": "By the latter, we mean that previous work only deals with wellresolved questions which have received the best answers, but cannot handle newly posted questions with no answers received.", "labels": [], "entities": []}, {"text": "In many real-world applications such as question routing and incentive mechanism design, however, it is usually required that the difficulty level of a question is known instantly after it is posted.", "labels": [], "entities": [{"text": "question routing", "start_pos": 40, "end_pos": 56, "type": "TASK", "confidence": 0.8743573725223541}, {"text": "incentive mechanism design", "start_pos": 61, "end_pos": 87, "type": "TASK", "confidence": 0.6749178965886434}]}, {"text": "To address the drawbacks, we propose further exploiting questions' textual descriptions (e.g., title, body, and tags) to perform QDE.", "labels": [], "entities": []}, {"text": "Preliminary observations have shown that a question's difficulty level can be indicated by its textual description ().", "labels": [], "entities": []}, {"text": "We take advantage of the observations, and assume that if two questions are close in their textual descriptions, they will also be close in their difficulty levels, i.e., the smoothness assumption.", "labels": [], "entities": []}, {"text": "We employ manifold regularization () to characterize the assumption.", "labels": [], "entities": []}, {"text": "Manifold regularization is a wellknown technique to preserve local invariance in manifold learning algorithms, i.e., nearby points are likely to have similar embeddings).", "labels": [], "entities": [{"text": "Manifold regularization", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.6664651483297348}]}, {"text": "Then, we propose a novel Regularized Competition Model (RCM), which formalizes QDE as minimizing a loss on questionuser comparisons with manifold regularization on questions' textual descriptions.", "labels": [], "entities": []}, {"text": "As the smoothness assumption offers extra information for inferring question difficulty levels, incorporating it will effectively deal with data sparsity.", "labels": [], "entities": []}, {"text": "Finally, we adopt a K-Nearest Neighbor approach to perform cold-start estimation, again by leveraging the smoothness assumption.", "labels": [], "entities": []}, {"text": "Experiments on two publicly available data sets collected from Stack Overflow show that 1) RCM performs significantly better than existing methods in the QDE task for both well-resolved and cold-start questions.", "labels": [], "entities": [{"text": "QDE task", "start_pos": 154, "end_pos": 162, "type": "TASK", "confidence": 0.7740018367767334}]}, {"text": "2) The performance of RCM is insensitive to the particular choice of the term weighting schema (determines how a question's textual description is represented) and the similarity measure (determines how the textual similarity between two questions is measured).", "labels": [], "entities": [{"text": "similarity measure", "start_pos": 168, "end_pos": 186, "type": "METRIC", "confidence": 0.9404560327529907}]}, {"text": "The results demonstrate the advantage of incorporating textual information for QDE.", "labels": [], "entities": []}, {"text": "Qualitative analysis further reveals that RCM might provide an automatic way to quantitatively measure the knowledge levels of words.", "labels": [], "entities": []}, {"text": "The main contributions of this paper include: 1) We take fully advantage of questions' textual descriptions to address data sparseness problem and cold-start problem which previous QDE methods suffer from.", "labels": [], "entities": []}, {"text": "To our knowledge, it is the first time that textual information is introduced in QDE.", "labels": [], "entities": [{"text": "QDE", "start_pos": 81, "end_pos": 84, "type": "DATASET", "confidence": 0.7119329571723938}]}, {"text": "2) We propose a novel QDE method that naturally combines question-user comparisons and questions' textual descriptions into a unified framework.", "labels": [], "entities": []}, {"text": "The proposed method performs QDE significantly better than existing methods.", "labels": [], "entities": [{"text": "QDE", "start_pos": 29, "end_pos": 32, "type": "TASK", "confidence": 0.6143976449966431}]}, {"text": "3) We demonstrate the practicability of estimating difficulty levels of cold-start questions purely based on their textual descriptions, making various applications feasible in practice.", "labels": [], "entities": []}, {"text": "As far as we know, it is the first work that considers cold-start estimation.", "labels": [], "entities": [{"text": "cold-start estimation", "start_pos": 55, "end_pos": 76, "type": "TASK", "confidence": 0.7870665788650513}]}, {"text": "4) We explore how a word's knowledge level can be automatically measured by RCM.", "labels": [], "entities": []}, {"text": "The rest of the paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes the problem formulation and the motivation of RCM.", "labels": [], "entities": [{"text": "problem formulation", "start_pos": 24, "end_pos": 43, "type": "TASK", "confidence": 0.7365813851356506}, {"text": "RCM", "start_pos": 66, "end_pos": 69, "type": "TASK", "confidence": 0.9278971552848816}]}, {"text": "Section 3 presents the details of RCM.", "labels": [], "entities": [{"text": "RCM", "start_pos": 34, "end_pos": 37, "type": "TASK", "confidence": 0.5112283229827881}]}, {"text": "Section 4 discusses cold-start estimation.", "labels": [], "entities": [{"text": "cold-start estimation", "start_pos": 20, "end_pos": 41, "type": "TASK", "confidence": 0.7554794549942017}]}, {"text": "Section 5 reports experiments and results.", "labels": [], "entities": []}, {"text": "Section 6 reviews related work.", "labels": [], "entities": []}, {"text": "Section 7 concludes the paper and discusses future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "We have conducted experiments to test the effectiveness of RCM in estimating difficulty scores of both well-resolved and cold-start questions.", "labels": [], "entities": []}, {"text": "Moreover, we have explored how a word's difficulty level can be quantitatively measured by RCM.", "labels": [], "entities": []}, {"text": "We obtained a publicly available data set of Stack Overflow between July 31, 2008 and August 1, 2012 , containing QA threads in various categories.", "labels": [], "entities": []}, {"text": "We considered the categories of \"C++ programming\" and \"mathematics\", and randomly sampled about 10,000 QA threads from each category, denoted by SO/CPP and SO/Math respectively.", "labels": [], "entities": []}, {"text": "For each question, we took the title and body fields as its textual description.", "labels": [], "entities": []}, {"text": "For both data sets, stop words in a standard list 8 and words whose total frequencies are less than 10 were removed.", "labels": [], "entities": []}, {"text": "gives the statistics of the data sets.", "labels": [], "entities": []}, {"text": "For evaluation, we randomly sampled 600 question pairs from each data set, and asked annotators to compare the difficulty levels of the questions in each pair.", "labels": [], "entities": []}, {"text": "We had two graduate students majoring in computer science annotate the SO/CPP questions, and two majoring in mathematics annotate the SO/Math questions.", "labels": [], "entities": [{"text": "SO/CPP questions", "start_pos": 71, "end_pos": 87, "type": "DATASET", "confidence": 0.6315612196922302}]}, {"text": "For each question, only the title, body, and tags were exposed to the annotators.", "labels": [], "entities": []}, {"text": "Given a question pair (q 1 , q 2 ), the annotators were asked to give one of the three labels: q 1 \u227b q 2 , q 2 \u227b q 1 , or q 1 = q 2 , which respectively means that question q 1 has a higher, lower, or equal difficulty level compared with question q 2 . We used Cohen's kappa coefficient to measure the inter-annotator agreement.", "labels": [], "entities": []}, {"text": "The result is \u03ba = 0.7533 on SO/CPP and \u03ba = 0.8017 on SO/Math, indicating that the inter-annotator agreement is quite substantial on both data sets.", "labels": [], "entities": [{"text": "SO/CPP", "start_pos": 28, "end_pos": 34, "type": "DATASET", "confidence": 0.7873426874478658}, {"text": "SO/Math", "start_pos": 53, "end_pos": 60, "type": "DATASET", "confidence": 0.7518194317817688}]}, {"text": "After removing the question pairs with inconsistent labels, we got 521 annotated SO/CPP question pairs and 539 annotated SO/Math question pairs.", "labels": [], "entities": []}, {"text": "We further randomly split the annotated question pairs into development/test/cold-start sets, with the ratio of 2:2:1.", "labels": [], "entities": []}, {"text": "The first two sets were used to evaluate the methods in estimating difficulty scores of resolved questions.", "labels": [], "entities": []}, {"text": "Specifically, the development set was used for parameter tuning and the test set was used for evaluation.", "labels": [], "entities": [{"text": "parameter tuning", "start_pos": 47, "end_pos": 63, "type": "TASK", "confidence": 0.7216385900974274}]}, {"text": "The last set was used to evaluate the methods in cold-start estimation, and the questions in this set were excluded from the learning process of RCM as well as any baseline method.", "labels": [], "entities": [{"text": "cold-start estimation", "start_pos": 49, "end_pos": 70, "type": "TASK", "confidence": 0.7035031318664551}]}, {"text": "We considered three baseline methods: PageRank (PR), TrueSkill (TS), and CM, which are based solely on the pairwise competitions.", "labels": [], "entities": [{"text": "TrueSkill (TS)", "start_pos": 53, "end_pos": 67, "type": "METRIC", "confidence": 0.8202482163906097}]}, {"text": "\u2022 PR first constructs a competitor graph, by creating an edge from competitor i to competitor j if j beats i in a competition.", "labels": [], "entities": []}, {"text": "A PageRank algorithm) is then utilized to estimate the relative importance of the nodes, i.e., question difficulty scores and user expertise scores.", "labels": [], "entities": []}, {"text": "The damping factor was set from 0.1 to 0.9 in steps of 0.1.", "labels": [], "entities": []}, {"text": "\u2022 TS has been applied to QDE by.", "labels": [], "entities": [{"text": "TS", "start_pos": 2, "end_pos": 4, "type": "METRIC", "confidence": 0.8701409697532654}]}, {"text": "We set the model parameters in the same way as they suggested.", "labels": [], "entities": []}, {"text": "\u2022 CM performs QDE by solving Eq.", "labels": [], "entities": []}, {"text": "We set \u03bb 1 in {0, 0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1}.", "labels": [], "entities": []}, {"text": "We compared RCM with the above baseline methods.", "labels": [], "entities": []}, {"text": "In RCM, both parameters \u03bb 1 and \u03bb 2 were set in {0, 0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1}.", "labels": [], "entities": []}, {"text": "We employed accuracy (ACC) as the evaluation metric: A question pair is regarded as correctly judged if the relative difficulty ranking given by an estimation method is consistent with that given by the annotators.", "labels": [], "entities": [{"text": "accuracy (ACC)", "start_pos": 12, "end_pos": 26, "type": "METRIC", "confidence": 0.9448283463716507}]}, {"text": "The higher the accuracy is, the better a method performs.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9996115565299988}]}], "tableCaptions": [{"text": " Table 1: Statistics of the data sets.", "labels": [], "entities": []}, {"text": " Table 2: ACC of different methods for well- resolved questions.", "labels": [], "entities": [{"text": "ACC", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9702519178390503}]}, {"text": " Table 4: Averaged ACC of different methods for  cold-start questions.", "labels": [], "entities": [{"text": "Averaged", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9457035660743713}, {"text": "ACC", "start_pos": 19, "end_pos": 22, "type": "METRIC", "confidence": 0.7332611680030823}]}]}