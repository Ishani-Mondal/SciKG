{"title": [{"text": "Improving Multi-documents Summarization by Sentence Compression based on Expanded Constituent Parse Trees", "labels": [], "entities": [{"text": "Improving Multi-documents Summarization", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.7315050760904948}]}], "abstractContent": [{"text": "In this paper, we focus on the problem of using sentence compression techniques to improve multi-document summariza-tion.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 48, "end_pos": 68, "type": "TASK", "confidence": 0.7174875736236572}]}, {"text": "We propose an innovative sentence compression method by considering every node in the constituent parse tree and deciding its status-remove or retain.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 25, "end_pos": 45, "type": "TASK", "confidence": 0.7454730868339539}]}, {"text": "Integer liner programming with discrimina-tive training is used to solve the problem.", "labels": [], "entities": []}, {"text": "Under this model, we incorporate various constraints to improve the linguistic quality of the compressed sentences.", "labels": [], "entities": []}, {"text": "Then we utilize a pipeline summarization framework where sentences are first compressed by our proposed compression model to obtain top-n candidates and then a sentence selection module is used to generate the final summary.", "labels": [], "entities": []}, {"text": "Compared with state-of-the-art algorithms, our model has similar ROUGE-2 scores but better linguistic quality on TAC data.", "labels": [], "entities": [{"text": "ROUGE-2", "start_pos": 65, "end_pos": 72, "type": "METRIC", "confidence": 0.9960599541664124}, {"text": "TAC data", "start_pos": 113, "end_pos": 121, "type": "DATASET", "confidence": 0.673583984375}]}], "introductionContent": [{"text": "Automatic summarization can be broadly divided into two categories: extractive and abstractive summarization.", "labels": [], "entities": [{"text": "Automatic summarization", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.5911595374345779}]}, {"text": "Extractive summarization focuses on selecting salient sentences from the document collection and concatenating them to form a summary; while abstractive summarization is generally considered more difficult, involving sophisticated techniques for meaning representation, content planning, surface realization, etc.", "labels": [], "entities": [{"text": "Extractive summarization", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8354025781154633}, {"text": "meaning representation", "start_pos": 246, "end_pos": 268, "type": "TASK", "confidence": 0.7124442905187607}, {"text": "surface realization", "start_pos": 288, "end_pos": 307, "type": "TASK", "confidence": 0.7930867969989777}]}, {"text": "There has been a surge of interest in recent years on generating compressed document summaries as a viable step towards abstractive summarization.", "labels": [], "entities": [{"text": "abstractive summarization", "start_pos": 120, "end_pos": 145, "type": "TASK", "confidence": 0.6317015886306763}]}, {"text": "These compressive summaries often contain more information than sentence-based extractive summaries since they can remove insignificant sentence constituents and make space for more salient information that is otherwise dropped due to the summary length constraint.", "labels": [], "entities": [{"text": "sentence-based extractive summaries", "start_pos": 64, "end_pos": 99, "type": "TASK", "confidence": 0.6515764792760214}]}, {"text": "Two general strategies have been used for compressive summarization.", "labels": [], "entities": [{"text": "compressive summarization", "start_pos": 42, "end_pos": 67, "type": "TASK", "confidence": 0.930328905582428}]}, {"text": "One is a pipeline approach, where sentencebased extractive summarization is followed or proceeded by sentence compression.", "labels": [], "entities": [{"text": "sentencebased extractive summarization", "start_pos": 34, "end_pos": 72, "type": "TASK", "confidence": 0.6044938365618387}, {"text": "sentence compression", "start_pos": 101, "end_pos": 121, "type": "TASK", "confidence": 0.7102195620536804}]}, {"text": "Another line of work uses joint compression and summarization.", "labels": [], "entities": [{"text": "joint compression", "start_pos": 26, "end_pos": 43, "type": "TASK", "confidence": 0.7880695760250092}, {"text": "summarization", "start_pos": 48, "end_pos": 61, "type": "TASK", "confidence": 0.9775047898292542}]}, {"text": "Such methods have been shown to achieve promising performance), but they are typically computationally expensive.", "labels": [], "entities": []}, {"text": "In this study, we propose an innovative sentence compression model based on expanded constituent parse trees.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 40, "end_pos": 60, "type": "TASK", "confidence": 0.75629523396492}]}, {"text": "Our model uses integer linear programming (ILP) to search the entire space of compression, and is discriminatively trained.", "labels": [], "entities": []}, {"text": "It is built based on the discriminative sentence compression model from) and), but our method uses an expanded constituent parse tree rather than only the leaf nodes in previous work.", "labels": [], "entities": []}, {"text": "Therefore we can extract rich features for every node in the constituent parser tree.", "labels": [], "entities": []}, {"text": "This is an advantage of treebased compression technique).", "labels": [], "entities": []}, {"text": "Similar to (), we use a pipeline summarization framework where multiple compression candidates are generated for each pre-selected important sentence, and then an ILP-based summarization model is used to select the final compressed sentences.", "labels": [], "entities": []}, {"text": "We evaluate our proposed method on the TAC 2008 and 2011 data sets using the standard ROUGE metric and human evaluation of the linguistic quality.", "labels": [], "entities": [{"text": "TAC 2008 and 2011 data sets", "start_pos": 39, "end_pos": 66, "type": "DATASET", "confidence": 0.9656083881855011}, {"text": "ROUGE", "start_pos": 86, "end_pos": 91, "type": "METRIC", "confidence": 0.8753135800361633}]}, {"text": "Our results show that using our proposed sentence compression model in the summarization system can yield significant performance gain in linguistic quality, without losing much performance on the ROUGE metric.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 41, "end_pos": 61, "type": "TASK", "confidence": 0.7281215488910675}, {"text": "summarization", "start_pos": 75, "end_pos": 88, "type": "TASK", "confidence": 0.974450409412384}, {"text": "ROUGE", "start_pos": 197, "end_pos": 202, "type": "METRIC", "confidence": 0.7177978157997131}]}], "datasetContent": [{"text": "Summarization Data For summarization experiments, we use the standard TAC data sets , which have been used in the NIST competitions.", "labels": [], "entities": [{"text": "summarization", "start_pos": 23, "end_pos": 36, "type": "TASK", "confidence": 0.9812652468681335}, {"text": "TAC data sets", "start_pos": 70, "end_pos": 83, "type": "DATASET", "confidence": 0.8067294061183929}, {"text": "NIST competitions", "start_pos": 114, "end_pos": 131, "type": "DATASET", "confidence": 0.9102811813354492}]}, {"text": "In particular, we used the TAC 2010 data set as training data for the SVR sentence pre-selection model, TAC 2009 data set as development set for parameter tuning, and the TAC 2008 and 2011 data as the test set for reporting the final summarization results.", "labels": [], "entities": [{"text": "TAC 2010 data set", "start_pos": 27, "end_pos": 44, "type": "DATASET", "confidence": 0.9524566233158112}, {"text": "TAC 2009 data set", "start_pos": 104, "end_pos": 121, "type": "DATASET", "confidence": 0.9559443444013596}, {"text": "TAC 2008 and 2011 data", "start_pos": 171, "end_pos": 193, "type": "DATASET", "confidence": 0.9240623831748962}]}, {"text": "The training data for the sentence compression module in the summarization system is summary guided compression corpus annotated by) using TAC2010 data.", "labels": [], "entities": [{"text": "sentence compression module", "start_pos": 26, "end_pos": 53, "type": "TASK", "confidence": 0.8050013581911722}, {"text": "summarization", "start_pos": 61, "end_pos": 74, "type": "TASK", "confidence": 0.9689155220985413}, {"text": "TAC2010 data", "start_pos": 139, "end_pos": 151, "type": "DATASET", "confidence": 0.929362028837204}]}, {"text": "In the compression module, for each word we also used its document level feature.", "labels": [], "entities": []}, {"text": "Compression Data We also evaluate our compression model using the data set from.", "labels": [], "entities": []}, {"text": "It includes 82 newswire articles with manually produced compression for each sentence.", "labels": [], "entities": []}, {"text": "We use the same partitions as, i.e., 1,188 sentences for training and 441 for testing.", "labels": [], "entities": []}, {"text": "Data Processing We use Stanford CoreNLP toolkit 3 to tokenize the sentences, extract name entity tags, and generate the dependency parse tree.", "labels": [], "entities": [{"text": "Stanford CoreNLP toolkit", "start_pos": 23, "end_pos": 47, "type": "DATASET", "confidence": 0.8932590484619141}]}, {"text": "Berkeley Parser () is adopted to obtain the constituent parse tree for every sentence and POS tag for every token.", "labels": [], "entities": []}, {"text": "We use Pocket CRF 4 to implement the CRF sentence compression model.", "labels": [], "entities": [{"text": "Pocket CRF 4", "start_pos": 7, "end_pos": 19, "type": "DATASET", "confidence": 0.9001640280087789}, {"text": "CRF sentence compression", "start_pos": 37, "end_pos": 61, "type": "TASK", "confidence": 0.717353105545044}]}, {"text": "SVMlight 5 is used for the summary sentence pre-selection model.", "labels": [], "entities": [{"text": "SVMlight 5", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.9340796172618866}]}, {"text": "Gurobi ILP solver 6 does all ILP decoding.", "labels": [], "entities": [{"text": "Gurobi ILP solver 6", "start_pos": 0, "end_pos": 19, "type": "DATASET", "confidence": 0.7459439784288406}]}], "tableCaptions": [{"text": " Table 2: Features used in our system besides those  used in (Clarke and Lapata, 2008).", "labels": [], "entities": []}, {"text": " Table 3: Summarization results on the TAC 2008  and 2011 data sets.", "labels": [], "entities": [{"text": "TAC 2008  and 2011 data sets", "start_pos": 39, "end_pos": 67, "type": "DATASET", "confidence": 0.9506762027740479}]}, {"text": " Table 4: Sentence compression results. The hu- man compression rate of the test set is 69%.", "labels": [], "entities": [{"text": "Sentence compression", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.8304844200611115}, {"text": "hu- man compression rate", "start_pos": 44, "end_pos": 68, "type": "METRIC", "confidence": 0.7176389575004578}]}, {"text": " Table 5: Sentence compression results: effect of  lexical features and expanded parse tree. ILP(I)  represents the system using only bottom nodes in  constituent parse tree. ILP(II) is our system. Imp  means the content importance value.", "labels": [], "entities": [{"text": "Sentence compression", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.9001788794994354}, {"text": "Imp", "start_pos": 198, "end_pos": 201, "type": "METRIC", "confidence": 0.9286344051361084}]}]}