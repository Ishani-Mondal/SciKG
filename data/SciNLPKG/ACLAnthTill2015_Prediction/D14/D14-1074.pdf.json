{"title": [{"text": "Chinese Poetry Generation with Recurrent Neural Networks", "labels": [], "entities": [{"text": "Chinese Poetry Generation", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.6348167359828949}]}], "abstractContent": [{"text": "We propose a model for Chinese poem generation based on recurrent neural networks which we argue is ideally suited to capturing poetic content and form.", "labels": [], "entities": [{"text": "Chinese poem generation", "start_pos": 23, "end_pos": 46, "type": "TASK", "confidence": 0.6677249471346537}]}, {"text": "Our generator jointly performs content selection (\"what to say\") and surface realization (\"how to say\") by learning representations of individual characters, and their combinations into one or more lines as well as how these mutually reinforce and constrain each other.", "labels": [], "entities": [{"text": "content selection", "start_pos": 31, "end_pos": 48, "type": "TASK", "confidence": 0.6997081339359283}, {"text": "surface realization", "start_pos": 69, "end_pos": 88, "type": "TASK", "confidence": 0.7679265737533569}]}, {"text": "Poem lines are generated incrementally by taking into account the entire history of what has been generated so far rather than the limited horizon imposed by the previous line or lexical n-grams.", "labels": [], "entities": []}, {"text": "Experimental results show that our model outperforms competitive Chi-nese poetry generation systems using both automatic and manual evaluation methods.", "labels": [], "entities": [{"text": "Chi-nese poetry generation", "start_pos": 65, "end_pos": 91, "type": "TASK", "confidence": 0.6441293557484945}]}], "introductionContent": [{"text": "Classical poems area significant part of China's cultural heritage.", "labels": [], "entities": []}, {"text": "Their popularity manifests itself in many aspects of everyday life, e.g., as a means of expressing personal emotion, political views, or communicating messages at festive occasions as well as funerals.", "labels": [], "entities": []}, {"text": "Amongst the many different types of classical Chinese poetry, quatrain and regulated verse are perhaps the best-known ones.", "labels": [], "entities": []}, {"text": "Both types of poem must meet a set of structural, phonological, and semantic requirements, rendering their composition a formidable task left to the very best scholars.", "labels": [], "entities": []}, {"text": "An example of a quatrain is shown in.", "labels": [], "entities": []}, {"text": "Quatrains have four lines, each five or seven characters long.", "labels": [], "entities": []}, {"text": "Characters in turn follow specific phonological patterns, within each line and across lines.", "labels": [], "entities": []}, {"text": "For instance, the final characters in the second, fourth and (optionally) first line must rhyme, Missing You (* Z PP Z) Red berries born in the warm southland.", "labels": [], "entities": []}, {"text": "(P P Z Z P) How many branches flush in the spring?", "labels": [], "entities": []}, {"text": "(* PP Z Z) Take home an armful, for my sake, (* Z Z P P) As a symbol of our love.: An example of a 5-char quatrain exhibiting one of the most popular tonal patterns.", "labels": [], "entities": []}, {"text": "The tone of each character is shown at the end of each line (within parentheses); P and Z are shorthands for Ping and Ze tones, respectively; * indicates that the tone is not fixed and can be either.", "labels": [], "entities": []}, {"text": "Rhyming characters are shown in boldface.", "labels": [], "entities": []}, {"text": "whereas there are no rhyming constraints for the third line.", "labels": [], "entities": []}, {"text": "Moreover, poems must follow a prescribed tonal pattern.", "labels": [], "entities": []}, {"text": "In traditional Chinese, every character has one tone, Ping (level tone) or Ze (downward tone).", "labels": [], "entities": []}, {"text": "The poem in exemplifies one of the most popular tonal patterns.", "labels": [], "entities": []}, {"text": "Besides adhering to the above formal criteria, poems must exhibit concise and accurate use of language, engage the reader/hearer, stimulate their imagination, and bring out their feelings.", "labels": [], "entities": []}, {"text": "In this paper we are concerned with generating traditional Chinese poems automatically.", "labels": [], "entities": []}, {"text": "Although computers are no substitute for poetic creativity, they can analyze very large online text repositories of poems, extract statistical patterns, maintain them in memory and use them to generate many possible variants.", "labels": [], "entities": []}, {"text": "Furthermore, while amateur poets may struggle to remember and apply formal tonal and structural constraints, it is relatively straightforward for the machine to check whether a candidate poem conforms to these requirements.", "labels": [], "entities": []}, {"text": "Poetry generation has received a fair amount of attention over the past years (see the discussion in Section 2), with dozens of computational systems written to produce poems of varying sophistication.", "labels": [], "entities": [{"text": "Poetry generation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8082723021507263}]}, {"text": "Beyond the long-term goal of building an autonomous intelligent system capable of creating meaningful poems, there are potential short-term applications for computer generated poetry in the ever growing industry of electronic entertainment and interactive fiction as well as in education.", "labels": [], "entities": []}, {"text": "An assistive environment for poem composition could allow teachers and students to create poems subject to their requirements, and enhance their writing experience.", "labels": [], "entities": [{"text": "poem composition", "start_pos": 29, "end_pos": 45, "type": "TASK", "confidence": 0.8616693615913391}]}, {"text": "We propose a model for Chinese poem generation based on recurrent neural networks.", "labels": [], "entities": [{"text": "Chinese poem generation", "start_pos": 23, "end_pos": 46, "type": "TASK", "confidence": 0.6657774349053701}]}, {"text": "Our generator jointly performs content selection (\"what to say\") and surface realization (\"how to say\").", "labels": [], "entities": [{"text": "surface realization", "start_pos": 69, "end_pos": 88, "type": "TASK", "confidence": 0.7455384433269501}]}, {"text": "Given a large collection of poems, we learn representations of individual characters, and their combinations into one or more lines as well as how these mutually reinforce and constrain each other.", "labels": [], "entities": []}, {"text": "Our model generates lines in a poem probabilistically: it estimates the probability of the current line given the probability of all previously generated lines.", "labels": [], "entities": []}, {"text": "We use a recurrent neural network to learn the representations of the lines generated so far which in turn serve as input to a recurrent language model) which generates the current line.", "labels": [], "entities": []}, {"text": "In contrast to previous approaches (, our generator makes no Markov assumptions about the dependencies of the words within a line and across lines.", "labels": [], "entities": []}, {"text": "We evaluate our approach on the task of quatrain generation (see fora human-written example).", "labels": [], "entities": [{"text": "quatrain generation", "start_pos": 40, "end_pos": 59, "type": "TASK", "confidence": 0.8284714221954346}]}, {"text": "Experimental results show that our model outperforms competitive Chinese poetry generation systems using both automatic and manual evaluation methods.", "labels": [], "entities": [{"text": "Chinese poetry generation", "start_pos": 65, "end_pos": 90, "type": "TASK", "confidence": 0.6179780960083008}]}], "datasetContent": [{"text": "Data We created a corpus of classical Chinese poems by collating several online resources: Tang Poems, Song Poems, Song Ci, Ming Poems, Qing Poems, and Tai Poems.", "labels": [], "entities": []}, {"text": "The corpus consists of 284,899 poems in total.", "labels": [], "entities": []}, {"text": "78,859 of these are quatrains and were used for training and evaluating our model.", "labels": [], "entities": []}, {"text": "1: Dataset partitions of our poem corpus. and QTEST were used for training the characterbased language models (see row POEMLM in Table 2).", "labels": [], "entities": []}, {"text": "We also trained word2vec embeddings on POEMLM.", "labels": [], "entities": [{"text": "POEMLM", "start_pos": 39, "end_pos": 45, "type": "DATASET", "confidence": 0.9462973475456238}]}, {"text": "In our experiments, we generated quatrains following the eight most popular tonal patterns according to.", "labels": [], "entities": []}, {"text": "Perplexity Evaluation Evaluation of machinegenerated poetry is a notoriously difficult task.", "labels": [], "entities": [{"text": "Perplexity Evaluation Evaluation of machinegenerated poetry", "start_pos": 0, "end_pos": 59, "type": "TASK", "confidence": 0.7821795245011648}]}, {"text": "Our evaluation studies were designed to assess criteria of grammaticality, meaningfulness, and poeticness.", "labels": [], "entities": []}, {"text": "As a sanity check, we first measured the perplexity of our model with respect to the goldstandard.", "labels": [], "entities": []}, {"text": "Intuitively, a better model should assign larger probability (and therefore lower perplexity) to goldstandard poems.", "labels": [], "entities": []}, {"text": "BLEU-based Evaluation We also used BLEU to evaluate our model's ability to generate the second, third and fourth line given previous goldstandard lines.", "labels": [], "entities": [{"text": "BLEU-based", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9701225757598877}, {"text": "BLEU", "start_pos": 35, "end_pos": 39, "type": "METRIC", "confidence": 0.9980411529541016}]}, {"text": "A problematic aspect of this evaluation is the need for human-authored references (for a partially generated poem) which we do not have.", "labels": [], "entities": []}, {"text": "We obtain references automatically following the method proposed in.", "labels": [], "entities": []}, {"text": "The main idea is that if two lines share a similar topic, the lines following them can be each other's references.", "labels": [], "entities": []}, {"text": "Let A and B denote two adjacent lines in a poem, with B following A.", "labels": [], "entities": []}, {"text": "Similarly, let line B follow line A in another poem.", "labels": [], "entities": []}, {"text": "If lines A and A share some keywords in the same cluster in the Shixuehanying taxonomy, then B and B can be used as references for both A and A . We use this algorithm on the Tang Poems section of our corpus to build references for poems in the QVALID and QTEST data sets.", "labels": [], "entities": [{"text": "Shixuehanying taxonomy", "start_pos": 64, "end_pos": 86, "type": "DATASET", "confidence": 0.8853976726531982}, {"text": "Tang Poems section", "start_pos": 175, "end_pos": 193, "type": "DATASET", "confidence": 0.9318007429440817}, {"text": "QVALID", "start_pos": 245, "end_pos": 251, "type": "DATASET", "confidence": 0.9248140454292297}, {"text": "QTEST data sets", "start_pos": 256, "end_pos": 271, "type": "DATASET", "confidence": 0.9464047352472941}]}, {"text": "Poems in QVALID (with autogenerated references) were used for MERT training and Poems in QTEST (with auto-generated references) were used for BLEU evaluation.", "labels": [], "entities": [{"text": "QVALID", "start_pos": 9, "end_pos": 15, "type": "DATASET", "confidence": 0.8961741924285889}, {"text": "MERT training", "start_pos": 62, "end_pos": 75, "type": "TASK", "confidence": 0.8491699695587158}, {"text": "QTEST", "start_pos": 89, "end_pos": 94, "type": "DATASET", "confidence": 0.903780460357666}, {"text": "BLEU", "start_pos": 142, "end_pos": 146, "type": "METRIC", "confidence": 0.9717893600463867}]}, {"text": "Human Evaluation Finally, we also evaluated the generated poems by eliciting human judg-  ments.", "labels": [], "entities": []}, {"text": "Specifically, we invited 30 experts 3 on Chinese poetry to assess the output of our generator (and comparison systems).", "labels": [], "entities": []}, {"text": "These experts were asked to rate the poems using a 1-5 scale on four dimensions: fluency (is the poem grammatical and syntactically well-formed?), coherence (is the poem thematically and logically structured?), meaningfulness (does the poem convey a meaningful message to the reader?) and poeticness (does the text display the features of a poem?).", "labels": [], "entities": []}, {"text": "We also asked our participants to evaluate system outputs by ranking the generated poems relative to each other as away of determining overall poem quality.", "labels": [], "entities": []}, {"text": "Participants rated the output of our model and three comparison systems.", "labels": [], "entities": []}, {"text": "These included SMT-based model (SMT), summarization-based system (SUM), and a random baseline which creates poems by randomly selecting phrases from the Shixuehanying taxonomy given some keywords as input.", "labels": [], "entities": [{"text": "SMT-based model (SMT)", "start_pos": 15, "end_pos": 36, "type": "TASK", "confidence": 0.7058130085468293}]}, {"text": "We also included human written poems whose content matched the input keywords.", "labels": [], "entities": []}, {"text": "All systems were provided with the same keywords (i.e., the same cluster names in the ShiXueHanYing taxonomy).", "labels": [], "entities": [{"text": "ShiXueHanYing taxonomy", "start_pos": 86, "end_pos": 108, "type": "DATASET", "confidence": 0.8844724297523499}]}, {"text": "In order to compare all models on equal footing, we randomly sampled 30 sets of keywords (with three keywords in each set) and generated 30 quatrains for each system according to two lengths, namely 5-char and 7-char.", "labels": [], "entities": []}, {"text": "Overall, we obtained ratings for 300 (5\u00d730\u00d72) poems.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Dataset partitions of our poem corpus.", "labels": [], "entities": []}, {"text": " Table 3: Perplexities for different models.", "labels": [], "entities": []}, {"text": " Table 4: BLEU-2 scores on 5-char and 7-char quatrains. Given i goldstandard lines, BLEU-2 scores are  computed for the next (i + 1)th lines.", "labels": [], "entities": [{"text": "BLEU-2", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9986434578895569}, {"text": "BLEU-2", "start_pos": 84, "end_pos": 90, "type": "METRIC", "confidence": 0.9985175728797913}]}, {"text": " Table 5: Mean ratings elicited by humans on 5-char and 7-char quatrains. Diacritics ** (p < 0.01)  and * (p < 0.05) indicate our model (RNNPG) is significantly better than all other systems except Human.  Diacritics ++ (p < 0.01) and + (p < 0.05) indicate Human is significantly better than all other systems.", "labels": [], "entities": [{"text": "Mean", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.97295081615448}]}]}