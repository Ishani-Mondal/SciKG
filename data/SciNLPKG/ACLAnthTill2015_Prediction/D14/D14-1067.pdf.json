{"title": [], "abstractContent": [{"text": "This paper presents a system which learns to answer questions on abroad range of topics from a knowledge base using few hand-crafted features.", "labels": [], "entities": []}, {"text": "Our model learns low-dimensional embeddings of words and knowledge base constituents; these representations are used to score natural language questions against candidate answers.", "labels": [], "entities": []}, {"text": "Training our system using pairs of questions and structured representations of their answers, and pairs of question paraphrases , yields competitive results on a recent benchmark of the literature.", "labels": [], "entities": []}], "introductionContent": [{"text": "Teaching machines how to automatically answer questions asked in natural language on any topic or in any domain has always been along standing goal in Artificial Intelligence.", "labels": [], "entities": []}, {"text": "With the rise of large scale structured knowledge bases (KBs), this problem, known as open-domain question answering (or open QA), boils down to being able to query efficiently such databases with natural language.", "labels": [], "entities": [{"text": "question answering (or open QA)", "start_pos": 98, "end_pos": 129, "type": "TASK", "confidence": 0.6076289755957467}]}, {"text": "These KBs, such as FREEBASE () encompass huge ever growing amounts of information and ease open QA by organizing a great variety of answers in a structured format.", "labels": [], "entities": [{"text": "FREEBASE", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9378635287284851}]}, {"text": "However, the scale and the difficulty for machines to interpret natural language still makes this task a challenging problem.", "labels": [], "entities": []}, {"text": "The state-of-the-art techniques in open QA can be classified into two main classes, namely, information retrieval based and semantic parsing based.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 92, "end_pos": 113, "type": "TASK", "confidence": 0.7486965656280518}, {"text": "semantic parsing", "start_pos": 124, "end_pos": 140, "type": "TASK", "confidence": 0.6932127475738525}]}, {"text": "Information retrieval systems first retrieve abroad set of candidate answers by querying the search API of KBs with a transformation of the question into a valid query and then use fine-grained detection heuristics to identify the exact answer ().", "labels": [], "entities": [{"text": "Information retrieval", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.8040965795516968}]}, {"text": "On the other hand, semantic parsing methods focus on the correct interpretation of the meaning of a question by a semantic parsing system.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 19, "end_pos": 35, "type": "TASK", "confidence": 0.7553543448448181}]}, {"text": "A correct interpretation converts a question into the exact database query that returns the correct answer.", "labels": [], "entities": []}, {"text": "Interestingly, recent works) have shown that such systems can be efficiently trained under indirect and imperfect supervision and hence scale to large-scale regimes, while bypassing most of the annotation costs.", "labels": [], "entities": []}, {"text": "Yet, even if both kinds of system have shown the ability to handle large-scale KBs, they still require experts to hand-craft lexicons, grammars, and KB schema to be effective.", "labels": [], "entities": []}, {"text": "This non-negligible human intervention might not be generic enough to conveniently scale up to new databases with other schema, broader vocabularies or languages other than English.", "labels": [], "entities": []}, {"text": "In contrast,) proposed a framework for open QA requiring almost no human annotation.", "labels": [], "entities": []}, {"text": "Despite being an interesting approach, this method is outperformed by other competing methods.", "labels": [], "entities": []}, {"text": "() introduced an embedding model, which learns lowdimensional vector representations of words and symbols (such as KBs constituents) and can be trained with even less supervision than the system of (Fader et al., 2013) while being able to achieve better prediction performance.", "labels": [], "entities": []}, {"text": "However, this approach is only compared with) which operates in a simplified setting and has not been applied in more realistic conditions nor evaluated against the best performing methods.", "labels": [], "entities": []}, {"text": "In this paper, we improve the model of (Bordes et al., 2014b) by providing the ability to answer more complicated questions.", "labels": [], "entities": []}, {"text": "The main contributions of the paper are: (1) a more sophisticated inference procedure that is both efficient and can consider longer paths) considered only answers directly connected to the question in the graph); and (2) a richer representation of the answers which encodes the questionanswer path and surrounding subgraph of the KB.", "labels": [], "entities": []}, {"text": "Our approach is competitive with the current stateof-the-art on the recent benchmark WEBQUES-TIONS () without using any lexicon, rules or additional system for part-of-speech tagging, syntactic or dependency parsing during training as most other systems do.", "labels": [], "entities": [{"text": "WEBQUES-TIONS", "start_pos": 85, "end_pos": 98, "type": "DATASET", "confidence": 0.5493340492248535}, {"text": "part-of-speech tagging", "start_pos": 160, "end_pos": 182, "type": "TASK", "confidence": 0.678695872426033}, {"text": "dependency parsing", "start_pos": 197, "end_pos": 215, "type": "TASK", "confidence": 0.6372652053833008}]}], "datasetContent": [{"text": "We compare our system in terms of F1 score as computed by the official evaluation script 2 (F1 (Berant)) but also with a slightly different F1 definition, termed F1 (Yao) which was used in (Yao and Van Durme, 2014) (the difference being the way that questions with no answers are dealt with), Available from www-nlp.stanford.edu/software/sempre/ and precision @ 1 (p@1) of the first candidate entity (even when there area set of correct answers), comparing to recently published systems.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9875697791576385}, {"text": "F1", "start_pos": 92, "end_pos": 94, "type": "METRIC", "confidence": 0.994178056716919}, {"text": "F1", "start_pos": 140, "end_pos": 142, "type": "METRIC", "confidence": 0.995366096496582}, {"text": "F1 (Yao)", "start_pos": 162, "end_pos": 170, "type": "METRIC", "confidence": 0.9416358321905136}, {"text": "precision @ 1 (p@1)", "start_pos": 350, "end_pos": 369, "type": "METRIC", "confidence": 0.9087039828300476}]}, {"text": "The upper part of indicates that our approach outperforms), and, and performs similarly as.", "labels": [], "entities": []}, {"text": "The lower part of compares various versions of our model.", "labels": [], "entities": []}, {"text": "Our default approach uses the Subgraph representation for answers and C 2 as the candidate answers set.", "labels": [], "entities": []}, {"text": "Replacing C 2 by C 1 induces a large drop in performance because many questions do not have answers thatare directly connected to their inluded entity (not in C 1 ).", "labels": [], "entities": []}, {"text": "However, using all 2-hops connections as a candidate set is also detrimental, because the larger number of candidates confuses (and slows a lot) our ranking based inference.", "labels": [], "entities": []}, {"text": "Our results also verify our hypothesis of Section 3.1, that a richer representation for answers (using the local subgraph) can store more pertinent information.", "labels": [], "entities": []}, {"text": "Finally, we demonstrate that we greatly improve upon the model of (), which actually corresponds to a setting with the Path representation and C 1 as candidate set.", "labels": [], "entities": []}, {"text": "We also considered an ensemble of our approach and that of).", "labels": [], "entities": []}, {"text": "As we only had access to their test predictions we used the following combination method.", "labels": [], "entities": []}, {"text": "Our approach gives a score S(q, a) for the answer it predicts.", "labels": [], "entities": [{"text": "S", "start_pos": 27, "end_pos": 28, "type": "METRIC", "confidence": 0.9300728440284729}]}, {"text": "We chose a threshold such that our approach predicts 50% of the time (when S(q, a) is above its value), and the other 50% of the time we use the prediction of) instead.", "labels": [], "entities": []}, {"text": "We aimed fora 50/50 ratio because both methods perform similarly.", "labels": [], "entities": []}, {"text": "The ensemble improves the state-of-the-art, and indicates that our models are significantly different in their design.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results on the WEBQUESTIONS test set.", "labels": [], "entities": [{"text": "WEBQUESTIONS test set", "start_pos": 25, "end_pos": 46, "type": "DATASET", "confidence": 0.818591833114624}]}]}