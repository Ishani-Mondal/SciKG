{"title": [{"text": "Latent Domain Phrase-based Models for Adaptation", "labels": [], "entities": [{"text": "Adaptation", "start_pos": 38, "end_pos": 48, "type": "TASK", "confidence": 0.657393217086792}]}], "abstractContent": [{"text": "Phrase-based models directly trained on mix-of-domain corpora can be sub-optimal.", "labels": [], "entities": []}, {"text": "In this paper we equip phrase-based models with a latent domain variable and present a novel method for adapting them to an in-domain task represented by a seed corpus.", "labels": [], "entities": []}, {"text": "We derive an EM algorithm which alternates between inducing domain-focused phrase pair estimates, and weights for mix-domain sentence pairs reflecting their relevance for the in-domain task.", "labels": [], "entities": []}, {"text": "By embedding our latent domain phrase model in a sentence-level model and training the two in tandem, we are able to adapt all core translation components together-phrase, lexical and reordering.", "labels": [], "entities": []}, {"text": "We show experiments on weighing sentence pairs for relevance as well as adapting phrase-based models, showing significant performance improvement in both tasks.", "labels": [], "entities": []}, {"text": "1 Mix vs. Latent Domain Models Domain adaptation is usually perceived as utilizing a small seed in-domain corpus to adapt an existing system trained on an out-of-domain corpus.", "labels": [], "entities": [{"text": "Domain adaptation", "start_pos": 31, "end_pos": 48, "type": "TASK", "confidence": 0.6811247915029526}]}, {"text": "Here we are interested in adapting an SMT system trained on a large mix-domain corpus C mix to an in-domain task represented by a seed parallel corpus C in.", "labels": [], "entities": [{"text": "SMT", "start_pos": 38, "end_pos": 41, "type": "TASK", "confidence": 0.9927880167961121}]}, {"text": "The mix-domain scenario is interesting because often a large corpus consists of sentence pairs representing diverse domains, e.g., news, politics, finance, sports, etc.", "labels": [], "entities": []}, {"text": "At the core of a standard state-of-the-art phrase-based system (Och and Ney, 2004) is a phrase table {{\u02dce{{\u02dce, \u02dc f } extracted from the word-aligned training data together with estimates for Pt (\u02dc e | \u02dc f) and Pt (\u02dc f | \u02dc e).", "labels": [], "entities": []}, {"text": "Because the translations of words often vary across domains, it is likely that in a mix-domain corpus C mix the translation ambiguity will increase with the domain diversity.", "labels": [], "entities": []}, {"text": "Furthermore, the statistics in C mix will reflect translation preferences averaged over the diverse domains.", "labels": [], "entities": []}, {"text": "In this sense, phrase-based models trained on C mix can be considered domain-confused.", "labels": [], "entities": []}, {"text": "This often leads to suboptimal performance (Gasc\u00f3 et al., 2012; Irvine et al., 2013).", "labels": [], "entities": []}, {"text": "Recent adaptation techniques can be seen as mixture models, where two or more phrase tables , estimated from in-and mix-domain corpora, are combined together by interpolation, fill-up, or multiple-decoding paths (Koehn and Schroeder, 2007; Bisazza et al., 2011; Sennrich, 2012; Raz-mara et al., 2012; Sennrich et al., 2013).", "labels": [], "entities": []}, {"text": "Here we are interested in the specific question how to induce a phrase-based model from C mix for in-domain translation?", "labels": [], "entities": []}, {"text": "We view this as in-domain focused training on C mix , a complementary adaptation step which might precede any further combination with other models, e.g., in-, mix-or general-domain.", "labels": [], "entities": []}, {"text": "The main challenge is how to induce from C mix a phrase-based model for the in-domain task, given only C in as evidence?", "labels": [], "entities": []}, {"text": "We present an approach whereby the contrast between in-domain prior distributions and \"out-domain\" distributions is exploited for softly inviting (or recruiting) C mix phrase pairs to either camp.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "We evaluate the ability of our model to retrieve \"hidden\" in-domain data in a large mix-domain corpus, i.e., we hide some in-domain data in a large mix-domain corpus.", "labels": [], "entities": []}, {"text": "We weigh sentence pairs under our model with P (D 1 | \u02dc e, \u02dc f ) and P (D 1 | e, f) respectively.", "labels": [], "entities": []}, {"text": "We report pseudoprecision/recall at the sentence-level using a range of cut-off criteria for selecting the top scoring instances in the mix-domain corpus.", "labels": [], "entities": [{"text": "recall", "start_pos": 26, "end_pos": 32, "type": "METRIC", "confidence": 0.9900652766227722}]}, {"text": "A good relevance model expects to score higher for the hidden in-domain data.", "labels": [], "entities": []}, {"text": "Baselines Two standard perplexity-based selection models in the literature have been implemented as the baselines: cross-entropy difference and bilingual cross-entropy difference (Axelrod et al., 2011), investigating their ability to retrieve the hiding data as well.", "labels": [], "entities": []}, {"text": "Training them over the data to learn the sentences with their relevance, we then rank the sentences to select top of pairs to evaluate the pseudo-precision/recall at the sentence-level.", "labels": [], "entities": [{"text": "recall", "start_pos": 156, "end_pos": 162, "type": "METRIC", "confidence": 0.997832715511322}]}, {"text": "To train the baselines, we construct interpolated 4-gram Kneser-Ney LMs using BerkeleyLM ( Figure 2: Intrinsic evaluation.", "labels": [], "entities": [{"text": "BerkeleyLM", "start_pos": 78, "end_pos": 88, "type": "METRIC", "confidence": 0.9581122398376465}]}, {"text": "helps us examine how the pseudo sentence invitation are done during each EM iteration.", "labels": [], "entities": []}, {"text": "For later iterations we observe a better pseudo-precision and pseudo-recall at sentencelevel (,).", "labels": [], "entities": []}, {"text": "also reveals a good learning capacity of our learning framework.", "labels": [], "entities": []}, {"text": "Nevertheless, we observe that the baselines do notwork well for this task.", "labels": [], "entities": []}, {"text": "This is not new, as pointed out in our previous work.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The data preparation.", "labels": [], "entities": [{"text": "data preparation", "start_pos": 14, "end_pos": 30, "type": "TASK", "confidence": 0.6501152962446213}]}, {"text": " Table 2. Here, note how the improvement could  be gained when doubling the training data.", "labels": [], "entities": []}, {"text": " Table 2: BLEU averaged over multiple runs.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9976789355278015}]}, {"text": " Table 3: Average entropy of distributions.", "labels": [], "entities": []}, {"text": " Table 6: Phrase entry examples.", "labels": [], "entities": [{"text": "Phrase entry", "start_pos": 10, "end_pos": 22, "type": "TASK", "confidence": 0.9101084470748901}]}, {"text": " Table 7: Metric scores for the systems, which are  averages over multiple runs.", "labels": [], "entities": [{"text": "Metric scores", "start_pos": 10, "end_pos": 23, "type": "METRIC", "confidence": 0.937838613986969}]}, {"text": " Table 8: Metric scores for the systems, which are  averages over multiple runs.", "labels": [], "entities": [{"text": "Metric scores", "start_pos": 10, "end_pos": 23, "type": "METRIC", "confidence": 0.9340764880180359}]}, {"text": " Table 9: Domain adaptation experiments. Metric  scores for the systems, which are averages over  multiple runs.", "labels": [], "entities": [{"text": "Domain adaptation", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.7765241861343384}]}]}