{"title": [{"text": "Parsing low-resource languages using Gibbs sampling for PCFGs with latent annotations", "labels": [], "entities": [{"text": "Parsing low-resource languages", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.8687663078308105}]}], "abstractContent": [{"text": "PCFGs with latent annotations have been shown to be a very effective model for phrase structure parsing.", "labels": [], "entities": [{"text": "phrase structure parsing", "start_pos": 79, "end_pos": 103, "type": "TASK", "confidence": 0.8871584137280782}]}, {"text": "We present a Bayesian model and algorithms based on a Gibbs sam-pler for parsing with a grammar with latent annotations.", "labels": [], "entities": []}, {"text": "For PCFG-LA, we present an additional Gibbs sampler algorithm to learn annotations from training data, which are parse trees with coarse (unannotated) symbols.", "labels": [], "entities": [{"text": "PCFG-LA", "start_pos": 4, "end_pos": 11, "type": "DATASET", "confidence": 0.8823844194412231}]}, {"text": "We show that a Gibbs sampling technique is capable of parsing sentences in a wide variety of languages and producing results that are on-par with or surpass previous approaches.", "labels": [], "entities": []}, {"text": "Our results for Kinyarwanda and Malagasy in particular demonstrate that low-resource language parsing can benefit substantially from a Bayesian approach.", "labels": [], "entities": [{"text": "low-resource language parsing", "start_pos": 72, "end_pos": 101, "type": "TASK", "confidence": 0.6032668153444926}]}], "introductionContent": [{"text": "Despite great progress over the past two decades on parsing, relatively little work has considered the problem of creating accurate parsers for low-resource languages.", "labels": [], "entities": [{"text": "parsing", "start_pos": 52, "end_pos": 59, "type": "TASK", "confidence": 0.9769691824913025}]}, {"text": "Existing work in this area focuses primarily on approaches that use some form of cross-lingual bootstrapping to improve performance.", "labels": [], "entities": []}, {"text": "For instance, use a parallel Chinese/English corpus and an English dependency grammar to induce an annotated Chinese corpus in order to train a Chinese dependency grammar.", "labels": [], "entities": []}, {"text": "also considers the benefits of using multiple languages to induce a monolingual grammar, making use of a measure for data reliability in order to weight training data based on confidence of annotation.", "labels": [], "entities": []}, {"text": "Bootstrapping approaches such as these achieve markedly improved results, but they are dependent on the existence of a parallel bilingual corpus.", "labels": [], "entities": []}, {"text": "Very few such corpora are readily available, particularly for low-resource languages, and creating such corpora obviously presents a challenge for many practical applications.", "labels": [], "entities": []}, {"text": "shows some of the difficulty in handling low-resource languages by examining various tasks using Q'anjob'al as an example.", "labels": [], "entities": []}, {"text": "Another approach is that of, who take a more linguistically-motivated approach by making use of linguistic universals to seed newly developed grammars.", "labels": [], "entities": []}, {"text": "This substantially reduces the effort by making it unnecessary to learn the basic parameters of a language, but it lacks the robustness of grammars learned from data.", "labels": [], "entities": []}, {"text": "Recent work on Probabilistic Context-Free Grammars with latent annotations (PCFG-LA) () have shown them to be effective models for syntactic parsing, especially when less training material is available ().", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 131, "end_pos": 148, "type": "TASK", "confidence": 0.8399288058280945}]}, {"text": "The coarse nonterminal symbols found in vanilla PCFGs are refined by latent variables; these latent annotations can model subtypes of grammar symbols that result in better grammars and enable better estimates of grammar productions.", "labels": [], "entities": []}, {"text": "In this paper, we provide a Gibbs sampler for learning PCFG-LA models and show its effectiveness for parsing lowresource languages such as Malagasy and Kinyawanda.", "labels": [], "entities": []}, {"text": "Previous PCFG-LA work focuses on the problem of parameter estimation, including expectationmaximization (EM) (), spectral learning (, and variational inference (;.", "labels": [], "entities": [{"text": "parameter estimation", "start_pos": 48, "end_pos": 68, "type": "TASK", "confidence": 0.7226530313491821}, {"text": "expectationmaximization (EM)", "start_pos": 80, "end_pos": 108, "type": "METRIC", "confidence": 0.7867814749479294}, {"text": "spectral learning", "start_pos": 113, "end_pos": 130, "type": "TASK", "confidence": 0.8103001713752747}]}, {"text": "Regardless of inference method, previous work has used the same method to parse new sentences: a Viterbi parse under anew sentence-specific PCFG obtained from an approximation of the original grammar ().", "labels": [], "entities": []}, {"text": "Here, we provide an alternative approach to parsing new sentences: an extension of the Gibbs sampling algorithm of, which learns rule probabilities in an unsupervised PCFG.", "labels": [], "entities": [{"text": "parsing new sentences", "start_pos": 44, "end_pos": 65, "type": "TASK", "confidence": 0.9100005626678467}]}, {"text": "We use a Gibbs sampler to collect sampled trees theoretically distributed from the true posterior distribution in order to parse.", "labels": [], "entities": []}, {"text": "Priors in a Bayesian model can control the sparsity of grammars (which the insideoutside algorithm fails to do), while naturally incorporating smoothing into the model).", "labels": [], "entities": []}, {"text": "We also build a Bayesian model for parsing with a treebank, and incorporate information from training data as a prior.", "labels": [], "entities": [{"text": "parsing", "start_pos": 35, "end_pos": 42, "type": "TASK", "confidence": 0.9815475940704346}]}, {"text": "Moreover, we extend the Gibbs sampler to learn and parse PCFGs with latent annotations.", "labels": [], "entities": []}, {"text": "Learning the latent annotations is a compute-intensive process.", "labels": [], "entities": []}, {"text": "We show how a small amount of training data can be used to bootstrap: after running a large number of sampling iterations on a small set, the resulting parameters are used to seed a smaller number of iterations on the full training data.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our goal is to understand parsing efficacy using sampling and latent annotations for low-resource languages, so we perform experiments on five languages with varying amount of training data.", "labels": [], "entities": []}, {"text": "We compare our results to a number of previously established baselines.", "labels": [], "entities": []}, {"text": "First, for all languages, we use both a standard unsmoothed PCFG and the Bikel parser, trained on the training corpus.", "labels": [], "entities": [{"text": "PCFG", "start_pos": 60, "end_pos": 64, "type": "DATASET", "confidence": 0.8903248906135559}]}, {"text": "Additionally, we compare to state-of-the-art results for both English and Chinese, which have an existing body of work in PCFGs using a Bayesian framework.", "labels": [], "entities": []}, {"text": "For Chinese, we compare to, using their results that only use the Chinese Treebank (CTB).", "labels": [], "entities": [{"text": "Chinese Treebank (CTB)", "start_pos": 66, "end_pos": 88, "type": "DATASET", "confidence": 0.9480034947395325}]}, {"text": "For English, we compare to.", "labels": [], "entities": []}, {"text": "Prior results for parsing the constituency version of the Italian data are available from, but as they make use of a different version of the treebank including extra sentences, and additionally use the extensive functional tags present in the corpus, we do not directly compare our results to theirs.", "labels": [], "entities": [{"text": "Italian data", "start_pos": 58, "end_pos": 70, "type": "DATASET", "confidence": 0.7203849405050278}]}, {"text": "As a preprocessing step, all trees are converted into Chomsky Normal-Form such that all non-terminal productions are binary and all unary chains are removed.", "labels": [], "entities": []}, {"text": "Additional standard normalization is performed.", "labels": [], "entities": []}, {"text": "Functional tags (e.g. the SBJ part of NP-SBJ), empty nodes (traces), and indices are removed.", "labels": [], "entities": []}, {"text": "Our binarization is simple: given a parent, select the rightmost child as the head and add a stand-in node that contains the remainder of the original children; the process then recurses.", "labels": [], "entities": []}, {"text": "This simple technique uses no explicit headfinding rules, which eases cross-linguistic applicability.", "labels": [], "entities": []}, {"text": "From this normalized data, we train latent PCFGs with K=1,2,4,8,16,32 (where K=1 is equivalent to the plain PCFG described in section 2).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: F1 scores for small English training data ex- periments. 'K' is the number of latent annotations - K=1 represents a vanilla, unannotated PCFG.", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9991670846939087}]}, {"text": " Table 4: F1 scores for experiments on sampled PCFGs.  Note that", "labels": [], "entities": [{"text": "F1 scores", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9745083451271057}]}, {"text": " Table 6: Effect of differing regimes for handling un- known words.", "labels": [], "entities": []}]}