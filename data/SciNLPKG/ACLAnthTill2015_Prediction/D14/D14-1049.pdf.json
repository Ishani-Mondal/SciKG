{"title": [{"text": "A Shortest-path Method for Arc-factored Semantic Role Labeling", "labels": [], "entities": [{"text": "Semantic Role Labeling", "start_pos": 40, "end_pos": 62, "type": "TASK", "confidence": 0.6548764606316885}]}], "abstractContent": [{"text": "We introduce a Semantic Role Labeling (SRL) parser that finds semantic roles fora predicate together with the syntactic paths linking predicates and arguments.", "labels": [], "entities": [{"text": "Semantic Role Labeling (SRL) parser", "start_pos": 15, "end_pos": 50, "type": "TASK", "confidence": 0.8141112881047385}]}, {"text": "Our main contribution is to formulate SRL in terms of shortest-path inference, on the assumption that the SRL model is restricted to arc-factored features of the syntactic paths behind semantic roles.", "labels": [], "entities": [{"text": "SRL", "start_pos": 38, "end_pos": 41, "type": "TASK", "confidence": 0.6466529965400696}]}, {"text": "Overall, our method for SRL is a novel way to exploit larger variability in the syntactic re-alizations of predicate-argument relations, moving away from pipeline architectures.", "labels": [], "entities": [{"text": "SRL", "start_pos": 24, "end_pos": 27, "type": "TASK", "confidence": 0.9930343627929688}]}, {"text": "Experiments show that our approach improves the robustness of the predictions, producing arc-factored models that perform closely to methods using unrestricted features from the syntax.", "labels": [], "entities": []}], "introductionContent": [{"text": "Semantic role labeling (SRL) consists of finding the arguments of a predicate and labeling them with semantic roles (.", "labels": [], "entities": [{"text": "Semantic role labeling (SRL)", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8516636590162913}]}, {"text": "The arguments fill roles that answer questions of the type \"who\" did \"what\" to \"whom\", \"how\", and \"why\" fora given sentence predicate.", "labels": [], "entities": []}, {"text": "Most approaches to SRL are based on a pipeline strategy, first parsing the sentence to obtain a syntactic tree and then identifying and classifying arguments (.", "labels": [], "entities": [{"text": "SRL", "start_pos": 19, "end_pos": 22, "type": "TASK", "confidence": 0.9908859729766846}]}, {"text": "SRL methods critically depend on features of the syntactic structure, and consequently parsing mistakes can harm the quality of semantic role predictions ().", "labels": [], "entities": [{"text": "SRL", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.970026969909668}]}, {"text": "To alleviate this dependence, previous work has explored k-best parsers, combination systems ( or joint syntactic-semantic models.", "labels": [], "entities": []}, {"text": "In this paper we take a different approach.", "labels": [], "entities": []}, {"text": "In our scenario SRL is the end goal, and we assume that syntactic parsing is only an intermediate step to extract features to support SRL predictions.", "labels": [], "entities": [{"text": "SRL", "start_pos": 16, "end_pos": 19, "type": "TASK", "confidence": 0.9719050526618958}, {"text": "syntactic parsing", "start_pos": 56, "end_pos": 73, "type": "TASK", "confidence": 0.7242863178253174}, {"text": "SRL predictions", "start_pos": 134, "end_pos": 149, "type": "TASK", "confidence": 0.9408888518810272}]}, {"text": "In this setting we define a model that, given a predicate, identifies each of the semantic roles together with the syntactic path that links the predicate with the argument.", "labels": [], "entities": []}, {"text": "Thus, following previous work, we take the syntactic path as the main source of syntactic features, but instead of just conditioning on it, we predict it together with the semantic role.", "labels": [], "entities": []}, {"text": "The main contribution of this paper is a formulation of SRL parsing in terms of efficient shortest-path inference, under the assumption that the SRL model is restricted to arc-factored features of the syntactic path linking the argument with the predicate.", "labels": [], "entities": [{"text": "SRL parsing", "start_pos": 56, "end_pos": 67, "type": "TASK", "confidence": 0.986645370721817}]}, {"text": "Our assumption -that features of an SRL model should factor over dependency arcs-is supported by some empirical frequencies.", "labels": [], "entities": [{"text": "SRL", "start_pos": 36, "end_pos": 39, "type": "TASK", "confidence": 0.9712715148925781}]}, {"text": "shows the most frequent path patterns on) data for several languages, where a path pattern is a sequence of ascending arcs from the predicate to some ancestor, followed by descending arcs to the argument.", "labels": [], "entities": []}, {"text": "For English the distribution of path patterns is rather simple: the majority of paths consists of a number of ascending arcs followed by zero or one descending arc.", "labels": [], "entities": []}, {"text": "Thus a common strategy in SRL systems, formulated by, is to look for arguments in the ancestors of the predicate and their direct descendants.", "labels": [], "entities": [{"text": "SRL", "start_pos": 26, "end_pos": 29, "type": "TASK", "confidence": 0.9821633696556091}]}, {"text": "However, in Czech and Japanese data we observe a large portion of paths with two or more descending arcs, which makes it difficult to characterize the syntactic scope in which arguments are found.", "labels": [], "entities": []}, {"text": "Also, in the datasets for German, Czech and Chinese the three most frequent patterns cover over the 90% of all arguments.", "labels": [], "entities": []}, {"text": "In contrast, Japanese exhibits much more variability and along tail of infrequent types of patterns.", "labels": [], "entities": []}, {"text": "In general it is not feasible to capture path patterns manually, and it is not desirable that a statistical system depends on rather sparse nonfactored path features.", "labels": [], "entities": []}, {"text": "For this reason in this paper we explore arc-factored models for SRL.", "labels": [], "entities": [{"text": "SRL", "start_pos": 65, "end_pos": 68, "type": "TASK", "confidence": 0.9728407263755798}]}, {"text": "Our method might be specially useful in applications were we are interested in some target semantic role, i.e. retrieving agent relations for some verb, since it processes semantic roles independently of each other.", "labels": [], "entities": []}, {"text": "Our method might also be generalizable to other kinds of semantic relations which strongly depend on syntactic patterns such as relation extraction in information extraction or discourse parsing.", "labels": [], "entities": [{"text": "relation extraction in information extraction", "start_pos": 128, "end_pos": 173, "type": "TASK", "confidence": 0.6977554202079773}, {"text": "discourse parsing", "start_pos": 177, "end_pos": 194, "type": "TASK", "confidence": 0.7002873420715332}]}], "datasetContent": [{"text": "We present experiments using the, for the verbal predicates of English.", "labels": [], "entities": []}, {"text": "Evaluation is based on precision, recall and F 1 over correct predicateargument relations 1 . Our system uses the feature set of the state-of-the-art system by, but ignoring the features that do not factor over single arcs in the path.", "labels": [], "entities": [{"text": "precision", "start_pos": 23, "end_pos": 32, "type": "METRIC", "confidence": 0.9995342493057251}, {"text": "recall", "start_pos": 34, "end_pos": 40, "type": "METRIC", "confidence": 0.9994456171989441}, {"text": "F 1", "start_pos": 45, "end_pos": 48, "type": "METRIC", "confidence": 0.9916325509548187}]}, {"text": "The focus of these experiments is to seethe performance of the shortest-path method with respect to the syntactic variability.", "labels": [], "entities": []}, {"text": "Rather than running the method with the full set of possible dependency arcs in a sentence, i.e. O(n 2 ), we only consider a fraction of the most likely dependencies.", "labels": [], "entities": []}, {"text": "To do so employ a probabilistic dependency-based model, following, that computes the distribution over head-label pairs fora given modifier, Pr(h, l | x, m).", "labels": [], "entities": []}, {"text": "Specifically, for each modifier token we only consider the dependencies or heads whose probability is above a factor \u03b3 of the most likely dependency for the given modifier.", "labels": [], "entities": []}, {"text": "Thus, \u03b3 = 1 selects only the most likely dependency (similar to a pipeline system, but without enforcing tree constraints), and as \u03b3 decreases more dependencies are considered, to the point where \u03b3 = 0 would select all possible dependencies.", "labels": [], "entities": []}, {"text": "shows the ratio of dependencies included with respect to a pipeline system for the development set.", "labels": [], "entities": []}, {"text": "As an example, if we set \u03b3 = 0.5, fora given modifier we consider the most likely dependency and also the dependencies with probability larger than 1/2 of the probability of the most likely one.", "labels": [], "entities": []}, {"text": "In this case the total number of dependencies is 10.3% larger than only considering the most likely one.", "labels": [], "entities": []}, {"text": "shows results of the method on development data, when training and testing with different \u03b3 values.", "labels": [], "entities": []}, {"text": "The general trend is that testing with the most restricted syntactic graph results in the best performance.", "labels": [], "entities": []}, {"text": "However, we observe that as we allow for more syntactic variability during training, the results largely improve.", "labels": [], "entities": []}, {"text": "Setting \u03b3 = 1 for both training and testing gives a semantic F 1 of 75.9.", "labels": [], "entities": [{"text": "F 1", "start_pos": 61, "end_pos": 64, "type": "METRIC", "confidence": 0.9088189601898193}]}, {"text": "This configuration is similar to a pipeline approach but considering only factored features.", "labels": [], "entities": []}, {"text": "If we allow to train with \u03b3 = 0.1 and we test with \u03b3 = 1 the results improve by 1.96 points to a semantic F 1 of 77.8 points.", "labels": [], "entities": [{"text": "semantic F 1", "start_pos": 97, "end_pos": 109, "type": "METRIC", "confidence": 0.6920875310897827}]}, {"text": "When syntactic variability is too large, e.g., \u03b3 = 0.01, no improvements are observed.", "labels": [], "entities": []}, {"text": "Finally shows results on the verbal English WSJ test set using our best configuration   from the development set.", "labels": [], "entities": [{"text": "verbal English WSJ test set", "start_pos": 29, "end_pos": 56, "type": "DATASET", "confidence": 0.6281473100185394}]}, {"text": "We compare to the state-of-the art system by that was the top-performing system for the English language in SRL at the CoNLL-2009 Shared Task.", "labels": [], "entities": [{"text": "SRL", "start_pos": 108, "end_pos": 111, "type": "TASK", "confidence": 0.5990486741065979}, {"text": "CoNLL-2009 Shared Task", "start_pos": 119, "end_pos": 141, "type": "DATASET", "confidence": 0.7679688533147176}]}, {"text": "We also show the results fora shortest-path system trained and tested with \u03b3 = 1.", "labels": [], "entities": []}, {"text": "In addition we include an equivalent pipeline system using all features, both factored and non-factored, as defined in Johansson (2009).", "labels": [], "entities": []}, {"text": "We observe that by not being able to capture non-factored features the final performance drops by 1.6 F 1 points.", "labels": [], "entities": [{"text": "F 1", "start_pos": 102, "end_pos": 105, "type": "METRIC", "confidence": 0.9341497421264648}]}], "tableCaptions": [{"text": " Table 2: Ratio of additional dependencies in the graphs with  respect to a single-tree pipeline model (\u03b3 = 1) on develop- ment data.", "labels": [], "entities": [{"text": "Ratio", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9513365626335144}]}, {"text": " Table 3: Results of our shortest-path system for different  number of allowed dependencies showing precision, recall  and F1 on development set for the verbal predicates of the  English language.", "labels": [], "entities": [{"text": "precision", "start_pos": 100, "end_pos": 109, "type": "METRIC", "confidence": 0.9994832277297974}, {"text": "recall", "start_pos": 111, "end_pos": 117, "type": "METRIC", "confidence": 0.9993509650230408}, {"text": "F1", "start_pos": 123, "end_pos": 125, "type": "METRIC", "confidence": 0.9996691942214966}]}, {"text": " Table 4: Test set results for verbal predicates of the in-domain  English dataset. The configurations are labeled as follows.", "labels": [], "entities": []}]}