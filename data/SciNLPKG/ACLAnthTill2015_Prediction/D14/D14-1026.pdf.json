{"title": [{"text": "A Human Judgment Corpus and a Metric for Arabic MT Evaluation", "labels": [], "entities": [{"text": "Arabic MT", "start_pos": 41, "end_pos": 50, "type": "TASK", "confidence": 0.5920142829418182}]}], "abstractContent": [{"text": "We present a human judgments dataset and an adapted metric for evaluation of Arabic machine translation.", "labels": [], "entities": [{"text": "Arabic machine translation", "start_pos": 77, "end_pos": 103, "type": "TASK", "confidence": 0.5407049159208933}]}, {"text": "Our medium-scale dataset is the first of its kind for Ara-bic with high annotation quality.", "labels": [], "entities": []}, {"text": "We use the dataset to adapt the BLEU score for Arabic.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 32, "end_pos": 42, "type": "METRIC", "confidence": 0.9533090591430664}]}, {"text": "Our score (AL-BLEU) provides partial credits for stem and morphological matchings of hypothesis and reference words.", "labels": [], "entities": [{"text": "AL-BLEU)", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.913048654794693}]}, {"text": "We evaluate BLEU, METEOR and AL-BLEU on our human judgments corpus and show that AL-BLEU has the highest correlation with human judgments.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 12, "end_pos": 16, "type": "METRIC", "confidence": 0.9985004663467407}, {"text": "METEOR", "start_pos": 18, "end_pos": 24, "type": "METRIC", "confidence": 0.9571473598480225}]}, {"text": "We are releasing the dataset and software to the research community.", "labels": [], "entities": []}], "introductionContent": [{"text": "Evaluation of Machine Translation (MT) continues to be a challenging research problem.", "labels": [], "entities": [{"text": "Evaluation of Machine Translation (MT)", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.8804044382912772}]}, {"text": "There is an ongoing effort in finding simple and scalable metrics with rich linguistic analysis.", "labels": [], "entities": []}, {"text": "A wide range of metrics have been proposed and evaluated mostly for European target languages.", "labels": [], "entities": []}, {"text": "These metrics are usually evaluated based on their correlation with human judgments on a set of MT output.", "labels": [], "entities": [{"text": "MT", "start_pos": 96, "end_pos": 98, "type": "TASK", "confidence": 0.9300569891929626}]}, {"text": "While there has been growing interest in building systems for translating into Arabic, the evaluation of Arabic MT is still an under-studied problem.", "labels": [], "entities": [{"text": "MT", "start_pos": 112, "end_pos": 114, "type": "TASK", "confidence": 0.7839217782020569}]}, {"text": "Standard MT metrics such as BLEU) or TER () have been widely used for evaluating Arabic MT).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 28, "end_pos": 32, "type": "METRIC", "confidence": 0.9986328482627869}, {"text": "TER", "start_pos": 37, "end_pos": 40, "type": "METRIC", "confidence": 0.997866690158844}, {"text": "MT", "start_pos": 88, "end_pos": 90, "type": "TASK", "confidence": 0.8644753098487854}]}, {"text": "These metrics use strict word and phrase matching between the MT output and reference translations.", "labels": [], "entities": [{"text": "MT", "start_pos": 62, "end_pos": 64, "type": "TASK", "confidence": 0.943861186504364}]}, {"text": "For morphologically rich target languages such as Arabic, such criteria are too simplistic and inadequate.", "labels": [], "entities": []}, {"text": "In this paper, we present: (a) the first human judgment dataset for Arabic MT (b) the Arabic Language BLEU (AL-BLEU), an extension of the BLEU score for Arabic MT evaluation.", "labels": [], "entities": [{"text": "MT", "start_pos": 75, "end_pos": 77, "type": "TASK", "confidence": 0.8190335035324097}, {"text": "Arabic Language BLEU (AL-BLEU)", "start_pos": 86, "end_pos": 116, "type": "METRIC", "confidence": 0.6687733630339304}, {"text": "BLEU score", "start_pos": 138, "end_pos": 148, "type": "METRIC", "confidence": 0.9753199517726898}, {"text": "MT evaluation", "start_pos": 160, "end_pos": 173, "type": "TASK", "confidence": 0.8857732713222504}]}, {"text": "Our annotated dataset is composed of the output of six MT systems with texts from a diverse set of topics.", "labels": [], "entities": [{"text": "MT", "start_pos": 55, "end_pos": 57, "type": "TASK", "confidence": 0.9502407908439636}]}, {"text": "A group often native Arabic speakers annotated this corpus with high-levels of inter-and intra-annotator agreements.", "labels": [], "entities": []}, {"text": "Our AL-BLEU metric uses a rich set of morphological, syntactic and lexical features to extend the evaluation beyond the exact matching.", "labels": [], "entities": []}, {"text": "We conduct different experiments on the newly built dataset and demonstrate that AL-BLEU shows a stronger average correlation with human judgments than the BLEU and METEOR scores.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 156, "end_pos": 160, "type": "METRIC", "confidence": 0.9981710910797119}, {"text": "METEOR", "start_pos": 165, "end_pos": 171, "type": "METRIC", "confidence": 0.9482074975967407}]}, {"text": "Our dataset and our AL-BLEU metric provide useful testbeds for further research on Arabic MT and its evaluation.", "labels": [], "entities": [{"text": "Arabic MT", "start_pos": 83, "end_pos": 92, "type": "TASK", "confidence": 0.5701606869697571}]}], "datasetContent": [{"text": "We describe here our procedure for compiling a diverse Arabic MT dataset and annotating it with human judgments.", "labels": [], "entities": [{"text": "Arabic MT dataset", "start_pos": 55, "end_pos": 72, "type": "DATASET", "confidence": 0.7335374156634012}]}, {"text": "An automatic evaluation metric is said to be successful if it is shown to have high agreement with human-performed evaluations).", "labels": [], "entities": []}, {"text": "We use Kendall's tau \u03c4 (, a coefficient to measure the correlation between the system rankings and the human judgments at the sentence level.", "labels": [], "entities": []}, {"text": "Kendall's tau \u03c4 is calculated as follows: where a concordant pair indicates two translations of the same sentence for which the ranks obtained from the manual ranking task and from the corresponding metric scores agree (they disagree in a discordant pair  are concordant).", "labels": [], "entities": []}, {"text": "Thus, an automatic evaluation metric with a higher \u03c4 value is making predictions that are more similar to the human judgments than an automatic evaluation metric with a lower \u03c4 . We calculate the \u03c4 score for each sentence and average the scores to reach the corpus-level correlation.", "labels": [], "entities": []}, {"text": "We conducted a set of experiments to compare the correlation of AL-BLEU against the state-of-the art MT evaluation metrics.", "labels": [], "entities": [{"text": "AL-BLEU", "start_pos": 64, "end_pos": 71, "type": "METRIC", "confidence": 0.8831498622894287}, {"text": "MT evaluation", "start_pos": 101, "end_pos": 114, "type": "TASK", "confidence": 0.8656272292137146}]}, {"text": "For this we use a subset of 900 sentences extracted from the dataset described in Section 3.1.", "labels": [], "entities": []}, {"text": "As mentioned above, the stem and morphological features in AL-BLEU are parameterized each by weights which are used to calculate the partial credits.", "labels": [], "entities": []}, {"text": "We optimize the value of each weight towards correlation with human judgment by hill climbing with 100 random restarts using a development set of 600 sentences.", "labels": [], "entities": []}, {"text": "The 300 remaining sentences (100 from each corpus) are kept for testing.", "labels": [], "entities": []}, {"text": "The development and test sets are composed of equal portions of sentences from the three sub-corpora (NIST, MEDAR, WIKI).", "labels": [], "entities": [{"text": "NIST", "start_pos": 102, "end_pos": 106, "type": "DATASET", "confidence": 0.9509283900260925}, {"text": "MEDAR", "start_pos": 108, "end_pos": 113, "type": "METRIC", "confidence": 0.5572104454040527}]}, {"text": "As baselines, we measured the correlation of BLEU and METEOR with human judgments collected for each sentence.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 45, "end_pos": 49, "type": "METRIC", "confidence": 0.9808291792869568}, {"text": "METEOR", "start_pos": 54, "end_pos": 60, "type": "METRIC", "confidence": 0.9804860353469849}]}, {"text": "We did not observe a strong correlation with the Arabic-tuned ME-TEOR.", "labels": [], "entities": [{"text": "ME-TEOR", "start_pos": 62, "end_pos": 69, "type": "DATASET", "confidence": 0.42618075013160706}]}, {"text": "We conducted our experiments on the standard METEOR which was a stronger baseline than its Arabic version.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 45, "end_pos": 51, "type": "METRIC", "confidence": 0.7472789883613586}]}, {"text": "In order to avoid the zero ngram counts and artificially low BLEU scores, we use a smoothed version of BLEU.", "labels": [], "entities": [{"text": "ngram counts", "start_pos": 27, "end_pos": 39, "type": "METRIC", "confidence": 0.8667562305927277}, {"text": "BLEU scores", "start_pos": 61, "end_pos": 72, "type": "METRIC", "confidence": 0.9788023829460144}, {"text": "BLEU", "start_pos": 103, "end_pos": 107, "type": "METRIC", "confidence": 0.9865671992301941}]}, {"text": "We follow to add a small value to both the matched n-grams and the total number of n-grams (epsilon value of 10 \u22123 ).", "labels": [], "entities": []}, {"text": "In order to reach an optimal ordering of partial matches, we conducted a set of experiments in which we compared different orders between the morphological and lexical matchings to settle with the final order which was presented in. shows a comparison of the average correlation with human judgments for BLEU, ME-TEOR and AL-BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 304, "end_pos": 308, "type": "METRIC", "confidence": 0.9975737929344177}]}, {"text": "AL-BLEU shows a strong improvement against BLEU and a competitive improvement against METEOR both on the test and development sets.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 43, "end_pos": 47, "type": "METRIC", "confidence": 0.9947142004966736}, {"text": "METEOR", "start_pos": 86, "end_pos": 92, "type": "METRIC", "confidence": 0.91636061668396}]}, {"text": "The example in shows a sample case of such improvement.", "labels": [], "entities": []}, {"text": "In the example, the sentence ranked the highest by the annotator has only two exact matching with the reference translation (which results in a low BLEU score).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 148, "end_pos": 152, "type": "METRIC", "confidence": 0.999301552772522}]}, {"text": "The stem and morphological matching of AL-BLEU, gives a score and ranking much closer to human judgments.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Inter-and intra-annotator agreement  scores for our annotation compared to the aver- age scores for five English to five European lan- guages and also English-Czech", "labels": [], "entities": [{"text": "aver- age scores", "start_pos": 89, "end_pos": 105, "type": "METRIC", "confidence": 0.8657010048627853}]}, {"text": " Table 2: Example of ranked MT outputs in our gold-standard dataset. The first two rows specify the  English input and the Arabic reference, respectively. The third row of the table lists the different MT  system as ranked by annotators, using BLEU scores (column 4) and AL-BLEU (column 6). The differ- ent translation candidates are given here along with their associated Bucklwalter transliteration. 3 This  example, shows clearly that AL-BLEU correlates better with human decision.", "labels": [], "entities": [{"text": "MT outputs", "start_pos": 28, "end_pos": 38, "type": "TASK", "confidence": 0.9002441167831421}, {"text": "MT", "start_pos": 202, "end_pos": 204, "type": "TASK", "confidence": 0.9866031408309937}, {"text": "BLEU", "start_pos": 244, "end_pos": 248, "type": "METRIC", "confidence": 0.9985910058021545}, {"text": "AL-BLEU", "start_pos": 271, "end_pos": 278, "type": "METRIC", "confidence": 0.9638115167617798}]}, {"text": " Table 4: Comparison of the average Kendall's \u03c4  correlation.", "labels": [], "entities": [{"text": "Kendall's \u03c4  correlation", "start_pos": 36, "end_pos": 60, "type": "METRIC", "confidence": 0.6855186745524406}]}]}