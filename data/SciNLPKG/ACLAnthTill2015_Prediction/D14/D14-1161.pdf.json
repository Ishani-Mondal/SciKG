{"title": [{"text": "Word Semantic Representations using Bayesian Probabilistic Tensor Factorization", "labels": [], "entities": [{"text": "Word Semantic Representations", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.5540745755036672}]}], "abstractContent": [{"text": "Many forms of word relatedness have been developed, providing different perspectives on word similarity.", "labels": [], "entities": [{"text": "word relatedness", "start_pos": 14, "end_pos": 30, "type": "TASK", "confidence": 0.7048565149307251}, {"text": "word similarity", "start_pos": 88, "end_pos": 103, "type": "TASK", "confidence": 0.6897622793912888}]}, {"text": "We introduce a Bayesian probabilistic tensor factoriza-tion model for synthesizing a single word vector representation and per-perspective linear transformations from any number of word similarity matrices.", "labels": [], "entities": []}, {"text": "The resulting word vectors, when combined with the per-perspective linear transformation, approximately recreate while also regulariz-ing and generalizing, each word similarity perspective.", "labels": [], "entities": []}, {"text": "Our method can combine manually created semantic resources with neural word embeddings to separate synonyms and antonyms, and is capable of generalizing to words outside the vocabulary of any particular perspective.", "labels": [], "entities": []}, {"text": "We evaluated the word embeddings with GRE antonym questions, the result achieves the state-of-the-art performance.", "labels": [], "entities": []}], "introductionContent": [{"text": "In recent years, vector space models (VSMs) have been proved successful in solving various NLP tasks including named entity recognition, part-of-speech tagging, parsing, semantic rolelabeling and answering synonym or analogy questions ().", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 111, "end_pos": 135, "type": "TASK", "confidence": 0.6196324030558268}, {"text": "part-of-speech tagging", "start_pos": 137, "end_pos": 159, "type": "TASK", "confidence": 0.6947554349899292}, {"text": "answering synonym or analogy questions", "start_pos": 196, "end_pos": 234, "type": "TASK", "confidence": 0.8578135967254639}]}, {"text": "Also, VSMs are reported performing well on tasks involving the measurement of word relatedness ().", "labels": [], "entities": [{"text": "VSMs", "start_pos": 6, "end_pos": 10, "type": "TASK", "confidence": 0.7901965975761414}, {"text": "word relatedness", "start_pos": 78, "end_pos": 94, "type": "TASK", "confidence": 0.6364632695913315}]}, {"text": "Many existing works are distributional models, based on the Distributional Hypothesis, that words occurring in similar contexts tend to have similar meanings).", "labels": [], "entities": []}, {"text": "The limitation is that word vectors developed from distributional models cannot reveal word relatedness if its information does not lie in word distributions.", "labels": [], "entities": []}, {"text": "For instance, they are believed to have difficulty distinguishing antonyms from synonyms, because the distribution of antonymous words are close, since the context of antonymous words are always similar to each other.", "labels": [], "entities": []}, {"text": "Although some research claims that in certain conditions there do exist differences between the contexts of different antonymous words (), the differences are subtle enough that it can hardly be detected by such language models, especially for rare words.", "labels": [], "entities": []}, {"text": "Another important class of lexical resource for word relatedness is a lexicon, such as WordNet or Roget's Thesaurus.", "labels": [], "entities": [{"text": "word relatedness", "start_pos": 48, "end_pos": 64, "type": "TASK", "confidence": 0.707174688577652}, {"text": "WordNet", "start_pos": 87, "end_pos": 94, "type": "DATASET", "confidence": 0.956572949886322}]}, {"text": "Manually producing or extending lexicons is much more labor intensive than generating VSM word vectors using a corpus.", "labels": [], "entities": []}, {"text": "Thus, lexicons are sparse with missing words and multiword terms as well as missing relationships between words.", "labels": [], "entities": []}, {"text": "Considering the synonym / antonym perspective as an example, WordNet answers less than 40% percent of the the GRE antonym questions provided by directly.", "labels": [], "entities": [{"text": "GRE antonym questions", "start_pos": 110, "end_pos": 131, "type": "DATASET", "confidence": 0.7086922327677408}]}, {"text": "Moreover, binary entries in lexicons do not indicate the degree of relatedness, such as the degree of lexical contrast between happy and sad or happy and depressed.", "labels": [], "entities": []}, {"text": "The lack of such information makes it less fruitful when adopted in NLP applications.", "labels": [], "entities": []}, {"text": "In this work, we propose a Bayesian tensor factorization model (BPTF) for synthesizing a composite word vector representation by combining multiple different sources of word relatedness.", "labels": [], "entities": []}, {"text": "The input is a set of word byword matrices, which maybe sparse, providing a number indicating the presence or degree of relatedness.", "labels": [], "entities": []}, {"text": "We treat word relatedness matrices from different perspectives as slices, forming a word relatedness tensor.", "labels": [], "entities": []}, {"text": "Then the composite word vectors can be efficiently obtained by performing BPTF.", "labels": [], "entities": [{"text": "BPTF", "start_pos": 74, "end_pos": 78, "type": "DATASET", "confidence": 0.4344303607940674}]}, {"text": "Furthermore, given any two words and any trained relatedness perspective, we can create or recreate the pair-wise word relatedness with regularization via per-perspective linear transformation.", "labels": [], "entities": []}, {"text": "This method allows one set of word vectors to represent word relatednesses from many different perspectives (e.g. LSA for topic relatedness / corpus occurrences, ISA relation and YAGO type) It is able to bring the advantages from both word relatedness calculated by distributional models, and manually created lexicons, since the former have much more vocabulary coverage and many variations, while the latter covers word relatedness that is hard to detect by distributional models.", "labels": [], "entities": [{"text": "YAGO type", "start_pos": 179, "end_pos": 188, "type": "METRIC", "confidence": 0.9563590884208679}]}, {"text": "We can use information from distributional perspectives to create (if does not exist) or re-create (with regularization) word relatedness from the lexicon's perspective.", "labels": [], "entities": []}, {"text": "We evaluate our model on distinguishing synonyms and antonyms.", "labels": [], "entities": []}, {"text": "There area number of related works (.", "labels": [], "entities": []}, {"text": "A number of sophisticated methods have been applied, producing competitive results using diverse approaches.", "labels": [], "entities": []}, {"text": "We use the GRE antonym questions) as a benchmark, and answer these questions by finding the most contrasting choice according to the created or recreated synonym / antonym word relatedness.", "labels": [], "entities": [{"text": "GRE antonym questions", "start_pos": 11, "end_pos": 32, "type": "DATASET", "confidence": 0.7957542936007181}]}, {"text": "The result achieves state-of-the-art performance.", "labels": [], "entities": []}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes the related work of word vector representations, the BPTF model and antonymy detection.", "labels": [], "entities": [{"text": "word vector representations", "start_pos": 40, "end_pos": 67, "type": "TASK", "confidence": 0.6985738774140676}, {"text": "antonymy detection", "start_pos": 88, "end_pos": 106, "type": "TASK", "confidence": 0.7463897168636322}]}, {"text": "Section 3 presents our BPTF model and the sampling method.", "labels": [], "entities": [{"text": "BPTF model", "start_pos": 23, "end_pos": 33, "type": "DATASET", "confidence": 0.7416257858276367}]}, {"text": "Section 4 shows the experimental evaluation and results with Section 5 providing conclusion and future work.", "labels": [], "entities": [{"text": "conclusion", "start_pos": 81, "end_pos": 91, "type": "METRIC", "confidence": 0.953840970993042}]}], "datasetContent": [{"text": "In this section, we evaluate our model by answering antonym questions.", "labels": [], "entities": []}, {"text": "This task is especially suitable for evaluating our model since the performance of straight-forward look-up from the thesauruses we considered is poor.", "labels": [], "entities": []}, {"text": "There are two major limitations: 1.", "labels": [], "entities": []}, {"text": "The thesaurus usually only contains antonym information for word pairs with a strong contrast.", "labels": [], "entities": []}, {"text": "2. The vocabulary of the antonym entries in the thesaurus is limited, and does not contain many words in the antonym questions.", "labels": [], "entities": []}, {"text": "On the other hand, distributional similarities can be trained from large corpora and hence have a large coverage for words.", "labels": [], "entities": []}, {"text": "This implies that we can treat the thesaurus data as the first slice, and the distributional similarities as the second slice, then use our model to create / recreate word relatedness on the first slice to answer antonym questions.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Word Relatedness Tensor", "labels": [], "entities": [{"text": "Word Relatedness Tensor", "start_pos": 10, "end_pos": 33, "type": "TASK", "confidence": 0.825495700041453}]}, {"text": " Table 2: Development and test results on the GRE antonym questions. *Note: to allow comparison, in  look-up we follow the approach used by (Yih et al., 2012): randomly guess an answer if the target word  is in the vocabulary while none of the choices are. Asterisk indicates the look-up results without random  guessing.", "labels": [], "entities": [{"text": "GRE antonym questions", "start_pos": 46, "end_pos": 67, "type": "DATASET", "confidence": 0.7474650144577026}]}]}