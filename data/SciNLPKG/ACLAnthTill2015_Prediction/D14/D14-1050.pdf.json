{"title": [], "abstractContent": [{"text": "We present an empirical study on the use of semantic information for Concept Seg-mentation and Labeling (CSL), which is an important step for semantic parsing.", "labels": [], "entities": [{"text": "Concept Seg-mentation and Labeling (CSL)", "start_pos": 69, "end_pos": 109, "type": "TASK", "confidence": 0.7089695717607226}, {"text": "semantic parsing", "start_pos": 142, "end_pos": 158, "type": "TASK", "confidence": 0.7726735472679138}]}, {"text": "We represent the alternative analyses output by a state-of-the-art CSL parser with tree structures, which we rerank with a classifier trained on two types of semantic tree kernels: one processing structures built with words, concepts and Brown clusters, and another one using semantic similarity among the words composing the structure.", "labels": [], "entities": []}, {"text": "The results on a corpus from the restaurant domain show that our semantic kernels exploiting similarity measures out-perform state-of-the-art rerankers.", "labels": [], "entities": []}], "introductionContent": [{"text": "Spoken Language Understanding aims to interpret user utterances and to convert them to logical forms or, equivalently, to database queries, which can then be used to satisfy the user's information needs.", "labels": [], "entities": [{"text": "Spoken Language Understanding", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.8591566483179728}]}, {"text": "This process is known as Concept Segmentation and Labeling (CSL), also called semantic parsing in the speech community: it maps utterances into meaning representations based on semantic constituents.", "labels": [], "entities": [{"text": "Concept Segmentation and Labeling (CSL)", "start_pos": 25, "end_pos": 64, "type": "TASK", "confidence": 0.8401998111179897}, {"text": "semantic parsing", "start_pos": 78, "end_pos": 94, "type": "TASK", "confidence": 0.7668128311634064}]}, {"text": "The latter are basically word sequences, often referred to as concepts, attributes or semantic tags.", "labels": [], "entities": []}, {"text": "CSL makes it easy to convert spoken questions such as \"cheap lebanese restaurants in doha with take out\" into database queries.", "labels": [], "entities": []}, {"text": "First, a language-specific semantic parser tokenizes, segments and labels the question:  Finally, a database query is formed from the list of labels and values, and is then executed against the database, e.g., MongoDB; a backoff mechanism maybe used if the query has not succeeded.", "labels": [], "entities": [{"text": "MongoDB", "start_pos": 210, "end_pos": 217, "type": "DATASET", "confidence": 0.9064356684684753}]}, {"text": "{$and [{cuisine:\"lebanese\"},{city:\"doha\"}, {price:\"low\"},{amenity:\"carry out\"}]} The state-of-the-art of CSL is represented by conditional models for sequence labeling such as Conditional Random Fields (CRFs) () trained with simple morphological and lexical features.", "labels": [], "entities": []}, {"text": "The basic CRF model was improved by means of reranking () using structural kernels).", "labels": [], "entities": []}, {"text": "Although these methods exploited sentence structure, they did not use syntax at all.", "labels": [], "entities": []}, {"text": "More recently, we applied shallow syntactic structures and discourse parsing with slightly better results ().", "labels": [], "entities": [{"text": "discourse parsing", "start_pos": 59, "end_pos": 76, "type": "TASK", "confidence": 0.7153501957654953}]}, {"text": "However, the most obvious models for semantic parsing, i.e., rerankers based on semantic structural kernels, had not been applied to semantic structures yet.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 37, "end_pos": 53, "type": "TASK", "confidence": 0.724148765206337}]}, {"text": "In this paper, we study the impact of semantic information conveyed by Brown Clusters (BCs) and semantic similarity, while also combining them with innovative features.", "labels": [], "entities": []}, {"text": "We use reranking, similarly to (, to select the best hypothesis annotated with concepts predicted by a local model.", "labels": [], "entities": []}, {"text": "The competing hypotheses are represented as innovative trees enriched with the semantic concepts and BC labels.", "labels": [], "entities": []}, {"text": "The trees can capture dependencies between sentence constituents, concepts and BCs.", "labels": [], "entities": []}, {"text": "However, extracting explicit features from them is rather difficult as their number is exponentially large.", "labels": [], "entities": []}, {"text": "Thus, we rely on (i) Support Vector Machines) to train the reranking functions and on (ii) structural kernels to automatically encode tree fragments that represent syntactic and semantic dependencies from words and concepts.", "labels": [], "entities": []}, {"text": "We further apply a semantic kernel (SK), namely the Smoothed Partial Tree), which uses the lexical similarity between the tree nodes, while computing the substructure space.", "labels": [], "entities": []}, {"text": "This is the first time that SKs are applied to reranking hypotheses.", "labels": [], "entities": []}, {"text": "This (i) makes the global sentence structure along with concepts available to the learning algorithm, and (ii) enables computing the similarity between lexicals in syntactic patterns that are enriched by concepts.", "labels": [], "entities": []}, {"text": "We tested our models on the Restaurant domain.", "labels": [], "entities": [{"text": "Restaurant domain", "start_pos": 28, "end_pos": 45, "type": "DATASET", "confidence": 0.8046549558639526}]}, {"text": "Our results show that: (i) The basic CRF parser, which uses semi-Markov CRF, or semi-CRF (), is already very accurate; it achieves F 1 scores over 83%, making any further improvement very hard.", "labels": [], "entities": [{"text": "CRF parser", "start_pos": 37, "end_pos": 47, "type": "TASK", "confidence": 0.8693239688873291}, {"text": "F 1 scores", "start_pos": 131, "end_pos": 141, "type": "METRIC", "confidence": 0.9869759480158488}]}, {"text": "(ii) The upper-bound performance of the reranker is very high as well, i.e., the correct annotation is generated in the list of the first 100 hypotheses in 98.72% of the cases.", "labels": [], "entities": []}, {"text": "(iii) SKs significantly improve over the semi-CRF baseline and our previous state-of-the-art reranker exploiting shallow syntactic patterns (, as shown by extensive comparisons using several systems.", "labels": [], "entities": [{"text": "SKs", "start_pos": 6, "end_pos": 9, "type": "TASK", "confidence": 0.834570050239563}]}, {"text": "(iv) Making BCs effective requires a deeper study.", "labels": [], "entities": [{"text": "BCs", "start_pos": 12, "end_pos": 15, "type": "TASK", "confidence": 0.9816663861274719}]}], "datasetContent": [{"text": "The experiments aim at investigating the role of feature vectors, PTK, SK and BCs in reranking.", "labels": [], "entities": [{"text": "BCs", "start_pos": 78, "end_pos": 81, "type": "METRIC", "confidence": 0.8742514848709106}]}, {"text": "We first describe the experimental setting and then we move into the analysis of the results.: Number of instances and pairs used to train the semi-CRF and rerankers, respectively.", "labels": [], "entities": []}, {"text": "In our experiments, we used questions annotated with semantic tags, which were collected through crowdsourcing on Amazon Mechanical Turk and made available 4 by.", "labels": [], "entities": []}, {"text": "We split the dataset into training, development and test sets.", "labels": [], "entities": []}, {"text": "shows the number of examples and example pairs we used for the semi-CRF and the reranker, respectively.", "labels": [], "entities": []}, {"text": "We subsequently split the training data randomly into 10 folds.", "labels": [], "entities": []}, {"text": "We used cross-validation, i.e., iteratively training with 9 folds and annotating the remaining fold, in order to generate the N -best lists of hypotheses for the entire training dataset.", "labels": [], "entities": []}, {"text": "We computed the 100-best hypotheses for each example.", "labels": [], "entities": []}, {"text": "We then used the development dataset to test and tune the hyper-parameters of our reranking model.", "labels": [], "entities": []}, {"text": "The results on the development set, which we will present in Section 4.2 below, were obtained using semi-CRF and reranking models trained on the training set.", "labels": [], "entities": []}, {"text": "Each hypothesis is represented by a semantic tree, a feature vector (explained in Section 3), and two extra features: (i) the semi-CRF probability of the hypothesis, and (ii) its reciprocal rank in the N -best list.", "labels": [], "entities": []}, {"text": "We used the SVM-Light-TK 5 to train the reranker with a combination of tree kernels and feature vectors.", "labels": [], "entities": [{"text": "SVM-Light-TK 5", "start_pos": 12, "end_pos": 26, "type": "DATASET", "confidence": 0.9437080025672913}]}, {"text": "We used the default parameters and a linear kernel for the feature vectors.", "labels": [], "entities": []}, {"text": "As a baseline, we picked the best-scoring hypothesis in the list, i.e., the output by the regular semi-CRF parser.", "labels": [], "entities": []}, {"text": "The setting is exactly the same as that described in ().", "labels": [], "entities": []}, {"text": "In all experiments, we used the harmonic mean of precision and recall (F 1 ) (van), computed at the token level and micro-averaged across the different semantic types.", "labels": [], "entities": [{"text": "precision", "start_pos": 49, "end_pos": 58, "type": "METRIC", "confidence": 0.9468708038330078}, {"text": "recall", "start_pos": 63, "end_pos": 69, "type": "METRIC", "confidence": 0.9992793202400208}, {"text": "F 1 )", "start_pos": 71, "end_pos": 76, "type": "METRIC", "confidence": 0.9096274375915527}]}, {"text": "6: Oracle F 1 score for N -best lists.", "labels": [], "entities": [{"text": "Oracle", "start_pos": 3, "end_pos": 9, "type": "DATASET", "confidence": 0.6285239458084106}, {"text": "F 1 score", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.8582105239232382}]}, {"text": "Clustering groups of similar words together provides away of generalizing them.", "labels": [], "entities": []}, {"text": "In this work, we explore the use of Brown clusters) in both feature vectors and tree kernels.", "labels": [], "entities": []}, {"text": "The Brown clustering algorithm uses an n-gram class model.", "labels": [], "entities": []}, {"text": "It first assigns each word to a distinct cluster, and then it merges different clusters in a bottom-up fashion.", "labels": [], "entities": []}, {"text": "The merge step is done in away that minimizes the loss in average mutual information between clusters.", "labels": [], "entities": []}, {"text": "The outcome is hierarchical clustering, which we use in our reranking algorithm.", "labels": [], "entities": []}, {"text": "To create the Brown clusters, we used the Yelp dataset of reviews.", "labels": [], "entities": [{"text": "Yelp dataset of reviews", "start_pos": 42, "end_pos": 65, "type": "DATASET", "confidence": 0.9556008875370026}]}, {"text": "It contains 335,022 reviews about 15,585 businesses; 5,575 of the businesses and 233,839 of the reviews are restaurant-related.", "labels": [], "entities": []}, {"text": "This dataset is very similar to the dataset of queries about restaurants we use in our experiments.", "labels": [], "entities": []}, {"text": "We compute the lexical similarity for SK by applying LSA ( to Tripadvisor data.", "labels": [], "entities": [{"text": "SK", "start_pos": 38, "end_pos": 40, "type": "TASK", "confidence": 0.8991833925247192}, {"text": "LSA", "start_pos": 53, "end_pos": 56, "type": "METRIC", "confidence": 0.8634377717971802}]}, {"text": "The dataset and the exact procedure for creating the LSA matrix are described in).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Number of instances and pairs used to  train the semi-CRF and rerankers, respectively.", "labels": [], "entities": []}, {"text": " Table 2: Oracle F 1 score for N -best lists.", "labels": [], "entities": [{"text": "Oracle F 1 score", "start_pos": 10, "end_pos": 26, "type": "METRIC", "confidence": 0.7204034626483917}]}]}