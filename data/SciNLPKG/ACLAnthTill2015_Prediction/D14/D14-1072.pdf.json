{"title": [], "abstractContent": [{"text": "Wikipedia's link structure is a valuable resource for natural language processing tasks, but only a fraction of the concepts mentioned in each article are annotated with hyperlinks.", "labels": [], "entities": []}, {"text": "In this paper, we study how to augment Wikipedia with additional high-precision links.", "labels": [], "entities": []}, {"text": "We present 3W, a system that identifies concept mentions in Wikipedia text, and links each mention to its referent page.", "labels": [], "entities": []}, {"text": "3W leverages rich semantic information present in Wikipedia to achieve high precision.", "labels": [], "entities": [{"text": "3W", "start_pos": 0, "end_pos": 2, "type": "DATASET", "confidence": 0.8900657296180725}, {"text": "precision", "start_pos": 76, "end_pos": 85, "type": "METRIC", "confidence": 0.9966778755187988}]}, {"text": "Our experiments demonstrate that 3W can add an average of seven new links to each Wikipedia article, at a precision of 0.98.", "labels": [], "entities": [{"text": "precision", "start_pos": 106, "end_pos": 115, "type": "METRIC", "confidence": 0.9893246293067932}]}], "introductionContent": [{"text": "Wikipedia forms a valuable resource for many Natural Language Processing and Information Extraction tasks, such as Entity Linking, Ontology Construction ( and Knowledge Base Population ().", "labels": [], "entities": [{"text": "Wikipedia", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.8799954652786255}, {"text": "Natural Language Processing and Information Extraction", "start_pos": 45, "end_pos": 99, "type": "TASK", "confidence": 0.6441196948289871}, {"text": "Entity Linking", "start_pos": 115, "end_pos": 129, "type": "TASK", "confidence": 0.8180267214775085}, {"text": "Ontology Construction", "start_pos": 131, "end_pos": 152, "type": "TASK", "confidence": 0.8254404962062836}]}, {"text": "Wikipedia's links provide disambiguated semantic information.", "labels": [], "entities": []}, {"text": "For example, when a system processes the text \"Chicago was received with critical acclaim\" from an article, the system does not need to infer the referent entity of \"Chicago\" if the word is already hyperlinked to the Wikipedia page of the Oscar-winning film.", "labels": [], "entities": []}, {"text": "Unfortunately, in Wikipedia only a fraction of the phrases that can be linked are in fact annotated with a hyperlink.", "labels": [], "entities": []}, {"text": "This is due to Wikipedia's conventions of only linking to each concept once, and only when the links have a certain level of utility for human readers.", "labels": [], "entities": []}, {"text": "We see this as an http://en.wikipedia.org/wiki/ Wikipedia: opportunity to improve Wikipedia as a resource for NLP systems.", "labels": [], "entities": []}, {"text": "Our experiments estimate that as of September 2013, there were an average of 30 references to Wikipedia concepts left unlinked within each of English Wikipedia's four million pages.", "labels": [], "entities": []}, {"text": "In this paper, our goal is to augment Wikipedia with additional high-precision links, in order to provide anew resource for systems that use Wikipedia's link structure as a foundation.", "labels": [], "entities": []}, {"text": "Identifying references to concepts (called mentions) in text and linking them to Wikipedia is a task known as Wikification.", "labels": [], "entities": [{"text": "Identifying references to concepts (called mentions) in text", "start_pos": 0, "end_pos": 60, "type": "TASK", "confidence": 0.8828651666641235}]}, {"text": "Wikification for general text has been addressed in a wide variety of recent work).", "labels": [], "entities": []}, {"text": "The major challenge of this task is to resolve the ambiguity of phrases, and recent work makes use of various kinds of information found in the document to tackle the challenge.", "labels": [], "entities": []}, {"text": "In contrast to this body of work, here we focus on the special case of Wikifying Wikipedia articles, instead of general documents.", "labels": [], "entities": []}, {"text": "This gives us an advantage over general-text systems due to Wikipedia's rich content and existing link structure.", "labels": [], "entities": []}, {"text": "We introduce 3W, a system that identifies mentions within Wikipedia and links each to its referent concept.", "labels": [], "entities": []}, {"text": "We show how a Wikipedia-specific Semantic Relatedness measure that leverages the link structure of Wikipedia () allows 3W to be radically more precise at high levels of yield when compared to baseline Wikifiers that target general text.", "labels": [], "entities": [{"text": "Semantic Relatedness", "start_pos": 33, "end_pos": 53, "type": "TASK", "confidence": 0.668180450797081}]}, {"text": "Our experiment shows that 3W can add on average seven new links per article at precision of 0.98, adding approximately 28 million new links to 4 million articles across English Wikipedia.", "labels": [], "entities": [{"text": "precision", "start_pos": 79, "end_pos": 88, "type": "METRIC", "confidence": 0.9984235763549805}, {"text": "English Wikipedia", "start_pos": 169, "end_pos": 186, "type": "DATASET", "confidence": 0.8692913055419922}]}], "datasetContent": [{"text": "In this section, we provide an evaluation of our system and its subcomponents.", "labels": [], "entities": []}, {"text": "We trained our initial ranker models from 100,000 randomly selected existing links (L).", "labels": [], "entities": []}, {"text": "These links were excluded when building feature values (i.e. the prior probability, or Semantic Relatedness).", "labels": [], "entities": []}, {"text": "We formed an evaluation set of new links by applying our mention extractor to 2,000 randomly selected articles, and then manually labeling 1,900 of the mentions with either the correct concept or \"no correct concept.\"", "labels": [], "entities": []}, {"text": "We trained and tested our system on the evaluation set, using 10-fold cross validation.", "labels": [], "entities": []}, {"text": "For each fold, we partitioned data: 10-fold cross validation performance of the initial rankers by Accuracy (excluded \u2205-candidate mentions), BOT Precision, BOT Recall, BOT F1 on the 100,000 existing links. into 3 parts.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 99, "end_pos": 107, "type": "METRIC", "confidence": 0.9987406134605408}, {"text": "BOT Precision", "start_pos": 141, "end_pos": 154, "type": "METRIC", "confidence": 0.8253949582576752}, {"text": "BOT", "start_pos": 156, "end_pos": 159, "type": "METRIC", "confidence": 0.9921227097511292}, {"text": "Recall", "start_pos": 160, "end_pos": 166, "type": "METRIC", "confidence": 0.7129960656166077}, {"text": "BOT", "start_pos": 168, "end_pos": 171, "type": "METRIC", "confidence": 0.991927981376648}, {"text": "F1", "start_pos": 172, "end_pos": 174, "type": "METRIC", "confidence": 0.7048642039299011}]}, {"text": "We used 760 mentions for training the final ranker.", "labels": [], "entities": []}, {"text": "The linker was trained with 950 mentions and we tested our system using the other 190 mentions.", "labels": [], "entities": []}, {"text": "Previous work has used various ML approaches for ranking, such as SVMs (.", "labels": [], "entities": []}, {"text": "We found logistic regression produces similar accuracy to SVMs, but is faster for our feature set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.9992231130599976}]}, {"text": "For the linker, we use an SVM with probabilistic output () to estimate a confidence score for each output link.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: 10-fold cross validation performance of the initial  rankers by Accuracy (excluded \u2205-candidate mentions), BOT  Precision, BOT Recall, BOT F1 on the 100,000 existing links.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.9986513257026672}, {"text": "BOT  Precision", "start_pos": 116, "end_pos": 130, "type": "METRIC", "confidence": 0.8029371798038483}, {"text": "BOT", "start_pos": 132, "end_pos": 135, "type": "METRIC", "confidence": 0.9750565886497498}, {"text": "Recall", "start_pos": 136, "end_pos": 142, "type": "METRIC", "confidence": 0.6497285962104797}, {"text": "BOT", "start_pos": 144, "end_pos": 147, "type": "METRIC", "confidence": 0.9915633201599121}, {"text": "F1", "start_pos": 148, "end_pos": 150, "type": "METRIC", "confidence": 0.7913211584091187}]}, {"text": " Table 2: 10-fold cross validation performance of the system  over 1,900 labeled mentions. Acc is disambiguation accuracy  of solvable mentions. Yield is the number of output new  mentions at precision \u2265 0.98, and %Yield is the percentage  of Yield over the solvable mentions (recall).", "labels": [], "entities": [{"text": "Acc", "start_pos": 91, "end_pos": 94, "type": "METRIC", "confidence": 0.9992399215698242}, {"text": "accuracy", "start_pos": 113, "end_pos": 121, "type": "METRIC", "confidence": 0.9688035249710083}, {"text": "precision", "start_pos": 192, "end_pos": 201, "type": "METRIC", "confidence": 0.9742805361747742}, {"text": "recall", "start_pos": 277, "end_pos": 283, "type": "METRIC", "confidence": 0.992012083530426}]}]}