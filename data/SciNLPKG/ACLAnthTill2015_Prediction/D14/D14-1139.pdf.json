{"title": [{"text": "Weakly-Supervised Learning with Cost-Augmented Contrastive Estimation", "labels": [], "entities": []}], "abstractContent": [{"text": "We generalize contrastive estimation in two ways that permit adding more knowledge to unsupervised learning.", "labels": [], "entities": [{"text": "contrastive estimation", "start_pos": 14, "end_pos": 36, "type": "TASK", "confidence": 0.7612178027629852}]}, {"text": "The first allows the modeler to specify not only the set of corrupted inputs for each observation , but also how bad each one is.", "labels": [], "entities": []}, {"text": "The second allows specifying structural preferences on the latent variable used to explain the observations.", "labels": [], "entities": []}, {"text": "They require setting additional hyperparameters, which can be problematic in unsupervised learning, so we investigate new methods for unsuper-vised model selection and system combination.", "labels": [], "entities": []}, {"text": "We instantiate these ideas for part-of-speech induction without tag dictionaries , improving over contrastive estimation as well as strong benchmarks from the PASCAL 2012 shared task.", "labels": [], "entities": [{"text": "part-of-speech induction", "start_pos": 31, "end_pos": 55, "type": "TASK", "confidence": 0.6847846210002899}, {"text": "PASCAL 2012 shared task", "start_pos": 159, "end_pos": 182, "type": "DATASET", "confidence": 0.8565143197774887}]}], "introductionContent": [{"text": "Unsupervised NLP aims to discover useful structure in unannotated text.", "labels": [], "entities": []}, {"text": "This structure might be part-of-speech (POS) tag sequences, morphological segmentation), or syntactic structure (), among others.", "labels": [], "entities": []}, {"text": "Unsupervised systems typically improve when researchers incorporate knowledge to bias learning to capture characteristics of the desired structure.", "labels": [], "entities": []}, {"text": "There are many successful examples of adding knowledge to improve learning without labeled examples, including: sparsity in POS tag distributions), short attachments for dependency parsing), agreement of word alignment models (), power law effects in lexical distributions), multilingual constraints (, and orthographic cues (), inter alia.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 170, "end_pos": 188, "type": "TASK", "confidence": 0.7297225594520569}, {"text": "agreement of word alignment", "start_pos": 191, "end_pos": 218, "type": "TASK", "confidence": 0.5718240961432457}]}, {"text": "Contrastive estimation (CE;) is a general approach to weakly-supervised learning with a particular way of incorporating knowledge.", "labels": [], "entities": [{"text": "Contrastive estimation (CE", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.6842241883277893}]}, {"text": "CE increases the likelihood of the observations at the expense of those in a particular neighborhood of each observation.", "labels": [], "entities": [{"text": "CE", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.9815951585769653}, {"text": "likelihood", "start_pos": 17, "end_pos": 27, "type": "METRIC", "confidence": 0.9888193011283875}]}, {"text": "The neighborhood typically contains corrupted versions of the observations.", "labels": [], "entities": []}, {"text": "The latent structure is marginalized out for both the observations and their corruptions; the intent is to learn latent structure that helps to explain why the observation was generated rather than any of the corrupted alternatives.", "labels": [], "entities": []}, {"text": "In this paper, we present anew objective function for weakly-supervised learning that generalizes CE by including two types of cost functions, one on observations and one on output structures.", "labels": [], "entities": []}, {"text": "The first ( \u00a74) allows us to specify not only the set of corrupted observations, but also how bad each corruption was.", "labels": [], "entities": []}, {"text": "We use n-gram language models to measure the severity of each corruption.", "labels": [], "entities": []}, {"text": "The second ( \u00a75) allows us to specify preferences on desired output structures, regardless of the input sentence.", "labels": [], "entities": []}, {"text": "For POS tagging, we attempt to learn language-independent tag frequencies by computing counts from treebanks for 11 languages not used in our POS induction experiments.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 4, "end_pos": 15, "type": "TASK", "confidence": 0.925422728061676}]}, {"text": "For example, we encourage tag sequences that contain adjacent nouns and penalize those that contain adjacent adpositions.", "labels": [], "entities": []}, {"text": "We consider several unsupervised ways to set hyperparameters for these cost functions ( \u00a77), including the recently-proposed log-likelihood estimator of.", "labels": [], "entities": []}, {"text": "We also circumvent hyperparameter selection via system combination, developing a novel voting scheme for POS induction that aligns tag identifiers across runs.", "labels": [], "entities": [{"text": "POS induction", "start_pos": 105, "end_pos": 118, "type": "TASK", "confidence": 0.8942249417304993}]}, {"text": "We evaluate our approach, which we call costaugmented contrastive estimation (CCE), on POS induction without tag dictionaries for five languages from the PASCAL shared task).", "labels": [], "entities": [{"text": "costaugmented contrastive estimation (CCE)", "start_pos": 40, "end_pos": 82, "type": "METRIC", "confidence": 0.5205124368270239}]}, {"text": "We find that CCE improves over both standard CE as well as strong baselines from the shared task.", "labels": [], "entities": [{"text": "CCE", "start_pos": 13, "end_pos": 16, "type": "METRIC", "confidence": 0.5892264246940613}, {"text": "CE", "start_pos": 45, "end_pos": 47, "type": "METRIC", "confidence": 0.9306415915489197}]}, {"text": "In particular, our final average accuracies are better than all entries in the shared task that use the same number of tags.", "labels": [], "entities": [{"text": "final average accuracies", "start_pos": 19, "end_pos": 43, "type": "METRIC", "confidence": 0.7144290804862976}]}], "datasetContent": [{"text": "Task and Datasets We consider POS induction without tag dictionaries using five freely-available datasets from the PASCAL shared task.", "labels": [], "entities": [{"text": "POS induction", "start_pos": 30, "end_pos": 43, "type": "TASK", "confidence": 0.8962406814098358}]}, {"text": "These include Danish (DA), using the Copenhagen Dependency Treebank v2 (Buch-; Dutch (NL), using the Alpino treebank (); Portuguese (PT), using the Floresta Sint\u00e1(c)tica treebank (); Slovene (SL), using the jos500k treebank (; and Swedish (SV), using the Talbanken treebank ().", "labels": [], "entities": [{"text": "Copenhagen Dependency Treebank v2", "start_pos": 37, "end_pos": 70, "type": "DATASET", "confidence": 0.885840430855751}, {"text": "Alpino treebank", "start_pos": 101, "end_pos": 116, "type": "DATASET", "confidence": 0.8695017099380493}, {"text": "Floresta Sint\u00e1(c)tica treebank", "start_pos": 148, "end_pos": 178, "type": "DATASET", "confidence": 0.6520816087722778}, {"text": "jos500k treebank", "start_pos": 207, "end_pos": 223, "type": "DATASET", "confidence": 0.7426851987838745}, {"text": "Talbanken treebank", "start_pos": 255, "end_pos": 273, "type": "DATASET", "confidence": 0.9053942561149597}]}, {"text": "We use their provided training, development, and test sets.", "labels": [], "entities": []}, {"text": "Evaluation We fix the number of tags in our models to 12, which matches the number of universal tags from.", "labels": [], "entities": []}, {"text": "We use both many-to-1 (M-1) and 1-to-1 (1-1) accuracy as our evaluation metrics, using the universal tags for the gold standard (which was done for the official evaluation for the shared task).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.9667162895202637}]}, {"text": "We note that our \u03c0 function assigns identities to tags (e.g., tag 1 is assumed to be NOUN), so we could use actual tagging accuracy when training with the \u03c0 cost function.", "labels": [], "entities": [{"text": "NOUN", "start_pos": 85, "end_pos": 89, "type": "METRIC", "confidence": 0.8210945129394531}, {"text": "accuracy", "start_pos": 123, "end_pos": 131, "type": "METRIC", "confidence": 0.98269122838974}]}, {"text": "But we use M-1 and 1-1 accuracy to enable easier comparison both among different settings and to prior work.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9927789568901062}]}, {"text": "Baselines From the shared task, we compare to all entries that used 12 tags.", "labels": [], "entities": []}, {"text": "These include http://wiki.cs.ox.ac.uk/ InducingLinguisticStructure/SharedTask 6 It is common to use a greedy algorithm to compute 1-to-1 accuracy, e.g., as in the shared task scoring script (http://www.dcs.shef.ac.uk/ \u02dc tcohn/ wils/eval.tar.gz), though the optimal mapping can be computed efficiently via the maximum weighted bipartite matching algorithm, as stated above.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 137, "end_pos": 145, "type": "METRIC", "confidence": 0.9886065125465393}]}, {"text": "We use the shared task scorer for all results here for ease of comparison.", "labels": [], "entities": []}, {"text": "When we instead evaluate using the optimal mapping, we find that accuracies are usually only slightly higher than those found by the greedy algorithm.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 65, "end_pos": 75, "type": "METRIC", "confidence": 0.9959179759025574}]}, {"text": "BROWN clusters, clusters obtained using the mkcls tool, and the featurized HMM with sparsity constraints trained using posterior regularization (PR), described by.", "labels": [], "entities": []}, {"text": "The PR system achieved the highest average 1-1 accuracy in the shared task.", "labels": [], "entities": [{"text": "PR", "start_pos": 4, "end_pos": 6, "type": "TASK", "confidence": 0.9466300010681152}, {"text": "accuracy", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.9833359718322754}]}, {"text": "We restrict our attention to systems that use 12 tags because the M-1 and 1-1 metrics are highly dependent upon the number of hypothesized tags.", "labels": [], "entities": []}, {"text": "In general, using more tags leads to higher M-1 and lower 1-1.", "labels": [], "entities": [{"text": "M-1", "start_pos": 44, "end_pos": 47, "type": "METRIC", "confidence": 0.9715825915336609}]}, {"text": "By keeping the number of tags fixed, we hope to provide a cleaner comparison among approaches.", "labels": [], "entities": []}, {"text": "We compare to two other baselines: an HMM trained with 500 iterations of EM and an HMM trained with 100 iterations of stepwise EM.", "labels": [], "entities": []}, {"text": "We used random initialization as done by Liang and Klein: we set each parameter in each multinomial to exp{1 + c}, where c \u223c U, then normalized to get probability distributions.", "labels": [], "entities": []}, {"text": "For stepwise EM, we used minibatch size 3 and stepsize reduction power 0.7.", "labels": [], "entities": [{"text": "EM", "start_pos": 13, "end_pos": 15, "type": "TASK", "confidence": 0.9357752799987793}]}, {"text": "For all models we trained, including both baselines and CCE, we used only the training data during training and used the unannotated development data for certain model selection criteria.", "labels": [], "entities": []}, {"text": "No labels were used except for final evaluation on the test data.", "labels": [], "entities": []}, {"text": "Therefore, we need away to handle unknown words in test data.", "labels": [], "entities": []}, {"text": "When running EM and stepwise EM, while reading in the final 10% of sentences in the training set, we replace novel words with the special token UNK.", "labels": [], "entities": []}, {"text": "We then replace unknown words in test data with UNK.", "labels": [], "entities": [{"text": "UNK", "start_pos": 48, "end_pos": 51, "type": "DATASET", "confidence": 0.873539924621582}]}], "tableCaptions": [{"text": " Table 1: Counts and costs for universal tags based  on treebanks for 11 languages not used in POS in- duction experiments.", "labels": [], "entities": []}, {"text": " Table 2: Results for observation cost functions. The CE baseline corresponds to rows where cost=\"none\".  Other rows are CCE. Best score for each column and each neighborhood is bold.", "labels": [], "entities": [{"text": "CE", "start_pos": 54, "end_pos": 56, "type": "METRIC", "confidence": 0.9556289911270142}, {"text": "CCE", "start_pos": 121, "end_pos": 124, "type": "METRIC", "confidence": 0.8403075337409973}]}, {"text": " Table 3: Unsupervised POS tagging accuracies for five languages, showing results for three systems from  the PASCAL shared task as well as three other baselines (EM, stepwise EM, and contrastive estimation).  All (C)CE results use the TRANS1 neighborhood. The best score in each column is bold.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 23, "end_pos": 34, "type": "TASK", "confidence": 0.7880094349384308}]}]}