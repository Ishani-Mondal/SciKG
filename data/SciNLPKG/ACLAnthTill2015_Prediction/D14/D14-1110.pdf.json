{"title": [{"text": "A Unified Model for Word Sense Representation and Disambiguation", "labels": [], "entities": [{"text": "Word Sense Representation and Disambiguation", "start_pos": 20, "end_pos": 64, "type": "TASK", "confidence": 0.7690330564975738}]}], "abstractContent": [{"text": "Most word representation methods assume that each word owns a single semantic vector.", "labels": [], "entities": [{"text": "word representation", "start_pos": 5, "end_pos": 24, "type": "TASK", "confidence": 0.7154068499803543}]}, {"text": "This is usually problematic because lexical ambiguity is ubiquitous, which is also the problem to be resolved byword sense disambiguation.", "labels": [], "entities": []}, {"text": "In this paper, we present a unified model for joint word sense representation and disambiguation, which will assign distinct representations for each word sense.", "labels": [], "entities": [{"text": "word sense representation", "start_pos": 52, "end_pos": 77, "type": "TASK", "confidence": 0.6234341462453207}]}, {"text": "1 The basic idea is that both word sense representation (WS-R) and word sense disambiguation (WS-D) will benefit from each other: (1) high-quality WSR will capture rich information about words and senses, which should be helpful for WSD, and (2) high-quality WSD will provide reliable disambiguat-ed corpora for learning better sense representations.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 67, "end_pos": 92, "type": "TASK", "confidence": 0.6113777061303457}, {"text": "WSD", "start_pos": 233, "end_pos": 236, "type": "TASK", "confidence": 0.9285169839859009}]}, {"text": "Experimental results show that, our model improves the performance of contextual word similarity compared to existing WSR methods, outperforms state-of-the-art supervised methods on domain-specific WSD, and achieves competitive performance on coarse-grained all-words WSD.", "labels": [], "entities": []}], "introductionContent": [{"text": "Word representation aims to build vectors for each word based on its context in a large corpus, usually capturing both semantic and syntactic information of words.", "labels": [], "entities": [{"text": "Word representation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7559325993061066}]}, {"text": "These representations can be used as features or inputs, which are widely employed in information retrieval (, document classification) and other NLP tasks.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 86, "end_pos": 107, "type": "TASK", "confidence": 0.7512010037899017}, {"text": "document classification)", "start_pos": 111, "end_pos": 135, "type": "TASK", "confidence": 0.814526379108429}]}, {"text": "Our sense representations can be downloaded at http: //pan.baidu.com/s/1eQcPK8i.", "labels": [], "entities": []}, {"text": "Most word representation methods assume each word owns a single vector.", "labels": [], "entities": [{"text": "word representation", "start_pos": 5, "end_pos": 24, "type": "TASK", "confidence": 0.724068358540535}]}, {"text": "However, this is usually problematic due to the homonymy and polysemy of many words.", "labels": [], "entities": []}, {"text": "To remedy the issue, proposed a multi-prototype vector space model, where the contexts of each word are first clustered into groups, and then each cluster generates a distinct prototype vector fora word by averaging overall context vectors within the cluster.", "labels": [], "entities": []}, {"text": "followed this idea, but introduced continuous distributed vectors based on probabilistic neural language models for word representations.", "labels": [], "entities": [{"text": "word representations", "start_pos": 116, "end_pos": 136, "type": "TASK", "confidence": 0.7151750773191452}]}, {"text": "These cluster-based models conduct unsupervised word sense induction by clustering word contexts and, thus, suffer from the following issues: \u2022 It is usually difficult for these cluster-based models to determine the number of clusters.", "labels": [], "entities": [{"text": "word sense induction", "start_pos": 48, "end_pos": 68, "type": "TASK", "confidence": 0.7481477359930674}]}, {"text": "simply cluster word contexts into static K clusters for each word, which is arbitrary and may introduce mistakes.", "labels": [], "entities": []}, {"text": "\u2022 These cluster-based models are typically offline , so they cannot be efficiently adapted to new senses, new words or new data.", "labels": [], "entities": []}, {"text": "\u2022 It is also troublesome to find the sense that a word prototype corresponds to; thus, these cluster-based models cannot be directly used to perform word sense disambiguation.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 149, "end_pos": 174, "type": "TASK", "confidence": 0.698332150777181}]}, {"text": "In reality, many large knowledge bases have been constructed with word senses available online, such as WordNet and Wikipedia.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 104, "end_pos": 111, "type": "DATASET", "confidence": 0.9692395329475403}]}, {"text": "Utilizing these knowledge bases to learn word representation and sense representation is a natural choice.", "labels": [], "entities": [{"text": "word representation", "start_pos": 41, "end_pos": 60, "type": "TASK", "confidence": 0.7068984508514404}, {"text": "sense representation", "start_pos": 65, "end_pos": 85, "type": "TASK", "confidence": 0.7155190110206604}]}, {"text": "In this paper, we present a unified model for both word sense representation and disambiguation based on these knowledge bases and large-scale text corpora.", "labels": [], "entities": [{"text": "word sense representation", "start_pos": 51, "end_pos": 76, "type": "TASK", "confidence": 0.6973627408345541}]}, {"text": "The unified model can (1) perform word sense disambiguation based on vector representations, and (2) learn continuous distributed vector representation for word and sense jointly.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 34, "end_pos": 59, "type": "TASK", "confidence": 0.716440757115682}]}, {"text": "The basic idea is that, the tasks of word sense representation (WSR) and word sense disambiguation (WSD) can benefit from each other: (1) high-quality WSR will capture rich semantic and syntactic information of words and senses, which should be helpful for WSD; (2) high-quality WS-D will provide reliable disambiguated corpora for learning better sense representations.", "labels": [], "entities": [{"text": "word sense representation (WSR)", "start_pos": 37, "end_pos": 68, "type": "TASK", "confidence": 0.778778021534284}, {"text": "word sense disambiguation (WSD)", "start_pos": 73, "end_pos": 104, "type": "TASK", "confidence": 0.7501802096764246}, {"text": "WSD", "start_pos": 257, "end_pos": 260, "type": "TASK", "confidence": 0.9392939805984497}]}, {"text": "By utilizing these knowledge bases, the problem mentioned above can be overcome: \u2022 The number of senses of a word can be decided by the expert annotators or web users.", "labels": [], "entities": []}, {"text": "\u2022 When anew sense appears, our model can be easily applied to obtain anew sense representation.", "labels": [], "entities": []}, {"text": "\u2022 Every sense vector has a corresponding sense in these knowledge bases.", "labels": [], "entities": []}, {"text": "We conduct experiments to investigate the performance of our model for both WSR and WS-D.", "labels": [], "entities": [{"text": "WSR", "start_pos": 76, "end_pos": 79, "type": "TASK", "confidence": 0.47689950466156006}, {"text": "WS-D", "start_pos": 84, "end_pos": 88, "type": "DATASET", "confidence": 0.7261968851089478}]}, {"text": "We evaluate the performance of WSR using a contextual word similarity task, and results show that out model can significantly improve the correlation with human judgments compared to baselines.", "labels": [], "entities": [{"text": "WSR", "start_pos": 31, "end_pos": 34, "type": "TASK", "confidence": 0.9328704476356506}]}, {"text": "We further evaluate the performance on both domain-specific WSD and coarse-grained allwords WSD, and results show that our model yields performance competitive with state-of-theart supervised approaches.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we first present the nearest neighbors of some words and their senses, showing that our sense vectors can capture the semantics of words.", "labels": [], "entities": []}, {"text": "Then, we use three tasks to evaluate our unified model: a contextual word similarity task to evaluate our sense representations, and two standard WSD tasks to evaluate our knowledge-based WSD algorithm based on the sense vectors.", "labels": [], "entities": []}, {"text": "Experimental results show that our model not only improves the correlation with human judgments on the contextual word similarity task but also outperforms state-of-the-art supervised WSD systems on domain-specific datasets and competes with them in a coarse-grained all-words setting.", "labels": [], "entities": [{"text": "contextual word similarity task", "start_pos": 103, "end_pos": 134, "type": "TASK", "confidence": 0.677858330309391}]}, {"text": "We choose Wikipedia as the corpus to train the word vectors because of its wide coverage of topics and words usages.", "labels": [], "entities": []}, {"text": "We use an English Wikipedia database dump from October 2013 2 , which includes roughly 3 million articles and 1 billion tokens.", "labels": [], "entities": [{"text": "English Wikipedia database dump from October 2013 2", "start_pos": 10, "end_pos": 61, "type": "DATASET", "confidence": 0.948914110660553}]}, {"text": "We use Wikipedia Extractor 3 to preprocess the Wikipedia pages and only save the content of the articles.", "labels": [], "entities": [{"text": "Wikipedia Extractor 3", "start_pos": 7, "end_pos": 28, "type": "DATASET", "confidence": 0.8839440743128458}]}, {"text": "We use word2vec 4 to train Skip-gram.", "labels": [], "entities": [{"text": "Skip-gram", "start_pos": 27, "end_pos": 36, "type": "DATASET", "confidence": 0.9371533989906311}]}, {"text": "We use the default parameters of word2vec and the dimension of the vector representations is 200.", "labels": [], "entities": []}, {"text": "We use WordNet 5 as our sense inventory.", "labels": [], "entities": [{"text": "WordNet 5", "start_pos": 7, "end_pos": 16, "type": "DATASET", "confidence": 0.9602122902870178}]}, {"text": "The datasets for different tasks are tagged with different versions of WordNet.  is 1.7 for the domain-specific WSD task and 2.1 for the coarse-grained WSD task.", "labels": [], "entities": [{"text": "WordNet.", "start_pos": 71, "end_pos": 79, "type": "DATASET", "confidence": 0.9380225539207458}]}, {"text": "We use the S2C algorithm described in Section 2.3 to perform word sense disambiguation to obtain more relevant occurrences for each sense.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 61, "end_pos": 86, "type": "TASK", "confidence": 0.6483954191207886}]}, {"text": "We compare S2C and L2R on the coarse-grained WS-D task in a all-words setting.", "labels": [], "entities": []}, {"text": "The experimental results of our model are obtained by setting the similarity threshold as \u03b4 = 0 and the score margin threshold as \u03b5 = 0.1.", "labels": [], "entities": [{"text": "similarity threshold", "start_pos": 66, "end_pos": 86, "type": "METRIC", "confidence": 0.9780442118644714}, {"text": "score margin threshold", "start_pos": 104, "end_pos": 126, "type": "METRIC", "confidence": 0.9344435532887777}]}, {"text": "The influence of parameters on our model can be found in Section 3.5.", "labels": [], "entities": []}, {"text": "shows the nearest neighbors of word vectors and sense vectors based on cosine similarity.", "labels": [], "entities": []}, {"text": "We see that our sense representations can identify different meanings of a word, allowing our model to capture more semantic and syntactic relationships between words and senses.", "labels": [], "entities": []}, {"text": "Note that each sense vector in our model corresponds to a sense in WordNet; thus, our sense vectors can be used to perform knowledge-based word sense disambiguation, whereas the vectors of cluster-based models cannot.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 67, "end_pos": 74, "type": "DATASET", "confidence": 0.949526846408844}, {"text": "word sense disambiguation", "start_pos": 139, "end_pos": 164, "type": "TASK", "confidence": 0.6161996225516001}]}], "tableCaptions": [{"text": " Table 3: Spearman's \u03c1 on the SCWS dataset. Our  Model-S uses one representation per word to com- pute similarities, while Our Model-M uses one  representation per sense to compute similarities.  AvgSim calculates the similarity with each sense  contributing equally, while AvgSimC weighs the  sense according to the probability of the word  choosing that sense in context c.", "labels": [], "entities": [{"text": "SCWS dataset", "start_pos": 30, "end_pos": 42, "type": "DATASET", "confidence": 0.9379239082336426}]}, {"text": " Table 4: Performance on the Sports and Finance  sections of the dataset from (", "labels": [], "entities": []}, {"text": " Table 5: Performance on Semeval-2007 coarse- grained all-words WSD. In the type column,  U, Semi and S stand for unsupervised, semi- supervised and supervised, respectively. The dif- ferences between the results in bold in each col- umn of the table are not statistically significant at  p < 0.05.", "labels": [], "entities": [{"text": "Semeval-2007 coarse- grained all-words WSD", "start_pos": 25, "end_pos": 67, "type": "TASK", "confidence": 0.45515867074330646}]}, {"text": " Table 6: Evaluation results on the coarse-grained  all-words WSD when the similarity threshold \u03b4  ranges from \u22120.1 to 0.3.", "labels": [], "entities": [{"text": "similarity threshold \u03b4", "start_pos": 75, "end_pos": 97, "type": "METRIC", "confidence": 0.9237169027328491}]}, {"text": " Table 7: Evaluation results on the coarse-grained  all-words WSD when the score margin threshold  \u03b5 ranges from 0.0 to 0.3.", "labels": [], "entities": [{"text": "WSD", "start_pos": 62, "end_pos": 65, "type": "DATASET", "confidence": 0.4648136496543884}, {"text": "score margin threshold  \u03b5", "start_pos": 75, "end_pos": 100, "type": "METRIC", "confidence": 0.9267405867576599}]}]}