{"title": [{"text": "Comparing Representations of Semantic Roles for String-To-Tree Decoding", "labels": [], "entities": []}], "abstractContent": [{"text": "We introduce new features for incorporating semantic predicate-argument structures in machine translation (MT).", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 86, "end_pos": 110, "type": "TASK", "confidence": 0.8414875507354737}]}, {"text": "The methods focus on the completeness of the semantic structures of the translations, as well as the order of the translated semantic roles.", "labels": [], "entities": []}, {"text": "We experiment with translation rules which contain the core arguments for the predicates in the source side of a MT system, and observe that using these rules significantly improves the translation quality.", "labels": [], "entities": [{"text": "MT", "start_pos": 113, "end_pos": 115, "type": "TASK", "confidence": 0.9605130553245544}]}, {"text": "We also present anew semantic feature that resembles a language model.", "labels": [], "entities": []}, {"text": "Our results show that the language model feature can also significantly improve MT results.", "labels": [], "entities": [{"text": "MT", "start_pos": 80, "end_pos": 82, "type": "TASK", "confidence": 0.9955310225486755}]}], "introductionContent": [{"text": "In recent years, there have been increasing efforts to incorporate semantics in statistical machine translation (SMT), and the use of predicateargument structures has provided promising improvements in translation quality.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 80, "end_pos": 117, "type": "TASK", "confidence": 0.7833682745695114}]}, {"text": "showed that shallow semantic parsing can improve the translation quality in a machine translation system.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 20, "end_pos": 36, "type": "TASK", "confidence": 0.7368666529655457}, {"text": "machine translation", "start_pos": 78, "end_pos": 97, "type": "TASK", "confidence": 0.7140394300222397}]}, {"text": "They introduced a two step model, in which they used a semantic parser to rerank the translation hypotheses of a phrase-based system.", "labels": [], "entities": []}, {"text": "used semantic features fora tree-to-string syntax based SMT system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 56, "end_pos": 59, "type": "TASK", "confidence": 0.9899691939353943}]}, {"text": "Their features modeled deletion and reordering for source side semantic roles, and they improved the translation quality.", "labels": [], "entities": []}, {"text": "incorporated the semantic structures into phrasebased SMT by adding syntactic and semantic features to their translation model.", "labels": [], "entities": [{"text": "phrasebased SMT", "start_pos": 42, "end_pos": 57, "type": "TASK", "confidence": 0.5972566306591034}]}, {"text": "They proposed two discriminative models which included features for predicate translation and argument reordering from source to target side.", "labels": [], "entities": [{"text": "predicate translation", "start_pos": 68, "end_pos": 89, "type": "TASK", "confidence": 0.8724638819694519}]}, {"text": "used semantic structures in a string-to-tree translation system by extracting translation rules enriched with semantic information, and showed that this can improve the translation quality.", "labels": [], "entities": []}, {"text": "used predicateargument structure reordering models for hierarchical phrase-based translation, and they used linguistically motivated constraints for phrase translation.", "labels": [], "entities": [{"text": "phrase-based translation", "start_pos": 68, "end_pos": 92, "type": "TASK", "confidence": 0.7109253257513046}, {"text": "phrase translation", "start_pos": 149, "end_pos": 167, "type": "TASK", "confidence": 0.8546358346939087}]}, {"text": "In this paper, we experiment with methods for incorporating semantics in a string-to-tree MT system.", "labels": [], "entities": [{"text": "MT", "start_pos": 90, "end_pos": 92, "type": "TASK", "confidence": 0.9052989482879639}]}, {"text": "These methods are designed to model the order of translation, as well as the completeness of the semantic structures.", "labels": [], "entities": []}, {"text": "We extract translation rules that include the complete semantic structure in the source side, and compare that with using semantic rules for the target side predicates.", "labels": [], "entities": []}, {"text": "We present a method for modeling the order of semantic role sequences that appear spread across multiple syntax-based translation rules, in order to overcome the problem that a rule representing the entire semantic structure of a predicate is often too large and too specific to apply to new sentences during decoding.", "labels": [], "entities": []}, {"text": "For this method, we compare the verb-specific roles of PropBank and the more general thematic roles of VerbNet.", "labels": [], "entities": [{"text": "PropBank", "start_pos": 55, "end_pos": 63, "type": "DATASET", "confidence": 0.9410184025764465}]}, {"text": "These essential arguments of a verbal predicate are called the core arguments.", "labels": [], "entities": []}, {"text": "Standard syntaxbased MT is incapable of ensuring that the target translation includes all of the core arguments of a predicate that appear in the source sentence.", "labels": [], "entities": [{"text": "MT", "start_pos": 21, "end_pos": 23, "type": "TASK", "confidence": 0.8403029441833496}]}, {"text": "To encourage the translation of the likely core arguments, we follow the work of, who use special translation rules with complete semantic structures of the predicates in the target side of their MT system.", "labels": [], "entities": [{"text": "translation", "start_pos": 17, "end_pos": 28, "type": "TASK", "confidence": 0.9679686427116394}, {"text": "MT", "start_pos": 196, "end_pos": 198, "type": "TASK", "confidence": 0.9499080777168274}]}, {"text": "Each of these rules includes a predicate and all of its core arguments.", "labels": [], "entities": []}, {"text": "Instead of incorporating only the target side semantic rules, we extract the special rules for both the source and the target sides, and compare the effectiveness of adding these rules to).", "labels": [], "entities": []}, {"text": "the system separately and simultaneously.", "labels": [], "entities": []}, {"text": "Besides the completeness of the arguments, it is also important for the arguments to appear in the correct order.", "labels": [], "entities": []}, {"text": "Our second method is designed to encourage correct order of translation for both the core and the non-core roles in the target sentence.", "labels": [], "entities": []}, {"text": "We designed anew feature that resembles the language model feature in a standard MT system.", "labels": [], "entities": []}, {"text": "We train a n-gram language model on sequences of semantic roles, by treating the semantic roles as the words in what we call the semantic language.", "labels": [], "entities": []}, {"text": "Our experimental results show that the language model feature significantly improves translation quality.", "labels": [], "entities": []}, {"text": "Semantic Role Labeling (SRL): We use semantic role labelers to annotate the training data that we use to extract the translation rules.", "labels": [], "entities": [{"text": "Semantic Role Labeling (SRL)", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8023645828167597}]}, {"text": "For target side SRL, the role labels are attached to the nonterminal nodes in the syntactic parse of each sentence.", "labels": [], "entities": [{"text": "target side SRL", "start_pos": 4, "end_pos": 19, "type": "TASK", "confidence": 0.589177131652832}]}, {"text": "For source side SRL, the labels annotate the spans from the source sentence that they cover.", "labels": [], "entities": [{"text": "source side SRL", "start_pos": 4, "end_pos": 19, "type": "TASK", "confidence": 0.5030941665172577}]}, {"text": "We train our semantic role labeler using two different standards:) and VerbNet).", "labels": [], "entities": [{"text": "semantic role labeler", "start_pos": 13, "end_pos": 34, "type": "TASK", "confidence": 0.6500733395417532}, {"text": "VerbNet", "start_pos": 71, "end_pos": 78, "type": "DATASET", "confidence": 0.6672530770301819}]}, {"text": "PropBank annotates the Penn Treebank with predicate-argument structures.It use generic labels (such as Arg0, Arg1, etc.) which are defined specifically for each verb.", "labels": [], "entities": [{"text": "PropBank", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9290246963500977}, {"text": "Penn Treebank", "start_pos": 23, "end_pos": 36, "type": "DATASET", "confidence": 0.8203963041305542}, {"text": "Arg0", "start_pos": 103, "end_pos": 107, "type": "METRIC", "confidence": 0.9908183217048645}, {"text": "Arg1", "start_pos": 109, "end_pos": 113, "type": "METRIC", "confidence": 0.943720817565918}]}, {"text": "We trained a semantic role labeler on the annotated Penn Treebank data and used the classifier to tag our training data.", "labels": [], "entities": [{"text": "Penn Treebank data", "start_pos": 52, "end_pos": 70, "type": "DATASET", "confidence": 0.9944527546564738}]}, {"text": "VerbNet is a verb lexicon that categorizes English verbs into hierarchical classes, and annotates them with thematic roles for the arguments that they accept.", "labels": [], "entities": [{"text": "VerbNet", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.911565363407135}]}, {"text": "Since the thematic roles use more meaningful labels (e.g. Agent, Patient, etc.), a language model trained on VerbNet labels maybe more likely to generalize across verbs than one trained on PropBank labels.", "labels": [], "entities": []}, {"text": "It may also provide more information, since VerbNet has a larger set of labels than PropBank.", "labels": [], "entities": [{"text": "VerbNet", "start_pos": 44, "end_pos": 51, "type": "DATASET", "confidence": 0.927309513092041}, {"text": "PropBank", "start_pos": 84, "end_pos": 92, "type": "DATASET", "confidence": 0.9614735841751099}]}, {"text": "To train the semantic role labeler on VerbNet, we used the mappings Figure 2: A deduction step in our baseline decoder provided by the SemLink project to annotate the Penn Treebank with the VerbNet roles.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 167, "end_pos": 180, "type": "DATASET", "confidence": 0.9934634268283844}]}, {"text": "These mappings map the roles in PropBank to the thematic roles of VerbNet.", "labels": [], "entities": [{"text": "PropBank", "start_pos": 32, "end_pos": 40, "type": "DATASET", "confidence": 0.931477427482605}, {"text": "VerbNet", "start_pos": 66, "end_pos": 73, "type": "DATASET", "confidence": 0.943733274936676}]}, {"text": "When there is no mapping fora role, we keep the role from Propbank.", "labels": [], "entities": [{"text": "Propbank", "start_pos": 58, "end_pos": 66, "type": "DATASET", "confidence": 0.9827594757080078}]}], "datasetContent": [{"text": "The data that we used for training the MT system was a Chinese-English corpus derived from newswire text from LDC.", "labels": [], "entities": [{"text": "MT", "start_pos": 39, "end_pos": 41, "type": "TASK", "confidence": 0.9710398316383362}, {"text": "LDC", "start_pos": 110, "end_pos": 113, "type": "DATASET", "confidence": 0.6236470341682434}]}, {"text": "The data consists of 250K sentences, which is 6.3M words in the English side.", "labels": [], "entities": []}, {"text": "Our language model was trained on the English side of the entire data, which consisted of 1.65M sentences (39.3M words).", "labels": [], "entities": []}, {"text": "Our development and test sets are from the newswire portion of NIST evaluations).", "labels": [], "entities": [{"text": "NIST evaluations", "start_pos": 63, "end_pos": 79, "type": "DATASET", "confidence": 0.8997347354888916}]}, {"text": "We used 392 sentences for the development set and 428 sentences for the test set.", "labels": [], "entities": []}, {"text": "These sentences have lengths smaller than 30, and they each have 4 reference translations.", "labels": [], "entities": []}, {"text": "We used our in-house stringto-tree decoder that uses Earley parsing.", "labels": [], "entities": []}, {"text": "Other than the features that we presented for our new methods, we used a set of nine standard features.", "labels": [], "entities": []}, {"text": "The rules for the baseline system were extracted using the GHKM method.", "labels": [], "entities": [{"text": "GHKM", "start_pos": 59, "end_pos": 63, "type": "DATASET", "confidence": 0.789907693862915}]}, {"text": "Our baseline GHKM rules also include composed rules, where larger rules are constructed by combining two levels of the regular GHKM rules.", "labels": [], "entities": []}, {"text": "We exclude any unary rules, and only keep rules that have scope up to 3.", "labels": [], "entities": []}, {"text": "For the semantic language model, we used the SRILM package) and trained a tri-gram language model with the default GoodTuring smoothing.", "labels": [], "entities": [{"text": "SRILM package", "start_pos": 45, "end_pos": 58, "type": "DATASET", "confidence": 0.8645326197147369}]}, {"text": "Our target side semantic role labeler uses a maximum entropy classifier to label parsed sentences.", "labels": [], "entities": []}, {"text": "We used Sections 02-22 of the Penn TreeBank to train the labeler, and sections 24 and 23 for development set and training set respectively.", "labels": [], "entities": [{"text": "Penn TreeBank", "start_pos": 30, "end_pos": 43, "type": "DATASET", "confidence": 0.9927164614200592}]}, {"text": "The labeler has a precision of 90% and a recall of 88%.", "labels": [], "entities": [{"text": "precision", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.9994415640830994}, {"text": "recall", "start_pos": 41, "end_pos": 47, "type": "METRIC", "confidence": 0.9991139769554138}]}, {"text": "We used the Chinese semantic role labeler of Wu and Palmer (2011) for source side SRL, which uses the LIBLINEAR () as a classifier.", "labels": [], "entities": [{"text": "LIBLINEAR", "start_pos": 102, "end_pos": 111, "type": "METRIC", "confidence": 0.9763594269752502}]}, {"text": "Minimum Error Rate Training (MERT) was used for tuning the feature weights.", "labels": [], "entities": [{"text": "Minimum Error Rate Training (MERT)", "start_pos": 0, "end_pos": 34, "type": "METRIC", "confidence": 0.8857415063040597}]}, {"text": "For all of our experiments, we ran 15 instances of MERT with random initial weight vectors, and used the weights of the top 3 results on the development set to test the systems on the test set.", "labels": [], "entities": []}, {"text": "We chose to use the top 3 runs (rather than the best run) of each system to account for the instability of MERT).", "labels": [], "entities": [{"text": "MERT", "start_pos": 107, "end_pos": 111, "type": "DATASET", "confidence": 0.5739554762840271}]}, {"text": "This method is designed to reflect the average performance of the MT system when trained with random restarts of MERT: we wish to discount runs in which the optimizer is stuck in a poor region of the weight space, but also to average across several good runs in order not to be mislead by the high variance of the single best run.", "labels": [], "entities": [{"text": "MT", "start_pos": 66, "end_pos": 68, "type": "TASK", "confidence": 0.9758039712905884}, {"text": "MERT", "start_pos": 113, "end_pos": 117, "type": "METRIC", "confidence": 0.6171258091926575}]}, {"text": "For each of our MT systems, we merged the results of the top 3 runs on the test set into one file, and ran a statistical significance test, comparing it to the merged top 3 results from our baseline system.", "labels": [], "entities": [{"text": "MT", "start_pos": 16, "end_pos": 18, "type": "TASK", "confidence": 0.9699743986129761}]}, {"text": "The 3 runs were merged by duplicating each run 3 times, and arranging them in the file so that the significance testing compares each run with all the runs of the baseline.", "labels": [], "entities": []}, {"text": "We performed significance testing using paired bootstrap resampling.", "labels": [], "entities": []}, {"text": "The difference is considered statistically significant if p < 0.05 using 1000 iterations of paired bootstrap resampling.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Comparisons of the methods with the  baseline. The BLEU scores are calculated on the  top 3 results from 15 runs MERT for each experi- ments. The p-values are calculated by comparing  each method against the baseline system.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 61, "end_pos": 65, "type": "METRIC", "confidence": 0.999529242515564}, {"text": "MERT", "start_pos": 123, "end_pos": 127, "type": "METRIC", "confidence": 0.9952724575996399}]}]}