{"title": [{"text": "A Cognitive Model of Semantic Network Learning", "labels": [], "entities": [{"text": "Semantic Network Learning", "start_pos": 21, "end_pos": 46, "type": "TASK", "confidence": 0.6581239104270935}]}], "abstractContent": [{"text": "Child semantic development includes learning the meaning of words as well as the semantic relations among words.", "labels": [], "entities": [{"text": "Child semantic development", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.7466035087903341}]}, {"text": "A presumed outcome of semantic development is the formation of a semantic network that reflects this knowledge.", "labels": [], "entities": [{"text": "semantic development", "start_pos": 22, "end_pos": 42, "type": "TASK", "confidence": 0.8431908488273621}]}, {"text": "We present an algorithm for simultaneously learning word meanings and gradually growing a semantic network, which adheres to the cognitive plausibility requirements of incrementality and limited computations.", "labels": [], "entities": []}, {"text": "We demonstrate that the semantic connections among words in addition to their context is necessary in forming a semantic network that resembles an adult's semantic knowledge.", "labels": [], "entities": []}], "introductionContent": [{"text": "Child semantic development includes the acquisition of word-to-concept mappings (part of word learning), and the formation of semantic connections among words/concepts.", "labels": [], "entities": [{"text": "Child semantic development", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.7454585631688436}]}, {"text": "There is considerable evidence that understanding the semantic properties of words improves child vocabulary acquisition.", "labels": [], "entities": [{"text": "child vocabulary acquisition", "start_pos": 92, "end_pos": 120, "type": "TASK", "confidence": 0.6352992157141367}]}, {"text": "In particular, children are sensitive to commonalities of semantic categories, and this abstract knowledge facilitates subsequent word learning ().", "labels": [], "entities": []}, {"text": "Furthermore, representation of semantic knowledge is significant as it impacts how word meanings are stored in, searched for, and retrieved from memory (.", "labels": [], "entities": [{"text": "representation of semantic knowledge", "start_pos": 13, "end_pos": 49, "type": "TASK", "confidence": 0.8176707029342651}]}, {"text": "Semantic knowledge is often represented as a graph (a semantic network) in which nodes correspond to words/concepts 1 , and edges specify the semantic relations).", "labels": [], "entities": []}, {"text": "demonstrated that a semantic network that encodes adult-level knowledge of words exhibits a small-world and scale-free structure.", "labels": [], "entities": []}, {"text": "That is, it is an overall sparse network with highly-connected local sub-networks, where these sub-networks are connected through high-degree hubs (nodes with many neighbours).", "labels": [], "entities": []}, {"text": "Much experimental research has investigated the underlying mechanisms of vocabulary learning and characteristics of semantic knowledge).", "labels": [], "entities": [{"text": "vocabulary learning", "start_pos": 73, "end_pos": 92, "type": "TASK", "confidence": 0.8571429252624512}]}, {"text": "However, existing computational models focus on certain aspects of semantic acquisition: Some researchers develop computational models of word learning without considering the acquisition of semantic connections that hold among words, or how this semantic knowledge is structured.", "labels": [], "entities": [{"text": "semantic acquisition", "start_pos": 67, "end_pos": 87, "type": "TASK", "confidence": 0.7806468307971954}]}, {"text": "Another line of work is to model formation of semantic categories but this work does not take into account how word meanings/concepts are acquired (.", "labels": [], "entities": []}, {"text": "Our goal in this work is to provide a cognitivelyplausible and unified account for both acquiring and representing semantic knowledge.", "labels": [], "entities": []}, {"text": "The requirements for cognitive plausibility enforce some constraints on a model to ensure that it is comparable with the cognitive process it is formulating ().", "labels": [], "entities": []}, {"text": "As we model semantic acquisition, the first requirement is incrementality, which means that the model learns gradually as it processes the input.", "labels": [], "entities": [{"text": "semantic acquisition", "start_pos": 12, "end_pos": 32, "type": "TASK", "confidence": 0.8153601288795471}]}, {"text": "Also, there is a limit on the number of computations the model performs at each step.", "labels": [], "entities": []}, {"text": "In this paper, we present an algorithm for si-multaneously learning word meanings and growing a semantic network, which adheres to the cognitive plausibility requirements of incrementality and limited computations.", "labels": [], "entities": []}, {"text": "We examine networks created by our model under various conditions, and explore what is required to obtain a structure that has appropriate semantic connections and has a small-world and scale-free structure.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate a semantic network in two regards: The semantic connectivity of the network -to what extent the semantically-related words are connected in the network; and the structure of the network -whether it exhibits a small-world and scale-free structure or not.", "labels": [], "entities": []}, {"text": "We use 20, 000 utterance-scene pairs as our training data.", "labels": [], "entities": []}, {"text": "Recall that we use clustering to help guide our semantic network growth algorithm.", "labels": [], "entities": []}, {"text": "Given the clustering algorithm in Section 3.3, we are interested to find the set of clusters that best explain the data.", "labels": [], "entities": []}, {"text": "(Other clustering algorithms can be used instead of this algorithm.)", "labels": [], "entities": []}, {"text": "We perform a search on the parameter space, and select the parameter values that result in the best clustering, based on the number of clusters and their average F-score.", "labels": [], "entities": [{"text": "F-score", "start_pos": 162, "end_pos": 169, "type": "METRIC", "confidence": 0.9937793612480164}]}, {"text": "The value of the clustering parameters are as follows: \u03b1 = 49, \u03bb 0 = 1.0, a 0 = 2.0, \u00b5 0 = 0.0, and \u03c3 0 = 0.05.", "labels": [], "entities": [{"text": "\u00b5 0", "start_pos": 85, "end_pos": 88, "type": "METRIC", "confidence": 0.9661420583724976}]}, {"text": "Two nouns with feature vectors F 1 and F 2 are connected in the network if cosine(F 1, F 2) is greater than or equal to 0.6.", "labels": [], "entities": []}, {"text": "(This threshold was selected following empirical examination of the similarity values we observe among the \"true\" meaning in our input generation lexicon.)", "labels": [], "entities": []}, {"text": "The weight on the edge that connects these nouns specifies their semantic distance, which is calculated as 1 \u2212 cosine(F 1, F 2).", "labels": [], "entities": []}, {"text": "Because we aim fora network creation method that is cognitively plausible in performing a limited number of word-to-word comparisons, we need to ensure that all the different methods of selecting the comparison set S(w) yield roughly similar numbers of such comparisons.", "labels": [], "entities": []}, {"text": "Keeping the size of S constant does not guarantee this, because each method can yield differing numbers of connections of the target word w to other words.", "labels": [], "entities": []}, {"text": "We thus parameterize the size of S for each method to keep the number of computations similar, based on experiments on the development data.", "labels": [], "entities": []}, {"text": "In development work we also found that having an increasing size of S overtime improved the results, as more words were compared as the knowledge of learned meanings improved.", "labels": [], "entities": []}, {"text": "To achieve this, we use a percentage of the words in the network as the size of S.", "labels": [], "entities": []}, {"text": "In practice, the setting of this parameter yields a number of comparisons across all methods that is about 8% of the maximum possible word-to-word comparisons that would be performed in the naive (computationally intensive) approach.", "labels": [], "entities": []}, {"text": "Note that all the Cluster-based, Random and Random+Context methods include a random selection mechanism; thus, we run each of these methods 50 times and report the average \u03c1, median ranks and size lcc (see Section 4).", "labels": [], "entities": []}, {"text": "For the networks (out of 50 runs) that exhibit a small-world structure (small-worldness greater than one), we report the average small-worldness.", "labels": [], "entities": []}, {"text": "We also report the percentage of runs whose resulting network exhibit a small-world structure.", "labels": [], "entities": []}, {"text": "Recall that the Upper-bound network is formed from examining a word's similarity to all other (observed) words when it is added to the network.", "labels": [], "entities": []}, {"text": "We can see that this network is highly connected (0.85) and has a small-world structure (5.5).", "labels": [], "entities": []}, {"text": "There is a statistically significant correlation of the network's similarity measures with the gold standard ones (\u22120.38).", "labels": [], "entities": []}, {"text": "For this Upper-bound structure, the median ranks of the first three associates are between 31 and 42.", "labels": [], "entities": []}, {"text": "These latter two measures on the Upper-bound network give an indication of the difficulty of learning a semantic network whose knowledge matches gold-standard similarities.", "labels": [], "entities": []}, {"text": "Considering the baseline networks, we note that the Random network is actually somewhat better (in connectivity and median ranks) than the Context network that we thought would provide a more informed baseline.", "labels": [], "entities": []}, {"text": "Interestingly, the correlation value for both networks is no worse than for the Upper-bound.", "labels": [], "entities": [{"text": "correlation", "start_pos": 19, "end_pos": 30, "type": "METRIC", "confidence": 0.9923409223556519}]}, {"text": "The combination of Random+Context yields a slightly lower correlation, and no better ranks or connectivity than Random.", "labels": [], "entities": [{"text": "correlation", "start_pos": 58, "end_pos": 69, "type": "METRIC", "confidence": 0.9758949875831604}]}, {"text": "Note that none of the baseline networks exhibit a small world structure (\u03c3 g 1 for all three, except for one out of 50 runs for the Random method).", "labels": [], "entities": []}, {"text": "Recall that the Random network is not a network resulting from randomly connecting word pairs, but one that incrementally compares each target word with a set of randomly chosen words when considering possible new connections.", "labels": [], "entities": []}, {"text": "We suspect that this approach performs reasonably well because it enables the model to find abroad All the reported co-efficients of correlation (\u03c1) are statistically significant at p < 0.01.", "labels": [], "entities": [{"text": "correlation (\u03c1)", "start_pos": 133, "end_pos": 148, "type": "METRIC", "confidence": 0.9595651924610138}]}, {"text": "range of similar words to the target; this might be effective especially because the learned meanings of words are changing overtime.", "labels": [], "entities": []}, {"text": "Turning to the Cluster-based methods, we see that indeed some diversity in the comparison set fora target word might be necessary to good performance.", "labels": [], "entities": []}, {"text": "We find that the measures on the Clusters-only network are roughly the same as on the Random one, but when we combine the two in Clusters+Random we see an improvement in the ranks achieved.", "labels": [], "entities": [{"text": "Clusters-only network", "start_pos": 33, "end_pos": 54, "type": "DATASET", "confidence": 0.8280607461929321}]}, {"text": "It is possible that the selection from clusters does not have sufficient diversity to find some of the valid new connections fora word.", "labels": [], "entities": []}, {"text": "We note that the best results overall occur with the Clusters+Context network, which combines two approaches to selecting words that have good potential to be similar to the target word.", "labels": [], "entities": []}, {"text": "The correlation coefficient for this network is at a respectable 0.36, and the median ranks are the second best of all the network-growth methods.", "labels": [], "entities": [{"text": "correlation", "start_pos": 4, "end_pos": 15, "type": "METRIC", "confidence": 0.9790142178535461}]}, {"text": "Importantly, this network shows the desired smallworld structure inmost of the runs (77%), with the highest connectivity and a small-world measure well over 1.", "labels": [], "entities": []}, {"text": "The fact that the Clusters+Context network is better overall than the networks of the Clustersonly and Context methods indicates that both clusters and context are important in making \"informed guesses\" about which words are likely to be similar to a target word.", "labels": [], "entities": []}, {"text": "Given the small number of similarity comparisons used in our experiments (only around 8% of all possible wordto-word comparisons), these observations suggest that both the linguistic context and the evolving relations among word usages (captured by the incremental clustering of learned meanings) contain information crucial to the process of growing a semantic network in a cognitively plausible way.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Connectivity and small-worldness measures for the Upper-bound, Baseline, and Cluster-based  network-growth methods; best performances across the Baseline and Cluster-based methods are shown  in bold. \u03c1: co-efficient of correlation between similarities of word pairs in network and in gold-standard;  1 st , 2 nd , 3 rd : median ranks of corresponding gold-standard associates given network similarities; size lcc :  proportion of network in the largest connected component; \u03c3 g : overall \"small-worldness\", should be  greater than 1; %: the percentage of runs whose resulting networks exhibit a small-world structure. Note  there are 1074 nouns in each network.", "labels": [], "entities": []}]}