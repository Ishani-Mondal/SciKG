{"title": [{"text": "Evaluating Neural Word Representations in Tensor-Based Compositional Settings", "labels": [], "entities": [{"text": "Evaluating Neural Word Representations in Tensor-Based Compositional Settings", "start_pos": 0, "end_pos": 77, "type": "TASK", "confidence": 0.6779465302824974}]}], "abstractContent": [{"text": "We provide a comparative study between neural word representations and traditional vector spaces based on co-occurrence counts, in a number of com-positional tasks.", "labels": [], "entities": []}, {"text": "We use three different semantic spaces and implement seven tensor-based compositional models, which we then test (together with simpler additive and multiplicative approaches) in tasks involving verb disambiguation and sentence similarity.", "labels": [], "entities": [{"text": "verb disambiguation", "start_pos": 195, "end_pos": 214, "type": "TASK", "confidence": 0.716156616806984}]}, {"text": "To check their scala-bility, we additionally evaluate the spaces using simple compositional methods on larger-scale tasks with less constrained language: paraphrase detection and dialogue act tagging.", "labels": [], "entities": [{"text": "paraphrase detection", "start_pos": 154, "end_pos": 174, "type": "TASK", "confidence": 0.8610664904117584}, {"text": "dialogue act tagging", "start_pos": 179, "end_pos": 199, "type": "TASK", "confidence": 0.6513111889362335}]}, {"text": "In the more constrained tasks, co-occurrence vectors are competitive, although choice of composi-tional method is important; on the larger-scale tasks, they are outperformed by neu-ral word embeddings, which show robust, stable performance across the tasks.", "labels": [], "entities": []}], "introductionContent": [{"text": "Neural word embeddings () have received much attention in the distributional semantics community, and have shown state-of-the-art performance in many natural language processing tasks.", "labels": [], "entities": [{"text": "natural language processing tasks", "start_pos": 150, "end_pos": 183, "type": "TASK", "confidence": 0.7024761363863945}]}, {"text": "While they have been compared with co-occurrence based models in simple similarity tasks at the word level (), we are aware of only one work that attempts a comparison of the two approaches in compositional settings, and this is limited to additive and multiplicative composition, compared against composition via a neural autoencoder.", "labels": [], "entities": []}, {"text": "The purpose of this paper is to provide a more complete picture regarding the potential of neural word embeddings in compositional tasks, and meaningfully compare them with the traditional distributional approach based on co-occurrence counts.", "labels": [], "entities": []}, {"text": "We are especially interested in investigating the performance of neural word vectors in compositional models involving general mathematical composition operators, rather than in the more task-or domain-specific deep-learning compositional settings they have generally been used with so far (for example, by, and many others).", "labels": [], "entities": []}, {"text": "In particular, this is the first large-scale study to date that applies neural word representations in tensor-based compositional distributional models of meaning similar to those formalized by.", "labels": [], "entities": []}, {"text": "We test a range of implementations based on this framework, together with additive and multiplicative approaches), in a variety of different tasks.", "labels": [], "entities": []}, {"text": "Specifically, we use the verb disambiguation task of and the transitive sentence similarity task of as small-scale focused experiments on pre-defined sentence structures.", "labels": [], "entities": [{"text": "verb disambiguation", "start_pos": 25, "end_pos": 44, "type": "TASK", "confidence": 0.7305123955011368}]}, {"text": "Additionally, we evaluate our vector spaces on paraphrase detection (using the Microsoft Research Paraphrase Corpus of) and dialogue act tagging using the Switchboard Corpus (see e.g. ().", "labels": [], "entities": [{"text": "paraphrase detection", "start_pos": 47, "end_pos": 67, "type": "TASK", "confidence": 0.8875419795513153}, {"text": "Microsoft Research Paraphrase Corpus", "start_pos": 79, "end_pos": 115, "type": "DATASET", "confidence": 0.8549539595842361}, {"text": "dialogue act tagging", "start_pos": 124, "end_pos": 144, "type": "TASK", "confidence": 0.6823758085568746}]}, {"text": "In all of the above tasks, we compare the neural word embeddings of with two vector spaces both based on co-occurrence counts and produced by standard distributional techniques, as described in detail below.", "labels": [], "entities": []}, {"text": "The general picture we get from the results is that in almost all cases the neural vectors are more effective than the traditional approaches.", "labels": [], "entities": []}, {"text": "We proceed as follows: Section 2 provides a concise introduction to distributional word representations in natural language processing.", "labels": [], "entities": [{"text": "distributional word representations", "start_pos": 68, "end_pos": 103, "type": "TASK", "confidence": 0.6254286468029022}]}, {"text": "Section 3 takes a closer look to the subject of compositionality in vector space models of meaning and describes the range of compositional operators examined here.", "labels": [], "entities": []}, {"text": "In Section we provide details about the vector spaces used in the experiments.", "labels": [], "entities": []}, {"text": "Our experimental work is described in detail in Section 5, and the results are discussed in Section 6.", "labels": [], "entities": []}, {"text": "Finally, Section 7 provides conclusions.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our experiments explore the use of the vector spaces above, together with the compositional operators described in Section 3, in a range of tasks all of which require semantic composition: verb sense disambiguation; sentence similarity; paraphrasing; and dialogue act tagging.", "labels": [], "entities": [{"text": "verb sense disambiguation", "start_pos": 189, "end_pos": 214, "type": "TASK", "confidence": 0.6409971714019775}, {"text": "sentence similarity", "start_pos": 216, "end_pos": 235, "type": "TASK", "confidence": 0.6923495084047318}, {"text": "dialogue act tagging", "start_pos": 255, "end_pos": 275, "type": "TASK", "confidence": 0.631663461526235}]}], "tableCaptions": [{"text": " Table 1: Word co-occurrence frequencies ex- tracted from the BNC (", "labels": [], "entities": [{"text": "BNC", "start_pos": 62, "end_pos": 65, "type": "DATASET", "confidence": 0.8784050345420837}]}, {"text": " Table 3: Spearman \u03c1 correlations of models with  human judgements for the word sense disam- biguation task. The best result (NWE Copy ob- ject) outperforms the nearest co-occurrence-based  competitor (KS14 Frobenius outer) with a statisti- cally significant difference (p < 0.05, t-test).", "labels": [], "entities": [{"text": "word sense disam- biguation task", "start_pos": 75, "end_pos": 107, "type": "TASK", "confidence": 0.7550181696812311}, {"text": "NWE Copy ob- ject", "start_pos": 126, "end_pos": 143, "type": "METRIC", "confidence": 0.7475830733776092}]}, {"text": " Table 4: Results for sentence similarity. There  is no statistically significant difference between  KS14 addition and NWE addition (the second best  result).", "labels": [], "entities": [{"text": "sentence similarity", "start_pos": 22, "end_pos": 41, "type": "TASK", "confidence": 0.6991120874881744}]}, {"text": " Table 5: Results for paraphrase detection (MSR) and dialog act tagging (SWDA) tasks. All top results  significantly outperform corresponding nearest competitors (for accuracy): p < 0.05, \u03c7 2 test.", "labels": [], "entities": [{"text": "paraphrase detection (MSR)", "start_pos": 22, "end_pos": 48, "type": "TASK", "confidence": 0.8849721789360047}, {"text": "dialog act tagging (SWDA) tasks", "start_pos": 53, "end_pos": 84, "type": "TASK", "confidence": 0.7623588825975146}, {"text": "accuracy", "start_pos": 167, "end_pos": 175, "type": "METRIC", "confidence": 0.998934805393219}]}]}