{"title": [{"text": "A Joint Segmentation and Classification Framework for Sentiment Analysis", "labels": [], "entities": [{"text": "Sentiment Analysis", "start_pos": 54, "end_pos": 72, "type": "TASK", "confidence": 0.976319432258606}]}], "abstractContent": [{"text": "In this paper, we propose a joint segmenta-tion and classification framework for sentiment analysis.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 81, "end_pos": 99, "type": "TASK", "confidence": 0.9622423946857452}]}, {"text": "Existing sentiment classification algorithms typically split a sentence as a word sequence, which does not effectively handle the inconsistent sentiment polarity between a phrase and the words it contains, such as \"not bad\" and \"a great deal of \".", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 9, "end_pos": 33, "type": "TASK", "confidence": 0.8484228849411011}]}, {"text": "We address this issue by developing a joint segmentation and classification framework (JSC), which simultaneously conducts sentence segmen-tation and sentence-level sentiment classification.", "labels": [], "entities": [{"text": "segmentation and classification framework (JSC)", "start_pos": 44, "end_pos": 91, "type": "TASK", "confidence": 0.6795816378934043}, {"text": "sentence-level sentiment classification", "start_pos": 150, "end_pos": 189, "type": "TASK", "confidence": 0.71821328997612}]}, {"text": "Specifically, we use a log-linear model to score each segmentation candidate , and exploit the phrasal information of top-ranked segmentations as features to build the sentiment classifier.", "labels": [], "entities": []}, {"text": "A marginal log-likelihood objective function is devised for the segmentation model, which is optimized for enhancing the sentiment classification performance.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 121, "end_pos": 145, "type": "TASK", "confidence": 0.9183721840381622}]}, {"text": "The joint model is trained only based on the annotated sentiment polarity of sentences, without any segmentation annotations.", "labels": [], "entities": []}, {"text": "Experiments on a benchmark Twitter sentiment classification dataset in SemEval 2013 show that, our joint model performs comparably with the state-of-the-art methods.", "labels": [], "entities": [{"text": "Twitter sentiment classification dataset in SemEval 2013", "start_pos": 27, "end_pos": 83, "type": "DATASET", "confidence": 0.7112878561019897}]}], "introductionContent": [{"text": "Sentiment classification, which classifies the sentiment polarity of a sentence (or document) as positive or negative, is a major research direction in the field of sentiment analysis.", "labels": [], "entities": [{"text": "Sentiment classification", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.9271571636199951}, {"text": "sentiment analysis", "start_pos": 165, "end_pos": 183, "type": "TASK", "confidence": 0.9132566750049591}]}, {"text": "Majority of existing approaches follow and treat sen-timent classification as a special case of text categorization task.", "labels": [], "entities": [{"text": "sen-timent classification", "start_pos": 49, "end_pos": 74, "type": "TASK", "confidence": 0.8495867252349854}]}, {"text": "Under this perspective, previous studies typically use pipelined methods with two steps.", "labels": [], "entities": []}, {"text": "They first produce sentence segmentations with separate text analyzers ( or bag-of-words ().", "labels": [], "entities": [{"text": "sentence segmentations", "start_pos": 19, "end_pos": 41, "type": "TASK", "confidence": 0.7484515905380249}]}, {"text": "Then, feature learning and sentiment classification algorithms take the segmentation results as inputs to build the sentiment classifier).", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 27, "end_pos": 51, "type": "TASK", "confidence": 0.9034858345985413}]}, {"text": "The major disadvantage of a pipelined method is the problem of error propagation, since sentence segmentation errors cannot be corrected by the sentiment classification model.", "labels": [], "entities": [{"text": "error propagation", "start_pos": 63, "end_pos": 80, "type": "TASK", "confidence": 0.7016389518976212}, {"text": "sentence segmentation", "start_pos": 88, "end_pos": 109, "type": "TASK", "confidence": 0.7160306423902512}, {"text": "sentiment classification", "start_pos": 144, "end_pos": 168, "type": "TASK", "confidence": 0.8219645023345947}]}, {"text": "A typical kind of error is caused by the polarity inconsistency between a phrase and the words it contains, such as not bad, bad and a great deal of, great.", "labels": [], "entities": []}, {"text": "The segmentations based on bag-of-words or syntactic chunkers are not effective enough to handle the polarity inconsistency phenomenons.", "labels": [], "entities": []}, {"text": "The reason lies in that bag-of-words segmentations regard each word as a separate unit, which losses the word order and does not capture the phrasal information.", "labels": [], "entities": []}, {"text": "The segmentations based on syntactic chunkers typically aim to identify noun groups, verb groups or named entities from a sentence.", "labels": [], "entities": []}, {"text": "However, many sentiment indicators are phrases constituted of adjectives, negations, adverbs or idioms (), which are splitted by syntactic chunkers.", "labels": [], "entities": []}, {"text": "Besides, a better approach would be to utilize the sentiment information to improve the segmentor.", "labels": [], "entities": []}, {"text": "Accordingly, the sentiment-specific segmentor will enhance the performance of sentiment classification in turn.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 78, "end_pos": 102, "type": "TASK", "confidence": 0.8411515653133392}]}, {"text": "In this paper, we propose a joint segmentation and classification framework (JSC) for sentiment analysis, which simultaneous conducts sentence segmentation and sentence-level sentiment classification.", "labels": [], "entities": [{"text": "segmentation and classification framework (JSC)", "start_pos": 34, "end_pos": 81, "type": "TASK", "confidence": 0.6425501235893795}, {"text": "sentiment analysis", "start_pos": 86, "end_pos": 104, "type": "TASK", "confidence": 0.9601723551750183}, {"text": "sentence segmentation", "start_pos": 134, "end_pos": 155, "type": "TASK", "confidence": 0.7210194915533066}, {"text": "sentence-level sentiment classification", "start_pos": 160, "end_pos": 199, "type": "TASK", "confidence": 0.683302770058314}]}, {"text": "The framework is illustrated in The joint segmentation and classification framework (JSC) for sentiment classification.", "labels": [], "entities": [{"text": "joint segmentation and classification framework (JSC)", "start_pos": 36, "end_pos": 89, "type": "TASK", "confidence": 0.7349183708429337}, {"text": "sentiment classification", "start_pos": 94, "end_pos": 118, "type": "TASK", "confidence": 0.8970140814781189}]}, {"text": "CG represents the candidate generation model, SC means the sentiment classification model and SEG stands for the segmentation ranking model.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 59, "end_pos": 83, "type": "TASK", "confidence": 0.8864947259426117}, {"text": "SEG", "start_pos": 94, "end_pos": 97, "type": "METRIC", "confidence": 0.9975086450576782}]}, {"text": "Down Arrow means the use of a specified model, and Up Arrow indicates the update of a model.", "labels": [], "entities": [{"text": "Up Arrow", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.8925742506980896}]}, {"text": "We develop (1) a candidate generation model to generate the segmentation candidates of a sentence, (2) a segmentation ranking model to score each segmentation candidate of a given sentence, and (3) a classification model to predict the sentiment polarity of each segmentation.", "labels": [], "entities": []}, {"text": "The phrasal information of top-ranked candidates from the segmentation model are utilized as features to build the sentiment classifier.", "labels": [], "entities": []}, {"text": "In turn, the predicted sentiment polarity of segmentation candidates from classification model are leveraged to update the segmentor.", "labels": [], "entities": []}, {"text": "We score each segmentation candidate with a log-linear model, and optimize the segmentor with a marginal log-likelihood objective.", "labels": [], "entities": []}, {"text": "We train the joint model from sentences annotated only with sentiment polarity, without any segmentation annotations.", "labels": [], "entities": []}, {"text": "We evaluate the effectiveness of our joint model on a benchmark Twitter sentiment classification dataset in SemEval 2013.", "labels": [], "entities": [{"text": "Twitter sentiment classification dataset in SemEval 2013", "start_pos": 64, "end_pos": 120, "type": "DATASET", "confidence": 0.7441437372139522}]}, {"text": "Results show that the joint model performs comparably with stateof-the-art methods, and consistently outperforms pipeline methods in various experiment settings.", "labels": [], "entities": []}, {"text": "The main contributions of the work presented in this paper are as follows.", "labels": [], "entities": []}, {"text": "\u2022 To our knowledge, this is the first work that automatically produces sentence segmentation for sentiment classification within a joint framework.", "labels": [], "entities": [{"text": "sentence segmentation", "start_pos": 71, "end_pos": 92, "type": "TASK", "confidence": 0.7404994368553162}, {"text": "sentiment classification", "start_pos": 97, "end_pos": 121, "type": "TASK", "confidence": 0.9432220160961151}]}, {"text": "\u2022 We show that the joint model yields comparable performance with the state-of-the-art methods on the benchmark Twitter sentiment classification datasets in SemEval 2013.", "labels": [], "entities": [{"text": "Twitter sentiment classification datasets in SemEval 2013", "start_pos": 112, "end_pos": 169, "type": "DATASET", "confidence": 0.7424932505403247}]}], "datasetContent": [{"text": "In this section, we conduct experiments to evaluate the effectiveness of the joint model.", "labels": [], "entities": []}, {"text": "We describe the experiment settings and the result analysis.", "labels": [], "entities": []}, {"text": "We conduct sentiment classification of tweets on a benchmark Twitter sentiment classification dataset in SemEval 2013.", "labels": [], "entities": [{"text": "sentiment classification of tweets", "start_pos": 11, "end_pos": 45, "type": "TASK", "confidence": 0.9303867518901825}, {"text": "benchmark Twitter sentiment classification dataset in SemEval 2013", "start_pos": 51, "end_pos": 117, "type": "TASK", "confidence": 0.780462734401226}]}, {"text": "We run 2-class (positive vs negative) classification as sentence segmentation has a great influence on the positive/negative polarity of tweets due to the polarity inconsistency between a phrase and its constitutes, such as not bad, bad.", "labels": [], "entities": [{"text": "sentence segmentation", "start_pos": 56, "end_pos": 77, "type": "TASK", "confidence": 0.7380077540874481}]}, {"text": "We leave 3-class classification (positive, negative, neutral) and fine-grained classification (very negative, negative, neutral, positive, very positive) in the future work.", "labels": [], "entities": []}, {"text": "Positive: Statistics of the SemEval 2013 Twitter sentiment classification dataset (positive vs negative).", "labels": [], "entities": [{"text": "SemEval 2013 Twitter sentiment classification dataset", "start_pos": 28, "end_pos": 81, "type": "DATASET", "confidence": 0.7703479826450348}]}, {"text": "The statistics of our dataset crawled from SemEval 2013 are given in.", "labels": [], "entities": [{"text": "SemEval 2013", "start_pos": 43, "end_pos": 55, "type": "DATASET", "confidence": 0.7578076422214508}]}, {"text": "The evaluation metric is the macro-F1 of sentiment classification.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 41, "end_pos": 65, "type": "TASK", "confidence": 0.8895590901374817}]}, {"text": "We train the joint model on the training set, tune parameters on the dev set and evaluate on the test set.", "labels": [], "entities": []}, {"text": "We train the sentiment classifier with) and utilize existing sentiment lexicons 3 to extract classificationspecific features.", "labels": [], "entities": []}, {"text": "We randomly crawl 100M tweets from February 1st, 2013 to April 30th, 2013 with Twitter API, and use them to learn the phrase embedding with Skip-Gram 4 . The vocabulary size of the phrase embedding is 926K, from unigram to 5-gram.", "labels": [], "entities": []}, {"text": "The parameter -c in SVM is tuned on the dev-set in both baseline and our method.", "labels": [], "entities": []}, {"text": "We run the L-BFGS for 50 iterations, and set the regularization factor \u03bb as 0.003.", "labels": [], "entities": []}, {"text": "The beam size N of the candidate generation model and the top-ranked segmentation number K are tuned on the dev-set.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 5: Statistics of the SemEval 2013 Twitter  sentiment classification dataset (positive vs nega- tive).", "labels": [], "entities": [{"text": "SemEval 2013 Twitter  sentiment classification dataset", "start_pos": 28, "end_pos": 82, "type": "DATASET", "confidence": 0.7650717993577322}]}, {"text": " Table 5. The evalua- tion metric is the macro-F1 of sentiment classifi- cation. We train the joint model on the training  set, tune parameters on the dev set and evaluate  on the test set. We train the sentiment classifier  with", "labels": [], "entities": []}, {"text": " Table 6: Macro-F1 for positive vs negative classi- fication of tweets.", "labels": [], "entities": []}]}