{"title": [{"text": "ReferItGame: Referring to Objects in Photographs of Natural Scenes", "labels": [], "entities": [{"text": "Referring to Objects in Photographs of Natural Scenes", "start_pos": 13, "end_pos": 66, "type": "TASK", "confidence": 0.845860093832016}]}], "abstractContent": [{"text": "In this paper we introduce anew game to crowd-source natural language referring expressions.", "labels": [], "entities": [{"text": "crowd-source natural language referring expressions", "start_pos": 40, "end_pos": 91, "type": "TASK", "confidence": 0.6710421621799469}]}, {"text": "By designing a two player game, we can both collect and verify referring expressions directly within the game.", "labels": [], "entities": []}, {"text": "To date, the game has produced a dataset containing 130,525 expressions, referring to 96,654 distinct objects, in 19,894 photographs of natural scenes.", "labels": [], "entities": []}, {"text": "This dataset is larger and more varied than previous REG datasets and allows us to study referring expressions in real-world scenes.", "labels": [], "entities": [{"text": "REG datasets", "start_pos": 53, "end_pos": 65, "type": "DATASET", "confidence": 0.7880253195762634}]}, {"text": "We provide an in depth analysis of the resulting dataset.", "labels": [], "entities": []}, {"text": "Based on our findings, we design anew optimization based model for generating referring expressions and perform experimental evaluations on 3 test sets.", "labels": [], "entities": []}], "introductionContent": [{"text": "Much of everyday language and discourse concerns the visual world around us, making understanding the relationship between objects in the physical world and language describing those objects an important challenge problem for AI.", "labels": [], "entities": []}, {"text": "From robotics, to image search, to situated language learning, and natural language grounding, there area number of research areas that would benefit from a better understanding of how people refer to physical entities in the world.", "labels": [], "entities": [{"text": "image search", "start_pos": 18, "end_pos": 30, "type": "TASK", "confidence": 0.7669358551502228}, {"text": "situated language learning", "start_pos": 35, "end_pos": 61, "type": "TASK", "confidence": 0.6156269212563833}, {"text": "natural language grounding", "start_pos": 67, "end_pos": 93, "type": "TASK", "confidence": 0.7175950606664022}]}, {"text": "Recent advances in automatic computer vision methods have started to make technologies for recognizing thousands of object categories a near reality (.", "labels": [], "entities": []}, {"text": "As a result, there has been a spurt of recent work trying to estimate higher level semantics, including exciting efforts to automatically produce natural language descriptions of images and video (Farhadi et * Indicates equal author contribution.", "labels": [], "entities": []}, {"text": "Common challenges encountered in these pursuits include the fact that descriptions can be highly task dependent, openended, and difficult to evaluate automatically.", "labels": [], "entities": []}, {"text": "Therefore, we look at the related, but more focused problem of referring expression generation (REG).", "labels": [], "entities": [{"text": "referring expression generation (REG)", "start_pos": 63, "end_pos": 100, "type": "TASK", "confidence": 0.8379505972067515}]}, {"text": "Previous work on REG has made significant progress toward understanding how people generate expressions to refer to objects (a recent survey of techniques is provided in ).", "labels": [], "entities": [{"text": "REG", "start_pos": 17, "end_pos": 20, "type": "TASK", "confidence": 0.9682552218437195}]}, {"text": "In this paper, we study the relatively unexplored setting of how people refer to objects in complex photographs of real-world cluttered scenes.", "labels": [], "entities": []}, {"text": "One initial stumbling block to examining this scenario is lack of existing relevant datasets, as previous collections for studying REG have used relatively focused domains such as graphics generated objects, crafts (, or small everyday (home and office) objects arrayed on a simple background (.", "labels": [], "entities": [{"text": "REG", "start_pos": 131, "end_pos": 134, "type": "TASK", "confidence": 0.9355030059814453}]}, {"text": "In this paper, we collect anew large-scale corpus, currently containing 130,525 expressions, referring to 96,654 distinct objects, in 19,894 photographs of real world scenes.", "labels": [], "entities": []}, {"text": "Some examples from our dataset are shown in.", "labels": [], "entities": []}, {"text": "To construct this corpus efficiently, we design anew two player referring expression game (ReferItGame) to crowd-source the data collection.", "labels": [], "entities": []}, {"text": "Popularized by efforts like the ESP game (von) and Peekaboom), Human Computation based games can bean effective way to engage users and collect large amounts of data inexpensively.", "labels": [], "entities": []}, {"text": "Two player games can also automate verification of human provided annotations.", "labels": [], "entities": []}, {"text": "Our resulting corpus is both more real-world and much bigger than previous datasets, allowing us to examine referring expression generation in anew setting at large scale.", "labels": [], "entities": [{"text": "referring expression generation", "start_pos": 108, "end_pos": 139, "type": "TASK", "confidence": 0.7091579437255859}]}, {"text": "To understand and quantify this new dataset, we perform an extensive set of analyses.", "labels": [], "entities": []}, {"text": "One significant difference from previous work is that we study how referring expressions vary for different categories.", "labels": [], "entities": []}, {"text": "We find that an object's category greatly influences the types of attributes used in their referring expression (e.g. people use color words to describe cars more often than mountains).", "labels": [], "entities": []}, {"text": "Additionally, we find that references to an object are sometimes made with respect to other nearby objects, e.g. \"the ball to left of the man\".", "labels": [], "entities": []}, {"text": "Interestingly, the types of reference objects (i.e. \"the man\") used in referring expressions is also biased toward some categories.", "labels": [], "entities": []}, {"text": "Finally, we find that the word used to refer to the object category itself displays consistencies across people.", "labels": [], "entities": []}, {"text": "This notion is related to ideas of entrylevel categories from Psychology.", "labels": [], "entities": []}, {"text": "Given these findings, we propose an optimization model for generating referring expressions that jointly selects which attributes to include in the expression, and what attribute values to generate.", "labels": [], "entities": []}, {"text": "This model incorporates both visual models for selecting attribute-values and object category specific priors.", "labels": [], "entities": []}, {"text": "Experimental evaluations indicate that our proposed model produces reasonable results for REG.", "labels": [], "entities": [{"text": "REG", "start_pos": 90, "end_pos": 93, "type": "TASK", "confidence": 0.9354878067970276}]}, {"text": "In summary, contributions of our paper include: \u2022 A two player online game to collect and verify natural language referring expressions.", "labels": [], "entities": []}, {"text": "\u2022 A new large-scale dataset containing natural language expressions referring to objects in photographs of real world scenes.", "labels": [], "entities": []}, {"text": "\u2022 Analyses of the collected dataset, including studying category-specific variations in referring expressions.", "labels": [], "entities": []}, {"text": "\u2022 An optimization based model to generate referring expressions for objects in realworld scenes with experimental evaluations on three labeled test sets.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "First we outline related work from the vision and language communities ( \u00a72).", "labels": [], "entities": []}, {"text": "Then we describe our online game for collecting referring expressions ( \u00a73) and provide an analysis of our new ReferItGame Dataset ( \u00a74).", "labels": [], "entities": [{"text": "ReferItGame Dataset", "start_pos": 111, "end_pos": 130, "type": "DATASET", "confidence": 0.8266383707523346}]}, {"text": "Finally, we present and evaluate our model for generating referring expressions ( \u00a75) and discuss conclusions and future work ( \u00a76).", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we describe the ReferItGame dataset \u2020 , including images and labels, processing the dataset, and analysis of the collection.", "labels": [], "entities": [{"text": "ReferItGame dataset \u2020", "start_pos": 32, "end_pos": 53, "type": "DATASET", "confidence": 0.8258089820543925}]}, {"text": "From the ImageCLEF dataset, we created a total of over 100k distinct games (one per object labeled in the dataset).", "labels": [], "entities": [{"text": "ImageCLEF dataset", "start_pos": 9, "end_pos": 26, "type": "DATASET", "confidence": 0.9692564606666565}]}, {"text": "For the games we imposed an ordering to allow for collecting the most interesting expressions first.", "labels": [], "entities": []}, {"text": "Initially we prioritized games for objects in images with multiple objects of the same category.", "labels": [], "entities": []}, {"text": "Once these games were completed, we prioritized ordering based on object category to include a comprehensive range of objects.", "labels": [], "entities": []}, {"text": "Finally, after successfully collecting referring expressions from the prioritized games, we posted games for the remaining objects.", "labels": [], "entities": []}, {"text": "In order to evaluate consistency of expression generation across people, we also include a probability of repeating previously played games during collection.", "labels": [], "entities": []}, {"text": "To date, we have collected 130,525 successfully completed games.", "labels": [], "entities": []}, {"text": "This includes 10,431 canned games (a person playing against the computer, not including the initial seed set) and 120,094 real games (two people playing).", "labels": [], "entities": []}, {"text": "96,654 distinct objects from 19,984 photographs are represented in the dataset.", "labels": [], "entities": []}, {"text": "This covers almost all of the objects present in the IAPR corpus.", "labels": [], "entities": [{"text": "IAPR corpus", "start_pos": 53, "end_pos": 64, "type": "DATASET", "confidence": 0.8675161898136139}]}, {"text": "The remaining objects from the collection were either too small or too ambiguous to result in successful games.", "labels": [], "entities": []}, {"text": "For data collection, we posted the game online for anyone on the web to play and encouraged participation through social media and the survey section of reddit.", "labels": [], "entities": [{"text": "data collection", "start_pos": 4, "end_pos": 19, "type": "TASK", "confidence": 0.7411780059337616}]}, {"text": "In this manner we collected over 4 thousand referring expressions over a period of 3 weeks.", "labels": [], "entities": []}, {"text": "To speedup data collection, we also posted the game on Mechanical Turk.", "labels": [], "entities": [{"text": "Mechanical Turk", "start_pos": 55, "end_pos": 70, "type": "DATASET", "confidence": 0.9362574517726898}]}, {"text": "Turkers were paid upon completion of 10 correct games (games where Player 2 clicks on the correct object of interest).", "labels": [], "entities": [{"text": "Turkers", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.6917759776115417}]}, {"text": "Turkers were pre-screened to have approval ratings above 80% and to be located in the US for language consistency.", "labels": [], "entities": []}, {"text": "Because of the size of the dataset, hand annotation of all referring expressions is prohibitive.", "labels": [], "entities": []}, {"text": "Therefore, similar to past work (, we design an automatic method to pre-process the expressions and extract object and attribute mentions.", "labels": [], "entities": []}, {"text": "These automatically processed expressions are used only for analysis and model training.", "labels": [], "entities": []}, {"text": "We also fully hand label portions of the dataset for evaluation ( \u00a75.2).", "labels": [], "entities": []}, {"text": "By examining the expressions in the collected dataset, we define a set of attributes with broad S ::= subject word color word ::= rel(S, color word) color word =color word | prep in(S, color word) color word =color word size word ::= rel(S, size word) size word =size word abs loc word ::= rel(S, abs loc word) abs loc word =abs loc word | prep on(S, orientation word) \u2227 \u00acprep of (S, ) abs loc word =on+orientation word rel loc word ::= RL RL ::= prep rel loc word(S, object word) RL=rel loc word | prep on(S, orientation word) \u2227 prep of (S, object word) RL=on orientation word | prep to(S, orientation word) \u2227 prep of (S, object word) RL=to orientation word | prep at(S, orientation word) \u2227 prep of (S, object word) RL=at orientation word generic word ::= amod(S, generic word) coverage of the attribute types used in the referring expressions.", "labels": [], "entities": []}, {"text": "We define the set of attributes fora referring expression as a 7-tuple R = {r 1 , r 2 , r 3 , r 4 , r 5 , r 6 , r 7 }: \u2022 r 1 is an entry-level category attribute, \u2022 r 2 is a color attribute, \u2022 r 3 is a size attribute, \u2022 r 4 is an absolute location attribute, \u2022 r 5 is a relative location relation attribute, \u2022 r 6 is a relative location object attribute, \u2022 r 7 is a generic attribute, Color and size attributes refer to the object color (e.g. \"blue\") and object size (e.g. \"tiny\") respectively.", "labels": [], "entities": []}, {"text": "Absolute location refers to the location of the object in the image (e.g. \"top of the image\").", "labels": [], "entities": []}, {"text": "Relative location relation and relative location object attributes allow for referring expressions that localize the object with respect to another object in the picture (e.g. \"the car to the left of the tree\").", "labels": [], "entities": []}, {"text": "Generic attributes coverall less frequently observed attribute types (e.g. \"wooden\" or \"round\").", "labels": [], "entities": []}, {"text": "The entry-level category attribute is related to the concept of entry-level categories first proposed by Psychologists in the 1970s and recently explored in visual recognition ().", "labels": [], "entities": [{"text": "visual recognition", "start_pos": 157, "end_pos": 175, "type": "TASK", "confidence": 0.7539721429347992}]}, {"text": "The idea of entry-level categories is that an object can belong to many different categories; an indigo bunting is an oscine, a bird, a vertebrate, a chordate, and soon.", "labels": [], "entities": []}, {"text": "But, a person looking at a picture of one would probably call it a bird (unless they are very familiar with ornithology).", "labels": [], "entities": []}, {"text": "Therefore, we include this attribute to capture how people name object categories in referring expressions.", "labels": [], "entities": []}, {"text": "Parsing the referring expressions: We parse the expressions using the most recent version of the StanfordCoreNLP parser (.", "labels": [], "entities": [{"text": "StanfordCoreNLP", "start_pos": 97, "end_pos": 112, "type": "DATASET", "confidence": 0.9523899555206299}]}, {"text": "We begin by traversing the parse tree in a breadth-first manner and selecting the head noun of the sentence to determine the object of the referring expression, denoted as subject word.", "labels": [], "entities": []}, {"text": "We pre-define a dictionary of attribute-values (color word, size word, abs location word, rel location word) for each of the attributes based on the observed data using a combination of POS-tagging and manual labeling.", "labels": [], "entities": []}, {"text": "We then apply a template-based approach on the collapsed dependency relations to recover the set of attributes (the main template rules are shown in).", "labels": [], "entities": []}, {"text": "The relationship rel indicates any linguistic binary relationship between the subject word Sand another word, including the amod relationship.", "labels": [], "entities": []}, {"text": "Orientation word captures the words like left, right, top and bottom.", "labels": [], "entities": []}, {"text": "For generic word we consider any modifier words other than those captured by our other attributes (color, size, location).", "labels": [], "entities": []}, {"text": "Using this template-based parser we can for instance parse the following expression: \"Red flower on top of pedestal\".", "labels": [], "entities": []}, {"text": "The first rule would match the prep(S, color word) relation, effectively recovering the attribute color word as \"red\".", "labels": [], "entities": []}, {"text": "The second rule would match the prep on(S, orientation word) \u2227 prep of (S, object word) relations, recovering rel loc word as \"on top of \" and object word as \"pedestal\".", "labels": [], "entities": []}, {"text": "The accuracy of our parser based processing is 91%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9997259974479675}, {"text": "parser based processing", "start_pos": 20, "end_pos": 43, "type": "TASK", "confidence": 0.8877470095952352}]}, {"text": "This was evaluated on 4,500 expressions   For example, this indicates that \"streets\" are often called \"road\", sometimes \"ground\", sometimes \"roadway\", etc.", "labels": [], "entities": []}, {"text": "Right: example objects predicted to portray some of our color attribute values.", "labels": [], "entities": []}, {"text": "Note sometimes our color predictor is quite accurate, and sometimes it makes mistakes (see the man in a red shirt predicted as \"yellow\"). that were manually parsed by a human annotator.", "labels": [], "entities": []}, {"text": "In the resulting dataset, we have a range of coverage over objects.", "labels": [], "entities": []}, {"text": "For 10,304 of the objects we have 2 or more referring expressions while for the rest of the objects we have collected only one expression.", "labels": [], "entities": []}, {"text": "This creates a dataset that emphasizes breadth while also containing enough data to study speaker variation.", "labels": [], "entities": []}, {"text": "Multiple attribute analyses are provided in.", "labels": [], "entities": []}, {"text": "We find that most expressions use 0, 1, or 2 attributes (in addition to the entry-level attribute object word), with very few expressions containing more than 2 attributes (frequencies are shown in.", "labels": [], "entities": []}, {"text": "We also examine what types of attributes are used most frequently, according to object category in and when associated with single or multiple occurrences of the same object category in an image in The frequency of attribute usage in images containing multiple objects of the same type increases for all types, compared to single object occurrences.", "labels": [], "entities": []}, {"text": "Perhaps more interestingly, the use of different attributes is highly category dependent.", "labels": [], "entities": []}, {"text": "People use more attribute words overall to describe some categories, like \"man\", \"woman\", or \"plant\", and the distribution of attribute types also varies by category.", "labels": [], "entities": []}, {"text": "For example, color attributes are used more frequently for categories like \"car\" or \"woman\" than for categories like \"sky\" or \"rock\".", "labels": [], "entities": []}, {"text": "We also examine which objects are most frequently used as points of reference, e.g.,\"the chair next to the man\" in We observe that people and some background categories like \"tree\" or \"wall\" are often used to help localize objects in Finally, we study entry-level category attributevalues to understand how people name objects in referring expressions.", "labels": [], "entities": []}, {"text": "Tag clouds indicating the frequencies of words used to name various object categories are provided in.", "labels": [], "entities": []}, {"text": "Objects like \"street\" are usually referred to as \"road\", but sometimes they are called \"ground\", \"roadway\", etc.", "labels": [], "entities": []}, {"text": "\"Bottles\" are usually called \"bottle\", but sometimes referred to as \"coke\" or \"beer\".", "labels": [], "entities": []}, {"text": "Interestingly, \"man\" is usually called \"man\" while \"woman\" is most often called \"person\" in the referring expressions.", "labels": [], "entities": []}, {"text": "We implement the proposed model using commercial binary integer linear programming software (IBM ILOG CPLEX).", "labels": [], "entities": [{"text": "IBM ILOG CPLEX)", "start_pos": 93, "end_pos": 108, "type": "DATASET", "confidence": 0.8208711296319962}]}, {"text": "This requires introducing a set of indicator variables for each of our multivalued attributes and another set of indicator variables to model pairwise interactions between our variables, as well as incorporating additional consistency constraints between variables.", "labels": [], "entities": []}, {"text": "Model parameters (\u03b1 and \u03b2) are tuned on data randomly sampled from the training set.", "labels": [], "entities": []}, {"text": "Test Sets: We evaluate our model on three test sets, each containing 500 objects.", "labels": [], "entities": []}, {"text": "For each object in the test sets we collect 3 referring expressions using the ReferItGame and manually label the attributes mentioned in each expression.", "labels": [], "entities": []}, {"text": "We find human agreement to be 72.31% on our dataset (where we measure agreement as mean matching accuracy of attribute values for pairs of users across images in our test sets).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 97, "end_pos": 105, "type": "METRIC", "confidence": 0.7907393574714661}]}, {"text": "The three test sets are created to evaluate different aspects of our data.", "labels": [], "entities": []}, {"text": "Test Set A contains objects sampled randomly from the entire dataset.", "labels": [], "entities": []}, {"text": "This test set is meant to closely resemble the full dataset distribution.", "labels": [], "entities": []}, {"text": "The goal of the other two test sets is to sample expressions for \"interesting\" objects.", "labels": [], "entities": []}, {"text": "We first identify categories that are mainly related to background content elements, e.g. \"sky, ground, floor, sand, sidewalk, etc\".", "labels": [], "entities": []}, {"text": "We consider these categories to be potentially less interesting for study than categories like people, animals, cars, etc.", "labels": [], "entities": []}, {"text": "Test Set B contains objects sampled from the most frequently occurring object categories in the dataset, selected to contain a balanced number of objects from each category, excluding the less interesting categories.", "labels": [], "entities": []}, {"text": "Test Set C contains objects sampled from images that contain at least 2 objects of the same category, excluding the less interesting categories.", "labels": [], "entities": []}, {"text": "Results: Qualitative examples are shown in our results to the human produced expressions.", "labels": [], "entities": []}, {"text": "For some images (left) we do quite well at predicting the correct attributes and values.", "labels": [], "entities": []}, {"text": "For others we do less well (right).", "labels": [], "entities": []}, {"text": "We also show example objects predicted for some color words in.", "labels": [], "entities": []}, {"text": "We see that our model can fail in several ways, such as generating the wrong attribute-value due to inaccurate predictions by visual models or selecting incorrect attributes to include in the generated expression.", "labels": [], "entities": []}, {"text": "Quantitative results: precision and recall measures for the 3 test sets are reported in, including evaluation of a baseline version of our model which incorporates only the prior potentials ( \u00a75.1.2) without any content based estimates.", "labels": [], "entities": [{"text": "Quantitative", "start_pos": 0, "end_pos": 12, "type": "METRIC", "confidence": 0.9551130533218384}, {"text": "precision", "start_pos": 22, "end_pos": 31, "type": "METRIC", "confidence": 0.9996293783187866}, {"text": "recall", "start_pos": 36, "end_pos": 42, "type": "METRIC", "confidence": 0.9993804693222046}]}, {"text": "We see that our model performs reasonably on both measures, and outperforms the baseline by a large margin on all test sets, with highest performance on the broadly sampled interesting category test set.", "labels": [], "entities": []}, {"text": "Note that our problem is somewhat different than traditional REG where the input is often attribute-value pairs and the task is to select which pairs to include in the expression.", "labels": [], "entities": []}, {"text": "Our goal is to jointly select which attributes to include and what values to predict from a list of all possible values for the attribute.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Baseline Model & Full Model perfor- mance on the three test sets (A,B,C).", "labels": [], "entities": []}]}