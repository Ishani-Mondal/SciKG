{"title": [{"text": "Aligning context-based statistical models of language with brain activity during reading", "labels": [], "entities": [{"text": "Aligning context-based", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8579989671707153}]}], "abstractContent": [{"text": "Many statistical models for natural language processing exist, including context-based neural networks that (1) model the previously seen context as a latent feature vector, (2) integrate successive words into the context using some learned representation (embedding), and (3) compute output probabilities for incoming words given the context.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 28, "end_pos": 55, "type": "TASK", "confidence": 0.6752818822860718}]}, {"text": "On the other hand, brain imaging studies have suggested that during reading, the brain (a) continuously builds a context from the successive words and every time it encounters a word it (b) fetches its properties from memory and (c) integrates it with the previous context with a degree of effort that is inversely proportional to how probable the word is.", "labels": [], "entities": []}, {"text": "This hints to a parallelism between the neural networks and the brain in modeling context (1 and a), representing the incoming words (2 and b) and integrating it (3 and c).", "labels": [], "entities": []}, {"text": "We explore this parallelism to better understand the brain processes and the neu-ral networks representations.", "labels": [], "entities": []}, {"text": "We study the alignment between the latent vectors used by neural networks and brain activity observed via Magnetoen-cephalography (MEG) when subjects read a story.", "labels": [], "entities": []}, {"text": "For that purpose we apply the neural network to the same text the subjects are reading, and explore the ability of these three vector representations to predict the observed word-byword brain activity.", "labels": [], "entities": []}, {"text": "Our novel results show that: before anew word i is read, brain activity is well predicted by the neural network latent representation of context and the predictability decreases as the brain integrates the word and changes its own representation of context.", "labels": [], "entities": []}, {"text": "Secondly , the neural network embedding of word i can predict the MEG activity when word i is presented to the subject, revealing that it is correlated with the brain's own representation of word i.", "labels": [], "entities": [{"text": "MEG activity", "start_pos": 66, "end_pos": 78, "type": "METRIC", "confidence": 0.9259125292301178}]}, {"text": "Moreover, we obtain that the activity is predicted in different regions of the brain with varying delay.", "labels": [], "entities": []}, {"text": "The delay is consistent with the placement of each region on the processing pathway that starts in the visual cortex and moves to higher level regions.", "labels": [], "entities": [{"text": "delay", "start_pos": 4, "end_pos": 9, "type": "METRIC", "confidence": 0.9534938335418701}]}, {"text": "Finally, we show that the output probability computed by the neural networks agrees with the brain's own assessment of the probability of word i, as it can be used to predict the brain activity after the word i's properties have been fetched from memory and the brain is in the process of integrating it into the context.", "labels": [], "entities": []}], "introductionContent": [{"text": "Natural language processing has recently seen a surge in increasingly complex models that achieve impressive goals.", "labels": [], "entities": [{"text": "Natural language processing", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.655537486076355}]}, {"text": "Models like deep neural networks and vector space models have become popular to solve diverse tasks like sentiment analysis and machine translation.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 105, "end_pos": 123, "type": "TASK", "confidence": 0.9608508348464966}, {"text": "machine translation", "start_pos": 128, "end_pos": 147, "type": "TASK", "confidence": 0.827580600976944}]}, {"text": "Because of the complexity of these models, it is not always clear how to assess and compare their performances as they might be useful for one task and not the other.", "labels": [], "entities": []}, {"text": "It is also not easy to interpret their very highdimensional and mostly unsupervised representations.", "labels": [], "entities": []}, {"text": "The brain is another computational system that processes language.", "labels": [], "entities": []}, {"text": "Since we can record brain activity using neuroimaging, we propose anew direction that promises to improve our understanding of both how the brain is processing language and of what the neural networks are modeling by aligning the brain data with the neural networks representations.", "labels": [], "entities": []}, {"text": "In this paper we study the representations of two kinds of neural networks that are built to predict the incoming word: recurrent and finite context models.", "labels": [], "entities": []}, {"text": "The first model is the Recurrent Neural Network Language) which uses the entire history of words to model context.", "labels": [], "entities": []}, {"text": "The second is the Neural Probabilistic Language Model (NPLM) which uses limited context constrained to the recent words (3 grams or 5 grams).", "labels": [], "entities": []}, {"text": "We trained these models on a large Harry Potter fan fiction corpus and we then used them to predict the words of chapter 9 of Harry Potter and the Sorcerer's Stone.", "labels": [], "entities": [{"text": "predict the words of chapter 9 of Harry Potter and the Sorcerer's Stone", "start_pos": 92, "end_pos": 163, "type": "TASK", "confidence": 0.8342189746243613}]}, {"text": "In parallel, we ran an MEG experiment in which 3 subject read the words of chapter 9 one by one while their brain activity was recorded.", "labels": [], "entities": []}, {"text": "We then looked for the alignment between the word-by-word vectors produced by the neural networks and the word-byword neural activity recorded by MEG.", "labels": [], "entities": [{"text": "MEG", "start_pos": 146, "end_pos": 149, "type": "DATASET", "confidence": 0.8761717081069946}]}, {"text": "Our neural networks have 3 key constituents: a hidden layer that summarizes the history of the previous words ; an embeddings vector that summarizes the (constant) properties of a given word and finally the output probability of a word given Reading comprehension is reflected in the subsequent activation of the left superior temporal cortex at 200-600 ms).", "labels": [], "entities": []}, {"text": "This sustained activation differentiates between words and nonwords (.", "labels": [], "entities": []}, {"text": "Apart from lexical-semantic aspects it also seems to be sensitive to phonological manipulation (.", "labels": [], "entities": []}, {"text": "As discussed above, in speech perception activation is concentrated to a rather small area in the brain and we have to rely on time information to dissociate between different processes.", "labels": [], "entities": [{"text": "speech perception activation", "start_pos": 23, "end_pos": 51, "type": "TASK", "confidence": 0.8270295461018881}]}, {"text": "Here, the different processes are separable both in timing and location.", "labels": [], "entities": [{"text": "timing", "start_pos": 52, "end_pos": 58, "type": "METRIC", "confidence": 0.9961981177330017}]}, {"text": "Because of that, one might think that it is easier to characterize language-related processes in the visual than auditory modality.", "labels": [], "entities": []}, {"text": "However, here the difficulties appear at another level.", "labels": [], "entities": []}, {"text": "In reading, activation is detected bilaterally in the occipital cortex, along the temporal lobes, in the parietal cortex and, in vocalized reading, also in the frontal lobes, at various times with respect to stimulus onset.", "labels": [], "entities": []}, {"text": "Interindividual variability further complicates the picture, resulting in practically excessive amounts of temporal and spatial information.", "labels": [], "entities": []}, {"text": "The areas and time windows depicted in, with specific roles in reading, form a limited subset of all active areas observed during reading.", "labels": [], "entities": []}, {"text": "In order to perform proper functional localization one needs to vary the stimuli and tasks systematically, in a parametric fashion.", "labels": [], "entities": []}, {"text": "Let us now consider how one may extract activation reflecting pre-lexical letter-string analysis and lexical-semantic processing.", "labels": [], "entities": []}], "datasetContent": [{"text": "To find which parts of brain activity are related to the neural network constituents (e.g. the RNNLM context vector), we run a prediction and classification experiment in a 10-fold cross validated fashion.", "labels": [], "entities": [{"text": "prediction and classification", "start_pos": 127, "end_pos": 156, "type": "TASK", "confidence": 0.7741895318031311}]}, {"text": "At every fold, we train a linear model to predict MEG data as a function of one of the feature sets, using 90% of the data.", "labels": [], "entities": []}, {"text": "On the remaining 10% of the data, we run a classification experiment.", "labels": [], "entities": [{"text": "classification", "start_pos": 43, "end_pos": 57, "type": "TASK", "confidence": 0.9661579132080078}]}, {"text": "MEG data is very noisy.", "labels": [], "entities": [{"text": "MEG data", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.8952876627445221}]}, {"text": "Therefore, classifying single word waveforms yields a low accuracy, peaking at 60%, which might lead to false negatives when looking for correspondences between neural network features and brain data.", "labels": [], "entities": [{"text": "classifying single word waveforms", "start_pos": 11, "end_pos": 44, "type": "TASK", "confidence": 0.8594402372837067}, {"text": "accuracy", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.9988549947738647}]}, {"text": "To reveal informative features, one can boost signal by either having several repetitions of the stimuli in the experiment and then averaging ( or by combining the words into larger chunks).", "labels": [], "entities": []}, {"text": "We chose the latter because the former sacrifices word and feature diversity.", "labels": [], "entities": []}, {"text": "At testing, we therefore repeat the following 300 times.", "labels": [], "entities": []}, {"text": "Two sets of words are chosen randomly from the test fold.", "labels": [], "entities": []}, {"text": "To form the first set, 20 words are sampled without replacement from the test sample (unseen by the classifier).", "labels": [], "entities": []}, {"text": "To form the second set, the k th word is chosen randomly from all words in the test fold having the same length as the k th word of the first set.", "labels": [], "entities": []}, {"text": "Since every fold of the data was used 9 times in the training phase and once in the testing phase, and since we use a high number of randomized comparisons, this averages out biases in the accuracy estimation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 189, "end_pos": 197, "type": "METRIC", "confidence": 0.9981301426887512}]}, {"text": "Classifying sets of 20 words improves the classification accuracy greatly while lowering its variance and makes it dissociable from chance performance.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.9835235476493835}, {"text": "variance", "start_pos": 93, "end_pos": 101, "type": "METRIC", "confidence": 0.980030357837677}]}, {"text": "We compare only between words of equal length, to minimize the effect of the low level visual features on the classification accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 125, "end_pos": 133, "type": "METRIC", "confidence": 0.9501696825027466}]}, {"text": "After averaging out the results of multiple folds, we end up with average accuracies that reveal how related one of the models' constituents (e.g. the RNNLM context vector) is to brain data.", "labels": [], "entities": []}], "tableCaptions": []}