{"title": [{"text": "Jointly Learning Word Representations and Composition Functions Using Predicate-Argument Structures", "labels": [], "entities": []}], "abstractContent": [{"text": "We introduce a novel compositional language model that works on Predicate-Argument Structures (PASs).", "labels": [], "entities": []}, {"text": "Our model jointly learns word representations and their composition functions using bag-of-words and dependency-based contexts.", "labels": [], "entities": []}, {"text": "Unlike previous word-sequence-based models, our PAS-based model composes arguments into predicates by using the category information from the PAS.", "labels": [], "entities": []}, {"text": "This enables our model to capture long-range dependencies between words and to better handle constructs such as verb-object and subject-verb-object relations.", "labels": [], "entities": []}, {"text": "We verify this experimentally using two phrase similarity datasets and achieve results comparable to or higher than the previous best results.", "labels": [], "entities": []}, {"text": "Our system achieves these results without the need for pre-trained word vectors and using a much smaller training corpus; despite this, for the subject-verb-object dataset our model improves upon the state of the art by as much as \u223c10% in relative performance.", "labels": [], "entities": []}], "introductionContent": [{"text": "Studies on embedding single words in a vector space have made notable successes in capturing their syntactic and semantic properties.", "labels": [], "entities": []}, {"text": "These embeddings have also been found to be a useful component for Natural Language Processing (NLP) systems; for example, and demonstrated how low-dimensional word vectors learned by Neural Network Language Models (NNLMs) are beneficial fora wide range of NLP tasks.", "labels": [], "entities": []}, {"text": "Recently, the main focus of research on vector space representation is shifting from word representations to phrase representations (.", "labels": [], "entities": [{"text": "vector space representation", "start_pos": 40, "end_pos": 67, "type": "TASK", "confidence": 0.6539627810319265}]}, {"text": "Combining the ideas of NNLMs and semantic composition, introduced a novel NNLM incorporating verb-object dependencies.", "labels": [], "entities": []}, {"text": "More recently, presented a NNLM that integrated syntactic dependencies.", "labels": [], "entities": [{"text": "NNLM", "start_pos": 27, "end_pos": 31, "type": "DATASET", "confidence": 0.8740025162696838}]}, {"text": "However, to the best of our knowledge, there is no previous work on integrating a variety of syntactic and semantic dependencies into NNLMs in order to learn composition functions as well as word representations.", "labels": [], "entities": []}, {"text": "The following question thus arises naturally: Can a variety of dependencies be used to jointly learn both stand-alone word vectors and their compositions, embedding them in the same vector space?", "labels": [], "entities": []}, {"text": "In this work, we bridge the gap between purely context-based () and compositional () NNLMs by using the flexible set of categories from Predicate-Argument-Structures (PASs).", "labels": [], "entities": []}, {"text": "More specifically, we propose a Compositional Log-Bilinear Language Model using PASs (PAS-CLBLM), an overview of which is shown in.", "labels": [], "entities": []}, {"text": "The model is trained by maximizing the accuracy of predicting target words from their bag-of-words and dependency-based context, which provides information about selectional preference.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.999010443687439}, {"text": "predicting target words", "start_pos": 51, "end_pos": 74, "type": "TASK", "confidence": 0.8674325744311014}]}, {"text": "As shown in (b), one of the advantages of the PAS-CLBLM is that the model can treat not only word vectors but also composed vectors as contexts.", "labels": [], "entities": []}, {"text": "Since the composed vectors The PAS-CLBLM predicts target words using not only context words but also composed vector representations derived from another level of predicate-argument structures.", "labels": [], "entities": [{"text": "PAS-CLBLM predicts target words", "start_pos": 31, "end_pos": 62, "type": "TASK", "confidence": 0.6530136168003082}]}, {"text": "Underlined words are target words and we only depict the bag-ofwords vector for the PAS-CLBLM. are treated as input to the language model in the same way as word vectors, these composed vectors are expected to become similar to word vectors for words with similar meanings.", "labels": [], "entities": []}, {"text": "Our empirical results demonstrate that the proposed model has the ability to learn meaningful representations for adjective-noun, noun-noun, and (subject-) verb-object dependencies.", "labels": [], "entities": []}, {"text": "On three tasks of measuring the semantic similarity between short phrases (adjective-noun, noun-noun, and verb-object), the learned composed vectors achieve scores (Spearman's rank correlation \u03c1) comparable to or higher than those of previous models.", "labels": [], "entities": [{"text": "Spearman's rank correlation \u03c1)", "start_pos": 165, "end_pos": 195, "type": "METRIC", "confidence": 0.7187091012795767}]}, {"text": "On a task involving more complex phrases (subject-verb-object), our learned composed vectors achieve state-of-the-art performance (\u03c1 = 0.50) with a training corpus that is an order of magnitude smaller than that used by previous work.", "labels": [], "entities": []}, {"text": "Moreover, the proposed model does not require any pre-trained word vectors produced by external models, but rather induces word vectors jointly while training.", "labels": [], "entities": []}], "datasetContent": [{"text": "The learned models were evaluated on four tasks of measuring the semantic similarity between short phrases.", "labels": [], "entities": []}, {"text": "We performed evaluation using the three tasks (AN, NN, and VO) in the dataset 3 provided by, and the SVO task in the dataset provided by.", "labels": [], "entities": [{"text": "AN", "start_pos": 47, "end_pos": 49, "type": "METRIC", "confidence": 0.9300025105476379}, {"text": "VO", "start_pos": 59, "end_pos": 61, "type": "METRIC", "confidence": 0.962240993976593}]}, {"text": "The datasets include pairs of short phrases extracted from the BNC.", "labels": [], "entities": [{"text": "BNC", "start_pos": 63, "end_pos": 66, "type": "DATASET", "confidence": 0.9483739137649536}]}, {"text": "AN, NN, and VO contain 108 phrase pairs of adjective-noun, nounnoun, and verb-object.", "labels": [], "entities": [{"text": "AN", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.6394910216331482}]}, {"text": "SVO contains 200 pairs of subject-verb-object phrases.", "labels": [], "entities": [{"text": "SVO", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7855786085128784}]}, {"text": "Each phrase pair has multiple human-ratings: the higher the rating is, the more semantically similar the phrases.", "labels": [], "entities": []}, {"text": "For example, the subject-verb-object phrase pair of \"student write name\" and \"student spell name\" has a high rating.", "labels": [], "entities": []}, {"text": "The pair \"people try door\" and \"people judge door\" has a low rating.", "labels": [], "entities": []}, {"text": "For evaluation we used the Spearman's rank correlation \u03c1 between the human-ratings and the cosine similarity between the composed vector pairs.", "labels": [], "entities": [{"text": "Spearman's rank correlation \u03c1", "start_pos": 27, "end_pos": 56, "type": "METRIC", "confidence": 0.6832246482372284}]}, {"text": "We mainly used non-averaged humanratings for each pair, and as described in Section 5.3, we also used averaged human-ratings for the SVO task.", "labels": [], "entities": []}, {"text": "Each phrase pair in the datasets was annotated by more than two annotators.", "labels": [], "entities": []}, {"text": "In the case of averaged human ratings, we averaged multiple human-ratings for each phrase pair, and in the case of non-averaged human-ratings, we treated each human-rating as a separate annotation.", "labels": [], "entities": []}, {"text": "With the PAS-CLBLM, we represented each phrase using the composition functions listed in.", "labels": [], "entities": []}, {"text": "When there was no composition present, we represented the phrase using element-wise addition.", "labels": [], "entities": []}, {"text": "the composed vector for each phrase was computed using the Wadd nl function, and when we trained the PAS-LBLM, we used the element-wise addition function.", "labels": [], "entities": []}, {"text": "To compute the composed vectors using the Wadd land Wadd nl functions, we used the categories of the predicates adj arg1, noun arg1, and verb arg12 listed in.", "labels": [], "entities": [{"text": "Wadd land Wadd nl", "start_pos": 42, "end_pos": 59, "type": "DATASET", "confidence": 0.9038206934928894}]}, {"text": "As a strong baseline, we trained the Skip-gram model of using the publicly available word2vec 5 software.", "labels": [], "entities": [{"text": "word2vec 5 software", "start_pos": 85, "end_pos": 104, "type": "DATASET", "confidence": 0.8877721627553304}]}, {"text": "We fed the POS-tagged BNC into word2vec since our models utilize POS tags and trained 50-dimensional word vectors using word2vec.", "labels": [], "entities": [{"text": "word2vec", "start_pos": 31, "end_pos": 39, "type": "DATASET", "confidence": 0.9144858717918396}]}, {"text": "For each phrase we then computed the representation using vector addition.", "labels": [], "entities": []}, {"text": "shows the correlation scores \u03c1 for the AN, NN, and VO tasks.", "labels": [], "entities": [{"text": "correlation", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9624466300010681}, {"text": "AN", "start_pos": 39, "end_pos": 41, "type": "METRIC", "confidence": 0.8426814079284668}, {"text": "VO", "start_pos": 51, "end_pos": 53, "type": "METRIC", "confidence": 0.8664906024932861}]}, {"text": "Human agreement denotes the inter-annotator agreement.", "labels": [], "entities": []}, {"text": "The word2vec baseline achieves unexpectedly high scores for these three tasks.", "labels": [], "entities": []}, {"text": "Previously these kinds of models () have mainly been evaluated for word analogy tasks and, to date, there has been no work using these word vectors for the task of measuring the semantic similarity between phrases.", "labels": [], "entities": [{"text": "word analogy tasks", "start_pos": 67, "end_pos": 85, "type": "TASK", "confidence": 0.8220635851224264}]}, {"text": "However, this experimental result suggests that word2vec can serve as a strong baseline for these kinds of tasks, in addition to word analogy tasks.", "labels": [], "entities": [{"text": "word analogy tasks", "start_pos": 129, "end_pos": 147, "type": "TASK", "confidence": 0.8101759155591329}]}], "tableCaptions": [{"text": " Table 2. When there was no composition present,  we represented the phrase using element-wise ad- dition.", "labels": [], "entities": []}, {"text": " Table 4: Spearman's rank correlation scores \u03c1 for the SVO task. Averaged denotes the \u03c1 calculated by  averaged human ratings, and Non-averaged denotes the \u03c1 calculated by non-averaged human ratings.", "labels": [], "entities": [{"text": "SVO task", "start_pos": 55, "end_pos": 63, "type": "TASK", "confidence": 0.5632922947406769}]}, {"text": " Table 6: Scores of the PAS-CLBLM with and  without BoW contexts.", "labels": [], "entities": [{"text": "BoW", "start_pos": 52, "end_pos": 55, "type": "DATASET", "confidence": 0.7959637641906738}]}, {"text": " Table 7: Nearest neighbor vectors for multiple  words. POS-tags are not shown for simplicity.", "labels": [], "entities": []}, {"text": " Table 8: L2-norms of the 50-dimensional weight  vectors of the composition function Wadd nl .", "labels": [], "entities": []}]}