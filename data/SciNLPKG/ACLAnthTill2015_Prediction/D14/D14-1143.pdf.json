{"title": [{"text": "Formalizing Word Sampling for Vocabulary Prediction as Graph-based Active Learning", "labels": [], "entities": [{"text": "Formalizing Word Sampling", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.732907215754191}, {"text": "Vocabulary Prediction", "start_pos": 30, "end_pos": 51, "type": "TASK", "confidence": 0.772506594657898}]}], "abstractContent": [{"text": "Predicting vocabulary of second language learners is essential to support their language learning; however, because of the large size of language vocabularies, we cannot collect information on the entire vocabulary.", "labels": [], "entities": []}, {"text": "For practical measurements, we need to sample a small portion of words from the entire vocabulary and predict the rest of the words.", "labels": [], "entities": []}, {"text": "In this study, we propose a novel framework for this sampling method.", "labels": [], "entities": []}, {"text": "Current methods rely on simple heuristic techniques involving inflexible manual tuning by educational experts.", "labels": [], "entities": []}, {"text": "We formalize these heuristic techniques as a graph-based non-interactive active learning method as applied to a special graph.", "labels": [], "entities": []}, {"text": "We show that by extending the graph, we can support additional function-ality such as incorporating domain speci-ficity and sampling from multiple corpora.", "labels": [], "entities": []}, {"text": "In our experiments, we show that our extended methods outperform other methods in terms of vocabulary prediction accuracy when the number of samples is small.", "labels": [], "entities": [{"text": "vocabulary prediction", "start_pos": 91, "end_pos": 112, "type": "TASK", "confidence": 0.7671307921409607}, {"text": "accuracy", "start_pos": 113, "end_pos": 121, "type": "METRIC", "confidence": 0.8998900651931763}]}], "introductionContent": [{"text": "Predicting the vocabulary of second language learners is essential to support them when they are reading.", "labels": [], "entities": []}, {"text": "Educational experts have been continuously studying methods for measuring the size of a learner's vocabulary, i.e., the number of words * The main body of this work was done when the first author was a Ph.D. candidate in the University of Tokyo and the paper was later greatly revised when the first author was a JSPS (Japan Society for the Promotion of Science) research fellow (PD) at National Institute of Informatics.", "labels": [], "entities": [{"text": "JSPS (Japan Society for the Promotion of Science) research fellow (PD)", "start_pos": 313, "end_pos": 383, "type": "TASK", "confidence": 0.6564609607060751}]}, {"text": "See http: //yoehara.com/ for details.", "labels": [], "entities": []}, {"text": "the learner knows, over the decades ().", "labels": [], "entities": []}, {"text": "formalized a more fine-grained measurement task called vocabulary prediction.", "labels": [], "entities": [{"text": "vocabulary prediction", "start_pos": 55, "end_pos": 76, "type": "TASK", "confidence": 0.8502038717269897}]}, {"text": "The goal of this task is to predict whether a learner knows a given word based on only a relatively small portion of his/her vocabulary.", "labels": [], "entities": []}, {"text": "This vocabulary prediction task can be further used for predicting the readability of texts.", "labels": [], "entities": [{"text": "vocabulary prediction", "start_pos": 5, "end_pos": 26, "type": "TASK", "confidence": 0.6993311196565628}]}, {"text": "By predicting vocabulary unknown to readers and showing the meaning of those specific words to readers, showed that the number of documents that learners can read increases.", "labels": [], "entities": []}, {"text": "Word sampling is essential for vocabulary prediction.", "labels": [], "entities": [{"text": "Word sampling", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.649396225810051}, {"text": "vocabulary prediction", "start_pos": 31, "end_pos": 52, "type": "TASK", "confidence": 0.8482272624969482}]}, {"text": "Because of the large size of language vocabularies, we usually cannot collect information on the entire vocabulary.", "labels": [], "entities": []}, {"text": "For practical measurements, we inevitably need to sample a small portion of words from the entire vocabulary and then predict the rest.", "labels": [], "entities": []}, {"text": "We refer to this sampling technique as word sampling.", "labels": [], "entities": [{"text": "word sampling", "start_pos": 39, "end_pos": 52, "type": "TASK", "confidence": 0.6645393967628479}]}, {"text": "Word sampling can greatly affect the performance of vocabulary prediction.", "labels": [], "entities": [{"text": "vocabulary prediction", "start_pos": 52, "end_pos": 73, "type": "TASK", "confidence": 0.7880021035671234}]}, {"text": "For example, if we consider only short everyday general domain words such as \"cat\" and \"dog\" as samples, the rest of the vocabulary is difficult to predict since learners likely know most of these words.", "labels": [], "entities": []}, {"text": "To more accurately measure a learner's vocabulary, we ideally must sample words that are representative of the entire set of words.", "labels": [], "entities": []}, {"text": "More specifically, we wish to sample words such that if a learner knows these words, he/she is likely to know the rest of the words in the given vocabulary, and vice versa.", "labels": [], "entities": []}, {"text": "To our knowledge, however, all current studies have relied on a simple heuristic method.", "labels": [], "entities": []}, {"text": "In this heuristic method, educational experts first somehow create groups of words with the aim that the words in a group are of similar difficulty for learn-ers.", "labels": [], "entities": []}, {"text": "To create groups of words, the experts typically make use of word frequencies and sometimes manually reclassify words based on experience.", "labels": [], "entities": []}, {"text": "Next, a fixed number of words are randomly sampled from each group via a uniform distribution.", "labels": [], "entities": []}, {"text": "We call this approach heuristic word sampling.", "labels": [], "entities": [{"text": "word sampling", "start_pos": 32, "end_pos": 45, "type": "TASK", "confidence": 0.6950971335172653}]}, {"text": "In this study, we propose a novel framework that formalizes word sampling as non-interactive graph-based active learning based on weighted graphs.", "labels": [], "entities": [{"text": "word sampling", "start_pos": 60, "end_pos": 73, "type": "TASK", "confidence": 0.7566080093383789}]}, {"text": "In our approach, nodes of a graph correspond to words, whereas the edge weights show how similar the difficulty levels of a word pair are.", "labels": [], "entities": []}, {"text": "Unlike interactive active learning algorithms used in the NLP community, which use expert annotators' human labels for sampling nodes, noninteractive active learning algorithms exclude expert annotators' human labels from the protocol (.", "labels": [], "entities": []}, {"text": "Given a weighted graph and using only its structure, without human labels, these algorithms sample nodes that are important for classification with algorithms called label propagation.", "labels": [], "entities": [{"text": "label propagation", "start_pos": 166, "end_pos": 183, "type": "TASK", "confidence": 0.7303051948547363}]}, {"text": "Excluding annotators' human labels from the protocol is beneficial for educational purposes since learners can share the same set of sampled words via, for example, printed handouts.", "labels": [], "entities": []}, {"text": "Formalizing the current methods as noninteractive graph-based active learning enables us to extend the sampling methods with additional functionality that current methods cannot handle without applying burdensome manual heuristics because we can flexibly design the weighted graphs fed to the active learning algorithms.", "labels": [], "entities": []}, {"text": "In our framework, this extension is achieved by extending the graph, namely, our framework can handle domain specificity and multiple corpora.", "labels": [], "entities": []}, {"text": "Domains are important when one wants to measure the vocabulary of learners.", "labels": [], "entities": []}, {"text": "For example, consider measuring non-native English speakers taking computer science graduate courses.", "labels": [], "entities": []}, {"text": "We may want to measure their English vocabulary with an emphasis on computer science rather than their general English vocabulary.", "labels": [], "entities": []}, {"text": "However, such an extension is impossible via current methods, and thus it is desirable to sample algorithms to be able to handle domain specificity.", "labels": [], "entities": []}, {"text": "Our framework can incorporate domain specificity between words in the form of edges between such words.", "labels": [], "entities": []}, {"text": "Handling multiple corpora is important when we cannot single out which corpus we should rely on.", "labels": [], "entities": []}, {"text": "The current technique used by educational experts to handle multiple corpora is to heuristically integrate multiple frequency lists from multiple corpora into a single list of words; however, such manual integration is burdensome.", "labels": [], "entities": []}, {"text": "Thus, automatic integration is desirable.", "labels": [], "entities": [{"text": "automatic integration", "start_pos": 6, "end_pos": 27, "type": "TASK", "confidence": 0.6050183773040771}]}, {"text": "Our framework converts multiple corpora into graphs, merges these graphs together, and then samples from the merged graph.", "labels": [], "entities": []}, {"text": "Our contributions as presented in this paper are summarized as follows: 1.", "labels": [], "entities": []}, {"text": "We formalize word sampling for vocabulary prediction as graph-based active learning.", "labels": [], "entities": [{"text": "word sampling", "start_pos": 13, "end_pos": 26, "type": "TASK", "confidence": 0.6968085616827011}, {"text": "vocabulary prediction", "start_pos": 31, "end_pos": 52, "type": "TASK", "confidence": 0.8225069344043732}]}, {"text": "2. Based on this formalization, we can perform more flexible word sampling that can handle domain specificity and multiple corpora.", "labels": [], "entities": [{"text": "word sampling", "start_pos": 61, "end_pos": 74, "type": "TASK", "confidence": 0.7010706514120102}]}, {"text": "The remaining parts of this paper are organized as follows.", "labels": [], "entities": []}, {"text": "In \u00a72, we explain the problem setting in detail.", "labels": [], "entities": []}, {"text": "We first explain how existing heuristic word sampling works and how it relies on the cluster assumption from the viewpoint of graphs.", "labels": [], "entities": [{"text": "word sampling", "start_pos": 40, "end_pos": 53, "type": "TASK", "confidence": 0.7135085165500641}]}, {"text": "Then, we introduce existing graph-based non-interactive active learning methods.", "labels": [], "entities": []}, {"text": "In \u00a73, we show that the existing heuristic word sampling is merely a special case of a non-interactive active learning method (.", "labels": [], "entities": [{"text": "word sampling", "start_pos": 43, "end_pos": 56, "type": "TASK", "confidence": 0.6866548508405685}]}, {"text": "Precisely, the existing sampling is identical to the case where a special graph called a \"multi-complete graph\" is fed to a non-interactive active learning method.", "labels": [], "entities": []}, {"text": "Since this method can take any weighted graphs other than this special graph, this immediately leads to away of devising new sampling methods by modifying graphs.", "labels": [], "entities": []}, {"text": "\u00a74 explains exactly how we can modify graphs for improving active learning.", "labels": [], "entities": []}, {"text": "\u00a75 evaluates the proposed method both quantitatively and qualitatively, and \u00a76 concludes our paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our proposed method both quantitatively and qualitatively.", "labels": [], "entities": []}, {"text": "In the quantitative evaluation, we measure the prediction accuracy of graphs.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.9697198271751404}]}, {"text": "Note that the heuristic word sampling method is identical to using Gu and Han's algorithm with a multiple complete graph; however, our proposed graphs have enriched relations between words.", "labels": [], "entities": [{"text": "word sampling", "start_pos": 24, "end_pos": 37, "type": "TASK", "confidence": 0.7590487897396088}]}, {"text": "In the qualitative evaluation, we explain in detail what words are appropriate as training examples for vocabulary prediction by presenting sampled examples.", "labels": [], "entities": [{"text": "vocabulary prediction", "start_pos": 104, "end_pos": 125, "type": "TASK", "confidence": 0.7463745176792145}]}, {"text": "To evaluate the accuracy of vocabulary prediction, we used the dataset that and used.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.9991204142570496}, {"text": "vocabulary prediction", "start_pos": 28, "end_pos": 49, "type": "TASK", "confidence": 0.8239165544509888}]}, {"text": "This dataset was gleaned from questionnaires answered by 15 English as a second language (ESL) learners.", "labels": [], "entities": []}, {"text": "Every learner was asked to answer how well he/she knew 11,999 English words.", "labels": [], "entities": []}, {"text": "The data was collected in January 2009.", "labels": [], "entities": []}, {"text": "One learner was unpaid, whereas the other 15 learners were paid.", "labels": [], "entities": []}, {"text": "We used the data from the 15 paid learners since the data from the unpaid learner was noisy.", "labels": [], "entities": []}, {"text": "Most of the learners were native Japanese speakers and graduate students.", "labels": [], "entities": []}, {"text": "Because most of the learners in this dataset were native Japanese speakers, words from SVL 12,000 were used for the learners in this dataset.", "labels": [], "entities": []}, {"text": "Note that SVL 12,000 is a collection of 12,000 words that are deemed important for Japanese learners of English, as judged by native English teachers.", "labels": [], "entities": []}, {"text": "Next, we required frequency lists for the words that appeared in the dataset.", "labels": [], "entities": []}, {"text": "To create frequency lists, lemmatization is important because the number of word types depends on the method used to lemmatize the words.", "labels": [], "entities": []}, {"text": "Note that in the field of vocabulary measurement, lemmatization is mainly performed by ignoring conjugation.", "labels": [], "entities": [{"text": "vocabulary measurement", "start_pos": 26, "end_pos": 48, "type": "TASK", "confidence": 0.7664789259433746}]}, {"text": "Lemmatizing the dataset resulted in a word list of 8,463 words.", "labels": [], "entities": []}, {"text": "We adjusted the size of the word list to around 8,000 by removing 463 randomly chosen words.", "labels": [], "entities": []}, {"text": "Note that all constituent words were labeled by the 15 ESL learners.", "labels": [], "entities": [{"text": "ESL learners", "start_pos": 55, "end_pos": 67, "type": "DATASET", "confidence": 0.7829487323760986}]}, {"text": "We created the following four graphs by spanning edges among the 8, 000 words.", "labels": [], "entities": []}, {"text": "In this subsection, we qualitatively evaluate our results to determine the types of nodes that are sampled when domain specificity is introduced.", "labels": [], "entities": []}, {"text": "Specifically, we evaluate what words are selected as samples in the \"BNC + domain\" graph.", "labels": [], "entities": [{"text": "BNC + domain\" graph", "start_pos": 69, "end_pos": 88, "type": "DATASET", "confidence": 0.7906027793884277}]}, {"text": "As noted above, in the \"BNC + domain\" graph, the computer science domain is introduced into \"BNC multi-complete\" to measure learners' vocabulary with a specific emphasis on the computer science domain.", "labels": [], "entities": [{"text": "BNC + domain\" graph", "start_pos": 24, "end_pos": 43, "type": "DATASET", "confidence": 0.7558276057243347}]}, {"text": "Thus, it is desirable that some words in the computer science domain are sampled from the \"BNC + domain\" graph; otherwise, we need to predict the learners' vocabulary for the computer science domain from general words rather than those in the computer science domain, which is extremely difficult.", "labels": [], "entities": []}, {"text": "shows the number of words in the computer science domain sampled in the first 30 samples.", "labels": [], "entities": []}, {"text": "Note that only \"BNC + domain\" and \"BNC + domain + COCA\" select samples from the computer science domain.", "labels": [], "entities": []}, {"text": "This indicates that in the other two methods, to measure vocabulary with an emphasis on the computer science domain, we need to predict learners' vocabulary from the general words, which is almost impossible with only 30 samples.", "labels": [], "entities": []}, {"text": "Furthermore, it is interesting to note that \"BNC + domain\" and \"BNC + domain + COCA\" select different samples from the computer science domain, except for the word \"client,\" although originally the same computer science domain wordlist was introduced to both graphs.", "labels": [], "entities": []}, {"text": "Since \"BNC + domain\" achieves competitive or slightly better accuracy than \"BNC multicomplete\" in the quantitative analysis and the qualitative analysis, we conclude that our method can successfully introduce domain specificity into the sampling methodology without reducing accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.9960141181945801}, {"text": "accuracy", "start_pos": 275, "end_pos": 283, "type": "METRIC", "confidence": 0.9933527708053589}]}], "tableCaptions": [{"text": " Table 1: Results of our quantitative experiments. LLGC is used for classification unless otherwise spec- ified. Bold letters indicate top accuracy. Asterisks (*) indicate that values are statistically significant  against baseline, heuristic sampling, i.e., \"BNC multi-complete\" (using sign test p < 0.01).  10  15  20  30  40  50  BNC multi-complete  64.15 (%) 67.54  73.73  73.66  74.92  74.82  BNC+domain  65.27  71.88  72.88  75.02  76.03 * 75.95  BNC+COCA  73.45  74.10  74.57  74.90  74.96  75.29  BNC+domain+COCA  75.23 *  75.71 * 75.18 * 75.35 * 75.47  76.44 *  BNC+domain+COCA (SVM)  58.99  57.74  60.44  70.79  69.29  74.46  BNC+domain+COCA (LR)  60.29  61.74  59.27  69.17  70.63  73.42", "labels": [], "entities": [{"text": "accuracy", "start_pos": 139, "end_pos": 147, "type": "METRIC", "confidence": 0.9625968337059021}]}]}