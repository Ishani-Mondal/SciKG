{"title": [{"text": "Coarse-grained Candidate Generation and Fine-grained Re-ranking for Chinese Abbreviation Prediction", "labels": [], "entities": [{"text": "Re-ranking", "start_pos": 53, "end_pos": 63, "type": "METRIC", "confidence": 0.6221625208854675}, {"text": "Chinese Abbreviation Prediction", "start_pos": 68, "end_pos": 99, "type": "TASK", "confidence": 0.6459179520606995}]}], "abstractContent": [{"text": "Correctly predicting abbreviations given the full forms is important in many natural language processing systems.", "labels": [], "entities": [{"text": "predicting abbreviations", "start_pos": 10, "end_pos": 34, "type": "TASK", "confidence": 0.8595981597900391}]}, {"text": "In this paper we propose a two-stage method to find the corresponding abbreviation given its full form.", "labels": [], "entities": []}, {"text": "We first use the contextual information given a large corpus to get abbreviation candidates for each full form and get a coarse-grained ranking through graph random walk.", "labels": [], "entities": []}, {"text": "This coarse-grained rank list fixes the search space inside the top-ranked candidates.", "labels": [], "entities": []}, {"text": "Then we use a similarity sensitive re-ranking strategy which can utilize the features of the candidates to give a fine-grained re-ranking and select the final result.", "labels": [], "entities": []}, {"text": "Our method achieves good results and outperforms the state-of-the-art systems.", "labels": [], "entities": []}, {"text": "One advantage of our method is that it only needs weak supervision and can get competitive results with fewer training data.", "labels": [], "entities": []}, {"text": "The candidate generation and coarse-grained ranking is totally unsupervised.", "labels": [], "entities": [{"text": "coarse-grained", "start_pos": 29, "end_pos": 43, "type": "METRIC", "confidence": 0.9534410834312439}]}, {"text": "The re-ranking phase can use a very small amount of training data to get a reasonably good result.", "labels": [], "entities": []}], "introductionContent": [{"text": "Abbreviation Prediction is defined as finding the meaningful short subsequence of characters given the original fully expanded form.", "labels": [], "entities": [{"text": "Abbreviation", "start_pos": 0, "end_pos": 12, "type": "METRIC", "confidence": 0.9730315208435059}]}, {"text": "As an example, \"HMM\" is the abbreviation for the corresponding full form \"Hidden Markov Model\".", "labels": [], "entities": []}, {"text": "While the existence of abbreviations is a common linguistic phenomenon, it causes many problems like spelling variation).", "labels": [], "entities": []}, {"text": "The different writing manners make it difficult to identify the terms conveying the same concept, which will hurt the performance of many applications, such as information retrieval (IR) systems and machine translation (MT) systems.", "labels": [], "entities": [{"text": "information retrieval (IR)", "start_pos": 160, "end_pos": 186, "type": "TASK", "confidence": 0.821471631526947}, {"text": "machine translation (MT)", "start_pos": 199, "end_pos": 223, "type": "TASK", "confidence": 0.8423114001750946}]}, {"text": "Previous works mainly treat the Chinese abbreviation generation task as a sequence labeling problem, which gives each character a label to indicate whether the given character in the full form should be kept in the abbreviation or not.", "labels": [], "entities": [{"text": "Chinese abbreviation generation task", "start_pos": 32, "end_pos": 68, "type": "TASK", "confidence": 0.6953434497117996}]}, {"text": "These methods show acceptable results.", "labels": [], "entities": []}, {"text": "However they rely heavily on the character-based features, which means it needs lots of training data to learn the weights of these context features.", "labels": [], "entities": []}, {"text": "The performance is good on some test sets that are similar to the training data, however, when it moves to an unseen context, this method may fail.", "labels": [], "entities": []}, {"text": "This is always true in real application contexts like the social media where there are tremendous new abbreviations burst out everyday.", "labels": [], "entities": []}, {"text": "A more intuitive way is to find the fullabbreviation pairs directly from a large text corpus.", "labels": [], "entities": []}, {"text": "A good source of texts is the news texts.", "labels": [], "entities": []}, {"text": "Ina news text, the full forms are often mentioned first.", "labels": [], "entities": []}, {"text": "Then in the rest of the news its corresponding abbreviation is mentioned as an alternative.", "labels": [], "entities": [{"text": "abbreviation", "start_pos": 47, "end_pos": 59, "type": "METRIC", "confidence": 0.9871701598167419}]}, {"text": "The co-occurrence of the full form and the abbreviation makes it easier for us to mine the abbreviation pairs from the large amount of news texts.", "labels": [], "entities": []}, {"text": "Therefore, given along full form, we can generate its abbreviation candidates from the given corpus, instead of doing the character tagging job.", "labels": [], "entities": [{"text": "character tagging", "start_pos": 122, "end_pos": 139, "type": "TASK", "confidence": 0.7072991132736206}]}, {"text": "For the abbreviation prediction task, the candidate abbreviation must be a sub-sequence of the given full form.", "labels": [], "entities": [{"text": "abbreviation prediction task", "start_pos": 8, "end_pos": 36, "type": "TASK", "confidence": 0.9268866578737894}]}, {"text": "An intuitive way is to select all the sub-sequences in the corpus as the candidates.", "labels": [], "entities": []}, {"text": "This will generate large numbers of irrelevant candidates.", "labels": [], "entities": []}, {"text": "Instead, we use a contextual graph random walk method, which can utilize the contextual information through the graph, to select a coarse grained list of candidates given the full form.", "labels": [], "entities": []}, {"text": "We only select the top-ranked candidates to reduce the search space.", "labels": [], "entities": []}, {"text": "On the other hand, the candidate generation process can only use limited contextual information to give a coarse-grained ranked list of candidates.", "labels": [], "entities": [{"text": "candidate generation", "start_pos": 23, "end_pos": 43, "type": "TASK", "confidence": 0.8821389377117157}]}, {"text": "During generation, can-didate level features cannot be included.", "labels": [], "entities": []}, {"text": "Therefore we propose a similarity sensitive re-ranking method to give a fine-grained ranked list.", "labels": [], "entities": []}, {"text": "We then select the final result based on the rank of each candidate.", "labels": [], "entities": []}, {"text": "The contribution of our work is two folds.", "labels": [], "entities": []}, {"text": "Firstly we propose an improved method for abbreviation generation.", "labels": [], "entities": [{"text": "abbreviation generation", "start_pos": 42, "end_pos": 65, "type": "TASK", "confidence": 0.9796070456504822}]}, {"text": "Compared to previous work, our method can perform well with less training data.", "labels": [], "entities": []}, {"text": "This is an advantage in the context of social media.", "labels": [], "entities": []}, {"text": "Secondly, we build anew abbreviation corpus and make it publicly available for future research on this topic.", "labels": [], "entities": []}, {"text": "The paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 1 gives the introduction.", "labels": [], "entities": []}, {"text": "In section 2 we describe the abbreviation task.", "labels": [], "entities": []}, {"text": "In section 3 we describe the candidate generation part and in section 4 we describe the re-ranking part.", "labels": [], "entities": []}, {"text": "Experiments are described in section 5.", "labels": [], "entities": []}, {"text": "We also give a detailed analysis of the results in section 5.", "labels": [], "entities": []}, {"text": "In section 6 related works are introduced, and the paper is concluded in the last section.", "labels": [], "entities": []}], "datasetContent": [{"text": "For the dataset, we collect 3210 abbreviation pairs from the Chinese Gigaword corpus.", "labels": [], "entities": [{"text": "Chinese Gigaword corpus", "start_pos": 61, "end_pos": 84, "type": "DATASET", "confidence": 0.9130093852678934}]}, {"text": "The abbreviation pairs include noun phrases, organization names and some other types.", "labels": [], "entities": []}, {"text": "The Chinese Gigaword corpus contains news texts from the year 1992 to 2007.", "labels": [], "entities": [{"text": "Chinese Gigaword corpus", "start_pos": 4, "end_pos": 27, "type": "DATASET", "confidence": 0.9095508654912313}]}, {"text": "We only collect those pairs whose full form and corresponding abbreviation appear in the same article for at least onetime.", "labels": [], "entities": []}, {"text": "For full forms with more than one reasonable reference, we keep the most frequently used one as its reference.", "labels": [], "entities": []}, {"text": "We use 80% abbreviation pairs as the training data and the rest as the testing data.", "labels": [], "entities": []}, {"text": "We use the top-K accuracy as the evaluation metrics.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.9468350410461426}]}, {"text": "The top-K accuracy is widely used as the measurement in previous work.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.8872249126434326}]}, {"text": "It measures what percentage of the reference abbreviations are found if we take the top k candidate abbreviations from all the results.", "labels": [], "entities": []}, {"text": "In our experiment, we compare the top-5 accuracy with baselines.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.982222318649292}]}, {"text": "We choose the top-10 candidates from the graph random walk are considered in re-ranking phase and the measurement used is top-1 accuracy because the final aim of the algorithm is to detect the exact abbreviation, rather than a list of candidates.: Top-5 accuracy of the candidate generation phase", "labels": [], "entities": [{"text": "accuracy", "start_pos": 128, "end_pos": 136, "type": "METRIC", "confidence": 0.993370771408081}, {"text": "accuracy", "start_pos": 254, "end_pos": 262, "type": "METRIC", "confidence": 0.982696533203125}]}], "tableCaptions": [{"text": " Table 4: Top-5 accuracy of the candidate genera- tion phase", "labels": [], "entities": [{"text": "accuracy", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.9825496673583984}]}, {"text": " Table 3: Generated Candidates. #Enum is the number of candidates generated by enumerating all possi- ble candidates. #Now is the number of candidates generated by our method.", "labels": [], "entities": []}, {"text": " Table 5: Comparison of the baseline method and  our method. CRF-char ('-' means minus) is the  baseline method without character features. CRF  is the baseline method. Our-char is our method  without character features. We define character  features as the features that consider the charac- ters from the original full form as their parts.", "labels": [], "entities": []}]}