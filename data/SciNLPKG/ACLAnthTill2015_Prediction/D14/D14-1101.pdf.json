{"title": [{"text": "Neural Networks Leverage Corpus-wide Information for Part-of-speech Tagging", "labels": [], "entities": [{"text": "Part-of-speech Tagging", "start_pos": 53, "end_pos": 75, "type": "TASK", "confidence": 0.7546294331550598}]}], "abstractContent": [{"text": "We propose a neural network approach to benefit from the non-linearity of corpus-wide statistics for part-of-speech (POS) tagging.", "labels": [], "entities": [{"text": "part-of-speech (POS) tagging", "start_pos": 101, "end_pos": 129, "type": "TASK", "confidence": 0.6278642594814301}]}, {"text": "We investigated several types of corpus-wide information for the words, such as word embeddings and POS tag distributions.", "labels": [], "entities": []}, {"text": "Since these statistics are encoded as dense continuous features, it is not trivial to combine these features comparing with sparse discrete features.", "labels": [], "entities": []}, {"text": "Our tagger is designed as a combination of a linear model for discrete features and a feed-forward neural network that captures the non-linear interactions among the continuous features.", "labels": [], "entities": []}, {"text": "By using several recent advances in the activation functions for neural networks, the proposed method marks new state-of-the-art accuracies for English POS tagging tasks.", "labels": [], "entities": [{"text": "POS tagging tasks", "start_pos": 152, "end_pos": 169, "type": "TASK", "confidence": 0.8138374487559}]}], "introductionContent": [{"text": "Almost all of the approaches to NLP tasks such as part-of-speech tagging and syntactic parsing mainly use sparse discrete features to represent local information such as word surfaces in a sizelimited window.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 50, "end_pos": 72, "type": "TASK", "confidence": 0.7248491942882538}, {"text": "syntactic parsing", "start_pos": 77, "end_pos": 94, "type": "TASK", "confidence": 0.7199543416500092}]}, {"text": "The non-linearity of those discrete features is often used in many NLP tasks since the simple conjunction (AND) of discrete features represents the co-occurrence of the features and is intuitively understandable.", "labels": [], "entities": []}, {"text": "In addition, the thresholding of these combinatorial features by simple counts effectively suppresses the combinatorial increase of the parameters.", "labels": [], "entities": []}, {"text": "At the same time, although global information had also been used in several reports;, the nonlinear interactions of these features were not well investigated since these features are often dense continuous features and the explicit non-linear expansions are counterintuitive and drastically increase the number of the model parameters.", "labels": [], "entities": []}, {"text": "In our work, we investigate neural networks used to represent the non-linearity of global information for POS tagging in a compact way.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 106, "end_pos": 117, "type": "TASK", "confidence": 0.8875716328620911}]}, {"text": "We focus on four kinds of corpus-wide information: (1) word embeddings, (2) POS tag distributions, (3) supertag distributions, and (4) context word distributions.", "labels": [], "entities": []}, {"text": "All of them are continuous dense features and we use a feed-forward neural network to exploit the non-linearity of these features.", "labels": [], "entities": []}, {"text": "Although all of them except (3) have been used for POS tagging in previous work (, we propose a neural network approach to capture the non-linear interactions of these features.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 51, "end_pos": 62, "type": "TASK", "confidence": 0.9040127992630005}]}, {"text": "By feeding these features into neural networks as an input vector, we can expect our tagger can handle not only the nonlinearity of the N-grams of the same kinds of features but also the non-linear interactions among the different kind of features.", "labels": [], "entities": []}, {"text": "Our tagger combines a linear model using sparse high-dimensional features and a neural network using continuous dense features.", "labels": [], "entities": []}, {"text": "Although seeks to solve NLP tasks without depending on the feature engineering of conventional NLP methods, our architecture is more practical because it integrates the neural networks into a well-tuned conventional method.", "labels": [], "entities": []}, {"text": "Thus, our tagger enjoys both the manually explored combinations of discrete features and the automatically learned non-linearity of the continuous features.", "labels": [], "entities": []}, {"text": "We also studied some of the newer activation functions: Rectified Linear Units, Maxout networks (, and L p -pooling (.", "labels": [], "entities": []}, {"text": "Deep neural networks have been a hot topic in many application areas such as computer vi-sion and voice recognition.", "labels": [], "entities": [{"text": "voice recognition", "start_pos": 98, "end_pos": 115, "type": "TASK", "confidence": 0.7750591337680817}]}, {"text": "However, although neural networks show state-of-the-art results on a few semantic tasks (), neural network approaches have not performed better than the state-of-the-art systems for traditional syntactic tasks.", "labels": [], "entities": []}, {"text": "Our neural tagger shows state-ofthe-art results: 97.51% accuracy in the standard benchmark on the Penn Treebank () and 98.02% accuracy in POS tagging on).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.9992164373397827}, {"text": "Penn Treebank", "start_pos": 98, "end_pos": 111, "type": "DATASET", "confidence": 0.9944587647914886}, {"text": "accuracy", "start_pos": 126, "end_pos": 134, "type": "METRIC", "confidence": 0.998051643371582}, {"text": "POS tagging", "start_pos": 138, "end_pos": 149, "type": "TASK", "confidence": 0.7380844056606293}]}, {"text": "In our experiments, we found that the selection of the activation functions led to large differences in the tagging accuracies.", "labels": [], "entities": []}, {"text": "We also observed that the POS tags of the words are effectively clustered by the hidden activations of the intermediate layer.", "labels": [], "entities": []}, {"text": "This observation is evidence that the neural network can find good representations for POS tagging.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 87, "end_pos": 98, "type": "TASK", "confidence": 0.9205643236637115}]}, {"text": "The remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 introduces our deterministic tagger and its learning algorithm.", "labels": [], "entities": []}, {"text": "Section 3 describes the continuous features that represent corpus-wide information and Section 4 is about the neural network we used.", "labels": [], "entities": []}, {"text": "Section 5 presents our empirical study of the effects of corpus-wide information and neural networks on English POS tagging tasks.", "labels": [], "entities": [{"text": "POS tagging tasks", "start_pos": 112, "end_pos": 129, "type": "TASK", "confidence": 0.7941093345483144}]}, {"text": "Section 6 describes related work, and Section 7 concludes and suggests items for future work.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Data set splits for PTB.", "labels": [], "entities": [{"text": "PTB", "start_pos": 30, "end_pos": 33, "type": "DATASET", "confidence": 0.6786386370658875}]}, {"text": " Table 2: Feature and window size selection: de- velopment accuracies of all tokens (All) and un- known tokens (Unk.) of linear models trained on  PTB (w2v: word2vec; glv: glove; pos: POS tag  distribution; stg: supertag distribution; cw: con- text word distribution).", "labels": [], "entities": [{"text": "PTB", "start_pos": 147, "end_pos": 150, "type": "DATASET", "confidence": 0.9275801181793213}]}, {"text": " Table 3: Development and test accuracies of all tokens and unknown tokens (%) on PTB.", "labels": [], "entities": [{"text": "PTB", "start_pos": 82, "end_pos": 85, "type": "DATASET", "confidence": 0.9554554224014282}]}, {"text": " Table 4: Test accuracies of all tokens and unknown tokens (%) comparing with the previously reported  results", "labels": [], "entities": [{"text": "accuracies", "start_pos": 15, "end_pos": 25, "type": "METRIC", "confidence": 0.5179579854011536}]}]}