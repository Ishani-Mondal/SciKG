{"title": [{"text": "Latent-Variable Synchronous CFGs for Hierarchical Translation", "labels": [], "entities": [{"text": "Hierarchical Translation", "start_pos": 37, "end_pos": 61, "type": "TASK", "confidence": 0.8280909955501556}]}], "abstractContent": [{"text": "Data-driven refinement of non-terminal categories has been demonstrated to be a reliable technique for improving mono-lingual parsing with PCFGs.", "labels": [], "entities": [{"text": "mono-lingual parsing", "start_pos": 113, "end_pos": 133, "type": "TASK", "confidence": 0.5109936147928238}]}, {"text": "In this paper , we extend these techniques to learn latent refinements of single-category synchronous grammars, so as to improve translation performance.", "labels": [], "entities": [{"text": "translation", "start_pos": 129, "end_pos": 140, "type": "TASK", "confidence": 0.9611956477165222}]}, {"text": "We compare two estimators for this latent-variable model: one based on EM and the other is a spectral algorithm based on the method of moments.", "labels": [], "entities": [{"text": "EM", "start_pos": 71, "end_pos": 73, "type": "METRIC", "confidence": 0.648966908454895}]}, {"text": "We evaluate their performance on a Chinese-English translation task.", "labels": [], "entities": [{"text": "Chinese-English translation task", "start_pos": 35, "end_pos": 67, "type": "TASK", "confidence": 0.7426640292008718}]}, {"text": "The results indicate that we can achieve significant gains over the baseline with both approaches , but in particular the moments-based estimator is both faster and performs better than EM.", "labels": [], "entities": []}], "introductionContent": [{"text": "Translation models based on synchronous contextfree grammars (SCFGs) treat the translation problem as a context-free parsing problem.", "labels": [], "entities": []}, {"text": "A parser constructs trees over the input sentence by parsing with the source language projection of asynchronous CFG, and each derivation induces translations in the target language.", "labels": [], "entities": [{"text": "CFG", "start_pos": 113, "end_pos": 116, "type": "DATASET", "confidence": 0.9208518862724304}]}, {"text": "However, in contrast to syntactic parsing, where linguistic intuitions can help elucidate the \"right\" tree structure fora grammatical sentence, no such intuitions are available for synchronous derivations, and so learning the \"right\" grammars is a central challenge.", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 24, "end_pos": 41, "type": "TASK", "confidence": 0.7718672454357147}]}, {"text": "Of course, learning synchronous grammars from parallel data is a widely studied problem, inter alia).", "labels": [], "entities": []}, {"text": "However, there has been less exploration of learning rich non-terminal categories, largely because previous efforts to learn such categories have been coupled with efforts to learn derivation structures-a computationally formidable challenge.", "labels": [], "entities": []}, {"text": "One popular approach has been to derive categories from source and/or target monolingual grammars (.", "labels": [], "entities": []}, {"text": "While often successful, accurate parsers are not available in many languages: a more appealing approach is therefore to learn the category structure from the data itself.", "labels": [], "entities": []}, {"text": "In this work, we take a different approach to previous work in synchronous grammar induction by assuming that reasonable tree structures fora parallel corpus can be chosen heuristically, and then, fixing the trees (thereby enabling us to sidestep the worst of the computational issues), we learn non-terminal categories as latent variables to explain the distribution of these synchronous trees.", "labels": [], "entities": [{"text": "synchronous grammar induction", "start_pos": 63, "end_pos": 92, "type": "TASK", "confidence": 0.8631146748860677}]}, {"text": "This technique has along history in monolingual parsing (;, where it reliably yields stateof-the-art phrase structure parsers based on generative models, but we are the first to apply it to translation.", "labels": [], "entities": [{"text": "monolingual parsing", "start_pos": 36, "end_pos": 55, "type": "TASK", "confidence": 0.7176243960857391}, {"text": "phrase structure parsers", "start_pos": 101, "end_pos": 125, "type": "TASK", "confidence": 0.685171107451121}]}, {"text": "We first generalize the concept of latent PCFGs to latent-variable SCFGs ( \u00a72).", "labels": [], "entities": []}, {"text": "We then follow by a presentation of the tensor-based formulation for our parameters, a representation that makes it convenient to marginalize over latent states.", "labels": [], "entities": []}, {"text": "Subsequently, two methods for parameter estimation are presented ( \u00a74): a spectral approach based on the method of moments, and an EM-based likelihood maximization.", "labels": [], "entities": [{"text": "parameter estimation", "start_pos": 30, "end_pos": 50, "type": "TASK", "confidence": 0.6601773500442505}, {"text": "EM-based likelihood maximization", "start_pos": 131, "end_pos": 163, "type": "METRIC", "confidence": 0.6701035300890604}]}, {"text": "Results on a Chinese-English evaluation set ( \u00a75) indicate significant gains over baselines and point to the promise of using latentvariable synchronous grammars in conjunction with a smaller, simpler set of rules instead of unwieldy and bloated grammars extracted via existing heuristics, where a large number of contextindependent but un-generalizable rules are utilized.", "labels": [], "entities": []}, {"text": "Hence, the hope is that this work pro-motes the move towards translation models that directly model the conditional likelihood of translation rules via (potentially feature-rich) latentvariable models which leverage information contained in the synchronous tree structure, instead of relying on a heuristic set of features based on empirical relative frequencies ( from non-hierarchical phrase-based translation.", "labels": [], "entities": []}], "datasetContent": [{"text": "The goal of the experimental section is to evaluate the performance of the latent-variable SCFG in comparison to a baseline without any additional NT annotations (MIN-GRAMMAR), and to compare the performance of the two parameter estimation algorithms.", "labels": [], "entities": []}, {"text": "We also compare L-SCFGs to a HIERO baseline.", "labels": [], "entities": [{"text": "HIERO baseline", "start_pos": 29, "end_pos": 43, "type": "DATASET", "confidence": 0.9300698935985565}]}, {"text": "The language pair of evaluation is Chinese-English (ZH-EN).", "labels": [], "entities": []}, {"text": "We score translations using BLEU ().", "labels": [], "entities": [{"text": "translations", "start_pos": 9, "end_pos": 21, "type": "TASK", "confidence": 0.960946798324585}, {"text": "BLEU", "start_pos": 28, "end_pos": 32, "type": "METRIC", "confidence": 0.9984508752822876}]}, {"text": "The latent-variable model is integrated into the standard MT pipeline by computing marginal probabilities for each rule in the parse forest of a source sentence using the algorithm in with the parameters estimated through the algorithms in, and is added as a feature for the rule during MERT.", "labels": [], "entities": [{"text": "MT pipeline", "start_pos": 58, "end_pos": 69, "type": "TASK", "confidence": 0.8957233726978302}]}, {"text": "These probabilities are conditioned on the LHS (X), and are thus joint probabilities fora source-target RHS pair.", "labels": [], "entities": []}, {"text": "We also write out as features the conditional relative frequencies\u02c6Pfrequencies\u02c6 frequencies\u02c6P (e|f ) and\u02c6Pand\u02c6 and\u02c6P (f |e) as estimated by our latent-variable model, i.e., conditioned on the source and target RHS.", "labels": [], "entities": []}, {"text": "Overall, we find that both the spectral and the EM-based estimators improve upon a minimal grammar baseline with only a single category, but the spectral approach does better.", "labels": [], "entities": []}, {"text": "In fact, it matches the performance of the standard HI-ERO baseline, despite learning on top of a minimal grammar.", "labels": [], "entities": [{"text": "HI-ERO baseline", "start_pos": 52, "end_pos": 67, "type": "DATASET", "confidence": 0.811469316482544}]}, {"text": "For the spectral models, we tuned MERT parameters separately for each rank on a set of parameters estimated from rule indicator features only; subsequent variations within a given rank, e.g., the addition of lexical or length features or smoothing, were evaluated with the same set of rank-specific weights from MERT.", "labels": [], "entities": []}, {"text": "For EM, we ran parameter estimation with 5 randomly initialized starting points for 50 iterations; we tuned the MERT parameters with EM parameters obtained after 25 th iterations.", "labels": [], "entities": [{"text": "EM", "start_pos": 4, "end_pos": 6, "type": "TASK", "confidence": 0.8911898732185364}, {"text": "MERT", "start_pos": 112, "end_pos": 116, "type": "METRIC", "confidence": 0.87157142162323}]}, {"text": "Similar to the spectral experiments, we fixed the MERT weight values and evaluated BLEU performance with parameters after every 5 iterations and chose the iteration with the highest score on the development set.", "labels": [], "entities": [{"text": "MERT weight", "start_pos": 50, "end_pos": 61, "type": "METRIC", "confidence": 0.9731586277484894}, {"text": "BLEU", "start_pos": 83, "end_pos": 87, "type": "METRIC", "confidence": 0.9975014328956604}]}, {"text": "The results are averaged over the 5 initializations, with standard deviation in parentheses.", "labels": [], "entities": []}, {"text": "Firstly, we can see a clear dependence on rank, with peak performance for the spectral and EM models occurring at m = 16.", "labels": [], "entities": []}, {"text": "In this instance, the spectral model roughly matches the performance of the HIERO baseline, but it only uses rules extracted from a minimal grammar, whose size is a fraction of the HIERO grammar.", "labels": [], "entities": [{"text": "HIERO baseline", "start_pos": 76, "end_pos": 90, "type": "DATASET", "confidence": 0.9287397265434265}, {"text": "HIERO grammar", "start_pos": 181, "end_pos": 194, "type": "DATASET", "confidence": 0.91460120677948}]}, {"text": "The gains seem to level off at this rank; additional ranks seem to add noise to the parameters.", "labels": [], "entities": []}, {"text": "Feature-wise, additional lexical and length features add little, prob-ably because much of this information is encapsulated in the rule indicator features.", "labels": [], "entities": []}, {"text": "For EM, m = 16 outperforms the minimal grammar baseline, but is not at the level of the spectral results.", "labels": [], "entities": []}, {"text": "All EM, spectral, and MLE results are statistically significant (p < 0.01) with respect to the MIN-GRAMMAR baseline (), and the improvement over the HIERO baseline achieved by them = 16 rule indicator configuration is also statistically significant.", "labels": [], "entities": [{"text": "MLE", "start_pos": 22, "end_pos": 25, "type": "METRIC", "confidence": 0.9917119741439819}, {"text": "MIN-GRAMMAR baseline", "start_pos": 95, "end_pos": 115, "type": "DATASET", "confidence": 0.8286227583885193}, {"text": "HIERO baseline", "start_pos": 149, "end_pos": 163, "type": "DATASET", "confidence": 0.7773970067501068}]}, {"text": "The two estimation algorithms differ significantly in their estimation time.", "labels": [], "entities": []}, {"text": "Given a feature covariance matrix, the spectral algorithm (SVD, which was done with Matlab, and correlation computation steps) form = 16 took 7 minutes, while the EM algorithm took 5 minutes for each iteration with this rank.", "labels": [], "entities": []}, {"text": "presents a comparison of the nonterminal span marginals for two sentences in the development set.", "labels": [], "entities": []}, {"text": "We visualize these differences through a heat map of the CKY parse chart, where the starting word of the span is on the rows, and the span end index is on the columns.", "labels": [], "entities": [{"text": "CKY parse chart", "start_pos": 57, "end_pos": 72, "type": "DATASET", "confidence": 0.8922487696011862}]}, {"text": "Each cell is shaded to represent the marginal of that particular non-terminal span, with higher likelihoods in blue and lower likelihoods in red.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Grammar sizes for the different systems; for the  latent-variable models, effective grammar sizes are provided.", "labels": [], "entities": []}, {"text": " Table 2. For  the latent-variable models, we provide the effec- tive grammar size, where the number of NTs on  the RHS of a rule is taken into account when com- puting the grammar size, by assuming each possi- ble latent variable configuration amongst the NTs  generates a different rule. Furthermore, all single- tons are mapped to the OOV rule, while we in- clude singletons in MIN-GRAMMAR. 8 Hence, ef- fective grammar size can be computed as", "labels": [], "entities": []}, {"text": " Table 3: Results for the ZH-EN corpus, comparing across  the baselines and the two parameter estimation techniques.  RI, Lex, and Len correspond to the rule indicator, lexical,  and length features respectively, and Sm denotes smoothing.  For the EM experiments, we selected the best scoring iter- ation by tuning weights for parameters obtained after 25 it- erations and evaluating other parameters with these weights.  Results for EM are averaged over 5 starting points, with stan- dard deviation given in parentheses. Spectral, EM, and MLE  performances compared to the MIN-GRAMMAR baseline are  statistically significant (p < 0.01).", "labels": [], "entities": [{"text": "ZH-EN corpus", "start_pos": 26, "end_pos": 38, "type": "DATASET", "confidence": 0.8651845157146454}, {"text": "stan- dard deviation", "start_pos": 479, "end_pos": 499, "type": "METRIC", "confidence": 0.8082441836595535}, {"text": "MLE", "start_pos": 540, "end_pos": 543, "type": "METRIC", "confidence": 0.9822155237197876}, {"text": "MIN-GRAMMAR baseline", "start_pos": 574, "end_pos": 594, "type": "DATASET", "confidence": 0.8605710566043854}]}]}