{"title": [{"text": "Learning to Translate: A Query-Specific Combination Approach for Cross-Lingual Information Retrieval", "labels": [], "entities": [{"text": "Cross-Lingual Information Retrieval", "start_pos": 65, "end_pos": 100, "type": "TASK", "confidence": 0.7343864639600118}]}], "abstractContent": [{"text": "When documents and queries are presented in different languages, the common approach is to translate the query into the document language.", "labels": [], "entities": []}, {"text": "While there area variety of query translation approaches, recent research suggests that combining multiple methods into a single \"structured query\" is the most effective.", "labels": [], "entities": [{"text": "query translation", "start_pos": 28, "end_pos": 45, "type": "TASK", "confidence": 0.7546388506889343}]}, {"text": "In this paper , we introduce a novel approach for producing a unique combination recipe for each query, as it has also been shown that the optimal combination weights differ substantially across queries and other task specifics.", "labels": [], "entities": []}, {"text": "Our query-specific combination method generates statistically significant improvements over other combination strategies presented in the literature , such as uniform and task-specific weighting.", "labels": [], "entities": []}, {"text": "An in-depth empirical analysis presents insights about the effect of data size, domain differences, labeling and tuning on the end performance of our approach .", "labels": [], "entities": []}], "introductionContent": [{"text": "Cross-lingual information retrieval (CLIR) is a special case of information retrieval (IR) in which documents and queries are presented in different languages.", "labels": [], "entities": [{"text": "Cross-lingual information retrieval (CLIR)", "start_pos": 0, "end_pos": 42, "type": "TASK", "confidence": 0.7886535177628199}, {"text": "information retrieval (IR)", "start_pos": 64, "end_pos": 90, "type": "TASK", "confidence": 0.8802383065223693}]}, {"text": "In order to overcome the language barrier, the most commonly adopted method is to translate queries into the document language.", "labels": [], "entities": []}, {"text": "Many methods have been introduced for translating queries for CLIR, ranging from word-by-word dictionary lookups ( to sophisticated use of machine translation (MT) systems (.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 139, "end_pos": 163, "type": "TASK", "confidence": 0.8404075384140015}]}, {"text": "Previous research has shown that combining evidence from different translation approaches is superior to any single query translation method).", "labels": [], "entities": []}, {"text": "While there are numerous combination-of-evidence techniques for both mono-lingual and cross-lingual IR, recent work suggests that there is no one-size-fits-all solution.", "labels": [], "entities": [{"text": "cross-lingual IR", "start_pos": 86, "end_pos": 102, "type": "TASK", "confidence": 0.5143123418092728}]}, {"text": "In fact, the optimal combination weights (i.e., weights assigned to each piece of evidence in a linear combination) differ greatly across queries, tasks, languages, and other variants (.", "labels": [], "entities": []}, {"text": "In this paper, we introduce a novel method for learning optimal combination weights when building a linear combination of existing query translation approaches.", "labels": [], "entities": []}, {"text": "From standard query-document relevance judgments we train a set of classifiers, which produce a unique combination recipe for each query, based on a large set of features extracted from the query and collection.", "labels": [], "entities": []}, {"text": "Experimental results show that the effectiveness of our method is significantly higher than state-of-the-art query translation methods and other combination strategies.", "labels": [], "entities": [{"text": "query translation", "start_pos": 109, "end_pos": 126, "type": "TASK", "confidence": 0.7636705338954926}]}], "datasetContent": [{"text": "We evaluated our approach on four different CLIR tasks: TREC 2002 English-Arabic CLIR, NTCIR-8 English-Chinese Advanced Cross-Lingual Infor-mation Access (ACLIA), and two forum post retrieval tasks as part of the DARPA Broad Operational Language Technologies (BOLT) program: English-Arabic (BOLT ar ) and English-Chinese (BOLT ch ).", "labels": [], "entities": [{"text": "TREC 2002 English-Arabic CLIR", "start_pos": 56, "end_pos": 85, "type": "DATASET", "confidence": 0.8093376010656357}, {"text": "NTCIR-8 English-Chinese Advanced Cross-Lingual Infor-mation Access (ACLIA)", "start_pos": 87, "end_pos": 161, "type": "METRIC", "confidence": 0.6876873440212674}]}, {"text": "The query language is English in all cases, and we preprocess the queries using BBN's information extraction toolkit SERIF).", "labels": [], "entities": [{"text": "BBN", "start_pos": 80, "end_pos": 83, "type": "DATASET", "confidence": 0.932390570640564}, {"text": "information extraction toolkit SERIF", "start_pos": 86, "end_pos": 122, "type": "TASK", "confidence": 0.5640077590942383}]}, {"text": "State-of-the-art English-Arabic (EnAr) and English-Chinese (En-Ch) MT systems were trained on parallel corpora released in NIST OpenMT 2012, in addition to parallel forum data collected as part of the BOLT program (10m EnAr words; 30m En-Ch words).", "labels": [], "entities": [{"text": "MT", "start_pos": 67, "end_pos": 69, "type": "TASK", "confidence": 0.9223348498344421}, {"text": "NIST OpenMT 2012", "start_pos": 123, "end_pos": 139, "type": "DATASET", "confidence": 0.8905411760012308}, {"text": "BOLT", "start_pos": 201, "end_pos": 205, "type": "METRIC", "confidence": 0.4934745728969574}]}, {"text": "From these data, word alignments were learned with GIZA++, using five iterations of each of IBM Models 1-4 and HMM.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 17, "end_pos": 32, "type": "TASK", "confidence": 0.7737765312194824}]}, {"text": "3-gram Chinese and 5-gram Arabic Kneser-Ney language models were trained from the Gigaword corpus (1b words each) and non-English side of the training corpus.", "labels": [], "entities": [{"text": "Gigaword corpus", "start_pos": 82, "end_pos": 97, "type": "DATASET", "confidence": 0.8596937656402588}]}, {"text": "Chinese and English parallel text were preprocessed through the Treebank Tokenizer, 4 while no special treatment was performed on Arabic.", "labels": [], "entities": []}, {"text": "For retrieval, we used Indri, a state-of-theart probabilistic relevance model that supports weighted query representations through operators #combine and #weight.", "labels": [], "entities": [{"text": "Indri", "start_pos": 23, "end_pos": 28, "type": "METRIC", "confidence": 0.8553568124771118}]}, {"text": "A character-based index was built for Chinese collections, whereas Arabic text was stemmed using Lucene before indexing.", "labels": [], "entities": [{"text": "Lucene", "start_pos": 97, "end_pos": 103, "type": "DATASET", "confidence": 0.9658465385437012}]}, {"text": "English text was preprocessed by Indri's implementation of the Porter stemmer.", "labels": [], "entities": [{"text": "Porter stemmer", "start_pos": 63, "end_pos": 77, "type": "DATASET", "confidence": 0.7867994606494904}]}, {"text": "Statistics for each collection and query set are summarized in.", "labels": [], "entities": []}, {"text": "Before performing any combination, we first ran the three baseline QT methods individually and evaluated the retrieved documents.", "labels": [], "entities": []}, {"text": "Mean average precision (MAP) was used to measure retrieval effectiveness, which is a widely used and stable metric, estimating the area under the precision-recall curve.", "labels": [], "entities": [{"text": "Mean average precision (MAP)", "start_pos": 0, "end_pos": 28, "type": "METRIC", "confidence": 0.9561965664227804}, {"text": "precision-recall", "start_pos": 146, "end_pos": 162, "type": "METRIC", "confidence": 0.9885056018829346}]}, {"text": "We set n = 10 for the n-best probabilistic translation method.", "labels": [], "entities": []}, {"text": "Baseline scores are reported in.", "labels": [], "entities": [{"text": "Baseline", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9727979302406311}]}, {"text": "The average precision (AP) of each query in these tasks was used to label the query and construct training data accordingly.", "labels": [], "entities": [{"text": "precision (AP)", "start_pos": 12, "end_pos": 26, "type": "METRIC", "confidence": 0.9034679085016251}]}, {"text": "In subsequent sections, we evaluate the effect of several variants in the training pipeline.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Summary of the CLIR tasks in our evaluations.", "labels": [], "entities": [{"text": "Summary", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.7473713159561157}]}, {"text": " Table 2: Retrieval effectiveness of baseline QT methods is presented on the left side, and a comparison of  labeling strategies is provided on the right side. All numbers represent MAP values, except for classifier  accuracy shown in percentage values (in parantheses). Analytically computed values are shown in italics.", "labels": [], "entities": [{"text": "Retrieval", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9911355972290039}, {"text": "accuracy", "start_pos": 217, "end_pos": 225, "type": "METRIC", "confidence": 0.8942604064941406}]}, {"text": " Table 3: A comparison of query combination approaches. For query-specific combination, MAP and  training data are shown for the most effective experiment of each train-tune setting. For each task, the  highest MAP achieved with our approach is shown in bold. Superscripts 1, 2, and 3 indicate statisti- cally significant improvements over baseline methods one-best, probabilistic 10-best, and word-based,  whereas * indicates improvements over all three. Superscript  \u2020 indicates results significantly better than  uniform and task-specific combination methods.", "labels": [], "entities": [{"text": "MAP", "start_pos": 211, "end_pos": 214, "type": "METRIC", "confidence": 0.9699022769927979}]}]}