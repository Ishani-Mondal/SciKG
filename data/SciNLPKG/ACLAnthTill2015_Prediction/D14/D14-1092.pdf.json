{"title": [{"text": "A Joint Model for Unsupervised Chinese Word Segmentation", "labels": [], "entities": [{"text": "Unsupervised Chinese Word Segmentation", "start_pos": 18, "end_pos": 56, "type": "TASK", "confidence": 0.5754897221922874}]}], "abstractContent": [{"text": "In this paper, we propose a joint model for unsupervised Chinese word segmentation (CWS).", "labels": [], "entities": [{"text": "unsupervised Chinese word segmentation (CWS)", "start_pos": 44, "end_pos": 88, "type": "TASK", "confidence": 0.7516457864216396}]}, {"text": "Inspired by the \"products of ex-perts\" idea, our joint model firstly combines two generative models, which are word-based hierarchical Dirichlet process model and character-based hidden Markov model, by simply multiplying their probabilities together.", "labels": [], "entities": []}, {"text": "Gibbs sampling is used for model inference.", "labels": [], "entities": [{"text": "model inference", "start_pos": 27, "end_pos": 42, "type": "TASK", "confidence": 0.7803806364536285}]}, {"text": "In order to further combine the strength of goodness-based model, we then integrated nVBE into our joint model by using it to initializing the Gibbs sampler.", "labels": [], "entities": []}, {"text": "We conduct our experiments on PKU and MSRA datasets provided by the second SIGHAN bakeoff.", "labels": [], "entities": [{"text": "PKU", "start_pos": 30, "end_pos": 33, "type": "DATASET", "confidence": 0.9203185439109802}, {"text": "MSRA datasets", "start_pos": 38, "end_pos": 51, "type": "DATASET", "confidence": 0.9057309627532959}, {"text": "SIGHAN bakeoff", "start_pos": 75, "end_pos": 89, "type": "DATASET", "confidence": 0.8464206159114838}]}, {"text": "Test results on these two datasets show that the joint model achieves much better results than all of its component models.", "labels": [], "entities": []}, {"text": "Statistical significance tests also show that it is significantly better than state-of-the-art systems, achieving the highest F-scores.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 126, "end_pos": 134, "type": "METRIC", "confidence": 0.9972425699234009}]}, {"text": "Finally, analysis indicates that compared with nVBE and HDP, the joint model has a stronger ability to solve both combinational and overlapping ambiguities in Chinese word segmentation.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 159, "end_pos": 184, "type": "TASK", "confidence": 0.6432785391807556}]}], "introductionContent": [{"text": "Unlike English and many other western languages, there are no explicit word boundaries in Chinese sentences.", "labels": [], "entities": []}, {"text": "Therefore, word segmentation is a crucial first step for many Chinese language processing tasks such as syntactic parsing, information retrieval and machine translation.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 11, "end_pos": 28, "type": "TASK", "confidence": 0.7604589760303497}, {"text": "Chinese language processing", "start_pos": 62, "end_pos": 89, "type": "TASK", "confidence": 0.6014027496178945}, {"text": "syntactic parsing", "start_pos": 104, "end_pos": 121, "type": "TASK", "confidence": 0.7443152368068695}, {"text": "information retrieval", "start_pos": 123, "end_pos": 144, "type": "TASK", "confidence": 0.8130412697792053}, {"text": "machine translation", "start_pos": 149, "end_pos": 168, "type": "TASK", "confidence": 0.8091461062431335}]}, {"text": "A great deal of supervised methods have been proposed for Chinese word segmentation.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 58, "end_pos": 83, "type": "TASK", "confidence": 0.6125231484572092}]}, {"text": "While successful, they require manually labeled resources and often suffer from issues like poor domain adaptability.", "labels": [], "entities": []}, {"text": "Thus, unsupervised word segmentation methods are still attractive to researchers due to its independence on domain and manually labeled corpora.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 19, "end_pos": 36, "type": "TASK", "confidence": 0.7442163527011871}]}, {"text": "Previous unsupervised approaches to word segmentation can be roughly classified into two types.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 36, "end_pos": 53, "type": "TASK", "confidence": 0.7431013733148575}]}, {"text": "The first type uses carefully designed goodness measure to identify word candidates.", "labels": [], "entities": []}, {"text": "Popular goodness measures include description length gain (DLG)), accessor variety (AV) ), boundary entropy (BE)) and normalized variation of branching entropy (nVBE) etc.", "labels": [], "entities": [{"text": "description length gain (DLG))", "start_pos": 34, "end_pos": 64, "type": "METRIC", "confidence": 0.7978023439645767}, {"text": "accessor variety (AV)", "start_pos": 66, "end_pos": 87, "type": "METRIC", "confidence": 0.8922714352607727}, {"text": "boundary entropy (BE))", "start_pos": 91, "end_pos": 113, "type": "METRIC", "confidence": 0.7207970023155212}]}, {"text": "Goodness measure based model is not segmentation model in a very strict meaning and is actually strong in generating word list without supervision.", "labels": [], "entities": []}, {"text": "It inherently lacks capability to deal with ambiguous string, which is one of main sources of segmentation errors and has been extensively explored in supervised Chinese word segmentation.", "labels": [], "entities": [{"text": "supervised Chinese word segmentation", "start_pos": 151, "end_pos": 187, "type": "TASK", "confidence": 0.6106305867433548}]}, {"text": "The second type focuses on designing sophisticated statistical model, usually nonparametric Bayesian models, to find the segmentation with highest posterior probability, given the observed character sequences.", "labels": [], "entities": []}, {"text": "Typical statistical models includes Hierarchical Dirichlet process (HDP) model), Nested PitmanYor process (NPY) model () etc, which are actually nonparametric language models and therefor can be categorized as word-based model.", "labels": [], "entities": []}, {"text": "Word-based model makes decision on wordhood of a candidate character sequence mainly based on information outside the sequence, namely, the wordhood of character sequences being adjacent to the concerned sequence.", "labels": [], "entities": []}, {"text": "Inspired by the success of character-based model in supervised word segmentation, we propose a Bayesian HMM model for unsupervised Chinese word segmentation.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 63, "end_pos": 80, "type": "TASK", "confidence": 0.7263022512197495}, {"text": "Chinese word segmentation", "start_pos": 131, "end_pos": 156, "type": "TASK", "confidence": 0.6378444333871206}]}, {"text": "With the Bayesian HMM model, we formulate the unsupervised segmentation tasks as procedure of tagging positional tags to characters.", "labels": [], "entities": [{"text": "tagging positional tags", "start_pos": 94, "end_pos": 117, "type": "TASK", "confidence": 0.8571193814277649}]}, {"text": "Different from word-based model, character-based model like HMM-based model as we propose make decisions on wordhood of a candidate character sequence based on information inside the sequence, namely, ability of characters to form words.", "labels": [], "entities": []}, {"text": "Although the Bayesian HMM model alone does not produce competitive results, it contributes substantially to the joint model as proposed in this paper.", "labels": [], "entities": []}, {"text": "Our joint model takes advantage from three different models: namely, a character-based model (HMM-based), a word-based model (HDP-based) and a goodness measure based model (nVBE model).", "labels": [], "entities": []}, {"text": "The combination of HDP-based model and HMM-based model enables to utilize information of both word-level and character-level.", "labels": [], "entities": []}, {"text": "We also show that using nVBE model as initialization model could further improve the performance to outperform the state-of-the-art systems and leads to improvement in both wordhood judgment and disambiguation ability.", "labels": [], "entities": [{"text": "wordhood judgment", "start_pos": 173, "end_pos": 190, "type": "TASK", "confidence": 0.8038464784622192}]}, {"text": "Word segmentation systems are usually evaluated with metrics like precision, recall and FScore, regardless of supervised or unsupervised.", "labels": [], "entities": [{"text": "Word segmentation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.6997529715299606}, {"text": "precision", "start_pos": 66, "end_pos": 75, "type": "METRIC", "confidence": 0.99937504529953}, {"text": "recall", "start_pos": 77, "end_pos": 83, "type": "METRIC", "confidence": 0.9990631937980652}, {"text": "FScore", "start_pos": 88, "end_pos": 94, "type": "METRIC", "confidence": 0.9981665015220642}]}, {"text": "Following normal practice, we evaluate our model and compare it with state-of-the-art systems using F-Score.", "labels": [], "entities": [{"text": "F-Score", "start_pos": 100, "end_pos": 107, "type": "METRIC", "confidence": 0.9205177426338196}]}, {"text": "However, we argue that the ability to solve segmentation ambiguities is also important when evaluating different types of unsupervised word segmentation systems.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 135, "end_pos": 152, "type": "TASK", "confidence": 0.7466424405574799}]}, {"text": "This paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we will introduce several related systems for unsupervised word segmentation.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 73, "end_pos": 90, "type": "TASK", "confidence": 0.7218828648328781}]}, {"text": "Then our joint model is presented in Section 3.", "labels": [], "entities": []}, {"text": "Section 4 shows our experiment results on the benchmark datasets and Section 5 concludes the paper.", "labels": [], "entities": [{"text": "benchmark datasets", "start_pos": 46, "end_pos": 64, "type": "DATASET", "confidence": 0.6510805636644363}]}], "datasetContent": [{"text": "In this section, we test our joint model on PKU and MSRA datesets provided by the Second Segmentation Bake-off).", "labels": [], "entities": [{"text": "PKU", "start_pos": 44, "end_pos": 47, "type": "DATASET", "confidence": 0.833442747592926}, {"text": "MSRA datesets", "start_pos": 52, "end_pos": 65, "type": "DATASET", "confidence": 0.8287025094032288}, {"text": "Second Segmentation Bake-off", "start_pos": 82, "end_pos": 110, "type": "DATASET", "confidence": 0.573599765698115}]}, {"text": "Most previous works reported their results on these two datasets, this will make it convenient to directly compare our joint model with theirs.", "labels": [], "entities": []}, {"text": "\u2022 nVBE: the model based on Variation of Branching Entropy in.", "labels": [], "entities": []}, {"text": "We re-implement their model on setting 3 1 . \u2022 HDP: the HDP-based model proposed by, initialized randomly.", "labels": [], "entities": []}, {"text": "\u2022 HDP+HMM: the model combining HDPbased model and HMM-based model as proposed in Section 3, initialized randomly.", "labels": [], "entities": []}, {"text": "\u2022 HDP+nVBE: the HDP-based model, initialized with the results of nVBE model.", "labels": [], "entities": []}, {"text": "\u2022 Joint: the \"HDP+HMM\" model initialized with nVBE model.", "labels": [], "entities": []}, {"text": "\u2022 ESA: the model proposed in, as mentioned above, the conducted experiments on four different settings, we report their results on setting 3.", "labels": [], "entities": [{"text": "ESA", "start_pos": 2, "end_pos": 5, "type": "METRIC", "confidence": 0.886719822883606}]}, {"text": "\u2022 NPY(2): the 2-gram language model presented by.", "labels": [], "entities": []}, {"text": "\u2022 NPY(3): the 3-gram language model presented by.", "labels": [], "entities": []}, {"text": "For all of our Gibbs samplers, we run 5 times to get the averaged F-Scores.", "labels": [], "entities": [{"text": "F-Scores", "start_pos": 66, "end_pos": 74, "type": "METRIC", "confidence": 0.9764545559883118}]}, {"text": "We also give the variance of the F-Scores in.", "labels": [], "entities": [{"text": "variance", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.9943682551383972}, {"text": "F-Scores", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.9960727691650391}]}, {"text": "For each run, we find that random initialization takes around 1,000 iterations to converge, while initialing with nVBE only takes as few as 10 iterations.", "labels": [], "entities": []}, {"text": "This makes: Experiment results and comparison to state-of-the-art systems.", "labels": [], "entities": []}, {"text": "The figures in parentheses denote the variance the of F-Scores.", "labels": [], "entities": [{"text": "F-Scores", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.9751465916633606}]}, {"text": "our joint model very efficient and possible to work in practical applications as well.", "labels": [], "entities": []}, {"text": "At last, a single sample (the last one) is used for evaluation.", "labels": [], "entities": []}, {"text": "From, we can see that the joint model (Joint) outperforms all the presented systems in F-Score on all testing corpora.", "labels": [], "entities": [{"text": "F-Score", "start_pos": 87, "end_pos": 94, "type": "METRIC", "confidence": 0.9263316988945007}]}, {"text": "Specifically, comparing \"HDP+HMM\" with \"HDP\", the former model increases the overall F-Score from 68.7% to 75.3% (+6.6%) in PKU corpora and from 69.9% to 76.3% (+6.4%) in MSRA corpora, which proves that the character information in the HMM-based model can actually enhance the performance of the HDP-based model.", "labels": [], "entities": [{"text": "F-Score", "start_pos": 85, "end_pos": 92, "type": "METRIC", "confidence": 0.9994248151779175}, {"text": "MSRA corpora", "start_pos": 171, "end_pos": 183, "type": "DATASET", "confidence": 0.9034805595874786}]}, {"text": "Comparing \"HDP+nVBE\" with \"HDP\", the former model also increases the overall F-Score by 10.6%/9.6% in PKU/MSRA corpora, which demonstrates that initializing the HDP-based model with nVBE will improve the performance by a large margin.", "labels": [], "entities": [{"text": "F-Score", "start_pos": 77, "end_pos": 84, "type": "METRIC", "confidence": 0.999119222164154}, {"text": "PKU/MSRA corpora", "start_pos": 102, "end_pos": 118, "type": "DATASET", "confidence": 0.800520196557045}]}, {"text": "Finally, the joint model \"Joint\" take advantage from both from the character-based HMM model and the nVBE model, it achieves a F-Score of 81.1% on PKU and 81.7% on MSRA.", "labels": [], "entities": [{"text": "F-Score", "start_pos": 127, "end_pos": 134, "type": "METRIC", "confidence": 0.9995330572128296}, {"text": "PKU", "start_pos": 147, "end_pos": 150, "type": "DATASET", "confidence": 0.8937411308288574}, {"text": "MSRA", "start_pos": 164, "end_pos": 168, "type": "DATASET", "confidence": 0.9484075903892517}]}, {"text": "This result outperforms all its component baselines such as \"HDP\", \"HDP+HMM\" and \"HDP+nVBE\".", "labels": [], "entities": []}, {"text": "Our joint model also shows competitive advantages over several state-of-the-art systems.", "labels": [], "entities": []}, {"text": "Compared with nVBE,the F-Score increases by 3.2% on PKU corpora and by 3.5% on MSRA corpora.", "labels": [], "entities": [{"text": "nVBE,the", "start_pos": 14, "end_pos": 22, "type": "DATASET", "confidence": 0.7171450853347778}, {"text": "F-Score", "start_pos": 23, "end_pos": 30, "type": "METRIC", "confidence": 0.9818493723869324}, {"text": "MSRA corpora", "start_pos": 79, "end_pos": 91, "type": "DATASET", "confidence": 0.9027450978755951}]}, {"text": "Compared with ESA, the F-Score increases by 3.7%/3.3% in PKU/MSRA corpora.", "labels": [], "entities": [{"text": "ESA", "start_pos": 14, "end_pos": 17, "type": "DATASET", "confidence": 0.5458862781524658}, {"text": "F-Score", "start_pos": 23, "end_pos": 30, "type": "METRIC", "confidence": 0.9963854551315308}, {"text": "PKU/MSRA corpora", "start_pos": 57, "end_pos": 73, "type": "DATASET", "confidence": 0.7509745061397552}]}, {"text": "Lastly, compared to the nonparametric Bayesian models (NPY(n)), our joint model still increases the FScore by 1.5% (NPY) and 1.0% (NPY(3)) on MSRA corpora.", "labels": [], "entities": [{"text": "FScore", "start_pos": 100, "end_pos": 106, "type": "METRIC", "confidence": 0.9863748550415039}]}, {"text": "Moreover, compared with the empirical topline figure 84.8%, our joint model achieves a pretty close F-Score.", "labels": [], "entities": [{"text": "F-Score", "start_pos": 100, "end_pos": 107, "type": "METRIC", "confidence": 0.9980589747428894}]}, {"text": "The differences are 3.7% on PKU corpora and 3.1% on MSRA corpora.", "labels": [], "entities": [{"text": "PKU", "start_pos": 28, "end_pos": 31, "type": "DATASET", "confidence": 0.8259103894233704}, {"text": "MSRA corpora", "start_pos": 52, "end_pos": 64, "type": "DATASET", "confidence": 0.9037822484970093}]}, {"text": "An phenomenon we should pay attention to is the poor performance of the HMM-based model.", "labels": [], "entities": []}, {"text": "With our implementation of the Bayesian HMM, we achieves a 34.3% F-Score on PKU corpora and a 34.9% F-Score on MSRA corpora, just slightly better than random segmentation.", "labels": [], "entities": [{"text": "F-Score", "start_pos": 65, "end_pos": 72, "type": "METRIC", "confidence": 0.9982680082321167}, {"text": "F-Score", "start_pos": 100, "end_pos": 107, "type": "METRIC", "confidence": 0.9971350431442261}]}, {"text": "The result show that the hidden Markov Model alone is not suitable for character-based Chinese word segmentation problem.", "labels": [], "entities": [{"text": "character-based Chinese word segmentation", "start_pos": 71, "end_pos": 112, "type": "TASK", "confidence": 0.5335807800292969}]}, {"text": "However, it still substantially contributes to the joint model.", "labels": [], "entities": []}, {"text": "We find that the variance of the results are rather small, this shows the stability of our Gibbs samplers.", "labels": [], "entities": []}, {"text": "From the segmentation results generated by the joint model, we also found that quite a large amount of errors it made are related to dates, numbers (both Chinese and English) and English words.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 9, "end_pos": 21, "type": "TASK", "confidence": 0.9686514735221863}]}, {"text": "This problem can be easily addressed during preprocessing by considering encoding information as previous work, and we believe this will bring us much better performance.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Experiment results and comparison to state-of-the-art systems. The figures in parentheses denote  the variance the of F-Scores.", "labels": [], "entities": [{"text": "F-Scores", "start_pos": 128, "end_pos": 136, "type": "METRIC", "confidence": 0.9820542335510254}]}, {"text": " Table 3: Statistics of combinational ambiguity.  This table shows the total number of mistakes  made by different systems at combinational am- biguous strings. The numbers in parentheses de- note the total number of combinational ambiguous  strings.", "labels": [], "entities": [{"text": "combinational ambiguity", "start_pos": 24, "end_pos": 47, "type": "TASK", "confidence": 0.9197864234447479}]}, {"text": " Table 4: Statistics of overlapping ambiguity. This  table shows the total number of mistakes made  by different systems at overlapping ambiguous  strings. The numbers in parentheses denote the to- tal number of overlapping ambiguous strings.", "labels": [], "entities": []}]}