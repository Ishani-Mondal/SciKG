{"title": [], "abstractContent": [{"text": "We describe anew dependency parser for English tweets, TWEEBOPARSER.", "labels": [], "entities": [{"text": "TWEEBOPARSER", "start_pos": 55, "end_pos": 67, "type": "METRIC", "confidence": 0.6014655232429504}]}, {"text": "The parser builds on several contributions: new syntactic annotations fora corpus of tweets (TWEEBANK), with conventions informed by the domain; adaptations to a statistical parsing algorithm; and anew approach to exploiting out-of-domain Penn Treebank data.", "labels": [], "entities": [{"text": "TWEEBANK", "start_pos": 93, "end_pos": 101, "type": "METRIC", "confidence": 0.8306241631507874}, {"text": "Penn Treebank data", "start_pos": 239, "end_pos": 257, "type": "DATASET", "confidence": 0.9716382225354513}]}, {"text": "Our experiments show that the parser achieves over 80% unlabeled attachment accuracy on our new, high-quality test set and measure the benefit of our contributions.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 76, "end_pos": 84, "type": "METRIC", "confidence": 0.8509308099746704}]}, {"text": "Our dataset and parser can be found at", "labels": [], "entities": []}], "introductionContent": [{"text": "In contrast to the edited, standardized language of traditional publications such as news reports, social media text closely represents language as it is used by people in their everyday lives.", "labels": [], "entities": []}, {"text": "These informal texts, which account forever larger proportions of written content, are of considerable interest to researchers, with applications such as sentiment analysis ().", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 154, "end_pos": 172, "type": "TASK", "confidence": 0.9650518894195557}]}, {"text": "However, their often nonstandard content makes them challenging for traditional NLP tools.", "labels": [], "entities": []}, {"text": "Among the tools currently available for tweets area POS tagger () and a named entity recognizer ()-but not a parser.", "labels": [], "entities": [{"text": "POS tagger", "start_pos": 52, "end_pos": 62, "type": "TASK", "confidence": 0.58930703997612}]}, {"text": "Important steps have been taken.", "labels": [], "entities": []}, {"text": "The English Web Treebank () represents an annotation effort on web text-which likely lies somewhere between newspaper text and social media messages in formality and care of editing-that was sufficient to support a shared task.", "labels": [], "entities": [{"text": "English Web Treebank", "start_pos": 4, "end_pos": 24, "type": "DATASET", "confidence": 0.9284282128016154}]}, {"text": "annotated a small test set of tweets to evaluate parsers trained on the Penn Treebank (, augmented using semi-supervision and in-domain data.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 72, "end_pos": 85, "type": "DATASET", "confidence": 0.9957923591136932}]}, {"text": "Others, such as, have used existing Penn Treebank-trained models on tweets.", "labels": [], "entities": [{"text": "Penn Treebank-trained", "start_pos": 36, "end_pos": 57, "type": "DATASET", "confidence": 0.9857872426509857}]}, {"text": "In this work, we argue that the Penn Treebank approach to annotation-while well-matched to edited genres like newswire-is poorly suited to more informal genres.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 32, "end_pos": 45, "type": "DATASET", "confidence": 0.9798581004142761}]}, {"text": "Our starting point is that rapid, small-scale annotation efforts performed by imperfectly-trained annotators should provide enough evidence to train an effective parser.", "labels": [], "entities": []}, {"text": "We see this starting point as a necessity, given observations about the rapidly changing nature of tweets, the attested difficulties of domain adaptation for parsing (, and the expense of creating Penn Treebank-style annotations.", "labels": [], "entities": [{"text": "parsing", "start_pos": 158, "end_pos": 165, "type": "TASK", "confidence": 0.9570708274841309}, {"text": "Penn Treebank-style annotations", "start_pos": 197, "end_pos": 228, "type": "DATASET", "confidence": 0.9718586405118307}]}, {"text": "This paper presents TWEEBOPARSER, the first syntactic dependency parser designed explicitly for English tweets.", "labels": [], "entities": [{"text": "TWEEBOPARSER", "start_pos": 20, "end_pos": 32, "type": "METRIC", "confidence": 0.8481414318084717}]}, {"text": "We developed this parser following current best practices in empirical NLP: we annotate a corpus (TWEEBANK) and train the parameters of a statistical parsing algorithm.", "labels": [], "entities": [{"text": "TWEEBANK", "start_pos": 98, "end_pos": 106, "type": "METRIC", "confidence": 0.8218482732772827}]}, {"text": "Our research contributions include: \u2022 a survey of key challenges posed by syntactic analysis of tweets (by humans or machines) and decisions motivated by those challenges and by our limited annotation-resource scenario ( \u00a72); \u2022 our annotation process and quantitative measures of the quality of the annotations ( \u00a73); \u2022 adaptations to a statistical dependency parsing algorithm to make it fully compatible with the above, and also to exploit information from outof-domain data cheaply and without a strong commitment ( \u00a74); and \u2022 an experimental analysis of the parser's unlabeled attachment accuracy-which surpasses 80%-and contributions of various important components ( \u00a75).", "labels": [], "entities": [{"text": "statistical dependency parsing", "start_pos": 337, "end_pos": 367, "type": "TASK", "confidence": 0.6612112919489542}, {"text": "accuracy-which", "start_pos": 592, "end_pos": 606, "type": "METRIC", "confidence": 0.67336106300354}]}, {"text": "The dataset and parser can be found at http://www.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our experiments quantify the contributions of various components of our approach.", "labels": [], "entities": []}, {"text": "We consider the direct use of TRAIN-PTB instead of TRAIN-NEW.", "labels": [], "entities": [{"text": "TRAIN-PTB", "start_pos": 30, "end_pos": 39, "type": "METRIC", "confidence": 0.921402096748352}, {"text": "TRAIN-NEW", "start_pos": 51, "end_pos": 60, "type": "METRIC", "confidence": 0.8587865233421326}]}, {"text": "test sets, with various options.", "labels": [], "entities": []}, {"text": "\"Baseline\" is offthe-shelf second-order TurboParser.", "labels": [], "entities": []}, {"text": "We consider augmenting it with Brown cluster features ( \u00a74.3; \"+ Brown\") and then also with the parsing adaptations of \u00a74.2 (\"+ Brown & PA\").", "labels": [], "entities": []}, {"text": "Another choice is whether to modify the POS tags attest time; the modified version (\"mod.", "labels": [], "entities": []}, {"text": "POS\") maps at-mentions to pronoun, and hashtags and URLs to noun.", "labels": [], "entities": []}, {"text": "We note that comparing these scores to our main parser ( \u00a75.3) conflates three very important independent variables: the amount of training data (39,832 Penn Treebank sentences vs. 1,473 Twitter utterances), the annotation method, and the source of the data.", "labels": [], "entities": [{"text": "Penn Treebank sentences", "start_pos": 153, "end_pos": 176, "type": "DATASET", "confidence": 0.97628386815389}]}, {"text": "However, we are encouraged that, on what we believe is the superior test set (TEST-NEW), our overall approach obtains a 7.8% gain with an order of magnitude less annotated data.", "labels": [], "entities": [{"text": "TEST-NEW", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.9650865793228149}]}, {"text": "(second block, italicized) shows the performance of the main parser on both test sets with gold-standard and automatic POS tagging and token selection.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 119, "end_pos": 130, "type": "TASK", "confidence": 0.7307980954647064}]}, {"text": "On TEST-NEW, with either goldstandard POS tags or gold-standard token selection, performance increases by 1.1%; with both, it increases by 2.3%.", "labels": [], "entities": []}, {"text": "On TEST-FOSTER, token selection matters much less, but POS tagging accounts fora drop of more than 6%.", "labels": [], "entities": [{"text": "TEST-FOSTER", "start_pos": 3, "end_pos": 14, "type": "DATASET", "confidence": 0.6262422800064087}, {"text": "token selection", "start_pos": 16, "end_pos": 31, "type": "TASK", "confidence": 0.9426954388618469}, {"text": "POS tagging", "start_pos": 55, "end_pos": 66, "type": "TASK", "confidence": 0.7305869162082672}]}, {"text": "This is consistent with Foster et al.'s finding: using a fine-grained Penn Treebank-trained POS tagger (achieving around 84% accuracy on Twitter), they saw 5-8% improvement in unlabeled dependency attachment accuracy using gold-standard POS tags.", "labels": [], "entities": [{"text": "Penn Treebank-trained POS tagger", "start_pos": 70, "end_pos": 102, "type": "DATASET", "confidence": 0.9344048649072647}, {"text": "accuracy", "start_pos": 125, "end_pos": 133, "type": "METRIC", "confidence": 0.9824919700622559}, {"text": "accuracy", "start_pos": 208, "end_pos": 216, "type": "METRIC", "confidence": 0.8681244850158691}]}, {"text": "We ablated each key element of our main parser-PTB features, Brown features, second order features and decoding, and the parsing adaptations of  Baselines: Second order 76.5 70.4 First order 76.1 70.4: Effects of gold-standard POS tagging and token selection (TS; \u00a75.5) and of feature ablation ( \u00a75.6).", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 227, "end_pos": 238, "type": "TASK", "confidence": 0.694313645362854}, {"text": "token selection", "start_pos": 243, "end_pos": 258, "type": "TASK", "confidence": 0.7666090428829193}]}, {"text": "The \"baselines\" are TurboParser without the parsing adaptations in \u00a74.2 and without Penn Treebank or Brown features.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 84, "end_pos": 97, "type": "DATASET", "confidence": 0.967999666929245}]}, {"text": "The best result in each column is bolded.", "labels": [], "entities": []}, {"text": "\u00a74.2-as well as each pair of these.", "labels": [], "entities": []}, {"text": "These conditions use automatic POS tags and token selection.", "labels": [], "entities": [{"text": "token selection", "start_pos": 44, "end_pos": 59, "type": "TASK", "confidence": 0.786072164773941}]}, {"text": "The \"\u2212 PA\" condition, which ablates parsing adaptations, is accomplished by deleting punctuation (in training and test data) and parsing using TurboParser's existing algorithm.", "labels": [], "entities": [{"text": "PA\" condition", "start_pos": 7, "end_pos": 20, "type": "METRIC", "confidence": 0.9645876884460449}, {"text": "parsing", "start_pos": 36, "end_pos": 43, "type": "TASK", "confidence": 0.9698426127433777}]}, {"text": "Further results with first-and second-order TurboParsers are plotted in.", "labels": [], "entities": []}, {"text": "Notably, a 2-3% gain is obtained by modifying the parsing algorithm, and our stackinginspired use of Penn Treebank data contributes in both cases, quite a lot on TEST-FOSTER (unsurprisingly given that test set's similarity to the Penn Treebank).", "labels": [], "entities": [{"text": "parsing", "start_pos": 50, "end_pos": 57, "type": "TASK", "confidence": 0.9671245217323303}, {"text": "Penn Treebank data", "start_pos": 101, "end_pos": 119, "type": "DATASET", "confidence": 0.9934864640235901}, {"text": "TEST-FOSTER", "start_pos": 162, "end_pos": 173, "type": "METRIC", "confidence": 0.7077609300613403}, {"text": "Penn Treebank", "start_pos": 230, "end_pos": 243, "type": "DATASET", "confidence": 0.9957487881183624}]}, {"text": "More surprisingly, we find that Brown cluster features do not consistently improve performance, at least not as instantiated here, with our small training set.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of our datasets. (A tweet with k annotations  in the training set is counted k times for the totals of tokens,  utterances, etc.).  \u2020 TEST-FOSTER contains 250 manually split  sentences. The number of tweets should be smaller but is not  recoverable from the data release.", "labels": [], "entities": [{"text": "TEST-FOSTER", "start_pos": 155, "end_pos": 166, "type": "METRIC", "confidence": 0.8210713863372803}]}, {"text": " Table 2: Performance of second-order TurboParser trained on  TRAIN-PTB, with various preprocessing options. The main  parser ( \u00a75.3) achieves 80.9% and 76.1% on the two test sets,  respectively; see  \u00a75.4 for discussion.", "labels": [], "entities": [{"text": "TRAIN-PTB", "start_pos": 62, "end_pos": 71, "type": "METRIC", "confidence": 0.8389643430709839}]}, {"text": " Table 3: Effects of gold-standard POS tagging and token  selection (TS;  \u00a75.5) and of feature ablation ( \u00a75.6). The \"base- lines\" are TurboParser without the parsing adaptations in  \u00a74.2  and without Penn Treebank or Brown features. The best result  in each column is bolded. See also", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 35, "end_pos": 46, "type": "TASK", "confidence": 0.7199090719223022}, {"text": "token  selection", "start_pos": 51, "end_pos": 67, "type": "TASK", "confidence": 0.7870388329029083}, {"text": "Penn Treebank", "start_pos": 201, "end_pos": 214, "type": "DATASET", "confidence": 0.9836229085922241}]}]}