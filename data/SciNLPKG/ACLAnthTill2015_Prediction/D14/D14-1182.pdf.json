{"title": [{"text": "Sometimes Average is Best: The Importance of Averaging for Prediction using MCMC Inference in Topic Modeling", "labels": [], "entities": [{"text": "Average", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.958454430103302}]}], "abstractContent": [{"text": "Markov chain Monte Carlo (MCMC) approximates the posterior distribution of latent variable models by generating many samples and averaging over them.", "labels": [], "entities": []}, {"text": "In practice, however, it is often more convenient to cut corners, using only a single sample or following a suboptimal averaging strategy.", "labels": [], "entities": []}, {"text": "We systematically study different strategies for averaging MCMC samples and show empirically that averaging properly leads to significant improvements in prediction.", "labels": [], "entities": []}], "introductionContent": [{"text": "Probabilistic topic models are powerful methods to uncover hidden thematic structures in text by projecting each document into a low dimensional space spanned by a set of topics, each of which is a distribution over words.", "labels": [], "entities": []}, {"text": "Topic models such as latent Dirichlet allocation ( and its extensions discover these topics from text, which allows for effective exploration, analysis, and summarization of the otherwise unstructured corpora.", "labels": [], "entities": [{"text": "latent Dirichlet allocation", "start_pos": 21, "end_pos": 48, "type": "TASK", "confidence": 0.5429292023181915}, {"text": "summarization", "start_pos": 157, "end_pos": 170, "type": "TASK", "confidence": 0.9803967475891113}]}, {"text": "In addition to exploratory data analysis, atypical goal of topic models is prediction.", "labels": [], "entities": []}, {"text": "Given a set of unannotated training data, unsupervised topic models try to learn good topics that can generalize to unseen text.", "labels": [], "entities": []}, {"text": "Supervised topic models jointly capture both the text and associated metadata such as a continuous response variable, single label or multiple labels () to predict metadata from text.", "labels": [], "entities": []}, {"text": "Probabilistic topic modeling requires estimating the posterior distribution.", "labels": [], "entities": []}, {"text": "Exact computation of the posterior is often intractable, which motivates approximate inference techniques.", "labels": [], "entities": []}, {"text": "One popular approach is Markov chain Monte Carlo (MCMC), a class of inference algorithms to approximate the target posterior distribution.", "labels": [], "entities": []}, {"text": "To make prediction, MCMC algorithms generate samples on training data to estimate corpus-level latent variables, and use them to generate samples to estimate document-level latent variables for test data.", "labels": [], "entities": []}, {"text": "The underlying theory requires averaging on both training and test samples, but in practice it is often convenient to cut corners: either skip averaging entirely by using just the values of the last sample or use a single training sample and average over test samples.", "labels": [], "entities": []}, {"text": "We systematically study non-averaging and averaging strategies when performing predictions using MCMC in topic modeling (Section 2).", "labels": [], "entities": []}, {"text": "Using popular unsupervised (LDA in Section 3) and supervised (SLDA in Section 4) topic models via thorough experimentation, we show empirically that cutting corners on averaging leads to consistently poorer prediction.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}