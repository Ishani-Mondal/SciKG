{"title": [{"text": "Improving Pivot-Based Statistical Machine Translation by Pivoting the Co-occurrence Count of Phrase Pairs", "labels": [], "entities": [{"text": "Improving Pivot-Based Statistical Machine Translation", "start_pos": 0, "end_pos": 53, "type": "TASK", "confidence": 0.8405699491500854}]}], "abstractContent": [{"text": "To overcome the scarceness of bilingual corpora for some language pairs in machine translation, pivot-based SMT uses pivot language as a \"bridge\" to generate source-target translation from source-pivot and pivot-target translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 75, "end_pos": 94, "type": "TASK", "confidence": 0.7213702946901321}, {"text": "SMT", "start_pos": 108, "end_pos": 111, "type": "TASK", "confidence": 0.7141061425209045}]}, {"text": "One of the key issues is to estimate the probabilities for the generated phrase pairs.", "labels": [], "entities": []}, {"text": "In this paper, we present a novel approach to calculate the translation probability by pivoting the co-occurrence count of source-pivot and pivot-target phrase pairs.", "labels": [], "entities": [{"text": "translation", "start_pos": 60, "end_pos": 71, "type": "TASK", "confidence": 0.9304574728012085}]}, {"text": "Experimental results on Europarl data and web data show that our method leads to significant improvements over the baseline systems.", "labels": [], "entities": [{"text": "Europarl data", "start_pos": 24, "end_pos": 37, "type": "DATASET", "confidence": 0.9926444292068481}]}], "introductionContent": [{"text": "Statistical Machine Translation (SMT) relies on large bilingual parallel data to produce high quality translation results.", "labels": [], "entities": [{"text": "Statistical Machine Translation (SMT)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.8413851062456766}]}, {"text": "Unfortunately, for some language pairs, large bilingual corpora are not readily available.", "labels": [], "entities": []}, {"text": "To alleviate the parallel data scarceness, a conventional solution is to introduce a \"bridge\" language (named pivot language) to connect the source and target language (de, where there are large amounts of source-pivot and pivot-target parallel corpora.", "labels": [], "entities": []}, {"text": "Among various pivot-based approaches, the triangulation method () is a representative work in pivot-based machine translation.", "labels": [], "entities": [{"text": "pivot-based machine translation", "start_pos": 94, "end_pos": 125, "type": "TASK", "confidence": 0.6407619019349416}]}, {"text": "The approach proposes to build a source-target phrase table by merging the source-pivot and pivot-target phrase table.", "labels": [], "entities": []}, {"text": "One of the key issues in this method is to estimate the translation probabilities for the generated source-target phrase pairs.", "labels": [], "entities": []}, {"text": "Conventionally, the probabilities are estimated by multiplying the posterior probabilities of source-pivot and pivottarget phrase pairs.", "labels": [], "entities": []}, {"text": "However, it has been shown that the generated probabilities are not accurate enough ().", "labels": [], "entities": []}, {"text": "One possible reason may lie in the non-uniformity of the probability space.", "labels": [], "entities": []}, {"text": "(a), we can see that the probability distributions of source-pivot and pivot-target language are calculated separately, and the source-target probability distributions are induced from the source-pivot and pivot-target probability distributions.", "labels": [], "entities": []}, {"text": "Because of the absence of the pivot language (e.g., p2 is in source-pivot probability space but not in pivot-target one), the induced source-target probability distribution is not complete, which will result in inaccurate probabilities.", "labels": [], "entities": []}, {"text": "To solve this problem, we propose a novel approach that utilizes the co-occurrence count of source-target phrase pairs to estimate phrase translation probabilities more precisely.", "labels": [], "entities": [{"text": "phrase translation probabilities", "start_pos": 131, "end_pos": 163, "type": "TASK", "confidence": 0.7454462150732676}]}, {"text": "Different from the triangulation method, which merges the source-pivot and pivot-target phrase pairs after training the translation model, we propose to merge the source-pivot and pivot-target phrase pairs immediately after the phrase extraction step, and estimate the co-occurrence count of the source-pivot-target phrase pairs.", "labels": [], "entities": [{"text": "phrase extraction", "start_pos": 228, "end_pos": 245, "type": "TASK", "confidence": 0.7151205986738205}]}, {"text": "Finally, we compute the translation probabilities according to the estimated co-occurrence counts, using the standard training method in phrase-based SMT (.", "labels": [], "entities": [{"text": "phrase-based SMT", "start_pos": 137, "end_pos": 153, "type": "TASK", "confidence": 0.5927454233169556}]}, {"text": "(b) shows, the source-target probability distributions are calculated in a complete probability space.", "labels": [], "entities": []}, {"text": "Thus, it will be more accurate than the traditional triangulation method.", "labels": [], "entities": []}, {"text": "(a) and (b) show the difference between the triangulation method and our co-occurrence count method.", "labels": [], "entities": []}, {"text": "Furthermore, it is common that a small standard bilingual corpus can be available between the source and target language.", "labels": [], "entities": []}, {"text": "The direct translation model trained with the standard bilingual corpus exceeds in translation performance, but its weakness lies in low phrase coverage.", "labels": [], "entities": [{"text": "direct translation", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.7242334485054016}]}, {"text": "However, the pivot model has characteristics characters.", "labels": [], "entities": []}, {"text": "Thus, it is important to combine the direct and pivot translation model to compensate mutually and further improve the translation performance.", "labels": [], "entities": []}, {"text": "To deal with this problem, we propose a mixed model by merging the phrase pairs extracted by pivot-based method and the phrase pairs extracted from the standard bilingual corpus.", "labels": [], "entities": []}, {"text": "Note that, this is different from the conventional interpolation method, which interpolates the direct and pivot translation model.", "labels": [], "entities": []}, {"text": "(b) and (c) for further illustration.", "labels": [], "entities": []}, {"text": "The remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we describe the related work.", "labels": [], "entities": []}, {"text": "We introduce the co-occurrence count method in Section 3, and the mixed model in Section 4.", "labels": [], "entities": []}, {"text": "In Section 5 and Section 6, we describe and analyze the experiments.", "labels": [], "entities": []}, {"text": "Section 7 gives a conclusion of the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our first experiment is carried out on Europarl 1 corpus, which is a multi-lingual corpus including 21 European languages (.", "labels": [], "entities": [{"text": "Europarl 1 corpus", "start_pos": 39, "end_pos": 56, "type": "DATASET", "confidence": 0.9579341808954874}]}, {"text": "In our work, we perform translations among French (fr), German (de) and Spanish (es).", "labels": [], "entities": []}, {"text": "Due to the richness of available language resources, we choose English (en) as the pivot language.", "labels": [], "entities": []}, {"text": "summarized the statistics of training data.", "labels": [], "entities": []}, {"text": "For the language model, the same monolingual data extracted from the Europarl are used.", "labels": [], "entities": [{"text": "Europarl", "start_pos": 69, "end_pos": 77, "type": "DATASET", "confidence": 0.9830354452133179}]}, {"text": "The word alignment is obtained by GIZA++) and the heuristics \"growdiag-final\" refinement rule ().", "labels": [], "entities": [{"text": "word alignment", "start_pos": 4, "end_pos": 18, "type": "TASK", "confidence": 0.6344598084688187}]}, {"text": "Our translation system is an in-house phrasebased system analogous to Moses ().", "labels": [], "entities": []}, {"text": "The baseline system is the triangulation method (, including an interpolated model which linearly interpolate the direct and pivot translation model.", "labels": [], "entities": []}, {"text": "We use WMT08 2 as our test data, which contains 2000 in-domain sentences and 2051 out-ofdomain sentences with single reference.", "labels": [], "entities": [{"text": "WMT08 2", "start_pos": 7, "end_pos": 14, "type": "DATASET", "confidence": 0.8735631108283997}]}, {"text": "The translation results are evaluated by caseinsensitive BLEU-4 metric ().", "labels": [], "entities": [{"text": "BLEU-4", "start_pos": 57, "end_pos": 63, "type": "METRIC", "confidence": 0.9935930371284485}]}, {"text": "The statistical significance tests using 95% confidence interval are measured with paired bootstrap resampling).", "labels": [], "entities": []}, {"text": "The experimental on Europarl is artificial, as the training data for directly translating between source and target language actually exists in the original data sets.", "labels": [], "entities": [{"text": "Europarl", "start_pos": 20, "end_pos": 28, "type": "DATASET", "confidence": 0.9882650375366211}]}, {"text": "Thus, we conducted several experiments on a more realistic scenario: translating Chinese (zh) to Japanese (jp) via English (en) with web crawled data.", "labels": [], "entities": []}, {"text": "As mentioned in Section 3.1, the source-pivot and pivot-target parallel corpora can be imbalanced in quantities.", "labels": [], "entities": []}, {"text": "If one parallel corpus was much larger than another, then minimum heuristic function would likely just take the counts from the smaller corpus.", "labels": [], "entities": []}, {"text": "In order to analyze this issue, we manually setup imbalanced corpora.", "labels": [], "entities": []}, {"text": "For source-pivot parallel corpora, we randomly select 1M, 2M, 3M, 4M and 5M Chinese-English sentence pairs.", "labels": [], "entities": []}, {"text": "On the other hand, we randomly select 1M EnglishJapanese sentence pairs as pivot-target parallel corpora.", "labels": [], "entities": []}, {"text": "The training data of Chinese-English and English-Japanese language pairs are summarized in.", "labels": [], "entities": []}, {"text": "For the Chinese-Japanese direct corpus, we randomly select 5K, 10K, 20K, 30K, 40K, 50K, 60K, 70K, 80K, 90K and 100K sentence pairs to simulate the lack of bilingual data.", "labels": [], "entities": []}, {"text": "We built a 1K in-house test set with four references.", "labels": [], "entities": []}, {"text": "For Japanese language model training, we used the monolingual part of English-Japanese corpus.", "labels": [], "entities": []}, {"text": "shows the results of different cooccurrence count merging methods.", "labels": [], "entities": []}, {"text": "First, the minimum method and the geometric mean method outperform the other two merging methods and the baseline system with different training corpus.", "labels": [], "entities": []}, {"text": "When the scale of source-pivot and pivot-target corpus is roughly balanced (zh-en-jp-1), the minimum method achieves an absolute improvement of 2.06 percentages points on BLEU over the baseline, which is also better than the other merging methods.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 171, "end_pos": 175, "type": "METRIC", "confidence": 0.9942600727081299}]}, {"text": "While, with the growth of source-pivot corpus, the gap between sourcepivot corpus and pivot-target corpus becomes bigger.", "labels": [], "entities": []}, {"text": "In this circumstance, the geometric mean method becomes better than the minimum method.", "labels": [], "entities": []}, {"text": "Compared to the minimum method, the geometric mean method considers both the sourcepivot and the pivot-target corpus, which may lead to a better result in the case of imbalanced training corpus.", "labels": [], "entities": []}, {"text": "Comparison of different merging methods on the imbalanced web data.", "labels": [], "entities": []}, {"text": "( zh-en-jp-1 means the translation system is trained with zh-en-1 as source-pivot corpus and en-jp as pivot-target corpus, and soon.", "labels": [], "entities": []}, {"text": ") Furthermore, with the imbalanced corpus zhen-jp-5, we compared the translation performance of our co-occurrence count model (with geometric mean merging method), triangulation model, interpolated model, mixed model and the direct translation models.", "labels": [], "entities": []}, {"text": "The co-occurrence count model can achieve an absolute improvement of 2.54 percentages points on BLEU over the baseline.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 96, "end_pos": 100, "type": "METRIC", "confidence": 0.9981709718704224}]}, {"text": "The triangulation method outperforms the direct translation when only 5K sentence pairs are available.", "labels": [], "entities": []}, {"text": "Meanwhile, the number is 10K when using the co-occurrence count method.", "labels": [], "entities": []}, {"text": "The co-occurrence count models interpolated with the direct model significantly outperform the other models.", "labels": [], "entities": []}, {"text": "In this experiment, the training data contains parallel sentences on various domains.", "labels": [], "entities": []}, {"text": "And the training corpora (Chinese-English and EnglishJapanese) are typically very different, since they are obtained on the web.", "labels": [], "entities": []}, {"text": "It indicates that our cooccurrence count method is robust in the realistic scenario.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Training data of Europarl corpus", "labels": [], "entities": [{"text": "Europarl corpus", "start_pos": 27, "end_pos": 42, "type": "DATASET", "confidence": 0.9197311699390411}]}, {"text": " Table 3: Comparison of different merging methods on out-of-domain test set.", "labels": [], "entities": []}, {"text": " Table 4: Training data of web corpus", "labels": [], "entities": [{"text": "Training data of web corpus", "start_pos": 10, "end_pos": 37, "type": "DATASET", "confidence": 0.6901426434516906}]}]}