{"title": [{"text": "Refining Word Segmentation Using a Manually Aligned Corpus for Statistical Machine Translation", "labels": [], "entities": [{"text": "Refining Word Segmentation", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.9019022186597189}, {"text": "Statistical Machine Translation", "start_pos": 63, "end_pos": 94, "type": "TASK", "confidence": 0.80751633644104}]}], "abstractContent": [{"text": "Languages that have no explicit word de-limiters often have to be segmented for statistical machine translation (SMT).", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 80, "end_pos": 117, "type": "TASK", "confidence": 0.8025279442469279}]}, {"text": "This is commonly performed by automated seg-menters trained on manually annotated corpora.", "labels": [], "entities": []}, {"text": "However, the word segmentation (WS) schemes of these annotated corpora are handcrafted for general usage, and may not be suitable for SMT.", "labels": [], "entities": [{"text": "word segmentation (WS)", "start_pos": 13, "end_pos": 35, "type": "TASK", "confidence": 0.8378004372119904}, {"text": "SMT", "start_pos": 134, "end_pos": 137, "type": "TASK", "confidence": 0.9957507848739624}]}, {"text": "An analysis was performed to test this hypothesis using a manually annotated word alignment (WA) corpus for Chinese-English SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 124, "end_pos": 127, "type": "TASK", "confidence": 0.6681495308876038}]}, {"text": "An analysis revealed that 74.60% of the sentences in the WA corpus if segmented using an automated segmenter trained on the Penn Chinese Treebank (CTB) will contain conflicts with the gold WA annotations.", "labels": [], "entities": [{"text": "WA corpus", "start_pos": 57, "end_pos": 66, "type": "DATASET", "confidence": 0.8507060706615448}, {"text": "Penn Chinese Treebank (CTB)", "start_pos": 124, "end_pos": 151, "type": "DATASET", "confidence": 0.9760607779026031}]}, {"text": "We formulated an approach based on word splitting with reference to the annotated WA to alleviate these conflicts.", "labels": [], "entities": [{"text": "word splitting", "start_pos": 35, "end_pos": 49, "type": "TASK", "confidence": 0.7314111292362213}]}, {"text": "Experimental results show that the refined WS reduced word alignment error rate by 6.82% and achieved the highest BLEU improvement (0.63 on average) on the Chinese-English open machine translation (OpenMT) corpora compared to related work.", "labels": [], "entities": [{"text": "WS", "start_pos": 43, "end_pos": 45, "type": "TASK", "confidence": 0.9353489875793457}, {"text": "word alignment error rate", "start_pos": 54, "end_pos": 79, "type": "TASK", "confidence": 0.723477192223072}, {"text": "BLEU", "start_pos": 114, "end_pos": 118, "type": "METRIC", "confidence": 0.999453604221344}, {"text": "Chinese-English open machine translation (OpenMT)", "start_pos": 156, "end_pos": 205, "type": "TASK", "confidence": 0.7072610088757106}]}], "introductionContent": [{"text": "Word segmentation is a prerequisite for many natural language processing (NLP) applications on those languages that have no explicit space between words, such as Arabic, Chinese and Japanese.", "labels": [], "entities": [{"text": "Word segmentation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7153628766536713}]}, {"text": "As the first processing step, WS affects all successive steps, thus it has a large potential impact on the final performance.", "labels": [], "entities": [{"text": "WS", "start_pos": 30, "end_pos": 32, "type": "TASK", "confidence": 0.9712920784950256}]}, {"text": "For SMT, the unsupervised WA, building translation models and reordering models, and decoding are all based on segmented words.", "labels": [], "entities": [{"text": "SMT", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.9941602945327759}]}, {"text": "Automated word segmenters built through supervised-learning methods, after decades of intensive research, have emerged as effective solutions to WS tasks and become widely used in many NLP applications.", "labels": [], "entities": [{"text": "word segmenters", "start_pos": 10, "end_pos": 25, "type": "TASK", "confidence": 0.7428654432296753}, {"text": "WS tasks", "start_pos": 145, "end_pos": 153, "type": "TASK", "confidence": 0.9369806349277496}]}, {"text": "For example, the Stanford word segmenter () 1 which is based on conditional random field (CRF) is employed to prepare the official corpus for NTCIR-9 Chinese-English patent translation task).", "labels": [], "entities": [{"text": "word segmenter", "start_pos": 26, "end_pos": 40, "type": "TASK", "confidence": 0.6414420455694199}, {"text": "conditional random field (CRF)", "start_pos": 64, "end_pos": 94, "type": "METRIC", "confidence": 0.7283933162689209}, {"text": "NTCIR-9 Chinese-English patent translation task", "start_pos": 142, "end_pos": 189, "type": "TASK", "confidence": 0.8226520538330078}]}, {"text": "However, one problem with applying these supervised-learning word segmenters to SMT is that the WS scheme of annotating the training corpus may not be optimal for SMT.", "labels": [], "entities": [{"text": "supervised-learning word segmenters", "start_pos": 41, "end_pos": 76, "type": "TASK", "confidence": 0.6901190479596456}, {"text": "SMT", "start_pos": 80, "end_pos": 83, "type": "TASK", "confidence": 0.9825744032859802}, {"text": "SMT", "start_pos": 163, "end_pos": 166, "type": "TASK", "confidence": 0.99333655834198}]}, {"text": "( noticed that the words in CTB are often too long for SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 55, "end_pos": 58, "type": "TASK", "confidence": 0.9925588369369507}]}, {"text": "For example, a full Chinese personal name which consists of a family name and a given name is always taken as a single word, but its counterpart in English is usually two words.", "labels": [], "entities": []}, {"text": "Manually WA corpora are precious resources for SMT research, but they used to be only available in small volumes due to the production cost.", "labels": [], "entities": [{"text": "SMT research", "start_pos": 47, "end_pos": 59, "type": "TASK", "confidence": 0.9844095408916473}]}, {"text": "For example,) initially annotated 447 English-French sentence pairs, which later became the test data set in ACL 2003 shared task on word alignment, and was used frequently thereafter ( For Chinese and English, the shortage of manually WA corpora has recently been relieved by the linguistic data consortium (LDC) 2 GALE Chinese-English word alignment and tagging training corpus (the GALE WA corpus) . The corpus is considerably large, containing 4,735 documents, 18,507 sentence pairs, 620,189 Chinese tokens, 518,137 English words, and 421,763 alignment annotations.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 133, "end_pos": 147, "type": "TASK", "confidence": 0.7391979247331619}, {"text": "GALE Chinese-English word alignment and tagging training", "start_pos": 316, "end_pos": 372, "type": "TASK", "confidence": 0.6989500011716571}, {"text": "GALE WA corpus", "start_pos": 385, "end_pos": 399, "type": "DATASET", "confidence": 0.7636054754257202}]}, {"text": "The corpus carries no Chinese WS annotation, and the WA annotation was performed between Chinese characters and English words.", "labels": [], "entities": [{"text": "WA", "start_pos": 53, "end_pos": 55, "type": "METRIC", "confidence": 0.6068796515464783}]}, {"text": "The alignment identifies minimum translation units and relations , referred as atomic blocks and atomic edges, respectively, in this paper.", "labels": [], "entities": []}, {"text": "shows an example that contains six atomic edges.", "labels": [], "entities": []}, {"text": "Visual inspection of the segmentation of an automatic segmenter with reference to a WA corpus revealed a number of inconsistencies.", "labels": [], "entities": [{"text": "WA corpus", "start_pos": 84, "end_pos": 93, "type": "DATASET", "confidence": 0.821330338716507}]}, {"text": "For example, consider the word \"bao fa\" in.", "labels": [], "entities": []}, {"text": "Empirically we observed that this word is segmented as a single token by an automatic segmenter trained on the CTB, however, this segmentation differs with the alignment in the WA corpus, since its two components are aligned to two different English words.", "labels": [], "entities": [{"text": "WA corpus", "start_pos": 177, "end_pos": 186, "type": "DATASET", "confidence": 0.7907745540142059}]}, {"text": "Our hypothesis was that the removal of these inconsistencies would benefit machine translation performance (this is explained further in Section 2.3), and we explored this idea in this work.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 75, "end_pos": 94, "type": "TASK", "confidence": 0.7369944155216217}]}, {"text": "This paper focuses on optimizing Chinese WS for Chinese-English SMT, but both the research method and the proposed solution are languageindependent.", "labels": [], "entities": [{"text": "SMT", "start_pos": 64, "end_pos": 67, "type": "TASK", "confidence": 0.6482870578765869}]}, {"text": "They can be applied to other language pairs.", "labels": [], "entities": []}, {"text": "The major contributions of this paper include, \u2022 analyze the CTB WS scheme for ChineseEnglish SMT; \u2022 propose a lexical word splitter to refine the WS; \u2022 achieve a BLEU improvement over a baseline Stanford word segmenter, and a state-of-theart extension, on Chinese-English OpenMT corpora.", "labels": [], "entities": [{"text": "CTB WS scheme", "start_pos": 61, "end_pos": 74, "type": "DATASET", "confidence": 0.8946809569994608}, {"text": "ChineseEnglish SMT", "start_pos": 79, "end_pos": 97, "type": "TASK", "confidence": 0.5121243745088577}, {"text": "word splitter", "start_pos": 119, "end_pos": 132, "type": "TASK", "confidence": 0.7531503438949585}, {"text": "BLEU", "start_pos": 163, "end_pos": 167, "type": "METRIC", "confidence": 0.9990851879119873}]}, {"text": "The rest of this paper is organized as follows: first, Section 2 analyzes WS using a WA corpus; next, Section 3 proposes a lexical word splitter to refine WS; then, Section 4 evaluates the proposed method on end-to-end SMT as well as word segmentation and alignment; after that, Section 5 compares this work to related work; finally, Section 6 concludes this paper.", "labels": [], "entities": [{"text": "WS", "start_pos": 74, "end_pos": 76, "type": "TASK", "confidence": 0.8900250196456909}, {"text": "word splitter", "start_pos": 131, "end_pos": 144, "type": "TASK", "confidence": 0.7538440227508545}, {"text": "SMT", "start_pos": 219, "end_pos": 222, "type": "TASK", "confidence": 0.9462798237800598}, {"text": "word segmentation", "start_pos": 234, "end_pos": 251, "type": "TASK", "confidence": 0.7450970113277435}]}], "datasetContent": [{"text": "In the last section we found that 9.26% of words produced by the CTB segmenter have the potential to cause problems for SMT, and propose a lexical word splitter to address this issue through segmentation refinement.", "labels": [], "entities": [{"text": "SMT", "start_pos": 120, "end_pos": 123, "type": "TASK", "confidence": 0.9971931576728821}, {"text": "word splitter", "start_pos": 147, "end_pos": 160, "type": "TASK", "confidence": 0.7583461701869965}, {"text": "segmentation refinement", "start_pos": 191, "end_pos": 214, "type": "TASK", "confidence": 0.9472863078117371}]}, {"text": "This section contains experiments designed to empirically evaluate the proposed lexical word splitter in three aspects: first, whether the WS accuracy is improved; second, whether the accuracy of the unsupervised WA during training SMT systems is improved; third, whether the end-to-end translation quality is improved.", "labels": [], "entities": [{"text": "word splitter", "start_pos": 88, "end_pos": 101, "type": "TASK", "confidence": 0.7304832637310028}, {"text": "WS", "start_pos": 139, "end_pos": 141, "type": "TASK", "confidence": 0.8562741875648499}, {"text": "accuracy", "start_pos": 142, "end_pos": 150, "type": "METRIC", "confidence": 0.7997896075248718}, {"text": "accuracy", "start_pos": 184, "end_pos": 192, "type": "METRIC", "confidence": 0.9981168508529663}, {"text": "SMT", "start_pos": 232, "end_pos": 235, "type": "TASK", "confidence": 0.9329153895378113}]}, {"text": "This section first describes the experimental methodology, then presents the experimental results, and finally illustrates the operation of our proposed method using areal example.", "labels": [], "entities": []}, {"text": "The GALE manual WA corpus and the Chinese to English corpus from the shared task of the NIST open machine translation (OpenMT) 2006 evaluation 6 were employed as the experimental corpus (.", "labels": [], "entities": [{"text": "GALE manual WA corpus", "start_pos": 4, "end_pos": 25, "type": "DATASET", "confidence": 0.8983196318149567}, {"text": "NIST open machine translation (OpenMT) 2006 evaluation 6", "start_pos": 88, "end_pos": 144, "type": "DATASET", "confidence": 0.7602789402008057}]}, {"text": "The experimental corpus for WS was constructed by first segmenting 2000 held out sentences from the GALE manual WA corpus with the Stanford segmenter, and then refining the segmentation with the gold alignment annotation.", "labels": [], "entities": [{"text": "WS", "start_pos": 28, "end_pos": 30, "type": "TASK", "confidence": 0.9772573709487915}, {"text": "GALE manual WA corpus", "start_pos": 100, "end_pos": 121, "type": "DATASET", "confidence": 0.905561551451683}]}, {"text": "For example, the gold segmentation for the examples in is presented in.", "labels": [], "entities": []}, {"text": "Note that this test corpus is intended to represent an oracle segmentation for our proposed method, and serves primarily to gauge the improvement of our method over the baseline Stanford segmenter, relative to an upper bound.", "labels": [], "entities": []}, {"text": "The experimental corpus for unsupervised WA was the union set of the NIST OpenMT training set and the 2000 test sentence pairs from GALE WA corpus.", "labels": [], "entities": [{"text": "NIST OpenMT training set", "start_pos": 69, "end_pos": 93, "type": "DATASET", "confidence": 0.9099978804588318}, {"text": "GALE WA corpus", "start_pos": 132, "end_pos": 146, "type": "DATASET", "confidence": 0.9079516927401224}]}, {"text": "We removed the United Nations corpus from the NIST OpenMT constraint training resources because it is out of domain.", "labels": [], "entities": [{"text": "United Nations corpus", "start_pos": 15, "end_pos": 36, "type": "DATASET", "confidence": 0.896982212861379}, {"text": "NIST OpenMT constraint training resources", "start_pos": 46, "end_pos": 87, "type": "DATASET", "confidence": 0.8766411304473877}]}, {"text": "The main result of this paper is the evaluation of the end-to-end performance of an SMT system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 84, "end_pos": 87, "type": "TASK", "confidence": 0.9943009614944458}]}, {"text": "The experimental corpus for this task was the NIST OpenMT corpus.", "labels": [], "entities": [{"text": "NIST OpenMT corpus", "start_pos": 46, "end_pos": 64, "type": "DATASET", "confidence": 0.8537738919258118}]}, {"text": "The data set of the NIST evaluation 2002 was used as a development set for MERT tuning, and the remaining data sets of the NIST evaluation from 2003 to 2006 were used as test sets.", "labels": [], "entities": [{"text": "NIST evaluation 2002", "start_pos": 20, "end_pos": 40, "type": "DATASET", "confidence": 0.9570935964584351}, {"text": "MERT tuning", "start_pos": 75, "end_pos": 86, "type": "TASK", "confidence": 0.9351424872875214}, {"text": "NIST evaluation", "start_pos": 123, "end_pos": 138, "type": "DATASET", "confidence": 0.9753449857234955}]}, {"text": "The English sentences were tokenized by Stanford toolkit 7 and converted to lowercase.", "labels": [], "entities": [{"text": "Stanford toolkit 7", "start_pos": 40, "end_pos": 58, "type": "DATASET", "confidence": 0.9132476449012756}]}, {"text": "The performance of WS was measured by precision, recall and F 1 of gold words, The performance of unsupervised WA in the SMT training procedure was measured through alignment error rate (AER)).", "labels": [], "entities": [{"text": "WS", "start_pos": 19, "end_pos": 21, "type": "TASK", "confidence": 0.9375322461128235}, {"text": "precision", "start_pos": 38, "end_pos": 47, "type": "METRIC", "confidence": 0.9995712637901306}, {"text": "recall", "start_pos": 49, "end_pos": 55, "type": "METRIC", "confidence": 0.9994910955429077}, {"text": "F 1", "start_pos": 60, "end_pos": 63, "type": "METRIC", "confidence": 0.9908882677555084}, {"text": "SMT training", "start_pos": 121, "end_pos": 133, "type": "TASK", "confidence": 0.9280517399311066}, {"text": "alignment error rate (AER))", "start_pos": 165, "end_pos": 192, "type": "METRIC", "confidence": 0.945460687081019}]}, {"text": "Sure alignment edges and possible alignment edges were not distinguished in this paper as no such tags are found in GALE manual WA corpus.", "labels": [], "entities": [{"text": "GALE manual WA corpus", "start_pos": 116, "end_pos": 137, "type": "DATASET", "confidence": 0.9471011459827423}]}, {"text": "The performance of SMT was measured using BLEU ().", "labels": [], "entities": [{"text": "SMT", "start_pos": 19, "end_pos": 22, "type": "TASK", "confidence": 0.9952443242073059}, {"text": "BLEU", "start_pos": 42, "end_pos": 46, "type": "METRIC", "confidence": 0.9983079433441162}]}], "tableCaptions": [{"text": " Table 1: GALE WA corpus.  \u2020 Sentences rejected by the annotators are excluded.", "labels": [], "entities": [{"text": "GALE WA corpus", "start_pos": 10, "end_pos": 24, "type": "DATASET", "confidence": 0.5871191124121348}]}, {"text": " Table 2: CTB WS on GALE WA corpus:  \u2020 All words are fully consistent;  \u2021 Alignment inconsistent plus  alignment inconsistent & extraction hindered", "labels": [], "entities": [{"text": "CTB WS", "start_pos": 10, "end_pos": 16, "type": "TASK", "confidence": 0.40153250098228455}, {"text": "GALE WA corpus", "start_pos": 20, "end_pos": 34, "type": "DATASET", "confidence": 0.6937691569328308}, {"text": "extraction", "start_pos": 128, "end_pos": 138, "type": "METRIC", "confidence": 0.9567063450813293}]}, {"text": " Table 3: Data set for learning the word splitting", "labels": [], "entities": []}, {"text": " Table 5: NIST Open machine translation 2006  Corpora.  \u2020 Number of sentence samples which  contain one Chinese sentence and four English ref- erence sentences.", "labels": [], "entities": [{"text": "NIST Open machine translation 2006  Corpora", "start_pos": 10, "end_pos": 53, "type": "DATASET", "confidence": 0.80205766359965}]}, {"text": " Table 6: Performance of WS", "labels": [], "entities": [{"text": "WS", "start_pos": 25, "end_pos": 27, "type": "TASK", "confidence": 0.832304060459137}]}, {"text": " Table 7: Performance of unsupervised WA using  different WS strategies", "labels": [], "entities": []}]}