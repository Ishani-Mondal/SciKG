{"title": [{"text": "Exact Decoding for Phrase-Based Statistical Machine Translation", "labels": [], "entities": [{"text": "Exact Decoding", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7934218645095825}, {"text": "Phrase-Based Statistical Machine Translation", "start_pos": 19, "end_pos": 63, "type": "TASK", "confidence": 0.7926129251718521}]}], "abstractContent": [{"text": "The combinatorial space of translation derivations in phrase-based statistical machine translation is given by the intersection between a translation lattice and a target language model.", "labels": [], "entities": [{"text": "translation derivations in phrase-based statistical machine translation", "start_pos": 27, "end_pos": 98, "type": "TASK", "confidence": 0.6294192033154624}]}, {"text": "We replace this intractable intersection by a tractable relaxation which incorporates a low-order up-perbound on the language model.", "labels": [], "entities": []}, {"text": "Exact optimisation is achieved through a coarse-to-fine strategy with connections to adap-tive rejection sampling.", "labels": [], "entities": []}, {"text": "We perform exact optimisation with unpruned language models of order 3 to 5 and show search-error curves for beam search and cube pruning on standard test sets.", "labels": [], "entities": [{"text": "beam search", "start_pos": 109, "end_pos": 120, "type": "TASK", "confidence": 0.7796710133552551}]}, {"text": "This is the first work to tractably tackle exact opti-misation with language models of orders higher than 3.", "labels": [], "entities": []}], "introductionContent": [{"text": "In Statistical Machine Translation (SMT), the task of producing a translation for an input string x = x 1 , x 2 , . .", "labels": [], "entities": [{"text": "Statistical Machine Translation (SMT)", "start_pos": 3, "end_pos": 40, "type": "TASK", "confidence": 0.8700477580229441}]}, {"text": ", x I is typically associated with finding the best derivation d * compatible with the input under a linear model.", "labels": [], "entities": []}, {"text": "In this view, a derivation is a structured output that represents a sequence of steps that covers the input producing a translation.", "labels": [], "entities": []}, {"text": "Equation 1 illustrates this decoding process.", "labels": [], "entities": []}, {"text": "The set D(x) is the space of all derivations compatible with x and supported by a model of translational equivalences.", "labels": [], "entities": []}, {"text": "The function f (d) = \u039b \u00b7 H(d) is a linear parameterisation of the model.", "labels": [], "entities": []}, {"text": "It assigns a real-valued score (or weight) to every derivation d \u2208 D(x), where \u039b \u2208 R m assigns a relative importance to different aspects of the derivation independently captured by m feature functions The fully parameterised model can be seen as a discrete weighted set such that feature functions factorise over the steps in a derivation.", "labels": [], "entities": []}, {"text": "That is, H k (d) = e\u2208d h k (e), where h k is a (local) feature function that assesses steps independently and d = e 1 , e 2 , . .", "labels": [], "entities": []}, {"text": ", e l is a sequence of l steps.", "labels": [], "entities": []}, {"text": "Under this assumption, each step is assigned the weight w(e) = \u039b\u00b7h 1 (e), h 2 (e), . .", "labels": [], "entities": []}, {"text": ", h m (e).", "labels": [], "entities": []}, {"text": "The set Dis typically finite, however, it contains a very large number of structures -exponential (or even factorial, see \u00a72) with the size of x -making exhaustive enumeration prohibitively slow.", "labels": [], "entities": []}, {"text": "Only in very restricted cases combinatorial optimisation techniques are directly applicable (), thus it is common to resort to heuristic techniques in order to find an approximation to d * (.", "labels": [], "entities": []}, {"text": "Evaluation exercises indicate that approximate search algorithms work well in practice ().", "labels": [], "entities": []}, {"text": "The most popular algorithms provide solutions with unbounded error, thus precisely quantifying their performance requires the development of a tractable exact decoder.", "labels": [], "entities": []}, {"text": "To date, most attempts were limited to short sentences and/or somewhat toy models trained with artificially small datasets ().", "labels": [], "entities": []}, {"text": "Other work has employed less common approximations to the model reducing its search space complexity ().", "labels": [], "entities": []}, {"text": "These do not answer whether or not current decoding algorithms perform well at real translation tasks with state-of-the-art models.", "labels": [], "entities": []}, {"text": "We propose an exact decoder for phrase-based SMT based on a coarse-to-fine search strategy ( . Ina nutshell, we relax the decoding problem with respect to the Language Model (LM) component.", "labels": [], "entities": [{"text": "SMT", "start_pos": 45, "end_pos": 48, "type": "TASK", "confidence": 0.6485052704811096}]}, {"text": "This coarse view is incrementally refined based on evidence col-lected via maximisation.", "labels": [], "entities": []}, {"text": "A refinement increases the complexity of the model only slightly, hence dynamic programming remains feasible throughout the search until convergence.", "labels": [], "entities": []}, {"text": "We test our decoding strategy with realistic models using standard data sets.", "labels": [], "entities": []}, {"text": "We also contribute with optimum derivations which can be used to assess future improvements to approximate decoders.", "labels": [], "entities": []}, {"text": "In the remaining sections we present the general model ( \u00a72), survey contributions to exact optimisation ( \u00a73), formalise our novel approach ( \u00a74), present experiments ( \u00a75) and conclude ( \u00a76).", "labels": [], "entities": [{"text": "conclude", "start_pos": 178, "end_pos": 186, "type": "METRIC", "confidence": 0.9588645100593567}]}], "datasetContent": [{"text": "We used the dataset made available by the Workshop on Statistical Machine Translation (WMT) () to train a German-English phrase-based system using the Moses toolkit () in a standard setup.", "labels": [], "entities": [{"text": "Statistical Machine Translation (WMT)", "start_pos": 54, "end_pos": 91, "type": "TASK", "confidence": 0.7758538176616033}]}, {"text": "For phrase extraction, we used both Europarl ( and News Commentaries (NC) totalling about 2.2M sentences.", "labels": [], "entities": [{"text": "phrase extraction", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.8826238214969635}, {"text": "Europarl", "start_pos": 36, "end_pos": 44, "type": "DATASET", "confidence": 0.977875828742981}, {"text": "News Commentaries (NC)", "start_pos": 51, "end_pos": 73, "type": "DATASET", "confidence": 0.812429141998291}]}, {"text": "For language modelling, in addition to the monolingual parts of Europarl Pre-processing: tokenisation, truecasing and automatic compound-splitting (German only).", "labels": [], "entities": [{"text": "language modelling", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.6493685692548752}, {"text": "Europarl", "start_pos": 64, "end_pos": 72, "type": "DATASET", "confidence": 0.9603348970413208}]}, {"text": "Following, we set the maximum phrase length to 5. and NC, we added News-2013 totalling about 25M sentences.", "labels": [], "entities": [{"text": "NC", "start_pos": 54, "end_pos": 56, "type": "METRIC", "confidence": 0.7475343942642212}, {"text": "News-2013", "start_pos": 67, "end_pos": 76, "type": "DATASET", "confidence": 0.9505335688591003}]}, {"text": "We performed language model interpolation and batch-mira tuning (Cherry and Foster, 2012) using newstest2010 (2,849 sentence pairs).", "labels": [], "entities": [{"text": "batch-mira tuning", "start_pos": 46, "end_pos": 63, "type": "TASK", "confidence": 0.7011934071779251}]}, {"text": "For tuning we used cube pruning with a large beam size (k = 5000) and a distortion limit d = 4.", "labels": [], "entities": [{"text": "distortion limit d", "start_pos": 72, "end_pos": 90, "type": "METRIC", "confidence": 0.9457248250643412}]}, {"text": "Unpruned language models were trained using lmplz) which employs modified Kneser-Ney smoothing.", "labels": [], "entities": []}, {"text": "We report results on newstest2012.", "labels": [], "entities": [{"text": "newstest2012", "start_pos": 21, "end_pos": 33, "type": "DATASET", "confidence": 0.982383131980896}]}, {"text": "Our exact decoder produces optimal translation derivations for all the 3,003 sentences in the test set.", "labels": [], "entities": []}, {"text": "summarises the performance of our novel decoder for language models of order n = 3 ton = 5.", "labels": [], "entities": []}, {"text": "For 3-gram LMs we also varied the distortion limit d (from 4 to 6).", "labels": [], "entities": [{"text": "distortion limit d", "start_pos": 34, "end_pos": 52, "type": "METRIC", "confidence": 0.9663147926330566}]}, {"text": "We report the average time (in seconds) to build the initial proposal, the total run time of the algorithm, the number of iterations N before convergence, and the size of the hypergraph in the end of the search (in thousands of nodes and thousands of edges , total decoding time including build, number of iterations (N), and number of nodes and edges (in thousands) at the end of the search.", "labels": [], "entities": []}, {"text": "It is insightful to understand how different aspects of the initial proposal impact on performance.", "labels": [], "entities": []}, {"text": "Increasing the translation option limit (tol) leads tog (0) having more edges (this dependency is linear with tol).", "labels": [], "entities": []}, {"text": "In this case, the number of nodes is only minimally affected -due to the possibility of a few new segmentations.", "labels": [], "entities": []}, {"text": "The maximum phrase length (mpl) introduces in g (0) more configurations of reordering constraints ([l, C] in).", "labels": [], "entities": []}, {"text": "However, not many more, due to C being limited by the distortion limit d.", "labels": [], "entities": [{"text": "distortion", "start_pos": 54, "end_pos": 64, "type": "METRIC", "confidence": 0.971396803855896}]}, {"text": "In practice, we observe little impact on time performance.", "labels": [], "entities": []}, {"text": "Increasing d introduces many more permutations of the input leading to exponentially many more nodes and edges.", "labels": [], "entities": []}, {"text": "Increasing the order n of the LM has no impact on g (0) and its impact on the overall search is expressed in terms of a higher number of nodes being locally intersected.", "labels": [], "entities": []}, {"text": "An increased hypergraph, be it due to additional nodes or additional edges, necessarily leads to slower iterations because at each iteration we must compute inside weights in time O(|V |+|E|).", "labels": [], "entities": []}, {"text": "The number of nodes has the larger impact on the number of iterations.", "labels": [], "entities": []}, {"text": "OS * is very efficient in ignoring hypotheses (edges) that cannot compete for an optimum.", "labels": [], "entities": []}, {"text": "For instance, we observe that running time depends linearly on tol only through the computation of inside weights, while the number of iterations is only minimally affected.", "labels": [], "entities": []}, {"text": "An in-|E0| = 178 with d = 6.", "labels": [], "entities": []}, {"text": "Observe the exponential dependency on distortion limit, which also leads to exponentially longer running times.", "labels": [], "entities": [{"text": "distortion", "start_pos": 38, "end_pos": 48, "type": "METRIC", "confidence": 0.9792354702949524}]}, {"text": "It is possible to reduce the size of the hypergraph throughout the search using the upperbound on the search error g * \u2212 f * to prune hypotheses that surely do not stand a chance of competing for the optimum ().", "labels": [], "entities": []}, {"text": "Another direction is to group edges connecting the same nonterminal nodes into one partial edge () -this is particularly convenient due to our method only visiting the 1-best derivation from g(d) at each iteration.", "labels": [], "entities": []}, {"text": "creased LM order, fora fixed distortion limit, impacts much more on the number of iterations than on the average running time of a single iteration.", "labels": [], "entities": []}, {"text": "Fixing d = 4, the average time per iteration is 0.1 (n = 3), 0.13 (n = 4) and 0.18 (n = 5).", "labels": [], "entities": []}, {"text": "Fixing a 3-gram LM, we observe 0.1 (d = 4), 0.17 (d = 5) and 0.31 (d = 6).", "labels": [], "entities": []}, {"text": "Note the exponential growth of the latter, due to a proposal encoding exponentially many more permutations.", "labels": [], "entities": []}, {"text": "shows the average degree of refinement of the nodes in the final proposal.", "labels": [], "entities": []}, {"text": "Nodes are shown by level of refinement, where m indicates that they store m words in their carry.", "labels": [], "entities": []}, {"text": "The table also shows the number of unique m-grams ever incorporated to the proposal.", "labels": [], "entities": []}, {"text": "This table illustrates well how our decoding algorithm moves from a coarse upperbound where every node stores an empty string to a variable-order representation which is sufficient to prove an optimum derivation.", "labels": [], "entities": []}, {"text": "In our approach a complete derivation is optimised from the proxy model at each iteration.", "labels": [], "entities": []}, {"text": "We observe that over 99% of these derivations project onto distinct strings.", "labels": [], "entities": []}, {"text": "In addition, while the optimum solution maybe found early in the search, a certificate of optimality requires refining the proxy until convergence (see \u00a74.1).", "labels": [], "entities": []}, {"text": "It turns out that most of the solutions are first encountered as late as in the last 6-10% of the iterations.", "labels": [], "entities": []}, {"text": "We use the optimum derivations obtained with our exact decoder to measure the number of search errors made by beam search and cube pruning with increasing beam sizes (see).", "labels": [], "entities": [{"text": "beam search", "start_pos": 110, "end_pos": 121, "type": "TASK", "confidence": 0.7980518639087677}]}, {"text": "Beam search reaches optimum derivations with beam sizes k \u2265 500 for all language models tested.", "labels": [], "entities": []}, {"text": "Cube pruning, on the other hand, still makes mistakes at k = 1000.", "labels": [], "entities": []}, {"text": "derivations in the vast majority of the cases (100% with a 3-gram LM) and translation quality in terms of BLEU is no different from OS * . However, with k < 10 4 both model scores and translation quality can be improved.", "labels": [], "entities": [{"text": "translation", "start_pos": 74, "end_pos": 85, "type": "TASK", "confidence": 0.9346941709518433}, {"text": "BLEU", "start_pos": 106, "end_pos": 110, "type": "METRIC", "confidence": 0.9985927939414978}, {"text": "translation", "start_pos": 184, "end_pos": 195, "type": "TASK", "confidence": 0.9442180395126343}]}, {"text": "shows a finer view on search errors as a function of beam size for LMs of order 3 to 5 (fixed d = 4).", "labels": [], "entities": []}, {"text": "In, we fix a 3-gram LM and vary the distortion limit (from 4 to 6).", "labels": [], "entities": [{"text": "distortion", "start_pos": 36, "end_pos": 46, "type": "METRIC", "confidence": 0.9839147329330444}]}, {"text": "Dotted lines correspond to beam search and dashed lines correspond to cube pruning.", "labels": [], "entities": [{"text": "beam search", "start_pos": 27, "end_pos": 38, "type": "TASK", "confidence": 0.8846853375434875}]}], "tableCaptions": [{"text": " Table 1: Performance of the exact decoder in  terms of: time to build g (0)", "labels": [], "entities": []}, {"text": " Table 2: Average number of nodes (in thousands)  whose LM state encode an m-gram, and average  number of unique LM states of order m in the fi- nal hypergraph for different n-gram LMs (d = 4  everywhere).", "labels": [], "entities": []}, {"text": " Table 3: Beam search and cube pruning search er- rors (out of 3,003 test samples) by beam size using  LMs of order 3 to 5 (d = 4).", "labels": [], "entities": [{"text": "Beam search", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.8619685173034668}]}, {"text": " Table 4: Translation quality in terms of BLEU as  a function of beam size in cube pruning with lan- guage models of order 3 to 5. The bottom row  shows BLEU for our exact decoder.", "labels": [], "entities": [{"text": "Translation", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.9756317734718323}, {"text": "BLEU", "start_pos": 42, "end_pos": 46, "type": "METRIC", "confidence": 0.9992042183876038}, {"text": "BLEU", "start_pos": 153, "end_pos": 157, "type": "METRIC", "confidence": 0.9985748529434204}]}]}