{"title": [{"text": "Online Topic Model for Twitter Considering Dynamics of User Interests and Topic Trends", "labels": [], "entities": []}], "abstractContent": [{"text": "Latent Dirichlet allocation (LDA) is a topic model that has been applied to various fields, including user profiling and event summarization on Twitter.", "labels": [], "entities": [{"text": "Latent Dirichlet allocation (LDA)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7599010119835535}, {"text": "user profiling", "start_pos": 102, "end_pos": 116, "type": "TASK", "confidence": 0.6928828060626984}, {"text": "event summarization", "start_pos": 121, "end_pos": 140, "type": "TASK", "confidence": 0.682464525103569}]}, {"text": "When LDA is applied to tweet collections, it generally treats all aggregated tweets of a user as a single document.", "labels": [], "entities": []}, {"text": "Twitter-LDA, which assumes a single tweet consists of a single topic, has been proposed and has shown that it is superior in topic semantic coherence.", "labels": [], "entities": []}, {"text": "However, Twitter-LDA is not capable of online inference.", "labels": [], "entities": []}, {"text": "In this study, we extend Twitter-LDA in the following two ways.", "labels": [], "entities": []}, {"text": "First, we model the generation process of tweets more accurately by estimating the ratio between topic words and general words for each user.", "labels": [], "entities": []}, {"text": "Second, we enable it to estimate the dynamics of user interests and topic trends online based on the topic tracking model (TTM), which models consumer purchase behaviors.", "labels": [], "entities": []}], "introductionContent": [{"text": "Microblogs such as Twitter, have prevailed rapidly in our society recently.", "labels": [], "entities": []}, {"text": "Twitter users post a message using 140 characters, which is called a tweet.", "labels": [], "entities": []}, {"text": "The characters limit allows users to post tweets easily about not only personal interest or real life but also public events such as traffic accidents or earthquakes.", "labels": [], "entities": []}, {"text": "There have been many studies on how to extract and utilize such information on tweets (.", "labels": [], "entities": []}, {"text": "Topic models, such as latent Dirichlet allocation (LDA) ( are widely used to identify latent topic structure in large collections of documents.", "labels": [], "entities": [{"text": "latent Dirichlet allocation (LDA)", "start_pos": 22, "end_pos": 55, "type": "TASK", "confidence": 0.6702206879854202}]}, {"text": "Recently, some studies have applied LDA to Twitter for user classification), detection of influential users, and soon.", "labels": [], "entities": [{"text": "user classification", "start_pos": 55, "end_pos": 74, "type": "TASK", "confidence": 0.6855647265911102}, {"text": "detection of influential users", "start_pos": 77, "end_pos": 107, "type": "TASK", "confidence": 0.8300439566373825}]}, {"text": "LDA is a generative document model, which assumes that each document is represented as a probability distribution over some topics, and that each word has a latent topic.", "labels": [], "entities": []}, {"text": "When we apply LDA to tweets, each tweet is treated as a single document.", "labels": [], "entities": []}, {"text": "This direct application does notwork well because a tweet is very short compared with traditional media such as newspapers.", "labels": [], "entities": []}, {"text": "To deal with the shortness of a tweet, some studies aggregated all the tweets of a user as a single document).", "labels": [], "entities": []}, {"text": "On the other hand, proposed \"Twitter-LDA,\" which is a model that considers the shortness of a tweet.", "labels": [], "entities": []}, {"text": "Twitter-LDA assumes that a single tweet consists of a single topic, and that tweets consist of topic and background words.", "labels": [], "entities": []}, {"text": "show that it works well at the point of semantic coherence of topics compared with LDA.", "labels": [], "entities": []}, {"text": "However, as with the case of LDA, Twitter-LDA cannot consider a sequence of tweets because it assumes that samples are exchangeable.", "labels": [], "entities": []}, {"text": "In Twitter, user interests and topic trends are dynamically changing.", "labels": [], "entities": []}, {"text": "In addition, when new data comes along, anew model must be generated again with all the data in Twitter-LDA because it does not assume online inference.", "labels": [], "entities": []}, {"text": "Therefore, it cannot efficiently analyze the large number of tweets generated everyday.", "labels": [], "entities": []}, {"text": "To overcome these difficulties, a model that considers the time sequence and has the capability of online inference is required.", "labels": [], "entities": []}, {"text": "In this study, we first propose an improved model based on Twitter-LDA, which assumes that the ratio between topic and background words differs for each user.", "labels": [], "entities": []}, {"text": "This study evaluates the proposed method based on perplexity and shows the efficacy of the new assumption in the improved model.", "labels": [], "entities": []}, {"text": "Second, we propose anew topic model called \"Twitter-TTM\" by extending the improved model based on the topic tracking model (TTM) (), which models the purchase behavior of consumers and is capable of online inference.", "labels": [], "entities": []}, {"text": "Finally, we demonstrate that Twitter-TTM can effectively capture the dynamics of user interests and topic trends in Twitter.", "labels": [], "entities": []}], "datasetContent": [{"text": "We performed an experiment to compare the predictive performances of LDA, TTM, and the improved model shown in Section 2.1.", "labels": [], "entities": [{"text": "TTM", "start_pos": 74, "end_pos": 77, "type": "METRIC", "confidence": 0.6086451411247253}]}, {"text": "In this experiment, LDA was applied as the method to aggregate all tweets of a user as a single document.", "labels": [], "entities": []}, {"text": "as the same as other general tweets because they reflected the user's interests.", "labels": [], "entities": []}, {"text": "After the above preprocessing, we obtained the final dataset with 14,139 users, 252,842 tweets, and 7,763 vocabularies.", "labels": [], "entities": []}, {"text": "Each model was inferred with collapsed Gibbs sampling () and the iteration was set at 500.", "labels": [], "entities": []}, {"text": "For a fair comparison, the hyper parameters in these models were optimized in each Gibbs sampling iteration by maximizing likelihood using fixed iterations.", "labels": [], "entities": []}, {"text": "This study employs perplexity as the evaluation index, which is the standard metric in information retrieval literature.", "labels": [], "entities": []}, {"text": "The perplexity of a held-out test set is defined as where w u represents words are contained in the tweets of user u and N is the number of words in the test set.", "labels": [], "entities": []}, {"text": "A lower perplexity means higher predictive performance.", "labels": [], "entities": []}, {"text": "We set the number of topics K at 50, 100, 150, 200, and 250 and evaluated the perplexity for each model in each K via a 10-fold cross-validation.", "labels": [], "entities": []}, {"text": "The results are shown in, which shows that the improved model performs better than the other models for any K.", "labels": [], "entities": []}, {"text": "Therefore, the new assumption of the improved model, that the rate between background and topic words is different for each user, could be more appropriate.", "labels": [], "entities": []}, {"text": "LDA performance worsens with an increase in K because the aggregated tweets of a single user neglect the topic of each tweet.", "labels": [], "entities": [{"text": "K", "start_pos": 44, "end_pos": 45, "type": "METRIC", "confidence": 0.9949946403503418}]}, {"text": "shows examples of the tweets of users with high and low rates of background words.", "labels": [], "entities": []}, {"text": "The users with a high background words rate tend to use basic words that are often used in any topics, such as \"like,\" \"about,\" and \"people,\" and they tend to tweet about their personal lives.", "labels": [], "entities": []}, {"text": "On the other hand, for users with a low background words rate, topical words are often used such as \"Arsenal,\" \"Justin,\" and \"Google\".", "labels": [], "entities": []}, {"text": "They tend to tweet about their interests, including music, sports, and movies.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Perplexity of each model in 10 runs  Number of topic K LDA  Twitter-LDA Improved-model  50  1586.7 (14.4) 2191.0 (28.4) 1555.3 (36.7)  100  1612.7 (11.9) 1933.9 (23.6) 1471.7 (22.3)  150  1635.3 (11.2) 1760.1 (15.7) 1372.3 (20.0)  200  1655.2 (13.0) 1635.4 (22.1) 1289.5 (13.3)  250  1672.7 (17.2) 1542.8 (12.5) 1231.1 (11.9)", "labels": [], "entities": [{"text": "Improved-model", "start_pos": 82, "end_pos": 96, "type": "METRIC", "confidence": 0.6929177045822144}]}, {"text": " Table 2: Example of tweets of users with high and low rate of background words  High rate of background words Low rate of background words  I hope today goes quickly  Team Arsenal v will Ozil be  I want to work in a cake  Making Justin smile and laugh as he is working on music  All need your support please  Google nexus briefly appears in Google play store", "labels": [], "entities": []}]}