{"title": [], "abstractContent": [{"text": "We present STIR (STrongly Incremen-tal Repair detection), a system that detects speech repairs and edit terms on transcripts incrementally with minimal la-tency.", "labels": [], "entities": [{"text": "STrongly Incremen-tal Repair detection)", "start_pos": 17, "end_pos": 56, "type": "TASK", "confidence": 0.7891772866249085}]}, {"text": "STIR uses information-theoretic measures from n-gram models as its principal decision features in a pipeline of classifiers detecting the different stages of repairs.", "labels": [], "entities": [{"text": "STIR", "start_pos": 0, "end_pos": 4, "type": "TASK", "confidence": 0.4696652293205261}]}, {"text": "Results on the Switchboard dis-fluency tagged corpus show utterance-final accuracy on a par with state-of-the-art in-cremental repair detection methods, but with better incremental accuracy, faster time-to-detection and less computational overhead.", "labels": [], "entities": [{"text": "Switchboard dis-fluency tagged corpus", "start_pos": 15, "end_pos": 52, "type": "DATASET", "confidence": 0.6947222501039505}, {"text": "accuracy", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.9940478801727295}, {"text": "in-cremental repair detection", "start_pos": 114, "end_pos": 143, "type": "TASK", "confidence": 0.8018723130226135}, {"text": "accuracy", "start_pos": 181, "end_pos": 189, "type": "METRIC", "confidence": 0.9648798108100891}]}, {"text": "We evaluate its performance using incremental metrics and propose new repair processing evaluation standards.", "labels": [], "entities": [{"text": "repair processing evaluation", "start_pos": 70, "end_pos": 98, "type": "TASK", "confidence": 0.8929844498634338}]}], "introductionContent": [{"text": "Self-repairs in spontaneous speech are annotated according to a well established three-phase structure from onwards, and as described in From a dialogue systems perspective, detecting repairs and assigning them the appropriate structure is vital for robust natural language understanding (NLU) in interactive systems.", "labels": [], "entities": [{"text": "natural language understanding (NLU)", "start_pos": 257, "end_pos": 293, "type": "TASK", "confidence": 0.7978174189726511}]}, {"text": "Downgrading the commitment of reparandum phases and assigning appropriate interregnum and repair phases permits computation of the user's intended meaning.", "labels": [], "entities": []}, {"text": "Furthermore, the recent focus on incremental dialogue systems (see e.g. () means that repair detection should operate without unnecessary processing overhead, and function efficiently within an incremental framework.", "labels": [], "entities": [{"text": "repair detection", "start_pos": 86, "end_pos": 102, "type": "TASK", "confidence": 0.9755387604236603}]}, {"text": "However, such left-to-right operability on its own is not sufficient: inline with the principle of strong incremental interpretation, a repair detector should give the best results possible as early as possible.", "labels": [], "entities": []}, {"text": "With one exception (, there has been no focus on evaluating or improving the incremental performance of repair detection.", "labels": [], "entities": [{"text": "repair detection", "start_pos": 104, "end_pos": 120, "type": "TASK", "confidence": 0.9821985065937042}]}, {"text": "In this paper we present STIR (Strongly Incremental Repair detection), a system which addresses the challenges of incremental accuracy, computational complexity and latency in selfrepair detection, by making local decisions based on relatively simple measures of fluency and similarity.", "labels": [], "entities": [{"text": "STIR", "start_pos": 25, "end_pos": 29, "type": "METRIC", "confidence": 0.7175977230072021}, {"text": "Strongly Incremental Repair detection)", "start_pos": 31, "end_pos": 69, "type": "TASK", "confidence": 0.7467604279518127}, {"text": "accuracy", "start_pos": 126, "end_pos": 134, "type": "METRIC", "confidence": 0.9748995304107666}, {"text": "selfrepair detection", "start_pos": 176, "end_pos": 196, "type": "TASK", "confidence": 0.7208647429943085}]}, {"text": "Section 2 reviews state-of-the-art methods; Section 3 summarizes the challenges and explains our general approach; Section 4 explains STIR in detail; Section 5 explains our experimental set-up and novel evaluation metrics; Section 6 presents and discusses our results and Section 7 concludes.", "labels": [], "entities": [{"text": "STIR", "start_pos": 134, "end_pos": 138, "type": "METRIC", "confidence": 0.4497576951980591}]}], "datasetContent": [{"text": "We train STIR on the Switchboard data described above, and test it on the standard Switchboard test data (PTB III files 4[0-1]*).", "labels": [], "entities": [{"text": "STIR", "start_pos": 9, "end_pos": 13, "type": "METRIC", "confidence": 0.43667978048324585}, {"text": "Switchboard data", "start_pos": 21, "end_pos": 37, "type": "DATASET", "confidence": 0.8150080740451813}, {"text": "Switchboard test data (PTB III files 4[0-1", "start_pos": 83, "end_pos": 125, "type": "DATASET", "confidence": 0.8276436269283295}]}, {"text": "In order to avoid overfitting of classifiers to the basic language models, we use a cross-fold training approach: we divide the corpus into 10 folds and use language models trained on 9 folds to obtain feature values for the 10th fold, repeating for all 10.", "labels": [], "entities": []}, {"text": "Classifiers are then trained as standard on the resulting featureannotated corpus.", "labels": [], "entities": []}, {"text": "This resulted in better feature utility for n-grams and better F-score results for detection in all components in the order of 5-6%.", "labels": [], "entities": [{"text": "F-score", "start_pos": 63, "end_pos": 70, "type": "METRIC", "confidence": 0.9990826845169067}]}, {"text": "5 Training the classifiers Each Random Forest classifier was limited to 20 trees of maximum depth 4 nodes, putting a ceiling on decoding time.", "labels": [], "entities": []}, {"text": "In making the classifiers cost-sensitive, MetaCost resamples the data in accordance with the cost functions: we found using 10 iterations over a resample of 25% of the training data gave the most effective trade-off between training time and accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 242, "end_pos": 250, "type": "METRIC", "confidence": 0.9977645874023438}]}, {"text": "We use 8 different cost functions in rp start with differing costs for false negatives and positives of the form below, where R is a repair element word and F is a fluent onset: We adopt a similar technique in rm start using 5 different cost functions and in rp end using 8 different settings, which when combined gives a total of 320 different cost function configurations.", "labels": [], "entities": []}, {"text": "We hypothesise that higher recall permitted in the pipeline's first components would result in better overall accuracy as these hypotheses become refined, though at the cost of the stability of the hy-potheses of the sequence and extra downstream processing in pruning false positives.", "labels": [], "entities": [{"text": "recall", "start_pos": 27, "end_pos": 33, "type": "METRIC", "confidence": 0.998801589012146}, {"text": "accuracy", "start_pos": 110, "end_pos": 118, "type": "METRIC", "confidence": 0.9980448484420776}]}, {"text": "We also experiment with the number of repair hypotheses permitted per word, using limits of 1-best and 2-best hypotheses.", "labels": [], "entities": []}, {"text": "We expect that allowing 2 hypotheses to be explored per rp start should allow greater final accuracy, but with the trade-off of greater decoding and training complexity, and possible incremental instability.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 92, "end_pos": 100, "type": "METRIC", "confidence": 0.9823067784309387}]}, {"text": "As we wish to explore the incrementality versus final accuracy trade-off that STIR can achieve we now describe the evaluation metrics we employ.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.8938010334968567}, {"text": "STIR", "start_pos": 78, "end_pos": 82, "type": "TASK", "confidence": 0.8246092200279236}]}, {"text": "Following () we divide our evaluation metrics into similarity metrics (measures of equality with or similarity to a gold standard), timing metrics (measures of the timing of relevant phenomena detected from the gold standard) and diachronic metrics (evolution of incremental hypotheses over time).", "labels": [], "entities": [{"text": "timing metrics", "start_pos": 132, "end_pos": 146, "type": "METRIC", "confidence": 0.9708642661571503}]}, {"text": "Similarity metrics For direct comparison to previous approaches we use the standard measure of overall accuracy, the F-score over reparandum words, which we abbreviate F rm (see 7): We are also interested in repair structural classification, we also measure F-score overall repair components (rm words, ed words as interregna and rp words), a metric we abbreviate F s . This is not measured in standard repair detection on Switchboard.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 103, "end_pos": 111, "type": "METRIC", "confidence": 0.9258078336715698}, {"text": "F-score", "start_pos": 117, "end_pos": 124, "type": "METRIC", "confidence": 0.9899200201034546}, {"text": "repair structural classification", "start_pos": 208, "end_pos": 240, "type": "TASK", "confidence": 0.8242383599281311}, {"text": "repair detection", "start_pos": 403, "end_pos": 419, "type": "TASK", "confidence": 0.8192291855812073}, {"text": "Switchboard", "start_pos": 423, "end_pos": 434, "type": "DATASET", "confidence": 0.9458047151565552}]}, {"text": "To investigate incremental accuracy we evaluate the delayed accuracy (DA) introduced by (, as described in section 2 against the utterance-final gold standard disfluency annotations, and use the mean of the 6 word F-scores.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9908255934715271}, {"text": "delayed accuracy (DA)", "start_pos": 52, "end_pos": 73, "type": "METRIC", "confidence": 0.8887729406356811}, {"text": "F-scores", "start_pos": 214, "end_pos": 222, "type": "METRIC", "confidence": 0.9149236083030701}]}], "tableCaptions": [{"text": " Table 2: Comparison of the best performing system settings using different measures", "labels": [], "entities": []}, {"text": " Table 3: Comparison of performance of systems with different stack capacities", "labels": [], "entities": []}]}