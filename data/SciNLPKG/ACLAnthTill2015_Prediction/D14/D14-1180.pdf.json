{"title": [{"text": "Type-based MCMC for Sampling Tree Fragments from Forests", "labels": [], "entities": [{"text": "Sampling Tree Fragments from Forests", "start_pos": 20, "end_pos": 56, "type": "TASK", "confidence": 0.9244462132453919}]}], "abstractContent": [{"text": "This paper applies type-based Markov Chain Monte Carlo (MCMC) algorithms to the problem of learning Synchronous Context-Free Grammar (SCFG) rules from a forest that represents all possible rules consistent with a fixed word alignment.", "labels": [], "entities": []}, {"text": "While type-based MCMC has been shown to be effective in a number of NLP applications, our setting, where the tree structure of the sentence is itself a hidden variable, presents a number of challenges to type-based inference.", "labels": [], "entities": []}, {"text": "We describe methods for defining variable types and efficiently indexing variables in order to overcome these challenges.", "labels": [], "entities": []}, {"text": "These methods lead to improvements in both log likelihood and BLEU score in our experiments .", "labels": [], "entities": [{"text": "log likelihood", "start_pos": 43, "end_pos": 57, "type": "METRIC", "confidence": 0.8665315806865692}, {"text": "BLEU score", "start_pos": 62, "end_pos": 72, "type": "METRIC", "confidence": 0.9822550117969513}]}], "introductionContent": [{"text": "In previous work, sampling methods have been used to learn Tree Substitution Grammar (TSG) rules from derivation trees) for TSG learning.", "labels": [], "entities": [{"text": "TSG learning", "start_pos": 124, "end_pos": 136, "type": "TASK", "confidence": 0.9429025650024414}]}, {"text": "Here, at each node in the derivation tree, there is a binary variable indicating whether the node is internal to a TSG rule or is a split point, which we refer to as a cut, between two rules.", "labels": [], "entities": []}, {"text": "The problem of extracting machine translation rules from word-aligned bitext is a similar problem in that we wish to automatically learn the best granularity for the rules with which to analyze each sentence.", "labels": [], "entities": [{"text": "extracting machine translation rules from word-aligned bitext", "start_pos": 15, "end_pos": 76, "type": "TASK", "confidence": 0.8278293609619141}]}, {"text": "The problem of rule extraction is more complex, however, because the tree structure of the sentence is also unknown.", "labels": [], "entities": [{"text": "rule extraction", "start_pos": 15, "end_pos": 30, "type": "TASK", "confidence": 0.8190878033638}]}, {"text": "In machine translation applications, most previous work on joint alignment and rule extraction models uses heuristic methods to extract rules from learned word alignment or bracketing structures (.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 3, "end_pos": 22, "type": "TASK", "confidence": 0.7581675052642822}, {"text": "joint alignment", "start_pos": 59, "end_pos": 74, "type": "TASK", "confidence": 0.7159937024116516}, {"text": "rule extraction", "start_pos": 79, "end_pos": 94, "type": "TASK", "confidence": 0.7054041773080826}]}, {"text": "present a MCMC algorithm schedule to learn Hiero-style SCFG rules by sampling tree fragments from phrase decomposition forests, which represent all possible rules that are consistent with a set of fixed word alignments.", "labels": [], "entities": []}, {"text": "Assuming fixed word alignments reduces the complexity of the sampling problem, and has generally been effective inmost stateof-the-art machine translation systems.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 135, "end_pos": 154, "type": "TASK", "confidence": 0.6981160044670105}]}, {"text": "The algorithm for sampling rules from a forest is as follows: from the root of the phrase decomposition forest, one samples a cut variable, denoting whether the current node is a cut, and an edge variable, denoting which incoming hyperedge is chosen, at each node of the current tree in a top-down manner.", "labels": [], "entities": []}, {"text": "This sampling schedule is efficient in that it only samples the current tree and will not waste time on updating variables that are unlikely to be used in any tree.", "labels": [], "entities": []}, {"text": "As with many other token-based Gibbs Sampling applications, sampling one node at a time can result in slow mixing due to the strong coupling between variables.", "labels": [], "entities": [{"text": "token-based Gibbs Sampling", "start_pos": 19, "end_pos": 45, "type": "TASK", "confidence": 0.5762109061082205}]}, {"text": "One general remedy is to sample blocks of coupled variables. and used blocked sampling algorithms that sample the whole tree structure associated with one sentence at a time for TSG and TAG learning.", "labels": [], "entities": [{"text": "TAG learning", "start_pos": 186, "end_pos": 198, "type": "TASK", "confidence": 0.7828728556632996}]}, {"text": "However, this kind of blocking does not deal with the coupling of variables correlated with the same type of structure across sentences.", "labels": [], "entities": []}, {"text": "introduced a type-based sampling schedule which updates a block of variables of the same type jointly.", "labels": [], "entities": []}, {"text": "The type of a variable is defined as the combination of new structural choices added when assigning different values to the variable.", "labels": [], "entities": []}, {"text": "Type-based MCMC tackles the coupling issue by assigning the same type to variables that are strongly coupled.", "labels": [], "entities": []}, {"text": "In this paper, we follow the phrase decomposition forest construction procedures of and present a type-based MCMC algorithm for sampling tree fragments from phrase decomposition forests which samples the variables of the same type jointly.", "labels": [], "entities": [{"text": "phrase decomposition forest construction", "start_pos": 29, "end_pos": 69, "type": "TASK", "confidence": 0.7515021562576294}]}, {"text": "We define the type of the cut variable for each node in our sampling schedule.", "labels": [], "entities": []}, {"text": "While type-based MCMC has been proven to be effective in a number of NLP applications, our sample-edge, sample-cut setting is more complicated as our tree structure is unknown.", "labels": [], "entities": []}, {"text": "We need additional steps to maintain the cut type information when the tree structure is changed as we sample the edge variable.", "labels": [], "entities": []}, {"text": "Like other typebased MCMC applications, we need bookkeeping of node sites to be sampled in order to loop through sites of the same type efficiently.", "labels": [], "entities": []}, {"text": "As noted by, indexing by the complete type information is too expensive in some applications like TSG learning.", "labels": [], "entities": [{"text": "indexing", "start_pos": 13, "end_pos": 21, "type": "TASK", "confidence": 0.9742729663848877}, {"text": "TSG learning", "start_pos": 98, "end_pos": 110, "type": "TASK", "confidence": 0.7000053226947784}]}, {"text": "Our setting is different from TSG learning in that the internal structure of each SCFG rule is abstracted away when deriving the rule type from the tree fragment sampled.", "labels": [], "entities": [{"text": "TSG learning", "start_pos": 30, "end_pos": 42, "type": "TASK", "confidence": 0.9308475852012634}]}, {"text": "We make the following contributions: 1.", "labels": [], "entities": []}, {"text": "We apply type-based MCMC to the setting of SCFG learning and have achieved better log likelihood and BLEU score result.", "labels": [], "entities": [{"text": "log likelihood", "start_pos": 82, "end_pos": 96, "type": "METRIC", "confidence": 0.7080041468143463}, {"text": "BLEU score", "start_pos": 101, "end_pos": 111, "type": "METRIC", "confidence": 0.9814260900020599}]}, {"text": "2. We present an innovative way of storing the type information by indexing on partial type information and then filtering the retrieved nodes according to the full type information, which enables efficient updates to maintain the type information while the amount of bookkeeping is reduced significantly.", "labels": [], "entities": []}, {"text": "3. We replace the two-stage sampling schedule of with a simpler and faster one-stage method.", "labels": [], "entities": []}, {"text": "4. We use parallel programming to do inexact type-based MCMC, which leads to a speedup of four times in comparison with nonparallel type-based MCMC, while the likelihood result of the Markov Chain does not change.", "labels": [], "entities": []}, {"text": "This strategy should also work with other type-based MCMC applications.", "labels": [], "entities": []}], "datasetContent": [{"text": "We used the same LDC Chinese-English parallel corpus as, 1 which is composed of newswire text.", "labels": [], "entities": [{"text": "LDC Chinese-English parallel corpus", "start_pos": 17, "end_pos": 52, "type": "DATASET", "confidence": 0.723053902387619}]}, {"text": "The corpus consists of 41K sentence pairs, which has 1M words on the English side.", "labels": [], "entities": []}, {"text": "The corpus has a 392-sentence development set with four references for parameter tuning, and a 428-sentence test set with four references for testing.", "labels": [], "entities": []}, {"text": "The development set and the test set have sentences with less than 30 words.", "labels": [], "entities": []}, {"text": "A trigram language model was used for all experiments.", "labels": [], "entities": []}, {"text": "We plotted the log likelihood graph to compare the convergence property of each sampling schedule and calculated BLEU () for evaluation.", "labels": [], "entities": [{"text": "log likelihood graph", "start_pos": 15, "end_pos": 35, "type": "METRIC", "confidence": 0.7938177585601807}, {"text": "BLEU", "start_pos": 113, "end_pos": 117, "type": "METRIC", "confidence": 0.9995343685150146}]}, {"text": "We use the top-down token-based sampling algorithm of Chung et al. as our baseline.", "labels": [], "entities": []}, {"text": "We use the same SCFG decoder for translation with both the baseline and the grammars sampled using our type-based MCMC sampler.", "labels": [], "entities": [{"text": "translation", "start_pos": 33, "end_pos": 44, "type": "TASK", "confidence": 0.9668039679527283}, {"text": "MCMC sampler", "start_pos": 114, "end_pos": 126, "type": "DATASET", "confidence": 0.8342379331588745}]}, {"text": "The features included in our experiments are differently normalized rule counts and lexical weightings () of each rule.", "labels": [], "entities": []}, {"text": "Weights are tuned using Pairwise Ranking Optimization (Hopkins and May, 2011) using a grammar extracted by the standard heuristic method and the development set.", "labels": [], "entities": []}, {"text": "The same weights are used throughout our experiments.", "labels": [], "entities": []}, {"text": "First we want to compare the DP likelihood of the baseline with our type-based MCMC sampler to see if type-based sampling would converge to a better sampling result.", "labels": [], "entities": [{"text": "DP likelihood", "start_pos": 29, "end_pos": 42, "type": "METRIC", "confidence": 0.8390401899814606}]}, {"text": "In order to verify if type-based MCMC really converges to a good optimum point, we use simulated annealing) to search possible better optimum points.", "labels": [], "entities": []}, {"text": "We sample from the real distribution modified by an annealing parameter \u03b2: We increase our \u03b2 from 0.1 to 1.3, and then decrease from 1.3 to 1.0, changing by 0.1 every 3 iterations.", "labels": [], "entities": []}, {"text": "We also run an inexact parallel approximation of type-based MCMC in comparison with the non-parallel sampling to find out if parallel programming is feasible to speedup typebased MCMC sampling without affecting the performance greatly.", "labels": [], "entities": []}, {"text": "We do not compare the PYP likelihood because the approximation renders it impossible to calculate the real PYP likelihood.", "labels": [], "entities": [{"text": "PYP likelihood", "start_pos": 22, "end_pos": 36, "type": "METRIC", "confidence": 0.8586814999580383}, {"text": "PYP likelihood", "start_pos": 107, "end_pos": 121, "type": "METRIC", "confidence": 0.7602510154247284}]}, {"text": "We also calculate the BLEU score to compare the grammars extracted using each sampling schedule.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 22, "end_pos": 32, "type": "METRIC", "confidence": 0.979424923658371}]}, {"text": "We just report the BLEU result of grammars sampled using PYP as for all our schedules, since PYP always performs better than DP.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 19, "end_pos": 23, "type": "METRIC", "confidence": 0.9996174573898315}]}, {"text": "As for parameter settings, we used d = 0.5 for the Pitman-Yor discount parameter.", "labels": [], "entities": []}, {"text": "Though we have a separate PYP for each rule length, we used same \u03b1 = 5 for all rule sizes in all experiments, including experiments using DP.", "labels": [], "entities": [{"text": "PYP", "start_pos": 26, "end_pos": 29, "type": "METRIC", "confidence": 0.9942783117294312}]}, {"text": "For rule length probability, a Poisson distribution where \u03bb = 2 was used for all experiments.", "labels": [], "entities": [{"text": "rule length probability", "start_pos": 4, "end_pos": 27, "type": "TASK", "confidence": 0.8235178589820862}]}, {"text": "For each sentence sample, we initialize all the nodes in the forest to be cut sites and choose an incoming edge for each node uniformly.", "labels": [], "entities": []}, {"text": "For each experiment, we run for 160 iterations.", "labels": [], "entities": []}, {"text": "For each DP experiment, we draw the log likelihood graph for each sampling schedule before it finally converges.", "labels": [], "entities": [{"text": "log likelihood graph", "start_pos": 36, "end_pos": 56, "type": "METRIC", "confidence": 0.767847498257955}]}, {"text": "For each PYP experiment, we tried averaging the grammars from every 10th iteration to construct a single grammar and use this grammar for decoding.", "labels": [], "entities": []}, {"text": "We tune the number of grammars included for averaging by comparing the BLEU score on the dev set and report the BLEU score result on the test with the same averaging of grammars.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 71, "end_pos": 81, "type": "METRIC", "confidence": 0.9741270840167999}, {"text": "BLEU score", "start_pos": 112, "end_pos": 122, "type": "METRIC", "confidence": 0.9792153239250183}]}, {"text": "As each tree fragment sampled from the forest represents a unique translation rule, we do not need to explicitly extract the rules; we merely need to collect them and count them.", "labels": [], "entities": []}, {"text": "However, the fragments sampled include purely non-lexical rules that do not conform to the rule constraints of Hiero, and rules that are not useful for translation.", "labels": [], "entities": [{"text": "translation", "start_pos": 152, "end_pos": 163, "type": "TASK", "confidence": 0.9716436862945557}]}, {"text": "In order to get rid of this type of rule, we prune every rule that has scope greater than two.", "labels": [], "entities": []}, {"text": "Whereas Hiero does not allow two adjacent nonterminals in the source side, our pruning criterion allows some rules of scope two that are not allowed by Hiero.", "labels": [], "entities": [{"text": "Hiero", "start_pos": 8, "end_pos": 13, "type": "DATASET", "confidence": 0.9233850836753845}]}, {"text": "For example, the following rule (only source side shown) has scope two but is not allowed by Hiero: shows the log likelihood result of our type-based MCMC sampling schedule and the baseline top-down sampling.", "labels": [], "entities": []}, {"text": "We can see that typebased sampling converges to a much better result than non-type-based top-down sampling.", "labels": [], "entities": []}, {"text": "This shows that type-based MCMC escapes some local optima that are hard for token-based methods to escape.", "labels": [], "entities": []}, {"text": "This further strengthens the idea that sampling a block of strongly coupled variables jointly The priors are the same as the work of.", "labels": [], "entities": []}, {"text": "The priors are set to be the same because other priors turnout not to affect much of the final performance and add additional difficulty for tuning.", "labels": [], "entities": []}, {"text": "Log likelihood: parallel type-based vs non-parallel type-based parallel type-based non-parallel type-based: parallelization result for type-based MCMC helps solve the slow mixing problem of tokenbased sampling methods.", "labels": [], "entities": []}, {"text": "Another interesting observation is that, even though theoretically these two sampling methods should finally converge to the same point, in practice a worse sampling algorithm is prone to get trapped at local optima, and it will be hard for its Markov chain to escape it.", "labels": [], "entities": []}, {"text": "We can also see from that the log likelihood result only improves slightly using simulated annealing.", "labels": [], "entities": [{"text": "log likelihood", "start_pos": 30, "end_pos": 44, "type": "METRIC", "confidence": 0.708332896232605}]}, {"text": "One possible explanation is that the Markov chain has already converged to a very good optimum point with type-based sampling and it is hard to search fora better optimum.", "labels": [], "entities": []}, {"text": "shows the parallelization result of typebased MCMC sampling when we run the program on five processors.", "labels": [], "entities": [{"text": "typebased MCMC sampling", "start_pos": 36, "end_pos": 59, "type": "TASK", "confidence": 0.5455391506354014}]}, {"text": "We can see from the graph that when running on five processors, the likelihood fi- nally converges to the same likelihood result as non-parallel type-based MCMC sampling.", "labels": [], "entities": []}, {"text": "However, when we use more processors, the likelihood eventually becomes lower than with non-parallel sampling.", "labels": [], "entities": [{"text": "likelihood", "start_pos": 42, "end_pos": 52, "type": "METRIC", "confidence": 0.9734333753585815}]}, {"text": "This is because when we increase the number of processors, we split the dataset into very small subsets.", "labels": [], "entities": []}, {"text": "As we maintain the bookkeeping for each subset separately and do not communicate the updates to each subset, the power of type-based sampling is weakened with bookkeeping for very few sites of each type.", "labels": [], "entities": []}, {"text": "In the extreme case, when we use too many processors in parallel, the bookkeeping would have a singleton site for each type.", "labels": [], "entities": []}, {"text": "In this case, the approximation would degrade to the scenario of approximating tokenbased sampling.", "labels": [], "entities": []}, {"text": "By choosing a proper size of division of the dataset and by maintaining local bookkeeping for each subset, the parallel approximation can converge to almost the same point as nonparallel sampling.", "labels": [], "entities": []}, {"text": "As shown in our experimental results, the speedup is very significant with the running time decreasing from thirty minutes per iteration to just seven minutes when running on five processors.", "labels": [], "entities": []}, {"text": "Part of the speedup comes from the smaller bookkeeping since with fewer sites for each index, there is less mismatch or conflict of sites.", "labels": [], "entities": []}, {"text": "shows the BLEU score results for typebased MCMC and the baseline.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.973660796880722}]}, {"text": "For non-typebased top-down sampling, the best BLEU score result on dev is achieved when averaging the grammars of every 10th iteration from the 0th to the 90th iteration, while our type-based method gets the best result by averaging over every 10th iteration from the 0th to the 100th iteration.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 46, "end_pos": 56, "type": "METRIC", "confidence": 0.9815624952316284}]}, {"text": "We can see that the BLEU score on dev for type-based MCMC and the corresponding BLEU score on test are both better than the result for the non-type-based method, though not significantly.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 20, "end_pos": 24, "type": "METRIC", "confidence": 0.999409556388855}, {"text": "BLEU", "start_pos": 80, "end_pos": 84, "type": "METRIC", "confidence": 0.9992461204528809}]}, {"text": "This shows that the better likelihood of our Markov Chain using type-based MCMC does result in better translation.", "labels": [], "entities": []}, {"text": "We have also done experiments calculating the BLEU score result of the inexact parallel implementation.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 46, "end_pos": 56, "type": "METRIC", "confidence": 0.98348268866539}]}, {"text": "We can see from that, while the likelihood of the approximation does not change in comparison with the exact type-based MCMC, there is a gap between the BLEU score results.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 153, "end_pos": 163, "type": "METRIC", "confidence": 0.9789794385433197}]}, {"text": "We think this difference might come from the inconsistency of the grammars sampled by each processor within each iteration, as they do not communicate the update within each iteration.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Comparisons of BLEU score results", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 25, "end_pos": 35, "type": "METRIC", "confidence": 0.9662420451641083}]}]}