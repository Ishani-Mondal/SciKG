{"title": [{"text": "A Model of Coherence Based on Distributed Sentence Representation", "labels": [], "entities": [{"text": "Distributed Sentence Representation", "start_pos": 30, "end_pos": 65, "type": "TASK", "confidence": 0.6804505089918772}]}], "abstractContent": [{"text": "Coherence is what makes a multi-sentence text meaningful, both logically and syntactically.", "labels": [], "entities": []}, {"text": "To solve the challenge of ordering a set of sentences into coherent order , existing approaches focus mostly on defining and using sophisticated features to capture the cross-sentence argumenta-tion logic and syntactic relationships.", "labels": [], "entities": []}, {"text": "But both argumentation semantics and cross-sentence syntax (such as coreference and tense rules) are very hard to formalize.", "labels": [], "entities": []}, {"text": "In this paper, we introduce a neural network model for the coherence task based on distributed sentence representation.", "labels": [], "entities": []}, {"text": "The proposed approach learns a syntactico-semantic representation for sentences automatically , using either recurrent or re-cursive neural networks.", "labels": [], "entities": []}, {"text": "The architecture obviated the need for feature engineering, and learns sentence representations, which are to some extent able to capture the 'rules' governing coherent sentence structure.", "labels": [], "entities": []}, {"text": "The proposed approach outperforms existing baselines and generates the state-of-art performance in standard coherence evaluation tasks 1 .", "labels": [], "entities": []}], "introductionContent": [{"text": "Coherence is a central aspect in natural language processing of multi-sentence texts.", "labels": [], "entities": [{"text": "natural language processing of multi-sentence texts", "start_pos": 33, "end_pos": 84, "type": "TASK", "confidence": 0.7290458281834921}]}, {"text": "It is essential in generating readable text that the text planner compute which ordering of clauses (or sentences; we use them interchangeably in this paper) is likely to support understanding and avoid confusion.", "labels": [], "entities": []}, {"text": "As define it, A text is coherent when it can be explained what role each clause plays with regard to the whole.", "labels": [], "entities": []}, {"text": "Several researchers in the 1980s and 1990s addressed the problem, the most influential of which include: Rhetorical Structure Theory (RST;), which defined about 25 relations that govern clause interdependencies and ordering and give rise to text tree structures; the stepwise assembly of semantic graphs to support adductive inference toward the best explanation; Discourse Representation Theory (DRT;), a formal semantic model of discourse contexts that constrain coreference and quantification scoping; the model of intentionoriented conversation blocks and their stack-based queueing to model attention flow, and more recently an inventory of a hundred or so binary inter-clause relations and associated annotated corpus (Penn Discourse Treebank.", "labels": [], "entities": [{"text": "Rhetorical Structure Theory (RST", "start_pos": 105, "end_pos": 137, "type": "TASK", "confidence": 0.7219618082046508}, {"text": "Discourse Representation Theory (DRT", "start_pos": 364, "end_pos": 400, "type": "TASK", "confidence": 0.7648039579391479}, {"text": "Penn Discourse Treebank", "start_pos": 725, "end_pos": 748, "type": "DATASET", "confidence": 0.9612255692481995}]}, {"text": "Work in text planning implemented some of these models, especially operationalized RST and explanation relations to govern the planning of coherent paragraphs.", "labels": [], "entities": [{"text": "text planning", "start_pos": 8, "end_pos": 21, "type": "TASK", "confidence": 0.725150853395462}, {"text": "operationalized RST", "start_pos": 67, "end_pos": 86, "type": "TASK", "confidence": 0.5434981286525726}]}, {"text": "Other computational work defined so called schemas, frames with fixed sequences of clause types to achieve stereotypical communicative intentions.", "labels": [], "entities": []}, {"text": "Little of this work survives.", "labels": [], "entities": []}, {"text": "Modern research tries simply to order a collection of clauses or sentences without giving an account of which order(s) is/are coherent or what the overall text structure is.", "labels": [], "entities": []}, {"text": "The research focuses on identifying and defining a set of increasingly sophisticated features by which algorithms can be trained to propose orderings.", "labels": [], "entities": []}, {"text": "Features being explored include the clause entities, organized into a grid;, coreference clues to ordering, named-entity categories, syntactic features (), and others.", "labels": [], "entities": []}, {"text": "Besides being time-intensive (feature engineering usually requites considerable effort and can depend greatly on upstream feature extraction algorithms), it is not immediately apparent which aspects of a clause or a coherent text to consider when deciding on ordering.", "labels": [], "entities": []}, {"text": "More importantly, the features developed to date are still incapable of fully specifying the acceptable ordering(s) within a context, let alone describe why they are coherent.", "labels": [], "entities": []}, {"text": "Recently, deep architectures, have been applied to various natural language processing tasks (see Section 2).", "labels": [], "entities": []}, {"text": "Such deep connectionist architectures learn a dense, low-dimensional representation of their problem in a hierarchical way that is capable of capturing both semantic and syntactic aspects of tokens (e.g.,)), entities, N-grams (, or phrases ( ).", "labels": [], "entities": []}, {"text": "More recent researches have begun looking at higher level distributed representations that transcend the token level, such as sentence-level ( or even discourse-level aspects.", "labels": [], "entities": []}, {"text": "Just as words combine to form meaningful sentences, can we take advantage of distributional semantic representations to explore the composition of sentences to form coherent meanings in paragraphs?", "labels": [], "entities": []}, {"text": "In this paper, we demonstrate that it is feasible to discover the coherent structure of a text using distributed sentence representations learned in a deep learning framework.", "labels": [], "entities": []}, {"text": "Specifically, we consider a WINDOW approach for sentences, as shown in, where positive examples are windows of sentences selected from original articles generated by humans, and negatives examples are generated by random replacements 2 . The semantic representations for terms and sentences are obtained through optimizing the neural network framework based on these positive vs negative ex-amples and the proposed model produces state-ofart performance in multiple standard evaluations for coherence models ().", "labels": [], "entities": []}, {"text": "The rest of this paper is organized as follows: We describe related work in Section 2, then describe how to obtain a distributed representation for sentences in Section 3, and the window composition in Section 4.", "labels": [], "entities": []}, {"text": "Experimental results are shown in Section 5, followed by a conclusion.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate the proposed coherence model on two common evaluation approaches adopted in existing work (): Sentence Ordering and Readability Assessment.", "labels": [], "entities": [{"text": "Sentence Ordering", "start_pos": 106, "end_pos": 123, "type": "TASK", "confidence": 0.8910886645317078}, {"text": "Readability Assessment", "start_pos": 128, "end_pos": 150, "type": "TASK", "confidence": 0.7182925045490265}]}, {"text": "Barzilay and Lapata's (2008) data corpus is from the Encyclopedia Britannica and the Britannica Elementary, the latter being anew version targeted at children.", "labels": [], "entities": [{"text": "Barzilay and Lapata's (2008) data corpus", "start_pos": 0, "end_pos": 40, "type": "DATASET", "confidence": 0.763889898856481}, {"text": "Encyclopedia Britannica", "start_pos": 53, "end_pos": 76, "type": "DATASET", "confidence": 0.871959388256073}, {"text": "Britannica Elementary", "start_pos": 85, "end_pos": 106, "type": "DATASET", "confidence": 0.9563450813293457}]}, {"text": "Both versions contain 107 articles.", "labels": [], "entities": []}, {"text": "The Encyclopedia Britannica corpus contains an average of 83.1 sentences per document and the Britannica Elementary contains 36.6.", "labels": [], "entities": [{"text": "Encyclopedia Britannica corpus", "start_pos": 4, "end_pos": 34, "type": "DATASET", "confidence": 0.6710346142450968}, {"text": "Britannica Elementary", "start_pos": 94, "end_pos": 115, "type": "DATASET", "confidence": 0.9687619805335999}]}, {"text": "The encyclopedia lemmas are written by different authors and consequently vary considerably in structure and vocabulary choice.", "labels": [], "entities": []}, {"text": "Early researchers assumed that the children version (Britannica Elementary) is easier to read, hence more coherent than documents in Encyclopedia Britannica.", "labels": [], "entities": [{"text": "children version (Britannica Elementary)", "start_pos": 35, "end_pos": 75, "type": "DATASET", "confidence": 0.7336292415857315}]}, {"text": "This is a somewhat questionable assumption that needs further investigation.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Comparison of Different Coherence  Frameworks. Reported baseline results are among  the best performance regarding each approach is  reprinted from prior work from (Barzilay and Lap- ata, 2008; Louis and Nenkova, 2012; Guinaudeau  and Strube, 2013).", "labels": [], "entities": []}, {"text": " Table 2: Comparison of Different Coherence  Frameworks on Readability Assessment. Re- ported baselines results are are taken from", "labels": [], "entities": [{"text": "Readability Assessment", "start_pos": 59, "end_pos": 81, "type": "TASK", "confidence": 0.7427474856376648}]}]}