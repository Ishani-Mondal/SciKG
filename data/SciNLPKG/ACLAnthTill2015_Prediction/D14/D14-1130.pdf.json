{"title": [{"text": "Human Effort and Machine Learnability in Computer Aided Translation", "labels": [], "entities": [{"text": "Computer Aided Translation", "start_pos": 41, "end_pos": 67, "type": "TASK", "confidence": 0.6874916752179464}]}], "abstractContent": [{"text": "Analyses of computer aided translation typically focus on either frontend interfaces and human effort, or backend translation and machine learnability of corrections.", "labels": [], "entities": [{"text": "computer aided translation", "start_pos": 12, "end_pos": 38, "type": "TASK", "confidence": 0.6363814771175385}, {"text": "backend translation", "start_pos": 106, "end_pos": 125, "type": "TASK", "confidence": 0.6654084920883179}]}, {"text": "However , this distinction is artificial in practice since the frontend and backend must work in concert.", "labels": [], "entities": []}, {"text": "We present the first holis-tic, quantitative evaluation of these issues by contrasting two assistive modes: post-editing and interactive machine translation (MT).", "labels": [], "entities": [{"text": "interactive machine translation (MT)", "start_pos": 125, "end_pos": 161, "type": "TASK", "confidence": 0.7958708008130392}]}, {"text": "We describe anew translator interface , extensive modifications to a phrase-based MT system, and a novel objective function for re-tuning to human corrections.", "labels": [], "entities": [{"text": "MT", "start_pos": 82, "end_pos": 84, "type": "TASK", "confidence": 0.867706835269928}]}, {"text": "Evaluation with professional bilingual translators shows that post-edit is faster than interactive at the cost of translation quality for French-English and English-German.", "labels": [], "entities": []}, {"text": "However, re-tuning the MT system to interactive output leads to larger, statistically significant reductions in HTER versus re-tuning to post-edit.", "labels": [], "entities": [{"text": "MT", "start_pos": 23, "end_pos": 25, "type": "TASK", "confidence": 0.9630082845687866}, {"text": "HTER", "start_pos": 112, "end_pos": 116, "type": "METRIC", "confidence": 0.4424619674682617}]}, {"text": "Analysis shows that tuning directly to HTER results in fine-grained corrections to subsequent machine output.", "labels": [], "entities": []}], "introductionContent": [{"text": "The goal of machine translation has always been to reduce human effort, whether by partial assistance or by outright replacement.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 12, "end_pos": 31, "type": "TASK", "confidence": 0.7911500036716461}]}, {"text": "However, preoccupation with the latter-fully automatic translation-at the exclusion of the former has been a feature of the research community since its first nascent steps in the 1950s.", "labels": [], "entities": []}, {"text": "Pessimistic about progress during that decade and future prospects, Bar-Hillel argued that more attention should be paid to a \"machine-post-editor partnership,\" whose decisive problem is \"the region of optimality in the continuum of possible divisions of labor.\"", "labels": [], "entities": []}, {"text": "Today, with human-quality, fully automatic machine translation (MT) elusive still, that decades-old recommendation remains current.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 43, "end_pos": 67, "type": "TASK", "confidence": 0.8563354730606079}]}, {"text": "This paper is the first to look at both sides of the partnership in a single user study.", "labels": [], "entities": []}, {"text": "We compare two common flavors of machine-assisted translation: post-editing and interactive MT.", "labels": [], "entities": [{"text": "machine-assisted translation", "start_pos": 33, "end_pos": 61, "type": "TASK", "confidence": 0.6669138669967651}, {"text": "MT", "start_pos": 92, "end_pos": 94, "type": "TASK", "confidence": 0.8710983395576477}]}, {"text": "We analyze professional, bilingual translators working in both modes, looking first at user productivity.", "labels": [], "entities": []}, {"text": "Does the additional machine assistance available in the interactive mode affect translation time and/or quality?", "labels": [], "entities": [{"text": "translation", "start_pos": 80, "end_pos": 91, "type": "TASK", "confidence": 0.9601055383682251}]}, {"text": "Then we turn to the machine side of the partnership.", "labels": [], "entities": []}, {"text": "The user study results in corrections to the baseline MT output.", "labels": [], "entities": [{"text": "MT", "start_pos": 54, "end_pos": 56, "type": "TASK", "confidence": 0.9296191334724426}]}, {"text": "Do these corrections help the MT system, and can it learn from them quickly enough to help the user?", "labels": [], "entities": [{"text": "MT", "start_pos": 30, "end_pos": 32, "type": "TASK", "confidence": 0.9782150983810425}]}, {"text": "We perform a re-tuning experiment in which we directly optimize human Translation Edit Rate (HTER), which correlates highly with human judgments of fluency and adequacy ().", "labels": [], "entities": [{"text": "human Translation Edit Rate (HTER)", "start_pos": 64, "end_pos": 98, "type": "METRIC", "confidence": 0.7737016550132206}]}, {"text": "It is also an intuitive measure of human effort, making fine distinctions between 0 (no editing) and 1 (complete rewrite).", "labels": [], "entities": []}, {"text": "We designed anew user interface (UI) for the experiment.", "labels": [], "entities": []}, {"text": "The interface places demands on the MT backend-not the other way around.", "labels": [], "entities": [{"text": "MT backend-not", "start_pos": 36, "end_pos": 50, "type": "TASK", "confidence": 0.7274095118045807}]}, {"text": "The most significant new MT system features are prefix decoding, for translation completion based on a user prefix; and dynamic phrase table augmentation, to handle target out-of-vocabulary (OOV) words.", "labels": [], "entities": [{"text": "MT", "start_pos": 25, "end_pos": 27, "type": "TASK", "confidence": 0.9901774525642395}, {"text": "translation completion", "start_pos": 69, "end_pos": 91, "type": "TASK", "confidence": 0.9690505564212799}]}, {"text": "Discriminative re-tuning is accomplished with a novel cross-entropy objective function.", "labels": [], "entities": []}, {"text": "We report three main findings: (1) post-editing is faster than interactive MT, corroborating; (2) interactive MT yields higher quality translation when baseline MT quality is high; and (3) re-tuning to interactive feedback leads to larger held-out HTER gains relative to post-edit.", "labels": [], "entities": []}, {"text": "Together these results show that a human-centered approach to computer aided translation (CAT) may involve tradeoffs between human effort and machine learnability.", "labels": [], "entities": [{"text": "computer aided translation (CAT)", "start_pos": 62, "end_pos": 94, "type": "TASK", "confidence": 0.8407122194766998}]}, {"text": "For example, if speed is the top priority, then a design geared toward post-editing is appropriate.", "labels": [], "entities": [{"text": "speed", "start_pos": 16, "end_pos": 21, "type": "METRIC", "confidence": 0.9823931455612183}]}, {"text": "However, if reductions in HTER ultimately correspond to lower human effort, then investing slightly more time in the interactive mode, which results in more learnable output, maybe optimal.", "labels": [], "entities": []}, {"text": "Mixed UI designs may offer a compromise.", "labels": [], "entities": []}, {"text": "Code and data from our experiments are available at: http://nlp.stanford.edu/software/phrasal/ A holistic comparison with human subjects necessarily involves many moving parts.", "labels": [], "entities": []}, {"text": "Section 2 briefly describes the interface, focusing on NLP components.", "labels": [], "entities": []}, {"text": "Section 3 describes changes to the backend MT system.", "labels": [], "entities": [{"text": "MT", "start_pos": 43, "end_pos": 45, "type": "TASK", "confidence": 0.8754329681396484}]}, {"text": "Section 4 explains the user study, and reports human translation time and quality results.", "labels": [], "entities": []}, {"text": "Section 5 describes the MT re-tuning experiment.", "labels": [], "entities": [{"text": "MT re-tuning", "start_pos": 24, "end_pos": 36, "type": "TASK", "confidence": 0.8673984706401825}]}, {"text": "Analysis (section 6) and related work (section 7) round out the paper.", "labels": [], "entities": []}, {"text": "shows the translator interface, which is designed for expert, bilingual translators.", "labels": [], "entities": []}, {"text": "Previous studies have shown that expert translators work and type quickly, so the interface is designed to be very responsive, and to be primarily operated by the keyboard.", "labels": [], "entities": []}, {"text": "Most aids can be accessed via either typing or four hot keys.", "labels": [], "entities": []}, {"text": "The current design focuses on the point of text entry and does not include conventional translator workbench features such as workflow management, spell checking, and text formatting tools.", "labels": [], "entities": [{"text": "text entry", "start_pos": 43, "end_pos": 53, "type": "TASK", "confidence": 0.6986155211925507}, {"text": "spell checking", "start_pos": 147, "end_pos": 161, "type": "TASK", "confidence": 0.7674908638000488}, {"text": "text formatting", "start_pos": 167, "end_pos": 182, "type": "TASK", "confidence": 0.7238755971193314}]}], "datasetContent": [{"text": "The human translators corrected the output of the BLEU-tuned, baseline MT system.", "labels": [], "entities": [{"text": "BLEU-tuned", "start_pos": 50, "end_pos": 60, "type": "METRIC", "confidence": 0.9961228966712952}, {"text": "MT", "start_pos": 71, "end_pos": 73, "type": "TASK", "confidence": 0.962862491607666}]}, {"text": "No updating of the MT system occurred during the experiment to eliminate a confound in the time and quality analyses.", "labels": [], "entities": [{"text": "MT", "start_pos": 19, "end_pos": 21, "type": "TASK", "confidence": 0.9634464979171753}]}, {"text": "Now we investigate re-tuning the MT system to the corrections by simply re-starting the online learning algorithm from the baseline weight vector w, this time scoring with HTER instead of BLEU.", "labels": [], "entities": [{"text": "MT", "start_pos": 33, "end_pos": 35, "type": "TASK", "confidence": 0.9631950259208679}, {"text": "HTER", "start_pos": 172, "end_pos": 176, "type": "METRIC", "confidence": 0.9759405255317688}, {"text": "BLEU", "start_pos": 188, "end_pos": 192, "type": "METRIC", "confidence": 0.9970322847366333}]}, {"text": "Conventional incremental MT learning experiments typically resemble domain adaptation: smallscale baselines are trained and tuned on mostly outof-domain data, and then re-tuned incrementally on in-domain data.", "labels": [], "entities": [{"text": "MT learning", "start_pos": 25, "end_pos": 36, "type": "TASK", "confidence": 0.9453026950359344}, {"text": "domain adaptation", "start_pos": 68, "end_pos": 85, "type": "TASK", "confidence": 0.7203842848539352}]}, {"text": "In contrast, we start with largescale systems.", "labels": [], "entities": []}, {"text": "This is more consistent with a professional translation environment where translators receive suggestions from state-of-the-art systems like Google Translate.", "labels": [], "entities": []}, {"text": "The lower part of shows the organization of the human corrections for re-tuning and testing.", "labels": [], "entities": []}, {"text": "Recall that for each unique source input, eight human translators produced a correction in each condition.", "labels": [], "entities": []}, {"text": "First, we filtered all corrections for which a log u was not recorded (due to technical problems).", "labels": [], "entities": []}, {"text": "Second, we de-duplicated the corrections so that each h was unique.", "labels": [], "entities": [{"text": "corrections", "start_pos": 29, "end_pos": 40, "type": "METRIC", "confidence": 0.9650381803512573}]}, {"text": "Finally, we split the unique (f, h) tuples according to a natural division in the: Main re-tuning results for interactive data.", "labels": [], "entities": [{"text": "Main", "start_pos": 83, "end_pos": 87, "type": "METRIC", "confidence": 0.9608278274536133}]}, {"text": "baseline is the BLEU-tuned system used in the translation user study.", "labels": [], "entities": [{"text": "BLEU-tuned", "start_pos": 16, "end_pos": 26, "type": "METRIC", "confidence": 0.995627760887146}, {"text": "translation user study", "start_pos": 46, "end_pos": 68, "type": "TASK", "confidence": 0.9025896588961283}]}, {"text": "re-tune is the baseline feature set re-tuned to HTER on int-tune.", "labels": [], "entities": [{"text": "re-tune", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.9591823816299438}, {"text": "HTER", "start_pos": 48, "end_pos": 52, "type": "DATASET", "confidence": 0.8165878057479858}]}, {"text": "retune+feat adds the human feature templates described in section 3.3.", "labels": [], "entities": []}, {"text": "bold indicates statistical significance relative to the baseline at p < 0.001; italic at p < 0.05 by the permutation test of. data.", "labels": [], "entities": [{"text": "statistical significance", "start_pos": 15, "end_pos": 39, "type": "METRIC", "confidence": 0.8657532930374146}]}, {"text": "There were five source segments per document, and each document was rendered as a single screen during the translation experiment.", "labels": [], "entities": []}, {"text": "Segment order was not randomized, so we could split the data as follows: assign the first three segments of each screen to tune, and the last two to test.", "labels": [], "entities": []}, {"text": "This is a clean split with no overlap.", "labels": [], "entities": []}, {"text": "This tune/test split has two attractive properties.", "labels": [], "entities": []}, {"text": "First, if we can quickly re-tune on the first few sentences on a screen and provide better translations for the last few, then presumably the user experience improves.", "labels": [], "entities": []}, {"text": "Second, source inputs fare repeatedeight translators translated each input in each condition.", "labels": [], "entities": []}, {"text": "This means that a reduction in HTER means better average suggestions for multiple human translators.", "labels": [], "entities": [{"text": "HTER", "start_pos": 31, "end_pos": 35, "type": "METRIC", "confidence": 0.8542848229408264}]}, {"text": "Contrast this experimental design with tuning to the corrections of a single human translator.", "labels": [], "entities": []}, {"text": "There the system might overfit to one human style, and may not generalize to other human translators.", "labels": [], "entities": []}, {"text": "contains the main results for re-tuning to interactive MT corrections.", "labels": [], "entities": [{"text": "MT corrections", "start_pos": 55, "end_pos": 69, "type": "TASK", "confidence": 0.9304662644863129}]}, {"text": "For both language pairs, we observe large statistically significant reductions in HTER.", "labels": [], "entities": []}, {"text": "However, the results for BLEU and TERwhich are computed with respect to the independent references-are mixed.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 25, "end_pos": 29, "type": "METRIC", "confidence": 0.9970710277557373}, {"text": "TERwhich", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9874665141105652}]}, {"text": "The lower En-De BLEU score is explained by a higher brevity penalty for the re-tuned output (0.918 vs. 0.862).", "labels": [], "entities": [{"text": "En-De", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.8216205835342407}, {"text": "BLEU score", "start_pos": 16, "end_pos": 26, "type": "METRIC", "confidence": 0.8763604760169983}]}, {"text": "However, the re-tuned 4-gram and 3-gram precisions are signif-: En-De test results for re-tuning to post-edit (pe) vs. interactive (int).", "labels": [], "entities": [{"text": "En-De", "start_pos": 64, "end_pos": 69, "type": "METRIC", "confidence": 0.9555654525756836}]}, {"text": "Features cannot be extracted from the post-edit data, so the re-tune+feat system cannot be learned.", "labels": [], "entities": []}, {"text": "The Fr-En results are similar but are omitted due to space.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Automatic assessment of translation qual- ity. Here we change the definitions of TER and  HTER slightly. TER is the human translations com- pared to the independent references. HTER is the  baseline MT compared to the human corrections.", "labels": [], "entities": [{"text": "TER", "start_pos": 91, "end_pos": 94, "type": "METRIC", "confidence": 0.9941808581352234}, {"text": "HTER", "start_pos": 100, "end_pos": 104, "type": "METRIC", "confidence": 0.9707416892051697}, {"text": "TER", "start_pos": 115, "end_pos": 118, "type": "METRIC", "confidence": 0.9828436374664307}, {"text": "HTER", "start_pos": 187, "end_pos": 191, "type": "METRIC", "confidence": 0.8347630500793457}, {"text": "MT", "start_pos": 209, "end_pos": 211, "type": "METRIC", "confidence": 0.6236681938171387}]}, {"text": " Table 2: Pairwise judgments for the manual qual- ity assessment. Inter-annotator agreement (IAA)  \u03ba scores are measured with the official WMT14  script. For comparison, the WMT14 IAA scores  are given in parentheses. EW (inter.) is expected  wins of interactive according to Eq. (6).", "labels": [], "entities": [{"text": "Inter-annotator agreement (IAA)  \u03ba scores", "start_pos": 66, "end_pos": 107, "type": "METRIC", "confidence": 0.8043943217822483}, {"text": "WMT14  script", "start_pos": 139, "end_pos": 152, "type": "DATASET", "confidence": 0.9422805905342102}, {"text": "WMT14 IAA", "start_pos": 174, "end_pos": 183, "type": "DATASET", "confidence": 0.7096267640590668}, {"text": "EW", "start_pos": 218, "end_pos": 220, "type": "METRIC", "confidence": 0.9975395202636719}]}, {"text": " Table 4: Gross statistics of MT training corpora.", "labels": [], "entities": [{"text": "MT training", "start_pos": 30, "end_pos": 41, "type": "TASK", "confidence": 0.9068869352340698}]}, {"text": " Table 5: Tuning, development, and test corpora  (#segments). tune and dev were used for baseline  system preparation. Re-tuning was performed on  int-tune and pe-tune, respectively. We report held- out results on the two test data sets. All sets are  supplied with independent references.", "labels": [], "entities": [{"text": "Re-tuning", "start_pos": 119, "end_pos": 128, "type": "METRIC", "confidence": 0.9757248163223267}]}, {"text": " Table 6: Main re-tuning results for interactive  data. baseline is the BLEU-tuned system used  in the translation user study. re-tune is the base- line feature set re-tuned to HTER on int-tune. re- tune+feat adds the human feature templates de- scribed in section 3.3. bold indicates statistical  significance relative to the baseline at p < 0.001;  italic at p < 0.05 by the permutation test of", "labels": [], "entities": [{"text": "BLEU-tuned", "start_pos": 72, "end_pos": 82, "type": "METRIC", "confidence": 0.9964191913604736}]}]}