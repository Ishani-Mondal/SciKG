{"title": [{"text": "Knowledge Graph and Text Jointly Embedding", "labels": [], "entities": []}], "abstractContent": [{"text": "We examine the embedding approach to reason new relational facts from a large-scale knowledge graph and a text corpus.", "labels": [], "entities": []}, {"text": "We propose a novel method of jointly embedding entities and words into the same continuous vector space.", "labels": [], "entities": []}, {"text": "The embedding process attempts to preserve the relations between entities in the knowledge graph and the concurrences of words in the text corpus.", "labels": [], "entities": []}, {"text": "Entity names and Wikipedia anchors are utilized to align the embeddings of entities and words in the same space.", "labels": [], "entities": []}, {"text": "Large scale experiments on Freebase and a Wikipedia/NY Times corpus show that jointly embedding brings promising improvement in the accuracy of predicting facts, compared to separately embedding knowledge graphs and text.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 27, "end_pos": 35, "type": "DATASET", "confidence": 0.9518653750419617}, {"text": "Wikipedia/NY Times corpus", "start_pos": 42, "end_pos": 67, "type": "DATASET", "confidence": 0.7935935139656067}, {"text": "accuracy", "start_pos": 132, "end_pos": 140, "type": "METRIC", "confidence": 0.9975696206092834}, {"text": "predicting facts", "start_pos": 144, "end_pos": 160, "type": "TASK", "confidence": 0.8865247666835785}]}, {"text": "Particularly, jointly embedding enables the prediction of facts containing entities out of the knowledge graph, which cannot be handled by previous embedding methods.", "labels": [], "entities": []}, {"text": "At the same time, concerning the quality of the word embeddings, experiments on the analogical reasoning task show that jointly embedding is comparable to or slightly better than word2vec (Skip-Gram).", "labels": [], "entities": [{"text": "analogical reasoning task", "start_pos": 84, "end_pos": 109, "type": "TASK", "confidence": 0.7744666337966919}]}], "introductionContent": [{"text": "Knowledge graphs such as Freebase ( and WordNet have become important resources for many AI & NLP applications such as Q & A.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 40, "end_pos": 47, "type": "DATASET", "confidence": 0.9082367420196533}]}, {"text": "Generally, a knowledge graph is a collection of relational facts that are often represented in the form of a triplet (head entity, relation, tail entity), e.g., \"(Obama, Born-in, Honolulu)\".", "labels": [], "entities": [{"text": "Obama, Born-in, Honolulu)\"", "start_pos": 163, "end_pos": 189, "type": "DATASET", "confidence": 0.91684423883756}]}, {"text": "An urgent issue for knowledge graphs is the coverage, e.g., even the largest knowledge graph of Freebase is still far from complete.", "labels": [], "entities": [{"text": "coverage", "start_pos": 44, "end_pos": 52, "type": "METRIC", "confidence": 0.9642630219459534}, {"text": "Freebase", "start_pos": 96, "end_pos": 104, "type": "DATASET", "confidence": 0.8695170283317566}]}, {"text": "Recently, targeting knowledge graph completion, a promising paradigm of embedding was proposed, which is able to reason new facts only from the knowledge graph).", "labels": [], "entities": [{"text": "knowledge graph completion", "start_pos": 20, "end_pos": 46, "type": "TASK", "confidence": 0.6621576646963755}]}, {"text": "Generally, in this series of methods, each entity is represented as a k-dimensional vector and each relation is characterized by an operation ink so that a candidate fact can be asserted by simple vector operations.", "labels": [], "entities": []}, {"text": "The embeddings are usually learnt by minimizing a global loss function of all the entities and relations in the knowledge graph.", "labels": [], "entities": []}, {"text": "Thus, the vector of an entity may encode global information from the entire graph, and hence scoring a candidate fact by designed vector operations plays a similar role to long range \"reasoning\" in the graph.", "labels": [], "entities": []}, {"text": "However, since this requires the vectors of both entities to score a candidate fact, this type of methods can only complete missing facts for which both entities exist in the knowledge graph.", "labels": [], "entities": []}, {"text": "However, a missing fact often contains entities out of the knowledge graph (called out-of-kb for short in this paper), e.g., one or both entities are phrases appearing in web text but not included in the knowledge graph yet.", "labels": [], "entities": []}, {"text": "How to deal with these facts is a significant obstacle to widely applying the embedding paradigm.", "labels": [], "entities": []}, {"text": "In addition to knowledge embedding, another interesting approach is the word embedding method word2vec (), which shows that learning word embeddings from an unlabeled text corpus can make the vectors connecting the pairs of words of some certain relation almost parallel, e.g., vec(\"China\") \u2212 vec(\"Beijing\") \u2248 vec(\"Japan\") \u2212 vec(\"Tokyo\").", "labels": [], "entities": []}, {"text": "However, it does not know the exact relation between the pairs.", "labels": [], "entities": []}, {"text": "Thus, it cannot be directly applied to complete knowledge graphs.", "labels": [], "entities": []}, {"text": "The capabilities and limitations of knowledge embedding and word embedding have inspired us to design a mechanism to mosaic the knowledge graph and the \"word graph\" together in a vector space so that we can score any candidate relational facts between entities and words . Therefore, we propose a novel method to jointly embed entities and words into the same vector space.", "labels": [], "entities": []}, {"text": "In our solution, we define a coherent probabilistic model for both knowledge and text, which is composed of three components: the knowledge model, text model, and alignment model.", "labels": [], "entities": []}, {"text": "Both the knowledge model and text model use the same core translation assumption for the fact modeling: a candidate fact (h, r, t) is scored based on h + r \u2212 t.", "labels": [], "entities": []}, {"text": "The only difference is, in the knowledge model the relation r is explicitly supervised and the goal is to fit the fact triplets, while in the text model we assume any pair of words hand t that concur in some text windows are of certain relation r but r is a hidden variable, and the goal is to fit the concurring pairs of words.", "labels": [], "entities": []}, {"text": "The alignment model guarantees the embeddings of entities and words/phrases lie in the same space and impels the two models to enhance each other.", "labels": [], "entities": []}, {"text": "Two mechanisms of alignment are introduced in this paper: utilizing names of entities and utilizing Wikipedia anchors.", "labels": [], "entities": []}, {"text": "This way of jointly embedding knowledge and text can be considered to be semi-supervised knowledge embedding: the knowledge graph provides explicit supervision of facts while the text corpus provides much more \"relation-unlabeled\" pairs of words.", "labels": [], "entities": []}, {"text": "We conduct extensive large scale experiments on Freebase and Wikipedia corpus, which show jointly embedding brings promising improvements to the accuracy of predicting facts, compared to separately embedding the knowledge graph and the text corpus, respectively.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 48, "end_pos": 56, "type": "DATASET", "confidence": 0.9692056775093079}, {"text": "Wikipedia corpus", "start_pos": 61, "end_pos": 77, "type": "DATASET", "confidence": 0.8962382972240448}, {"text": "accuracy", "start_pos": 145, "end_pos": 153, "type": "METRIC", "confidence": 0.9982805252075195}, {"text": "predicting facts", "start_pos": 157, "end_pos": 173, "type": "TASK", "confidence": 0.8932092189788818}]}, {"text": "Particularly, jointly embedding enables the prediction of a candidate fact with out-of-kb entities, which cannot be handled by any existing embedding methods.", "labels": [], "entities": []}, {"text": "We also use embeddings to provide a prior score to help fact extraction on the benchmark data set of Freebase+NYTimes and also observe very promising improvements.", "labels": [], "entities": [{"text": "fact extraction", "start_pos": 56, "end_pos": 71, "type": "TASK", "confidence": 0.7800617516040802}, {"text": "benchmark data set of Freebase+NYTimes", "start_pos": 79, "end_pos": 117, "type": "DATASET", "confidence": 0.845890462398529}]}, {"text": "Meanwhile, concerning the quality of word embeddings, experiments on the analogical reasoning task show that jointly embedding is comparable to or slightly better than word2vec (Skip-Gram).", "labels": [], "entities": [{"text": "analogical reasoning task", "start_pos": 73, "end_pos": 98, "type": "TASK", "confidence": 0.7571489016215006}]}, {"text": "A knowledge graph is embedded into a low-dimensional continuous vector space while certain properties of it are preserved).", "labels": [], "entities": []}, {"text": "Generally, each entity is represented as a point in that space while each relation is interpreted as an operation over entity embeddings.", "labels": [], "entities": []}, {"text": "For instance,) interprets a relation as a translation from the head entity to the tail entity.", "labels": [], "entities": []}, {"text": "The embedding representations are usually learnt by minimizing a global loss function involving all entities and relations so that each entity embedding encodes both local and global connectivity patterns of the original graph.", "labels": [], "entities": []}, {"text": "Thus, we can reason new facts from learnt embeddings.", "labels": [], "entities": []}, {"text": "Generally, word embeddings are learned from a given text corpus without supervision by predicting the context of each word or predicting the current word given its context ().", "labels": [], "entities": []}, {"text": "Although relations between words are not explicitly modeled, continuous bag-of-words (CBOW) and Skip-gram () learn word embeddings capturing many syntactic and semantic relations between words where a relation is also represented as the translation between word embeddings.", "labels": [], "entities": []}, {"text": "Another pivotal channel for knowledge graph completion is extracting relational facts from external sources such as free text).", "labels": [], "entities": [{"text": "knowledge graph completion", "start_pos": 28, "end_pos": 54, "type": "TASK", "confidence": 0.706011692682902}]}, {"text": "This series of methods focuses on identifying local text patterns that express a certain relation and making predictions based on them.", "labels": [], "entities": []}, {"text": "However, they have not fully utilized the evidences from a knowledge graph, e.g., knowledge embedding is able to reason new facts without any external sources.", "labels": [], "entities": []}, {"text": "Actually, knowledge embedding is very complementary to traditional extraction methods, which was first confirmed by . To estimate the plausibility of a candidate fact, they added scores from embeddings to scores from an extractor, which showed significant improvement.", "labels": [], "entities": []}, {"text": "However, as pointed out in the introduction, their knowledge embedding method cannot predict facts involving out-of-kb entities.", "labels": [], "entities": []}], "datasetContent": [{"text": "We empirically evaluate and compare related models with regards to three tasks: triplet classification (, improving relation extraction ( , and the analogical reasoning task ().", "labels": [], "entities": [{"text": "triplet classification", "start_pos": 80, "end_pos": 102, "type": "TASK", "confidence": 0.755367785692215}, {"text": "relation extraction", "start_pos": 116, "end_pos": 135, "type": "TASK", "confidence": 0.7672604620456696}]}, {"text": "The related models include: for knowledge embedding alone,, pTransE (proposed in this paper); for word embedding alone, Skip-gram (Mikolov et al., 2013b); for both knowledge and text, we use \"respectively\" to refer to the embeddings learnt by TransE/pTransE and Skip-gram, respectively, \"jointly\" to refer to our jointly embedding method, in which \"anchor\" and \"name\" refer to \"Alignment by Wikipedia Anchors\" and \"Alignment by Names of Entities\", respectively.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Data: triplets used in our experiments.", "labels": [], "entities": []}, {"text": " Table 2: Data: the number of e \u2212 e, w \u2212 e, e \u2212  w, w \u2212 w triplets/analogies where w represents  the out-of-kb entity, which is regarded as word and  replaced by its corresponding entity name.", "labels": [], "entities": []}, {"text": " Table 3: Triplet Classification: comparison be- tween TransE and pTransE over e \u2212 e triplets.", "labels": [], "entities": []}, {"text": " Table 4: Triplet classification: accuracy (%) over various types of triplets.", "labels": [], "entities": [{"text": "Triplet classification", "start_pos": 10, "end_pos": 32, "type": "TASK", "confidence": 0.8360100388526917}, {"text": "accuracy", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9996993541717529}]}, {"text": " Table 6: Phrases Analogical Reasoning Task.", "labels": [], "entities": [{"text": "Phrases Analogical Reasoning", "start_pos": 10, "end_pos": 38, "type": "TASK", "confidence": 0.7195036212603251}]}, {"text": " Table 7: Constructed Analogical Reasoning  Task.", "labels": [], "entities": [{"text": "Constructed Analogical Reasoning  Task", "start_pos": 10, "end_pos": 48, "type": "TASK", "confidence": 0.7354503497481346}]}, {"text": " Table 5: Words Analogical Reasoning Task.", "labels": [], "entities": [{"text": "Words Analogical Reasoning", "start_pos": 10, "end_pos": 36, "type": "TASK", "confidence": 0.7295638124148051}]}]}