{"title": [{"text": "Fear the REAPER: A System for Automatic Multi-Document Summarization with Reinforcement Learning", "labels": [], "entities": [{"text": "Automatic Multi-Document Summarization", "start_pos": 30, "end_pos": 68, "type": "TASK", "confidence": 0.6001940965652466}]}], "abstractContent": [{"text": "This paper explores alternate algorithms, reward functions and feature sets for performing multi-document summarization using reinforcement learning with a high focus on reproducibility.", "labels": [], "entities": [{"text": "multi-document summarization", "start_pos": 91, "end_pos": 119, "type": "TASK", "confidence": 0.5748635828495026}]}, {"text": "We show that ROUGE results can be improved using a unigram and bigram similarity metric when training a learner to select sentences for summarization.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 13, "end_pos": 18, "type": "METRIC", "confidence": 0.948409378528595}, {"text": "summarization", "start_pos": 136, "end_pos": 149, "type": "TASK", "confidence": 0.961959183216095}]}, {"text": "Learners are trained to summarize document clusters based on various algorithms and reward functions and then evaluated using ROUGE.", "labels": [], "entities": [{"text": "summarize document clusters", "start_pos": 24, "end_pos": 51, "type": "TASK", "confidence": 0.8842754562695821}, {"text": "ROUGE", "start_pos": 126, "end_pos": 131, "type": "METRIC", "confidence": 0.9094648361206055}]}, {"text": "Our experiments show a statistically significant improvement of 1.33%, 1.58%, and 2.25% for ROUGE-1, ROUGE-2 and ROUGE-L scores, respectively, when compared with the performance of the state of the art in automatic summarization with reinforcement learning on the DUC2004 dataset.", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 92, "end_pos": 99, "type": "METRIC", "confidence": 0.9722450375556946}, {"text": "ROUGE-L", "start_pos": 113, "end_pos": 120, "type": "METRIC", "confidence": 0.8398407101631165}, {"text": "summarization", "start_pos": 215, "end_pos": 228, "type": "TASK", "confidence": 0.9374509453773499}, {"text": "DUC2004 dataset", "start_pos": 264, "end_pos": 279, "type": "DATASET", "confidence": 0.9797486662864685}]}, {"text": "Furthermore query focused extensions of our approach show an improvement of 1.37% and 2.31% for ROUGE-2 and ROUGE-SU4 respectively over query focused extensions of the state of the art with reinforcement learning on the DUC2006 dataset.", "labels": [], "entities": [{"text": "DUC2006 dataset", "start_pos": 220, "end_pos": 235, "type": "DATASET", "confidence": 0.9848992228507996}]}], "introductionContent": [{"text": "The multi-document summarization problem has received much attention recently) due to its ability to reduce large quantities of text to a human processable amount as well as its application in other fields such as question answering ().", "labels": [], "entities": [{"text": "multi-document summarization problem", "start_pos": 4, "end_pos": 40, "type": "TASK", "confidence": 0.7076626022656759}, {"text": "question answering", "start_pos": 214, "end_pos": 232, "type": "TASK", "confidence": 0.9110440313816071}]}, {"text": "We expect this trend to further increase as the amount of linguistic data on the web from sources such as social media, wikipedia, and online newswire increases.", "labels": [], "entities": []}, {"text": "This paper focuses specifically on utilizing reinforcement learning) to create a policy for summarizing clusters of multiple documents related to the same topic.", "labels": [], "entities": [{"text": "summarizing clusters of multiple documents related to the same topic", "start_pos": 92, "end_pos": 160, "type": "TASK", "confidence": 0.7887935936450958}]}, {"text": "The task of extractive automated multidocument summarization) is to select a subset of textual units, in this case sentences, from the source document cluster to form a summary of the cluster in question.", "labels": [], "entities": [{"text": "multidocument summarization", "start_pos": 33, "end_pos": 60, "type": "TASK", "confidence": 0.5806516855955124}]}, {"text": "This extractive approach allows the learner to construct a summary without concern for the linguistic quality of the sentences generated, as the source documents are assumed to be of a certain linguistic quality.", "labels": [], "entities": []}, {"text": "This paper aims to expand on the techniques used in which uses a reinforcement learner, specifically T D(\u03bb), to create summaries of document clusters.", "labels": [], "entities": [{"text": "summaries of document clusters", "start_pos": 119, "end_pos": 149, "type": "TASK", "confidence": 0.7544029653072357}]}, {"text": "We achieve this through introducing anew algorithm, varying the feature space and utilizing alternate reward functions.", "labels": [], "entities": []}, {"text": "The T D(\u03bb) learner used in Ryang and Abekawa (2012) is a very early reinforcement learning implementation.", "labels": [], "entities": [{"text": "Ryang and Abekawa (2012)", "start_pos": 27, "end_pos": 51, "type": "DATASET", "confidence": 0.8591171006361643}]}, {"text": "We explore the option of leveraging more recent research in reinforcement learning algorithms to improve results.", "labels": [], "entities": []}, {"text": "To this end we explore the use of SARSA which is a derivative of T D(\u03bb) that models the action space in addition to the state space modelled by T D(\u03bb).", "labels": [], "entities": [{"text": "SARSA", "start_pos": 34, "end_pos": 39, "type": "METRIC", "confidence": 0.7328149080276489}]}, {"text": "Furthermore we explore the use of an algorithm not based on temporal difference methods, but instead on policy iteration techniques.", "labels": [], "entities": []}, {"text": "Approximate Policy Iteration ( generates a policy, then evaluates and iterates until convergence.", "labels": [], "entities": []}, {"text": "The reward function in Ryang and Abekawa (2012) is a delayed reward based on tf * idf values.", "labels": [], "entities": []}, {"text": "We further explore the reward space by introducing similarity metric calculations used in ROUGE) and base our ideas on.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 90, "end_pos": 95, "type": "METRIC", "confidence": 0.7594560384750366}]}, {"text": "The difference between immediate rewards and delayed rewards is that the learner re-ceives immediate feedback at every action in the former and feedback only at the end of the episode in the latter.", "labels": [], "entities": []}, {"text": "We explore the performance difference of both reward types.", "labels": [], "entities": []}, {"text": "Finally we develop query focused extensions to both reward functions and present their results on more recent Document Understanding Conference (DUC) datasets which ran a query focused task.", "labels": [], "entities": [{"text": "Document Understanding Conference (DUC) datasets", "start_pos": 110, "end_pos": 158, "type": "DATASET", "confidence": 0.5979695107255664}]}, {"text": "We first evaluate our systems using the DUC2004 dataset for comparison with the results in.", "labels": [], "entities": [{"text": "DUC2004 dataset", "start_pos": 40, "end_pos": 55, "type": "DATASET", "confidence": 0.9864068925380707}]}, {"text": "We then present the results of query focused reward functions against the DUC2006 dataset to provide reference with a more recent dataset and a more recent task, specifically a query-focused summarization task.", "labels": [], "entities": [{"text": "DUC2006 dataset", "start_pos": 74, "end_pos": 89, "type": "DATASET", "confidence": 0.9905639588832855}, {"text": "summarization task", "start_pos": 191, "end_pos": 209, "type": "TASK", "confidence": 0.8123022317886353}]}, {"text": "Evaluations are performed using ROUGE for ROUGE-1, ROUGE-2 and ROUGE-L values for general summarization, while ROUGE-2 and ROUGE-SU4 is used for query-focused summarization.", "labels": [], "entities": [{"text": "summarization", "start_pos": 90, "end_pos": 103, "type": "TASK", "confidence": 0.8597493767738342}]}, {"text": "Furthermore we selected a small subset of query focused summaries to be subjected to human evaluations and present the results.", "labels": [], "entities": []}, {"text": "Our implementation is named REAPER (Relatedness-focused Extractive Automatic summary Preparation Exploiting Reinfocement learning) thusly for its ability to harvest a document cluster for ideal sentences for performing the automatic summarization task.", "labels": [], "entities": [{"text": "REAPER", "start_pos": 28, "end_pos": 34, "type": "METRIC", "confidence": 0.9950534701347351}, {"text": "Relatedness-focused Extractive Automatic summary Preparation Exploiting Reinfocement learning)", "start_pos": 36, "end_pos": 130, "type": "TASK", "confidence": 0.7233239743444655}, {"text": "summarization task", "start_pos": 233, "end_pos": 251, "type": "TASK", "confidence": 0.8866528868675232}]}, {"text": "REAPER is not just a reward function and feature set, it is a full framework for implementing summarization tasks using reinforcement learning and is available online for experimentation.", "labels": [], "entities": [{"text": "REAPER", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.8277317881584167}, {"text": "summarization tasks", "start_pos": 94, "end_pos": 113, "type": "TASK", "confidence": 0.8983548879623413}]}, {"text": "The primary contributions of our experiments are as follows: \u2022 Exploration of T D(\u03bb), SARSA and Approximate Policy Iteration.", "labels": [], "entities": [{"text": "T D(\u03bb)", "start_pos": 78, "end_pos": 84, "type": "METRIC", "confidence": 0.7925133824348449}, {"text": "SARSA", "start_pos": 86, "end_pos": 91, "type": "METRIC", "confidence": 0.3416259288787842}]}, {"text": "\u2022 Alternate REAPER reward function.", "labels": [], "entities": [{"text": "REAPER", "start_pos": 12, "end_pos": 18, "type": "METRIC", "confidence": 0.9123400449752808}]}, {"text": "\u2022 Alternate REAPER feature set.", "labels": [], "entities": [{"text": "REAPER", "start_pos": 12, "end_pos": 18, "type": "METRIC", "confidence": 0.7833324670791626}]}, {"text": "\u2022 Query focused extensions of automatic summarization using reinforcement learning.", "labels": [], "entities": [{"text": "summarization", "start_pos": 40, "end_pos": 53, "type": "TASK", "confidence": 0.8304718136787415}]}], "datasetContent": [{"text": "Our state space S is represented simply as a threetuple [s a f ] in which sis the set of textual units (sentences) that have been added to the summary, a is a sequence of actions that have been performed on the summary and f is a boolean with value 0 representing non-terminal states and 1 representing a summary that has been terminated.", "labels": [], "entities": []}, {"text": "The individual units in our action space are defined as ] where xi is a textual unit as described earlier, let us define Di as the set [:insert xi ] for all xi \u2208 D where Dis the document set.", "labels": [], "entities": []}, {"text": "We also have one additional action [:finish] and thus we can define our action space.", "labels": [], "entities": []}, {"text": "The actions eligible to be executed on any given state sis defined by a function actions(A, s): The state-action transitions are defined below: Insertion adds both the content of the textual unit xi to the set s as well as the action itself to set a.", "labels": [], "entities": []}, {"text": "Conversely finishing does not alter s or abut it flips the f bit to on.", "labels": [], "entities": []}, {"text": "Notice from (10) that once a state is terminal any further actions have no effect.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Experimental results with ROUGE-1,  ROUGE-2 and ROUGE-L scores on DUC2004.", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 36, "end_pos": 43, "type": "METRIC", "confidence": 0.9792587757110596}, {"text": "ROUGE-2", "start_pos": 46, "end_pos": 53, "type": "METRIC", "confidence": 0.9547004103660583}, {"text": "ROUGE-L", "start_pos": 58, "end_pos": 65, "type": "METRIC", "confidence": 0.9806252717971802}, {"text": "DUC2004", "start_pos": 76, "end_pos": 83, "type": "DATASET", "confidence": 0.9477109909057617}]}, {"text": " Table 2: REAPER run 10 times on the DUC2004.", "labels": [], "entities": [{"text": "REAPER", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9723079204559326}, {"text": "DUC2004", "start_pos": 37, "end_pos": 44, "type": "DATASET", "confidence": 0.9717954397201538}]}, {"text": " Table 3: REAPER with delayed and immediate re- wards on DUC2004.", "labels": [], "entities": [{"text": "REAPER", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9830343723297119}, {"text": "DUC2004", "start_pos": 57, "end_pos": 64, "type": "DATASET", "confidence": 0.9709959626197815}]}, {"text": " Table 4: REAPER with alternate feature spaces on  DUC2004.", "labels": [], "entities": [{"text": "REAPER", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9522940516471863}, {"text": "DUC2004", "start_pos": 51, "end_pos": 58, "type": "DATASET", "confidence": 0.958304226398468}]}, {"text": " Table 4. Experiments  were run using REAPER reward, T D(\u03bb), and the  specified feature set.", "labels": [], "entities": [{"text": "REAPER reward", "start_pos": 38, "end_pos": 51, "type": "METRIC", "confidence": 0.9599351584911346}, {"text": "T D(\u03bb)", "start_pos": 53, "end_pos": 59, "type": "METRIC", "confidence": 0.9286123991012574}]}, {"text": " Table 5: REAPER with alternate algorithms on  DUC2004.", "labels": [], "entities": [{"text": "REAPER", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.851791501045227}, {"text": "DUC2004", "start_pos": 47, "end_pos": 54, "type": "DATASET", "confidence": 0.8882021903991699}]}, {"text": " Table 7: Human evaluation scores on DUC2004.", "labels": [], "entities": [{"text": "DUC2004", "start_pos": 37, "end_pos": 44, "type": "DATASET", "confidence": 0.8969108462333679}]}]}