{"title": [{"text": "Testing for Significance of Increased Correlation with Human Judgment", "labels": [], "entities": []}], "abstractContent": [{"text": "Automatic metrics are widely used in machine translation as a substitute for human assessment.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 37, "end_pos": 56, "type": "TASK", "confidence": 0.7388453483581543}]}, {"text": "With the introduction of any new metric comes the question of just how well that metric mimics human assessment of translation quality.", "labels": [], "entities": []}, {"text": "This is often measured by correlation with human judgment.", "labels": [], "entities": []}, {"text": "Significance tests are generally not used to establish whether improvements over existing methods such as BLEU are statistically significant or have occurred simply by chance, however.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 106, "end_pos": 110, "type": "METRIC", "confidence": 0.9899751543998718}]}, {"text": "In this paper, we introduce a significance test for comparing correlations of two metrics, along with an open-source implementation of the test.", "labels": [], "entities": []}, {"text": "When applied to a range of metrics across seven language pairs, tests show that fora high proportion of metrics, there is insufficient evidence to conclude significant improvement over BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 185, "end_pos": 189, "type": "METRIC", "confidence": 0.9892332553863525}]}], "introductionContent": [{"text": "Within machine translation (MT), efforts are ongoing to improve evaluation metrics and find better ways to automatically assess translation quality.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 7, "end_pos": 31, "type": "TASK", "confidence": 0.8765313863754273}]}, {"text": "The process of validating anew metric involves demonstration that it correlates better with human judgment than a standard metric such as BLEU ().", "labels": [], "entities": [{"text": "BLEU", "start_pos": 138, "end_pos": 142, "type": "METRIC", "confidence": 0.9959068298339844}]}, {"text": "However, although it is standard practice in MT evaluation to measure increases in automatic metric scores with significance tests), this has not been the casein papers proposing new metrics.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 45, "end_pos": 58, "type": "TASK", "confidence": 0.9668325185775757}]}, {"text": "Thus it is possible that some reported improvements in correlation with human judgment are attributable to chance rather than a systematic improvement.", "labels": [], "entities": []}, {"text": "In this paper, we motivate and introduce a novel significance test to assess the statistical significance of differences in correlation with human judgment for pairs of automatic metrics.", "labels": [], "entities": []}, {"text": "We apply tests to the WMT-12 shared metrics task to compare each of the participating methods, and find that fora high proportion of metrics, there is not enough evidence to conclude that they significantly outperform BLEU.", "labels": [], "entities": [{"text": "WMT-12 shared metrics task", "start_pos": 22, "end_pos": 48, "type": "TASK", "confidence": 0.5469311699271202}, {"text": "BLEU", "start_pos": 218, "end_pos": 222, "type": "METRIC", "confidence": 0.9922755360603333}]}], "datasetContent": [{"text": "Figure 2a is a heatmap of the degree to which automatic metrics correlate with one another when computed on the same data set, in the form of the Pearson's correlation between each pair of metrics that participated in the WMT-12 metrics task for Spanish-to-English evaluation.", "labels": [], "entities": [{"text": "Pearson's correlation", "start_pos": 146, "end_pos": 167, "type": "METRIC", "confidence": 0.927193264166514}, {"text": "WMT-12 metrics task for Spanish-to-English evaluation", "start_pos": 222, "end_pos": 275, "type": "TASK", "confidence": 0.46281698346138}]}, {"text": "Metrics are ordered in all tables from highest to lowest correlation with human assessment.", "labels": [], "entities": [{"text": "Metrics", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.9788727760314941}]}, {"text": "In addition, for the purposes of significance testing, we take the absolute value of all correlations, in order to compare error-based metrics with non-error based ones.", "labels": [], "entities": [{"text": "significance testing", "start_pos": 33, "end_pos": 53, "type": "TASK", "confidence": 0.8899236619472504}]}, {"text": "In general, the correlation is high amongst all pairs of metrics, with a high proportion of paired metrics achieving a correlation in excess of r = 0.9.", "labels": [], "entities": [{"text": "correlation", "start_pos": 16, "end_pos": 27, "type": "METRIC", "confidence": 0.9859232902526855}]}, {"text": "Two exceptions to this are TERRORCAT () and SAGAN), as seen in the regions of yellow and white.", "labels": [], "entities": [{"text": "TERRORCAT", "start_pos": 27, "end_pos": 36, "type": "METRIC", "confidence": 0.995097815990448}, {"text": "SAGAN)", "start_pos": 44, "end_pos": 50, "type": "METRIC", "confidence": 0.9688427746295929}]}, {"text": "shows the results of Williams significance tests for all pairs of metrics.", "labels": [], "entities": [{"text": "Williams significance", "start_pos": 21, "end_pos": 42, "type": "METRIC", "confidence": 0.4842020124197006}]}, {"text": "Since we are interested in not only identifying significant differences in correlations, but ultimately ranking competing metrics, we use a one-sided test.", "labels": [], "entities": []}, {"text": "Here again, the metrics are ordered from highest to lowest (absolute) correlation with human judgment.", "labels": [], "entities": []}, {"text": "For the Spanish-to-English systems, approximately 60% of WMT-12 metric pairs show a significant difference in correlation with human judgment at p < 0.05 (for one of the two metric directions).", "labels": [], "entities": []}, {"text": "As expected, the higher the correlation with human judgment, the more metrics a given method is superior to at a level of statistical significance.", "labels": [], "entities": []}, {"text": "Although TERRORCAT () achieves the highest absolute correlation with human judgment, it is not significantly better (p \u2265 0.05) than the four next-best metrics (METEOR (  jar, 2011) and POSF).", "labels": [], "entities": [{"text": "TERRORCAT", "start_pos": 9, "end_pos": 18, "type": "METRIC", "confidence": 0.9978737831115723}, {"text": "METEOR (  jar, 2011)", "start_pos": 160, "end_pos": 180, "type": "DATASET", "confidence": 0.7947524686654409}, {"text": "POSF", "start_pos": 185, "end_pos": 189, "type": "METRIC", "confidence": 0.8794082999229431}]}, {"text": "There is not enough evidence to conclude, therefore, that this metric is any better at evaluating Spanish-toEnglish MT system quality than the next four metrics.", "labels": [], "entities": [{"text": "MT", "start_pos": 116, "end_pos": 118, "type": "TASK", "confidence": 0.77926105260849}]}, {"text": "shows the results of significance tests for the six other language pairs used in the WMT-12 metrics shared task.", "labels": [], "entities": [{"text": "WMT-12 metrics shared task", "start_pos": 85, "end_pos": 111, "type": "TASK", "confidence": 0.5287795960903168}]}, {"text": "3 For no language pair is there an outright winner amongst the metrics, with proportions of significant differences between metrics fora given language pair ranging from 3% for Czech-to-English to 82% for Englishto-French (p < 0.05).", "labels": [], "entities": []}, {"text": "The number of metrics that significantly outperform BLEU fora given language pair is only 34% (p < 0.05), and no method significantly outperforms BLEU overall language pairs -indeed, even the best methods achieve statistical significance over BLEU for only a small minority of language pairs.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 52, "end_pos": 56, "type": "METRIC", "confidence": 0.910474419593811}, {"text": "BLEU", "start_pos": 146, "end_pos": 150, "type": "METRIC", "confidence": 0.8473618030548096}, {"text": "BLEU", "start_pos": 243, "end_pos": 247, "type": "METRIC", "confidence": 0.9214537143707275}]}, {"text": "This underlines the dangers of assessing metrics based solely on correlation numbers, and emphasizes the importance of statistical testing.", "labels": [], "entities": []}, {"text": "It is important to note that the number of com-peting metrics a metric significantly outperforms should not be used as the criterion for ranking competing metrics.", "labels": [], "entities": []}, {"text": "This is due to the fact that the power of the Williams test to identify significant differences between correlations changes depending on the degree to which the pair of metrics correlate with each other.", "labels": [], "entities": []}, {"text": "Therefore, a metric that happens to correlate strongly with many other metrics would beat an unfair advantage, were numbers of significant wins to be used to rank metrics.", "labels": [], "entities": []}, {"text": "For this reason, it is best to interpret pairwise metric tests in isolation.", "labels": [], "entities": []}, {"text": "As part of this research, we have made available an open-source implementation of statistical tests tailored to the assessment of MT metrics available at https://github.com/ ygraham/significance-williams.", "labels": [], "entities": [{"text": "MT", "start_pos": 130, "end_pos": 132, "type": "TASK", "confidence": 0.9796140789985657}]}], "tableCaptions": []}