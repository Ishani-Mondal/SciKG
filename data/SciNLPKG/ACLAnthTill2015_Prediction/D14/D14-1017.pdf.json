{"title": [{"text": "Unsupervised Word Alignment Using Frequency Constraint in Posterior Regularized EM", "labels": [], "entities": [{"text": "Unsupervised Word Alignment", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.5857629875342051}]}], "abstractContent": [{"text": "Generative word alignment models, such as IBM Models, are restricted to one-to-many alignment, and cannot explicitly represent many-to-many relationships in a bilingual text.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 11, "end_pos": 25, "type": "TASK", "confidence": 0.7632720470428467}]}, {"text": "The problem is partially solved either by introducing heuris-tics or by agreement constraints such that two directional word alignments agree with each other.", "labels": [], "entities": []}, {"text": "In this paper, we focus on the posterior regularization framework (Ganchev et al., 2010) that can force two directional word alignment models to agree with each other during training , and propose new constraints that can take into account the difference between function words and content words.", "labels": [], "entities": []}, {"text": "Experimental results on French-to-English and Japanese-to-English alignment tasks show statistically significant gains over the previous posterior regularization baseline.", "labels": [], "entities": [{"text": "Japanese-to-English alignment tasks", "start_pos": 46, "end_pos": 81, "type": "TASK", "confidence": 0.6893883546193441}]}, {"text": "We also observed gains in Japanese-to-English translation tasks, which prove the effectiveness of our methods under grammatically different language pairs.", "labels": [], "entities": [{"text": "Japanese-to-English translation tasks", "start_pos": 26, "end_pos": 63, "type": "TASK", "confidence": 0.7746809720993042}]}], "introductionContent": [{"text": "Word alignment is an important component in statistical machine translation (SMT).", "labels": [], "entities": [{"text": "Word alignment", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7446928471326828}, {"text": "statistical machine translation (SMT)", "start_pos": 44, "end_pos": 81, "type": "TASK", "confidence": 0.8062939395507177}]}, {"text": "For instance phrase-based SMT () is based on the concept of phrase pairs that are automatically extracted from bilingual data and rely on word alignment annotation.", "labels": [], "entities": [{"text": "phrase-based SMT", "start_pos": 13, "end_pos": 29, "type": "TASK", "confidence": 0.605804979801178}]}, {"text": "Similarly, the model for hierarchical phrase-based SMT is built from exhaustively extracted phrases that are, in turn, heavily reliant on word alignment.", "labels": [], "entities": [{"text": "SMT", "start_pos": 51, "end_pos": 54, "type": "TASK", "confidence": 0.8178752660751343}, {"text": "word alignment", "start_pos": 138, "end_pos": 152, "type": "TASK", "confidence": 0.6908270418643951}]}, {"text": "The Generative word alignment models, such as the IBM Models () and HMM (, are popular methods for automatically aligning bilingual texts, but are restricted to represent one-to-many correspondence of each word.", "labels": [], "entities": [{"text": "Generative word alignment", "start_pos": 4, "end_pos": 29, "type": "TASK", "confidence": 0.774317463239034}, {"text": "IBM Models", "start_pos": 50, "end_pos": 60, "type": "DATASET", "confidence": 0.9127973914146423}]}, {"text": "To resolve this weakness, various symmetrization methods are proposed. and propose various heuristic methods to combine two directional models to represent many-to-many relationships.", "labels": [], "entities": []}, {"text": "As an alternative to heuristic methods, filtering methods employ a threshold to control the trade-off between precision and recall based on a score estimated from the posterior probabilities from two directional models.", "labels": [], "entities": [{"text": "precision", "start_pos": 110, "end_pos": 119, "type": "METRIC", "confidence": 0.9987303614616394}, {"text": "recall", "start_pos": 124, "end_pos": 130, "type": "METRIC", "confidence": 0.9942523837089539}]}, {"text": "proposed arithmetic means of two models as a score for the filtering, whereas reported better results using geometric means.", "labels": [], "entities": []}, {"text": "The joint training method () enforces agreement between two directional models.", "labels": [], "entities": []}, {"text": "Posterior regularization () is an alternative agreement method which directly encodes agreement during training.", "labels": [], "entities": [{"text": "Posterior regularization", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.6196902394294739}]}, {"text": "DeNero and Macherey (2011) and also enforce agreement during decoding.", "labels": [], "entities": []}, {"text": "However, these agreement models do not take into account the difference in language pairs, which is crucial for linguistically different language pairs, such as Japanese and English: although content words maybe aligned with each other by introducing some agreement constraints, function words are difficult to align.", "labels": [], "entities": []}, {"text": "We focus on the posterior regularization framework and improve upon the previous work by proposing new constraint functions that take into account the difference in languages in terms of content words and function words.", "labels": [], "entities": []}, {"text": "In particular, we differentiate between content words and function words by frequency in bilingual data, following.", "labels": [], "entities": []}, {"text": "Experimental results show that the proposed methods achieved better alignment qualities on the French-English Hansard data and the JapaneseEnglish Kyoto free translation task (KFTT) measured by AER and F-measure.", "labels": [], "entities": [{"text": "French-English Hansard data", "start_pos": 95, "end_pos": 122, "type": "DATASET", "confidence": 0.7486475308736166}, {"text": "JapaneseEnglish Kyoto free translation", "start_pos": 131, "end_pos": 169, "type": "TASK", "confidence": 0.6766437441110611}, {"text": "AER", "start_pos": 194, "end_pos": 197, "type": "METRIC", "confidence": 0.9986504912376404}, {"text": "F-measure", "start_pos": 202, "end_pos": 211, "type": "METRIC", "confidence": 0.9769071340560913}]}, {"text": "In translation evaluations, we achieved statistically significant gains 153 in BLEU scores in the NTCIR10.", "labels": [], "entities": [{"text": "translation", "start_pos": 3, "end_pos": 14, "type": "TASK", "confidence": 0.9686214923858643}, {"text": "BLEU", "start_pos": 79, "end_pos": 83, "type": "METRIC", "confidence": 0.9981727600097656}, {"text": "NTCIR10", "start_pos": 98, "end_pos": 105, "type": "DATASET", "confidence": 0.9399086833000183}]}], "datasetContent": [{"text": "The data sets used in our experiments are the French-English Hansard Corpus, and two data sets for Japanese-English tasks: the Kyoto free translation task (KFTT) and NTCIR10.", "labels": [], "entities": [{"text": "Hansard Corpus", "start_pos": 61, "end_pos": 75, "type": "DATASET", "confidence": 0.8559106290340424}, {"text": "Kyoto free translation task (KFTT", "start_pos": 127, "end_pos": 160, "type": "TASK", "confidence": 0.684362143278122}, {"text": "NTCIR10", "start_pos": 166, "end_pos": 173, "type": "DATASET", "confidence": 0.9388654828071594}]}, {"text": "The Hansard Corpus consists of parallel texts drawn from official records of the proceedings of the Canadian Parliament.", "labels": [], "entities": [{"text": "Hansard Corpus", "start_pos": 4, "end_pos": 18, "type": "DATASET", "confidence": 0.9801952540874481}]}, {"text": "The KFTT (Neubig, 2011) is derived from Japanese Wikipedia articles related to Kyoto, which is professionally translated into English.", "labels": [], "entities": [{"text": "KFTT (Neubig, 2011)", "start_pos": 4, "end_pos": 23, "type": "DATASET", "confidence": 0.9216270744800568}]}, {"text": "NTCIR10 comes from patent data employed in a machine translation shared task (.", "labels": [], "entities": [{"text": "NTCIR10", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9377031326293945}, {"text": "machine translation shared task", "start_pos": 45, "end_pos": 76, "type": "TASK", "confidence": 0.8069292604923248}]}, {"text": "The statistics of these data are presented in.", "labels": [], "entities": []}, {"text": "Sentences of over 40 words on both source and target sides are removed for training alignment models.", "labels": [], "entities": []}, {"text": "We used a word alignment toolkit cicada 2 for training the IBM Model 4 with our proposed methods.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.8154914379119873}]}, {"text": "Training is bootstrapped from IBM Model 1, followed by HMM and IBM Model 4.", "labels": [], "entities": [{"text": "HMM", "start_pos": 55, "end_pos": 58, "type": "DATASET", "confidence": 0.851536750793457}, {"text": "IBM", "start_pos": 63, "end_pos": 66, "type": "DATASET", "confidence": 0.8892699480056763}]}, {"text": "When generating the final bidirectional word alignment, we use a grow-diag-final heuristic for the Japanese-English tasks and an intersection heuristic in the French-English task, judged by preliminary studies.", "labels": [], "entities": []}, {"text": "Following, we automatically decide the threshold for word frequency to discriminate between content words and function words.", "labels": [], "entities": []}, {"text": "Specifically, the threshold is determined by the ratio of highly frequent words.", "labels": [], "entities": []}, {"text": "The threshold this the maximum frequency that satisfies the following equation: w\u2208(f req(w)>th) f req(w) w\u2208all f req(w) > r.", "labels": [], "entities": []}, {"text": "Here, we empirically set r = 0.5 by preliminary studies.", "labels": [], "entities": []}, {"text": "This method is based on the intuition that content words and function words exist in a document at a constant rate.", "labels": [], "entities": []}, {"text": "We measure the impact of our proposed methods on the quality of word alignment measured   by AER and F-measure.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 64, "end_pos": 78, "type": "TASK", "confidence": 0.7697955667972565}, {"text": "AER", "start_pos": 93, "end_pos": 96, "type": "METRIC", "confidence": 0.9946324825286865}, {"text": "F-measure", "start_pos": 101, "end_pos": 110, "type": "METRIC", "confidence": 0.9686074256896973}]}, {"text": "Since there exists no distinction for sure-possible alignments in the KFTT data, we use only sure alignment for our evaluation, both for the FrenchEnglish and the Japanese-English tasks.", "labels": [], "entities": [{"text": "KFTT data", "start_pos": 70, "end_pos": 79, "type": "DATASET", "confidence": 0.9513644278049469}, {"text": "FrenchEnglish", "start_pos": 141, "end_pos": 154, "type": "DATASET", "confidence": 0.9712774753570557}]}, {"text": "The baseline method is symmetric constraint ( shown in.", "labels": [], "entities": []}, {"text": "The numbers in bold and in italics indicate the best score and the second best score, respectively.", "labels": [], "entities": []}, {"text": "The differences between f2f,f2c and baseline in KFTT are statistically significant at p < 0.05 using the signtest, but in hansard corpus, there exist no significant differences between the baseline and the proposed methods.", "labels": [], "entities": [{"text": "KFTT", "start_pos": 48, "end_pos": 52, "type": "DATASET", "confidence": 0.9093549251556396}, {"text": "hansard corpus", "start_pos": 122, "end_pos": 136, "type": "DATASET", "confidence": 0.9417405724525452}]}, {"text": "In terms of F-measure, it is clear that the f2f method is the most effective method in KFTT, and both f2f and f2c methods exceed the original posterior regularized model of.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 12, "end_pos": 21, "type": "METRIC", "confidence": 0.9522171020507812}, {"text": "KFTT", "start_pos": 87, "end_pos": 91, "type": "DATASET", "confidence": 0.8109370470046997}]}, {"text": "We also compared these methods with filtering methods (), in addition to heuristic methods.", "labels": [], "entities": []}, {"text": "We plot precision/recall curves and AER by varying the threshold between 0.1 and 0.9 with 0.1 increments.", "labels": [], "entities": [{"text": "precision", "start_pos": 8, "end_pos": 17, "type": "METRIC", "confidence": 0.999196469783783}, {"text": "recall", "start_pos": 18, "end_pos": 24, "type": "METRIC", "confidence": 0.9626144170761108}, {"text": "AER", "start_pos": 36, "end_pos": 39, "type": "METRIC", "confidence": 0.9995999932289124}]}, {"text": "From Figures, it can be seen that our proposed methods are superior to the baseline in terms of both precision-recall and AER.", "labels": [], "entities": [{"text": "precision-recall", "start_pos": 101, "end_pos": 117, "type": "METRIC", "confidence": 0.9986550807952881}, {"text": "AER", "start_pos": 122, "end_pos": 125, "type": "METRIC", "confidence": 0.9983262419700623}]}, {"text": "Next, we performed a translation evaluation, measured by BLEU ().", "labels": [], "entities": [{"text": "translation", "start_pos": 21, "end_pos": 32, "type": "TASK", "confidence": 0.9744417071342468}, {"text": "BLEU", "start_pos": 57, "end_pos": 61, "type": "METRIC", "confidence": 0.9993483424186707}]}, {"text": "We compared the grow-diag-final and filtering method () for creating phrase tables.", "labels": [], "entities": []}, {"text": "The threshold for the filtering factor was set to 0.1 which was the best setting in the word alignment experiment in section 4.2 under KFTT.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 88, "end_pos": 102, "type": "TASK", "confidence": 0.8192518353462219}, {"text": "KFTT", "start_pos": 135, "end_pos": 139, "type": "DATASET", "confidence": 0.9576094746589661}]}, {"text": "From the English side of the training data, we trained a word using the 5-gram model with SRILM).", "labels": [], "entities": [{"text": "SRILM", "start_pos": 90, "end_pos": 95, "type": "METRIC", "confidence": 0.9575369954109192}]}, {"text": "\"Moses\" toolkit was used as a decoder () and the model parameters were tuned by k-best MIRA.", "labels": [], "entities": [{"text": "MIRA", "start_pos": 87, "end_pos": 91, "type": "METRIC", "confidence": 0.9488694071769714}]}, {"text": "In order to avoid tuning instability, we evaluated the average of five runs).", "labels": [], "entities": []}, {"text": "The results are summarized in.", "labels": [], "entities": []}, {"text": "Our proposed methods achieved large gains in NTCIR10 task with the filtered method, but observed no gain in the KFTT with the filtered method.", "labels": [], "entities": [{"text": "KFTT", "start_pos": 112, "end_pos": 116, "type": "DATASET", "confidence": 0.7990236878395081}]}, {"text": "In NTCIR10 task with GDF, the gain in BLEU was smaller than that of KFTT.", "labels": [], "entities": [{"text": "NTCIR10", "start_pos": 3, "end_pos": 10, "type": "DATASET", "confidence": 0.7409201264381409}, {"text": "BLEU", "start_pos": 38, "end_pos": 42, "type": "METRIC", "confidence": 0.9990687966346741}, {"text": "KFTT", "start_pos": 68, "end_pos": 72, "type": "DATASET", "confidence": 0.9286788105964661}]}, {"text": "We calculate p-values and the difference between symmetric and c2c (the most effective proposed constraint) are lower than 0.05 in kftt with GDF and NTCIR10 with filtered method.", "labels": [], "entities": [{"text": "GDF", "start_pos": 141, "end_pos": 144, "type": "DATASET", "confidence": 0.8680210113525391}, {"text": "NTCIR10", "start_pos": 149, "end_pos": 156, "type": "DATASET", "confidence": 0.8997754454612732}]}, {"text": "There seems to be no clear tendency in the improved alignment qualities and the translation qualities, as shown in numerous previous studies ().", "labels": [], "entities": [{"text": "translation", "start_pos": 80, "end_pos": 91, "type": "TASK", "confidence": 0.8144365549087524}]}], "tableCaptions": [{"text": " Table 1: The statistics of the data sets", "labels": [], "entities": []}, {"text": " Table 2: Results of word alignment evaluation with the heuristics-based method (GDF)", "labels": [], "entities": [{"text": "word alignment evaluation", "start_pos": 21, "end_pos": 46, "type": "TASK", "confidence": 0.8797562718391418}]}, {"text": " Table 3: Results of translation evaluation", "labels": [], "entities": [{"text": "translation evaluation", "start_pos": 21, "end_pos": 43, "type": "TASK", "confidence": 0.9713057279586792}]}, {"text": " Table 3. Our proposed methods achieved large  gains in NTCIR10 task with the filtered method,  but observed no gain in the KFTT with the filtered  method. In NTCIR10 task with GDF, the gain in  BLEU was smaller than that of KFTT. We cal- culate p-values and the difference between sym- metric and c2c (the most effective proposed con- straint) are lower than 0.05 in kftt with GDF and  NTCIR10 with filtered method. There seems to  be no clear tendency in the improved alignment  qualities and the translation qualities, as shown in  numerous previous studies (", "labels": [], "entities": [{"text": "BLEU", "start_pos": 195, "end_pos": 199, "type": "METRIC", "confidence": 0.9990604519844055}]}]}