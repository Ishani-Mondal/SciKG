{"title": [{"text": "Leveraging Effective Query Modeling Techniques for Speech Recognition and Summarization", "labels": [], "entities": [{"text": "Leveraging Effective Query Modeling", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.5760339125990868}, {"text": "Speech Recognition", "start_pos": 51, "end_pos": 69, "type": "TASK", "confidence": 0.789543867111206}, {"text": "Summarization", "start_pos": 74, "end_pos": 87, "type": "TASK", "confidence": 0.6594395041465759}]}], "abstractContent": [{"text": "Statistical language modeling (LM) that purports to quantify the acceptability of a given piece of text has long been an interesting yet challenging research area.", "labels": [], "entities": [{"text": "Statistical language modeling (LM)", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.8544993201891581}]}, {"text": "In particular, language modeling for information retrieval (IR) has enjoyed remarkable empirical success; one emerging stream of the LM approach for IR is to employ the pseudo-relevance feedback process to enhance the representation of an input query so as to improve retrieval effectiveness.", "labels": [], "entities": [{"text": "information retrieval (IR)", "start_pos": 37, "end_pos": 63, "type": "TASK", "confidence": 0.8706137418746949}]}, {"text": "This paper presents a continuation of such a general line of research and the main contribution is threefold.", "labels": [], "entities": []}, {"text": "First, we propose a principled framework which can unify the relationships among several widely-used query modeling formulations.", "labels": [], "entities": []}, {"text": "Second, on top of the successfully developed framework, we propose an extended query modeling formulation by incorporating critical query specific information cues to guide the model estimation.", "labels": [], "entities": []}, {"text": "Third, we further adopt and formalize such a framework to the speech recognition and summarization tasks.", "labels": [], "entities": [{"text": "speech recognition and summarization", "start_pos": 62, "end_pos": 98, "type": "TASK", "confidence": 0.7607935965061188}]}, {"text": "A series of empirical experiments reveal the feasibility of such an LM framework and the performance merits of the deduced models on these two tasks.", "labels": [], "entities": []}], "introductionContent": [{"text": "Along with the rapidly growing popularity of the Internet and the ubiquity of social web communications, tremendous volumes of multimedia contents, such as broadcast radio and television programs, digital libraries and soon, are made available to the public.", "labels": [], "entities": []}, {"text": "Research on multimedia content understanding and organization has witnessed a booming interest over the past decade.", "labels": [], "entities": [{"text": "multimedia content understanding", "start_pos": 12, "end_pos": 44, "type": "TASK", "confidence": 0.633234570423762}]}, {"text": "By virtue of the developed techniques, a variety of functionalities were created to help distill important content from multimedia collections, or provide locations of important speech segments in a video accompanied with their corresponding transcripts, for users to listen to or to digest.", "labels": [], "entities": []}, {"text": "Statistical language modeling (LM), which manages to quantify the acceptability of a given word sequence in a natural language or capture the statistical characteristics of a given piece of text, has been proved to offer both efficient and effective modeling abilities in many practical applications of natural language processing and speech recognition a ;).", "labels": [], "entities": [{"text": "Statistical language modeling (LM)", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.875524510939916}, {"text": "speech recognition", "start_pos": 335, "end_pos": 353, "type": "TASK", "confidence": 0.7192379534244537}]}, {"text": "The LM approach was first introduced for the information retrieval (IR) problems in the late 1990s, indicating very good potential, and was subsequently extended in a wide array of followup studies.", "labels": [], "entities": [{"text": "information retrieval (IR)", "start_pos": 45, "end_pos": 71, "type": "TASK", "confidence": 0.8513860702514648}]}, {"text": "One typical realization of the LM approach for IR is to access the degree of relevance between a query and a document by computing the likelihood of the query generated by the document (usually referred to as the querylikelihood approach).", "labels": [], "entities": [{"text": "IR", "start_pos": 47, "end_pos": 49, "type": "TASK", "confidence": 0.9877333641052246}]}, {"text": "A document is deemed to be relevant to a given query if the corresponding document model is more likely to generate the query.", "labels": [], "entities": []}, {"text": "On the other hand, the KullbackLeibler divergence measure (denoted by KLM for short hereafter), which quantifies the degree of relevance between a document and a query from a more rigorous information-theoretic perspective, has been proposed (  b ; Baeza-Yates and Ribeiro-Neto, 2011).", "labels": [], "entities": []}, {"text": "KLM not only can bethought as a natural generalization of the querylikelihood approach, but also has the additional merit of being able to accommodate extra information cues to improve the performance of document ranking.", "labels": [], "entities": []}, {"text": "For example, a main challenge facing such a measure is that since a given query usually consists of few words, the true information need is hard to be inferred from the surface statistics of a query.", "labels": [], "entities": []}, {"text": "As such, one emerging stream of thought for KLM is to employ the pseudo-relevance feedback process to construct an enhanced query model (or representation) so as to achieve better retrieval effectiveness.", "labels": [], "entities": [{"text": "KLM", "start_pos": 44, "end_pos": 47, "type": "TASK", "confidence": 0.9273548126220703}]}, {"text": "Following this line of research, the major contribution of this paper is three-fold: 1) we analyze several widely-used query models and then propose a principled framework to unify the relationships among them; 2) on top of the successfully developed query models, we propose an extended modeling formulation by incorporating additional query-specific information cues to guide the model estimation; 3) we explore a novel use of these query models by adapting them to the speech recognition and summarization tasks.", "labels": [], "entities": [{"text": "speech recognition and summarization", "start_pos": 472, "end_pos": 508, "type": "TASK", "confidence": 0.7337096631526947}]}, {"text": "As we will see, a series of experiments indeed demonstrate the effectiveness of the proposed models on these two tasks.", "labels": [], "entities": []}], "datasetContent": [{"text": "The speech corpus consists of about 196 hours of Mandarin broadcast news collected by the Academia Sinica and the Public Television Service Foundation of Taiwan between), which is publicly available and has been segmented into separate stories and transcribed manually.", "labels": [], "entities": []}, {"text": "Each story contains the speech of one studio anchor, as well as several field reporters and interviewees.", "labels": [], "entities": []}, {"text": "A subset of 25-hour speech data compiled during November 2001 to December 2002 was used to bootstrap the acoustic model training.", "labels": [], "entities": []}, {"text": "The vocabulary size is about 72 thousand words.", "labels": [], "entities": []}, {"text": "The background language model was estimated from a background text corpus consisting of 170 million Chinese characters collected from the Chinese Gigaword Corpus released by LDC.", "labels": [], "entities": [{"text": "Chinese Gigaword Corpus released by LDC", "start_pos": 138, "end_pos": 177, "type": "DATASET", "confidence": 0.8551838994026184}]}, {"text": "The dataset for use in the speech recognition experiments is compiled by a subset of 3-hour speech data from the corpus within 2003 (1.5 hours for development and 1.5 hours for test).", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 27, "end_pos": 45, "type": "TASK", "confidence": 0.8277181684970856}]}, {"text": "The contemporaneous (in-domain) text corpus used for training the various LM adaptation methods was collected between 2001 and 2003 from the corpus (excluding the test set), which consists of one million Chinese characters of the orthographic broadcast news transcripts.", "labels": [], "entities": [{"text": "LM adaptation", "start_pos": 74, "end_pos": 87, "type": "TASK", "confidence": 0.9296673536300659}]}, {"text": "In this paper, all the LM adaptation experiments were performed in word graph rescoring.", "labels": [], "entities": [{"text": "LM adaptation", "start_pos": 23, "end_pos": 36, "type": "TASK", "confidence": 0.9536597430706024}, {"text": "word graph rescoring", "start_pos": 67, "end_pos": 87, "type": "TASK", "confidence": 0.6921804547309875}]}, {"text": "The associated word graphs of the speech data were built beforehand with atypical LVCSR system).", "labels": [], "entities": []}, {"text": "In addition, the summarization task also employs the same broadcast news corpus as well.", "labels": [], "entities": [{"text": "summarization task", "start_pos": 17, "end_pos": 35, "type": "TASK", "confidence": 0.8984423875808716}]}, {"text": "A subset of 205 broadcast news documents compiled between November 2001 and August 2002 was reserved for the summarization experiments (185 for development and 20 for test).", "labels": [], "entities": []}, {"text": "A subset of about 100,000 text news documents, compiled during the same period as the documents to be summarized, was employed to estimate the related summarization models compared in this paper.", "labels": [], "entities": []}, {"text": "We adopted three variants of the widely-used ROUGE metric (i.e., ROUGE-1, ROGUE-2 and ROUGE-L) for the assessment of summarization performance.", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 65, "end_pos": 72, "type": "METRIC", "confidence": 0.9043788909912109}, {"text": "ROGUE-2", "start_pos": 74, "end_pos": 81, "type": "METRIC", "confidence": 0.9056705832481384}, {"text": "ROUGE-L", "start_pos": 86, "end_pos": 93, "type": "METRIC", "confidence": 0.8801528811454773}, {"text": "summarization", "start_pos": 117, "end_pos": 130, "type": "TASK", "confidence": 0.975996196269989}]}, {"text": "The summarization ratio, defined as the ratio of the number of words in the automatic (or manual) summary to that in the reference transcript of a spoken document, was set to 10% in this research.", "labels": [], "entities": [{"text": "summarization ratio", "start_pos": 4, "end_pos": 23, "type": "METRIC", "confidence": 0.8634423017501831}]}, {"text": "In the first part of experiments, we evaluate the effectiveness of the various query models applied to the speech recognition task.", "labels": [], "entities": [{"text": "speech recognition task", "start_pos": 107, "end_pos": 130, "type": "TASK", "confidence": 0.885253926118215}]}, {"text": "The corresponding results with respect to different numbers of top-ranked documents being used for estimating their component models are shown in.", "labels": [], "entities": []}, {"text": "Also worth mentioning is that the baseline system with the background trigram language model, which was trained with the SRILM toolkit () and Good-Turing smoothing, results in a Chinese character error rate (CER) of 20.08% on the test set.", "labels": [], "entities": [{"text": "SRILM toolkit", "start_pos": 121, "end_pos": 134, "type": "DATASET", "confidence": 0.784159928560257}, {"text": "Chinese character error rate (CER)", "start_pos": 178, "end_pos": 212, "type": "METRIC", "confidence": 0.7668642827442714}]}, {"text": "Consulting we notice two particularities.", "labels": [], "entities": []}, {"text": "One is that there is more fluctuation in the CER results of SMM than in those of RM.", "labels": [], "entities": [{"text": "fluctuation", "start_pos": 26, "end_pos": 37, "type": "METRIC", "confidence": 0.9653937220573425}, {"text": "CER", "start_pos": 45, "end_pos": 48, "type": "METRIC", "confidence": 0.7942646145820618}, {"text": "SMM", "start_pos": 60, "end_pos": 63, "type": "TASK", "confidence": 0.9768962264060974}]}, {"text": "The reason might be that, for SMM, the extraction of relevance information from the top-ranked documents is conducted with no involvement of the test utterance (i.e., the query; or its corresponding search histories), as elaborated earlier in Section 2.", "labels": [], "entities": [{"text": "SMM", "start_pos": 30, "end_pos": 33, "type": "TASK", "confidence": 0.9934492707252502}]}, {"text": "When too many feedback documents are being used, there would be a concern for SMM to be distracted from being able to appropriate model the test utterance, which is probably caused by some dominant distracting (or irrelevant) feedback documents.", "labels": [], "entities": [{"text": "SMM", "start_pos": 78, "end_pos": 81, "type": "TASK", "confidence": 0.9904594421386719}]}, {"text": "The other interesting observation is that RSMM only achieves a comparable (even worse) result when compared to SMM.", "labels": [], "entities": [{"text": "RSMM", "start_pos": 42, "end_pos": 46, "type": "TASK", "confidence": 0.9572978615760803}, {"text": "SMM", "start_pos": 111, "end_pos": 114, "type": "TASK", "confidence": 0.9550036787986755}]}, {"text": "A possible reason is that the prior constraint of the RSMM may contain too much noisy information so as to bias the model estimation.", "labels": [], "entities": []}, {"text": "Furthermore, it is evident that the proposed QMM is the best-performing method among all the query models compared in the paper.", "labels": [], "entities": []}, {"text": "Although the improvements made by QMM are not as pronounced as expected, we believe that QMM has demonstrated its potential to be applied to other related applications.", "labels": [], "entities": []}, {"text": "On the other hand, we compare the various query models with two well-practiced language models, namely the cache model (Cache) ( and the latent Dirichlet allocation (LDA) ().", "labels": [], "entities": []}, {"text": "The CER results of these two models are also shown in, respectively.", "labels": [], "entities": [{"text": "CER", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.9056139588356018}]}, {"text": "For the cache model, bigram cache was used since it can yield better results than the unigram and trigram cache models in our experiments.", "labels": [], "entities": []}, {"text": "It is worthy to notice that the LDA model was trained with the entire set of contemporaneous text document collection (c.f.", "labels": [], "entities": []}, {"text": "Section 4), while all of the query models explored in the paper were estimated based on a subset of the corpus selected by an initial round of retrieval.", "labels": [], "entities": []}, {"text": "The results reveal that most of these query models can achieve superior performance over the two conventional language models.", "labels": [], "entities": []}, {"text": "In the second part of experiments, we evaluate the utilities of the various query models as applied to the speech summarization task.", "labels": [], "entities": [{"text": "speech summarization task", "start_pos": 107, "end_pos": 132, "type": "TASK", "confidence": 0.7781123121579488}]}, {"text": "At the outset, we assess the performance level of the baseline KLM method by comparison with two well-practiced unsupervised methods, viz.", "labels": [], "entities": [{"text": "KLM", "start_pos": 63, "end_pos": 66, "type": "TASK", "confidence": 0.9281113147735596}]}, {"text": "the vector space model (VSM), and its extension, maximal marginal relevance (MMR)).", "labels": [], "entities": [{"text": "maximal marginal relevance (MMR))", "start_pos": 49, "end_pos": 82, "type": "METRIC", "confidence": 0.8389120300610861}]}, {"text": "The corresponding results are shown in and can be aligned with several related literature reviews.", "labels": [], "entities": []}, {"text": "By looking at the results, we find that KLM outperforms VSM by a large margin, confirming the applicability of the language modeling framework for speech summarization.", "labels": [], "entities": [{"text": "speech summarization", "start_pos": 147, "end_pos": 167, "type": "TASK", "confidence": 0.6163804084062576}]}, {"text": "Furthermore, MMR that presents an extension of VSM performs on par with KLM for the text summarization task (TD) and exhibits superior performance over KLM for the speech summarization task (SD).", "labels": [], "entities": [{"text": "text summarization task (TD)", "start_pos": 84, "end_pos": 112, "type": "TASK", "confidence": 0.830411563316981}, {"text": "speech summarization task (SD)", "start_pos": 164, "end_pos": 194, "type": "TASK", "confidence": 0.8569464385509491}]}, {"text": "We now turn to evaluate the effectiveness of the various query models (viz.", "labels": [], "entities": []}, {"text": "RM, SMM, RSMM and QMM) in conjunction with the pseudo-relevance feedback process for enhancing the sentence model involved in the KLM method.", "labels": [], "entities": []}, {"text": "The corresponding results are also shown in.", "labels": [], "entities": []}, {"text": "Two noteworthy observations can be drawn from.", "labels": [], "entities": []}, {"text": "One is that all these query models can considerably improve the summarization performance of the KLM method, which corroborates the advantage of using them for enhanced sentence representations.", "labels": [], "entities": [{"text": "summarization", "start_pos": 64, "end_pos": 77, "type": "TASK", "confidence": 0.9740493893623352}]}, {"text": "The other is that QMM is the best-performing one among all the formulations studied in this paper for both the TD and SD cases.", "labels": [], "entities": []}, {"text": "Going one step further, we explore to use extra prosodic features that are deemed complementary to the LM cue provided by QMM for speech summarization.", "labels": [], "entities": [{"text": "speech summarization", "start_pos": 130, "end_pos": 150, "type": "TASK", "confidence": 0.6546362936496735}]}, {"text": "To this end, a support vector machine (SVM) based summarization model is trained to integrate a set of 28 commonly-used prosodic features () for representing each spoken sentence, since SVM is arguably one of the state-of-the-art supervised methods that can make use of a diversity of indicative features for text or speech summarization (.", "labels": [], "entities": [{"text": "summarization", "start_pos": 50, "end_pos": 63, "type": "TASK", "confidence": 0.9535825848579407}, {"text": "text or speech summarization", "start_pos": 309, "end_pos": 337, "type": "TASK", "confidence": 0.6422940492630005}]}, {"text": "The sentence ranking scores derived by QMM and SVM are in turn integrated through a simple log-linear combination.", "labels": [], "entities": []}, {"text": "The corresponding results are shown in, demonstrating consistent improvements with respect to all the three variants of the ROUGE metric as compared to that using either QMM or SVM in isolation.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 124, "end_pos": 129, "type": "METRIC", "confidence": 0.9656592011451721}]}, {"text": "We also investigate using SVM to additionally integrate a richer set of lexical and relevance features to complement QMM and further enhance the summarization effectiveness.", "labels": [], "entities": [{"text": "summarization", "start_pos": 145, "end_pos": 158, "type": "TASK", "confidence": 0.9743587374687195}]}, {"text": "However, due to space limitation, we omit the details here.", "labels": [], "entities": []}, {"text": "As aside note, there is a sizable gap between the TD and SD cases, indicating room for further improvements.", "labels": [], "entities": []}, {"text": "We may seek remedies, such as robust indexing schemes, to compensate for imperfect speech recognition.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 83, "end_pos": 101, "type": "TASK", "confidence": 0.7370935380458832}]}], "tableCaptions": [{"text": " Table 1. The speech recognition results (in CER  (%)) achieved by various language models along  with different numbers of latent topics/pseudo- relevance feedback documents.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 14, "end_pos": 32, "type": "TASK", "confidence": 0.7910223603248596}, {"text": "CER", "start_pos": 45, "end_pos": 48, "type": "METRIC", "confidence": 0.9336535334587097}]}, {"text": " Table 2. The summarization results (in F-scores)  achieved by various language models along with  text and spoken documents.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9917570948600769}]}]}