{"title": [{"text": "An Experimental Comparison of Active Learning Strategies for Partially Labeled Sequences", "labels": [], "entities": []}], "abstractContent": [{"text": "Active learning (AL) consists of asking human annotators to annotate automatically selected data that are assumed to bring the most benefit in the creation of a classifier.", "labels": [], "entities": [{"text": "Active learning (AL)", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8298029839992523}]}, {"text": "AL allows to learn accurate systems with much less annotated data than what is required by pure supervised learning algorithms, hence limiting the tedious effort of annotating a large collection of data.", "labels": [], "entities": []}, {"text": "We experimentally investigate the behavior of several AL strategies for sequence labeling tasks (in a partially-labeled scenario) tailored on Partially-Labeled Conditional Random Fields, on four sequence labeling tasks: phrase chunking, part-of-speech tagging, named-entity recognition, and bio-entity recognition.", "labels": [], "entities": [{"text": "phrase chunking", "start_pos": 220, "end_pos": 235, "type": "TASK", "confidence": 0.8086796998977661}, {"text": "part-of-speech tagging", "start_pos": 237, "end_pos": 259, "type": "TASK", "confidence": 0.734124019742012}, {"text": "named-entity recognition", "start_pos": 261, "end_pos": 285, "type": "TASK", "confidence": 0.705178901553154}, {"text": "bio-entity recognition", "start_pos": 291, "end_pos": 313, "type": "TASK", "confidence": 0.7266063541173935}]}], "introductionContent": [{"text": "Today, the state-of-the-art methods inmost natural language processing tasks are supervised machine learning approaches.", "labels": [], "entities": []}, {"text": "Their main problem lies in their need of large human-annotated training corpus, which requires a tedious and expensive work from domain experts.", "labels": [], "entities": []}, {"text": "The process of active learning (AL) employs one or more human annotators by asking them to label new samples which are supposed to be the most informative in the creation of anew classifier.", "labels": [], "entities": [{"text": "active learning (AL)", "start_pos": 15, "end_pos": 35, "type": "TASK", "confidence": 0.7080409705638886}]}, {"text": "A classifier is incrementally retrained with all the data labeled by the annotator.", "labels": [], "entities": []}, {"text": "AL has been demonstrated to work well and to produce accurate classifiers while saving much human annotation effort.", "labels": [], "entities": []}, {"text": "One critical issue is to define a measure of the informativeness which should reflect how much new information anew example would give in the learning of anew classifier once annotated.", "labels": [], "entities": []}, {"text": "A lot of work has been done on the AL field in the past years (see for an exhaustive overview).", "labels": [], "entities": [{"text": "AL field", "start_pos": 35, "end_pos": 43, "type": "TASK", "confidence": 0.8385213911533356}]}, {"text": "In particular, AL proved its usefulness in sequence labeling tasks).", "labels": [], "entities": [{"text": "sequence labeling tasks", "start_pos": 43, "end_pos": 66, "type": "TASK", "confidence": 0.765627384185791}]}, {"text": "Yet, researchers have always adopted as annotation unit an entire sequence (i.e., the annotator is asked to annotate the whole sequence) while it looks like it could be much more relevant to ask for labeling only small parts of it (e.g., the ones with highest ambiguity).", "labels": [], "entities": []}, {"text": "A few works have investigated this idea.", "labels": [], "entities": []}, {"text": "For instance, proposed to use Partially-Labeled Conditional Random Fields (PL-CRFs) (), a semi-supervised variation of Conditional Random Fields (CRFs) () able to deal with partially-labeled sequences, thus enabling to adopt as annotation unit single tokens and still learning from full sequences.", "labels": [], "entities": []}, {"text": "AL with partially labeled sequences has proven to be effective in substantially reducing the amount of annotated data with respect to common AL approaches (see).", "labels": [], "entities": []}, {"text": "In this work we focus on AL strategies for partially labeled sequences adopting the single token as annotation unit and PL-CRFs as learning algorithm given its nature in dealing with partially labeled sequences.", "labels": [], "entities": []}, {"text": "We propose several AL strategies based on measures of uncertainty adapted for the AL with partially labeled sequences scenario and tailored on PL-CRFs.", "labels": [], "entities": []}, {"text": "We further propose two strategies that exploit the finer granularity given by the partially-labeled scenario.", "labels": [], "entities": []}, {"text": "We also show that the choice of single-token annotation can bring to unpredictable results on sequence labeling tasks in which the structure of the sequences is not regular, e.g., named-entity recognition.", "labels": [], "entities": [{"text": "sequence labeling tasks", "start_pos": 94, "end_pos": 117, "type": "TASK", "confidence": 0.6860541701316833}, {"text": "named-entity recognition", "start_pos": 180, "end_pos": 204, "type": "TASK", "confidence": 0.72275510430336}]}, {"text": "We propose a first solution to the problem of unpredictability.", "labels": [], "entities": []}, {"text": "The aim of this work is thoroughly compare the effectiveness and the behavior of all the proposed AL strategies on four standard sequence labeling tasks, phrase chunking, partof-speech tagging, named-entity recognition and bioentity recognition.", "labels": [], "entities": [{"text": "phrase chunking", "start_pos": 154, "end_pos": 169, "type": "TASK", "confidence": 0.8331975638866425}, {"text": "partof-speech tagging", "start_pos": 171, "end_pos": 192, "type": "TASK", "confidence": 0.8234445154666901}, {"text": "named-entity recognition", "start_pos": 194, "end_pos": 218, "type": "TASK", "confidence": 0.748065710067749}, {"text": "bioentity recognition", "start_pos": 223, "end_pos": 244, "type": "TASK", "confidence": 0.7731728255748749}]}, {"text": "The remainder of this paper is as follows.", "labels": [], "entities": []}, {"text": "In Section 2 we summarize the related work in AL, in Section 3 we describe PL-CRFs, the semi-supervised algorithm we adopt in this work.", "labels": [], "entities": []}, {"text": "Section 4 describes in details the AL framework and the AL strategies we propose.", "labels": [], "entities": []}, {"text": "Section 5 provides a description of the experimental setting, the datasets, and discusses the empirical results.", "labels": [], "entities": []}, {"text": "Section 6 summarizes our findings.", "labels": [], "entities": []}], "datasetContent": [{"text": "We have experimented and evaluated the AL strategies of Section 4 on four sequence labeling tasks, part-of-speech tagging, phrase chunking, named-entity recognition and bio-entity recognition.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 99, "end_pos": 121, "type": "TASK", "confidence": 0.7560219764709473}, {"text": "phrase chunking", "start_pos": 123, "end_pos": 138, "type": "TASK", "confidence": 0.8361777067184448}, {"text": "named-entity recognition", "start_pos": 140, "end_pos": 164, "type": "TASK", "confidence": 0.6999233365058899}, {"text": "bio-entity recognition", "start_pos": 169, "end_pos": 191, "type": "TASK", "confidence": 0.7147679179906845}]}, {"text": "We used the CoNLL2000 dataset) for the phrase chunking task, the CoNLL2003 dataset, for the named-entity recognition task, the NLPBA2004 dataset (), for the biomedical entity recognition task and the CoNLL2000POS dataset 2 for the part-of-speech labeling task.", "labels": [], "entities": [{"text": "CoNLL2000 dataset", "start_pos": 12, "end_pos": 29, "type": "DATASET", "confidence": 0.9775335788726807}, {"text": "phrase chunking task", "start_pos": 39, "end_pos": 59, "type": "TASK", "confidence": 0.8386836449305216}, {"text": "CoNLL2003 dataset", "start_pos": 65, "end_pos": 82, "type": "DATASET", "confidence": 0.9788315296173096}, {"text": "named-entity recognition task", "start_pos": 92, "end_pos": 121, "type": "TASK", "confidence": 0.8017639815807343}, {"text": "NLPBA2004 dataset", "start_pos": 127, "end_pos": 144, "type": "DATASET", "confidence": 0.9387937486171722}, {"text": "biomedical entity recognition task", "start_pos": 157, "end_pos": 191, "type": "TASK", "confidence": 0.6950787380337715}, {"text": "CoNLL2000POS dataset", "start_pos": 200, "end_pos": 220, "type": "DATASET", "confidence": 0.9338541626930237}, {"text": "part-of-speech labeling task", "start_pos": 231, "end_pos": 259, "type": "TASK", "confidence": 0.8335998853047689}]}, {"text": "All the datasets are publicly available and are standard benchmarks in sequence labeling tasks.", "labels": [], "entities": [{"text": "sequence labeling tasks", "start_pos": 71, "end_pos": 94, "type": "TASK", "confidence": 0.7241529226303101}]}, {"text": "shows some statistics of the datasets in terms of dimensions, number of labels, distribution of the labels, etc.", "labels": [], "entities": []}, {"text": "The data heterogeneity of the different datasets allowed us to test the AL strategies on different \"experimental settings\", thus to have a more robust empirical evaluation.", "labels": [], "entities": []}, {"text": "We tested the AL strategies described in Section 4 on test sets composed by 2012 sequences and 47377 tokens for the CoNLL2000 and CoNLL2000POS datasets, by 3452 sequences and 46394 tokens for the CoNLL2003 dataset and by 3856 sequences and 101039 tokens for the NLPBA2004 dataset.", "labels": [], "entities": [{"text": "CoNLL2000", "start_pos": 116, "end_pos": 125, "type": "DATASET", "confidence": 0.9698922634124756}, {"text": "CoNLL2000POS datasets", "start_pos": 130, "end_pos": 151, "type": "DATASET", "confidence": 0.8863553106784821}, {"text": "CoNLL2003 dataset", "start_pos": 196, "end_pos": 213, "type": "DATASET", "confidence": 0.9802590608596802}, {"text": "NLPBA2004 dataset", "start_pos": 262, "end_pos": 279, "type": "DATASET", "confidence": 0.977598249912262}]}, {"text": "We chose an initial training set T 1 of \u223c5 sequences on CoNLL2000 and CoNLL2000POS datasets, \u223c7 sequences on CoNLL2003 dataset and \u223c4 sequences on NLPBA2004 dataset, fora total of \u223c100 labeled tokens for each dataset.", "labels": [], "entities": [{"text": "CoNLL2000", "start_pos": 56, "end_pos": 65, "type": "DATASET", "confidence": 0.97548907995224}, {"text": "CoNLL2000POS datasets", "start_pos": 70, "end_pos": 91, "type": "DATASET", "confidence": 0.9267816841602325}, {"text": "CoNLL2003 dataset", "start_pos": 109, "end_pos": 126, "type": "DATASET", "confidence": 0.9837939739227295}, {"text": "NLPBA2004 dataset", "start_pos": 147, "end_pos": 164, "type": "DATASET", "confidence": 0.9763250946998596}]}, {"text": "The dimension of the batch update B has been chosen as a trade-off between an ideal casein which the system is retrained after every single annotation (i.e., B = 1) and a practical case with higher B to limit the algorithmic complexity (since the PL-CRF classifier must be retrained every iteration).", "labels": [], "entities": []}, {"text": "We used in our experiments B = 50.", "labels": [], "entities": [{"text": "B", "start_pos": 27, "end_pos": 28, "type": "METRIC", "confidence": 0.9980205297470093}]}, {"text": "We fixed the number of AL iterations n at 40 because what matters here is how the strategies behave in the beginning of AL process when the annotation effort remains low.", "labels": [], "entities": []}, {"text": "For each strategy and for each dataset, we report averaged results of three runs with a different randomly sampled initial training set T 1 . For each dataset we adopted a standard set of features.", "labels": [], "entities": []}, {"text": "For the CoNLL2000 dataset we adopted the same standard features used in () for the same dataset, for the CoNLL2003 and the NLPBA2004 dataset we adopted the features used in () for the CoNLL2003 dataset, while for the CoNLL2000POS dataset we used the features presented in.", "labels": [], "entities": [{"text": "CoNLL2000 dataset", "start_pos": 8, "end_pos": 25, "type": "DATASET", "confidence": 0.9793108403682709}, {"text": "CoNLL2003", "start_pos": 105, "end_pos": 114, "type": "DATASET", "confidence": 0.9696739912033081}, {"text": "NLPBA2004 dataset", "start_pos": 123, "end_pos": 140, "type": "DATASET", "confidence": 0.9739042520523071}, {"text": "CoNLL2003 dataset", "start_pos": 184, "end_pos": 201, "type": "DATASET", "confidence": 0.9638495147228241}, {"text": "CoNLL2000POS dataset", "start_pos": 217, "end_pos": 237, "type": "DATASET", "confidence": 0.9775804877281189}]}, {"text": "As evaluation measure we adopted the token variant of the F 1 measure, introduced by . This variant, instead of the entire annotation (chunk/entity), calculates T P s, F P s, and F N s, singularly for each token that compose the annotation, bringing to a finer evaluation.", "labels": [], "entities": []}, {"text": "In (left) we can notice that there are some strategies that are consistently worse than the RAND strategy.", "labels": [], "entities": []}, {"text": "The difference between the strategies below the RAND strategy and the RAND strategy itself might be due to the fact that those strategies ask to label tokens that are \"outliers\" (if we imagine tokens as points of the features space) that rarely appear in the training and test set, and on which the classifier is very uncertain.", "labels": [], "entities": []}, {"text": "Given that we are in a semi-supervised setting, with very few training examples, these \"outliers\" can introduce a lot of noise in the created models and so yielding poor results.", "labels": [], "entities": []}, {"text": "This phenomenon does not happen in the RAND strategy given that it samples uniformly from the unlabeled set and given that the \"outliers\" (special cases) are not many, the probability of randomly selecting an \"outlier\" is low.", "labels": [], "entities": [{"text": "RAND", "start_pos": 39, "end_pos": 43, "type": "TASK", "confidence": 0.8940586447715759}]}], "tableCaptions": [{"text": " Table 1: Training Data Statistics. #S is the number of total sequences in the dataset, #T is the number of tokens  in the dataset, #L is the number of positive labels (labels different from the negative label O), AAL is the average  length, in tokens, of annotations (sequence of tokens that refer to the same instance of a label), APT is the average  number of token in a sequence annotated with a positive label, ASL is the average length of a sequence, AA is the  average number of annotations in a sequence, %AC is the percentage of sequences with more than one positive  annotation, %DAC is the percentage of sequences that have two or more annotations with different labels.", "labels": [], "entities": [{"text": "AAL", "start_pos": 214, "end_pos": 217, "type": "METRIC", "confidence": 0.9947206974029541}, {"text": "APT", "start_pos": 333, "end_pos": 336, "type": "METRIC", "confidence": 0.9881674647331238}, {"text": "ASL", "start_pos": 416, "end_pos": 419, "type": "METRIC", "confidence": 0.9804816842079163}, {"text": "AA", "start_pos": 457, "end_pos": 459, "type": "METRIC", "confidence": 0.9912827014923096}, {"text": "DAC", "start_pos": 590, "end_pos": 593, "type": "METRIC", "confidence": 0.9442369937896729}]}]}