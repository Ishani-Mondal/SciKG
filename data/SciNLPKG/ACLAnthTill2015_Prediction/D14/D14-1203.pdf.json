{"title": [{"text": "Type-Aware Distantly Supervised Relation Extraction with Linked Arguments", "labels": [], "entities": [{"text": "Type-Aware Distantly Supervised Relation Extraction", "start_pos": 0, "end_pos": 51, "type": "TASK", "confidence": 0.6729645311832428}]}], "abstractContent": [{"text": "Distant supervision has become the leading method for training large-scale relation extractors, with nearly universal adoption in recent TAC knowledge-base population competitions.", "labels": [], "entities": [{"text": "relation extractors", "start_pos": 75, "end_pos": 94, "type": "TASK", "confidence": 0.8482702076435089}, {"text": "TAC knowledge-base population competitions", "start_pos": 137, "end_pos": 179, "type": "TASK", "confidence": 0.5357582867145538}]}, {"text": "However, there are still many questions about the best way to learn such extractors.", "labels": [], "entities": []}, {"text": "In this paper we investigate four orthogonal improvements: integrating named entity linking (NEL) and coreference resolution into argument identification for training and extraction, enforcing type constraints of linked arguments , and partitioning the model by relation type signature.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 102, "end_pos": 124, "type": "TASK", "confidence": 0.9105308055877686}, {"text": "argument identification", "start_pos": 130, "end_pos": 153, "type": "TASK", "confidence": 0.7442052364349365}]}, {"text": "We evaluate sentential extraction performance on two datasets: the popular set of NY Times articles partially annotated by Hoffmann et al.", "labels": [], "entities": [{"text": "sentential extraction", "start_pos": 12, "end_pos": 33, "type": "TASK", "confidence": 0.8991391360759735}]}, {"text": "(2011) and anew dataset, called GORECO, that is comprehensively annotated for 48 common relations.", "labels": [], "entities": [{"text": "GORECO", "start_pos": 32, "end_pos": 38, "type": "METRIC", "confidence": 0.6987233757972717}]}, {"text": "We find that using NEL for argument identification boosts performance over the traditional approach (named entity recognition with string match), and there is further improvement from using argument types.", "labels": [], "entities": [{"text": "argument identification", "start_pos": 27, "end_pos": 50, "type": "TASK", "confidence": 0.7231309562921524}, {"text": "entity recognition", "start_pos": 107, "end_pos": 125, "type": "TASK", "confidence": 0.7726742923259735}]}, {"text": "Our best system boosts precision by 44% and recall by 70%.", "labels": [], "entities": [{"text": "precision", "start_pos": 23, "end_pos": 32, "type": "METRIC", "confidence": 0.9997870326042175}, {"text": "recall", "start_pos": 44, "end_pos": 50, "type": "METRIC", "confidence": 0.9997938275337219}]}], "introductionContent": [{"text": "Relation extractors are commonly trained by distant supervision (also known as knowledge-based weak supervision (), an autonomous technique that creates a labeled training set by heuristically matching the contents of a knowledge base (KB) to mentions (substrings) in a textual corpus.", "labels": [], "entities": [{"text": "Relation extractors", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.9580745100975037}]}, {"text": "For example, if a KB contained the ground tuple BornIn(Albert Einstein, Ulm) then a distant supervision system might label the sentence \"While 1 was born in 2 , he moved to Munich at an early age.\" as a positive training instance of the BornIn relation.", "labels": [], "entities": []}, {"text": "Although distant supervision is a simple idea and often creates data with false positives, it has become ubiquitous; for example, all top-performing systems in recent TAC-KBP slot filling competitions used the method.", "labels": [], "entities": [{"text": "TAC-KBP slot filling competitions", "start_pos": 167, "end_pos": 200, "type": "TASK", "confidence": 0.7281433269381523}]}, {"text": "Surprisingly, however, many aspects of distant supervision are poorly studied.", "labels": [], "entities": []}, {"text": "In response we perform an extensive search of ways to improve distant supervision and the extraction process, including using named entity linking (NEL) and coreference to identify arguments for distant supervision and extraction, as well as using type constraints and partitioning the trained model by relation type signatures.", "labels": [], "entities": []}, {"text": "The first step in the distant supervision process is argument identification) -finding textual mentions referring to entities that might be in some relation.", "labels": [], "entities": [{"text": "argument identification", "start_pos": 53, "end_pos": 76, "type": "TASK", "confidence": 0.7648007869720459}]}, {"text": "Next comes matching, where KB facts, e.g. tuples such as R(e 1 , e 2 ), are associated with sentences mentioning entities e 1 and e 2 in the assumption that many of these sentences describe the relation R.", "labels": [], "entities": []}, {"text": "Most previous systems perform these steps by first using named entity recognition (NER) to identify possible arguments and then using a simple string match, but this crude approach misses many possible instances.", "labels": [], "entities": [{"text": "named entity recognition (NER)", "start_pos": 57, "end_pos": 87, "type": "TASK", "confidence": 0.8038045763969421}]}, {"text": "Since the separately-studied task of named entity linking (NEL) is precisely what is needed to perform distant supervision, it is interesting to see if today's optimized linkers lead to improved performance when used to train extractors.", "labels": [], "entities": [{"text": "named entity linking (NEL)", "start_pos": 37, "end_pos": 63, "type": "TASK", "confidence": 0.8248710731665293}]}, {"text": "Coreference, the task of clustering mentions that describe the same entity, may also be useful for increasing the number of candidate arguments.", "labels": [], "entities": []}, {"text": "Consider the following variant of our previous example: \"While [he] 1 was born in 2 , 3 moved to Munich at an early age.\"", "labels": [], "entities": []}, {"text": "Since mentions 1 and 3 corefer, one could consider using either the pair 1, 2 or 3, 2 (or both) for training.", "labels": [], "entities": []}, {"text": "Intuitively, it seems that 1, 2 is more representative of BornIn and might generalize better, so we consider the use of coreference at both training and extraction time.", "labels": [], "entities": []}, {"text": "Semantic relations often have selectional preferences (also known as type signatures); for example, BornIn holds between people and locations.", "labels": [], "entities": []}, {"text": "Therefore, it seems promising to include entity types, whether coarse or fine grained in the distantly supervised relation extraction process.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 114, "end_pos": 133, "type": "TASK", "confidence": 0.7370792478322983}]}, {"text": "We consider two ways of adding this information.", "labels": [], "entities": []}, {"text": "By using NEL to get linked entities, we can impose type constraints on the relation extraction system to only allow relations over appropriately typed mentions.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 75, "end_pos": 94, "type": "TASK", "confidence": 0.7357580959796906}]}, {"text": "We also investigate using coarse types from NER to learn separate models for different relation type signatures in order to make the models more effective.", "labels": [], "entities": []}, {"text": "In summary, this paper represents the following contributions: \u2022 We explore several dimensions for improving distantly supervised relation extraction, including better argument identification during training and extraction using both NEL and coreference, partitioning the model by relation type signatures, and enforcing type constraints of linked arguments as a postprocessing step.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 130, "end_pos": 149, "type": "TASK", "confidence": 0.7155912518501282}, {"text": "argument identification", "start_pos": 168, "end_pos": 191, "type": "TASK", "confidence": 0.7371375858783722}]}, {"text": "While some of these ideas may seem straightforward, to our knowledge they have not been systematically studied.", "labels": [], "entities": []}, {"text": "And, as we show, they lead to dramatic improvements.", "labels": [], "entities": []}, {"text": "\u2022 Since previous datasets are incapable of measuring an extractor's true recall, we introduce GORECO, anew exhaustively-labeled dataset with gold annotations for sentential instances of 48 relations across 128 newswire documents from the ACE 2004 corpus).", "labels": [], "entities": [{"text": "recall", "start_pos": 73, "end_pos": 79, "type": "METRIC", "confidence": 0.9605713486671448}, {"text": "GORECO", "start_pos": 94, "end_pos": 100, "type": "METRIC", "confidence": 0.9867553114891052}, {"text": "newswire documents from the ACE 2004 corpus", "start_pos": 210, "end_pos": 253, "type": "DATASET", "confidence": 0.65255954010146}]}, {"text": "\u2022 We demonstrate that NEL argument identification boosts both precision and recall, and using type constraints with linked arguments further boosts precision, yielding a 43% increase in precision and 27% boost to recall.", "labels": [], "entities": [{"text": "NEL argument identification", "start_pos": 22, "end_pos": 49, "type": "TASK", "confidence": 0.9117674628893534}, {"text": "precision", "start_pos": 62, "end_pos": 71, "type": "METRIC", "confidence": 0.9992008805274963}, {"text": "recall", "start_pos": 76, "end_pos": 82, "type": "METRIC", "confidence": 0.9984973669052124}, {"text": "precision", "start_pos": 148, "end_pos": 157, "type": "METRIC", "confidence": 0.9995143413543701}, {"text": "precision", "start_pos": 186, "end_pos": 195, "type": "METRIC", "confidence": 0.9994024038314819}, {"text": "recall", "start_pos": 213, "end_pos": 219, "type": "METRIC", "confidence": 0.9967947602272034}]}, {"text": "Using coreference during training argument identification gives an additional 7% improvement to precision and further boosts recall by 9%.", "labels": [], "entities": [{"text": "training argument identification", "start_pos": 25, "end_pos": 57, "type": "TASK", "confidence": 0.5743537247180939}, {"text": "precision", "start_pos": 96, "end_pos": 105, "type": "METRIC", "confidence": 0.9997124075889587}, {"text": "recall", "start_pos": 125, "end_pos": 131, "type": "METRIC", "confidence": 0.9996393918991089}]}, {"text": "Partitioning the model by relation type signature offers further benefits, so our best system yields a total boost of 44% to precision and 70% to recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 125, "end_pos": 134, "type": "METRIC", "confidence": 0.9995953440666199}, {"text": "recall", "start_pos": 146, "end_pos": 152, "type": "METRIC", "confidence": 0.9982681274414062}]}], "datasetContent": [{"text": "Relation extraction is often evaluated from a macro-reading perspective (, in which the extracted facts, R(e 1 , e 2 ), are judged true or false independent of any supporting sentence.", "labels": [], "entities": [{"text": "Relation extraction", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.9684805572032928}]}, {"text": "For these experiments, however, we take a micro-reading approach in order to strictly evaluate whether a relation extractor is able to extract every fact expressed by a sentence s\u2192R(m 1 , m 2 ).", "labels": [], "entities": []}, {"text": "Micro-reading is more difficult, but it provides fully semantic information at the sentence and document level allowing detailed justifications, and, for our purposes, allows us to better understand the effects of our modifications.", "labels": [], "entities": []}, {"text": "In order to fairly evaluate different systems, even those using different methods of argument identification, we want to use gold evaluation data allowing for varying mention types.", "labels": [], "entities": [{"text": "argument identification", "start_pos": 85, "end_pos": 108, "type": "TASK", "confidence": 0.7357510924339294}]}, {"text": "We additionally use Hoffmann et al.", "labels": [], "entities": []}, {"text": "(2011)'s sentential evaluation as-is in order to better compare with prior work.", "labels": [], "entities": []}, {"text": "For our training corpus, we use the TAC-KBP 2009) English newswire corpus containing one million documents with 27 million sentences.", "labels": [], "entities": [{"text": "TAC-KBP 2009) English newswire corpus", "start_pos": 36, "end_pos": 73, "type": "DATASET", "confidence": 0.9339921772480011}]}, {"text": "(2011) generated their gold data by taking the union of sentential instances where some system being evaluated extracted a relation as well as the sentential instances matching arguments in the KB.", "labels": [], "entities": []}, {"text": "They took a random sample of these sentential instances and manually labeled them with either a single relation or NA.", "labels": [], "entities": []}, {"text": "Although this process provides good coverage, since is is sampled from extractions over a large corpus, it does not allow one to measure true recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 142, "end_pos": 148, "type": "METRIC", "confidence": 0.9752798080444336}]}, {"text": "Indeed, Hoffmann's method significantly overestimates recall, since the random sample is only over sentential instances where a program detected an extraction or a KB match was found.", "labels": [], "entities": [{"text": "recall", "start_pos": 54, "end_pos": 60, "type": "METRIC", "confidence": 0.9985893368721008}]}, {"text": "Furthermore, this test set only contains sentential instances in which arguments are marked using NER, which makes it impossible to determine if the use of NEL or coreference confers any benefit.", "labels": [], "entities": []}, {"text": "Finally, it does not allow for the possibility that there maybe multiple relations that should be extracted fora pair of arguments.", "labels": [], "entities": []}, {"text": "For example, a CeoOf relation, and an EmployedBy relation might both be present for (Larry Page, Google).", "labels": [], "entities": []}, {"text": "To address these issues, we manually annotate a full set of documents with relation annotations.", "labels": [], "entities": []}, {"text": "Because we are evaluating changing various aspects of the distant supervision process, we cannot use's distant supervision data as-is as others did on the Hoffmann et al.", "labels": [], "entities": []}, {"text": "(2011) sentential evaluation.", "labels": [], "entities": [{"text": "sentential evaluation", "start_pos": 7, "end_pos": 28, "type": "TASK", "confidence": 0.9458916485309601}]}, {"text": "Instead, we use the TAC-KBP data described above.", "labels": [], "entities": [{"text": "TAC-KBP data", "start_pos": 20, "end_pos": 32, "type": "DATASET", "confidence": 0.7522784173488617}]}, {"text": "In order to allow for variations on mentions (NER, NEL, and coreference each has its own definition of what a mention boundary should be), we want gold relation annotations over coreference clusters broadly defined to allow mentions obtained from NER and NEL, as well as gold coreference mentions.", "labels": [], "entities": []}, {"text": "So as long as a relation extraction system extracts a relation annotation s\u2192R(m 1 , m 2 ) where m 1 and m 2 are allowed options (based on text spans), it will get credit for extracting the relation annotation.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 16, "end_pos": 35, "type": "TASK", "confidence": 0.7470617294311523}]}, {"text": "We introduce the GORECO (gold relations and coreference) evaluation to satisfy these constraints.", "labels": [], "entities": [{"text": "GORECO", "start_pos": 17, "end_pos": 23, "type": "METRIC", "confidence": 0.9900833368301392}]}, {"text": "We start with an existing gold coreference dataset, ACE 2004 () newswire, consisting of 128 documents.", "labels": [], "entities": [{"text": "ACE 2004 () newswire", "start_pos": 52, "end_pos": 72, "type": "DATASET", "confidence": 0.9510282427072525}]}, {"text": "To get relation annotations over coreference clusters, we define two human annotation tasks and use the BRAT () tool for visualization and relation and coreference annotations.", "labels": [], "entities": [{"text": "BRAT", "start_pos": 104, "end_pos": 108, "type": "METRIC", "confidence": 0.9925300478935242}]}, {"text": "We conduct experiments to determine how changing distantly supervised relation extraction along various dimensions affects performance.", "labels": [], "entities": [{"text": "distantly supervised relation extraction", "start_pos": 49, "end_pos": 89, "type": "TASK", "confidence": 0.6871092244982719}]}, {"text": "We examine the choice of argument identification during training and extraction, as well as the effects of model type partitioning, and type constraints.", "labels": [], "entities": [{"text": "argument identification", "start_pos": 25, "end_pos": 48, "type": "TASK", "confidence": 0.7151523977518082}, {"text": "model type partitioning", "start_pos": 107, "end_pos": 130, "type": "TASK", "confidence": 0.6703631083170573}]}, {"text": "We consider the space of all combinations of these dimensions, but focus on specific combinations where we find improvements.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Evaluation of different versions of the relation extraction system on the GORECO test set. For  nearly all systems, partitioning the model by argument types boosts F1, as does using NEL at either  training or extraction time, and using coreference at training time with type constraints (w/TC) raises F1  except with coreference at extraction time and when combined with type partitioning.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 50, "end_pos": 69, "type": "TASK", "confidence": 0.7410482168197632}, {"text": "GORECO test set", "start_pos": 84, "end_pos": 99, "type": "DATASET", "confidence": 0.971425990263621}, {"text": "F1", "start_pos": 174, "end_pos": 176, "type": "METRIC", "confidence": 0.9989497065544128}, {"text": "F1", "start_pos": 311, "end_pos": 313, "type": "METRIC", "confidence": 0.999333918094635}]}, {"text": " Table 2: By-relation evaluation of the best system  (NEL with type constraints and type partitioning)  on the GORECO test set. The true positives (TP)  are the number of gold relations over coreference  clusters that matched, so multiple extractions can  match a single true positive.", "labels": [], "entities": [{"text": "GORECO test set", "start_pos": 111, "end_pos": 126, "type": "DATASET", "confidence": 0.9454002579053243}, {"text": "true positives (TP)", "start_pos": 132, "end_pos": 151, "type": "METRIC", "confidence": 0.7812696099281311}]}]}