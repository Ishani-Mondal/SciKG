{"title": [{"text": "Dependency-Based Bilingual Language Models for Reordering in Statistical Machine Translation", "labels": [], "entities": [{"text": "Reordering", "start_pos": 47, "end_pos": 57, "type": "TASK", "confidence": 0.9709203243255615}, {"text": "Statistical Machine Translation", "start_pos": 61, "end_pos": 92, "type": "TASK", "confidence": 0.6763898630936941}]}], "abstractContent": [{"text": "This paper presents a novel approach to improve reordering in phrase-based machine translation by using richer, syntactic representations of units of bilingual language models (BiLMs).", "labels": [], "entities": [{"text": "phrase-based machine translation", "start_pos": 62, "end_pos": 94, "type": "TASK", "confidence": 0.6229880849520365}]}, {"text": "Our method to include syntactic information is simple in implementation and requires minimal changes in the decoding algorithm.", "labels": [], "entities": []}, {"text": "The approach is evaluated in a series of Arabic-English and Chinese-English translation experiments.", "labels": [], "entities": []}, {"text": "The best models demonstrate significant improvements in BLEU and TER over the phrase-based baseline, as well as over the lexicalized BiLM by Niehues et al.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 56, "end_pos": 60, "type": "METRIC", "confidence": 0.9978972673416138}, {"text": "TER", "start_pos": 65, "end_pos": 68, "type": "METRIC", "confidence": 0.9940296411514282}]}, {"text": "Further improvements of up to 0.45 BLEU for Arabic-English and up to 0.59 BLEU for Chinese-English are obtained by combining our dependency BiLM with a lexicalized BiLM.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 35, "end_pos": 39, "type": "METRIC", "confidence": 0.9985268115997314}, {"text": "BLEU", "start_pos": 74, "end_pos": 78, "type": "METRIC", "confidence": 0.9975530505180359}]}, {"text": "An improvement of 0.98 BLEU is obtained for Chinese-English in the setting of an increased distortion limit.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 23, "end_pos": 27, "type": "METRIC", "confidence": 0.99950110912323}, {"text": "distortion", "start_pos": 91, "end_pos": 101, "type": "METRIC", "confidence": 0.9507331848144531}]}], "introductionContent": [{"text": "In statistical machine translation (SMT) reordering (also called distortion) refers to the order in which source words are translated to generate the translation in the target language.", "labels": [], "entities": [{"text": "statistical machine translation (SMT) reordering", "start_pos": 3, "end_pos": 51, "type": "TASK", "confidence": 0.8192983525139945}]}, {"text": "Word orders can differ significantly across languages.", "labels": [], "entities": []}, {"text": "For instance, Arabic declarative sentences can be verbinitial, while the corresponding English translation should realize the verb after the subject, hence requiring a reordering.", "labels": [], "entities": []}, {"text": "Determining the correct reordering during decoding is a major challenge for SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 76, "end_pos": 79, "type": "TASK", "confidence": 0.9946411848068237}]}, {"text": "This problem has received a lot of attention in the literature (see, e.g.,,,), as choosing the correct reordering improves readability of the translation and can have a substantial impact on translation quality.", "labels": [], "entities": []}, {"text": "In this paper, we only consider those approaches that include a reordering feature function into the loglinear interpolation used during decoding.", "labels": [], "entities": []}, {"text": "The simplest reordering model is linear distortion ( which scores the distance between phrases translated at steps t and t + 1 of the derivation.", "labels": [], "entities": [{"text": "linear distortion", "start_pos": 33, "end_pos": 50, "type": "METRIC", "confidence": 0.8036259412765503}]}, {"text": "This model ignores any contextual information, as the distance between translated phrases is its only parameter.", "labels": [], "entities": []}, {"text": "Lexical distortion modeling) conditions reordering probabilities on the phrase pairs translated at the current and previous steps.", "labels": [], "entities": [{"text": "Lexical distortion modeling", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.8655064503351847}]}, {"text": "Unlike linear distortion, it characterizes reordering not in terms of distance but type: monotone, swap, or discontinuous.", "labels": [], "entities": []}, {"text": "In this paper, we base our approach to reordering on bilingual language models ().", "labels": [], "entities": []}, {"text": "Instead of directly characterizing reordering, they model sequences of elementary translation events as a Markov process.", "labels": [], "entities": []}, {"text": "Originally, used this kind of model as the translation model, while more recently it has been used as an additional model in PBSMT systems).", "labels": [], "entities": [{"text": "translation", "start_pos": 43, "end_pos": 54, "type": "TASK", "confidence": 0.9735987782478333}]}, {"text": "We adopt and generalize the approach of to investigate several variations of bilingual language models.", "labels": [], "entities": []}, {"text": "Our method consists of labeling elementary translation events (tokens of bilingual LMs) with their different contextual properties.", "labels": [], "entities": [{"text": "labeling elementary translation events (tokens of bilingual LMs", "start_pos": 23, "end_pos": 86, "type": "TASK", "confidence": 0.8058923019303216}]}, {"text": "What kind of contextual information should be incorporated in a reordering model?", "labels": [], "entities": []}, {"text": "Lexical information has been used by but is known to suffer from data sparsity.", "labels": [], "entities": []}, {"text": "Also previous contributions to bilingual language modeling () have mostly used lexical information, although and label bilingual to-kens with a rich set of POS tags.", "labels": [], "entities": [{"text": "bilingual language modeling", "start_pos": 31, "end_pos": 58, "type": "TASK", "confidence": 0.6424913307030996}]}, {"text": "But in general, reordering is considered to be a syntactic phenomenon and thus the relevant features are syntactic.", "labels": [], "entities": []}, {"text": "Syntactic information is incorporated in tree-based approaches in SMT, allowing one to provide a more detailed definition of translation events and to redefine decoding as parsing of a source string (, of a target string, or both.", "labels": [], "entities": [{"text": "SMT", "start_pos": 66, "end_pos": 69, "type": "TASK", "confidence": 0.9908085465431213}]}, {"text": "Reordering is a result of a given derivation, and CYK-based decoding used in tree-based approaches is more syntax-aware than the simple PBSMT decoding algorithm.", "labels": [], "entities": []}, {"text": "Although tree-based approaches potentially offer a more accurate model of translation, they are also a lot more complex and requiring more intricate optimization and estimation techniques.", "labels": [], "entities": [{"text": "translation", "start_pos": 74, "end_pos": 85, "type": "TASK", "confidence": 0.9748075604438782}]}, {"text": "Our idea is to keep the simplicity of PBSMT but move towards the expressiveness typical of treebased models.", "labels": [], "entities": [{"text": "simplicity", "start_pos": 24, "end_pos": 34, "type": "METRIC", "confidence": 0.9879989624023438}]}, {"text": "We incrementally buildup the syntactic representation of a translation during decoding by adding precomputed fragments from the source parse tree.", "labels": [], "entities": []}, {"text": "The idea to combine the merits of the two SMT paradigms has been proposed before, where introduce incremental decoding fora tree-based model.", "labels": [], "entities": [{"text": "SMT", "start_pos": 42, "end_pos": 45, "type": "TASK", "confidence": 0.9902729392051697}]}, {"text": "On a very general level, our approach is similar to theirs in that it keeps track of a sequence of source syntactic subtrees that are being translated at consecutive decoding steps.", "labels": [], "entities": []}, {"text": "An important difference is that they keep track of whether the visited subtrees have been fully translated, while in our approach, once a syntactic structural unit has been added to the history, it is not updated anymore.", "labels": [], "entities": []}, {"text": "In this paper, we focus on source syntactic information.", "labels": [], "entities": []}, {"text": "During decoding we have full access to the source sentence, which allows us to obtain a better syntactic analysis (than fora partial sentence) and to precompute the units that the model operates with.", "labels": [], "entities": []}, {"text": "We investigate the following research questions: How well can we capture reordering regularities of a language pair by incorporating source syntactic parameters into the units of a bilingual language model?", "labels": [], "entities": []}, {"text": "What kind of source syntactic parameters are necessary and sufficient?", "labels": [], "entities": []}, {"text": "Our contributions can be summarized as follows: We argue that the contextual information used in the original bilingual models) is insufficient and introduce a simple model that exploits source-side syntax to improve reordering (Sections 2 and 3).", "labels": [], "entities": []}, {"text": "We perform a thorough comparison between different variants of our general model and compare them to the original approach.", "labels": [], "entities": []}, {"text": "We carryout translation experiments on multiple test sets, two language pairs (ArabicEnglish and Chinese-English), and with respect to two metrics (BLEU and TER).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 148, "end_pos": 152, "type": "METRIC", "confidence": 0.9982619881629944}, {"text": "TER", "start_pos": 157, "end_pos": 160, "type": "METRIC", "confidence": 0.9788357615470886}]}, {"text": "Finally, we present a preliminary analysis of the reorderings resulting from the proposed models (Section 4).", "labels": [], "entities": []}], "datasetContent": [{"text": "We are interested in how a translation system with an integrated dependency-based BiLM feaand several gale corpora.", "labels": [], "entities": []}, {"text": "ture performs as compared to the standard PB-SMT baseline and, more importantly, to the original BiLM model.", "labels": [], "entities": [{"text": "ture", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9190766215324402}, {"text": "PB-SMT baseline", "start_pos": 42, "end_pos": 57, "type": "DATASET", "confidence": 0.838579535484314}]}, {"text": "We consider two variants of BiLM discussed by: the standard one, and the simplest syntactic one, Pos\u2022Pos.", "labels": [], "entities": []}, {"text": "Results for the experiments can be found in.", "labels": [], "entities": []}, {"text": "In the discussion below we mostly focus on the experimental results for the large, combined test set MT08+MT09..a-b compares the performance of the baseline and original BiLM systems.\u2022Lex yields strongly significant improvements over the baseline for BLEU and weakly significant improvements for TER.", "labels": [], "entities": [{"text": "MT08", "start_pos": 101, "end_pos": 105, "type": "DATASET", "confidence": 0.8529112935066223}, {"text": "MT09.", "start_pos": 106, "end_pos": 111, "type": "DATASET", "confidence": 0.7861281633377075}, {"text": "BLEU", "start_pos": 251, "end_pos": 255, "type": "METRIC", "confidence": 0.9532402753829956}, {"text": "TER", "start_pos": 296, "end_pos": 299, "type": "METRIC", "confidence": 0.9591723680496216}]}, {"text": "Therefore, for the rest of the experiments we are interested in obtaining further improvements over Pos\u2192Pos\u2022Pos.c) demonstrates the effect of adding minimal dependency information to a BiLM.", "labels": [], "entities": []}, {"text": "It results in strongly significant improvements over the baseline and weak improvements over\u2022Lex in terms of BLEU.", "labels": [], "entities": [{"text": "Lex", "start_pos": 93, "end_pos": 96, "type": "METRIC", "confidence": 0.600473940372467}, {"text": "BLEU", "start_pos": 109, "end_pos": 113, "type": "METRIC", "confidence": 0.9978302121162415}]}, {"text": "We additionally ran experiments with the different target functions).", "labels": [], "entities": []}, {"text": "\u2022Pos shows the highest results, and \u2022 the lowest ones: this implies that a rather expressive source syntactic representation alone still benefits from target-side syntactic information.", "labels": [], "entities": []}, {"text": "Below, our dependency-based systems only use \u2022Pos.", "labels": [], "entities": []}, {"text": "Next, we tested the effect of adding more source   So far, we can conclude that source parent information helps improve translation performance.", "labels": [], "entities": [{"text": "translation", "start_pos": 120, "end_pos": 131, "type": "TASK", "confidence": 0.9705482721328735}]}, {"text": "Increased specificity of a parent (parent specified by a grandparent) tends to further improve performance.", "labels": [], "entities": []}, {"text": "Up to now, we have only used syntactic information and obtained considerable improvements over Pos\u2022Pos, surpassing the improvement provided by Can we gain further improvements by also adding lexical information?", "labels": [], "entities": []}, {"text": "To this end, we conduct experiments combining the best performing dependency-based BiLM (Pos\u2192Pos\u2192Pos\u2022Pos) and the lexicalized BiLM (Lex\u2022Lex).", "labels": [], "entities": []}, {"text": "We hypothesize that the two models improve different aspects of translation: Lex\u2022Lex is biased towards improving lexical choice and Pos\u2192Pos\u2192Pos\u2022Pos towards improving reordering.", "labels": [], "entities": [{"text": "translation", "start_pos": 64, "end_pos": 75, "type": "TASK", "confidence": 0.9685232043266296}]}, {"text": "Combining these two models, we may improve both aspects.", "labels": [], "entities": []}, {"text": "The metric results for the combined set indeed support this hypothesis.f).", "labels": [], "entities": []}, {"text": "The results of the Chinese-English experiments are shown in.", "labels": [], "entities": []}, {"text": "In the discussion below we mostly focus on the experimental results for the large, combined test set MT06+MT08.", "labels": [], "entities": [{"text": "MT06", "start_pos": 101, "end_pos": 105, "type": "DATASET", "confidence": 0.8744462132453918}, {"text": "MT08", "start_pos": 106, "end_pos": 110, "type": "DATASET", "confidence": 0.8563870787620544}]}, {"text": "We observe the same general pattern for the Pos\u2192Pos source function.c) as for Arabic-English: the system with the \u2022Pos target function has the highest scores ().", "labels": [], "entities": []}, {"text": "All of the Pos\u2192Pos\u2022 configurations show statistically significant improvements over the PBSMT baseline.", "labels": [], "entities": [{"text": "PBSMT baseline", "start_pos": 88, "end_pos": 102, "type": "DATASET", "confidence": 0.9525138735771179}]}, {"text": "For TER, two of the three Pos\u2192Pos\u2022 variants significantly outperform Lex\u2022Lex.", "labels": [], "entities": [{"text": "TER", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.661278486251831}, {"text": "Lex\u2022Lex", "start_pos": 69, "end_pos": 76, "type": "DATASET", "confidence": 0.7249836126963297}]}, {"text": "The system with sibling information (    We use 4-gram precision as a metric of how much of the reference set word order is preserved.", "labels": [], "entities": [{"text": "precision", "start_pos": 55, "end_pos": 64, "type": "METRIC", "confidence": 0.9135146737098694}]}, {"text": "shows the corresponding results for both languages.", "labels": [], "entities": []}, {"text": "Just as in the previous two sections, configurations with parental information produce the best results.", "labels": [], "entities": []}, {"text": "For Arabic, all of the dependency configurations outperform Lex\u2022Lex.", "labels": [], "entities": []}, {"text": "But the system with two feature functions, one of which is Lex\u2022Lex, still obtains the best results, which may suggest that the lexicalized BiLM also helps to differentiate between word orders.", "labels": [], "entities": [{"text": "BiLM", "start_pos": 139, "end_pos": 143, "type": "METRIC", "confidence": 0.8992505073547363}]}, {"text": "For Chinese, Pos\u2192Pos\u2192Pos\u2022Pos and the system combining the latter and Lex\u2022Lex also obtain the best results.", "labels": [], "entities": []}, {"text": "However, other dependency-based configurations do not outperform All the experiments so far were run with a distortion limit of 5.", "labels": [], "entities": []}, {"text": "But both of the languages, especially Chinese, often require reorderings over a longer distance.", "labels": [], "entities": []}, {"text": "We performed additional experiments with a distortion limit of 10 for the Lex\u2022Lex and Pos\u2192Pos\u2192Pos\u2022Pos systems.", "labels": [], "entities": [{"text": "distortion limit", "start_pos": 43, "end_pos": 59, "type": "METRIC", "confidence": 0.9592253863811493}, {"text": "Lex\u2022Lex", "start_pos": 74, "end_pos": 81, "type": "DATASET", "confidence": 0.804771343866984}]}, {"text": "It is more difficult to translate with a higher distortion limit (  as the set of permutations grows larger thereby making it more difficult to differentiate between correct and incorrect continuations of the current hypothesis.", "labels": [], "entities": []}, {"text": "It has also been noted that higher distortion limits are more likely to result in improvements for Chinese rather than Arabic to English translation.", "labels": [], "entities": []}, {"text": "We compared performance of fixed BiLM models at distortion lengths of 5 and 10.", "labels": [], "entities": []}, {"text": "ArabicEnglish results did not reveal statistically significant differences between the two distortion limits for Pos\u2192Pos\u2192Pos\u2022Pos.", "labels": [], "entities": [{"text": "ArabicEnglish", "start_pos": 0, "end_pos": 13, "type": "DATASET", "confidence": 0.9226498603820801}]}, {"text": "On the other hand, for Lex\u2022Lex BLEU decreases when using a distortion limit of 10 compared to a limit of 5.", "labels": [], "entities": [{"text": "Lex\u2022Lex", "start_pos": 23, "end_pos": 30, "type": "DATASET", "confidence": 0.7384744683901469}, {"text": "BLEU", "start_pos": 31, "end_pos": 35, "type": "METRIC", "confidence": 0.9866952896118164}, {"text": "distortion", "start_pos": 59, "end_pos": 69, "type": "METRIC", "confidence": 0.9859678745269775}]}, {"text": "This implies that the dependency BiLM is more robust in the more challenging reordering setting than the lexicalized BiLM.", "labels": [], "entities": []}, {"text": "Chinese-English results for Pos\u2192Pos\u2192Pos\u2022Pos do show significant improvements over the distortion limit of 5 (up to 0.49 BLEU higher than the best result in).", "labels": [], "entities": [{"text": "distortion", "start_pos": 86, "end_pos": 96, "type": "METRIC", "confidence": 0.9783667922019958}, {"text": "BLEU", "start_pos": 120, "end_pos": 124, "type": "METRIC", "confidence": 0.9983277916908264}]}, {"text": "This indicates that the dependency-based BiLM is better capable to take advantage of the increased distortion limit and discriminate between correct and incorrect reordering choices.", "labels": [], "entities": []}, {"text": "Comparing the results for Pos\u2192Pos\u2192Pos\u2022Pos and Lex\u2022Lex at a distortion limit of 10, we obtain strongly significant improvements for all metrics.", "labels": [], "entities": [{"text": "distortion", "start_pos": 59, "end_pos": 69, "type": "METRIC", "confidence": 0.9437354803085327}]}, {"text": "For Chinese, a larger distortion limit helps for both configurations, but more so for our dependency BiLM, yielding an improvement of 0.98 BLEU over the original, lexicalized BiLM.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 139, "end_pos": 143, "type": "METRIC", "confidence": 0.9987490177154541}]}], "tableCaptions": [{"text": " Table 1: Training data for Arabic-English and  Chinese-English experiments.", "labels": [], "entities": []}, {"text": " Table 2: BLEU and TER scores for Arabic-English experiments. Statistically significant improvements  over the baseline (a) are marked at the p < .01 level and at the p < .05 level. Additionally, \u00b7, and  \u00b7, indicate significant improvements with respect to BiLM Lex\u2022Lex (b). Since TER is an error rate, lower  scores are better.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9975602626800537}, {"text": "TER", "start_pos": 19, "end_pos": 22, "type": "METRIC", "confidence": 0.9962573051452637}, {"text": "BiLM Lex\u2022Lex (b)", "start_pos": 257, "end_pos": 273, "type": "METRIC", "confidence": 0.8203027163233075}, {"text": "TER", "start_pos": 281, "end_pos": 284, "type": "METRIC", "confidence": 0.9948967099189758}]}, {"text": " Table 4: BLEU and TER scores for Chinese-English PBSMT baseline and BiLM pipelines. See Table 2  for the notation regarding statistical significance.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.999015212059021}, {"text": "TER", "start_pos": 19, "end_pos": 22, "type": "METRIC", "confidence": 0.9972314238548279}]}, {"text": " Table 5: Different combinations of a target contextual function with the Pos\u2192Pos source contextual func- tion for Chinese-English. See Table 2 for the notation regarding statistical significance.", "labels": [], "entities": []}, {"text": " Table 6: 4-gram precision scores for Arabic-English and Chinese-English baseline and BiLM systems.", "labels": [], "entities": [{"text": "precision scores", "start_pos": 17, "end_pos": 33, "type": "METRIC", "confidence": 0.9548704028129578}]}, {"text": " Table 7: BLEU, TER and 4-gram precision scores for Arabic-English Lex\u2022Lex and Pos\u2192Pos\u2192Pos\u2022Pos  with a distortion limit of 10.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9995498061180115}, {"text": "TER", "start_pos": 16, "end_pos": 19, "type": "METRIC", "confidence": 0.9989722967147827}, {"text": "precision", "start_pos": 31, "end_pos": 40, "type": "METRIC", "confidence": 0.9768437743186951}]}, {"text": " Table 8:  BLEU, TER and 4-gram precision scores for Chinese-English Lex\u2022Lex and  Pos\u2192Pos\u2192Pos\u2022Pos with a distortion limit of 10.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 11, "end_pos": 15, "type": "METRIC", "confidence": 0.9995181560516357}, {"text": "TER", "start_pos": 17, "end_pos": 20, "type": "METRIC", "confidence": 0.9989989399909973}, {"text": "precision", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.9756041169166565}]}]}