{"title": [{"text": "Policy Learning for Domain Selection in an Extensible Multi-domain Spoken Dialogue System", "labels": [], "entities": [{"text": "Domain Selection", "start_pos": 20, "end_pos": 36, "type": "TASK", "confidence": 0.7212623059749603}]}], "abstractContent": [{"text": "This paper proposes a Markov Decision Process and reinforcement learning based approach for domain selection in a multi-domain Spoken Dialogue System built on a distributed architecture.", "labels": [], "entities": [{"text": "domain selection", "start_pos": 92, "end_pos": 108, "type": "TASK", "confidence": 0.6962820738554001}]}, {"text": "In the proposed framework, the domain selection problem is treated as sequential planning instead of classification, such that confirmation and clarification interaction mechanisms are supported.", "labels": [], "entities": []}, {"text": "In addition, it is shown that by using a model parameter tying trick, the extensibility of the system can be preserved, where dialogue components in new domains can be easily plugged in, without retraining the domain selection policy.", "labels": [], "entities": [{"text": "model parameter tying", "start_pos": 41, "end_pos": 62, "type": "TASK", "confidence": 0.7047317226727804}]}, {"text": "The experimental results based on human subjects suggest that the proposed model marginally outperforms a non-trivial baseline.", "labels": [], "entities": []}], "introductionContent": [{"text": "Due to growing demand for natural humanmachine interaction, over the last decade Spoken Dialogue Systems (SDS) have been increasingly deployed in various commercial applications ranging from traditional call centre automation (e.g. AT&T \"Lets Go!\" bus information system () to mobile personal assistants and knowledge navigators (e.g. Apple's Siri R , Google Now TM , Microsoft Cortana, etc.) or voice interaction for smart household appliance control (e.g. Samsung Evolution Kit for Smart TVs).", "labels": [], "entities": []}, {"text": "Furthermore, latest progress in openvocabulary Automatic Speech Recognition (ASR) is pushing SDS from traditional single-domain information systems towards more complex multidomain speech applications, of which typical examples are those voice assistant mobile applications.", "labels": [], "entities": [{"text": "openvocabulary Automatic Speech Recognition (ASR)", "start_pos": 32, "end_pos": 81, "type": "TASK", "confidence": 0.7391301734106881}, {"text": "SDS", "start_pos": 93, "end_pos": 96, "type": "TASK", "confidence": 0.9832891821861267}]}, {"text": "Recent advances in SDS have shown that statistical approaches to dialogue management can result in marginal improvement in both the naturalness and the task success rate for domainspecific dialogues (.", "labels": [], "entities": [{"text": "SDS", "start_pos": 19, "end_pos": 22, "type": "TASK", "confidence": 0.983693540096283}, {"text": "dialogue management", "start_pos": 65, "end_pos": 84, "type": "TASK", "confidence": 0.8130702674388885}]}, {"text": "State-of-the-art statistical SDS treat the dialogue problem as a sequential decision making process, and employ established planning models, such as Markov Decision Processes (MDPs) () or Partially Observable Markov Decision Processes (POMDPs), in conjunction with reinforcement learning techniques to seek optimal dialogue policies that maximise long-term expected (discounted) rewards and are robust to ASR errors.", "labels": [], "entities": [{"text": "ASR", "start_pos": 405, "end_pos": 408, "type": "TASK", "confidence": 0.9813687801361084}]}, {"text": "However, to the best of our knowledge, most of the existing multi-domain SDS in public use are rule-based (e.g. ().", "labels": [], "entities": []}, {"text": "The application of statistical models in multi-domain dialogue systems is still preliminary. and utilised a distributed architecture () to integrate expert dialogue systems in different domains into a unified framework, where a central controller trained as a data-driven classifier selects a domain expert at each turn to address user's query.", "labels": [], "entities": []}, {"text": "Alternatively, adopted the well-known Information State mechanism to construct a multi-domain SDS and proposed a discriminative classification model for more accurate state updates.", "labels": [], "entities": []}, {"text": "More recently, Ga\u0161i\u00b4 proposed that by a simple expansion of the kernel function in Gaussian Process (GP) reinforcement learning (), one can adapt pre-trained dialogue policies to handle unseen slots for SDS in extended domains.", "labels": [], "entities": []}, {"text": "In this paper, we use a voice assistant applica-  tion (similar to Apple's Siri but in Chinese language) as an example to demonstrate a novel MDP-based approach for central interaction management in a complex multi-domain dialogue system.", "labels": [], "entities": [{"text": "central interaction management", "start_pos": 165, "end_pos": 195, "type": "TASK", "confidence": 0.642948975165685}]}, {"text": "The voice assistant employs a distributed architecture similar to (, and handles mixed interactions of multi-turn dialogues across different domains and single-turn queries powered by a collection of information access services (such as web search, Question Answering (QA), etc.).", "labels": [], "entities": [{"text": "Question Answering (QA)", "start_pos": 249, "end_pos": 272, "type": "TASK", "confidence": 0.8454898655414581}]}, {"text": "In our system, the dialogues in each domain are managed by an individual domain expert SDS, and the single-turn services are used to handle those so-called out-of-domain requests.", "labels": [], "entities": []}, {"text": "We use featurised representations to summarise the current dialogue states in each domain (see Section 3 for more details), and let the central controller (the MDP model) choose one of the following system actions at each turn: (1) addressing user's query based on a domain expert, (2) treating it as an out-of-domain request, (3) asking user to confirm whether he/she wants to continue a domain expert's dialogue or to switch to out-of-domain services, and (4) clarifying user's intention between two domains.", "labels": [], "entities": []}, {"text": "The Gaussian Process Temporal Difference (GPTD) algorithm () is adopted here for policy optimisation based on human subjects, where a parameter tying trick is applied to preserve the extensibility of the system, such that new domain experts (dialogue systems) can be flexibly plugged in without the need of re-training the central controller.", "labels": [], "entities": [{"text": "Gaussian Process Temporal Difference (GPTD)", "start_pos": 4, "end_pos": 47, "type": "TASK", "confidence": 0.7371256010872977}]}, {"text": "Comparing to the previous classification-based methods (), the proposed approach not only has the advantage of action selection in consideration of long-term rewards, it can also yield more robust policies that allow clarifications and confirmations to mitigate ASR and Spoken Language Understanding (SLU) errors.", "labels": [], "entities": [{"text": "ASR", "start_pos": 262, "end_pos": 265, "type": "TASK", "confidence": 0.93598473072052}]}, {"text": "Our human evaluation results show that the proposed system with a trained MDP policy achieves significantly better naturalness in domain switching tasks than a non-trivial baseline with a hand-crafted policy.", "labels": [], "entities": [{"text": "domain switching tasks", "start_pos": 130, "end_pos": 152, "type": "TASK", "confidence": 0.7842207749684652}]}, {"text": "The remainder of this paper is organised as follows.", "labels": [], "entities": []}, {"text": "Section 2 defines the terminology used throughout the paper.", "labels": [], "entities": []}, {"text": "Section 3 briefly overviews the distributed architecture of our system.", "labels": [], "entities": []}, {"text": "The MDP model and the policy optimisation algorithm are introduced in Section 4 and Section 5, respectively.", "labels": [], "entities": [{"text": "policy optimisation", "start_pos": 22, "end_pos": 41, "type": "TASK", "confidence": 0.8035708963871002}]}, {"text": "After this, experimental settings and evaluation results are described in Section 6.", "labels": [], "entities": []}, {"text": "Finally, we discuss some possible improvements in Section 7 and conclude ourselves in Section 8.", "labels": [], "entities": [{"text": "Section 7", "start_pos": 50, "end_pos": 59, "type": "DATASET", "confidence": 0.7663906514644623}]}], "datasetContent": [{"text": "We conducted paired comparison experiments in four scenarios to compare between the system with the GPTD-learnt central control policy and a non-trivial baseline.", "labels": [], "entities": [{"text": "GPTD-learnt central control policy", "start_pos": 100, "end_pos": 134, "type": "DATASET", "confidence": 0.8464486598968506}]}, {"text": "The baseline is a publicly deployed version of the voice assistant application.", "labels": [], "entities": []}, {"text": "The central control policy of the baseline system is handcrafted, which has a separate list of semantic matching rules for each domain to enable domain switching.", "labels": [], "entities": [{"text": "domain switching", "start_pos": 145, "end_pos": 161, "type": "TASK", "confidence": 0.7110557407140732}]}, {"text": "The first two scenarios aim to test the performance of the two systems on (i) switching between a domain expert and out-of-domain services, and (ii) switching between two domain experts, where only the two training domains (travel information and restaurant search) were considered.", "labels": [], "entities": []}, {"text": "Scenarios (iii) and (iv) are similar to scenarios (i) and (ii) respectively, but at this time, the users were required to carryout the tests surrounding the movie search domain (which is addressed by anew domain expert not used in the training phase).", "labels": [], "entities": []}, {"text": "There were 13 users who participated this experiment.", "labels": [], "entities": []}, {"text": "In each scenario, every user was required to test the two systems with an identical goal and similar queries.", "labels": [], "entities": []}, {"text": "After each test, the users were asked to score the two systems separately according to the scoring standard in.", "labels": [], "entities": []}, {"text": "The average scores received by the two systems are shown in, where we also compute the statistical significance (the p-values) of the results based on paired t-tests.", "labels": [], "entities": []}, {"text": "It can be found that the learnt policy works significantly better than the rule-based policy in scenarios (ii) and (iv), but in scenarios (i) and (iii) the differences between two systems are statistically insignificant.", "labels": [], "entities": []}, {"text": "Moreover, the learnt policy preserves the extensibility of the entire system as expected, of which strong evidences are given by the results in scenarios (iii) and (iv).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Paired comparison experiments between  the system with a trained GPTD policy and the  rule-based baseline.", "labels": [], "entities": [{"text": "GPTD policy", "start_pos": 75, "end_pos": 86, "type": "DATASET", "confidence": 0.8507182598114014}]}, {"text": " Table 4: Feature weights learnt by GPTD. See Ta- ble 1 for the meanings of the features.", "labels": [], "entities": [{"text": "GPTD", "start_pos": 36, "end_pos": 40, "type": "DATASET", "confidence": 0.9430181980133057}]}]}