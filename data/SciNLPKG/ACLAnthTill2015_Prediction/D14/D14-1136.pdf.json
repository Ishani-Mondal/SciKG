{"title": [{"text": "Semantic Parsing Using Content and Context: A Case Study from Requirements Elicitation", "labels": [], "entities": [{"text": "Semantic Parsing", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8431834280490875}]}], "abstractContent": [{"text": "We present a model for the automatic semantic analysis of requirements elicitation documents.", "labels": [], "entities": [{"text": "automatic semantic analysis of requirements elicitation documents", "start_pos": 27, "end_pos": 92, "type": "TASK", "confidence": 0.7977785766124725}]}, {"text": "Our target semantic representation employs live sequence charts, a multi-modal visual language for scenario-based programming, which can be directly translated into executable code.", "labels": [], "entities": []}, {"text": "The architecture we propose integrates sentence-level and discourse-level processing in a generative probabilistic framework for the analysis and disambiguation of individual sentences in context.", "labels": [], "entities": [{"text": "analysis and disambiguation of individual sentences in context", "start_pos": 133, "end_pos": 195, "type": "TASK", "confidence": 0.7419207021594048}]}, {"text": "We show empirically that the discourse-based model consistently outperforms the sentence-based model when constructing a system that reflects all the static (entities, properties) and dynamic (behavioral scenarios) requirements in the document.", "labels": [], "entities": []}], "introductionContent": [{"text": "Requirements elicitation is a process whereby a system analyst gathers information from a stakeholder about a desired system (software or hardware) to be implemented.", "labels": [], "entities": [{"text": "Requirements elicitation", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.9325679838657379}]}, {"text": "The knowledge collected by the analyst maybe static, referring to the conceptual model (the entities, their properties, the possible values) or dynamic, referring to the behavior that the system should follow (who does what to whom, when, how, etc).", "labels": [], "entities": []}, {"text": "A stakeholder interested in the system typically has a specific static and dynamic domain in mind, but he or she cannot necessarily prescribe any formal models or code artifacts.", "labels": [], "entities": []}, {"text": "The term requirements elicitation we use here refers to apiece of discourse in natural language, by means of which a stakeholder communicates their desiderata to the system analyst.", "labels": [], "entities": []}, {"text": "The role of a system analyst is to understand the different requirements and transform them into formal constructs, formal diagrams or executable code.", "labels": [], "entities": []}, {"text": "Moreover, the analyst needs to consolidate the different pieces of information to uncover a single shared domain.", "labels": [], "entities": []}, {"text": "Studies in software engineering aim to develop intuitive symbolic systems with which human agents can encode requirements that would then be unambiguously translated into a formal model ().", "labels": [], "entities": []}, {"text": "More recently,  defined a natural fragment of English that can be used for specifying requirements which can be effectively translated into live sequence charts (LSC), a formal language for specifying the dynamic behavior of reactive systems.", "labels": [], "entities": []}, {"text": "However, the grammar that underlies this language fragment is highly ambiguous, and all disambiguation has to be conducted manually by a human agent.", "labels": [], "entities": []}, {"text": "Indeed, it is commonly accepted that the more natural a controlled language fragment is, the harder it is to develop an unambiguous translation mechanism.", "labels": [], "entities": []}, {"text": "In this paper we accept the ambiguity of requirements descriptions as a premise, and aim to answer the following question: can we automatically recover a formal representation of the complete system -one that best reflects the humanperceived interpretation of the entire document?", "labels": [], "entities": []}, {"text": "Recent advances in natural language processing, with an eye to semantic parsing), use different formalisms and various kinds of learning signals for statistical semantic parsing.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 19, "end_pos": 46, "type": "TASK", "confidence": 0.6603809197743734}, {"text": "semantic parsing", "start_pos": 63, "end_pos": 79, "type": "TASK", "confidence": 0.7337788343429565}, {"text": "statistical semantic parsing", "start_pos": 149, "end_pos": 177, "type": "TASK", "confidence": 0.6804886261622111}]}, {"text": "In particular, the model of induces input parsers from format descriptions.", "labels": [], "entities": []}, {"text": "However, rarely do these models take into account the entire document's context.", "labels": [], "entities": []}, {"text": "The key idea we promote here is that discourse context provides substantial disambiguating information for sentence analysis.", "labels": [], "entities": [{"text": "sentence analysis", "start_pos": 107, "end_pos": 124, "type": "TASK", "confidence": 0.7134079933166504}]}, {"text": "We suggest a novel: An LSC scenario: \"When the user clicks the button, the display color must change to red.\" model for integrated sentence-level and discourselevel processing, in a joint generative probabilistic framework.", "labels": [], "entities": []}, {"text": "The input for the requirements elicitation task is given in a simplified, yet highly ambiguous, fragment of English, as specified in.", "labels": [], "entities": []}, {"text": "The output, in contrast, is a sequence of unambiguous and well-formed live sequence charts (LSC)) describing the dynamic behavior of the system, tied to a single shared code-base called a system model (SM).", "labels": [], "entities": []}, {"text": "Our solution takes the form of a hidden markov model (HMM) where emission probabilities reflect the grammaticality and interpretability of textual requirements via a probabilistic grammar and transition probabilities model the overlap between SM snapshots of a single, shared, domain.", "labels": [], "entities": []}, {"text": "Using efficient viterbi decoding, we search for the best sequence of domain snapshots that has most likely generated the entire requirements document.", "labels": [], "entities": []}, {"text": "We empirically show that such an integrated model consistently outperforms a sentence-based model learned from the same set of data.", "labels": [], "entities": []}, {"text": "The remainder of this document is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2 we describe the task, and spell out our formal assumptions concerning the input and the output.", "labels": [], "entities": []}, {"text": "In Section 3 we present our target semantic representation and a specially tailored notion of grounding for anchoring the requirements in a code-base.", "labels": [], "entities": []}, {"text": "In Section 4 we develop our sentence-based and discourse-based models, and in Section 5 we evaluate the models on various case studies.", "labels": [], "entities": []}, {"text": "In Section 6 we discuss applications and future extensions, and in Section 7 we summarize and conclude.", "labels": [], "entities": []}], "datasetContent": [{"text": "We set out to evaluate the accuracy of a semantic parser for requirements documents, in the two modes of analysis presented above.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9988254904747009}]}, {"text": "Our evaluation methodology is as standardly assumed in machine learning and NLP: given a set of annotated examples -that is, given a set of requirements documents, where each requirement is annotated with its correct LSC representation and each document is associated with a complete SM -we partition this set into a training set and a test set that are disjoint.", "labels": [], "entities": []}, {"text": "We train our statistical model on the examples in the training set and automatically analyze the requirements in the test set.", "labels": [], "entities": []}, {"text": "We then compare the predicted semantic analyses of the test set with the human-annotated (henceforth, gold) semantic analyses of this test set, and empirically quantify our prediction accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 184, "end_pos": 192, "type": "METRIC", "confidence": 0.9703029990196228}]}, {"text": "Our semantic LSC objects have the form of a tree (reflecting the sequence of nested events in our scenarios).", "labels": [], "entities": []}, {"text": "Therefore, we can use standard tree evaluation metrics, such as ParseEval (, to evaluate the accuracy of the prediction.", "labels": [], "entities": [{"text": "ParseEval", "start_pos": 64, "end_pos": 73, "type": "METRIC", "confidence": 0.7506071925163269}, {"text": "accuracy", "start_pos": 93, "end_pos": 101, "type": "METRIC", "confidence": 0.9987809062004089}]}, {"text": "Overall, we define three metrics to evaluate the accuracy of the LSC trees: POS: the POS metric is the percentage of part-of-speech tags predicted correctly.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.9988237023353577}, {"text": "POS", "start_pos": 76, "end_pos": 79, "type": "METRIC", "confidence": 0.9655107259750366}]}, {"text": "LSC-F1: F1 is the harmonic means of the precision and recall of the predicted tree.", "labels": [], "entities": [{"text": "F1", "start_pos": 8, "end_pos": 10, "type": "METRIC", "confidence": 0.9983795881271362}, {"text": "precision", "start_pos": 40, "end_pos": 49, "type": "METRIC", "confidence": 0.9991807341575623}, {"text": "recall", "start_pos": 54, "end_pos": 60, "type": "METRIC", "confidence": 0.9959010481834412}]}, {"text": "LSC-EM: EM is 1 if the predicted tree is an exact match to the gold tree, and 0 otherwise.", "labels": [], "entities": [{"text": "EM", "start_pos": 8, "end_pos": 10, "type": "METRIC", "confidence": 0.9738367199897766}]}, {"text": "In the case of SM trees, as opposed to the LSC trees, we cannot assume identity of the yield between the gold and parse trees for the same sen-   SM-TED: TED is the normalized edit distance between the predicted and gold SM trees, subtracted from a unity.", "labels": [], "entities": [{"text": "TED", "start_pos": 154, "end_pos": 157, "type": "METRIC", "confidence": 0.9868885278701782}]}, {"text": "SM-EM: EM is 1 if the predicted SM is an exact match with the gold SM, 0 otherwise.", "labels": [], "entities": [{"text": "EM", "start_pos": 7, "end_pos": 9, "type": "METRIC", "confidence": 0.9599452018737793}]}, {"text": "We have a small seed of correctly annotated requirements-specification case studies that describe simple reactive systems in the LSC language.", "labels": [], "entities": [{"text": "LSC language", "start_pos": 129, "end_pos": 141, "type": "DATASET", "confidence": 0.7762319147586823}]}, {"text": "Each document contains a sequence of requirements, each of which is annotated with the correct LSC diagram.", "labels": [], "entities": []}, {"text": "The entire program is grounded in a java implementation.", "labels": [], "entities": []}, {"text": "As training data, we use the case studies provided by . lists the case studies and basic statistics concerning these data.", "labels": [], "entities": []}, {"text": "As our annotated seed is quite small, it is hard to generalize from it to unseen examples.", "labels": [], "entities": []}, {"text": "In particular, we are not guaranteed to have observed all possible structures that are theoretically permitted by the assumed grammar.", "labels": [], "entities": []}, {"text": "To cope with this, we create a synthetic set of examples using the grammar of  in generation mode, and randomly generate trees t \u2208 T req . The grammar we use to generate the synthetic examples clearly over-generates.", "labels": [], "entities": []}, {"text": "That is to say, it creates many trees that do not have a sound interpretation.", "labels": [], "entities": []}, {"text": "In fact, only 3000 our of 10000 generated examples turnout to have a sound semantic interpretation grounded in an SM.", "labels": [], "entities": []}, {"text": "Nonetheless, these data allow us to smooth the syntactic distributions that are observed in the seed, and increase the coverage of the grammar learned from it.", "labels": [], "entities": []}, {"text": "presents the results for parsing the Phone document, our development set, with the sentence-based model, varying the training data.", "labels": [], "entities": [{"text": "parsing", "start_pos": 25, "end_pos": 32, "type": "TASK", "confidence": 0.9716690182685852}, {"text": "Phone document", "start_pos": 37, "end_pos": 51, "type": "DATASET", "confidence": 0.6925220191478729}]}, {"text": "We see that despite the small size of the seed, adding it to our set if synthetics examples substantially improves results over a model trained on synthetic examples only.", "labels": [], "entities": []}, {"text": "In our next experiment, we provide empirical upper-bounds and lower-bounds for the discoursebased model.", "labels": [], "entities": []}, {"text": "presents the results of the discourse-based model for N > 1 on the Phone example.", "labels": [], "entities": [{"text": "Phone example", "start_pos": 67, "end_pos": 80, "type": "DATASET", "confidence": 0.964526355266571}]}, {"text": "Gen-Only presents the results of the discourse-based model with a PCFG learned from synthetic trees only, incorporating transitions obeying the max-overlap assumption.", "labels": [], "entities": []}, {"text": "Already here, we see a mild improvement for N > 1 relative to the N = 1 results, indicating that even a weak signal such as the overlap between neighboring sentences already improves sentence disambiguation in context.", "labels": [], "entities": [{"text": "sentence disambiguation", "start_pos": 183, "end_pos": 206, "type": "TASK", "confidence": 0.7241767644882202}]}, {"text": "We next present the results of an Oracle experiment, where every requirement is assigned the highest scoring tree in terms of LSC-F1 with respect to the gold tree, keeping the same transitions.", "labels": [], "entities": []}, {"text": "Again we see that results improve with N , indicating that the syntactic model alone does not provide optimal disambiguation.", "labels": [], "entities": []}, {"text": "These results provides an upper bound on the parser performance for each N . Gen+Seed presents results of the discourse-based model where the PCFG interpolates the seed set and the synthetic train set, with max-overlap transitions.", "labels": [], "entities": []}, {"text": "Here, we see larger improvements over the synthetic-only PCFG.", "labels": [], "entities": []}, {"text": "That is, modeling grammaticality of individual sentences improves the interpretation of the document.", "labels": [], "entities": []}, {"text": "Next we compare the performance for different implementations of the gap(m i , m j ) function.", "labels": [], "entities": []}, {"text": "We estimate probability distributions that reflect each of the assumptions we discussed, and add an additional method called hybrid, in which we interpolate the max-expansion and max-overlap estimates (equal weights).", "labels": [], "entities": []}, {"text": "In, the trend from the previous experiment persists.", "labels": [], "entities": []}, {"text": "Notably, the hybrid model provides a larger error reduction than its components used separately, indicating that in order to capture discourse context we may need to balance possibly conflicting factors.", "labels": [], "entities": [{"text": "error reduction", "start_pos": 44, "end_pos": 59, "type": "METRIC", "confidence": 0.9470204710960388}]}, {"text": "In no emissions we rely solely on the probability of state transitions, and again increasing N leads to improvement.", "labels": [], "entities": []}, {"text": "This result confirms that context is indispensable for sentence interpretationeven when probabilities for the sentence's seman-  We finally perform a cross-fold experiment in which we leave one document out as a test set and take the rest as our seed.", "labels": [], "entities": [{"text": "sentence interpretationeven", "start_pos": 55, "end_pos": 82, "type": "TASK", "confidence": 0.7285953909158707}]}, {"text": "The results are provided in.", "labels": [], "entities": []}, {"text": "The discourse-based model outperforms the sentence-based model N = 1 in all cases.", "labels": [], "entities": []}, {"text": "Moreover, the drop in N = 128 for Phone seems incidental to this set, and the other cases level off beforehand.", "labels": [], "entities": [{"text": "Phone", "start_pos": 34, "end_pos": 39, "type": "DATASET", "confidence": 0.8610190153121948}]}, {"text": "Despite our small seed, the persistent improvement on all metrics is consistent with our hypothesis that modeling the interpretation process within the discourse has substantial benefits for automatic understanding of the text.", "labels": [], "entities": [{"text": "automatic understanding of the text", "start_pos": 191, "end_pos": 226, "type": "TASK", "confidence": 0.7351761519908905}]}], "tableCaptions": [{"text": " Table 2: Seed Gold-Annotated Requirements", "labels": [], "entities": [{"text": "Seed Gold-Annotated Requirements", "start_pos": 10, "end_pos": 42, "type": "TASK", "confidence": 0.6560808221499125}]}, {"text": " Table 3: Sentence-Based modeling: Accuracy re- sults on the Phone development set.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9802068471908569}, {"text": "Phone development set", "start_pos": 61, "end_pos": 82, "type": "DATASET", "confidence": 0.8836656014124552}]}, {"text": " Table 4: Discourse-Based Modeling: Accuracy re- sults on the Phone dev set. The Oracle selects the  highest scoring LSC tree among the N-candidates,  providing an upper bound on accuracy. Gen-Only  selects the most probable tree, relying on synthetic  examples only, providing a lower bound.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.9845038652420044}, {"text": "Phone dev set", "start_pos": 62, "end_pos": 75, "type": "DATASET", "confidence": 0.8723700841267904}, {"text": "accuracy", "start_pos": 179, "end_pos": 187, "type": "METRIC", "confidence": 0.999129593372345}]}]}