{"title": [], "abstractContent": [{"text": "Syllable weight encodes mostly the same information for English word segmentation as dictionary stress Abstract Stress is a useful cue for English word segmentation.", "labels": [], "entities": [{"text": "English word segmentation", "start_pos": 56, "end_pos": 81, "type": "TASK", "confidence": 0.580808162689209}, {"text": "Abstract", "start_pos": 103, "end_pos": 111, "type": "METRIC", "confidence": 0.982190728187561}, {"text": "English word segmentation", "start_pos": 139, "end_pos": 164, "type": "TASK", "confidence": 0.6341434915860494}]}, {"text": "A wide range of computational models have found that stress cues enable a 2-10% improvement in segmen-tation accuracy, depending on the kind of model, by using input that has been annotated with stress using a pronouncing dictionary.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 109, "end_pos": 117, "type": "METRIC", "confidence": 0.9760470390319824}]}, {"text": "However, stress is neither invariably produced nor unambiguously identifiable in real speech.", "labels": [], "entities": []}, {"text": "Heavy syllables, i.e. those with long vowels or syllable codas, attract stress in English.", "labels": [], "entities": []}, {"text": "We devise Adaptor Grammar word segmentation models that exploit either stress, or syllable weight, or both, and evaluate the utility of syllable weight as a cue to word boundaries.", "labels": [], "entities": [{"text": "Adaptor Grammar word segmentation", "start_pos": 10, "end_pos": 43, "type": "TASK", "confidence": 0.5578701198101044}]}, {"text": "Our results suggest that syllable weight encodes largely the same information for word segmentation in English that annotated dictionary stress does.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 82, "end_pos": 99, "type": "TASK", "confidence": 0.7125795483589172}]}], "introductionContent": [{"text": "One of the first skills a child must develop in the course of language acquisition is the ability to segment speech into words.", "labels": [], "entities": []}, {"text": "Stress has long been recognized as a useful cue for English word segmentation, following the observation that words in English are predominantly stress-initial, together with the result that 9-month-old English-learning infants prefer stressinitial stimuli.", "labels": [], "entities": [{"text": "English word segmentation", "start_pos": 52, "end_pos": 77, "type": "TASK", "confidence": 0.643579920132955}]}, {"text": "A range of statistical () and rule-based) models have used stress information to improve word segmentation.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 89, "end_pos": 106, "type": "TASK", "confidence": 0.7516034841537476}]}, {"text": "However, that work uses stress-marked input prepared by marking vowels that are listed as stressed in a pronouncing dictionary.", "labels": [], "entities": []}, {"text": "This pre-processing step glosses over the fact that stress identification itself involves a nontrival learning problem, since stress has many possible phonetic reflexes and no known invariants.", "labels": [], "entities": [{"text": "stress identification", "start_pos": 52, "end_pos": 73, "type": "TASK", "confidence": 0.7422106862068176}]}, {"text": "One known strong correlate of stress in English is syllable weight: heavy syllables, which end in a consonant or have along vowel, attract stress in English.", "labels": [], "entities": []}, {"text": "We present experiments with Bayesian Adaptor Grammars () that suggest syllable weight encodes largely the same information for word segmentation that dictionary stress information does.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 127, "end_pos": 144, "type": "TASK", "confidence": 0.7248035371303558}]}, {"text": "to compare the utility of syllable weight and stress cues for finding word boundaries, both individually and in combination.", "labels": [], "entities": []}, {"text": "We describe how a shortcoming of Adaptor Grammars prevents us from comparing stress and weight cues in combination with the full range of phonotactic cues for word segmentation, and design two experiments to workaround this limitation.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 159, "end_pos": 176, "type": "TASK", "confidence": 0.7389557510614395}]}, {"text": "The first experiment uses grammars that provide parallel analyses for syllable weight and stress, and learns initial/non-initial phonotactic distinctions.", "labels": [], "entities": []}, {"text": "In this first experiment, syllable weight cues are actually more useful than stress cues at larger input sizes.", "labels": [], "entities": []}, {"text": "The second experiment focuses on incorporating phonotactic cues for typical word-final consonant clusters (such as inflectional morphemes), at the expense of parallel structures.", "labels": [], "entities": []}, {"text": "In this second experiment, weight cues merely match stress cues at larger input sizes, and the learning curve for the combined weightand-stress grammar follows almost perfectly with the stress-only grammar.", "labels": [], "entities": []}, {"text": "This second experiment suggests that the advantage of weight over stress in the first experiment was purely due to poor modeling of word-final consonant clusters by the stress-only grammar, not weight per se.", "labels": [], "entities": []}, {"text": "All together, these results indicate that syllable weight is highly redundant with dictionary-based stress for the purposes of English word segmentation; in fact, in our experiments, there is no detectable difference between relying on syllable weight and relying on dictionary stress.", "labels": [], "entities": [{"text": "English word segmentation", "start_pos": 127, "end_pos": 152, "type": "TASK", "confidence": 0.5989077190558115}]}], "datasetContent": [{"text": "We applied the same experimental set-up used by, to their dataset, as described below.", "labels": [], "entities": []}, {"text": "To understand how different modeling assumptions interact with corpus size, we train on prefixes of each corpus with increasing input size: 100, 200, 500, 1,000, 2,000, 5,000, and 10,000 utterances.", "labels": [], "entities": []}, {"text": "We set our hyperparameters to encourage onset maximization.", "labels": [], "entities": []}, {"text": "The hyperparameter for syllable nodes to rewrite to an onset followed by a rhyme was 10, and the hyperparameter for syllable nodes to rewrite to a rhyme only was 1.", "labels": [], "entities": []}, {"text": "Similarly, the hyperparameter for rhyme nodes to include a coda was 1, and the hyperparameter for rhyme nodes to exclude the coda was 10.", "labels": [], "entities": []}, {"text": "All other hyperparameters specified vague priors.", "labels": [], "entities": []}, {"text": "We ran eight chains of each model for 1,000 iterations, collecting 20 samples with a lag of 10 iterations between samples and a burn-in of 800 iterations.", "labels": [], "entities": []}, {"text": "We used the same batchinitialization and table-label resampling to encourage the model to mix.", "labels": [], "entities": []}, {"text": "After gathering the samples, we used them to perform a single minimum Bayes risk decoding of a separate, held-out test set.", "labels": [], "entities": []}, {"text": "This test set was constructed by taking the last 1,000 utterances of each corpus.", "labels": [], "entities": []}, {"text": "We use a common test-set instead of just evaluating on the training data to ensure that performance figures are comparable across input sizes; when we see learning curves slope upward, we can be confident that the increase is due to learning rather than easier evaluation sets.", "labels": [], "entities": []}, {"text": "We measured our models' performance with the usual token f-score metric, the harmonic mean of how many proposed word tokens are correct (token precision) and how many of the actual word tokens are recovered (token recall).", "labels": [], "entities": [{"text": "token precision)", "start_pos": 137, "end_pos": 153, "type": "METRIC", "confidence": 0.6367656886577606}, {"text": "recall", "start_pos": 214, "end_pos": 220, "type": "METRIC", "confidence": 0.7843136191368103}]}, {"text": "For example, a model may propose \"the in side\" when the true segmentation is \"the inside.\"", "labels": [], "entities": []}, {"text": "This segmentation would have a token precision of 1 3 , since one of three predicted words matches the true word token (even though the other predicted words are valid word types), and a token recall of 1 2 , since it correctly recovered one of two words, yield a token f-score of 0.4.", "labels": [], "entities": [{"text": "precision", "start_pos": 37, "end_pos": 46, "type": "METRIC", "confidence": 0.5914706587791443}]}, {"text": "We evaluated on a dataset drawn from the Alex portion of the Providence corpus ().", "labels": [], "entities": [{"text": "Alex portion of the Providence corpus", "start_pos": 41, "end_pos": 78, "type": "DATASET", "confidence": 0.8385359644889832}]}, {"text": "This dataset contains 17, 948 utterances with 72, 859 word tokens directed to one child from the age of 16 months to 41 months.", "labels": [], "entities": []}, {"text": "We used aversion of this dataset that contained annotations of primary stress that B\u00f6rschinger and Johnson (2014) added to this input using an extended The mean number of syllables per word token was 1.2, and only three word tokens had more than five syllables.", "labels": [], "entities": [{"text": "The mean number of syllables", "start_pos": 152, "end_pos": 180, "type": "METRIC", "confidence": 0.9036167979240417}]}, {"text": "Of the 40, 323 word tokens with a stressed syllable, 27, 258 were monosyllabic.", "labels": [], "entities": []}, {"text": "Of the 13, 065 polysyllabic word tokens with a stressed syllable, 9, 931 were stress-initial.", "labels": [], "entities": []}, {"text": "Turning to the 32, 536 word tokens with no stress (i.e., the function words), all but 23 were monosyllabic (the 23 were primarily contractions, such as \"couldn't\").", "labels": [], "entities": []}, {"text": "The goal of this first experiment is to provide the most direct comparison possible between grammars that attend to stress cues and grammars that attend to syllable weight cues.", "labels": [], "entities": []}, {"text": "As these are both hypothesized to be useful byway of an initial/noninitial distinction, we defined a word to bean initial syllable SyllI followed by zero to three syllables, and syllables to consist of an optional onset SyllI \u2192 (OnsetI) RhymeI Syll \u2192 (Onset) Rhyme In the baseline grammar, presented in, rhymes rewrite to a vowel followed by an optional consonant coda.", "labels": [], "entities": []}, {"text": "Rhymes then rewrite to be heavy or light in the weight grammar, as in, to be stressed or unstressed in the stress grammar, as in.", "labels": [], "entities": []}, {"text": "In the combination grammar, rhymes rewrite to be heavy or light and stressed or unstressed, as in.", "labels": [], "entities": []}, {"text": "LongVowel and ShortVowel both re-write to all vowels.", "labels": [], "entities": [{"text": "LongVowel", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.582675039768219}, {"text": "ShortVowel", "start_pos": 14, "end_pos": 24, "type": "METRIC", "confidence": 0.5996969938278198}]}, {"text": "An additional grammar that restricted them to rewrite to long and short vowels, respectively, led to virtually identical performance, suggesting that vowel quantity can be learned for the purposes of word segmentation from distributional cues.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 200, "end_pos": 217, "type": "TASK", "confidence": 0.7114266902208328}]}, {"text": "We will also present evidence that the model did manage to learn most of the contrast.", "labels": [], "entities": []}, {"text": "Vowel counts by quantity outperforms both the baseline and the weight-only grammar early in learning.", "labels": [], "entities": []}, {"text": "The weight-only grammar rapidly improves in performance at larger training data sizes, increasing its advantage over the baseline, while the advantage of the stressonly grammar slows and appears to disappear at the largest training data size.", "labels": [], "entities": []}, {"text": "At 10,000 utterances, the improvement of the weight-only grammar over the stress-only grammar is significant according to an independent samples t-test (t = 7.2, p < 0.001, 14 degrees of freedom).", "labels": [], "entities": []}, {"text": "This pattern suggests that annotated dictionary stress is easy to take advantage of at low data sizes, but that, with sufficient data, syllable weight can provide even more information about word boundaries.", "labels": [], "entities": []}, {"text": "The best overall performance early in learning is obtained by the combined grammar, suggesting that syllable weight and dictionary stress provide information about word segmentation that is not redundant.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 164, "end_pos": 181, "type": "TASK", "confidence": 0.7202229797840118}]}, {"text": "An examination of the final segmentation suggests that the weight grammar has learned that initial syllables tend to be heavy.", "labels": [], "entities": []}, {"text": "Specifically, across eight runs, 98.1% of RhymeI symbols rewrote to HeavyRhyme, whereas only 54.5% of Rhyme symbols (i.e. non-initial rhymes) rewrote to HeavyRhyme.: Segmentation Token F-score for Experiment I at 10,000 utterances across eight runs.", "labels": [], "entities": [{"text": "HeavyRhyme", "start_pos": 68, "end_pos": 78, "type": "DATASET", "confidence": 0.9441846013069153}, {"text": "F-score", "start_pos": 185, "end_pos": 192, "type": "METRIC", "confidence": 0.8971027731895447}]}, {"text": "Experiment I suggested that, under a basic initial/non-initial distinction, syllable weight eventually encodes more information about word boundaries than does dictionary stress.", "labels": [], "entities": []}, {"text": "This is a surprising result, since we initially investigated syllable weight as a noisy proxy for dictionary stress.", "labels": [], "entities": []}, {"text": "One possible source of the 'extra' advantage that the syllable weight grammar exhibited has to do with the importance of word-final codas, which can encode word-final morphemes in English.", "labels": [], "entities": []}, {"text": "Even though the grammars did not explicitly model them, the weight grammar could implicitly capture a bias for or against having a coda in non-initial position, while the stress grammar could not.", "labels": [], "entities": []}, {"text": "This is because most word tokens are one or two syllables, and only one of the two rhyme types of the weight grammar included a coda.", "labels": [], "entities": []}, {"text": "Thus, the HeavyRhyme symbol could simultaneously capture the most important aspects of both stress and coda constraints.", "labels": [], "entities": [{"text": "HeavyRhyme symbol", "start_pos": 10, "end_pos": 27, "type": "DATASET", "confidence": 0.9208173751831055}]}, {"text": "To see if the extra advantage of the syllable weight grammar can be attributed to the influence of word-final codas, we formulated a set of grammars that model word-final codas and also can learn stress and/or syllable weight patterns.", "labels": [], "entities": []}, {"text": "These grammars are more similar in structure to the ones that used.", "labels": [], "entities": []}, {"text": "For the baseline and weight grammar, we again defined words to consist of up to four syllables with an initial SyllI syllable, but this time distinguished final syllables SyllF in polysyllabic words.", "labels": [], "entities": []}, {"text": "The nonstress grammars use the following rules for producing syllables: SyllF \u2192 (Onset) RhymeF For the stress grammar, we followed B\u00f6rschinger and Johnson (2014) in distinguishing stressed and unstressed syllables, rather than simply stressed rhymes as in Experiment I, to allow the model to learn likely stress patterns at the word level.", "labels": [], "entities": []}, {"text": "A word can consist of up to four syllables, and any syllable and any number of syllables maybe stressed, as in.", "labels": [], "entities": []}, {"text": "The baseline grammar is similar to the previous one, except it distinguishes word-final codas, as in.", "labels": [], "entities": []}, {"text": "The weight grammar, presented in, rewrites rhymes to a nucleus followed by an optional coda and distinguishes nuclei in open syllables according to their position in the word.", "labels": [], "entities": []}, {"text": "The stress grammar, presented in, is the all-stress-patterns model (without the unique stress constraint).", "labels": [], "entities": []}, {"text": "This grammar introduces additional distinctions at the syllable level to learn likely stress patterns, and distinguishes final from non-final codas.", "labels": [], "entities": []}, {"text": "The combined model is identical to the stress model, except Vowel non-terminals in closed and wordinternal syllables are replaced with Nucleus nonterminals, and Vowel non-terminals in word-inital (-final) open syllables are replaced with NucleusI (NucleusF) non-terminals.", "labels": [], "entities": []}, {"text": "To summarize, the stress models distinguish stressed and unstressed syllables in initial, final, and internal position.", "labels": [], "entities": []}, {"text": "The weight models distinguish the vowels of initial open syllables, the vowels of final open syllables, and other vowels, allowing them to take advantage of an important cue from syllable weight for word segmentation: if an initial vowel is open, it should usually belong.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 199, "end_pos": 216, "type": "TASK", "confidence": 0.7252699583768845}]}, {"text": "shows segmentation performance on the Alex corpus with these more complete models.", "labels": [], "entities": [{"text": "Alex corpus", "start_pos": 38, "end_pos": 49, "type": "DATASET", "confidence": 0.9087167680263519}]}, {"text": "While the performance of the weight grammars is virtually unchanged compared to, the two grammars that do not model syllable weight improve dramatically.", "labels": [], "entities": []}, {"text": "This result supports our proposal that much of the advantage of the weight: Segmentation Token F-score for Experiment II at 10,000 utterances across eight runs.", "labels": [], "entities": [{"text": "F-score", "start_pos": 95, "end_pos": 102, "type": "METRIC", "confidence": 0.762445330619812}]}, {"text": "grammars over stress in Experiment I was due to modeling of word-final coda phonotactics.", "labels": [], "entities": []}, {"text": "presents token f-score at 10,000 training utterances averaged across eight runs, along with the standard deviation in f-score.", "labels": [], "entities": [{"text": "f-score", "start_pos": 15, "end_pos": 22, "type": "METRIC", "confidence": 0.829857349395752}]}, {"text": "We see that the noweight:nostress grammar is several standard deviations than the grammars that model syllable weight and/or stress, while the syllable weight and/or stress grammars exhibit a high degree of overlap.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Segmentation Token F-score for Experi- ment I at 10,000 utterances across eight runs.", "labels": [], "entities": [{"text": "Segmentation Token", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.7459684610366821}, {"text": "F-score", "start_pos": 29, "end_pos": 36, "type": "METRIC", "confidence": 0.7214205861091614}, {"text": "Experi- ment I", "start_pos": 41, "end_pos": 55, "type": "TASK", "confidence": 0.48890651762485504}]}, {"text": " Table 2: Segmentation Token F-score for Experi- ment II at 10,000 utterances across eight runs.", "labels": [], "entities": [{"text": "Segmentation Token", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.7660910189151764}, {"text": "F-score", "start_pos": 29, "end_pos": 36, "type": "METRIC", "confidence": 0.6814873814582825}, {"text": "Experi- ment II", "start_pos": 41, "end_pos": 56, "type": "TASK", "confidence": 0.5021101087331772}]}]}