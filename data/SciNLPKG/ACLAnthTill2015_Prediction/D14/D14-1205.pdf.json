{"title": [{"text": "Joint Inference for Knowledge Base Population", "labels": [], "entities": [{"text": "Knowledge Base Population", "start_pos": 20, "end_pos": 45, "type": "DATASET", "confidence": 0.8288381298383077}]}], "abstractContent": [{"text": "Populating Knowledge Base (KB) with new knowledge facts from reliable text resources usually consists of linking name mentions to KB entities and identifying relationship between entity pairs.", "labels": [], "entities": [{"text": "Populating Knowledge Base (KB)", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.6462339262167612}]}, {"text": "However , the task often suffers from errors propagating from upstream entity linkers to downstream relation extractors.", "labels": [], "entities": []}, {"text": "In this paper, we propose a novel joint inference framework to allow interactions between the two subtasks and find an optimal assignment by addressing the coherence among preliminary local predictions: whether the types of entities meet the expectations of relations explicitly or implicitly , and whether the local predictions are globally compatible.", "labels": [], "entities": []}, {"text": "We further measure the confidence of the extracted triples by looking at the details of the complete extraction process.", "labels": [], "entities": []}, {"text": "Experiments show that the proposed framework can significantly reduce the error propagations thus obtain more reliable facts, and outperforms competitive baselines with state-of-the-art relation extraction models.", "labels": [], "entities": [{"text": "error propagations", "start_pos": 74, "end_pos": 92, "type": "TASK", "confidence": 0.6617592871189117}, {"text": "relation extraction", "start_pos": 186, "end_pos": 205, "type": "TASK", "confidence": 0.7383341491222382}]}], "introductionContent": [{"text": "Recent advances in natural language processing have made it possible to construct structured KBs from online encyclopedia resources, at an unprecedented scale and much more efficiently than traditional manual edit.", "labels": [], "entities": []}, {"text": "However, in those KBs, entities which are popular to the community usually contain more knowledge facts, e.g., the basketball player LeBron James, the actor Nicholas Cage, etc., while most other entities often have fewer facts.", "labels": [], "entities": []}, {"text": "On the other hand, knowledge facts should be updated as the development of entities, such as changes in the cabinet, a marriage event, or an acquisition between two companies, etc.", "labels": [], "entities": []}, {"text": "In order to address the above issues, we could consult populating existing KBs from reliable text resources, e.g., newswire, which usually involves enriching KBs with new entities and populating KBs with new knowledge facts, in the form of <Entity, Relation, Entity> triple.", "labels": [], "entities": []}, {"text": "In this paper, we will focus on the latter, identifying relationship between two existing KB entities.", "labels": [], "entities": []}, {"text": "This task can be intuitively considered in a pipeline paradigm, that is, name mentions in the texts are first linked to entities in the KB (entity linking, EL), and then the relationship between them are identified (relation extraction, RE).", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 216, "end_pos": 235, "type": "TASK", "confidence": 0.7004074156284332}, {"text": "RE", "start_pos": 237, "end_pos": 239, "type": "METRIC", "confidence": 0.8762204647064209}]}, {"text": "It is worth mentioning that the first task EL is different from the task of named entity recognition (NER) in traditional information extraction (IE) tasks, where NER recognizes and classifies the entity mentions (to several predefined types) in the texts, but EL focuses on linking the mentions to their corresponding entities in the KB.", "labels": [], "entities": [{"text": "named entity recognition (NER)", "start_pos": 76, "end_pos": 106, "type": "TASK", "confidence": 0.8461121320724487}, {"text": "information extraction (IE) tasks", "start_pos": 122, "end_pos": 155, "type": "TASK", "confidence": 0.8807253142197927}]}, {"text": "Such pipeline systems often suffer from errors propagating from upstream to downstream, since only the local best results are selected to the next step.", "labels": [], "entities": []}, {"text": "One idea to solve the problem is to allow interactions among the local predictions of both subtasks and jointly select an optimal assignment to eliminate possible errors in the pipeline.", "labels": [], "entities": []}, {"text": "Let us first look at an example.", "labels": [], "entities": []}, {"text": "Suppose we are extracting knowledge facts from two sentences in: in sentence, if we are more confident to extract the relation fb:org.headquarters 1 , we will be then prompted to select Bryant University, which indeed favors the RE prediction that requires an organization to be its subject.", "labels": [], "entities": [{"text": "Bryant University", "start_pos": 186, "end_pos": 203, "type": "DATASET", "confidence": 0.968375951051712}, {"text": "RE prediction", "start_pos": 229, "end_pos": 242, "type": "TASK", "confidence": 0.5494357049465179}]}, {"text": "On the other side, if we are sure to link to Kobe Bryant in sentence, we will probably select fb:pro athlete.teams, whose subject position expects an athlete, e.g., an NBA player.", "labels": [], "entities": []}, {"text": "It is not difficult to see that the argument type expectations of relations can encourage the two subtasks interact with each other and select coherent predictions for both of them.", "labels": [], "entities": []}, {"text": "In KBs with well-defined schemas, such as Freebase, type requirements can be collected and utilized explicitly ( . However, in other KBs with less reliable or even no schemas, it is more appropriate to implicitly capture the type expectations fora given relation ( . Furthermore, previous RE approaches usually process each triple individually, which ignores whether those local predictions are compatible with each other.", "labels": [], "entities": []}, {"text": "For example, suppose the local predictions of the two sentences above are <Kobe Bryant, fb:org.headquarters, Smithfield, Rhode Island> and <Kobe Bryant, fb:pro athlete.teams, Los Angeles Lakers>, respectively, which, in fact, disagree with each other with respect to the KB, since, inmost cases, these two relations cannot share subjects.", "labels": [], "entities": []}, {"text": "Now we can see that either the relation predictions or the EL results for \"Bryant\" are incorrect.", "labels": [], "entities": [{"text": "EL", "start_pos": 59, "end_pos": 61, "type": "METRIC", "confidence": 0.9942846894264221}]}, {"text": "Those disagreements provide us an effective way to remove the possible incorrect predictions that cause the incompatibilities.", "labels": [], "entities": []}, {"text": "On the other hand, the automatically extracted knowledge facts inevitably contain errors, especially for those triples collected from open domain.", "labels": [], "entities": []}, {"text": "Extractions with confidence scores will be more than useful for users to make proper decisions according to their requirements, such as trading recall for precision, or supporting approximate queries.", "labels": [], "entities": [{"text": "recall", "start_pos": 144, "end_pos": 150, "type": "METRIC", "confidence": 0.9544944763183594}, {"text": "precision", "start_pos": 155, "end_pos": 164, "type": "METRIC", "confidence": 0.9654373526573181}]}, {"text": "In this paper, we propose a joint framework to populate an existing KB with new knowledge facts extracted from reliable text resources.", "labels": [], "entities": []}, {"text": "The joint framework is designed to address the error propagation issue in a pipeline system, where subtasks are optimized in isolation and locally.", "labels": [], "entities": []}, {"text": "We find an optimal configuration from top k results of both subtasks, which maximizes the scores of each step, fulfills the argument type expectations of relations, which can be captured explicitly or implicitly, in the KB, and avoids globally incoherent predictions.", "labels": [], "entities": []}, {"text": "We formulate this optimization problem in an Integer Linear Program (ILP) framework, and further adopt a logistic regression model to measure the reliability of the whole process, and assign confidences to all extracted triples to facilitate further applications.", "labels": [], "entities": []}, {"text": "The experiments on a real-world case study show that our framework can eliminate error propagations in the pipeline systems by taking relations' argument type expectations and global compatibilities into account, thus outperforms the pipeline approaches based on state-ofthe-art relation extractors by a large margin.", "labels": [], "entities": []}, {"text": "Furthermore, we investigate both explicit and implicit type clues for relations, and provide suggestions about which to choose according to the characteristics of existing KBs.", "labels": [], "entities": []}, {"text": "Additionally, our proposed confidence estimations can help to achieve a precision of over 85% fora considerable amount of high quality extractions.", "labels": [], "entities": [{"text": "precision", "start_pos": 72, "end_pos": 81, "type": "METRIC", "confidence": 0.9989389777183533}]}, {"text": "In the rest of the paper, we first review related work and then define the knowledge base population task that we will address in this paper.", "labels": [], "entities": []}, {"text": "Next we detail the proposed framework and present our experiments and results.", "labels": [], "entities": []}, {"text": "Finally, we conclude this paper with future directions.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate the proposed framework in a realworld scenario: given a set of news texts with entity mentions and a KB, a model should find more and accurate new knowledge facts between pairs of those entities.", "labels": [], "entities": []}, {"text": "We use New York Times dataset from 2005 to 2007 as the text corpus, and Freebase as the KB.", "labels": [], "entities": [{"text": "New York Times dataset from 2005", "start_pos": 7, "end_pos": 39, "type": "DATASET", "confidence": 0.8855071663856506}, {"text": "Freebase", "start_pos": 72, "end_pos": 80, "type": "DATASET", "confidence": 0.9535965919494629}]}, {"text": "We divide the corpus into two equal parts, one for creating training data for the RE models using the distant supervision strategy (we do not need training data for EL), and the other as the testing data.", "labels": [], "entities": []}, {"text": "For the convenience of experimentation, we randomly sample a subset of entities for testing.", "labels": [], "entities": []}, {"text": "We first collect all sentences containing two mentions which may refer to the sampled entities, and prune them according to: (1)there should be no more than 10 words between the two mentions; (2)the prior probability of the mention referring to the target entity is higher than a threshold (set to 0.1 in this paper), which is set to filter the impossible mappings; (3)the mention pairs should not belong to different clauses.", "labels": [], "entities": []}, {"text": "The resulting test set is split into 10 parts and a development set, each with 3500 entity pairs roughly, which leads to averagely 200,000 variables and 900,000 constraints per split and may take 1 hour for Cplex to solve.", "labels": [], "entities": []}, {"text": "Note that we do not count the triples that will be evaluated in the testing data when we learn the preferences and the clues from the KB.", "labels": [], "entities": []}, {"text": "We compare our framework with three baselines.", "labels": [], "entities": []}, {"text": "The first one, ME-pl, is the pipeline system constructed by the entity linker in ) and the MaxEnt version of Mintz++ extractor mentioned in ().", "labels": [], "entities": []}, {"text": "The second and third baselines are the pipeline systems constructed by the same linker and two state-ofthe-art DS approaches,) and MIML-RE (), respectively.", "labels": [], "entities": [{"text": "MIML-RE", "start_pos": 131, "end_pos": 138, "type": "DATASET", "confidence": 0.622626543045044}]}, {"text": "They are referred to as MultiR-pl and MIML-pl in the rest of this paper.", "labels": [], "entities": [{"text": "MultiR-pl", "start_pos": 24, "end_pos": 33, "type": "DATASET", "confidence": 0.7745039463043213}, {"text": "MIML-pl", "start_pos": 38, "end_pos": 45, "type": "DATASET", "confidence": 0.4698828458786011}]}, {"text": "We also implement several variants of our framework to investigate the following two components in our framework: whether to use explicit (E) or implicit (I) argument type expectations, whether to take global (G) compatibilities into account, resulting in four variants: ME-JE, ME-JI, ME-JEG, ME-JIG.", "labels": [], "entities": []}, {"text": "We tune the parameters in the objective function on the development set to be re = 1, el = 4, sp = 1.", "labels": [], "entities": []}, {"text": "The numbers of preliminary results retained to the inference step are set top = 2, q = 3.", "labels": [], "entities": []}, {"text": "Three metrics used in our experiments include: (1)the precision of extracted triples, which is the ratio of the number of correct triples and the number of total extracted triples; (2)the number of correct triples (NoC); (3)the number of correct triples in the results ranked in top n.", "labels": [], "entities": [{"text": "precision", "start_pos": 54, "end_pos": 63, "type": "METRIC", "confidence": 0.9989567995071411}, {"text": "NoC)", "start_pos": 215, "end_pos": 219, "type": "METRIC", "confidence": 0.8863625228404999}]}, {"text": "The third metric is crucial for KBP, since most users are only interested in the knowledge facts with high confidences.", "labels": [], "entities": [{"text": "KBP", "start_pos": 32, "end_pos": 35, "type": "TASK", "confidence": 0.7101239562034607}]}, {"text": "We compare the extracted triples against: The results of our joint frameworks and the three baselines.", "labels": [], "entities": []}], "tableCaptions": []}