{"title": [{"text": "Modeling Interestingness with Deep Neural Networks", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper presents a deep semantic similarity model (DSSM), a special type of deep neural networks designed for text analysis, for recommending target documents to be of interest to a user based on a source document that she is reading.", "labels": [], "entities": [{"text": "text analysis", "start_pos": 113, "end_pos": 126, "type": "TASK", "confidence": 0.7566085457801819}]}, {"text": "We observe, identify, and detect naturally occurring signals of interestingness in click transitions on the Web between source and target documents, which we collect from commercial Web browser logs.", "labels": [], "entities": []}, {"text": "The DSSM is trained on millions of Web transitions, and maps source-target document pairs to feature vectors in a latent space in such away that the distance between source documents and their corresponding interesting targets in that space is minimized.", "labels": [], "entities": []}, {"text": "The effectiveness of the DSSM is demonstrated using two interestingness tasks: automatic highlighting and contextual entity search.", "labels": [], "entities": []}, {"text": "The results on large-scale, real-world da-tasets show that the semantics of documents are important for modeling interest-ingness and that the DSSM leads to significant quality improvement on both tasks, outperforming not only the classic document models that do not use semantics but also state-of-the-art topic models.", "labels": [], "entities": []}], "introductionContent": [{"text": "Tasks of predicting what interests a user based on the document she is reading are fundamental to many online recommendation systems.", "labels": [], "entities": []}, {"text": "A recent survey is due to.", "labels": [], "entities": []}, {"text": "In this paper, we exploit the use of a deep semantic model for two such interestingness tasks in which document semantics play a crucial role: automatic highlighting and contextual entity search.", "labels": [], "entities": [{"text": "automatic highlighting", "start_pos": 143, "end_pos": 165, "type": "TASK", "confidence": 0.6749714314937592}]}, {"text": "In this task we want a recommendation system to automatically discover the entities (e.g., a person, location, organization etc.) that interest a user when reading a document and to highlight the corresponding text spans, referred to as keywords afterwards.", "labels": [], "entities": []}, {"text": "We show in this study that document semantics are among the most important factors that influence what is perceived as interesting to the user.", "labels": [], "entities": []}, {"text": "For example, we observe in Web browsing logs that when a user reads an article about a movie, she is more likely to browse to an article about an actor or character than to another movie or the director.", "labels": [], "entities": []}, {"text": "After identifying the keywords that represent the entities of interest to the user, we also want the system to recommend new, interesting documents by searching the Web for supplementary information about these entities.", "labels": [], "entities": []}, {"text": "The task is challenging because the same keywords often refer to different entities, and interesting supplementary information to the highlighted entity is highly sensitive to the semantic context.", "labels": [], "entities": []}, {"text": "For example, \"Paul Simon\" can refer to many people, such as the singer and the senator.", "labels": [], "entities": []}, {"text": "Consider an article about the music of Paul Simon and another about his life.", "labels": [], "entities": []}, {"text": "Related content about his upcoming concert tour is much more interesting in the first context, while an article about his family is more interesting in the second.", "labels": [], "entities": []}, {"text": "At the heart of these two tasks is the notion of interestingness.", "labels": [], "entities": []}, {"text": "In this paper, we model and make use of this notion of interestingness with a deep semantic similarity model (DSSM).", "labels": [], "entities": []}, {"text": "The model, extending from the deep neural networks shown recently to be highly effective for speech recognition () and computer vision (), is semantic because it maps documents to feature vectors in a latent semantic space, also known as semantic representations.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 93, "end_pos": 111, "type": "TASK", "confidence": 0.7505911886692047}]}, {"text": "The model is deep because it employs a neural network with several hidden layers including a special convolutional-pooling structure to identify keywords and extract hidden semantic features at different levels of abstractions, layer by layer.", "labels": [], "entities": []}, {"text": "The semantic representation is computed through a deep neural network after its training by backpropagation with respect to an objective tailored to the respective interestingness tasks.", "labels": [], "entities": []}, {"text": "We obtain naturally occurring \"interest\" signals by observing Web browser transitions, from a source document to a target document, in Web usage logs of a commercial browser.", "labels": [], "entities": []}, {"text": "Our training data is sampled from these transitions.", "labels": [], "entities": []}, {"text": "The use of the DSSM to model interestingness is motivated by the recent success of applying related deep neural networks to computer vision (), speech recognition (), text processing (), and Web search ().", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 144, "end_pos": 162, "type": "TASK", "confidence": 0.7215141355991364}, {"text": "Web search", "start_pos": 191, "end_pos": 201, "type": "TASK", "confidence": 0.7448411285877228}]}, {"text": "Among them, () is most relevant to our work.", "labels": [], "entities": []}, {"text": "They also use a deep neural network to map documents to feature vectors in a latent semantic space.", "labels": [], "entities": []}, {"text": "However, their model is designed to represent the relevance between queries and documents, which differs from the notion of interestingness between documents studied in this paper.", "labels": [], "entities": []}, {"text": "It is often the case that a user is interested in a document because it provides supplementary information about the entities or concepts she encounters when reading another document although the overall contents of the second documents is not highly relevant.", "labels": [], "entities": []}, {"text": "For example, a user maybe interested in knowing more about the history of University of Washington after reading the news about President Obama's visit to Seattle.", "labels": [], "entities": []}, {"text": "To better model interestingness, we extend the model of in two significant aspects.", "labels": [], "entities": []}, {"text": "First, while Huang et al. treat a document as a bag of words for semantic mapping, the DSSM treats a document as a sequence of words and tries to discover prominent keywords.", "labels": [], "entities": [{"text": "semantic mapping", "start_pos": 65, "end_pos": 81, "type": "TASK", "confidence": 0.7456023693084717}]}, {"text": "These keywords represent the entities or concepts that might interest users, via the convolutional and max-pooling layers which are related to the deep models used for computer vision () and speech recognition (Deng et al., 2013a) but are not used in Huang et al.'s model.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 191, "end_pos": 209, "type": "TASK", "confidence": 0.7290701121091843}]}, {"text": "The DSSM then forms the high-level semantic representation of the whole document based on these keywords.", "labels": [], "entities": []}, {"text": "Second, instead of directly computing the document relevance score using cosine similarity in the learned semantic space, as in, we feed the features derived from the semantic representations of documents to a ranker which is trained in a supervised manner.", "labels": [], "entities": []}, {"text": "As a result, a document that is not highly relevant to another document a user is reading (i.e., the distance between their derived feature We stress here that, although the click signal is available to form a dataset and a gold standard ranker (to be described in vectors is big) may still have a high score of interestingness because the former provides useful information about an entity mentioned in the latter.", "labels": [], "entities": []}, {"text": "Such information and entity are encoded, respectively, by (some subsets of) the semantic features in their corresponding documents.", "labels": [], "entities": []}, {"text": "In Sections 4 and 5, we empirically demonstrate that the aforementioned two extensions lead to significant quality improvements for the two interestingness tasks presented in this paper.", "labels": [], "entities": []}, {"text": "Before giving a formal description of the DSSM in Section 3, we formally define the interestingness function, and then introduce our data set of naturally occurring interest signals.", "labels": [], "entities": []}], "datasetContent": [{"text": "Recall from Section 1 that in this task, a system must select \ud97b\udf59 most interesting keywords in a document that a user is reading.", "labels": [], "entities": []}, {"text": "To evaluate our models using the click transition data described in Section 2, we simulate the task as follows.", "labels": [], "entities": []}, {"text": "We use the set of anchors in a source document \ud97b\udf59 to simulate the set of candidate keywords that maybe of interest to the user while reading \ud97b\udf59, and treat the text of a document that is linked by an anchor in \ud97b\udf59 as a target document \ud97b\udf59.", "labels": [], "entities": []}, {"text": "As shown in, to apply DSSM to a specific task, we need to define the focus in source and target documents.", "labels": [], "entities": [{"text": "DSSM", "start_pos": 22, "end_pos": 26, "type": "TASK", "confidence": 0.9578863978385925}]}, {"text": "In this task, the focus in sis defined as the anchor text, and the focus int is defined as the first 10 tokens int.", "labels": [], "entities": []}, {"text": "We evaluate the performance of a highlighting system against a gold standard interestingness function \ud97b\udf59 \ud97b\udf59 which scores the interestingness of an anchor as the number of user clicks on \ud97b\udf59 from the anchor in \ud97b\udf59 in our data.", "labels": [], "entities": []}, {"text": "We consider the ideal selection to then consist of the \ud97b\udf59 most interesting anchors according to \ud97b\udf59 \ud97b\udf59 . A natural metric for this task is Normalized Discounted Cumulative Gain (NDCG) (Jarvelin and Kekalainen 2000).", "labels": [], "entities": [{"text": "Normalized Discounted Cumulative Gain (NDCG)", "start_pos": 135, "end_pos": 179, "type": "METRIC", "confidence": 0.6497552948338645}]}, {"text": "We evaluate our models on the EVAL dataset described in Section 2.", "labels": [], "entities": [{"text": "EVAL dataset", "start_pos": 30, "end_pos": 42, "type": "DATASET", "confidence": 0.9164498448371887}]}, {"text": "We utilize the transition distributions in EVAL to create three other test sets, following the stratified sampling methodology commonly employed in the IR community, for the frequently, less frequently, and rarely viewed source pages, referred to as HEAD, TORSO, and TAIL, respectively.", "labels": [], "entities": [{"text": "HEAD", "start_pos": 250, "end_pos": 254, "type": "METRIC", "confidence": 0.6832743287086487}, {"text": "TORSO", "start_pos": 256, "end_pos": 261, "type": "METRIC", "confidence": 0.9609239101409912}, {"text": "TAIL", "start_pos": 267, "end_pos": 271, "type": "METRIC", "confidence": 0.9893745183944702}]}, {"text": "We obtain these sets by first sorting the unique source documents according to their frequency of occurrence in EVAL.", "labels": [], "entities": [{"text": "EVAL", "start_pos": 112, "end_pos": 116, "type": "DATASET", "confidence": 0.9289015531539917}]}, {"text": "We then partition the set so that HEAD corresponds to all transitions from the source pages at the top of the list that account for 20% of the transitions in EVAL; TAIL corresponds to the transitions at the bottom also accounting for 20% of the transitions in EVAL; and TORSO corresponds to the remaining transitions.", "labels": [], "entities": [{"text": "HEAD", "start_pos": 34, "end_pos": 38, "type": "METRIC", "confidence": 0.9818930625915527}, {"text": "TAIL", "start_pos": 164, "end_pos": 168, "type": "METRIC", "confidence": 0.9924902319908142}, {"text": "TORSO", "start_pos": 270, "end_pos": 275, "type": "METRIC", "confidence": 0.9960179924964905}]}, {"text": "source document sand from user session information in the browser log.", "labels": [], "entities": []}, {"text": "The document features include: position of the anchor in the document, frequency of the anchor, and anchor density in the paragraph.", "labels": [], "entities": [{"text": "frequency", "start_pos": 71, "end_pos": 80, "type": "METRIC", "confidence": 0.9806166887283325}]}, {"text": "We construct the evaluation data set for this second task by randomly sampling a set of documents from a traffic-weighted set of Web documents.", "labels": [], "entities": []}, {"text": "Ina second step, we identify the entity names in each document using an in-house named entity recognizer.", "labels": [], "entities": []}, {"text": "We issue each entity name as a query to a commercial search engine, and retain up to the top-100 retrieved documents as candidate target documents.", "labels": [], "entities": []}, {"text": "We form for each entity a source document which consists of the entity text and its surrounding text defined by a 200-word window.", "labels": [], "entities": []}, {"text": "We define the focus (as in) in \ud97b\udf59 as the entity text, and the focus in \ud97b\udf59 as the first 10 tokens in \ud97b\udf59.", "labels": [], "entities": []}, {"text": "The final evaluation data set contains 10,000 source documents.", "labels": [], "entities": [{"text": "evaluation data set", "start_pos": 10, "end_pos": 29, "type": "DATASET", "confidence": 0.7541744112968445}]}, {"text": "On average, each source document is associated with 87 target documents.", "labels": [], "entities": []}, {"text": "Finally, the source-target document pairs are labeled in terms of interestingness by paid annotators.", "labels": [], "entities": []}, {"text": "The label is on a 5-level scale, 0 to 4, with 4 meaning the target document is the most interesting to the source document and 0 meaning the target is of no interest.", "labels": [], "entities": []}, {"text": "We test our models on two scenarios.", "labels": [], "entities": []}, {"text": "The first is a ranking scenario where \ud97b\udf59 interesting documents are displayed to the user.", "labels": [], "entities": []}, {"text": "Here, we select the top-\ud97b\udf59 ranked documents according to their interestingness scores.", "labels": [], "entities": []}, {"text": "We measure the performance via NDCG at truncation levels 1 and 3.", "labels": [], "entities": []}, {"text": "The second scenario is to display to the user all interesting results.", "labels": [], "entities": []}, {"text": "In this scenario, we select all target documents with an interestingness score exceeding a predefined threshold.", "labels": [], "entities": []}, {"text": "We evaluate this scenario using ROC analysis and, specifically, the area under the curve (AUC).", "labels": [], "entities": [{"text": "ROC analysis", "start_pos": 32, "end_pos": 44, "type": "METRIC", "confidence": 0.8756580948829651}, {"text": "AUC", "start_pos": 90, "end_pos": 93, "type": "METRIC", "confidence": 0.9069117307662964}]}], "tableCaptions": [{"text": " Table 1 summarizes the results of various models  over the three test sets using NDCG at truncation  levels 1, 5, and 10.  Rows 1 to 3 are simple heuristic baselines.  RAND selects \ud97b\udf59 random anchors, 1stK selects  the first \ud97b\udf59 anchors and LastK the last \ud97b\udf59 anchors.  The other models in Table 1 are boosted tree  based rankers trained on TRAIN_2 described in  Section 2. They vary only in their features. The  ranker in Row 4 uses Non-Semantic Features  (NSF) only. These features are derived from the", "labels": [], "entities": []}, {"text": " Table 2: Contextual entity search task perfor- mance (NDCG @ K and AUC). * indicates sta- tistical significance over all non-shaded single  model results (Rows 1 to 6) using t-test (\ud97b\udf59 \ud97b\udf59  0.05). # indicates statistical significance over re- sults in Row 7. ## indicates statistical signifi- cance over results in Rows 7 and 8.", "labels": [], "entities": []}]}