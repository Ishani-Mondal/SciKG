{"title": [{"text": "POS Tagging of English-Hindi Code-Mixed Social Media Content", "labels": [], "entities": [{"text": "POS Tagging of English-Hindi Code-Mixed Social Media Content", "start_pos": 0, "end_pos": 60, "type": "TASK", "confidence": 0.6902538910508156}]}], "abstractContent": [{"text": "Code-mixing is frequently observed in user generated content on social media, especially from multilingual users.", "labels": [], "entities": []}, {"text": "The linguistic complexity of such content is compounded by presence of spelling variations , transliteration and non-adherance to formal grammar.", "labels": [], "entities": []}, {"text": "We describe our initial efforts to create a multi-level annotated corpus of Hindi-English code-mixed text collated from Facebook forums , and explore language identification , back-transliteration, normalization and POS tagging of this data.", "labels": [], "entities": [{"text": "language identification", "start_pos": 150, "end_pos": 173, "type": "TASK", "confidence": 0.7163065522909164}, {"text": "POS tagging", "start_pos": 216, "end_pos": 227, "type": "TASK", "confidence": 0.8082873821258545}]}, {"text": "Our results show that language identification and transliteration for Hindi are two major challenges that impact POS tagging accuracy .", "labels": [], "entities": [{"text": "language identification", "start_pos": 22, "end_pos": 45, "type": "TASK", "confidence": 0.7079494148492813}, {"text": "POS tagging", "start_pos": 113, "end_pos": 124, "type": "TASK", "confidence": 0.962801992893219}, {"text": "accuracy", "start_pos": 125, "end_pos": 133, "type": "METRIC", "confidence": 0.9229283928871155}]}], "introductionContent": [{"text": "Code-Switching and Code-Mixing are typical and well-studied phenomena of multilingual societies.", "labels": [], "entities": []}, {"text": "Linguists differentiate between the two, where Code-Switching is juxtaposition within the same speech exchange of passages of speech belonging to two different grammatical systems or sub-systems, and CodeMixing (CM) refers to the embedding of linguistic units such as phrases, words and morphemes of one language into an utterance of another language).", "labels": [], "entities": []}, {"text": "The first example in features CM where English words are embedded in a Hindi sentence, whereas the second example shows codeswitching.", "labels": [], "entities": []}, {"text": "Here, we will use CM to imply both.", "labels": [], "entities": []}, {"text": "Work on computa- * This work was done during authors' internship at Microsoft Research India.", "labels": [], "entities": []}, {"text": "tional models of CM have been few and far between (, primarily due to the paucity of CM data in conventional text-corpora which makes data-intensive methods hard to apply.", "labels": [], "entities": []}, {"text": "Solorio and in their work on English-Spanish CM use models built on smaller datasets to predict valid switching points to synthetically generate data from monolingual corpora, and in another work (2008b) describe parts-of-speech (POS) tagging of CM text.", "labels": [], "entities": [{"text": "parts-of-speech (POS) tagging of CM text", "start_pos": 213, "end_pos": 253, "type": "TASK", "confidence": 0.748765729367733}]}, {"text": "CM though typically observed in spoken language is now increasingly more common in text, thanks to the proliferation of the Computer Mediated Communication channels, especially social media like.", "labels": [], "entities": []}, {"text": "Social media content is tremendously important for studying trends, reviews, events, humanbehaviour as well as linguistic analysis, and therefore in recent times has spurred a lot of interest in automatic processing of such data.", "labels": [], "entities": [{"text": "linguistic analysis", "start_pos": 111, "end_pos": 130, "type": "TASK", "confidence": 0.7427628040313721}]}, {"text": "Nevertheless, CM on social media has not been studied from a computational aspect.", "labels": [], "entities": [{"text": "CM", "start_pos": 14, "end_pos": 16, "type": "TASK", "confidence": 0.9729800820350647}]}, {"text": "Moreover, social media content presents additional challenges due to contractions, non-standard spellings and nongrammatical constructions.", "labels": [], "entities": []}, {"text": "Furthermore, for languages written in scripts other than Roman, like Hindi, Bangla, Japanese, Chinese and Arabic, Roman transliterations are typically used for representing the words (.", "labels": [], "entities": []}, {"text": "This can prove a challenge for language identification and segregation of the two languages.", "labels": [], "entities": [{"text": "language identification", "start_pos": 31, "end_pos": 54, "type": "TASK", "confidence": 0.7213720977306366}]}, {"text": "In this paper, we describe our initial efforts to POS tag social media content from English-Hindi (henceforth En-Hi) bilinguals while trying to address the challenges of CM, transliteration and non-standard spelling, as well as lack of annotated data.", "labels": [], "entities": [{"text": "POS tag social media content from English-Hindi (henceforth En-Hi) bilinguals", "start_pos": 50, "end_pos": 127, "type": "TASK", "confidence": 0.8871974547704061}]}, {"text": "POS tagging is one of the fundamental pre-processing steps for NLP, and while there have been works on POS tagging of social media data ( and of CM (), but we do not know of any work on POS tagging of CM text from social media that involves transliteration.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.7812043726444244}, {"text": "POS tagging of social media", "start_pos": 103, "end_pos": 130, "type": "TASK", "confidence": 0.8363917231559753}, {"text": "POS tagging of CM text from social media", "start_pos": 186, "end_pos": 226, "type": "TASK", "confidence": 0.878902330994606}]}, {"text": "The salient contributions of this work are in formalizing the problem and related challenges for processing of En-Hi social media data, creation of an annotated dataset and some initial experiments for language identification, transliteration, normalization and POS tagging of this data.", "labels": [], "entities": [{"text": "En-Hi social media data", "start_pos": 111, "end_pos": 134, "type": "DATASET", "confidence": 0.6227527782320976}, {"text": "language identification", "start_pos": 202, "end_pos": 225, "type": "TASK", "confidence": 0.7567683160305023}, {"text": "POS tagging", "start_pos": 262, "end_pos": 273, "type": "TASK", "confidence": 0.8100001811981201}]}], "datasetContent": [{"text": "We conducted three different experiments as follows.", "labels": [], "entities": []}, {"text": "In the first experiment, we assume that we know the language identities and normalized/transliterated forms of the words, and only do the POS tagging.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 138, "end_pos": 149, "type": "TASK", "confidence": 0.7007508128881454}]}, {"text": "This experiment gives us an idea of the accuracy of POS tagging task, if normalization, transliteration and language identification could be done perfectly.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9995707869529724}, {"text": "POS tagging task", "start_pos": 52, "end_pos": 68, "type": "TASK", "confidence": 0.9051212072372437}, {"text": "language identification", "start_pos": 108, "end_pos": 131, "type": "TASK", "confidence": 0.7524921000003815}]}, {"text": "We conduct this experiments with two different En POS taggers: the Stanford POS tagger which is trained on formal English text (Model 1a) and the Twitter POS tagger (Model 1b).", "labels": [], "entities": []}, {"text": "In the next experiment (Model 2), we assume that only the language identity of the words are known, but for Hindi we apply our model to generate the back transliterations.", "labels": [], "entities": []}, {"text": "For English, we apply the Twitter POS tagger directly because it can handle unnormalized social media text.", "labels": [], "entities": []}, {"text": "The third experiment (Model 3) assumes that nothing is known.", "labels": [], "entities": []}, {"text": "So language identifier is first applied, and based on the language detected, we apply the Hi translitertaion module, and Hi POS tagger, or the En tagger.", "labels": [], "entities": []}, {"text": "This is the most challenging and realistic setting.", "labels": [], "entities": []}, {"text": "Note that the matrix information is not used in any of our experiments, though it could be potentially useful for POS tagging and could be explored in future.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 114, "end_pos": 125, "type": "TASK", "confidence": 0.8489035069942474}]}, {"text": "gives a summary of the four models along with the POS tagging accuracies (in %).", "labels": [], "entities": [{"text": "POS tagging accuracies", "start_pos": 50, "end_pos": 72, "type": "METRIC", "confidence": 0.5418334106604258}]}, {"text": "It shows token level as well as chunk leve accuracies (CA), i.e., what percentage of chunks have been correctly POS tagged.", "labels": [], "entities": [{"text": "chunk leve accuracies (CA)", "start_pos": 32, "end_pos": 58, "type": "METRIC", "confidence": 0.7498513609170914}]}, {"text": "As can be seen, Hi POS tagging has relatively low accuracies than En POS tagging at word level for all cases.", "labels": [], "entities": [{"text": "Hi POS tagging", "start_pos": 16, "end_pos": 30, "type": "TASK", "confidence": 0.6909102102120718}, {"text": "accuracies", "start_pos": 50, "end_pos": 60, "type": "METRIC", "confidence": 0.9600503444671631}]}, {"text": "This is primarily due to the errors of the transliteration module, which in turn, is because the transliteration does not address spelling contractions.", "labels": [], "entities": []}, {"text": "This is also reflected in the drop in the accuracies for the case where LI is unknown.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 42, "end_pos": 52, "type": "METRIC", "confidence": 0.998225748538971}, {"text": "LI", "start_pos": 72, "end_pos": 74, "type": "METRIC", "confidence": 0.9836385250091553}]}, {"text": "The very low CA for En for model 3 is primarily because some of the Hi chunks are incorrectly identified as En by the language identification module (see).", "labels": [], "entities": [{"text": "CA", "start_pos": 13, "end_pos": 15, "type": "METRIC", "confidence": 0.9937469959259033}]}, {"text": "However, the gradual drop of token and chunk level accuracies from model 1 to model 3 clearly shows the effect of gradual error accumulation from each of the modules.", "labels": [], "entities": []}, {"text": "We observe that Nouns were usually confused most with Verbs and vice versa, while the Adj were mostly confused with Nouns, Pronouns with Determiners, and Adpositions with Conjunctions.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Confusion matrix, precision and recall of  the language identification module.", "labels": [], "entities": [{"text": "precision", "start_pos": 28, "end_pos": 37, "type": "METRIC", "confidence": 0.9995447993278503}, {"text": "recall", "start_pos": 42, "end_pos": 48, "type": "METRIC", "confidence": 0.9992434978485107}, {"text": "language identification module", "start_pos": 57, "end_pos": 87, "type": "TASK", "confidence": 0.7687856753667196}]}, {"text": " Table 2: POS Tagging accuracies for the different models. K=Known, NK = Not Known. LI = Language  labels, HN = Hindi normalized forms, Acc. = Token level accuracy, CA = Chunk level accuracy.", "labels": [], "entities": [{"text": "POS Tagging", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.7084765136241913}, {"text": "Acc. = Token level accuracy", "start_pos": 136, "end_pos": 163, "type": "METRIC", "confidence": 0.7630978971719742}, {"text": "accuracy", "start_pos": 182, "end_pos": 190, "type": "METRIC", "confidence": 0.5728899836540222}]}]}