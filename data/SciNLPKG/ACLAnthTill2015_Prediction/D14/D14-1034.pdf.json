{"title": [{"text": "An Unsupervised Model for Instance Level Subcategorization Acquisition", "labels": [], "entities": [{"text": "Instance Level Subcategorization Acquisition", "start_pos": 26, "end_pos": 70, "type": "TASK", "confidence": 0.7806651443243027}]}], "abstractContent": [{"text": "Most existing systems for subcategoriza-tion frame (SCF) acquisition rely on supervised parsing and infer SCF distributions at type, rather than instance level.", "labels": [], "entities": [{"text": "subcategoriza-tion frame (SCF) acquisition", "start_pos": 26, "end_pos": 68, "type": "TASK", "confidence": 0.6699992765982946}]}, {"text": "These systems suffer from poor portability across domains and their benefit for NLP tasks that involve sentence-level processing is limited.", "labels": [], "entities": []}, {"text": "We propose anew unsuper-vised, Markov Random Field-based model for SCF acquisition which is designed to address these problems.", "labels": [], "entities": [{"text": "SCF acquisition", "start_pos": 67, "end_pos": 82, "type": "TASK", "confidence": 0.9860993921756744}]}, {"text": "The system relies on supervised POS tagging rather than parsing, and is capable of learning SCFs at instance level.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 32, "end_pos": 43, "type": "TASK", "confidence": 0.7469355165958405}]}, {"text": "We perform evaluation against gold standard data which shows that our system outperforms several supervised and type-level SCF baselines.", "labels": [], "entities": []}, {"text": "We also conduct task-based evaluation in the context of verb similarity prediction, demonstrating that a vector space model based on our SCFs substantially outper-forms a lexical model and a model based on a supervised parser 1 .", "labels": [], "entities": [{"text": "verb similarity prediction", "start_pos": 56, "end_pos": 82, "type": "TASK", "confidence": 0.7717567582925161}]}], "introductionContent": [{"text": "Subcategorization frame (SCF) acquisition involves identifying the arguments of a predicate and generalizing about its syntactic frames, where each frame specifies the syntactic type and number of arguments permitted by the predicate.", "labels": [], "entities": [{"text": "Subcategorization frame (SCF) acquisition", "start_pos": 0, "end_pos": 41, "type": "TASK", "confidence": 0.8089961111545563}]}, {"text": "For example, in sentences (1)-(3) the verb distinguish takes three different frames, the difference between which is not evident when considering the phrase structure categorization: (1) Direct Transitive: [They]NP  As SCFs describe the syntactic realization of the verbal predicate-argument structure, they are highly valuable fora variety of NLP tasks.", "labels": [], "entities": []}, {"text": "For example, verb subcategorization information has proven useful for tasks such as parsing (, semantic role labeling (), verb clustering, (Schulte im) and machine translation (hye.", "labels": [], "entities": [{"text": "parsing", "start_pos": 84, "end_pos": 91, "type": "TASK", "confidence": 0.9737508893013}, {"text": "semantic role labeling", "start_pos": 95, "end_pos": 117, "type": "TASK", "confidence": 0.6314833164215088}, {"text": "verb clustering", "start_pos": 122, "end_pos": 137, "type": "TASK", "confidence": 0.7447824776172638}, {"text": "machine translation", "start_pos": 156, "end_pos": 175, "type": "TASK", "confidence": 0.7971973717212677}]}, {"text": "The argumentadjunct distinction is difficult even for humans, and is further complicated by the fact that both arguments and adjuncts can appear frequently in potential argument head positions ().", "labels": [], "entities": []}, {"text": "SCFs are also highly sensitive to domain variation so that both the frames themselves and their probabilities vary depending on the meaning and behavior of predicates in the domain in question (e.g. (), Section 4).", "labels": [], "entities": []}, {"text": "Because of the strong impact of domain variation, SCF information is best acquired automatically.", "labels": [], "entities": [{"text": "SCF information", "start_pos": 50, "end_pos": 65, "type": "TASK", "confidence": 0.8631664216518402}]}, {"text": "Existing data-driven SCF induction systems, however, do not port well between domains.", "labels": [], "entities": [{"text": "SCF induction", "start_pos": 21, "end_pos": 34, "type": "TASK", "confidence": 0.9312483668327332}]}, {"text": "Most existing systems rely on handwritten rules or simple cooccurrence statistics; applied to the grammatical dependency output of supervised statistical parsers.", "labels": [], "entities": []}, {"text": "Even the handful of recent systems that use modern machine learning techniques; Van de Cruys et al., use supervised parsers to pre-process the data 2 . Supervised parsers are notoriously sensitive to domain variation ().", "labels": [], "entities": []}, {"text": "As annotation of data for each new domain is unrealistic, current SCF systems suffer from poor portability.", "labels": [], "entities": []}, {"text": "This problem is compounded for the many systems that employ manually developed SCF rules because rules are inherently ignorant to domain-specific preferences.", "labels": [], "entities": []}, {"text": "The few SCF studies that focused on specific domains (e.g. biomedicine) have reported poor performance due to these reasons (.", "labels": [], "entities": []}, {"text": "Another limitation of most current SCF systems is that they produce a type-level SCF lexicon (i.e. a lexicon which lists, fora given predicate, different SCF types with their relative frequencies).", "labels": [], "entities": []}, {"text": "Such a lexicon provides a useful high-level profile of the syntactic behavior of the predicate in question, but is less useful for downstream NLP tasks (e.g. information extraction, parsing, machine translation) that involve sentence processing and can therefore benefit from SCF information at instance level.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 158, "end_pos": 180, "type": "TASK", "confidence": 0.7413769066333771}, {"text": "parsing", "start_pos": 182, "end_pos": 189, "type": "TASK", "confidence": 0.6476677656173706}, {"text": "machine translation)", "start_pos": 191, "end_pos": 211, "type": "TASK", "confidence": 0.8146981795628866}]}, {"text": "Sentences (1)-(3) demonstrate this limitation -a prior distribution over the possible syntactic frames of distinguish provides only a weak signal to a sentence level NLP application that needs to infer the verbal argument structure of its input sentences.", "labels": [], "entities": []}, {"text": "We propose anew unsupervised model for SCF induction which addresses these problems with existing systems.", "labels": [], "entities": [{"text": "SCF induction", "start_pos": 39, "end_pos": 52, "type": "TASK", "confidence": 0.9853048920631409}]}, {"text": "Our model does not use a parser or hand-written rules, only a part-of-speech (POS) tagger is utilizes in order to produce features for machine learning.", "labels": [], "entities": []}, {"text": "While POS taggers are also sensitive to domain variation, they can be adapted to domains more easily than parsers because they require much smaller amounts of annotated data (.", "labels": [], "entities": [{"text": "POS taggers", "start_pos": 6, "end_pos": 17, "type": "TASK", "confidence": 0.821913331747055}]}, {"text": "However, as we demonstrate in our experiments, domain adaptation of POS tagging may not even be necessary to obtain good results on the SCF acquisition task.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 68, "end_pos": 79, "type": "TASK", "confidence": 0.8031946420669556}, {"text": "SCF acquisition task", "start_pos": 136, "end_pos": 156, "type": "TASK", "confidence": 0.959534764289856}]}, {"text": "Our model, based on the Markov Random Field (MRF) framework, performs instance-based SCF learning.", "labels": [], "entities": [{"text": "SCF learning", "start_pos": 85, "end_pos": 97, "type": "TASK", "confidence": 0.8936580419540405}]}, {"text": "It encodes syntactic similarities among verb instances across different verb types (derived from a lexical and POS-based feature representation of verb instances) as well as prior beliefs on the tendencies of specific instances of the same verb type to take the same SCF.", "labels": [], "entities": []}, {"text": "We evaluate our model against corpora annotated with verb instance SCFs ().", "labels": [], "entities": []}, {"text": "In addition, following the Levin verb clustering tradition which ties verb meanings with their syntactic properties, we evaluate the semantic predictive power of our clusters.", "labels": [], "entities": [{"text": "Levin verb clustering", "start_pos": 27, "end_pos": 48, "type": "TASK", "confidence": 0.5703573226928711}]}, {"text": "In the former evaluation, our model outperforms a number of strong baselines, including supervised and type-level ones, achieving an accuracy of up to 69.2%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 133, "end_pos": 141, "type": "METRIC", "confidence": 0.9996281862258911}]}, {"text": "In the latter evaluation a vector space model that utilized our induced SCFs substantially outperforms the output of a type-level SCF system that uses the fully trained Stanford parser.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our model is unique compared to existing systems in two respects.", "labels": [], "entities": []}, {"text": "First, it does not utilize supervision in the form of either a supervised syntactic parser and/or manually crafted SCF rules.", "labels": [], "entities": []}, {"text": "Consequently, it induces unnamed frames (clusters) that are not directly comparable to the named frames induced by previous systems.", "labels": [], "entities": []}, {"text": "Second, it induces syntactic frames at the verb instance, rather than type, level.", "labels": [], "entities": []}, {"text": "Evaluation, and especially comparison to previous work, is therefore challenging.", "labels": [], "entities": []}, {"text": "We therefore evaluate our system in two ways.", "labels": [], "entities": []}, {"text": "First, we compare its output, as well as the output of a number of clustering baselines, to the gold standard annotation of corpora from two different domains (the only publicly available ones with instance level SCF annotation, to the best of our knowledge).", "labels": [], "entities": []}, {"text": "Second, in order to compare the output of our system to a rule-based SCF system that utilizes a supervised syntactic parser, we turn to a task-based evaluation.", "labels": [], "entities": []}, {"text": "We aim to predict the degree of similarity between verb pairs and, following ( , we do so using a syntactic-based vector space model (VSM).", "labels": [], "entities": []}, {"text": "We construct three VSMs -(a) one that derives features from our clusters; (b) one whose features come from the output of a state-of-the-art verb type level, rule based, SCF system) that uses a modern parser ( ; and (c) a standard lexical VSM.", "labels": [], "entities": []}, {"text": "Below we show that our system compares favorably in both evaluations.", "labels": [], "entities": []}, {"text": "We experimented with two datasets taken from different domains: labor legislation and environment ().", "labels": [], "entities": []}, {"text": "These datasets were created through web crawling followed by domain filtering.", "labels": [], "entities": []}, {"text": "Each sentence in both datasets may contain multiple verbs but only one target verb has been manually annotated with a SCF.", "labels": [], "entities": []}, {"text": "The labour legislation domain dataset contains 4415 annotated verb instances (and hence also sentences) of 117 types, and the environmental domain dataset contains 4503 annotated verb instances of 116 types.", "labels": [], "entities": [{"text": "labour legislation domain dataset", "start_pos": 4, "end_pos": 37, "type": "DATASET", "confidence": 0.6347565874457359}, {"text": "environmental domain dataset", "start_pos": 126, "end_pos": 154, "type": "DATASET", "confidence": 0.6353588004906973}]}, {"text": "In both datasets no verb type accounts for more than 4% of the instances and only up to 35 verb types account for 1% of the instances or more.", "labels": [], "entities": []}, {"text": "The lexical difference between the corpora is substantial: they share only 42 annotated verb types in total, of which only 2 verb types (responsible for 4.1% and 5.2% of the instances in the environment and labor legislation domains respectively) belong to the 20 most frequent types (responsible for 37.9% and 46.85% of the verb instances in the respective domains) of each corpus.", "labels": [], "entities": []}, {"text": "The 29 members of the SCF inventory are detailed in (., presenting the distribution of the 5 highest frequency frames in each corpus, demonstrates that, in addition to the significant lexical difference, the corpora differ to some extent in their syntactic properties.", "labels": [], "entities": []}, {"text": "This is reflected by the substantially different frequencies of the \"dobj:iobj-prep:su\" and \"dobj:su\" frames.", "labels": [], "entities": []}, {"text": "As a pre-processing step we first POS tagged the datasets with the Stanford tagger () trained on the standard POS training sections of the WSJ PennTreebank corpus.", "labels": [], "entities": [{"text": "WSJ PennTreebank corpus", "start_pos": 139, "end_pos": 162, "type": "DATASET", "confidence": 0.8852822184562683}]}, {"text": "Experimental Protocol The computational complexity of our algorithm does not allow us to run it on thousands of verb instances in a feasible time.", "labels": [], "entities": []}, {"text": "We therefore repeatedly sampled 5% of the sentences from each dataset, ran our algorithm as well as the baselines (see below) and report the average performance of each method.", "labels": [], "entities": []}, {"text": "The number of repetitions was 40 and samples were drawn from a uniform distribution while still promising that the distribution of gold standard SCFs in each sample is identical to their distribution in the entire dataset.", "labels": [], "entities": []}, {"text": "Before running this protocol, 5% of each corpus was kept as held-out data on which hyperparameter tuning was performed.", "labels": [], "entities": []}, {"text": "We compare our system's output to instance-level gold standard annotation.", "labels": [], "entities": []}, {"text": "We use standard measures for clustering evaluation, one measure from each of the two leading measure types: the V measure, which is an information theoretic measure, and greedy many-toone accuracy, which is a mapping-based measure.", "labels": [], "entities": [{"text": "clustering evaluation", "start_pos": 29, "end_pos": 50, "type": "TASK", "confidence": 0.9176197350025177}, {"text": "accuracy", "start_pos": 188, "end_pos": 196, "type": "METRIC", "confidence": 0.5687386393547058}]}, {"text": "For the latter, each induced cluster is first mapped to the gold SCF frame that annotates the highest number of verb instances this induced cluster also annotates and then a standard instance-level accuracy score is computed (see, e.g.,).", "labels": [], "entities": [{"text": "accuracy score", "start_pos": 198, "end_pos": 212, "type": "METRIC", "confidence": 0.9647709727287292}]}, {"text": "Both measures scale from 100 (perfect match with gold standard) to 0 (no match).", "labels": [], "entities": []}, {"text": "As mentioned above, comparing the performance of our system with respect to a gold standard to the performance of previous type-level systems that used hand-crafted rules and/or supervised syntactic parsers would be challenging.", "labels": [], "entities": []}, {"text": "We therefore compare our model to the following baselines: (a) The most frequent class (MFC) baseline which assigns all verb instances with the SCF that is the most frequent one in the gold standard annotation of the data; (b) The Random baseline which simply assigns every verb instance with a randomly selected SCF; (c) Algorithm 1 of section 3.2 which generates unsupervised verb instance clustering such that verb instances of the same type are assigned to the same cluster; and (d) Finally, we also compare our model against versions where everything is kept fixed, except a subset of potentials which is omitted.", "labels": [], "entities": []}, {"text": "This enables us to study the intricacies of our model and the relative importance of its components.", "labels": [], "entities": []}, {"text": "For all models, the number of induced clusters is equal to the number of SCFs in the gold standard.", "labels": [], "entities": []}, {"text": "Results presents the results, demonstrating that our full model substantially outperforms all baselines.", "labels": [], "entities": []}, {"text": "For the first two simple heuristic baselines (MFC and Random) the margin is higher than 20% for both the greedy M-1 mapping measure and the V measure.", "labels": [], "entities": [{"text": "margin", "start_pos": 66, "end_pos": 72, "type": "METRIC", "confidence": 0.9813712239265442}]}, {"text": "Note tat the V score of the MFC baseline is 0 by definition, as it assigns all items to the same cluster.", "labels": [], "entities": [{"text": "MFC baseline", "start_pos": 28, "end_pos": 40, "type": "DATASET", "confidence": 0.752267062664032}]}, {"text": "The poor performance of these simple baselines is an indication of the difficulty of our task.", "labels": [], "entities": []}, {"text": "Recall that the type level clustering induced by Algorithm 1 is the main source of type level information our model utilizes (through its singleton potentials).", "labels": [], "entities": [{"text": "type level clustering", "start_pos": 16, "end_pos": 37, "type": "TASK", "confidence": 0.676252027352651}]}, {"text": "The comparison to the output of this algorithm (the Type Pre-clustering baseline) therefore shows the quality of the instance level refinement our model provides.", "labels": [], "entities": []}, {"text": "As seen in table 3, our model outperforms this baseline by 6.9% for the M-1 measure and 5.2% for the V measure.", "labels": [], "entities": []}, {"text": "In order to compare our model to its components we exclude either the EC potentials (\u03c6 and \u03be) only (Model -EC), or the EC and the singleton potentials (\u03b8 i , Model -EC -Type pre-clustering: Results for our full model, the baselines (Type Pre-clustering: the pre-clustering algorithm (Algorithm 1 of section 3.2), MFC: the most frequent class (SCF) in the gold standard annotation and Random: random SCF assignment) and the model components.", "labels": [], "entities": []}, {"text": "The full model outperforms all other models across measures and datasets. from the type level information encoded through the singleton potentials than from the EC potentials.", "labels": [], "entities": []}, {"text": "Yet, EC potentials do lead to an improvement of up to 1.5% in M-1 and up to 1.1% in V and are therefore responsible for up to 26.1% and 21.2% of the improvement over the type pre-clustering baseline in terms of M-1 and V, respectively.", "labels": [], "entities": [{"text": "V", "start_pos": 84, "end_pos": 85, "type": "METRIC", "confidence": 0.9724844694137573}]}, {"text": "We next evaluate our model in the context of vector space modeling for verb similarity prediction.", "labels": [], "entities": [{"text": "verb similarity prediction", "start_pos": 71, "end_pos": 97, "type": "TASK", "confidence": 0.8070973753929138}]}, {"text": "Since most previous word similarity works used noun datasets, we constructed anew verb pair dataset, following the protocol used in the collection of the wordSimilarity-353 dataset ().", "labels": [], "entities": []}, {"text": "Our dataset consists of 143 verb pairs, constructed from 122 unique verb lemma types.", "labels": [], "entities": []}, {"text": "The participating verbs appear \u2265 10 times in the concatenation of the labour legislation and the environment datasets.", "labels": [], "entities": []}, {"text": "Only pairs of verbs that were considered at least remotely similar by human judges (independent of those that provided the similarity scores) were included.", "labels": [], "entities": []}, {"text": "A similarity score between 1 and 10 was assigned to each pair by 10 native English speaking annotators and were then averaged in order to get a unique pair score.", "labels": [], "entities": [{"text": "similarity score", "start_pos": 2, "end_pos": 18, "type": "METRIC", "confidence": 0.9530132710933685}]}, {"text": "Our first baseline is a standard VSM based on lexical collocations.", "labels": [], "entities": []}, {"text": "In this model features correspond to the number of collocations inside a size 2 window of the represented verb with each of the 5000 most frequent nouns in the Google n-gram corpus.", "labels": [], "entities": [{"text": "Google n-gram corpus", "start_pos": 160, "end_pos": 180, "type": "DATASET", "confidence": 0.6557344297568003}]}, {"text": "Since our corpora are limited in size, we use the collocation counts from the Google corpus.", "labels": [], "entities": [{"text": "Google corpus", "start_pos": 78, "end_pos": 91, "type": "DATASET", "confidence": 0.8403038084506989}]}, {"text": "We used our model to generate a vector representation of each verb in the following way.", "labels": [], "entities": []}, {"text": "We run the model 5000 times, each time over a set of verbs consisting of one instance of each of the 122 verb types participating in the verb similarity set.", "labels": [], "entities": []}, {"text": "The output of each such run is transformed to a binary vector for each participating verb, where all coordinates are assigned the value of 0, except from the one that corresponds to the cluster to which the verb was assigned which has the value of 1.", "labels": [], "entities": []}, {"text": "The final vector representation is a concatenation of the 5000 binary vectors.", "labels": [], "entities": []}, {"text": "Note that for this task we did not use the graph cut algorithm to generate a final clustering from the multiple MRF runs.", "labels": [], "entities": []}, {"text": "Instead we concatenated the output of all these runs into one feature representation that facilitates similarity prediction.", "labels": [], "entities": [{"text": "similarity prediction", "start_pos": 102, "end_pos": 123, "type": "TASK", "confidence": 0.7704980671405792}]}, {"text": "For our model we estimated the verb pair similarity using the Tanimato similarity score for binary vectors: For the baseline model, where the features are collocation counts, we used the standard cosine similarity.", "labels": [], "entities": [{"text": "Tanimato similarity score", "start_pos": 62, "end_pos": 87, "type": "METRIC", "confidence": 0.7501876850922903}]}, {"text": "Our second baseline is identical to our model, except that: (a) the data is parsed with the Stanford parser (version 3.3.0, ( ) which was trained with sections 2-21 of the WSJ corpus; (b) the phrase structure output of the parser is transformed to the CoNLL dependency format using the official CoNLL 2007 conversion script; and then (c) the SCF of each verb instance is inferred using the rule-based system used by).", "labels": [], "entities": [{"text": "WSJ corpus", "start_pos": 172, "end_pos": 182, "type": "DATASET", "confidence": 0.9797307848930359}, {"text": "CoNLL 2007 conversion script", "start_pos": 295, "end_pos": 323, "type": "DATASET", "confidence": 0.8463765531778336}]}, {"text": "The vector space representation for each verb is then created using the process we described for our model and the same holds for vector comparison.", "labels": [], "entities": []}, {"text": "This baseline allows direct comparison of frames induced by our SCF model with those derived from a supervised parser's output.", "labels": [], "entities": []}, {"text": "We computed the Pearson correlation between the scores of each of the models and the human scores.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 16, "end_pos": 35, "type": "METRIC", "confidence": 0.9702707231044769}]}, {"text": "The results demonstrate the superiority of our model in predicting verb similarity: the correlation of our model with the human scores is 0.642 while the correlation of the lexical collocation baseline is 0.522 and that of the supervised parser baseline is only 0.266.", "labels": [], "entities": [{"text": "predicting verb similarity", "start_pos": 56, "end_pos": 82, "type": "TASK", "confidence": 0.8973408540089926}]}, {"text": "The results indicate that in addition to their good alignment with SCFs, our clusters are also highly useful for verb meaning representation.", "labels": [], "entities": [{"text": "verb meaning representation", "start_pos": 113, "end_pos": 140, "type": "TASK", "confidence": 0.7661211093266805}]}, {"text": "This is inline with the verb clustering theory of the Levin tradition which ties verb meaning with their syntactic properties.", "labels": [], "entities": [{"text": "verb clustering", "start_pos": 24, "end_pos": 39, "type": "TASK", "confidence": 0.7206199169158936}]}, {"text": "We consider this an intriguing direction of future work.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Top 5 most frequent SCFs for the Environment and Labour Legislation datasets used in our  experiments.", "labels": [], "entities": [{"text": "Labour Legislation datasets", "start_pos": 59, "end_pos": 86, "type": "DATASET", "confidence": 0.6820027629534403}]}, {"text": " Table 3: Results for our full model, the baselines (Type Pre-clustering: the pre-clustering algorithm  (Algorithm 1 of section 3.2), MFC: the most frequent class (SCF) in the gold standard annotation and  Random: random SCF assignment) and the model components. The full model outperforms all other  models across measures and datasets.", "labels": [], "entities": []}]}