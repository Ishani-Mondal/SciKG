{"title": [{"text": "Queries as a Source of Lexicalized Commonsense Knowledge", "labels": [], "entities": []}], "abstractContent": [{"text": "The role of Web search queries has been demonstrated in the extraction of attributes of instances and classes, or of sets of related instances and their class labels.", "labels": [], "entities": []}, {"text": "This paper explores the acquisition of open-domain commonsense knowledge, usually available as factual knowledge, from Web search queries.", "labels": [], "entities": []}, {"text": "Similarly to previous work in open-domain information extraction , knowledge extracted from text-in this case, from queries-takes the form of lexicalized assertions associated with open-domain classes.", "labels": [], "entities": [{"text": "open-domain information extraction", "start_pos": 30, "end_pos": 64, "type": "TASK", "confidence": 0.6364374061425527}]}, {"text": "Experimental results indicate that facts extracted from queries complement, and have competitive accuracy levels relative to, facts extracted from Web documents by previous methods .", "labels": [], "entities": [{"text": "accuracy", "start_pos": 97, "end_pos": 105, "type": "METRIC", "confidence": 0.9980787038803101}]}], "introductionContent": [{"text": "Motivation: Open-domain information extraction methods () aim at distilling text into knowledge assertions about classes, instances and relations among them).", "labels": [], "entities": [{"text": "Open-domain information extraction", "start_pos": 12, "end_pos": 46, "type": "TASK", "confidence": 0.6087715228398641}]}, {"text": "Ideally, the assertions would complement or expand upon knowledge available in popular, human-created resources such as Wikipedia) and Freebase (, reducing costs and scalability issues associated with manual editing, curation and maintenance of knowledge.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 135, "end_pos": 143, "type": "DATASET", "confidence": 0.9290785193443298}]}, {"text": "Candidate knowledge assertions extracted from text for various instances and classes ( must satisfy several constraints in order to be useful.", "labels": [], "entities": []}, {"text": "First, their boundaries must be correctly identified within the larger context (e.g., a document sentence) from which they are extracted.", "labels": [], "entities": []}, {"text": "In practice, this is a challenge with arbitrary Web documents, where even instances and class labels that are complex nouns, and thus still shorter than candidate assertions, are difficult to precisely detect and pick out from surrounding text (.", "labels": [], "entities": []}, {"text": "This causes the extraction of assertions like companies may \"be in the process\", hurricanes may \"run from june\", or video games may \"make people\" ).", "labels": [], "entities": []}, {"text": "Second, the assertions must be correctly associated with their corresponding instance or class.", "labels": [], "entities": []}, {"text": "In practice, tagging and parsing errors over documents of arbitrary quality may cause the extracted assertions to be associated with the wrong instances or classes.", "labels": [], "entities": []}, {"text": "Examples are video games may \"watch movies\", or video games may \"read a book\".", "labels": [], "entities": []}, {"text": "Third, the assertions, even if true, must refer to relevant properties or facts, rather than to statements of little or no practical interest to anyone.", "labels": [], "entities": []}, {"text": "In practice, relevant properties maybe difficult to distinguish from uninteresting statements in Web documents.", "labels": [], "entities": []}, {"text": "Consequently, assertions extracted from Web documents include the facts that companies may \"say in a statement\", or that hurricanes may \"be just around the corner\" or may \"be in effect\".", "labels": [], "entities": []}, {"text": "Contributions: This paper explores the use of Web search queries, as opposed to Web documents, as a textual source from which knowledge pertaining to open-domain classes can be extracted.", "labels": [], "entities": []}, {"text": "Previous explorations of the role of queries in information extraction include the acquisition of attributes of instances ( and of classes (Van Durme and Pas\u00b8ca; the acquisition of sets of related instances and their class labels (Van Durme and Pas\u00b8ca; the disambiguation of instances mentioned in queries relative to entries in external knowledge repositories () and its application in query expansion (; and the extraction of the most salient of the instances mentioned in a given Web document (.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 48, "end_pos": 70, "type": "TASK", "confidence": 0.7312302589416504}, {"text": "query expansion", "start_pos": 387, "end_pos": 402, "type": "TASK", "confidence": 0.719299927353859}]}, {"text": "In comparison, this paper shows that queries also lend themselves to the acquisition of factual knowledge beyond attributes, like the facts that companies may \"buy back stock\", hurricanes may \"need warm water\", and video games may \"come out on tuesdays\".", "labels": [], "entities": []}, {"text": "To extract knowledge assertions for diverse classes of interest to Web users, the method applies simple extraction patterns to queries.", "labels": [], "entities": []}, {"text": "The presence of the source queries, from which the assertions are extracted, is in itself deemed evidence that the Web users who submitted the queries find the assertions to be relevant and not just random statements.", "labels": [], "entities": []}, {"text": "Experimental results indicate that knowledge assertions extracted from queries complement, and have competitive accuracy levels relative to, knowledge extracted from Web documents by previous methods.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 112, "end_pos": 120, "type": "METRIC", "confidence": 0.9974061846733093}]}], "datasetContent": [{"text": "Textual Data Sources: The experiments rely on a random sample of around 1 billion fullyanonymized Web search queries in English.", "labels": [], "entities": []}, {"text": "The sample is drawn from queries submitted to a general-purpose Web search engine.", "labels": [], "entities": []}, {"text": "Each query is available independently from other queries, and is accompanied by its frequency of occurrence in Target Class (class descriptors to be looked up in queries) Actor  Target Classes: shows the set of 40 target classes for evaluating the extracted facts.", "labels": [], "entities": []}, {"text": "Similar evaluation strategies were followed in previous work.", "labels": [], "entities": []}, {"text": "As illustrated earlier in, a target class consists in a small set of phrase descriptors.", "labels": [], "entities": []}, {"text": "The phrase descriptors are selected such that they best approximate the meaning of the class.", "labels": [], "entities": []}, {"text": "In general, the descriptors can be selected and expanded with any strategy from any source.", "labels": [], "entities": []}, {"text": "One such possible source might be synonym sets from WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 52, "end_pos": 59, "type": "DATASET", "confidence": 0.9629208445549011}]}, {"text": "Following a stricter strategy, the sets of descriptors in our experiments contain only one phrase each, manually selected to match the target class.", "labels": [], "entities": []}, {"text": "Examples are the sets of phrase descriptors {actors} for the class Actor and {nba teams} for NbaTeam.", "labels": [], "entities": []}, {"text": "The occurrence of a descriptor (nba teams) in a query (\"how do nba teams make money\") is deemed equivalent to a mention of the corresponding class (NbaTeam) in that query.", "labels": [], "entities": []}, {"text": "Each set of descriptors of a class is then expanded (not shown in), to also include the singular forms of the descriptors (e.g., nba team for nba teams).", "labels": [], "entities": []}, {"text": "Further inclusion of additional descriptors would increase the coverage of the extracted facts.", "labels": [], "entities": [{"text": "coverage", "start_pos": 63, "end_pos": 71, "type": "METRIC", "confidence": 0.9624292254447937}]}, {"text": "Experimental Runs: The baseline run RD is the extraction method introduced in ).", "labels": [], "entities": []}, {"text": "The method produces triples of an instance or a class, a text fragment capturing a fact, and another instance or class.", "labels": [], "entities": []}, {"text": "In these experiments, the second and third elements of each triple are concatenated together, giving pairs of an instance or a class, and a fact applying to it.", "labels": [], "entities": []}, {"text": "The baseline run is applied to around 500 million Web documents in English.", "labels": [], "entities": []}, {"text": "1 In addition to the baseline run, the method introduced in this paper constitutes the second experimental run R Q . Facts extracted by the two experimental runs are directly comparable: both are text snippets extracted from the respective sources of text -documents in the case of RD , or queries in the case of R Q . Parameter Settings: Queries that match any of the extraction patterns from are syntactically parsed (, in order to verify that the first token of an extracted fact is the head verb of the query.", "labels": [], "entities": []}, {"text": "Extracted facts that do not satisfy the constraint are discarded.", "labels": [], "entities": []}, {"text": "A positive side effect of doing so is to avoid extraction from some of the particularly subjective queries.", "labels": [], "entities": []}, {"text": "For example, facts extracted from the queries \"why is (A) evil\" or \"why is (B) ugly\", where (A) and (B) are the name of a company and actress respectively, are discarded.", "labels": [], "entities": []}, {"text": "Accuracy: The measurement of recall requires knowledge of the complete set of items (in our case, facts) to be extracted.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9899124503135681}, {"text": "recall", "start_pos": 29, "end_pos": 35, "type": "METRIC", "confidence": 0.9960719347000122}]}, {"text": "Unfortunately, this number is often unavailable in information extraction tasks in general (), and fact extraction in particular.", "labels": [], "entities": [{"text": "information extraction tasks", "start_pos": 51, "end_pos": 79, "type": "TASK", "confidence": 0.8749296069145203}, {"text": "fact extraction", "start_pos": 99, "end_pos": 114, "type": "TASK", "confidence": 0.8086916506290436}]}, {"text": "Indeed, the manual enumeration of all facts of each target class, to measure recall, is unfeasible.", "labels": [], "entities": [{"text": "recall", "start_pos": 77, "end_pos": 83, "type": "METRIC", "confidence": 0.9972903728485107}]}, {"text": "Therefore, the evaluation focuses on the assessment of accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 55, "end_pos": 63, "type": "METRIC", "confidence": 0.9976950287818909}]}, {"text": "Following evaluation methodology from prior work, the top 50 facts, from a ranked lists extracted for each target class, are manually assigned correctness labels.", "labels": [], "entities": []}, {"text": "A fact is marked as vital, if it must be present among representative facts of the class; okay, if it provides useful but non-essential information; and wrong, if it is incorrect.", "labels": [], "entities": []}, {"text": "For example, the facts \"run on kerosene\", \"be delayed\" and \"fly wiki\" are annotated as vital, okay and wrong respectively for the class Aircraft.", "labels": [], "entities": []}, {"text": "Precision is the sum of the correctness values of the facts, divided by the number of facts.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9834647178649902}]}, {"text": "shows a sample of facts extracted from queries by run R Q , which are judged to be vital or okay.", "labels": [], "entities": []}, {"text": "provides a comparison of precision at ranks 10, 20 and 50, for each of the 40 target classes and as an average overall target classes.", "labels": [], "entities": [{"text": "precision", "start_pos": 25, "end_pos": 34, "type": "METRIC", "confidence": 0.9995724558830261}]}, {"text": "The scores vary from one class to another and between the two runs, for example 0.22 (R D ) and 0.73 (R Q ) for the class Currency at rank 50, but 0.77 (R D ) and 0.59 (R Q ) for Treaty.", "labels": [], "entities": [{"text": "Treaty", "start_pos": 179, "end_pos": 185, "type": "DATASET", "confidence": 0.8314660787582397}]}, {"text": "Run R Q fails to extract any facts for two of the target classes, SkyBody and SportEvent.", "labels": [], "entities": [{"text": "SkyBody", "start_pos": 66, "end_pos": 73, "type": "DATASET", "confidence": 0.9864845871925354}, {"text": "SportEvent", "start_pos": 78, "end_pos": 88, "type": "DATASET", "confidence": 0.9450346827507019}]}, {"text": "Therefore, it receives no credit for those classes during the computation of precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 77, "end_pos": 86, "type": "METRIC", "confidence": 0.9971796274185181}]}, {"text": "Over all target classes, run R Q is superior to run RD , with relative precision boosts of 65% (0.71 vs. 0.43) at rank 10, 67% at rank 20, and 65% at rank 50.", "labels": [], "entities": [{"text": "precision boosts", "start_pos": 71, "end_pos": 87, "type": "METRIC", "confidence": 0.976020872592926}]}, {"text": "The results show that facts extracted from Class: Actor (may): RD: [do a great job, get the part, play their roles, play their parts, play their characters, be on a theatre, die aged 81, be all great, deliver their lines, portray their characters, take on a role, be best known for his role, play the role of god, be people, give great performances, bring the characters to life, wear a mask, be the one, have chemistry, turn director, read the script, ..]", "labels": [], "entities": [{"text": "RD", "start_pos": 63, "end_pos": 65, "type": "METRIC", "confidence": 0.9491145014762878}]}, {"text": "RQ: [prepare fora role, get an agent, do love scenes, get paid, be left handed, need to warm up, get started, get paid so much, memorize their lines, get ripped so fast, remember their lines, make themselves cry, learn their lines, jump out of a window in times square, lose weight so fast, play dead, be paid, kiss, remember lines, memorize lines, get discovered, get paid for movies, go uncredited, say break a leg, get their start, have perfect skin, become actors, ..]", "labels": [], "entities": [{"text": "RQ", "start_pos": 0, "end_pos": 2, "type": "DATASET", "confidence": 0.7600916028022766}]}, {"text": "Class: Car (may): RD: [get a tax write-off, can be more competitive than airline rates, be in good condition, be first for secondhand cars, be in the shop, relocate to a usa firm, be in motion, come to a stop, hire companies, be in great shape, be for sale, hire service from spain, ride home, be on fire, use the autos.com, come to a halt, catch fire, be on road, be on display, goon sale, hit a tree, be available for delivery, stop in front, be a necessity, go off the road, pullout in front, hire services, run out of gas, ..]", "labels": [], "entities": [{"text": "RD", "start_pos": 18, "end_pos": 20, "type": "METRIC", "confidence": 0.9076721668243408}]}, {"text": "RQ: [backfire, burn oil, save ostriches from extinction, pull to the right, pull to the left, catch on fire, run hot, sputter, get repossessed, have atop speed, be called a car, have gears, get impounded, be called cars, go to auction, called whip, made of steel, get hot in the sun, shake at high speed, changed america, totaled, cutout, cutoff while driving, fail emissions, protect from lightning, run rich, lose oil, become electrically charged, cutoff, flip over, know tire pressure, have a maximum speed, require premium gas, shake at high speeds, stall out, cause acid rain, fog up, get stuck in park, need an oil change, ..]", "labels": [], "entities": [{"text": "RQ", "start_pos": 0, "end_pos": 2, "type": "DATASET", "confidence": 0.5313698649406433}]}, {"text": "Class: Company (may): RD: [say in a statement, specialize in local moves, be in the process, go out of business, have been in business, be in business, do business, file for bankruptcy, make money, be on track, say in a press release, be a place, have cutback on health insurance, state in a press release, be on the verge, save money, be in talks, have helped thousands of consumers, reduce costs, go bust, be in the midst, say in a release, be founded in 1999, be in trouble, be founded in 2000, be losing money, ..]", "labels": [], "entities": [{"text": "RD", "start_pos": 22, "end_pos": 24, "type": "METRIC", "confidence": 0.6289144158363342}]}, {"text": "RQ: [buy back stock, go public, buyback shares, incorporate in delaware, pay dividends, merge, go global, go international, use financial statements, verify education, expand internationally, go green, verify employment, need a website, choose to form as a corporation, do market research, go private, diversify, go into administration, get on angies list, pay dividend, struck off, buyback their shares, get audited, need a mission statement, repurchase common stock, spin off, get listed on the nyse, create value, distribute dividends, need a strategic plan, ..]", "labels": [], "entities": [{"text": "RQ", "start_pos": 0, "end_pos": 2, "type": "DATASET", "confidence": 0.4987902045249939}]}, {"text": "Class: Mountain (may): RD: [spot fever, meet the sea, be covered with snow, be covered in snow, be the place, come into view, be on fire, be fun, fly fishing, be volcano, be moved out of their places, enjoy the exhilaration, meet the ocean, be available for hire, keep their secrets, win the mwc in 2010, ..]", "labels": [], "entities": [{"text": "RD", "start_pos": 23, "end_pos": 25, "type": "METRIC", "confidence": 0.9199550151824951}, {"text": "win the mwc in 2010", "start_pos": 284, "end_pos": 303, "type": "DATASET", "confidence": 0.6459810614585877}]}, {"text": "RQ: [affect rainfall, affect the climate of an area, affect climate, be measured, be formed, be created, be made, grow, affect weather, have snow on top, affect solar radiation, affect temperature, be formed ks2, affect the weather, be built, affect people, look blue, tops cold, affect neighboring climates, be formed video, help shape the development of greek civilization, be made for kids, occur, affect the climate, be formed, be formed wikipedia, have roots, affect precipitation, exist, affect life on earth, be formed kids, float in avatar, erode, have snow on the top, affect the political character of greece, help rain form, ..]: Comparative top facts extracted fora sample of classes from documents (R D ) or queries (R Q ) queries have higher levels of accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 766, "end_pos": 774, "type": "METRIC", "confidence": 0.9985659718513489}]}, {"text": "Facts from Documents vs. Queries: compares the top facts extracted by the two experimental runs fora sample of target classes.", "labels": [], "entities": []}, {"text": "Most commonly, erroneous facts are extracted by run RD due to the extraction of relatively uninteresting properties (a Company may \"say in a statement\" or \"be in the process\").", "labels": [], "entities": []}, {"text": "Other errors in RD are caused by wrong boundary detection of facts within documents (a Company may \"be in the midst\"), or by the association of a fact with the wrong instance or class (a Car may \"hire companies\" or \"hire services\").", "labels": [], "entities": [{"text": "RD", "start_pos": 16, "end_pos": 18, "type": "TASK", "confidence": 0.990444004535675}]}, {"text": "As for facts extracted by run R Q , they are sometimes too informal, due to the more conversational nature of queries when compared to documents.", "labels": [], "entities": []}, {"text": "Queries may suggest that a Car may \"know tire pressure\".", "labels": [], "entities": []}, {"text": "Occasionally, similarly to facts from documents, they have wrong boundaries (a Mountain may \"be made for kids\" or \"be formed wikipedia\"); and they may correspond to less interesting, or too specific, properties (a Company may \"incorporate in delaware\").", "labels": [], "entities": []}, {"text": "Lastly, queries may appear to be questions, but occasionally they really are not.", "labels": [], "entities": []}, {"text": "An example is the query \"why did the actor jump out of the window in times square\", which may refer to a joke.", "labels": [], "entities": []}, {"text": "When such queries match one of the extraction patterns, they produce wrong facts.", "labels": [], "entities": []}, {"text": "Overall, corroborates the scores from.", "labels": [], "entities": []}, {"text": "It suggests that a) facts extracted by either RD or R Q still need refinement, before they can capture essential characteristics of the respective classes and nothing else; and b) facts extracted in run R Q have higher quality than facts extracted in run RD . Indeed, because factseeking queries inquire about the value (or reason, or manner) of some relations of an instance, the facts themselves tend to be more relevant than facts extracted from arbitrary document sentences.", "labels": [], "entities": []}, {"text": "An issue related to facts extracted from text is their ability to capture the kind of \"obvious\" commonsense knowledge () that would be essential for machine-driven reasoning.", "labels": [], "entities": []}, {"text": "If it is obvious that \"teachers give lectures\", how likely is it for such information to be explicitly stated in documents or, even more interestingly, inquired about in queries?", "labels": [], "entities": []}, {"text": "Anecdotal evidence gathered during experimentation suggests that queries do produce many commonsense facts, perhaps even surprisingly so given that a) queries tend to be shorter and grammatically simpler than document sentences; and b) the patterns in Table 1 are relatively more restrictive than the patterns used in ).", "labels": [], "entities": []}, {"text": "Indeed, the patterns in, when applied to queries like \"why do teachers give homework\", \"why do teachers give grades\", actually produce commonsense knowledge that teachers give homework, grades (to their students).", "labels": [], "entities": []}, {"text": "In fact, the quality of equivalent facts extracted from documents in ) maybe lower.", "labels": [], "entities": []}, {"text": "Concretely, facts extracted in ) state that what teachers give is students, class, homework and feedback, in this order.", "labels": [], "entities": []}, {"text": "The first two of these extractions are errors, likely caused by the incorrect detection of complex entities and their inter-dependencies in document sentences (.", "labels": [], "entities": []}, {"text": "A necessary condition for the usefulness of extracted facts is that the source text contain consistent, true information.", "labels": [], "entities": []}, {"text": "But both documents and queries may contain contradictory or false information, whether due to unsupported conjectures, unintended errors or systematic campaigns that fall under the scope of adversarial information retrieval ().", "labels": [], "entities": []}, {"text": "The phenomena potentially affect prior work on Web-based open-domain extraction, and potentially affect the quality of facts extracted from queries in this paper.", "labels": [], "entities": [{"text": "open-domain extraction", "start_pos": 57, "end_pos": 79, "type": "TASK", "confidence": 0.6888708174228668}]}, {"text": "For example, facts extracted from queries like \"why do companies like obamacare\" and \"why do companies hate obamacare\" would be inconsistent, if not incorrect.", "labels": [], "entities": []}, {"text": "Occasionally, facts extracted from the two text sources refer to the same properties.", "labels": [], "entities": []}, {"text": "For example, a VideoGame may \"be good for the hand-eye coordination\", according to documents; and may \"improve hand eye coordination\", according to queries.", "labels": [], "entities": [{"text": "VideoGame", "start_pos": 15, "end_pos": 24, "type": "DATASET", "confidence": 0.9502888321876526}]}, {"text": "Nevertheless, facts derived from queries likely serve as a complement, rather than replacement, of facts from documents.", "labels": [], "entities": []}, {"text": "In particular, facts extracted from queries make no attempt to isolate the value of the respective properties, whereas facts extracted from documents usually do.", "labels": [], "entities": []}], "tableCaptions": []}