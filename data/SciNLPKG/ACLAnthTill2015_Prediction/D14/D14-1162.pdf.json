{"title": [{"text": "GloVe: Global Vectors for Word Representation", "labels": [], "entities": [{"text": "Word Representation", "start_pos": 26, "end_pos": 45, "type": "TASK", "confidence": 0.7352505624294281}]}], "abstractContent": [{"text": "Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic , but the origin of these regularities has remained opaque.", "labels": [], "entities": []}, {"text": "We analyze and make explicit the model properties needed for such regularities to emerge in word vectors.", "labels": [], "entities": []}, {"text": "The result is anew global log-bilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods.", "labels": [], "entities": []}, {"text": "Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word co-occurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus.", "labels": [], "entities": []}, {"text": "The model produces a vector space with meaningful sub-structure, as evidenced by its performance of 75% on a recent word analogy task.", "labels": [], "entities": [{"text": "word analogy task", "start_pos": 116, "end_pos": 133, "type": "TASK", "confidence": 0.8326928019523621}]}, {"text": "It also outperforms related models on similarity tasks and named entity recognition.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 59, "end_pos": 83, "type": "TASK", "confidence": 0.6719895899295807}]}], "introductionContent": [{"text": "Semantic vector space models of language represent each word with a real-valued vector.", "labels": [], "entities": []}, {"text": "These vectors can be used as features in a variety of applications, such as information retrieval, document classification), question answering (, named entity recognition (, and parsing ( ).", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 76, "end_pos": 97, "type": "TASK", "confidence": 0.791247546672821}, {"text": "document classification", "start_pos": 99, "end_pos": 122, "type": "TASK", "confidence": 0.7710693180561066}, {"text": "question answering", "start_pos": 125, "end_pos": 143, "type": "TASK", "confidence": 0.9053015112876892}, {"text": "named entity recognition", "start_pos": 147, "end_pos": 171, "type": "TASK", "confidence": 0.6161748866240183}]}, {"text": "Most word vector methods rely on the distance or angle between pairs of word vectors as the primary method for evaluating the intrinsic quality of such a set of word representations.", "labels": [], "entities": []}, {"text": "Recently, introduced anew evaluation scheme based on word analogies that probes the finer structure of the word vector space by examining not the scalar distance between word vectors, but rather their various dimensions of difference.", "labels": [], "entities": []}, {"text": "For example, the analogy \"king is to queen as man is to woman\" should be encoded in the vector space by the vector equation king \u2212 queen = man \u2212 woman.", "labels": [], "entities": []}, {"text": "This evaluation scheme favors models that produce dimensions of meaning, thereby capturing the multi-clustering idea of distributed representations.", "labels": [], "entities": []}, {"text": "The two main model families for learning word vectors are: 1) global matrix factorization methods, such as latent semantic analysis (LSA)) and 2) local context window methods, such as the skip-gram model of.", "labels": [], "entities": []}, {"text": "Currently, both families suffer significant drawbacks.", "labels": [], "entities": []}, {"text": "While methods like LSA efficiently leverage statistical information, they do relatively poorly on the word analogy task, indicating a sub-optimal vector space structure.", "labels": [], "entities": [{"text": "word analogy task", "start_pos": 102, "end_pos": 119, "type": "TASK", "confidence": 0.789220949014028}]}, {"text": "Methods like skip-gram may do better on the analogy task, but they poorly utilize the statistics of the corpus since they train on separate local context windows instead of on global co-occurrence counts.", "labels": [], "entities": []}, {"text": "In this work, we analyze the model properties necessary to produce linear directions of meaning and argue that global log-bilinear regression models are appropriate for doing so.", "labels": [], "entities": []}, {"text": "We propose a specific weighted least squares model that trains on global word-word co-occurrence counts and thus makes efficient use of statistics.", "labels": [], "entities": []}, {"text": "The model produces a word vector space with meaningful substructure, as evidenced by its state-of-the-art performance of 75% accuracy on the word analogy dataset.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 125, "end_pos": 133, "type": "METRIC", "confidence": 0.9990469813346863}]}, {"text": "We also demonstrate that our methods outperform other current methods on several word similarity tasks, and also on a common named entity recognition (NER) benchmark.", "labels": [], "entities": [{"text": "word similarity tasks", "start_pos": 81, "end_pos": 102, "type": "TASK", "confidence": 0.8041625221570333}, {"text": "common named entity recognition (NER)", "start_pos": 118, "end_pos": 155, "type": "TASK", "confidence": 0.7814973592758179}]}, {"text": "We provide the source code for the model as well as trained word vectors at http://nlp.", "labels": [], "entities": []}, {"text": "stanford.edu/projects/glove/.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conduct experiments on the word analogy task of, a variety of word similarity tasks, as described in (, and on the CoNLL-2003 shared benchmark Word analogies.", "labels": [], "entities": [{"text": "word analogy task", "start_pos": 30, "end_pos": 47, "type": "TASK", "confidence": 0.8202732602755228}, {"text": "CoNLL-2003 shared benchmark Word analogies", "start_pos": 118, "end_pos": 160, "type": "DATASET", "confidence": 0.8923888444900513}]}, {"text": "The word analogy task consists of questions like, \"a is to b as c is to ?\" The dataset contains 19,544 such questions, divided into a semantic subset and a syntactic subset.", "labels": [], "entities": [{"text": "word analogy task", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.7887175679206848}]}, {"text": "The semantic questions are typically analogies about people or places, like \"Athens is to Greece as Berlin is to ?\".", "labels": [], "entities": []}, {"text": "The syntactic questions are typically analogies about verb tenses or forms of adjectives, for example \"dance is to dancing as fly is to ?\".", "labels": [], "entities": []}, {"text": "To correctly answer the question, the model should uniquely identify the missing term, with only an exact correspondence counted as a correct match.", "labels": [], "entities": []}, {"text": "We answer the question \"a is to b as c is to ?\" by finding the word d whose representation w dis closest tow b \u2212 w a + w c according to the cosine similarity.", "labels": [], "entities": []}, {"text": "While the analogy task is our primary focus since it tests for interesting vector space substructures, we also evaluate our model on a variety of word similarity tasks in.", "labels": [], "entities": []}, {"text": "These include), MC, RG, SCWS (, and RW (.", "labels": [], "entities": [{"text": "RW", "start_pos": 36, "end_pos": 38, "type": "DATASET", "confidence": 0.8460671901702881}]}, {"text": "The CoNLL-2003 English benchmark dataset for NER is a collection of documents from Reuters newswire articles, annotated with four entity types: person, location, organization, and miscellaneous.", "labels": [], "entities": [{"text": "CoNLL-2003 English benchmark dataset", "start_pos": 4, "end_pos": 40, "type": "DATASET", "confidence": 0.8434086292982101}]}, {"text": "We train models on CoNLL-03 training data on test on three datasets: 1) ConLL-03 testing data, 2) ACE Phase 2 (2001-02) and ACE-2003 data, and 3) MUC7 Formal Run test set.", "labels": [], "entities": [{"text": "CoNLL-03 training data", "start_pos": 19, "end_pos": 41, "type": "DATASET", "confidence": 0.8611979484558105}, {"text": "ConLL-03 testing data", "start_pos": 72, "end_pos": 93, "type": "DATASET", "confidence": 0.7595361868540446}, {"text": "ACE-2003 data", "start_pos": 124, "end_pos": 137, "type": "DATASET", "confidence": 0.8890839219093323}, {"text": "MUC7 Formal Run test set", "start_pos": 146, "end_pos": 170, "type": "DATASET", "confidence": 0.691745913028717}]}, {"text": "We adopt the BIO2 annotation standard, as well as all the preprocessing steps described in (.", "labels": [], "entities": [{"text": "BIO2", "start_pos": 13, "end_pos": 17, "type": "DATASET", "confidence": 0.8468887805938721}]}, {"text": "We use a comprehensive set of discrete features that comes with the standard distribution of the Stanford NER model (.", "labels": [], "entities": [{"text": "Stanford NER model", "start_pos": 97, "end_pos": 115, "type": "DATASET", "confidence": 0.8822051684061686}]}, {"text": "A total of 437,905 discrete features were generated for the CoNLL-2003 training dataset.", "labels": [], "entities": [{"text": "CoNLL-2003 training dataset", "start_pos": 60, "end_pos": 87, "type": "DATASET", "confidence": 0.9362579782803854}]}, {"text": "In addition, 50-dimensional vectors for each word of a five-word context are added and used as continuous features.", "labels": [], "entities": []}, {"text": "With these features as input, we trained a conditional random field (CRF) with exactly the same setup as the CRF join model of ().", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Co-occurrence probabilities for target words ice and steam with selected context words from a 6  billion token corpus. Only in the ratio does noise from non-discriminative words like water and fashion  cancel out, so that large values (much greater than 1) correlate well with properties specific to ice, and  small values (much less than 1) correlate well with properties specific of steam.", "labels": [], "entities": []}, {"text": " Table 2: Results on the word analogy task, given  as percent accuracy. Underlined scores are best  within groups of similarly-sized models; bold  scores are best overall. HPCA vectors are publicly  available 2 ; (i)vLBL results are from (Mnih et al.,  2013); skip-gram (SG) and CBOW results are  from (Mikolov et al., 2013a,b); we trained SG  \u2020  and CBOW  \u2020 using the word2vec tool 3 . See text  for details and a description of the SVD models.", "labels": [], "entities": [{"text": "word analogy task", "start_pos": 25, "end_pos": 42, "type": "TASK", "confidence": 0.8488600254058838}, {"text": "accuracy", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.9979058504104614}]}, {"text": " Table 2. 3COSMUL performed  worse than cosine similarity in almost all of our experiments.", "labels": [], "entities": []}, {"text": " Table 3: Spearman rank correlation on word simi- larity tasks. All vectors are 300-dimensional. The  CBOW  *  vectors are from the word2vec website  and differ in that they contain phrase vectors.", "labels": [], "entities": []}, {"text": " Table 4: F1 score on NER task with 50d vectors.  Discrete is the baseline without word vectors. We  use publicly-available vectors for HPCA, HSMN,  and CW. See text for details.  Model  Dev Test ACE MUC7  Discrete 91.0 85.4 77.4  73.4  SVD  90.8 85.7 77.3  73.7  SVD-S 91.0 85.5 77.6  74.3  SVD-L 90.5 84.8 73.6  71.5  HPCA 92.6 88.7 81.7  80.7  HSMN 90.5 85.7 78.7  74.7  CW  92.2 87.4 81.7  80.2  CBOW 93.1 88.2 82.2  81.1  GloVe  93.2 88.3 82.9  82.2", "labels": [], "entities": [{"text": "F1 score", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.968498021364212}, {"text": "NER task", "start_pos": 22, "end_pos": 30, "type": "TASK", "confidence": 0.822391003370285}, {"text": "HPCA", "start_pos": 136, "end_pos": 140, "type": "DATASET", "confidence": 0.9253284931182861}, {"text": "Model  Dev Test ACE MUC7  Discrete 91.0 85.4 77.4  73.4  SVD  90.8 85.7 77.3  73.7  SVD-S 91.0 85.5 77.6  74.3  SVD-L 90.5 84.8 73.6  71.5  HPCA 92.6 88.7 81.7  80.7  HSMN 90.5 85.7 78.7  74.7  CW  92.2 87.4 81.7  80.2  CBOW 93.1 88.2 82.2  81.1  GloVe  93.2 88.3 82.9  82.2", "start_pos": 180, "end_pos": 454, "type": "DATASET", "confidence": 0.8794057285785675}]}]}