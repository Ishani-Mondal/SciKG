{"title": [{"text": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation", "labels": [], "entities": [{"text": "Learning Phrase Representations", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.611458937327067}, {"text": "Statistical Machine Translation", "start_pos": 62, "end_pos": 93, "type": "TASK", "confidence": 0.7648998697598776}]}], "abstractContent": [{"text": "In this paper, we propose a novel neu-ral network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN).", "labels": [], "entities": []}, {"text": "One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols.", "labels": [], "entities": []}, {"text": "The encoder and de-coder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence.", "labels": [], "entities": []}, {"text": "The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 21, "end_pos": 52, "type": "TASK", "confidence": 0.6322462757428488}, {"text": "RNN Encoder-Decoder", "start_pos": 163, "end_pos": 182, "type": "DATASET", "confidence": 0.8682534694671631}]}, {"text": "Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.", "labels": [], "entities": []}], "introductionContent": [{"text": "Deep neural networks have shown great success in various applications such as objection recognition (see, e.g., () and speech recognition (see, e.g.,).", "labels": [], "entities": [{"text": "objection recognition", "start_pos": 78, "end_pos": 99, "type": "TASK", "confidence": 0.9201093018054962}, {"text": "speech recognition", "start_pos": 119, "end_pos": 137, "type": "TASK", "confidence": 0.8830190896987915}]}, {"text": "Furthermore, many recent works showed that neural networks can be successfully used in a number of tasks in natural language processing (NLP).", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 108, "end_pos": 141, "type": "TASK", "confidence": 0.7972141603628794}]}, {"text": "These include, but are not limited to, language modeling (, paraphrase detection) and word embedding extraction ().", "labels": [], "entities": [{"text": "language modeling", "start_pos": 39, "end_pos": 56, "type": "TASK", "confidence": 0.788103461265564}, {"text": "paraphrase detection", "start_pos": 60, "end_pos": 80, "type": "TASK", "confidence": 0.8309997022151947}, {"text": "word embedding extraction", "start_pos": 86, "end_pos": 111, "type": "TASK", "confidence": 0.7390365799268087}]}, {"text": "In the field of statistical machine translation (SMT), deep neural networks have begun to show promising results.) summarizes a successful usage of feedforward neural networks in the framework of phrase-based SMT system.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 16, "end_pos": 53, "type": "TASK", "confidence": 0.8099063336849213}, {"text": "SMT", "start_pos": 209, "end_pos": 212, "type": "TASK", "confidence": 0.7127104997634888}]}, {"text": "Along this line of research on using neural networks for SMT, this paper focuses on a novel neural network architecture that can be used as apart of the conventional phrase-based SMT system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 57, "end_pos": 60, "type": "TASK", "confidence": 0.993467390537262}, {"text": "SMT", "start_pos": 179, "end_pos": 182, "type": "TASK", "confidence": 0.8349936008453369}]}, {"text": "The proposed neural network architecture, which we will refer to as an RNN Encoder-Decoder, consists of two recurrent neural networks (RNN) that act as an encoder and a decoder pair.", "labels": [], "entities": []}, {"text": "The encoder maps a variable-length source sequence to a fixed-length vector, and the decoder maps the vector representation back to a variable-length target sequence.", "labels": [], "entities": []}, {"text": "The two networks are trained jointly to maximize the conditional probability of the target sequence given a source sequence.", "labels": [], "entities": []}, {"text": "Additionally, we propose to use a rather sophisticated hidden unit in order to improve both the memory capacity and the ease of training.", "labels": [], "entities": []}, {"text": "The proposed RNN Encoder-Decoder with a novel hidden unit is empirically evaluated on the task of translating from English to French.", "labels": [], "entities": [{"text": "translating from English to French", "start_pos": 98, "end_pos": 132, "type": "TASK", "confidence": 0.8584214329719544}]}, {"text": "We train the model to learn the translation probability of an English phrase to a corresponding French phrase.", "labels": [], "entities": []}, {"text": "The model is then used as apart of a standard phrase-based SMT system by scoring each phrase pair in the phrase table.", "labels": [], "entities": [{"text": "SMT", "start_pos": 59, "end_pos": 62, "type": "TASK", "confidence": 0.9046785235404968}]}, {"text": "The empirical evaluation reveals that this approach of scoring phrase pairs with an RNN Encoder-Decoder improves the translation performance.", "labels": [], "entities": []}, {"text": "We qualitatively analyze the trained RNN Encoder-Decoder by comparing its phrase scores with those given by the existing translation model.", "labels": [], "entities": []}, {"text": "The qualitative analysis shows that the RNN Encoder-Decoder is better at capturing the linguistic regularities in the phrase table, indirectly explaining the quantitative improvements in the overall translation performance.", "labels": [], "entities": []}, {"text": "The further analysis of the model reveals that the RNN EncoderDecoder learns a continuous space representation of a phrase that preserves both the semantic and syntactic structure of the phrase.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our approach on the English/French translation task of the WMT'14 workshop.", "labels": [], "entities": [{"text": "English/French translation task", "start_pos": 32, "end_pos": 63, "type": "TASK", "confidence": 0.6682577490806579}, {"text": "WMT'14 workshop", "start_pos": 71, "end_pos": 86, "type": "DATASET", "confidence": 0.7938508987426758}]}], "tableCaptions": [{"text": " Table 1: BLEU scores computed on the develop- ment and test sets using different combinations of  approaches. WP denotes a word penalty, where  we penalizes the number of unknown words to  neural networks.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9984012246131897}, {"text": "WP", "start_pos": 111, "end_pos": 113, "type": "METRIC", "confidence": 0.8970418572425842}]}]}