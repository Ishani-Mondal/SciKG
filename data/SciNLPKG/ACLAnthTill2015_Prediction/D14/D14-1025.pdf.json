{"title": [{"text": "Fitting Sentence Level Translation Evaluation with Many Dense Features", "labels": [], "entities": [{"text": "Fitting Sentence Level Translation Evaluation", "start_pos": 0, "end_pos": 45, "type": "TASK", "confidence": 0.7171327352523804}]}], "abstractContent": [{"text": "Sentence level evaluation in MT has turned out far more difficult than corpus level evaluation.", "labels": [], "entities": [{"text": "Sentence level evaluation", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.8997388283411661}, {"text": "MT", "start_pos": 29, "end_pos": 31, "type": "TASK", "confidence": 0.9721996188163757}]}, {"text": "Existing sentence level metrics employ a limited set of features, most of which are rather sparse at the sentence level, and their intricate models are rarely trained for ranking.", "labels": [], "entities": []}, {"text": "This paper presents a simple linear model exploiting 33 relatively dense features, some of which are novel while others are known but seldom used, and train it under the learning-to-rank framework.", "labels": [], "entities": []}, {"text": "We evaluate our metric on the standard WMT12 data showing that it outperforms the strong baseline METEOR.", "labels": [], "entities": [{"text": "WMT12 data", "start_pos": 39, "end_pos": 49, "type": "DATASET", "confidence": 0.9415209591388702}, {"text": "METEOR", "start_pos": 98, "end_pos": 104, "type": "METRIC", "confidence": 0.8936846256256104}]}, {"text": "We also analyze the contribution of individual features and the choice of training data, language-pair vs. target-language data, providing new insights into this task.", "labels": [], "entities": []}], "introductionContent": [{"text": "Evaluating machine translation (MT) output at the sentence/ segment level has turned out far more challenging than corpus/ system level.", "labels": [], "entities": [{"text": "Evaluating machine translation (MT) output", "start_pos": 0, "end_pos": 42, "type": "TASK", "confidence": 0.8725145884922573}]}, {"text": "Yet, sentence level evaluation can be useful because it allows fast, finegrained analysis of system performance on individual sentences.", "labels": [], "entities": [{"text": "sentence level evaluation", "start_pos": 5, "end_pos": 30, "type": "TASK", "confidence": 0.6489791770776113}]}, {"text": "It is instructive to contrast two widely used metrics, METEOR) and BLEU (), on sentence level evaluation.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 55, "end_pos": 61, "type": "METRIC", "confidence": 0.9833106994628906}, {"text": "BLEU", "start_pos": 67, "end_pos": 71, "type": "METRIC", "confidence": 0.9983757734298706}]}, {"text": "METEOR constantly shows better correlation with human ranking than BLEU ().", "labels": [], "entities": [{"text": "METEOR", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.8080394268035889}, {"text": "BLEU", "start_pos": 67, "end_pos": 71, "type": "METRIC", "confidence": 0.9989198446273804}]}, {"text": "Arguably, this shows that sentence level evaluation demands finer grained and trainable models over less sparse features.", "labels": [], "entities": [{"text": "sentence level evaluation", "start_pos": 26, "end_pos": 51, "type": "TASK", "confidence": 0.6312214831511179}]}, {"text": "Ngrams, the core of BLEU, are sparse at the sentence level, and a mismatch for longer ngrams implies that BLEU falls back on shorter ngrams.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 20, "end_pos": 24, "type": "METRIC", "confidence": 0.9739298820495605}, {"text": "BLEU", "start_pos": 106, "end_pos": 110, "type": "METRIC", "confidence": 0.994602620601654}]}, {"text": "In contrast, METEOR has a trainable model and incorporates a small, yet wider set of features that are less sparse than ngrams.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 13, "end_pos": 19, "type": "DATASET", "confidence": 0.7265536189079285}]}, {"text": "We think that METEOR's features and its training approach only suggest that sentence level evaluation should be treated as a modelling challenge.", "labels": [], "entities": [{"text": "sentence level evaluation", "start_pos": 76, "end_pos": 101, "type": "TASK", "confidence": 0.6494675874710083}]}, {"text": "This calls for questions such as what model, what features and what training objective are better suited for modelling sentence level evaluation.", "labels": [], "entities": [{"text": "modelling sentence level evaluation", "start_pos": 109, "end_pos": 144, "type": "TASK", "confidence": 0.7536454796791077}]}, {"text": "We start out by explicitly formulating sentence level evaluation as the problem of ranking a set of competing hypothesis.", "labels": [], "entities": []}, {"text": "Given data consisting of human ranked system outputs, the problem then is to formulate an easy to train model for ranking.", "labels": [], "entities": []}, {"text": "One particular existing approach) looks especially attractive because we think it meshes well with a range of effective techniques for learning-to-rank (Li, 2011).", "labels": [], "entities": []}, {"text": "We deliberately select a linear modelling approach inspired by), which is easily trainable for ranking and allows analysis of the individual contributions of features.", "labels": [], "entities": []}, {"text": "Besides presenting anew metric and a set of known, but also a set of novel features, we target three questions of interest to the MT community: \u2022 What kind of features are more helpful for sentence level evaluation?", "labels": [], "entities": [{"text": "MT", "start_pos": 130, "end_pos": 132, "type": "TASK", "confidence": 0.9728930592536926}, {"text": "sentence level evaluation", "start_pos": 189, "end_pos": 214, "type": "TASK", "confidence": 0.6185837090015411}]}, {"text": "\u2022 How does a simple linear model trained for ranking compare to the well-developed metric ME-TEOR on sentence level evaluation?", "labels": [], "entities": []}, {"text": "\u2022 Should we train the model for each language pair separately or fora target language?", "labels": [], "entities": []}, {"text": "Our new metric dubbed BEER 1 outperforms ME-TEOR on WMT12 data showing the effectiveness of dense features in a learning-to-rank framework.", "labels": [], "entities": [{"text": "BEER 1", "start_pos": 22, "end_pos": 28, "type": "METRIC", "confidence": 0.9818296134471893}, {"text": "ME-TEOR", "start_pos": 41, "end_pos": 48, "type": "METRIC", "confidence": 0.9244347214698792}, {"text": "WMT12 data", "start_pos": 52, "end_pos": 62, "type": "DATASET", "confidence": 0.9247286319732666}]}, {"text": "The metric and the code are available as free software 2 .", "labels": [], "entities": []}], "datasetContent": [{"text": "We use human judgments from the WMT tasks: WMT13 is used for training whereas WMT12 for testing.", "labels": [], "entities": [{"text": "WMT tasks", "start_pos": 32, "end_pos": 41, "type": "TASK", "confidence": 0.716877818107605}, {"text": "WMT13", "start_pos": 43, "end_pos": 48, "type": "DATASET", "confidence": 0.7316381931304932}, {"text": "WMT12", "start_pos": 78, "end_pos": 83, "type": "DATASET", "confidence": 0.8224518299102783}]}, {"text": "The baseline is METEOR's latest version), one of the best metrics on sentence level.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 16, "end_pos": 22, "type": "METRIC", "confidence": 0.5979997515678406}]}, {"text": "To avoid contaminating the results with differences with METEOR due to resources, we use the same alignment, tokenization and lowercasing (-norm in METEOR) algorithms, and the same tables of function words, synonyms, paraphrases and stemmers.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 57, "end_pos": 63, "type": "DATASET", "confidence": 0.8195499777793884}]}, {"text": "Kendall \u03c4 correlation is borrowed from WMT12 (: #concordant represents the number of pairs ordered in the same way by metric and by human, #discordant the number of opposite orderings and #ties the number of tied rankings by metric.", "labels": [], "entities": [{"text": "Kendall \u03c4 correlation", "start_pos": 0, "end_pos": 21, "type": "METRIC", "confidence": 0.6611349582672119}, {"text": "WMT12", "start_pos": 39, "end_pos": 44, "type": "DATASET", "confidence": 0.8706813454627991}]}, {"text": "Beside testing our full metric BEER, we perform experiments where we remove one kind of the following features at a time: 1.", "labels": [], "entities": [{"text": "BEER", "start_pos": 31, "end_pos": 35, "type": "METRIC", "confidence": 0.8499682545661926}]}, {"text": "char n-gram features (P, Rand F-score) 2.", "labels": [], "entities": [{"text": "Rand F-score)", "start_pos": 25, "end_pos": 38, "type": "METRIC", "confidence": 0.8401039640108744}]}, {"text": "all word features (P, Rand F-score for all, function and content words), 3.", "labels": [], "entities": [{"text": "Rand F-score", "start_pos": 22, "end_pos": 34, "type": "METRIC", "confidence": 0.9343474507331848}]}, {"text": "all function and content words features shows the results sorted by their average Kendall \u03c4 correlation with human judgment.", "labels": [], "entities": [{"text": "Kendall \u03c4 correlation", "start_pos": 82, "end_pos": 103, "type": "METRIC", "confidence": 0.7336461742719015}]}], "tableCaptions": [{"text": " Table 1: Kendall \u03c4 scores on WMT12 data", "labels": [], "entities": [{"text": "WMT12 data", "start_pos": 30, "end_pos": 40, "type": "DATASET", "confidence": 0.9630765318870544}]}, {"text": " Table  2. Training for each language pair separately does not  give significant improvement over training for the tar- get language only. A possible reason could be that by  training for the target language we have more training  data (in this case four times more).", "labels": [], "entities": []}, {"text": " Table 2: Kendall \u03c4 scores on WMT12 for different  training data", "labels": [], "entities": [{"text": "WMT12", "start_pos": 30, "end_pos": 35, "type": "DATASET", "confidence": 0.8924223184585571}]}]}