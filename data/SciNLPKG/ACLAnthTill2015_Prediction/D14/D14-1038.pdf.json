{"title": [{"text": "ReNoun: Fact Extraction for Nominal Attributes", "labels": [], "entities": [{"text": "Fact Extraction", "start_pos": 8, "end_pos": 23, "type": "TASK", "confidence": 0.701220840215683}]}], "abstractContent": [{"text": "Search engines are increasingly relying on large knowledge bases of facts to provide direct answers to users' queries.", "labels": [], "entities": []}, {"text": "However , the construction of these knowledge bases is largely manual and does not scale to the long and heavy tail of facts.", "labels": [], "entities": []}, {"text": "Open information extraction tries to address this challenge, but typically assumes that facts are expressed with verb phrases, and therefore has had difficulty extracting facts for noun-based relations.", "labels": [], "entities": [{"text": "Open information extraction", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.6437183022499084}]}, {"text": "We describe ReNoun, an open information extraction system that complements previous efforts by focusing on nominal attributes and on the long tail.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 28, "end_pos": 50, "type": "TASK", "confidence": 0.7495790421962738}]}, {"text": "ReNoun's approach is based on leveraging a large on-tology of noun attributes mined from a text corpus and from user queries.", "labels": [], "entities": []}, {"text": "ReNoun creates a seed set of training data by using specialized patterns and requiring that the facts mention an attribute in the ontol-ogy.", "labels": [], "entities": []}, {"text": "ReNoun then generalizes from this seed set to produce a much larger set of extractions that are then scored.", "labels": [], "entities": []}, {"text": "We describe experiments that show that we extract facts with high precision and for attributes that cannot be extracted with verb-based techniques .", "labels": [], "entities": [{"text": "precision", "start_pos": 66, "end_pos": 75, "type": "METRIC", "confidence": 0.9848942160606384}]}], "introductionContent": [{"text": "One of the major themes driving the current evolution of search engines is to make the search experience more efficient and mobile friendly for users by providing them concrete answers to queries.", "labels": [], "entities": []}, {"text": "These answers, that apply to queries about entities that the search engine knows about (e.g., famous individuals, organizations or locations) complement the links that the search en- * Work done during an internship at Google Research.", "labels": [], "entities": []}, {"text": "To support such answers, the search engine maintains a knowledge base that describes various attributes of an entity (e.g.,).", "labels": [], "entities": []}, {"text": "Upon receiving a query, the search engine tries to recognize whether the answer is in its knowledge base.", "labels": [], "entities": []}, {"text": "For the most part, the aforementioned knowledge bases are constructed using manual techniques and carefully supervised information extraction algorithms.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 119, "end_pos": 141, "type": "TASK", "confidence": 0.7199392914772034}]}, {"text": "As a result, they obtain high coverage on head attributes, but low coverage on tail ones, such as those shown in.", "labels": [], "entities": [{"text": "coverage", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9547486901283264}]}, {"text": "For example, they may have the answer for the query \"Sarkozy's wife\", but not for \"Hollande's exgirlfriend\" or \"Google's philanthropic arm\".", "labels": [], "entities": []}, {"text": "In addition to broadening the scope of query answering, extending the coverage of the knowledge base to long tail attributes can also facilitate providing Web answers to the user.", "labels": [], "entities": [{"text": "query answering", "start_pos": 39, "end_pos": 54, "type": "TASK", "confidence": 0.7233202904462814}]}, {"text": "Specifically, the search engine can use lower-confidence facts to corroborate an answer that appears in text in one of the top Web results and highlight them to the user.", "labels": [], "entities": []}, {"text": "This paper describes ReNoun, an openinformation extraction system that focuses on extracting facts for long tail attributes.", "labels": [], "entities": [{"text": "openinformation extraction", "start_pos": 32, "end_pos": 58, "type": "TASK", "confidence": 0.7304679155349731}]}, {"text": "The observation underlying our approach is that attributes from the long tail are typically expressed as nouns, whereas most previous work on open-information extraction (e.g.,) extend techniques for extracting attributes expressed in verb form.", "labels": [], "entities": [{"text": "open-information extraction", "start_pos": 142, "end_pos": 169, "type": "TASK", "confidence": 0.7258968502283096}]}, {"text": "Hence, the main contribution of our work is to develop an extraction system that complements previous efforts, focuses on nominal attributes and is effective for the long tail.", "labels": [], "entities": []}, {"text": "To that end, ReNoun begins with a large but imperfect ontology of nominal attributes that is extracted from text and the query stream (.", "labels": [], "entities": []}, {"text": "ReNoun proceeds by using a small set of highprecision extractors that exploit the nominal na- Yorker's best staff writers.: Examples of noun phrases as attributes, none which are part of a verb phrase.", "labels": [], "entities": []}, {"text": "Additionally, the first two attributes do not occur within a verb phrase in a large corpus (see \u00a7 2 for details) in a setting where they can be associated with a triple.", "labels": [], "entities": []}, {"text": "ture of the attributes to obtain a training set, and then generalizes from the training set via distant supervision to find a much larger set of extraction patterns.", "labels": [], "entities": []}, {"text": "Finally, ReNoun scores extracted facts by considering how frequently their patterns extract triples and the coherence of these patterns, i.e., whether they extract triples for semantically similar attributes.", "labels": [], "entities": []}, {"text": "Our experiments demonstrate that ReNoun extracts a large body of high precision facts, and that these facts are not extracted with techniques based on verb phrases.", "labels": [], "entities": []}], "datasetContent": [{"text": "We describe a set of experiments that validate the contributions of ReNoun.", "labels": [], "entities": []}, {"text": "In Sections 7.2 and 7.3 we validate our noun-centric approach: we show that extractions based on verb phrases cannot yield the results of ReNoun and that NomBank, the resource used by state of the art in semantic rolelabeling for nouns, will not suffice either.", "labels": [], "entities": []}, {"text": "In Sections 7.4-7.6 we evaluate the different components of ReNoun and its overall quality, and in Section 7.7 we discuss the cases in which ReNoun was unable to extract any facts.", "labels": [], "entities": [{"text": "ReNoun", "start_pos": 60, "end_pos": 66, "type": "DATASET", "confidence": 0.9093108177185059}]}], "tableCaptions": [{"text": " Table 6. The scheme FREQ is identical  to FREQ COH except that all coherences are set  to 1. PATTERN counts the number of distinct pat- terns that extract the fact while PATTERN COH  sums the pattern coherences. We generated a ran- dom sample of 252 FH and LT nouns with no en- tity disambiguation errors by the underlying nat- ural language processing pipeline. The justifi- cation is that none of the schemes we consider  here capture such errors. Accounting for such  errors requires elaborate signals from the entity  linking system, which we leave for future work.  For each scoring scheme, we computed the Spear- man's rank correlation coefficient \u03c1 between the  scores and manual judgments (by three judges). A  larger \u03c1 indicates more correlation, and comput- ing \u03c1 was statistically significant (p-value<0.01)  for all schemes.", "labels": [], "entities": [{"text": "FREQ", "start_pos": 21, "end_pos": 25, "type": "METRIC", "confidence": 0.9858076572418213}, {"text": "PATTERN", "start_pos": 94, "end_pos": 101, "type": "METRIC", "confidence": 0.9927494525909424}, {"text": "Spear- man's rank correlation coefficient \u03c1", "start_pos": 613, "end_pos": 656, "type": "METRIC", "confidence": 0.6121054627001286}]}, {"text": " Table 8: Analysis of attributes with no extractions.", "labels": [], "entities": []}]}