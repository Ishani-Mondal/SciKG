{"title": [{"text": "Analysing recall loss in named entity slot filling", "labels": [], "entities": [{"text": "recall loss", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9571953117847443}]}], "abstractContent": [{"text": "State-of-the-art fact extraction is heavily constrained by recall, as demonstrated by recent performance in TAC Slot Filling.", "labels": [], "entities": [{"text": "fact extraction", "start_pos": 17, "end_pos": 32, "type": "TASK", "confidence": 0.6930646002292633}, {"text": "recall", "start_pos": 59, "end_pos": 65, "type": "METRIC", "confidence": 0.9974048733711243}, {"text": "TAC Slot Filling", "start_pos": 108, "end_pos": 124, "type": "TASK", "confidence": 0.6345521807670593}]}, {"text": "We isolate this recall loss for NE slots by systematically analysing each stage of the slot filling pipeline as a filter over correct answers.", "labels": [], "entities": [{"text": "recall", "start_pos": 16, "end_pos": 22, "type": "METRIC", "confidence": 0.9986109733581543}]}, {"text": "Recall is critical as candidates never generated can never be recovered, whereas precision can always be increased in downstream processing.", "labels": [], "entities": [{"text": "Recall", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9860479831695557}, {"text": "precision", "start_pos": 81, "end_pos": 90, "type": "METRIC", "confidence": 0.9991123080253601}]}, {"text": "We provide precise, empirical confirmation of previously hypothesised sources of recall loss in slot filling.", "labels": [], "entities": [{"text": "recall loss", "start_pos": 81, "end_pos": 92, "type": "METRIC", "confidence": 0.9619164168834686}, {"text": "slot filling", "start_pos": 96, "end_pos": 108, "type": "TASK", "confidence": 0.8525658845901489}]}, {"text": "While NE type constraints substantially reduce the search space with only a minor recall penalty, we find that 10% to 39% of slot fills will be entirely ignored by most systems.", "labels": [], "entities": [{"text": "recall", "start_pos": 82, "end_pos": 88, "type": "METRIC", "confidence": 0.9990043044090271}]}, {"text": "One in six correct answers are lost if coreference is not used, but this can be mostly retained by simple name matching rules.", "labels": [], "entities": [{"text": "name matching", "start_pos": 106, "end_pos": 119, "type": "TASK", "confidence": 0.7061170786619186}]}], "introductionContent": [{"text": "The TAC Knowledge Base Population (KBP) Slot Filling (SF) consists of extracting named attributes from text.", "labels": [], "entities": [{"text": "TAC Knowledge Base Population (KBP) Slot Filling (SF)", "start_pos": 4, "end_pos": 57, "type": "TASK", "confidence": 0.7424590537945429}]}, {"text": "Given a query, e.g. John Kerry, a system searches a corpus for documents which contain the entity.", "labels": [], "entities": []}, {"text": "It then fills a list of slots, named attributes such as (per:spouse, Teresa Heinz).", "labels": [], "entities": []}, {"text": "The top TAC SF 2013 (TAC13) system scored 37.3% F-score (, and the median F-score was 16.9%.", "labels": [], "entities": [{"text": "TAC SF 2013 (TAC13) system", "start_pos": 8, "end_pos": 34, "type": "DATASET", "confidence": 0.883766600063869}, {"text": "F-score", "start_pos": 48, "end_pos": 55, "type": "METRIC", "confidence": 0.9993755221366882}, {"text": "F-score", "start_pos": 74, "end_pos": 81, "type": "METRIC", "confidence": 0.9874702095985413}]}, {"text": "Recall for SF systems is especially low, with many systems using precise extractors with low recall.", "labels": [], "entities": [{"text": "Recall", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9943705201148987}, {"text": "SF", "start_pos": 11, "end_pos": 13, "type": "TASK", "confidence": 0.9650278687477112}, {"text": "recall", "start_pos": 93, "end_pos": 99, "type": "METRIC", "confidence": 0.9979619979858398}]}, {"text": "Precision ranges from 9% to 40% greater than recall for the top 5 systems in TAC13, and unsurprisingly, has the highest recall at 33%.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9965067505836487}, {"text": "recall", "start_pos": 45, "end_pos": 51, "type": "METRIC", "confidence": 0.9995999932289124}, {"text": "TAC13", "start_pos": 77, "end_pos": 82, "type": "DATASET", "confidence": 0.8004634380340576}, {"text": "recall", "start_pos": 120, "end_pos": 126, "type": "METRIC", "confidence": 0.9993007183074951}]}, {"text": "Closing the recall gap without substantially increasing the search space is critical to improving SF results.  and identify many of the challenges of SF, and suggest that inference, coreference and named entity recognition (NER) are key sources of error.", "labels": [], "entities": [{"text": "recall", "start_pos": 12, "end_pos": 18, "type": "METRIC", "confidence": 0.9877809286117554}, {"text": "SF", "start_pos": 98, "end_pos": 100, "type": "TASK", "confidence": 0.9137964248657227}, {"text": "SF", "start_pos": 150, "end_pos": 152, "type": "TASK", "confidence": 0.9097875952720642}, {"text": "named entity recognition (NER)", "start_pos": 198, "end_pos": 228, "type": "TASK", "confidence": 0.7843333383401235}]}, {"text": "Min and Grishman categorise the slot fills found by human annotators but not found in the aggregated output of all systems.", "labels": [], "entities": []}, {"text": "However, this approach only allows them to hypothesise the likely source of recall loss.", "labels": [], "entities": [{"text": "recall", "start_pos": 76, "end_pos": 82, "type": "METRIC", "confidence": 0.9849488139152527}]}, {"text": "For instance, it is impossible to distinguish candidate generation errors from answer merging errors.", "labels": [], "entities": [{"text": "answer merging", "start_pos": 79, "end_pos": 93, "type": "TASK", "confidence": 0.741022139787674}]}, {"text": "categorise these errors at a high level, without specific analysis of candidate generation pipeline components such as coreference.", "labels": [], "entities": []}, {"text": "In this paper, we take this analysis further by performing a systematic recall analysis that allows us to pinpoint the cause of every recall error (candidates lost that can never be recovered) and estimate upper bounds on recall in existing approaches.", "labels": [], "entities": [{"text": "recall", "start_pos": 72, "end_pos": 78, "type": "METRIC", "confidence": 0.9892340898513794}, {"text": "recall error", "start_pos": 134, "end_pos": 146, "type": "METRIC", "confidence": 0.962925910949707}, {"text": "recall", "start_pos": 222, "end_pos": 228, "type": "METRIC", "confidence": 0.9962791800498962}]}, {"text": "We implement a collection of na\u00a8\u0131vena\u00a8\u0131ve SF systems utilizing a set of increasingly restrictive filters over documents and named entities (NEs).", "labels": [], "entities": []}, {"text": "TAC has three slot types: NE, string and value slots.", "labels": [], "entities": [{"text": "TAC", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8241417407989502}]}, {"text": "We consider only those slots filled by NEs as there are widely-used, high accuracy tools available for NER, and focusing on NEs only allows us to precisely gauge performance of filters.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.9758056402206421}]}, {"text": "String slots do not have reliable classifiers, and value slots require more normalisation than directly returning a token span.", "labels": [], "entities": []}, {"text": "Otherwise, this evaluation is not specifically dependent on the nature of NEs, and we expect similar results for other slot types.", "labels": [], "entities": []}, {"text": "We focus on systems which first generate candidates and then process them, the approach of the majority of TAC systems.", "labels": [], "entities": []}, {"text": "Our filters apply hard constraints over NEs commonly used in the literature, accounting fora typical SF candidate generation pipeline-matching the query term, the form of candidate fills and the distance between the query and the candidate-but not performing any further scoring or thresholding.", "labels": [], "entities": []}, {"text": "We compare sev-eral forms of coreference as filters, motivated by the need for efficient coreference resolution when processing large corpora.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 89, "end_pos": 111, "type": "TASK", "confidence": 0.8380465805530548}]}, {"text": "Complementing these unsupervised experiments, we implement a maximum recall bootstrap to identify which fills are reachable from training data.", "labels": [], "entities": [{"text": "recall", "start_pos": 69, "end_pos": 75, "type": "METRIC", "confidence": 0.9698745608329773}]}, {"text": "We find \u223c10% of recall is ignored by most systems due to NER bounds errors, and despite stateof-the-art coreference, 8% is lost when queries and fills occur in different sentences.", "labels": [], "entities": [{"text": "recall", "start_pos": 16, "end_pos": 22, "type": "METRIC", "confidence": 0.9985033273696899}]}, {"text": "Using NE type constraints is very effective, reducing recall by only 2% fora search space reduction of 81%.", "labels": [], "entities": [{"text": "recall", "start_pos": 54, "end_pos": 60, "type": "METRIC", "confidence": 0.9991395473480225}]}, {"text": "Without any coreference, 16% of typed fills are lost, but 12% of this recall can be recovered using fast na\u00a8\u0131vena\u00a8\u0131ve name matching rules, reducing the search space to 59% that of full coreference.", "labels": [], "entities": [{"text": "recall", "start_pos": 70, "end_pos": 76, "type": "METRIC", "confidence": 0.9932954907417297}, {"text": "name matching", "start_pos": 118, "end_pos": 131, "type": "TASK", "confidence": 0.6834892928600311}]}, {"text": "15% of recall is lost if a SF approach, such as a bootstrapping, requires that dependency paths be nonunique in a corpus.", "labels": [], "entities": [{"text": "recall", "start_pos": 7, "end_pos": 13, "type": "METRIC", "confidence": 0.998944103717804}]}, {"text": "We show that most remaining candidates are reachable via bootstrapping from a small number of seeds.", "labels": [], "entities": []}, {"text": "Our results provide systematic confirmation that effective coreference and NER are critical to high recall slot filling.", "labels": [], "entities": [{"text": "coreference", "start_pos": 59, "end_pos": 70, "type": "TASK", "confidence": 0.9677767157554626}, {"text": "NER", "start_pos": 75, "end_pos": 78, "type": "TASK", "confidence": 0.7929574251174927}, {"text": "recall slot filling", "start_pos": 100, "end_pos": 119, "type": "TASK", "confidence": 0.7738110522429148}]}], "datasetContent": [{"text": "We begin with a set of queries (a query being a NE entity grounded in a mention in a document) and, for each query q, the documents D q known to contain any slot fill for q, as determined by oracle information retrieval (IR) from human annotation and judged system output.", "labels": [], "entities": []}, {"text": "Filling every slot in q with every n-gram in D q constitutes a system with nearly perfect recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 90, "end_pos": 96, "type": "METRIC", "confidence": 0.9985612034797668}]}, {"text": "We apply a series of increasingly restrictive filters over this set.", "labels": [], "entities": []}, {"text": "As in, SF systems in practice must retrieve relevant documents and generate candidates.", "labels": [], "entities": [{"text": "SF", "start_pos": 7, "end_pos": 9, "type": "TASK", "confidence": 0.9726129770278931}]}, {"text": "We propose filters that allow for analysis of recall lost during these stages.", "labels": [], "entities": [{"text": "recall", "start_pos": 46, "end_pos": 52, "type": "METRIC", "confidence": 0.9811192750930786}]}, {"text": "We ignore the remaining stages and evaluate the set of candidates directly.", "labels": [], "entities": []}, {"text": "Filters define what documents or NEs are allowed to pass through, based on constraints imposed by query matching, entity form, and sentence and syntactic context.", "labels": [], "entities": []}, {"text": "We combine these filters in series in a number of configurations.", "labels": [], "entities": []}, {"text": "The use or absence of coreference varies across our configurations, as the need to identify the query mention and terms that refer to the query mention is critical.", "labels": [], "entities": []}, {"text": "Finally, we experiment with a boot-strapping training process, to reflect constraints implicitly applied by a training approach.", "labels": [], "entities": []}, {"text": "The SF typical system pipeline presented in Section 3 applies to most, but not all SF approaches.", "labels": [], "entities": []}, {"text": "The following filters directly apply only to systems that use NER as the method of candidate generation, and where candidate generation is distinct from answer extraction.", "labels": [], "entities": [{"text": "candidate generation", "start_pos": 83, "end_pos": 103, "type": "TASK", "confidence": 0.7131940871477127}, {"text": "answer extraction", "start_pos": 153, "end_pos": 170, "type": "TASK", "confidence": 0.7917984426021576}]}, {"text": "Fourteen of the eighteen teams participating in TAC13 submitted system reports (.", "labels": [], "entities": [{"text": "TAC13", "start_pos": 48, "end_pos": 53, "type": "DATASET", "confidence": 0.6268635392189026}]}, {"text": "Eleven of these systems identify NEs with NER and pass these to an answer extraction process.", "labels": [], "entities": [{"text": "NEs with NER", "start_pos": 33, "end_pos": 45, "type": "TASK", "confidence": 0.6852759520212809}, {"text": "answer extraction", "start_pos": 67, "end_pos": 84, "type": "TASK", "confidence": 0.7807465195655823}]}, {"text": "The remaining three systems either do not document whether they rely on or do not rely on NER for candidate generation for name slots.", "labels": [], "entities": []}, {"text": "We include a high recall baseline based on noun phrases (NPs) to cover these systems.", "labels": [], "entities": [{"text": "recall", "start_pos": 18, "end_pos": 24, "type": "METRIC", "confidence": 0.9977618455886841}]}, {"text": "We evaluate our filters on the TAC KBP English Slot Filling 2011 corpus, queries and task specification.", "labels": [], "entities": [{"text": "TAC KBP English Slot Filling 2011 corpus", "start_pos": 31, "end_pos": 71, "type": "DATASET", "confidence": 0.9168833579335894}]}, {"text": "As we aim to determine recall upper bounds and recall loss, we use only the documents D from the TAC KBP Source Data that are known to contain at least one correct slot fill in the TAC KBP 2011 English Slot Filling Assessment Results (LDC, 2011).", "labels": [], "entities": [{"text": "recall upper bounds", "start_pos": 23, "end_pos": 42, "type": "METRIC", "confidence": 0.9628018538157145}, {"text": "recall loss", "start_pos": 47, "end_pos": 58, "type": "METRIC", "confidence": 0.9674853682518005}, {"text": "TAC KBP Source Data", "start_pos": 97, "end_pos": 116, "type": "DATASET", "confidence": 0.9538314193487167}, {"text": "TAC KBP 2011 English Slot Filling Assessment Results (LDC, 2011)", "start_pos": 181, "end_pos": 245, "type": "DATASET", "confidence": 0.8600231317373422}]}, {"text": "We restrict the assessment results and the evaluation process to all slot types that are filled by name content types as opposed to value or string.", "labels": [], "entities": []}, {"text": "We also do not evaluate the per:alternate names or org:alternate names slots, as extraction of fills for these slots typically falls outside the RE task: while X also known as Y or similar may appear in text, X and Y are typically mentioned independently across documents.", "labels": [], "entities": []}, {"text": "There are 100 TAC11 queries, 50 PER and 50 ORG.", "labels": [], "entities": [{"text": "TAC11", "start_pos": 14, "end_pos": 19, "type": "METRIC", "confidence": 0.8568524122238159}, {"text": "PER", "start_pos": 32, "end_pos": 35, "type": "METRIC", "confidence": 0.9984098672866821}, {"text": "ORG", "start_pos": 43, "end_pos": 46, "type": "METRIC", "confidence": 0.9963891506195068}]}, {"text": "There are 535 fills in our reduced evaluation, 1,171 correct responses over these fills: 56% of the original evaluation slots.", "labels": [], "entities": []}, {"text": "The distribution of fills per slot is listed in), motivated by efficiency reasons, as the full CoreNLP requires parsing and a more complex model.", "labels": [], "entities": [{"text": "CoreNLP", "start_pos": 95, "end_pos": 102, "type": "DATASET", "confidence": 0.8727976083755493}, {"text": "parsing", "start_pos": 112, "end_pos": 119, "type": "TASK", "confidence": 0.9608849287033081}]}, {"text": "The rules do not require deep processing and can run quickly overlarge volumes of text.", "labels": [], "entities": []}, {"text": "All NEs from a document are matched by processing in decreasing length order.", "labels": [], "entities": []}, {"text": "Two names are marked coreferent where, ignoring titles and case: they match exactly; they have a matching final word; they have a matching initial word; or one is an acronym of the other.", "labels": [], "entities": []}, {"text": "If multiple conditions are matched, the earliest (the most strict match) is used.", "labels": [], "entities": []}, {"text": "The NON-UNIQUE filter requires that a dependency path occurs more than once between NEs in the full TAC KBP Source Data, comprised of 1.8M documents and 318M NE pairs.", "labels": [], "entities": [{"text": "TAC KBP Source Data", "start_pos": 100, "end_pos": 119, "type": "DATASET", "confidence": 0.8604938387870789}]}, {"text": "There are 38.6M distinct lemmatised dependency paths, 5M of which occur more than once.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Number of fills for slots in the evaluation.", "labels": [], "entities": [{"text": "Number", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9724389910697937}]}, {"text": " Table 2: Results on D given sets of filters config- urations. The ellipses indicate the previous line.", "labels": [], "entities": []}, {"text": " Table 3: Error types for COREF and NOCOREF.", "labels": [], "entities": [{"text": "Error", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9803426861763}, {"text": "COREF", "start_pos": 26, "end_pos": 31, "type": "METRIC", "confidence": 0.45614853501319885}, {"text": "NOCOREF", "start_pos": 36, "end_pos": 43, "type": "DATASET", "confidence": 0.5716831684112549}]}]}