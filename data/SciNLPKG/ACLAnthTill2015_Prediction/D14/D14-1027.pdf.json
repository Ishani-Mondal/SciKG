{"title": [{"text": "Learning to Differentiate Better from Worse Translations", "labels": [], "entities": [{"text": "Learning to Differentiate Better from Worse Translations", "start_pos": 0, "end_pos": 56, "type": "TASK", "confidence": 0.7057050211088998}]}], "abstractContent": [{"text": "We present a pairwise learning-to-rank approach to machine translation evaluation that learns to differentiate better from worse translations in the context of a given reference.", "labels": [], "entities": [{"text": "machine translation evaluation", "start_pos": 51, "end_pos": 81, "type": "TASK", "confidence": 0.8518340984980265}]}, {"text": "We integrate several layers of linguistic information encapsulated in tree-based structures, making use of both the reference and the system output simultaneously , thus bringing our ranking closer to how humans evaluate translations.", "labels": [], "entities": []}, {"text": "Most importantly, instead of deciding upfront which types of features are important, we use the learning framework of preference re-ranking kernels to learn the features automatically.", "labels": [], "entities": []}, {"text": "The evaluation results show that learning in the proposed framework yields better correlation with humans than computing the direct similarity over the same type of structures.", "labels": [], "entities": []}, {"text": "Also, we show our structural kernel learning (SKL) can be a general framework for MT evaluation, in which syntactic and semantic information can be naturally incorporated.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 82, "end_pos": 95, "type": "TASK", "confidence": 0.9688029289245605}]}], "introductionContent": [{"text": "We have seen in recent years fast improvement in the overall quality of machine translation (MT) systems.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 72, "end_pos": 96, "type": "TASK", "confidence": 0.8503003001213074}]}, {"text": "This was only possible because of the use of automatic metrics for MT evaluation, such as BLEU (), which is the defacto standard; and more recently: TER () and METEOR (), among other emerging MT evaluation metrics.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 67, "end_pos": 80, "type": "TASK", "confidence": 0.9525507688522339}, {"text": "BLEU", "start_pos": 90, "end_pos": 94, "type": "METRIC", "confidence": 0.998609185218811}, {"text": "TER", "start_pos": 149, "end_pos": 152, "type": "METRIC", "confidence": 0.9909026026725769}, {"text": "METEOR", "start_pos": 160, "end_pos": 166, "type": "METRIC", "confidence": 0.9157590866088867}, {"text": "MT evaluation", "start_pos": 192, "end_pos": 205, "type": "TASK", "confidence": 0.9057580232620239}]}, {"text": "These automatic metrics provide fast and inexpensive means to compare the output of different MT systems, without the need to ask for human judgments each time the MT system has been changed.", "labels": [], "entities": [{"text": "MT", "start_pos": 94, "end_pos": 96, "type": "TASK", "confidence": 0.9765514135360718}]}, {"text": "As a result, this has enabled rapid development in the field of statistical machine translation (SMT), by allowing to train and tune systems as well as to track progress in away that highly correlates with human judgments.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 64, "end_pos": 101, "type": "TASK", "confidence": 0.8225679894288381}]}, {"text": "Today, MT evaluation is an active field of research, and modern metrics perform analysis at various levels, e.g., lexical), including synonymy and paraphrasing; syntactic (); semantic (; and discourse).", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 7, "end_pos": 20, "type": "TASK", "confidence": 0.9860257208347321}]}, {"text": "Automatic MT evaluation metrics compare the output of a system to one or more human references in order to produce a similarity score.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 10, "end_pos": 23, "type": "TASK", "confidence": 0.9617006480693817}, {"text": "similarity score", "start_pos": 117, "end_pos": 133, "type": "METRIC", "confidence": 0.9590629637241364}]}, {"text": "The quality of such a metric is typically judged in terms of correlation of the scores it produces with scores given by human judges.", "labels": [], "entities": []}, {"text": "As a result, some evaluation metrics have been trained to reproduce the scores assigned by humans as closely as possible.", "labels": [], "entities": []}, {"text": "Unfortunately, humans have a hard time assigning an absolute score to a translation.", "labels": [], "entities": []}, {"text": "Hence, direct human evaluation scores such as adequacy and fluency, which were widely used in the past, are now discontinued in favor of ranking-based evaluations, where judges are asked to rank the output of 2 to 5 systems instead.", "labels": [], "entities": []}, {"text": "It has been shown that using such ranking-based assessments yields much higher inter-annotator agreement.", "labels": [], "entities": []}, {"text": "While evaluation metrics still produce numerical scores, in part because MT evaluation shared tasks at NIST and WMT ask for it, there has also been work on a ranking formulation of the MT evaluation task fora given set of outputs.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 73, "end_pos": 86, "type": "TASK", "confidence": 0.9123652577400208}, {"text": "NIST", "start_pos": 103, "end_pos": 107, "type": "DATASET", "confidence": 0.9453831911087036}, {"text": "WMT", "start_pos": 112, "end_pos": 115, "type": "DATASET", "confidence": 0.6931170225143433}, {"text": "MT evaluation task", "start_pos": 185, "end_pos": 203, "type": "TASK", "confidence": 0.9024434685707092}]}, {"text": "This was shown to yield higher correlation with human judgments).", "labels": [], "entities": []}, {"text": "Learning automatic metrics in a pairwise setting, i.e., learning to distinguish between two alternative translations and to decide which of the two is better (which is arguably one of the easiest ways to produce a ranking), emulates closely how human judges perform evaluation assessments in reality.", "labels": [], "entities": []}, {"text": "Instead of learning a similarity function between a translation and the reference, they learn how to differentiate a better from a worse translation given a corresponding reference.", "labels": [], "entities": []}, {"text": "While the pairwise setting does not provide an absolute quality scoring metric, it is useful for most evaluation and MT development scenarios.", "labels": [], "entities": [{"text": "MT development", "start_pos": 117, "end_pos": 131, "type": "TASK", "confidence": 0.9224689304828644}]}, {"text": "In this paper, we propose a pairwise learning setting similar to that of Duh, but we extend it to anew level, both in terms of feature representation and learning framework.", "labels": [], "entities": []}, {"text": "First, we integrate several layers of linguistic information encapsulated in tree-based structures; only used lexical and POS matches as features.", "labels": [], "entities": []}, {"text": "Second, we use information about both the reference and two alternative translations simultaneously, thus bringing our ranking closer to how humans rank translations.", "labels": [], "entities": []}, {"text": "Finally, instead of deciding upfront which types of features between hypotheses and references are important, we use a our structural kernel learning (SKL) framework to generate and select them automatically.", "labels": [], "entities": []}, {"text": "The structural kernel learning (SKL) framework we propose consists in: (i) designing a structural representation, e.g., using syntactic and discourse trees of translation hypotheses and a references; and (ii) applying structural kernels, to such representations in order to automatically inject structural features in the preference re-ranking algorithm.", "labels": [], "entities": [{"text": "structural kernel learning (SKL)", "start_pos": 4, "end_pos": 36, "type": "TASK", "confidence": 0.639703576763471}]}, {"text": "We use this method with translation-reference pairs to directly learn the features themselves, instead of learning the importance of a predetermined set of features.", "labels": [], "entities": []}, {"text": "A similar learning framework has been proven to be effective for question answering (, and textual entailment recognition ().", "labels": [], "entities": [{"text": "question answering", "start_pos": 65, "end_pos": 83, "type": "TASK", "confidence": 0.9121589362621307}, {"text": "textual entailment recognition", "start_pos": 91, "end_pos": 121, "type": "TASK", "confidence": 0.7923394441604614}]}, {"text": "Our goals are twofold: (i) in the short term, to demonstrate that structural kernel learning is suitable for this task, and can effectively learn to rank hypotheses at the segment-level; and (ii) in the long term, to show that this approach provides a unified framework that allows to integrate several layers of linguistic analysis and information and to improve over the state-of-the-art.", "labels": [], "entities": []}, {"text": "Below we report the results of some initial experiments using syntactic and discourse structures.", "labels": [], "entities": []}, {"text": "We show that learning in the proposed framework yields better correlation with humans than applying the traditional translation-reference similarity metrics using the same type of structures.", "labels": [], "entities": []}, {"text": "We also show that the contributions of syntax and discourse information are cumulative.", "labels": [], "entities": []}, {"text": "Finally, despite the limited information we use, we achieve correlation at the segment level that outperforms BLEU and other metrics at WMT12, e.g., our metric would have been ranked higher in terms of correlation with human judgments compared to TER, NIST, and BLEU in the WMT12 Metrics shared task).", "labels": [], "entities": [{"text": "correlation", "start_pos": 60, "end_pos": 71, "type": "METRIC", "confidence": 0.9854985475540161}, {"text": "BLEU", "start_pos": 110, "end_pos": 114, "type": "METRIC", "confidence": 0.9989010095596313}, {"text": "WMT12", "start_pos": 136, "end_pos": 141, "type": "DATASET", "confidence": 0.9387630224227905}, {"text": "TER", "start_pos": 247, "end_pos": 250, "type": "METRIC", "confidence": 0.8166436553001404}, {"text": "NIST", "start_pos": 252, "end_pos": 256, "type": "DATASET", "confidence": 0.8337254524230957}, {"text": "BLEU", "start_pos": 262, "end_pos": 266, "type": "METRIC", "confidence": 0.996371865272522}, {"text": "WMT12 Metrics shared task", "start_pos": 274, "end_pos": 299, "type": "DATASET", "confidence": 0.9030612856149673}]}], "datasetContent": [{"text": "We experimented with datasets of segment-level human rankings of system outputs from the WMT11 and the WMT12 Metrics shared tasks: we used the WMT11 dataset for training and the WMT12 dataset for testing.", "labels": [], "entities": [{"text": "WMT11", "start_pos": 89, "end_pos": 94, "type": "DATASET", "confidence": 0.9633108377456665}, {"text": "WMT12 Metrics shared tasks", "start_pos": 103, "end_pos": 129, "type": "DATASET", "confidence": 0.9173881262540817}, {"text": "WMT11 dataset", "start_pos": 143, "end_pos": 156, "type": "DATASET", "confidence": 0.9899346232414246}, {"text": "WMT12 dataset", "start_pos": 178, "end_pos": 191, "type": "DATASET", "confidence": 0.9925069510936737}]}, {"text": "We focused on translating into English only, for which the datasets can be split by source language: Czech (cs), German (de), Spanish (es), and French (fr).", "labels": [], "entities": []}, {"text": "There were about 10,000 non-tied human judgments per language pair per dataset.", "labels": [], "entities": []}, {"text": "We scored our pairwise system predictions with respect to the WMT12 human judgments using the Kendall's Tau (\u03c4 ), which was official at WMT12.", "labels": [], "entities": [{"text": "WMT12 human judgments", "start_pos": 62, "end_pos": 83, "type": "DATASET", "confidence": 0.9064668416976929}, {"text": "Kendall's Tau (\u03c4 )", "start_pos": 94, "end_pos": 112, "type": "METRIC", "confidence": 0.8015602181355158}, {"text": "WMT12", "start_pos": 136, "end_pos": 141, "type": "DATASET", "confidence": 0.9818284511566162}]}, {"text": "presents the \u03c4 scores for all metric variants introduced in this paper: for the individual language pairs and overall.", "labels": [], "entities": []}, {"text": "The left-hand side of the table shows the results when using as similarity the direct kernel calculation between the corresponding structures of the candidate translation and the reference 3 , e.g., as in ).", "labels": [], "entities": []}, {"text": "The right-hand side contains the results for structured kernel learning.", "labels": [], "entities": []}, {"text": "We can make the following observations: (i) The overall results for all SKL-trained metrics are higher than the ones when applying direct similarity, showing that learning tree structures is better than just calculating similarity.", "labels": [], "entities": []}, {"text": "(ii) Regarding the linguistic representation, we see that, when learning tree structures, syntactic and discourse-based trees yield similar improvements with a slight advantage for the former.", "labels": [], "entities": []}, {"text": "More interestingly, when both structures are put together in a combined tree, the improvement is cumulative and yields the best results by a sizable margin.", "labels": [], "entities": []}, {"text": "This provides positive evidence towards our goal of a unified tree-based representation with multiple layers of linguistic information.", "labels": [], "entities": []}, {"text": "(iii) Comparing to the best evaluation metrics that participated in the WMT12 Metrics shared task, we find that our approach is competitive and would have been ranked among the top 3 participants.", "labels": [], "entities": [{"text": "WMT12 Metrics shared task", "start_pos": 72, "end_pos": 97, "type": "DATASET", "confidence": 0.7630826085805893}]}, {"text": "Furthermore, our result (0.237) is ahead of the correlation obtained by popular metrics such as TER (0.217), NIST (0.214) and BLEU (0.185) at WMT12.", "labels": [], "entities": [{"text": "correlation", "start_pos": 48, "end_pos": 59, "type": "METRIC", "confidence": 0.9585101008415222}, {"text": "TER (0.217)", "start_pos": 96, "end_pos": 107, "type": "METRIC", "confidence": 0.9470265209674835}, {"text": "NIST (0.214)", "start_pos": 109, "end_pos": 121, "type": "METRIC", "confidence": 0.6891564726829529}, {"text": "BLEU (0.185)", "start_pos": 126, "end_pos": 138, "type": "METRIC", "confidence": 0.9577498584985733}, {"text": "WMT12", "start_pos": 142, "end_pos": 147, "type": "DATASET", "confidence": 0.9821260571479797}]}, {"text": "This is very encouraging and shows the potential of our new proposal.", "labels": [], "entities": []}, {"text": "In this paper, we have presented only the first exploratory results.", "labels": [], "entities": []}, {"text": "Our approach can be easily extended with richer linguistic structures and further combined with some of the already existing strong evaluation metrics.", "labels": [], "entities": []}, {"text": "Note that the results in were for training on WMT11 and testing on WMT12 for each language pair in isolation.", "labels": [], "entities": [{"text": "WMT11", "start_pos": 46, "end_pos": 51, "type": "DATASET", "confidence": 0.9745736718177795}, {"text": "WMT12", "start_pos": 67, "end_pos": 72, "type": "DATASET", "confidence": 0.9788185358047485}]}, {"text": "Next, we study the impact of the choice of training language pair.", "labels": [], "entities": []}, {"text": "Table 2 shows cross-language evaluation results for DIS+SYN: lines 1-4 show results when training on WMT11 for one language pair, and then testing for each language pair of WMT12.", "labels": [], "entities": [{"text": "WMT11", "start_pos": 101, "end_pos": 106, "type": "DATASET", "confidence": 0.9246723055839539}, {"text": "WMT12", "start_pos": 173, "end_pos": 178, "type": "DATASET", "confidence": 0.9477039575576782}]}, {"text": "We can see that the overall differences in performance (see the last column: all) when training on different source languages are rather small, ranging from 0.209 to 0.221, which suggests that our approach is quite independent of the source language used for training.", "labels": [], "entities": []}, {"text": "Still, looking at individual test languages, we can see that for de-en and es-en, it is best to train on the same language; this also holds for fr-en, but there it is equally good to train on es-en.", "labels": [], "entities": []}, {"text": "Interestingly, training on es-en improves a bit for cs-en.", "labels": [], "entities": []}, {"text": "These somewhat mixed results have motivated us to try tuning on the full WMT11 dataset; as line 5 shows, this yielded improvements for all language pairs except for es-en.", "labels": [], "entities": [{"text": "WMT11 dataset", "start_pos": 73, "end_pos": 86, "type": "DATASET", "confidence": 0.9561574161052704}]}, {"text": "Comparing to line 4 in, we see that the overall Tau improved from 0.231 to 0.237.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Kendall's (\u03c4 ) correlation with human judgements on WMT12 for each language pair.", "labels": [], "entities": [{"text": "Kendall's (\u03c4 ) correlation", "start_pos": 10, "end_pos": 36, "type": "METRIC", "confidence": 0.7451422413190206}, {"text": "WMT12", "start_pos": 62, "end_pos": 67, "type": "DATASET", "confidence": 0.8925829529762268}]}, {"text": " Table 2: Kendall's (\u03c4 ) on WMT12 for cross- language training with DIS+SYN.", "labels": [], "entities": [{"text": "Kendall's (\u03c4 )", "start_pos": 10, "end_pos": 24, "type": "METRIC", "confidence": 0.797459888458252}, {"text": "WMT12", "start_pos": 28, "end_pos": 33, "type": "DATASET", "confidence": 0.8301225900650024}]}]}