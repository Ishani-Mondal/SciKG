{"title": [{"text": "An I-vector Based Approach to Compact Multi-Granularity Topic Spaces Representation of Textual Documents", "labels": [], "entities": [{"text": "Compact Multi-Granularity Topic Spaces Representation of Textual Documents", "start_pos": 30, "end_pos": 104, "type": "TASK", "confidence": 0.6874067820608616}]}], "abstractContent": [{"text": "Various studies highlighted that topic-based approaches give a powerful spoken content representation of documents.", "labels": [], "entities": []}, {"text": "Nonetheless, these documents may contain more than one main theme, and their automatic transcription inevitably contains errors.", "labels": [], "entities": []}, {"text": "In this study, we propose an original and promising framework based on a compact representation of a textual document , to solve issues related to topic space granularity.", "labels": [], "entities": []}, {"text": "Firstly, various topic spaces are estimated with different numbers of classes from a Latent Dirichlet Allocation.", "labels": [], "entities": []}, {"text": "Then, this multiple topic space representation is compacted into an elementary segment , called c-vector, originally developed in the context of speaker recognition.", "labels": [], "entities": [{"text": "speaker recognition", "start_pos": 145, "end_pos": 164, "type": "TASK", "confidence": 0.8619699478149414}]}, {"text": "Experiments are conducted on the DECODA corpus of conversations.", "labels": [], "entities": [{"text": "DECODA corpus of conversations", "start_pos": 33, "end_pos": 63, "type": "DATASET", "confidence": 0.934158518910408}]}, {"text": "Results show the effectiveness of the proposed multi-view compact representation paradigm.", "labels": [], "entities": []}, {"text": "Our identification system reaches an accuracy of 85%, with a significant gain of 9 points compared to the baseline (best single topic space configuration).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.9996922016143799}]}], "introductionContent": [{"text": "Automatic Speech Recognition (ASR) systems frequently fail on noisy conditions and high Word Error Rates (WER) make the analysis of the automatic transcriptions difficult.", "labels": [], "entities": [{"text": "Automatic Speech Recognition (ASR)", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.7815330376227697}, {"text": "Word Error Rates (WER)", "start_pos": 88, "end_pos": 110, "type": "METRIC", "confidence": 0.887932280699412}]}, {"text": "Speech analytics suffer from these transcription issues that maybe overcome by improving the ASR robustness and/or the tolerance of speech analytic systems to ASR errors.", "labels": [], "entities": [{"text": "ASR", "start_pos": 93, "end_pos": 96, "type": "TASK", "confidence": 0.9155849814414978}]}, {"text": "This paper proposes anew method to improve the robustness of speech analytics by combining a semantic multi-model approach and a noise reduction technique based on the i-vector paradigm.", "labels": [], "entities": [{"text": "speech analytics", "start_pos": 61, "end_pos": 77, "type": "TASK", "confidence": 0.7148566395044327}]}, {"text": "This method is evaluated in the application framework of the RATP call centre (Paris Public Transportation Authority), focusing on the theme identification task.", "labels": [], "entities": [{"text": "RATP call centre (Paris Public Transportation Authority)", "start_pos": 61, "end_pos": 117, "type": "DATASET", "confidence": 0.8866889211866591}, {"text": "theme identification task", "start_pos": 135, "end_pos": 160, "type": "TASK", "confidence": 0.8054871559143066}]}, {"text": "Telephone conversations area particular case of human-human interaction whose automatic processing raises problems, especially due to the speech recognition step required to obtain the transcription of the speech contents.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 138, "end_pos": 156, "type": "TASK", "confidence": 0.6906866431236267}]}, {"text": "First, the speaker's behavior maybe unexpected and the training/test mismatch maybe very large.", "labels": [], "entities": []}, {"text": "Second, the speech signal maybe strongly impacted by various sources of variability: environment and channel noises, acquisition devices, etc.", "labels": [], "entities": []}], "datasetContent": [{"text": "The proposed c-vector representation of automatic transcriptions is evaluated in the context of the theme identification of a human-human telephone conversation in the customer care service (CCS) of the RATP Paris transportation system.", "labels": [], "entities": [{"text": "RATP Paris transportation system", "start_pos": 203, "end_pos": 235, "type": "DATASET", "confidence": 0.7451449036598206}]}, {"text": "The metric used to identify of the best theme is the Mahalanobis metric.", "labels": [], "entities": [{"text": "Mahalanobis metric", "start_pos": 53, "end_pos": 71, "type": "DATASET", "confidence": 0.789148598909378}]}, {"text": "The proposed c-vector approach is applied to the same classification task and corpus proposed in) (state-of-the-art in text classification in).", "labels": [], "entities": [{"text": "text classification", "start_pos": 119, "end_pos": 138, "type": "TASK", "confidence": 0.7539694607257843}]}, {"text": "Experiments are conducted using the multiple topic spaces estimated with an LDA approach.", "labels": [], "entities": []}, {"text": "From these multiple topic spaces, a classical way is to find the one that reaches the best performance.", "labels": [], "entities": []}, {"text": "presents the theme classification performance obtained on the development and test sets using various topic-based representation configurations with the EFR normalization algorithm (baseline).", "labels": [], "entities": [{"text": "theme classification", "start_pos": 13, "end_pos": 33, "type": "TASK", "confidence": 0.7060948610305786}]}, {"text": "For sake of comparison, experiments are conducted using the automatic transcriptions only (ASR) only.", "labels": [], "entities": []}, {"text": "The conditions indicated by the abbreviations between parentheses are considered for the development (Dev) and the test (Test) sets.", "labels": [], "entities": []}, {"text": "Only homogenous conditions (ASR for both training and validations sets) are considered in this study.", "labels": [], "entities": [{"text": "ASR", "start_pos": 28, "end_pos": 31, "type": "METRIC", "confidence": 0.9707010388374329}]}, {"text": "Authors in) notice that results collapse dramatically when heterogenous conditions are employed (TRS or TRS+ASR for training set and ASR for validation set).", "labels": [], "entities": [{"text": "TRS", "start_pos": 97, "end_pos": 100, "type": "METRIC", "confidence": 0.8724757432937622}, {"text": "TRS+ASR", "start_pos": 104, "end_pos": 111, "type": "METRIC", "confidence": 0.7499186992645264}, {"text": "ASR", "start_pos": 133, "end_pos": 136, "type": "METRIC", "confidence": 0.9767327904701233}]}, {"text": "First of all, we can see that this baseline approach reached a classification accuracy of 83% and 76%, respectively on the development and the test sets.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.920880913734436}]}, {"text": "However, we note that the classification performance is rather unstable, and may completely change from a topic space configuration to another.", "labels": [], "entities": []}, {"text": "The gap between the lower and the higher classification results is also important, with a difference of 25 points on the development set (the same trend is observed on the test set).", "labels": [], "entities": []}, {"text": "As a result, finding the best topic space size seems crucial for this classification task, particularly in the context of highly imperfect automatic dialogue transcriptions containing more than one theme.", "labels": [], "entities": []}, {"text": "The topic space that yields the best accuracy with the baseline method (n = 15 topics) is presented in.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.9987210631370544}]}, {"text": "This figure presents each of the 15 topics and their 10 most representative words (highest P (w|z)).", "labels": [], "entities": []}, {"text": "Several topics contain more or less the same representative words, such as topics 3, 6 and 9.", "labels": [], "entities": []}, {"text": "This figure points out some interesting topics that allow us to distinguish a theme from the others.", "labels": [], "entities": []}, {"text": "For example: \u2022 topics 2, 10 and 15 represent some words related to itinerary problems, \u2022 the transportation cards theme is mostly represented in topic 4 and 15 (Imagine and Navigo are names of transportation cards), \u2022 the words which represent the time schedules theme are contained in topic 5,6,7 and less in topic 9, \u2022 state of the traffic could be discussed with words such as: departure, line, service, day.", "labels": [], "entities": []}, {"text": "These words and others are contained in topic 13, \u2022 topics 4 and 12 are related to the infractions theme with to words fine, pass, zone or ticket, \u2022 but topic 12 could be related to theme fares or special offers as well . presents results obtained with the proposed c-vector approach coupled with the EFR algorithm.", "labels": [], "entities": []}, {"text": "We can firstly note that this compact representation allows it to outperform the best topic space configuration (baseline), with again of 9.4 points on the development data and of 9 points on the test data.", "labels": [], "entities": []}, {"text": "Moreover, if we consider the differ-   and test sets, the gap between accuracies is much smaller: classification accuracy does not go below 82.6%, while it reached 56% for the worst topic-based configuration.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 113, "end_pos": 121, "type": "METRIC", "confidence": 0.9735075235366821}]}, {"text": "Indeed, as shown in Table 2, the difference between the maximum and the minimum theme classification accuracies is of 20% using the baseline approach while it is only of 2.4% using the c-vector method.", "labels": [], "entities": []}, {"text": "We can conclude that this original c-vector approach allows one to better handle the variabilities Figure 5: Topic space (15 topics) that obtains the best accuracy with the baseline system (see).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 155, "end_pos": 163, "type": "METRIC", "confidence": 0.9987063407897949}]}, {"text": "contained in dialogue conversations: in a classification context, better accuracy can be obtained and the results can be more consistent when varying the c-vector size and the number of Gaussians.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 73, "end_pos": 81, "type": "METRIC", "confidence": 0.9993026256561279}]}], "tableCaptions": [{"text": " Table 1: Theme classification accuracy (%) with different c-vectors and GMM-UBM sizes.", "labels": [], "entities": [{"text": "Theme classification", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.7718229293823242}, {"text": "accuracy", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.9651911854743958}]}, {"text": " Table 2: Maximum (M ax), minimum (M in) and Difference (M ax \u2212 M in) theme classification accu- racies (%) using the baseline and the proposed c-vector approaches.", "labels": [], "entities": [{"text": "Difference", "start_pos": 45, "end_pos": 55, "type": "METRIC", "confidence": 0.99573814868927}]}]}