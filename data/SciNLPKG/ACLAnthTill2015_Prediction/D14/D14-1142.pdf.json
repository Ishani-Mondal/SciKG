{"title": [{"text": "Can characters reveal your native language? A language-independent approach to native language identification", "labels": [], "entities": []}], "abstractContent": [{"text": "A common approach in text mining tasks such as text categorization, authorship identification or plagiarism detection is to rely on features like words, part-of-speech tags, stems, or some other high-level linguistic features.", "labels": [], "entities": [{"text": "text mining tasks", "start_pos": 21, "end_pos": 38, "type": "TASK", "confidence": 0.7956449588139852}, {"text": "text categorization", "start_pos": 47, "end_pos": 66, "type": "TASK", "confidence": 0.719815120100975}, {"text": "authorship identification or plagiarism detection", "start_pos": 68, "end_pos": 117, "type": "TASK", "confidence": 0.741580069065094}]}, {"text": "In this work, an approach that uses character n-grams as features is proposed for the task of native language identification.", "labels": [], "entities": [{"text": "native language identification", "start_pos": 94, "end_pos": 124, "type": "TASK", "confidence": 0.6545867820580801}]}, {"text": "Instead of doing standard feature selection, the proposed approach combines several string kernels using multiple kernel learning.", "labels": [], "entities": []}, {"text": "Kernel Ridge Regression and Kernel Discriminant Analysis are independently used in the learning stage.", "labels": [], "entities": [{"text": "Kernel Ridge Regression", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.5787681639194489}, {"text": "Kernel Discriminant Analysis", "start_pos": 28, "end_pos": 56, "type": "TASK", "confidence": 0.7752869327863058}]}, {"text": "The empirical results obtained in all the experiments conducted in this work indicate that the proposed approach achieves state of the art performance in native language identification, reaching an accuracy that is 1.7% above the top scoring system of the 2013 NLI Shared Task.", "labels": [], "entities": [{"text": "native language identification", "start_pos": 154, "end_pos": 184, "type": "TASK", "confidence": 0.7048953374226888}, {"text": "accuracy", "start_pos": 198, "end_pos": 206, "type": "METRIC", "confidence": 0.9992126226425171}, {"text": "2013 NLI Shared Task", "start_pos": 256, "end_pos": 276, "type": "DATASET", "confidence": 0.7220825999975204}]}, {"text": "Furthermore , the proposed approach has an important advantage in that it is language independent and linguistic theory neutral.", "labels": [], "entities": []}, {"text": "In the cross-corpus experiment, the proposed approach shows that it can also be topic independent, improving the state of the art system by 32.3%.", "labels": [], "entities": []}], "introductionContent": [{"text": "Using words as basic units is natural in textual analysis tasks such as text categorization, authorship identification or plagiarism detection.", "labels": [], "entities": [{"text": "authorship identification", "start_pos": 93, "end_pos": 118, "type": "TASK", "confidence": 0.8057796359062195}, {"text": "plagiarism detection", "start_pos": 122, "end_pos": 142, "type": "TASK", "confidence": 0.7828468680381775}]}, {"text": "Perhaps surprisingly, recent results indicate that methods handling the text at the character level can also be very effective (;.", "labels": [], "entities": []}, {"text": "By disregarding features of natural language such as words, phrases, or meaning, an approach that works at the character level has an important advantage in that it is language independent and linguistic theory neutral.", "labels": [], "entities": []}, {"text": "This paper presents a state of the art machine learning system for native language identification that works at the character level.", "labels": [], "entities": [{"text": "native language identification", "start_pos": 67, "end_pos": 97, "type": "TASK", "confidence": 0.6420034468173981}]}, {"text": "The proposed system is inspired by the system of, but includes some variations and improvements.", "labels": [], "entities": []}, {"text": "A major improvement is that several string kernels are combined via multiple kernel learning).", "labels": [], "entities": []}, {"text": "Despite the fact that the (histogram) intersection kernel is very popular in computer vision (, it has never been used before in text mining.", "labels": [], "entities": [{"text": "text mining", "start_pos": 129, "end_pos": 140, "type": "TASK", "confidence": 0.7703250348567963}]}, {"text": "In this work, the intersection kernel is used for the first time in a text categorization task, alone and in combination with other kernels.", "labels": [], "entities": []}, {"text": "The intersection kernel lies somewhere in the middle between the kernel that takes into account only the presence of n-grams and the kernel based on the frequency of n-grams (p-spectrum string kernel).", "labels": [], "entities": []}, {"text": "Two kernel classifiers are proposed for the learning task, namely Kernel Ridge Regression (KRR) and Kernel Discriminant Analysis (KDA).", "labels": [], "entities": [{"text": "Kernel Discriminant Analysis (KDA", "start_pos": 100, "end_pos": 133, "type": "TASK", "confidence": 0.678531676530838}]}, {"text": "The KDA classifier is able to avoid the classmasking problem, which may often arise in the context of native language identification.", "labels": [], "entities": [{"text": "native language identification", "start_pos": 102, "end_pos": 132, "type": "TASK", "confidence": 0.7225425044695536}]}, {"text": "Several experiments are conducted to evaluate the performance of the approach proposed in this work.", "labels": [], "entities": []}, {"text": "While multiple kernel learning seems to produce a more robust system, the two kernel classifiers obtained mixed results in the experiments.", "labels": [], "entities": []}, {"text": "Overall, the empirical results indicate that the approach proposed in this paper achieves state of the art performance in native language identification, while being both lan-guage independent and linguistic theory neutral.", "labels": [], "entities": [{"text": "native language identification", "start_pos": 122, "end_pos": 152, "type": "TASK", "confidence": 0.7027469078699747}]}, {"text": "Furthermore, the approach based on string kernels does not need any expert knowledge of words or phrases in the language.", "labels": [], "entities": []}, {"text": "The paper is organized as follows.", "labels": [], "entities": []}, {"text": "Related work is presented in Section 2.", "labels": [], "entities": []}, {"text": "Section 3 presents several similarity measures for strings, including string kernels and Local Rank Distance.", "labels": [], "entities": []}, {"text": "The learning methods used in the experiments are described in Section 4.", "labels": [], "entities": []}, {"text": "Section 5 presents details about the experiments.", "labels": [], "entities": []}, {"text": "Finally, the conclusions are drawn in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "This section describes the results on the TOEFL11 corpus.", "labels": [], "entities": [{"text": "TOEFL11 corpus", "start_pos": 42, "end_pos": 56, "type": "DATASET", "confidence": 0.9699703454971313}]}, {"text": "Thus, results for the 2013 Closed NLI Shared Task are also included.", "labels": [], "entities": [{"text": "2013 Closed NLI Shared Task", "start_pos": 22, "end_pos": 49, "type": "DATASET", "confidence": 0.8200445175170898}]}, {"text": "In the closed shared task the goal is to predict the native language of testing examples, restricted to learning only from the training and the development data.", "labels": [], "entities": []}, {"text": "The additional information from prompts or the English language proficiency level were not used in the proposed approach.", "labels": [], "entities": []}, {"text": "The regularization parameters were tuned on the development set.", "labels": [], "entities": []}, {"text": "In this case, the systems were trained on the entire training set.", "labels": [], "entities": []}, {"text": "A 10-fold crossvalidation (CV) procedure was done on the training and the development sets.", "labels": [], "entities": []}, {"text": "The folds were provided along with the TOEFL11 corpus.", "labels": [], "entities": [{"text": "TOEFL11 corpus", "start_pos": 39, "end_pos": 53, "type": "DATASET", "confidence": 0.9534511566162109}]}, {"text": "Finally, the results of the proposed systems are also reported on the NLI Shared Task test set.", "labels": [], "entities": [{"text": "NLI Shared Task test set", "start_pos": 70, "end_pos": 94, "type": "DATASET", "confidence": 0.8821087002754211}]}, {"text": "For testing, the systems were trained on both the training set and the development set.", "labels": [], "entities": []}, {"text": "The results are summarized in.", "labels": [], "entities": []}, {"text": "The results presented in show that string kernels can reach state of the art accuracy levels for this task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.9987413287162781}]}, {"text": "Overall, it seems that KDA is able to obtain better results than KRR.", "labels": [], "entities": [{"text": "KDA", "start_pos": 23, "end_pos": 26, "type": "DATASET", "confidence": 0.5429282784461975}, {"text": "KRR", "start_pos": 65, "end_pos": 68, "type": "DATASET", "confidence": 0.8804444670677185}]}, {"text": "The intersection kernel alone is able to obtain slightly better results than the presence bits kernel.", "labels": [], "entities": []}, {"text": "The kernel based on LRD gives significantly lower accuracy rates, but it is able to improve the performance when it is  87.0% 84.1% 84.8%: Accuracy rates on TOEFL11 corpus of various classification systems based on string kernels compared with other state of the art approaches.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 50, "end_pos": 58, "type": "METRIC", "confidence": 0.9987649917602539}, {"text": "Accuracy", "start_pos": 139, "end_pos": 147, "type": "METRIC", "confidence": 0.9779698848724365}, {"text": "TOEFL11 corpus", "start_pos": 157, "end_pos": 171, "type": "DATASET", "confidence": 0.9708414673805237}]}, {"text": "The best accuracy rates on each set of experiments are highlighted in bold.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 9, "end_pos": 17, "type": "METRIC", "confidence": 0.9992768168449402}]}, {"text": "The weights a 1 and a 2 from the weighted sums of kernels are computed by kernel alignment.", "labels": [], "entities": [{"text": "kernel alignment", "start_pos": 74, "end_pos": 90, "type": "TASK", "confidence": 0.7660481035709381}]}, {"text": "combined with the blended p-grams presence bits kernel.", "labels": [], "entities": []}, {"text": "In fact, most of the kernel combinations give better results than each of their components.", "labels": [], "entities": []}, {"text": "The best kernel combination is that of the presence bits kernel and the intersection kernel.", "labels": [], "entities": []}, {"text": "Results are quite similar when they are combined either by summing them up or by kernel alignment.", "labels": [], "entities": []}, {"text": "The best performance on the test set (85.3%) is obtained by the system that combines these two kernels via kernel alignment and learns using KDA.", "labels": [], "entities": [{"text": "KDA", "start_pos": 141, "end_pos": 144, "type": "DATASET", "confidence": 0.8563437461853027}]}, {"text": "This system is 1.7% better than the state of the art system of Jarvis et al. based on SVM and word features, this being the top scoring system in the NLI 2013 Shared Task.", "labels": [], "entities": [{"text": "NLI 2013 Shared Task", "start_pos": 150, "end_pos": 170, "type": "DATASET", "confidence": 0.7079138904809952}]}, {"text": "It is also 2.6% better than the state of the art system based on string kernels of.", "labels": [], "entities": []}, {"text": "On the cross validation procedure, there are three systems that reach the accuracy rate of 84.1%.", "labels": [], "entities": [{"text": "cross validation", "start_pos": 7, "end_pos": 23, "type": "TASK", "confidence": 0.7167084515094757}, {"text": "accuracy rate", "start_pos": 74, "end_pos": 87, "type": "METRIC", "confidence": 0.9892993271350861}]}, {"text": "The results on the ICLE corpus using a 5-fold cross validation procedure are summarized in.", "labels": [], "entities": [{"text": "ICLE corpus", "start_pos": 19, "end_pos": 30, "type": "DATASET", "confidence": 0.842928946018219}]}, {"text": "To adequately compare the results with a state of the art system, the same 5-fold cross validation procedure used by was also used in this experiment.", "labels": [], "entities": []}, {"text": "shows that the results obtained by the presence bits kernel and by the intersection kernel are systematically better than the state of the art system of.", "labels": [], "entities": []}, {"text": "While both KRR and KDA produce accuracy rates that are better than the state of the art accuracy rate, it seems that KRR is slightly better in this experiment.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.9971390962600708}, {"text": "accuracy", "start_pos": 88, "end_pos": 96, "type": "METRIC", "confidence": 0.9989203214645386}, {"text": "KRR", "start_pos": 117, "end_pos": 120, "type": "METRIC", "confidence": 0.8076993823051453}]}, {"text": "Again, the idea of combining kernels seems to produce more robust systems.", "labels": [], "entities": []}, {"text": "The best systems are based on combining the presence bits kernel either with the kernel based on LRD or the intersection kernel.", "labels": [], "entities": []}, {"text": "Overall, the reported accuracy rates are higher than the state of the art accuracy rate.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9891234040260315}, {"text": "accuracy", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.9990160465240479}]}, {"text": "The best performance (91.3%) is achieved by the KRR classifier based on combining the presence bits kernel with  the kernel based on LRD.", "labels": [], "entities": []}, {"text": "This represents an 1.2% improvement over the state of the art accuracy rate of.", "labels": [], "entities": [{"text": "accuracy rate", "start_pos": 62, "end_pos": 75, "type": "METRIC", "confidence": 0.9867231845855713}]}, {"text": "Two more systems are able to obtain accuracy rates greater than 91.0%.", "labels": [], "entities": [{"text": "accuracy rates", "start_pos": 36, "end_pos": 50, "type": "METRIC", "confidence": 0.9797845780849457}]}, {"text": "These are the KRR classifier based on the presence bits kernel (91.2%) and the KDA classifier based on the sum of the presence bits kernel and the intersection kernel (91.0%).", "labels": [], "entities": []}, {"text": "The overall results on the ICLE corpus show that the string kernels approach can reach state of the art accuracy levels.", "labels": [], "entities": [{"text": "ICLE corpus", "start_pos": 27, "end_pos": 38, "type": "DATASET", "confidence": 0.7412361800670624}, {"text": "accuracy", "start_pos": 104, "end_pos": 112, "type": "METRIC", "confidence": 0.9973430037498474}]}, {"text": "It is worth mentioning the purpose of this experiment was to use the same approach determined to work well in the TOEFL11 corpus.", "labels": [], "entities": [{"text": "TOEFL11 corpus", "start_pos": 114, "end_pos": 128, "type": "DATASET", "confidence": 0.9586820304393768}]}, {"text": "To serve this purpose, the range of n-grams was not tuned on this data set.", "labels": [], "entities": []}, {"text": "Furthermore, other classifiers were not tested in this experiment.", "labels": [], "entities": []}, {"text": "Nevertheless, better results can probably be obtained by adding these aspects into the equation.", "labels": [], "entities": []}, {"text": "In this experiment, various systems based on KRR or KDA are trained on the TOEFL11 corpus and tested on the TOEFL11-Big corpus.", "labels": [], "entities": [{"text": "TOEFL11 corpus", "start_pos": 75, "end_pos": 89, "type": "DATASET", "confidence": 0.9711658358573914}, {"text": "TOEFL11-Big corpus", "start_pos": 108, "end_pos": 126, "type": "DATASET", "confidence": 0.9714345633983612}]}, {"text": "The kernel based on LRD was not included in this experiment since it is more computationally expensive.", "labels": [], "entities": []}, {"text": "Therefore, only the presence bits kernel and the intersection kernel were evaluated on the TOEFL11-Big corpus.", "labels": [], "entities": [{"text": "TOEFL11-Big corpus", "start_pos": 91, "end_pos": 109, "type": "DATASET", "confidence": 0.9775389432907104}]}, {"text": "The results are summarized in.", "labels": [], "entities": []}, {"text": "The same regularization parameters determined to Method Test Ensemble model 35.4% KRR and\u02c6kand\u02c6 and\u02c6k: Accuracy rates on TOEFL11-Big corpus of various classification systems based on string kernels compared with a state of the art approach.", "labels": [], "entities": [{"text": "KRR", "start_pos": 82, "end_pos": 85, "type": "METRIC", "confidence": 0.9392232894897461}, {"text": "Accuracy", "start_pos": 103, "end_pos": 111, "type": "METRIC", "confidence": 0.9942016005516052}, {"text": "TOEFL11-Big corpus", "start_pos": 121, "end_pos": 139, "type": "DATASET", "confidence": 0.9421228468418121}]}, {"text": "The systems are trained on the TOEFL11 corpus and tested on the TOEFL11-Big corpus.", "labels": [], "entities": [{"text": "TOEFL11 corpus", "start_pos": 31, "end_pos": 45, "type": "DATASET", "confidence": 0.9680631160736084}, {"text": "TOEFL11-Big corpus", "start_pos": 64, "end_pos": 82, "type": "DATASET", "confidence": 0.9811041355133057}]}, {"text": "The best accuracy rate is highlighted in bold.", "labels": [], "entities": [{"text": "accuracy rate", "start_pos": 9, "end_pos": 22, "type": "METRIC", "confidence": 0.9583332538604736}]}, {"text": "The weights a 1 and a 2 from the weighted sums of kernels are computed by kernel alignment.", "labels": [], "entities": [{"text": "kernel alignment", "start_pos": 74, "end_pos": 90, "type": "TASK", "confidence": 0.7660481035709381}]}, {"text": "work well on the TOEFL11 development set were used.", "labels": [], "entities": [{"text": "TOEFL11 development set", "start_pos": 17, "end_pos": 40, "type": "DATASET", "confidence": 0.9714455207188925}]}, {"text": "The most interesting fact is that all the proposed systems are at least 30% better than the state of the art system.", "labels": [], "entities": []}, {"text": "Considering that the TOEFL11-Big corpus contains 87 thousand samples, the 30% improvement is significant without any doubt.", "labels": [], "entities": [{"text": "TOEFL11-Big corpus", "start_pos": 21, "end_pos": 39, "type": "DATASET", "confidence": 0.9506625235080719}]}, {"text": "Diving into details, it can be observed that the results obtained by KRR are higher than those obtained by KDA.", "labels": [], "entities": [{"text": "KRR", "start_pos": 69, "end_pos": 72, "type": "DATASET", "confidence": 0.6075640916824341}, {"text": "KDA", "start_pos": 107, "end_pos": 110, "type": "DATASET", "confidence": 0.8753631711006165}]}, {"text": "However, both methods perform very well compared to the state of the art.", "labels": [], "entities": []}, {"text": "Again, kernel combinations are better than each of their individual kernels alone.", "labels": [], "entities": []}, {"text": "It is important to mention that the significant performance increase is not due to the learning method (KRR or KDA), but rather due to the string kernels that work at the character level.", "labels": [], "entities": []}, {"text": "It is not only the case that string kernels are language independent, but for the same reasons they can also be topic independent.", "labels": [], "entities": []}, {"text": "Since the topics (prompts) from TOEFL11 are different from the topics from TOEFL11-Big, it becomes clear that a method that uses words as features is strongly affected, since the distribution of words per topic can be completely different.", "labels": [], "entities": [{"text": "TOEFL11", "start_pos": 32, "end_pos": 39, "type": "DATASET", "confidence": 0.903356671333313}, {"text": "TOEFL11-Big", "start_pos": 75, "end_pos": 86, "type": "DATASET", "confidence": 0.9019830226898193}]}, {"text": "But mistakes that reveal the native language can be captured by character ngrams that can appear more often even in different topics.", "labels": [], "entities": []}, {"text": "The results indicate that this is also the case of the approach based on string kernels, which seems to be more robust to such topic variations of the data set.", "labels": [], "entities": []}, {"text": "The best system has an accuracy rate that is 32.3% better than the state of the art system of.", "labels": [], "entities": [{"text": "accuracy rate", "start_pos": 23, "end_pos": 36, "type": "METRIC", "confidence": 0.9849121272563934}]}, {"text": "Overall, the empirical results indicate that the string kernels approach can achieve significantly better results than other state of the art approaches.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Accuracy rates on TOEFL11 corpus of various classification systems based on string kernels  compared with other state of the art approaches. The best accuracy rates on each set of experiments are  highlighted in bold. The weights a 1 and a 2 from the weighted sums of kernels are computed by kernel  alignment.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9959174990653992}, {"text": "TOEFL11 corpus", "start_pos": 28, "end_pos": 42, "type": "DATASET", "confidence": 0.9681209623813629}, {"text": "accuracy", "start_pos": 160, "end_pos": 168, "type": "METRIC", "confidence": 0.9980447292327881}]}, {"text": " Table 3: Accuracy rates on ICLE corpus of vari- ous classification systems based on string kernels  compared with a state of the art approach. The ac- curacy rates are reported for the same 5-fold CV  procedure as in (Tetreault et al., 2012). The best  accuracy rate is highlighted in bold.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9854236245155334}, {"text": "ICLE corpus", "start_pos": 28, "end_pos": 39, "type": "DATASET", "confidence": 0.8367810845375061}, {"text": "accuracy", "start_pos": 254, "end_pos": 262, "type": "METRIC", "confidence": 0.9992566704750061}]}, {"text": " Table 4: Accuracy rates on TOEFL11-Big corpus  of various classification systems based on string  kernels compared with a state of the art approach.  The systems are trained on the TOEFL11 corpus  and tested on the TOEFL11-Big corpus. The best  accuracy rate is highlighted in bold. The weights  a 1 and a 2 from the weighted sums of kernels are  computed by kernel alignment.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9966603517532349}, {"text": "TOEFL11-Big corpus", "start_pos": 28, "end_pos": 46, "type": "DATASET", "confidence": 0.9608234465122223}, {"text": "TOEFL11 corpus", "start_pos": 182, "end_pos": 196, "type": "DATASET", "confidence": 0.9528659582138062}, {"text": "TOEFL11-Big corpus", "start_pos": 216, "end_pos": 234, "type": "DATASET", "confidence": 0.9624449610710144}, {"text": "accuracy", "start_pos": 246, "end_pos": 254, "type": "METRIC", "confidence": 0.9982342720031738}]}]}