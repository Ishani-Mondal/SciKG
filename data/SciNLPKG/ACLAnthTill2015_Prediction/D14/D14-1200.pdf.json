{"title": [{"text": "Modeling Joint Entity and Relation Extraction with Table Representation", "labels": [], "entities": [{"text": "Modeling Joint Entity and Relation Extraction", "start_pos": 0, "end_pos": 45, "type": "TASK", "confidence": 0.8618552684783936}]}], "abstractContent": [{"text": "This paper proposes a history-based struc-tured learning approach that jointly extracts entities and relations in a sentence.", "labels": [], "entities": []}, {"text": "We introduce a novel simple and flexible table representation of entities and relations.", "labels": [], "entities": []}, {"text": "We investigate several feature settings , search orders, and learning methods with inexact search on the table.", "labels": [], "entities": []}, {"text": "The experimental results demonstrate that a joint learning approach significantly out-performs a pipeline approach by incorporating global features and by selecting appropriate learning methods and search orders .", "labels": [], "entities": []}], "introductionContent": [{"text": "Extraction of entities and relations from texts has been traditionally treated as a pipeline of two separate subtasks: entity recognition and relation extraction.", "labels": [], "entities": [{"text": "Extraction of entities and relations from texts", "start_pos": 0, "end_pos": 47, "type": "TASK", "confidence": 0.8890699659075055}, {"text": "entity recognition", "start_pos": 119, "end_pos": 137, "type": "TASK", "confidence": 0.7258297502994537}, {"text": "relation extraction", "start_pos": 142, "end_pos": 161, "type": "TASK", "confidence": 0.7614490389823914}]}, {"text": "This separation makes the task easy to deal with, but it ignores underlying dependencies between and within subtasks.", "labels": [], "entities": []}, {"text": "First, since entity recognition is not affected by relation extraction, errors in entity recognition are propagated to relation extraction.", "labels": [], "entities": [{"text": "entity recognition", "start_pos": 13, "end_pos": 31, "type": "TASK", "confidence": 0.7940130531787872}, {"text": "relation extraction", "start_pos": 51, "end_pos": 70, "type": "TASK", "confidence": 0.7581227123737335}, {"text": "entity recognition", "start_pos": 82, "end_pos": 100, "type": "TASK", "confidence": 0.7306210696697235}, {"text": "relation extraction", "start_pos": 119, "end_pos": 138, "type": "TASK", "confidence": 0.8095002770423889}]}, {"text": "Second, relation extraction is often treated as a multi-class classification problem on pairs of entities, so dependencies between pairs are ignored.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 8, "end_pos": 27, "type": "TASK", "confidence": 0.9272129237651825}]}, {"text": "Examples of these dependencies are illustrated in.", "labels": [], "entities": []}, {"text": "For dependencies between subtasks, a Live in relation requires PER and LOC entities, and vice versa.", "labels": [], "entities": [{"text": "PER", "start_pos": 63, "end_pos": 66, "type": "METRIC", "confidence": 0.9615803956985474}]}, {"text": "For in-subtask dependencies, the Live in relation between \"Mrs. Tsutayama\" and \"Japan\" can be inferred from the two other relations.", "labels": [], "entities": [{"text": "Live in relation", "start_pos": 33, "end_pos": 49, "type": "METRIC", "confidence": 0.9455090363820394}, {"text": "Japan", "start_pos": 80, "end_pos": 85, "type": "DATASET", "confidence": 0.7914670705795288}]}, {"text": "Figure 1 also shows that the task has a flexible graph structure.", "labels": [], "entities": []}, {"text": "This structure usually does not coverall the words in a sentence differently from other natural language processing (NLP) tasks such as part-of-speech (POS) tagging and depenMrs.", "labels": [], "entities": [{"text": "part-of-speech (POS) tagging", "start_pos": 136, "end_pos": 164, "type": "TASK", "confidence": 0.6877156615257263}]}, {"text": "Tsuruyama is from Kumamoto Prefecture in Japan . dency parsing, so local constraints are considered to be more important in the task.", "labels": [], "entities": [{"text": "dency parsing", "start_pos": 49, "end_pos": 62, "type": "TASK", "confidence": 0.8687107861042023}]}, {"text": "Joint learning approaches) incorporate these dependencies and local constraints in their models; however most approaches are time-consuming and employ complex structures consisting of multiple models.", "labels": [], "entities": []}, {"text": "recently proposed a history-based structured learning approach that is simpler and more computationally efficient than other approaches.", "labels": [], "entities": []}, {"text": "While this approach is promising, it still has a complexity in search and restricts the search order partly due to its semi-Markov representation, and thus the potential of the historybased learning is not fully investigated.", "labels": [], "entities": []}, {"text": "In this paper, we introduce an entity and relation table to address the difficulty in representing the task.", "labels": [], "entities": []}, {"text": "We propose a joint extraction of entities and relations using a history-based structured learning on the table.", "labels": [], "entities": []}, {"text": "This table representation simplifies the task into a table-filling problem, and makes the task flexible enough to incorporate several enhancements that have not been addressed in the previous history-based approach, such as search orders in decoding, global features from relations to entities, and several learning methods with inexact search.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we first introduce the corpus and evaluation metrics that we employed for evaluation.", "labels": [], "entities": []}, {"text": "We then show the performance on the training data set with explaining the parameters used Character n-grams (n=2,3,4) (Entity) Attributes by parsers (base form, POS) Word types (all-capitalized, initial-capitalized, all-digits, all-puncts, alldigits-or-puncts) Contextual Word n-grams (n=1,2,3) within a context window size of 2 Word pair Entity Entity lexical features of each word (Relation) Contextual Word n-grams (n=1,2,3) within a context window size of 2 Shortest path Walk features (word-dependency-word or dependency-worddependency) on the shortest paths in parsers' outputs n-grams (n=2,3) of words and dependencies on the paths n-grams (n=1,2) of token modifier-modifiee pairs on the paths The length of the paths  for the test set evaluation, and show the performance on the test data set.", "labels": [], "entities": []}, {"text": "We used an entity and relation recognition corpus by . The corpus defines four named entity types Location, Organization, Person, and Other and five relation types Kill, Live In, Located In, OrgBased In and Work For.", "labels": [], "entities": []}, {"text": "All the entities were words in the original corpus because all the spaces in entities were replaced with slashes.", "labels": [], "entities": []}, {"text": "Previous systems) used these word boundaries as they were, treated the boundaries as given, and focused the entity classification problem alone.", "labels": [], "entities": [{"text": "entity classification", "start_pos": 108, "end_pos": 129, "type": "TASK", "confidence": 0.7164531201124191}]}, {"text": "Differently from such systems, we recovered these spaces by replacing these slashes with spaces to evaluate the entity boundary detection performance on this corpus.", "labels": [], "entities": [{"text": "entity boundary detection", "start_pos": 112, "end_pos": 137, "type": "TASK", "confidence": 0.6326880753040314}]}, {"text": "Due to this replacement and the inclusion of the boundary detection problem, our task is more challenging than the original task, and our results are not comparable with those by the previous systems.", "labels": [], "entities": [{"text": "boundary detection", "start_pos": 49, "end_pos": 67, "type": "TASK", "confidence": 0.7358518093824387}]}, {"text": "The corpus contains 1,441 sentences that contain at least one relation.", "labels": [], "entities": []}, {"text": "Instead of 5-fold cross validation on the entire corpus by the previous systems, we split the data set into training (1,153 sentences) and blind test (288 sentences) data sets and developed the system on the training data set.", "labels": [], "entities": [{"text": "training data set", "start_pos": 208, "end_pos": 225, "type": "DATASET", "confidence": 0.8213230768839518}]}, {"text": "We tuned the hyper-parameters using a 5-fold cross validation on the training data set, and evaluated the performance on the test set.", "labels": [], "entities": [{"text": "training data set", "start_pos": 69, "end_pos": 86, "type": "DATASET", "confidence": 0.8260437448819479}]}, {"text": "We prepared a pipeline approach as a baseline.", "labels": [], "entities": []}, {"text": "We first trained an entity recognition model using the local and global features, and then trained a relation extraction model using the local features and global features without global \"Relation\" features in.", "labels": [], "entities": [{"text": "entity recognition", "start_pos": 20, "end_pos": 38, "type": "TASK", "confidence": 0.7463571727275848}, {"text": "relation extraction", "start_pos": 101, "end_pos": 120, "type": "TASK", "confidence": 0.7683316171169281}]}, {"text": "We did not employ the global \"Relation\" features in this baseline since it is common to treat relation extraction as a multi-class classification problem.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 94, "end_pos": 113, "type": "TASK", "confidence": 0.8715171813964844}]}, {"text": "We extracted features using the results from two syntactic parsers Enju ( and LRDEP.", "labels": [], "entities": [{"text": "LRDEP", "start_pos": 78, "end_pos": 83, "type": "METRIC", "confidence": 0.7800254225730896}]}, {"text": "We employed feature hashing and limited the feature space to 2 24 . The numbers of features greatly varied for categories and targets.", "labels": [], "entities": []}, {"text": "They also caused biased predictions that prefer entities to relations in our preliminary experiments.", "labels": [], "entities": []}, {"text": "We thus chose to re-scale the features as follows.", "labels": [], "entities": []}, {"text": "We normalized local features for each feature category and then for each target.", "labels": [], "entities": []}, {"text": "We also normalized global features for each feature category, but we did not normalize them for each target since normalization was impossible during decoding.", "labels": [], "entities": []}, {"text": "We instead scaled the global features, and the scaling factor was tuned by using the same 5-fold cross validation above.", "labels": [], "entities": []}, {"text": "We used the F1 score on relations with entities as our primary evaluation measure and used it for tuning parameters.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9823271930217743}]}, {"text": "In this measure, a relation with two entities is considered correct when the offsets and types of the entities and the type of the relation are all correct.", "labels": [], "entities": []}, {"text": "We also evaluated the F1 scores for entities and relations individually on the test data set by checking their corresponding cells.", "labels": [], "entities": [{"text": "F1", "start_pos": 22, "end_pos": 24, "type": "METRIC", "confidence": 0.9986788630485535}]}, {"text": "An entity is correct when the offset and type are correct, and a relation is correct when the type is correct and the last words of two entities are correct.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 6: Performance of entity and relation extraction on the test data set (precision / recall / F1 score).  The  \u2020 denotes the default parameter setting in  \u00a73.2 and \u22c6 represents a significant improvement over the  underlined \"Pipeline\" baseline (p<0.05). Labels (a)-(f) correspond to those in", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 36, "end_pos": 55, "type": "TASK", "confidence": 0.7254846841096878}, {"text": "precision", "start_pos": 78, "end_pos": 87, "type": "METRIC", "confidence": 0.9986831545829773}, {"text": "recall", "start_pos": 90, "end_pos": 96, "type": "METRIC", "confidence": 0.7133846879005432}, {"text": "F1 score", "start_pos": 99, "end_pos": 107, "type": "METRIC", "confidence": 0.963646411895752}]}, {"text": " Table 7: Results of entity classification and relation extraction on the data set using the 5-fold cross  validation (precision / recall / F1 score).", "labels": [], "entities": [{"text": "entity classification", "start_pos": 21, "end_pos": 42, "type": "TASK", "confidence": 0.7906454205513}, {"text": "relation extraction", "start_pos": 47, "end_pos": 66, "type": "TASK", "confidence": 0.8011521100997925}, {"text": "precision", "start_pos": 119, "end_pos": 128, "type": "METRIC", "confidence": 0.9982218146324158}, {"text": "recall", "start_pos": 131, "end_pos": 137, "type": "METRIC", "confidence": 0.6924962401390076}, {"text": "F1 score", "start_pos": 140, "end_pos": 148, "type": "METRIC", "confidence": 0.9356650412082672}]}]}