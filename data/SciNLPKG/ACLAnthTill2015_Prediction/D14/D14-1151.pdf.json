{"title": [{"text": "Using Mined Coreference Chains as a Resource fora Semantic Task", "labels": [], "entities": []}], "abstractContent": [{"text": "We propose to use coreference chains extracted from a large corpus as a resource for semantic tasks.", "labels": [], "entities": []}, {"text": "We extract three million coreference chains and train word embeddings on them.", "labels": [], "entities": []}, {"text": "Then, we compare these embeddings to word vectors derived from raw text data and show that coreference-based word embeddings improve F 1 on the task of antonym classification by up to .09.", "labels": [], "entities": [{"text": "F 1", "start_pos": 133, "end_pos": 136, "type": "METRIC", "confidence": 0.9898491203784943}, {"text": "antonym classification", "start_pos": 152, "end_pos": 174, "type": "TASK", "confidence": 0.728019654750824}]}], "introductionContent": [{"text": "After more than a decade of work on coreference resolution, coreference resolution systems have reached a certain level of maturity (e.g.,).", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 36, "end_pos": 58, "type": "TASK", "confidence": 0.9580386579036713}, {"text": "coreference resolution", "start_pos": 60, "end_pos": 82, "type": "TASK", "confidence": 0.9114876389503479}]}, {"text": "While accuracy is far from perfect and many phenomena such as bridging still pose difficult research problems, the quality of the output of these systems is high enough to be useful for many applications.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 6, "end_pos": 14, "type": "METRIC", "confidence": 0.9988844990730286}]}, {"text": "In this paper, we propose to run coreference resolution systems on large corpora, to collect the coreference chains found and to use them as a resource for solving semantic tasks.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 33, "end_pos": 55, "type": "TASK", "confidence": 0.9023429751396179}]}, {"text": "This amounts to using mined coreference chains as an automatically compiled resource similar to the way cooccurrence statistics, dependency pairs and aligned parallel corpora are used in many applications in NLP.", "labels": [], "entities": []}, {"text": "Coreference chains have interesting complementary properties compared to these other resources.", "labels": [], "entities": []}, {"text": "For example, it is difficult to distinguish true semantic similarity (e.g., \"cows\" -\"cattle\") from mere associational relatedness (e.g., \"cows\" -\"milk\") based on cooccurrence statistics.", "labels": [], "entities": []}, {"text": "In contrast, coreference chains should be able to make that distinction since only \"cows\" and \"cattle\" can occur in the same coreference chain, not \"cows\" and \"milk\".", "labels": [], "entities": []}, {"text": "As a proof of concept we compile a resource of mined coreference chains from the Gigaword corpus and apply it to the task of identifying antonyms.", "labels": [], "entities": [{"text": "Gigaword corpus", "start_pos": 81, "end_pos": 96, "type": "DATASET", "confidence": 0.9621687233448029}]}, {"text": "We induce distributed representations for words based on (i) cooccurrence statistics and (ii) mined coreference chains and show that a combination of both outperforms cooccurrence statistics on antonym identification.", "labels": [], "entities": [{"text": "antonym identification", "start_pos": 194, "end_pos": 216, "type": "TASK", "confidence": 0.7549618184566498}]}, {"text": "In summary, we make two contributions.", "labels": [], "entities": []}, {"text": "First, we propose to use coreference chains mined from large corpora as a resource in NLP and publish the first such resource.", "labels": [], "entities": []}, {"text": "Second, in a proof of concept study, we show that they can be used to solve a semantic task -antonym identification -better than is possible with existing resources.", "labels": [], "entities": [{"text": "semantic task -antonym identification", "start_pos": 78, "end_pos": 115, "type": "TASK", "confidence": 0.6186354398727417}]}, {"text": "We focus on the task of finding antonyms in this paper since antonyms usually are distributionally similar but semantically dissimilar words.", "labels": [], "entities": []}, {"text": "Hence, it is often not possible to distinguish them from synonyms with distributional models only.", "labels": [], "entities": []}, {"text": "In contrast, we expect that the coreference-based representations can provide useful complementary information to this task.", "labels": [], "entities": []}, {"text": "In general, coreferencebased similarity can however be used as an additional feature for any task that distributional similarity is useful for.", "labels": [], "entities": []}, {"text": "Thus, our coreference resource can be applied to a variety of NLP tasks, e.g. finding alternative names for entities (in away similar to Wikipedia anchors) for tasks in the context of knowledge base population.", "labels": [], "entities": []}, {"text": "The remainder of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we describe how we create word embeddings and how our antonym classifier works.", "labels": [], "entities": []}, {"text": "The word embeddings are then evaluated qualitatively, quantitatively and for the task of antonym detection (Section 3).", "labels": [], "entities": [{"text": "antonym detection", "start_pos": 89, "end_pos": 106, "type": "TASK", "confidence": 0.7325760573148727}]}, {"text": "Section 4 discusses related work and Section 5 concludes.", "labels": [], "entities": []}, {"text": "2 System description 2.1 Coreference-based embeddings Standard word embeddings derived from text data may not be able to distinguish between semantic text-based coref.-based his my, their, her, your, our he, him, himself, zechariah, ancestor woman man, girl, believer, pharisee, guy girl, prostitute, lupita, betsy, lehia: Nearest neighbors of \"his\" / \"woman\" for text-based & coreference-based embeddings association and true synonymy.", "labels": [], "entities": []}, {"text": "As a result, synonyms and antonyms maybe mapped to similar word vectors ().", "labels": [], "entities": []}, {"text": "For many NLP tasks, however, information about true synonymy or antonymy maybe important.", "labels": [], "entities": []}, {"text": "In this paper, we develop two different word embeddings: embeddings calculated on raw text data and embeddings derived from automatically extracted coreference chains.", "labels": [], "entities": []}, {"text": "For the calculation of the vector representations, the word2vec toolkit 1 by is applied.", "labels": [], "entities": []}, {"text": "We use the skip-gram model for our experiments because its results for semantic similarity are better according to.", "labels": [], "entities": []}, {"text": "We train a first model on a subset of English Gigaword data.", "labels": [], "entities": [{"text": "English Gigaword data", "start_pos": 38, "end_pos": 59, "type": "DATASET", "confidence": 0.8858153422673544}]}, {"text": "In the following sections, we call the resulting embeddings text-based.", "labels": [], "entities": []}, {"text": "To improve the semantic similarities of the vectors, we prepare another training text consisting of coreference chains.", "labels": [], "entities": []}, {"text": "We use CoreNLP () to extract coreference chains from the Gigaword corpus.", "labels": [], "entities": [{"text": "Gigaword corpus", "start_pos": 57, "end_pos": 72, "type": "DATASET", "confidence": 0.9539983570575714}]}, {"text": "Then we build a skip-gram model on these coreference chains.", "labels": [], "entities": []}, {"text": "The extracted coreference chains are provided as an additional resource to this paper 3 . Although they have been developed using only a publicly available toolkit, we expect this resource to be helpful for other researchers since the process to extract the coreference chains of such a large text corpus takes several weeks on multi-core machines.", "labels": [], "entities": []}, {"text": "In total, we extracted 3.1M coreference chains.", "labels": [], "entities": []}, {"text": "2.7M of them consist of at least two different markables.", "labels": [], "entities": []}, {"text": "The median (mean) length of the chains is 3 (4.0) and the median (mean) length of a markable is 1 (2.7).", "labels": [], "entities": [{"text": "median (mean) length", "start_pos": 58, "end_pos": 78, "type": "METRIC", "confidence": 0.7262941479682923}]}, {"text": "To train word embeddings, the markables of each coreference chain are concatenated to one text line.", "labels": [], "entities": []}, {"text": "These lines are used as input sentences for word2vec.", "labels": [], "entities": []}, {"text": "We refer to the resulting embeddings as coreference-based.", "labels": [], "entities": []}], "datasetContent": [{"text": "We formalize antonym detection as a binary classification task.", "labels": [], "entities": [{"text": "antonym detection", "start_pos": 13, "end_pos": 30, "type": "TASK", "confidence": 0.7074882686138153}]}, {"text": "Given a target word wand one of its nearest neighbors v, the classifier decides whether v is an antonym of w.", "labels": [], "entities": []}, {"text": "Our data set is a set of pairs, each consisting of a target word wand a candidate v.", "labels": [], "entities": []}, {"text": "For all word types of our vocabulary, we search for antonyms using the online dictionary Merriam Webster.", "labels": [], "entities": [{"text": "Merriam Webster", "start_pos": 89, "end_pos": 104, "type": "DATASET", "confidence": 0.9525453150272369}]}, {"text": "The resulting list is provided as an additional resource . It contains 6225 words with antonyms.", "labels": [], "entities": []}, {"text": "Positive training examples are collected by checking if the 500 nearest text-based neighbors of w contain one of the antonyms listed by Webster.", "labels": [], "entities": []}, {"text": "Negative training examples are created by replacing the antonym with a random word from the 500 nearest neighbors that is not listed as http://www.merriam-webster.com 6 https://code.google.com/p/cistern an antonym.", "labels": [], "entities": []}, {"text": "By selecting both the positive and the negative examples from the nearest neighbors of the word vectors, we intend to develop a task which is hard to solve: The classifier has to find the small portion of semantically dissimilar words (i.e., antonyms) among distributionally very similar words.", "labels": [], "entities": []}, {"text": "The total number of positive and negative examples is 2337 each.", "labels": [], "entities": []}, {"text": "The data are split into training (80%), development (10%) and test (10%) sets.", "labels": [], "entities": []}, {"text": "In initial experiments, we found only a small difference in antonym classification performance between text-based and coreference-based features.", "labels": [], "entities": [{"text": "antonym classification", "start_pos": 60, "end_pos": 82, "type": "TASK", "confidence": 0.584360808134079}]}, {"text": "When analyzing the errors, we realized that our rationale for using coreference-based embeddings only applies to nouns, not to other parts of speech.", "labels": [], "entities": []}, {"text": "This will be discussed in detail below.", "labels": [], "entities": []}, {"text": "We therefore run our experiments in two modes: all word classification (all pairs are considered) and noun classification (only pairs are considered for which the target word is a noun).", "labels": [], "entities": [{"text": "word classification", "start_pos": 51, "end_pos": 70, "type": "TASK", "confidence": 0.7065129429101944}, {"text": "noun classification", "start_pos": 102, "end_pos": 121, "type": "TASK", "confidence": 0.8590129017829895}]}, {"text": "We use the Stanford part-of-speech tagger ( to determine whether a word is a noun or not.", "labels": [], "entities": []}, {"text": "Our classifier is a radial basis function (rbf) support vector machine (SVM).", "labels": [], "entities": []}, {"text": "The rbf kernel performed better than a linear kernel in initial experiments.", "labels": [], "entities": []}, {"text": "The SVM parameters C and \u03b3 are optimized on the development set.", "labels": [], "entities": []}, {"text": "The representation of target-candidate pairs consists of the features described in Section 2.", "labels": [], "entities": []}, {"text": "We perform the experiments with the three different feature sets described in Section 2: text-based, coreference-based and all features.", "labels": [], "entities": []}, {"text": "For all word classification, coreference-based features do not improve performance on the development set (e.g., F 1 is .74 for text-based vs .72 for text+coref).", "labels": [], "entities": [{"text": "word classification", "start_pos": 8, "end_pos": 27, "type": "TASK", "confidence": 0.7425988912582397}, {"text": "F 1", "start_pos": 113, "end_pos": 116, "type": "METRIC", "confidence": 0.9825633466243744}]}, {"text": "On the test set, however, the combination of all features (text+coref) has better performance than text-based alone: .66 vs .63.", "labels": [], "entities": []}, {"text": "For noun classification, using coreferencebased features in addition to text-based features improves results on development set (F 1 is .78 vs .73) and test set (.69 vs .60).", "labels": [], "entities": [{"text": "noun classification", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.8830752968788147}, {"text": "F 1", "start_pos": 129, "end_pos": 132, "type": "METRIC", "confidence": 0.9578607678413391}]}, {"text": "These results show that mined coreference chains area useful resource and provide information that is complementary to other methods.", "labels": [], "entities": []}, {"text": "Even though adding coreference-based embeddings improves performance on antonym classification, the experiments also show that using only coreference-based embeddings is almost always worse than using only text-based embeddings.", "labels": [], "entities": [{"text": "antonym classification", "start_pos": 72, "end_pos": 94, "type": "TASK", "confidence": 0.7405341267585754}]}, {"text": "This is not surprising given that the amount of training data for the word embeddings is different in the two cases.", "labels": [], "entities": []}, {"text": "Coreference chains provide only a small subset of the word-word relations that are given to the word2vec skip-gram model when applied to raw text.", "labels": [], "entities": []}, {"text": "If the sizes of the training data sets were similar in the two cases, we would expect performance to be comparable.", "labels": [], "entities": []}, {"text": "In the beginning, our hypothesis was that coreference information should be helpful for antonym classification in general.", "labels": [], "entities": [{"text": "antonym classification", "start_pos": 88, "end_pos": 110, "type": "TASK", "confidence": 0.8093767166137695}]}, {"text": "When we performed an error analysis for our initial results, we realized that this hypothesis only holds for nouns.", "labels": [], "entities": []}, {"text": "Other types of words cooccurring in coreference chains are not more likely to be synonyms than words cooccurring in text windows.", "labels": [], "entities": []}, {"text": "Two contexts that illustrate this point are \"bright sides, but also difficult and dark ones\" and \"a series of black and white shots\" (elements of coreference chains in italics).", "labels": [], "entities": []}, {"text": "Thus, adjectives with opposite meanings can cooccur in coreference chains just as they can cooccur in window-based contexts.", "labels": [], "entities": []}, {"text": "For nouns, it is much less likely that the same coreference chain will contain both a noun and its antonym sinceby definition -markables in a coreference chain refer to the same identical entity.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Nearest neighbors of \"his\" / \"woman\" for text-based & coreference-based embeddings", "labels": [], "entities": []}, {"text": " Table 3: Results for different feature sets. Best result in each column in bold.", "labels": [], "entities": []}, {"text": " Table 2: Cosine similarity of words in the same  coreference chain", "labels": [], "entities": []}]}