{"title": [{"text": "Improving Word Alignment using Word Similarity", "labels": [], "entities": [{"text": "Improving Word Alignment", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8928061525026957}, {"text": "Word Similarity", "start_pos": 31, "end_pos": 46, "type": "TASK", "confidence": 0.6365482807159424}]}], "abstractContent": [{"text": "We show that semantic relationships can be used to improve word alignment, in addition to the lexical and syntactic features that are typically used.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 59, "end_pos": 73, "type": "TASK", "confidence": 0.7923599183559418}]}, {"text": "In this paper, we present a method based on a neural network to automatically derive word similarity from monolingual data.", "labels": [], "entities": []}, {"text": "We present an extension to word alignment models that exploits word similarity.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 27, "end_pos": 41, "type": "TASK", "confidence": 0.7953466176986694}]}, {"text": "Our experiments , in both large-scale and resource-limited settings, show improvements in word alignment tasks as well as translation tasks.", "labels": [], "entities": [{"text": "word alignment tasks", "start_pos": 90, "end_pos": 110, "type": "TASK", "confidence": 0.8580875396728516}, {"text": "translation tasks", "start_pos": 122, "end_pos": 139, "type": "TASK", "confidence": 0.9210085272789001}]}], "introductionContent": [{"text": "Word alignment is an essential step for learning translation rules in statistical machine translation.", "labels": [], "entities": [{"text": "Word alignment", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7354336231946945}, {"text": "learning translation rules", "start_pos": 40, "end_pos": 66, "type": "TASK", "confidence": 0.7085114320119222}, {"text": "statistical machine translation", "start_pos": 70, "end_pos": 101, "type": "TASK", "confidence": 0.6442109843095144}]}, {"text": "The task is to find word-level translation correspondences in parallel text.", "labels": [], "entities": [{"text": "word-level translation correspondences", "start_pos": 20, "end_pos": 58, "type": "TASK", "confidence": 0.7193479736646017}]}, {"text": "Formally, given a source sentence e consisting of words e 1 , e 2 , . .", "labels": [], "entities": []}, {"text": ", e land a target sentence f consisting of words f 1 , f 2 , . .", "labels": [], "entities": []}, {"text": ", f m , we want to infer an alignment a, a sequence of indices a 1 , a 2 , . .", "labels": [], "entities": []}, {"text": ", am which indicates, for each target word f i , the corresponding source word ea i or a null word.", "labels": [], "entities": []}, {"text": "Machine translation systems, including state-of-the-art systems, then use the word-aligned corpus to extract translation rules.", "labels": [], "entities": [{"text": "Machine translation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7048183232545853}]}, {"text": "The most widely used methods, the IBM models () and HMM (, define a probability distribution p(f , a | e) that models how each target word f i is generated from a source word ea i with respect to an alignment a.", "labels": [], "entities": []}, {"text": "The models, however, tend to misalign low-frequency words as they have insufficient training samples.", "labels": [], "entities": []}, {"text": "The problem can get worse in low-resource languages.", "labels": [], "entities": []}, {"text": "Two branches of research have tried to alleviate the problem.", "labels": [], "entities": []}, {"text": "The \u2020 Most of the work reported here was performed while the second author was at the University of Southern California.", "labels": [], "entities": []}, {"text": "first branch relies solely on the parallel data; however, additional assumptions about the data are required.", "labels": [], "entities": []}, {"text": "This includes, but is not limited to, applying prior distributions () or smoothing techniques ().", "labels": [], "entities": []}, {"text": "The other branch uses information learned from monolingual data, which is generally easier to acquire than parallel data.", "labels": [], "entities": []}, {"text": "Previous work in this branch mostly involves applying syntactic constraints and syntactic features () into the models.", "labels": [], "entities": []}, {"text": "The use of syntactic relationships can, however, be limited between historically unrelated language pairs.", "labels": [], "entities": []}, {"text": "Our motivation lies in the fact that a meaningful sentence is not merely a grammatically structured sentence; its semantics can provide insightful information for the task.", "labels": [], "entities": []}, {"text": "For example, suppose that the models are uncertain about aligning e to f . If the models are informed that e is semantically related toe , f is semantically related to f , and f is a translation of e , it should intuitively increase the probability that f is a translation of e.", "labels": [], "entities": []}, {"text": "Our work focuses on using such a semantic relationship, in particular, word similarity, to improve word alignments.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 99, "end_pos": 114, "type": "TASK", "confidence": 0.757346898317337}]}, {"text": "In this paper, we propose a method to learn similar words from monolingual data (Section 2) and an extension to word alignment models in which word similarity can be incorporated (Section 3).", "labels": [], "entities": [{"text": "word alignment", "start_pos": 112, "end_pos": 126, "type": "TASK", "confidence": 0.7477318942546844}]}, {"text": "We demonstrate its application in word alignment and translation (Section 4) and then briefly discuss the novelty of our work in comparison to other methods (Section 5).", "labels": [], "entities": [{"text": "word alignment", "start_pos": 34, "end_pos": 48, "type": "TASK", "confidence": 0.8163111507892609}]}], "datasetContent": [{"text": "We conducted word alignment experiments on 2 language pairs: Chinese-English and Arabic-English.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 13, "end_pos": 27, "type": "TASK", "confidence": 0.7797325253486633}]}, {"text": "For Chinese-English, we used 9.5M+12.3M words of parallel text from the NIST 2009 constrained task 1 and evaluated on 39.6k+50.9k words of hand-aligned data (LDC2010E63, LDC2010E37).", "labels": [], "entities": [{"text": "NIST 2009 constrained task 1", "start_pos": 72, "end_pos": 100, "type": "DATASET", "confidence": 0.9199181199073792}]}, {"text": "For ArabicEnglish, we used 4.2M+5.4M words of parallel text from the NIST 2009 constrained task 2 and evaluated on 10.7k+15.1k words of handaligned data (LDC2006E86).", "labels": [], "entities": [{"text": "NIST 2009 constrained task 2", "start_pos": 69, "end_pos": 97, "type": "DATASET", "confidence": 0.901818311214447}]}, {"text": "To demonstrate performance under resource-limited settings, we additionally experimented on only the first eighth of the full data, specifically, 1.2M+1.6M words for Chinese-English and 1.0M+1.4M words for Arabic-English.", "labels": [], "entities": []}, {"text": "We trained word similarity models on the Xinhua portions of English Gigaword (LDC2007T07), Chinese Gigaword (LDC2007T38), and Arabic Gigaword (LDC2011T1), which are 402M, 323M, and 125M words, respectively.", "labels": [], "entities": []}, {"text": "The vocabulary V was the 30,000 most frequent words from each corpus Source word frequency and the k = 10 most similar words were used.", "labels": [], "entities": []}, {"text": "We modified GIZA++ () to incorporate word similarity.", "labels": [], "entities": []}, {"text": "For all experiments, we used the default configuration of GIZA++: 5 iterations each of IBM Model 1, 2, HMM, 3 and 4.", "labels": [], "entities": []}, {"text": "We aligned the parallel texts in both forward and backward directions and symmetrized them using grow-diag-final-and ().", "labels": [], "entities": []}, {"text": "We evaluated alignment quality using precision, recall, and F1.", "labels": [], "entities": [{"text": "alignment", "start_pos": 13, "end_pos": 22, "type": "TASK", "confidence": 0.8880365490913391}, {"text": "precision", "start_pos": 37, "end_pos": 46, "type": "METRIC", "confidence": 0.999733030796051}, {"text": "recall", "start_pos": 48, "end_pos": 54, "type": "METRIC", "confidence": 0.9996486902236938}, {"text": "F1", "start_pos": 60, "end_pos": 62, "type": "METRIC", "confidence": 0.9998533725738525}]}, {"text": "The results in suggest that our modeling approach produces better word alignments.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 66, "end_pos": 81, "type": "TASK", "confidence": 0.7346362471580505}]}, {"text": "We found that our models not only learned smoother translation models for low frequency words but also ranked the conditional probabilities more accurately with respect to the correct translations.", "labels": [], "entities": []}, {"text": "To illustrate this, we categorized the alignment links from the Chinese-English low-resource experiment into bins with respect to the English source word frequency and individually evaluated them.", "labels": [], "entities": []}, {"text": "As shown in, the gain for low frequency words is particularly large.", "labels": [], "entities": []}, {"text": "We also ran end-to-end translation experiments.", "labels": [], "entities": []}, {"text": "For both languages, we used subsets of the NIST   build a hierarchical phrase-based translation system (Chiang, 2007) trained using MIRA.", "labels": [], "entities": [{"text": "NIST", "start_pos": 43, "end_pos": 47, "type": "DATASET", "confidence": 0.9472302794456482}, {"text": "phrase-based translation", "start_pos": 71, "end_pos": 95, "type": "TASK", "confidence": 0.6838590204715729}, {"text": "MIRA", "start_pos": 132, "end_pos": 136, "type": "METRIC", "confidence": 0.507926344871521}]}, {"text": "Then, we evaluated the translation quality using BLEU () and ME-TEOR, and performed significance testing using bootstrap resampling) with 1,000 samples.", "labels": [], "entities": [{"text": "translation", "start_pos": 23, "end_pos": 34, "type": "TASK", "confidence": 0.9512885808944702}, {"text": "BLEU", "start_pos": 49, "end_pos": 53, "type": "METRIC", "confidence": 0.9987139701843262}, {"text": "ME-TEOR", "start_pos": 61, "end_pos": 68, "type": "METRIC", "confidence": 0.946025013923645}]}, {"text": "Under the resource-limited settings, our methods consistently show 1.1-1.3 BLEU (0.8-1.2 METEOR) improvements on Chinese-English and 0.8-0.9 BLEU (0.8-0.9 METEOR) improvements on Arabic-English, as shown in.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 75, "end_pos": 79, "type": "METRIC", "confidence": 0.9965019226074219}, {"text": "METEOR", "start_pos": 89, "end_pos": 95, "type": "METRIC", "confidence": 0.7704548835754395}, {"text": "BLEU", "start_pos": 141, "end_pos": 145, "type": "METRIC", "confidence": 0.9978033900260925}]}, {"text": "These improvements are statistically significant (p < 0.01).", "labels": [], "entities": []}, {"text": "On the full data, our method improves ChineseEnglish translation by 0.3-0.5 BLEU (0.3 ME-TEOR), which is unfortunately not statistically significant, and Arabic-English translation by 0.5-0.6 BLEU (0.5 METEOR), which is statistically significant (p < 0.01).", "labels": [], "entities": [{"text": "ChineseEnglish translation", "start_pos": 38, "end_pos": 64, "type": "TASK", "confidence": 0.6077430248260498}, {"text": "BLEU", "start_pos": 76, "end_pos": 80, "type": "METRIC", "confidence": 0.997769832611084}, {"text": "ME-TEOR", "start_pos": 86, "end_pos": 93, "type": "METRIC", "confidence": 0.804585874080658}, {"text": "BLEU", "start_pos": 192, "end_pos": 196, "type": "METRIC", "confidence": 0.9972600936889648}, {"text": "METEOR", "start_pos": 202, "end_pos": 208, "type": "METRIC", "confidence": 0.9334491491317749}]}], "tableCaptions": [{"text": " Table 2: Assuming that word similarity is sym- metric, i.e. p(f | f ) \u2248 p(f | f ), works as well  as computing p(f | f ) using Bayes' rule.", "labels": [], "entities": []}, {"text": " Table 3: Experimental results. Our model improves alignments and translations on both language pairs.", "labels": [], "entities": []}]}