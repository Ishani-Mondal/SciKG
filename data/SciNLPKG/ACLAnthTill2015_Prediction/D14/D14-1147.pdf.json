{"title": [{"text": "Predicting Chinese Abbreviations with Minimum Semantic Unit and Global Constraints", "labels": [], "entities": [{"text": "Predicting Chinese Abbreviations", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.6233264406522115}]}], "abstractContent": [{"text": "We propose anew Chinese abbreviation prediction method which can incorporate rich local information while generating the abbreviation globally.", "labels": [], "entities": [{"text": "Chinese abbreviation prediction", "start_pos": 16, "end_pos": 47, "type": "TASK", "confidence": 0.563678503036499}]}, {"text": "Different to previous character tagging methods, we introduce the minimum semantic unit, which is more fine-grained than character but more coarse-grained than word, to capture word level information in the sequence labeling framework.", "labels": [], "entities": [{"text": "character tagging", "start_pos": 22, "end_pos": 39, "type": "TASK", "confidence": 0.7243284285068512}, {"text": "sequence labeling", "start_pos": 207, "end_pos": 224, "type": "TASK", "confidence": 0.6476916968822479}]}, {"text": "To solve the \"character dupli-cation\" problem in Chinese abbreviation prediction, we also use a substring tagging strategy to generate local substring tagging candidates.", "labels": [], "entities": [{"text": "Chinese abbreviation prediction", "start_pos": 49, "end_pos": 80, "type": "TASK", "confidence": 0.6184953848520914}]}, {"text": "We use an integer linear programming (ILP) formulation with various constraints to globally decode the final abbreviation from the generated candidates.", "labels": [], "entities": []}, {"text": "Experiments show that our method outper-forms the state-of-the-art systems, without using any extra resource.", "labels": [], "entities": []}], "introductionContent": [{"text": "Abbreviation is defined as a shortened description of the original fully expanded form.", "labels": [], "entities": [{"text": "Abbreviation", "start_pos": 0, "end_pos": 12, "type": "METRIC", "confidence": 0.9585124850273132}]}, {"text": "For example, \"NLP\" is the abbreviation for the corresponding full form \"Natural Language Processing\".", "labels": [], "entities": []}, {"text": "The existence of abbreviations makes it difficult to identify the terms conveying the same concept in the information retrieval (IR) systems and machine translation (MT) systems.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 145, "end_pos": 169, "type": "TASK", "confidence": 0.8421042919158935}]}, {"text": "Therefore, it is important to maintain a dictionary of the prevalent original full forms and the corresponding abbreviations.", "labels": [], "entities": []}, {"text": "Previous works on Chinese abbreviation generation focus on the sequence labeling method, which give each character in the full form an extra label to indicate whether it is kept in the abbreviation.", "labels": [], "entities": [{"text": "Chinese abbreviation generation", "start_pos": 18, "end_pos": 49, "type": "TASK", "confidence": 0.6834486921628317}]}, {"text": "One drawback of the character tagging strategy is that Chinese characters only contain limited amount of information.", "labels": [], "entities": [{"text": "character tagging", "start_pos": 20, "end_pos": 37, "type": "TASK", "confidence": 0.811757355928421}]}, {"text": "Using characterbased method alone is not enough for Chinese abbreviation generation.", "labels": [], "entities": [{"text": "Chinese abbreviation generation", "start_pos": 52, "end_pos": 83, "type": "TASK", "confidence": 0.7163779735565186}]}, {"text": "Intuitively we can think of a word as the basic tagging unit to incorporate more information.", "labels": [], "entities": []}, {"text": "However, if the basic tagging unit is word, we need to design lots of tags to represent which characters are kept for each unit.", "labels": [], "entities": []}, {"text": "For a word with n characters, we should design at least 2 n labels to coverall possible situations.", "labels": [], "entities": []}, {"text": "This reduces the generalization ability of the proposed model.", "labels": [], "entities": []}, {"text": "Besides, the Chinese word segmentation errors may also hurt the performance.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 13, "end_pos": 38, "type": "TASK", "confidence": 0.5700002213319143}]}, {"text": "Therefore we propose the idea of \"Minimum Semantic Unit\" (MSU) which is the minimum semantic unit in Chinese language.", "labels": [], "entities": [{"text": "Minimum Semantic Unit\" (MSU)", "start_pos": 34, "end_pos": 62, "type": "METRIC", "confidence": 0.549987929207938}]}, {"text": "Some of the MSUs are words, while others are more fine-grained than words.", "labels": [], "entities": []}, {"text": "The task of selecting representative characters in the full form can be further broken down into selecting representative characters in the MSUs.", "labels": [], "entities": [{"text": "MSUs", "start_pos": 140, "end_pos": 144, "type": "DATASET", "confidence": 0.8606857061386108}]}, {"text": "We model this using the MSU-based tagging method, which can both utilize semantic information while keeping the tag set small.", "labels": [], "entities": [{"text": "MSU-based tagging", "start_pos": 24, "end_pos": 41, "type": "TASK", "confidence": 0.5900694727897644}]}, {"text": "Meanwhile, the sequence labeling method performs badly when the \"character duplication\" phenomenon exists.", "labels": [], "entities": [{"text": "sequence labeling", "start_pos": 15, "end_pos": 32, "type": "TASK", "confidence": 0.7248068451881409}, {"text": "character duplication\"", "start_pos": 65, "end_pos": 87, "type": "TASK", "confidence": 0.7561366458733877}]}, {"text": "Many Chinese long phrases contain duplicated characters, which we refer to as the \"character duplication\" phenomenon.", "labels": [], "entities": []}, {"text": "There is no sound criterion for the character tagging models to decide which of the duplicated character should be kept in the abbreviation and which one to be skipped.", "labels": [], "entities": [{"text": "character tagging", "start_pos": 36, "end_pos": 53, "type": "TASK", "confidence": 0.7504636347293854}]}, {"text": "An example is \" \"(Beijing University of Aeronautics and Astronautics) whose abbreviation is \"\".", "labels": [], "entities": []}, {"text": "The character \"\" appears twice in the full form and only one is kept in the abbreviation.", "labels": [], "entities": []}, {"text": "In these cases, we can break the long phase into local substrings.", "labels": [], "entities": []}, {"text": "We can find the representative characters in the substrings instead of the long full form and let the decoding phase to integrate useful information globally.", "labels": [], "entities": []}, {"text": "We utilize this sub-string based approach and obtain this local tagging information by labeling on the sub-string of the full character sequence.", "labels": [], "entities": []}, {"text": "Given the MSU-based and substring-based methods mentioned above, we can get a list of potential abbreviation candidates.", "labels": [], "entities": [{"text": "MSU-based", "start_pos": 10, "end_pos": 19, "type": "DATASET", "confidence": 0.9099308848381042}]}, {"text": "Some of these candidates may not agree on keeping or skipping of some specific characters.", "labels": [], "entities": []}, {"text": "To integrate their advantages while considering the consistency, we further propose a global decoding strategy using Integer Linear Programming(ILP).", "labels": [], "entities": []}, {"text": "The constraints in ILP can naturally incorporate 'non-local' information in contrast to probabilistic constraints that are estimated from training examples.", "labels": [], "entities": []}, {"text": "We can also use linguistic constraints like \"adjacent identical characters is not allowed\" to decode the correct abbreviation in examples like the previous \"\" example.", "labels": [], "entities": []}, {"text": "Experiments show that our Chinese abbreviation prediction system outperforms the state-ofthe-art systems.", "labels": [], "entities": [{"text": "Chinese abbreviation prediction", "start_pos": 26, "end_pos": 57, "type": "TASK", "confidence": 0.5681843956311544}]}, {"text": "In order to reduce the size of the search space, we further propose pruning constraints that are learnt from the training corpus.", "labels": [], "entities": []}, {"text": "Experiment shows that the average number of constraints is reduced by about 30%, while the top-1 accuracy is not affected.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 97, "end_pos": 105, "type": "METRIC", "confidence": 0.9802290201187134}]}, {"text": "The paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 1 gives the introduction.", "labels": [], "entities": []}, {"text": "In section 2 we describe our method, including the MSUs, the substringbased tagging strategy and the ILP decoding process.", "labels": [], "entities": []}, {"text": "Experiments are described in section 3.", "labels": [], "entities": []}, {"text": "We also give a detailed analysis of the results in section 3.", "labels": [], "entities": []}, {"text": "In section 4 related works are introduced, and the paper is concluded in the last section.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use the abbreviation corpus provided by Institute of Computational Linguistics (ICL) of Peking University in our experiments.", "labels": [], "entities": []}, {"text": "The corpus is similar to the corpus used in;.", "labels": [], "entities": []}, {"text": "It contains 8, 015 Chinese abbreviations, including noun phrases, organization names and some other types.", "labels": [], "entities": []}, {"text": "Some examples are presented in TABLE 8.", "labels": [], "entities": [{"text": "TABLE 8", "start_pos": 31, "end_pos": 38, "type": "DATASET", "confidence": 0.8054211735725403}]}, {"text": "We use 80% abbreviations as training data and the rest as testing data.", "labels": [], "entities": []}, {"text": "In some cases, along phrase may contain more than one abbreviation.", "labels": [], "entities": []}, {"text": "For these cases, the corpus just keeps their most commonly used abbreviation for each full form.", "labels": [], "entities": []}, {"text": "The evaluation metric used in our experiment is the top-K accuracy, which is also used by, and if R i and S j have a same position but the position gets different labels, then y i + x j \u2264 1 5.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.9126595258712769}]}, {"text": "\u2200T i \u2208 T , S j \u2208 S, if Ti and S j have a same position but the position gets different labels, then z i + x j \u2264 1 6.", "labels": [], "entities": []}, {"text": "\u2200R i \u2208 R, T j \u2208 T , if R i and T j have a same position but the position gets different labels, then xi + z j \u2264 1 7.", "labels": [], "entities": []}, {"text": "\u2200S i , S j \u2208 S if Si and S j have a same position but the position gets different labels, then z i + z j \u2264 1 8.", "labels": [], "entities": []}, {"text": "\u2200S i , S j \u2208 S if the last character Si keeps is the same as the first character S j keeps, then z i + z j \u2264 1  In our experiment, top-10 candidates are considered in re-ranking phrase and the measurement used is top-1 accuracy (which is the accuracy we usually refer to) because the final aim of the algorithm is to detect the exact abbreviation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 219, "end_pos": 227, "type": "METRIC", "confidence": 0.9734528064727783}, {"text": "accuracy", "start_pos": 242, "end_pos": 250, "type": "METRIC", "confidence": 0.9771366715431213}]}, {"text": "CRF++ , an open source linear chain CRF tool, is used in the sequence labeling part.", "labels": [], "entities": [{"text": "sequence labeling", "start_pos": 61, "end_pos": 78, "type": "TASK", "confidence": 0.636261448264122}]}, {"text": "For ILP part, we use lpsolve 8 , which is also an open source tool.", "labels": [], "entities": []}, {"text": "The parameters of these tools are tuned through cross-validation on the training data.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 8: Examples of the corpus (Noun Phrase, Organization, Coordinate Phrase, Proper Noun)", "labels": [], "entities": []}, {"text": " Table 9: Top-K (K \u2264 5) results of character-based  tagging and MSU-based tagging", "labels": [], "entities": [{"text": "character-based  tagging", "start_pos": 35, "end_pos": 59, "type": "TASK", "confidence": 0.5922185331583023}, {"text": "MSU-based tagging", "start_pos": 64, "end_pos": 81, "type": "TASK", "confidence": 0.6000249683856964}]}, {"text": " Table 14: Comparison of testing time of raw input  and pruned input", "labels": [], "entities": []}, {"text": " Table 15: Comparison with the state-of-the-art  systems", "labels": [], "entities": []}]}