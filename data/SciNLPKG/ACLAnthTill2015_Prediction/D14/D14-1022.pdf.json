{"title": [], "abstractContent": [{"text": "We propose a simple and effective approach to learn translation spans for the hierarchical phrase-based translation model.", "labels": [], "entities": []}, {"text": "Our model evaluates if a source span should be covered by translation rules during decoding, which is integrated into the translation system as soft constraints.", "labels": [], "entities": []}, {"text": "Compared to syntactic constraints , our model is directly acquired from an aligned parallel corpus and does not require parsers.", "labels": [], "entities": []}, {"text": "Rich source side contextual features and advanced machine learning methods were utilized for this learning task.", "labels": [], "entities": []}, {"text": "The proposed approach was evaluated on NTCIR-9 Chinese-English and Japanese-English translation tasks and showed significant improvement over the baseline system.", "labels": [], "entities": [{"text": "NTCIR-9 Chinese-English and Japanese-English translation tasks", "start_pos": 39, "end_pos": 101, "type": "TASK", "confidence": 0.777156929175059}]}], "introductionContent": [{"text": "The hierarchical phrase-based (HPB) translation model () has been widely adopted in statistical machine translation (SMT) tasks.", "labels": [], "entities": [{"text": "phrase-based (HPB) translation", "start_pos": 17, "end_pos": 47, "type": "TASK", "confidence": 0.6650588631629943}, {"text": "statistical machine translation (SMT) tasks", "start_pos": 84, "end_pos": 127, "type": "TASK", "confidence": 0.8042830654553005}]}, {"text": "The HPB translation rules based on the synchronous context free grammar (SCFG) are simple and powerful.", "labels": [], "entities": [{"text": "HPB translation", "start_pos": 4, "end_pos": 19, "type": "TASK", "confidence": 0.6696740090847015}]}, {"text": "One drawback of the HPB model is the applications of translation rules to the input sentence are highly ambiguous.", "labels": [], "entities": []}, {"text": "For example, a rule whose English side is \"X1 by X2\" can be applied to any word sequence that has \"by\" in them.", "labels": [], "entities": []}, {"text": "In, this rule can be applied to the whole sentence as well as to \"experiment by tomorrow\".", "labels": [], "entities": []}, {"text": "In order to tackle rule application ambiguities, a few previous works used syntax trees.", "labels": [], "entities": []}, {"text": "utilized a syntactic feature in the HPB model, which represents if the source span covered by a translation rule is a syntactic constituent.", "labels": [], "entities": []}, {"text": "However, the experimental results showed this feature gave no significant improvement.", "labels": [], "entities": []}, {"text": "Instead of using the undifferentiated constituency feature,) defined different soft syntactic features for different constituent types and obtained substantial performance improvement.", "labels": [], "entities": []}, {"text": "Later, () introduced joint probability synchronous grammars to integrate flexible linguistic information.", "labels": [], "entities": []}, {"text": "( proposed the soft syntactic constraint model based on discriminative classifiers for each constituent type and integrated all of them into the translation model.", "labels": [], "entities": []}, {"text": "() focused on hierarchical rule selection using many features including syntax constituents.", "labels": [], "entities": [{"text": "hierarchical rule selection", "start_pos": 14, "end_pos": 41, "type": "TASK", "confidence": 0.6069721380869547}]}, {"text": "These works have demonstrated the benefits of using syntactic features in the HPB model.", "labels": [], "entities": []}, {"text": "However, high quality syntax parsers are not always easily obtained for many languages.", "labels": [], "entities": []}, {"text": "Without this problem, word alignment constraints can also be used to guide the application of the rules.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 22, "end_pos": 36, "type": "TASK", "confidence": 0.7853486239910126}]}, {"text": "Suppose that we want to translate the English sentence into the Chinese sentence in, a translation rule can be applied to the source span \"finish this experiment by tomorrow\".", "labels": [], "entities": []}, {"text": "Nonetheless, if a rule is applied to \"experiment by\", then the Chinese translation cannot be correctly obtained, because the target span projected from \"ex-periment by\" contains words projected from the source words outside \"experiment by\".", "labels": [], "entities": []}, {"text": "In general, a translation rule projects one continuous source word sequence (source span) into one continuous target word sequence.", "labels": [], "entities": []}, {"text": "Meanwhile, the word alignment links between the source and target sentence define the source spans where translation rules are applicable.", "labels": [], "entities": []}, {"text": "In this paper, we calla source span that can be covered by a translation rule without violating word alignment links a translation span.", "labels": [], "entities": []}, {"text": "Translation spans that have been correctly identified can guide translation rules to function properly, thus) attempted to use extra machine learning approaches to determine boundaries of translation spans.", "labels": [], "entities": []}, {"text": "They used two separate classifiers to learn the beginning and ending boundaries of translation spans, respectively.", "labels": [], "entities": []}, {"text": "A source word is marked as beginning (ending) boundary if it is the first (last) word of a translation span.", "labels": [], "entities": []}, {"text": "However, a source span whose first and last words are both boundaries is not always a translation span.", "labels": [], "entities": []}, {"text": "In, \"I\" is a beginning boundary since it is the first word of translation span \"I will\" and \"experiment\" is an ending boundary since it is the last word of translation span \"finish this experiment\" , but \"I will finish this experiment\" is not a translation span.", "labels": [], "entities": []}, {"text": "This happens because the translation spans are nested or hierarchical.", "labels": [], "entities": []}, {"text": "Note that () also learned phrase boundaries to constrain decoding, but their approach identified boundaries only for monotone translation.", "labels": [], "entities": []}, {"text": "In this paper, taking fully into account that translation spans being nested, we propose an approach to learn hierarchical translation spans directly from an aligned parallel corpus that makes more accurate identification over translation spans.", "labels": [], "entities": []}, {"text": "The rest of the paper is structured as follows: In Section 2, we briefly review the HPB translation model.", "labels": [], "entities": [{"text": "HPB translation", "start_pos": 84, "end_pos": 99, "type": "TASK", "confidence": 0.7590525150299072}]}, {"text": "Section 3 describes our approach.", "labels": [], "entities": []}, {"text": "We describe experiments in Section 4 and conclude in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated the effectiveness of the proposed approach for Chinese-to-English (CE) and Japaneseto-English (JE) translation tasks.", "labels": [], "entities": [{"text": "Japaneseto-English (JE) translation tasks", "start_pos": 88, "end_pos": 129, "type": "TASK", "confidence": 0.6478881786266962}]}, {"text": "The datasets officially provided for the patent machine translation task at NTCIR-9 (Goto et al., 2011) were used in our experiments.", "labels": [], "entities": [{"text": "machine translation task", "start_pos": 48, "end_pos": 72, "type": "TASK", "confidence": 0.7616696655750275}, {"text": "NTCIR-9", "start_pos": 76, "end_pos": 83, "type": "DATASET", "confidence": 0.9320648908615112}]}, {"text": "The detailed training set statistics are given in  sets were both provided for CE task while only the test set was provided for JE task.", "labels": [], "entities": [{"text": "CE", "start_pos": 79, "end_pos": 81, "type": "METRIC", "confidence": 0.5418526530265808}, {"text": "JE task", "start_pos": 128, "end_pos": 135, "type": "TASK", "confidence": 0.5024611949920654}]}, {"text": "Therefore, we used the sentences from the NTCIR-8 JE test set as the development set.", "labels": [], "entities": [{"text": "NTCIR-8 JE test set", "start_pos": 42, "end_pos": 61, "type": "DATASET", "confidence": 0.9214451760053635}]}, {"text": "Word segmentation was done by BaseSeg () for Chinese and Mecab 2 for Japanese.", "labels": [], "entities": [{"text": "Word segmentation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.6720139235258102}]}, {"text": "To learn the classifiers for each translation task, the training set and development set were put together to obtain symmetric word alignment using GIZA++ and the growdiag-final-and heuristic ().", "labels": [], "entities": [{"text": "symmetric word alignment", "start_pos": 117, "end_pos": 141, "type": "TASK", "confidence": 0.6502481202284495}]}, {"text": "The source span instances extracted from the aligned training and development sets were used as the training and validation data for the classifiers.", "labels": [], "entities": []}, {"text": "The toolkit) was adopted to train ME classifiers using the classical quasi-newton optimization algorithm with limited memory.", "labels": [], "entities": [{"text": "ME classifiers", "start_pos": 34, "end_pos": 48, "type": "TASK", "confidence": 0.7739618420600891}]}, {"text": "The NNs are trained by the toolkit NPLM ().", "labels": [], "entities": [{"text": "NPLM", "start_pos": 35, "end_pos": 39, "type": "DATASET", "confidence": 0.8916128873825073}]}, {"text": "We chose \"rectifier\" as the activation function and the logarithmic loss function for NNs.", "labels": [], "entities": []}, {"text": "The number of epochs was set to 20.", "labels": [], "entities": []}, {"text": "Other parameters were set to default: Classification accuracies.", "labels": [], "entities": [{"text": "Classification", "start_pos": 38, "end_pos": 52, "type": "TASK", "confidence": 0.9206946492195129}, {"text": "accuracies", "start_pos": 53, "end_pos": 63, "type": "METRIC", "confidence": 0.7186194062232971}]}, {"text": "The Rate column represents ratio of positive instances to negative instances; the P and N columns give classification accuracies for positive and negative instances. values.", "labels": [], "entities": []}, {"text": "The training time of one classifier on a 12-core 3.47GHz Xeon X5690 machine was 0.5h (2.5h) using ME (NN) approach for CE task; 1h (4h) using ME (NN) approach for JE task . The classification results are shown in.", "labels": [], "entities": [{"text": "JE task", "start_pos": 163, "end_pos": 170, "type": "TASK", "confidence": 0.7398294508457184}]}, {"text": "Instead of the undifferentiated classification accuracy, we present separate classification accuracies for positive and negative instances.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.9201849102973938}]}, {"text": "The big difference between classification accuracies for positive and negative instances was caused by the unbalanced rate of positive and negative instances in the training corpus.", "labels": [], "entities": []}, {"text": "For example, if there are more positive training instances, then the classifier will tend to classify new instances as positive and the classification accuracy for positive instances will be higher.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 151, "end_pos": 159, "type": "METRIC", "confidence": 0.9828579425811768}]}, {"text": "In our classification tasks, there are less positive instances for longer span lengths.", "labels": [], "entities": []}, {"text": "Since the word order difference of JE task is much more significant than that of CE task, there are more negative Japanese translation span instances than Chinese.", "labels": [], "entities": [{"text": "JE task", "start_pos": 35, "end_pos": 42, "type": "TASK", "confidence": 0.6823430359363556}]}, {"text": "In JE tasks, the ME classifiers C 8 , C 9 and C 10 predicted all new instances to be negative due to the heavily unbalanced instance distribution.", "labels": [], "entities": [{"text": "JE tasks", "start_pos": 3, "end_pos": 11, "type": "TASK", "confidence": 0.8281484842300415}]}, {"text": "As shown in, NN outperformed ME approach for our classification tasks.", "labels": [], "entities": [{"text": "ME", "start_pos": 29, "end_pos": 31, "type": "METRIC", "confidence": 0.9881200790405273}, {"text": "classification tasks", "start_pos": 49, "end_pos": 69, "type": "TASK", "confidence": 0.8903397619724274}]}, {"text": "As the span length growing, the advantage of NN became more significant.", "labels": [], "entities": []}, {"text": "Since the classification accuracies deceased to be quite low for source spans with more than 10 words, only {C 1 , ..., C 10 } were integrated into the HPB translation system.", "labels": [], "entities": [{"text": "HPB translation", "start_pos": 152, "end_pos": 167, "type": "TASK", "confidence": 0.6451788395643234}]}, {"text": "For each translation task, the recent version of Moses HPB decoder ( with the training scripts was used as the baseline (Base).", "labels": [], "entities": [{"text": "translation task", "start_pos": 9, "end_pos": 25, "type": "TASK", "confidence": 0.9182619154453278}, {"text": "Moses HPB decoder", "start_pos": 49, "end_pos": 66, "type": "DATASET", "confidence": 0.9627683560053507}]}, {"text": "We used the default parameters for Moses, and a 5-gram language model was trained on the target side of the training corpus by IRST LM Toolkit 3 with improved Kneser-Ney smoothing.", "labels": [], "entities": [{"text": "IRST LM Toolkit 3", "start_pos": 127, "end_pos": 144, "type": "DATASET", "confidence": 0.9249529093503952}]}, {"text": "{C 1 , ..., C 10 } were integrated into the baseline with different weights, which were tuned by MERT together with other feature weights (language model, word penalty,...) under the log-linear framework: Translation results.", "labels": [], "entities": [{"text": "MERT", "start_pos": 97, "end_pos": 101, "type": "DATASET", "confidence": 0.5488301515579224}, {"text": "Translation", "start_pos": 205, "end_pos": 216, "type": "TASK", "confidence": 0.9663147330284119}]}, {"text": "The symbol ++ (--) represents a significant difference at the p < 0.01 level and -represents a significant difference at the p < 0.05 level against the BLM.", "labels": [], "entities": [{"text": "BLM", "start_pos": 152, "end_pos": 155, "type": "DATASET", "confidence": 0.9534485340118408}]}, {"text": "We compare our method with the baseline and the boundary learning method (BLM) () based on Maximum Entropy Markov Models with Markov order 2.", "labels": [], "entities": [{"text": "boundary learning method (BLM)", "start_pos": 48, "end_pos": 78, "type": "METRIC", "confidence": 0.6700789630413055}]}, {"text": "reports BLEU () and TER) scores.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 8, "end_pos": 12, "type": "METRIC", "confidence": 0.9991033673286438}, {"text": "TER) scores", "start_pos": 20, "end_pos": 31, "type": "METRIC", "confidence": 0.963416854540507}]}, {"text": "Significance tests are conducted using bootstrap sampling.", "labels": [], "entities": []}, {"text": "Our ME classifiers achieve comparable translation improvement with the BLM and NN classifiers enhance translation system significantly compared to others.", "labels": [], "entities": [{"text": "BLM", "start_pos": 71, "end_pos": 74, "type": "METRIC", "confidence": 0.6750104427337646}]}, {"text": "also shows that the relative gain was higher for higher n-grams, which is reasonable since the higher n-grams have higher ambiguities in the translation rule application.", "labels": [], "entities": []}, {"text": "It is true that because of multiple parallel sentences, a source span can be applied with transla-tion rules in one sentence pair but not in another sentence pair.", "labels": [], "entities": []}, {"text": "So we used the probability score as a feature in the decoding.", "labels": [], "entities": [{"text": "probability score", "start_pos": 15, "end_pos": 32, "type": "METRIC", "confidence": 0.9310190379619598}]}, {"text": "That is, we did not use classification results directly but use the probability score for softly constraining the decoding process.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Classification accuracies. The Rate column represents ratio of positive instances to negative  instances; the P and N columns give classification accuracies for positive and negative instances.", "labels": [], "entities": []}, {"text": " Table 3: Translation results. The symbol ++ (--)  represents a significant difference at the p < 0.01  level and -represents a significant difference at the  p < 0.05 level against the BLM.", "labels": [], "entities": [{"text": "Translation", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.9835365414619446}, {"text": "BLM", "start_pos": 186, "end_pos": 189, "type": "DATASET", "confidence": 0.9359291791915894}]}]}