{"title": [{"text": "#TAGSPACE: Semantic Embeddings from Hashtags", "labels": [], "entities": [{"text": "TAGSPACE", "start_pos": 1, "end_pos": 9, "type": "METRIC", "confidence": 0.9115434288978577}, {"text": "Hashtags", "start_pos": 36, "end_pos": 44, "type": "DATASET", "confidence": 0.4159678518772125}]}], "abstractContent": [{"text": "We describe a convolutional neural network that learns feature representations for short textual posts using hashtags as a supervised signal.", "labels": [], "entities": []}, {"text": "The proposed approach is trained on up to 5.5 billion words predicting 100,000 possible hashtags.", "labels": [], "entities": []}, {"text": "As well as strong performance on the hashtag prediction task itself, we show that its learned representation of text (ignoring the hash-tag labels) is useful for other tasks as well.", "labels": [], "entities": [{"text": "hashtag prediction task", "start_pos": 37, "end_pos": 60, "type": "TASK", "confidence": 0.8524601658185323}]}, {"text": "To that end, we present results on a document recommendation task, where it also outperforms a number of baselines.", "labels": [], "entities": [{"text": "document recommendation task", "start_pos": 37, "end_pos": 65, "type": "TASK", "confidence": 0.7905631264050802}]}], "introductionContent": [{"text": "Hashtags (single tokens often composed of natural language n-grams or abbreviations, prefixed with the character '#') are ubiquitous on social networking services, particularly in short textual documents (a.k.a. posts).", "labels": [], "entities": []}, {"text": "Authors use hashtags to diverse ends, many of which can be seen as labels for classical NLP tasks: disambiguation (chips #futurism vs. chips #junkfood); identification of named entities (#sf49ers); sentiment (#dislike); and topic annotation (#yoga).", "labels": [], "entities": [{"text": "identification of named entities", "start_pos": 153, "end_pos": 185, "type": "TASK", "confidence": 0.8633594959974289}]}, {"text": "Hashtag prediction is the task of mapping text to its accompanying hashtags.", "labels": [], "entities": [{"text": "Hashtag prediction", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9390718638896942}]}, {"text": "In this work we propose a novel model for hashtag prediction, and show that this task is also a useful surrogate for learning good representations of text.", "labels": [], "entities": [{"text": "hashtag prediction", "start_pos": 42, "end_pos": 60, "type": "TASK", "confidence": 0.8735081255435944}]}, {"text": "Latent representations, or embeddings, are vectorial representations of words or documents, traditionally learned in an unsupervised manner overlarge corpora.", "labels": [], "entities": []}, {"text": "For example LSA) and its variants, and more recent neuralnetwork inspired methods like those of,  and word2vec () learn word embeddings.", "labels": [], "entities": []}, {"text": "In the word embedding paradigm, each word is represented as a vector in Rn , where n is a hyperparameter that controls capacity.", "labels": [], "entities": []}, {"text": "The embeddings of words comprising a text are combined using a model-dependent, possibly learned function, producing a point in the same embedding space.", "labels": [], "entities": []}, {"text": "A similarity measure (for example, inner product) gauges the pairwise relevance of points in the embedding space.", "labels": [], "entities": []}, {"text": "Unsupervised word embedding methods train with a reconstruction objective in which the embeddings are used to predict the original text.", "labels": [], "entities": []}, {"text": "For example, word2vec tries to predict all the words in the document, given the embeddings of surrounding words.", "labels": [], "entities": []}, {"text": "We argue that hashtag prediction provides a more direct form of supervision: the tags area labeling by the author of the salient aspects of the text.", "labels": [], "entities": [{"text": "hashtag prediction", "start_pos": 14, "end_pos": 32, "type": "TASK", "confidence": 0.7437052428722382}]}, {"text": "Hence, predicting them may provide stronger semantic guidance than unsupervised learning alone.", "labels": [], "entities": [{"text": "predicting them", "start_pos": 7, "end_pos": 22, "type": "TASK", "confidence": 0.9086098968982697}]}, {"text": "The abundance of hashtags in real posts provides a huge labeled dataset for learning potentially sophisticated models.", "labels": [], "entities": []}, {"text": "In this work we develop a convolutional network for large scale ranking tasks, and apply it to hashtag prediction.", "labels": [], "entities": [{"text": "hashtag prediction", "start_pos": 95, "end_pos": 113, "type": "TASK", "confidence": 0.8732272386550903}]}, {"text": "Our model represents both words and the entire textual post as embeddings as intermediate steps.", "labels": [], "entities": []}, {"text": "We show that our method outperforms existing unsupervised (word2vec) and supervised (WSABIE (Weston et al., 2011)) embedding methods, and other baselines, at the hashtag prediction task.", "labels": [], "entities": [{"text": "WSABIE (Weston et al., 2011)) embedding", "start_pos": 85, "end_pos": 124, "type": "DATASET", "confidence": 0.8988757133483887}, {"text": "hashtag prediction task", "start_pos": 162, "end_pos": 185, "type": "TASK", "confidence": 0.8259847561518351}]}, {"text": "We then probe our model's generality, by transfering its learned representations to the task of personalized document recommendation: for each of M users, given N previous positive interactions with documents (likes, clicks, etc.), predict the N + 1'th document the user will positively interact with.", "labels": [], "entities": [{"text": "personalized document recommendation", "start_pos": 96, "end_pos": 132, "type": "TASK", "confidence": 0.6203646063804626}]}, {"text": "To perform well on this task, the representation should capture the user's interest in textual content.", "labels": [], "entities": []}, {"text": "We find representations trained on hashtag prediction outperform representations from unsupervised learning, and that our convolu-: #TAGSPACE convolutional network f (w, t) for scoring a (document, hashtag) pair.", "labels": [], "entities": [{"text": "hashtag prediction", "start_pos": 35, "end_pos": 53, "type": "TASK", "confidence": 0.8428922891616821}, {"text": "TAGSPACE", "start_pos": 133, "end_pos": 141, "type": "METRIC", "confidence": 0.8862479329109192}]}, {"text": "tional architecture performs better than WSABIE trained on the same hashtag task.", "labels": [], "entities": [{"text": "WSABIE", "start_pos": 41, "end_pos": 47, "type": "DATASET", "confidence": 0.6535415053367615}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Datasets used in hashtag prediction.", "labels": [], "entities": [{"text": "hashtag prediction", "start_pos": 27, "end_pos": 45, "type": "TASK", "confidence": 0.9312529563903809}]}, {"text": " Table 3: Hashtag test results for people dataset.", "labels": [], "entities": [{"text": "people dataset", "start_pos": 35, "end_pos": 49, "type": "DATASET", "confidence": 0.7278857678174973}]}, {"text": " Table 4: Hashtag test results for pages dataset.", "labels": [], "entities": []}, {"text": " Table 5: Document recommendation task results.", "labels": [], "entities": [{"text": "Document recommendation task", "start_pos": 10, "end_pos": 38, "type": "TASK", "confidence": 0.8641901810963949}]}]}