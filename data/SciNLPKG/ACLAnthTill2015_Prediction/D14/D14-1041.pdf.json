{"title": [], "abstractContent": [{"text": "The current approaches to Semantic Role Labeling (SRL) usually perform role classification for each predicate separately and the interaction among individual predi-cate's role labeling is ignored if there is more than one predicate in a sentence.", "labels": [], "entities": [{"text": "Semantic Role Labeling (SRL)", "start_pos": 26, "end_pos": 54, "type": "TASK", "confidence": 0.8453492025534312}, {"text": "role classification", "start_pos": 71, "end_pos": 90, "type": "TASK", "confidence": 0.7403813302516937}]}, {"text": "In this paper, we prove that different predicates in a sentence could help each other during SRL.", "labels": [], "entities": [{"text": "SRL", "start_pos": 93, "end_pos": 96, "type": "TASK", "confidence": 0.9896502494812012}]}, {"text": "In multi-predicate role labeling , there are mainly two key points: argument identification and role labeling of the arguments shared by multiple predicates.", "labels": [], "entities": [{"text": "multi-predicate role labeling", "start_pos": 3, "end_pos": 32, "type": "TASK", "confidence": 0.6538181602954865}, {"text": "argument identification", "start_pos": 68, "end_pos": 91, "type": "TASK", "confidence": 0.7263902425765991}, {"text": "role labeling", "start_pos": 96, "end_pos": 109, "type": "TASK", "confidence": 0.7269129008054733}]}, {"text": "To address these issues, in the stage of argument identification, we propose novel predicate-related features which help remove many argument identification errors; in the stage of argument classification, we adopt a discriminative reranking approach to perform role classification of the shared arguments, in which a large set of global features are proposed.", "labels": [], "entities": [{"text": "argument identification", "start_pos": 41, "end_pos": 64, "type": "TASK", "confidence": 0.7290593534708023}, {"text": "argument identification", "start_pos": 133, "end_pos": 156, "type": "TASK", "confidence": 0.7379666864871979}, {"text": "argument classification", "start_pos": 181, "end_pos": 204, "type": "TASK", "confidence": 0.7285618335008621}, {"text": "role classification", "start_pos": 262, "end_pos": 281, "type": "TASK", "confidence": 0.7708568871021271}]}, {"text": "We conducted experiments on two standard benchmarks: Chinese PropBank and English PropBank.", "labels": [], "entities": [{"text": "Chinese PropBank", "start_pos": 53, "end_pos": 69, "type": "DATASET", "confidence": 0.9002748429775238}, {"text": "English PropBank", "start_pos": 74, "end_pos": 90, "type": "DATASET", "confidence": 0.9115647971630096}]}, {"text": "The experimental results show that our approach can significantly improve SRL performance, especially in Chinese Prop-Bank.", "labels": [], "entities": [{"text": "SRL", "start_pos": 74, "end_pos": 77, "type": "TASK", "confidence": 0.9882102012634277}, {"text": "Chinese Prop-Bank", "start_pos": 105, "end_pos": 122, "type": "DATASET", "confidence": 0.9190792739391327}]}], "introductionContent": [{"text": "Semantic Role Labeling (SRL) is a kind of shallow semantic parsing task and its goal is to recognize some related phrases and assign a joint structure (WHO did WHAT to WHOM, WHEN, WHERE, WHY, HOW) to each predicate of a sentence ().", "labels": [], "entities": [{"text": "Semantic Role Labeling (SRL)", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8142757068077723}, {"text": "semantic parsing task", "start_pos": 50, "end_pos": 71, "type": "TASK", "confidence": 0.8056515057881674}]}, {"text": "Because of the ability of encoding semantic information, SR-L has been applied in many tasks of NLP, such as question and answering), information extraction (Surdeanu et The justices will be forced to reconsider the questions.", "labels": [], "entities": [{"text": "question and answering", "start_pos": 109, "end_pos": 131, "type": "TASK", "confidence": 0.8038180073102316}, {"text": "information extraction", "start_pos": 134, "end_pos": 156, "type": "TASK", "confidence": 0.8227846324443817}]}, {"text": "[ A1 ] [ Pred ] [ A0 ] [ A1 ] Figure 1: A sentence from English PropBank, with an argument shared by multiple predicates al.,), and machine translation (.", "labels": [], "entities": [{"text": "Pred", "start_pos": 9, "end_pos": 13, "type": "METRIC", "confidence": 0.9736551642417908}, {"text": "English PropBank", "start_pos": 56, "end_pos": 72, "type": "DATASET", "confidence": 0.7776809334754944}, {"text": "machine translation", "start_pos": 132, "end_pos": 151, "type": "TASK", "confidence": 0.7103400677442551}]}, {"text": "Currently, an SRL system works as follows: first identify argument candidates and then perform classification for each argument candidate.", "labels": [], "entities": [{"text": "SRL", "start_pos": 14, "end_pos": 17, "type": "TASK", "confidence": 0.9668505191802979}]}, {"text": "However, this process only focuses on one independent predicate without considering the internal relations of multiple predicates in a sentence.", "labels": [], "entities": []}, {"text": "According to our statistics, more than 80% sentences in Propbank carry multiple predicates.", "labels": [], "entities": [{"text": "Propbank", "start_pos": 56, "end_pos": 64, "type": "DATASET", "confidence": 0.9127528071403503}]}, {"text": "One example is shown in, in which there are two predicates 'Force' and 'Reconsider'.", "labels": [], "entities": [{"text": "Reconsider", "start_pos": 72, "end_pos": 82, "type": "METRIC", "confidence": 0.9649894833564758}]}, {"text": "Moreover, the constituent 'the justices' is shared by the two predicates and is labeled as A1 for 'Force' but as A0 for 'Reconsider'.", "labels": [], "entities": []}, {"text": "We call this phenomenon of the shared arguments Role Transition . Intuitively, all predicates in a sentence are closely related to each other and the internal relations between them would be helpful for SRL.", "labels": [], "entities": [{"text": "SRL", "start_pos": 203, "end_pos": 206, "type": "TASK", "confidence": 0.9818344712257385}]}, {"text": "This paper has made deep investigation on multi-predicate semantic role labeling.", "labels": [], "entities": [{"text": "multi-predicate semantic role labeling", "start_pos": 42, "end_pos": 80, "type": "TASK", "confidence": 0.727384552359581}]}, {"text": "We think there are mainly two key points: argument identification and role labeling of the arguments shared by multiple predicates.", "labels": [], "entities": [{"text": "argument identification", "start_pos": 42, "end_pos": 65, "type": "TASK", "confidence": 0.7965537309646606}, {"text": "role labeling", "start_pos": 70, "end_pos": 83, "type": "TASK", "confidence": 0.7037186026573181}]}, {"text": "We adopt different strategies to address these two issues.", "labels": [], "entities": []}, {"text": "During argument identification, there area large number of identification errors caused by the poor performance of auto syntax trees.", "labels": [], "entities": [{"text": "argument identification", "start_pos": 7, "end_pos": 30, "type": "TASK", "confidence": 0.7676695883274078}]}, {"text": "However, many of these errors can be removed, if we take other predicates into consideration.", "labels": [], "entities": []}, {"text": "To achieve this purpose, we propose novel predicates-related features which have been proved to be effective to recog-nize many identification errors.", "labels": [], "entities": []}, {"text": "After these features added, the precision of argument identification improves significantly by 1.6 points and 0.9 points in experiments on Chinese PropBank and English PropBank respectively, with a slight loss in recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.9995123147964478}, {"text": "argument identification", "start_pos": 45, "end_pos": 68, "type": "TASK", "confidence": 0.7267573922872543}, {"text": "Chinese PropBank", "start_pos": 139, "end_pos": 155, "type": "DATASET", "confidence": 0.9017437696456909}, {"text": "English PropBank", "start_pos": 160, "end_pos": 176, "type": "DATASET", "confidence": 0.8375576734542847}, {"text": "recall", "start_pos": 213, "end_pos": 219, "type": "METRIC", "confidence": 0.9982181191444397}]}, {"text": "Role labeling of the shared arguments is another key point.", "labels": [], "entities": [{"text": "Role labeling", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.7508727610111237}]}, {"text": "The predicates and their shared argument could be considered as a joint structure, with strong dependencies between the shared argument's roles.", "labels": [], "entities": []}, {"text": "If we consider linguistic basis for joint modeling of the shared argument's roles, there are at least two types of information to be captured.", "labels": [], "entities": []}, {"text": "The first type of information is the compatibility of Role Transition among the shared argument's roles.", "labels": [], "entities": [{"text": "compatibility", "start_pos": 37, "end_pos": 50, "type": "METRIC", "confidence": 0.9482489228248596}]}, {"text": "A noun phrase maybe labeled as A0 fora predicate and at the same time, it can be labeled as A1 for another predicate.", "labels": [], "entities": [{"text": "A1", "start_pos": 92, "end_pos": 94, "type": "METRIC", "confidence": 0.9581581950187683}]}, {"text": "However, there are few cases that a noun phrase is labeled as A0 fora predicate and as AM-ADV for another predicate at the same time.", "labels": [], "entities": []}, {"text": "Secondly, joint modeling the shared arguments could explore global information.", "labels": [], "entities": []}, {"text": "Motivated by the above observations, we attempt to jointly model the shared arguments' roles.", "labels": [], "entities": []}, {"text": "Specifically, we utilize the discriminative reranking approach that has been successfully employed in many NLP tasks.", "labels": [], "entities": []}, {"text": "Typically, this method first creates a list of n-best candidates from abase system, and then reranks them with arbitrary features (both local and global), which are either not computable or are computationally intractable within the base model.", "labels": [], "entities": []}, {"text": "We conducted experiments on Chinese PropBank and English PropBank.", "labels": [], "entities": [{"text": "Chinese PropBank", "start_pos": 28, "end_pos": 44, "type": "DATASET", "confidence": 0.9022366106510162}, {"text": "English PropBank", "start_pos": 49, "end_pos": 65, "type": "DATASET", "confidence": 0.932287335395813}]}, {"text": "Results show that compared with a state-of-the-art base model, the accuracy of our joint model improves significantly by 2.4 points and 1.5 points on Chinese PropBank and English PropBank respectively, which suggests that there are substantial gains to be made by jointly modeling the shared arguments of multiple predicates.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 67, "end_pos": 75, "type": "METRIC", "confidence": 0.9995760321617126}, {"text": "Chinese PropBank", "start_pos": 150, "end_pos": 166, "type": "DATASET", "confidence": 0.8606539666652679}, {"text": "English PropBank", "start_pos": 171, "end_pos": 187, "type": "DATASET", "confidence": 0.8572010397911072}]}, {"text": "Our contributions can be summarized as follows: \u2022 To the best of our knowledge, this is the first work to investigate the mutual effect of multiple predicates' semantic role labeling.", "labels": [], "entities": [{"text": "predicates' semantic role labeling", "start_pos": 148, "end_pos": 182, "type": "TASK", "confidence": 0.6173562631011009}]}, {"text": "\u2022 We present a rich set of features for argument identification and shared arguments' classification that yield promising performance.", "labels": [], "entities": [{"text": "argument identification", "start_pos": 40, "end_pos": 63, "type": "TASK", "confidence": 0.7600361406803131}]}, {"text": "\u2022 We evaluate our method on two standard benchmarks: Chinese PropBank and English PropBank.", "labels": [], "entities": [{"text": "Chinese PropBank", "start_pos": 53, "end_pos": 69, "type": "DATASET", "confidence": 0.8855061531066895}, {"text": "English PropBank", "start_pos": 74, "end_pos": 90, "type": "DATASET", "confidence": 0.8880877494812012}]}, {"text": "Our approach performs well in both, which suggests its good universality.", "labels": [], "entities": []}, {"text": "The remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 gives an overview of our approach.", "labels": [], "entities": []}, {"text": "We discuss the mutual effect of multi-predicate' argument identification and argument classification in Section 3 and Section 4 respectively.", "labels": [], "entities": [{"text": "multi-predicate' argument identification", "start_pos": 32, "end_pos": 72, "type": "TASK", "confidence": 0.6738447025418282}, {"text": "argument classification", "start_pos": 77, "end_pos": 100, "type": "TASK", "confidence": 0.7528477907180786}]}, {"text": "The experiments and results are presented in Section 5.", "labels": [], "entities": []}, {"text": "Some discussion and analysis can be found in Section 6.", "labels": [], "entities": [{"text": "Section 6", "start_pos": 45, "end_pos": 54, "type": "DATASET", "confidence": 0.9180131256580353}]}, {"text": "Section 7 discusses the related works.", "labels": [], "entities": []}, {"text": "Finally, the conclusion and future work are in Section 8.", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate the performance of our approach, we have conducted on two standard benchmarks: Chinese PropBank and English PropBank.", "labels": [], "entities": [{"text": "Chinese PropBank", "start_pos": 91, "end_pos": 107, "type": "DATASET", "confidence": 0.8842686116695404}, {"text": "English PropBank", "start_pos": 112, "end_pos": 128, "type": "DATASET", "confidence": 0.891365110874176}]}, {"text": "The experimental setting is as follows: Chinese: We use Chinese Proposition Bank 1.0.", "labels": [], "entities": [{"text": "Chinese Proposition Bank 1.0", "start_pos": 56, "end_pos": 84, "type": "DATASET", "confidence": 0.9567061811685562}]}, {"text": "All data are divided into three parts.", "labels": [], "entities": []}, {"text": "648 files (from chtb 081.fid to chtb 899.fid) are used as the training set.", "labels": [], "entities": []}, {"text": "40 files (from chtb 041.fid to chtb 080.fid) constitutes the development set.", "labels": [], "entities": []}, {"text": "The test set consists of 72 files (chtb 001.fid to chtb 040.fid and chtb 900.fid to chtb 931.fid).", "labels": [], "entities": []}, {"text": "This data setting is the same as in).", "labels": [], "entities": []}, {"text": "We adopt Berkeley Parser 1 to carryout auto parsing for SRL and the parser is retrained on the training set.", "labels": [], "entities": [{"text": "SRL", "start_pos": 56, "end_pos": 59, "type": "TASK", "confidence": 0.9435350894927979}]}, {"text": "We used n =10 joint assignments for training the joint model and testing.", "labels": [], "entities": []}, {"text": "English: We choose English Propbank as the evaluation corpus.", "labels": [], "entities": []}, {"text": "According to the traditional partition, the training set consists of the annotations in Sections 2 to 21, the development set is Section 24, and the test set is Section 23.", "labels": [], "entities": []}, {"text": "This data setting is the same as in ().", "labels": [], "entities": []}, {"text": "We adopt Charniak Parser 2 to carryout auto parsing for SRL and the parser is retrained on the training set.", "labels": [], "entities": [{"text": "SRL", "start_pos": 56, "end_pos": 59, "type": "TASK", "confidence": 0.9683277606964111}]}, {"text": "We used n =10 joint assignments for training the joint model and testing.", "labels": [], "entities": []}, {"text": "We first investigate the performance of our approach in Argument Identification.", "labels": [], "entities": [{"text": "Argument Identification", "start_pos": 56, "end_pos": 79, "type": "TASK", "confidence": 0.9104035794734955}]}, {"text": "For the task of Argument Identification (AI), we 1 http://code.google.com/p/berkeleyparser/ 2 https://github.com/BLLIP/bllip-parser adopt auto parser to produce auto parsing trees for SRL.", "labels": [], "entities": [{"text": "Argument Identification (AI)", "start_pos": 16, "end_pos": 44, "type": "TASK", "confidence": 0.8733368992805481}, {"text": "SRL", "start_pos": 184, "end_pos": 187, "type": "TASK", "confidence": 0.96601402759552}]}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "We can see that in the experiment of Chinese, the F1 score reaches to 78.79% with base features.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 50, "end_pos": 58, "type": "METRIC", "confidence": 0.977098822593689}]}, {"text": "While after additional predicates-related features are added, the precision has improved by 1.6 points with slight loss in recall, which leads to the improvement of 0.6 points in F1.", "labels": [], "entities": [{"text": "precision", "start_pos": 66, "end_pos": 75, "type": "METRIC", "confidence": 0.9998002648353577}, {"text": "recall", "start_pos": 123, "end_pos": 129, "type": "METRIC", "confidence": 0.9997187256813049}, {"text": "F1", "start_pos": 179, "end_pos": 181, "type": "METRIC", "confidence": 0.9987598657608032}]}, {"text": "The similar effect occurred in the experiment of English.", "labels": [], "entities": []}, {"text": "After additional features added in the identification module, the precision is improved by about 0.9 points with a slight loss in recall, leading to an improvement of 0.3 points in F1.", "labels": [], "entities": [{"text": "precision", "start_pos": 66, "end_pos": 75, "type": "METRIC", "confidence": 0.9997931122779846}, {"text": "recall", "start_pos": 130, "end_pos": 136, "type": "METRIC", "confidence": 0.9997429251670837}, {"text": "F1", "start_pos": 181, "end_pos": 183, "type": "METRIC", "confidence": 0.998318076133728}]}, {"text": "However, the improvement in English is slight smaller than in Chinese.", "labels": [], "entities": []}, {"text": "We think the main reason is that there are less parse errors in English than in Chinese.", "labels": [], "entities": []}, {"text": "All results demonstrate that the novel predicted-related features are effective in recognizing many identification errors which are difficult to discriminate with base features.: Comparison with Base Features in Argument Identification.", "labels": [], "entities": [{"text": "Argument Identification", "start_pos": 212, "end_pos": 235, "type": "TASK", "confidence": 0.7107091099023819}]}, {"text": "Scores marked by \"*\" are significantly better (p < 0.05) than base features.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3. We can  see that in the experiment of Chinese, the F1 score  reaches to 78.79% with base features. While after  additional predicates-related features are added,  the precision has improved by 1.6 points with s- light loss in recall, which leads to the improve- ment of 0.6 points in F1. The similar effect oc- curred in the experiment of English. After addi- tional features added in the identification module,  the precision is improved by about 0.9 points with  a slight loss in recall, leading to an improvement  of 0.3 points in F1. However, the improvemen- t in English is slight smaller than in Chinese. We  think the main reason is that there are less parse er- rors in English than in Chinese. All results demon- strate that the novel predicted-related features are  effective in recognizing many identification errors  which are difficult to discriminate with base fea- tures.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.9871895015239716}, {"text": "precision", "start_pos": 177, "end_pos": 186, "type": "METRIC", "confidence": 0.9982062578201294}, {"text": "recall", "start_pos": 236, "end_pos": 242, "type": "METRIC", "confidence": 0.9794613122940063}, {"text": "F1", "start_pos": 294, "end_pos": 296, "type": "METRIC", "confidence": 0.9626598358154297}, {"text": "precision", "start_pos": 427, "end_pos": 436, "type": "METRIC", "confidence": 0.9971042275428772}, {"text": "recall", "start_pos": 492, "end_pos": 498, "type": "METRIC", "confidence": 0.9945843815803528}, {"text": "F1", "start_pos": 544, "end_pos": 546, "type": "METRIC", "confidence": 0.9941108822822571}]}, {"text": " Table 3: Comparison with Base Features in Ar- gument Identification. Scores marked by \"*\" are  significantly better (p < 0.05) than base features.", "labels": [], "entities": [{"text": "Ar- gument", "start_pos": 43, "end_pos": 53, "type": "METRIC", "confidence": 0.8327109416325887}]}, {"text": " Table 4: Performance of the Base Model in Argu- ment Classification", "labels": [], "entities": [{"text": "Argu- ment Classification", "start_pos": 43, "end_pos": 68, "type": "TASK", "confidence": 0.5672197192907333}]}, {"text": " Table 6: Features performance in the Joint Model.", "labels": [], "entities": []}, {"text": " Table 7: Results on auto parse trees. Base mean- s the baseline system, +AI meaning predcates- related features added in AI, + AC meaning joint  module added.", "labels": [], "entities": [{"text": "Base mean- s", "start_pos": 39, "end_pos": 51, "type": "METRIC", "confidence": 0.9386457055807114}]}, {"text": " Table 8: Comparison with Other Methods", "labels": [], "entities": []}, {"text": " Table  9. In example (1), the argument is a preposition- al phrase '( n \u00ca ] t I \u00a1 Y \u00b2 \" \u00f6'  (at the same time of compulsory education) and  shared by two predicates '-0' (witness) and 'i  '' (expand). In the corpus, a prepositional phrase  is commonly labeled as ARGM-LOC and ARGM- TMP. Thus, the base model labeled the argument", "labels": [], "entities": [{"text": "ARGM-LOC", "start_pos": 264, "end_pos": 272, "type": "DATASET", "confidence": 0.6549348831176758}, {"text": "ARGM- TMP", "start_pos": 277, "end_pos": 286, "type": "DATASET", "confidence": 0.7150180339813232}]}]}