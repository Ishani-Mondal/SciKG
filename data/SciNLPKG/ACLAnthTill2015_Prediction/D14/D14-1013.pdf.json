{"title": [{"text": "Combining Punctuation and Disfluency Prediction: An Empirical Study", "labels": [], "entities": [{"text": "Combining Punctuation and Disfluency Prediction", "start_pos": 0, "end_pos": 47, "type": "TASK", "confidence": 0.6198748528957367}]}], "abstractContent": [{"text": "Punctuation prediction and disfluency prediction can improve downstream natural language processing tasks such as machine translation and information extraction.", "labels": [], "entities": [{"text": "Punctuation prediction", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.6973689943552017}, {"text": "disfluency prediction", "start_pos": 27, "end_pos": 48, "type": "TASK", "confidence": 0.7348744869232178}, {"text": "machine translation", "start_pos": 114, "end_pos": 133, "type": "TASK", "confidence": 0.820821076631546}, {"text": "information extraction", "start_pos": 138, "end_pos": 160, "type": "TASK", "confidence": 0.8099072873592377}]}, {"text": "Combining the two tasks can potentially improve the efficiency of the overall pipeline system and reduce error propagation.", "labels": [], "entities": []}, {"text": "In this work 1 , we compare various methods for combining punctuation prediction (PU) and disfluency prediction (DF) on the Switchboard corpus.", "labels": [], "entities": [{"text": "disfluency prediction (DF)", "start_pos": 90, "end_pos": 116, "type": "TASK", "confidence": 0.7094821631908417}, {"text": "Switchboard corpus", "start_pos": 124, "end_pos": 142, "type": "DATASET", "confidence": 0.8885596692562103}]}, {"text": "We compare an isolated prediction approach with a cascade approach, a rescoring approach, and three joint model approaches.", "labels": [], "entities": []}, {"text": "For the cascade approach, we show that the soft cascade method is better than the hard cascade method.", "labels": [], "entities": []}, {"text": "We also use the cascade models to generate an n-best list, use the bi-directional cascade models to perform rescoring, and compare that with the results of the cascade models.", "labels": [], "entities": []}, {"text": "For the joint model approach, we compare mixed-label Linear-chain Conditional Random Field (LCRF), cross-product LCRF and 2-layer Factorial Conditional Random Field (FCRF) with soft-cascade LCRF.", "labels": [], "entities": []}, {"text": "Our results show that the various methods linking the two tasks are not significantly different from one another, although they perform better than the isolated prediction method by 0.5-1.5% in the F1 score.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 198, "end_pos": 206, "type": "METRIC", "confidence": 0.9735727906227112}]}, {"text": "Moreover, the clique order of features also shows a marked difference.", "labels": [], "entities": []}], "introductionContent": [{"text": "The raw output from automatic speech recognition (ASR) systems does not have sentence bound-aries or punctuation symbols.", "labels": [], "entities": [{"text": "automatic speech recognition (ASR)", "start_pos": 20, "end_pos": 54, "type": "TASK", "confidence": 0.8101077179114023}]}, {"text": "Spontaneous speech also contains a significant proportion of disfluency.", "labels": [], "entities": []}, {"text": "Researchers have shown that splitting input sequences into sentences and adding in punctuation symbols improve machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 111, "end_pos": 130, "type": "TASK", "confidence": 0.8060140609741211}]}, {"text": "Moreover, disfluencies in speech also introduce noise in downstream tasks like machine translation and information extraction ().", "labels": [], "entities": [{"text": "machine translation", "start_pos": 79, "end_pos": 98, "type": "TASK", "confidence": 0.8048481047153473}, {"text": "information extraction", "start_pos": 103, "end_pos": 125, "type": "TASK", "confidence": 0.7662109136581421}]}, {"text": "Thus, punctuation prediction (PU) and disfluency prediction (DF) are two important post-processing tasks for automatic speech recognition because they improve not only the readability of ASR output, but also the performance of downstream Natural Language Processing (NLP) tasks.", "labels": [], "entities": [{"text": "punctuation prediction (PU)", "start_pos": 6, "end_pos": 33, "type": "TASK", "confidence": 0.8152583718299866}, {"text": "disfluency prediction (DF)", "start_pos": 38, "end_pos": 64, "type": "TASK", "confidence": 0.7433732509613037}, {"text": "automatic speech recognition", "start_pos": 109, "end_pos": 137, "type": "TASK", "confidence": 0.6096700131893158}, {"text": "ASR output", "start_pos": 187, "end_pos": 197, "type": "TASK", "confidence": 0.9097476005554199}]}, {"text": "The task of punctuation prediction is to insert punctuation symbols into conversational speech texts.", "labels": [], "entities": [{"text": "punctuation prediction", "start_pos": 12, "end_pos": 34, "type": "TASK", "confidence": 0.8620913326740265}]}, {"text": "Punctuation prediction on long, unsegmented texts also achieves the purpose of sentence boundary prediction, because sentence boundaries are identified by sentence-end punctuation symbols: periods, question marks, and exclamation marks.", "labels": [], "entities": [{"text": "Punctuation prediction", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8786846995353699}, {"text": "sentence boundary prediction", "start_pos": 79, "end_pos": 107, "type": "TASK", "confidence": 0.7113408843676249}]}, {"text": "Consider the following example, How do you feel about the Viet Nam War ? Yeah , I saw that as well . The question mark splits the sequence into two sentences.", "labels": [], "entities": []}, {"text": "This paper deals with this task which is more challenging than that on text that has already been split into sentences.", "labels": [], "entities": []}, {"text": "The task of disfluency prediction is to identify word tokens that are spoken incorrectly due to speech disfluency.", "labels": [], "entities": [{"text": "disfluency prediction", "start_pos": 12, "end_pos": 33, "type": "TASK", "confidence": 0.7975772023200989}]}, {"text": "There are two main types of disfluencies: filler words and edit words.", "labels": [], "entities": []}, {"text": "Filler words mainly include filled pauses (e.g., 'uh', 'um') and discourse markers (e.g., \"I mean\", \"you know\").", "labels": [], "entities": [{"text": "Filler words", "start_pos": 0, "end_pos": 12, "type": "TASK", "confidence": 0.8712447583675385}]}, {"text": "As they are insertions in spontaneous speech to indicate pauses or mark boundaries in discourse, they do not convey useful content information.", "labels": [], "entities": []}, {"text": "Edit words are words that are spoken wrongly and then corrected by the speaker.", "labels": [], "entities": []}, {"text": "For example, consider the following utterance: I want a flight", "labels": [], "entities": []}], "datasetContent": [{"text": "We use the Switchboard corpus (LDC99T42) in our experiment with the same train/develop/test split as and).", "labels": [], "entities": [{"text": "Switchboard corpus (LDC99T42)", "start_pos": 11, "end_pos": 40, "type": "DATASET", "confidence": 0.8881877779960632}]}, {"text": "The corpus statistics are shown in.", "labels": [], "entities": []}, {"text": "Since the proportion of exclamation marks and incomplete SU boundaries is too small, we convert all exclamation marks to periods and remove all incomplete SU boundaries (treat as no punctuation).", "labels": [], "entities": []}, {"text": "In the Switchboard corpus, the utterances of each speaker have already been segmented into short sentences when used in (Qian and).", "labels": [], "entities": [{"text": "Switchboard corpus", "start_pos": 7, "end_pos": 25, "type": "DATASET", "confidence": 0.8242513239383698}]}, {"text": "In our work, we concatenate the utterances of each speaker to form one long sequence of words for use as the input to punctuation prediction and disfluency prediction.", "labels": [], "entities": [{"text": "punctuation prediction", "start_pos": 118, "end_pos": 140, "type": "TASK", "confidence": 0.8430444300174713}, {"text": "disfluency prediction", "start_pos": 145, "end_pos": 166, "type": "TASK", "confidence": 0.8562712073326111}]}, {"text": "This form of input where, utterances are not pre-segmented into short sentences, better reflects the real-world scenarios and provides a more realistic test setting for punctuation and disfluency prediction.", "labels": [], "entities": [{"text": "disfluency prediction", "start_pos": 185, "end_pos": 206, "type": "TASK", "confidence": 0.7222186177968979}]}, {"text": "Punctuation prediction also gives rise to sentence segmentation in this setting.", "labels": [], "entities": [{"text": "Punctuation prediction", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8671451807022095}, {"text": "sentence segmentation", "start_pos": 42, "end_pos": 63, "type": "TASK", "confidence": 0.7181495130062103}]}, {"text": "*: each conversation produces two long/sentencejoined sequences, one from each speaker.", "labels": [], "entities": []}, {"text": "(statistical significance at p=0.01).", "labels": [], "entities": [{"text": "statistical significance", "start_pos": 1, "end_pos": 25, "type": "METRIC", "confidence": 0.9285056591033936}]}, {"text": "However, hard cascade has a higher upper-bound than soft cascade.", "labels": [], "entities": []}, {"text": "This observation can be explained as follows.", "labels": [], "entities": []}, {"text": "For hard cascade, the input sequence is modified prior to feature extraction.", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 58, "end_pos": 76, "type": "TASK", "confidence": 0.7055049389600754}]}, {"text": "Therefore, many of the features generated by the feature templates given in will be affected by these modifications.", "labels": [], "entities": []}, {"text": "So, provided that the modifications are based on the correct information, the resulting features will not contain unwanted artefacts caused by the absence of the sentence boundary information for the presence of disfluencies.", "labels": [], "entities": []}, {"text": "For example, in \"do you do you feel that it was worthy\", the punctuation prediction system tends to insert a sentence-end punctuation after the first \"do you\" because the speaker restarts the sentence.", "labels": [], "entities": []}, {"text": "If the disfluency was correctly predicted in the first step, then the hard cascade method would have removed the first \"do you\" and eliminated the confusion.", "labels": [], "entities": []}, {"text": "Similarly, in \"I 'm sorry . I 'm not going with you tomorrow . \", the first \"I 'm\" is likely to be incorrectly detected as disfluent tokens since consecutive repetitions area strong indication of disfluency.", "labels": [], "entities": []}, {"text": "In the case of hard cascade, PU\u2192DF, the input sequence would have been split into sentences and the repetition feature would not be activated.", "labels": [], "entities": [{"text": "repetition", "start_pos": 100, "end_pos": 110, "type": "METRIC", "confidence": 0.9706672430038452}]}, {"text": "However, since the hard cascade method has a greater influence on the features for the second step, it is also more sensitive to the prediction errors from the first step.", "labels": [], "entities": []}, {"text": "Another observation from is that the improvement of the soft cascade over the isolate baseline is much larger on DF (1.4% absolute) than that on PU (only 0.5% absolute).", "labels": [], "entities": []}, {"text": "The same holds true for the hard cascade, despite the fact that there are more DF labels than PU labels in this corpus (see) and the first step prediction is more accurate on DF than on PU.", "labels": [], "entities": []}, {"text": "This suggests that their mutual influence is not symmetrical, in the way that the output from punctuation prediction provides more useful information for disfluency prediction than the other way round.", "labels": [], "entities": [{"text": "punctuation prediction", "start_pos": 94, "end_pos": 116, "type": "TASK", "confidence": 0.7645864188671112}, {"text": "disfluency prediction", "start_pos": 154, "end_pos": 175, "type": "TASK", "confidence": 0.7742237746715546}]}], "tableCaptions": [{"text": " Table 1: Corpus statistics for all the experiments.", "labels": [], "entities": []}, {"text": " Table 3: Feature templates for disfluency predic- tion, or punctuation prediction, or joint prediction  for all the experiments in this paper.", "labels": [], "entities": [{"text": "punctuation prediction", "start_pos": 60, "end_pos": 82, "type": "TASK", "confidence": 0.6626508980989456}]}, {"text": " Table 4: Baseline results showing the degradation  by joining utterances into long sentences, remov- ing precision/recall balancing, and reducing the  clique order of features. All models are trained  using M3N.", "labels": [], "entities": [{"text": "precision", "start_pos": 106, "end_pos": 115, "type": "METRIC", "confidence": 0.9964371919631958}, {"text": "recall", "start_pos": 116, "end_pos": 122, "type": "METRIC", "confidence": 0.7427706122398376}]}, {"text": " Table 5: Performance comparison between the  hard cascade method and the soft cascade method  with respect to the baseline isolated prediction.  All models are trained using M3N without balanc- ing precision and recall.", "labels": [], "entities": [{"text": "balanc- ing", "start_pos": 187, "end_pos": 198, "type": "METRIC", "confidence": 0.8923892180124918}, {"text": "precision", "start_pos": 199, "end_pos": 208, "type": "METRIC", "confidence": 0.5763593316078186}, {"text": "recall", "start_pos": 213, "end_pos": 219, "type": "METRIC", "confidence": 0.9960896968841553}]}, {"text": " Table 6: Performance comparison between the  rescoring method and the soft-cascade method  with respect to the baseline isolated prediction.  The rescoring is done on 2n 2 hypotheses. All  models are trained using M3N without balancing  precision and recall. Figures in the bracket are the  oracle F1 scores of the 2n 2 hypotheses. *:on the  development set, the best overall result is obtained  at n = 10.", "labels": [], "entities": [{"text": "precision", "start_pos": 238, "end_pos": 247, "type": "METRIC", "confidence": 0.9982128143310547}, {"text": "recall", "start_pos": 252, "end_pos": 258, "type": "METRIC", "confidence": 0.995831310749054}, {"text": "F1", "start_pos": 299, "end_pos": 301, "type": "METRIC", "confidence": 0.8740748167037964}]}, {"text": " Table 7: Performance comparison among 2- layer FCRF, mixed-label LCRF and cross-product  LCRF, with respect to the soft-cascade and the iso- lated prediction baseline. All models are trained  using GRMM (Sutton, 2006), with reduced clique  orders.", "labels": [], "entities": [{"text": "GRMM", "start_pos": 199, "end_pos": 203, "type": "METRIC", "confidence": 0.8669469952583313}]}]}