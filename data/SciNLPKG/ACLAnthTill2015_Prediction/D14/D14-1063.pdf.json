{"title": [{"text": "Translation Rules with Right-Hand Side Lattices", "labels": [], "entities": [{"text": "Translation", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.9392375946044922}]}], "abstractContent": [{"text": "In Corpus-Based Machine Translation, the search space of the translation candidates fora given input sentence is often defined by a set of (cycle-free) context-free grammar rules.", "labels": [], "entities": [{"text": "Corpus-Based Machine Translation", "start_pos": 3, "end_pos": 35, "type": "TASK", "confidence": 0.6134586036205292}]}, {"text": "This happens naturally in Syntax-Based Machine Translation and Hierarchical Phrase-Based Machine Translation (where the representation will be the set of the target-side half of the synchronous rules used to parse the input sentence).", "labels": [], "entities": [{"text": "Syntax-Based Machine Translation", "start_pos": 26, "end_pos": 58, "type": "TASK", "confidence": 0.808570921421051}, {"text": "Hierarchical Phrase-Based Machine Translation", "start_pos": 63, "end_pos": 108, "type": "TASK", "confidence": 0.5580109506845474}]}, {"text": "But it is also possible to describe Phrase-Based Machine Translation in this framework.", "labels": [], "entities": [{"text": "Phrase-Based Machine Translation", "start_pos": 36, "end_pos": 68, "type": "TASK", "confidence": 0.8414695660273234}]}, {"text": "We propose a natural extension to this representation by using lattice-rules that allow to easily encode an exponential number of variations of each rules.", "labels": [], "entities": []}, {"text": "We also demonstrate how the representation of the search space has an impact on decoding efficiency, and how it is possible to optimize this representation.", "labels": [], "entities": []}], "introductionContent": [{"text": "A popular approach to modern Machine Translation is to decompose the translation problem into a modeling step and a search step.", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 29, "end_pos": 48, "type": "TASK", "confidence": 0.8082830607891083}]}, {"text": "The modeling step will consist in defining implicitly a set of possible translations T for each input sentence.", "labels": [], "entities": []}, {"text": "Each translation in T being associated with a real-valued model score.", "labels": [], "entities": []}, {"text": "The search step will then consist in finding the translation in T with the highest model score.", "labels": [], "entities": []}, {"text": "The search is non-trivial because it is usually impossible to enumerate all members of T (its cardinality being typically exponentially dependent on the size of the sentence to be translated).", "labels": [], "entities": []}, {"text": "Since at least, a common way of representing T has been through a cycle-free context-free grammar.", "labels": [], "entities": []}, {"text": "In such a grammar, T is represented as a set of context-free rules such as can be seen on figure 1.", "labels": [], "entities": []}, {"text": "These rules themselves can be generated by the modeling step through the use of phrase tables, synchronous parsing, tree-tostring rules, etc.", "labels": [], "entities": []}, {"text": "If the model score of each translation is taken to be the sum of rule scores independently given to each rule, the search for the optimal translation is easy with some classic dynamic programming techniques.", "labels": [], "entities": []}, {"text": "However, if the model score is going to take into account informations such as the language model score of each sentence, it cannot be expressed in such away.", "labels": [], "entities": []}, {"text": "Since the language model score has proven empirically to be a very good source of information, proposed an approximate search algorithm called cube pruning.", "labels": [], "entities": []}, {"text": "We propose hereto represent T using context-free lattice-rules such as shown in figure 2.", "labels": [], "entities": []}, {"text": "This allows us to compactly encode a large number of rules.", "labels": [], "entities": []}, {"text": "One benefit is that it adds flexibility to the modeling step, making it easier: many choices such as whether or not a function word should be included, the relative position of words and non-terminal in the translation, as well as morphological variations can be delegated to the search step by encoding them in the lattice rules.", "labels": [], "entities": []}, {"text": "While it is true that the same could be achieved by an explicit enumeration, lattice rules make this easier and more efficient.", "labels": [], "entities": []}, {"text": "In particular, we show that a decoding algorithm working with such lattice rules can be more efficient than one working directly on the enumeration of the rules encoded in the lattice.", "labels": [], "entities": []}, {"text": "A distinct but related idea of this paper is to consider how transforming the structure of the rules defining T can lead to improvements in the speed/memory performances of the decoding.", "labels": [], "entities": []}, {"text": "In particular, we propose a method to merge and reduce the size of the lattice rules and show that it translates into better performances at decoding time.", "labels": [], "entities": []}, {"text": "In this paper, we will first define more precisely our concept of lattice-rules, then try to give some motivation for them in the context of a tree-to-tree MT system (section 3).", "labels": [], "entities": [{"text": "MT", "start_pos": 156, "end_pos": 158, "type": "TASK", "confidence": 0.9448341727256775}]}, {"text": "In section 4, we then propose an algorithm for preprocessing a representation given in a latticerule form that allows for more efficient search.", "labels": [], "entities": []}, {"text": "In section 5, we describe a decoding algorithm specially designed for handling lattice-rules.", "labels": [], "entities": []}, {"text": "In section 6, we perform some experiments demonstrating the merit of our approach.", "labels": [], "entities": []}], "datasetContent": [{"text": "We now describe a set of experiments aimed at evaluating our approach.", "labels": [], "entities": []}, {"text": "We use the Japanese-English data from the NTCIR-10 Patent MT task.", "labels": [], "entities": [{"text": "NTCIR-10 Patent MT task", "start_pos": 42, "end_pos": 65, "type": "DATASET", "confidence": 0.8951815515756607}]}, {"text": "The training data contains 3 millions parallel sentences for Japanese-English.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Impact of the lattice representation on performances.", "labels": [], "entities": []}, {"text": " Table 2: Impact on BLEU of using flexible  lattice rules.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 20, "end_pos": 24, "type": "METRIC", "confidence": 0.9978693723678589}]}, {"text": " Table 3: Evaluation of the performances of our lattice-rule decoder compared with a state-of- the-art decoder using an expanded flat representation of the lattice rules. \"Nb wins\" is the  number of times one of the decoder found a strictly better model score than the other one, out  of 1800 search.", "labels": [], "entities": []}]}