{"title": [{"text": "Abstractive Summarization of Product Reviews Using Discourse Structure", "labels": [], "entities": [{"text": "Abstractive Summarization of Product Reviews", "start_pos": 0, "end_pos": 44, "type": "TASK", "confidence": 0.8371476650238037}]}], "abstractContent": [{"text": "We propose a novel abstractive summa-rization system for product reviews by taking advantage of their discourse structure.", "labels": [], "entities": []}, {"text": "First, we apply a discourse parser to each review and obtain a discourse tree representation for every review.", "labels": [], "entities": []}, {"text": "We then modify the discourse trees such that every leaf node only contains the aspect words.", "labels": [], "entities": []}, {"text": "Second , we aggregate the aspect discourse trees and generate a graph.", "labels": [], "entities": []}, {"text": "We then select a subgraph representing the most important aspects and the rhetorical relations between them using a PageRank algorithm, and transform the selected subgraph into an aspect tree.", "labels": [], "entities": []}, {"text": "Finally, we generate a natural language summary by applying a template-based NLG framework.", "labels": [], "entities": []}, {"text": "Quantitative and qualitative analysis of the results , based on two user studies, show that our approach significantly outperforms ex-tractive and abstractive baselines.", "labels": [], "entities": []}], "introductionContent": [{"text": "Most existing works on sentiment summarization focus on predicting the overall rating on an entity () or estimating ratings for product features ().", "labels": [], "entities": [{"text": "sentiment summarization", "start_pos": 23, "end_pos": 46, "type": "TASK", "confidence": 0.9353103041648865}]}, {"text": "However, the opinion summaries in such systems are extractive, meaning that they generate a summary by concatenating extracts that are representative of opinion on the entity or its aspects.", "labels": [], "entities": []}, {"text": "Comparing extractive and abstractive summaries for evaluative texts has shown that an abstractive approach is more appropriate for summarizing evaluative text The contribution of the first two authors to this paper was equal.", "labels": [], "entities": [{"text": "summarizing evaluative text", "start_pos": 131, "end_pos": 158, "type": "TASK", "confidence": 0.9039438565572103}]}, {"text": "This finding is also supported by a previous study in the context of summarizing news articles ().", "labels": [], "entities": [{"text": "summarizing news articles", "start_pos": 69, "end_pos": 94, "type": "TASK", "confidence": 0.9337926308314005}]}, {"text": "To the best of our knowledge, there are only three previous works on abstractive opinion summarization ().", "labels": [], "entities": [{"text": "abstractive opinion summarization", "start_pos": 69, "end_pos": 102, "type": "TASK", "confidence": 0.6836018959681193}]}, {"text": "The first work proposes a graph-based method for generating ultra concise opinion summaries that are more suitable for viewing on devices with small screens.", "labels": [], "entities": [{"text": "generating ultra concise opinion summaries", "start_pos": 49, "end_pos": 91, "type": "TASK", "confidence": 0.6729992747306823}]}, {"text": "This method does not provide a well-formed grammatical abstract and the generated summary only contains words that occur in the original texts.", "labels": [], "entities": []}, {"text": "Therefore, this approach is more extractive than abstractive.", "labels": [], "entities": []}, {"text": "Another limitation is that the generated summaries do not contain any information about the distribution of opinions.", "labels": [], "entities": []}, {"text": "In the second work, ( ) addresses some of the aforementioned problems and generates well-formed grammatical abstracts that describe the distribution of opinion over the entity and its features.", "labels": [], "entities": []}, {"text": "However, for each product, this approach requires a feature taxonomy handcrafted by humans as an input, which is not scalable.", "labels": [], "entities": []}, {"text": "To partially address this problem) has proposed a method for the automatic generation of a product attribute hierarchy that leverages ConceptNet ().", "labels": [], "entities": []}, {"text": "However, the resulting ontology tree has been used only for sentiment classification and not for classification.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 60, "end_pos": 84, "type": "TASK", "confidence": 0.9585682153701782}, {"text": "classification", "start_pos": 97, "end_pos": 111, "type": "TASK", "confidence": 0.9643623232841492}]}, {"text": "In the third and most recent study, proposed Starlet-H as a hybrid abstractive/extractive sentiment summarizer.", "labels": [], "entities": [{"text": "extractive sentiment summarizer", "start_pos": 79, "end_pos": 110, "type": "TASK", "confidence": 0.6459555923938751}]}, {"text": "Starlet-H uses extractive summarization techniques to select salient quotes from the input reviews and embeds them into the abstractive summary to exemplify, justify or provide evidence for the aggregate positive or negative opinions.", "labels": [], "entities": []}, {"text": "However, Starlet-H assumes a limited number of aspects as input and needs a large amount of training data to learn the ordering of aspects for summary generation.", "labels": [], "entities": [{"text": "summary generation", "start_pos": 143, "end_pos": 161, "type": "TASK", "confidence": 0.837822437286377}]}, {"text": "Highlighting the reasons behind opinions in reviews was also previously proposed in ().", "labels": [], "entities": []}, {"text": "However, their approach is extractive and similar to ( does not cover the distribution of opinions.", "labels": [], "entities": []}, {"text": "Furthermore, it aims to explain the opinion on only one aspect, rather than explaining the overall opinion on the product, its aspects and how they affect each other.", "labels": [], "entities": []}, {"text": "To address some of the above mentioned limitations , in this paper we propose a novel abstractive summarization framework that generates an aspect-based abstract from multiple reviews of a product.", "labels": [], "entities": []}, {"text": "In our framework, anything that is evaluated in the review is considered an aspect, including the product itself.", "labels": [], "entities": []}, {"text": "We propose a natural language generation (NLG) framework that takes aspects and their structured relation as input and generates an abstractive summary.", "labels": [], "entities": [{"text": "natural language generation (NLG)", "start_pos": 13, "end_pos": 46, "type": "TASK", "confidence": 0.8281360268592834}]}, {"text": "However, unlike ( , our method assumes no domain knowledge about the entity in terms of a user-defined feature taxonomy.", "labels": [], "entities": []}, {"text": "On the other hand, in contrast with Starlet-H, we do not limit the input reviews to a small number of aspects and our aspect ordering method takes advantage of rhetorical information and does not require any training data.", "labels": [], "entities": [{"text": "aspect ordering", "start_pos": 118, "end_pos": 133, "type": "TASK", "confidence": 0.7518315017223358}]}, {"text": "Our method relies on the discourse structure and discourse relations of reviews to infer the importance of aspects as well as the association between them (e.g., which aspects relate to each other).", "labels": [], "entities": []}, {"text": "Researchers have recently started using the discourse structure of text in sentiment analysis and have shown its advantage in improving sentiment classification accuracy (e.g., ().", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 75, "end_pos": 93, "type": "TASK", "confidence": 0.9661460220813751}, {"text": "sentiment classification", "start_pos": 136, "end_pos": 160, "type": "TASK", "confidence": 0.8700197041034698}, {"text": "accuracy", "start_pos": 161, "end_pos": 169, "type": "METRIC", "confidence": 0.8913812637329102}]}, {"text": "However, to the best of our knowledge, none of the existing works have looked into exploiting discourse structure in abstractive review summarization.", "labels": [], "entities": []}, {"text": "In our work, importance of aspects, derived from the reviews' discourse structure and relations, is used to rank and select aspects to be included in the summary.", "labels": [], "entities": []}, {"text": "More specifically, we start with the most important (highest ranked) aspects to generate a summary and add more aspects to the system until a summary of desired length is obtained.", "labels": [], "entities": []}, {"text": "Aspect association is considered to better explain how the opinions on aspects affect each other (e.g., opinion over specific aspects affect the opinion over the more general ones).", "labels": [], "entities": [{"text": "Aspect association", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7767942249774933}]}, {"text": "Consider the following sentence as an example summary generated by our system for the entity Camera Canon G3: \"All reviewers who commented on the camera, thought that it was really good mainly because of the photo quality.\"", "labels": [], "entities": [{"text": "entity Camera Canon G3", "start_pos": 86, "end_pos": 108, "type": "DATASET", "confidence": 0.7392027229070663}]}, {"text": "This summary encapsulates all the following key pieces of information: 1) camera and photo quality are the most important aspects, 2) People have positive opinion on camera in general and on photo quality as one of its features, and finally 3) photo quality is the main reason behind users satisfaction on camera.", "labels": [], "entities": []}, {"text": "Such summary helps users understand the reason behind a rating of a product or its aspects without going through all reviews or reading scattered opinions on different aspects in multiple sentences of an extractive summary.", "labels": [], "entities": []}, {"text": "This paper makes the following contributions: 1.", "labels": [], "entities": []}, {"text": "We propose a novel content selection and structuring strategy for review summarization, that assumes no prior domain knowledge, by taking advantage of the discourse structure of reviews.", "labels": [], "entities": [{"text": "review summarization", "start_pos": 66, "end_pos": 86, "type": "TASK", "confidence": 0.6325049102306366}]}, {"text": "2. We propose a novel product-independent template-based NLG framework to generate an abstract based on the selected content, without relying on deep syntactic knowledge or sophisticated NLG methods.", "labels": [], "entities": []}, {"text": "Our framework, similarly to ( , can effectively convey the distribution of opinions.", "labels": [], "entities": []}, {"text": "3. We present the first study that investigates the use of discourse structure information in both content selection and abstract generation for multidocument summarization.", "labels": [], "entities": [{"text": "content selection", "start_pos": 99, "end_pos": 116, "type": "TASK", "confidence": 0.712761402130127}, {"text": "multidocument summarization", "start_pos": 145, "end_pos": 172, "type": "TASK", "confidence": 0.674977034330368}]}, {"text": "Quantitative and qualitative analysis over evaluation results of two user studies on a set of user reviews on twelve different products show that our system is an effective abstractive system for review summarization.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conduct our experiments using the customer reviews of twelve products obtained from (Hu and Liu, 2004a): 4 digital cameras, 1 DVD player, 1 MP3 player, 2 routers, 2 phones, 1 diaper and 1 antivirus.", "labels": [], "entities": []}, {"text": "The reviews were collected from Amazon.com and Cnet.com.", "labels": [], "entities": [{"text": "Cnet.com", "start_pos": 47, "end_pos": 55, "type": "DATASET", "confidence": 0.950697124004364}]}, {"text": "We use manually annotated aspects and their associated sentiment from the same dataset.", "labels": [], "entities": []}, {"text": "We compare the summaries generated by our system with two state-of-the-art extractive baselines and a simpler version of our abstractive system, as follows: 1) MEAD-LexRank (LR): we use the LexRank () implementation inside the MEAD summarization framework ( ), which outperforms other algorithms implemented in the MEAD framework.", "labels": [], "entities": []}, {"text": "2) MEADStar (MEAD*): a state-of-the-art extractive opinion summarization system , which is adapted from the open source summarization framework MEAD.", "labels": [], "entities": [{"text": "extractive opinion summarization", "start_pos": 40, "end_pos": 72, "type": "TASK", "confidence": 0.5854806204636892}, {"text": "summarization framework MEAD", "start_pos": 120, "end_pos": 148, "type": "TASK", "confidence": 0.6605095664660136}]}, {"text": "MEAD* orders aspects by the number of sentences evaluating that aspect, and selects a sentence from each aspect until it reaches the word limit.", "labels": [], "entities": []}, {"text": "The sentence that is selected for each aspect is the one with the highest sum of polarity/strength evaluations for any aspect.", "labels": [], "entities": []}, {"text": "3) Simple Abstractive (SA): we sort the aspects of each product based on dir-moi (Equation 1).", "labels": [], "entities": [{"text": "Simple Abstractive (SA)", "start_pos": 3, "end_pos": 26, "type": "METRIC", "confidence": 0.7709331154823303}]}, {"text": "Then, for each aspect, we generate a sentence based on a simple template \"quantifier + polarityverb\" until the summary reaches the word limit.", "labels": [], "entities": []}, {"text": "We limit the length of our summaries to 150 words.", "labels": [], "entities": []}, {"text": "In our experiment we use the default parameter in Equation 4 without tuning (i.e. \u03b1 = 0.5).", "labels": [], "entities": []}, {"text": "Our system starts the content selection process with 10 aspects and generates a summary based on a AHT with 10 aspects.", "labels": [], "entities": [{"text": "AHT", "start_pos": 99, "end_pos": 102, "type": "METRIC", "confidence": 0.46964970231056213}]}, {"text": "We add one aspect, reproduce the AHT and regenerate the summary.", "labels": [], "entities": [{"text": "AHT", "start_pos": 33, "end_pos": 36, "type": "METRIC", "confidence": 0.8691575527191162}]}, {"text": "We repeat this process until the word limit is reached.", "labels": [], "entities": []}, {"text": "On one hand, the lack of product reviews datasets with human written summaries, and on the other hand, the difficulty of generating human-written summaries for reviews, makes review summary evaluation a very challenging task.", "labels": [], "entities": [{"text": "review summary evaluation", "start_pos": 175, "end_pos": 200, "type": "TASK", "confidence": 0.5877671639124552}]}, {"text": "We evaluate the summaries generated by our system by performing two user studies based on pairwise preferences using a popular crowdsourcing service.", "labels": [], "entities": []}, {"text": "The user preference evaluation is an effective method for opinion summarization (e.g.,).", "labels": [], "entities": [{"text": "opinion summarization", "start_pos": 58, "end_pos": 79, "type": "TASK", "confidence": 0.7327828705310822}]}, {"text": "The main motivations behind pairwise preferences evaluation is two-fold: i) raters can make a preference decision more efficiently than a scoring judgment; and ii) rater agreement is higher in preference decisions than in scoring judgments ().", "labels": [], "entities": [{"text": "agreement", "start_pos": 170, "end_pos": 179, "type": "METRIC", "confidence": 0.5163577795028687}]}, {"text": "In both user studies, for each product, we run six pairwise comparisons for four summaries.", "labels": [], "entities": []}, {"text": "In each rating assignment, two summaries of the same product were placed in random order.", "labels": [], "entities": []}, {"text": "Raters were shown the name of each product along with the relevant summaries and were asked to express their preference for one summary over the other using a simple set of criteria.", "labels": [], "entities": []}, {"text": "For two summaries S 1 and S 2 raters should choose one of the following three options: 1) Prefer S 1 , 2) Prefer S 2 , 3) No preference.", "labels": [], "entities": [{"text": "Prefer", "start_pos": 90, "end_pos": 96, "type": "METRIC", "confidence": 0.9916006326675415}, {"text": "Prefer", "start_pos": 106, "end_pos": 112, "type": "METRIC", "confidence": 0.9279724359512329}]}, {"text": "Raters were specifically instructed that their rating should express \"overall satisfaction with the information provided by the summary\".", "labels": [], "entities": []}, {"text": "Raters were also asked to provide a brief comment justifying their choice.", "labels": [], "entities": []}, {"text": "Over 48 raters participated in each study, and each comparison was evaluated by at least five raters generating more than 360 judgments for each user study.", "labels": [], "entities": []}, {"text": "We pre-select the high skilled raters to ensure a higher quality results.", "labels": [], "entities": []}, {"text": "The main difference between the two user studies is that in \"user study 1\", we show two summaries to the raters and ask them to choose the one they prefer without showing them the original reviews.", "labels": [], "entities": []}, {"text": "In contrast, in \"user study 2\", we show two summaries with links to the full text of the reviews for the raters to explore.", "labels": [], "entities": []}, {"text": "In order to make sure that the raters read the reviews, we ask them to write a short summary of the reviews before rating the automatic summaries.", "labels": [], "entities": []}, {"text": "We ran two different user studies because: i) for each product there might be many reviews to be included; ii) there is no guaranty that raters, in various evaluation settings, read   the reviews (partially or completely); and iii) there is no evidence regarding the depth that each rater would look into the reviews.", "labels": [], "entities": []}, {"text": "Therefore, choosing between user study 1 and 2 is not a straightforward decision.", "labels": [], "entities": []}, {"text": "In other words, designing the two user studies in this way helps us to answer the question: \"Does the fact that raters can read all the reviews affect their ratings?\".", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Results of pairwise preference user studies. Statistically significant improvements (p < 0.01)  over the baselines are demonstrated by bold fonts. Italic fonts indicate statistical significance (p < 0.01)  of abstractive methods (SA and Our System) over extractive approaches (LR and MEAD*).", "labels": [], "entities": [{"text": "MEAD", "start_pos": 294, "end_pos": 298, "type": "METRIC", "confidence": 0.9785984754562378}]}, {"text": " Table 4: System preference results. Statistically significant improvements (p < 0.01) over the baselines  are demonstrated by bold fonts.", "labels": [], "entities": []}]}