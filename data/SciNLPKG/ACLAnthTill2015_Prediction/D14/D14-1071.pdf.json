{"title": [{"text": "Joint Relational Embeddings for Knowledge-based Question Answering", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 48, "end_pos": 66, "type": "TASK", "confidence": 0.6763277649879456}]}], "abstractContent": [{"text": "Transforming a natural language (NL) question into a corresponding logical form (LF) is central to the knowledge-based question answering (KB-QA) task.", "labels": [], "entities": [{"text": "Transforming a natural language (NL) question", "start_pos": 0, "end_pos": 45, "type": "TASK", "confidence": 0.7662498503923416}, {"text": "knowledge-based question answering (KB-QA) task", "start_pos": 103, "end_pos": 150, "type": "TASK", "confidence": 0.76309392281941}]}, {"text": "Unlike most previous methods that achieve this goal based on mappings between lex-icalized phrases and logical predicates, this paper goes one step further and proposes a novel embedding-based approach that maps NL-questions into LFs for KB-QA by leveraging semantic associations between lexical representations and KB-properties in the latent space.", "labels": [], "entities": []}, {"text": "Experimental results demonstrate that our proposed method outperforms three KB-QA base-line methods on two publicly released QA data sets.", "labels": [], "entities": [{"text": "QA data sets", "start_pos": 125, "end_pos": 137, "type": "DATASET", "confidence": 0.81776296099027}]}], "introductionContent": [{"text": "Knowledge-based question answering (KB-QA) involves answering questions posed in natural language (NL) using existing knowledge bases (KBs).", "labels": [], "entities": [{"text": "Knowledge-based question answering (KB-QA)", "start_pos": 0, "end_pos": 42, "type": "TASK", "confidence": 0.7641943097114563}]}, {"text": "As most KBs are structured databases, how to transform the input question into its corresponding structured query for KB (KB-query) as a logical form (LF), also known as semantic parsing, is the central task for KB-QA systems.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 170, "end_pos": 186, "type": "TASK", "confidence": 0.7495257258415222}]}, {"text": "Previous works) usually leveraged mappings between NL phrases and logical predicates as lexical triggers to perform transformation tasks in semantic parsing, but they had to deal with two limitations: (i) as the meaning of a logical predicate often has different natural language expression (NLE) forms, the lexical triggers extracted fora predicate may at times are limited in size; (ii) entities detected by the named entity recognition (NER) component will be used to compose the logical forms together with the logical predicates, so their types should be consistent with the predicates as well.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 140, "end_pos": 156, "type": "TASK", "confidence": 0.751394122838974}]}, {"text": "However, most NER components used in existing KB-QA systems are independent from the NLE-to-predicate mapping procedure.", "labels": [], "entities": [{"text": "NLE-to-predicate mapping", "start_pos": 85, "end_pos": 109, "type": "TASK", "confidence": 0.6266549974679947}]}, {"text": "We present a novel embedding-based KB-QA method that takes all the aforementioned limitations into account, and maps NLE-to-entity and NLE-to-predicate simultaneously using simple vector operations for structured query construction.", "labels": [], "entities": [{"text": "structured query construction", "start_pos": 202, "end_pos": 231, "type": "TASK", "confidence": 0.674920380115509}]}, {"text": "First, low-dimensional embeddings of n-grams, entity types, and predicates are jointly learned from an existing knowledge base and from entries <entity subj , NL relation phrase, entity obj > that are mined from NL texts labeled as KBproperties with weak supervision.", "labels": [], "entities": []}, {"text": "Each such entry corresponds to an NL expression of a triple <entity subj , predicate, entity obj > in the KB.", "labels": [], "entities": []}, {"text": "These embeddings are used to measure the semantic associations between lexical phrases and two properties of the KB, entity type and logical predicate.", "labels": [], "entities": []}, {"text": "Next, given an NL-question, all possible structured queries as candidate LFs are generated and then they are ranked by the similarity between the embeddings of observed features (n-grams) in the NL-question and the embeddings of logical features in the structured queries.", "labels": [], "entities": []}, {"text": "Last, answers are retrieved from the KB using the selected LFs.", "labels": [], "entities": []}, {"text": "The contributions of this work are two-fold: (1) as a smoothing technique, the low-dimensional embeddings can alleviate the coverage issues of lexical triggers; (2) our joint approach integrates entity span selection and predicate mapping tasks for KB-QA.", "labels": [], "entities": [{"text": "entity span selection", "start_pos": 195, "end_pos": 216, "type": "TASK", "confidence": 0.633757879336675}, {"text": "predicate mapping", "start_pos": 221, "end_pos": 238, "type": "TASK", "confidence": 0.7707794606685638}]}, {"text": "For this we built independent entity embeddings as the additional component, solving the entity disambiguation problem.", "labels": [], "entities": [{"text": "entity disambiguation", "start_pos": 89, "end_pos": 110, "type": "TASK", "confidence": 0.7279530763626099}]}], "datasetContent": [{"text": "Experimental Setting We first performed preprocessing, including lowercase transformation, lemmatization and tokenization, on NLE-KB pairs and evaluation data.", "labels": [], "entities": []}, {"text": "We used 71,310 n-grams (uni-, bi-, tri-), 990 entity types, and 660 predicates as relational components shown in Section 3.1.", "labels": [], "entities": []}, {"text": "The sum of these three numbers (72,960) equals the size of the embeddings we are going to learn.", "labels": [], "entities": []}, {"text": "In, we evaluated the quality of NLE-KB pairs (MP and QP) described in Section 3.2.", "labels": [], "entities": []}, {"text": "We can see that the quality of QP pairs is good, mainly due to human efforts.", "labels": [], "entities": []}, {"text": "Also, we obtained MP pairs that have an acceptable quality using threshold 3.0 for Equation 1, which leverages the redundancy information in the large-scale data (WIKIPEDIA).", "labels": [], "entities": [{"text": "Equation", "start_pos": 83, "end_pos": 91, "type": "METRIC", "confidence": 0.9723138809204102}]}, {"text": "For our embedding learning, we set the embedding dimension n to 100, the learning rate (\u03bb) for SGD to 0.0001, and the iteration number to 30.", "labels": [], "entities": [{"text": "learning rate (\u03bb)", "start_pos": 73, "end_pos": 90, "type": "METRIC", "confidence": 0.9612899899482727}]}, {"text": "To make the decoding procedure computable, we kept only the popular KB-entity in the dictionary to map different entity mentions into a KB-entity.", "labels": [], "entities": []}, {"text": "We used two publicly released data sets for QA evaluations:) includes the annotated lambda calculus forms for each question, and covers 81 domains and 635 Freebase relations; WebQ.", "labels": [], "entities": [{"text": "QA evaluations", "start_pos": 44, "end_pos": 58, "type": "TASK", "confidence": 0.8143319189548492}, {"text": "WebQ", "start_pos": 175, "end_pos": 179, "type": "DATASET", "confidence": 0.9712828993797302}]}, {"text": "( provides 5,810 question-answer pairs that are built by collecting common questions from Web-query logs and by manually labeling answers.", "labels": [], "entities": []}, {"text": "We used the previous three approaches) as our baselines.", "labels": [], "entities": []}, {"text": "reports the overall performances of our proposed KB-QA method on the two evaluation data sets and compares them with those of the three baselines.", "labels": [], "entities": []}, {"text": "Note that we did not re-implement the baseline systems, but just borrowed the evaluation results reported in their: Ablation of the relationship types papers.", "labels": [], "entities": [{"text": "Ablation", "start_pos": 116, "end_pos": 124, "type": "METRIC", "confidence": 0.9486328959465027}]}, {"text": "Although the KB used by our system is much larger than FREEBASE, we still think that the experimental results are directly comparable because we disallow all the entities that are not included in FREEBASE.", "labels": [], "entities": [{"text": "FREEBASE", "start_pos": 55, "end_pos": 63, "type": "METRIC", "confidence": 0.6089523434638977}, {"text": "FREEBASE", "start_pos": 196, "end_pos": 204, "type": "DATASET", "confidence": 0.9309707880020142}]}, {"text": "shows that our method outperforms the baselines on both Free917 and WebQ.", "labels": [], "entities": [{"text": "WebQ", "start_pos": 68, "end_pos": 72, "type": "DATASET", "confidence": 0.8575220108032227}]}, {"text": "We think that using the low-dimensional embeddings of n-grams rather than the lexical triggers greatly improves the coverage issue.", "labels": [], "entities": []}, {"text": "Unlike the previous methods which perform entity disambiguation and predicate prediction separately, our method jointly performs these two tasks.", "labels": [], "entities": [{"text": "entity disambiguation", "start_pos": 42, "end_pos": 63, "type": "TASK", "confidence": 0.7223028987646103}, {"text": "predicate prediction", "start_pos": 68, "end_pos": 88, "type": "TASK", "confidence": 0.6988353431224823}]}, {"text": "More precisely, we consider the relationships C-T and C-P simultaneously to rank candidate KB-queries.", "labels": [], "entities": []}, {"text": "In, the most independent NER in KB-QA systems may detect David as the subject entity, but our joint approach can predict the appropriate subject entity The City of David by leveraging not only the relationships with other components but also other relationships at once.", "labels": [], "entities": []}, {"text": "The syntaxbased (grammar formalism) approaches such as Combinatory Categorial Grammar (CCG) may experience errors if a question has grammatical errors.", "labels": [], "entities": []}, {"text": "However, our bag-of-words model-based approach can handle any question as long as the question contains keywords that can help in understanding it. shows the contributions of the relationships (R) between relational components C, T , and P. For each row, we remove the similarity from each of the relationship types described in Section 3.1.", "labels": [], "entities": []}, {"text": "We can see that the C-P relationship plays a crucial role in translating NL-questions to KB-queries, while the other two relationships are slightly helpful.", "labels": [], "entities": [{"text": "translating NL-questions", "start_pos": 61, "end_pos": 85, "type": "TASK", "confidence": 0.8418398499488831}]}, {"text": "Result Analysis Since the majority of questions in WebQ.", "labels": [], "entities": [{"text": "Result Analysis", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.808698445558548}, {"text": "WebQ", "start_pos": 51, "end_pos": 55, "type": "DATASET", "confidence": 0.9398625493049622}]}, {"text": "tend to be more natural and diverse, our method cannot find the correct answers to many questions.", "labels": [], "entities": []}, {"text": "The errors can be caused by any of the following reasons.", "labels": [], "entities": []}, {"text": "First, some NLEs cannot be easily linked to existing KB-predicates, making it difficult to find the answer entity.", "labels": [], "entities": []}, {"text": "Second, some entities can be mentioned in several different ways, e.g., nickname (shaq\u2192Shaquille O'neal) and family name (hitler\u2192Adolf Hitler).", "labels": [], "entities": []}, {"text": "Third, in terms of KB coverage issues, we cannot detect the entities that are unpopular.", "labels": [], "entities": [{"text": "KB coverage", "start_pos": 19, "end_pos": 30, "type": "TASK", "confidence": 0.8863165974617004}]}, {"text": "Last, feature representation fora question can fail when the question consists of rare n-grams.", "labels": [], "entities": []}, {"text": "The two training sets shown in Section 3.2 are complementary: QP pairs provide more opportunities for us to learn the semantic associations between interrogative words and predicates.", "labels": [], "entities": []}, {"text": "Such resources are especially important for understanding NL-questions, as most of them start with such 5W1H words; on the other hand, MP pairs enrich the semantic associations between context information (n-gram features) and predicates.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Accuracy on the evaluation data", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9992300271987915}]}, {"text": " Table 4: Ablation of the relationship types", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9887312054634094}]}]}