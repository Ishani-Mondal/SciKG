{"title": [{"text": "Relieving the Computational Bottleneck: Joint Inference for Event Extraction with High-Dimensional Features", "labels": [], "entities": [{"text": "Event Extraction", "start_pos": 60, "end_pos": 76, "type": "TASK", "confidence": 0.7058736681938171}]}], "abstractContent": [{"text": "Several state-of-the-art event extraction systems employ models based on Support Vector Machines (SVMs) in a pipeline architecture , which fails to exploit the joint dependencies that typically exist among events and arguments.", "labels": [], "entities": [{"text": "event extraction", "start_pos": 25, "end_pos": 41, "type": "TASK", "confidence": 0.7689218521118164}]}, {"text": "While there have been attempts to overcome this limitation using Markov Logic Networks (MLNs), it remains challenging to perform joint inference in MLNs when the model encodes many high-dimensional sophisticated features such as those essential for event extraction.", "labels": [], "entities": [{"text": "event extraction", "start_pos": 249, "end_pos": 265, "type": "TASK", "confidence": 0.7400769591331482}]}, {"text": "In this paper, we propose anew model for event extraction that combines the power of MLNs and SVMs, dwarfing their limitations.", "labels": [], "entities": [{"text": "event extraction", "start_pos": 41, "end_pos": 57, "type": "TASK", "confidence": 0.8348365724086761}]}, {"text": "The key idea is to reliably learn and process high-dimensional features using SVMs; encode the output of SVMs as low-dimensional, soft formulas in MLNs; and use the superior joint in-ferencing power of MLNs to enforce joint consistency constraints over the soft formulas.", "labels": [], "entities": []}, {"text": "We evaluate our approach for the task of extracting biomedical events on the BioNLP 2013, 2011 and 2009 Genia shared task datasets.", "labels": [], "entities": [{"text": "extracting biomedical events", "start_pos": 41, "end_pos": 69, "type": "TASK", "confidence": 0.8553405404090881}, {"text": "BioNLP 2013, 2011 and 2009 Genia shared task datasets", "start_pos": 77, "end_pos": 130, "type": "DATASET", "confidence": 0.8813042342662811}]}, {"text": "Our approach yields the best F1 score to date on the BioNLP'13 (53.61) and BioNLP'11 (58.07) datasets and the second-best F1 score to date on the BioNLP'09 dataset (58.16).", "labels": [], "entities": [{"text": "F1 score", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.9823810458183289}, {"text": "BioNLP'13", "start_pos": 53, "end_pos": 62, "type": "DATASET", "confidence": 0.9162505269050598}, {"text": "BioNLP'11 (58.07) datasets", "start_pos": 75, "end_pos": 101, "type": "DATASET", "confidence": 0.6874262392520905}, {"text": "F1", "start_pos": 122, "end_pos": 124, "type": "METRIC", "confidence": 0.9973581433296204}, {"text": "BioNLP'09 dataset", "start_pos": 146, "end_pos": 163, "type": "DATASET", "confidence": 0.9821677803993225}]}], "introductionContent": [{"text": "Event extraction is the task of extracting and labeling all instances in a text document that correspond to a pre-defined event type.", "labels": [], "entities": [{"text": "Event extraction is the task of extracting and labeling all instances in a text document that correspond to a pre-defined event type", "start_pos": 0, "end_pos": 132, "type": "Description", "confidence": 0.7292869605801322}]}, {"text": "This task is quite challenging fora multitude of reasons: events are often nested, recursive and have several arguments; there is no clear distinction between arguments and events; etc.", "labels": [], "entities": []}, {"text": "For instance, consider the BioNLP Genia event extraction shared task.", "labels": [], "entities": [{"text": "BioNLP Genia event extraction shared task", "start_pos": 27, "end_pos": 68, "type": "TASK", "confidence": 0.7388858199119568}]}, {"text": "In this task, participants are asked to extract instances of a pre-defined set of biomedical events from text.", "labels": [], "entities": []}, {"text": "An event is identified by a keyword called the trigger and can have an arbitrary number of arguments that correspond to pre-defined argument types.", "labels": [], "entities": []}, {"text": "The task is complicated by the fact that an event may serve as an argument of another event (nested events).", "labels": [], "entities": []}, {"text": "An example of the task is shown in.", "labels": [], "entities": []}, {"text": "As we can see, event E13 takes as arguments two events, E14 and E12, which in turn has E11 as one of its arguments.", "labels": [], "entities": []}, {"text": "A standard method that has been frequently employed to perform this shared task uses a pipeline architecture with three steps: (1) detect if a token is a trigger and assign a trigger type label to it; (2) for every detected trigger, determine all its arguments and assign types to each detected argument; and (3) combine the extracted triggers and arguments to obtain events.", "labels": [], "entities": []}, {"text": "Though adopted by the top-performing systems such as the highest scoring system on the BioNLP'13 Genia shared task , this approach is problematic for at least two reasons.", "labels": [], "entities": [{"text": "BioNLP'13 Genia shared task", "start_pos": 87, "end_pos": 114, "type": "DATASET", "confidence": 0.8480506837368011}]}, {"text": "First, as is typical in pipeline architectures, errors may propagate from one stage to the next.", "labels": [], "entities": []}, {"text": "Second, since each event/argument is identified and assigned a type independently of the others, it fails to capture the relationship between a trigger and its neighboring triggers, an argument and its neighboring arguments, etc.", "labels": [], "entities": []}, {"text": "More recently, researchers have investigated joint inference techniques for event extraction using Markov Logic Networks (MLNs) (e.g.,,,), a statistical relational model that enables us to model the dependencies between different instances of a data sample.", "labels": [], "entities": [{"text": "event extraction", "start_pos": 76, "end_pos": 92, "type": "TASK", "confidence": 0.7189071327447891}]}, {"text": "However, it is extremely challenging to make joint inference using MLNs work well in practice.", "labels": [], "entities": []}, {"text": "One reason is that it is generally difficult to model sophisticated linguistic features using MLNs.", "labels": [], "entities": []}, {"text": "demonstrated that HOIL-1L interacting protein (HOIP), a ubiquitin ligase that can catalyze the assembly of linear polyubiquitin chains, is recruited to DC40 in a TRAF2-dependent manner following engagement of CD40 . .", "labels": [], "entities": []}, {"text": ".: Example of event extraction in the BioNLP Genia task.", "labels": [], "entities": [{"text": "event extraction", "start_pos": 14, "end_pos": 30, "type": "TASK", "confidence": 0.7561942040920258}]}, {"text": "The table in (b) shows all the events extracted from sentence (a).", "labels": [], "entities": []}, {"text": "Note that successful extraction of E13 depends on E12 and E14.", "labels": [], "entities": [{"text": "E13", "start_pos": 35, "end_pos": 38, "type": "DATASET", "confidence": 0.8342245221138}]}, {"text": "culty stems from the fact that some of these features are extremely high dimensional (e.g.,,,,,), and to reliably learn weights of formulas that encode such features, one would require an enormous number of data samples.", "labels": [], "entities": []}, {"text": "Moreover, even the complexity of approximate inference on such models is quite high, often prohibitively so.", "labels": [], "entities": []}, {"text": "For example, a trigram can be encoded as an MLN formula, Word( For any given position (p), this formula has W 3 groundings, where Wis the number of possible words, making it too large for learning/inference.", "labels": [], "entities": []}, {"text": "Therefore, current MLN-based systems tend to include a highly simplified model ignoring powerful linguistic features.", "labels": [], "entities": []}, {"text": "This is problematic because such features are essential for event extraction.", "labels": [], "entities": [{"text": "event extraction", "start_pos": 60, "end_pos": 76, "type": "TASK", "confidence": 0.8211056888103485}]}, {"text": "Our contributions in this paper are two-fold.", "labels": [], "entities": []}, {"text": "First, we propose a novel model for biomedical event extraction based on MLNs that addresses the aforementioned limitations by leveraging the power of Support Vector Machines (SVMs)) to handle high-dimensional features.", "labels": [], "entities": [{"text": "biomedical event extraction", "start_pos": 36, "end_pos": 63, "type": "TASK", "confidence": 0.757400373617808}]}, {"text": "Specifically, we (1) learn SVM models using rich linguistic features for trigger and argument detection and type labeling; (2) design an MLN composed of soft formulas (each of which encodes a soft constraint whose associated weight indicates how important it is to satisfy the constraint) and hard formulas (constraints that always need to be satisfied, thus having a weight of \u221e) to capture the relational dependencies between triggers and arguments; and (3) encode the SVM output as prior knowledge in the MLN in the form of soft formulas, whose weights are computed using the confidence values generated by the SVMs.", "labels": [], "entities": [{"text": "trigger and argument detection", "start_pos": 73, "end_pos": 103, "type": "TASK", "confidence": 0.7038174867630005}, {"text": "type labeling", "start_pos": 108, "end_pos": 121, "type": "TASK", "confidence": 0.7332637161016464}]}, {"text": "This formulation naturally allows SVMs and MLNs to complement each other's strengths and weaknesses: learning in a large and sparse feature space is much easier with SVMs than with MLNs, whereas modeling relational dependencies is much easier with MLNs than with SVMs.", "labels": [], "entities": []}, {"text": "Our second contribution concerns making inference with this MLN feasible.", "labels": [], "entities": []}, {"text": "Recall that inference involves detecting and assigning the type label to all the triggers and arguments.", "labels": [], "entities": []}, {"text": "We show that existing Maximum-a-posteriori (MAP) inference methods, even the most advanced approximate ones (e.g.,,, ), are infeasible on our proposed MLN because of their high memory cost.", "labels": [], "entities": []}, {"text": "Consequently, we identify decompositions of the MLN into disconnected components and solve each independently, thereby drastically reducing the memory requirements.", "labels": [], "entities": []}, {"text": "We evaluate our approach on the BioNLP 2009, 2011 and 2013 Genia shared task datasets.", "labels": [], "entities": [{"text": "BioNLP 2009, 2011 and 2013 Genia shared task datasets", "start_pos": 32, "end_pos": 85, "type": "DATASET", "confidence": 0.8590125918388367}]}, {"text": "On the BioNLP'13 dataset, our model significantly outperforms state-of-the-art pipeline approaches and achieves the best F1 score to date.", "labels": [], "entities": [{"text": "BioNLP'13 dataset", "start_pos": 7, "end_pos": 24, "type": "DATASET", "confidence": 0.9729364514350891}, {"text": "F1 score", "start_pos": 121, "end_pos": 129, "type": "METRIC", "confidence": 0.9858847260475159}]}, {"text": "On the BioNLP'11 and BioNLP'09 datasets, our scores are slightly better and slightly worse respectively than the best reported results.", "labels": [], "entities": [{"text": "BioNLP'11", "start_pos": 7, "end_pos": 16, "type": "DATASET", "confidence": 0.9036361575126648}, {"text": "BioNLP'09 datasets", "start_pos": 21, "end_pos": 39, "type": "DATASET", "confidence": 0.9512825012207031}]}, {"text": "However, they are significantly better than state-of-the-art MLNbased systems.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our system on the BioNLP'13 ( ), '11 () and '09 (Kim: Statistics on the BioNLP datasets, which consist of annotated papers/abstracts from PubMed.", "labels": [], "entities": [{"text": "BioNLP'13", "start_pos": 30, "end_pos": 39, "type": "DATASET", "confidence": 0.873202919960022}, {"text": "BioNLP datasets", "start_pos": 84, "end_pos": 99, "type": "DATASET", "confidence": 0.940883070230484}, {"text": "PubMed", "start_pos": 150, "end_pos": 156, "type": "DATASET", "confidence": 0.9502530694007874}]}, {"text": "(x, y, z): x in training, yin development and z in test.", "labels": [], "entities": []}, {"text": "#TT indicates the total number of trigger types.", "labels": [], "entities": [{"text": "TT", "start_pos": 1, "end_pos": 3, "type": "METRIC", "confidence": 0.9676238894462585}]}, {"text": "The total number of argument types is 2.", "labels": [], "entities": []}, {"text": "et al., 2009) Genia datasets for the main event extraction shared task.", "labels": [], "entities": [{"text": "main event extraction shared task", "start_pos": 37, "end_pos": 70, "type": "TASK", "confidence": 0.838408601284027}]}, {"text": "Note that this task is the most important one for Genia and therefore has the most active participation.", "labels": [], "entities": [{"text": "Genia", "start_pos": 50, "end_pos": 55, "type": "TASK", "confidence": 0.9285100102424622}]}, {"text": "Statistics on the datasets are shown in.", "labels": [], "entities": []}, {"text": "All our evaluations use the online tool provided by the shared task organizers.", "labels": [], "entities": []}, {"text": "We report scores obtained using the approximate span, recursive evaluation.", "labels": [], "entities": []}, {"text": "To generate features, we employ the supporting resources provided by the organizers.", "labels": [], "entities": []}, {"text": "Specifically, sentence split and tokenization are done using the GENIA tools, while part-of-speech information is provided by the BLLIP parser that uses the self-trained biomedical model.", "labels": [], "entities": [{"text": "sentence split", "start_pos": 14, "end_pos": 28, "type": "TASK", "confidence": 0.7743758857250214}, {"text": "tokenization", "start_pos": 33, "end_pos": 45, "type": "TASK", "confidence": 0.9561697244644165}, {"text": "GENIA", "start_pos": 65, "end_pos": 70, "type": "DATASET", "confidence": 0.8721084594726562}, {"text": "BLLIP", "start_pos": 130, "end_pos": 135, "type": "METRIC", "confidence": 0.766626238822937}]}, {"text": "Also, we create dependency features from the parse trees provided by two dependency parsers, the Enju parser ( and the aforementioned BLLIP parser that uses the selftrained biomedical model, which results in two sets of dependency features.", "labels": [], "entities": []}, {"text": "For MAP inference, we use Gurobi, a parallelized ILP solver.", "labels": [], "entities": [{"text": "MAP inference", "start_pos": 4, "end_pos": 17, "type": "TASK", "confidence": 0.9736813306808472}, {"text": "ILP solver", "start_pos": 49, "end_pos": 59, "type": "TASK", "confidence": 0.6210261434316635}]}, {"text": "After inference, a postprocessing step is required to generate biomedical events from the extracted triggers and arguments.", "labels": [], "entities": []}, {"text": "Specifically, for binding events, we employ a learning-based method similar to, while for the other events, we employ a rule-based approach similar to.", "labels": [], "entities": []}, {"text": "Both the SVM baseline system and the combined MLN+SVM system employ the same post-processing strategy.", "labels": [], "entities": []}, {"text": "During weight learning, in order to combat the problem of different initializations yielding radically different parameter estimates, we start at several different initialization points and average the weights obtained after 100 iterations of gradient descent.", "labels": [], "entities": [{"text": "weight learning", "start_pos": 7, "end_pos": 22, "type": "TASK", "confidence": 0.7841270864009857}]}, {"text": "However, we noticed that if we simply choose random initialization points, the variance of the weights was quite high and some initialization points were much worse than others.", "labels": [], "entities": []}, {"text": "To counter this, we use the following method to systematically  initialize the weights.", "labels": [], "entities": []}, {"text": "Let n i be the number of satisfied groundings of formula f i in the training data and mi be the total number of possible groundings off i . We use a threshold \u03b3 to determine whether we wish to make the initial weight positive or negative.", "labels": [], "entities": []}, {"text": "If n i mi \u2264 \u03b3, then we choose the initial weight uniformly at random from the range.", "labels": [], "entities": []}, {"text": "Otherwise, we chose it from the range [0, 0.1].", "labels": [], "entities": []}, {"text": "These steps ensure that the weights generated from different initialization points have smaller variance.", "labels": [], "entities": []}, {"text": "Also, in the testing phase, we set the scale parameters for the soft evidence as \u03b1 = \u03b2 = max c\u2208C |c|, where C is the set of SVM confidence values.", "labels": [], "entities": []}, {"text": "Among the three datasets, the BioNLP'13 dataset is most \"realistic\" one because it is the only one that contains full papers and no abstracts.", "labels": [], "entities": [{"text": "BioNLP'13 dataset", "start_pos": 30, "end_pos": 47, "type": "DATASET", "confidence": 0.9689943492412567}]}, {"text": "As a result, it is also the most challenging dataset among the three.", "labels": [], "entities": []}, {"text": "shows the results of our system along with the results of other top systems published in the official evaluation of BioNLP'13.", "labels": [], "entities": [{"text": "BioNLP'13", "start_pos": 116, "end_pos": 125, "type": "DATASET", "confidence": 0.8340908288955688}]}, {"text": "Our system achieves the best F1-score (an improvement of 2.64 points over the top-performing system) and has a much higher recall (mainly because our system detects more regulation events which outnumber other event types in the dataset) and a slightly higher precision than the winning system.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.9993404746055603}, {"text": "recall", "start_pos": 123, "end_pos": 129, "type": "METRIC", "confidence": 0.9996618032455444}, {"text": "precision", "start_pos": 260, "end_pos": 269, "type": "METRIC", "confidence": 0.9993995428085327}]}, {"text": "Of the top five teams, NCBI is the only other joint inference system, which adopts joint pattern matching to predict triggers and arguments at the same time.", "labels": [], "entities": [{"text": "NCBI", "start_pos": 23, "end_pos": 27, "type": "DATASET", "confidence": 0.961719274520874}]}, {"text": "These results illustrate the challenge in using joint inference effectively.", "labels": [], "entities": []}, {"text": "NCBI performed much worse than the SVM-based pipeline systems, EVEX and TEES2.1.", "labels": [], "entities": [{"text": "NCBI", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9602450132369995}, {"text": "EVEX", "start_pos": 63, "end_pos": 67, "type": "DATASET", "confidence": 0.8988437652587891}, {"text": "TEES2.1", "start_pos": 72, "end_pos": 79, "type": "DATASET", "confidence": 0.7604576945304871}]}, {"text": "It was also worse than BIOSEM, a rulebased system that uses considerable domain expertise.", "labels": [], "entities": [{"text": "BIOSEM", "start_pos": 23, "end_pos": 29, "type": "DATASET", "confidence": 0.7722856998443604}]}, {"text": "Nevertheless, it was better than DLUTNLP, another SVM-based system.", "labels": [], "entities": [{"text": "DLUTNLP", "start_pos": 33, "end_pos": 40, "type": "DATASET", "confidence": 0.8375517129898071}]}, {"text": "compares our baseline pipeline model with our combined model.", "labels": [], "entities": []}, {"text": "We can clearly see that the combined model has a significantly better F1 score than the pipeline model on most event types.: Results on the BioNLP'11 test data.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 70, "end_pos": 78, "type": "METRIC", "confidence": 0.9883723258972168}, {"text": "BioNLP'11 test data", "start_pos": 140, "end_pos": 159, "type": "DATASET", "confidence": 0.9664743940035502}]}, {"text": "The regulation events are considered the most complex events to detect because they have a recursive structure.", "labels": [], "entities": []}, {"text": "At the same time, this structure yields a large number of joint dependencies.", "labels": [], "entities": []}, {"text": "The advantage of using a rich model such as MLNs can be clearly seen in this case; the combined model yields a 10 point and 6 point increase in F1-score on the test data and development data respectively compared to the pipeline model.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 144, "end_pos": 152, "type": "METRIC", "confidence": 0.9994413256645203}]}, {"text": "shows the results on the BioNLP'11 dataset.", "labels": [], "entities": [{"text": "BioNLP'11 dataset", "start_pos": 25, "end_pos": 42, "type": "DATASET", "confidence": 0.9710713624954224}]}, {"text": "We can see that our system is marginally better than Miwa12, which is a pipeline-based system.", "labels": [], "entities": [{"text": "Miwa12", "start_pos": 53, "end_pos": 59, "type": "DATASET", "confidence": 0.9252130389213562}]}, {"text": "It is also more than two points better than Riedel11, a state-of-the-art structured prediction-based joint inference system.", "labels": [], "entities": []}, {"text": "Reidel11 incorporates the Stanford predictions) as features in the model.", "labels": [], "entities": [{"text": "Reidel11", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9386448264122009}]}, {"text": "On the two hardest, most complex tasks, detecting regulation events (which have recursive structures and more joint dependencies than other event types) and detecting binding events (which may have multiple arguments), our system performs better than both Miwa12 and Riedel11.", "labels": [], "entities": [{"text": "Miwa12", "start_pos": 256, "end_pos": 262, "type": "DATASET", "confidence": 0.9445933699607849}]}, {"text": "4 Specifically, our system's F1 score for regulation events is 46.84, while those of Miwa12 and Riedel11 are 45.46 and 44.94 respectively.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.9881568253040314}, {"text": "Miwa12", "start_pos": 85, "end_pos": 91, "type": "DATASET", "confidence": 0.9483523368835449}]}, {"text": "Our system's F1 score for the binding event is 58.79, while those of Miwa12 and Riedel11 are 56.64 and 48.49 respectively.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.9864673018455505}, {"text": "binding event", "start_pos": 30, "end_pos": 43, "type": "TASK", "confidence": 0.8278958797454834}, {"text": "Miwa12", "start_pos": 69, "end_pos": 75, "type": "DATASET", "confidence": 0.9466354250907898}]}, {"text": "These results clearly demonstrate the effectiveness of enforcing joint dependencies along with high-dimensional features.", "labels": [], "entities": []}, {"text": "shows the results on the BioNLP'09 dataset.", "labels": [], "entities": [{"text": "BioNLP'09 dataset", "start_pos": 25, "end_pos": 42, "type": "DATASET", "confidence": 0.9686212539672852}]}, {"text": "Our system has a marginally lower score (by 0.11 points) than Miwa12, which is the best performing system on this dataset.", "labels": [], "entities": [{"text": "Miwa12", "start_pos": 62, "end_pos": 68, "type": "DATASET", "confidence": 0.8983732461929321}]}, {"text": "Specifically, our system achieves a higher recall but a lower precision than Miwa12.", "labels": [], "entities": [{"text": "recall", "start_pos": 43, "end_pos": 49, "type": "METRIC", "confidence": 0.9996597766876221}, {"text": "precision", "start_pos": 62, "end_pos": 71, "type": "METRIC", "confidence": 0.9993476271629333}, {"text": "Miwa12", "start_pos": 77, "end_pos": 83, "type": "DATASET", "confidence": 0.9377420544624329}]}, {"text": "However, note that Miwa12 used coreference features while we are able to achieve  similar accuracy without the use of co-reference data.", "labels": [], "entities": [{"text": "Miwa12", "start_pos": 19, "end_pos": 25, "type": "DATASET", "confidence": 0.9001452326774597}, {"text": "accuracy", "start_pos": 90, "end_pos": 98, "type": "METRIC", "confidence": 0.9977967739105225}]}, {"text": "The F1 score of Miwa10, which does not use co-reference features, is nearly 2 points lower than that of our system.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9813067018985748}, {"text": "Miwa10", "start_pos": 16, "end_pos": 22, "type": "DATASET", "confidence": 0.9291609525680542}]}, {"text": "Our system also has a higher F1 score than Reidel11, which is the best joint inference-based system for this task.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.990840345621109}]}, {"text": "On the regulation events, our system (47.55) outperforms both Miwa12 (45.99) and, while on the binding event, our system (59.88) is marginally worse than Miwa12 (59.91) and significantly better than Riedel11 (52.6).", "labels": [], "entities": [{"text": "Miwa12", "start_pos": 62, "end_pos": 68, "type": "DATASET", "confidence": 0.933686375617981}, {"text": "Miwa12", "start_pos": 154, "end_pos": 160, "type": "DATASET", "confidence": 0.9183094501495361}]}, {"text": "As mentioned earlier, these are the hardest events to extract.", "labels": [], "entities": []}, {"text": "Also, existing MLN-based joint inference systems such as RiedelMLN and PoonMLN do not achieve stateof-the-art results because they do not leverage complex, high-dimensional features.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Recall (Rec.), Precision (Prec.) and F1  score on the BioNLP'13 test data.", "labels": [], "entities": [{"text": "Recall (Rec.)", "start_pos": 10, "end_pos": 23, "type": "METRIC", "confidence": 0.9071028530597687}, {"text": "Precision (Prec.)", "start_pos": 25, "end_pos": 42, "type": "METRIC", "confidence": 0.9530336856842041}, {"text": "F1  score", "start_pos": 47, "end_pos": 56, "type": "METRIC", "confidence": 0.9767674505710602}, {"text": "BioNLP'13 test data", "start_pos": 64, "end_pos": 83, "type": "DATASET", "confidence": 0.9499396483103434}]}, {"text": " Table 4: Results on the BioNLP'11 test data.", "labels": [], "entities": [{"text": "BioNLP'11 test data", "start_pos": 25, "end_pos": 44, "type": "DATASET", "confidence": 0.9430789947509766}]}, {"text": " Table 5: Results on the BioNLP'09 test data. \"\u2212\"  indicates that the corresponding values are not  known.", "labels": [], "entities": [{"text": "BioNLP'09 test data", "start_pos": 25, "end_pos": 44, "type": "DATASET", "confidence": 0.9378426273663839}]}]}