{"title": [{"text": "Chinese Zero Pronoun Resolution: An Unsupervised Probabilistic Model Rivaling Supervised Resolvers", "labels": [], "entities": [{"text": "Chinese Zero Pronoun Resolution", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.8121677041053772}]}], "abstractContent": [{"text": "State-of-the-art Chinese zero pronoun resolution systems are supervised, thus relying on training data containing manually resolved zero pronouns.", "labels": [], "entities": [{"text": "Chinese zero pronoun resolution", "start_pos": 17, "end_pos": 48, "type": "TASK", "confidence": 0.6239063739776611}]}, {"text": "To eliminate the reliance on annotated data, we present a generative model for unsuper-vised Chinese zero pronoun resolution.", "labels": [], "entities": [{"text": "Chinese zero pronoun resolution", "start_pos": 93, "end_pos": 124, "type": "TASK", "confidence": 0.5353148579597473}]}, {"text": "At the core of our model is a novel hypothesis: a probabilistic pronoun resolver trained on overt pronouns in an unsuper-vised manner can be used to resolve zero pronouns.", "labels": [], "entities": []}, {"text": "Experiments demonstrate that our unsupervised model rivals its state-of-the-art supervised counterparts in performance when resolving the Chinese zero pronouns in the OntoNotes corpus.", "labels": [], "entities": [{"text": "OntoNotes corpus", "start_pos": 167, "end_pos": 183, "type": "DATASET", "confidence": 0.8997121155261993}]}], "introductionContent": [{"text": "A zero pronoun (ZP) is a gap in a sentence that is found when a phonetically null form is used to refer to a real-world entity.", "labels": [], "entities": []}, {"text": "An anaphoric zero pronoun (AZP) is a ZP that corefers with one or more preceding noun phrases (NPs) in the associated text.", "labels": [], "entities": []}, {"text": "Below is an example taken from the Chinese TreeBank (CTB), where the ZP (denoted as *pro*) refers to \u4fc4\u7f57\u65af (Russia).", "labels": [], "entities": [{"text": "Chinese TreeBank (CTB)", "start_pos": 35, "end_pos": 57, "type": "DATASET", "confidence": 0.9317677021026611}]}, {"text": "[\u4fc4\u7f57\u65af] \u4f5c\u4e3a\u7c73\u6d1b\u820d\u592b\u7ef4\u5947\u4e00\u8d2f\u7684\u652f\u6301\u8005\uff0c *pro* \u66fe\u7ecf\u63d0\u51fa\u8c03\u505c\u8fd9\u573a\u653f\u6cbb\u5371\u673a\u3002", "labels": [], "entities": []}, {"text": "( is a consistent supporter of Milo\u0161evi\u0107, *pro* has proposed to mediate the political crisis.)", "labels": [], "entities": []}, {"text": "As we can see, ZPs lack grammatical attributes that are useful for overt pronoun resolution such as number and gender.", "labels": [], "entities": [{"text": "overt pronoun resolution", "start_pos": 67, "end_pos": 91, "type": "TASK", "confidence": 0.832105795542399}]}, {"text": "This makes ZP resolution more challenging than overt pronoun resolution.", "labels": [], "entities": [{"text": "ZP resolution", "start_pos": 11, "end_pos": 24, "type": "TASK", "confidence": 0.7951128780841827}, {"text": "overt pronoun resolution", "start_pos": 47, "end_pos": 71, "type": "TASK", "confidence": 0.7802431384722391}]}, {"text": "Automatic ZP resolution is typically composed of two steps.", "labels": [], "entities": [{"text": "ZP resolution", "start_pos": 10, "end_pos": 23, "type": "TASK", "confidence": 0.8163740634918213}]}, {"text": "The first step, AZP identification, involves extracting ZPs that are anaphoric.", "labels": [], "entities": [{"text": "AZP identification", "start_pos": 16, "end_pos": 34, "type": "TASK", "confidence": 0.8577326238155365}]}, {"text": "The second step, AZP resolution, aims to identify an antecedent of an AZP.", "labels": [], "entities": [{"text": "AZP resolution", "start_pos": 17, "end_pos": 31, "type": "TASK", "confidence": 0.834565281867981}]}, {"text": "State-of-the-art ZP resolvers have tackled both of these steps in a supervised manner, training a classifier for AZP identification and another one for AZP resolution (e.g.,,).", "labels": [], "entities": [{"text": "ZP resolvers", "start_pos": 17, "end_pos": 29, "type": "TASK", "confidence": 0.8251606822013855}, {"text": "AZP identification", "start_pos": 113, "end_pos": 131, "type": "TASK", "confidence": 0.7069961130619049}, {"text": "AZP resolution", "start_pos": 152, "end_pos": 166, "type": "TASK", "confidence": 0.6993774622678757}]}, {"text": "In this paper, we focus on the second task, AZP resolution, designing a model that assumes as input the AZPs in a document and resolves each of them.", "labels": [], "entities": [{"text": "AZP resolution", "start_pos": 44, "end_pos": 58, "type": "TASK", "confidence": 0.7667085826396942}]}, {"text": "Note that the task of AZP resolution alone is by no means easy: even when gold-standard AZPs are given, state-of-the-art supervised resolvers can only achieve an F-score of 47.7% for resolving Chinese AZPs.", "labels": [], "entities": [{"text": "AZP resolution", "start_pos": 22, "end_pos": 36, "type": "TASK", "confidence": 0.8616389036178589}, {"text": "F-score", "start_pos": 162, "end_pos": 169, "type": "METRIC", "confidence": 0.99888676404953}]}, {"text": "For the sake of completeness, we will evaluate our AZP resolution model using both gold-standard AZPs as well as AZPs automatically identified by a rule-based approach that we propose in this paper.", "labels": [], "entities": [{"text": "AZP resolution", "start_pos": 51, "end_pos": 65, "type": "TASK", "confidence": 0.637763500213623}]}, {"text": "Our contribution lies in the proposal of the first unsupervised probabilistic model for AZP resolution that rivals its supervised counterparts in performance when evaluated on the Chinese portion of the OntoNotes 5.0 corpus.", "labels": [], "entities": [{"text": "AZP resolution", "start_pos": 88, "end_pos": 102, "type": "TASK", "confidence": 0.8057667016983032}, {"text": "Chinese portion of the OntoNotes 5.0 corpus", "start_pos": 180, "end_pos": 223, "type": "DATASET", "confidence": 0.6483882410185677}]}, {"text": "Its main advantage is that it does not require training data with manually resolved AZPs.", "labels": [], "entities": []}, {"text": "This, together with the fact that its underlying generative process is not language-dependent, enables it to be applied to languages where such annotated data is not readily available.", "labels": [], "entities": []}, {"text": "At its core is a novel hypothesis: we can apply a probabilistic pronoun resolution model trained on overt pronouns in an unsupervised manner to resolve zero pronouns.", "labels": [], "entities": [{"text": "probabilistic pronoun resolution", "start_pos": 50, "end_pos": 82, "type": "TASK", "confidence": 0.6732070744037628}]}, {"text": "Motivated by and work on unsupervised English pronoun resolution, we train our unsupervised resolver on Chinese overt pronouns using the Expectation-Maximization (EM) algorithm).", "labels": [], "entities": [{"text": "English pronoun resolution", "start_pos": 38, "end_pos": 64, "type": "TASK", "confidence": 0.6493832270304362}]}], "datasetContent": [{"text": "We employ the Chinese portion of the OntoNotes 5.0 corpus that was used in the official CoNLL-2012 shared task ().", "labels": [], "entities": [{"text": "OntoNotes 5.0 corpus", "start_pos": 37, "end_pos": 57, "type": "DATASET", "confidence": 0.7966722846031189}]}, {"text": "In the CoNLL-2012 data, the training set and development set contain ZP coreference annotations, but the test set does not.", "labels": [], "entities": [{"text": "CoNLL-2012 data", "start_pos": 7, "end_pos": 22, "type": "DATASET", "confidence": 0.9641441702842712}]}, {"text": "Therefore, we train our models on the training set and perform evaluation on the development set.", "labels": [], "entities": []}, {"text": "Statistics on the datasets are shown in.", "labels": [], "entities": []}, {"text": "The documents in these datasets come from six sources, namely Broadcast News (BN), Newswire (NW), Broadcast Conversation (BC), Telephone Conversation (TC), Web Blog (WB) and Magazine (MZ).", "labels": [], "entities": [{"text": "Broadcast News (BN)", "start_pos": 62, "end_pos": 81, "type": "DATASET", "confidence": 0.8965570211410523}, {"text": "Magazine (MZ)", "start_pos": 174, "end_pos": 187, "type": "DATASET", "confidence": 0.7372658848762512}]}, {"text": "We sort the candidate antecedents of p as follows.", "labels": [], "entities": []}, {"text": "We first consider the subject candidate antecedents in the same sentence asp from right to left, then the other candidate antecedents in the same sentence from right to left.", "labels": [], "entities": []}, {"text": "Next, we consider the candidate antecedents in the previous sentence, also preferring candidates that are subjects, but in left-to-right order.", "labels": [], "entities": []}, {"text": "Finally, we consider the candidate antecedents two sentences back, following the subject-first, left-to-right order.", "labels": [], "entities": []}, {"text": "We express the results of ZP resolution in terms of recall (R), precision (P) and F-score (F).", "labels": [], "entities": [{"text": "ZP resolution", "start_pos": 26, "end_pos": 39, "type": "TASK", "confidence": 0.8172211050987244}, {"text": "recall (R)", "start_pos": 52, "end_pos": 62, "type": "METRIC", "confidence": 0.940927043557167}, {"text": "precision (P)", "start_pos": 64, "end_pos": 77, "type": "METRIC", "confidence": 0.9544002711772919}, {"text": "F-score (F)", "start_pos": 82, "end_pos": 93, "type": "METRIC", "confidence": 0.9567774385213852}]}, {"text": "Following, we evaluate our model in three settings.", "labels": [], "entities": []}, {"text": "In Setting 1, we assume the availability of gold syntactic parse trees and gold AZPs.", "labels": [], "entities": [{"text": "AZPs", "start_pos": 80, "end_pos": 84, "type": "METRIC", "confidence": 0.860028088092804}]}, {"text": "In Setting 2, we employ gold syntactic parse trees and system (i.e., automatically identified) AZPs.", "labels": [], "entities": []}, {"text": "Finally, in Setting 3, we employ system syntactic parse trees and system AZPs.", "labels": [], "entities": []}, {"text": "The gold and system syntactic parse trees, as well as the gold AZPs, are obtained from the CoNLL-2012 shared task dataset, while the system AZPs are identified by the rule-based approach described in the Appendix.", "labels": [], "entities": [{"text": "CoNLL-2012 shared task dataset", "start_pos": 91, "end_pos": 121, "type": "DATASET", "confidence": 0.8584427684545517}]}, {"text": "Since our AZP identification approach does not rely on any labeled data, we are effectively evaluating an endto-end unsupervised AZP resolver in Setting 3.", "labels": [], "entities": [{"text": "AZP identification", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.8155737817287445}]}, {"text": "Impact of P (p a |c a , l=1) and P (l=1|k c ).", "labels": [], "entities": []}, {"text": "Recall that our model is composed of five probability terms, P (p a |c a , l=1) for each of the four grammatical attributes and P (l=1|k c ), the context probability.", "labels": [], "entities": []}, {"text": "To investigate the contribution of context and each attribute to overall performance, we conduct ablation experiments.", "labels": [], "entities": []}, {"text": "Specifically, in each ablation experiment, we remove exactly one probability term from the model and retrain it.", "labels": [], "entities": []}, {"text": "Note that it is difficult to directly compare the outputs produced under Settings 2 and 3: the AZPs identified by the best baseline are quite different from those identified by our rule-based system, as can be inferred from the AZP identification results in  Ablation results under Settings 1 and 3 are shown in.", "labels": [], "entities": [{"text": "AZPs", "start_pos": 95, "end_pos": 99, "type": "METRIC", "confidence": 0.9258936047554016}, {"text": "Ablation", "start_pos": 259, "end_pos": 267, "type": "METRIC", "confidence": 0.8164626359939575}]}, {"text": "As we can see, under Setting 1, after Number is ablated, performance does not drop.", "labels": [], "entities": []}, {"text": "We attribute this to the fact that almost all candidate antecedents are singular.", "labels": [], "entities": []}, {"text": "On the other hand, when we ablate any of the remaining three attributes, performance drops significantly by 2.3\u22123.0% in overall F-score.", "labels": [], "entities": [{"text": "F-score", "start_pos": 128, "end_pos": 135, "type": "METRIC", "confidence": 0.9977849721908569}]}, {"text": "11 Similar trends can be observed with respect to Setting 3: after Number is ablated, performance only decreases by 0.2%, while ablating any of the other three attributes results in a drop of 0.6%.", "labels": [], "entities": []}, {"text": "Results after ablating context are shown in the last row of.", "labels": [], "entities": []}, {"text": "As we can see, the F-score drops significantly by 14.7% and 3.8% under Settings 1 and 3 respectively.", "labels": [], "entities": [{"text": "F-score", "start_pos": 19, "end_pos": 26, "type": "METRIC", "confidence": 0.999265730381012}]}, {"text": "These results illustrate the importance of context features in our model.", "labels": [], "entities": []}, {"text": "Recall that we employed eight context features to encode the relationship between a pronoun and a candidate antecedent.", "labels": [], "entities": []}, {"text": "To determine the relative contribution of these eight features to overall performance, we conduct ablation experiments under Settings 1 and 3.", "labels": [], "entities": []}, {"text": "In these ablation experiments, all four grammatical attributes are retained in the model.", "labels": [], "entities": []}, {"text": "Ablation results are shown in rows 2\u22129 of Table 11.", "labels": [], "entities": [{"text": "Ablation", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9914044141769409}]}, {"text": "To facilitate comparison, the F-score of the model in which all eight context features are used is shown in row 1.", "labels": [], "entities": [{"text": "F-score", "start_pos": 30, "end_pos": 37, "type": "METRIC", "confidence": 0.9984594583511353}]}, {"text": "As we can see, feature 8 (the rule-based feature) is the most useful feature: its removal causes the F-scores of our resolver to drop significantly by 6.4% under Setting 1 and 1.5% under Setting 3.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 101, "end_pos": 109, "type": "METRIC", "confidence": 0.9728094935417175}]}], "tableCaptions": [{"text": " Table 2: Learned values of P (p Ani |c Ani , l=1).", "labels": [], "entities": []}, {"text": " Table 3: Learned values of P (p Gen |c Gen , l=1).", "labels": [], "entities": []}, {"text": " Table 4: Learned values of P (p N um |c N um , l=1).", "labels": [], "entities": []}, {"text": " Table 5: Learned values of P (p P er |c P er , l=1)  (same speaker).", "labels": [], "entities": []}, {"text": " Table 6: Learned values of P (p P er |c P er , l=1)  (different speakers).", "labels": [], "entities": []}, {"text": " Table 7: Statistics on the training and test sets.", "labels": [], "entities": []}, {"text": " Table 8: AZP resolution results of the baseline systems on the test set.", "labels": [], "entities": [{"text": "AZP resolution", "start_pos": 10, "end_pos": 24, "type": "METRIC", "confidence": 0.8245564103126526}]}, {"text": " Table 9: AZP resolution results of the best baseline and our unsupervised model on the test set.", "labels": [], "entities": [{"text": "AZP", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9925468564033508}, {"text": "resolution", "start_pos": 14, "end_pos": 24, "type": "METRIC", "confidence": 0.5381024479866028}]}, {"text": " Table 10: Probability term ablation results.", "labels": [], "entities": [{"text": "Probability term ablation", "start_pos": 11, "end_pos": 36, "type": "TASK", "confidence": 0.5542179346084595}]}, {"text": " Table 11: Context feature ablation results.", "labels": [], "entities": []}, {"text": " Table 12: AZP identification results on the test set.", "labels": [], "entities": [{"text": "AZP", "start_pos": 11, "end_pos": 14, "type": "METRIC", "confidence": 0.9543665647506714}]}]}