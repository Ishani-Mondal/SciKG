{"title": [{"text": "Semantic Parsing with Relaxed Hybrid Trees", "labels": [], "entities": [{"text": "Semantic Parsing", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.816369354724884}]}], "abstractContent": [{"text": "We propose a novel model for parsing natural language sentences into their formal semantic representations.", "labels": [], "entities": [{"text": "parsing natural language sentences", "start_pos": 29, "end_pos": 63, "type": "TASK", "confidence": 0.8672469705343246}]}, {"text": "The model is able to perform integrated lexicon acquisition and semantic parsing, mapping each atomic element in a complete semantic representation to a contiguous word sequence in the input sentence in a re-cursive manner, where certain overlap-pings amongst such word sequences are allowed.", "labels": [], "entities": [{"text": "lexicon acquisition", "start_pos": 40, "end_pos": 59, "type": "TASK", "confidence": 0.7858388125896454}, {"text": "semantic parsing", "start_pos": 64, "end_pos": 80, "type": "TASK", "confidence": 0.7490999400615692}]}, {"text": "It defines distributions over the novel relaxed hybrid tree structures which jointly represent both sentences and semantics.", "labels": [], "entities": []}, {"text": "Such structures allow tractable dynamic programming algorithms to be developed for efficient learning and decoding.", "labels": [], "entities": []}, {"text": "Trained under a discriminative setting , our model is able to incorporate a rich set of features where certain unbounded long-distance dependencies can be captured in a principled manner.", "labels": [], "entities": []}, {"text": "We demonstrate through experiments that by exploiting a large collection of simple features, our model is shown to be competitive to previous works and achieves state-of-the-art performance on standard benchmark data across four different languages.", "labels": [], "entities": []}, {"text": "The system and code can be downloaded from http://statnlp.org/research/sp/.", "labels": [], "entities": []}], "introductionContent": [{"text": "Semantic parsing, the task of transforming natural language sentences into formal representations of their underlying semantics, is one of the classic goals for natural language processing and artificial intelligence.", "labels": [], "entities": [{"text": "Semantic parsing", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8576076626777649}, {"text": "natural language processing", "start_pos": 161, "end_pos": 188, "type": "TASK", "confidence": 0.692094067732493}]}, {"text": "This area of research recently has received a significant amount of attention.", "labels": [], "entities": []}, {"text": "Various models have been proposed over the past few years ( Following previous research efforts, we perform semantic parsing under a setting where the semantics for complete sentences are provided as training data, but detailed word-level semantic information is not explicitly given during the training phase.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 108, "end_pos": 124, "type": "TASK", "confidence": 0.7443962693214417}]}, {"text": "As one example, consider the following natural language sentence paired with its corresponding semantic representation: What rivers do not run through, traverse(stateid( tn )))) The training data consists of a set of sentences paired with semantic representations.", "labels": [], "entities": []}, {"text": "Our goal is to learn from such pairs a model, which can be effectively used for parsing novel sentences into their semantic representations.", "labels": [], "entities": []}, {"text": "Certain assumptions about the semantics are typically made.", "labels": [], "entities": []}, {"text": "One common assumption is that the semantics can be represented ascertain recursive structures such as trees, which consist of atomic semantic units as tree nodes.", "labels": [], "entities": []}, {"text": "For example, the above semantics can be converted into an equivalent tree structure as illustrated in.", "labels": [], "entities": []}, {"text": "We will provide more details about such tree structured semantic representations in Section 2.1.", "labels": [], "entities": []}, {"text": "Currently, most state-of-the-art approaches that deal with such tree structured semantic representations either cast the semantic parsing problem as a statistical string-to-string transformation problem (), which ignores the potentially useful structural information of the tree, or employ latent-variable models to capture the correspondences between words and tree nodes using a generative approach (.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 121, "end_pos": 137, "type": "TASK", "confidence": 0.7534966468811035}]}, {"text": "While generative models can be used to flexibly model the correspondences between individual words and semantic nodes of the tree, such an approach is limited to modeling local dependencies and is unable to flexibly incorporate a large set of potentially useful features.", "labels": [], "entities": []}, {"text": "In this work, we propose a novel model for parsing natural language into tree structured semantic representations.", "labels": [], "entities": [{"text": "parsing natural language into tree structured semantic representations", "start_pos": 43, "end_pos": 113, "type": "TASK", "confidence": 0.8333941549062729}]}, {"text": "Specifically, we propose a novel relaxed hybrid tree representation which jointly encodes both natural language sentences and semantics; such representations can be effectively learned with a latent-variable discriminative model where long-distance dependencies can be captured.", "labels": [], "entities": []}, {"text": "We present dynamic programming algorithms for efficient learning and decoding.", "labels": [], "entities": []}, {"text": "With a large collection of simple features, our model reports state-of-the-art results on benchmark data annotated with four different languages.", "labels": [], "entities": []}, {"text": "Furthermore, although we focus our discussions on semantic parsing in this work, our proposed model is a general.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 50, "end_pos": 66, "type": "TASK", "confidence": 0.7989910840988159}]}, {"text": "Essentially our model is a discriminative string-to-tree model which recursively maps overlapping contiguous word sequences to tree nodes at different levels, where efficient dynamic programming algorithms can be used.", "labels": [], "entities": []}, {"text": "Such a model may find applications in other areas of natural language processing, such as statistical machine translation and information extraction.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 90, "end_pos": 121, "type": "TASK", "confidence": 0.7275416453679403}, {"text": "information extraction", "start_pos": 126, "end_pos": 148, "type": "TASK", "confidence": 0.8545531928539276}]}], "datasetContent": [{"text": "We present evaluations on the standard GeoQuery dataset which is publicly available.", "labels": [], "entities": [{"text": "GeoQuery dataset", "start_pos": 39, "end_pos": 55, "type": "DATASET", "confidence": 0.9608814716339111}]}, {"text": "This dataset has been used for evaluations in various semantic parsing works (.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 54, "end_pos": 70, "type": "TASK", "confidence": 0.73175910115242}]}, {"text": "It consists of 880 natural language sentences paired with their corresponding formal semantic representations.", "labels": [], "entities": []}, {"text": "Each semantic representation is a tree structured representation derived from a Prolog query that can be used to interact with a database of U.S. geography facts for retrieving answers.", "labels": [], "entities": [{"text": "Prolog", "start_pos": 80, "end_pos": 86, "type": "DATASET", "confidence": 0.936600923538208}]}, {"text": "The original dataset was fully annotated in English, and recently Jones et al. released anew version of this dataset with three additional language annotations.", "labels": [], "entities": []}, {"text": "For all the experiments, we used the identical experimental setup as described in.", "labels": [], "entities": []}, {"text": "Specifically, we trained on 600 instances, and evaluated on the remaining 280.", "labels": [], "entities": []}, {"text": "We note that there exist two different versions of the GeoQuery dataset annotated with completely different semantic representations.", "labels": [], "entities": [{"text": "GeoQuery dataset", "start_pos": 55, "end_pos": 71, "type": "DATASET", "confidence": 0.9198164641857147}]}, {"text": "Besides the version that we use in this work, which is annotated with tree structured semantic representations, the other version is annotated with lambda calculus expressions.", "labels": [], "entities": []}, {"text": "Results obtained from these two versions are not comparable.", "labels": [], "entities": []}, {"text": "Like many previous works, we focus on tree structured semantic representations for evaluations in this work since our model is designed for handling the class of semantic representations with recursive tree structures.", "labels": [], "entities": []}, {"text": "We used the standard evaluation criteria for judging the correctness of the outputs.", "labels": [], "entities": []}, {"text": "Specifically, our system constructs Prolog queries from the output parses, and uses such queries to retrieve answers from the GeoQuery database.", "labels": [], "entities": [{"text": "GeoQuery database", "start_pos": 126, "end_pos": 143, "type": "DATASET", "confidence": 0.9294893443584442}]}, {"text": "An output is considered correct if and only if it retrieves the: Performance on the benchmark data, using four different languages as inputs.", "labels": [], "entities": []}, {"text": "RHT: relaxed hybrid tree (this work).", "labels": [], "entities": [{"text": "RHT", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9262901544570923}]}, {"text": "same answers as the gold standard ().", "labels": [], "entities": []}, {"text": "We report accuracy scores -the percentage of inputs with correct answers, and F1 measuresthe harmonic mean of precision (the proportion of correct answers out of inputs with an answer) and recall (the proportion of correct answers out of all inputs).", "labels": [], "entities": [{"text": "accuracy scores", "start_pos": 10, "end_pos": 25, "type": "METRIC", "confidence": 0.9785033762454987}, {"text": "F1", "start_pos": 78, "end_pos": 80, "type": "METRIC", "confidence": 0.9993100166320801}, {"text": "precision", "start_pos": 110, "end_pos": 119, "type": "METRIC", "confidence": 0.9985504746437073}, {"text": "recall", "start_pos": 189, "end_pos": 195, "type": "METRIC", "confidence": 0.9994969367980957}]}, {"text": "By adopting such an evaluation method we will be able to directly compare our model's performance against those of the previous works.", "labels": [], "entities": []}, {"text": "The evaluations were conducted under such a setting in order to make comparisons to previous works.", "labels": [], "entities": []}, {"text": "We would like to stress that our model is designed for general-purpose semantic parsing that is not only natural language-independent, but also task-independent.", "labels": [], "entities": [{"text": "general-purpose semantic parsing", "start_pos": 55, "end_pos": 87, "type": "TASK", "confidence": 0.6343320608139038}]}, {"text": "We thus distinguish our work from several previous works in the literature which focused on semantic parsing under other assumptions.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 92, "end_pos": 108, "type": "TASK", "confidence": 0.807997465133667}]}, {"text": "Specifically, for example, works such as () essentially performed semantic parsing under different settings where the goal was to optimize the performance of certain downstream NLP tasks such as answering questions, and different semantic formalisms and languagespecific features were usually involved.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 66, "end_pos": 82, "type": "TASK", "confidence": 0.7298509031534195}]}, {"text": "For all our experiments, we used the L-BFGS algorithm for learning the feature weights, where feature weights were all initialized to zeros and the regularization hyper-parameter \u03ba was set to 0.01.", "labels": [], "entities": []}, {"text": "We set the maximum number of L-BFGS steps to 100.", "labels": [], "entities": []}, {"text": "When all the features are considered, our model creates over 2 million features for each language on the dataset (English: 2.1M, Thai: 2.3M, German: 2.7M, Greek: 2.6M).", "labels": [], "entities": []}, {"text": "Our model requires (on average) a per-instance learning time of 0.428 seconds and a per-instance decoding time of 0.235 seconds, on an Intel machine with a 2.2 GHz CPU.", "labels": [], "entities": []}, {"text": "Our implementation is in Java.", "labels": [], "entities": []}, {"text": "Here the per-instance learning time refers to the time spent on computing the instance-level log-likelihood as well as the expected feature counts (needed for the gradients).", "labels": [], "entities": []}, {"text": "shows the evaluation results of our system as well as those of several other comparable previous works which share the same experimental setup as ours.", "labels": [], "entities": []}, {"text": "UBL-S is the system presented in which performs semantic parsing with the CCG based on mapping between graphs, and is the only non-tree based top-performing system.", "labels": [], "entities": [{"text": "UBL-S", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8405866026878357}, {"text": "semantic parsing", "start_pos": 48, "end_pos": 64, "type": "TASK", "confidence": 0.7124597430229187}]}, {"text": "Their system, similar to ours, also uses a discriminative log-linear model where two types of features are defined.", "labels": [], "entities": []}, {"text": "WASP is a model based on statistical phrase-based machine translation as we have described earlier.", "labels": [], "entities": [{"text": "WASP", "start_pos": 0, "end_pos": 4, "type": "TASK", "confidence": 0.6402785778045654}, {"text": "statistical phrase-based machine translation", "start_pos": 25, "end_pos": 69, "type": "TASK", "confidence": 0.599913701415062}]}, {"text": "The hybrid tree model (HYBRIDTREE+) performs learning using a generative process which is augmented with an additional discriminative-reranking stage, where certain global features are incorporated ().", "labels": [], "entities": []}, {"text": "The Bayesian tree transducer model (TREETRANS) learns under a Bayesian generative framework, using hyper-parameters manually tuned on the German training data.", "labels": [], "entities": [{"text": "TREETRANS", "start_pos": 36, "end_pos": 45, "type": "METRIC", "confidence": 0.9276179671287537}, {"text": "German training data", "start_pos": 138, "end_pos": 158, "type": "DATASET", "confidence": 0.7206686536471049}]}, {"text": "We can observe from that the semantic parser based on relaxed hybrid tree gives competitive performance when all the features (described in Sec 3.4) are used.", "labels": [], "entities": []}, {"text": "It significantly outperforms the hybrid tree model that is augmented with a discriminative reranking step.", "labels": [], "entities": []}, {"text": "The model reports the best accuracy and F1 scores on English and Thai and best accuracy score on Greek.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9996898174285889}, {"text": "F1", "start_pos": 40, "end_pos": 42, "type": "METRIC", "confidence": 0.9997404217720032}, {"text": "accuracy score", "start_pos": 79, "end_pos": 93, "type": "METRIC", "confidence": 0.9800083637237549}]}, {"text": "The scores on German are lower than those of UBL-S and TREETRANS, mainly because the span features appear not to be effective for this language, as we will discuss next.", "labels": [], "entities": [{"text": "TREETRANS", "start_pos": 55, "end_pos": 64, "type": "METRIC", "confidence": 0.8999418616294861}]}, {"text": "We report in: Results when certain types of features (local features, span features and character-level features) are excluded.", "labels": [], "entities": []}, {"text": "ness vary across different languages.", "labels": [], "entities": []}, {"text": "The local features, which capture local dependencies, are of particular importance.", "labels": [], "entities": []}, {"text": "Performance on three languages (English, Thai, and Greek) will drop when such features are excluded.", "labels": [], "entities": []}, {"text": "Character-level features are very helpful for the three European languages (English, German, and Greek), but appear to be harmful for Thai.", "labels": [], "entities": []}, {"text": "This indicates the character-level features that we propose do not perform effective morphological analysis for this Asian language.", "labels": [], "entities": []}, {"text": "The span features, which are able to capture certain long-distance dependencies, also play important roles.", "labels": [], "entities": []}, {"text": "Specifically, if such features are excluded, our model's performance on three languages (Greek, English, Thai) will drop.", "labels": [], "entities": []}, {"text": "Such features do not appear to be helpful for Thai and appear to be harmful for German.", "labels": [], "entities": []}, {"text": "Clearly, such long-distance features are not contributing useful information to the model when these two languages are considered.", "labels": [], "entities": []}, {"text": "This is especially the case for German, where we believe such features are contributing substantial noisy information to the model.", "labels": [], "entities": []}, {"text": "What underlying languagespecific, syntactic properties are generally causing these gaps in the performances?", "labels": [], "entities": []}, {"text": "We believe this is an important question that needs to be addressed in future research.", "labels": [], "entities": []}, {"text": "As we have mentioned, to make an appropriate comparison with previous works, only simple features are used.", "labels": [], "entities": []}, {"text": "We believe that our system's performance can be further improved when additional informative languagespecific features can be extracted from effective language tools and incorporated into our system.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: The complete list of word association pat- terns. Here #Args means the number of arguments  for a semantic unit.", "labels": [], "entities": [{"text": "Args", "start_pos": 66, "end_pos": 70, "type": "METRIC", "confidence": 0.9990793466567993}]}, {"text": " Table 3: Performance on the benchmark data, using four different languages as inputs. RHT: relaxed  hybrid tree (this work).", "labels": [], "entities": [{"text": "RHT", "start_pos": 87, "end_pos": 90, "type": "METRIC", "confidence": 0.7953667044639587}]}, {"text": " Table 4: Results when certain types of features (local features, span features and character-level features)  are excluded.", "labels": [], "entities": []}]}