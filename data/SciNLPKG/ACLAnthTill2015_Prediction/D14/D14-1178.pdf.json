{"title": [{"text": "Random Manhattan Integer Indexing: Incremental L 1 Normed Vector Space Construction", "labels": [], "entities": [{"text": "Random Manhattan Integer Indexing", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.6125647649168968}]}], "abstractContent": [{"text": "Vector space models (VSMs) are mathematically well-defined frameworks that have been widely used in the distributional approaches to semantics.", "labels": [], "entities": []}, {"text": "In VSMs, high-dimensional vectors represent linguistic entities.", "labels": [], "entities": []}, {"text": "In an application, the similarity of vectors-and thus the entities that they represent-is computed by a distance formula.", "labels": [], "entities": []}, {"text": "The high dimensionality of vectors , however, is a barrier to the performance of methods that employ VSMs.", "labels": [], "entities": []}, {"text": "Consequently, a dimensionality reduction technique is employed to alleviate this problem.", "labels": [], "entities": [{"text": "dimensionality reduction", "start_pos": 16, "end_pos": 40, "type": "TASK", "confidence": 0.7854030132293701}]}, {"text": "This paper introduces a novel technique called Random Manhattan Indexing (RMI) for the construction of 1 normed VSMs at reduced dimensionality.", "labels": [], "entities": [{"text": "Random Manhattan Indexing (RMI)", "start_pos": 47, "end_pos": 78, "type": "TASK", "confidence": 0.5743237485488256}]}, {"text": "RMI combines the construction of a VSM and dimension reduction into an incre-mental and thus scalable two-step procedure.", "labels": [], "entities": [{"text": "RMI", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.6680968403816223}]}, {"text": "In order to attain its goal, RMI employs the sparse Cauchy random projections.", "labels": [], "entities": [{"text": "RMI", "start_pos": 29, "end_pos": 32, "type": "TASK", "confidence": 0.9472115635871887}]}, {"text": "We further introduce Random Man-hattan Integer Indexing (RMII): a compu-tationally enhanced version of RMI.", "labels": [], "entities": [{"text": "Random Man-hattan Integer Indexing (RMII)", "start_pos": 21, "end_pos": 62, "type": "TASK", "confidence": 0.6733143670218331}]}, {"text": "As shown in the reported experiments, RMI and RMII can be used reliably to estimate the 1 distances between vectors in a vector space of low dimensionality.", "labels": [], "entities": [{"text": "RMI", "start_pos": 38, "end_pos": 41, "type": "METRIC", "confidence": 0.8757331967353821}, {"text": "RMII", "start_pos": 46, "end_pos": 50, "type": "METRIC", "confidence": 0.9699511528015137}]}], "introductionContent": [{"text": "Distributional semantics embraces a set of methods that decipher the meaning of linguistic entities using their usages in large corpora.", "labels": [], "entities": [{"text": "Distributional semantics", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.7935208976268768}]}, {"text": "In these methods, the distributional properties of linguistic entities in various contexts, which are collected from their observations in corpora, are compared to quantify their meaning.", "labels": [], "entities": []}, {"text": "Vector spaces are intuitive, mathematically well-defined frameworks to represent and process such information.", "labels": [], "entities": []}, {"text": "Ina vector space model (VSM), linguistic entities are represented by vectors and a distance formula is employed to measure their distributional similarities.", "labels": [], "entities": []}, {"text": "Ina VSM, each element s i of the standard basis of the vector space (informally, each dimension of the VSM) represents a context element.", "labels": [], "entities": []}, {"text": "Given n context elements, an entity whose meaning is being analyzed is expressed by a vector v as a linear combination of s i and scalars \u03b1 i \u2208 R such that v = \u03b1 1 s 1 + \u00b7 \u00b7 \u00b7 + \u03b1 n s n . The value of \u03b1 i is derived from the frequency of the occurrences of the entity that v represents in/with the context element that s i represents.", "labels": [], "entities": []}, {"text": "As a result, the values assigned to the coordinates of a vector (i.e. \u03b1 i ) exhibit the correlation of entities and context elements in an ndimensional real vector space Rn . Each vector can be written as a 1\u00d7n row matrix, e.g. (\u03b1 1 , \u00b7 \u00b7 \u00b7 , \u03b1 n ).", "labels": [], "entities": []}, {"text": "Therefore, a group of m vectors in a vector space is often represented by a matrix M m\u00d7n . Latent semantic analysis (LSA) is a familiar technique that employs a word-by-document VSM ().", "labels": [], "entities": [{"text": "Latent semantic analysis (LSA)", "start_pos": 91, "end_pos": 121, "type": "TASK", "confidence": 0.8066073258717855}]}, {"text": "In this wordby-document model, the meaning of words (i.e. the linguistic entities) is described by their occurrences in documents (i.e. the context elements).", "labels": [], "entities": []}, {"text": "Given m words and n distinct documents, each word is represented by an n-dimensional vector vi = (\u03b1 i1 , \u00b7 \u00b7 \u00b7 , \u03b1 in ), where \u03b1 ij is a numeric value that associates the word vi represents to the document d j , for 1 < j < n.", "labels": [], "entities": []}, {"text": "For instance, the value of \u03b1 ij may correspond to the frequency of the word in the document.", "labels": [], "entities": []}, {"text": "It is hypothesized that the relevance of words can be assessed by counting the documents in which they co-occur.", "labels": [], "entities": []}, {"text": "Therefore, words with similar vectors are assumed to have the same meaning ().", "labels": [], "entities": []}, {"text": "Figure 1: Illustration of a word-by-document model consisting of 2 words and 3 documents.", "labels": [], "entities": []}, {"text": "The words are represented in a 3-dimensional vector space, in which each s i (each dimension) represents each of the 3 documents in the model.", "labels": [], "entities": []}, {"text": "v 1 = (\u03b1 11 , \u03b1 12 , \u03b1 13 ) and v 2 = (\u03b1 21 , \u03b1 22 , \u03b1 23 ) represent the two words in the model.", "labels": [], "entities": []}, {"text": "The dashed line shows the Euclidean distance between the two vectors that represent words, while the sum of dash-dotted lines is the Manhattan distance between them.", "labels": [], "entities": [{"text": "Manhattan distance", "start_pos": 133, "end_pos": 151, "type": "METRIC", "confidence": 0.9578405022621155}]}, {"text": "In order to assess the similarity between vectors, a vector space V is endowed with a norm structure.", "labels": [], "entities": []}, {"text": "A norm . is a function that maps vectors from V to the set of non-negative real numbers, i.e. V \u2192 [0, \u221e).", "labels": [], "entities": []}, {"text": "The pair of (V, .) is then called a normed space.", "labels": [], "entities": []}, {"text": "Ina normed space, the similarity between vectors is assessed by their distances.", "labels": [], "entities": []}, {"text": "The distance between vectors is defined by a function that satisfies certain axioms and assigns areal value to each pair of vectors, i.e. The smaller the distance between two vectors, the more similar they are.", "labels": [], "entities": []}, {"text": "Euclidean space is the most familiar example of a normed space.", "labels": [], "entities": []}, {"text": "It is a vector space that is endowed by the 2 norm.", "labels": [], "entities": []}, {"text": "In Euclidean space, the 2 norm-which is also called the Euclidean normof a vector v = (v 1 , \u00b7 \u00b7 \u00b7 , v n ) is defined as Using the definition of distance given in Equation 1 and the 2 norm, the Euclidean distance is measured as In, the dashed line shows the Euclidean distance between the two vectors.", "labels": [], "entities": []}, {"text": "In 2 normed vector spaces, various similarity metrics are defined using different normalization of the Euclidean distance between vectors, e.g. the cosine similarity.", "labels": [], "entities": []}, {"text": "The similarity between vectors, however, can also be computed in 1 normed spaces.", "labels": [], "entities": [{"text": "similarity", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9595569372177124}]}, {"text": "The 1 norm for v is given by where |.| signifies the modulus.", "labels": [], "entities": []}, {"text": "The distance in an 1 normed vector space is often called the Manhattan or the city block distance.", "labels": [], "entities": []}, {"text": "According to the definition given in Equation 1, the Manhattan distance between two vectors v and u is given by In, the collection of the dash-dotted lines is the 1 distance between the two vectors.", "labels": [], "entities": [{"text": "Manhattan distance", "start_pos": 53, "end_pos": 71, "type": "METRIC", "confidence": 0.9804537296295166}]}, {"text": "Similar to the 2 spaces, various normalizations of the 1 distance 4 define a family of 1 normed similarity metrics.", "labels": [], "entities": []}, {"text": "As the number of text units that are being modelled in a VSM increases, the number of context elements that are required to be utilized to capture their meaning escalates.", "labels": [], "entities": []}, {"text": "This phenomenon is explained using power-law distributions of text units in context elements (e.g. the familiar Zipfian distribution of words).", "labels": [], "entities": []}, {"text": "As a result, extremely highdimensional vectors, which are also sparse-i.e. most of the elements of the vectors are zerorepresent text units.", "labels": [], "entities": []}, {"text": "The high dimensionality of the vectors results in setbacks, which are colloquially known as the curse of dimensionality.", "labels": [], "entities": []}, {"text": "For instance, in a word-by-document model that consists of a large number of documents, a word appears only in a few documents, and the rest of the documents are irrelevant to the meaning of the word.", "labels": [], "entities": []}, {"text": "Few common documents between words results in sparsity of the vectors; and the presence of irrelevant documents introduces noise.", "labels": [], "entities": []}, {"text": "Dimension reduction, which usually follows the construction of a VSM, alleviates the problems The definition of the norm is generalized top spaces with vp = i |vi| p 1/p , which is beyond the scope of this paper.", "labels": [], "entities": [{"text": "Dimension reduction", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7796138823032379}]}, {"text": "As long as the axioms in the distance definition hold.", "labels": [], "entities": []}, {"text": "listed above by reducing the number of context elements that are employed for the construction of the VSM.", "labels": [], "entities": [{"text": "VSM", "start_pos": 102, "end_pos": 105, "type": "DATASET", "confidence": 0.7524095177650452}]}, {"text": "In its simple form, dimensionality reduction can be performed using a selection process: choose a subset of contexts and eliminate the rest using a heuristic.", "labels": [], "entities": [{"text": "dimensionality reduction", "start_pos": 20, "end_pos": 44, "type": "TASK", "confidence": 0.8760533630847931}]}, {"text": "Alternatively, transformation methods can be employed.", "labels": [], "entities": []}, {"text": "A transformation method maps a vector space V n onto a V m of lowered dimension, i.e. \u03c4 : V n \u2192 V m , m n.", "labels": [], "entities": []}, {"text": "The vector space at reduced dimension, i.e. V m , is often the best approximation of the original V n in a sense.", "labels": [], "entities": []}, {"text": "LSA employs a dimension reduction technique called truncated singular value decomposition (SVD).", "labels": [], "entities": [{"text": "dimension reduction", "start_pos": 14, "end_pos": 33, "type": "TASK", "confidence": 0.6717405766248703}, {"text": "truncated singular value decomposition (SVD)", "start_pos": 51, "end_pos": 95, "type": "TASK", "confidence": 0.7380711606570652}]}, {"text": "Ina standard truncated SVD, the transformation guarantees the least distortion in the 2 distances.", "labels": [], "entities": []}, {"text": "Besides the problem of high computational complexity of SVD computation, 6 which can be addressed by incremental techniques (see e.g.), matrix factorization methods such as truncated SVD are data-sensitive: if the structure of the data being analyzed changes, i.e. when either the linguistic entities or context elements are updated, e.g. some are removed or new ones are added, the transformation should be recomputed and reapplied to the whole VSM to reflect the updates.", "labels": [], "entities": [{"text": "SVD computation", "start_pos": 56, "end_pos": 71, "type": "TASK", "confidence": 0.8939780592918396}]}, {"text": "In addition, a VSM at the original high dimension must be first constructed.", "labels": [], "entities": []}, {"text": "Following the construction of the VSM, the dimension of the VSM is reduced in an independent process.", "labels": [], "entities": [{"text": "VSM", "start_pos": 34, "end_pos": 37, "type": "DATASET", "confidence": 0.8388818502426147}, {"text": "VSM", "start_pos": 60, "end_pos": 63, "type": "DATASET", "confidence": 0.890326201915741}]}, {"text": "Therefore, the VSM at reduced dimension is available for processing only after the whole sequence of these processes.", "labels": [], "entities": []}, {"text": "Construction of the VSM at its original dimension is computationally expensive and a delay in access to the VSM at reduced dimension is not desirable.", "labels": [], "entities": []}, {"text": "Hence, the application of truncated SVD is not suitable in several applications, particularly when dealing with frequently updated big text-data such as applications in the web context.", "labels": [], "entities": []}, {"text": "Random indexing (RI) is an alternative method that solves the problems stated above by combining the construction of a vector space and the dimensionality reduction process.", "labels": [], "entities": [{"text": "Random indexing (RI)", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8448986530303955}]}, {"text": "RI, which is introduced in, constructs a VSM directly at reduced dimension.", "labels": [], "entities": []}, {"text": "Unlike methods that first construct a VSM at its original high dimension and conduct a dimensionality reduction Please note that there are matrix factorization techniques that guarantee the least distortion in the 1 distances, see e.g..", "labels": [], "entities": []}, {"text": "Matrix factorization techniques, in general.", "labels": [], "entities": []}, {"text": "afterwards, the RI method avoids the construction of the original high-dimensional VSM.", "labels": [], "entities": [{"text": "RI", "start_pos": 16, "end_pos": 18, "type": "METRIC", "confidence": 0.4855477809906006}]}, {"text": "Instead, it merges the vector space construction and the dimensionality reduction process.", "labels": [], "entities": [{"text": "vector space construction", "start_pos": 23, "end_pos": 48, "type": "TASK", "confidence": 0.6423404614130656}]}, {"text": "RI, thus, significantly enhances the computational complexity of deriving a VSM from text.", "labels": [], "entities": []}, {"text": "However, the application of the RI technique (likewise the standard truncated SVD in LSA) is limited to 2 normed spaces, i.e. when similarities are assessed using a measure based on the 2 distance.", "labels": [], "entities": []}, {"text": "It can be verified that using RI causes large distortions in the 1 distances between vectors (Brinkman and).", "labels": [], "entities": [{"text": "Brinkman", "start_pos": 94, "end_pos": 102, "type": "METRIC", "confidence": 0.9874694347381592}]}, {"text": "Hence, if the similarities are computed using the 1 distance, then the RI technique is not suitable for the VSM construction.", "labels": [], "entities": [{"text": "RI", "start_pos": 71, "end_pos": 73, "type": "METRIC", "confidence": 0.8150931000709534}, {"text": "VSM construction", "start_pos": 108, "end_pos": 124, "type": "TASK", "confidence": 0.8692259788513184}]}, {"text": "Depending on the distribution of vectors in a VSM, the performance of similarity measures based on the 1 and the 2 norms varies from one task to another.", "labels": [], "entities": []}, {"text": "For instance, it is known that the 1 distance is more robust to the presence of outliers and non-Gaussian noise than the 2 distance (e.g. seethe problem description in).", "labels": [], "entities": []}, {"text": "Hence, the 1 distance can be more reliable than the 2 distance in certain applications.", "labels": [], "entities": []}, {"text": "For instance, suggest that the 1 distance outperforms other similarity metrics in a term classification task.", "labels": [], "entities": [{"text": "term classification task", "start_pos": 84, "end_pos": 108, "type": "TASK", "confidence": 0.8102480371793112}]}, {"text": "In another experiment, observed that the 1 distance gives more desirable results than the Cosine and the 2 measures.", "labels": [], "entities": [{"text": "Cosine", "start_pos": 90, "end_pos": 96, "type": "METRIC", "confidence": 0.9903430938720703}]}, {"text": "In this paper, we introduce a novel method called Random Manhattan Indexing (RMI).", "labels": [], "entities": [{"text": "Random Manhattan Indexing (RMI)", "start_pos": 50, "end_pos": 81, "type": "TASK", "confidence": 0.65016441543897}]}, {"text": "RMI constructs a vector space model directly at reduced dimension while it preserves the pairwise 1 distances between vectors in the original highdimensional VSM.", "labels": [], "entities": []}, {"text": "We then introduced a computationally enhanced version of RMI called Random Manhattan Integer Indexing (RMII).", "labels": [], "entities": [{"text": "Random Manhattan Integer Indexing (RMII)", "start_pos": 68, "end_pos": 108, "type": "TASK", "confidence": 0.7158708359513964}]}, {"text": "RMI and RMII, similar to RI, merge the construction of a VSM and dimension reduction into an incremental and thus efficient and scalable process.", "labels": [], "entities": []}, {"text": "In Section 2, we explain and evaluate the RMI method.", "labels": [], "entities": [{"text": "RMI", "start_pos": 42, "end_pos": 45, "type": "TASK", "confidence": 0.9512173533439636}]}, {"text": "In Section 3, the RMII method is explained.", "labels": [], "entities": [{"text": "RMII", "start_pos": 18, "end_pos": 22, "type": "TASK", "confidence": 0.7449599504470825}]}, {"text": "We compare the proposed method with RI in Section 4.", "labels": [], "entities": [{"text": "RI", "start_pos": 36, "end_pos": 38, "type": "METRIC", "confidence": 0.8706628084182739}]}, {"text": "We conclude in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "We report the performance of the RMI method with respect to its ability to preserve the relative 1 distance between linguistic entities in a VSM.", "labels": [], "entities": [{"text": "RMI", "start_pos": 33, "end_pos": 36, "type": "TASK", "confidence": 0.9150036573410034}]}, {"text": "Therefore, instead of a task-specific evaluation, we show that the relative 1 distance between a set of words in a high-dimensional word-bydocument model remains intact when the model is constructed at reduced dimensionality using the RMI technique.", "labels": [], "entities": []}, {"text": "We further explore the effect of the RMI's parameter setting in the observed results.", "labels": [], "entities": []}, {"text": "Depending on the structure of the data that is being analyzed and the objective of the task in hand, the performance of the 1 distance for similarity measurement varies from one application to another.", "labels": [], "entities": []}, {"text": "The purpose of our reported evaluation, thus, is not to show the superiority of the 1 distance (thus RMI) to another similarity measure (e.g. the 2 distance or the cosine similarity) and employed techniques for dimensionality reduction (e.g. RI or truncated SVD) in a specific task.", "labels": [], "entities": [{"text": "RMI", "start_pos": 101, "end_pos": 104, "type": "METRIC", "confidence": 0.9695545434951782}]}, {"text": "If, in a task, the 1 distance shows higher performance than the 2 distance, then the RMI technique is preferable to the RI technique or truncated SVD.", "labels": [], "entities": []}, {"text": "Contrariwise, if the 2 norm shows higher performance than the 1 , then RI or truncated SVD are more desirable than the RMI method.", "labels": [], "entities": [{"text": "RI", "start_pos": 71, "end_pos": 73, "type": "METRIC", "confidence": 0.9423062205314636}]}, {"text": "In our experiment, a word-by-document model is first constructed from the UKWaC corpus at its original high dimension.", "labels": [], "entities": [{"text": "UKWaC corpus", "start_pos": 74, "end_pos": 86, "type": "DATASET", "confidence": 0.9945104420185089}]}, {"text": "UKWaC is a freely available corpus of 2,692,692 web documents, nearly 2 billion tokens and 4 million types (.", "labels": [], "entities": [{"text": "UKWaC", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.983794629573822}]}, {"text": "Therefore, a word-by-document model constructed from this corpus using the classic onedimension-per-context-element method has a dimension of 2.69 million.", "labels": [], "entities": []}, {"text": "In order to keep the experiments computationally tractable, the reported results are limited to 31 words from this model, which are listed in In the designed experiment, a word from the list is taken as the reference and its 1 distance to the remaining 30 words is calculated using the vector representations in the high-dimensional VSM.", "labels": [], "entities": [{"text": "VSM", "start_pos": 333, "end_pos": 336, "type": "DATASET", "confidence": 0.9124176502227783}]}, {"text": "These 30 words are then sorted in ascending order by the calculated 1 distance.", "labels": [], "entities": []}, {"text": "The procedure is repeated for all the 31 words in the list, one by one.", "labels": [], "entities": []}, {"text": "Therefore, the procedure results in 31 sorted lists, each containing 30 words.", "labels": [], "entities": []}, {"text": "shows an example of the obtained sorted list, in which the reference is the word 'research'.", "labels": [], "entities": []}, {"text": "The procedure described above is replicated to obtain the lists of sorted words from VSMs that are constructed by the RMI method at reduced 11 E.g. seethe experiments in.", "labels": [], "entities": []}, {"text": "UkWaC can be obtained from http://goo.gl/ 3isfIE.", "labels": [], "entities": [{"text": "UkWaC", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9489896297454834}]}, {"text": "Please note that the number of possible arrangements of 30 words without repetition in a list in which the order is important (i.e. all permutations of 30 words) is 30!.: Words employed in the experiments.", "labels": [], "entities": []}, {"text": "dimensionality, when the method's parametersi.e. the dimensionality of VSM and the number of non-zero elements in index vectors-are set differently.", "labels": [], "entities": [{"text": "VSM", "start_pos": 71, "end_pos": 74, "type": "DATASET", "confidence": 0.7073678970336914}]}, {"text": "We expect the obtained relative 1 distances between each reference word and the 30 other words in an RMI-constructed VSM to be the same as the obtained relative distances in the original high-dimensional VSM.", "labels": [], "entities": []}, {"text": "Therefore, for each VSM that is constructed by the RMI technique, the resulting sorted lists of words are compared by the sorted lists that are obtained from the original high-dimensional VSM.", "labels": [], "entities": []}, {"text": "We employ the Spearman's rank correlation coefficient (\u03c1) to compare the sorted lists of words and thus the degree of distance preservation in the RMI-constructed VSMs at reduced dimensionality.", "labels": [], "entities": [{"text": "Spearman's rank correlation coefficient (\u03c1)", "start_pos": 14, "end_pos": 57, "type": "METRIC", "confidence": 0.7664127871394157}, {"text": "RMI-constructed VSMs", "start_pos": 147, "end_pos": 167, "type": "TASK", "confidence": 0.5371266901493073}]}, {"text": "The Spearman's rank correlation measures the strength of association between two ranked variables, i.e. two lists of sorted words in our experiments.", "labels": [], "entities": [{"text": "Spearman's rank correlation", "start_pos": 4, "end_pos": 31, "type": "METRIC", "confidence": 0.566744439303875}]}, {"text": "Given a list of sorted words obtained from the original high-dimensional VSM (list o ) and its corresponding list obtained from a VSM of reduced dimensionality (list RMI ), the Spearman's rank correlation for the two lists is calculated by", "labels": [], "entities": [{"text": "Spearman's rank correlation", "start_pos": 177, "end_pos": 204, "type": "METRIC", "confidence": 0.557616576552391}]}], "tableCaptions": []}