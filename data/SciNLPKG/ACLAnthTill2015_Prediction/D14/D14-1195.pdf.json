{"title": [{"text": "Joint Decoding of Tree Transduction Models for Sentence Compression", "labels": [], "entities": [{"text": "Joint Decoding of Tree Transduction", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.553901880979538}, {"text": "Sentence Compression", "start_pos": 47, "end_pos": 67, "type": "TASK", "confidence": 0.8533914387226105}]}], "abstractContent": [{"text": "In this paper, we provide anew method for decoding tree transduction based sentence compression models augmented with language model scores, by jointly decoding two components.", "labels": [], "entities": [{"text": "decoding tree transduction based sentence compression", "start_pos": 42, "end_pos": 95, "type": "TASK", "confidence": 0.749597504734993}]}, {"text": "In our proposed solution , rich local discriminative features can be easily integrated without increasing computational complexity.", "labels": [], "entities": []}, {"text": "Utilizing an unobvious fact that the resulted two components can be independently decoded, we conduct efficient joint decoding based on dual decomposition.", "labels": [], "entities": []}, {"text": "Experimental results show that our method outperforms traditional beam search decoding and achieves the state-of-the-art performance.", "labels": [], "entities": []}], "introductionContent": [{"text": "Sentence compression is the task of generating a grammatical and shorter summary fora long sentence while preserving its most important information.", "labels": [], "entities": [{"text": "Sentence compression", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.9320854544639587}]}, {"text": "One specific instantiation is deletion-based compression, namely generating a compression by dropping words.", "labels": [], "entities": []}, {"text": "Various approaches have been proposed to challenge the task of deletion-based compression.", "labels": [], "entities": [{"text": "deletion-based compression", "start_pos": 63, "end_pos": 89, "type": "TASK", "confidence": 0.8661772012710571}]}, {"text": "Earlier pioneering works) considered several insightful approaches, including noisy-channel based generative models and discriminative decision tree models.", "labels": [], "entities": []}, {"text": "Structured discriminative compression models) are capable of integrating rich features and have been proved effective for this task.", "labels": [], "entities": [{"text": "Structured discriminative compression", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.6668299833933512}]}, {"text": "Another powerful paradigm for sentence compression should be mentioned here is constraints-based compression,including integer linear programming solutions) and first-order Markov logic networks (.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 30, "end_pos": 50, "type": "TASK", "confidence": 0.7882763743400574}, {"text": "constraints-based compression,including integer linear programming", "start_pos": 79, "end_pos": 145, "type": "TASK", "confidence": 0.8641047477722168}]}, {"text": "A notable class of methods that explicitly deal with syntactic structures are tree transduction models (.", "labels": [], "entities": []}, {"text": "In such models asynchronous grammar is extracted from a corpus of parallel syntax trees with leaves aligned.", "labels": [], "entities": []}, {"text": "Compressions are generated from the grammar with learned weights.", "labels": [], "entities": []}, {"text": "Previous works have noticed that local coherence is usually needed by introducing ngram language model scores, which will make accurate decoding intractable.", "labels": [], "entities": []}, {"text": "Traditional approaches conduct beam search to find approximate solutions.", "labels": [], "entities": [{"text": "beam search", "start_pos": 31, "end_pos": 42, "type": "TASK", "confidence": 0.8303645849227905}]}, {"text": "In this paper we propose a joint decoding strategy to challenge this decoding task.", "labels": [], "entities": []}, {"text": "We address the problem as jointly decoding a simple tree transduction model that only considers rule weights and an ngram compression model.", "labels": [], "entities": []}, {"text": "Although either part can be independently solved by dynamic programming, the naive way to integrate two groups of partial scores into a huge dynamic programming chart table is computationally impractical.", "labels": [], "entities": []}, {"text": "We provide an effective dual decomposition solution that utilizes the efficient decoding of both parts.", "labels": [], "entities": []}, {"text": "By integrating rich structured features that cannot be efficiently involved in normal formulation, results get significantly improved.", "labels": [], "entities": []}], "datasetContent": [{"text": "We assessed the compression results by the F1-score of grammatical relations (provided by a dependency parser) of generated compressions against the gold-standard compression).", "labels": [], "entities": [{"text": "F1-score", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.9978868365287781}]}, {"text": "All systems were controlled to produce similar compression ratios (CR) for fair comparison.", "labels": [], "entities": [{"text": "compression ratios (CR)", "start_pos": 47, "end_pos": 70, "type": "METRIC", "confidence": 0.9703767418861389}]}, {"text": "We also reported manual evaluation on a sampled subset of 30 sentences from each dataset.", "labels": [], "entities": []}, {"text": "Three unpaid volunteers with self-reported fluency in English were asked to rate every candidate.", "labels": [], "entities": []}, {"text": "Ratings are in the form of 1-5 scores for each compression.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Dataset partition (number of sentences)", "labels": [], "entities": []}, {"text": " Table 2: Results of automatic evaluation. ( \u2020:  sig. diff. from T3+LM(DD); *: sig. diff. from  T3+Discr.(DD) for p < 0.01)", "labels": [], "entities": []}, {"text": " Table 3: Results of human rating. ( \u2020: sig.  diff. from T3+LM(DD); *: sig. diff. from  T3+Discr.(DD), for p < 0.01)", "labels": [], "entities": []}]}