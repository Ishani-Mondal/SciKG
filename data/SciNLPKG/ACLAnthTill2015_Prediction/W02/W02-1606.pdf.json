{"title": [{"text": "Word Sense Disambiguation in a Korean-to-Japanese MT System Using Neural Networks", "labels": [], "entities": [{"text": "Word Sense Disambiguation", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.6466826498508453}]}], "abstractContent": [{"text": "This paper presents a method to resolve word sense ambiguity in a Korean-to-Japanese machine translation system using neural networks.", "labels": [], "entities": [{"text": "resolve word sense ambiguity", "start_pos": 32, "end_pos": 60, "type": "TASK", "confidence": 0.8271220922470093}, {"text": "Korean-to-Japanese machine translation", "start_pos": 66, "end_pos": 104, "type": "TASK", "confidence": 0.6339051226774851}]}, {"text": "The execution of our neural network model is based on the concept codes of a thesaurus.", "labels": [], "entities": []}, {"text": "Most previous word sense disambiguation approaches based on neural networks have limitations due to their huge feature set size.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 14, "end_pos": 39, "type": "TASK", "confidence": 0.7513906558354696}]}, {"text": "By contrast, we reduce the number of features of the network to a practical size by using concept codes as features rather than the lexical words themselves.", "labels": [], "entities": []}], "introductionContent": [{"text": "Korean-to-Japanese machine translation (MT) employs a direct MT strategy, where a Korean homograph maybe translated into a different Japanese equivalent depending on which sense is used in a given context.", "labels": [], "entities": [{"text": "Korean-to-Japanese machine translation (MT)", "start_pos": 0, "end_pos": 43, "type": "TASK", "confidence": 0.7794225960969925}, {"text": "MT", "start_pos": 61, "end_pos": 63, "type": "TASK", "confidence": 0.9775547981262207}]}, {"text": "Thus, word sense disambiguation (WSD) is essential to the selection of an appropriate Japanese target word.", "labels": [], "entities": [{"text": "word sense disambiguation (WSD)", "start_pos": 6, "end_pos": 37, "type": "TASK", "confidence": 0.7822426557540894}]}, {"text": "Much research on word sense disambiguation has revealed that several different types of information can contribute to the resolution of lexical ambiguity.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 17, "end_pos": 42, "type": "TASK", "confidence": 0.7114971180756887}, {"text": "resolution of lexical ambiguity", "start_pos": 122, "end_pos": 153, "type": "TASK", "confidence": 0.8433132171630859}]}, {"text": "These include surrounding words (an unordered set of words surrounding a target word), local collocations (a short sequence of words near a target word, taking word order into account), syntactic relations (selectional restrictions), parts of speech, morphological forms, etc.", "labels": [], "entities": []}, {"text": "Some researchers use neural networks in their word sense disambiguation systems Because of its strong capability in classification (.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 46, "end_pos": 71, "type": "TASK", "confidence": 0.5900250673294067}]}, {"text": "Since, however, most such methods require a few thousands of features or large amounts of hand-written data for training, it is not clear that the same neural network models will be applicable to real world applications.", "labels": [], "entities": []}, {"text": "We propose a word sense disambiguation method that combines both the neural net-based approach and the work of, especially focusing on the practicality of the method for application to real world MT systems.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 13, "end_pos": 38, "type": "TASK", "confidence": 0.6600451866785685}, {"text": "MT", "start_pos": 196, "end_pos": 198, "type": "TASK", "confidence": 0.9619418382644653}]}, {"text": "To reduce the number of input features of neural networks to a practical size, we use concept codes of a thesaurus as features.", "labels": [], "entities": []}, {"text": "In this paper, Yale Romanization is used to represent Korean expressions.", "labels": [], "entities": [{"text": "Yale Romanization", "start_pos": 15, "end_pos": 32, "type": "DATASET", "confidence": 0.9406458139419556}]}], "datasetContent": [{"text": "For an experimental evaluation, 10 ambiguous Korean nouns were selected, along with a total of 500 test sentences in which one homograph appears.", "labels": [], "entities": []}, {"text": "In order to follow the ambiguity distribution described in Section 3.2, we set the number of test nouns with two senses to 8 (80%).", "labels": [], "entities": []}, {"text": "The test sentences were randomly selected from the KIBS (Korean Information Base System) corpus.", "labels": [], "entities": [{"text": "KIBS (Korean Information Base System) corpus", "start_pos": 51, "end_pos": 95, "type": "DATASET", "confidence": 0.8908244222402573}]}, {"text": "The experimental results are shown in, where result A is the case when the most frequent sense was taken as the answer.", "labels": [], "entities": []}, {"text": "To compare it with our approach (result C), we also performed the experiment using Li's method (result B).", "labels": [], "entities": []}, {"text": "For sense disambiguation, Li's method features which are similar to our method.", "labels": [], "entities": [{"text": "sense disambiguation", "start_pos": 4, "end_pos": 24, "type": "TASK", "confidence": 0.7397611439228058}]}, {"text": "However, unlike our method, which combines all features by using neural networks, Li considers only one clue at each decision step.", "labels": [], "entities": []}, {"text": "As shown in the table, our approach exceeded Li's )) , ( ( max ) (  inmost of the results except 'kancang' and 'censin'.", "labels": [], "entities": []}, {"text": "This result shows that word sense disambiguation can be improved by combining several clues together (e.g. neural networks) rather than using them independently (e.g. Li's method).", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 23, "end_pos": 48, "type": "TASK", "confidence": 0.7740709185600281}]}, {"text": "The performance for each stage of the proposed method is shown in.", "labels": [], "entities": []}, {"text": "Symbols COL, VSR, NN and MFS in the table indicate 4 stages of our method in, respectively.", "labels": [], "entities": [{"text": "COL", "start_pos": 8, "end_pos": 11, "type": "METRIC", "confidence": 0.9771308898925781}, {"text": "VSR", "start_pos": 13, "end_pos": 16, "type": "METRIC", "confidence": 0.8768957853317261}]}, {"text": "In the NN stage, the 3-layer model did not show a performance superior to the 2-layer model because of the lack of training samples.", "labels": [], "entities": []}, {"text": "Since the 2-layer model has fewer parameters to be trained, it is more efficient to generalize for limited training corpora than the 3-layer model.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2. Concept codes and frequencies in CFP  ({<C i ,f i >}, type 2 , nwun(eye))", "labels": [], "entities": [{"text": "CFP", "start_pos": 43, "end_pos": 46, "type": "DATASET", "confidence": 0.9248566627502441}]}, {"text": " Table 3. Comparison of WSD Results", "labels": [], "entities": [{"text": "WSD", "start_pos": 24, "end_pos": 27, "type": "TASK", "confidence": 0.5818997621536255}]}]}