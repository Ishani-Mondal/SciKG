{"title": [{"text": "A Reliable Indexing Method fora Practical QA System", "labels": [], "entities": []}], "abstractContent": [{"text": "We propose a fast and reliable Question-answering (QA) system in Korean, which uses a predictive answer indexer based on 2-pass scoring method.", "labels": [], "entities": [{"text": "Question-answering (QA)", "start_pos": 31, "end_pos": 54, "type": "TASK", "confidence": 0.6903457269072533}]}, {"text": "The indexing process is as follows.", "labels": [], "entities": [{"text": "indexing", "start_pos": 4, "end_pos": 12, "type": "TASK", "confidence": 0.9661308526992798}]}, {"text": "The predictive answer indexer first extracts all answer candidates in a document.", "labels": [], "entities": []}, {"text": "Then, using 2-pass scoring method, it gives scores to the adjacent content words that are closely related with each answer candidate.", "labels": [], "entities": []}, {"text": "Next, it stores the weighted content words with each candidate into a database.", "labels": [], "entities": []}, {"text": "Using this technique, along with a complementary analysis of questions, the proposed QA system saves response time and enhances the precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 132, "end_pos": 141, "type": "METRIC", "confidence": 0.9984837174415588}]}], "introductionContent": [{"text": "Traditional Information Retrieval (IR) focuses on searching and ranking a list of documents in response to a user's question.", "labels": [], "entities": [{"text": "Traditional Information Retrieval (IR)", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.8527305126190186}]}, {"text": "However, in many cases, a user has a specific question and want for IR systems to return the answer itself rather than a list of documents).", "labels": [], "entities": []}, {"text": "To satisfy this need, the concept of Question Answering (QA) comes up, and a lot of researches have been carried out, as shown in the proceedings of AAAI (AAAI (n.d.)) and TREC (Text REtrieval Conference) (TREC (n.d.)).", "labels": [], "entities": [{"text": "Question Answering (QA)", "start_pos": 37, "end_pos": 60, "type": "TASK", "confidence": 0.889242148399353}, {"text": "AAAI", "start_pos": 149, "end_pos": 153, "type": "METRIC", "confidence": 0.717802882194519}, {"text": "TREC (Text REtrieval Conference)", "start_pos": 172, "end_pos": 204, "type": "TASK", "confidence": 0.5231827696164449}]}, {"text": "A QA system searches a large collection of texts, and filters out inadequate phrases or sentences within the texts.", "labels": [], "entities": []}, {"text": "Owing to the filtering process, a user can promptly approach to his/her answer phrases without troublesome tasks.", "labels": [], "entities": []}, {"text": "Unfortunately, most of the previous researches have passed over the following problems that occurs in real fields like World Wide Web (WWW): Users want to find answers as soon as possible.", "labels": [], "entities": []}, {"text": "If a QA system does not respond to their questions within a few seconds, they will keep a suspicious eye on usefulness of the system.", "labels": [], "entities": []}, {"text": "\u00a1 Users express their intentions by using various syntactic forms.", "labels": [], "entities": []}, {"text": "The fact makes it difficult that a QA system performs well at any domains.", "labels": [], "entities": []}, {"text": "Ultimately, the QA system cannot be easily converted into any domains.", "labels": [], "entities": []}, {"text": "\u00a1 A QA system cannot correctly respond to all of the users' questions.", "labels": [], "entities": []}, {"text": "It can answer the questions that are included in the predefined categories such as person, date, and time.", "labels": [], "entities": []}, {"text": "To solve the problems, we propose a practical QA system using a predictive answer indexer in Korean -MAYA (MAke Your Answer).", "labels": [], "entities": [{"text": "Korean -MAYA (MAke Your Answer)", "start_pos": 93, "end_pos": 124, "type": "DATASET", "confidence": 0.6626410000026226}]}, {"text": "MAYA focuses on resolving the practical problems such as real-time response and domain portability.", "labels": [], "entities": [{"text": "MAYA", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8789187669754028}, {"text": "domain portability", "start_pos": 80, "end_pos": 98, "type": "TASK", "confidence": 0.6563286483287811}]}, {"text": "We can easily add new categories to MAYA by only supplementing domain dictionaries and rules.", "labels": [], "entities": [{"text": "MAYA", "start_pos": 36, "end_pos": 40, "type": "DATASET", "confidence": 0.8611371517181396}]}, {"text": "We do not have to revise the searching engine of MAYA because the indexer is designed as a separate component that extracts candidate answers.", "labels": [], "entities": [{"text": "MAYA", "start_pos": 49, "end_pos": 53, "type": "DATASET", "confidence": 0.8892908096313477}]}, {"text": "Users can promptly obtain answer phrases on retrieval time because MAYA indexes answer candidates in advance.", "labels": [], "entities": []}, {"text": "This paper is organized as follows.", "labels": [], "entities": []}, {"text": "First, we review the previous works of the QA systems.", "labels": [], "entities": []}, {"text": "Second, we present our system, and describe the applied NLP techniques.", "labels": [], "entities": []}, {"text": "Third, we analyze the result of our experiments.", "labels": [], "entities": []}, {"text": "Finally, we draw conclusions.", "labels": [], "entities": []}], "datasetContent": [{"text": "In Equation 6, rank i is the rank of the first correct answer given by the ith question.", "labels": [], "entities": []}, {"text": "n is the number of questions.", "labels": [], "entities": []}, {"text": "For ranking answer candidates, MAYA uses the weighted sums of global scores and local scores, as shown in Equation 4.", "labels": [], "entities": [{"text": "MAYA", "start_pos": 31, "end_pos": 35, "type": "DATASET", "confidence": 0.7822167873382568}]}, {"text": "To set the weighting factors, we evaluated performances of MAYA according to the values of the weighting factors.", "labels": [], "entities": [{"text": "MAYA", "start_pos": 59, "end_pos": 63, "type": "DATASET", "confidence": 0.7717524766921997}]}, {"text": "shows overall MRAR as the values of the weighting factors are changed.", "labels": [], "entities": [{"text": "MRAR", "start_pos": 14, "end_pos": 18, "type": "METRIC", "confidence": 0.985767662525177}]}, {"text": "In, the boldface MRARs are the highest scores in each test bed.", "labels": [], "entities": [{"text": "MRARs", "start_pos": 17, "end_pos": 22, "type": "METRIC", "confidence": 0.986380398273468}]}, {"text": "We set \u03b1 and \u03b2 to 0.1 and 0.9 on the basis of the experiment.", "labels": [], "entities": []}, {"text": "To evaluate the performance of MAYA, we compared MAYA with Lee2000 (Lee, ) and) in KorQATeC 1.0 because we could not obtain any experimental results on Lee2000 in WEBTEC.", "labels": [], "entities": [{"text": "Lee2000", "start_pos": 59, "end_pos": 66, "type": "DATASET", "confidence": 0.9599717855453491}, {"text": "WEBTEC", "start_pos": 163, "end_pos": 169, "type": "DATASET", "confidence": 0.9392410516738892}]}, {"text": "As shown in, the performance of MAYA is higher than those of the other systems.", "labels": [], "entities": [{"text": "MAYA", "start_pos": 32, "end_pos": 36, "type": "DATASET", "confidence": 0.6280394792556763}]}, {"text": "The fact means that the scoring features of MAYA are useful.", "labels": [], "entities": [{"text": "MAYA", "start_pos": 44, "end_pos": 48, "type": "DATASET", "confidence": 0.7713848948478699}]}, {"text": "In, Lee2000 (50-byte) returns 50-byte span of phrases that include answer candidates, and the others return answer candidates in themselves.", "labels": [], "entities": [{"text": "Lee2000", "start_pos": 4, "end_pos": 11, "type": "DATASET", "confidence": 0.9592651724815369}]}, {"text": "MRAR-1 is MRAR except questions for which the QA system fails in finding correct answers.", "labels": [], "entities": [{"text": "MRAR-1", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.7198802828788757}, {"text": "MRAR", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9142892956733704}]}, {"text": "As shown in, the average retrieval time of the IR system (Lee,) is 0.026 second per query on a PC server with dual Intel Pentium III.", "labels": [], "entities": [{"text": "retrieval time", "start_pos": 25, "end_pos": 39, "type": "METRIC", "confidence": 0.8779484033584595}]}, {"text": "MAYA consumes 0.048 second per query.", "labels": [], "entities": []}, {"text": "The difference of the retrieval times between the IR system and MAYA is not so big, which means that the retrieval speed of MAYA is fast enough to be negligible.", "labels": [], "entities": []}, {"text": "also shows the difference of the response times between MAYA and a QA system without a predictive answer indexer.", "labels": [], "entities": []}, {"text": "We call the QA system without an answer indexer Incomplete-MAYA.", "labels": [], "entities": []}, {"text": "Incomplete-MAYA finds and ranks answer candidates on retrieval time.", "labels": [], "entities": []}, {"text": "Hence, it does not need additive indexing time except indexing time for the underlying IR system.", "labels": [], "entities": []}, {"text": "In the experiment on the response time, we made Incomplete-MAYA process answer candidates just in top 30 documents that are retrieved by the underlying IR system.", "labels": [], "entities": []}, {"text": "If Incomplete-MAYA finds and ranks answer candidates in the whole retrieved documents, it will take longer response time than the response time in.", "labels": [], "entities": []}, {"text": "As shown in, the response time of MAYA is about 110 times faster than that of Incomplete-MAYA.", "labels": [], "entities": [{"text": "response time", "start_pos": 17, "end_pos": 30, "type": "METRIC", "confidence": 0.9237968623638153}]}, {"text": "Although MAYA consumes 19.120 seconds per megabyte for creating the answer DB, we conclude that MAYA is more efficient because most of the users are impatient fora system to show answers within a few milliseconds.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3. The performances of MAYA according  to the values of the weighting factors  \u03b1  \u03b2", "labels": [], "entities": [{"text": "MAYA", "start_pos": 30, "end_pos": 34, "type": "DATASET", "confidence": 0.49232837557792664}]}, {"text": " Table 4. The performances of the QA systems in  KorQATeC 1.0", "labels": [], "entities": [{"text": "KorQATeC 1.0", "start_pos": 49, "end_pos": 61, "type": "DATASET", "confidence": 0.8688037991523743}]}, {"text": " Table 5. The difference of response times", "labels": [], "entities": []}]}