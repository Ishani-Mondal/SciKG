{"title": [{"text": "Acquiring Collocations for Lexical Choice between Near-Synonyms", "labels": [], "entities": [{"text": "Acquiring Collocations", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8228108286857605}, {"text": "Lexical Choice between Near-Synonyms", "start_pos": 27, "end_pos": 63, "type": "TASK", "confidence": 0.9107349663972855}]}], "abstractContent": [{"text": "We extend a lexical knowledge-base of near-synonym differences with knowledge about their collocational behaviour.", "labels": [], "entities": []}, {"text": "This type of knowledge is useful in the process of lexical choice between near-synonyms.", "labels": [], "entities": []}, {"text": "We acquire collocations for the near-synonyms of interest from a corpus (only collocations with the appropriate sense and part-of-speech).", "labels": [], "entities": []}, {"text": "For each word that collocates with a near-synonym we use a differential test to learn whether the word forms a less-preferred collo-cation or an anti-collocation with other near-synonyms in the same cluster.", "labels": [], "entities": []}, {"text": "For this task we use a much larger corpus (the Web).", "labels": [], "entities": []}, {"text": "We also look at associations (longer-distance co-occurrences) as a possible source of learning more about nuances that the near-synonyms may carry.", "labels": [], "entities": []}], "introductionContent": [{"text": "Edmonds and Hirst (2002 to appear) developed a lexical choice process for natural language generation (NLG) or machine translation (MT) that can decide which near-synonyms are most appropriate in a particular situation.", "labels": [], "entities": [{"text": "natural language generation (NLG)", "start_pos": 74, "end_pos": 107, "type": "TASK", "confidence": 0.8093095123767853}, {"text": "machine translation (MT)", "start_pos": 111, "end_pos": 135, "type": "TASK", "confidence": 0.8342941761016845}]}, {"text": "The lexical choice process has to choose between clusters of near-synonyms (to convey the basic meaning), and then to choose between the near-synonyms in each cluster.", "labels": [], "entities": []}, {"text": "To group near-synonyms in clusters we trust lexicographers' judgment in dictionaries of synonym differences.", "labels": [], "entities": []}, {"text": "For example task, job, duty, assignment, chore, stint, hitch all refer to a one-time piece of work, but which one to choose depends on the duration of the work, the commitment and the effort involved, etc.", "labels": [], "entities": []}, {"text": "In order to convey desired nuances of meaning and to avoid unwanted implications, knowledge about the differences among near-synonyms is necessary.", "labels": [], "entities": []}, {"text": "I-Saurus, a prototype implementation of to appear), uses a small number of hand-built clusters of near-synonyms.", "labels": [], "entities": []}, {"text": "Our goal is to automatically acquire knowledge about distinctions among near-synonyms from a dictionary of synonym differences and from other sources such as free text, in order to build anew lexical resource, which can be used in lexical choice.", "labels": [], "entities": []}, {"text": "Preliminary results on automatically acquiring a lexical knowledge-base of near-synonym differences were presented in).", "labels": [], "entities": []}, {"text": "We acquired denotational (implications, suggestions, denotations), attitudinal (favorable, neutral, or pejorative), and stylistic distinctions from Choose the Right Word (hereafter CTRW) . We used an unsupervised decision-list algorithm to learn all the words used to express distinctions and then applied information extraction techniques.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 306, "end_pos": 328, "type": "TASK", "confidence": 0.7656049728393555}]}, {"text": "Another type of knowledge that can help in the process of choosing between near-synonyms is collocational behaviour, because one must not choose a near-synonym that does not collocate well with the other word choices for the sentence.", "labels": [], "entities": []}, {"text": "I-Saurus does not include such knowledge.", "labels": [], "entities": []}, {"text": "The focus of the work we present in this paper is to add knowledge about collocational behaviour to our lexical knowledge-base of near-synonym differences.", "labels": [], "entities": []}, {"text": "The lexical choice process implemented in I-Saurus gen-erates all the possible sentences with a given meaning, and ranks them according to the degree to which they satisfy a set of preferences given as input (these are the denotational, attitudinal, and stylistic nuances mentioned above).", "labels": [], "entities": []}, {"text": "We can refine the ranking so that it favors good collocations, and penalizes sentences containing words that do not collocate well.", "labels": [], "entities": []}, {"text": "We acquire collocates of all near-synonyms in CTRW from free text.", "labels": [], "entities": [{"text": "CTRW", "start_pos": 46, "end_pos": 50, "type": "DATASET", "confidence": 0.9409775733947754}]}, {"text": "We combine several statistical measures, unlike other researchers who rely on only one measure to rank collocations.", "labels": [], "entities": []}, {"text": "Then we acquire knowledge about less-preferred collocations and anti-collocations 2 . For example daunting task is a preferred collocation, while daunting job is less preferred (it should not be used in lexical choice unless there is no better alternative), and daunting duty is an anti-collocation (it must not be used in lexical choice).", "labels": [], "entities": []}, {"text": "Like , we use the t-test and mutual information.", "labels": [], "entities": []}, {"text": "Unlike them we use the Web as a corpus for this task, we distinguish three different types of collocations, and we apply sense disambiguation to collocations.", "labels": [], "entities": []}, {"text": "Collocations are defined in different ways by different researchers.", "labels": [], "entities": []}, {"text": "For us collocations consist of consecutive words that appear together much more often than by chance.", "labels": [], "entities": []}, {"text": "We also include words separated by a few non-content words (short-distance co-occurrence in the same sentence).", "labels": [], "entities": []}, {"text": "We are interested in collocations to be used in lexical choice.", "labels": [], "entities": []}, {"text": "Therefore we need to extract lexical collocations (between open-class words), not grammatical collocations (which could contain closedclass words, for example put on).", "labels": [], "entities": []}, {"text": "For now, we consider only two-word fixed collocations.", "labels": [], "entities": []}, {"text": "In future work we will consider longer and more flexible collocations.", "labels": [], "entities": []}, {"text": "We are also interested in acquiring words that strongly associate with our near-synonyms, especially words that associate with only one of the nearsynonyms in the cluster.", "labels": [], "entities": []}, {"text": "Using these strong associations, we plan to learn about nuances of nearsynonyms in order to validate and extend our lexical knowledge-base of near-synonym differences.", "labels": [], "entities": []}, {"text": "In our first experiment, described in sections 2 and 3 (with results in section 4, and evaluation in section 5), we acquire knowledge about the collocational behaviour of the near-synonyms.", "labels": [], "entities": []}, {"text": "In step 1 (section 2), we acquire potential collocations from the British National Corpus (BNC) 3 , combining several measures.", "labels": [], "entities": [{"text": "British National Corpus (BNC) 3", "start_pos": 66, "end_pos": 97, "type": "DATASET", "confidence": 0.976651268345969}]}, {"text": "In section 3 we present: (step2) select collocations for the near-synonyms in CTRW; (step 3) filter out wrongly selected collocations using mutual information on the Web; (step 4) for each cluster we compose new collocations by combining the collocate of one near-synonym with the the other near-synonym, and we apply the differential ttest to classify them into preferred collocations, lesspreferred collocations, and anti-collocations.", "labels": [], "entities": [{"text": "CTRW", "start_pos": 78, "end_pos": 82, "type": "DATASET", "confidence": 0.9544841647148132}]}, {"text": "Section 6 sketches our second experiment, involving word associations.", "labels": [], "entities": [{"text": "word associations", "start_pos": 52, "end_pos": 69, "type": "TASK", "confidence": 0.726132720708847}]}, {"text": "The last two sections present related work, and conclusions and future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our evaluation has two purposes: to get a quantitative measure of the quality of our results, and to choose thresholds in a principled way.", "labels": [], "entities": []}, {"text": "As described in the previous sections, in step 1 we selected potential collocations from BNC (the ones selected by at least two of the five measures).", "labels": [], "entities": []}, {"text": "Then, we selected collocations for each of the nearsynonyms in CTRW (step 2).", "labels": [], "entities": [{"text": "CTRW", "start_pos": 63, "end_pos": 67, "type": "DATASET", "confidence": 0.9225276708602905}]}, {"text": "We need to evaluate the MI filter (step 3), which filters out the bigrams that are not true collocations, based on their mutual information computed on the Web.", "labels": [], "entities": []}, {"text": "We also need to evaluate step 4, the three way classification based on the differential t-test on the Web.", "labels": [], "entities": []}, {"text": "For evaluation purposes we selected three clusters from CTRW, with a total of 24 near-synonyms.", "labels": [], "entities": [{"text": "CTRW", "start_pos": 56, "end_pos": 60, "type": "DATASET", "confidence": 0.9541594386100769}]}, {"text": "For these, we obtained 916 collocations from BNC according to the method described in section 2.", "labels": [], "entities": [{"text": "BNC", "start_pos": 45, "end_pos": 48, "type": "DATASET", "confidence": 0.8805109858512878}]}, {"text": "We had two human judges reviewing these collocations to determine which of them are true collocations and which are not.", "labels": [], "entities": []}, {"text": "We presented the collocations to the judges in random order, and each collocation was presented twice.", "labels": [], "entities": []}, {"text": "The first judge was consistent (judged a collocation in the same way both times it appeared) in 90.4% of the cases.", "labels": [], "entities": [{"text": "consistent", "start_pos": 20, "end_pos": 30, "type": "METRIC", "confidence": 0.9782981276512146}]}, {"text": "The second judge was consistent in 88% of the cases.", "labels": [], "entities": [{"text": "consistent", "start_pos": 21, "end_pos": 31, "type": "METRIC", "confidence": 0.9836097955703735}]}, {"text": "The agreement between the two judges was 67.5% (computed in a strict way, that is we considered agreement only when the two judges had the same opinion including the cases when they were not consistent).", "labels": [], "entities": [{"text": "agreement", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9692172408103943}]}, {"text": "The consistency and agreement figures show how difficult the task is for humans.", "labels": [], "entities": [{"text": "consistency", "start_pos": 4, "end_pos": 15, "type": "METRIC", "confidence": 0.9983400106430054}, {"text": "agreement", "start_pos": 20, "end_pos": 29, "type": "METRIC", "confidence": 0.9668007493019104}]}, {"text": "We used the data annotated by the two judges to build a standard solution, so we can evaluate the results of our MI filter.", "labels": [], "entities": []}, {"text": "In the standard solution a bigram was considered a true collocation if both judges considered it so.", "labels": [], "entities": []}, {"text": "We used the standard solution to evaluate the results of the filtering, for various values of the threshold T mi . That is, if a bigram had the value of MI on the Web lower than a threshold T mi , it was filtered out.", "labels": [], "entities": []}, {"text": "We choose the value of T mi so that the accuracy of our filtering program is the highest.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.999679446220398}]}, {"text": "By accuracy we mean the number of true collocations (as given by the standard solution) identified by our program over the total number of bigrams we used in the evaluation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 3, "end_pos": 11, "type": "METRIC", "confidence": 0.9994296431541443}]}, {"text": "The best accuracy was 70.7% for T mi = 0.0017.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 9, "end_pos": 17, "type": "METRIC", "confidence": 0.9997814297676086}, {"text": "T mi", "start_pos": 32, "end_pos": 36, "type": "METRIC", "confidence": 0.971072793006897}]}, {"text": "We used this value of the threshold when running our programs for all CTRW.", "labels": [], "entities": [{"text": "CTRW", "start_pos": 70, "end_pos": 74, "type": "DATASET", "confidence": 0.9243923425674438}]}, {"text": "As a result of this first part of the evaluation, we can say that after filtering collocations based on MI on the Web, approximately 70.7% of the remaining bigrams are true collocation.", "labels": [], "entities": []}, {"text": "This value is not absolute, because we used a sample of the data for the evaluation.", "labels": [], "entities": []}, {"text": "The 70.7% accuracy is much better than a baseline (approximately 50% for random choice).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9988572597503662}]}, {"text": "Next, we proceeded with evaluating the differential t-test three-way classifier.", "labels": [], "entities": []}, {"text": "For each cluster, for each collocation, new collocations were formed from the collocate and all the near-synonyms in the cluster.", "labels": [], "entities": []}, {"text": "In order to learn the classifier, and to evaluate its results, we had the two judges manually classify a sample data into preferred collocations, lesspreferred collocations, and anti-collocations.", "labels": [], "entities": []}, {"text": "We used 2838 collocations obtained for the same three clusters from 401 collocations (out of the initial 916) that remained after filtering.", "labels": [], "entities": []}, {"text": "We built a standard solution for this task, based on the classifications of Step Baseline Our system Filter (MI on the Web) 50% 70.7% Dif.", "labels": [], "entities": []}, {"text": "t-test classifier 71.4% 84.1%: Accuracy of our main steps.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.9970272183418274}]}, {"text": "When the judges agreed, the class was clear.", "labels": [], "entities": []}, {"text": "When they did not agree, we designed simple rules, such as: when one judge chose the class preferred collocation, and the other judge chose the class anti-collocation, the class in the solution was less-preferred collocation.", "labels": [], "entities": []}, {"text": "The agreement between judges was 80%; therefore we are confident that the quality of our standard solution is high.", "labels": [], "entities": []}, {"text": "We used this standard solution as training data to learn a decision tree 6 for our three-way classifier.", "labels": [], "entities": []}, {"text": "The features in the decision tree are the t-test between each collocation and the collocation from the same group that has maximum frequency on the Web, and the t-test between the current collocation and the collocation that has minimum frequency (as presented in).", "labels": [], "entities": []}, {"text": "We could have set aside apart of the training data as a test set.", "labels": [], "entities": []}, {"text": "Instead, we did 10-fold cross validation to quantify the accuracy on unseen data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.9991325736045837}]}, {"text": "The accuracy on the test set was 84.1% (compared with a baseline that chooses the most frequent class, anti-collocations, and achieves an accuracy of 71.4%).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9997370839118958}, {"text": "accuracy", "start_pos": 138, "end_pos": 146, "type": "METRIC", "confidence": 0.9993934631347656}]}, {"text": "We also experimented with including MI as a feature in the decision tree, and with manually choosing thresholds (without a decision tree) for the three-way classification, but the accuracy was lower than 84.1%.", "labels": [], "entities": [{"text": "MI", "start_pos": 36, "end_pos": 38, "type": "TASK", "confidence": 0.6126723289489746}, {"text": "accuracy", "start_pos": 180, "end_pos": 188, "type": "METRIC", "confidence": 0.999599039554596}]}, {"text": "The three-way classifier can fix some of the mistakes of the MI filter.", "labels": [], "entities": []}, {"text": "If a wrong collocation remained after the MI filter, the classifier can classify it in the anti-collocations class.", "labels": [], "entities": []}, {"text": "We can conclude that the collocational knowledge we acquired has acceptable quality.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: The second column shows the number of  hits for the collocation daunting x, where x is one  of the near-synonyms in the first column. The third  column shows the mutual information, the fourth  column, the differential t-test between the colloca- tion with maximum frequency (daunting task) and  daunting x, and the last column, the t-test between  daunting x and the collocation with minimum fre- quency (daunting hitch).", "labels": [], "entities": [{"text": "fre- quency", "start_pos": 403, "end_pos": 414, "type": "METRIC", "confidence": 0.9245650172233582}]}]}