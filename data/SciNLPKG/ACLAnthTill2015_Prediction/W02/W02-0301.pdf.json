{"title": [{"text": "Tuning Support Vector Machines for Biomedical Named Entity Recognition", "labels": [], "entities": [{"text": "Biomedical Named Entity Recognition", "start_pos": 35, "end_pos": 70, "type": "TASK", "confidence": 0.6256982833147049}]}], "abstractContent": [{"text": "We explore the use of Support Vector Machines (SVMs) for biomedical named entity recognition.", "labels": [], "entities": [{"text": "biomedical named entity recognition", "start_pos": 57, "end_pos": 92, "type": "TASK", "confidence": 0.661111131310463}]}, {"text": "To make the SVM training with the available largest corpus-the GENIA corpus-tractable, we propose to split the nonentity class into sub-classes, using part-of-speech information.", "labels": [], "entities": [{"text": "SVM", "start_pos": 12, "end_pos": 15, "type": "TASK", "confidence": 0.9436362385749817}, {"text": "GENIA corpus-tractable", "start_pos": 63, "end_pos": 85, "type": "DATASET", "confidence": 0.974296897649765}]}, {"text": "In addition , we explore new features such as word cache and the states of an HMM trained by unsupervised learning.", "labels": [], "entities": []}, {"text": "Experiments on the GENIA corpus show that our class splitting technique not only enables the training with the GENIA corpus but also improves the accuracy.", "labels": [], "entities": [{"text": "GENIA corpus", "start_pos": 19, "end_pos": 31, "type": "DATASET", "confidence": 0.9778711497783661}, {"text": "class splitting", "start_pos": 46, "end_pos": 61, "type": "TASK", "confidence": 0.7101943045854568}, {"text": "GENIA corpus", "start_pos": 111, "end_pos": 123, "type": "DATASET", "confidence": 0.9739897847175598}, {"text": "accuracy", "start_pos": 146, "end_pos": 154, "type": "METRIC", "confidence": 0.9979269504547119}]}, {"text": "The proposed new features also contribute to improve the accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.9986276626586914}]}, {"text": "We compare our SVM-based recognition system with a system using Maximum Entropy tagging method.", "labels": [], "entities": [{"text": "SVM-based recognition", "start_pos": 15, "end_pos": 36, "type": "TASK", "confidence": 0.7958449423313141}]}], "introductionContent": [{"text": "Application of natural language processing (NLP) is now a key research topic in bioinformatics.", "labels": [], "entities": [{"text": "Application of natural language processing (NLP)", "start_pos": 0, "end_pos": 48, "type": "TASK", "confidence": 0.7758212462067604}]}, {"text": "Since it is practically impossible fora researcher to grasp all of the huge amount of knowledge provided in the form of natural language, e.g., journal papers, there is a strong demand for biomedical information extraction (IE), which extracts knowledge automatically from biomedical papers using NLP techniques ().", "labels": [], "entities": [{"text": "biomedical information extraction (IE)", "start_pos": 189, "end_pos": 227, "type": "TASK", "confidence": 0.810569187005361}]}, {"text": "The process called named entity recognition, which finds entities that fill the information slots, e.g., proteins, DNAs, RNAs, cells etc., in the biomedical context, is an important building block in such biomedical IE systems.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 19, "end_pos": 43, "type": "TASK", "confidence": 0.6970679561297098}, {"text": "IE", "start_pos": 216, "end_pos": 218, "type": "TASK", "confidence": 0.6820181608200073}]}, {"text": "Conceptually, named entity recognition consists of two tasks: identification, which finds the region of a named entity in a text, and classification, which determines the semantic class of that named entity.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 14, "end_pos": 38, "type": "TASK", "confidence": 0.6397721072038015}]}, {"text": "The following illustrates biomedical named entity recognition.", "labels": [], "entities": [{"text": "biomedical named entity recognition", "start_pos": 26, "end_pos": 61, "type": "TASK", "confidence": 0.6825179308652878}]}, {"text": "\" Machine learning approach has been applied to biomedical named entity recognition (;).", "labels": [], "entities": [{"text": "biomedical named entity recognition", "start_pos": 48, "end_pos": 83, "type": "TASK", "confidence": 0.6035623475909233}]}, {"text": "However, no work has achieved sufficient recognition accuracy.", "labels": [], "entities": [{"text": "recognition", "start_pos": 41, "end_pos": 52, "type": "TASK", "confidence": 0.9040568470954895}, {"text": "accuracy", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.9421496987342834}]}, {"text": "One reason is the lack of annotated corpora for training as is often the case of anew domain. and  trained their model with only 100 annotated paper abstracts from the MEDLINE database (National Library of Medicine, 1999), and used only 77 annotated paper abstracts.", "labels": [], "entities": [{"text": "MEDLINE database (National Library of Medicine, 1999)", "start_pos": 168, "end_pos": 221, "type": "DATASET", "confidence": 0.9089471995830536}]}, {"text": "In addition, it is difficult to compare the techniques used in each study because they used a closed and different corpus.", "labels": [], "entities": []}, {"text": "To overcome such a situation, the GENIA corpus () has been developed, and at this time it is the largest biomedical annotated corpus available to public, containing 670 annotated abstracts of the MEDLINE database.", "labels": [], "entities": [{"text": "GENIA corpus", "start_pos": 34, "end_pos": 46, "type": "DATASET", "confidence": 0.9367745816707611}, {"text": "MEDLINE database", "start_pos": 196, "end_pos": 212, "type": "DATASET", "confidence": 0.9382061660289764}]}, {"text": "Another reason for low accuracies is that biomedical named entities are essentially hard to recognize using standard feature sets compared with the named entities in newswire articles ( ).", "labels": [], "entities": [{"text": "accuracies", "start_pos": 23, "end_pos": 33, "type": "METRIC", "confidence": 0.9719537496566772}]}, {"text": "Thus, we need to employ powerful machine learning techniques which can incorporate various and complex features in a consistent way.", "labels": [], "entities": []}, {"text": "Support Vector Machines (SVMs) and Maximum Entropy (ME) method) are powerful learning methods that satisfy such requirements, and are applied successfully to other NLP tasks.", "labels": [], "entities": []}, {"text": "In this paper, we apply Support Vector Machines to biomedical named entity recognition and train them with the GENIA corpus.", "labels": [], "entities": [{"text": "biomedical named entity recognition", "start_pos": 51, "end_pos": 86, "type": "TASK", "confidence": 0.6289734616875648}, {"text": "GENIA corpus", "start_pos": 111, "end_pos": 123, "type": "DATASET", "confidence": 0.9596051275730133}]}, {"text": "We formulate the named entity recognition as the classification of each word with context to one of the classes that represent region and named entity's semantic class.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 17, "end_pos": 41, "type": "TASK", "confidence": 0.7395838499069214}]}, {"text": "Although there is a previous work that applied SVMs to biomedical named entity task in this formulation (), their method to construct a classifier using SVMs, one-vs-rest, fails to train a classifier with entire GENIA corpus, since the cost of SVM training is super-linear to the size of training samples.", "labels": [], "entities": [{"text": "GENIA corpus", "start_pos": 212, "end_pos": 224, "type": "DATASET", "confidence": 0.9382285177707672}]}, {"text": "Even with a more feasible method, pairwise, which is employed in ), we cannot train a classifier in a reasonable time, because we have a large number of samples that belong to the non-entity class in this formulation.", "labels": [], "entities": []}, {"text": "To solve this problem, we propose to split the non-entity class to several sub-classes, using part-ofspeech information.", "labels": [], "entities": []}, {"text": "We show that this technique not only enables the training feasible but also improves the accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 89, "end_pos": 97, "type": "METRIC", "confidence": 0.9988945126533508}]}, {"text": "In addition, we explore new features such as word cache and the states of an unsupervised HMM for named entity recognition using SVMs.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 98, "end_pos": 122, "type": "TASK", "confidence": 0.6438856522242228}]}, {"text": "In the experiments, we show the effect of using these features and compare the overall performance of our SVMbased recognition system with a system using the Maximum Entropy method, which is an alternative to the SVM method.", "labels": [], "entities": [{"text": "SVMbased recognition", "start_pos": 106, "end_pos": 126, "type": "TASK", "confidence": 0.6878886818885803}]}], "datasetContent": [{"text": "To conduct experiments, we divided 670 abstracts of the GENIA corpus (Ver.", "labels": [], "entities": [{"text": "GENIA corpus", "start_pos": 56, "end_pos": 68, "type": "DATASET", "confidence": 0.9707995653152466}]}, {"text": "1.1) into the training part (590 abstracts; 4,487 sentences; 133,915 words) and the test part (80 abstracts; 622 sentences; 18,211 words).", "labels": [], "entities": []}, {"text": "Texts are tokenized by using Penn Treebank's tokenizer.", "labels": [], "entities": [{"text": "Penn Treebank's tokenizer", "start_pos": 29, "end_pos": 54, "type": "DATASET", "confidence": 0.9671027213335037}]}, {"text": "An HMM for the HMM state features was trained with raw abstracts of the GENIA corpus (39,116 sentences).", "labels": [], "entities": [{"text": "GENIA corpus", "start_pos": 72, "end_pos": 84, "type": "DATASET", "confidence": 0.9751468598842621}]}, {"text": "The number of states is 160.", "labels": [], "entities": []}, {"text": "The vocabulary for the word feature is constructed by taking the most frequent 10,000 words from the above raw abstracts, the prefix/suffix/prefix list by taking the most frequent 10,000 prefixes/suffixes/substrings.", "labels": [], "entities": []}, {"text": "The performance is measured by precision, recall, and F-score, which are the standard measures for the named entity recognition.", "labels": [], "entities": [{"text": "precision", "start_pos": 31, "end_pos": 40, "type": "METRIC", "confidence": 0.9997695088386536}, {"text": "recall", "start_pos": 42, "end_pos": 48, "type": "METRIC", "confidence": 0.9995757937431335}, {"text": "F-score", "start_pos": 54, "end_pos": 61, "type": "METRIC", "confidence": 0.9993621706962585}, {"text": "named entity recognition", "start_pos": 103, "end_pos": 127, "type": "TASK", "confidence": 0.6556707123915354}]}, {"text": "Systems based on the BIO representation may produce an inconsistent class sequence such as \"O B-DNA I-RNA O\".", "labels": [], "entities": []}, {"text": "We interpret such outputs as follows: once a named entity starts with \"B-C\" then we interpret that the named entity with class \"C\" ends only when we see another \"B-\" or \"O-\" tag.", "labels": [], "entities": []}, {"text": "We have implemented SMO algorithm (Platt, 1998) and techniques described in for soft margin SVMs in C++ programming language, and implemented support codes for pairwise classification and parallel training in Java programming language.", "labels": [], "entities": [{"text": "SMO", "start_pos": 20, "end_pos": 23, "type": "TASK", "confidence": 0.9669271111488342}, {"text": "pairwise classification", "start_pos": 160, "end_pos": 183, "type": "TASK", "confidence": 0.7635020017623901}]}, {"text": "To obtain POS information required for features and class splitting, we used an English POS tagger described in ().", "labels": [], "entities": [{"text": "class splitting", "start_pos": 52, "end_pos": 67, "type": "TASK", "confidence": 0.7488629817962646}]}], "tableCaptions": [{"text": " Table 2: Training time and accuracy with/without  the class splitting technique. The number of training  samples includes SOS and EOS (special words for  the start/end of a sentence).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.9994660019874573}, {"text": "class splitting", "start_pos": 55, "end_pos": 70, "type": "TASK", "confidence": 0.7550649642944336}, {"text": "SOS", "start_pos": 123, "end_pos": 126, "type": "METRIC", "confidence": 0.9473440051078796}, {"text": "EOS", "start_pos": 131, "end_pos": 134, "type": "METRIC", "confidence": 0.981036901473999}]}, {"text": " Table 3: Effect of each feature set assessed by  adding/subtracting (F-score). Changes in bold face  means positive effect.", "labels": [], "entities": [{"text": "F-score", "start_pos": 70, "end_pos": 77, "type": "METRIC", "confidence": 0.995389461517334}]}]}