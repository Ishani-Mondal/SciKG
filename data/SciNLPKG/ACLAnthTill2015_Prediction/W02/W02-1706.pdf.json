{"title": [{"text": "XML-Based NLP Tools for Analysing and Annotating Medical Language", "labels": [], "entities": [{"text": "Analysing and Annotating Medical Language", "start_pos": 24, "end_pos": 65, "type": "TASK", "confidence": 0.7190758943557739}]}], "abstractContent": [{"text": "We describe the use of a suite of highly flexible XML-based NLP tools in a project for processing and interpreting text in the medical domain.", "labels": [], "entities": []}, {"text": "The main aim of the paper is to demonstrate the central role that XML markup and XML NLP tools have played in the analysis process and to describe the resultant annotated corpus of MEDLINE abstracts.", "labels": [], "entities": []}, {"text": "In addition to the XML tools, we have succeeded in integrating a variety of non-XML 'off the shelf' NLP tools into our pipelines, so that their output is added into the markup.", "labels": [], "entities": []}, {"text": "We demonstrate the utility of the annotations that result in two ways.", "labels": [], "entities": []}, {"text": "First, we investigate how they can be used to improve parse coverage of a hand-crafted grammar that generates logical forms.", "labels": [], "entities": [{"text": "parse coverage", "start_pos": 54, "end_pos": 68, "type": "TASK", "confidence": 0.8850258588790894}]}, {"text": "And second, we investigate how they contribute to automatic lexical semantic acquisition processes.", "labels": [], "entities": [{"text": "automatic lexical semantic acquisition processes", "start_pos": 50, "end_pos": 98, "type": "TASK", "confidence": 0.6887121140956879}]}], "introductionContent": [{"text": "In this paper we describe our use of XML for an analysis of medical language which involves a number of complex linguistic processing stages.", "labels": [], "entities": []}, {"text": "The ultimate aim of the project is to to acquire lexical semantic information from MEDLINE through parsing, however, a fundamental tenet of our approach is that higher-level NLP activities benefit hugely from being based on a reliable and well-considerered initial stage of tokenisation.", "labels": [], "entities": []}, {"text": "This is particularly true for language tasks in the biomedical and other technical domains since general purpose NLP technology may stumble at the first hurdle when confronted with character strings that represent specialised technical vocabulary.", "labels": [], "entities": []}, {"text": "Once firm foundations are laid then one can achieve better performance from e.g. chunkers and parsers than might otherwise be the case.", "labels": [], "entities": []}, {"text": "We show how well-founded tools, especially XMLbased ones, can enable a variety of NLP components to be bundled together in different ways to achieve different types of analysis.", "labels": [], "entities": []}, {"text": "Note that in fields such as information extraction (IE) it is common to use statistical text classification methods for data analysis.", "labels": [], "entities": [{"text": "information extraction (IE)", "start_pos": 28, "end_pos": 55, "type": "TASK", "confidence": 0.8667435586452484}, {"text": "statistical text classification", "start_pos": 76, "end_pos": 107, "type": "TASK", "confidence": 0.6409365733464559}]}, {"text": "Our more linguistic approach maybe of assistence in IE: see for discussion of methods for IE from MEDLINE.", "labels": [], "entities": [{"text": "IE", "start_pos": 52, "end_pos": 54, "type": "TASK", "confidence": 0.7686341404914856}, {"text": "IE", "start_pos": 90, "end_pos": 92, "type": "TASK", "confidence": 0.9617628455162048}, {"text": "MEDLINE", "start_pos": 98, "end_pos": 105, "type": "DATASET", "confidence": 0.9256365299224854}]}, {"text": "Our processing paradigm is XML-based.", "labels": [], "entities": []}, {"text": "As a mark-up language for NLP tasks, XML is expressive and flexible yet constrainable.", "labels": [], "entities": []}, {"text": "Furthermore, there exist a wide range of XML-based tools for NLP applications which lend themselves to a modular, pipelined approach to processing whereby linguistic knowledge is computed and added as XML annotations in an incremental fashion.", "labels": [], "entities": []}, {"text": "In processing MEDLINE abstracts we have built a number of such pipelines using as key components the programs distributed with the LT TTT and LT XML toolsets (;).", "labels": [], "entities": [{"text": "LT TTT and LT XML toolsets", "start_pos": 131, "end_pos": 157, "type": "DATASET", "confidence": 0.8031221032142639}]}, {"text": "We have also successfully integrated non-XML publicdomain tools into our pipelines and incorporated their output into the XML mark-up using the LT XML program xmlperl).", "labels": [], "entities": []}, {"text": "In Section 2 we describe our use of XML-based tokenisation tools and techniques and in Sections 3 and 4 we describe two different approaches to analysing MEDLINE data which are built on top of the tokenisation.", "labels": [], "entities": []}, {"text": "The first approach uses a handcoded grammar to give complete syntactic and semantic analyses of sentences.", "labels": [], "entities": []}, {"text": "The second approach performs a shallower statistically-based analysis which yields 'grammatical relations' rather than full logical forms.", "labels": [], "entities": []}, {"text": "This information about grammatical relations is used in a statistically-trained model which disambiguates the semantic relations in noun compounds headed by deverbal nominalisations.", "labels": [], "entities": []}, {"text": "For this second approach we compare two separate methods of shallow analysis which require the use of two different part-of-speech taggers.", "labels": [], "entities": [{"text": "shallow analysis", "start_pos": 60, "end_pos": 76, "type": "TASK", "confidence": 0.8440652787685394}]}, {"text": "By pre-parsing we mean identification of word tokens and sentence boundaries and other lowerlevel processing tasks such as part-of-speech (POS) tagging and lemmatisation.", "labels": [], "entities": [{"text": "identification of word tokens and sentence boundaries", "start_pos": 23, "end_pos": 76, "type": "TASK", "confidence": 0.8440494537353516}, {"text": "part-of-speech (POS) tagging", "start_pos": 123, "end_pos": 151, "type": "TASK", "confidence": 0.6413164377212525}]}, {"text": "These initial stages of processing form the foundation of our NLP work with MEDLINE abstracts and our methods are flexible enough that the representation of pre-parsing can be easily tailored to suit the input needs of subsequent higher-level processors.", "labels": [], "entities": []}, {"text": "We start by converting the OHSUMED corpus from its original format to an XML format (see).", "labels": [], "entities": [{"text": "OHSUMED corpus", "start_pos": 27, "end_pos": 41, "type": "DATASET", "confidence": 0.8974287211894989}]}, {"text": "From this point on we pass the data through pipelines which are composed of calls to a variety of XML-based tools from the LT TTT and LT XML toolsets.", "labels": [], "entities": [{"text": "LT TTT", "start_pos": 123, "end_pos": 129, "type": "DATASET", "confidence": 0.9084385335445404}, {"text": "LT XML toolsets", "start_pos": 134, "end_pos": 149, "type": "DATASET", "confidence": 0.8225191036860148}]}, {"text": "The core program in our pipelines is the LT TTT program fsgmatch, a general purpose transducer which processes an input stream and rewrites it using rules provided in a hand-written grammar file, where the rewrite usually takes the form of the addition of XML mark-up.", "labels": [], "entities": []}, {"text": "Typically, fsgmatch rules specify patterns over sequences of XML elements and use a regular expression language to identify patterns inside the character strings (PCDATA) which are the content of elements.", "labels": [], "entities": []}, {"text": "For example, the following rule for decimals such as \".25\" is searching fora sequence of two S elements where the first contains the string \".\" as its PCDATA content and the second has been identified as a cardinal number (C='CD', e.g. any sequence of digits).", "labels": [], "entities": []}, {"text": "When these two S elements are found, they are wrapped in a W element with the attribute C='CD' (targ sg).", "labels": [], "entities": []}, {"text": "(Here S elements encode character sequences, see below, and W elements encode words.)", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}