{"title": [{"text": "An Empirical Verification of Coverage and Correctness fora General-Purpose Sentence Generator", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper describes a general-purpose sentence generation system that can achieve both broad scale coverage and high quality while aiming to be suitable fora variety of generation tasks.", "labels": [], "entities": [{"text": "sentence generation", "start_pos": 39, "end_pos": 58, "type": "TASK", "confidence": 0.7245835810899734}]}, {"text": "We measure the coverage and correctness empirically using a section of the Penn Tree-bank corpus as a test set.", "labels": [], "entities": [{"text": "coverage", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.955832839012146}, {"text": "correctness", "start_pos": 28, "end_pos": 39, "type": "METRIC", "confidence": 0.9325142502784729}, {"text": "Penn Tree-bank corpus", "start_pos": 75, "end_pos": 96, "type": "DATASET", "confidence": 0.995822548866272}]}, {"text": "We also describe novel features that help make the generator flexible and easier to use fora variety of tasks.", "labels": [], "entities": []}, {"text": "To our knowledge, this is the first empirical measurement of coverage reported in the literature, and the highest reported measurements of correctness.", "labels": [], "entities": [{"text": "coverage", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.9572269916534424}]}], "introductionContent": [{"text": "Natural language generation (NLG) is a subtask of a wide variety of applications.", "labels": [], "entities": [{"text": "Natural language generation (NLG)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7742749253908793}]}, {"text": "Such applications include machine translation, human-computer dialogue, summarization, report creation, automatic technical documentation, proof/decision explanation, customized instructions, item and event descriptions, question answering, tutorials, stories, and more.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 26, "end_pos": 45, "type": "TASK", "confidence": 0.8195991516113281}, {"text": "summarization", "start_pos": 72, "end_pos": 85, "type": "TASK", "confidence": 0.9869568347930908}, {"text": "report creation", "start_pos": 87, "end_pos": 102, "type": "TASK", "confidence": 0.7018868178129196}, {"text": "proof/decision explanation", "start_pos": 139, "end_pos": 165, "type": "TASK", "confidence": 0.6085212528705597}, {"text": "question answering", "start_pos": 221, "end_pos": 239, "type": "TASK", "confidence": 0.797064483165741}]}, {"text": "While many applications use a custom-built generator, a general-purpose system can facilitate reuse of resources and reduce the costs of building applications.", "labels": [], "entities": []}, {"text": "Research into general-purpose generation has tended to focus on sentence realization, which is one of the most common recurring subtasks of generation.", "labels": [], "entities": [{"text": "general-purpose generation", "start_pos": 14, "end_pos": 40, "type": "TASK", "confidence": 0.8082135021686554}, {"text": "sentence realization", "start_pos": 64, "end_pos": 84, "type": "TASK", "confidence": 0.7217148840427399}]}, {"text": "Sentence realization is the process of transforming a syntactic sentence plan into a linearlyordered, grammatical string of morphologically inflected words.", "labels": [], "entities": [{"text": "Sentence realization", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.9232995510101318}]}, {"text": "To be generally useful, a realizer needs to be able to handle a wide array of syntactic phenomena.", "labels": [], "entities": []}, {"text": "It also needs to produce grammatically correct output.", "labels": [], "entities": []}, {"text": "Prominent general-purpose realization systems developed to date include FUF/Surge and), RealPro), Penman/KPML (, and Nitrogen (, . These systems have demonstrated their general usefulness by being deployed in a variety of different applications.", "labels": [], "entities": [{"text": "FUF/Surge", "start_pos": 72, "end_pos": 81, "type": "DATASET", "confidence": 0.8014776706695557}, {"text": "Penman/KPML", "start_pos": 98, "end_pos": 109, "type": "DATASET", "confidence": 0.7688265442848206}]}, {"text": "However, it is still difficult ascertain the degree to which they have achieved broad coverage of natural language or high quality output because no empirical evaluation has been performed.", "labels": [], "entities": []}, {"text": "At best, suites of example inputs have been used for regression testing.", "labels": [], "entities": [{"text": "regression testing", "start_pos": 53, "end_pos": 71, "type": "TASK", "confidence": 0.9510742127895355}]}, {"text": "However, such regression suites are biased towards the capabilities of their respective systems and consist of relatively few inputs compared to the variety of input classes that are possible.", "labels": [], "entities": []}, {"text": "For example, there are currently 500 test inputs distributed with Surge, and about 210 for English with KPML.", "labels": [], "entities": [{"text": "Surge", "start_pos": 66, "end_pos": 71, "type": "TASK", "confidence": 0.728022575378418}, {"text": "KPML", "start_pos": 104, "end_pos": 108, "type": "DATASET", "confidence": 0.9547767043113708}]}, {"text": "At any rate, no matter how large the regression suite maybe, the inherent irregularity of natural language makes regression testing inadequate as a means of assessing coverage or quality.", "labels": [], "entities": []}, {"text": "In practice, there is a seemingly irreconcilable conflict between broad coverage and high quality output.", "labels": [], "entities": []}, {"text": "shows some examples of this expressibility problem.", "labels": [], "entities": []}, {"text": "High quality output is thus much easier to achieve with smaller-scale applications or in limited domains.", "labels": [], "entities": []}, {"text": "In this paper we introduce HALogen, a generalpurpose sentence generator that achieves both broad coverage of English and high quality output as measured against an unseen section of the Penn Treebank (.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 186, "end_pos": 199, "type": "DATASET", "confidence": 0.9968070089817047}]}, {"text": "We compare it to the only other generation system that has performed such an evaluation, a limited-purpose system named Fergus ().", "labels": [], "entities": [{"text": "Fergus", "start_pos": 120, "end_pos": 126, "type": "DATASET", "confidence": 0.9855426549911499}]}, {"text": "HALogen's development has been guided in part by the question, \"What is the simplest input notation that suffices to represent and correctly generate all valid sentences, and yet at the same time is easy for applications to use?\"", "labels": [], "entities": []}, {"text": "We describe novel aspects that help make the generator flexible and easier to use fora variety of applications.", "labels": [], "entities": []}, {"text": "Section 2 gives a brief overview of the HALogen sentence generation system.", "labels": [], "entities": [{"text": "HALogen sentence generation", "start_pos": 40, "end_pos": 67, "type": "TASK", "confidence": 0.9096126953760783}]}, {"text": "Section 3 describes the setup of the empirical evaluation, and Section 4 discusses the results.", "labels": [], "entities": []}, {"text": "Finally, Section 5 concludes.", "labels": [], "entities": []}], "datasetContent": [{"text": "The goal of an empirical evaluation of coverage and quality is to measure the extent to which any and every valid English sentence can be represented and generated.", "labels": [], "entities": []}, {"text": "Generation is usually notoriously difficult to evaluate because grammaticality is difficult to measure automatically, and more than one output can be acceptable.", "labels": [], "entities": []}, {"text": "However, although variations in output are usually acceptable in the context of specific applications, different applications can have different constraints on the kinds of variation in output that they accept.", "labels": [], "entities": []}, {"text": "By demonstrating the capability to produce any desired sentence exactly, a system can assure that all possible application constraints on the output can be met.", "labels": [], "entities": []}, {"text": "Thus, these experiments focus on whether a desired sentence can be produced exactly, though this kind of measurement is harsher than necessary.", "labels": [], "entities": []}, {"text": "Section 23 of the Penn Treebank is used to evaluate coverage and quality.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 18, "end_pos": 31, "type": "DATASET", "confidence": 0.9871155917644501}]}, {"text": "Inputs to HALogen were automatically constructed from the Treebank annotation and then regenerated by the system.", "labels": [], "entities": []}, {"text": "The output was then compared to the original sentence.", "labels": [], "entities": []}, {"text": "The Penn Treebank offers several advantages as a test set.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 4, "end_pos": 17, "type": "DATASET", "confidence": 0.9934879541397095}]}, {"text": "It contains real-world sentences, it is large, and can be assumed to exhibit a very broad array of syntactic phenomena.", "labels": [], "entities": []}, {"text": "It is not biased towards system-specific capabilities, since it was collected independently.", "labels": [], "entities": []}, {"text": "It also acts as a standard for linguistic representation, offering the potential of interoperability with other natural language programs based on it, such as parsers.", "labels": [], "entities": [{"text": "linguistic representation", "start_pos": 31, "end_pos": 56, "type": "TASK", "confidence": 0.7466041743755341}]}, {"text": "At the same time, there are limits to its usefulness.", "labels": [], "entities": []}, {"text": "It only represents the domain of newspaper text, and thus does not test the stylistic, structural, and content variations that can occur in other domains such as question answering or dialogue.", "labels": [], "entities": [{"text": "question answering", "start_pos": 162, "end_pos": 180, "type": "TASK", "confidence": 0.8477816581726074}]}, {"text": "It also does not evaluate how the system handles nonsensical inputs or inputs that might not be expressible in grammatical English.", "labels": [], "entities": []}, {"text": "The resulting structure is a hierarchical functional dependency tree.", "labels": [], "entities": []}, {"text": "The generator's primary tasks in this evaluation are to determine the linear order of constituents, perform morphological inflections, and insert needed function words.", "labels": [], "entities": []}, {"text": "Six experiments were run to evaluate HALogen's performance with inputs that were underspecified in different ways.", "labels": [], "entities": [{"text": "HALogen", "start_pos": 37, "end_pos": 44, "type": "TASK", "confidence": 0.9050059914588928}]}, {"text": "The ability to handle underspecification eases the information burden on client applications.", "labels": [], "entities": []}, {"text": "It also makes the generator more flexible in meeting the varying needs and constraints of different types of applications.", "labels": [], "entities": []}, {"text": "The experiments use only a subset of the relations that HALogen actually recognizes.", "labels": [], "entities": []}, {"text": "No semantic relations are used, since they either cannot be straightforwardly derived from the Penn treebank annotation, or are too ambiguous to be adequately handled on the scale of the experiments in this paper.", "labels": [], "entities": [{"text": "Penn treebank annotation", "start_pos": 95, "end_pos": 119, "type": "DATASET", "confidence": 0.9750703175862631}]}, {"text": "In the first experiment, labeled \"Almost fully spec\" in, the inputs contain nearly enough detail to fully determine a unique output.", "labels": [], "entities": []}, {"text": "The inputs contain as much detail as it was possible to straight-forwardly obtain from the Treebank annotation.", "labels": [], "entities": [{"text": "Treebank annotation", "start_pos": 91, "end_pos": 110, "type": "DATASET", "confidence": 0.9231097102165222}]}, {"text": "An example is shown in.", "labels": [], "entities": []}, {"text": "In this experiment, adjuncts are represented either as premodifiers, postmodifiers or within-modifiers.", "labels": [], "entities": []}, {"text": "(Withinmodifiers are verbal modifiers that come between the subject and the object).", "labels": [], "entities": []}, {"text": "A flag in the generator is set so that constituents with the same role occurring at the same level of nesting (such as modifiers) will be ordered in the output in the same relative order in which they appear in the input (and adjacent to each other).", "labels": [], "entities": []}, {"text": "This order flag allows applications that plan discourse structure before doing sentence realization to control the coherence across sentences.", "labels": [], "entities": [{"text": "sentence realization", "start_pos": 79, "end_pos": 99, "type": "TASK", "confidence": 0.7595223784446716}]}, {"text": "For example, dialogue systems often want old or background information to appear before new information.", "labels": [], "entities": []}, {"text": "(Partial order constraints can also be specified by using extra levels of nesting in the input.", "labels": [], "entities": []}, {"text": "However, this capability is not exercised in these experiments.)", "labels": [], "entities": []}, {"text": "In the second experiment, \"Permute same-roles,\" the permutation flag set in the first experiment is reversed.", "labels": [], "entities": []}, {"text": "Constituents with the same role are permuted in place, and the statistical ranker is expected to choose the most likely order.", "labels": [], "entities": []}, {"text": "An exception occurs if there happen to be more than five constituents with the same role.", "labels": [], "entities": []}, {"text": "For computational reasons, the constituents are not permuted in this case.", "labels": [], "entities": []}, {"text": "Instead, they are placed in reverse order in the output (to avoid unfairly inflating the accuracy results).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 89, "end_pos": 97, "type": "METRIC", "confidence": 0.9989815354347229}]}, {"text": "Everything else remains the same as in the first experiment.", "labels": [], "entities": []}, {"text": "Figure 6: Fragment of a minimally specified input, and its output The third experiment, \"Permute, no dir\" is like the second, but in addition, all modifiers are mapped to the :adjunct relation, thus increasing the number of constituents that get permuted.", "labels": [], "entities": []}, {"text": "The statistical ranker must not only determine the order of the modifiers with respect to each other, but must determine the direction of each one with respect to the head.", "labels": [], "entities": []}, {"text": "The fourth experiment, \"Underspec det\", is like the first except that common determiners are left unspecified.", "labels": [], "entities": []}, {"text": "Specifically, \"the\", \"a\", \"an\", \"any\", and \"some\" are dropped from the input.", "labels": [], "entities": []}, {"text": "The nulldeterminer feature is also dropped from all nominal phrases that had no determiner in the original Treebank annotation.", "labels": [], "entities": []}, {"text": "This experiment tests the ability of the generator to supply the appropriate determiner, or figure out that none is needed.", "labels": [], "entities": []}, {"text": "The fifth experiment, \"No leaf, clause feats\", is also like the first experiment except that all the leaf and clause properties listed in are dropped from the input.", "labels": [], "entities": []}, {"text": "Only the value of the :lex feature is retained for the input.", "labels": [], "entities": []}, {"text": "The sixth experiment, \"Min spec,\" represents the opposite extreme from the first experiment.", "labels": [], "entities": []}, {"text": "All the information dropped in experiments 2-5 is also dropped in this experiment.", "labels": [], "entities": []}, {"text": "An example of such an input and its output is shown in.", "labels": [], "entities": []}], "tableCaptions": []}