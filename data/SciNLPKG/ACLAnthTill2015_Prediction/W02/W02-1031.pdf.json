{"title": [{"text": "The SuperARV Language Model: Investigating the Effectiveness of Tightly Integrating Multiple Knowledge Sources", "labels": [], "entities": [{"text": "Tightly Integrating Multiple Knowledge Sources", "start_pos": 64, "end_pos": 110, "type": "TASK", "confidence": 0.8213209986686707}]}], "abstractContent": [{"text": "A new almost-parsing language model incorporating multiple knowledge sources that is based upon the concept of Constraint Dependency Grammars is presented in this paper.", "labels": [], "entities": []}, {"text": "Lexical features and syntactic constraints are tightly integrated into a uniform linguistic structure called a SuperARV that is associated with a word in the lexicon.", "labels": [], "entities": []}, {"text": "The Super-ARV language model reduces perplexity and word error rate compared to trigram, part-of-speech-based, and parser-based language models.", "labels": [], "entities": [{"text": "word error rate", "start_pos": 52, "end_pos": 67, "type": "METRIC", "confidence": 0.6569559872150421}]}, {"text": "The relative contributions of the various knowledge sources to the strength of our model are also investigated by using constraint relaxation at the level of the knowledge sources.", "labels": [], "entities": []}, {"text": "We have found that although each knowledge source contributes to language model quality, lexical features are an outstanding contributor when they are tightly integrated with word identity and syntactic constraints.", "labels": [], "entities": []}, {"text": "Our investigation also suggests possible reasons for the reported poor performance of several probabilistic dependency grammar models in the literature.", "labels": [], "entities": []}], "introductionContent": [{"text": "The purpose of a language model (LM) is to determine the a priori probability of a word sequence w 1 , . .", "labels": [], "entities": []}, {"text": ", w n , P (w 1 , . .", "labels": [], "entities": []}, {"text": "Language modeling is essential in a wide variety of applications; we focus on speech recognition in our research.", "labels": [], "entities": [{"text": "Language modeling", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7253098487854004}, {"text": "speech recognition", "start_pos": 78, "end_pos": 96, "type": "TASK", "confidence": 0.7646900117397308}]}, {"text": "Although wordbased LMs (with bigram and trigram being the most common) remain the mainstay in many continuous speech recognition systems, recent efforts have explored a variety of ways to improve LM performance.", "labels": [], "entities": [{"text": "continuous speech recognition", "start_pos": 99, "end_pos": 128, "type": "TASK", "confidence": 0.6450196703275045}]}, {"text": "Class-based LMs attempt to deal with data sparseness and generalize better to unseen word sequences by first grouping words into classes and then using these classes to compute n-gram probabilities.", "labels": [], "entities": []}, {"text": "Partof-Speech (POS) tags were initially used as classes by in a conditional probabilistic model (which predicts the tag sequence fora word sequence first and then uses it to predict the word sequence): However, Jelinek's POS LM is less effective at predicting word candidates than an n-gram word-based LM because it deletes important lexical information for predicting the next word.", "labels": [], "entities": [{"text": "predicting word candidates", "start_pos": 249, "end_pos": 275, "type": "TASK", "confidence": 0.7667458852132162}]}, {"text": "Heeman's (1998) POS LM achieves a perplexity reduction compared to a trigram LM by instead redefining the speech recognition problem as determining: where T is the POS sequence t N 1 associated with the word sequence W = w N 1 given the speech utterance A.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 106, "end_pos": 124, "type": "TASK", "confidence": 0.7129131108522415}]}, {"text": "The LM P (W, T ) is a joint probabilistic model that accounts for both the sequence of words w N 1 and their tag assignments t N 1 by estimating the joint probabilities of words and tags: 2001;) that incorporate syntactic information.", "labels": [], "entities": []}, {"text": "These LMs capture the hierarchical characteristics of a language rather than specific information about words and their lexical features (e.g., case, number).", "labels": [], "entities": []}, {"text": "In an attempt to incorporate even more knowledge into a structured LM, Goodman (1997) has developed a probabilistic feature grammar (PFG) that conditions not only on structure but also on a small set of grammatical features (e.g., number) and has achieved parse accuracy improvement.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 262, "end_pos": 270, "type": "METRIC", "confidence": 0.921974778175354}]}, {"text": "Goodman's work suggests that integrating lexical features with word identity and syntax would benefit LM predictiveness.", "labels": [], "entities": [{"text": "LM predictiveness", "start_pos": 102, "end_pos": 119, "type": "TASK", "confidence": 0.9137850701808929}]}, {"text": "PFG uses only a small set of lexical features because it integrates those features at the level of the production rules, causing a significant increase in grammar size and a concomitant data sparsity problem that preclude the addition of richer features.", "labels": [], "entities": [{"text": "PFG", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.853060245513916}]}, {"text": "This sparseness problem can be addressed by associating lexical features directly with words.", "labels": [], "entities": []}, {"text": "We hypothesize that high levels of word prediction capability can be achieved by tightly integrating structural constraints and lexical features at the word level.", "labels": [], "entities": [{"text": "word prediction", "start_pos": 35, "end_pos": 50, "type": "TASK", "confidence": 0.7909043431282043}]}, {"text": "Hence, we develop anew dependencygrammar almost-parsing LM, SuperARV LM, which uses enriched tags called SuperARVs.", "labels": [], "entities": []}, {"text": "In Section 2, we introduce our SuperARV LM.", "labels": [], "entities": []}, {"text": "Section 3 compares the performance of the SuperARV LM to other LMs.", "labels": [], "entities": [{"text": "SuperARV LM", "start_pos": 42, "end_pos": 53, "type": "DATASET", "confidence": 0.7976874709129333}]}, {"text": "Section 4 investigates the knowledge source contributions by constraint relaxation.", "labels": [], "entities": []}, {"text": "Conclusions appear in Section 5.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Comparing perplexity results for each LM on the WSJ PTB test set. 3gram represents the word-based", "labels": [], "entities": [{"text": "WSJ PTB test set", "start_pos": 58, "end_pos": 74, "type": "DATASET", "confidence": 0.9639583975076675}]}, {"text": " Table 3: Comparing perplexity results for each LM on", "labels": [], "entities": []}, {"text": " Table 4: Comparing WER and SAC after rescoring lattices using each LM on WSJ CSR 5k-and 20k-test sets.", "labels": [], "entities": [{"text": "WER", "start_pos": 20, "end_pos": 23, "type": "METRIC", "confidence": 0.9106804132461548}, {"text": "SAC", "start_pos": 28, "end_pos": 31, "type": "METRIC", "confidence": 0.9393322467803955}, {"text": "WSJ CSR 5k-and 20k-test sets", "start_pos": 74, "end_pos": 102, "type": "DATASET", "confidence": 0.9342690825462341}]}, {"text": " Table 6: Comparing perplexity results for each LM on", "labels": [], "entities": []}]}