{"title": [{"text": "Question Terminology and Representation for Question Type Classiication", "labels": [], "entities": [{"text": "Question Terminology", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.7047691643238068}, {"text": "Question Type Classiication", "start_pos": 44, "end_pos": 71, "type": "TASK", "confidence": 0.7686510483423868}]}], "abstractContent": [{"text": "Question terminology is a set of terms which appear in keywords, idioms and xed expressions commonly observed in questions.", "labels": [], "entities": [{"text": "Question terminology", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.6461329609155655}]}, {"text": "This paper investigates ways to automatically extract question terminology from a corpus of questions and represent them for the purpose of classifying by question type.", "labels": [], "entities": []}, {"text": "Our key interest is to see whether or not semantic features can enhance the representation of strongly lexical nature of question sentences.", "labels": [], "entities": []}, {"text": "We compare two feature sets: one with lexical features only, and another with a mixture of lexical and semantic features.", "labels": [], "entities": []}, {"text": "For evaluation, we measure the classiication accuracy made by two ma chine learning algorithms, C5.0 and PEBLS, by using a procedure called domain cross-validation, which eectively measures the domain transferability of features.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.9267593622207642}, {"text": "PEBLS", "start_pos": 105, "end_pos": 110, "type": "METRIC", "confidence": 0.7951186299324036}]}], "introductionContent": [{"text": "In Information Retrieval (IR), text categorization and clustering, documents are usually indexed and represented by domain terminology: terms which are particular to the domain/topic of a document.", "labels": [], "entities": [{"text": "Information Retrieval (IR)", "start_pos": 3, "end_pos": 29, "type": "TASK", "confidence": 0.8753426790237426}, {"text": "text categorization", "start_pos": 31, "end_pos": 50, "type": "TASK", "confidence": 0.7078632712364197}]}, {"text": "However, when documents must be retrieved or categorized according to criteria which do not correspond to the domains, such a s genre (text style) () or subjectivity (e.g. opinion vs. factual description)), we must use diierent, domain-independent features to index and represent documents.", "labels": [], "entities": []}, {"text": "In those tasks, selection of the features is in fact one of the most critical factors which aaect the performance of a system.", "labels": [], "entities": []}, {"text": "Question type classiication is one of such tasks, where categories are question types (e.g. 'how-to', 'why' and 'where').", "labels": [], "entities": []}, {"text": "In recent years, question type has been successfully used in many Question-Answering (Q&A) systems for determining the kind of entity or concept being asked and extracting an appropriate answer).", "labels": [], "entities": [{"text": "question type", "start_pos": 17, "end_pos": 30, "type": "TASK", "confidence": 0.7103648334741592}]}, {"text": "Just like genre, question types cut across domainss for instance, we can ask 'how-to' questions in the cooking domain, the legal domain etc.", "labels": [], "entities": []}, {"text": "However, features that constitute question types are diierent from those used for genre classiication (typically part-of-speech or meta-lingusitic features) in that features are strongly lexical due to the large amount of idiosyncrasy (keywords, idioms or syntactic constructions) that is frequently observed in question sentences.", "labels": [], "entities": []}, {"text": "For example, we can easily think of question patterns such as \\What is the best way to ..\" and \\What do I have to do to ..\".", "labels": [], "entities": []}, {"text": "In this regard, terms which id en tify question type are considered to form a terminology of their own, which we deene as question terminology.", "labels": [], "entities": []}, {"text": "Terms in question terminology have some characteristics.", "labels": [], "entities": []}, {"text": "First, they are mostly domainindependent, non-content words.", "labels": [], "entities": []}, {"text": "Second, they include many closed-class words (such as interrogatives, modals and pronouns), and some open-class words (e.g. the noun \\way\" and the verb \\do\").", "labels": [], "entities": []}, {"text": "Ina way, question terminology is a complement of domain terminology.", "labels": [], "entities": [{"text": "question terminology", "start_pos": 9, "end_pos": 29, "type": "TASK", "confidence": 0.7463786005973816}]}, {"text": "Automatic extraction of question terminology is a rather diicult task, since question terms are mixed in with content terms.", "labels": [], "entities": [{"text": "Automatic extraction of question terminology", "start_pos": 0, "end_pos": 44, "type": "TASK", "confidence": 0.7910536766052246}]}, {"text": "Another complicating factor is paraphrasing { there are many ways to ask the same question.", "labels": [], "entities": []}, {"text": "For example, -\\How can I clean teapots?\"", "labels": [], "entities": []}, {"text": "-\\In what way can we clean teapots?\"", "labels": [], "entities": []}, {"text": "-\\What is the best way to clean teapots?\"", "labels": [], "entities": []}, {"text": "-\\What method is used for cleaning teapots?\"", "labels": [], "entities": []}, {"text": "-\\How do I go about cleaning teapots?\"", "labels": [], "entities": []}, {"text": "In this paper, we present the results of our investigation on how to automatically extract question terminology from a corpus of questions and represent them for the purpose of classifying by question type.", "labels": [], "entities": []}, {"text": "It is an extension of our previous work), where we compared automatic and manual techniques to select features from questions, but only (stemmed) words were considered for features.", "labels": [], "entities": []}, {"text": "The focus of the current work is to investigate the kind(s) of features, rather than selection techniques, which are best suited for representing questions for classiication.", "labels": [], "entities": []}, {"text": "Specifically, from a large dataset of questions, we automatically extracted two sets of features: one set consisting of terms (i.e., lexical features) only, and another set consisting of a mixture of terms and semantic concepts (i.e., semantic features).", "labels": [], "entities": []}, {"text": "Our particular interest is to see whether or not semantic concepts can enhance the representation of strongly lexical nature of question sentences.", "labels": [], "entities": []}, {"text": "To this end, we apply two machine learning algorithms (C5.0 and PEBLS), and compare the classiication accuracy produced for the two feature sets.", "labels": [], "entities": [{"text": "PEBLS", "start_pos": 64, "end_pos": 69, "type": "METRIC", "confidence": 0.8041443824768066}, {"text": "accuracy", "start_pos": 102, "end_pos": 110, "type": "METRIC", "confidence": 0.9179279208183289}]}, {"text": "The results show that there is no signiicant increase by either algorithm by the addition of semantic features.", "labels": [], "entities": []}, {"text": "The original motivation behind our work on question terminology was to improve the retrieval accuracy of our system called FAQFinder ().", "labels": [], "entities": [{"text": "question terminology", "start_pos": 43, "end_pos": 63, "type": "TASK", "confidence": 0.7441126704216003}, {"text": "accuracy", "start_pos": 93, "end_pos": 101, "type": "METRIC", "confidence": 0.9855387210845947}, {"text": "FAQFinder", "start_pos": 123, "end_pos": 132, "type": "DATASET", "confidence": 0.6178382635116577}]}, {"text": "FAQFinder is a web-based, natural language Q&A system which uses Usenet Frequently Asked Questions (FAQ) \ud97b\udf59les to answer users' questions.", "labels": [], "entities": [{"text": "FAQFinder", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.9654973745346069}]}, {"text": "show an example session with FAQFinder.", "labels": [], "entities": [{"text": "FAQFinder", "start_pos": 29, "end_pos": 38, "type": "DATASET", "confidence": 0.8658199906349182}]}, {"text": "First, the user enters a question in natural language.", "labels": [], "entities": []}, {"text": "The system then searches the FAQ \ud97b\udf59les for questions that are similar to the user's.", "labels": [], "entities": [{"text": "FAQ", "start_pos": 29, "end_pos": 32, "type": "DATASET", "confidence": 0.6001358032226562}]}, {"text": "Based on the results of the search, FAQFinder displays a maximum of 5 FAQ questions which are ranked the highest by the system's similarity measure.", "labels": [], "entities": [{"text": "FAQFinder", "start_pos": 36, "end_pos": 45, "type": "DATASET", "confidence": 0.7566003799438477}]}, {"text": "Currently FAQFinder incorporates question type as one of the four metrics in measuring the similarity between the user's question and FAQ questions.", "labels": [], "entities": [{"text": "FAQFinder", "start_pos": 10, "end_pos": 19, "type": "DATASET", "confidence": 0.8845003843307495}, {"text": "FAQ", "start_pos": 134, "end_pos": 137, "type": "DATASET", "confidence": 0.8531011939048767}]}, {"text": "In the present implementation, the system uses a small set of manually selected words to determine the type of a question.", "labels": [], "entities": []}, {"text": "The goal of our work here is to derive optimal features which would produce improved classiication accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 99, "end_pos": 107, "type": "METRIC", "confidence": 0.9765616655349731}]}, {"text": "The other three metrics are vector similarity, semantic similarity and coverage).", "labels": [], "entities": [{"text": "coverage", "start_pos": 71, "end_pos": 79, "type": "METRIC", "confidence": 0.9660890102386475}]}, {"text": "Descriptive deenitions of these types are found in).", "labels": [], "entities": []}, {"text": "shows example FAQ questions which we had used to develop the question types.", "labels": [], "entities": []}, {"text": "Note that our question types are general question categories.", "labels": [], "entities": []}, {"text": "They are aimed to cover a wide variety of questions entered by t he F AQFinder users.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Classiication accuracy (%) on the  training set by using reduced feature sets", "labels": [], "entities": [{"text": "accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9550375938415527}]}, {"text": " Table 3: Classiication accuracy (%) on the testsets  Feature set  #  FAQFinder  AskJeeves  features C5.0 PEBLS C5.0 PEBLS  LEX  117  67.8 66.6 77.3 73.9  LEXSEM  164  67.5 67.1 73.7 71.1", "labels": [], "entities": [{"text": "accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9503294229507446}, {"text": "FAQFinder", "start_pos": 70, "end_pos": 79, "type": "DATASET", "confidence": 0.7148687839508057}, {"text": "PEBLS C5.0 PEBLS  LEX  117  67.8 66.6 77.3 73.9  LEXSEM  164  67.5 67.1 73.7 71.1", "start_pos": 106, "end_pos": 187, "type": "DATASET", "confidence": 0.7673866599798203}]}]}