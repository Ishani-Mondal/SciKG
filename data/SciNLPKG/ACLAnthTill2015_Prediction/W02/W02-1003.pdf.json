{"title": [], "abstractContent": [{"text": "We demonstrate a problem with the standard technique for learning probabilistic decision lists.", "labels": [], "entities": []}, {"text": "We describe a simple, in-cremental algorithm that avoids this problem , and show how to implement it efficiently.", "labels": [], "entities": []}, {"text": "We also show a variation that adds thresholding to the standard sorting algorithm for decision lists, leading to similar improvements.", "labels": [], "entities": []}, {"text": "Experimental results show that the new algorithm produces substantially lower error rates and entropy, while simultaneously learning lists that are over an order of magnitude smaller than those produced by the standard algorithm.", "labels": [], "entities": []}], "introductionContent": [{"text": "Decision lists have been used fora variety of natural language tasks, including accent restoration, word sense disambiguation, finding the past tense of English verbs, and several other problems.", "labels": [], "entities": [{"text": "accent restoration", "start_pos": 80, "end_pos": 98, "type": "TASK", "confidence": 0.6979619413614273}, {"text": "word sense disambiguation", "start_pos": 100, "end_pos": 125, "type": "TASK", "confidence": 0.6853708227475485}, {"text": "finding the past tense of English verbs", "start_pos": 127, "end_pos": 166, "type": "TASK", "confidence": 0.8443298850740705}]}, {"text": "We show a problem with the standard algorithm for learning probabilistic decision lists, and we introduce an incremental algorithm that consistently works better.", "labels": [], "entities": []}, {"text": "While the obvious implementation for this algorithm would be very slow, we also show how to efficiently implement it.", "labels": [], "entities": []}, {"text": "The new algorithm produces smaller lists, while simultaneously substantially reducing entropy (by about 40%), and error rates (by about 25% relative.)", "labels": [], "entities": [{"text": "error rates", "start_pos": 114, "end_pos": 125, "type": "METRIC", "confidence": 0.9786379337310791}]}, {"text": "Decision lists area very simple, easy to understand formalism.", "labels": [], "entities": []}, {"text": "Consider a word sense disambiguation task, such as distinguishing the financial sense of the word \"bank\" from the river sense.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 11, "end_pos": 36, "type": "TASK", "confidence": 0.6094698707262675}]}, {"text": "We might want the decision list to be probabilistic ( so that, for instance, the probabilities can be propagated to an understanding algorithm.", "labels": [], "entities": []}, {"text": "If no other rule is used, the last rule always triggers, ensuring that some probability is always returned.", "labels": [], "entities": []}, {"text": "The standard algorithm for learning decision lists is very simple.", "labels": [], "entities": [{"text": "learning decision lists", "start_pos": 27, "end_pos": 50, "type": "TASK", "confidence": 0.7185348272323608}]}, {"text": "The goal is to minimize the entropy of the decision list, where entropy represents how uncertain we are about a particular decision.", "labels": [], "entities": []}, {"text": "For each rule, we find the expected entropy using that rule, then sort all rules by their entropy, and output the rules in order, lowest entropy first.", "labels": [], "entities": []}, {"text": "Decision lists are fairly widely used for many reasons.", "labels": [], "entities": []}, {"text": "Most importantly, the rule outputs they produce are easily understood by humans.", "labels": [], "entities": []}, {"text": "This can make decision lists useful as a data analysis tool: the decision list can be examined to determine which factors are most important.", "labels": [], "entities": []}, {"text": "It can also make them useful when the rules must be used by humans, such as when producing guidelines to help doctors determine whether a particular drug should be administered.", "labels": [], "entities": []}, {"text": "Decision lists also tend to be relatively small and fast and easy to apply in practice.", "labels": [], "entities": []}, {"text": "Unfortunately, as we will describe, the standard algorithm for learning decision lists has an important flaw: it often chooses a rule order that is suboptimal in important ways.", "labels": [], "entities": []}, {"text": "In particular, sometimes the algorithm will use a rule that appears good -has lower average entropy -in place of one that is good -lowers the expected entropy given its location in the list.", "labels": [], "entities": []}, {"text": "We will describe a simple incremental algorithm that consistently works better than the basic sorting algorithm.", "labels": [], "entities": []}, {"text": "Essentially, the algorithm builds the list in reverse order, and, before adding a rule to the list, computes how much the rule will reduce entropy at that position.", "labels": [], "entities": []}, {"text": "This computation is potentially very expensive, but we show how to compute it efficiently so that the algorithm can still run quickly.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we give experimental results, showing that our new algorithm substantially outperforms the standard algorithm.", "labels": [], "entities": []}, {"text": "We also show that while accuracy is competitive with TBLs, two linear classifiers are more accurate than the decision list algorithms.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9991518259048462}]}, {"text": "Many of the problems that probabilistic decision list algorithms have been used for are very similar: in a given text context, determine which of two choices is most appropriate.", "labels": [], "entities": []}, {"text": "Accent restoration, word sense disambiguation, and other problems all fall into this framework, and typically use similar feature types.", "labels": [], "entities": [{"text": "Accent restoration", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8308517336845398}, {"text": "word sense disambiguation", "start_pos": 20, "end_pos": 45, "type": "TASK", "confidence": 0.7099478443463644}]}, {"text": "We thus chose one problem of this type, grammar checking, and believe that our results should carryover at least to these other, closely related problems.", "labels": [], "entities": [{"text": "grammar checking", "start_pos": 40, "end_pos": 56, "type": "TASK", "confidence": 0.9073413908481598}]}, {"text": "In particular, we chose to use exactly the same training, test, problems, and feature sets used by).", "labels": [], "entities": []}, {"text": "These problems consisted of trying to guess which of two confusable words, e.g. \"their\" or \"there\", a user intended.", "labels": [], "entities": []}, {"text": "Banko and Brill chose this data to be representative of typical machine learning problems, and, by trying it across data sizes and different pairs of words, it exhibits a good deal of different behaviors.", "labels": [], "entities": []}, {"text": "Banko and Brill used a standard set of features, including words within a window of 2, part-of-speech tags within a window of 2, pairs of word or tag features, and whether or not a given word occurred within a window of 9.", "labels": [], "entities": []}, {"text": "Altogether, they had 55 feature types.", "labels": [], "entities": []}, {"text": "They used all features of each type that occurred at least twice in the training data.", "labels": [], "entities": []}, {"text": "We ran our comparisons using 7 different algorithms.", "labels": [], "entities": []}, {"text": "The first three were variations on the standard probabilistic decision list learner.", "labels": [], "entities": []}, {"text": "In particular, first we ran the standard sorted decision list learner, equivalent to the algorithm of, with a threshold of negative infinity.", "labels": [], "entities": []}, {"text": "That is, we included all rules that had a predicted entropy at least as good as the unigram distribution, whether or not they would actually improve entropy on the training data.", "labels": [], "entities": []}, {"text": "We call this \"Sorted: \u2212\u221e.\"", "labels": [], "entities": []}, {"text": "Next, we ran the same learner with a threshold of 0 (\"Sorted: 0\"): that is, we included all rules that had a predicted entropy at least as good as the unigram distribution, and that would at least improve entropy on the training data.", "labels": [], "entities": []}, {"text": "Then we ran the algorithm with a threshold of 3 (\"Sorted: 3\"), in an attempt to avoid overfitting.", "labels": [], "entities": []}, {"text": "Next, we ran our incremental algorithm, again with a threshold of reducing training entropy by at least 3 bits.", "labels": [], "entities": []}, {"text": "In addition to comparing the various decision list algorithms, we also tried several other algorithms.", "labels": [], "entities": []}, {"text": "First, since probabilistic decision lists are probabilistic analogs of TBLs, we compared to TBL.", "labels": [], "entities": []}, {"text": "Furthermore, after doing our research on decision lists, we had several successes using simple linear models, such as a perceptron model and a maximum entropy (maxent) model).", "labels": [], "entities": []}, {"text": "For the perceptron algorithm, we used a variation that includes a margin requirement, \u03c4 (Zaragoza for 100 iterations or until no change for each training instance).", "labels": [], "entities": [{"text": "Zaragoza", "start_pos": 89, "end_pos": 97, "type": "METRIC", "confidence": 0.9104148745536804}]}, {"text": "shows this incredibly simple algorithm.", "labels": [], "entities": []}, {"text": "We use q(x j ) to represent the vector of answers to questions about input x j ; w j is a weight vector; we assume that the output, y j is -1 or +1; and \u03c4 is a margin.", "labels": [], "entities": []}, {"text": "We assume that one of the questions is TRUE, eliminating the need fora separate threshold variable.", "labels": [], "entities": [{"text": "TRUE", "start_pos": 39, "end_pos": 43, "type": "METRIC", "confidence": 0.9980496168136597}]}, {"text": "When \u03c4 = 0, the algorithm reduces to the standard perceptron algorithm.", "labels": [], "entities": []}, {"text": "The inclusion of a non-zero margin and running to convergence guarantees convergence for separable data to a solution that works nearly as well as a linear support vector machine.", "labels": [], "entities": [{"text": "convergence", "start_pos": 50, "end_pos": 61, "type": "METRIC", "confidence": 0.9332627654075623}, {"text": "convergence", "start_pos": 73, "end_pos": 84, "type": "METRIC", "confidence": 0.9697436094284058}]}, {"text": "Given the extreme simplicity of the algorithm and the fact that it works so well (not just compared to the algorithms in this paper, but compared to several others we have tried), the perceptron with margin is our favorite algorithm when we don't need probabilities, and model size is not an issue.", "labels": [], "entities": []}, {"text": "Most of our algorithms have one or more parameters that need to be tuned.", "labels": [], "entities": []}, {"text": "We chose 5 additional confusable word pairs for parameter tuning and chose parameter values that worked well on entropy and error rate across data sizes, as measured on these 5 additional word pairs.", "labels": [], "entities": [{"text": "parameter tuning", "start_pos": 48, "end_pos": 64, "type": "TASK", "confidence": 0.737683892250061}]}, {"text": "For the smoothing discount value we used 0.7.", "labels": [], "entities": []}, {"text": "For thresholds for both the sorted and the incremental learner, we used 3 bits.", "labels": [], "entities": []}, {"text": "For the perceptron algorithm, we set \u03c4 to 20.", "labels": [], "entities": []}, {"text": "For TBL's minimum number of errors to fix, the traditional value of  Since sometimes one learning algorithm is better atone size, and worse at another, we tried three training sizes: 1, 10 and 50 million words.", "labels": [], "entities": []}, {"text": "In, we show the error rates of each algorithm at different training sizes, averaged across the 10 words in the test set.", "labels": [], "entities": []}, {"text": "We computed the geometric mean of error rate, across the ten word pairs.", "labels": [], "entities": [{"text": "geometric mean of error rate", "start_pos": 16, "end_pos": 44, "type": "METRIC", "confidence": 0.6998392939567566}]}, {"text": "We chose the geometric mean, because otherwise, words with the largest error rates would disproportionately dominate the results., shows the geometric mean of the model sizes, where the model size is the number of rules.", "labels": [], "entities": []}, {"text": "For maxent and perceptron models, we counted size as the total number of features, since these models store a value for every feature.", "labels": [], "entities": []}, {"text": "For Sorted: \u2212\u221e and Sorted: 0, the size is similar to a maxent or perceptron model -almost every rule is used.", "labels": [], "entities": []}, {"text": "Sorted: 3 drastically reduces the model sizeby a factor of roughly 20 -while improving performance.", "labels": [], "entities": []}, {"text": "Incremental: 3 is smaller still, by about an additional factor of 2 to 5, although its accuracy is slightly worse than Sorted: 3.", "labels": [], "entities": [{"text": "Incremental", "start_pos": 0, "end_pos": 11, "type": "METRIC", "confidence": 0.8650138974189758}, {"text": "accuracy", "start_pos": 87, "end_pos": 95, "type": "METRIC", "confidence": 0.9996305704116821}]}, {"text": "shows the entropy of each algorithm.", "labels": [], "entities": []}, {"text": "Since entropy is logarthmic, we use the arithmetic mean.", "labels": [], "entities": [{"text": "arithmetic mean", "start_pos": 40, "end_pos": 55, "type": "METRIC", "confidence": 0.9420356750488281}]}, {"text": "Notice that the traditional probabilistic decision list learning algorithm -equivalent to Sorted: \u2212\u221e -always has a higher error rate, higher entropy, and larger size than Sorted: 0.", "labels": [], "entities": [{"text": "decision list learning", "start_pos": 42, "end_pos": 64, "type": "TASK", "confidence": 0.6962458292643229}]}, {"text": "Similarly, Sorted: 3 has lower entropy, higher accuracy, and smaller models than Sorted: 0.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.9987303614616394}]}, {"text": "Finally, Incremental: 3 has slightly higher error rates, but slightly lower entropies, and 1/2 to 1/5 as many rules.", "labels": [], "entities": [{"text": "error rates", "start_pos": 44, "end_pos": 55, "type": "METRIC", "confidence": 0.967320054769516}, {"text": "entropies", "start_pos": 76, "end_pos": 85, "type": "METRIC", "confidence": 0.9604613780975342}]}, {"text": "If one wants a probabilistic decision list learner, this is clearly the algorithm to use.", "labels": [], "entities": []}, {"text": "However, if probabilities are not needed, then TBL can produce lower error rates, with still fewer rules.", "labels": [], "entities": [{"text": "TBL", "start_pos": 47, "end_pos": 50, "type": "METRIC", "confidence": 0.6671394109725952}, {"text": "error rates", "start_pos": 69, "end_pos": 80, "type": "METRIC", "confidence": 0.9543243944644928}]}, {"text": "On the other hand, if one wants either the lowest entropies or highest accuracies, then it appears that linear models, such as maxent or the perceptron algorithm with margin work even better, at the expense of producing much larger models.", "labels": [], "entities": []}, {"text": "Clearly, the new algorithm works very well when small size and probabilities are needed.", "labels": [], "entities": []}, {"text": "It would be interesting to try combining this algorithm with decision trees in someway.", "labels": [], "entities": []}, {"text": "Both and were able to get improvements on the simple decision list structure by adding additional splits -Yarowsky by adding them at the root, and Florian et al. by adding them at the leaves.", "labels": [], "entities": []}, {"text": "Notice however that the chief advantage of decision lists over linear models is their compact size and understandability, and our techniques simultaneously improve those aspects; adding additional splits will almost certainly lead to larger models, not smaller.", "labels": [], "entities": []}, {"text": "It would also be interesting to try more sophisticated smoothing techniques, such as those of Yarowsky.", "labels": [], "entities": []}, {"text": "We have shown that a simple, incremental algorithm for learning probabilistic decision lists can produce models that are significantly more accurate, have significantly lower entropy, and are significantly smaller than those produced by the standard sorted learning algorithm.", "labels": [], "entities": []}, {"text": "The new algorithm comes at the cost of some increased time, space, and complexity, but variations on it, such as the sorted algorithm with thresholding, or the techniques of Section 2.2.1, can be used to trade off space, time, and list size.", "labels": [], "entities": []}, {"text": "Overall, given the substantial improvements from this algorithm, it should be widely used whenever the advantages -compactness and understandability -of probabilistic decision lists are needed.", "labels": [], "entities": []}], "tableCaptions": []}