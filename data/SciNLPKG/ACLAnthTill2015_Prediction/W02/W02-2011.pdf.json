{"title": [{"text": "Combining labelled and unlabelled data: a case study on Fisher kernels and transductive inference for biological entity recognition", "labels": [], "entities": [{"text": "biological entity recognition", "start_pos": 102, "end_pos": 131, "type": "TASK", "confidence": 0.667818009853363}]}], "abstractContent": [{"text": "We address the problem of using partially labelled data, eg large collections were only little data is annotated, for extracting biological entities.", "labels": [], "entities": []}, {"text": "Our approach relies on a combination of probabilistic models, which we use to model the generation of entities and their context, and kernel machines, which implement p o werful cate-gorisers based on a similarity measure and some labelled data.", "labels": [], "entities": []}, {"text": "This combination takes the form of the so-called Fisher kernels which implement a similarity based on an underlying probabilistic model.", "labels": [], "entities": []}, {"text": "Such k ernels are compared with trans-ductive inference, an alternative approach to combining labelled and unlabelled data, again coupled with Support Vector Machines.", "labels": [], "entities": []}, {"text": "Experiments are performed on a database of abstracts extracted from Medline.", "labels": [], "entities": [{"text": "Medline", "start_pos": 68, "end_pos": 75, "type": "DATASET", "confidence": 0.9816926717758179}]}], "introductionContent": [{"text": "The availability of electronic databases of rapidly increasing sizes has encouraged the development of me t ho d st ha t ca n ta pi n to these databases to automatically generate knowledge, for example by retrieving relevant information or extracting entities and their relationships.", "labels": [], "entities": []}, {"text": "Machine learning seems especially relevant in this context, because it helps performing these tasks with a minimum of user interaction.", "labels": [], "entities": [{"text": "Machine learning", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.7877680957317352}]}, {"text": "A number of problems like en tity extraction or ltering can be mapped to supervised techniques like categorisation.", "labels": [], "entities": [{"text": "en tity extraction", "start_pos": 26, "end_pos": 44, "type": "TASK", "confidence": 0.7554285526275635}]}, {"text": "In addition, modern supervised classiication methods like Support Vector Machines have pr oven to be eecient an d versatile.", "labels": [], "entities": []}, {"text": "They do, however, rely on the availability of labelled data, where labels indicate eg whether a document is relevant or whether a candidate expression is an interesting entity.", "labels": [], "entities": []}, {"text": "This causes two important problems that motivate our work: 1) annotating data is often a diicult and costly task involving a lot of human work 1 , such that large collections of labelled data are diicult to obtain, and 2) interannotator agreement tends to below in e g genomics collections (), thus calling for methods that are able to deal with noise and incomplete data.", "labels": [], "entities": []}, {"text": "On the other hand, unsupervised techniques do not require labelled data and can thus be applied regardless of the annotation problems.", "labels": [], "entities": []}, {"text": "Unsupervised learning, however, tend to be less data-eecient than its supervised counterpart, requiring many more examples to discover signiicant features in the data, and is incapable of solving the same kinds of problems.", "labels": [], "entities": []}, {"text": "For example, an eecient clustering technique may be able to distribute documents in a number of well-deened clusters.", "labels": [], "entities": []}, {"text": "However, it will be unable to decide which clusters are relevant without a minimum of supervision.", "labels": [], "entities": []}, {"text": "This motivates our study of techniques that rely on a combination of supervised and unsupervised learning, in order to leverage the availability of large collections of unlabelled data and use a limited amount of labelled documents.", "labels": [], "entities": []}, {"text": "The focus of this study is on a particular application to the genomics literature.", "labels": [], "entities": []}, {"text": "In genomics, avast amount of kn o wledge still resides in large collections of scientiic papers such a s Medline, and several approaches have been proposed to extract, (semi-)automatically, information from such papers.", "labels": [], "entities": [{"text": "Medline", "start_pos": 105, "end_pos": 112, "type": "DATASET", "confidence": 0.9058833718299866}]}, {"text": "These approaches range from purely statistical ones to symbolic ones relying on linguistic and knowledge processing tools (, for example).", "labels": [], "entities": []}, {"text": "Furthermore, due to the nature of the problem at hand, meth-1 If automatic annotation was available, we would basically have solved our Machine Learning problem ods derived from machine learning are called for,), whether supervised, unsupervised or relying on a combination of both.", "labels": [], "entities": []}, {"text": "Let us insist on the fact that our work is primarily concerned with combining labelled and unlabelled data, and entity extraction is used as an application in this context.", "labels": [], "entities": [{"text": "entity extraction", "start_pos": 112, "end_pos": 129, "type": "TASK", "confidence": 0.7895186841487885}]}, {"text": "As a consequence, it is not our purpose at this point to compare our experimental results to those obtained by speciic machine learning techniques applied to entity extraction.", "labels": [], "entities": [{"text": "entity extraction", "start_pos": 158, "end_pos": 175, "type": "TASK", "confidence": 0.7793225049972534}]}, {"text": "Although we certainly hope that our work can be useful for entity extraction, we rather think of it as a methodological study which can hopefully be applied to diierent applications where unlabelled data maybe used to improve the results of supervised learning algorithms.", "labels": [], "entities": [{"text": "entity extraction", "start_pos": 59, "end_pos": 76, "type": "TASK", "confidence": 0.8437526524066925}]}, {"text": "In addition, performing a fair comparison of our work on standard information extraction benchmarks is not straightforward: either we would need to obtain a large amount of unlabelled data that is comparable to the benchmark, or we would need to \\un-label\" a portion of the data.", "labels": [], "entities": []}, {"text": "In both cases, comparing to existing results is diicult as the amount of information used is diierent.", "labels": [], "entities": []}], "datasetContent": [{"text": "For our experiments, we used 184 abstracts from the Medline site.", "labels": [], "entities": [{"text": "Medline site", "start_pos": 52, "end_pos": 64, "type": "DATASET", "confidence": 0.9820133447647095}]}, {"text": "In these articles, genes, proteins and RNAs were manually annotated by a biologist as part of the BioMIRE project.", "labels": [], "entities": []}, {"text": "These articles contain 1405 occurrences of gene names, 792 of protein names and 81 of RNA names.", "labels": [], "entities": []}, {"text": "All these entities are considered relevant biological entities.", "labels": [], "entities": []}, {"text": "We focus hereon the task of identifying names corresponding to such en tities in running texts, without diierentiating genes from proteins or RNAs.", "labels": [], "entities": []}, {"text": "Once candidates for biological entity names have been identiied, this task amounts to a binary categorisation, relevant candidates corresponding to biological entity names.", "labels": [], "entities": []}, {"text": "We divided these abstracts in a training and development set (122 abstracts), and a test set (62 abstracts).", "labels": [], "entities": []}, {"text": "We then retained diierent portions of the training labels, to be used as labelled data, whereas the rest of the data is considered unlabelled.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Features of hairless in \\...of Drosophila  suppressor of hairless and of its human...\".", "labels": [], "entities": []}, {"text": " Table 4: F 1 scores(in %) using diierent propor- tions of annotated data for the following models:  SVM with inductive inference (SVM) and lin- ear (lin) kernel, second degree polynomial ker- nel (d=2), and RBF kernel (rbf)) SVM with  transductive inference (TSVM) and linear (lin)  kernel or second degree polynomial (d=2) ker- nel.", "labels": [], "entities": [{"text": "F 1", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9231126010417938}]}, {"text": " Table 5: F 1 scores(in %) using diierent propor- tions of annotated data for the following mod- els: standard SVM with linear (lin) and second  degree polynomial kernel (d=2)) Combination  of linear kernel and Fisher kernel obtained from  a PLSA with 4 classes (lin+FK4) or 8 classes  (lin+FK8), and combination of linear and all  Fisher kernels obtained from PLSA using 4, 8,  12 and 16 classes (lin+combi).", "labels": [], "entities": [{"text": "F 1 scores", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9616230328877767}]}]}