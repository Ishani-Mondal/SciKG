{"title": [{"text": "Combining Machine Learning and rule-based approaches in Spanish and Japanese sentence realization", "labels": [], "entities": [{"text": "sentence realization", "start_pos": 77, "end_pos": 97, "type": "TASK", "confidence": 0.7135422229766846}]}], "abstractContent": [{"text": "In this paper we describe two parallel experiments on the integration of machine learning (ML) methods into the Spanish and Japanese rule-based sentence realization modules developed at Microsoft Research.", "labels": [], "entities": [{"text": "Spanish and Japanese rule-based sentence realization", "start_pos": 112, "end_pos": 164, "type": "TASK", "confidence": 0.6312807500362396}]}, {"text": "The paper explores the use of decision trees (DT) for the lexical selection of the copula in Spanish and the insertion of a locative postposition in Japanese.", "labels": [], "entities": []}, {"text": "We show that it is possible to machine-learn the contexts for these two non-trivial linguistic phenomena with high accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 115, "end_pos": 123, "type": "METRIC", "confidence": 0.994304358959198}]}], "introductionContent": [{"text": "The two experiments described in this paper were carried out in the framework of the Spanish and Japanese sentence generation modules that are part of MSR-MT, the multilingual Machine Translation system developed at Microsoft Research.", "labels": [], "entities": [{"text": "Spanish and Japanese sentence generation", "start_pos": 85, "end_pos": 125, "type": "TASK", "confidence": 0.6651087760925293}, {"text": "multilingual Machine Translation", "start_pos": 163, "end_pos": 195, "type": "TASK", "confidence": 0.6618597110112509}]}, {"text": "MSR-MT is a hybrid system that uses hand-written, rulebased linguistic components for analysis and generation, and example-based, statistical components for transfer ().", "labels": [], "entities": [{"text": "MSR-MT", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.813497006893158}]}, {"text": "The output of the analysis, as well as the input to generation is an annotated predicate-argument structure or logical form (LF).", "labels": [], "entities": []}, {"text": "Transfer takes place between source LF and target LF using an automatically generated knowledge base known as MindNet, built by aligning logical forms of bilingual text.", "labels": [], "entities": []}, {"text": "As described in (), the rule-based generation module generates the surface string in the target language from the transferred LF.", "labels": [], "entities": []}, {"text": "Here we explore the integration of a machine learning technique into two generation components in order to deal with two different sentence realization problems: the selection of the copula in Spanish and the insertion of a locative postposition in Japanese.", "labels": [], "entities": [{"text": "sentence realization", "start_pos": 131, "end_pos": 151, "type": "TASK", "confidence": 0.753963828086853}]}, {"text": "As shown by ( among others, many linguistic operations can be viewed as classification tasks, thus lending themselves to statistical methods such as decision tree classifiers.", "labels": [], "entities": []}, {"text": "Following the questions raised by) on the impact that the type of corpus has on the quality of the stochastic generation components, we wanted to perform our experiments using two very different types of texts.", "labels": [], "entities": []}, {"text": "For this purpose we built two different models for each experiment: one using text coming from the Encarta encyclopedia and another using text from technical and computer manuals.", "labels": [], "entities": [{"text": "Encarta encyclopedia", "start_pos": 99, "end_pos": 119, "type": "DATASET", "confidence": 0.9688375294208527}]}, {"text": "Our goals can be summarized as follows: \u2022 To integrate a ML approach fora welldefined linguistic operation into an otherwise totally hand-coded rule-based generation module; \u2022 To evaluate the usefulness of such an approach vs. hand-coded rules; \u2022 To evaluate the impact of the type of the training data on the accuracy of the model.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 310, "end_pos": 318, "type": "METRIC", "confidence": 0.9985138773918152}]}, {"text": "To build the statistical models, we used the WinMine toolkit () which has been used to build a machine-learned generation module ).", "labels": [], "entities": []}, {"text": "As training data, we used logical forms produced by analyzing text in the languages of interest, Spanish and Japanese, respectively.", "labels": [], "entities": []}, {"text": "The data was automatically split 70/30 for training and parameter tuning by the WinMine toolkit, which then built different decision trees with different degrees of granularity, by manipulating the prior probability of tree structures to favor simpler ones.", "labels": [], "entities": [{"text": "parameter tuning", "start_pos": 56, "end_pos": 72, "type": "TASK", "confidence": 0.7159047722816467}]}, {"text": "The best model was chosen and then evaluated using a different blind set of sentences.", "labels": [], "entities": []}, {"text": "We also performed an evaluation across text types.", "labels": [], "entities": []}], "datasetContent": [{"text": "Even though our main interest is to use the result of this experiment in an application environment such as MT, we used Spanish texts for evaluation purposes.", "labels": [], "entities": [{"text": "MT", "start_pos": 108, "end_pos": 110, "type": "TASK", "confidence": 0.8835688829421997}]}, {"text": "It may seem that evaluating the results using Spanish data constitutes an artificial environment: after all, we are generating Spanish sentences from structures resulting from the analysis of the same Spanish sentences.", "labels": [], "entities": []}, {"text": "Nonetheless, this enables us to perform an automatic evaluation of the results.", "labels": [], "entities": []}, {"text": "The procedure is the following: we analyze and regenerate the Spanish sentences (with the right copula in them) and we create a master file with the results; we then run regression testing against this file by removing the copula and recalculating it using the model.", "labels": [], "entities": []}, {"text": "The number of changes equals the number of regressions . We used two blind testing sets of 10K sentences each, one for each type of text (Encarta and technical manuals).", "labels": [], "entities": []}, {"text": "Since we were interested in evaluating the usefulness of the ML approach with respect to encoding the information in the form of a rule, we also measured the accuracy of a not-toocomplex-but-not-too-dumb hand-coded rule that uses some of the linguistic insights revealed by the inspection of the models.", "labels": [], "entities": [{"text": "ML", "start_pos": 61, "end_pos": 63, "type": "TASK", "confidence": 0.9504768252372742}, {"text": "accuracy", "start_pos": 158, "end_pos": 166, "type": "METRIC", "confidence": 0.9993378520011902}]}, {"text": "From these results we observe that there is an expected correlation between the type of text and the type of model: Model A is the best model for the Encarta text and Model B is the best model for the technical text.", "labels": [], "entities": [{"text": "Encarta text", "start_pos": 150, "end_pos": 162, "type": "DATASET", "confidence": 0.9632143378257751}]}, {"text": "Interestingly, the model trained on technical data increases its accuracy when tested on text from Encarta.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.9994155168533325}, {"text": "Encarta", "start_pos": 99, "end_pos": 106, "type": "DATASET", "confidence": 0.8964484930038452}]}, {"text": "This is consistent with the fact that all three methods have better results on text from Encarta.", "labels": [], "entities": []}, {"text": "The reasons are not clear but one possible explanation is that, as seen with the values for the baseline above (82% vs. 68%), in this type of text the copula insertion is \"easier\" to predict.", "labels": [], "entities": []}, {"text": "The hand-coded rule does a poorer job overall.", "labels": [], "entities": []}, {"text": "Error analysis shows that the rule is slightly more biased towards estar than the model.", "labels": [], "entities": []}, {"text": "The formulation of the contextual constraints is necessarily simpler in the rule than in the models (which each have over a hundred branching conditions).", "labels": [], "entities": []}, {"text": "Both the models and the rule perform poorly on <copula+AJP> constructions (cases 4 and 5 above) defaulting to ser most of the time.", "labels": [], "entities": []}, {"text": "Parallel to the Spanish experiment, we used two types of blind test data; one from Encarta (1K sentences) and the other from technical documents (1K sentences).", "labels": [], "entities": [{"text": "Encarta", "start_pos": 83, "end_pos": 90, "type": "DATASET", "confidence": 0.6084395051002502}]}, {"text": "Using the DT model, we regenerated the test data and compared the regenerated strings with the original sentences to find out how many sentences were the same as the original sentences with respect to the assignment of de/ni for location nouns.", "labels": [], "entities": []}, {"text": "We did the same using the hand-coded rule.", "labels": [], "entities": []}, {"text": "Our hand-coded rule for the choice between de and ni used the subcategorization features of the predicates available in our Japanese dictionary.", "labels": [], "entities": []}, {"text": "Basically, the hand-code rule assigned ni to a location noun if the parent verb belongs to one of the following types of verbs: (i) directional motion verbs (e.g., iku 'to go'); (ii) verbs that require a locative argument (e.g., oku 'to put'); and (iii) existential verbs (e.g., aru or iru 'to be/to exist').", "labels": [], "entities": []}, {"text": "For other types of verbs, the rule assigned de to a location noun.", "labels": [], "entities": []}, {"text": "In the Japanese experiment, the two models performed slightly better than the hand-coded rule with respect to the same domain test set.", "labels": [], "entities": []}, {"text": "However, with respect to the different domain test set, they performed worse than the hand-coded rule.", "labels": [], "entities": []}, {"text": "This means that, unlike the Spanish experiment described in Section 2, both Japanese models are sensitive to the domain of the test sets: Model A (Encarta Model) achieves 70.90% accuracy for the Encarta test data but its accuracy sharply drops for the technical data (56.00%).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 178, "end_pos": 186, "type": "METRIC", "confidence": 0.9990037083625793}, {"text": "Encarta test data", "start_pos": 195, "end_pos": 212, "type": "DATASET", "confidence": 0.9420391519864401}, {"text": "accuracy", "start_pos": 221, "end_pos": 229, "type": "METRIC", "confidence": 0.9993925094604492}]}, {"text": "Model B (Technical Model) achieves 78.50% for the technical test data but its accuracy drops sharply again for the Encarta data (57.90%).", "labels": [], "entities": [{"text": "technical test data", "start_pos": 50, "end_pos": 69, "type": "DATASET", "confidence": 0.6208255688349406}, {"text": "accuracy", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.9995997548103333}, {"text": "Encarta data", "start_pos": 115, "end_pos": 127, "type": "DATASET", "confidence": 0.9709481596946716}]}, {"text": "That the two models are sensitive to the domain of the test sets makes sense: the types of predicates used in Encarta data are quite different from those used in technical documents.", "labels": [], "entities": [{"text": "Encarta data", "start_pos": 110, "end_pos": 122, "type": "DATASET", "confidence": 0.9126298129558563}]}, {"text": "Thus, it is reasonable to assume that the set of distinguishing features selected by Model A may notwork for technical documents and vice versa.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Size, baseline and overall accuracy for the  two models", "labels": [], "entities": [{"text": "Size", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9516921639442444}, {"text": "baseline", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.9909720420837402}, {"text": "accuracy", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.9982622265815735}]}, {"text": " Table 3: Precision and recall for ser/estar", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9958680868148804}, {"text": "recall", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.9994226694107056}]}, {"text": " Table 5: Comparison of models according to text  type and use of lemma of the adjective", "labels": [], "entities": []}, {"text": " Table 7: Comparison of transfer vs. DT results on  the task of copula selection", "labels": [], "entities": [{"text": "copula selection", "start_pos": 64, "end_pos": 80, "type": "TASK", "confidence": 0.6819751113653183}]}, {"text": " Table 8: Size, baseline and accuracy of the models", "labels": [], "entities": [{"text": "baseline", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.9678993821144104}, {"text": "accuracy", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.9996542930603027}]}, {"text": " Table 9: Precision and recall for de/ni in the two  models", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9987862706184387}, {"text": "recall", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.9996055960655212}]}]}