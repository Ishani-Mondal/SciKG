{"title": [{"text": "English-Japanese Example-Based Machine Translation Using Abstract Linguistic Representations", "labels": [], "entities": [{"text": "Example-Based Machine Translation", "start_pos": 17, "end_pos": 50, "type": "TASK", "confidence": 0.7686797380447388}]}], "abstractContent": [{"text": "This presentation describes an example-based English-Japanese machine translation system in which an abstract linguistic representation layer is used to extract and store bilingual translation knowledge, transfer patterns between languages, and generate output strings.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 62, "end_pos": 81, "type": "TASK", "confidence": 0.7196004986763}]}, {"text": "Abstraction permits structural neutral-izations that facilitate learning of translation examples across languages with radically different surface structure characteristics , and allows MT development to proceed within a largely language-independent NLP architecture.", "labels": [], "entities": [{"text": "MT development", "start_pos": 186, "end_pos": 200, "type": "TASK", "confidence": 0.9263398051261902}]}, {"text": "Comparative evaluation indicates that after training in a domain the English-Japanese system is statistically indistinguishable from a non-customized commercially available MT system in the same domain.", "labels": [], "entities": []}], "introductionContent": [{"text": "In the wake of the pioneering work of, and, Machine Translation (MT) research has increasingly focused on the issue of how to acquire translation knowledge from aligned parallel texts.", "labels": [], "entities": [{"text": "Machine Translation (MT)", "start_pos": 44, "end_pos": 68, "type": "TASK", "confidence": 0.8603091955184936}, {"text": "translation knowledge from aligned parallel texts", "start_pos": 134, "end_pos": 183, "type": "TASK", "confidence": 0.8314179480075836}]}, {"text": "While much of this research effort has focused on acquisition of correspondences between individual lexical items or between unstructured strings of words, closer attention has begun to be paid to the learning of structured phrasal units:, for example, describe a method for automatically extracting correspondences between dependency relations in Japanese and English.", "labels": [], "entities": []}, {"text": "Similarly,) seeks to match corresponding Japanese and English phrases containing information about hierarchical structures, including partially completed parses.", "labels": [], "entities": []}, {"text": "explicitly assume that dependency relations between words will generally be preserved across languages.", "labels": [], "entities": []}, {"text": "However, when languages are as different as Japanese and English with respect to their syntactic and informational structures, grammatical or dependency relations may not always be preserved: the English sentence \"the network failed\" has quite a different grammatical structure from its Japanese translation equivalent \u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306b\u969c\u5bb3\u304c \u767a\u751f\u3057\u305f 'a defect arose in the network.'", "labels": [], "entities": []}, {"text": "One issue for example-based MT, then, is to capture systematic divergences through generic learning applicable to multiple language pairs.", "labels": [], "entities": [{"text": "MT", "start_pos": 28, "end_pos": 30, "type": "TASK", "confidence": 0.9864283204078674}]}, {"text": "In this presentation we describe the MSR-MT English-Japanese system, an example-based MT system that learns structured phrase-sized translation units.", "labels": [], "entities": []}, {"text": "Unlike the systems discussed in, MSR-MT places the locus of translation knowledge acquisition at a greater level of abstraction than surface relations, pushing it into a semanticallymotivated layer called LOGICAL FORM (LF).", "labels": [], "entities": [{"text": "translation knowledge acquisition", "start_pos": 60, "end_pos": 93, "type": "TASK", "confidence": 0.894977847735087}, {"text": "LOGICAL FORM (LF)", "start_pos": 205, "end_pos": 222, "type": "METRIC", "confidence": 0.8682882785797119}]}, {"text": "Abstraction has the effect of neutralizing (or at least minimizing) differences in word order and syntactic structure, so that mappings between structural relations associated with lexical items can readily be acquired within a general MT architecture.", "labels": [], "entities": [{"text": "MT architecture", "start_pos": 236, "end_pos": 251, "type": "TASK", "confidence": 0.9102218449115753}]}, {"text": "In Section 1 below, we present an overview of the characteristics of the system, with special reference to English-Japanese MT.", "labels": [], "entities": [{"text": "MT", "start_pos": 124, "end_pos": 126, "type": "TASK", "confidence": 0.6721866726875305}]}, {"text": "Section 2 discusses a class of structures learned through phrase alignment, Section 3 presents the results of comparative evaluation, and Section 4 some factors that contributed to the evaluation results.", "labels": [], "entities": [{"text": "phrase alignment", "start_pos": 58, "end_pos": 74, "type": "TASK", "confidence": 0.7721202075481415}]}, {"text": "Section 5 addresses directions for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "In May 2002, we compared output of the MSR-MT English-Japanese system with a commercially available desktop MT system.", "labels": [], "entities": [{"text": "MSR-MT English-Japanese system", "start_pos": 39, "end_pos": 69, "type": "DATASET", "confidence": 0.8690908749898275}]}, {"text": "3 Toshiba's The Honyaku Office v2.0 desktop MT system was selected for this purpose.", "labels": [], "entities": [{"text": "Honyaku Office v2.0 desktop MT", "start_pos": 16, "end_pos": 46, "type": "DATASET", "confidence": 0.860770332813263}]}, {"text": "The Honyaku is a trademark of the Toshiba Corporation.", "labels": [], "entities": [{"text": "Honyaku", "start_pos": 4, "end_pos": 11, "type": "DATASET", "confidence": 0.7236068844795227}, {"text": "Toshiba Corporation", "start_pos": 34, "end_pos": 53, "type": "DATASET", "confidence": 0.9493054747581482}]}, {"text": "Another desktop system was also considered for evaluation; however, comparative evaluation with that system indicated that the Toshiba system performed marginally, though not significantly, better on our technical documentation.", "labels": [], "entities": [{"text": "Toshiba system", "start_pos": 127, "end_pos": 141, "type": "DATASET", "confidence": 0.9107212722301483}]}], "tableCaptions": [{"text": " Table 2. Number of Transfers and Nodes Transferred per Sentence", "labels": [], "entities": [{"text": "Number", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9510469436645508}, {"text": "Transfers and Nodes Transferred", "start_pos": 20, "end_pos": 51, "type": "TASK", "confidence": 0.5229918733239174}]}, {"text": " Table 3. Sources of Different Word Classes at Transfer", "labels": [], "entities": []}]}