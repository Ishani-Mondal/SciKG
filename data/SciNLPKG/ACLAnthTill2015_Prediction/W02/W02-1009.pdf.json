{"title": [], "abstractContent": [{"text": "This paper proposes a novel class of PCFG parameterizations that support linguistically reasonable priors over PCFGs.", "labels": [], "entities": []}, {"text": "To estimate the parameters is to discover a notion of relatedness among context-free rules such that related rules tend to have related probabilities.", "labels": [], "entities": []}, {"text": "The prior favors grammars in which the relationships are simple to describe and have few major exceptions.", "labels": [], "entities": []}, {"text": "A basic version that bases relatedness on weighted edit distance yields superior smoothing of grammars learned from the Penn Treebank (20% reduction of rule perplexity over the best previous method).", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 120, "end_pos": 133, "type": "DATASET", "confidence": 0.9946920573711395}]}, {"text": "1 A Sketch of the Concrete Problem This paper uses anew kind of statistical model to smooth the probabilities of PCFG rules.", "labels": [], "entities": []}, {"text": "It focuses on \"flat\" or \"dependency-style\" rules.", "labels": [], "entities": []}, {"text": "These resemble subcategoriza-tion frames, but include adjuncts as well as arguments.", "labels": [], "entities": []}, {"text": ".. ] These other rules arise if put can add, drop, reorder, or retype its dependents.", "labels": [], "entities": []}, {"text": "These edit operations on rules are semantically motivated and quite common (Table 1).", "labels": [], "entities": []}, {"text": "We wish to learn contextual probabilities for the edit operations, based on an observed sample of flat rules.", "labels": [], "entities": []}, {"text": "In English we should discover, for example, that it is quite common to add or delete PP at the right edge of a rule.", "labels": [], "entities": []}, {"text": "These contextual edit probabilities will help us guess the true probabilities of novel or little-observed rules.", "labels": [], "entities": []}, {"text": "However, rules are often idiosyncratic.", "labels": [], "entities": []}, {"text": "Our smoothing method should not keep us from noticing (given enough evidence) that put takes a PP more often than most verbs.", "labels": [], "entities": []}, {"text": "Hence this paper's proposal is a Bayesian smoothing method that allows idiosyncrasy in the grammar while presuming regularity to be more likely a priori.", "labels": [], "entities": []}, {"text": "The model will assign a positive probability to each of the infinitely many formally possible rules.", "labels": [], "entities": []}, {"text": "The following bizarre rule is not observed in training, and seems very unlikely.", "labels": [], "entities": [{"text": "bizarre rule", "start_pos": 14, "end_pos": 26, "type": "METRIC", "confidence": 0.9398843944072723}]}, {"text": "But there is no formal reason to rule it out, and it might help us parse an unlikely test sentence.", "labels": [], "entities": [{"text": "parse", "start_pos": 67, "end_pos": 72, "type": "TASK", "confidence": 0.9655753970146179}]}, {"text": "So the model will allow it some tiny probability: \u2022 S \u2192 NP Adv PP put PP PP PP NP AdjP S 2 Background and Other Approaches A PCFG is a conditional probability function p(RHS | LHS).", "labels": [], "entities": []}, {"text": "1 For example, p(V NP PP | VP) gives the probability of the rule VP \u2192 V NP PP.", "labels": [], "entities": []}, {"text": "With lexicalized non-terminals, it has form p(V put NP pizza PP in | VP put).", "labels": [], "entities": []}, {"text": "Usually one makes an independence assumption and defines this as p(V put NP PP | VP put) times factors that choose dependent headwords pizza and in according to the selectional preferences of put.", "labels": [], "entities": []}, {"text": "This paper is about estimating the first factor, p(V put NP PP | VP put).", "labels": [], "entities": []}, {"text": "In supervised learning, it is simplest to use a maximum likelihood estimate (perhaps with backoff from put).", "labels": [], "entities": [{"text": "maximum likelihood estimate", "start_pos": 48, "end_pos": 75, "type": "METRIC", "confidence": 0.7052866121133169}]}, {"text": "Charniak (1997) calls this a \"Treebank grammar\" and gambles that assigning 0 probability to rules unseen in training data will not hurt parsing accuracy too much.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 144, "end_pos": 152, "type": "METRIC", "confidence": 0.8945173025131226}]}, {"text": "However, there are four reasons not to use a Treebank grammar.", "labels": [], "entities": [{"text": "Treebank grammar", "start_pos": 45, "end_pos": 61, "type": "DATASET", "confidence": 0.9351732134819031}]}, {"text": "First, ignoring unseen rules necessarily sacrifices some accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.998883068561554}]}, {"text": "Second, we will show that it improves accuracy to flatten the parse trees and use flat, dependency-style rules like p(NP put NP PP | S put); this avoids overly strong independence assumptions, but it increases the number of unseen rules and so makes Treebank grammars less tenable.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.9988683462142944}]}, {"text": "Third, backing off from the word is a crude technique that does not distinguish among words.", "labels": [], "entities": [{"text": "backing off from the word", "start_pos": 7, "end_pos": 32, "type": "TASK", "confidence": 0.8438618421554566}]}, {"text": "2 Fourth, one would eventually like to reduce or eliminate supervision, and then generalization is important to constrain the search to reasonable grammars.", "labels": [], "entities": []}, {"text": "To smooth the distribution p(RHS | LHS), one can define it in terms of a set of parameters and then estimate those parameters.", "labels": [], "entities": []}, {"text": "Most researchers have used an n-gram model (Eisner, 1996; Charniak, 2000) or more general Markov model (Alshawi, 1996) to model the sequence of nonterminals in the RHS.", "labels": [], "entities": [{"text": "RHS", "start_pos": 164, "end_pos": 167, "type": "DATASET", "confidence": 0.7329661250114441}]}, {"text": "The sequence V put NP PP in our example is then assumed to be emitted by some Markov model of VP put rules (again with backoff from put).", "labels": [], "entities": []}, {"text": "Collins (1997, model 2) uses a more sophisticated model in which all arguments in this sequence are generated jointly, as in a Treebank grammar, and then a Markov process is used to insert adjuncts among the arguments.", "labels": [], "entities": []}, {"text": "While Treebank models overfit the training data, Markov models underfit.", "labels": [], "entities": []}, {"text": "A simple compromise (novel to this paper) is a hybrid Treebank/Markov model, which backs off from a Treebank model to a Markov.", "labels": [], "entities": []}, {"text": "Like this paper's main proposal, it can learn well-observed id-iosyncratic rules but generalizes when data are sparse.", "labels": [], "entities": []}, {"text": "3 1 Nonstandardly, this allows infinitely many rules with p>0. 2 One might do better by backing off to word clusters, which Charniak (1997) did find provided a small benefit.", "labels": [], "entities": []}, {"text": "3 Carroll and Rooth (1998) used a similar hybrid technique Association for Computational Linguistics.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "To evaluate the quality of generalization, we used preparsed training data D and testing data E.", "labels": [], "entities": [{"text": "generalization", "start_pos": 27, "end_pos": 41, "type": "TASK", "confidence": 0.9774407744407654}]}, {"text": "Each dataset consisted of a collection of flat rules such as S put \u2192 NP put NP PP extracted from the Penn Treebank ().", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 101, "end_pos": 114, "type": "DATASET", "confidence": 0.996477335691452}]}, {"text": "Thus, p(D | \u03b8, \u03c0) and p(E | \u03b8, \u03c0) were each defined as a product of rule probabilities of the form p \u03b8,\u03c0 (NP put NP PP | S put ).", "labels": [], "entities": []}, {"text": "The learner attempted to maximize p(\u03b8, \u03c0) \u00b7 p(D | \u03b8, \u03c0) by gradient ascent.", "labels": [], "entities": []}, {"text": "This amounts to learning the generalizations and exceptions that related the training rules D.", "labels": [], "entities": []}, {"text": "The evaluation measure was then the perplexity on test data, \u2212 log 2 p(E | \u03b8, \u03c0)/|E| . To get a good (low) perplexity score, the model had to assign reasonable probabilities to the many novel rules in E).", "labels": [], "entities": []}, {"text": "For many of these rules, even the frame was novel.", "labels": [], "entities": [{"text": "frame", "start_pos": 34, "end_pos": 39, "type": "METRIC", "confidence": 0.9879353046417236}]}, {"text": "Note that although the training data was preparsed into rules, it was not annotated with the paths in that generated those rules, so estimating \u03b8 and \u03c0 was still an unsupervised learning problem.", "labels": [], "entities": []}, {"text": "The transformation graph had about 14 features per arc).", "labels": [], "entities": []}, {"text": "In the finite part of the transformation graph that was actually explored (including bad arcs that compete with good ones), about 70000 distinct features were encountered, though after training, only a few hundred  feature weights were substantial, and only a few thousand were even far enough from zero to affect performance.", "labels": [], "entities": []}, {"text": "There was also a parameter \u03c0 e for each observed rule e.", "labels": [], "entities": []}, {"text": "Results are given in, which compares the transformation model to various competing models discussed in section 2.", "labels": [], "entities": []}, {"text": "The best (smallest) perplexities appear in boldface.", "labels": [], "entities": []}, {"text": "The key results: \u2022 The transformation model was the winner, reducing perplexity by 20% over the best model replicated from previous literature (a bigram model).", "labels": [], "entities": [{"text": "perplexity", "start_pos": 69, "end_pos": 79, "type": "METRIC", "confidence": 0.975800096988678}]}, {"text": "\u2022 Much of this improvement could be explained by the transformation model's ability to model exceptions.", "labels": [], "entities": []}, {"text": "Adding this ability more directly to the bigram model, using the new Treebank/Markov approach of section 2, also reduced perplexity from the bigram model, by 6% or 14% depending on whether Katz or one-count backoff was used, versus the transformation model's 20%.", "labels": [], "entities": []}, {"text": "\u2022 Averaging the transformation model with the best competing model (Treebank/bigram) improved it by an additional 6%.", "labels": [], "entities": []}, {"text": "So using transformations yields a total perplexity reduction of 12% over Treebank/bigram, and 24% over the best previous model from the literature (bigram).", "labels": [], "entities": [{"text": "Treebank", "start_pos": 73, "end_pos": 81, "type": "DATASET", "confidence": 0.9582812190055847}]}, {"text": "\u2022 What would be the cost of achieving such a perplexity improvement by additional annotation?", "labels": [], "entities": []}, {"text": "Training the averaged model on only the first half of the training set, with no further tuning of any options, yielded a test set perplexity of 118.0.", "labels": [], "entities": []}, {"text": "So by using transformations, we can achieve about the same perplexity as the best model without transformations, using only half as much training data.", "labels": [], "entities": []}, {"text": "\u2022 Furthermore, comparing shows that the transformation model had the most graceful performance degradation when the dataset was reduced in size.", "labels": [], "entities": []}, {"text": "Figure 2: Probabilities of test set flat rules under the averaged model, plotted against the corresponding probabilities under the best transformation-free model.", "labels": [], "entities": []}, {"text": "Improvements fall above the main diagonal; dashed diagonals indicate a factor of two.", "labels": [], "entities": []}, {"text": "The three log-log plots (at different scales!) partition the rules by the number of training observations: 0 (left graph), 1 (middle), \u2265 2 (right).", "labels": [], "entities": []}, {"text": "This is an encouraging result for the use of the method in less supervised contexts (although results on a noisy dataset would be more convincing in this regard).", "labels": [], "entities": []}, {"text": "\u2022 The competing models from the literature are best used to predict flat rules directly, rather than by summing over their possible non-flat internal structures, as has been done in the past.", "labels": [], "entities": []}, {"text": "This result is significant in itself., it shows the inappropriateness of the traditional independence assumptions that buildup a frame by several rule expansions (section 2).", "labels": [], "entities": []}, {"text": "shows that averaging the transformation model with the Treebank/bigram model improves the latter not merely on balance, but across the board.", "labels": [], "entities": []}, {"text": "In other words, there is no evident class of phenomena for which incorporating transformations would be a bad idea.", "labels": [], "entities": []}, {"text": "\u2022 Transformations particularly helped raise the estimates of the low-probability novel rules in test data, as hoped.", "labels": [], "entities": []}, {"text": "\u2022 Transformations also helped on test rules that had been observed once in training with relatively infrequent words.", "labels": [], "entities": []}, {"text": "(In other words, the transformation model does not discount singletons too much.)", "labels": [], "entities": []}, {"text": "\u2022 Transformations hurt slightly on balance for rules observed more than once in training, but the effect was tiny.", "labels": [], "entities": []}, {"text": "All these differences are slightly exaggerated if one compares the transformation model directly with the Treebank/bigram model, without averaging.", "labels": [], "entities": []}, {"text": "The transformation model was designed to use edit operations in order to generalize appropriately from a word's observed frames to new frames that are likely to appear with that word in test data.", "labels": [], "entities": []}, {"text": "To directly test the model's success at such generalization, we compared it to the bigram model on a pseudo-disambiguation task.", "labels": [], "entities": []}, {"text": "Each instance of the task consisted of a pair of rules from test data, expressed as (word, frame) pairs (w 1 , f 1 ) and (w 2 , f 2 ), such that f 1 and f 2 are \"novel\" frames that did not appear in training data (with any headword).", "labels": [], "entities": []}, {"text": "Each model was then asked: Does f 1 go with w 1 and f 2 with w 2 , or vice-versa?", "labels": [], "entities": []}, {"text": "In other words, which is bigger, p( Since the frames were novel, the model had to make the choice according to whether f 1 or f 2 looked more like the frames that had actually been observed with w 1 in the past, and likewise w 2 . What this means depends on the model.", "labels": [], "entities": []}, {"text": "The bigram model takes two frames to lookalike if they contain many bigrams in common.", "labels": [], "entities": []}, {"text": "The transformation model takes two frames to lookalike if they are connected by a path of probable transformations.", "labels": [], "entities": []}, {"text": "The test data contained 62 distinct rules (w, f ) in which f was a novel frame.", "labels": [], "entities": []}, {"text": "This yielded 62\u00b761 2 = 1891 pairs of rules, leading to 1811 task instances after obvious ties were discarded.", "labels": [], "entities": []}, {"text": "Baseline performance on this difficult task is 50% (random guess).", "labels": [], "entities": []}, {"text": "The bigram model chose correctly in 1595 of the 1811 instances (88.1%).", "labels": [], "entities": []}, {"text": "Parameters for \"memorizing\" specific frames do not help on this task, which involves only novel frames, so the Treebank/bigram model had the same performance.", "labels": [], "entities": []}, {"text": "By contrast, the transformation model got 1669 of 1811 correct (92.2%), fora morethan-34% reduction in error rate.", "labels": [], "entities": [{"text": "error rate", "start_pos": 103, "end_pos": 113, "type": "METRIC", "confidence": 0.9727394282817841}]}, {"text": "(The development set showed similar results.)", "labels": [], "entities": []}, {"text": "However, since the 1811 task instances were derived non-independently from just 62 novel rules, this result is based on a rather small sample.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Each Insert arc has 14 features. The features of any  given arc are found by instantiating the tuples above, as shown.  Each instantiated tuple has a weight specified in \u03b8.", "labels": [], "entities": []}, {"text": " Table 3: Properties of the experimental data. \"Novel\" means  not observed in training. \"Frame\" was defined in section 4.", "labels": [], "entities": [{"text": "Frame", "start_pos": 89, "end_pos": 94, "type": "METRIC", "confidence": 0.9534189701080322}]}, {"text": " Table 4: Perplexity of the test set under various models. (a) Full  training set. (b) Half training set (sections 0-7 only).", "labels": [], "entities": []}]}