{"title": [], "abstractContent": [{"text": "We explore how machine learning can be employed to learn rulesets for the traditional modules of content planning and surface realization.", "labels": [], "entities": [{"text": "content planning and surface realization", "start_pos": 97, "end_pos": 137, "type": "TASK", "confidence": 0.7480805337429046}]}, {"text": "Our approach takes advantage of semantically annotated corpora to induce preferences for content planning and constraints on realizations of these plans.", "labels": [], "entities": []}, {"text": "We applied this methodology to an annotated corpus of indicative summaries to derive constraint rules that can assist in generating summaries for new, unseen material.", "labels": [], "entities": []}], "introductionContent": [{"text": "Traditional natural language generation (NLG) approaches rely heavily on human experts to code discourse, semantic, and lexical resources.", "labels": [], "entities": [{"text": "natural language generation (NLG)", "start_pos": 12, "end_pos": 45, "type": "TASK", "confidence": 0.8293755054473877}]}, {"text": "These resources are used by systems to determine the discourse and sentential structure of the text, and its word choice.", "labels": [], "entities": []}, {"text": "This process can be very time consuming, involving experts that examine target documents and distill proper discourse plans and lexicons that can produce the desired text.", "labels": [], "entities": []}, {"text": "In this paper, we investigate a novel approach which automatically acquires such knowledge using an annotated training corpus.", "labels": [], "entities": []}, {"text": "Our method constructs summarization system components by first learning high-level content planning patterns and then learning low-level constraints on how to realize these content plans in natural language.", "labels": [], "entities": []}, {"text": "By applying this approach to a training corpus consisting of documents belonging to the same domain and genre, the system can generate a model for production of similar texts.", "labels": [], "entities": []}, {"text": "We show how this framework can be applied to automatic text summarization by using a corpus of annotated bibliography entries as the training corpus to produce a model of indicative summaries.", "labels": [], "entities": [{"text": "automatic text summarization", "start_pos": 45, "end_pos": 73, "type": "TASK", "confidence": 0.5877356231212616}]}, {"text": "These entries discuss different books but express the same reoccurring types of information using different surface forms.", "labels": [], "entities": []}, {"text": "While the corpus from which plans and realization patterns are acquired is restricted to input documents of the same genre that exhibit structural regularity, the learned plans can be applied to other domains and genres.", "labels": [], "entities": []}, {"text": "In this paper, we draw on input from the genre of annotated bibliography entries, but will apply the learned plans to generate summaries of web-available consumer health texts.", "labels": [], "entities": []}, {"text": "A content plan consists of predicates specifying what kind of information should occur in what order in a generated summary.", "labels": [], "entities": []}, {"text": "Each predicate will ultimately be realized by one of the lexicalized phrases that are associated with it.", "labels": [], "entities": []}, {"text": "The research we present focuses on learning rules that can predict the order of predicates in a text and acquiring the lexicalized phrases associated with each predicate, and is illustrated in.", "labels": [], "entities": []}, {"text": "The acquisition of the content planning ruleset works by finding occurrence patterns of predicates in manually annotated training corpora.", "labels": [], "entities": []}, {"text": "This module determines what predicates are required or optional in the plan, and uncovers ordering constraints between them.", "labels": [], "entities": []}, {"text": "Our approach in acquiring content planning rules differs from related work in its integration of contextual constraints.", "labels": [], "entities": [{"text": "acquiring content planning rules", "start_pos": 16, "end_pos": 48, "type": "TASK", "confidence": 0.8265000879764557}]}, {"text": "A second acquisition component for partial surface realization considers frequent lexical dependency patterns that are unique to specific predicates (e.g., the Audience predicate in bibliography entries) as predicate realizations and uncovers constraints governing their usage.", "labels": [], "entities": [{"text": "partial surface realization", "start_pos": 35, "end_pos": 62, "type": "TASK", "confidence": 0.6845058997472128}]}, {"text": "These patterns distinguish between constituents that determine the semantics of a predicate (which we calla predicate's attributes) as well as other associated text constituents that are used to convey the information (e.g., surrounding common phrases) in different surface forms.", "labels": [], "entities": []}, {"text": "In this paper, we first describe the role of indicative summaries and show how their generation can be viewed as an instance of our task.", "labels": [], "entities": []}, {"text": "We then explain how our two acquisition algorithms function, drawing examples from indicative summary generation.", "labels": [], "entities": [{"text": "indicative summary generation", "start_pos": 83, "end_pos": 112, "type": "TASK", "confidence": 0.6117509802182516}]}, {"text": "We examine the acquisition process for content planning first, and partial surface realization second.", "labels": [], "entities": [{"text": "content planning", "start_pos": 39, "end_pos": 55, "type": "TASK", "confidence": 0.8109906911849976}, {"text": "partial surface realization", "start_pos": 67, "end_pos": 94, "type": "TASK", "confidence": 0.7689587672551473}]}, {"text": "We show how these learned constraints can be applied to generate new summaries in the conclusion.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Distribution of content-based topicality  predicates in the 100-entry annotated corpus.", "labels": [], "entities": []}, {"text": " Table 2: Distribution of metadata and document- derivable predicates in the 100-entry corpus.", "labels": [], "entities": []}, {"text": " Table 4: The number of features used by ripper  to determine the output feature of target predicates.", "labels": [], "entities": []}]}