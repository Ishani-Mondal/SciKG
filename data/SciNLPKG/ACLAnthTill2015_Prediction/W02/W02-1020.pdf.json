{"title": [], "abstractContent": [{"text": "Text prediction is a form of interactive machine translation that is well suited to skilled translators.", "labels": [], "entities": [{"text": "Text prediction", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.8612646162509918}, {"text": "machine translation", "start_pos": 41, "end_pos": 60, "type": "TASK", "confidence": 0.7745952010154724}]}, {"text": "In principle it can assist in the production of a target text with minimal disruption to a translator's normal routine.", "labels": [], "entities": []}, {"text": "However, recent evaluations of a prototype prediction system showed that it significantly decreased the productivity of most translators who used it.", "labels": [], "entities": []}, {"text": "In this paper, we analyze the reasons for this and propose a solution which consists in seeking predictions that maximize the expected benefit to the translator, rather than just trying to anticipate some amount of upcoming text.", "labels": [], "entities": []}, {"text": "Using a model of a \"typ-ical translator\" constructed from data collected in the evaluations of the prediction prototype, we show that this approach has the potential to turn text prediction into a help rather than a hindrance to a translator.", "labels": [], "entities": [{"text": "text prediction", "start_pos": 174, "end_pos": 189, "type": "TASK", "confidence": 0.7380018532276154}]}], "introductionContent": [{"text": "The idea of using text prediction as a tool for translators was first introduced by Church and Hovy as one of many possible applications for \"crummy\" machine translation technology.", "labels": [], "entities": [{"text": "text prediction", "start_pos": 18, "end_pos": 33, "type": "TASK", "confidence": 0.7911466062068939}, {"text": "machine translation", "start_pos": 150, "end_pos": 169, "type": "TASK", "confidence": 0.7013257741928101}]}, {"text": "Text prediction can be seen as a form of interactive MT that is well suited to skilled translators.", "labels": [], "entities": [{"text": "Text prediction", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.8515504002571106}, {"text": "MT", "start_pos": 53, "end_pos": 55, "type": "TASK", "confidence": 0.9261866807937622}]}, {"text": "Compared to the traditional form of IMT based on Kay's original work-in which the user's role is to help disambiguate the source textprediction is less obtrusive and more natural, allowing the translator to focus on and directly control the contents of the target text.", "labels": [], "entities": []}, {"text": "Predictions can benefit a translator in several ways: by accelerating typing, by suggesting translations, and by serving as an implicit check against errors.", "labels": [], "entities": []}, {"text": "The first implementation of a predictive tool for translators was described in (, in the form of a simple word-completion system based on statistical models.", "labels": [], "entities": []}, {"text": "Various enhancements to this were carried out as part of the TransType project (), including the addition of a realistic user interface, better models, and the capability of predicting multi-word lexical units.", "labels": [], "entities": [{"text": "predicting multi-word lexical units", "start_pos": 174, "end_pos": 209, "type": "TASK", "confidence": 0.8803781419992447}]}, {"text": "In the final TransType prototype for English to French translation, the translator is presented with a short popup menu of predictions after each character typed.", "labels": [], "entities": []}, {"text": "These maybe incorporated into the text with a special command or rejected by continuing to type normally.", "labels": [], "entities": []}, {"text": "Although TransType is capable of correctly anticipating over 70% of the characters in a freely-typed translation (within the domain of its training corpus), this does not mean that users can translate in 70% less time when using the tool.", "labels": [], "entities": []}, {"text": "In fact, in atrial with skilled translators, the users' rate of text production declined by an average of 17% as a result of using TransType ().", "labels": [], "entities": []}, {"text": "There are two main reasons for this.", "labels": [], "entities": []}, {"text": "First, it takes time to read the system's proposals, so that in cases where they are wrong or too short, the net effect will be to slow the translator down.", "labels": [], "entities": []}, {"text": "Second, translators do not always act \"rationally\" when confronted with a proposal; that is, they do not always accept correct proposals and they occasionally accept incorrect ones.", "labels": [], "entities": []}, {"text": "Many of the former cases correspond to translators simply ignoring proposals altogether, which is understandable behaviour given the first point.", "labels": [], "entities": [{"text": "translators simply ignoring proposals", "start_pos": 39, "end_pos": 76, "type": "TASK", "confidence": 0.8405131846666336}]}, {"text": "This paper describes anew approach to text prediction intended to address these problems.", "labels": [], "entities": [{"text": "text prediction", "start_pos": 38, "end_pos": 53, "type": "TASK", "confidence": 0.8571986258029938}]}, {"text": "The main idea is to make predictions that maximize the expected benefit to the user in each context, rather than systematically proposing a fixed amount of text after each character typed.", "labels": [], "entities": []}, {"text": "The expected benefit is estimated from two components: a statistical translation model that gives the probability that a candidate prediction will be corrector incorrect, and a user model that determines the benefit to the translator in either case.", "labels": [], "entities": []}, {"text": "The user model takes into account the cost of reading a proposal, as well as the random nature of the decision to accept it or not.", "labels": [], "entities": []}, {"text": "This approach can be characterized as making fewer but better predictions: in general, predictions will be longer in contexts where the translation model is confident, shorter where it is less so, and absent in contexts where it is very uncertain.", "labels": [], "entities": []}, {"text": "Other novel aspects of the work we describe here are the use of a more accurate statistical translation model than has previously been employed for text prediction, and the use of a decoder to generate predictions of arbitrary length, rather than just single words or lexicalized units as in the TransType prototype.", "labels": [], "entities": [{"text": "text prediction", "start_pos": 148, "end_pos": 163, "type": "TASK", "confidence": 0.8394955992698669}]}, {"text": "The translation model is based on the maximum entropy principle and is designed specifically for this application.", "labels": [], "entities": [{"text": "translation", "start_pos": 4, "end_pos": 15, "type": "TASK", "confidence": 0.9714728593826294}]}, {"text": "To evaluate our approach to prediction, we simulated the actions of a translator over a large corpus of previously-translated text.", "labels": [], "entities": [{"text": "prediction", "start_pos": 28, "end_pos": 38, "type": "TASK", "confidence": 0.9653666019439697}]}, {"text": "The result is an increase of over 10% in translator productivity when using the predictive tool.", "labels": [], "entities": []}, {"text": "This is a considerable improvement over the -17% observed in the TransType trials.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated the predictor for English to French translation on a section of the Canadian Hansard corpus, after training the model on a chronologically earlier section.", "labels": [], "entities": [{"text": "English to French translation", "start_pos": 31, "end_pos": 60, "type": "TASK", "confidence": 0.6254438832402229}, {"text": "Canadian Hansard corpus", "start_pos": 81, "end_pos": 104, "type": "DATASET", "confidence": 0.8562279542287191}]}, {"text": "The test corpus consisted of 5,020 sentence pairs and approximately 100k words in each language; details of the training corpus are given in.", "labels": [], "entities": []}, {"text": "To simulate a translator's responses to predictions, we relied on the user model, accepting probabilistically according top, determining the associated benefit using B(x, h, s, k, a), and advancing the cursor k characters in the case of an: Results for different user simulations.", "labels": [], "entities": [{"text": "B", "start_pos": 166, "end_pos": 167, "type": "METRIC", "confidence": 0.9902552962303162}]}, {"text": "Numbers give % reductions in keystrokes.", "labels": [], "entities": []}, {"text": "Here k was obtained by comparing x to the known x * from the test corpus.", "labels": [], "entities": []}, {"text": "It may seem artificial to measure performance according to the objective function for the predictor, but this is biased only to the extent that it misrepresents an actual user's characteristics.", "labels": [], "entities": []}, {"text": "There are two cases: either the user is a better candidate-types more slowly, reacts more quickly and rationallythan assumed by the model, or a worse one.", "labels": [], "entities": []}, {"text": "The predictor will not be optimized in either case, but the simulation will only overestimate the benefit in the second case.", "labels": [], "entities": []}, {"text": "By being conservative in estimating the parameters of the user model, we feel we have minimized the number of translators who would fall into this category, and thus can hope to obtain realistic lower bounds for the average benefit across all translators.", "labels": [], "entities": []}, {"text": "contains results for two different translation models.", "labels": [], "entities": []}, {"text": "The top portion corresponds to the MEMD2B maximum entropy model described in; the bottom portion corresponds to the linear combination of a trigram and IBM 2 used in the TransType experiments ().", "labels": [], "entities": []}, {"text": "Columns give the maximum permitted number of words in predictions.", "labels": [], "entities": []}, {"text": "For each simulation, the predictor optimized benefits for the corresponding user model.", "labels": [], "entities": []}, {"text": "Several conclusions can be drawn from these results.", "labels": [], "entities": []}, {"text": "First, it is clear that estimating expected benefit is a much better strategy than making fixed-wordlength proposals, since the latter causes an increase in time for all values of M . In general, making \"exact\" estimates of string prefix probabilities works better than a linear approximation, but the difference is fairly small.", "labels": [], "entities": []}, {"text": "Second, the MEMD2B model significantly outperforms the trigram+IBM2 combination, producing better results for every predictor configuration tested.", "labels": [], "entities": []}, {"text": "The figure of -11.5% in bold corresponds to the TransType configuration, and corroborates the validity of the simulation.", "labels": [], "entities": []}, {"text": "Third, there are large drops in benefit due to reading times and probabilistic acceptance.", "labels": [], "entities": []}, {"text": "The biggest cost is due to reading, which lowers the best possible keystroke reduction by almost 50% for M = 5.", "labels": [], "entities": [{"text": "reading", "start_pos": 27, "end_pos": 34, "type": "METRIC", "confidence": 0.9062190651893616}, {"text": "keystroke reduction", "start_pos": 67, "end_pos": 86, "type": "METRIC", "confidence": 0.8128979206085205}]}, {"text": "Probabilistic acceptance causes a further drop of about 15% for M = 5.", "labels": [], "entities": [{"text": "acceptance", "start_pos": 14, "end_pos": 24, "type": "METRIC", "confidence": 0.8314889669418335}, {"text": "M", "start_pos": 64, "end_pos": 65, "type": "METRIC", "confidence": 0.8167257308959961}]}, {"text": "The main disappointment in these results is that performance peaks at M = 3 rather than continuing to improve as the predictor is allowed to consider longer word sequences.", "labels": [], "entities": [{"text": "M", "start_pos": 70, "end_pos": 71, "type": "METRIC", "confidence": 0.9739883542060852}]}, {"text": "Since the predictor knows B(x, h, s, k), the most likely cause for this is that the estimates for p( \u02c6 w m |h, s) become worse with increasing m.", "labels": [], "entities": [{"text": "B", "start_pos": 26, "end_pos": 27, "type": "METRIC", "confidence": 0.9848233461380005}]}, {"text": "Significantly, performance lev-els off at three words, just as the search loses direct contact with h through the trigram.", "labels": [], "entities": []}, {"text": "To correct for this, we used modified probabilities of the form \u03bb m p( \u02c6 w m |h, s), where \u03bb m is a length-specific correction factor, tuned so as to optimize benefit on a cross-validation corpus.", "labels": [], "entities": []}, {"text": "The results are shown in the corr row of table 2, for exact character-probability estimates.", "labels": [], "entities": []}, {"text": "In this case, performance improves with M , reaching a maximum keystroke reduction of 12.6% at M = 5.", "labels": [], "entities": [{"text": "M", "start_pos": 40, "end_pos": 41, "type": "METRIC", "confidence": 0.9862878322601318}, {"text": "keystroke reduction", "start_pos": 63, "end_pos": 82, "type": "METRIC", "confidence": 0.8831087052822113}]}], "tableCaptions": [{"text": " Table 1: Approximate times in seconds to generate  predictions of maximum word sequence length M ,  on a 1.2GHz processor, for the MEMD model.", "labels": [], "entities": []}, {"text": " Table 2: Results for different predictor configura- tions. Numbers give % reductions in keystrokes.", "labels": [], "entities": []}, {"text": " Table 3: Results for different user simulations.  Numbers give % reductions in keystrokes.", "labels": [], "entities": []}]}