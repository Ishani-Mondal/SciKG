{"title": [{"text": "An Analysis of the AskMSR Question-Answering System", "labels": [], "entities": [{"text": "AskMSR Question-Answering", "start_pos": 19, "end_pos": 44, "type": "DATASET", "confidence": 0.8172532916069031}]}], "abstractContent": [{"text": "We describe the architecture of the AskMSR question answering system and systematically evaluate contributions of different system components to accuracy.", "labels": [], "entities": [{"text": "AskMSR question answering", "start_pos": 36, "end_pos": 61, "type": "TASK", "confidence": 0.7321717341740926}, {"text": "accuracy", "start_pos": 145, "end_pos": 153, "type": "METRIC", "confidence": 0.9977537989616394}]}, {"text": "The system differs from most question answering systems in its dependency on data redundancy rather than sophisticated linguistic analyses of either questions or candidate answers.", "labels": [], "entities": [{"text": "question answering", "start_pos": 29, "end_pos": 47, "type": "TASK", "confidence": 0.8218466341495514}]}, {"text": "Because a wrong answer is often worse than no answer, we also explore strategies for predicting when the question answering system is likely to give an incorrect answer.", "labels": [], "entities": []}], "introductionContent": [{"text": "Question answering has recently received attention from the information retrieval, information extraction, machine learning, and natural language processing communities;).", "labels": [], "entities": [{"text": "Question answering", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.951398491859436}, {"text": "information retrieval", "start_pos": 60, "end_pos": 81, "type": "TASK", "confidence": 0.7640215754508972}, {"text": "information extraction", "start_pos": 83, "end_pos": 105, "type": "TASK", "confidence": 0.7247567623853683}]}, {"text": "The goal of a question answering system is to retrieve answers to questions rather than full documents or best-matching passages, as most information retrieval systems currently do.", "labels": [], "entities": [{"text": "question answering", "start_pos": 14, "end_pos": 32, "type": "TASK", "confidence": 0.7682317197322845}]}, {"text": "The TREC Question Answering Track, which has motivated much of the recent work in the field, focuses on fact-based, short-answer questions such as \"Who killed Abraham Lincoln?\" or \"How tall is Mount Everest?\"", "labels": [], "entities": [{"text": "TREC Question Answering", "start_pos": 4, "end_pos": 27, "type": "TASK", "confidence": 0.627366175254186}]}, {"text": "In this paper we describe our approach to short answer tasks like these, although the techniques we propose are more broadly applicable.", "labels": [], "entities": []}, {"text": "Most question answering systems use a variety of linguistic resources to help in understanding the user's query and matching sections in documents.", "labels": [], "entities": [{"text": "question answering", "start_pos": 5, "end_pos": 23, "type": "TASK", "confidence": 0.8223351538181305}]}, {"text": "The most common linguistic resources include: part-of-speech tagging, parsing, named entity extraction, semantic relations, dictionaries, WordNet, etc.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 46, "end_pos": 68, "type": "TASK", "confidence": 0.7043894827365875}, {"text": "named entity extraction", "start_pos": 79, "end_pos": 102, "type": "TASK", "confidence": 0.6757016082604727}, {"text": "WordNet", "start_pos": 138, "end_pos": 145, "type": "DATASET", "confidence": 0.9160774946212769}]}, {"text": "We chose instead to focus on the Web as a gigantic data repository with tremendous redundancy that can be exploited for question answering.", "labels": [], "entities": [{"text": "question answering", "start_pos": 120, "end_pos": 138, "type": "TASK", "confidence": 0.9033991098403931}]}, {"text": "We view our approach as complimentary to more linguistic approaches, but have chosen to see how far we can get initially by focusing on data per se as a key resource available to drive our system design.", "labels": [], "entities": []}, {"text": "Recently, other researchers have also looked to the web as a resource for question answering).", "labels": [], "entities": [{"text": "question answering", "start_pos": 74, "end_pos": 92, "type": "TASK", "confidence": 0.901094913482666}]}, {"text": "These systems typically perform complex parsing and entity extraction for both queries and best matching Web pages, and maintain local caches of pages or term weights.", "labels": [], "entities": [{"text": "entity extraction", "start_pos": 52, "end_pos": 69, "type": "TASK", "confidence": 0.6930735409259796}]}, {"text": "Our approach is distinguished from these in its simplicity and efficiency in the use of the Web as a large data resource.", "labels": [], "entities": []}, {"text": "Automatic QA from a single, small information source is extremely challenging, since there is likely to be only one answer in the source to any user's question.", "labels": [], "entities": []}, {"text": "Given a source, such as the TREC corpus, that contains only a relatively small number of formulations of answers to a query, we maybe faced with the difficult task of mapping questions to answers byway of uncovering complex lexical, syntactic, or semantic relationships between question string and answer string.", "labels": [], "entities": [{"text": "TREC corpus", "start_pos": 28, "end_pos": 39, "type": "DATASET", "confidence": 0.8547223508358002}]}, {"text": "The need for anaphor resolution and synonymy, the presence of alternate syntactic formulations and indirect answers all make answer finding a potentially challenging task.", "labels": [], "entities": [{"text": "anaphor resolution", "start_pos": 13, "end_pos": 31, "type": "TASK", "confidence": 0.7820804119110107}, {"text": "answer finding", "start_pos": 125, "end_pos": 139, "type": "TASK", "confidence": 0.8929221034049988}]}, {"text": "However, the greater the \"+the Louvre Museum +is located\" \"+the Louvre Museum +is +in\" \"+the Louvre Museum +is near\" \"+the Louvre Museum +is\" Louvre AND Museum AND near in Paris France 59% museums 12% hostels 10%.", "labels": [], "entities": [{"text": "Louvre Museum", "start_pos": 31, "end_pos": 44, "type": "DATASET", "confidence": 0.9308870732784271}, {"text": "Louvre Museum", "start_pos": 64, "end_pos": 77, "type": "DATASET", "confidence": 0.9537003040313721}, {"text": "Louvre Museum", "start_pos": 93, "end_pos": 106, "type": "DATASET", "confidence": 0.9311075210571289}]}, {"text": "System Architecture answer redundancy in the source data collection, the more likely it is that we can find an answer that occurs in a simple relation to the question.", "labels": [], "entities": []}, {"text": "Therefore, the less likely it is that we will need to solve the aforementioned difficulties facing natural language processing systems.", "labels": [], "entities": []}, {"text": "In this paper, we describe the architecture of the AskMSR Question Answering System and evaluate contributions of different system components to accuracy.", "labels": [], "entities": [{"text": "AskMSR Question Answering", "start_pos": 51, "end_pos": 76, "type": "TASK", "confidence": 0.765567938486735}, {"text": "accuracy", "start_pos": 145, "end_pos": 153, "type": "METRIC", "confidence": 0.997694194316864}]}, {"text": "Because a wrong answer is often worse than no answer, we also explore strategies for predicting when the question answering system is likely to give an incorrect answer.", "labels": [], "entities": []}], "datasetContent": [{"text": "For experimental evaluations we used the first 500 TREC-9 queries (201-700)).", "labels": [], "entities": [{"text": "TREC-9", "start_pos": 51, "end_pos": 57, "type": "METRIC", "confidence": 0.6778017282485962}]}, {"text": "We used the patterns provided by NIST for automatic scoring.", "labels": [], "entities": [{"text": "NIST", "start_pos": 33, "end_pos": 37, "type": "DATASET", "confidence": 0.9686398506164551}]}, {"text": "A few patterns were slightly modified to accommodate the fact that some of the answer strings returned using the Web were not available for judging in TREC-9.", "labels": [], "entities": [{"text": "TREC-9", "start_pos": 151, "end_pos": 157, "type": "DATASET", "confidence": 0.7551052570343018}]}, {"text": "We did this in a very conservative manner allowing for more specific correct answers (e.g., Edward J. Smith vs. Edward Smith) but not more general ones (e.g., Smith vs. Edward Smith), and also allowing for simple substitutions (e.g., 9 months vs. nine months).", "labels": [], "entities": []}, {"text": "There also are substantial time differences between the Web and TREC databases (e.g., the correct answer to Who is the president of Bolivia?", "labels": [], "entities": [{"text": "Who is the president of Bolivia?", "start_pos": 108, "end_pos": 140, "type": "TASK", "confidence": 0.5409797004290989}]}, {"text": "changes over time), but we did not modify the answer key to accommodate these time differences, because it would make comparison with earlier TREC results impossible.", "labels": [], "entities": []}, {"text": "These changes influence the absolute scores somewhat but do not change relative performance, which is our focus here.", "labels": [], "entities": [{"text": "absolute scores", "start_pos": 28, "end_pos": 43, "type": "METRIC", "confidence": 0.9510719776153564}]}, {"text": "All runs are completely automatic, starting with queries and generating a ranked list of 5 candidate answers.", "labels": [], "entities": []}, {"text": "For the experiments reported in this paper we used Google as a backend because it provides query-relevant summaries that make our n-gram mining efficient.", "labels": [], "entities": [{"text": "n-gram mining", "start_pos": 130, "end_pos": 143, "type": "TASK", "confidence": 0.718054324388504}]}, {"text": "Candidate answers area maximum of 50 bytes long, and typically much shorter than that.", "labels": [], "entities": []}, {"text": "We report the Mean Reciprocal Rank (MRR) of the first correct answer, the Number of Questions Correctly Answered (NAns), and the proportion of Questions Correctly Answered (%Ans).", "labels": [], "entities": [{"text": "Mean Reciprocal Rank (MRR)", "start_pos": 14, "end_pos": 40, "type": "METRIC", "confidence": 0.9752336144447327}, {"text": "Number of Questions Correctly Answered (NAns)", "start_pos": 74, "end_pos": 119, "type": "METRIC", "confidence": 0.800422839820385}, {"text": "proportion of Questions Correctly Answered (%Ans)", "start_pos": 129, "end_pos": 178, "type": "METRIC", "confidence": 0.6032794751226902}]}], "tableCaptions": [{"text": " Table 1. Componential analysis of the AskMSR QA system.", "labels": [], "entities": [{"text": "AskMSR QA system", "start_pos": 39, "end_pos": 55, "type": "DATASET", "confidence": 0.9298017024993896}]}]}