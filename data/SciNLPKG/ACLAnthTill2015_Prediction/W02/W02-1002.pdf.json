{"title": [{"text": "Conditional Structure versus Conditional Estimation in NLP Models", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper separates conditional parameter estimation , which consistently raises test set accuracy on statistical NLP tasks, from conditional model structures , such as the conditional Markov model used for maximum-entropy tagging, which tend to lower accuracy.", "labels": [], "entities": [{"text": "conditional parameter estimation", "start_pos": 21, "end_pos": 53, "type": "TASK", "confidence": 0.6993048389752706}, {"text": "accuracy", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.9605271220207214}, {"text": "maximum-entropy tagging", "start_pos": 208, "end_pos": 231, "type": "TASK", "confidence": 0.6415058076381683}, {"text": "accuracy", "start_pos": 253, "end_pos": 261, "type": "METRIC", "confidence": 0.9966639876365662}]}, {"text": "Error analysis on part-of-speech tagging shows that the actual tagging errors made by the conditionally structured model derive not only from label bias, but also from other ways in which the independence assumptions of the conditional model structure are unsuited to linguistic sequences.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 18, "end_pos": 40, "type": "TASK", "confidence": 0.7039951980113983}]}, {"text": "The paper presents new word-sense disambiguation and POS tagging experiments, and integrates apparently conflicting reports from other recent work.", "labels": [], "entities": [{"text": "word-sense disambiguation", "start_pos": 23, "end_pos": 48, "type": "TASK", "confidence": 0.7335410118103027}, {"text": "POS tagging", "start_pos": 53, "end_pos": 64, "type": "TASK", "confidence": 0.8191297352313995}]}], "introductionContent": [{"text": "The success and widespread adoption of probabilistic models in NLP has led to numerous variant methods for any given task, and it can be difficult to tell what aspects of a system have led to its relative successes or failures.", "labels": [], "entities": []}, {"text": "As an example, maximum entropy taggers have achieved very good performance), but almost identical performance has also come from finely tuned HMM models).", "labels": [], "entities": []}, {"text": "Are any performance gains due to the sequence model used, the maximum entropy approach to parameter estimation, or the features employed by the system?", "labels": [], "entities": []}, {"text": "Recent experiments have given conflicting recommendations.", "labels": [], "entities": []}, {"text": "finds that a conditionally trained PCFG marginally outperforms a standard jointly trained PCFG, but that a conditional shiftreduce model performs worse than a joint formulation.", "labels": [], "entities": []}, {"text": "suggest on abstract grounds that conditional models will suffer from a phenomenon called label bias -see section 3 -but is this a significant effect for real NLP problems?", "labels": [], "entities": []}, {"text": "We suggest that the results in the literature, along with the new results we present in this work, can be explained by the following generalizations: \u2022 The ability to include better features in a wellfounded fashion leads to better performance.", "labels": [], "entities": []}, {"text": "\u2022 For fixed features, assumptions implicit in the model structure have a large impact on errors.", "labels": [], "entities": []}, {"text": "\u2022 Maximizing the objective being evaluated has a reliably positive, but often small, effect.", "labels": [], "entities": [{"text": "Maximizing", "start_pos": 2, "end_pos": 12, "type": "TASK", "confidence": 0.9656853079795837}]}, {"text": "It is especially important to study these issues using NLP data sets: NLP tasks are marked by their complexity and sparsity, and, as we show, conclusions imported from the machine-learning literature do not always hold in these characteristic contexts.", "labels": [], "entities": []}, {"text": "In previous work, the structure of a model and the method of parameter estimation were often both changed simultaneously (for reasons of naturalness or computational ease), but in this paper we seek to tease apart the separate effects of these two factors.", "labels": [], "entities": []}, {"text": "In section 2, we take the Naive-Bayes model, applied to word-sense disambiguation (WSD), and train it to maximize various objective functions.", "labels": [], "entities": [{"text": "word-sense disambiguation (WSD)", "start_pos": 56, "end_pos": 87, "type": "TASK", "confidence": 0.8232732057571411}]}, {"text": "Our experiments reaffirm that discriminative objectives like conditional likelihood improve test-set accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 101, "end_pos": 109, "type": "METRIC", "confidence": 0.9917037487030029}]}, {"text": "In section 3, we examine two different model structures for part-of-speech (POS) tagging.", "labels": [], "entities": [{"text": "part-of-speech (POS) tagging", "start_pos": 60, "end_pos": 88, "type": "TASK", "confidence": 0.6019391477108001}]}, {"text": "There, we analyze how assumptions latent in conditional structures lower tagging accuracy and produce strange qualitative behaviors.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 81, "end_pos": 89, "type": "METRIC", "confidence": 0.8719730973243713}]}, {"text": "Finally, we discuss related recent findings by other researchers.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}