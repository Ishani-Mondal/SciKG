{"title": [{"text": "Evaluation of Direct Speech Translation Method Using Inductive Learning for Conversations in the Travel Domain", "labels": [], "entities": [{"text": "Direct Speech Translation", "start_pos": 14, "end_pos": 39, "type": "TASK", "confidence": 0.6504557828108469}]}], "abstractContent": [{"text": "This paper evaluates a direct speech translation Method with waveforms using the Inductive Learning method for short conversation.", "labels": [], "entities": [{"text": "direct speech translation", "start_pos": 23, "end_pos": 48, "type": "TASK", "confidence": 0.7459230621655782}]}, {"text": "The method is able to work without conventional speech recognition and speech synthesis because syntactic expressions are not needed for translation in the proposed method.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 48, "end_pos": 66, "type": "TASK", "confidence": 0.7311888039112091}, {"text": "speech synthesis", "start_pos": 71, "end_pos": 87, "type": "TASK", "confidence": 0.7083848118782043}]}, {"text": "We focus only on acoustic characteristics of speech wave-forms of source and target languages without obtaining character strings from utterances.", "labels": [], "entities": []}, {"text": "This speech translation method can be utilized for any language because the system has no processing dependent on an individual character of a specific language.", "labels": [], "entities": [{"text": "speech translation", "start_pos": 5, "end_pos": 23, "type": "TASK", "confidence": 0.7362916171550751}]}, {"text": "Therefore, we can utilize the speech of a handicapped person who is notable to be treated by conventional speech recognition systems, because we do not need to segment the speech into phonemes, syllables, or words to realize speech translation.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 106, "end_pos": 124, "type": "TASK", "confidence": 0.7610845863819122}, {"text": "speech translation", "start_pos": 225, "end_pos": 243, "type": "TASK", "confidence": 0.7737699747085571}]}, {"text": "Our method is realized by learning translation rules that have acoustic correspondence between two languages inductively.", "labels": [], "entities": []}, {"text": "In this paper, we deal with a translation between Japanese and English.", "labels": [], "entities": [{"text": "translation", "start_pos": 30, "end_pos": 41, "type": "TASK", "confidence": 0.9657955169677734}]}], "introductionContent": [{"text": "Speech is the most common means of communication for us because the information contained in Figure 1: Comparison of conventional and our approach.", "labels": [], "entities": []}, {"text": "speech is sufficient to play a fundamental role in conversation.", "labels": [], "entities": []}, {"text": "Thus, it is much better that the processing deals with speech directly.", "labels": [], "entities": []}, {"text": "However, conventional approaches of speech translation need a text result, obtained by speech recognition, for machine translation although several errors or unrecognized portions maybe included in the result.", "labels": [], "entities": [{"text": "speech translation", "start_pos": 36, "end_pos": 54, "type": "TASK", "confidence": 0.7506294548511505}, {"text": "machine translation", "start_pos": 111, "end_pos": 130, "type": "TASK", "confidence": 0.7479471564292908}]}, {"text": "A text is translated through morphological analysis, syntactic analysis, and parsing of the sentence of the target language.", "labels": [], "entities": []}, {"text": "Finally, the speech synthesis stage produces speech output of the target language.(A) shows the whole procedure of a traditional speech translation approach.", "labels": [], "entities": [{"text": "speech synthesis", "start_pos": 13, "end_pos": 29, "type": "TASK", "confidence": 0.7231841385364532}, {"text": "speech translation", "start_pos": 129, "end_pos": 147, "type": "TASK", "confidence": 0.7792054712772369}]}, {"text": "The procedure has several complicated processes that do not give satisfying results.", "labels": [], "entities": []}, {"text": "Therefore, the lack of accuracy in each stage culminates into a poor final result.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9994781613349915}]}, {"text": "For example, character strings obtained by speech recognition may represent different inforAssociation for Computational Linguistics.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 43, "end_pos": 61, "type": "TASK", "confidence": 0.7265437245368958}]}, {"text": "Algorithms and Systems, Philadelphia, July 2002, pp.", "labels": [], "entities": [{"text": "Algorithms and Systems, Philadelphia, July 2002", "start_pos": 0, "end_pos": 47, "type": "DATASET", "confidence": 0.5830900259315968}]}, {"text": "Proceedings of the Workshop on Speech-to-Speech Translation: Figure 2: Processing structure.", "labels": [], "entities": [{"text": "Speech-to-Speech Translation", "start_pos": 31, "end_pos": 59, "type": "TASK", "confidence": 0.7035216689109802}]}, {"text": "mation than the original speech.", "labels": [], "entities": []}, {"text": "attempted to recognize several vowels and consonants using Neural Networks that had different structures with TDNN (ATR Lab., 1995), however, they could not obtain a high accuracy of recognition.", "labels": [], "entities": [{"text": "TDNN", "start_pos": 110, "end_pos": 114, "type": "DATASET", "confidence": 0.9100098013877869}, {"text": "ATR Lab., 1995)", "start_pos": 116, "end_pos": 131, "type": "DATASET", "confidence": 0.9225093364715576}, {"text": "accuracy", "start_pos": 171, "end_pos": 179, "type": "METRIC", "confidence": 0.9991999268531799}]}, {"text": "They confirmed that distinguishing the boundaries of words, syllables, or phonemes is a task of great difficulty.", "labels": [], "entities": [{"text": "distinguishing the boundaries of words, syllables", "start_pos": 20, "end_pos": 69, "type": "TASK", "confidence": 0.8433755551065717}]}, {"text": "Then, they only focused on speech waveform itself, not character strings obtained by speech recognition to realize speech translation.", "labels": [], "entities": [{"text": "speech translation", "start_pos": 115, "end_pos": 133, "type": "TASK", "confidence": 0.7139885574579239}]}, {"text": "Murakami et.al decided on dealing with the correspondence of acoustic characteristics of speech waveform instead of character strings between two utterances.", "labels": [], "entities": []}, {"text": "Our approach handles the acoustic characteristics of speech without lexical expression through a much simpler structure than the reports of or because we believe that simplification of the system would prevent inaccuracies in the translation.", "labels": [], "entities": []}, {"text": "shows the processing stages of our approach.", "labels": [], "entities": []}, {"text": "If speech translation can be realized by analyzing the correspondence in character strings obtained by speech recognition, we can also buildup speech translation by dealing with the correspondence in acoustic characteristics.", "labels": [], "entities": [{"text": "speech translation", "start_pos": 3, "end_pos": 21, "type": "TASK", "confidence": 0.7462870478630066}, {"text": "speech recognition", "start_pos": 103, "end_pos": 121, "type": "TASK", "confidence": 0.7458441257476807}, {"text": "speech translation", "start_pos": 143, "end_pos": 161, "type": "TASK", "confidence": 0.7158284485340118}]}, {"text": "In our method, we extract acoustic common parts and different parts by comparing two examples of acoustic characteristics of speech between two translation pairs within the same language.", "labels": [], "entities": []}, {"text": "Then we generate translation rules and register them in a translation dictionary.", "labels": [], "entities": []}, {"text": "The rules also have the location information of acquired parts for speech synthesis on time-domain.", "labels": [], "entities": [{"text": "speech synthesis", "start_pos": 67, "end_pos": 83, "type": "TASK", "confidence": 0.7141972184181213}]}, {"text": "The translation rules are acquired not only by comparing speech utterances but also using the Inductive Learning Method (K.), still keeping acoustic information within the rules.", "labels": [], "entities": []}, {"text": "Deciding the correspondence of meaning between two languages is a unique condition to realize our method.", "labels": [], "entities": []}, {"text": "Ina translation phase, when an unknown utterance of a source language is applied to be translated, the system compares this sentence with all acoustic information of all rules within the source language.", "labels": [], "entities": []}, {"text": "Then several matched rules are utilized and referred to their corresponding parts of the target language.", "labels": [], "entities": []}, {"text": "Finally, we obtain roughly synthesized target speech by simply concatenating several suitable parts of rules in the target language according to the information of location.", "labels": [], "entities": []}, {"text": "shows an overview of the processing structure of our method.", "labels": [], "entities": []}, {"text": "Our method has several advantages over other approaches.", "labels": [], "entities": []}, {"text": "First, the performance of the translation is not affected by the lack of accuracy in speech recognition because we do not need the segmentation of speech into words, syllables, or phonemes.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 73, "end_pos": 81, "type": "METRIC", "confidence": 0.9974157810211182}, {"text": "speech recognition", "start_pos": 85, "end_pos": 103, "type": "TASK", "confidence": 0.7365646064281464}, {"text": "segmentation of speech into words, syllables", "start_pos": 131, "end_pos": 175, "type": "TASK", "confidence": 0.7928540962082999}]}, {"text": "Therefore, our method can be applied for all languages without having to make processing changes in the machine translation stage because there is no processing dependent on any specific language.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 104, "end_pos": 123, "type": "TASK", "confidence": 0.7168494910001755}]}, {"text": "With conventional methods, several processes in the machine translation stage must be altered if the target language is to be changed because morphological analysis and syntactic analysis are dependent on each individual character of language completely.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 52, "end_pos": 71, "type": "TASK", "confidence": 0.7074107229709625}]}, {"text": "Any difference in language has no affect on the ability of the proposed method, fundamentally because we focus on the acoustic characteristics of speech, not on the character strings of languages.", "labels": [], "entities": []}, {"text": "It is very important to approach speech translation with anew methodology that is independent of individual characters of any language.", "labels": [], "entities": [{"text": "speech translation", "start_pos": 33, "end_pos": 51, "type": "TASK", "confidence": 0.7563509345054626}]}, {"text": "We also expect our approach can be utilized in speech recuperation systems for people with a speech impediment because our method is able to deal with various types of speech that is notable to be treated by conventional speech recognition systems for normal voice.", "labels": [], "entities": [{"text": "speech recuperation", "start_pos": 47, "end_pos": 66, "type": "TASK", "confidence": 0.7177681177854538}]}, {"text": "have successfully obtained several samples of translation by applying our method using local recorded speech data and spontaneous conversation speech.", "labels": [], "entities": [{"text": "translation", "start_pos": 46, "end_pos": 57, "type": "TASK", "confidence": 0.9787866473197937}]}, {"text": "In this paper, we adopt speech data of travel conversations to the proposed method.", "labels": [], "entities": []}, {"text": "We evaluate the performance of the method through experiments and offer discussion on behaviors of the system.", "labels": [], "entities": []}], "datasetContent": [{"text": "To determine that the obviously lowest distance value in the distance curve is a common part, we adopt a threshold calculated by statistical information.", "labels": [], "entities": []}, {"text": "We calculate the variance of distance values shown as \u03c3 and the mean value within the curve.", "labels": [], "entities": []}, {"text": "The threshold is conducted as \u03b8 = 4\u03c3 2 from the equation of the Gaussian distribution and the standardized normal distribution.", "labels": [], "entities": []}, {"text": "A point of the smallest distance value within a curve is represented by x and a parameter m shows the mean value of distances.", "labels": [], "entities": []}, {"text": "A common part is detected if (x \u2212 m) 2 > \u03b8, because the portion of reference speech has much similarity with the \"test vector\" of the distance curve in a point, and that common part is represented by \"0\".", "labels": [], "entities": []}, {"text": "Otherwise the speech portion for \"test vector\" is regarded as a different part and represented by \"1\".", "labels": [], "entities": []}, {"text": "If several common parts are decided continuously, we deal with them as one common part, and the first point in that part will be the start point finally.", "labels": [], "entities": []}, {"text": "In our method, the acoustic similarities evaluated by several calculations are only the factor for judgment in classifying common or different parts in the speech samples.", "labels": [], "entities": []}, {"text": "If an unknown speech utterance of a source language can be replaced with acoustic information from rules in the dictionary, the speech will be translated and synthesized roughly without losing it's meaning.", "labels": [], "entities": []}, {"text": "Each matched rule includes certain equivalent correspondence parts of the target language.", "labels": [], "entities": []}, {"text": "The system needs to decide the most suitable candidates of rules from the rule dictionary for each translation.", "labels": [], "entities": []}, {"text": "If the level of similarity between the whole applied unknown speech and all parts of the rules is higher than a rate of agreement as in, the rules that include appropriate parts can become candidates for current translation.", "labels": [], "entities": [{"text": "agreement", "start_pos": 120, "end_pos": 129, "type": "METRIC", "confidence": 0.9527201056480408}]}, {"text": "82 utterances of limited domain have been applied to the system for translation.", "labels": [], "entities": [{"text": "translation", "start_pos": 68, "end_pos": 79, "type": "TASK", "confidence": 0.9699128866195679}]}, {"text": "Regretfully, we could not obtain any complete translated utterances, although several samples have been incompletely translated by adapting translation rules.", "labels": [], "entities": []}], "tableCaptions": []}