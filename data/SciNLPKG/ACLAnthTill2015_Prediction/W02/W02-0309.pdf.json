{"title": [{"text": "Biomedical Text Retrieval in Languages with a Complex Morphology", "labels": [], "entities": [{"text": "Biomedical Text Retrieval", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.775858740011851}]}], "abstractContent": [{"text": "Document retrieval in languages with a rich and complex morphology-particularly in terms of derivation and (single-word) composition-suffers from serious performance degradation with the stemming-only query-term-to-text-word matching paradigm.", "labels": [], "entities": [{"text": "Document retrieval", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9249163866043091}, {"text": "stemming-only query-term-to-text-word matching", "start_pos": 187, "end_pos": 233, "type": "TASK", "confidence": 0.6503996749718984}]}, {"text": "We propose an alternative approach in which morphologically complex word forms are segmented into relevant subwords (such as stems, named entities, acronyms), and subwords constitute the basic unit for indexing and retrieval.", "labels": [], "entities": []}, {"text": "We evaluate our approach on a large biomedical document collection.", "labels": [], "entities": []}], "introductionContent": [{"text": "Morphological alterations of a search term have a negative impact on the recall performance of an information retrieval (IR) system, since they preclude a direct match between the search term proper and its morphological variants in the documents to be retrieved.", "labels": [], "entities": [{"text": "recall", "start_pos": 73, "end_pos": 79, "type": "METRIC", "confidence": 0.9947673082351685}, {"text": "information retrieval (IR)", "start_pos": 98, "end_pos": 124, "type": "TASK", "confidence": 0.7716022968292237}]}, {"text": "In order to cope with such variation, morphological analysis is concerned with the reverse processing of inflection (e.g., 'search ed', 'search ing') 1 , derivation (e.g., 'search er' or 'search able') and composition (e.g., German 'Blut hoch druck').", "labels": [], "entities": []}, {"text": "The goal is to map all occurring morphological variants to some canonical base forme.g., 'search' in the examples from above.", "labels": [], "entities": []}, {"text": "The efforts required for performing morphological analysis vary from language to language.", "labels": [], "entities": [{"text": "morphological analysis", "start_pos": 36, "end_pos": 58, "type": "TASK", "confidence": 0.6846729069948196}]}, {"text": "For English, known for its limited number of inflection patterns, lexicon-free general-purpose stem-mers) demonstrably improve retrieval performance.", "labels": [], "entities": []}, {"text": "This has been reported for other languages, too, dependent on the generality of the chosen approach).", "labels": [], "entities": []}, {"text": "When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist.", "labels": [], "entities": []}, {"text": "This is particularly true for the medical domain.", "labels": [], "entities": []}, {"text": "From an IR view, a lot of specialized research has already been carried out for medical applications, with emphasis on the lexico-semantic aspects of dederivation and decomposition.", "labels": [], "entities": [{"text": "IR", "start_pos": 8, "end_pos": 10, "type": "TASK", "confidence": 0.95046466588974}]}, {"text": "While one may argue that single-word compounds are quite rare in English (which is not the casein the medical domain either), this is certainly not true for German and other basically agglutinative languages known for excessive single-word nominal compounding.", "labels": [], "entities": [{"text": "single-word nominal compounding", "start_pos": 228, "end_pos": 259, "type": "TASK", "confidence": 0.7479749520619711}]}, {"text": "This problem becomes even more pressing for technical sublanguages, such as medical German (e.g., 'Blut druck mess ger\u00e4t' translates to 'device for measuring blood pressure').", "labels": [], "entities": []}, {"text": "The problem one faces from an IR point of view is that besides fairly standardized nominal compounds, which already form a regular part of the sublanguage proper, a myriad of ad hoc compounds are formed on the fly which cannot be anticipated when formulating a retrieval query though they appear in relevant documents.", "labels": [], "entities": [{"text": "IR", "start_pos": 30, "end_pos": 32, "type": "TASK", "confidence": 0.9058366417884827}]}, {"text": "Hence, enumerating morphological variants in a semi-automatically generated lexicon, such as proposed for French (), turns out to be infeasible, at least for German and related languages.", "labels": [], "entities": []}, {"text": "Furthermore, medical terminology is characterized by atypical mix of Latin and Greek roots with the corresponding host language (e.g., German), often referred to as neo-classical compounding.", "labels": [], "entities": []}, {"text": "While this is simply irrelevant for general-purpose morphological analyzers, dealing with such phenomena is crucial for any attempt to cope adequately with medical free-texts in an IR setting.", "labels": [], "entities": []}, {"text": "We here propose an approach to document retrieval which is based on the idea of segmenting query and document terms into basic subword units.", "labels": [], "entities": [{"text": "document retrieval", "start_pos": 31, "end_pos": 49, "type": "TASK", "confidence": 0.7810322940349579}]}, {"text": "Hence, this approach combines procedures for deflection, dederivation and decomposition.", "labels": [], "entities": []}, {"text": "Subwords cannot be equated with linguistically significant morphemes, in general, since their granularity maybe coarser than that of morphemes (cf. our discussion in Section 2).", "labels": [], "entities": []}, {"text": "We validate our claims in Section 4 on a substantial biomedical document collection (cf. Section 3).", "labels": [], "entities": []}], "datasetContent": [{"text": "As document collection for our experiments we chose the CD-ROM edition of MSD, a Germanlanguage handbook of clinical medicine.", "labels": [], "entities": [{"text": "CD-ROM edition of MSD, a Germanlanguage handbook of clinical medicine", "start_pos": 56, "end_pos": 125, "type": "DATASET", "confidence": 0.8205234625122764}]}, {"text": "It contains 5,517 handbook-style articles (about 2.4 million text tokens) on abroad range of clinical topics using biomedical terminology.", "labels": [], "entities": []}, {"text": "In our retrieval experiments we tried to cover a wide range of topics from clinical medicine.", "labels": [], "entities": []}, {"text": "Due to the importance of searching health-related contents both for medical professionals and the general public we collected two sets of user queries, viz.", "labels": [], "entities": []}, {"text": "expert queries and layman queries.", "labels": [], "entities": []}, {"text": "A semantic weight \u00a7 =2 is assigned to all subwords and some semantically important suffixes, such as '-tomie' or '-itis'; \u00a7 =1 is assigned to prefixes and derivational suffixes; \u00a7 =0 holds for inflectional suffixes and infixes.", "labels": [], "entities": []}, {"text": "A large collection of multiple choice questions from the nationally standardized year 5 examination questionnaire for medical students in Germany constituted the basis of this query set.", "labels": [], "entities": []}, {"text": "Out of a total of 580 questions we selected 210 ones explicitly addressing clinical issues (in conformance with the range of topics covered by MSD).", "labels": [], "entities": []}, {"text": "We then asked 63 students (between the 3rd and 5th study year) from our university's Medical School during regular classroom hours to formulate free-form natural language queries in order to retrieve documents that would help in answering these questions, assuming an ideal search engine.", "labels": [], "entities": []}, {"text": "Acronyms and abbreviations were allowed, but the length of each query was restricted to a maximum often terms.", "labels": [], "entities": []}, {"text": "Each student was assigned ten topics at random, so we ended up with 630 queries from which 25 were randomly chosen for further consideration (the set contained no duplicate queries).", "labels": [], "entities": []}, {"text": "The operators of a Germanlanguage medical search engine (http://www.", "labels": [], "entities": [{"text": "Germanlanguage medical search engine", "start_pos": 19, "end_pos": 55, "type": "DATASET", "confidence": 0.9076652675867081}]}, {"text": "dr-antonius.de/) provided us with a set of 38,600 logged queries.", "labels": [], "entities": []}, {"text": "A random sample (\u00a8 =400) was classified by a medical expert whether they contained medical jargon or the wording of laymen.", "labels": [], "entities": []}, {"text": "Only those queries which were univocally classified as layman queries (through the use of non-technical terminology) ended up in a subset of 125 queries from which 27 were randomly chosen for our study.", "labels": [], "entities": []}, {"text": "The judgments for identifying relevant documents in the whole test collection (5,517 documents) for each of the 25 expert and 27 layman queries were carried out by three medical experts (none of them was involved in the system development).", "labels": [], "entities": []}, {"text": "Given such a time-consuming task, we investigated only a small number of user queries in our experiments.", "labels": [], "entities": []}, {"text": "This also elucidates why we did not address interrater reliability.", "labels": [], "entities": []}, {"text": "The queries and the relevance judgments were hidden from the developers of the subword dictionary.", "labels": [], "entities": []}, {"text": "For unbiased evaluation of our approach, we used a home-grown search engine (implemented in the PYTHON script language).", "labels": [], "entities": [{"text": "PYTHON script language", "start_pos": 96, "end_pos": 118, "type": "DATASET", "confidence": 0.855885366598765}]}, {"text": "It crawls text/HTML files, produces an inverted file index, and assigns salience weights to terms and documents based on a simple tf-idf metric.", "labels": [], "entities": []}, {"text": "The retrieval process relies on the vector space model, with the cosine measure expressing the similarity between a query and a document.", "labels": [], "entities": []}, {"text": "The search engine produces a ranked output of documents.", "labels": [], "entities": []}, {"text": "We also incorporate proximity data, since this information becomes particularly important in the segmentation of complex word forms.", "labels": [], "entities": [{"text": "segmentation of complex word forms", "start_pos": 97, "end_pos": 131, "type": "TASK", "confidence": 0.7943661928176879}]}, {"text": "So a distinction must be made between a document containing 'append ectomy' and 'thyroid itis' and another one containing 'append ic itis' and 'thyroid ectomy'.", "labels": [], "entities": []}, {"text": "Our proximity criterion assigns a higher ranking to adjacent and a lower one to distant search terms.", "labels": [], "entities": []}, {"text": "This is achieved by an adjacency offset, \u00a9 , which is added to the cosine measure of each document.", "labels": [], "entities": [{"text": "\u00a9", "start_pos": 41, "end_pos": 42, "type": "METRIC", "confidence": 0.9925693869590759}]}, {"text": "For a query consisting of\u00a8terms , the minimal distance between a pair of terms in a document, ( ), is referred to by . The offset is then calculated as follows: We distinguished four different conditions for the retrieval experiments, viz.", "labels": [], "entities": []}, {"text": "plain token match, trigram match, plain subword match, and subword match incorporating synonym expansion: Plain Token Match (WS).", "labels": [], "entities": [{"text": "Plain Token Match (WS)", "start_pos": 106, "end_pos": 128, "type": "TASK", "confidence": 0.42942580084005993}]}, {"text": "A direct match between text tokens in a document and those in a query is tried.", "labels": [], "entities": []}, {"text": "No normalizing term processing (stemming, etc.) is done prior to indexing or evaluating the query.", "labels": [], "entities": [{"text": "normalizing term processing (stemming", "start_pos": 3, "end_pos": 40, "type": "TASK", "confidence": 0.6016493260860443}]}, {"text": "The search was run on an index covering the entire document collection (182,306 index terms).", "labels": [], "entities": []}, {"text": "This scenario serves as the baseline for determining the benefits of our approach.", "labels": [], "entities": []}, {"text": "Trigram Match (TG).", "labels": [], "entities": [{"text": "Trigram Match (TG)", "start_pos": 0, "end_pos": 18, "type": "DATASET", "confidence": 0.8925173997879028}]}, {"text": "As an alternative lexiconfree indexing approach (which is more robust relative to misspellings and suffix variations) we considered each document and each query indexed by all of their substrings with character length '3'.", "labels": [], "entities": []}, {"text": "Subword Match (SU).", "labels": [], "entities": [{"text": "Subword Match (SU)", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.6013050556182862}]}, {"text": "We created an index building upon the principles of the subword approach as described in Section 2.", "labels": [], "entities": []}, {"text": "Morphological segmentation yielded a shrunk index, with 39,315 index terms remaining.", "labels": [], "entities": [{"text": "Morphological segmentation", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.889354407787323}]}, {"text": "This equals a reduction rate of 78% compared with the number of text types in the collection.", "labels": [], "entities": []}, {"text": "4 The following add-ons were supplied for further parametrizing the retrieval process: Orthographic Normalization (O).", "labels": [], "entities": [{"text": "Orthographic Normalization (O)", "start_pos": 87, "end_pos": 117, "type": "METRIC", "confidence": 0.889029061794281}]}, {"text": "Ina preprocessing step, orthographic normalization rules (cf. Section 2) were applied to queries and documents.", "labels": [], "entities": []}, {"text": "Adjacency Boost (A).", "labels": [], "entities": [{"text": "Adjacency Boost (A)", "start_pos": 0, "end_pos": 19, "type": "DATASET", "confidence": 0.6606702446937561}]}, {"text": "Information about the position of each index term in the document (see above) is made available for the search process.", "labels": [], "entities": []}, {"text": "The assessment of the experimental results is based on the aggregation of all 52 selected queries on the one hand, and on a separate analysis of expert vs. layman queries, on the other hand.", "labels": [], "entities": []}, {"text": "In particular, we calculated the average interpolated precision values at fixed recall levels (we chose a continuous increment of 10%) based on the consideration of the top 200 documents retrieved.", "labels": [], "entities": [{"text": "precision", "start_pos": 54, "end_pos": 63, "type": "METRIC", "confidence": 0.7996145486831665}, {"text": "recall", "start_pos": 80, "end_pos": 86, "type": "METRIC", "confidence": 0.9956580400466919}]}, {"text": "Additionally, we provide the average of the precision values at all eleven fixed recall levels (11pt recall), and the average of the precision values at the recall levels of 20%, 50%, and 80% (3pt recall).", "labels": [], "entities": [{"text": "precision", "start_pos": 44, "end_pos": 53, "type": "METRIC", "confidence": 0.9992387294769287}, {"text": "recall levels (11pt recall)", "start_pos": 81, "end_pos": 108, "type": "METRIC", "confidence": 0.753591721256574}, {"text": "precision", "start_pos": 133, "end_pos": 142, "type": "METRIC", "confidence": 0.9985979199409485}, {"text": "recall", "start_pos": 157, "end_pos": 163, "type": "METRIC", "confidence": 0.9855565428733826}]}, {"text": "We here discuss the results from the analysis of the complete query set the data of which is given in and visualized in.", "labels": [], "entities": []}, {"text": "For our baseline (WS), the direct match between query terms and document terms, precision is already poor at low recall points  precision drops from 19.1% to 3.7%.", "labels": [], "entities": [{"text": "precision", "start_pos": 80, "end_pos": 89, "type": "METRIC", "confidence": 0.9994679093360901}, {"text": "recall", "start_pos": 113, "end_pos": 119, "type": "METRIC", "confidence": 0.9785578846931458}, {"text": "precision", "start_pos": 128, "end_pos": 137, "type": "METRIC", "confidence": 0.9892815351486206}]}, {"text": "When we take term proximity (adjacency) into account (WSA), we observe a small though statistically insignificant increase in precision at all recall points, 1.6% on average.", "labels": [], "entities": [{"text": "WSA)", "start_pos": 54, "end_pos": 58, "type": "METRIC", "confidence": 0.7813409864902496}, {"text": "precision", "start_pos": 126, "end_pos": 135, "type": "METRIC", "confidence": 0.9974991679191589}, {"text": "recall", "start_pos": 143, "end_pos": 149, "type": "METRIC", "confidence": 0.9948723912239075}]}, {"text": "Orthographic normalization only (WSO), however, caused, interestingly, a marginal decrease of precision, 0.6% on average.", "labels": [], "entities": [{"text": "Orthographic normalization only (WSO)", "start_pos": 0, "end_pos": 37, "type": "METRIC", "confidence": 0.6233897159496943}, {"text": "precision", "start_pos": 94, "end_pos": 103, "type": "METRIC", "confidence": 0.999679684638977}]}, {"text": "When both parameters, orthographic normalization and adjacency, are combined (WSAO), they produce an increase of precision at nine from eleven recall points, 2.5% on average compared with WS.", "labels": [], "entities": [{"text": "precision", "start_pos": 113, "end_pos": 122, "type": "METRIC", "confidence": 0.9990912675857544}, {"text": "recall", "start_pos": 143, "end_pos": 149, "type": "METRIC", "confidence": 0.9972572922706604}]}, {"text": "None of these differences are statistically significant when the two-tailed Wilcoxon testis applied at all eleven recall levels.", "labels": [], "entities": [{"text": "recall", "start_pos": 114, "end_pos": 120, "type": "METRIC", "confidence": 0.997581958770752}]}, {"text": "Trigram indexing (TG) yields the poorest results of all methodologies being tested.", "labels": [], "entities": [{"text": "Trigram indexing (TG)", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.7859534204006196}]}, {"text": "It is comparable to WS at low recall levels (f y g i q s r ), but at high ones its precision decreases almost dramatically.", "labels": [], "entities": [{"text": "WS", "start_pos": 20, "end_pos": 22, "type": "TASK", "confidence": 0.8278806209564209}, {"text": "recall", "start_pos": 30, "end_pos": 36, "type": "METRIC", "confidence": 0.9985993504524231}, {"text": "precision", "start_pos": 83, "end_pos": 92, "type": "METRIC", "confidence": 0.9993384480476379}]}, {"text": "Unless very high rates of misspellings are to be expected (this explains the favorable results for trigram indexing in ()) one cannot really recommend this method.", "labels": [], "entities": [{"text": "trigram indexing", "start_pos": 99, "end_pos": 115, "type": "TASK", "confidence": 0.8248153030872345}]}, {"text": "The subword approach (SU) clearly outperforms the previously discussed approaches.", "labels": [], "entities": []}, {"text": "We compare it herewith WSAO, the best-performing lexicon-free method.", "labels": [], "entities": [{"text": "WSAO", "start_pos": 23, "end_pos": 27, "type": "DATASET", "confidence": 0.7315032482147217}]}, {"text": "Within this setting, the gain in precision for SU ranges from 6.5% to 14% (f ) are in boldface in.", "labels": [], "entities": [{"text": "precision", "start_pos": 33, "end_pos": 42, "type": "METRIC", "confidence": 0.9996377229690552}, {"text": "SU", "start_pos": 47, "end_pos": 49, "type": "TASK", "confidence": 0.7753806710243225}]}, {"text": "The data for the comparison between expert and layman queries is given in the, respectively, and they are visualized in the, respectively.", "labels": [], "entities": []}, {"text": "The prima facie observation that layman recall data is higher than those of the experts is of little value, since the queries were acquired in quite different ways (cf. Section 3).", "labels": [], "entities": [{"text": "recall", "start_pos": 40, "end_pos": 46, "type": "METRIC", "confidence": 0.9457172155380249}]}, {"text": "The adjacency criterion for word index search (WSA) has no influence on the layman queries, probably because they contain fewer search terms.", "labels": [], "entities": [{"text": "word index search (WSA)", "start_pos": 28, "end_pos": 51, "type": "TASK", "confidence": 0.8098288277784983}]}, {"text": "This may also explain the poor performance of trigram search.", "labels": [], "entities": [{"text": "trigram search", "start_pos": 46, "end_pos": 60, "type": "TASK", "confidence": 0.8212380111217499}]}, {"text": "A considerably higher gain for the subword indexing approach (SU) is evident from the data for layman queries.", "labels": [], "entities": [{"text": "subword indexing approach (SU)", "start_pos": 35, "end_pos": 65, "type": "TASK", "confidence": 0.6633861611286799}]}, {"text": "Compared with WSAO, the average gain in precision amounts to 9.6% for layman queries, but only 5.6% for expert queries.", "labels": [], "entities": [{"text": "WSAO", "start_pos": 14, "end_pos": 18, "type": "DATASET", "confidence": 0.7681006789207458}, {"text": "precision", "start_pos": 40, "end_pos": 49, "type": "METRIC", "confidence": 0.9994997978210449}]}, {"text": "The difference is also obvious when we compare the statistically significant differences (\u0083 \u0088 \u0084 \u0089 \u0086 1 r ) in both tables (bold face).", "labels": [], "entities": [{"text": "statistically significant differences (\u0083 \u0088 \u0084 \u0089 \u0086 1 r )", "start_pos": 51, "end_pos": 105, "type": "METRIC", "confidence": 0.7500949881293557}]}, {"text": "This is also compatible with the finding that the rate of query result mismatches (cases where a query did not yield any document as an answer) equals zero for SU, but amounts to 8% and 29.6% for expert and laymen queries, respectively, running under the token match paradigm WS* (cf.).", "labels": [], "entities": []}, {"text": "When we compare the results for synonym class indexing (\u0090 \u0092 \u0091 ), we note a small, though statistically insignificant improvement for layman queries at some recall points.", "labels": [], "entities": [{"text": "synonym class indexing", "start_pos": 32, "end_pos": 54, "type": "TASK", "confidence": 0.821702778339386}]}, {"text": "We attribute the different re-   sults partly to the lower baseline for layman queries, partly to the probably more accentuated vocabulary mismatch between layman queries and documents using expert terminology.", "labels": [], "entities": [{"text": "re-   sults", "start_pos": 27, "end_pos": 38, "type": "METRIC", "confidence": 0.8519135117530823}]}, {"text": "However, this difference is below the level we expected.", "labels": [], "entities": []}, {"text": "In forthcoming releases of the subword dictionary in which coverage, stop word lists and synonym classes will be augmented, we hope to demonstrate the added value of the subword approach more convincingly.", "labels": [], "entities": []}, {"text": "Generalizing the interpretation of our data in the light of these findings, we recognize a substantial increase of retrieval performance when query and text tokens are segmented according to the principles of the subword model.", "labels": [], "entities": []}, {"text": "The gain is still not overwhelming.", "labels": [], "entities": []}, {"text": "With regard to orthographic normalization, we expected a higher performance benefit because of the well-known spelling problems for German medical terms of Latin or Greek origin.", "labels": [], "entities": [{"text": "orthographic normalization", "start_pos": 15, "end_pos": 41, "type": "TASK", "confidence": 0.8957644402980804}]}, {"text": "For our experiments, however, we used quite a homogeneous document collection following the spelling standards of medical publishers.", "labels": [], "entities": []}, {"text": "The same standards apparently applied to the original multiple choice questions, by which the acquisition of expert queries was guided (cf. Section 3).", "labels": [], "entities": []}, {"text": "In the layman queries, there were only few Latin or Greek terms, and, therefore, they did not take advantage of the spelling normalization.", "labels": [], "entities": []}, {"text": "However, the experience with medical text retrieval (especially on medical reports which exhibit a high rate of spelling variations) shows that orthographic normalization is a desider-  atum for enhanced retrieval quality.", "labels": [], "entities": [{"text": "medical text retrieval", "start_pos": 29, "end_pos": 51, "type": "TASK", "confidence": 0.6471530993779501}]}, {"text": "The proximity (adjacency) of search terms as a crucial parameter for output ranking proved useful, so we use it as default for subword and synonym class indexing.", "labels": [], "entities": [{"text": "proximity (adjacency)", "start_pos": 4, "end_pos": 25, "type": "METRIC", "confidence": 0.856231153011322}]}, {"text": "Whereas the usefulness of Subword Indexing became evident, we could not provide sufficient evidence for Synonym Class Indexing, so far.", "labels": [], "entities": [{"text": "Subword Indexing", "start_pos": 26, "end_pos": 42, "type": "TASK", "confidence": 0.9158098101615906}, {"text": "Synonym Class Indexing", "start_pos": 104, "end_pos": 126, "type": "TASK", "confidence": 0.8737867077191671}]}, {"text": "However, synonym mapping is still incomplete in the current state of our subword dictionary.", "labels": [], "entities": [{"text": "synonym mapping", "start_pos": 9, "end_pos": 24, "type": "TASK", "confidence": 0.935862809419632}]}, {"text": "A question we have to deal within the future is an alternative way to evaluate the comparative value of synonym class indexing.", "labels": [], "entities": [{"text": "synonym class indexing", "start_pos": 104, "end_pos": 126, "type": "TASK", "confidence": 0.8231419523557028}]}, {"text": "We have reason to believe that precision cannot betaken as the sole measure for the advantages of a query expansion in cases where the subword approach is already superior (for all layman and expert queries this method retrieved relevant documents, whereas word-based methods failed in 29.6% of the layman queries and 8% of the expert queries, cf.).", "labels": [], "entities": [{"text": "precision", "start_pos": 31, "end_pos": 40, "type": "METRIC", "confidence": 0.9991598129272461}]}, {"text": "It would be interesting to evaluate the retrieval effectiveness (in terms of precision and recall) of different versions of the synonym class indexing approach in those cases where retrieval using word or subword indexes fails due to a complete mismatch between query and documents.", "labels": [], "entities": [{"text": "precision", "start_pos": 77, "end_pos": 86, "type": "METRIC", "confidence": 0.9989257454872131}, {"text": "recall", "start_pos": 91, "end_pos": 97, "type": "METRIC", "confidence": 0.9976812601089478}]}, {"text": "This will become even more interesting when mappings of our synonym identifiers to a large medical thesaurus) are incorporated into our system.", "labels": [], "entities": []}, {"text": "Alternatively, we may think of user-centered comparative studies).", "labels": [], "entities": []}, {"text": "Before we developed our own search engine, we used the AltaVista TM Search Engine 3.0 (http:// solutions.altavista.com) as our testbed, a widely distributed, easy to install off-the-shelf IR system.", "labels": [], "entities": [{"text": "AltaVista TM Search Engine 3.0", "start_pos": 55, "end_pos": 85, "type": "DATASET", "confidence": 0.7977541089057922}]}, {"text": "For the conditions WSA, SU, and SY, we give the comparative results in.", "labels": [], "entities": [{"text": "WSA", "start_pos": 19, "end_pos": 22, "type": "DATASET", "confidence": 0.5513042211532593}]}, {"text": "The experiments were run on an earlier version of the dictionary -hence, the different results.", "labels": [], "entities": []}, {"text": "AltaVista TM yielded a superior performance for all three major test scenarios compared with our home-grown engine.", "labels": [], "entities": [{"text": "AltaVista TM", "start_pos": 0, "end_pos": 12, "type": "DATASET", "confidence": 0.9110753238201141}]}, {"text": "This is not at all surprising given all the tuning  efforts that went into AltaVista TM . The data reveals clearly that commercially available search engines comply with our indexing approach.", "labels": [], "entities": [{"text": "AltaVista TM", "start_pos": 75, "end_pos": 87, "type": "DATASET", "confidence": 0.8334672749042511}]}, {"text": "In an experimental setting, however, their use is hardly justifiable because their internal design remains hidden and, therefore, cannot be modified under experimental conditions.", "labels": [], "entities": []}, {"text": "The benefit of the subword indexing method is apparently higher for the commercial IR system.", "labels": [], "entities": [{"text": "subword indexing", "start_pos": 19, "end_pos": 35, "type": "TASK", "confidence": 0.7415276765823364}, {"text": "IR", "start_pos": 83, "end_pos": 85, "type": "TASK", "confidence": 0.9501853585243225}]}, {"text": "For AltaVista TM the average precision gain was 15.9% for SU and 11.5% for SY, whereas our simple tfidfdriven search engine gained only 5.3% for SU and 3.4% for SY.", "labels": [], "entities": [{"text": "AltaVista TM", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.8119122385978699}, {"text": "precision gain", "start_pos": 29, "end_pos": 43, "type": "METRIC", "confidence": 0.9874612987041473}]}, {"text": "Given the imbalanced benefit for both systems (other things being equal), it seems highly likely that the parameters feeding AltaVista TM profit even more from the subword approach than our simple prototype system.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Precision/Recall Table for All Queries", "labels": [], "entities": [{"text": "Precision/Recall Table", "start_pos": 10, "end_pos": 32, "type": "METRIC", "confidence": 0.6117801442742348}]}, {"text": " Table 3: Precision/Recall Table for Expert Queries", "labels": [], "entities": [{"text": "Precision/Recall", "start_pos": 10, "end_pos": 26, "type": "TASK", "confidence": 0.5139336188634237}]}, {"text": " Table 4: Precision/Recall Table for Layman Queries", "labels": [], "entities": [{"text": "Precision/Recall", "start_pos": 10, "end_pos": 26, "type": "TASK", "confidence": 0.5722600917021433}]}, {"text": " Table 5: Query / Document Mismatch", "labels": [], "entities": [{"text": "Query / Document Mismatch", "start_pos": 10, "end_pos": 35, "type": "TASK", "confidence": 0.6035946682095528}]}, {"text": " Table 6: Precision/Recall Table for Expert Queries comparing", "labels": [], "entities": [{"text": "Precision/Recall Table", "start_pos": 10, "end_pos": 32, "type": "METRIC", "confidence": 0.6323247775435448}]}]}