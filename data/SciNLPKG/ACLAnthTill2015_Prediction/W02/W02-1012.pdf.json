{"title": [{"text": "Extensions to HMM-based Statistical Word Alignment Models", "labels": [], "entities": [{"text": "HMM-based Statistical Word Alignment", "start_pos": 14, "end_pos": 50, "type": "TASK", "confidence": 0.7383560389280319}]}], "abstractContent": [{"text": "This paper describes improved HMM-based word level alignment models for statistical machine translation.", "labels": [], "entities": [{"text": "HMM-based word level alignment", "start_pos": 30, "end_pos": 60, "type": "TASK", "confidence": 0.8116702884435654}, {"text": "statistical machine translation", "start_pos": 72, "end_pos": 103, "type": "TASK", "confidence": 0.7409609258174896}]}, {"text": "We present a method for using part of speech tag information to improve alignment accuracy , and an approach to modeling fertility and correspondence to the empty word in an HMM alignment model.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 82, "end_pos": 90, "type": "METRIC", "confidence": 0.9136807322502136}, {"text": "fertility", "start_pos": 121, "end_pos": 130, "type": "METRIC", "confidence": 0.9615989923477173}, {"text": "HMM alignment", "start_pos": 174, "end_pos": 187, "type": "TASK", "confidence": 0.7769942879676819}]}, {"text": "We present accuracy results from evaluating Viterbi alignments against human-judged alignments on the Canadian Hansards corpus, as compared to a bigram HMM, and IBM model 4.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.9992110729217529}, {"text": "Canadian Hansards corpus", "start_pos": 102, "end_pos": 126, "type": "DATASET", "confidence": 0.9434345960617065}]}, {"text": "The results show up to 16% alignment error reduction .", "labels": [], "entities": [{"text": "alignment error reduction", "start_pos": 27, "end_pos": 52, "type": "METRIC", "confidence": 0.8524763186772665}]}], "introductionContent": [{"text": "The main task in statistical machine translation is to model the string translation probability as the source language string and \u00a7 as the target language string in accordance with the noisy channel terminology used in the IBM models of.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 17, "end_pos": 48, "type": "TASK", "confidence": 0.6569349865118662}]}, {"text": "Word-level translation models assume a pairwise mapping between the words of the source and target strings.", "labels": [], "entities": [{"text": "Word-level translation", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.6742933839559555}]}, {"text": "This mapping is generated by alignment models.", "labels": [], "entities": []}, {"text": "In this paper we present extensions to the HMM alignment model of ().", "labels": [], "entities": [{"text": "HMM alignment", "start_pos": 43, "end_pos": 56, "type": "TASK", "confidence": 0.9091781079769135}]}, {"text": "Some of our extensions are applicable to other alignment models as well and are of general utility.", "labels": [], "entities": []}, {"text": "For most language pairs huge amounts of parallel corpora are not readily available whereas monolingual resources such as taggers are more often available.", "labels": [], "entities": []}, {"text": "Little research has gone into exploring the po-tential of part of speech information to better model translation probabilities and permutation probabilities.", "labels": [], "entities": []}, {"text": "uses a very broad classification of words (content, function and several punctuation classes) to estimate class-specific parameters for translation models.", "labels": [], "entities": []}, {"text": "adapt English tags for Chinese language modeling using Coerced Markov Models.", "labels": [], "entities": [{"text": "Chinese language modeling", "start_pos": 23, "end_pos": 48, "type": "TASK", "confidence": 0.6506537000338236}]}, {"text": "They use English POS classes as states of the Markov Model to generate Chinese language words.", "labels": [], "entities": []}, {"text": "In this paper we use POS tag information to incorporate prior knowledge of word translation and to model local word order variation.", "labels": [], "entities": [{"text": "word translation", "start_pos": 75, "end_pos": 91, "type": "TASK", "confidence": 0.7628670036792755}]}, {"text": "We show that using this information can help in the translation modeling task.", "labels": [], "entities": [{"text": "translation modeling", "start_pos": 52, "end_pos": 72, "type": "TASK", "confidence": 0.9879709184169769}]}, {"text": "Many alignment models assume a one to many mapping from source language words to target language words, such as the IBM models 1-5 of and the HMM alignment model of depend only on the alignment of the previous word # \" # $ \u00a7 if using a first order HMM.", "labels": [], "entities": []}, {"text": "Therefore, source words are not awarded/penalized for being aligned to more than one target word.", "labels": [], "entities": []}, {"text": "We present an extension to HMM alignment that approximately models word fertility.", "labels": [], "entities": [{"text": "HMM alignment", "start_pos": 27, "end_pos": 40, "type": "TASK", "confidence": 0.797690749168396}]}, {"text": "Another assumption of existing alignment models is that there is a special Null word in the source sentence from which all target words that do not have other correspondences in the source language are generated.", "labels": [], "entities": []}, {"text": "Use of such a Null word has proven problematic in many models.", "labels": [], "entities": []}, {"text": "We also assume the existence of a special Null word in the source language that generates words in the target language.", "labels": [], "entities": []}, {"text": "However, we define a different model that better constrains and conditions generation from Null.", "labels": [], "entities": []}, {"text": "We assume that the generation probability of words by Null depends on other words in the target sentence.", "labels": [], "entities": []}, {"text": "Next we present the general equations for decomposition of the translation probability using part of speech tags and later we will go into more detail of our extensions.", "labels": [], "entities": []}, {"text": "\u0084 \u0081 \u00a5 \u00a7 without loss of generality is: gives the index of the word is aligned.", "labels": [], "entities": []}, {"text": "The models we present in this paper will differ in the decompositions of alignment probabilities, tag translation and word translation probabilities in Eqn.", "labels": [], "entities": [{"text": "tag translation", "start_pos": 98, "end_pos": 113, "type": "TASK", "confidence": 0.7774322926998138}, {"text": "word translation", "start_pos": 118, "end_pos": 134, "type": "TASK", "confidence": 0.6933759152889252}]}, {"text": "1. Section 3 describes the baseline model in more detail.", "labels": [], "entities": []}, {"text": "Section 4 illustrates examples where the baseline model performs poorly.", "labels": [], "entities": []}, {"text": "Section 5 presents our extensions and Section 6 presents experimental results.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Percentage of words in the corpus by frequency  = 1", "labels": [], "entities": []}, {"text": " Table 2: Alignment Error Rate by Model and Corpus Size  Corpus Baseline Null  SG  Tags Tags+SG Tags+Null Tags+Null+SG  5K  17.53  16.86 16.72 16.20  15.31  15.36  15.14  15K  15.03  14.29 13.52 13.90  12.63  13.22  12.52  25K  13.85  13.05 12.79 13.10  11.91  12.30  11.79  35K  13.19  11.98 12.03 12.60  11.45  11.56  11.07  50K  12.63  11.76 11.78 12.10  11.19  11.11  10.69", "labels": [], "entities": [{"text": "Alignment Error", "start_pos": 10, "end_pos": 25, "type": "TASK", "confidence": 0.9014385342597961}, {"text": "SG  5K  17.53  16.86 16.72 16.20  15.31  15.36  15.14  15K  15.03  14.29 13.52 13.90  12.63  13.22  12.52  25K  13.85  13.05 12.79 13.10  11.91  12.30  11.79  35K  13.19  11.98 12.03 12.60  11.45  11.56  11.07  50K  12.63  11.76 11.78 12.10  11.19  11.11  10.69", "start_pos": 116, "end_pos": 377, "type": "TASK", "confidence": 0.7779653973695708}]}]}