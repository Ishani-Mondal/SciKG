{"title": [{"text": "Combining Sample Selection and Error-Driven Pruning for Machine Learning of Coreference Rules", "labels": [], "entities": []}], "abstractContent": [{"text": "Most machine learning solutions to noun phrase coreference resolution recast the problem as a classification task.", "labels": [], "entities": [{"text": "noun phrase coreference resolution", "start_pos": 35, "end_pos": 69, "type": "TASK", "confidence": 0.7833874821662903}]}, {"text": "We examine three potential problems with this reformulation, namely, skewed class distributions , the inclusion of \"hard\" training instances, and the loss of transitivity inherent in the original coreference relation.", "labels": [], "entities": []}, {"text": "We show how these problems can be handled via intelligent sample selection and error-driven pruning of classification rule-sets.", "labels": [], "entities": []}, {"text": "The resulting system achieves an F-measure of 69.5 and 63.4 on the MUC-6 and MUC-7 coreference resolution data sets, respectively, surpassing the performance of the best MUC-6 and MUC-7 coreference systems.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 33, "end_pos": 42, "type": "METRIC", "confidence": 0.9995278120040894}, {"text": "MUC-6", "start_pos": 67, "end_pos": 72, "type": "DATASET", "confidence": 0.9607957601547241}, {"text": "MUC-7 coreference resolution data sets", "start_pos": 77, "end_pos": 115, "type": "DATASET", "confidence": 0.8764939904212952}]}, {"text": "In particular, the system outperforms the best-performing learning-based coreference system to date.", "labels": [], "entities": []}], "introductionContent": [{"text": "Noun phrase coreference resolution refers to the problem of determining which noun phrases (NPs) refer to each real-world entity mentioned in a document.", "labels": [], "entities": [{"text": "Noun phrase coreference resolution", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.7470696866512299}]}, {"text": "Machine learning approaches to this problem have been reasonably successful, operating primarily by recasting the problem as a classification task (e.g.,,).", "labels": [], "entities": []}, {"text": "Specifically, an inductive learning algorithm is used to train a classifier that decides whether or not two NPs in a document are coreferent.", "labels": [], "entities": []}, {"text": "Training data are typically created by relying on coreference chains from the training documents: training instances are generated by pairing each NP with each of its preceding NPs; instances are labeled as positive if the two NPs are in the same coreference chain, and labeled as negative otherwise.", "labels": [], "entities": []}, {"text": "A separate clustering mechanism then coordinates the possibly contradictory pairwise coreference classification decisions and constructs a partition on the set of NPs with one cluster for each set of coreferent NPs.", "labels": [], "entities": [{"text": "coreference classification", "start_pos": 85, "end_pos": 111, "type": "TASK", "confidence": 0.8414263427257538}]}, {"text": "Although, in principle, any clustering algorithm can be used, most previous work uses a singlelink clustering algorithm to impose coreference partitions.", "labels": [], "entities": []}, {"text": "An implicit assumption in the choice of the single-link clustering algorithm is that coreference resolution is viewed as anaphora resolution, i.e. the goal during clustering is to find an antecedent for each anaphoric NP in a document.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 85, "end_pos": 107, "type": "TASK", "confidence": 0.9395422637462616}]}, {"text": "Three intrinsic properties of coreference 4 , however, make the formulation of the problem as a classification-based single-link clustering task potentially undesirable: Coreference is a rare relation.", "labels": [], "entities": [{"text": "classification-based single-link clustering task", "start_pos": 96, "end_pos": 144, "type": "TASK", "confidence": 0.6612749844789505}]}, {"text": "That is, most NP pairs in a document are not coreferent.", "labels": [], "entities": []}, {"text": "Con-sequently, generating training instances by pairing each NP with each of its preceding NPs creates highly skewed class distributions, in which the number of positive instances is overwhelmed by the number of negative instances.", "labels": [], "entities": []}, {"text": "For example, the standard coreference data sets contain only 2% positive instances.", "labels": [], "entities": []}, {"text": "Unfortunately, learning in the presence of such skewed class distributions remains an open area of research in the machine learning community (e.g.,,,).", "labels": [], "entities": []}, {"text": "Coreference is a discourse-level problem with different solutions for different types of NPs.", "labels": [], "entities": []}, {"text": "The interpretation of a pronoun, for example, maybe dependent only on its closest antecedent and not on the rest of the members of the same coreference chain.", "labels": [], "entities": []}, {"text": "Proper name resolution, on the other hand, maybe better served by ignoring locality constraints altogether and relying on string-matching or more sophisticated aliasing techniques.", "labels": [], "entities": [{"text": "name resolution", "start_pos": 7, "end_pos": 22, "type": "TASK", "confidence": 0.8528047502040863}]}, {"text": "Consequently, generating positive instances from all pairs of NPs from the same coreference chain can potentially make the learning task harder: all but a few coreference links derived from any chain might be hard to identify based on the available contextual cues.", "labels": [], "entities": []}, {"text": "Coreference is an equivalence relation.", "labels": [], "entities": []}, {"text": "Recasting the problem as a classification task precludes enforcement of the transitivity constraint.", "labels": [], "entities": []}, {"text": "After training, for example, the classifier might determine that A is coreferent with B, and B with C, but that A and C are not coreferent.", "labels": [], "entities": []}, {"text": "Hence, the clustering mechanism is needed to coordinate these possibly contradictory pairwise classifications.", "labels": [], "entities": []}, {"text": "In addition, because the coreference classifiers are trained independent of the clustering algorithm to be used, improvements in classification accuracy do not guarantee corresponding improvements in clustering-level accuracy, i.e. overall performance on the coreference resolution task might not improve.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 144, "end_pos": 152, "type": "METRIC", "confidence": 0.8920629620552063}, {"text": "accuracy", "start_pos": 217, "end_pos": 225, "type": "METRIC", "confidence": 0.8549128174781799}, {"text": "coreference resolution task", "start_pos": 259, "end_pos": 286, "type": "TASK", "confidence": 0.895618200302124}]}, {"text": "This paper examines each of the above issues.", "labels": [], "entities": []}, {"text": "First, to address the problem of skewed class distributions, we apply a technique for negative instance selection similar to that proposed in.", "labels": [], "entities": []}, {"text": "In contrast to results reported there, however, we show empirically that system performance increases noticeably in response to negative example selection, with increases in F-measure of 3-5%.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 174, "end_pos": 183, "type": "METRIC", "confidence": 0.9991973042488098}]}, {"text": "Second, in an attempt to avoid the inclusion of \"hard\" training instances, we present a corpus-based method for implicit selection of positive instances.", "labels": [], "entities": []}, {"text": "The approach is a fully automated variant of the example selection algorithm introduced in.", "labels": [], "entities": [{"text": "example selection", "start_pos": 49, "end_pos": 66, "type": "TASK", "confidence": 0.7350604832172394}]}, {"text": "With positive example selection, system performance (F-measure) again increases, by 12-14%.", "labels": [], "entities": [{"text": "F-measure)", "start_pos": 53, "end_pos": 63, "type": "METRIC", "confidence": 0.9189924895763397}]}, {"text": "Finally, to more tightly tie the classification-and clustering-level coreference decisions, we propose an error-driven rule pruning algorithm that optimizes the coreference classifier ruleset with respect to the clustering-level coreference scoring function.", "labels": [], "entities": []}, {"text": "Overall, the use of pruning boosts system performance from an F-measure of 69.3 to 69.5, and from 57.2 to 63.4 for the MUC-6 and MUC-7 data sets, respectively, enabling the system to achieve performance that surpasses that of the best MUC coreference systems by 4.6% and 1.6%.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 62, "end_pos": 71, "type": "METRIC", "confidence": 0.9993165731430054}, {"text": "MUC-6", "start_pos": 119, "end_pos": 124, "type": "DATASET", "confidence": 0.9469579458236694}, {"text": "MUC-7 data sets", "start_pos": 129, "end_pos": 144, "type": "DATASET", "confidence": 0.8857524394989014}]}, {"text": "In particular, the system outperforms the best-performing learningbased coreference system () by 6.9% and 3.0%.", "labels": [], "entities": []}, {"text": "The remainder of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "In sections 2 and 3, we present the machine learning framework underlying the baseline coreference system and examine the effect of negative sample selection.", "labels": [], "entities": []}, {"text": "Section 4 presents our corpus-based algorithm for selection of positive instances.", "labels": [], "entities": []}, {"text": "Section 5 describes and evaluates the error-driven pruning algorithm.", "labels": [], "entities": []}, {"text": "We conclude with future work in section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "with POS-SELECT only and then with both POS-SELECT and NEG-SELECT.", "labels": [], "entities": [{"text": "NEG-SELECT", "start_pos": 55, "end_pos": 65, "type": "DATASET", "confidence": 0.7527278661727905}]}, {"text": "With POS-SELECT only, the system achieves an F-measure of 64.1 (for MUC-6) and 53.8 (for MUC-7).", "labels": [], "entities": [{"text": "POS-SELECT", "start_pos": 5, "end_pos": 15, "type": "METRIC", "confidence": 0.8376257419586182}, {"text": "F-measure", "start_pos": 45, "end_pos": 54, "type": "METRIC", "confidence": 0.9994000196456909}, {"text": "MUC-6", "start_pos": 68, "end_pos": 73, "type": "DATASET", "confidence": 0.8478677868843079}, {"text": "MUC-7", "start_pos": 89, "end_pos": 94, "type": "DATASET", "confidence": 0.82281094789505}]}, {"text": "When POS-SELECT and NEG-SELECT are used in combination, however, the system achieves an F-measure of 69.3 (for MUC-6) and 57.2 (for MUC-7).", "labels": [], "entities": [{"text": "NEG-SELECT", "start_pos": 20, "end_pos": 30, "type": "DATASET", "confidence": 0.7378987669944763}, {"text": "F-measure", "start_pos": 88, "end_pos": 97, "type": "METRIC", "confidence": 0.9992555975914001}, {"text": "MUC-6", "start_pos": 111, "end_pos": 116, "type": "DATASET", "confidence": 0.8387946486473083}, {"text": "MUC-7", "start_pos": 132, "end_pos": 137, "type": "DATASET", "confidence": 0.86701899766922}]}, {"text": "The experimental results are largely consistent with our hypothesis.", "labels": [], "entities": []}, {"text": "System performance improves dramatically with positive sample selection using POS-SELECT both in the absence and presence of negative sample selection.", "labels": [], "entities": []}, {"text": "Without negative sample selection, F-measure increases from 52.4 to 64.1 (for MUC-6), and from 41.3 to 53.8 (for MUC-7).", "labels": [], "entities": [{"text": "F-measure", "start_pos": 35, "end_pos": 44, "type": "METRIC", "confidence": 0.9987040758132935}, {"text": "MUC-6", "start_pos": 78, "end_pos": 83, "type": "DATASET", "confidence": 0.8404083251953125}, {"text": "MUC-7", "start_pos": 113, "end_pos": 118, "type": "DATASET", "confidence": 0.823803186416626}]}, {"text": "Similarly, with negative sample selection, F-measure increases from 55.2 to 69.3 (for MUC-6), and from 46.0 to 57.2 (for MUC-7).", "labels": [], "entities": [{"text": "F-measure", "start_pos": 43, "end_pos": 52, "type": "METRIC", "confidence": 0.999235987663269}, {"text": "MUC-6", "start_pos": 86, "end_pos": 91, "type": "DATASET", "confidence": 0.8550571799278259}, {"text": "MUC-7", "start_pos": 121, "end_pos": 126, "type": "DATASET", "confidence": 0.8308533430099487}]}, {"text": "In addition, our results indicate that applying both negative and positive sample selection leads to better performance than applying positive sample selection alone: Fmeasure increases from 64.1 to 69.3, and from 53.8 to 57.2 for the MUC-6 and MUC-7 data sets, respectively.", "labels": [], "entities": [{"text": "Fmeasure", "start_pos": 167, "end_pos": 175, "type": "METRIC", "confidence": 0.9997307658195496}, {"text": "MUC-6", "start_pos": 235, "end_pos": 240, "type": "DATASET", "confidence": 0.9564988613128662}, {"text": "MUC-7 data sets", "start_pos": 245, "end_pos": 260, "type": "DATASET", "confidence": 0.935995876789093}]}, {"text": "Nevertheless, reducing the number of negative instances (via negative sample selection) improves recall but damages precision: we see statistically significant gains in recall and statistically significant drops in precision for both data sets.", "labels": [], "entities": [{"text": "recall", "start_pos": 97, "end_pos": 103, "type": "METRIC", "confidence": 0.9993346333503723}, {"text": "precision", "start_pos": 116, "end_pos": 125, "type": "METRIC", "confidence": 0.999561607837677}, {"text": "recall", "start_pos": 169, "end_pos": 175, "type": "METRIC", "confidence": 0.9991156458854675}, {"text": "precision", "start_pos": 215, "end_pos": 224, "type": "METRIC", "confidence": 0.9989204406738281}]}, {"text": "In particular, precision drops precipitously from 78.0 to 55.1 for the MUC-7 data set.", "labels": [], "entities": [{"text": "precision", "start_pos": 15, "end_pos": 24, "type": "METRIC", "confidence": 0.9996753931045532}, {"text": "MUC-7 data set", "start_pos": 71, "end_pos": 85, "type": "DATASET", "confidence": 0.9745526313781738}]}, {"text": "We hypothesize that POS-SELECT does not guarantee that hard positive instances will be avoided and that the inclusion of these hard instances is responsible for the poorer precision of the system.", "labels": [], "entities": [{"text": "POS-SELECT", "start_pos": 20, "end_pos": 30, "type": "DATASET", "confidence": 0.6603218913078308}, {"text": "precision", "start_pos": 172, "end_pos": 181, "type": "METRIC", "confidence": 0.9981968998908997}]}, {"text": "Anaphors that do not have easy antecedents can never be removed automatically via the induction of new rules using POS-SELECT.", "labels": [], "entities": [{"text": "POS-SELECT", "start_pos": 115, "end_pos": 125, "type": "DATASET", "confidence": 0.782618522644043}]}, {"text": "In fact, RIPPER will possibly induce rules to handle these hard instances as long as such kind of anaphors occur sufficiently frequently in the data set relative to the number of negative instances.", "labels": [], "entities": []}, {"text": "12 Although it might be beneficial to acquire these rules at the classification level (according to the learning algorithm), they can be detrimental to system performance at the clustering level, especially if the rules cover a large number of examples with a lot of exceptions.", "labels": [], "entities": []}, {"text": "Consequently, it is necessary to know which rules are worthy of keeping at the clustering level and not the classification level.", "labels": [], "entities": []}, {"text": "We will address this issue in the next section.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Effects of sample selection and error-driven pruning.", "labels": [], "entities": []}]}