{"title": [{"text": "Manual and Automatic Evaluation of Summaries", "labels": [], "entities": [{"text": "Automatic Evaluation of Summaries", "start_pos": 11, "end_pos": 44, "type": "TASK", "confidence": 0.6991798430681229}]}], "abstractContent": [{"text": "In this paper we discuss manual and automatic evaluations of summaries using data from the Document Understanding Conference 2001 (DUC-2001).", "labels": [], "entities": [{"text": "Document Understanding Conference 2001 (DUC-2001)", "start_pos": 91, "end_pos": 140, "type": "TASK", "confidence": 0.7020171284675598}]}, {"text": "We first show the instability of the manual evaluation.", "labels": [], "entities": []}, {"text": "Specifically, the low inter-human agreement indicates that more reference summaries are needed.", "labels": [], "entities": []}, {"text": "To investigate the feasibility of automated summary evaluation based on the recent BLEU method from machine translation, we use accumulative n-gram overlap scores between system and human summaries.", "labels": [], "entities": [{"text": "summary evaluation", "start_pos": 44, "end_pos": 62, "type": "TASK", "confidence": 0.6966383159160614}, {"text": "BLEU", "start_pos": 83, "end_pos": 87, "type": "METRIC", "confidence": 0.9953563809394836}, {"text": "machine translation", "start_pos": 100, "end_pos": 119, "type": "TASK", "confidence": 0.6975264251232147}]}, {"text": "The initial results provide encouraging correlations with human judgments, based on the Spearman rank-order correlation coefficient.", "labels": [], "entities": [{"text": "Spearman rank-order correlation coefficient", "start_pos": 88, "end_pos": 131, "type": "METRIC", "confidence": 0.6133774071931839}]}, {"text": "However, relative ranking of systems needs to take into account the instability.", "labels": [], "entities": []}], "introductionContent": [{"text": "Previous efforts in large-scale evaluation of text summarization include TIPSTER SUMMAC () and the Document Understanding Conference (DUC) sponsored by the National Institute of.", "labels": [], "entities": [{"text": "text summarization", "start_pos": 46, "end_pos": 64, "type": "TASK", "confidence": 0.693798378109932}, {"text": "Document Understanding Conference (DUC)", "start_pos": 99, "end_pos": 138, "type": "TASK", "confidence": 0.7387190709511439}, {"text": "National", "start_pos": 156, "end_pos": 164, "type": "DATASET", "confidence": 0.9472434520721436}]}, {"text": "DUC aims to compile standard training and test collections that can be shared among researchers and to provide common and large scale evaluations in single and multiple document summarization for their participants.", "labels": [], "entities": [{"text": "DUC", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9535242915153503}]}, {"text": "In this paper we discuss manual and automatic evaluations of summaries using data from the Document Understanding Conference 2001.", "labels": [], "entities": [{"text": "Document Understanding Conference 2001", "start_pos": 91, "end_pos": 129, "type": "DATASET", "confidence": 0.6856927275657654}]}, {"text": "Section 2 gives a brief overview of the evaluation procedure used in and the Summary Evaluation Environment (SEE) interface used to support the DUC-2001 human evaluation protocol.", "labels": [], "entities": [{"text": "DUC-2001 human evaluation protocol", "start_pos": 144, "end_pos": 178, "type": "TASK", "confidence": 0.6359923481941223}]}, {"text": "Section 3 discusses evaluation metrics.", "labels": [], "entities": []}, {"text": "Section 4 shows the instability of manual evaluations.", "labels": [], "entities": []}, {"text": "Section 5 outlines a method of automatic summary evaluation using accumulative n-gram matching score (NAMS) and proposes a view that casts summary evaluation as a decision making process.", "labels": [], "entities": [{"text": "summary evaluation", "start_pos": 41, "end_pos": 59, "type": "TASK", "confidence": 0.643926814198494}, {"text": "accumulative n-gram matching score (NAMS)", "start_pos": 66, "end_pos": 107, "type": "METRIC", "confidence": 0.8344460768359048}, {"text": "summary evaluation", "start_pos": 139, "end_pos": 157, "type": "TASK", "confidence": 0.7962486147880554}]}, {"text": "It shows that the NAMS method is bounded and inmost cases not usable, given only a single reference summary to compare with.", "labels": [], "entities": [{"text": "NAMS", "start_pos": 18, "end_pos": 22, "type": "DATASET", "confidence": 0.7106721997261047}]}, {"text": "Section 6 discusses why this is so, illustrating various forms of mismatching between human and system summaries.", "labels": [], "entities": []}, {"text": "We conclude with lessons learned and future directions.", "labels": [], "entities": []}], "datasetContent": [{"text": "For each document or document set, one human summary was created as the 'ideal' model summary at each specified length.", "labels": [], "entities": []}, {"text": "Two other human summaries were also created at each length.", "labels": [], "entities": []}, {"text": "In addition, baseline summaries were created automatically for each length as reference points.", "labels": [], "entities": []}, {"text": "For the multi-document summarization task, one baseline, lead baseline, took the first 50, 100, 200, and 400 words in the last document in the collection.", "labels": [], "entities": []}, {"text": "A second baseline, coverage baseline, took the first sentence in the first document, the first sentence in the second document and soon until it had a summary of 50, 100, 200, or 400 words.", "labels": [], "entities": [{"text": "coverage", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9415697455406189}]}, {"text": "Only one baseline (baseline1) was created for the single document summarization task.", "labels": [], "entities": [{"text": "single document summarization task", "start_pos": 50, "end_pos": 84, "type": "TASK", "confidence": 0.6276254057884216}]}, {"text": "NIST assessors who created the 'ideal' written summaries did pairwise comparisons of their summaries to the system-generated summaries, other assessors' summaries, and baseline summaries.", "labels": [], "entities": [{"text": "NIST", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9349977374076843}]}, {"text": "They used the Summary Evaluation Environment (SEE) 2.0 developed by one of the authors (Lin 2001) to support the process.", "labels": [], "entities": []}, {"text": "Using SEE, the assessors compared the system's text (the peer text) to the ideal (the model text).", "labels": [], "entities": [{"text": "SEE", "start_pos": 6, "end_pos": 9, "type": "METRIC", "confidence": 0.7416700124740601}]}, {"text": "As shown in, each text was decomposed into a list of units and displayed in separate windows.", "labels": [], "entities": []}, {"text": "In DUC-2001 the sentence was used as the smallest unit of evaluation.", "labels": [], "entities": [{"text": "DUC-2001", "start_pos": 3, "end_pos": 11, "type": "DATASET", "confidence": 0.8997402787208557}]}, {"text": "SEE 2.0 provides interfaces for assessors to judge both the content and the quality of summaries.", "labels": [], "entities": [{"text": "SEE 2.0", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.6989935040473938}]}, {"text": "To measure content, assessors step through each model unit, mark all system units sharing content with the current model unit (shown in green highlight in the model summary window), and specify that the marked system units express all, most, some or hardly any of the content of the current model unit.", "labels": [], "entities": []}, {"text": "To measure quality, assessors rate grammaticality 1 , cohesion 2 , and coherence 3 at five different levels: all, most, some, hardly any, or none.", "labels": [], "entities": []}, {"text": "For example, as shown in, an assessor marked system units 1.1 and 10.4 (shown in red underlines) as sharing some content with the current model unit 2.2 (highlighted green).", "labels": [], "entities": []}, {"text": "One goal of DUC-2001 was to debug the evaluation procedures and identify stable metrics that could serve as common reference points.", "labels": [], "entities": [{"text": "DUC-2001", "start_pos": 12, "end_pos": 20, "type": "DATASET", "confidence": 0.8450132608413696}]}, {"text": "NIST did not define any official performance metric in DUC-2001.", "labels": [], "entities": [{"text": "NIST", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9851491451263428}, {"text": "DUC-2001", "start_pos": 55, "end_pos": 63, "type": "DATASET", "confidence": 0.9698693752288818}]}, {"text": "It released the raw evaluation results to DUC-2001 participants and encouraged them to propose metrics that would help progress the field.", "labels": [], "entities": [{"text": "DUC-2001", "start_pos": 42, "end_pos": 50, "type": "DATASET", "confidence": 0.8990105390548706}]}, {"text": "Inspired by recent progress in automatic evaluation of machine translation (BLEU;, we would like to apply the same idea in the evaluation of summaries.", "labels": [], "entities": [{"text": "evaluation of machine translation", "start_pos": 41, "end_pos": 74, "type": "TASK", "confidence": 0.5705329701304436}, {"text": "BLEU", "start_pos": 76, "end_pos": 80, "type": "METRIC", "confidence": 0.9936384558677673}]}, {"text": "Following BLEU, we used the automatically computed accumulative n-gram matching scores (NAMS) between a model unit (MU) and a system summary (S) 4 as performance indicator, considering multi-document summaries.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9847618341445923}, {"text": "accumulative n-gram matching scores (NAMS)", "start_pos": 51, "end_pos": 93, "type": "METRIC", "confidence": 0.784794922385897}]}, {"text": "Only content words were used in forming n-grams.", "labels": [], "entities": []}, {"text": "NAMS is defined as follows: The whole system summary was used to compute NAMS against a model unit.", "labels": [], "entities": [{"text": "NAMS", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8315748572349548}]}, {"text": "C1: a 1 = 1 and a 2 = a 3 = a 4 = 0; C2: a 1 = 1/3, a 2 = 2/3, and a 3 = a 4 = 0; C3: a 1 = 1/6, a 2 = 2/6, a 3 = 3/6, and a 4 = 0; C1 is simply unigram matching.", "labels": [], "entities": []}, {"text": "C2 and C3 give more credit to longer n-gram matches.", "labels": [], "entities": []}, {"text": "To examine the effect of stemmers in helping the ngram matching, we also tested all configurations with two different stemmers (Lovin's and Porter's).", "labels": [], "entities": [{"text": "ngram matching", "start_pos": 49, "end_pos": 63, "type": "TASK", "confidence": 0.7177591174840927}]}, {"text": "shows the results with and without using stemmers and their Spearman rank-order correlation coefficients (rho) compared against the original retention ranking from.", "labels": [], "entities": [{"text": "Spearman rank-order correlation coefficients (rho)", "start_pos": 60, "end_pos": 110, "type": "METRIC", "confidence": 0.7895775863102504}]}, {"text": "X-nG is configuration n without using any stemmer, L-nG with the Lovin stemmer, and P-nG with the Porter stemmer.", "labels": [], "entities": []}, {"text": "The results in indicate that unigram matching provides a good approximation, but the best correlation is achieved using C2 with the Porter stemmer.", "labels": [], "entities": [{"text": "unigram matching", "start_pos": 29, "end_pos": 45, "type": "TASK", "confidence": 0.7375920116901398}]}, {"text": "Using stemmers did improve correlation.", "labels": [], "entities": [{"text": "correlation", "start_pos": 27, "end_pos": 38, "type": "METRIC", "confidence": 0.9331145286560059}]}, {"text": "Notice that rank inversion remains within the performance groups identified in Section 4.", "labels": [], "entities": []}, {"text": "For example, the retention ranking of Baseline1, U, and Wis 14, 16, and 15 respectively.", "labels": [], "entities": [{"text": "retention", "start_pos": 17, "end_pos": 26, "type": "METRIC", "confidence": 0.9961038827896118}, {"text": "Baseline1", "start_pos": 38, "end_pos": 47, "type": "DATASET", "confidence": 0.8489779829978943}]}, {"text": "The P-2G ranking of these three systems is 15, 14, and 16.", "labels": [], "entities": []}, {"text": "The only system crossing performance groups is Y.", "labels": [], "entities": []}, {"text": "Y should be grouped with N and T but the automatic evaluations place it lower, in the group with Baseline2, L, and P.", "labels": [], "entities": []}, {"text": "The primary reason for Y's behavior maybe that its summaries consist mainly of headlines, whose abbreviated style differs from the language models derived from normal newspaper text.", "labels": [], "entities": []}, {"text": "For comparison, we also ran IBM's BLEU evaluation script 5 over the same model and system summary set.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 34, "end_pos": 38, "type": "METRIC", "confidence": 0.8427903056144714}]}, {"text": "The Spearman rank-order correlation coefficient (\u03c1) for the single document task is 0.66 using one reference summary and 0.82 using three reference summaries; while Spearman \u03c1 for the multidocument task is 0.67 using one reference and 0.70 using three.", "labels": [], "entities": [{"text": "rank-order correlation coefficient (\u03c1)", "start_pos": 13, "end_pos": 51, "type": "METRIC", "confidence": 0.8607513755559921}]}], "tableCaptions": [{"text": " Table 1. Pairwise relative system performance  (single document summarization task). 1 2 .4  0", "labels": [], "entities": []}]}