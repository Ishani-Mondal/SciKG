{"title": [{"text": "Learning a Translation Lexicon from Monolingual Corpora", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper presents work on the task of constructing a word-level translation lexicon purely from unrelated mono-lingual corpora.", "labels": [], "entities": [{"text": "word-level translation lexicon", "start_pos": 55, "end_pos": 85, "type": "TASK", "confidence": 0.7474173406759897}]}, {"text": "We combine various clues such as cognates, similar context, preservation of word similarity , and word frequency.", "labels": [], "entities": []}, {"text": "Experimental results for the construction of a German-English noun lexicon are reported.", "labels": [], "entities": []}, {"text": "Noun translation accuracy of 39% scored against a parallel test corpus could be achieved.", "labels": [], "entities": [{"text": "Noun translation", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.6821672916412354}, {"text": "accuracy", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.9621533155441284}]}], "introductionContent": [{"text": "Recently, there has been a surge in research in machine translation that is based on empirical methods.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 48, "end_pos": 67, "type": "TASK", "confidence": 0.8195494115352631}]}, {"text": "The seminal work by at IBM on the Candide system laid the foundation for much of the current work in Statistical Machine Translation (SMT).", "labels": [], "entities": [{"text": "Statistical Machine Translation (SMT)", "start_pos": 101, "end_pos": 138, "type": "TASK", "confidence": 0.8946137527624766}]}, {"text": "Some of this work has been re-implemented and is freely available for research purposes.", "labels": [], "entities": []}, {"text": "Roughly speaking, SMT divides the task of translation into two steps: a word-level translation model and a model for word reordering during the translation process.", "labels": [], "entities": [{"text": "SMT", "start_pos": 18, "end_pos": 21, "type": "TASK", "confidence": 0.9941949248313904}, {"text": "word-level translation", "start_pos": 72, "end_pos": 94, "type": "TASK", "confidence": 0.6657392084598541}]}, {"text": "The statistical models are trained on parallel corpora: large amounts of text in one language along with their translation in another.", "labels": [], "entities": []}, {"text": "Various parallel texts have recently become available, mostly from government sources such as parliament proceedings (the Canadian Hansard, the minutes of the European parliament 1 ) or law texts (from Hong Kong).", "labels": [], "entities": [{"text": "Canadian Hansard", "start_pos": 122, "end_pos": 138, "type": "DATASET", "confidence": 0.7561069428920746}]}, {"text": "Still, for most language pairs, parallel texts are hard to come by.", "labels": [], "entities": []}, {"text": "This is clearly the case for low-density languages such as Tamil, Swahili, or Tetun.", "labels": [], "entities": []}, {"text": "Furthermore, texts derived from parliament speeches may not be appropriate fora particular targeted domain.", "labels": [], "entities": []}, {"text": "Specific parallel texts can be constructed by hand for the purpose of training an SMT system, but this is a very costly endeavor.", "labels": [], "entities": [{"text": "SMT", "start_pos": 82, "end_pos": 85, "type": "TASK", "confidence": 0.9921208620071411}]}, {"text": "On the other hand, the digital revolution and the wide-spread use of the World Wide Web have proliferated vast amounts of monolingual corpora.", "labels": [], "entities": []}, {"text": "Publishing text in one language is a much more natural human activity than producing parallel texts.", "labels": [], "entities": []}, {"text": "To illustrate this point: The worldwide web alone contains currently over two billion pages, a number that is still growing exponentially.", "labels": [], "entities": []}, {"text": "According to Google, 2 the word directory occurs 61 million times, empathy 383,000 times, and reflex 787,000 times.", "labels": [], "entities": [{"text": "reflex", "start_pos": 94, "end_pos": 100, "type": "METRIC", "confidence": 0.9773553609848022}]}, {"text": "In the Hansard, each of these words occurs only once.", "labels": [], "entities": [{"text": "Hansard", "start_pos": 7, "end_pos": 14, "type": "DATASET", "confidence": 0.9696975350379944}]}, {"text": "The objective of this research to build a translation lexicon solely from monolingual corpora.", "labels": [], "entities": []}, {"text": "Specifically, we want to automatically generate a one-to-one mapping of German and English nouns.", "labels": [], "entities": []}, {"text": "We are testing our mappings against a bilingual lexicon of 9,206 German and 10,645 English nouns.", "labels": [], "entities": []}, {"text": "The two monolingual corpora should be in a fairly comparable domain.", "labels": [], "entities": []}, {"text": "For our experiments we use the 1990-1992 Wall Street Journal corpus on the English side and the 1995-1996 German news wire (DPA) corpus on the German side.", "labels": [], "entities": [{"text": "Wall Street Journal corpus", "start_pos": 41, "end_pos": 67, "type": "DATASET", "confidence": 0.9064759016036987}, {"text": "German news wire (DPA) corpus", "start_pos": 106, "end_pos": 135, "type": "DATASET", "confidence": 0.7216081023216248}]}, {"text": "Both corpora are news sources in the general sense.", "labels": [], "entities": []}, {"text": "However, they span different time periods and have a different orientation: the World Street Journal covers mostly business news, the German news wire mostly German politics.", "labels": [], "entities": [{"text": "World Street Journal", "start_pos": 80, "end_pos": 100, "type": "DATASET", "confidence": 0.9820631146430969}]}, {"text": "For experiments on training probabilistic translation lexicons from parallel corpora and similar tasks on the same test corpus, refer to our earlier work].", "labels": [], "entities": [{"text": "training probabilistic translation lexicons from parallel corpora", "start_pos": 19, "end_pos": 84, "type": "TASK", "confidence": 0.742744675704411}]}], "datasetContent": [{"text": "We are trying to build a one-to-one GermanEnglish translation lexicon for the use in a machine translation system.", "labels": [], "entities": [{"text": "GermanEnglish translation lexicon", "start_pos": 36, "end_pos": 69, "type": "TASK", "confidence": 0.68427046140035}, {"text": "machine translation", "start_pos": 87, "end_pos": 106, "type": "TASK", "confidence": 0.71516452729702}]}, {"text": "To evaluate this performance we use two different measurements: Firstly, we record how many correct word-pairs we have constructed.", "labels": [], "entities": []}, {"text": "This is done by checking the generated wordpairs against an existing bilingual lexicon.", "labels": [], "entities": []}, {"text": "In essence, we try to recreate this lexicon, which contains 9,206 distinct German and 10,645 distinct English nouns and 19,782 lexicon entries.", "labels": [], "entities": []}, {"text": "For a machine translation system, it is often more important to get more frequently used words right than obscure ones.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 6, "end_pos": 25, "type": "TASK", "confidence": 0.7334494292736053}]}, {"text": "Thus, our second evaluation measurement tests the word translations proposed by the acquired lexicon against the actual word-level translations in a 5,000 sentence aligned parallel corpus.", "labels": [], "entities": []}, {"text": "The starting point to extending the lexicon is the seed lexicon of identically spelled words, as described in Section 2.1.", "labels": [], "entities": []}, {"text": "It consists of 1339 entries, of which are (88.9%) correct according to the existing bilingual lexicon.", "labels": [], "entities": []}, {"text": "Due to computational constraints, we focus on the additional mapping of only 1,000 German and English words.", "labels": [], "entities": []}, {"text": "These 1,000 words are chosen from the 1,000 most frequent lexicon entries in the dictionary, without duplications of words.", "labels": [], "entities": []}, {"text": "This frequency is defined by the sum of two word frequencies of the words in the entry, as found in the monolingual corpora.", "labels": [], "entities": []}, {"text": "We did not collect statistics of the actual use of lexical entries in, say, a parallel corpus.", "labels": [], "entities": []}, {"text": "Ina different experimental set-up we also simply tried to match the 1,000 most frequent German words with the 1,000 most frequent English words.", "labels": [], "entities": []}, {"text": "The results do not differ significantly.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: While identical 3- letter words are only translations of each other  60% of the time, this is true for 98% of 10-letter  words. Clearly, for shorter words, the acciden- tal existence of an identically spelled word in  the other language word is much higher. This  includes words such as fee, ton, art, and tag.", "labels": [], "entities": [{"text": "acciden- tal existence", "start_pos": 170, "end_pos": 192, "type": "METRIC", "confidence": 0.9488234221935272}]}, {"text": " Table 1: Testing the assumption that identically  spelled words are in fact translations of each  other: The accuracy of this assumption depends  highly on the length of the words (see Section  2.1)", "labels": [], "entities": [{"text": "accuracy", "start_pos": 110, "end_pos": 118, "type": "METRIC", "confidence": 0.9994681477546692}]}, {"text": " Table 5: Overview of results. We evaluate how  many correct lexicon entries where added (En- tries), and how well the resulting translation  lexicon performs compared to the actual word- level translations in a parallel corpus (Corpus).", "labels": [], "entities": []}]}