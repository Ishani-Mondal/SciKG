{"title": [{"text": "An Evaluation of Procedural Instructional Text", "labels": [], "entities": [{"text": "Procedural Instructional Text", "start_pos": 17, "end_pos": 46, "type": "TASK", "confidence": 0.7989793221155802}]}], "abstractContent": [{"text": "This paper presents an evaluation of the instructional text generated by Isolde, an authoring tool for technical writers that automates the production of procedural on-line help.", "labels": [], "entities": []}, {"text": "The evaluation compares the effectiveness of the instructional text produced by Isolde with that of professionally authored instructions, such as MS Word Help.", "labels": [], "entities": [{"text": "MS Word Help", "start_pos": 146, "end_pos": 158, "type": "DATASET", "confidence": 0.7312495311101278}]}, {"text": "The results suggest that the documentation produced by Isolde is of comparable quality to similar texts found in commercial manuals.", "labels": [], "entities": []}], "introductionContent": [{"text": "Instructional text is a useful and relatively constrained sub-language and has thus been a popular target for research-oriented natural language generation (NLG) systems.", "labels": [], "entities": [{"text": "natural language generation (NLG)", "start_pos": 128, "end_pos": 161, "type": "TASK", "confidence": 0.8246472080548605}]}, {"text": "Much work has been done in this area, e.g.,, demonstrating that existing technology is adequate for generating draft instructions.", "labels": [], "entities": []}, {"text": "However, only a few of these projects have been formally evaluated, e.g.,, and the evaluations performed have focussed on the fluency and grammaticality of the output text rather than on its effectiveness.", "labels": [], "entities": []}, {"text": "This tends to be the case, in fact, for evaluations of NLG systems in general.", "labels": [], "entities": []}, {"text": "People are asked to rate the acceptability of the generated texts or to compare them to human-authored texts -e.g.,), without measuring the actual impact of the texts on their intended users.", "labels": [], "entities": []}, {"text": "The evaluation of the STOP system) is a notable exception to this trend.", "labels": [], "entities": [{"text": "STOP", "start_pos": 22, "end_pos": 26, "type": "TASK", "confidence": 0.6856851577758789}]}, {"text": "STOP produced texts tailored to individual smokers intended to convince them to stop smoking.", "labels": [], "entities": [{"text": "STOP", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.676409900188446}]}, {"text": "As an evaluation, the researchers performed a largescale study of how effective the generated texts were at achieving this goal.", "labels": [], "entities": []}, {"text": "Rather than checking the output text for errors, or comparing its fluency with that of hand-written text, they compared how often readers of STOP's individually tailored texts actually stopped smoking as compared to readers of generic, generated texts.", "labels": [], "entities": []}, {"text": "Thus, the evaluation assessed the relative effectiveness of tailored and generic texts at achieving their intended goals.", "labels": [], "entities": []}, {"text": "In our evaluation, we also sought to perform an evaluation of the effectiveness of the instructional texts produced by Isolde.", "labels": [], "entities": []}, {"text": "Isolde 1 is an authoring tool for technical writers that automatically generates parts of a system's on-line help.", "labels": [], "entities": []}, {"text": "In this domain, the writer's goal is to help the users achieve their goals.", "labels": [], "entities": []}, {"text": "It is thus crucial to assess the effectiveness of the instructional texts in areal task.", "labels": [], "entities": []}, {"text": "Unlike the STOP evaluation, however, we compared the effectiveness of our generated texts with that of human-authored texts.", "labels": [], "entities": [{"text": "STOP", "start_pos": 11, "end_pos": 15, "type": "TASK", "confidence": 0.7951533794403076}]}, {"text": "In this paper, we first introduce the type of texts that Isolde generates and give an overview of Isolde.", "labels": [], "entities": []}, {"text": "We then present the evaluation we conducted and draw some conclusions.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our experiment attempted to assess the effectiveness of the help generated by Isolde as compared with manually authored help.", "labels": [], "entities": []}, {"text": "For both types of texts, we: \u2022 Measured the user's performance in accomplishing a specific task (i.e., task achievement and time needed); and \u2022 Asked the users to rate the usefulness of the texts, the adequacy of the content and the coherence of the organization.", "labels": [], "entities": []}, {"text": "We did not evaluate grammatical correctness as in AGILE ().", "labels": [], "entities": [{"text": "AGILE", "start_pos": 50, "end_pos": 55, "type": "METRIC", "confidence": 0.5545197129249573}]}, {"text": "Given the aim of documentation, we need to ensure that the generated instructions allow the users to achieve or learn about their task, whatever the quality or the complexity of the text generated.", "labels": [], "entities": []}, {"text": "Our methodology involved four steps: (1) choosing three tasks in Word; (2) designing the three corresponding task models; (3) producing the on-line help with Isolde for these models, and (4) evaluating the help on two user groups: one group received the Word help, the other the automatically generated help.", "labels": [], "entities": []}, {"text": "Participants to the experiment were not aware of which help they were using.", "labels": [], "entities": []}, {"text": "Task Selection: To compare the effectiveness of Isolde help with that of manually authored help, we asked users to perform areal task, using the online help as a resource.", "labels": [], "entities": []}, {"text": "We decided to work with Word, as it provides both a task environment and on-line instructions.", "labels": [], "entities": [{"text": "Word", "start_pos": 24, "end_pos": 28, "type": "DATASET", "confidence": 0.9650534987449646}]}, {"text": "Thus, our aim was to select 3 tasks, 2 simple and 1 complex 3 that would be new for the subjects, to help prevent introducing a bias based on prior knowledge and encourage users to read the help.", "labels": [], "entities": []}, {"text": "We also chose tasks such that their Word help text was \"self-contained\" (i.e., without extensive reference to other parts of the help) and the text generated by Isolde for the task would be of similar reading complexity.", "labels": [], "entities": []}, {"text": "Our final constraint was that the 2 simple tasks had the same number of elementary actions.", "labels": [], "entities": []}, {"text": "We had no prior assumption as to what task would be easier to model or document than any other task.", "labels": [], "entities": []}, {"text": "With these constraints, we chose: \u2022 Task 1: create a document template and save it in a specific directory; \u2022 Task 2: create index entries; \u2022 Task 3: create mailing labels by merging a label template with an address list, and save the label pattern.", "labels": [], "entities": []}, {"text": "Task Design: To generate the on-line help for to these tasks, we first had to design the task models.", "labels": [], "entities": [{"text": "Task Design", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.7253613173961639}]}, {"text": "We did so using Tamot.", "labels": [], "entities": []}, {"text": "As typically done by technical writers, we executed the tasks step by step, recording all the steps.", "labels": [], "entities": []}, {"text": "Feedback expressions (e.g., display of windows, confirmation messages) were also included, either as system actions, or as notes or warnings with canned text.", "labels": [], "entities": []}, {"text": "The aim was to be as close as possible to the system behavior.", "labels": [], "entities": []}, {"text": "Hypertext Generation: When the task model was completed, we generated the corresponding on-line help.", "labels": [], "entities": [{"text": "Hypertext Generation", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8032353818416595}]}, {"text": "We then used the).", "labels": [], "entities": []}, {"text": "This was to ensure both texts would be of similar reading complexity and would thus involve the same amount of time to consult.", "labels": [], "entities": []}, {"text": "For the experiment, only one of the help texts was accessible to the user, displayed in a Netscape browser to preserve anonymity.", "labels": [], "entities": []}, {"text": "Formation of Subject Groups: A total of 35 subjects did the experiment (3 tasks per subject), split into 2 groups.", "labels": [], "entities": []}, {"text": "Subjects were randomly assigned to a group, but we ensured an even number of men and women in each group.", "labels": [], "entities": []}, {"text": "The subjects were not expert in Word, but they knew how to use the software.", "labels": [], "entities": [{"text": "Word", "start_pos": 32, "end_pos": 36, "type": "DATASET", "confidence": 0.8860920071601868}]}, {"text": "The experiment consisted of asking subjects to perform the 3 tasks described above with Word.", "labels": [], "entities": [{"text": "Word", "start_pos": 88, "end_pos": 92, "type": "DATASET", "confidence": 0.916655421257019}]}, {"text": "For each task, they were given some directions as to what was expected of them (e.g., create a template with the CSIRO logo, and save it in a specific directory).", "labels": [], "entities": [{"text": "CSIRO logo", "start_pos": 113, "end_pos": 123, "type": "DATASET", "confidence": 0.9062851667404175}]}, {"text": "The directions did not include explanations on how to achieve the task.", "labels": [], "entities": []}, {"text": "These were to be found in the on-line help provided.", "labels": [], "entities": []}, {"text": "Subjects could consult the help at anytime (i.e., before or while performing the task).", "labels": [], "entities": []}, {"text": "Subjects were told to read the directions and ask for clarification if required before starting on the task.", "labels": [], "entities": []}, {"text": "After each task, they filled out a questionnaire asking them to rate the help they used.", "labels": [], "entities": []}, {"text": "The questionnaire aimed at evaluating the usefulness of the help, the quality of its content (i.e., its quantity and relevance) and the coherence of its organization.", "labels": [], "entities": []}, {"text": "The questions asked and the factors of acceptability that they rate are shown in.", "labels": [], "entities": []}, {"text": "Each question was answered using a six-point scale, assigning letter grades A (high) through F (low).", "labels": [], "entities": []}, {"text": "These letters were later converted into digits from 6 to 1 for the statistical analysis.", "labels": [], "entities": [{"text": "statistical analysis", "start_pos": 67, "end_pos": 87, "type": "TASK", "confidence": 0.9158088564872742}]}, {"text": "The questionnaire also included questions that checked the users' previous level of familiarity with the task.", "labels": [], "entities": []}, {"text": "During the experiment, we recorded the time to measure the user's performance.", "labels": [], "entities": []}, {"text": "We limited the allowable time (10 minutes for the simple tasks and 15 minutes for the complex task) to encourage the users to consult the help instead of exploring the application by themselves . We observed whether subjects consulted the help or not, and the number of times they did so.", "labels": [], "entities": []}, {"text": "Finally, to evaluate the success rate on each task, we recorded the errors made.", "labels": [], "entities": []}, {"text": "The marking scale was set as follows: \u2022 For Task 1, the subject lost one point if the document was saved in the wrong directory and two points if it was not saved in the template format.", "labels": [], "entities": []}, {"text": "\u2022 For Task 2, the subject lost one point for each index entry that they marked incorrectly.", "labels": [], "entities": []}, {"text": "\u2022 For Task 3, the subject lost one point if the mailing labels were not created, and another point if the label pattern was not saved.", "labels": [], "entities": []}, {"text": "Group 1 was assigned Word help for Tasks 1 and 3, and Isolde help for Task 2, while Group 2 was assigned Isolde help for Task 1 and 3, and Word for Task 2.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Comparison of Word and Isolde readability score", "labels": [], "entities": []}, {"text": " Table 3: Time Performance (in seconds)", "labels": [], "entities": [{"text": "Time", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9347934126853943}]}, {"text": " Table 4: Anova result combining all the tasks", "labels": [], "entities": [{"text": "Anova", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.974452555179596}]}]}