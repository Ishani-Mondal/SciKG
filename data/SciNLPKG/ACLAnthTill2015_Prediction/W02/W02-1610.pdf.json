{"title": [{"text": "Learning Domain-Specific Transfer Rules: An Experiment with Korean to English Translation", "labels": [], "entities": [{"text": "Korean to English Translation", "start_pos": 60, "end_pos": 89, "type": "TASK", "confidence": 0.547421433031559}]}], "abstractContent": [{"text": "We describe the design of an MT system that employs transfer rules induced from parsed bitexts and present evaluation results.", "labels": [], "entities": [{"text": "MT", "start_pos": 29, "end_pos": 31, "type": "TASK", "confidence": 0.9914916157722473}]}, {"text": "The system learns lexico-structural transfer rules using syntactic pattern matching, statistical co-occurrence and error-driven filtering.", "labels": [], "entities": [{"text": "syntactic pattern matching", "start_pos": 57, "end_pos": 83, "type": "TASK", "confidence": 0.6711472074190775}]}, {"text": "In an experiment with domain-specific Korean to English translation, the approach yielded substantial improvements over three base-line systems.", "labels": [], "entities": [{"text": "domain-specific Korean to English translation", "start_pos": 22, "end_pos": 67, "type": "TASK", "confidence": 0.5960781514644623}]}], "introductionContent": [{"text": "In this paper, we describe the design of an MT system that employs transfer rules induced from parsed bitexts and present evaluation results for Korean to English translation.", "labels": [], "entities": [{"text": "MT", "start_pos": 44, "end_pos": 46, "type": "TASK", "confidence": 0.9925878643989563}, {"text": "Korean to English translation", "start_pos": 145, "end_pos": 174, "type": "TASK", "confidence": 0.5729077383875847}]}, {"text": "Our approach is based on lexico-structural transfer, and extends recent work reported in () about Korean to English transfer in particular.", "labels": [], "entities": [{"text": "lexico-structural transfer", "start_pos": 25, "end_pos": 51, "type": "TASK", "confidence": 0.7715034186840057}]}, {"text": "Whereas Han et al. focus on high quality domainspecific translation using handcrafted transfer rules, in this work we instead focus on automating the acquisition of such rules.", "labels": [], "entities": []}, {"text": "The proposed approach is inspired by examplebased machine translation (EBMT; and is similar to the recent works of) and () where transfer rules are also derived after aligning the source and target nodes of corresponding parses.", "labels": [], "entities": [{"text": "examplebased machine translation", "start_pos": 37, "end_pos": 69, "type": "TASK", "confidence": 0.6669323146343231}]}, {"text": "However, while) and () only consider parses and rules with lexical labels and syntactic roles, our approach uses parses containing any syntactic information provided by parsers (lexical labels, syntactic roles, tense, number, person, etc.), and derives rules consisting of any source and target tree sub-patterns matching a subset of the parse features.", "labels": [], "entities": []}, {"text": "A more detailed description of the differences can be found in).", "labels": [], "entities": []}], "datasetContent": [{"text": "For the automatic evaluation, we used the Bleu metric from IBM ().", "labels": [], "entities": [{"text": "Bleu metric", "start_pos": 42, "end_pos": 53, "type": "METRIC", "confidence": 0.9225755631923676}, {"text": "IBM", "start_pos": 59, "end_pos": 62, "type": "DATASET", "confidence": 0.7776191234588623}]}, {"text": "The Bleu metric combines several modified N-gram precision measures (N = 1 to 4), and uses brevity penalties to penalize translations that are shorter than the reference sentences.", "labels": [], "entities": [{"text": "Bleu", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.8962990641593933}, {"text": "precision", "start_pos": 49, "end_pos": 58, "type": "METRIC", "confidence": 0.8638207316398621}]}, {"text": "shows the Bleu N-gram precision scores for each of the four systems.", "labels": [], "entities": [{"text": "Bleu N-gram precision scores", "start_pos": 10, "end_pos": 38, "type": "METRIC", "confidence": 0.7859896421432495}]}, {"text": "Our system (Lex+Induced) had better precision scores than each of the baseline systems, except in the case of 4-grams, where it slightly trailed Babelfish.", "labels": [], "entities": [{"text": "precision scores", "start_pos": 36, "end_pos": 52, "type": "METRIC", "confidence": 0.9735490083694458}, {"text": "Babelfish", "start_pos": 145, "end_pos": 154, "type": "DATASET", "confidence": 0.9166298508644104}]}, {"text": "The statistical baseline system (GIZA++/RW) performed poorly, as might have been expected given the small amount of training data.", "labels": [], "entities": [{"text": "GIZA++/RW)", "start_pos": 33, "end_pos": 43, "type": "METRIC", "confidence": 0.7844825237989426}]}, {"text": "shows the Bleu overall precision scores.", "labels": [], "entities": [{"text": "Bleu overall", "start_pos": 10, "end_pos": 22, "type": "METRIC", "confidence": 0.8005017936229706}, {"text": "precision", "start_pos": 23, "end_pos": 32, "type": "METRIC", "confidence": 0.6732984185218811}]}, {"text": "Our system (Lex+Induced) improved substantially over both the Lex Only and Babelfish baseline systems.", "labels": [], "entities": []}, {"text": "The score for the statistical baseline system (GIZA++/RW) is not meaningful, due to the absence of 3-gram and 4-gram matches.: Bleu overall precision scores  For the human evaluation, we asked two English native speakers to rank the quality of the translation results produced by the Babelfish, Lex Only and Lex+Induced, with preference given to fidelity over fluency.", "labels": [], "entities": [{"text": "GIZA++/RW)", "start_pos": 47, "end_pos": 57, "type": "METRIC", "confidence": 0.9541515111923218}, {"text": "Bleu overall", "start_pos": 127, "end_pos": 139, "type": "METRIC", "confidence": 0.9286501705646515}, {"text": "precision", "start_pos": 140, "end_pos": 149, "type": "METRIC", "confidence": 0.5296773314476013}]}, {"text": "(The translation results of the statistical system were not yet available when the evaluation was performed.)", "labels": [], "entities": [{"text": "translation", "start_pos": 5, "end_pos": 16, "type": "TASK", "confidence": 0.9447314739227295}]}, {"text": "A rank of 1 was assigned to the best translation, a rank of two to the second best and a rank of 3 to the third, with ties allowed.", "labels": [], "entities": []}, {"text": "shows the pairwise comparisons of the three systems.", "labels": [], "entities": []}, {"text": "The top section indicates that the Babelfish and Lex Only baseline systems are essentially tied, with neither system preferred more frequently than the other.", "labels": [], "entities": [{"text": "Lex Only baseline", "start_pos": 49, "end_pos": 66, "type": "DATASET", "confidence": 0.8359545469284058}]}, {"text": "In contrast, the middle and bottom sections show that our system improves substantially over both baseline systems; most strikingly, our system (Lex+Induced) was preferred almost 20% more frequently than the Babelfish baseline (46% to 27%, with ties 27% of the time", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Average sizes for sentences and parses in  corpus", "labels": [], "entities": []}, {"text": " Table 2: Average sizes for sentences and parses in  training set", "labels": [], "entities": []}, {"text": " Table 3: Average sizes for sentences and parses in  test set", "labels": [], "entities": []}, {"text": " Table 5: Bleu N-gram precision scores", "labels": [], "entities": [{"text": "Bleu N-gram precision scores", "start_pos": 10, "end_pos": 38, "type": "METRIC", "confidence": 0.7738588154315948}]}, {"text": " Table 6: Bleu overall precision scores", "labels": [], "entities": [{"text": "Bleu", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9560074806213379}, {"text": "precision", "start_pos": 23, "end_pos": 32, "type": "METRIC", "confidence": 0.5926228761672974}]}, {"text": " Table 7: Human evaluation results", "labels": [], "entities": []}]}