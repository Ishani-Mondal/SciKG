{"title": [{"text": "Modeling Consensus: Classifier Combination for Word Sense Disambiguation", "labels": [], "entities": [{"text": "Word Sense Disambiguation", "start_pos": 47, "end_pos": 72, "type": "TASK", "confidence": 0.6992761691411337}]}], "abstractContent": [{"text": "This paper demonstrates the substantial empirical success of classifier combination for the word sense disambiguation task.", "labels": [], "entities": [{"text": "classifier combination", "start_pos": 61, "end_pos": 83, "type": "TASK", "confidence": 0.8649155795574188}, {"text": "word sense disambiguation task", "start_pos": 92, "end_pos": 122, "type": "TASK", "confidence": 0.8141732066869736}]}, {"text": "It investigates more than 10 classifier combination methods, including second order classifier stacking, over 6 major structurally different base classifiers (enhanced Na\u00efve Bayes, cosine, Bayes Ratio, decision lists, transformation-based learning and maximum variance boosted mixture models).", "labels": [], "entities": [{"text": "classifier stacking", "start_pos": 84, "end_pos": 103, "type": "TASK", "confidence": 0.7749909460544586}]}, {"text": "The paper also includes in-depth performance analysis sensitive to properties of the feature space and component classifiers.", "labels": [], "entities": []}, {"text": "When evaluated on the standard SENSEVAL1 and 2 data sets on 4 languages (English, Spanish, Basque, and Swedish), classifier combination performance exceeds the best published results on these data sets.", "labels": [], "entities": [{"text": "SENSEVAL1 and 2 data sets", "start_pos": 31, "end_pos": 56, "type": "DATASET", "confidence": 0.7108776688575744}]}], "introductionContent": [{"text": "Classifier combination has been extensively studied in the last decade, and has been shown to be successful in improving the performance of diverse NLP applications, including POS tagging (), base noun phrase chunking (), parsing) and word sense disambiguation).", "labels": [], "entities": [{"text": "Classifier combination", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8294283449649811}, {"text": "POS tagging", "start_pos": 176, "end_pos": 187, "type": "TASK", "confidence": 0.8202625811100006}, {"text": "base noun phrase chunking", "start_pos": 192, "end_pos": 217, "type": "TASK", "confidence": 0.6114637553691864}, {"text": "parsing", "start_pos": 222, "end_pos": 229, "type": "TASK", "confidence": 0.9550651907920837}, {"text": "word sense disambiguation", "start_pos": 235, "end_pos": 260, "type": "TASK", "confidence": 0.7186365524927775}]}, {"text": "There are several reasons why classifier combination is useful.", "labels": [], "entities": []}, {"text": "First, by consulting the output of multiple classifiers, the system will improve its robustness.", "labels": [], "entities": []}, {"text": "Second, it is possible that the problem can be decomposed into orthogonal feature spaces (e.g. linguistic constraints and word occurrence statistics) and it is often better to train different classifiers in each of the feature spaces and then combine their output, instead of designing a complex system that handles the multimodal information.", "labels": [], "entities": []}, {"text": "Third, it has been shown by that it is possible to reduce the classification error by a factor of \u00bd AE (AE is the number of classifiers) by combination, if the classifiers' errors are uncorrelated and unbiased.", "labels": [], "entities": [{"text": "AE", "start_pos": 100, "end_pos": 102, "type": "METRIC", "confidence": 0.997223973274231}, {"text": "AE", "start_pos": 104, "end_pos": 106, "type": "METRIC", "confidence": 0.9552783966064453}]}, {"text": "The target task studied here is word sense disambiguation in the SENSEVAL evaluation framework) with comparative tests in English, Spanish, Swedish and Basque lexical-sample sense tagging over a combined sample of 37730 instances of 234 polysemous words.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 32, "end_pos": 57, "type": "TASK", "confidence": 0.7890217900276184}, {"text": "Basque lexical-sample sense tagging", "start_pos": 152, "end_pos": 187, "type": "TASK", "confidence": 0.5164261609315872}]}, {"text": "This paper offers a detailed comparative evaluation and description of the problem of classifier combination over a structurally and procedurally diverse set of six both well established and original classifiers: extended Na\u00efve Bayes, BayesRatio, Cosine, non-hierarchical Decision Lists, Transformation Based Learning (TBL), and the MMVC classifiers, briefly described in Section 4.", "labels": [], "entities": [{"text": "classifier combination", "start_pos": 86, "end_pos": 108, "type": "TASK", "confidence": 0.8821834027767181}, {"text": "MMVC classifiers", "start_pos": 333, "end_pos": 349, "type": "DATASET", "confidence": 0.7569854259490967}]}, {"text": "These systems have different space-searching strategies, ranging from discriminant functions (BayesRatio) to data likelihood (Bayes, Cosine) to decision rules (TBL, Decision Lists), and therefore are amenable to combination.", "labels": [], "entities": []}], "datasetContent": [{"text": "To empirically test the combination methods presented in the previous section, we ran experiments on the SENSEVAL1 English data and data from four SENSEVAL2 lexical sample tasks: English(EN), Spanish(ES), Basque(EU) and Swedish(SV).", "labels": [], "entities": [{"text": "SENSEVAL1 English data", "start_pos": 105, "end_pos": 127, "type": "DATASET", "confidence": 0.8136058648427328}]}, {"text": "Unless explicitly stated otherwise, all the results in the following section were obtained by performing 5-fold cross-validation 6 . To avoid the potential for over-optimization, a single final evaluation system was run once on the otherwise untouched test data, as presented in Section 6.3.", "labels": [], "entities": []}, {"text": "The data consists of contexts associated with a specific word to be sense tagged (target word); the context size varies from 1 sentence (Spanish) to 5 sentences (English, Swedish).", "labels": [], "entities": []}, {"text": "presents some statistics collected on the training data for the five data sets.", "labels": [], "entities": []}, {"text": "Some of the tasks are quite challenging (e.g. SENSEVAL2 English task) -as illustrated by the mean participating systems' accuracies in Table 5.", "labels": [], "entities": []}, {"text": "Outlining the claim that feature selection is important for WSD, presents the marginal loss in performance of either only using one of the positional feature classes or excluding one of the positional feature classes relative to the algorithm's full performance using all available feature classes.", "labels": [], "entities": [{"text": "feature selection", "start_pos": 25, "end_pos": 42, "type": "TASK", "confidence": 0.7045897543430328}, {"text": "WSD", "start_pos": 60, "end_pos": 63, "type": "TASK", "confidence": 0.9663969874382019}]}, {"text": "It is interesting to note that the feature-attractive methods (NB,BR,Cosine) depend heavily on the BagOfWords features, while discriminative methods are most dependent on LocalContext features.", "labels": [], "entities": []}, {"text": "For an extensive evaluation of factors influencing the WSD performance (including representational features), we refer the readers to.  classifier combination methods for 5 classifiers, NB (Na\u00efve Bayes), BR (BayesRatio), TBL, DL and MMVC, including the average classifier accuracy and the best classification accuracy.", "labels": [], "entities": [{"text": "WSD", "start_pos": 55, "end_pos": 58, "type": "TASK", "confidence": 0.982698917388916}, {"text": "BR", "start_pos": 204, "end_pos": 206, "type": "METRIC", "confidence": 0.9512761235237122}, {"text": "accuracy", "start_pos": 272, "end_pos": 280, "type": "METRIC", "confidence": 0.9566376209259033}, {"text": "accuracy", "start_pos": 309, "end_pos": 317, "type": "METRIC", "confidence": 0.9052024483680725}]}, {"text": "Before examining the results, it is worth mentioning that the methods which estimate parameters are doing soon a smaller training size (3/5, to be precise), and this can have an effect on how well the parameters are estimated.", "labels": [], "entities": []}, {"text": "After the parameters are estimated, however, the interpolation is done between probability distributions that are computed on 4/5 of the training data, similarly to the methods that do not estimate any parameters.", "labels": [], "entities": []}, {"text": "The unweighted averaging model of probability interpolation (Equation (1)) performs well, obtaining over 1% mean absolute performance over the best classifier 7 , the difference in performance is statistically significant in all cases except Swedish and Spanish.", "labels": [], "entities": []}, {"text": "Of the classifier combination techniques, rank-based combination and performancebased voting perform best.", "labels": [], "entities": []}, {"text": "Their mean 2% absolute improvement over the single best classifier is significant in all languages.", "labels": [], "entities": [{"text": "absolute improvement", "start_pos": 14, "end_pos": 34, "type": "METRIC", "confidence": 0.9546891152858734}]}, {"text": "Also, their accuracy improvement relative to uniform-weight probability interpolation is statistically significant in aggregate and for all languages except Basque (where there is generally a small difference among all classifiers).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9990342855453491}]}], "tableCaptions": [{"text": " Table 1: Training set characteristics", "labels": [], "entities": []}, {"text": " Table 2: Individual feature type contribution to perfor-", "labels": [], "entities": []}, {"text": " Table 3: Classifier combination accuracy over 5 base  classifiers: NB, BR, TBL, DL, MMVC. Best perform- ing methods are shown in bold.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.9660821557044983}, {"text": "BR", "start_pos": 72, "end_pos": 74, "type": "METRIC", "confidence": 0.9268155097961426}]}, {"text": " Table 4: Accuracy for different EM-weighted probability  interpolation models for SENSEVAL2", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9985291957855225}, {"text": "SENSEVAL2", "start_pos": 83, "end_pos": 92, "type": "TASK", "confidence": 0.8532341122627258}]}, {"text": " Table 5: Final Performance (Frozen Systems) on SENSEVAL Lexical Sample WSD Test Data", "labels": [], "entities": [{"text": "SENSEVAL Lexical Sample WSD Test Data", "start_pos": 48, "end_pos": 85, "type": "DATASET", "confidence": 0.6469966173171997}]}]}