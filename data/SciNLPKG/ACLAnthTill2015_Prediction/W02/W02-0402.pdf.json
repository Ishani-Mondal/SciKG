{"title": [{"text": "Selecting Sentences for Multidocument Summaries using Randomized Local Search", "labels": [], "entities": [{"text": "Multidocument Summaries", "start_pos": 24, "end_pos": 47, "type": "TASK", "confidence": 0.6842949390411377}]}], "abstractContent": [{"text": "We present and evaluate a randomized local search procedure for selecting sentences to include in a multidocument summary.", "labels": [], "entities": []}, {"text": "The search favors the inclusion of adjacent sentences while penalizing the selection of repetitive material, in order to improve intelligibility without unduly affecting informativeness.", "labels": [], "entities": []}, {"text": "Sentence similarity is determined using both surface-oriented measures and semantic groups obtained from merging the output templates of an information extraction subsystem.", "labels": [], "entities": [{"text": "Sentence similarity", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8943706750869751}]}, {"text": "Ina comparative evaluation against two DUC-like baselines and three simpler versions of our system, we found that our randomized local search method provided substantial improvements in both content and intelligibility, while the use of the IE groups also appeared to contribute a small further improvement in content.", "labels": [], "entities": []}], "introductionContent": [{"text": "Improving the intellibility of multidocument summaries remains a significant challenge.", "labels": [], "entities": []}, {"text": "While most previous approaches to multidocument summarization have addressed the problem of reducing repetition, less attention has been paid to problems of coherence and cohesion.", "labels": [], "entities": [{"text": "multidocument summarization", "start_pos": 34, "end_pos": 61, "type": "TASK", "confidence": 0.7469155788421631}]}, {"text": "Ina typical extractive system (e.g.), sentences are selected for inclusion in the summary one at a time, with later choices sensitive to their similarity to earlier ones; the selected sentences are then ordered either chronologically or by relevance.", "labels": [], "entities": []}, {"text": "The resulting summaries often jump incoherently from topic to topic, and contain broken cohesive links, such as dangling anaphors or unmet presuppositions.", "labels": [], "entities": []}, {"text": "present an improved method of ordering sentences in the context of MultiGen, a multidocument summarizer that identifies sets of similar sentences, termed themes, and reformulates their common phrases as new text.", "labels": [], "entities": []}, {"text": "In their approach, topically related themes are identified and kept together in the resulting summary, in order to help improve cohesion and reduce topic switching.", "labels": [], "entities": [{"text": "topic switching", "start_pos": 148, "end_pos": 163, "type": "TASK", "confidence": 0.7365885972976685}]}, {"text": "In this paper, we pursue a related but simpler idea in an extractive context, namely to favor the selection of blocks of adjacent sentences in constructing a multidocument summary.", "labels": [], "entities": []}, {"text": "Here, the challenge is to improve intelligibility without unduly sacrificing informativeness; for example, selecting the beginning of the most recent article in a document set will usually produce a highly intelligible text, but one that is not very representative of the document set as a whole.", "labels": [], "entities": []}, {"text": "To manage this tradeoff, we have developed a randomized local search procedure (cf.) to select the highest ranking set of sentences for the summary, where the inclusion of adjacent sentences is favored and the selection of repetitive material is penalized.", "labels": [], "entities": []}, {"text": "The method involves greedily searching for the best combination of sentences to swap in and out of the current summary until no more improvements are possible; noise strategies include occasionally adding a sentence to the current summary, regardless of its score, and restarting the local search from random starting points fora fixed number of iterations.", "labels": [], "entities": []}, {"text": "In determining sentence similarity, we have used surface-oriented similarity measures obtained from Columbia's SimFinder tool (), as well as semantic groups obtained from merging the output templates of an information extraction (IE) subsystem.", "labels": [], "entities": [{"text": "sentence similarity", "start_pos": 15, "end_pos": 34, "type": "TASK", "confidence": 0.7085334360599518}]}, {"text": "In related work, describes an approach to balancing informativeness and intelligibility that also involves searching through sets of sentences to select.", "labels": [], "entities": []}, {"text": "In contrast to our approach, Marcu employs abeam search through possible summaries of progressively greater length, which seems less amenable to an anytime formulation; this maybe an important practical consideration, since Marcu reports search times in hours, whereas we have found that less than a minute of searching is usually effective.", "labels": [], "entities": []}, {"text": "In other related work, suggest pairing extracted sentences with their corresponding lead sentences; we have not directly compared our search-based approach to Lin and Hovy's simpler method.", "labels": [], "entities": []}, {"text": "In order to evaluate our approach, we compared 200-word summaries generated by our system to those of two baselines that are similar to those used in DUC 2001, and to three simpler versions of the system, where a simple marginal relevance selection procedure was used instead of the selection search, and/or the IE groups were ignored.", "labels": [], "entities": [{"text": "DUC 2001", "start_pos": 150, "end_pos": 158, "type": "DATASET", "confidence": 0.9513014256954193}]}, {"text": "In general, we found that our randomized local search method provided substantial improvements in both content and intelligibility over the DUC-like baselines and the simplest variant of our system, which used marginal relevance selection and no IE groups (with the exception that the last article baseline was always ranked first in intelligibility).", "labels": [], "entities": []}, {"text": "The use of the IE groups also appeared to contribute a small further improvement in content when used with our selection search.", "labels": [], "entities": [{"text": "IE groups", "start_pos": 15, "end_pos": 24, "type": "DATASET", "confidence": 0.7235433757305145}]}, {"text": "We discuss these results in greater detail in the final section of the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "To select the inputs for the evaluation, we took five subsets of the articles from TDT2 topic 89 -all the articles up to the end of days 1 through 5 after the quake.", "labels": [], "entities": [{"text": "TDT2 topic 89", "start_pos": 83, "end_pos": 96, "type": "DATASET", "confidence": 0.9308313727378845}]}, {"text": "We chose to use TDT2 topic 89 so that we could assess the impact of the IE quality on the results, given that we had previously created manual IE annotations for these articles ().", "labels": [], "entities": [{"text": "TDT2 topic 89", "start_pos": 16, "end_pos": 29, "type": "DATASET", "confidence": 0.8436863819758097}, {"text": "IE", "start_pos": 72, "end_pos": 74, "type": "TASK", "confidence": 0.8643000721931458}]}, {"text": "1 1 Although our decision to use subsets of TDT2 topic 89 as inputs meant that our training/tuning and test data overlapped, we do not believe that this choice overly compromises our results, since -as will be discussed in this section and the next -the impact of the IE groups For each input document set, we ran the RIP-TIDES system to produce overview summaries of 200 words or less.", "labels": [], "entities": [{"text": "TDT2 topic 89", "start_pos": 44, "end_pos": 57, "type": "DATASET", "confidence": 0.8183499773343405}]}, {"text": "For comparison purposes, we also ran two baselines, similar to those used in DUC 2001, and three simpler versions of the system, fora total of six summary types: Last The first N sentences of the latest article in the document set, up to the word limit.", "labels": [], "entities": [{"text": "DUC 2001", "start_pos": 77, "end_pos": 85, "type": "DATASET", "confidence": 0.9440772533416748}]}, {"text": "Leads The lead sentences from the latest articles in the document set, up to the word limit, listed in chronological order.", "labels": [], "entities": []}, {"text": "MR The top ranking sentences selected according to their thresholded marginal relevance, up to the word limit, listed in chronological order, using RIPTIDES to score the sentences, except with the IE groups zeroed out.", "labels": [], "entities": [{"text": "RIPTIDES", "start_pos": 148, "end_pos": 156, "type": "METRIC", "confidence": 0.9959632158279419}]}, {"text": "MR+IE The MR summarization method, but with the IE groups included for the RIP-TIDES sentence scorer.", "labels": [], "entities": [{"text": "MR+IE", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.5165513654549917}, {"text": "MR summarization", "start_pos": 10, "end_pos": 26, "type": "TASK", "confidence": 0.48326341807842255}, {"text": "RIP-TIDES sentence scorer", "start_pos": 75, "end_pos": 100, "type": "TASK", "confidence": 0.5287869473298391}]}, {"text": "Search The RIPTIDES overview, except with the IE groups zeroed out for sentence scoring.", "labels": [], "entities": [{"text": "RIPTIDES", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.7152142524719238}]}], "tableCaptions": [{"text": " Table 1: Content Rankings on TDT2 Topic 89, using Simulated IE. The scores for the two judges  at each time point are separated by commas.", "labels": [], "entities": [{"text": "TDT2 Topic 89", "start_pos": 30, "end_pos": 43, "type": "DATASET", "confidence": 0.970641016960144}, {"text": "Simulated IE", "start_pos": 51, "end_pos": 63, "type": "METRIC", "confidence": 0.4109814316034317}]}, {"text": " Table 2: Intelligibility Rankings on TDT2 Topic 89, using Simulated IE.", "labels": [], "entities": [{"text": "TDT2 Topic 89", "start_pos": 38, "end_pos": 51, "type": "DATASET", "confidence": 0.9486224253972372}]}, {"text": " Table 3: Content Rankings on TDT2 Topic 89, using Actual IE.", "labels": [], "entities": [{"text": "TDT2 Topic 89", "start_pos": 30, "end_pos": 43, "type": "DATASET", "confidence": 0.9786344170570374}, {"text": "Actual IE", "start_pos": 51, "end_pos": 60, "type": "METRIC", "confidence": 0.6618609428405762}]}, {"text": " Table 4: Intelligibility Rankings on TDT2 Topic 89, using Actual IE.", "labels": [], "entities": [{"text": "TDT2 Topic 89", "start_pos": 38, "end_pos": 51, "type": "DATASET", "confidence": 0.9595916072527567}]}]}