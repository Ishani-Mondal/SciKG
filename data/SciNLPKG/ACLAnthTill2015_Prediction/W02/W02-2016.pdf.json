{"title": [{"text": "Japanese Dependency Analysis using Cascaded Chunking", "labels": [], "entities": [{"text": "Japanese Dependency Analysis", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.6240658660729727}, {"text": "Cascaded Chunking", "start_pos": 35, "end_pos": 52, "type": "TASK", "confidence": 0.553538978099823}]}], "abstractContent": [{"text": "In this paper, we propose anew statistical Japanese dependency parser using a cascaded chunking model.", "labels": [], "entities": [{"text": "Japanese dependency parser", "start_pos": 43, "end_pos": 69, "type": "TASK", "confidence": 0.5346660713354746}]}, {"text": "Conventional Japanese statistical dependency parsers are mainly based on a probabilistic model, which is not always efficient or scalable.", "labels": [], "entities": [{"text": "statistical dependency parsers", "start_pos": 22, "end_pos": 52, "type": "TASK", "confidence": 0.6232130229473114}]}, {"text": "We propose anew method that is simple and efficient, since it parses a sentence deterministically only deciding whether the current segment modifies the segment on its immediate right hand side.", "labels": [], "entities": []}, {"text": "Experiments using the Kyoto University Corpus show that the method outperforms previous systems as well as improves the parsing and training efficiency.", "labels": [], "entities": [{"text": "Kyoto University Corpus", "start_pos": 22, "end_pos": 45, "type": "DATASET", "confidence": 0.983429471651713}, {"text": "parsing", "start_pos": 120, "end_pos": 127, "type": "TASK", "confidence": 0.9815912246704102}]}], "introductionContent": [{"text": "Dependency analysis has been recognized as a basic process in Japanese sentence analysis, and a number of studies have been proposed.", "labels": [], "entities": [{"text": "Dependency analysis", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8815081715583801}, {"text": "Japanese sentence analysis", "start_pos": 62, "end_pos": 88, "type": "TASK", "confidence": 0.6379954715569814}]}, {"text": "Japanese dependency structure is usually defined in terms of the relationship between phrasal units called bunsetsu segments (hereafter \"segments\").", "labels": [], "entities": []}, {"text": "Most of the previous statistical approaches for Japanese dependency analysis;;) are based on a probabilistic model consisting of the following two steps.", "labels": [], "entities": [{"text": "dependency analysis", "start_pos": 57, "end_pos": 76, "type": "TASK", "confidence": 0.8268057107925415}]}, {"text": "First, they estimate modification probabilities, in other words, how probable one segment tends to modify another.", "labels": [], "entities": []}, {"text": "Second the optimal combination of dependencies is searched from the all candidates dependencies.", "labels": [], "entities": []}, {"text": "Such a probabilistic model is not always efficient since it needs to calculate the probabilities for all possible dependencies and creates n \u02d9 (n \u2212 1)/2 (where n is the number of segments in a sentence) training examples per sentence.", "labels": [], "entities": []}, {"text": "In addition, the probabilistic model assumes that each pairs of dependency structure is independent.", "labels": [], "entities": []}, {"text": "In this paper, we propose anew Japanese dependency parser which is more efficient and simpler than the probabilistic model, yet performs better in training and testing on the Kyoto University Corpus.", "labels": [], "entities": [{"text": "Japanese dependency parser", "start_pos": 31, "end_pos": 57, "type": "TASK", "confidence": 0.5597871939341227}, {"text": "Kyoto University Corpus", "start_pos": 175, "end_pos": 198, "type": "DATASET", "confidence": 0.9759454727172852}]}, {"text": "The method parses a sentence deterministically only deciding whether the current segment modifies segment on its immediate right hand side.", "labels": [], "entities": []}, {"text": "Moreover, it does not assume the independence constraint between dependencies", "labels": [], "entities": []}], "datasetContent": [{"text": "We used the following two annotated corpora for our experiments.", "labels": [], "entities": []}, {"text": "\u2022 Standard data set This data set consists of the Kyoto University text corpus Version 2.0 (.", "labels": [], "entities": [{"text": "Standard data set This data set", "start_pos": 2, "end_pos": 33, "type": "DATASET", "confidence": 0.8044823110103607}, {"text": "Kyoto University text corpus Version 2.0", "start_pos": 50, "end_pos": 90, "type": "DATASET", "confidence": 0.9684098462263743}]}, {"text": "We used 7,958 sentences from the articles on January 1st to January 7th as training examples, and 1,246 sentences from the articles on January 9th as the test data.", "labels": [], "entities": []}, {"text": "This data set was used in () and).", "labels": [], "entities": []}, {"text": "\u2022 Large data set In order to investigate the scalability of the cascaded chunking model, we prepared larger data set.", "labels": [], "entities": []}, {"text": "We used all 38,383 sentences of the Kyoto University text corpus Version 3.0.", "labels": [], "entities": [{"text": "Kyoto University text corpus Version 3.0", "start_pos": 36, "end_pos": 76, "type": "DATASET", "confidence": 0.9575483302275339}]}, {"text": "The training and test data were generated by a two-fold cross validation.", "labels": [], "entities": []}, {"text": "The feature sets used in our experiments are shown in.", "labels": [], "entities": []}, {"text": "The static features are basically taken from Uchimoto's list (.", "labels": [], "entities": []}, {"text": "Head Word (HW) is the rightmost content word in the segment.", "labels": [], "entities": []}, {"text": "Functional Word (FW) is set as follows: -FW = the rightmost functional word, if there is a functional word in the segment -FW = the rightmost inflection form, if there is a predicate in the segment -FW = same as the HW, otherwise.", "labels": [], "entities": [{"text": "Functional Word (FW)", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.6825538277626038}]}, {"text": "The static features include the information on existence of brackets, question marks and punctuation marks, etc.", "labels": [], "entities": []}, {"text": "Besides, there are features that show the relative relation of two segments, such as distance, and existence of brackets, quotation marks and punctuation marks between them.", "labels": [], "entities": [{"text": "distance", "start_pos": 85, "end_pos": 93, "type": "METRIC", "confidence": 0.985584557056427}]}, {"text": "For a segment X and its dynamic feature Y (where Y is of type A or B), we set the Functional Representation (FR) feature of X based on the FW of X (X-FW) as follows: -FR = lexical form of X-FW if POS of X-FW is particle, adverb, adnominal or conjunction -FR = inflectional form of X-FW if X-FW has an inflectional form.", "labels": [], "entities": [{"text": "FW", "start_pos": 139, "end_pos": 141, "type": "METRIC", "confidence": 0.9882152080535889}, {"text": "FR", "start_pos": 167, "end_pos": 169, "type": "METRIC", "confidence": 0.9719642996788025}, {"text": "FR", "start_pos": 255, "end_pos": 257, "type": "METRIC", "confidence": 0.9723719358444214}]}, {"text": "-FR = the POS tag of X-FW, otherwise.", "labels": [], "entities": [{"text": "FR", "start_pos": 1, "end_pos": 3, "type": "METRIC", "confidence": 0.9980199337005615}, {"text": "POS", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9484725594520569}]}, {"text": "For a segment X and its dynamic feature C, we set POS tag and POS-subcategory of the HW of X.", "labels": [], "entities": [{"text": "POS tag", "start_pos": 50, "end_pos": 57, "type": "METRIC", "confidence": 0.9309408366680145}]}, {"text": "All our experiments are carried out on AlphaSever 8400 (21164A 500Mhz) for training and Linux (PentiumIII 1GHz) for testing.", "labels": [], "entities": [{"text": "AlphaSever 8400", "start_pos": 39, "end_pos": 54, "type": "DATASET", "confidence": 0.8798103332519531}]}, {"text": "We used a third degree polynomial kernel function, which is exactly the same setting in).", "labels": [], "entities": []}, {"text": "Performance on the test data is measured using dependency accuracy and sentence accuracy.", "labels": [], "entities": [{"text": "dependency accuracy", "start_pos": 47, "end_pos": 66, "type": "METRIC", "confidence": 0.606464296579361}, {"text": "accuracy", "start_pos": 80, "end_pos": 88, "type": "METRIC", "confidence": 0.679021418094635}]}, {"text": "Dependency accuracy is the percentage of correct dependencies out of all dependency relations.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.8136019706726074}]}, {"text": "Sentence accuracy is the percentage of sentences in which all dependencies are determined correctly.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 9, "end_pos": 17, "type": "METRIC", "confidence": 0.8569640517234802}]}, {"text": "The results for the new cascaded chunking model as well as for the previous probabilistic model based on SVMs () are summarized in.", "labels": [], "entities": []}, {"text": "We cannot employ the experiments for the probabilistic model using large dataset, since the data size is too large for our current SVMs learning program to terminate in a realistic time period.", "labels": [], "entities": []}, {"text": "Even though the number of training examples used for the cascaded chunking model is less than a quarter of that for the probabilistic model, and the used feature set is the same, dependency accuracy and sentence accuracy are improved using the cascaded chunking model (89.09% \u2192 89.29%, 46.17% \u2192 47.53%).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 190, "end_pos": 198, "type": "METRIC", "confidence": 0.9540040493011475}, {"text": "accuracy", "start_pos": 212, "end_pos": 220, "type": "METRIC", "confidence": 0.9259485602378845}]}, {"text": "The time required for training and parsing are significantly reduced by applying the cascaded chunking model (336h.\u21928h, 2.1sec.\u2192 0.5sec.).", "labels": [], "entities": [{"text": "parsing", "start_pos": 35, "end_pos": 42, "type": "TASK", "confidence": 0.9814291000366211}]}], "tableCaptions": [{"text": " Table 1: Features used in our experiments", "labels": [], "entities": []}, {"text": " Table 2: Cascaded Chunking model vs Probabilistic model", "labels": [], "entities": []}]}