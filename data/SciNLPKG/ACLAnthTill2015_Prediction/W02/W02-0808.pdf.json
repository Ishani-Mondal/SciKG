{"title": [], "abstractContent": [{"text": "This paper describes an experiment that uses translation equivalents derived from parallel corpora to determine sense distinctions that can be used for automatic sense-tagging and other disambiguation tasks.", "labels": [], "entities": []}, {"text": "Our results show that sense distinctions derived from cross-lingual information are at least as reliable as those made by human annotators.", "labels": [], "entities": []}, {"text": "Because our approach is fully automated through all its steps, it could provide means to obtain large samples of \"sense-tagged\" data without the high cost of human annotation.", "labels": [], "entities": []}], "introductionContent": [{"text": "It is well known that the most nagging issue for word sense disambiguation (WSD) is the definition of just what a word sense is.", "labels": [], "entities": [{"text": "word sense disambiguation (WSD)", "start_pos": 49, "end_pos": 80, "type": "TASK", "confidence": 0.8280790944894155}]}, {"text": "At its base, the problem is a philosophical and linguistic one that is far from being resolved.", "labels": [], "entities": []}, {"text": "However, work in automated language processing has led to efforts to find practical means to distinguish word senses, at least to the degree that they are useful for natural language processing tasks such as summarization, document retrieval, and machine translation.", "labels": [], "entities": [{"text": "summarization", "start_pos": 208, "end_pos": 221, "type": "TASK", "confidence": 0.98084956407547}, {"text": "document retrieval", "start_pos": 223, "end_pos": 241, "type": "TASK", "confidence": 0.7301819026470184}, {"text": "machine translation", "start_pos": 247, "end_pos": 266, "type": "TASK", "confidence": 0.8120847642421722}]}, {"text": "suggest that for the purposes of WSD, the different senses of a word could be determined by considering only sense distinctions that are lexicalized cross-linguistically.", "labels": [], "entities": [{"text": "WSD", "start_pos": 33, "end_pos": 36, "type": "TASK", "confidence": 0.9843115210533142}]}, {"text": "In particular, they propose that some set of target languages be identified, and that the sense distinctions to be considered for language processing applications and evaluation be restricted to those that are realized lexically in some minimum subset of those languages.", "labels": [], "entities": []}, {"text": "This idea would seem to provide an answer, at least in part, to the problem of determining different senses of a word: intuitively, one assumes that if another language lexicalizes a word in two or more ways, there must be a conceptual motivation.", "labels": [], "entities": []}, {"text": "If we look at enough languages, we would be likely to find the significant lexical differences that delimit different senses of a word.", "labels": [], "entities": []}, {"text": "Several studies have used parallel texts for WSD (e.g., as well as to define semantic properties of and relations among lexemes).", "labels": [], "entities": [{"text": "WSD", "start_pos": 45, "end_pos": 48, "type": "TASK", "confidence": 0.9790850877761841}]}, {"text": "More recently, two studies have examined the use of cross-lingual lexicalization as a criterion for validating sense distinctions: Ide (1999) used translation equivalents derived from aligned versions of Orwell's Nineteen Eighty-Four among five languages from four different languages families, while used translations generated by native speakers presented with isolated sentences in English.", "labels": [], "entities": []}, {"text": "In both of these studies, translation information was used to validate sense distinctions provided in lexicons such as.", "labels": [], "entities": [{"text": "translation", "start_pos": 26, "end_pos": 37, "type": "TASK", "confidence": 0.9577121138572693}]}, {"text": "Although the results are promising, especially for coarsegrained sense distinctions, they rest on the acceptance of a previously established set of senses.", "labels": [], "entities": [{"text": "coarsegrained sense distinctions", "start_pos": 51, "end_pos": 83, "type": "TASK", "confidence": 0.5653783082962036}]}, {"text": "Given the substantial divergences among sense distinctions in dictionaries and lexicons, together with the ongoing debate within the WSD community concerning which sense distinctions, if any, are appropriate for language processing applications, fitting cross-linguistic information to pre-established sense inventories may not be the optimal approach.", "labels": [], "entities": [{"text": "WSD", "start_pos": 133, "end_pos": 136, "type": "TASK", "confidence": 0.8684611916542053}]}, {"text": "This paper builds on previously reported) that uses translation equivalents derived from a parallel corpus to determine sense distinctions that can be used to automatically sense-tag the data.", "labels": [], "entities": []}, {"text": "Our results show that sense distinctions derived from cross-lingual information are at least as reliable as those made by human annotators.", "labels": [], "entities": []}, {"text": "Our approach therefore provides a promising means to automatically identify sense distinctions.", "labels": [], "entities": [{"text": "identify sense distinctions", "start_pos": 67, "end_pos": 94, "type": "TASK", "confidence": 0.6603321234385172}]}], "datasetContent": [{"text": "We constructed a multilingual lexicon based on the Orwell corpus, using a method outlined in Tufis and Barbu).", "labels": [], "entities": [{"text": "Orwell corpus", "start_pos": 51, "end_pos": 64, "type": "DATASET", "confidence": 0.9754593372344971}]}, {"text": "The complete English Orwell contains 7,069 different lemmas, while the computed lexicon comprises 1,233 entries, out of which 845 have (possibly multiple) translation equivalents in all languages.", "labels": [], "entities": []}, {"text": "We then conducted a preliminary study using a subset of 33 nouns covering a range of frequencies and degrees of ambiguity).", "labels": [], "entities": []}, {"text": "For each noun in the sample, we extracted all sentences from the English Nineteen Eighty-Four containing the lemma in question, together with the parallel sentences from each of the six translations.", "labels": [], "entities": []}, {"text": "The aligned sentences were automatically scanned to extract translation equivalents.", "labels": [], "entities": []}, {"text": "1 A vector was then created for each occurrence, representing all possible lexical translations in the six parallel versions: if a given word is used to translate that occurrence, the vector contains a 1 in the corresponding position, 0 otherwise.", "labels": [], "entities": []}, {"text": "The vectors for each ambiguous word were fed to an agglomerative clustering algorithm, where the resulting clusters are taken to represent different senses and sub-senses of the word in question.", "labels": [], "entities": []}, {"text": "The clusters produced by the algorithm were compared with sense assignments made by two human annotators on the basis of WordNet 1.6.", "labels": [], "entities": [{"text": "WordNet 1.6", "start_pos": 121, "end_pos": 132, "type": "DATASET", "confidence": 0.9230108559131622}]}, {"text": "In order to compare the algorithm results with the annotators' sense assignments, we normalized the data as follows: for each annotator and the algorithm, each of the 33 words was represented as a vector of length n(n-1)/2, where n is the number of occurrences of the word in the corpus.", "labels": [], "entities": []}, {"text": "The positions in the vector represent a \"yes-no\" assignment for each pair of occurrences, indicating whether or not they were judged to have the same sense (the same WordNet sense for the annotators, and the same cluster for the algorithm).", "labels": [], "entities": []}, {"text": "Representing the clustering algorithm results in this form required some means to \"flatten\" the cluster hierarchies, which typically extend to 5 or 6 levels, to conform more closely to the completely flat WordNet-based data.", "labels": [], "entities": [{"text": "WordNet-based data", "start_pos": 205, "end_pos": 223, "type": "DATASET", "confidence": 0.9641410410404205}]}, {"text": "Therefore, clusters with a minimum distance value (as assigned by the clustering algorithm) at or below 1.7 were combined, and each leaf of the resulting collapsed tree was treated as a different sense.", "labels": [], "entities": []}, {"text": "This yielded a set of sense distinctions for each word roughly similar in number to those assigned by the annotators.", "labels": [], "entities": []}, {"text": "The cluster output for glass in is an example of the results obtained from the clustering algorithm.", "labels": [], "entities": []}, {"text": "For clarity, the occurrences have been manually labeled with WordNet 1.6 senses ().", "labels": [], "entities": [{"text": "WordNet 1.6 senses", "start_pos": 61, "end_pos": 79, "type": "DATASET", "confidence": 0.8823831876118978}]}, {"text": "The tree shows that the algorithm correctly grouped occurrences corresponding to WordNet sense 1 (a solid material) in one of the two main branches, and those corresponding to sense 2 (drinking vessel) in the other.", "labels": [], "entities": []}, {"text": "The top group is further divided into two sub-clusters, the lower of which refer to a looking glass and a magnifying glass, respectively.", "labels": [], "entities": []}, {"text": "While this is a particularly clear example of good results from the clustering algorithm, results for other words are, for the most part, similarly reasonable.", "labels": [], "entities": []}, {"text": "The results of the first experiment are summarized in, which shows the percentage of agreement between the cluster algorithm and each annotator, between the two annotators, and for the algorithm and both annotators taken together.", "labels": [], "entities": [{"text": "agreement", "start_pos": 85, "end_pos": 94, "type": "METRIC", "confidence": 0.9765267372131348}]}, {"text": "The percentages are similar to those reported in earlier work; for example,  Comparison of sense differentiation achieved using translation equivalents, as determined by the clustering algorithm, with those assigned by human annotators suggests that use of translation equivalents for word sense tagging and disambiguation is worth pursuing.", "labels": [], "entities": [{"text": "Comparison of sense differentiation", "start_pos": 77, "end_pos": 112, "type": "TASK", "confidence": 0.858902782201767}, {"text": "word sense tagging", "start_pos": 285, "end_pos": 303, "type": "TASK", "confidence": 0.6925407648086548}]}, {"text": "Agreement levels are comparable to (and in some cases higher than) those obtained in earlier studies tagging with WordNet senses.", "labels": [], "entities": [{"text": "Agreement", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9504076838493347}]}, {"text": "Furthermore, the pairwise difference in agreement between the human annotators and the annotators and the clustering algorithm is only 10-13%, which is also similar to scores obtained in other studies.", "labels": [], "entities": [{"text": "agreement", "start_pos": 40, "end_pos": 49, "type": "METRIC", "confidence": 0.9716220498085022}]}, {"text": "In the second phase, the experiment was broadened to include 76 nouns from the multi-lingual lexicon, including words with varying ambiguity (the range in number of WordNet senses is 2 to 29, average 7.09) and semantic characteristics (e.g., abstract vs. concrete: \"thought\", \"stuff\", \"meaning\", \"feeling\" vs. \"hand\", \"boot\", \"glass\", \"girl\", etc.).", "labels": [], "entities": []}, {"text": "The LL score is set at a maximum value to ensure high precision for the extracted translation equivalents, which minimizes sense clustering errors due to incorrect word alignment.", "labels": [], "entities": [{"text": "precision", "start_pos": 54, "end_pos": 63, "type": "METRIC", "confidence": 0.9966714382171631}]}, {"text": "In this second experiment, we increased the number of annotators to four.", "labels": [], "entities": []}, {"text": "The results of the clustering algorithm and the sense assignments made by the human annotators were normalized differently than in the earlier experiment, by ignoring sense numbers and interpreting the annotators' sense assignments as clusters only.", "labels": [], "entities": []}, {"text": "To see why this was necessary, consider the following set of sense assignments for the seven occurrences of \"youth\" in Nineteen Eighty-Four:", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3. Thirty-four of  the 40 occurrences appear in the clusters common  to the two classifications; therefore, the agreement  rate is 85%.", "labels": [], "entities": [{"text": "agreement  rate", "start_pos": 119, "end_pos": 134, "type": "METRIC", "confidence": 0.9868379533290863}]}, {"text": " Table 4 Agreement rates among baseline, the four  annotators, gold standard, and the algorithm", "labels": [], "entities": []}]}