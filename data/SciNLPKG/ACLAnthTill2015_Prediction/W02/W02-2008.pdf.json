{"title": [{"text": "Avery very large corpus doesn't always yield reliable estimates", "labels": [], "entities": []}], "abstractContent": [{"text": "Banko and Brill (2001) suggested that the development of very large training corpora maybe more effective for progress in empirical Natural Language Processing than improving methods that use existing smaller training corpora.", "labels": [], "entities": []}, {"text": "This work tests their claim by exploring whether a very large corpus can eliminate the sparseness problems associated with estimating unigram probabilities.", "labels": [], "entities": []}, {"text": "We do this by empirically investigating the convergence behaviour of unigram probability estimates on a one billion word corpus.", "labels": [], "entities": []}, {"text": "When using one billion words, as expected, we do find that many of our estimates do converge to their eventual value.", "labels": [], "entities": []}, {"text": "However, we also find that for some words, no such convergence occurs.", "labels": [], "entities": []}, {"text": "This leads us to conclude that simply relying upon large corpora is not in itself sufficient: we must pay attention to the statistical modelling as well.", "labels": [], "entities": []}], "introductionContent": [{"text": "The quantity and reliability of linguistic information is primarily determined by the size of the training corpus: with limited data available, extracting statistics for any given language phenomenon and its surrounding context is unreliable.", "labels": [], "entities": []}, {"text": "Overcoming the sparse distribution of linguistic events is a key design problem in any statistical NLP system.", "labels": [], "entities": []}, {"text": "For some tasks, corpus size is no longer a limiting factor, since it has become feasible to acquire homogeneous document collections two or three orders of magnitude larger than existing resources.", "labels": [], "entities": []}, {"text": "report on confusion set disambiguation experiments where they apply relatively simple learning methods to a one billion word training corpus.", "labels": [], "entities": [{"text": "confusion set disambiguation", "start_pos": 10, "end_pos": 38, "type": "TASK", "confidence": 0.7334796289602915}]}, {"text": "Their experiments show a logarithmic trend in performance as corpus size increases without performance reaching an upper bound.", "labels": [], "entities": []}, {"text": "This leads them to believe that the development of large scale training material will yield superior results than further experimentation with machine learning methods on existing smaller scale training corpora.", "labels": [], "entities": []}, {"text": "Recent work has replicated the results on the much more complex task of automatic thesaurus extraction, showing that contextual statistics, collected over a very large corpus, significantly improve system performance (.", "labels": [], "entities": [{"text": "automatic thesaurus extraction", "start_pos": 72, "end_pos": 102, "type": "TASK", "confidence": 0.6500400801499685}]}, {"text": "Other research has shown that query statistics from a web search engine can be used as a substitute for counts collected from large corpora).", "labels": [], "entities": []}, {"text": "To further investigate the benefits of using very large corpora we empirically analyse the convergence behaviour of unigram probability estimates fora range of words with different relative frequencies.", "labels": [], "entities": []}, {"text": "By dramatically increasing the size of the training corpus, we expect our confidence in the probability estimates for each word to increase.", "labels": [], "entities": []}, {"text": "As theory predicts, unigram probability estimates for many words do converge as corpus size grows.", "labels": [], "entities": []}, {"text": "However, contrary to intuition, we found that for many commonplace words, for example tightness, there was no sign of convergence as corpus size approaches one billion words.", "labels": [], "entities": []}, {"text": "This suggests that for at least some words, simply using a much larger corpus to reduce sparseness will not yield reliable estimates.", "labels": [], "entities": []}, {"text": "This leads us to conclude that effective use of large corpora demands, rather than discourages, further research into sophisticated statistical language modelling methods.", "labels": [], "entities": [{"text": "statistical language modelling", "start_pos": 132, "end_pos": 162, "type": "TASK", "confidence": 0.6837202906608582}]}, {"text": "In our case, this means adding extra conditioning to the model.", "labels": [], "entities": []}, {"text": "Only then could we reasonably predict how much training material would be required to ameliorate sparse statistics problems in NLP.", "labels": [], "entities": []}, {"text": "The next section briefly introduces the relevant limit theorems from statistics.", "labels": [], "entities": []}, {"text": "Section 3 describes our experimental procedure and the collection of the billion word corpus.", "labels": [], "entities": []}, {"text": "Section 4 gives examples of words with convergent and non-convergent behaviour covering a range of relative frequencies.", "labels": [], "entities": []}, {"text": "We conclude with a discussion of the implications for language modelling and the use of very large corpora that our results present.", "labels": [], "entities": [{"text": "language modelling", "start_pos": 54, "end_pos": 72, "type": "TASK", "confidence": 0.8418564200401306}]}], "datasetContent": [{"text": "We would like to answer the question: how much training material is required to estimate the unigram probability of a given word with arbitrary confidence.", "labels": [], "entities": []}, {"text": "This is clearly dependent on the relative frequency of the word in question.", "labels": [], "entities": []}, {"text": "Words which appear to have similar probability estimates on small corpora can exhibit quite different convergence behaviour as the sample size increases.", "labels": [], "entities": []}, {"text": "To demonstrate this we compiled a homogeneous corpus of 1.145 billion words of newspaper and newswire text from three existing corpora: the North American News Text Corpus, NANC, the NANC Supplement ( and the Reuters Corpus Volume 1,).", "labels": [], "entities": [{"text": "North American News Text Corpus", "start_pos": 140, "end_pos": 171, "type": "DATASET", "confidence": 0.7729263722896575}, {"text": "NANC", "start_pos": 173, "end_pos": 177, "type": "DATASET", "confidence": 0.9116260409355164}, {"text": "NANC Supplement", "start_pos": 183, "end_pos": 198, "type": "DATASET", "confidence": 0.939185231924057}, {"text": "Reuters Corpus Volume 1", "start_pos": 209, "end_pos": 232, "type": "DATASET", "confidence": 0.9588121026754379}]}, {"text": "The number of words in each corpus is shown in.", "labels": [], "entities": []}, {"text": "These corpora were concatenated together in the order given in without randomising the individual sentence order.", "labels": [], "entities": []}, {"text": "This emulates the process of collecting a large quantity of text and then calculating statistics based counts from the entire collection.", "labels": [], "entities": []}, {"text": "Random shuffling removes the discourse features and natural clustering of words which has such a significant influence on the probability estimates.", "labels": [], "entities": []}, {"text": "We investigate the large-sample convergence behaviour of words that appear at least once in a standard small training corpus, the Penn Treebank (PTB).", "labels": [], "entities": [{"text": "Penn Treebank (PTB)", "start_pos": 130, "end_pos": 149, "type": "DATASET", "confidence": 0.9753156304359436}]}, {"text": "The next section describes the convergence behaviour for words with frequency ranging from the most common down to hapax legomena.", "labels": [], "entities": []}, {"text": "From the entire 1.145 billion word corpus we calculated the gold-standard unigram probability estimate, that is, the relative frequency for each word.", "labels": [], "entities": [{"text": "unigram probability estimate", "start_pos": 74, "end_pos": 102, "type": "METRIC", "confidence": 0.6456046005090078}]}, {"text": "We also calculated the probability estimates for each word using increasing subsets of the full corpus.", "labels": [], "entities": []}, {"text": "These subset corpora were sampled every 5 million words up to 1.145 billion.", "labels": [], "entities": []}, {"text": "To determine the rate of convergence to the goldstandard probability estimate as the training set increases, we plotted the ratio between the subset and gold-standard estimates.", "labels": [], "entities": []}, {"text": "Note that the horizontal lines on all of the graphs are the same distance apart.", "labels": [], "entities": []}, {"text": "The exception is, where there are no lines shows the convergence behaviour of some very frequent closed-class words selected from the PTB.", "labels": [], "entities": [{"text": "PTB", "start_pos": 134, "end_pos": 137, "type": "DATASET", "confidence": 0.9462822079658508}]}, {"text": "This graph shows that for most of these extremely common words, the probability estimates are accurate to within approximately \u00b110% (a ratio of 1 \u00b1 0.1) of their final value fora very small corpus of only 5 million words (the size of the first subset sample).", "labels": [], "entities": []}, {"text": "Some function words, for example, the and in, display much more stable probability estimates even amongst the function words, suggesting their usage is very uniform throughout the corpus.", "labels": [], "entities": []}, {"text": "By chance, there are also some open-class words, such 1.2", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Components of the billion word corpus", "labels": [], "entities": []}]}