{"title": [{"text": "Manipulating Large Corpora for Text Classification", "labels": [], "entities": [{"text": "Text Classification", "start_pos": 31, "end_pos": 50, "type": "TASK", "confidence": 0.7275111377239227}]}], "abstractContent": [{"text": "In this paper, we address the problem of dealing with a large collection of data and propose a method for text classification which manipulates data using two well-known machine learning techniques, Naive Bayes(NB) and Support Vector Ma-chines(SVMs).", "labels": [], "entities": [{"text": "text classification", "start_pos": 106, "end_pos": 125, "type": "TASK", "confidence": 0.7755677700042725}]}, {"text": "NB is based on the assumption of word independence in a text, which makes the computation of it far more efficient.", "labels": [], "entities": []}, {"text": "SVMs, on the other hand, have the potential to handle large feature spaces, which makes it possible to produce better performance.", "labels": [], "entities": []}, {"text": "The training data for SVMs are extracted using NB classifiers according to the category hierarchies , which makes it possible to reduce the amount of computation necessary for classification without sacrificing accuracy.", "labels": [], "entities": [{"text": "SVMs", "start_pos": 22, "end_pos": 26, "type": "TASK", "confidence": 0.9573900103569031}, {"text": "accuracy", "start_pos": 211, "end_pos": 219, "type": "METRIC", "confidence": 0.9919902682304382}]}], "introductionContent": [{"text": "As the volume of online documents has drastically increased, text classification has become more important, and a growing number of statistical and machine learning techniques have been applied to the task(,,,,),).", "labels": [], "entities": [{"text": "text classification", "start_pos": 61, "end_pos": 80, "type": "TASK", "confidence": 0.837770938873291}]}, {"text": "Most of them use the Reuters-21578 articles 1 in the evalu-ations of their methods, since the corpus has become a benchmark, and their results are thus easily compared with other results.", "labels": [], "entities": [{"text": "Reuters-21578 articles 1", "start_pos": 21, "end_pos": 45, "type": "DATASET", "confidence": 0.9606508612632751}]}, {"text": "It is generally agreed that these methods using statistical and machine learning techniques are effective for classification task, since most of them showed significant improvement (the performance over 0.85 F1 score) for Reuters-21578,,).", "labels": [], "entities": [{"text": "classification task", "start_pos": 110, "end_pos": 129, "type": "TASK", "confidence": 0.9213258028030396}, {"text": "F1 score", "start_pos": 208, "end_pos": 216, "type": "METRIC", "confidence": 0.9798993766307831}, {"text": "Reuters-21578", "start_pos": 222, "end_pos": 235, "type": "DATASET", "confidence": 0.974168062210083}]}, {"text": "More recently, some researchers have applied their techniques to larger corpora such as web pages in Internet applications,,).", "labels": [], "entities": []}, {"text": "The increasing number of documents and categories, however, often hampers the development of practical classification systems, mainly due to statistical, computational, and representational problems.", "labels": [], "entities": []}, {"text": "There are at least two strategies for solving these problems.", "labels": [], "entities": []}, {"text": "One is to use category hierarchies.", "labels": [], "entities": []}, {"text": "The idea behind this is that when humans organize extensive data sets into fine-grained categories, category hierarchies are often employed to make the large collection of categories more manageable.", "labels": [], "entities": []}, {"text": "McCallum et. al. presented a method called 'shrinkage' to improve parameter estimates by taking advantage of a hierarchy).", "labels": [], "entities": []}, {"text": "They tested their method using three different real-world datasets: 20,000 articles from UseNet, 6,440 web pages from the industry sector, and 14,831 pages from Yahoo, and showed improved performance.", "labels": [], "entities": []}, {"text": "Dumais et. al. used SVMs and classified hierarchical web content consisting of 50,078 web pages for training, and 10,024 for testing, with promising results.", "labels": [], "entities": []}, {"text": "The other is to use \u00a2 \u00a1 \u00a4 \u00a3 \u00a5 \u00a7 \u00a6 \u00a9 \u00a8 methods which are learning algorithms that construct a set of classifiers and then classify new data by taking a (weighted) vote of their predictions).", "labels": [], "entities": [{"text": "\u00a9 \u00a8", "start_pos": 34, "end_pos": 37, "type": "METRIC", "confidence": 0.9045558571815491}]}, {"text": "One of the methods for constructing ensembles manipulates the training examples to generate multiple hypotheses.", "labels": [], "entities": []}, {"text": "The most straightforward way is called \u00a1 . It presents the learning algorithm with a training set that consists of a sample of \u00a6 examples drawn randomly with replacement from the original training set.", "labels": [], "entities": []}, {"text": "The second method is to construct the training sets by leaving out disjoint subsets of the training data.", "labels": [], "entities": []}, {"text": "The third is illustrated by the AD-ABOOST algorithm.", "labels": [], "entities": []}, {"text": "Dietterich has compared these methods.", "labels": [], "entities": []}, {"text": "He reported that in low-noise data, AD-ABOOST performs well, while in high-noise cases, it yields overfitting because ADABOOST puts a large amount of weight on the mislabeled examples.", "labels": [], "entities": []}, {"text": "Bagging works well on both the noisy and the noisefree data because it focuses on the statistical problem which arises when the amount of training data available is too small, and noise increases this statistical problem.", "labels": [], "entities": [{"text": "Bagging", "start_pos": 0, "end_pos": 7, "type": "TASK", "confidence": 0.9636638164520264}]}, {"text": "However, it is not clear whether 'works well' means that it exponentially reduces the amount of computation necessary for classification, while sacrificing only a small amount of accuracy, or whether it is statistically significantly better than other methods.", "labels": [], "entities": [{"text": "classification", "start_pos": 122, "end_pos": 136, "type": "TASK", "confidence": 0.9772277474403381}, {"text": "accuracy", "start_pos": 179, "end_pos": 187, "type": "METRIC", "confidence": 0.9987558126449585}]}, {"text": "In this paper, we address the problem of dealing with a large collection of data and report on an empirical study for text classification which manipulates data using two well-known machine learning techniques, Naive Bayes(NB) and Support Vector Machines(SVMs).", "labels": [], "entities": [{"text": "text classification", "start_pos": 118, "end_pos": 137, "type": "TASK", "confidence": 0.7777813673019409}]}, {"text": "NB probabilistic classifiers are based on the assumption of word independence in a text which makes the computation of the NB classifiers far more efficient.", "labels": [], "entities": [{"text": "NB probabilistic classifiers", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.5690287252267202}]}, {"text": "SVMs, on the other hand, have the potential to handle large feature spaces, since SVMs use overfitting protection which does not necessarily depend on the number of features, and thus makes it possible to produce better performance.", "labels": [], "entities": []}, {"text": "The basic idea of our approach is quite simple: We solve simple classification problems using NB and more complex and difficult problems using SVMs.", "labels": [], "entities": []}, {"text": "As in previous research, we use category hierarchies.", "labels": [], "entities": []}, {"text": "We use all the training data for NB.", "labels": [], "entities": [{"text": "NB", "start_pos": 33, "end_pos": 35, "type": "TASK", "confidence": 0.748995840549469}]}, {"text": "The training data for SVMs, on the other hand, is extracted using NB classifiers.", "labels": [], "entities": [{"text": "SVMs", "start_pos": 22, "end_pos": 26, "type": "TASK", "confidence": 0.9316632151603699}]}, {"text": "The training data is learned by NB using cross-validation according to the hierarchical structure of categories, and only the documents which could not classify correctly by NB classifiers in each category level are extracted as the training data of SVMs.", "labels": [], "entities": [{"text": "NB", "start_pos": 32, "end_pos": 34, "type": "DATASET", "confidence": 0.876079261302948}]}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "The next section provides the basic framework of NB and SVMs.", "labels": [], "entities": []}, {"text": "We then describe our classification method.", "labels": [], "entities": [{"text": "classification", "start_pos": 21, "end_pos": 35, "type": "TASK", "confidence": 0.964419960975647}]}, {"text": "Finally, we report some experiments using 279,303 documents in the Reuters 1996 corpus with a discussion of evaluation.", "labels": [], "entities": [{"text": "Reuters 1996 corpus", "start_pos": 67, "end_pos": 86, "type": "DATASET", "confidence": 0.9781447251637777}]}], "datasetContent": [{"text": "We evaluated the method using the, and stop word removal.", "labels": [], "entities": []}, {"text": "We use ten-fold cross validation for learning NB parameters.", "labels": [], "entities": []}, {"text": "For evaluating the effectiveness of category assignments, we use the standard recall, precision, and \u00bd \u00bf \u00be measures.", "labels": [], "entities": [{"text": "recall", "start_pos": 78, "end_pos": 84, "type": "METRIC", "confidence": 0.9994182586669922}, {"text": "precision", "start_pos": 86, "end_pos": 95, "type": "METRIC", "confidence": 0.9993602633476257}, {"text": "\u00bd \u00bf \u00be", "start_pos": 101, "end_pos": 106, "type": "METRIC", "confidence": 0.781783660252889}]}, {"text": "Recall denotes the ratio of correct assignments by the system divided by the total number of correct assignments.", "labels": [], "entities": [{"text": "Recall", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9816863536834717}]}, {"text": "Precision is the ratio of correct assignments by the system divided by the total number of the system's assignments. and precision (\u00c0 ) with an equal weight is", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9941375255584717}, {"text": "precision (\u00c0 )", "start_pos": 121, "end_pos": 135, "type": "METRIC", "confidence": 0.8733711242675781}]}], "tableCaptions": [{"text": " Table 1: Top level categories", "labels": [], "entities": []}, {"text": " Table 3: # of selected documents and categorization accuracy", "labels": [], "entities": []}, {"text": " Table 4: Categorization accuracy by category level", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9914394617080688}]}, {"text": " Table 5: Non-hierarchical v.s. Hierarchical categorization accuracy", "labels": [], "entities": []}]}