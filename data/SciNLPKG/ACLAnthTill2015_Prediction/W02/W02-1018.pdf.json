{"title": [{"text": "A Phrase-Based, Joint Probability Model for Statistical Machine Translation", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 44, "end_pos": 75, "type": "TASK", "confidence": 0.8421031832695007}]}], "abstractContent": [{"text": "We present a joint probability model for statistical machine translation, which automatically learns word and phrase equivalents from bilingual corpora.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 41, "end_pos": 72, "type": "TASK", "confidence": 0.7184987962245941}]}, {"text": "Translations produced with parameters estimated using the joint model are more accurate than translations produced using IBM Model 4. 1 Motivation Most of the noisy-channel-based models used in statistical machine translation (MT) (Brown et al., 1993) are conditional probability models.", "labels": [], "entities": [{"text": "statistical machine translation (MT)", "start_pos": 194, "end_pos": 230, "type": "TASK", "confidence": 0.7884716441233953}]}, {"text": "In the noisy-channel framework, each source sentence e in a parallel corpus is assumed to \"generate\" a target sentence f by means of a stochastic process, whose parameters are estimated using traditional EM techniques (Dempster et al., 1977).", "labels": [], "entities": []}, {"text": "The generative model explains how source words are mapped into target words and how target words are reordered to yield well-formed target sentences.", "labels": [], "entities": []}, {"text": "A variety of methods are used to account for the reordering stage: word-based (Brown et al., 1993), template-based (Och et al., 1999), and syntax-based (Yamada and Knight, 2001), to name just a few.", "labels": [], "entities": []}, {"text": "Although these models use different generative processes to explain how translated words are reordered in a target language, at the lexical level they are quite similar ; all these models assume that source words are individually translated into target words.", "labels": [], "entities": []}, {"text": "1 1 The individual words may contain a non-existent element, called NULL.", "labels": [], "entities": []}, {"text": "We suspect that MT researchers have so far chosen to automatically learn translation lexicons defined only over words for primarily pragmatic reasons.", "labels": [], "entities": [{"text": "MT", "start_pos": 16, "end_pos": 18, "type": "TASK", "confidence": 0.9815426468849182}]}, {"text": "Large scale bilingual corpora with vocabularies in the range of hundreds of thousands yield very large translation lexicons.", "labels": [], "entities": []}, {"text": "Tuning the probabilities associated with these large lexicons is a difficult enough task to deter one from trying to scale up to learning phrase-based lexicons.", "labels": [], "entities": []}, {"text": "Unfortunately, trading space requirements and efficiency for explanatory power often yields non-intuitive results.", "labels": [], "entities": []}, {"text": "Consider, for example, the parallel corpus of three sentence pairs shown in Figure 1.", "labels": [], "entities": []}, {"text": "Intuitively, if we allow any Source words to be aligned to any Target words, the best alignment that we can come up with is the one in Figure 1.c.", "labels": [], "entities": []}, {"text": "Sentence pair (S2, T2) offers strong evidence that \"b c\" in language S means the same thing as \"x\" in language T.", "labels": [], "entities": []}, {"text": "On the basis of this evidence, we expect the system to also learn from sentence pair (S1, T1) that \"a\" in language S means the same thing as \"y\" in language T.", "labels": [], "entities": []}, {"text": "Unfortunately , if one works with translation models that do not allow Target words to be aligned to more than one Source word-as it is the casein the IBM models (Brown et al., 1993)-it is impossible to learn that the phrase \"b c\" in language S means the same thing as word \"x\" in language T.", "labels": [], "entities": []}, {"text": "The IBM Model 4 (Brown et al., 1993), for example, converges to the word alignments shown in Figure 1.b and learns the translation probabilities shown in Figure 1.a.", "labels": [], "entities": []}, {"text": "2 Since in the IBM model one cannot link a Target word to more than a Source word, the training procedure 2 To train the IBM-4 model, we used Giza (Al-Onaizan et al., 1999).", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "To evaluate our system, we trained both Giza (IBM Model 4) ( and our joint probability model on a French-English parallel corpus of 100,000 sentence pairs from the Hansard corpus.", "labels": [], "entities": [{"text": "Hansard corpus", "start_pos": 164, "end_pos": 178, "type": "DATASET", "confidence": 0.9248357117176056}]}, {"text": "The sentences in the corpus were at most 20 words long.", "labels": [], "entities": []}, {"text": "The English side had a total of 1,073,480 words (21,484 unique tokens).", "labels": [], "entities": [{"text": "English", "start_pos": 4, "end_pos": 11, "type": "DATASET", "confidence": 0.9724240303039551}]}, {"text": "The French side had a total of 1,177,143 words (28,132 unique tokens).", "labels": [], "entities": [{"text": "French", "start_pos": 4, "end_pos": 10, "type": "DATASET", "confidence": 0.932390570640564}]}, {"text": "We translated 500 unseen sentences, which were uniformly distributed across lengths 6, 8, 10, 15, and 20.", "labels": [], "entities": []}, {"text": "For each group of 100 sentences, we manually determined the number of sentences translated perfectly by the IBM model decoder of   ability model.", "labels": [], "entities": [{"text": "IBM model decoder", "start_pos": 108, "end_pos": 125, "type": "DATASET", "confidence": 0.9013819297154745}]}, {"text": "We also evaluated the translations automatically, using the IBM-Bleu metric).", "labels": [], "entities": [{"text": "translations", "start_pos": 22, "end_pos": 34, "type": "TASK", "confidence": 0.9590117335319519}]}, {"text": "The results in show that the phrased-based translation model proposed in this paper significantly outperforms IBM Model 4 on both the subjective and objective metrics.", "labels": [], "entities": [{"text": "phrased-based translation", "start_pos": 29, "end_pos": 54, "type": "TASK", "confidence": 0.7260460555553436}]}], "tableCaptions": [{"text": " Table 1: Comparison of IBM and Phrase-Based, Joint Probability Models on a translation task.", "labels": [], "entities": [{"text": "translation task", "start_pos": 76, "end_pos": 92, "type": "TASK", "confidence": 0.9218533337116241}]}]}