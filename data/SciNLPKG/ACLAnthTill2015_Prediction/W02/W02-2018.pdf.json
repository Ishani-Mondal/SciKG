{"title": [{"text": "A comparison of algorithms for maximum entropy parameter estimation", "labels": [], "entities": [{"text": "maximum entropy parameter estimation", "start_pos": 31, "end_pos": 67, "type": "TASK", "confidence": 0.6800573021173477}]}], "abstractContent": [{"text": "Conditional maximum entropy (ME) models provide a general purpose machine learning technique which has been successfully applied to fields as diverse as computer vision and econometrics, and which is used fora wide variety of classification problems in natural language processing.", "labels": [], "entities": []}, {"text": "However, the flexibility of ME models is not without cost.", "labels": [], "entities": []}, {"text": "While parameter estimation for ME models is conceptually straightforward, in practice ME models for typical natural language tasks are very large, and may well contain many thousands of free parameters.", "labels": [], "entities": []}, {"text": "In this paper, we consider a number of algorithms for estimating the parameters of ME models , including iterative scaling, gradient ascent, conjugate gradient, and variable metric methods.", "labels": [], "entities": []}, {"text": "Surprisingly , the standardly used iterative scaling algorithms perform quite poorly in comparison to the others, and for all of the test problems, a limited-memory variable metric algorithm outperformed the other choices.", "labels": [], "entities": []}], "introductionContent": [{"text": "Maximum entropy (ME) models, variously known as log-linear, Gibbs, exponential, and multinomial logit models, provide a general purpose machine learning technique for classification and prediction which has been successfully applied to fields as diverse as computer vision and econometrics.", "labels": [], "entities": [{"text": "classification and prediction", "start_pos": 167, "end_pos": 196, "type": "TASK", "confidence": 0.8845363060633341}]}, {"text": "In natural language processing, recent years have seen ME techniques used for sentence boundary detection, part of speech tagging, parse selection and ambiguity resolution, and stochastic attribute-value grammars, to name just a few applications.", "labels": [], "entities": [{"text": "sentence boundary detection", "start_pos": 78, "end_pos": 105, "type": "TASK", "confidence": 0.7083720167477926}, {"text": "speech tagging", "start_pos": 115, "end_pos": 129, "type": "TASK", "confidence": 0.7323934882879257}, {"text": "parse selection and ambiguity resolution", "start_pos": 131, "end_pos": 171, "type": "TASK", "confidence": 0.8865685105323792}]}, {"text": "A leading advantage of ME models is their flexibility: they allow stochastic rule systems to be augmented with additional syntactic, semantic, and pragmatic features.", "labels": [], "entities": []}, {"text": "However, the richness of the representations is not without cost.", "labels": [], "entities": []}, {"text": "Even modest ME models can require considerable computational resources and very large quantities of annotated training data in order to accurately estimate the model's parameters.", "labels": [], "entities": []}, {"text": "While parameter estimation for ME models is conceptually straightforward, in practice ME models for typical natural language tasks are usually quite large, and frequently contain hundreds of thousands of free parameters.", "labels": [], "entities": []}, {"text": "Estimation of such large models is not only expensive, but also, due to sparsely distributed features, sensitive to round-off errors.", "labels": [], "entities": []}, {"text": "Thus, highly efficient, accurate, scalable methods are required for estimating the parameters of practical models.", "labels": [], "entities": []}, {"text": "In this paper, we consider a number of algorithms for estimating the parameters of ME models, including Generalized Iterative Scaling and Improved Iterative Scaling, as well as general purpose optimization techniques such as gradient ascent, conjugate gradient, and variable metric methods.", "labels": [], "entities": []}, {"text": "Surprisingly, the widely used iterative scaling algorithms perform quite poorly, and for all of the test problems, a limited memory variable metric algorithm outperformed the other choices.", "labels": [], "entities": []}], "datasetContent": [{"text": "To compare the algorithms described in \u00a72, we applied the implementation outlined in the previous section to four training data sets (described in of stochastic attribute value grammars, one with a small set of SCFG-like features, and with a very large set of fine-grained lexical features ().", "labels": [], "entities": []}, {"text": "The 'summary' dataset is part of a sentence extraction task (Osborne, to appear), and the 'shallow' dataset is drawn from a text chunking application).", "labels": [], "entities": [{"text": "sentence extraction task", "start_pos": 35, "end_pos": 59, "type": "TASK", "confidence": 0.770629366238912}]}, {"text": "These datasets vary widely in their size and composition, and are representative of the kinds of datasets typically encountered in applying ME models to NLP classification tasks.", "labels": [], "entities": [{"text": "NLP classification tasks", "start_pos": 153, "end_pos": 177, "type": "TASK", "confidence": 0.881920576095581}]}, {"text": "The results of applying each of the parameter estimation algorithms to each of the datasets is summarized in.", "labels": [], "entities": []}, {"text": "For each run, we report the KL divergence between the fitted model and the training data at convergence, the prediction accuracy of fitted model on a held-out test set (the fraction of contexts for which the event with the highest probability under the model also had the highest probability under the reference distribution), the number of iterations required, the number of log-likelihood and gradient evaluations required (algorithms which use a line search may require several function evaluations per iteration), and the total elapsed time (in seconds).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 120, "end_pos": 128, "type": "METRIC", "confidence": 0.9598575830459595}]}, {"text": "There area few things to observe about these results.", "labels": [], "entities": []}, {"text": "First, while IIS converges in fewer steps the GIS, it takes substantially more time.", "labels": [], "entities": [{"text": "IIS", "start_pos": 13, "end_pos": 16, "type": "TASK", "confidence": 0.9767617583274841}, {"text": "GIS", "start_pos": 46, "end_pos": 49, "type": "DATASET", "confidence": 0.7522685527801514}]}, {"text": "At least for this implementation, the additional bookkeeping overhead required by IIS more than cancels any improvements in speed offered by accelerated convergence.", "labels": [], "entities": []}, {"text": "This maybe a misleading conclusion, however, since a more finely tuned implementation of IIS may well take much less time per iteration than the one used for these experiments.", "labels": [], "entities": [{"text": "IIS", "start_pos": 89, "end_pos": 92, "type": "TASK", "confidence": 0.9266062378883362}]}, {"text": "However, even if each iteration of IIS could be made as fast as an iteration of GIS (which seems unlikely), the benefits of IIS over GIS would in these cases be quite modest.", "labels": [], "entities": []}, {"text": "Second, note that for three of the four datasets, the KL divergence at convergence is roughly the same for all of the algorithms.", "labels": [], "entities": [{"text": "KL divergence", "start_pos": 54, "end_pos": 67, "type": "METRIC", "confidence": 0.7188054323196411}]}, {"text": "For the 'summary' dataset, however, they differ by up to two orders of magnitude.", "labels": [], "entities": []}, {"text": "This is an indication that the convergence test in is sensitive to the rate of convergence and thus to the choice of algorithm.", "labels": [], "entities": []}, {"text": "Any degree of precision desired could be reached by any of the algorithms, with the appropriate value of \u03b5.", "labels": [], "entities": [{"text": "precision", "start_pos": 14, "end_pos": 23, "type": "METRIC", "confidence": 0.9986816048622131}]}, {"text": "However, GIS, say, would require many more iterations than reported in to reach the precision achieved by the limited memory variable metric algorithm.", "labels": [], "entities": [{"text": "precision", "start_pos": 84, "end_pos": 93, "type": "METRIC", "confidence": 0.9985496401786804}]}, {"text": "Third, the prediction accuracy is, inmost cases, more or less the same for all of the algorithms.", "labels": [], "entities": [{"text": "prediction", "start_pos": 11, "end_pos": 21, "type": "TASK", "confidence": 0.7504841089248657}, {"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.8627609014511108}]}, {"text": "Some variability is to be expected-all of the data sets being considered here are badly ill-conditioned, and many different models will yield the same likelihood.", "labels": [], "entities": []}, {"text": "Ina few cases, however, the prediction accuracy differs more substantially.", "labels": [], "entities": [{"text": "prediction", "start_pos": 28, "end_pos": 38, "type": "TASK", "confidence": 0.6759123802185059}, {"text": "accuracy", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.8473955392837524}]}, {"text": "For the two SAVG data sets, GIS has a small advantage over the other methods.", "labels": [], "entities": [{"text": "SAVG data sets", "start_pos": 12, "end_pos": 26, "type": "DATASET", "confidence": 0.8025611042976379}, {"text": "GIS", "start_pos": 28, "end_pos": 31, "type": "METRIC", "confidence": 0.5118395090103149}]}, {"text": "More dramatically, both iterative scaling methods perform very poorly on the 'shallow' dataset.", "labels": [], "entities": []}, {"text": "In this case, the training data is very sparse.", "labels": [], "entities": []}, {"text": "Many features are nearly 'pseudo-minimal' in the sense of, and so receive weights approaching \u2212\u221e.", "labels": [], "entities": []}, {"text": "Smoothing the reference probabilities would likely improve the results for all of the methods and reduce the observed differences.", "labels": [], "entities": []}, {"text": "However, this does suggest that gradient-based methods are robust to certain problems with the training data.", "labels": [], "entities": []}, {"text": "Finally, the most significant lesson to be drawn from these results is that, with the exception of steepest ascent, gradient-based methods outperform iterative scaling by a wide margin for almost all the datasets, as measured by both number of function evaluations and by the total elapsed time.", "labels": [], "entities": []}, {"text": "And, in each case, the limited memory variable metric algo-  rithm performs substantially better than any of the competing methods.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Datasets used in experiments", "labels": [], "entities": []}, {"text": " Table 2: Results of comparison.", "labels": [], "entities": []}]}