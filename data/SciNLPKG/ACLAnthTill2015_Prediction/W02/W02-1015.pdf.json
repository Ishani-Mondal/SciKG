{"title": [{"text": "Handling noisy training and testing data", "labels": [], "entities": []}], "abstractContent": [{"text": "In the field of empirical natural language processing, researchers constantly deal with large amounts of marked-up data; whether the markup is done by the researcher or someone else, human nature dictates that it will have errors in it.", "labels": [], "entities": [{"text": "empirical natural language processing", "start_pos": 16, "end_pos": 53, "type": "TASK", "confidence": 0.6838591247797012}]}, {"text": "This paper will more fully characterise the problem and discuss whether and when (and how) to correct the errors.", "labels": [], "entities": []}, {"text": "The discussion is illustrated with specific examples involving function tagging in the Penn treebank.", "labels": [], "entities": [{"text": "function tagging", "start_pos": 63, "end_pos": 79, "type": "TASK", "confidence": 0.7378251552581787}, {"text": "Penn treebank", "start_pos": 87, "end_pos": 100, "type": "DATASET", "confidence": 0.9926072657108307}]}], "introductionContent": [], "datasetContent": [{"text": "As a sort of case study in the meta-algorithms presented in the previous sections, we will look at the problem of function tagging in the treebank.", "labels": [], "entities": [{"text": "function tagging", "start_pos": 114, "end_pos": 130, "type": "TASK", "confidence": 0.7245102524757385}]}, {"text": "describe an algorithm for marking sentence constituents with function tags such as SBJ (for sentence subjects) and TMP (for temporal phrases).", "labels": [], "entities": []}, {"text": "We trained this algorithm on sections 02-21 of the treebank and ran it on section 24 (the development corpus), then analysed the output.", "labels": [], "entities": []}, {"text": "First, we printed out every constituent with a function tag error.", "labels": [], "entities": []}, {"text": "We then examined the sentence in which each occurred, and determined whether the error was in the algorithm or in the treebank, or elsewhere, as reported in.", "labels": [], "entities": []}, {"text": "Of the errors we examined, less than half were due solely to an algorithmic failure in the function tagger itself.", "labels": [], "entities": []}, {"text": "The next largest category was parse error: this function tagging algorithm requires parsed input, and in these cases, that input was incorrect and led the function tagger astray; had the tagger received the treebank parse, it would have given correct output.", "labels": [], "entities": [{"text": "parse error", "start_pos": 30, "end_pos": 41, "type": "TASK", "confidence": 0.7181327491998672}, {"text": "function tagging", "start_pos": 48, "end_pos": 64, "type": "TASK", "confidence": 0.7212854325771332}]}, {"text": "In just under a fifth of the reported \"errors\", the algorithm was correct and the treebank was definitely wrong.", "labels": [], "entities": []}, {"text": "The remainder of cases we have identified either as Type C errors-wherein the tagger agreed with many training examples, but the \"correct\" tag agreed with many others-or at least \"dubious\", in the cases that weren't common enough to be systematic inconsistencies but where the guidelines did not clearly prefer the treebank tag over the tagger output, or vice versa.", "labels": [], "entities": []}, {"text": "Next, we compiled all the noted treebank errors and their corrections.", "labels": [], "entities": []}, {"text": "The most common correction involved simply adding, removing, or changing a function tag to what the algorithm output (with a net effect of improving our score).", "labels": [], "entities": []}, {"text": "However, it should be noted that when classifying reported errors, we examined their contexts, and in so doing discovered other sorts of treebank error.", "labels": [], "entities": []}, {"text": "Mistags and misparses did not directly affect us; some function tag corrections actually decreased our score.", "labels": [], "entities": []}, {"text": "All corrections were applied anyway, in the hope of cleaner evaluations for future researchers.", "labels": [], "entities": [{"text": "corrections", "start_pos": 4, "end_pos": 15, "type": "METRIC", "confidence": 0.9443745613098145}]}, {"text": "Finally, we re-evaluated the algorithm's output on the corrected development corpus.", "labels": [], "entities": []}, {"text": "9 Precision, recall, and F-measure are calculated as in).", "labels": [], "entities": [{"text": "Precision", "start_pos": 2, "end_pos": 11, "type": "METRIC", "confidence": 0.9990310668945312}, {"text": "recall", "start_pos": 13, "end_pos": 19, "type": "METRIC", "confidence": 0.9990284442901611}, {"text": "F-measure", "start_pos": 25, "end_pos": 34, "type": "METRIC", "confidence": 0.9993191957473755}]}, {"text": "The false error rate is simply the percent by which the error is reduced; in terms of the performance on the treebank version (t) and the fixed version (f ), This is the percentage of the reported errors that are due to treebank error.", "labels": [], "entities": [{"text": "false error rate", "start_pos": 4, "end_pos": 20, "type": "METRIC", "confidence": 0.8893877665201823}]}, {"text": "The topicalisation result is nice, but since the TPC tag is fairly rare (121 occurrences in section 24), these numbers may not be robust.", "labels": [], "entities": []}, {"text": "It is interesting, though, that the false error rate on the two major tag groups is so similar-roughly 20% in precision and 5% in recall for each, leading to 10% in F-measure.", "labels": [], "entities": [{"text": "false error rate", "start_pos": 36, "end_pos": 52, "type": "METRIC", "confidence": 0.9187787969907125}, {"text": "precision", "start_pos": 110, "end_pos": 119, "type": "METRIC", "confidence": 0.9993100166320801}, {"text": "recall", "start_pos": 130, "end_pos": 136, "type": "METRIC", "confidence": 0.9993135929107666}, {"text": "F-measure", "start_pos": 165, "end_pos": 174, "type": "METRIC", "confidence": 0.9928469061851501}]}, {"text": "First of all, this parallelism strengthens our assertion that the false error rate, though calculated on a development corpus, can be presumed to apply equally to the test corpus, since it indicates that the human missed tag and mistag rates maybe roughly constant.", "labels": [], "entities": [{"text": "false error rate", "start_pos": 66, "end_pos": 82, "type": "METRIC", "confidence": 0.8901729385058085}]}, {"text": "Second, the much higher improvement on precision indicates that the majority of treebank error (at least in the realm of function tagging) is due to human annotators forgetting a tag.", "labels": [], "entities": [{"text": "precision", "start_pos": 39, "end_pos": 48, "type": "METRIC", "confidence": 0.9994307160377502}, {"text": "function tagging", "start_pos": 121, "end_pos": 137, "type": "TASK", "confidence": 0.7151281833648682}]}], "tableCaptions": [{"text": " Table 2: Function tagging results, adjusted for treebank error", "labels": [], "entities": [{"text": "Function tagging", "start_pos": 10, "end_pos": 26, "type": "TASK", "confidence": 0.7342738509178162}]}]}