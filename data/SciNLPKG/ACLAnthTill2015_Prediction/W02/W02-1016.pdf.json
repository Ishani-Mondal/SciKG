{"title": [], "abstractContent": [{"text": "We describe and evaluate the application of a spectral clustering technique (Ng et al., 2002) to the unsupervised clustering of German verbs.", "labels": [], "entities": []}, {"text": "Our previous work has shown that standard clustering techniques succeed in inducing Levin-style semantic classes from verb subcategorisa-tion information.", "labels": [], "entities": []}, {"text": "But clustering in the very high dimensional spaces that we use is fraught with technical and conceptual difficulties.", "labels": [], "entities": []}, {"text": "Spectral clustering performs a dimensionality reduction on the verb frame patterns, and provides a robustness and efficiency that standard clustering methods do not display indirect use.", "labels": [], "entities": [{"text": "Spectral clustering", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8261026740074158}]}, {"text": "The clustering results are evaluated according to the alignment (Christianini et al., 2002) between the Gram matrix defined by the cluster output and the corresponding matrix defined by a gold standard.", "labels": [], "entities": []}], "introductionContent": [{"text": "Standard multivariate clustering technology (such as k-Means) can be applied to the problem of inferring verb classes from information about the estimated prevalence of verb frame patterns (Schulte im).", "labels": [], "entities": []}, {"text": "But one of the problems with multivariate clustering is that it is something of a black art when applied to high-dimensional natural language data.", "labels": [], "entities": [{"text": "multivariate clustering", "start_pos": 29, "end_pos": 52, "type": "TASK", "confidence": 0.7876733541488647}]}, {"text": "The search space is very large, and the available techniques for searching this large space do not offer guarantees of global optimality.", "labels": [], "entities": []}, {"text": "In response to this insight, the present work applies a spectral clustering technique () to the verb frame patterns.", "labels": [], "entities": []}, {"text": "At the heart of this approach is a transformation of the original input into a set of orthogonal eigenvectors.", "labels": [], "entities": []}, {"text": "We work in the space defined by the first few eigenvectors, using standard clustering techniques in the reduced space.", "labels": [], "entities": []}, {"text": "The spectral clustering technique has been shown to handle difficult clustering problems in image processing, offers principled methods for initializing cluster centers, and (in the version that we use) has no random component.", "labels": [], "entities": [{"text": "spectral clustering", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.7098377794027328}, {"text": "image processing", "start_pos": 92, "end_pos": 108, "type": "TASK", "confidence": 0.7520618140697479}]}, {"text": "The clustering results are evaluated according to their alignment with a gold standard.", "labels": [], "entities": []}, {"text": "Alignment is Pearson correlation between corresponding elements of the Gram matrices, which has been suggested as a measure of agreement between a clustering and a distance measure ().", "labels": [], "entities": [{"text": "Alignment", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.7931480407714844}, {"text": "Pearson correlation", "start_pos": 13, "end_pos": 32, "type": "METRIC", "confidence": 0.9182625412940979}]}, {"text": "We are also able to use this measure to quantify the fit between a clustering result and the distance matrix that serves as input to clustering.", "labels": [], "entities": []}, {"text": "The evidence is that the spectral technique is more effective than the methods that have previously been tried.", "labels": [], "entities": []}], "datasetContent": [{"text": "We clustered the verb frames data using our version of the algorithm in ().", "labels": [], "entities": []}, {"text": "To calculate the distance d between two verbs v 1 and v 2 we used a range of measures: the cosine of the angle between the two vectors of frame probabilities, a flattened version of the cosine measure in which all non-zero counts are replaced by 1.0 (labelled bcos, for binarized cosine, in Table 3), and skew divergence, recently shown as an effective measure for distributional similarity.", "labels": [], "entities": []}, {"text": "This last is defined in terms of KL-divergence, and includes a free weight parameter w, which we set to 0.9, following, Skew-divergence is asymmetric in its arguments, but our technique needs asymmetric measure,so we calculate it in both directions and use the larger value.", "labels": [], "entities": []}, {"text": "contains four results for each of three distance measures (cos,bcos and skew).", "labels": [], "entities": []}, {"text": "The first line of each set gives the results when the spectral algorithm is provided with the prior knowledge that k = 14.", "labels": [], "entities": []}, {"text": "The second line gives the results when the standard k-Means algorithm is used, again with k = 14.", "labels": [], "entities": []}, {"text": "In the third line of each set, the value of k is determined from the eigenvalues, as described above.", "labels": [], "entities": []}, {"text": "For cos 12 clusters are chosen, for bcos the chosen value is 17, and for skew it is 16.", "labels": [], "entities": []}, {"text": "The final line of each set gives the results when the standard algorithm is used, but k is set to the value selected for that distance measure by the spectral method.", "labels": [], "entities": []}, {"text": "For standard k-Means, the initialization strategy from () does not apply (and does notwork well in any case), so we used 100 random replications of the initialization, each time initializing the cluster centers with k randomly chosen data points.", "labels": [], "entities": []}, {"text": "We report the result that had the highest alignment with the distance measure (cf. Section 5.1).", "labels": [], "entities": [{"text": "alignment", "start_pos": 42, "end_pos": 51, "type": "METRIC", "confidence": 0.987299382686615}, {"text": "distance measure", "start_pos": 61, "end_pos": 77, "type": "METRIC", "confidence": 0.9345569014549255}]}, {"text": "(Meil\u02d8 a and Shi, 2001) provide analysis indicating that their MNcut algorithm (another spectral clustering technique) will be exact when the eigenvectors used for clustering are piecewise constant.: Performance of the clustering algorithms the eigenvectors appear to correspond to a partition of the data into a small number of tight clusters.", "labels": [], "entities": []}, {"text": "Taken as a whole they induce the clusterings reported in.", "labels": [], "entities": []}, {"text": "Pearson correlation between corresponding elements of the Gram matrices has been suggested as a measure of agreement between a clustering and a distance measure ().", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 0, "end_pos": 19, "type": "METRIC", "confidence": 0.8384310007095337}, {"text": "agreement", "start_pos": 107, "end_pos": 116, "type": "METRIC", "confidence": 0.9485712051391602}]}, {"text": "Since we can convert a clustering into a distance measure, alignment can be used in a number of ways, including comparison of clusterings against each other.", "labels": [], "entities": []}, {"text": "For evaluation, three alignment-based measures are particularly relevant: \u2022 The alignment between the gold standard and the distance measure reflects the presence or absence in the distance measure of evidential support for the relationships that the clustering algorithm is supposed to infer.", "labels": [], "entities": []}, {"text": "This is the column labelled \"Support\" in.", "labels": [], "entities": [{"text": "Support", "start_pos": 29, "end_pos": 36, "type": "METRIC", "confidence": 0.9696758985519409}]}, {"text": "\u2022 The alignment between the clusters inferred by the algorithm and the distance measure reflects the confidence that the algorithm has in the relationships that it has chosen.", "labels": [], "entities": []}, {"text": "This is the column labelled \"Confidence\" in.", "labels": [], "entities": []}, {"text": "\u2022 The alignment between the gold standard and the inferred clusters reflects the quality of the result.", "labels": [], "entities": []}, {"text": "This is the column labelled \"Quality\" in.", "labels": [], "entities": [{"text": "Quality", "start_pos": 29, "end_pos": 36, "type": "METRIC", "confidence": 0.9709345102310181}]}, {"text": "We hope that when the algorithms are confident they will also be right, and that when the data strongly supports a distinction the algorithms will find it. contains our data.", "labels": [], "entities": []}, {"text": "The columns based on various forms of alignment have been discussed above.", "labels": [], "entities": [{"text": "alignment", "start_pos": 38, "end_pos": 47, "type": "TASK", "confidence": 0.8311764001846313}]}, {"text": "Clusterings are also sets of pairs, so, when the Gram matrices are discrete, we can also provide the standard measures of precision, recall and F-measure.", "labels": [], "entities": [{"text": "precision", "start_pos": 122, "end_pos": 131, "type": "METRIC", "confidence": 0.9996758699417114}, {"text": "recall", "start_pos": 133, "end_pos": 139, "type": "METRIC", "confidence": 0.9994232654571533}, {"text": "F-measure", "start_pos": 144, "end_pos": 153, "type": "METRIC", "confidence": 0.9943892359733582}]}, {"text": "Usually it is irrelevant whether we choose alignment or the standard measures, but the latter can yield unexpected results for extreme clusterings (many small clusters or few very big clusters).", "labels": [], "entities": []}, {"text": "The remaining columns provide these conventional performance measures.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Probability distribution for glauben", "labels": [], "entities": []}, {"text": " Table 3: Performance of the clustering algorithms", "labels": [], "entities": []}]}