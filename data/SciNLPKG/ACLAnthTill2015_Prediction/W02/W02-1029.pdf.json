{"title": [{"text": "Ensemble Methods for Automatic Thesaurus Extraction", "labels": [], "entities": [{"text": "Automatic Thesaurus Extraction", "start_pos": 21, "end_pos": 51, "type": "TASK", "confidence": 0.7357666293780009}]}], "abstractContent": [{"text": "Ensemble methods are state of the art for many NLP tasks.", "labels": [], "entities": [{"text": "NLP tasks", "start_pos": 47, "end_pos": 56, "type": "TASK", "confidence": 0.9006234705448151}]}, {"text": "Recent work by Banko and Brill (2001) suggests that this would not necessarily be true if very large training corpora were available.", "labels": [], "entities": []}, {"text": "However, their results are limited by the simplicity of their evaluation task and individual classifiers.", "labels": [], "entities": []}, {"text": "Our work explores ensemble efficacy for the more complex task of automatic thesaurus extraction on up to 300 million words.", "labels": [], "entities": [{"text": "automatic thesaurus extraction", "start_pos": 65, "end_pos": 95, "type": "TASK", "confidence": 0.6428562104701996}]}, {"text": "We examine our conflicting results in terms of the constraints on, and complexity of, different contextual representations , which contribute to the sparseness-and noise-induced bias behaviour of NLP systems on very large corpora.", "labels": [], "entities": []}], "introductionContent": [{"text": "Ensemble learning is a machine learning technique that combines the output of several different classifiers with the goal of improving classification performance.", "labels": [], "entities": [{"text": "Ensemble learning", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8318381607532501}]}, {"text": "The classifiers within the ensemble may differ in several ways, such as the learning algorithm or knowledge representation used, or data they were trained on.", "labels": [], "entities": []}, {"text": "Ensemble learning has been successfully applied to numerous NLP tasks, including POS tagging (), chunking), word sense disambiguation) and statistical parsing.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 81, "end_pos": 92, "type": "TASK", "confidence": 0.8630533814430237}, {"text": "word sense disambiguation", "start_pos": 108, "end_pos": 133, "type": "TASK", "confidence": 0.6803825497627258}, {"text": "statistical parsing", "start_pos": 139, "end_pos": 158, "type": "TASK", "confidence": 0.8619117438793182}]}, {"text": "presents a good introduction to ensemble methods.", "labels": [], "entities": []}, {"text": "Ensemble methods ameliorate learner bias by amortising individual classifier bias of over different systems.", "labels": [], "entities": []}, {"text": "For an ensemble to be more effective than its constituents, the individual classifiers must have better than 50% accuracy and must produce diverse erroneous classifications.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 113, "end_pos": 121, "type": "METRIC", "confidence": 0.9986748695373535}]}, {"text": "call this complementary disagreement complementarity.", "labels": [], "entities": []}, {"text": "Although ensembles are often effective on problems with small training sets, recent work suggests this may not be true as dataset size increases.", "labels": [], "entities": []}, {"text": "found that for confusion set disambiguation with corpora larger than 100 million words, the best individual classifiers outperformed ensemble methods.", "labels": [], "entities": [{"text": "confusion set disambiguation", "start_pos": 15, "end_pos": 43, "type": "TASK", "confidence": 0.647933304309845}]}, {"text": "One limitation of their results is the simplicity of the task and methods used to examine the efficacy of ensemble methods.", "labels": [], "entities": []}, {"text": "However, both the task and applied methods are constrained by the ambitious use of one billion words of training material.", "labels": [], "entities": []}, {"text": "Disambiguation is relatively simple because confusion sets are rarely larger than four elements.", "labels": [], "entities": [{"text": "Disambiguation", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.9687960147857666}]}, {"text": "The individual methods must be inexpensive because of the computational burden of the massive training set, so they must perform limited processing of the training corpus and can only consider a fairly narrow context surrounding each instance.", "labels": [], "entities": []}, {"text": "We explore the value of ensemble methods for the more complex task of automatic thesaurus extraction, training on corpora of up to 300 million words.", "labels": [], "entities": [{"text": "automatic thesaurus extraction", "start_pos": 70, "end_pos": 100, "type": "TASK", "confidence": 0.6148717999458313}]}, {"text": "The increased complexity leads to results contradicting, which we explore using ensembles of different contextual complexity.", "labels": [], "entities": []}, {"text": "This work emphasises the link between contextual complexity and the problems of representation sparseness and noise as corpus size increases, which in turn impacts on learner bias and ensemble efficacy.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our experiments use a large quantity of text which we have grouped into a range of corpus sizes.", "labels": [], "entities": []}, {"text": "The approximately 300 million word corpus is a random conflation of the BNC and the Reuters corpus (respective sizes in).", "labels": [], "entities": [{"text": "BNC", "start_pos": 72, "end_pos": 75, "type": "DATASET", "confidence": 0.9693323373794556}, {"text": "Reuters corpus", "start_pos": 84, "end_pos": 98, "type": "DATASET", "confidence": 0.9597730040550232}]}, {"text": "We then create corpus subsets down to 128 th (2.3 million words) of the original corpus by randomly sentence selection.", "labels": [], "entities": []}, {"text": "Ensemble voting methods for this task are quite interesting because the result consists of an ordered set of extracted synonyms rather than a single class label.", "labels": [], "entities": []}, {"text": "To test for subtle ranking effects we implemented three different methods of combination: MEAN mean rank of each term over the ensemble; HARMONIC the harmonic mean rank of each term; MIXTURE ranking based on the mean score for each term.", "labels": [], "entities": [{"text": "MEAN mean rank", "start_pos": 90, "end_pos": 104, "type": "METRIC", "confidence": 0.9748574495315552}, {"text": "HARMONIC", "start_pos": 137, "end_pos": 145, "type": "METRIC", "confidence": 0.9962059259414673}, {"text": "MIXTURE", "start_pos": 183, "end_pos": 190, "type": "METRIC", "confidence": 0.6468736529350281}]}, {"text": "The individual extractor scores are not normalised because each extractor uses the same similarity measure and weight function.", "labels": [], "entities": []}, {"text": "We assigned a rank of 201 and similarity score of zero to terms that did not appear in the 200 synonyms returned by the individual extractors.", "labels": [], "entities": [{"text": "similarity score", "start_pos": 30, "end_pos": 46, "type": "METRIC", "confidence": 0.9917228519916534}]}, {"text": "Finally, we build ensembles from all the available extractor methods (e.g. MEAN( * )) and the top three performing extractors (e.g. MEAN).", "labels": [], "entities": [{"text": "MEAN", "start_pos": 75, "end_pos": 79, "type": "METRIC", "confidence": 0.8066244721412659}, {"text": "MEAN", "start_pos": 132, "end_pos": 136, "type": "DATASET", "confidence": 0.6705114245414734}]}, {"text": "To measure the complementary disagreement between ensemble constituents we calculated both the complementarity C and the Spearman rank-order correlation R s . where r(x) is the rank of synonym x.", "labels": [], "entities": [{"text": "Spearman rank-order correlation R s", "start_pos": 121, "end_pos": 156, "type": "METRIC", "confidence": 0.6993151426315307}]}, {"text": "The Spearman rank-order correlation coefficient is the linear correlation coefficient between the rankings of elements of A and B.", "labels": [], "entities": [{"text": "Spearman rank-order correlation coefficient", "start_pos": 4, "end_pos": 47, "type": "METRIC", "confidence": 0.6069912686944008}]}, {"text": "R sis a useful non-parametric comparison for when the rank order is more relevant than the actual values in the distribution.", "labels": [], "entities": []}, {"text": "The evaluation is performed on thesaurus entries extracted for 70 single word noun terms.", "labels": [], "entities": []}, {"text": "To avoid sample bias, the words were randomly selected from WordNet such that they covered a range of values for the following word properties: frequency Penn Treebank and BNC frequencies; number of senses WordNet and Macquarie senses; specificity depth in the WordNet hierarchy; concreteness distribution across WordNet subtrees.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 60, "end_pos": 67, "type": "DATASET", "confidence": 0.9631426930427551}, {"text": "BNC", "start_pos": 172, "end_pos": 175, "type": "METRIC", "confidence": 0.8814862966537476}, {"text": "WordNet", "start_pos": 206, "end_pos": 213, "type": "DATASET", "confidence": 0.9565638899803162}]}, {"text": "shows some of the selected terms with frequency and synonym set information.", "labels": [], "entities": []}, {"text": "For each term we extracted a thesaurus entry with 200 potential synonyms and their similarity scores.", "labels": [], "entities": []}, {"text": "The simplest evaluation measure is direct comparison of the extracted thesaurus with a manuallycreated gold standard.", "labels": [], "entities": []}, {"text": "However, on smaller corpora direct matching is often too  With this gold standard resource in place, it is possible to use precision and recall measures to evaluate the quality of the extracted thesaurus.", "labels": [], "entities": [{"text": "precision", "start_pos": 123, "end_pos": 132, "type": "METRIC", "confidence": 0.9993795156478882}, {"text": "recall", "start_pos": 137, "end_pos": 143, "type": "METRIC", "confidence": 0.996981680393219}]}, {"text": "To help overcome the problems of coarse-grained direct comparisons we use several measures of system performance: direct matches (DIRECT), inverse rank (INVR), and top n synonyms precision (P(n)).", "labels": [], "entities": [{"text": "inverse rank (INVR)", "start_pos": 139, "end_pos": 158, "type": "METRIC", "confidence": 0.9402451753616333}, {"text": "top n synonyms precision", "start_pos": 164, "end_pos": 188, "type": "METRIC", "confidence": 0.5140586048364639}]}, {"text": "INVR is the sum of the inverse rank of each matching synonym, e.g. gold standard matches at ranks 3, 5 and 28 give an inverse rank score of 569.", "labels": [], "entities": [{"text": "INVR", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.922717273235321}, {"text": "inverse rank score", "start_pos": 118, "end_pos": 136, "type": "METRIC", "confidence": 0.9429477651913961}]}, {"text": "With at most 200 synonyms, the maximum INVR score is 5.878.", "labels": [], "entities": [{"text": "INVR score", "start_pos": 39, "end_pos": 49, "type": "METRIC", "confidence": 0.8323365449905396}]}, {"text": "Top n precision is the percentage of matching synonyms in the top n extracted synonyms.", "labels": [], "entities": [{"text": "precision", "start_pos": 6, "end_pos": 15, "type": "METRIC", "confidence": 0.8905379176139832}]}, {"text": "We use n = 1, 5 and 10.", "labels": [], "entities": []}, {"text": "three extractors are combined to form the top-three ensemble.", "labels": [], "entities": []}, {"text": "CASS and the other window methods perform significantly worse than SEXTANT and MINI-PAR.", "labels": [], "entities": [{"text": "CASS", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.6802527904510498}]}, {"text": "Interestingly, W(L 1 R 1 * ) performs almost as well as W(L 1 R 1 ) on larger corpora, suggesting that position information is not as useful with large corpora, perhaps because the left and right set of words for each term becomes relatively disjoint.", "labels": [], "entities": []}, {"text": "presents the evaluation results for all the individual extractors and the six ensembles on the full corpus.", "labels": [], "entities": []}, {"text": "At 300 million words all of the ensemble methods outperform the individual extractors.", "labels": [], "entities": []}, {"text": "These results disagree with those obtained for confusion set disambiguation.", "labels": [], "entities": [{"text": "confusion set disambiguation", "start_pos": 47, "end_pos": 75, "type": "TASK", "confidence": 0.794263998667399}]}, {"text": "The best performing ensembles, MIXTURE( * ) and MEAN( * ), combine the results from all of the individual extractors.", "labels": [], "entities": [{"text": "MIXTURE", "start_pos": 31, "end_pos": 38, "type": "METRIC", "confidence": 0.7928611636161804}, {"text": "MEAN", "start_pos": 48, "end_pos": 52, "type": "METRIC", "confidence": 0.9880081415176392}]}, {"text": "MIXTURE( * ) performs approximately 5% better than SEXTANT, the best individual extractor.", "labels": [], "entities": [{"text": "MIXTURE", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.599914014339447}, {"text": "SEXTANT", "start_pos": 51, "end_pos": 58, "type": "METRIC", "confidence": 0.8217012286186218}]}, {"text": "compares the performance behaviour over the range of corpus sizes for the best three individSystem DIRECT P(1) P shows that ensemble methods are of more value (at least in percentage terms) for smaller training sets.", "labels": [], "entities": []}, {"text": "The trend in the graph suggests that the individual extractors will not outperform the ensemble methods, unless the behaviour changes as corpus size is increased further.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Training Corpora Statistics", "labels": [], "entities": [{"text": "Training Corpora Statistics", "start_pos": 10, "end_pos": 37, "type": "DATASET", "confidence": 0.8818958004315695}]}, {"text": " Table 2: Examples of the 70 thesaurus evaluation terms", "labels": [], "entities": []}, {"text": " Table 3: Extractor performance at 300MW", "labels": [], "entities": []}, {"text": " Table 4: Agreement between ensemble members", "labels": [], "entities": []}, {"text": " Table 6: Complex ensembles better than individuals", "labels": [], "entities": []}]}