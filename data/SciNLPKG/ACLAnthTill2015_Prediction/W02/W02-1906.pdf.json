{"title": [{"text": "Passage Selection to Improve Question Answering", "labels": [], "entities": [{"text": "Passage Selection", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8269104063510895}, {"text": "Improve Question Answering", "start_pos": 21, "end_pos": 47, "type": "TASK", "confidence": 0.8120312293370565}]}], "abstractContent": [{"text": "Open-Domain Question Answering systems (QA) performs the task of detecting text fragments in a collection of documents that contain the response to user's queries.", "labels": [], "entities": [{"text": "Open-Domain Question Answering", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.5473483800888062}, {"text": "detecting text fragments in a collection of documents that contain the response to user's queries", "start_pos": 65, "end_pos": 162, "type": "TASK", "confidence": 0.6420016698539257}]}, {"text": "These systems use high complexity tools that reduce its applicability to the treatment of small amounts of text.", "labels": [], "entities": []}, {"text": "Consequently, when working on large document collections, QA systems apply Information Retrieval (IR) techniques to reduce drastically text collections to a tractable quantity of relevant text.", "labels": [], "entities": [{"text": "Information Retrieval (IR)", "start_pos": 75, "end_pos": 101, "type": "TASK", "confidence": 0.7367539048194885}]}, {"text": "In this paper, we propose a novel Passage Retrieval (PR) model that performs this task with better performance for QA purposes than current best IR systems", "labels": [], "entities": [{"text": "Passage Retrieval (PR)", "start_pos": 34, "end_pos": 56, "type": "TASK", "confidence": 0.8887773036956788}]}], "introductionContent": [{"text": "Information Retrieval (IR) systems receive as input a user's query, and they have to return a set of documents sorted by their relevance to the query.", "labels": [], "entities": [{"text": "Information Retrieval (IR)", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8321236193180084}]}, {"text": "There are different techniques to carryout the document extraction process, but most of them are based on pattern matching modules that depend on the number of times that a query term appear in each document, as well as the importance or discrimination value of each term in the document collection.", "labels": [], "entities": [{"text": "document extraction", "start_pos": 47, "end_pos": 66, "type": "TASK", "confidence": 0.7352838963270187}]}, {"text": "Question Answering (QA) systems try to improve the output generated by IR systems by means of returning just small pieces of text that are supposed to contain the response.", "labels": [], "entities": [{"text": "Question Answering (QA)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.8360786080360413}]}, {"text": "Usually, QA systems combine IR and Natural Language Processing (NLP) techniques to perform their task.", "labels": [], "entities": []}, {"text": "This combination allows text understanding until a minimum level that permits a precise answer detection and extraction.", "labels": [], "entities": [{"text": "text understanding", "start_pos": 24, "end_pos": 42, "type": "TASK", "confidence": 0.8883154392242432}, {"text": "answer detection and extraction", "start_pos": 88, "end_pos": 119, "type": "TASK", "confidence": 0.7539856731891632}]}, {"text": "Nevertheless, since NLP techniques are computationally expensive, QA systems need to reduce the amount of text where these techniques have to be applied.", "labels": [], "entities": []}, {"text": "In this way, they usually work on the output of IR systems that select the most relevant documents to the query by supposing that they will contain the answer required.", "labels": [], "entities": []}, {"text": "Most applied IR systems are mainly based on three models: the cosine model, the pivoted cosine model 1, and the probabilistic model (OKAPI).", "labels": [], "entities": [{"text": "IR", "start_pos": 13, "end_pos": 15, "type": "TASK", "confidence": 0.9807301759719849}]}, {"text": "Moreover, IR systems usually employ query expansion techniques that frequently improve their precision.", "labels": [], "entities": [{"text": "IR", "start_pos": 10, "end_pos": 12, "type": "TASK", "confidence": 0.9849950671195984}, {"text": "query expansion", "start_pos": 36, "end_pos": 51, "type": "TASK", "confidence": 0.6979532390832901}, {"text": "precision", "start_pos": 93, "end_pos": 102, "type": "METRIC", "confidence": 0.9951352477073669}]}, {"text": "These techniques can be based on thesaurus or on the incorporation of the most frequent terms in the top M relevant documents.", "labels": [], "entities": []}, {"text": "Currently, several Passage Retrieval (PR) systems have also been proposed for this task[5][8].", "labels": [], "entities": [{"text": "Passage Retrieval (PR)", "start_pos": 19, "end_pos": 41, "type": "TASK", "confidence": 0.9114840865135193}]}, {"text": "PR systems deal with fragments of text in order to determine the relevance of a document to a query, as well as to detect document extracts that are likely to contain the expected answer (instead of full documents).", "labels": [], "entities": []}, {"text": "Although PR systems apply IR-based techniques to perform their work, they have revealed to be more effective than IR systems for QA tasks.", "labels": [], "entities": [{"text": "QA tasks", "start_pos": 129, "end_pos": 137, "type": "TASK", "confidence": 0.8747180998325348}]}, {"text": "In this paper, we are analysing the importance of the IR-n PR system for QA n as it was used in last TREC-10 Conference.", "labels": [], "entities": [{"text": "IR-n PR", "start_pos": 54, "end_pos": 61, "type": "TASK", "confidence": 0.6182205229997635}]}, {"text": "The following section briefly presents the backgrounds in IR, PR and QA.", "labels": [], "entities": [{"text": "IR", "start_pos": 58, "end_pos": 60, "type": "TASK", "confidence": 0.9819548726081848}, {"text": "PR", "start_pos": 62, "end_pos": 64, "type": "TASK", "confidence": 0.8815208673477173}, {"text": "QA", "start_pos": 69, "end_pos": 71, "type": "TASK", "confidence": 0.6409468054771423}]}, {"text": "Section 3 shows the architecture of IR-n.", "labels": [], "entities": [{"text": "IR-n", "start_pos": 36, "end_pos": 40, "type": "TASK", "confidence": 0.5669710040092468}]}, {"text": "Section 4 presents the evaluation accomplished and finally, section 5 details conclusions and work in progress.", "labels": [], "entities": []}], "datasetContent": [{"text": "This section presents the experiments developed for training and evaluating our approach.", "labels": [], "entities": []}, {"text": "The experiments have been run on the TREC-9 QA Track question set and document collections.", "labels": [], "entities": [{"text": "TREC-9 QA Track question set", "start_pos": 37, "end_pos": 65, "type": "DATASET", "confidence": 0.908862316608429}]}, {"text": "In order to evaluate our proposal we decided to compare the quality of the information retrieved by our approaches with the ranked list retrieved by the ATT IR system.", "labels": [], "entities": [{"text": "ATT IR system", "start_pos": 153, "end_pos": 166, "type": "DATASET", "confidence": 0.8735091288884481}]}, {"text": "For this evaluation, the 682 questions included in the data collection were processed and the number N of sentences per passage was set to 20.", "labels": [], "entities": []}, {"text": "shows the results of this evaluation experiment.", "labels": [], "entities": []}, {"text": "This table shows the percentage of questions whose answer can be found into the first n documents returned by the ATT IR system and the best n passages returned by IR-n Ref and IR-n respectively.", "labels": [], "entities": [{"text": "ATT IR system", "start_pos": 114, "end_pos": 127, "type": "DATASET", "confidence": 0.8274738987286886}, {"text": "IR-n Ref", "start_pos": 164, "end_pos": 172, "type": "TASK", "confidence": 0.4429982155561447}]}, {"text": "These results are also presented in These data confirm training results.", "labels": [], "entities": []}, {"text": "In this case, both approaches perform better than ATT system and improvements range form 6 to 12 points for 20 sentences passage length.", "labels": [], "entities": [{"text": "ATT", "start_pos": 50, "end_pos": 53, "type": "METRIC", "confidence": 0.7166315913200378}]}], "tableCaptions": [{"text": " Table 1. Number of questions rightly answered  (training set of 100 questions).", "labels": [], "entities": [{"text": "Number", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9537153840065002}]}, {"text": " Table 2. ATT-system versus IR-n systems.", "labels": [], "entities": []}]}