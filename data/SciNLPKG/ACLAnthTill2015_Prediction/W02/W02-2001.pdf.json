{"title": [{"text": "Extracting the Unextractable: A Case Study on Verb-particles", "labels": [], "entities": [{"text": "Extracting the Unextractable", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8645897308985392}]}], "abstractContent": [{"text": "This paper proposes a series of techniques for extracting English verb-particle constructions from raw text corpora.", "labels": [], "entities": [{"text": "extracting English verb-particle constructions from raw text corpora", "start_pos": 47, "end_pos": 115, "type": "TASK", "confidence": 0.8265118822455406}]}, {"text": "We initially propose three basic methods, based on tagger output, chunker output and a chunk grammar, respectively, with the chunk grammar method optionally combining with an attachment resolution module to determine the syntactic structure of verb-preposition pairs in ambiguous constructs.", "labels": [], "entities": []}, {"text": "We then combine the three methods together into a single classifier, and add in a number of extra lexical and frequentistic features, producing a final F-score of 0.865 over the WSJ.", "labels": [], "entities": [{"text": "F-score", "start_pos": 152, "end_pos": 159, "type": "METRIC", "confidence": 0.9987457990646362}, {"text": "WSJ", "start_pos": 178, "end_pos": 181, "type": "DATASET", "confidence": 0.9826725721359253}]}], "introductionContent": [{"text": "There is growing awareness of the pervasiveness and idiosyncrasy of multiword expressions (MWEs), and the need fora robust, structured handling thereof ().", "labels": [], "entities": []}, {"text": "Examples of MWEs are lexically fixed expressions (e.g. ad hoc), idioms (e.g. see double), light verb constructions (e.g. make a mistake) and institutionalised phrases (e.g. kindle excitement).", "labels": [], "entities": []}, {"text": "MWEs pose a challenge to NLP due to their syntactic and semantic idiosyncrasies, which are often unpredictable from their component parts.", "labels": [], "entities": []}, {"text": "Largescale manual annotation of MWEs is infeasible due to their sheer volume (at least equivalent to the number of simplex words (Jackendoff, 1997)), productivity and domain-specificity.", "labels": [], "entities": [{"text": "Largescale manual annotation of MWEs", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.5274795889854431}]}, {"text": "Ideally, therefore, we would like to have some means of automatically extracting MWEs from a given domain or corpus, allowing us to pre-tune our grammar prior to deployment.", "labels": [], "entities": []}, {"text": "It is this task of extraction that we target in this paper.", "labels": [], "entities": []}, {"text": "This research represents a component of the LinGO multiword expression project, 1 which is targeted at extracting, adequately handling and representing MWEs of all types.", "labels": [], "entities": []}, {"text": "As a research testbed and target resource to expand/domain-tune, we use the LinGO English Resource Grammar (LinGO-ERG), a linguistically-precise HPSG-based grammar underdevelopment at CSLI).", "labels": [], "entities": [{"text": "LinGO English Resource Grammar (LinGO-ERG)", "start_pos": 76, "end_pos": 118, "type": "DATASET", "confidence": 0.9072076593126569}]}, {"text": "The particular MWE type we target for extraction is the English verb-particle construction.", "labels": [], "entities": []}, {"text": "Verb-particle constructions (\"VPCs\") consist of ahead verb and one or more obligatory particles, in the form of intransitive prepositions (e.g. hand in), adjectives (e.g. cut short) or verbs (e.g. let go); for the purposes of this paper, we will focus exclusively on prepositional particles-by far the most common and productive of the three typesand further restrict our attention to single-particle VPCs (i.e. we ignore VPCs such as get along together ).", "labels": [], "entities": [{"text": "Verb-particle constructions (\"VPCs\")", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.7081864178180695}]}, {"text": "We define VPCs to optionally select for an NP complement, i.e. to occur both transitively (e.g. hand in the paper ) and intransitively (e.g. battle on).", "labels": [], "entities": []}, {"text": "One aspect of VPCs that makes them difficult to extract (cited in, e.g.,) is that the verb and particle can be non-contiguous, e.g. hand the paper in and battle right on.", "labels": [], "entities": []}, {"text": "This sets them apart from conventional collocations and terminology (see, e.g., and) in that they cannot be captured effectively using N-grams, due to the variability in the number and type of words potentially interceding between the verb and particle.", "labels": [], "entities": []}, {"text": "We are aiming for an extraction technique which is applicable to any raw text corpus, allowing us to tune grammars to novel domains.", "labels": [], "entities": []}, {"text": "Any linguistic annotation required during the extraction process, therefore, is produced through automatic means, and it is only for reasons of accessibility and comparability with other research that we choose to work over the Wall Street Journal section of the Penn Treebank (.", "labels": [], "entities": [{"text": "Wall Street Journal section of the Penn Treebank", "start_pos": 228, "end_pos": 276, "type": "DATASET", "confidence": 0.9515214413404465}]}, {"text": "That is, other than in establishing upper bounds on the performance of the different extraction methods, we use only the raw text component of the treebank.", "labels": [], "entities": []}, {"text": "In this paper, we first outline distinguishing features of VPCs relevant to the extraction process ( \u00a7 2).", "labels": [], "entities": []}, {"text": "We then present and evaluate a number of simple methods for extracting VPCs based on, respectively, POS tagging ( \u00a7 3), the output of a full text chunk parser ( \u00a7 4), and a chunk grammar ( \u00a7 5).", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 100, "end_pos": 111, "type": "TASK", "confidence": 0.6709583401679993}]}, {"text": "Finally, we detail enhancements to the basic methods ( \u00a7 6) and give a brief description of related research ( \u00a7 7) before concluding the paper ( \u00a7 8).", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: POS-based extraction results", "labels": [], "entities": [{"text": "POS-based extraction", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.8409218788146973}]}, {"text": " Table 3: Chunk tag-based extraction results", "labels": [], "entities": [{"text": "Chunk tag-based extraction", "start_pos": 10, "end_pos": 36, "type": "TASK", "confidence": 0.655634214480718}]}, {"text": " Table 4: Chunk grammar-based extraction results", "labels": [], "entities": [{"text": "Chunk grammar-based extraction", "start_pos": 10, "end_pos": 40, "type": "TASK", "confidence": 0.6579206387201945}]}, {"text": " Table 5: Consolidated extraction results", "labels": [], "entities": [{"text": "Consolidated extraction", "start_pos": 10, "end_pos": 33, "type": "TASK", "confidence": 0.7668484449386597}]}]}