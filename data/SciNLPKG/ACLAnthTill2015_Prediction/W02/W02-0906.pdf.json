{"title": [], "abstractContent": [{"text": "This paper presents experiments performed on lexical knowledge acquisition in the form of verbal argumental information.", "labels": [], "entities": [{"text": "lexical knowledge acquisition", "start_pos": 45, "end_pos": 74, "type": "TASK", "confidence": 0.6469459235668182}]}, {"text": "The system obtains the data from raw corpora after the application of a partial parser and statistical filters.", "labels": [], "entities": []}, {"text": "We used two different statistical filters to acquire the argumental information: Mutual Information, and Fisher's Exact test.", "labels": [], "entities": []}, {"text": "Due to the characteristics of agglutinative languages like Basque, the usual classification of arguments in terms of their syntactic category (such as NP or PP) is not suitable.", "labels": [], "entities": []}, {"text": "For that reason, the arguments will be classified in 48 different kinds of case markers, which makes the system fine grained if compared to equivalent systems that have been developed for other languages.", "labels": [], "entities": []}], "introductionContent": [{"text": "In recent years a considerable effort has been done on the acquisition of lexical information.", "labels": [], "entities": [{"text": "acquisition of lexical information", "start_pos": 59, "end_pos": 93, "type": "TASK", "confidence": 0.8098531514406204}]}, {"text": "As several authors point out, this information is useful fora wide range of applications.", "labels": [], "entities": []}, {"text": "For example, J. show how adding subcategorization information improves the performance of a parser.", "labels": [], "entities": []}, {"text": "With this in mind our aim is to obtain a system that automatically discriminates between subcategorized elements of verbs (arguments) and non-subcategorized ones (adjuncts).", "labels": [], "entities": []}, {"text": "We have evaluated our system in two ways: comparing the results to a gold standard and estimating the coverage over sentences in the corpus.", "labels": [], "entities": [{"text": "coverage", "start_pos": 102, "end_pos": 110, "type": "METRIC", "confidence": 0.9600961804389954}]}, {"text": "The purpose was to find out which was the impact of each approach on this particular task.", "labels": [], "entities": []}, {"text": "The two methods of evaluation yield significantly different results.", "labels": [], "entities": []}, {"text": "Basque is the subject of this study.", "labels": [], "entities": []}, {"text": "A language that, in contrast to languages like English, has limited resources in the form of digital corpora, computational lexicons, grammars or annotated treebanks.", "labels": [], "entities": []}, {"text": "Therefore, any effort like the one presented here, oriented to create lexical resources, has to be driven to do as much automatic work as possible, minimizing development costs.", "labels": [], "entities": []}, {"text": "The paper is divided into 4 sections.", "labels": [], "entities": []}, {"text": "The first section is devoted to explain the theoretical motivations underlying the process.", "labels": [], "entities": []}, {"text": "The second section is a description of the different stages of the system.", "labels": [], "entities": []}, {"text": "The third section presents the results obtained.", "labels": [], "entities": []}, {"text": "The fourth section is a review of previous work on automatic subcategorization acquisition.", "labels": [], "entities": [{"text": "subcategorization acquisition", "start_pos": 61, "end_pos": 90, "type": "TASK", "confidence": 0.7671997249126434}]}, {"text": "Finally, we present the main conclusions.", "labels": [], "entities": []}], "datasetContent": [{"text": "We found in the literature two main approaches to evaluate a system like the one proposed in this paper (T.: \u2022 Comparing the obtained information with a gold standard.", "labels": [], "entities": []}, {"text": "\u2022 Calculating the coverage of the obtained information on a corpus.", "labels": [], "entities": []}, {"text": "This can give an estimate of how well the information obtained could help a parser on that corpus.", "labels": [], "entities": []}, {"text": "Under the former approach a further distinction emerges: using a dictionary as a gold standard, or performing manual evaluation, where some linguists extract the subcategorization frames appearing in a corpus and comparing them with the set of subcategorization frames obtained automatically.", "labels": [], "entities": []}, {"text": "We decided to evaluate the system both ways, that is to say, using a gold standard and calculating the coverage over a corpus.", "labels": [], "entities": []}, {"text": "The intention was to determine, all things being equal, the impact of doing it one way or the other.", "labels": [], "entities": []}, {"text": "From the 640 analyzed verbs, we selected 10 for evaluation.", "labels": [], "entities": []}, {"text": "For each of these verbs we extracted from the corpus the list of all their dependents.", "labels": [], "entities": []}, {"text": "The list was a set of bare verb-case pairs, that is, no context was involved and, therefore, as the sense of the given verb could not be derived, different senses of the verb were taken into account.", "labels": [], "entities": []}, {"text": "We provided 4 human annotators/taggers with this list and they marked each dependent as either argument or adjunct.", "labels": [], "entities": []}, {"text": "The taggers accomplished the task three times.", "labels": [], "entities": []}, {"text": "Once, with the simple guideline of the implicational test and obligatoriness test, but with no further consensus.", "labels": [], "entities": []}, {"text": "The inter-tagger agreement was low (57%).", "labels": [], "entities": []}, {"text": "The taggers gathered and realized that the problem came mostly from semantics.", "labels": [], "entities": []}, {"text": "While some taggers tagged the verbcase pairs assuming a concrete semantic domain the others took into account a wider rage of senses (moreover, in some cases the senses did not even match).", "labels": [], "entities": []}, {"text": "So the tagging was repeated when all of them considered the same semantics to the different verbs.", "labels": [], "entities": []}, {"text": "The inter-tagger agreement raised up to a 80%.", "labels": [], "entities": []}, {"text": "The taggers gathered again to discuss, deciding over the non clear pairs.", "labels": [], "entities": []}, {"text": "The list obtained from merging 2 the 4 lists in one is taken to be our gold standard.", "labels": [], "entities": []}, {"text": "Notice that when the annotators decided whether a possible argument was really an argument or not, no context was involved.", "labels": [], "entities": []}, {"text": "In other words, they were deciding over bare pairs of verbs and cases.", "labels": [], "entities": []}, {"text": "Therefore different senses of the verb were considered because there was noway to disambiguate the specific meaning of the verb.", "labels": [], "entities": []}, {"text": "So the evaluation is an approximation of how well would the system perform over any corpus.", "labels": [], "entities": []}, {"text": "shows the results in terms of Precision and Recall.", "labels": [], "entities": [{"text": "Precision", "start_pos": 30, "end_pos": 39, "type": "METRIC", "confidence": 0.9968998432159424}, {"text": "Recall", "start_pos": 44, "end_pos": 50, "type": "METRIC", "confidence": 0.9969465136528015}]}, {"text": "Precision Recall F-score MI 62% 50% 55% Fisher 64% 44% 52%  The initial corpus was divided in two parts, one for training the system and another one for evaluating it.", "labels": [], "entities": [{"text": "Precision Recall F-score", "start_pos": 0, "end_pos": 24, "type": "METRIC", "confidence": 0.6804201205571493}, {"text": "MI", "start_pos": 25, "end_pos": 27, "type": "METRIC", "confidence": 0.6311123967170715}]}, {"text": "From the fraction reserved for evaluation we extracted 200 sentences corresponding to the same 10 verbs used in the \"gold standard\" based evaluation.", "labels": [], "entities": []}, {"text": "In this case, the task carried out by the annotators consisted in extracting, for each of the 200 sentences, the elements (arguments/adjuncts) linked to the corresponding verb.", "labels": [], "entities": []}, {"text": "Each element was marked as argument or adjunct.", "labels": [], "entities": []}, {"text": "Note that in this case the annotation takes place inside the context of the sentence.", "labels": [], "entities": []}, {"text": "In other words, the verb shows precise semantics.", "labels": [], "entities": []}, {"text": "We performed a simple evaluation on the sentences), calculating precision and recall over each argument marked by the annotators 3 . For example, if a verb appeared in a sentence with two arguments and the statistical filters were recognizing them as arguments, both precision and recall would be 100%.", "labels": [], "entities": [{"text": "precision", "start_pos": 64, "end_pos": 73, "type": "METRIC", "confidence": 0.9994004964828491}, {"text": "recall", "start_pos": 78, "end_pos": 84, "type": "METRIC", "confidence": 0.9988141059875488}, {"text": "precision", "start_pos": 267, "end_pos": 276, "type": "METRIC", "confidence": 0.9994592070579529}, {"text": "recall", "start_pos": 281, "end_pos": 287, "type": "METRIC", "confidence": 0.9987473487854004}]}, {"text": "If, on the contrary, only one was found, then precision would be 100%, and recall 50%.", "labels": [], "entities": [{"text": "precision", "start_pos": 46, "end_pos": 55, "type": "METRIC", "confidence": 0.9998807907104492}, {"text": "recall", "start_pos": 75, "end_pos": 81, "type": "METRIC", "confidence": 0.999618411064148}]}], "tableCaptions": [{"text": " Table 3. Summary of several systems on subcategorization information.", "labels": [], "entities": []}]}