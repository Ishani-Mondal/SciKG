{"title": [{"text": "IMPROVEMENTS IN NON-VERBAL CUE IDENTIFICATION USING MULTILINGUAL PHONE STRINGS", "labels": [], "entities": [{"text": "IMPROVEMENTS", "start_pos": 0, "end_pos": 12, "type": "METRIC", "confidence": 0.7361695766448975}, {"text": "NON-VERBAL CUE IDENTIFICATION USING MULTILINGUAL PHONE STRINGS", "start_pos": 16, "end_pos": 78, "type": "METRIC", "confidence": 0.6004890331200191}]}], "abstractContent": [], "introductionContent": [{"text": "Today's state-of-the-art front-ends for multilingual speechto-speech translation systems apply monolingual speech recognizers trained fora single language and/or accent.", "labels": [], "entities": [{"text": "multilingual speechto-speech translation", "start_pos": 40, "end_pos": 80, "type": "TASK", "confidence": 0.6243029435475668}]}, {"text": "The monolingual speech engine is usually adaptable to an unknown speaker overtime using unsupervised training methods; however, if the speaker was seen during training, their specialized acoustic model will be applied, since it achieves better performance.", "labels": [], "entities": []}, {"text": "In order to make full use of specialized acoustic models in this proposed scenario, it is necessary to automatically identify the speaker with high accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 148, "end_pos": 156, "type": "METRIC", "confidence": 0.9920748472213745}]}, {"text": "Furthermore, monolingual speech recognizers currently rely on the fact that language and/or accent will be selected beforehand by the user.", "labels": [], "entities": [{"text": "monolingual speech recognizers", "start_pos": 13, "end_pos": 43, "type": "TASK", "confidence": 0.6354066828886668}]}, {"text": "This requires the user's cooperation and an interface which easily allows for such selection.", "labels": [], "entities": []}, {"text": "Both requirements are awkward and error-prone, especially when translation services are provided for many languages using small devices like PDAs or telephones.", "labels": [], "entities": []}, {"text": "For these scenarios, front-ends are desired which automatically identify the spoken language or accent.", "labels": [], "entities": []}, {"text": "We believe that the automatic identification of an utterance's non-verbal cues, such as language, accent and speaker, are necessary to the successful deployment of speech-to-speech translation systems.", "labels": [], "entities": [{"text": "speech-to-speech translation", "start_pos": 164, "end_pos": 192, "type": "TASK", "confidence": 0.753748744726181}]}, {"text": "Currently, approaches based on Gaussian Mixture Models (GMMs) are the most widely and successfully used methods for speaker identification.", "labels": [], "entities": [{"text": "speaker identification", "start_pos": 116, "end_pos": 138, "type": "TASK", "confidence": 0.8738043010234833}]}, {"text": "Although GMMs have been applied successfully to close-speaking microphone scenarios under matched training and testing conditions, their performance degrades dramatically under mismatched conditions.", "labels": [], "entities": []}, {"text": "For language and accent identification, phone recognition together with phone N-gram modeling has been the most successful approach in the past.", "labels": [], "entities": [{"text": "language and accent identification", "start_pos": 4, "end_pos": 38, "type": "TASK", "confidence": 0.6844162195920944}, {"text": "phone recognition", "start_pos": 40, "end_pos": 57, "type": "TASK", "confidence": 0.8357404470443726}]}, {"text": "More recently, Kohler introduced an approach for speaker recognition where a phonotactic N-gram model is used.", "labels": [], "entities": [{"text": "speaker recognition", "start_pos": 49, "end_pos": 68, "type": "TASK", "confidence": 0.8834410905838013}]}, {"text": "In, we extended Kohler's approach to accent and language identification as well as to speaker identification under mismatched conditions.", "labels": [], "entities": [{"text": "accent and language identification", "start_pos": 37, "end_pos": 71, "type": "TASK", "confidence": 0.598915196955204}, {"text": "speaker identification", "start_pos": 86, "end_pos": 108, "type": "TASK", "confidence": 0.7440172433853149}]}, {"text": "The term \"mismatched condition\" describes a situation in which the testing conditions, e.g. microphone distance, are quite different from what had been seen during training.", "labels": [], "entities": []}, {"text": "In that work, we explored a common framework for the identification of language, accent and speaker using multilingual phone strings produced by phone recognizers trained on data from different languages.", "labels": [], "entities": []}, {"text": "In this paper, we propose and evaluate some improvements, comparing classification accuracy as well as realtime performance in our framework.", "labels": [], "entities": [{"text": "classification", "start_pos": 68, "end_pos": 82, "type": "TASK", "confidence": 0.9489222168922424}, {"text": "accuracy", "start_pos": 83, "end_pos": 91, "type": "METRIC", "confidence": 0.9425001740455627}]}, {"text": "Furthermore, we investigate the benefits that are to be drawn from additional phone recognizers.", "labels": [], "entities": [{"text": "phone recognizers", "start_pos": 78, "end_pos": 95, "type": "TASK", "confidence": 0.7203322350978851}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1. GMM performance under matched and mis- matched conditions for 10 second segments", "labels": [], "entities": []}, {"text": " Table 2. MPM-pp SID rate on varying test lengths at Dis 0", "labels": [], "entities": [{"text": "SID rate", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.7234434485435486}, {"text": "Dis", "start_pos": 53, "end_pos": 56, "type": "METRIC", "confidence": 0.9577398896217346}]}, {"text": " Table 3. MPM-pp SID rate on varying test lengths at  matched training and testing distances", "labels": [], "entities": [{"text": "MPM-pp SID", "start_pos": 10, "end_pos": 20, "type": "TASK", "confidence": 0.5135063081979752}]}, {"text": " Table 4. MPM-pp SID rate on varying test lengths at mis- matched training and testing distance", "labels": [], "entities": [{"text": "SID rate", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.6511316001415253}]}, {"text": " Table 5. Comparison of MPM-pp classification using base- line and improved phone recognizers on matched conditions  for 60 seconds of audio (SID rate in %)", "labels": [], "entities": [{"text": "MPM-pp classification", "start_pos": 24, "end_pos": 45, "type": "TASK", "confidence": 0.8162421882152557}, {"text": "SID rate", "start_pos": 142, "end_pos": 150, "type": "METRIC", "confidence": 0.9748182892799377}]}, {"text": " Table 6. Number of speakers, total number of utterances  and total length of audio for native and non-native classes", "labels": [], "entities": []}, {"text": " Table 7. Average phonotactic perplexities for native and  non-native classes using 6 phone recognizers (top) versus 7  (bottom)", "labels": [], "entities": []}, {"text": " Table 9. Confusion matrix for 3-way proficiency classifica- tion using 6 phone recognizers (top) versus 7 (bottom)", "labels": [], "entities": []}, {"text": " Table 10. Data  set 1 was used for training the phonotactic models, while  data set 4 was completely held-out during training and used  to evaluate the end-to-end performance of the complete  classifier. Data sets 2 and 3 were used as development sets  while experimenting with different decision strategies.", "labels": [], "entities": []}, {"text": " Table 10. Number of speakers per data set, total number of  utterances and total length of audio per language", "labels": [], "entities": []}, {"text": " Table 11. Realtime factors for baseline and improved phone  recognizers", "labels": [], "entities": [{"text": "phone  recognizers", "start_pos": 54, "end_pos": 72, "type": "TASK", "confidence": 0.7170916795730591}]}, {"text": " Table 12. Multiple languages vs multiple English phone  recognizers (SID rates in %)", "labels": [], "entities": [{"text": "English phone  recognizers", "start_pos": 42, "end_pos": 68, "type": "TASK", "confidence": 0.7105152010917664}]}]}