{"title": [{"text": "Learning Rules for Chinese Prosodic Phrase Prediction", "labels": [], "entities": [{"text": "Chinese Prosodic Phrase Prediction", "start_pos": 19, "end_pos": 53, "type": "TASK", "confidence": 0.6766988188028336}]}], "abstractContent": [{"text": "This paper describes a rule-learning approach towards Chinese prosodic phrase prediction for TTS systems.", "labels": [], "entities": [{"text": "Chinese prosodic phrase prediction", "start_pos": 54, "end_pos": 88, "type": "TASK", "confidence": 0.6029725447297096}]}, {"text": "Firstly, we prepared a speech corpus having about 3000 sentences and manually labelled the sentences with two-level prosodic structure.", "labels": [], "entities": []}, {"text": "Secondly, candidate features related to prosodic phrasing and the corresponding prosodic boundary labels are extracted from the corpus text to establish an example database.", "labels": [], "entities": []}, {"text": "A series of comparative experiments is conducted to figure out the most effective features from the candidates.", "labels": [], "entities": []}, {"text": "Lastly, two typical rule learning algorithms (C4.5 and TBL) are applied on the example database to induce prediction rules.", "labels": [], "entities": []}, {"text": "The paper also suggests general evaluation parameters for prosodic phrase prediction.", "labels": [], "entities": [{"text": "prosodic phrase prediction", "start_pos": 58, "end_pos": 84, "type": "TASK", "confidence": 0.7216783364613851}]}, {"text": "With these parameters, our methods are compared with RNN and bigram based statistical methods on the same corpus.", "labels": [], "entities": []}, {"text": "The experiments show that the automatic rule-learning approach can achieve better prediction accuracy than the non-rule based methods and yet retain the advantage of the simplicity and understandability of rule systems.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 93, "end_pos": 101, "type": "METRIC", "confidence": 0.9518082737922668}]}, {"text": "Thus it is justified as an effective alternative to prosodic phrase prediction.", "labels": [], "entities": [{"text": "prosodic phrase prediction", "start_pos": 52, "end_pos": 78, "type": "TASK", "confidence": 0.6008460223674774}]}], "introductionContent": [{"text": "Prosodic phrase prediction or prosodic phrasing plays an important role in improving the naturalness and intelligence of TTS systems.", "labels": [], "entities": [{"text": "Prosodic phrase prediction", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.6067404051621755}, {"text": "TTS", "start_pos": 121, "end_pos": 124, "type": "TASK", "confidence": 0.9517820477485657}]}, {"text": "Linguistic research shows that the utterance produced by human is structured in a hierarchy of prosodic units, including phonological phrase, intonation phrase and utterance.", "labels": [], "entities": []}, {"text": "But the output of text analysis of TTS systems is often a structure of syntactic units, such as words or phrases, which are not equivalent to the prosodic ones.", "labels": [], "entities": [{"text": "TTS", "start_pos": 35, "end_pos": 38, "type": "TASK", "confidence": 0.8987534642219543}]}, {"text": "Therefore the object of prosodic phrasing is to map the syntactic structure into its prosodic counterpart.", "labels": [], "entities": []}, {"text": "A lot of methods have been introduced to predict prosodic phrase in English text such as Classification and Regression Tree (, Hidden Markov Model ().", "labels": [], "entities": [{"text": "Classification", "start_pos": 89, "end_pos": 103, "type": "TASK", "confidence": 0.9285386800765991}]}, {"text": "For Chinese prosodic phrasing, the traditional method is based on handcrafted rules.", "labels": [], "entities": [{"text": "prosodic phrasing", "start_pos": 12, "end_pos": 29, "type": "TASK", "confidence": 0.7024393379688263}]}, {"text": "Recurrent Neural Network () as well as POS bigram and CART based methods) is also experimented recently.", "labels": [], "entities": []}, {"text": "Due to the difference in training corpus and evaluation methods between researchers, these results are generally less comparable.", "labels": [], "entities": []}, {"text": "In this paper, a rule-learning approach is proposed to predict prosodic phrase in unrestricted Chinese text.", "labels": [], "entities": []}, {"text": "Rule-based systems are simple and easy to understand.", "labels": [], "entities": []}, {"text": "But handcrafted rules are usually difficult to construct, maintain and evaluate.", "labels": [], "entities": []}, {"text": "Thus two typical rule-learning algorithms (C4.5 induction and transformation-based learning) are employed to automatically induce prediction rules from examples instead of human.", "labels": [], "entities": []}, {"text": "Generally speaking, automatic rule-learning has two obvious advantages over the previous methods: 1) Statistical methods like bigram or HMM usually need large training corpus to avoid sparse data problem while rule-learning doesn't have the restriction.", "labels": [], "entities": []}, {"text": "In the case of prosodic phrase prediction, the corpus with prosodic labelling is often relatively small.", "labels": [], "entities": [{"text": "prosodic phrase prediction", "start_pos": 15, "end_pos": 41, "type": "TASK", "confidence": 0.6824700733025869}]}, {"text": "Rule-learning is just suitable for this task.", "labels": [], "entities": []}, {"text": "2) CART, RNN or other neural network methods have good learning ability but the learned knowledge is represented as trees or network weights, which are not so much understandable as rules.", "labels": [], "entities": []}, {"text": "Once rules are learned from examples, they can be analyzed by human to check if they agree with the common linguistic knowledge.", "labels": [], "entities": []}, {"text": "We can add prediction rules converted from our linguistic knowledge to the rule set, which is especially useful when the training corpus doesn't cover wide enough phenomena of prosodic phrasing.", "labels": [], "entities": []}, {"text": "Furthermore, we can try to interpret and understand rules learned by machine so as to enrich our linguistic knowledge.", "labels": [], "entities": []}, {"text": "Hence rule-learning also helps us mine knowledge from examples.", "labels": [], "entities": []}, {"text": "Since features related to prosodic phrasing come from various linguistic sources, several comparative experiments are conducted to select the most effective features from the candidates.", "labels": [], "entities": []}, {"text": "The paper also suggests general evaluation parameters for prosodic phrase prediction.", "labels": [], "entities": [{"text": "prosodic phrase prediction", "start_pos": 58, "end_pos": 84, "type": "TASK", "confidence": 0.7216783364613851}]}, {"text": "With these parameters, our methods are compared with RNN and bigram based statistical methods on the same corpus.", "labels": [], "entities": []}, {"text": "The experiments show that the automatic rule-learning approach can achieve better prediction accuracy than the non-rule based methods and yet retain the advantage of the simplicity and understandability of rule systems.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 93, "end_pos": 101, "type": "METRIC", "confidence": 0.9518080353736877}]}, {"text": "The paper proceeds as follows.", "labels": [], "entities": []}, {"text": "Section 2 introduces the rule-learning algorithms we used.", "labels": [], "entities": []}, {"text": "Section 3 describes prosodic phrase prediction and its evaluation parameters.", "labels": [], "entities": [{"text": "prosodic phrase prediction", "start_pos": 20, "end_pos": 46, "type": "TASK", "confidence": 0.7034476896127065}]}, {"text": "Section 4 discusses the feature selection and rule-learning experiments in detail.", "labels": [], "entities": [{"text": "feature selection", "start_pos": 24, "end_pos": 41, "type": "TASK", "confidence": 0.7907952666282654}]}, {"text": "Section 5 reports the evaluation results of rule based and none-rule based methods.", "labels": [], "entities": []}, {"text": "Section 6 presents the conclusion and the view of future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "As a classification task, prosodic phrase prediction should be evaluated with consideration on all the classes.", "labels": [], "entities": [{"text": "prosodic phrase prediction", "start_pos": 26, "end_pos": 52, "type": "TASK", "confidence": 0.6927961707115173}]}, {"text": "The rules induced from examples are applied on a test corpus to predict the label of each boundary.", "labels": [], "entities": []}, {"text": "The predicted labels are compared with labels given by human, which are thought to be true, to get a confusion matrix as follows: F is a combination of recall and precision rate, suggested by.", "labels": [], "entities": [{"text": "F", "start_pos": 130, "end_pos": 131, "type": "METRIC", "confidence": 0.9843764305114746}, {"text": "recall", "start_pos": 152, "end_pos": 158, "type": "METRIC", "confidence": 0.9987659454345703}, {"text": "precision rate", "start_pos": 163, "end_pos": 177, "type": "METRIC", "confidence": 0.9901132881641388}]}, {"text": "Once the example database is established, we can begin to induce rules from it with rule learners.", "labels": [], "entities": []}, {"text": "If all the features were used in one experiment, the feature space would get too large to learn rules quickly.", "labels": [], "entities": []}, {"text": "Moreover we want to eliminate less significant features from the database.", "labels": [], "entities": []}, {"text": "A series of comparative experiments is carried out to figure out the effective features.", "labels": [], "entities": []}, {"text": "C4.5 learner is used to perform the learning task in the following experiments.", "labels": [], "entities": []}, {"text": "Since POS features are widely used, a baseline experiment is performed with only two POS features that are POS_0 and POS_1.", "labels": [], "entities": []}, {"text": "The POS tag set has total 30 tags from the tagger.", "labels": [], "entities": [{"text": "POS tag set", "start_pos": 4, "end_pos": 15, "type": "DATASET", "confidence": 0.8951127529144287}]}, {"text": "According to the feature selection results, we know some features are effective to prosodic phrase prediction but some are not.", "labels": [], "entities": [{"text": "prosodic phrase prediction", "start_pos": 83, "end_pos": 109, "type": "TASK", "confidence": 0.6114589472611746}]}, {"text": "And the solely using of effective features doesn't result in a high enough accuracy rate.", "labels": [], "entities": [{"text": "accuracy rate", "start_pos": 75, "end_pos": 88, "type": "METRIC", "confidence": 0.9851417243480682}]}, {"text": "In order to improve the prediction accuracy, we combine the effective features such as WLEN{-1, 1}, BTYPE{-1}, SLEN and POS{-1,1} in LSET tag set together to induce C4.5 rules.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.964695155620575}, {"text": "WLEN", "start_pos": 87, "end_pos": 91, "type": "METRIC", "confidence": 0.7709019184112549}, {"text": "BTYPE", "start_pos": 100, "end_pos": 105, "type": "METRIC", "confidence": 0.9602256417274475}, {"text": "SLEN", "start_pos": 111, "end_pos": 115, "type": "METRIC", "confidence": 0.9896530508995056}, {"text": "POS", "start_pos": 120, "end_pos": 123, "type": "METRIC", "confidence": 0.9193933010101318}, {"text": "LSET tag set", "start_pos": 133, "end_pos": 145, "type": "DATASET", "confidence": 0.7314154903093973}]}, {"text": "A general TBL toolkit () is used in our TBL experiments.", "labels": [], "entities": []}, {"text": "The analysis on C4.5 rules casts lights on the design of the transformation rule templates of TBL.", "labels": [], "entities": [{"text": "TBL", "start_pos": 94, "end_pos": 97, "type": "DATASET", "confidence": 0.8398324251174927}]}, {"text": "Since the same features as C4.5 learning are used in the rule templates, linguistic knowledge, which has been embodied by C4.5 rules, should also be captured by transformation rule templates.", "labels": [], "entities": []}, {"text": "Suppose a C4.5 rule, \"if (POS_0 == n && POS_1 == u) then BTYPE_0 = B 0 \", has a high prediction accuracy, it is reasonable to make this rule as an instantiation of TBL rule templates.", "labels": [], "entities": [{"text": "BTYPE_0", "start_pos": 57, "end_pos": 64, "type": "DATASET", "confidence": 0.6291083494822184}, {"text": "accuracy", "start_pos": 96, "end_pos": 104, "type": "METRIC", "confidence": 0.9013015031814575}]}, {"text": "lists some of the rule templates used in TBL experiments.", "labels": [], "entities": [{"text": "TBL", "start_pos": 41, "end_pos": 44, "type": "TASK", "confidence": 0.8600589036941528}]}, {"text": "The left part of a rule template is a list of features, and the right is the target, BTYPE_0.", "labels": [], "entities": [{"text": "BTYPE_0", "start_pos": 85, "end_pos": 92, "type": "DATASET", "confidence": 0.7856166164080302}]}, {"text": "For example, \"POS_0 POS_1 => BTYPE_0\", which is a short form of \"if (POS_0 == X && POS_1 == Y) then BTYPE_0 = Z\", means if current POS were X and the next POS were Y, the boundary label would be Z.", "labels": [], "entities": [{"text": "BTYPE_0", "start_pos": 29, "end_pos": 36, "type": "DATASET", "confidence": 0.7771931489308676}]}, {"text": "X, Y, Z are template variables.", "labels": [], "entities": []}, {"text": "Let X=n Y=u Z=B 0 , the template is instantiated into the C4.5 rule above.", "labels": [], "entities": []}, {"text": "Due to the mechanism of TBL rules, there exist rule templates like \"BTYPE_0 POS_0 POS_1 => BTYPE_0\", in which the former BTYPE_0 is the label before applying the rule and the latter is after applying it.", "labels": [], "entities": [{"text": "BTYPE_0 POS_0 POS_1", "start_pos": 68, "end_pos": 87, "type": "DATASET", "confidence": 0.8764256305164762}, {"text": "BTYPE_0", "start_pos": 91, "end_pos": 98, "type": "DATASET", "confidence": 0.7940825422604879}]}, {"text": "That's actually what transformation means.", "labels": [], "entities": []}, {"text": "When training, the initial boundary labels are all set to B 1 . At each step, the algorithm tries all the possible values for template variables to find an instantiated rule that can achieve the best score.", "labels": [], "entities": []}, {"text": "When testing, the initial boundary labels are set the same way, and then transformation rules are applied one by one.", "labels": [], "entities": []}, {"text": "To evaluate the generalization ability of the acquired rules, 5-fold cross validation tests are executed on the corpus for both C4.5 and TBL.", "labels": [], "entities": [{"text": "TBL", "start_pos": 137, "end_pos": 140, "type": "DATASET", "confidence": 0.8350887298583984}]}, {"text": "We reimplemented the RNN algorithm and POS bigram statistical model to predict prosodic word boundary on the same corpus for comparison.", "labels": [], "entities": []}, {"text": "Since our corpus is not large enough for HMM training and the CART method is also decision-tree based as C4.5, we didn't realize them in our experiments.", "labels": [], "entities": [{"text": "HMM training", "start_pos": 41, "end_pos": 53, "type": "TASK", "confidence": 0.9133994877338409}]}, {"text": "The evaluation results are shown in.", "labels": [], "entities": []}, {"text": "Both the C4.5 rules and the TBL rules outperform the RNN algorithm and POS bigram method because the overall accuracy rates Acc 2 of the rule based methods are higher.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 109, "end_pos": 117, "type": "METRIC", "confidence": 0.9990037083625793}, {"text": "Acc", "start_pos": 124, "end_pos": 127, "type": "METRIC", "confidence": 0.9770727157592773}]}, {"text": "TBL achieves comparable accuracy with C4.5 induction, which demonstrates that the design of transformation rule templates is successful.", "labels": [], "entities": [{"text": "TBL", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.5259938836097717}, {"text": "accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9995490908622742}]}, {"text": "Comparing Acc 1 and Acc 2 in, we discover that prosodic word boundaries can be more accurately predicted than prosodic phrase ones.", "labels": [], "entities": [{"text": "Acc", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9764190912246704}, {"text": "Acc", "start_pos": 20, "end_pos": 23, "type": "METRIC", "confidence": 0.9353870153427124}]}, {"text": "It can be explained as follows.", "labels": [], "entities": []}, {"text": "Prosodic word is the smallest prosodic unit in the prosodic hierarchy, which has more relation with the word level features such as POS, word length etc.", "labels": [], "entities": [{"text": "POS", "start_pos": 132, "end_pos": 135, "type": "METRIC", "confidence": 0.9784924387931824}]}, {"text": "Prosodic phrase is a larger prosodic unit less related to word level features, thus it cannot be predicted accurately using these features.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Confusion matrix  C ij s are the counts of boundaries whose true  label are B i but predicted as B j . From these  counts, we can deduce the evaluation  parameters for prosodic phrasing.", "labels": [], "entities": []}, {"text": " Table 3: Results of feature selection (F 0 , F 1 , F 2 , Acc 1 , Acc 2 are defined in section 3.2)", "labels": [], "entities": [{"text": "feature selection", "start_pos": 21, "end_pos": 38, "type": "TASK", "confidence": 0.7204978466033936}, {"text": "Acc", "start_pos": 58, "end_pos": 61, "type": "METRIC", "confidence": 0.9708508849143982}, {"text": "Acc 2", "start_pos": 66, "end_pos": 71, "type": "METRIC", "confidence": 0.9629249572753906}]}]}