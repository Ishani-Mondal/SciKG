{"title": [{"text": "Content Planner Construction via Evolutionary Algorithms and a Corpus-based Fitness Function", "labels": [], "entities": [{"text": "Content Planner Construction", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.6973447302977244}]}], "abstractContent": [{"text": "In this paper, we present a novel technique to learn a tree-like structure fora content planner from an aligned corpus of semantic inputs and corresponding, human-produced, outputs.", "labels": [], "entities": []}, {"text": "We apply a stochas-tic search mechanism with a two-level fitness function.", "labels": [], "entities": []}, {"text": "As a first stage, we use high level order constraints to quickly discard unpromising planners.", "labels": [], "entities": []}, {"text": "As a second stage, alignments between regenerated text and human output are employed.", "labels": [], "entities": []}, {"text": "We evaluate our approach by using the existing symbolic planner in our system as a gold standard , obtaining a 66% improvement over a random baseline in just 20 generations of genetic search.", "labels": [], "entities": []}], "introductionContent": [{"text": "Ina standard generation pipeline, a content planner is responsible for the higher level document structuring and information selection.", "labels": [], "entities": [{"text": "information selection", "start_pos": 113, "end_pos": 134, "type": "TASK", "confidence": 0.719153419137001}]}, {"text": "Any non-trivial multi-sentential/multi-paragraph generator will require a complex content planner, responsible for deciding, for instance, the distribution of the information among the different paragraphs, bulleted lists, and other textual elements.", "labels": [], "entities": []}, {"text": "Information-rich inputs require a thorough filtering, resulting in a small amount of the available data being conveyed in the output.", "labels": [], "entities": []}, {"text": "Furthermore, the task of building a content planner is normally recognized as tightly coupled with the semantics and idiosyncrasies of each particular domain.", "labels": [], "entities": []}, {"text": "The AI planning community is aware that machine learning techniques can bring a general solution to problems that require customization for every particular instantiation.", "labels": [], "entities": []}, {"text": "The automatic (or semi-automatic) construction of a complete content planner for unrestricted domains is a highly desirable goal.", "labels": [], "entities": []}, {"text": "While there are general tools and techniques to deal with surface realization () and sentence planning, the inherent dependency on each domain makes the content planning problem difficult to deal within a unified framework; it requires sophisticated planning methodologies, for example, DPOCL.", "labels": [], "entities": [{"text": "sentence planning", "start_pos": 85, "end_pos": 102, "type": "TASK", "confidence": 0.7235527634620667}]}, {"text": "The main problem is that the space of possible planners is so large.", "labels": [], "entities": []}, {"text": "For example, in the experiments reported here, it contains all the possible orderings of 82 units of information.", "labels": [], "entities": []}, {"text": "In this paper, we present a technique for learning the structure of tree-like planners, similar to the one manually built for our MAGIC system ().", "labels": [], "entities": []}, {"text": "The overall architecture for our learning of content planners is shown in.", "labels": [], "entities": []}, {"text": "As input we utilize an aligned corpus of semantic inputs aligned with human-produced discourse.", "labels": [], "entities": []}, {"text": "We also take advantage of the definition of the atomic operators (messages) from our existing system.", "labels": [], "entities": []}, {"text": "We learn these tree-like planners by means of a genetic search process.", "labels": [], "entities": []}, {"text": "The plan produced as output by such planners is a sequence of semantic structures, defined by the atomic operators.", "labels": [], "entities": []}, {"text": "The learning technique is complementary to approaches proposed for generation in summarization (, that utilize semantically annotated text to build content planners.", "labels": [], "entities": []}, {"text": "Our domain is the generation of post cardiacsurgery medical reports or briefings.", "labels": [], "entities": [{"text": "generation of post cardiacsurgery medical reports or briefings", "start_pos": 18, "end_pos": 80, "type": "TASK", "confidence": 0.727621890604496}]}, {"text": "MAGIC produces such a briefing given the output from inferences computed over raw data collected in the operating room).", "labels": [], "entities": []}, {"text": "Since we have a fully operational system, it serves as a development environment in which we can experiment with the automatic reproduction of the existing planner.", "labels": [], "entities": []}, {"text": "Once the learning system has been fully developed, we can move to other domains and learn new planners.", "labels": [], "entities": []}, {"text": "We will also eventually experiment with learning improved versions of the MAGIC planner through evaluation with healthcare providers.", "labels": [], "entities": [{"text": "MAGIC planner", "start_pos": 74, "end_pos": 87, "type": "DATASET", "confidence": 0.689605176448822}]}], "datasetContent": [{"text": "The framework described in the paper was implemented as follows: We employed a population of 2000 chromosomes, discarding 25% of the worsefitted ones in each cycle.", "labels": [], "entities": []}, {"text": "The vacant places were filled with 40% chromosomes generated by mutation and 60% by cross-over.", "labels": [], "entities": []}, {"text": "The mutation operator was applied with a 40% probability of performing anode insertion or deletion and 60% chance of choosing a shuffle mutation.", "labels": [], "entities": []}, {"text": "The population was started from a chromosome with one root node connected to a random ordering of the 82 operators and then not implemented in the current set of experiments.", "labels": [], "entities": []}, {"text": "nodes were inserted and shuffled 40 times.", "labels": [], "entities": []}, {"text": "The search algorithm was executed by 20 generations in 8d 14h (total CPU time, using about 20 machines in parallel to compute the verbalizations 8 ).", "labels": [], "entities": []}, {"text": "The best chromosome obtained was compared with the current planner in an intrinsic evaluation, described below.", "labels": [], "entities": []}, {"text": "Given the size of both planners, an automatic evaluation process was needed.", "labels": [], "entities": []}, {"text": "We use a metric that captures the structural similarities between the two planners.", "labels": [], "entities": []}, {"text": "In our metric, we recorded for each pair of atomic operators, the list of internal nodes that dominates both of them.", "labels": [], "entities": []}, {"text": "This information was used to build a matrix of counts with the size of such lists (for example, in the original planner \"age-node\" and \"name-node\" are both dominated by \"demographics-node\", \"overview-node\" and \"discourse-node\"; therefore, their entry in the table is the size of that list, i.e., 3).", "labels": [], "entities": []}, {"text": "Given the fact that both the MAGIC planner and our learned planners have the same set of atomic operators, it is possible to score their similarity by subtracting the associated matrices and then computing the average of the absolute values of this difference matrix.", "labels": [], "entities": [{"text": "MAGIC planner", "start_pos": 29, "end_pos": 42, "type": "DATASET", "confidence": 0.7423045039176941}]}, {"text": "Lower values will indicate closer similarity, with a perfect match receiving the value of 0.", "labels": [], "entities": []}, {"text": "Applying this metric to the MAGIC planner and our best planner we obtained the score 1.16.", "labels": [], "entities": [{"text": "MAGIC planner", "start_pos": 28, "end_pos": 41, "type": "DATASET", "confidence": 0.8181789815425873}]}, {"text": "We compare this score against 2000 random planners from the initial population of the genetic search.", "labels": [], "entities": []}, {"text": "Taking the average of their scores we obtain 3.08.", "labels": [], "entities": []}, {"text": "Finally, if we compare our learned planner against the random ones we obtain 2.92.", "labels": [], "entities": []}, {"text": "We clearly improve over this baseline.", "labels": [], "entities": []}, {"text": "An example of the learned planner is seen in (d).", "labels": [], "entities": []}], "tableCaptions": []}