{"title": [{"text": "Extracting Exact Answers to Questions Based on Structural Links *", "labels": [], "entities": [{"text": "Extracting Exact Answers to Questions Based on Structural Links", "start_pos": 0, "end_pos": 63, "type": "TASK", "confidence": 0.8468881977929009}]}], "abstractContent": [{"text": "This paper presents a novel approach to extracting phrase-level answers in a question answering system.", "labels": [], "entities": [{"text": "extracting phrase-level answers in a question answering system", "start_pos": 40, "end_pos": 102, "type": "TASK", "confidence": 0.6825378015637398}]}, {"text": "This approach uses structural support provided by an integrated Natural Language Processing (NLP) and Information Extraction (IE) system.", "labels": [], "entities": [{"text": "Information Extraction (IE)", "start_pos": 102, "end_pos": 129, "type": "TASK", "confidence": 0.7360822796821594}]}, {"text": "Both questions and the sentence-level candidate answer strings are parsed by this NLP/IE system into binary dependency structures.", "labels": [], "entities": []}, {"text": "Phrase-level answer extraction is modelled by comparing the structural similarity involving the question-phrase and the candidate answer-phrase.", "labels": [], "entities": [{"text": "Phrase-level answer extraction", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.7783222794532776}]}, {"text": "There are two types of structural support.", "labels": [], "entities": []}, {"text": "The first type involves predefined, specific entity associations such as Affiliation, Position, Age fora person entity.", "labels": [], "entities": [{"text": "Affiliation", "start_pos": 73, "end_pos": 84, "type": "METRIC", "confidence": 0.9040136933326721}]}, {"text": "If a question asks about one of these associations, the answer-phrase can be determined as long as the system decodes such pre-defined dependency links correctly, despite the syntactic difference used in expressions between the question and the candidate answer string.", "labels": [], "entities": []}, {"text": "The second type involves generic grammatical relationships such as V-S (verb-subject), V-O (verb-object).", "labels": [], "entities": []}, {"text": "Preliminary experimental results show an improvement in both precision and recall in extracting phrase-level answers, compared with a baseline system which only uses Named Entity constraints.", "labels": [], "entities": [{"text": "precision", "start_pos": 61, "end_pos": 70, "type": "METRIC", "confidence": 0.9993858337402344}, {"text": "recall", "start_pos": 75, "end_pos": 81, "type": "METRIC", "confidence": 0.9986239671707153}]}, {"text": "The proposed methods are particularly effective in cases where the question-phrase does not correspond to a known named entity type and in cases where there are multiple candidate answer-phrases satisfying the named entity constraints.", "labels": [], "entities": []}], "introductionContent": [{"text": "Natural language Question Answering (QA) is recognized as a capability with great potential.", "labels": [], "entities": [{"text": "Natural language Question Answering (QA)", "start_pos": 0, "end_pos": 40, "type": "TASK", "confidence": 0.7822189245905194}]}, {"text": "The NIST-sponsored Text Retrieval Conference (TREC) has been the driving force for developing this technology through its QA track since TREC-8).", "labels": [], "entities": [{"text": "NIST-sponsored Text Retrieval Conference (TREC)", "start_pos": 4, "end_pos": 51, "type": "TASK", "confidence": 0.7227094599178859}]}, {"text": "There has been significant progress and interest in QA research in recent years (.", "labels": [], "entities": [{"text": "QA research", "start_pos": 52, "end_pos": 63, "type": "TASK", "confidence": 0.9215031564235687}]}, {"text": "QA is different than search engines in two aspects: (i) instead of a string of keyword search terms, the query is a natural language question, necessitating question parsing, (ii) instead of a list of documents or URLs, a list of candidate answers at phrase level or sentence level are expected to be returned in response to a query, hence the need for text processing beyond keyword indexing, typically supported by Natural Language Processing (NLP) and Information Extraction (IE).", "labels": [], "entities": [{"text": "question parsing", "start_pos": 157, "end_pos": 173, "type": "TASK", "confidence": 0.7563467919826508}]}, {"text": "Examples of the use of NLP and IE in Question Answering include shallow parsing, semantic parsing), Named Entity tagging () and high-level IE).", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 37, "end_pos": 55, "type": "TASK", "confidence": 0.7549271583557129}, {"text": "shallow parsing", "start_pos": 64, "end_pos": 79, "type": "TASK", "confidence": 0.7022870779037476}, {"text": "semantic parsing", "start_pos": 81, "end_pos": 97, "type": "TASK", "confidence": 0.7087574750185013}, {"text": "Named Entity tagging", "start_pos": 100, "end_pos": 120, "type": "TASK", "confidence": 0.6068232357501984}]}, {"text": "Identifying exact or phrase-level answers is a much more challenging task than sentence-level answers.", "labels": [], "entities": [{"text": "Identifying exact or phrase-level answers", "start_pos": 0, "end_pos": 41, "type": "TASK", "confidence": 0.8532202005386352}]}, {"text": "Good performance on the latter can be achieved by using sophisticated passage retrieval techniques and/or shallow level NLP/IE processing ().", "labels": [], "entities": [{"text": "passage retrieval", "start_pos": 70, "end_pos": 87, "type": "TASK", "confidence": 0.8798698484897614}]}, {"text": "The phrase-level answer identification involves sophisticated NLP/IE and it is difficult to apply only IR techniques for this task).", "labels": [], "entities": [{"text": "phrase-level answer identification", "start_pos": 4, "end_pos": 38, "type": "TASK", "confidence": 0.6932462354501089}]}, {"text": "These two tasks are closely related.", "labels": [], "entities": []}, {"text": "Many systems (e.g. take a two-stage approach.", "labels": [], "entities": []}, {"text": "The first stage involves retrieving sentences or paragraphs in documents as candidate answer strings.", "labels": [], "entities": []}, {"text": "Stage Two focuses on extracting phrase-level exact answers from the candidate answer strings.", "labels": [], "entities": [{"text": "extracting phrase-level exact answers from the candidate answer strings", "start_pos": 21, "end_pos": 92, "type": "TASK", "confidence": 0.7421434587902493}]}, {"text": "This paper focuses on methods involving Stage Two.", "labels": [], "entities": []}, {"text": "The input is a sentence pair consisting of a question and a sentence-level candidate answer string.", "labels": [], "entities": []}, {"text": "The output is defined to be a phrase, called answer-point, extracted from the candidate answer string.", "labels": [], "entities": []}, {"text": "In order to identify the answerpoint, the pair of strings are parsed by the same system to generate binary dependency structures for both specific entity associations and generic grammatical relationships.", "labels": [], "entities": []}, {"text": "An integrated Natural Language Processing (NLP) and Information Extraction (IE) engine is used to extract named entities (NE) and their associations and to decode grammatical relationships.", "labels": [], "entities": [{"text": "Information Extraction (IE)", "start_pos": 52, "end_pos": 79, "type": "TASK", "confidence": 0.7766935706138611}]}, {"text": "The system searches for an answer-point by comparing the structural similarity involving the question-phrase and a candidate answer-phrase.", "labels": [], "entities": []}, {"text": "Generic grammatical relationships are used as a back-off for specific entity associations when the question goes beyond the scope of the specific associations or when the system fails to identify the answer-point which meets the specific entity association constraints.", "labels": [], "entities": []}, {"text": "The proposed methods are particularly helpful in cases where the question-phrase does not correspond to a known named entity type and in cases where there are multiple candidate answerpoints to select from.", "labels": [], "entities": []}, {"text": "The rest of the paper is structured as follows: Section 1 presents the NLP/IE engine used, sections 2 discusses how to identify and formally represent what is being asked, section 3 presents the algorithm on identifying exact answers leveraging structural support, section 4 presents case studies and benchmarks, and section 5 is the conclusion.", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to conduct the feasibility study on the proposed method, we selected the first 100 questions from the TREC-8 QA track pool and the corresponding first candidate answer sentences for this preliminary experiment.", "labels": [], "entities": [{"text": "TREC-8 QA track pool", "start_pos": 111, "end_pos": 131, "type": "DATASET", "confidence": 0.8751349449157715}]}, {"text": "The Stage One processing for generating candidate answer sentences was conducted by the existing ranking module of our QA system.", "labels": [], "entities": []}, {"text": "The Stage Two processing for answer-point identification was accomplished by using the algorithm described in Section 3.", "labels": [], "entities": [{"text": "answer-point identification", "start_pos": 29, "end_pos": 56, "type": "TASK", "confidence": 0.8788449764251709}]}, {"text": "As shown in, out of the 100 questionanswer pairs we selected, 9 have detected association links involving asking/answer points, 44 are found to have grammar links involving asking/answer points.", "labels": [], "entities": []}, {"text": "As for NE asking points, 76 questions were identified to require some type of NE as answers.", "labels": [], "entities": [{"text": "NE asking", "start_pos": 7, "end_pos": 16, "type": "TASK", "confidence": 0.8587037920951843}]}, {"text": "Assume that a baseline answer-point identification system only uses NE asking points as constraints, out of the 76 questions requiring NEs as answers, 41 answer-points were identified successfully because there was only one NE in the answer string which matches the required NE type.", "labels": [], "entities": [{"text": "answer-point identification", "start_pos": 23, "end_pos": 50, "type": "TASK", "confidence": 0.6805175393819809}]}, {"text": "The failed cases in matching NE asking point constraints include two situations: (i) no NE exists in the answer string; (ii) multiple NEs satisfy the type constraints of NE asking points (i.e. more than one candidate answer-points found from the answer string) or there is type conflict during the matching of NE asking/answer points.", "labels": [], "entities": []}, {"text": "Therefore, the baseline system would achieve 54% precision and 41% recall based on the standard precision and recall formulas: In comparison, in our answer-point identification system which leverages structural support from both the entity association links and grammar links as well as the NE asking points, both the precision and recall are raised: from the baseline 54% to 83% for precision and from 41% to 71% for recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 49, "end_pos": 58, "type": "METRIC", "confidence": 0.9989874958992004}, {"text": "recall", "start_pos": 67, "end_pos": 73, "type": "METRIC", "confidence": 0.9992939233779907}, {"text": "precision", "start_pos": 96, "end_pos": 105, "type": "METRIC", "confidence": 0.987457811832428}, {"text": "recall", "start_pos": 110, "end_pos": 116, "type": "METRIC", "confidence": 0.9485417008399963}, {"text": "precision", "start_pos": 318, "end_pos": 327, "type": "METRIC", "confidence": 0.9993054866790771}, {"text": "recall", "start_pos": 332, "end_pos": 338, "type": "METRIC", "confidence": 0.998238205909729}, {"text": "precision", "start_pos": 384, "end_pos": 393, "type": "METRIC", "confidence": 0.9962102174758911}, {"text": "recall", "start_pos": 418, "end_pos": 424, "type": "METRIC", "confidence": 0.9960801005363464}]}, {"text": "The significant improvement in precision and recall is attributed to the performance of structural matching in identifying exact answers.", "labels": [], "entities": [{"text": "precision", "start_pos": 31, "end_pos": 40, "type": "METRIC", "confidence": 0.999373733997345}, {"text": "recall", "start_pos": 45, "end_pos": 51, "type": "METRIC", "confidence": 0.9993465542793274}]}, {"text": "This demonstrates the benefits of making use of sophisticated NLP/IE technology, beyond NE and shallow parsing.", "labels": [], "entities": []}, {"text": "Using grammar links alone, exact answers were identified for 39 out of the 44 candidate answerpoints satisfying the types of grammar links in 100 cases.", "labels": [], "entities": []}, {"text": "During matching, 6 cases failed either due to the parsing error or due to the type conflict between the asking/answer points (e.g. violating the type constraints such as manner-modifier on the answer-point for 'how' question).", "labels": [], "entities": [{"text": "matching", "start_pos": 7, "end_pos": 15, "type": "TASK", "confidence": 0.9608095288276672}]}, {"text": "The high precision and modest recall in using the grammar constraints is understandable as the grammar links impose very strong constraints on both the nodes and the structural type.", "labels": [], "entities": [{"text": "precision", "start_pos": 9, "end_pos": 18, "type": "METRIC", "confidence": 0.9987272620201111}, {"text": "recall", "start_pos": 30, "end_pos": 36, "type": "METRIC", "confidence": 0.9986820816993713}]}, {"text": "The high precision performance indicates that grammar links not only have the distinguishing power to identify exact answers in the presence of multiple NE options but also recognize answers in the absence of asking point types.", "labels": [], "entities": [{"text": "precision", "start_pos": 9, "end_pos": 18, "type": "METRIC", "confidence": 0.9965648055076599}]}, {"text": "Even stronger structural support comes from the semantic relations decoded by the entity association extraction module.", "labels": [], "entities": [{"text": "entity association extraction", "start_pos": 82, "end_pos": 111, "type": "TASK", "confidence": 0.6349955499172211}]}, {"text": "In this case, the performance is naturally high-precision (89%) low-recall (8%) as predefined association links are by nature more sparse than generic grammatical relations.", "labels": [], "entities": []}, {"text": "In the following, we illustrate with some examples with questions from the TREC-8 QA task on how the match function identified in Section 3 applies to different question types.", "labels": [], "entities": [{"text": "TREC-8 QA task", "start_pos": 75, "end_pos": 89, "type": "DATASET", "confidence": 0.7588786482810974}]}, {"text": "This case requires (i) exact match in its original verb form between 'written' and 'write'; (ii) V-O type match; and (iii) asking/answer point match through indirect link based on equivalence link S-P.", "labels": [], "entities": [{"text": "exact", "start_pos": 23, "end_pos": 28, "type": "METRIC", "confidence": 0.9864120483398438}]}, {"text": "When there are no NE constraints on the answer point, a proper name or an initialcapitalized NP is preferred over an ordinary, lower-case NP as an answer point.", "labels": [], "entities": []}, {"text": "This heuristic is built-in so that 'Op.", "labels": [], "entities": []}, {"text": "126' is output as the answerpoint in this case instead of 'a late work'.", "labels": [], "entities": []}], "tableCaptions": []}