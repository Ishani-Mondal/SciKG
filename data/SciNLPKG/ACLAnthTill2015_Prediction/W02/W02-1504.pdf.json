{"title": [{"text": "Machine Translation as a testbed for multilingual analysis", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.811181902885437}]}], "abstractContent": [{"text": "We propose that machine translation (MT) is a useful application for evaluating and deriving the development of NL components, especially in a wide-coverage analysis system.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 16, "end_pos": 40, "type": "TASK", "confidence": 0.8651425838470459}]}, {"text": "Given the architecture of our MT system, which is a transfer system based on linguistic modules, correct analysis is expected to be a prerequisite for correct translation, suggesting a correlation between the two, given relatively mature transfer and generation components.", "labels": [], "entities": [{"text": "MT", "start_pos": 30, "end_pos": 32, "type": "TASK", "confidence": 0.9618541598320007}]}, {"text": "We show through error analysis that there is indeed a strong correlation between the quality of the translated output and the subjectively determined goodness of the analysis.", "labels": [], "entities": []}, {"text": "We use this correlation as a guide for development of a coordinated parallel analysis effort in 7 languages.", "labels": [], "entities": []}], "introductionContent": [{"text": "The question of how to test natural language analysis systems has been central to all natural language work in the past two decades.", "labels": [], "entities": []}, {"text": "It is a difficult question, for which researchers have found only partial answers.", "labels": [], "entities": []}, {"text": "The most common answer is component testing, where the component is compared against a standard of goodness, usually the Penn Treebank for English, allowing a numerical score of precision and recall (e.g..", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 121, "end_pos": 134, "type": "DATASET", "confidence": 0.9877549111843109}, {"text": "precision", "start_pos": 178, "end_pos": 187, "type": "METRIC", "confidence": 0.9994111061096191}, {"text": "recall", "start_pos": 192, "end_pos": 198, "type": "METRIC", "confidence": 0.9987547397613525}]}, {"text": "Such methods have limitations, however, and need to be supplemented by additional methods.", "labels": [], "entities": []}, {"text": "One limitation is the availability of annotated corpora, which do not exist for all languages.", "labels": [], "entities": []}, {"text": "Secondly, comparison to an annotated corpus can only measure how well a system produces the kind of analysis for which the corpus is annotated, e.g. labeled bracketing of surface syntax.", "labels": [], "entities": [{"text": "labeled bracketing of surface syntax", "start_pos": 149, "end_pos": 185, "type": "TASK", "confidence": 0.7471570849418641}]}, {"text": "Evaluation of analysis of deeper, more semantically descriptive, levels requires additional annotated corpora, which may not exist.", "labels": [], "entities": []}, {"text": "A more fundamental limitation of such methods is that they measure the goodness of a grammar without taking into account what the grammar is good for.", "labels": [], "entities": []}, {"text": "This limitation is overcome, we claim, only by measuring the goodness of a grammar by its success in real-world applications.", "labels": [], "entities": []}, {"text": "We propose that machine translation (MT) is a good application to evaluate and drive the development of analysis components when the transfer component is based on linguistic modules.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 16, "end_pos": 40, "type": "TASK", "confidence": 0.861758828163147}]}, {"text": "Multi-lingual applications such as MT allow evaluation of system components that overcomes the limitations mentioned above, and therefore serves as a useful complement to other evaluation techniques.", "labels": [], "entities": [{"text": "MT", "start_pos": 35, "end_pos": 37, "type": "TASK", "confidence": 0.955778181552887}]}, {"text": "Another significant advantage to using MT as a testbed for the analysis system is that it prioritizes analysis problems, highlighting those problems that have the greatest negative effect on translation output.", "labels": [], "entities": [{"text": "MT", "start_pos": 39, "end_pos": 41, "type": "TASK", "confidence": 0.9120774269104004}]}, {"text": "In this paper, we give an overview of NLPWin, a multi-application natural language analysis and generation system underdevelopment at Microsoft Research, incorporating analysis systems for 7 languages (Chinese, English, French, German, Japanese, Korean and Spanish).", "labels": [], "entities": []}, {"text": "Our discussion focuses on a description of the three components of the analysis system (called sketch, portrait and logical form) with a particular emphasis on the logical form derived as the endproduct, which serves as the medium for transfer in our MT system.", "labels": [], "entities": [{"text": "MT", "start_pos": 251, "end_pos": 253, "type": "TASK", "confidence": 0.9643301963806152}]}, {"text": "We also give an overview of the architecture of the MSR-MT system, and of the evaluation we use to measure correctness of the translations.", "labels": [], "entities": []}, {"text": "We demonstrate the correlation between the scores assigned to translation outputs and the correctness of the analysis, using as illustration two languagepairs at different stages of development: SpanishEnglish (SE) translation, as a testbed for the Spanish analysis system, and French-English (FE) translation, as a testbed for the French analysis system.", "labels": [], "entities": [{"text": "French-English (FE) translation", "start_pos": 278, "end_pos": 309, "type": "TASK", "confidence": 0.655748438835144}]}], "datasetContent": [{"text": "Seven evaluators are asked to evaluate the same set of sentences.", "labels": [], "entities": []}, {"text": "For each sentence, raters are presented with a reference sentence, the original English sentence from which the human French and Spanish translations were derived, and MSR-MT's machine translation.", "labels": [], "entities": [{"text": "MSR-MT", "start_pos": 168, "end_pos": 174, "type": "DATASET", "confidence": 0.8806851506233215}]}, {"text": "In order to maintain Microsoft manuals are written in English and translated by hand into other languages.", "labels": [], "entities": []}, {"text": "We use these translations as input to our system, and translate them back into English.", "labels": [], "entities": []}, {"text": "consistency among raters who may have different levels of fluency in the source language, raters are not shown the original French or Spanish sentence (for similar methodologies, see.", "labels": [], "entities": []}, {"text": "All the raters enter scores reflecting the absolute quality of the translation as compared to the reference translation given.", "labels": [], "entities": []}, {"text": "The overall score of a sentence is the average of the scores given by the seven raters.", "labels": [], "entities": []}, {"text": "Scores range from 1 to 4, with 1 meaning unacceptable (not comprehensible), 2 meaning possibly acceptable (some information is transferred accurately), 3 meaning acceptable (not perfect, but accurate transfer of all important information, and 4 meaning ideal (grammatically correct and all the important information is transferred).", "labels": [], "entities": []}], "tableCaptions": []}