{"title": [], "abstractContent": [{"text": "This paper examines the role that summaries can play in document retrieval.", "labels": [], "entities": [{"text": "summaries", "start_pos": 34, "end_pos": 43, "type": "TASK", "confidence": 0.96523517370224}, {"text": "document retrieval", "start_pos": 56, "end_pos": 74, "type": "TASK", "confidence": 0.7284748256206512}]}, {"text": "Thirty searches are applied to full-text and summaries only in large document collections, and the results are evaluated using two different evaluation scopes.", "labels": [], "entities": []}, {"text": "The results support the view that those customer segments who want smaller answer sets focused on highly relevant documents benefit from limiting their searches to summaries.", "labels": [], "entities": []}, {"text": "On the other hand, those customer segments who wish to retrieve all references to some topic should continue to search full-text.", "labels": [], "entities": []}], "introductionContent": [{"text": "The goal that motivated the creation of Searchable LEAD in news documents in the LexisNexis collection was to provide some customer segments with a tool that helps them focus their retrieval results on a limited number of highly relevant documents, where a highly relevant document with respect to some query is a document that is substantially about the query topic.", "labels": [], "entities": [{"text": "LexisNexis collection", "start_pos": 81, "end_pos": 102, "type": "DATASET", "confidence": 0.8040230870246887}]}, {"text": "A good, general purpose document summary should capture the major topics presented in a document.", "labels": [], "entities": []}, {"text": "Presumably if we can capture major topics in summaries, then a search that is restricted to summaries should do a better job of limiting retrieval results to highly relevant documents about those topics.", "labels": [], "entities": []}, {"text": "To support this, we created Searchable LEAD, a process that identifies and labels the leading sentences or paragraphs of news documents as a separate searchable LEAD field.", "labels": [], "entities": []}, {"text": "A customer's query, e.g., BUSH AND GORE, could easily be limited to the LEAD, e.g., LEAD(BUSH AND GORE), or HEADLINE and LEAD combination, e.g., HLEAD(BUSH AND GORE).", "labels": [], "entities": [{"text": "BUSH", "start_pos": 26, "end_pos": 30, "type": "METRIC", "confidence": 0.9374263286590576}, {"text": "GORE", "start_pos": 35, "end_pos": 39, "type": "METRIC", "confidence": 0.6702172756195068}, {"text": "LEAD", "start_pos": 72, "end_pos": 76, "type": "METRIC", "confidence": 0.9792018532752991}, {"text": "LEAD", "start_pos": 84, "end_pos": 88, "type": "METRIC", "confidence": 0.994274914264679}, {"text": "HLEAD", "start_pos": 145, "end_pos": 150, "type": "METRIC", "confidence": 0.9982288479804993}]}, {"text": "Through three separate experiments, the value of leading text as a general purpose summary for news documents has been verified.", "labels": [], "entities": []}, {"text": "This paper describes a fourth experiment that investigates whether and how searches limited to this type of summary benefit the targeted customers In many information retrieval experiments, a single user perspective, i.e., a single answer key, is used to evaluate the results.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 155, "end_pos": 176, "type": "TASK", "confidence": 0.6962965279817581}]}, {"text": "If that perspective matches that of the targeted customer set, the evaluation is meaningful.", "labels": [], "entities": []}, {"text": "However, different customer segments perform information seeking tasks with different goals and perspectives in mind, even when they are interested in the same topic.", "labels": [], "entities": []}, {"text": "Just as potential search tool enhancements are not one-size-fitsall, a one-size-fits-all answer key should not be used to determine the value of a search aid for two sets of customers with fundamentally different goals.", "labels": [], "entities": []}, {"text": "In this experiment, the results of each query were evaluated using two different user perspectives, highly relevant references only and all references.", "labels": [], "entities": []}, {"text": "Through this approach we were able to determine whether Searchable LEAD satisfied the goal that motivated its creation.", "labels": [], "entities": []}], "datasetContent": [{"text": "Most information retrieval experiments calculate recall, precision and the corresponding fmeasure from a single evaluation perspective or evaluation scope.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 5, "end_pos": 26, "type": "TASK", "confidence": 0.7274610996246338}, {"text": "recall", "start_pos": 49, "end_pos": 55, "type": "METRIC", "confidence": 0.9989084005355835}, {"text": "precision", "start_pos": 57, "end_pos": 66, "type": "METRIC", "confidence": 0.998673677444458}]}, {"text": "All documents are judged to be relevant or irrelevant with respect to that one scope.", "labels": [], "entities": []}, {"text": "However, commercial information services now report that they handle millions of searches a day for their customers.", "labels": [], "entities": []}, {"text": "It is not reasonable to assume that all of the people using those services have the same perspective on relevance, and yet that is often how we evaluate new search aids and features.", "labels": [], "entities": []}, {"text": "Our customers employ a variety of search strategies, depending on their topics, information interests, and the point they are at in their information seeking task.", "labels": [], "entities": []}, {"text": "At one end, we see some customers just starting out on an information seeking task, where they typically are looking fora few highly relevant documents to help introduce themselves to the topic.", "labels": [], "entities": [{"text": "information seeking task", "start_pos": 58, "end_pos": 82, "type": "TASK", "confidence": 0.8057274222373962}]}, {"text": "Basically they are trying to provide themselves with a good starting point.", "labels": [], "entities": []}, {"text": "At the other extreme, we see customers in public relations, competitive intelligence or in the due diligence phase of their information seeking task.", "labels": [], "entities": [{"text": "information seeking task", "start_pos": 124, "end_pos": 148, "type": "TASK", "confidence": 0.7878827651341757}]}, {"text": "These customers often want to retrieve all references to the topic, even those documents that provide even the most limited or mundane information.", "labels": [], "entities": []}, {"text": "Although some may see this simply as the customary recall-precision trade-off, that is not the case.", "labels": [], "entities": [{"text": "recall-precision", "start_pos": 51, "end_pos": 67, "type": "METRIC", "confidence": 0.9922440648078918}]}, {"text": "A document that contains a passing reference to some topic is relevant to those with the all reference evaluation scope (retrieval of that document is considered successful recall), but it is irrelevant to those with a highly relevant reference evaluation scope (retrieval of that document is considered a precision error).", "labels": [], "entities": [{"text": "recall", "start_pos": 173, "end_pos": 179, "type": "METRIC", "confidence": 0.9685534834861755}, {"text": "precision", "start_pos": 306, "end_pos": 315, "type": "METRIC", "confidence": 0.998079776763916}]}, {"text": "A document's relevance with respect to some customer's evaluation scope is what drives customer perceptions of the resulting recall and precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 125, "end_pos": 131, "type": "METRIC", "confidence": 0.9988191723823547}, {"text": "precision", "start_pos": 136, "end_pos": 145, "type": "METRIC", "confidence": 0.9882148504257202}]}, {"text": "Instead of a recall-precision trade-off, we have multiple evaluation scopes for which recall and precision are determined.", "labels": [], "entities": [{"text": "recall-precision", "start_pos": 13, "end_pos": 29, "type": "METRIC", "confidence": 0.9915540814399719}, {"text": "recall", "start_pos": 86, "end_pos": 92, "type": "METRIC", "confidence": 0.9990139007568359}, {"text": "precision", "start_pos": 97, "end_pos": 106, "type": "METRIC", "confidence": 0.9984228610992432}]}, {"text": "We recognize the differences in evaluation scopes in a single user overtime when proposing learning systems and personalization tools that adapt retrieval or routing results to a user's changing interests (e.g.,), but we do not recognize these differences when we use single answer key evaluations.", "labels": [], "entities": []}, {"text": "As a result, over the years, we have seen a number of potentially useful search enhancements dismissed not because they failed to show improvement for any targeted subset of customers, but rather because they failed to show improvement when using a single general evaluation standard).", "labels": [], "entities": []}, {"text": "Query expansion functionality such as some types of morphological or synonym expansion, for example, may produce a drop in precision that offsets any improvements to recall, but we have found that customer segments who require retrieving all references to their topic are willing to put up with a lot of irrelevant information to make sure that they see everything.", "labels": [], "entities": [{"text": "synonym expansion", "start_pos": 69, "end_pos": 86, "type": "TASK", "confidence": 0.7055226564407349}, {"text": "precision", "start_pos": 123, "end_pos": 132, "type": "METRIC", "confidence": 0.9990277290344238}, {"text": "recall", "start_pos": 166, "end_pos": 172, "type": "METRIC", "confidence": 0.9973577857017517}]}, {"text": "Of course, those customers would still like to have better precision, but they require better recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 59, "end_pos": 68, "type": "METRIC", "confidence": 0.9989079236984253}, {"text": "recall", "start_pos": 94, "end_pos": 100, "type": "METRIC", "confidence": 0.9987319111824036}]}, {"text": "This was also a problem with the limited retrieval experiment reported in.", "labels": [], "entities": []}, {"text": "Although Searchable LEAD was introduced specifically to support the subset of users seeking only highly relevant documents, did not make this distinction when evaluating their test of twelve Boolean queries.", "labels": [], "entities": []}, {"text": "For each query evaluated in the experiment reported here, two user evaluation scopes were created.", "labels": [], "entities": []}, {"text": "One represented Searchable LEAD's targeted customer segment and its desire to retrieve only highly relevant documents; the other represented the due diligence customer segment, which prefers to retrieve all documents that contain information about the topic regardless of how little or how much.", "labels": [], "entities": []}, {"text": "The purpose of Searchable LEAD as a retrieval aid is to help some customer segments retrieve a highly relevant documents about some topic, and to minimize the number of irrelevant documents and documents that only contain passing references to the topic in the answer set.", "labels": [], "entities": []}, {"text": "If Searchable LEAD works, one would expect that queries restricted to the LEAD field would result in higher precision than queries applied to the full-text would.", "labels": [], "entities": [{"text": "precision", "start_pos": 108, "end_pos": 117, "type": "METRIC", "confidence": 0.9959757924079895}]}, {"text": "For the all reference evaluation scope, one would expect recall to fall when shifting from full-text to LEAD.", "labels": [], "entities": [{"text": "recall", "start_pos": 57, "end_pos": 63, "type": "METRIC", "confidence": 0.9993793964385986}]}, {"text": "After all, a general summary like LEAD typically only includes information on major points in the document.", "labels": [], "entities": []}, {"text": "The impact on recall for the highly relevant reference evaluation scope is less certain.", "labels": [], "entities": [{"text": "recall", "start_pos": 14, "end_pos": 20, "type": "METRIC", "confidence": 0.9990322589874268}]}, {"text": "Because the Searchable LEAD represents an acceptable summary in only 94% of general news articles, and a lower figure in other types of documents found in the LexisNexis NEWS library, it is also reasonable to assume that some decline in recall would also occur with this evaluation scope.", "labels": [], "entities": [{"text": "LexisNexis NEWS library", "start_pos": 159, "end_pos": 182, "type": "DATASET", "confidence": 0.7207022309303284}, {"text": "recall", "start_pos": 237, "end_pos": 243, "type": "METRIC", "confidence": 0.9991695880889893}]}, {"text": "Given that relevant documents with this evaluation scope must include all the targeted information, recall errors as defined by this scope may actually eliminate information redundancy, and thus are not necessarily critical to the customer.", "labels": [], "entities": [{"text": "recall", "start_pos": 100, "end_pos": 106, "type": "METRIC", "confidence": 0.9888455271720886}]}, {"text": "However, the way in which basic pieces of information are presented can also be revealing, so such redundant documents may still be useful.", "labels": [], "entities": []}, {"text": "Calculating recall in these cases thus is still worthwhile.", "labels": [], "entities": [{"text": "recall", "start_pos": 12, "end_pos": 18, "type": "METRIC", "confidence": 0.9980114698410034}]}, {"text": "Recall and precision rates were calculated for each query for each evaluation scope-text scope combination.", "labels": [], "entities": [{"text": "Recall", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9928027987480164}, {"text": "precision rates", "start_pos": 11, "end_pos": 26, "type": "METRIC", "confidence": 0.9823996424674988}]}, {"text": "For each full-text/LEAD pair, recall and precision rates were compared to see how consistent increases and decreases were with respect to expectations.", "labels": [], "entities": [{"text": "recall", "start_pos": 30, "end_pos": 36, "type": "METRIC", "confidence": 0.9996323585510254}, {"text": "precision", "start_pos": 41, "end_pos": 50, "type": "METRIC", "confidence": 0.9987055063247681}]}], "tableCaptions": [{"text": " Table 1. LEAD as Summary Acceptability  Rates for Document Types  Zhou (1999) reported the results of an  experiment where Searchable LEADs were", "labels": [], "entities": []}, {"text": " Table 2. Averages for thirty queries using the  highly relevant reference evaluation  scope.  (NOTE: The f-measure listed under LEAD is  lower than both the corresponding recall and  precision. Keep in mind that the figures above  represent averages for thirty queries. The f- measure .450 thus is not based on a recall rate of  .593 and a precision rate of .516 but rather it is  the average of thirty individual f-measures. This  also explains f-measures provided in", "labels": [], "entities": [{"text": "NOTE", "start_pos": 96, "end_pos": 100, "type": "METRIC", "confidence": 0.7328774929046631}, {"text": "LEAD", "start_pos": 129, "end_pos": 133, "type": "METRIC", "confidence": 0.6556567549705505}, {"text": "recall", "start_pos": 172, "end_pos": 178, "type": "METRIC", "confidence": 0.9981759786605835}, {"text": "precision", "start_pos": 184, "end_pos": 193, "type": "METRIC", "confidence": 0.9882843494415283}, {"text": "f- measure .450", "start_pos": 275, "end_pos": 290, "type": "METRIC", "confidence": 0.7798002481460571}, {"text": "recall rate", "start_pos": 314, "end_pos": 325, "type": "METRIC", "confidence": 0.9798622131347656}, {"text": "precision rate", "start_pos": 341, "end_pos": 355, "type": "METRIC", "confidence": 0.9826963841915131}]}, {"text": " Table 3. Averages for thirty queries using the  all reference evaluation scope.  The trends represented by the results in these  tables were generally consistent with the results  for individual queries. When using the highly  relevant reference evaluation scope, precision  rates and f-measures increased for 26 of the  thirty queries when shifting from full-text to  LEAD. When using the all reference evaluation  scope, recall rates decreased or stayed steady  and f-measures decreased for all thirty queries  when shifting from full-text to LEAD.", "labels": [], "entities": [{"text": "precision", "start_pos": 265, "end_pos": 274, "type": "METRIC", "confidence": 0.999306321144104}, {"text": "recall", "start_pos": 424, "end_pos": 430, "type": "METRIC", "confidence": 0.998823344707489}]}, {"text": " Table 4. Averages for thirty queries combining  both the highly relevant reference and  all reference evaluation scopes.  As for other retrieval tasks, when creating a  document categorization system, we did gain  some benefits when weighting terms found in  headlines and leading text in news documents a  bit higher", "labels": [], "entities": [{"text": "weighting terms found in  headlines and leading text in news documents", "start_pos": 234, "end_pos": 304, "type": "TASK", "confidence": 0.6978624571453441}]}]}