{"title": [{"text": "A Minimum Message Length Approach for Argument Interpretation", "labels": [], "entities": [{"text": "Minimum Message Length Approach", "start_pos": 2, "end_pos": 33, "type": "TASK", "confidence": 0.7409827262163162}, {"text": "Argument Interpretation", "start_pos": 38, "end_pos": 61, "type": "TASK", "confidence": 0.742192804813385}]}], "abstractContent": [{"text": "We describe a mechanism which receives as input a segmented argument composed of NL sentences, and generates an interpretation.", "labels": [], "entities": []}, {"text": "Our mechanism relies on the Minimum Message Length Principle for the selection of an interpretation among candidate options.", "labels": [], "entities": [{"text": "Minimum Message Length Principle", "start_pos": 28, "end_pos": 60, "type": "METRIC", "confidence": 0.6330630034208298}]}, {"text": "This enables our mechanism to cope with noisy input in terms of wording, beliefs and argument structure ; and reduces its reliance on a particular knowledge representation.", "labels": [], "entities": []}, {"text": "The performance of our system was evaluated by distorting automatically generated arguments , and passing them to the system for interpretation.", "labels": [], "entities": []}, {"text": "In 75% of the cases, the interpretations produced by the system matched precisely or almost-precisely the representation of the original arguments.", "labels": [], "entities": []}], "introductionContent": [{"text": "Discourse interpretation is at the cornerstone of human-computer communication, and an essential component of any dialogue system.", "labels": [], "entities": [{"text": "Discourse interpretation", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8151892423629761}]}, {"text": "In order to produce an interpretation from a user's NL utterances, the concepts referenced by the user's words must be identified, the propositions built using these concepts must be understood, and the relations between these propositions must be determined.", "labels": [], "entities": []}, {"text": "Each of these tasks is fraught with uncertainty.", "labels": [], "entities": []}, {"text": "In this paper, we focus on the interpretation of argumentative discourse, which is composed of implications.", "labels": [], "entities": [{"text": "interpretation of argumentative discourse", "start_pos": 31, "end_pos": 72, "type": "TASK", "confidence": 0.7995995134115219}]}, {"text": "We present a mechanism for the interpretation of NL arguments which is based on the application of the Minimum Message Length (MML) Principle for the evaluation of candidate interpretations (.", "labels": [], "entities": [{"text": "interpretation of NL arguments", "start_pos": 31, "end_pos": 61, "type": "TASK", "confidence": 0.8695707321166992}, {"text": "Minimum Message Length (MML) Principle", "start_pos": 103, "end_pos": 141, "type": "METRIC", "confidence": 0.714767085654395}]}, {"text": "The MML principle provides a uniform and incremental framework for combining the uncertainty arising from different stages of the interpretation process.", "labels": [], "entities": []}, {"text": "This enables our mechanism to cope with noisy input in terms of wording, beliefs and argument structure, and to factor out the elements of an interpretation which rely on a particular knowledge representation.", "labels": [], "entities": []}, {"text": "Our interpretation mechanism is embedded in a web-based argumentation system called BIAS (Bayesian Interactive Argumentation System).", "labels": [], "entities": [{"text": "BIAS", "start_pos": 84, "end_pos": 88, "type": "METRIC", "confidence": 0.9790867567062378}]}, {"text": "BIAS uses Bayesian Networks (BNs) as its knowledge representation and reasoning formalism.", "labels": [], "entities": [{"text": "BIAS", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9424238801002502}]}, {"text": "It is designed to be a comprehensive argumentation system which will eventually engage in an unrestricted interaction with users.", "labels": [], "entities": []}, {"text": "However, the current version of BIAS performs two activities: it generates its own arguments (from a BN) and interprets users' arguments (generating a Bayesian subnet as an interpretation of these arguments).", "labels": [], "entities": []}, {"text": "In this paper we focus on the interpretation task.(a) shows a simple argument given by a user, and(d) shows a subset of a BN which contains the preferred interpretation of the user's argument; the nodes corresponding to the user's input are shaded.", "labels": [], "entities": []}, {"text": "The user's argument is obtained through a web interface (the uncertainty value of the consequent is entered using a drop-down menu).", "labels": [], "entities": []}, {"text": "In this example, the user's input differs structurally from the system's interpretation, the belief value for the consequent differs from that in the domain BN, and the wording of the statements differs from the canonical wording of the BN nodes.", "labels": [], "entities": []}, {"text": "Still, the system found a reasonable interpretation in the context of its domain model.", "labels": [], "entities": []}, {"text": "The results obtained in this informal trial are validated by our automated evaluation.", "labels": [], "entities": []}, {"text": "This evalua- tion, which assesses baseline performance, consists of passing distorted versions of the system's arguments back to the system for interpretation.", "labels": [], "entities": []}, {"text": "In 75% of the cases, the interpretations produced by the system matched the original arguments (in BN form) precisely or almost-precisely.", "labels": [], "entities": []}, {"text": "In the next section, we review related research.", "labels": [], "entities": []}, {"text": "We then describe the application of the MML criterion to the evaluation of interpretations.", "labels": [], "entities": []}, {"text": "In Section 4, we outline the argument interpretation process.", "labels": [], "entities": [{"text": "argument interpretation", "start_pos": 29, "end_pos": 52, "type": "TASK", "confidence": 0.7602226138114929}]}, {"text": "The results of our evaluation are reported in Section 5, followed by concluding remarks.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our evaluation consisted of an automated experiment where the system interpreted noisy versions of its own arguments.", "labels": [], "entities": []}, {"text": "These arguments were generated from different sub-nets of its domain BN, and they were distorted at the BN level and at the NL level.", "labels": [], "entities": []}, {"text": "At the BN level, we changed the beliefs in the nodes, and we inserted and deleted nodes and arcs.", "labels": [], "entities": []}, {"text": "At the NL level, we distorted the wording of the propositions in the resultant arguments.", "labels": [], "entities": []}, {"text": "All these distortions were performed for BNs of different sizes (3, 5, 7 and 9 arcs).", "labels": [], "entities": []}, {"text": "Our measure of performance is the edit-distance between the original BN used to generate an argument, and the BN produced as the interpretation of this argument.", "labels": [], "entities": []}, {"text": "That is, we counted the number of differences between the source BN and the interpretation.", "labels": [], "entities": [{"text": "BN", "start_pos": 65, "end_pos": 67, "type": "METRIC", "confidence": 0.9016788005828857}]}, {"text": "For instance, two BNs that differ by one arc have an edit-distance of 2 (one addition and one deletion), while a perfect match has an editdistance of 0.", "labels": [], "entities": []}, {"text": "Overall, our results were as follows.", "labels": [], "entities": []}, {"text": "Our system produced an interpretation in 86% of the 5400 trials.", "labels": [], "entities": [{"text": "interpretation", "start_pos": 23, "end_pos": 37, "type": "TASK", "confidence": 0.9370930790901184}]}, {"text": "In 75% of the 5400 cases, the generated interpretations had an edit-distance of 3 or less from the original BN, and in 50% of the cases, the interpretations matched perfectly the original BN.", "labels": [], "entities": [{"text": "edit-distance", "start_pos": 63, "end_pos": 76, "type": "METRIC", "confidence": 0.9840270280838013}, {"text": "BN", "start_pos": 108, "end_pos": 110, "type": "METRIC", "confidence": 0.6906830072402954}]}, {"text": "depicts the frequency of edit distances for the different BN sizes under all noise conditions.", "labels": [], "entities": []}, {"text": "We plotted edit-distances of 0, , plus the category NI, which stands for \"No Interpretation\".", "labels": [], "entities": []}, {"text": "As shown in, the 0 edit-distance has the highest frequency, and performance deteriorates as BN size increases.", "labels": [], "entities": [{"text": "frequency", "start_pos": 49, "end_pos": 58, "type": "METRIC", "confidence": 0.9677742123603821}, {"text": "BN size", "start_pos": 92, "end_pos": 99, "type": "METRIC", "confidence": 0.8790965378284454}]}, {"text": "Nonetheless, for BNs of 7 arcs or less, the vast majority of the interpretations have an edit distance of 3 or less.", "labels": [], "entities": [{"text": "BNs", "start_pos": 17, "end_pos": 20, "type": "METRIC", "confidence": 0.5073251724243164}, {"text": "edit distance", "start_pos": 89, "end_pos": 102, "type": "METRIC", "confidence": 0.9779885709285736}]}, {"text": "Only for BNs of 9 arcs the number of NIs exceeds the number of perfect matches.", "labels": [], "entities": [{"text": "BNs", "start_pos": 9, "end_pos": 12, "type": "METRIC", "confidence": 0.9452966451644897}, {"text": "NIs", "start_pos": 37, "end_pos": 40, "type": "METRIC", "confidence": 0.967056393623352}]}, {"text": "provides a different view of these results.", "labels": [], "entities": []}, {"text": "It displays edit-distance as a percentage of the possible changes fora BN of a particular size (the xaxis is divided into buckets of 10%).", "labels": [], "entities": []}, {"text": "For example, if a selected interpretation differs from its source-BN by the insertion of one arc, the percent-edit-distance will be , where is the number of arcs in the source-BN.", "labels": [], "entities": []}, {"text": "The results shown in are consistent with the previous results, with the vast majority of the edits being in the [0,10)% bucket.", "labels": [], "entities": []}, {"text": "That is, most of the interpretations are within 10% of their source-BNs.", "labels": [], "entities": []}, {"text": "We also tested each kind of noise separately,  maintaining the other kinds of noise at 0%.", "labels": [], "entities": []}, {"text": "All the distortions were between 0 and 40%.", "labels": [], "entities": [{"text": "distortions", "start_pos": 8, "end_pos": 19, "type": "METRIC", "confidence": 0.99776291847229}]}, {"text": "We performed 1560 trials for word noise, arc noise and node insertions, and 2040 trials for belief noise, which warranted additional observations.", "labels": [], "entities": [{"text": "node insertions", "start_pos": 55, "end_pos": 70, "type": "TASK", "confidence": 0.6605368256568909}]}, {"text": "show the recognition accuracy of our system (in terms of average edit distance) as a function of arc noise, belief noise and word noise percentages, respectively.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9798144698143005}]}, {"text": "The performance for the different BN sizes (in arcs) is also shown.", "labels": [], "entities": []}, {"text": "Our system's performance for node insertions is similar to that obtained for belief noise (the graph was not included owing to space limitations).", "labels": [], "entities": [{"text": "node insertions", "start_pos": 29, "end_pos": 44, "type": "TASK", "confidence": 0.7223128229379654}]}, {"text": "Our results show that the two main factors that affect recognition performance are BN size and word noise, while the average edit distance remains stable for belief and arc noise, as well as for node insertions (the only exception occurs for 40% arc noise and size 9 BNs).", "labels": [], "entities": []}, {"text": "Specifically, for arc noise, belief noise and node insertions, the average edit distance was 3 or less for all noise percentages, while for word noise, the average edit distance was higher for several word-noise and BN-size combinations.", "labels": [], "entities": [{"text": "node insertions", "start_pos": 46, "end_pos": 61, "type": "TASK", "confidence": 0.6771141588687897}]}, {"text": "Further, performance deteriorated as the percentage of word noise increased.", "labels": [], "entities": []}, {"text": "The impact of word noise on performance reinforces our intention to implement a more principled sentence comparison procedure (Section 4.1), with the expectation that it will improve this aspect of our system's performance.", "labels": [], "entities": []}], "tableCaptions": []}