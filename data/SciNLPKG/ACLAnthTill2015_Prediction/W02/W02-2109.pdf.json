{"title": [{"text": "From Discourse Plans to Believable Behavior Generation", "labels": [], "entities": []}], "abstractContent": [{"text": "Developing an embodied conversational agent that is able to exhibit a human-like behavior while communicating with other virtual or human agents requires enriching atypical NLG architecture.", "labels": [], "entities": []}, {"text": "The purpose of this paper is to describe our efforts in this direction and to illustrate our approach to the generation of an Agent that shows a personality, asocial intelligence and is able to react emotionally to events occurring in the environment, consistently with her goals and with the context in which the conversation takes place.", "labels": [], "entities": []}], "introductionContent": [{"text": "Humans communicate combining signals of different nature.", "labels": [], "entities": []}, {"text": "Body posture, gestures (pointing at something, describing object dimensions,...), facial expressions, gaze (making eye contact, looking down or up, at a particular object,\u2026) maybe combined with speech.", "labels": [], "entities": []}, {"text": "The way in which people communicate, and therefore the employed signals, is influenced by their personality, goals and affective state and by the context in which the conversation takes place.", "labels": [], "entities": []}, {"text": "Developing a \"computer conversationalist\" that is embedded, for instance, in a virtual human-like body and is able to exhibit these added dimensions of communication requires moving from natural language generation to multimodal behavior generation.", "labels": [], "entities": [{"text": "multimodal behavior generation", "start_pos": 218, "end_pos": 248, "type": "TASK", "confidence": 0.6335427761077881}]}, {"text": "The purpose of this paper is to describe our efforts in this direction and to illustrate how atypical NLG architecture has been changed to generate context-adapted behavior in a Conversational Embodied Agent.", "labels": [], "entities": []}, {"text": "Our Agent shows a personality and asocial intelligence and is able to react emotionally to events occurring in the environment, consistently with the context in which the conversation takes place and with her goals.", "labels": [], "entities": []}, {"text": "To achieve such a context-adaptable multimodal behavior we employed atypical pipelined NLG architecture): given a communicative goal to be achieved, the behavior generator plans the communication content at an abstract level (\"what to say\") and then realises it at the surface level according to expressive capabilities of the used \"body\" and to the conversational context (\"how to say it\").", "labels": [], "entities": []}, {"text": "These two steps maybe more or less complex and may require an additional phase, aimed at \"optimising\" the communication from the content and style viewpoint (\"sentence planner\").", "labels": [], "entities": []}, {"text": "Planning the behaviour of our Agent maybe approached in two alternative ways.", "labels": [], "entities": []}, {"text": "In the first one, the planner may decide which verbal and nonverbal signal/s to employ in every conversational move; in the second one, it may define only the communicative function/s to attach to every move and leave, to the surface realizer, the task of deciding which signal/s to employ.", "labels": [], "entities": []}, {"text": "The first perspective requires exploiting knowledge about mental and physical capabilities of the agent during planning.", "labels": [], "entities": []}, {"text": "The planner must, as usual, establish the discourse steps that the agent will carryout to achieve the given communicative goal; in addition, it has to indicate the combination of signals through which every step of the planned discourse will be rendered.", "labels": [], "entities": []}, {"text": "The main advantage of this solution is that the dialog move will be planned consistently with the agent's state, by establishing how verbal and nonverbal components will be combined so as to avoid introducing signal redundancies or to decide to introduce them on purpose: for instance, by expressing a deictic act only with gaze or with both gaze and words.", "labels": [], "entities": []}, {"text": "Obviously, this solution is bodydependent.", "labels": [], "entities": []}, {"text": "In the second perspective, the plan move will be signal-independent and the \"body\" generator will interpret what was produced by the planner to realise it according to the specific body capabilities.", "labels": [], "entities": []}, {"text": "To avoid signal redundancy or conflicts, this second approach requires bodydependent rules at an intermediate sentence planning level.", "labels": [], "entities": []}, {"text": "This is the perspective we adopt.", "labels": [], "entities": []}, {"text": "In this view, the Agent is seen as an entity made up of two main components, a 'Mind' and a 'Body', which are interfaced by a common I/O language, so as to overcome integration problems and to allow their independence and modularity.", "labels": [], "entities": []}, {"text": "During the conversation, the Agent's Mind decides what to communicate, by considering the dialogue history, the conversational context and her own current cognitive state.", "labels": [], "entities": []}, {"text": "The Body \"reads\" what the Mind decides to communicate and interprets and renders it at the surface level, according to the available communicative channels: different bodies may have different expressive capabilities and therefore may use different channels.", "labels": [], "entities": []}, {"text": "To achieve a rich expressiveness, the output of the Agent's Mind cannot be just a combination of symbolic descriptions of communicative acts.", "labels": [], "entities": []}, {"text": "It should include, as well, a specification of the 'meanings' that the Body will have to attach to each of them.", "labels": [], "entities": []}, {"text": "The Mind of our believable conversationalist has to be able to perform the following functions: select an appropriate dialog move, decide whether, in correspondence of that move, an emotion is triggered and, finally, specify the meanings that have to be conveyed through the selected move.", "labels": [], "entities": []}, {"text": "These meanings include the communicative functions that are typically used in human-human dialogs: topic-comment, affective, meta-cognitive, performative, deictic, adjectival and belief relation functions ().", "labels": [], "entities": []}, {"text": "To specify the format of the dialog move that should act as an interface between the Agent's Mind and her Body, we designed a Mind-Body interface that takes as input a specification of a discourse plan in an XML language (DPML: Discourse Plan Markup Language) and enriches this plan with the meanings that have to be attached to it, by producing an input to the Body in anew XML language (APML: Affective Presentation Markup Language).", "labels": [], "entities": []}, {"text": "This approach is also motivated by the fact that the Body we use in the MagiCster project 1 is a 3D realistic face called Greta ( ) and a synthetic voice.", "labels": [], "entities": [{"text": "Greta", "start_pos": 122, "end_pos": 127, "type": "METRIC", "confidence": 0.8713324666023254}]}, {"text": "This paper is structured as follows.", "labels": [], "entities": []}, {"text": "After describing our two-layered system architecture, we will describe the Mind-Body interface.", "labels": [], "entities": []}, {"text": "To illustrate how this architecture works, we will use an example from the medical domain.", "labels": [], "entities": []}, {"text": "Conclusions will be discussed in the last Section.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}