{"title": [{"text": "A Simple but Powerful Automatic Term Extraction Method", "labels": [], "entities": [{"text": "Automatic Term Extraction", "start_pos": 22, "end_pos": 47, "type": "TASK", "confidence": 0.6521307528018951}]}], "abstractContent": [{"text": "In this paper, we propose anew idea for the automatic recognition of domain specific terms.", "labels": [], "entities": [{"text": "automatic recognition of domain specific terms", "start_pos": 44, "end_pos": 90, "type": "TASK", "confidence": 0.7564315448204676}]}, {"text": "Our idea is based on the statistics between a compound noun and its component single-nouns.", "labels": [], "entities": []}, {"text": "More precisely, we focus basically on how many nouns adjoin the noun in question to form compound nouns.", "labels": [], "entities": []}, {"text": "We propose several scoring methods based on this idea and experimentally evaluate them on the NTCIR1 TMREC test collection.", "labels": [], "entities": [{"text": "NTCIR1 TMREC test collection", "start_pos": 94, "end_pos": 122, "type": "DATASET", "confidence": 0.9419437795877457}]}, {"text": "The results are very promising especially in the low recall area.", "labels": [], "entities": [{"text": "recall", "start_pos": 53, "end_pos": 59, "type": "METRIC", "confidence": 0.997680127620697}]}], "introductionContent": [{"text": "Automatic term recognition, ATR in short, aims at extracting domain specific terms from a corpus of a certain academic or technical domain.", "labels": [], "entities": [{"text": "Automatic term recognition", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.7081712484359741}, {"text": "extracting domain specific terms from a corpus of a certain academic or technical domain", "start_pos": 50, "end_pos": 138, "type": "TASK", "confidence": 0.6865164786577225}]}, {"text": "The majority of domain specific terms are compound nouns, in other words, uninterrupted collocations.", "labels": [], "entities": []}, {"text": "85% of domain specific terms are said to be compound nouns.", "labels": [], "entities": []}, {"text": "They include single-nouns of the remaining 15% very frequently as their components, where \"single-noun\" means a noun which could not be further divided into several shorter and more basic nouns.", "labels": [], "entities": []}, {"text": "In other words, the majority of compound nouns consist of the much smaller number of the remaining 15% single-noun terms and other single-nouns.", "labels": [], "entities": []}, {"text": "In this situation, it is natural to pay attention to the relation among single-nouns and compound nouns, especially how single-noun terms contribute to makeup compound noun terms.", "labels": [], "entities": []}, {"text": "Another important feature of domain specific terms is termhood proposed in where \"termhood\" refers to the degree that a linguistic unit is related to a domain-specific concept.", "labels": [], "entities": []}, {"text": "Thus, what we really have to pursue is an ATR method which directly uses the notion of termhood.", "labels": [], "entities": []}, {"text": "Considering these factors, the way of making up compound nouns must be heavily related to the termhood of the compound nouns.", "labels": [], "entities": []}, {"text": "The first reason is that termhood is usually calculated based on term frequency and bias of term frequency like inverse document frequency.", "labels": [], "entities": []}, {"text": "Even though these calculations give a good approximation of termhood, still they are not directly related to termhood because these calculations are based on superficial statistics.", "labels": [], "entities": []}, {"text": "That means that they are not necessarily meanings in a writer's mind but meanings in actual use.", "labels": [], "entities": []}, {"text": "Apparently, termhood is intended to reflect this type of meaning.", "labels": [], "entities": []}, {"text": "The second reason is that if a certain single-noun, say N, expresses the key concept of a domain that the document treats, the writer of the document must be using N not only frequently but also in various ways.", "labels": [], "entities": []}, {"text": "For instance, he/she composes quite a few compound nouns using N and uses these compound nouns in documents he/she writes.", "labels": [], "entities": []}, {"text": "Thus, we focus on the relation among single-nouns and compound nouns in pursuing new ATR methods.", "labels": [], "entities": [{"text": "ATR", "start_pos": 85, "end_pos": 88, "type": "TASK", "confidence": 0.9200143218040466}]}, {"text": "The first attempt to make use of this relation has been done by) through the number of distinct single-nouns that come to the left or right of a single-noun term when used in compound noun terms.", "labels": [], "entities": []}, {"text": "Using this type of number associated with a single-noun term, Nakagawa and Mori proposed a scoring function for term candidates.", "labels": [], "entities": []}, {"text": "Their term extraction method however is just one example of employing the relation among single-nouns and compound nouns.", "labels": [], "entities": [{"text": "term extraction", "start_pos": 6, "end_pos": 21, "type": "TASK", "confidence": 0.7083121687173843}]}, {"text": "Note that this relation is essentially based on a noun bigram.", "labels": [], "entities": []}, {"text": "In this paper, we expand the relation based on noun bigrams that might be the components of longer compound nouns.", "labels": [], "entities": []}, {"text": "Then we experimentally evaluate the power of several variations of scoring functions based on the noun bigram relation using the NTCIR1 TMREC test collection.", "labels": [], "entities": [{"text": "NTCIR1 TMREC test collection", "start_pos": 129, "end_pos": 157, "type": "DATASET", "confidence": 0.9292934983968735}]}, {"text": "By this experimental clarification, we could conclude that the single-noun term's power of generating compound noun terms is useful and essential in ATR.", "labels": [], "entities": [{"text": "ATR", "start_pos": 149, "end_pos": 152, "type": "TASK", "confidence": 0.9767343997955322}]}, {"text": "In this paper, section 1 gives the background of ATR methods.", "labels": [], "entities": [{"text": "ATR", "start_pos": 49, "end_pos": 52, "type": "TASK", "confidence": 0.9654479026794434}]}, {"text": "Section 2 describes the proposed method of the noun bigram based scoring function for term extraction.", "labels": [], "entities": [{"text": "term extraction", "start_pos": 86, "end_pos": 101, "type": "TASK", "confidence": 0.7486430406570435}]}, {"text": "Section 3 describes the experimental results and discusses them.", "labels": [], "entities": []}], "datasetContent": [{"text": "In our experiment, we use the NTCIR1 TMREC test collection (.", "labels": [], "entities": [{"text": "NTCIR1 TMREC test collection", "start_pos": 30, "end_pos": 58, "type": "DATASET", "confidence": 0.8490830361843109}]}, {"text": "As an activity of TMREC, they have provided us with a Japanese test collection of a term recognition task., FGM(CN,k) and MC-value(CN) in descending order.", "labels": [], "entities": [{"text": "Japanese test collection", "start_pos": 54, "end_pos": 78, "type": "DATASET", "confidence": 0.770133892695109}, {"text": "term recognition task.", "start_pos": 84, "end_pos": 106, "type": "TASK", "confidence": 0.7697194814682007}, {"text": "FGM", "start_pos": 108, "end_pos": 111, "type": "METRIC", "confidence": 0.9854822158813477}]}, {"text": "As for parameter k of and, we choose k=1 because its performance is the best among various values of kin the range from 0 to 4.", "labels": [], "entities": []}, {"text": "Thus, henceforth, we omit k from GM and FGM, like GM(CN) and FGM(CN).", "labels": [], "entities": [{"text": "FGM", "start_pos": 40, "end_pos": 43, "type": "DATASET", "confidence": 0.8094030618667603}, {"text": "GM", "start_pos": 50, "end_pos": 52, "type": "DATASET", "confidence": 0.772490382194519}, {"text": "FGM", "start_pos": 61, "end_pos": 64, "type": "DATASET", "confidence": 0.7847558856010437}]}, {"text": "We use GM(CN) as the baseline.", "labels": [], "entities": [{"text": "GM(CN)", "start_pos": 7, "end_pos": 13, "type": "DATASET", "confidence": 0.876155361533165}]}, {"text": "In evaluation, we conduct experiments where we pickup the highest ranked term candidate down to the PNth highest ranked term candidate by these three scoring methods, and evaluate the set of selected terms with the number of correct terms, we call it CT, within it.", "labels": [], "entities": [{"text": "CT", "start_pos": 251, "end_pos": 253, "type": "METRIC", "confidence": 0.9298883080482483}]}, {"text": "In the following figures, we only show CT because recall is CT/8834, where 8834 is the number of all correct terms, precision is CT/PN.", "labels": [], "entities": [{"text": "CT", "start_pos": 39, "end_pos": 41, "type": "METRIC", "confidence": 0.994692862033844}, {"text": "recall", "start_pos": 50, "end_pos": 56, "type": "METRIC", "confidence": 0.9991810917854309}, {"text": "precision", "start_pos": 116, "end_pos": 125, "type": "METRIC", "confidence": 0.9995298385620117}]}, {"text": "Another measure NTCIR1 provides us with is the terms which include the correct term as its part.", "labels": [], "entities": [{"text": "NTCIR1", "start_pos": 16, "end_pos": 22, "type": "DATASET", "confidence": 0.8472564220428467}]}, {"text": "We call it \"longer term\" or LT.", "labels": [], "entities": []}, {"text": "They are sometimes valued terms and also indicate in what context the correct terms are used.", "labels": [], "entities": []}, {"text": "Then we also use the number of longer terms in our evaluation.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. CT of each ranking method for PN  larger than 3000", "labels": [], "entities": [{"text": "CT", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9984525442123413}]}, {"text": " Table 2. LT of each ranking method for PN  larger than 3000", "labels": [], "entities": [{"text": "LT", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.988813579082489}]}]}