{"title": [], "abstractContent": [{"text": "We present abroad coverage Japanese grammar written in the HPSG formalism with MRS semantics.", "labels": [], "entities": [{"text": "HPSG", "start_pos": 59, "end_pos": 63, "type": "DATASET", "confidence": 0.933078408241272}]}, {"text": "The grammar is created for use in real world applications, such that robustness and performance issues play an important role.", "labels": [], "entities": []}, {"text": "It is connected to a POS tagging and word segmentation tool.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 21, "end_pos": 32, "type": "TASK", "confidence": 0.7224852740764618}, {"text": "word segmentation", "start_pos": 37, "end_pos": 54, "type": "TASK", "confidence": 0.7174150496721268}]}, {"text": "This grammar is being developed in a multilingual context, requiring MRS structures that are easily comparable across languages.", "labels": [], "entities": [{"text": "MRS", "start_pos": 69, "end_pos": 72, "type": "TASK", "confidence": 0.9115673303604126}]}], "introductionContent": [{"text": "Natural language processing technology has recently reached a point where applications that rely on deep linguistic processing are becoming feasible.", "labels": [], "entities": [{"text": "Natural language processing", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.6476411521434784}]}, {"text": "Such applications (e.g. message extraction systems, machine translation and dialogue understanding systems) require natural language understanding, or at least an approximation thereof.", "labels": [], "entities": [{"text": "message extraction", "start_pos": 24, "end_pos": 42, "type": "TASK", "confidence": 0.8086716532707214}, {"text": "machine translation", "start_pos": 52, "end_pos": 71, "type": "TASK", "confidence": 0.806294858455658}, {"text": "dialogue understanding", "start_pos": 76, "end_pos": 98, "type": "TASK", "confidence": 0.7579748928546906}, {"text": "natural language understanding", "start_pos": 116, "end_pos": 146, "type": "TASK", "confidence": 0.7018391291300455}]}, {"text": "This, in turn, requires rich and highly precise information as the output of a parse.", "labels": [], "entities": []}, {"text": "However, if the technology is to meet the demands of real-world applications, this must not come at the cost of robustness.", "labels": [], "entities": []}, {"text": "Robustness requires not only wide coverage by the grammar (in both syntax and semantics), but also large and extensible lexica as well as interfaces to preprocessing systems for named entity recognition, non-linguistic structures such as addresses, etc.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 178, "end_pos": 202, "type": "TASK", "confidence": 0.6931631366411845}]}, {"text": "Furthermore, applications built on deep NLP technology should be extensible to multiple languages.", "labels": [], "entities": []}, {"text": "This requires flexible yet well-defined output structures that can be adapted to grammars of many different languages.", "labels": [], "entities": []}, {"text": "Finally, for use in real-world applications, NLP systems meeting the above desiderata must also be efficient.", "labels": [], "entities": []}, {"text": "In this paper, we describe the development of abroad coverage grammar for Japanese that is used in an automatic email response application.", "labels": [], "entities": [{"text": "abroad coverage grammar", "start_pos": 46, "end_pos": 69, "type": "TASK", "confidence": 0.7333563466866811}]}, {"text": "The grammar is based on work done in the Verbmobil project (Siegel 2000) on machine translation of spoken dialogues in the domain of travel planning.", "labels": [], "entities": [{"text": "Verbmobil project (Siegel 2000)", "start_pos": 41, "end_pos": 72, "type": "DATASET", "confidence": 0.8841616014639536}, {"text": "machine translation of spoken dialogues", "start_pos": 76, "end_pos": 115, "type": "TASK", "confidence": 0.8242447972297668}]}, {"text": "It has since been greatly extended to accommodate written Japanese and new domains.", "labels": [], "entities": []}, {"text": "The grammar is couched in the theoretical framework of Head-Driven Phrase Structure Grammar (HPSG), with semantic representations in Minimal Recursion Semantics (MRS) (.", "labels": [], "entities": [{"text": "Head-Driven Phrase Structure Grammar (HPSG)", "start_pos": 55, "end_pos": 98, "type": "TASK", "confidence": 0.6928920873573848}]}, {"text": "HPSG is well suited to the task of multilingual development of broad coverage grammars: It is flexible enough (analyses can be shared across languages but also tailored as necessary), and has a rich theoretical literature from which to draw analyzes and inspiration.", "labels": [], "entities": [{"text": "HPSG", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9413044452667236}]}, {"text": "The characteristic type hierarchy of HPSG also facilitates the development of grammars that are easy to extend.", "labels": [], "entities": [{"text": "HPSG", "start_pos": 37, "end_pos": 41, "type": "DATASET", "confidence": 0.8881567120552063}]}, {"text": "MRS is a flat semantic formalism that works well with typed feature structures and is flexible in that it provides structures that are under-specified for scopal information.", "labels": [], "entities": []}, {"text": "These structures give compact representations of ambiguities that are often irrelevant to the task at hand.", "labels": [], "entities": []}, {"text": "HPSG and MRS have the further advantage that there are practical and useful open-source tools for writing, testing, and efficiently processing grammars written in these formalisms.", "labels": [], "entities": [{"text": "HPSG", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9766058921813965}, {"text": "MRS", "start_pos": 9, "end_pos": 12, "type": "TASK", "confidence": 0.5299736857414246}]}, {"text": "The tools we are using in this project include the LKB system (Copestake 2002) for grammar development, [incr tsdb()] for testing the grammar and tracking changes, and PET (Callmeier 2000), a very efficient HPSG parser, for processing.", "labels": [], "entities": [{"text": "LKB system (Copestake 2002)", "start_pos": 51, "end_pos": 78, "type": "DATASET", "confidence": 0.7275932729244232}, {"text": "grammar development", "start_pos": 83, "end_pos": 102, "type": "TASK", "confidence": 0.8581792414188385}]}, {"text": "We also use the ChaSen tokenizer and POS tagger.", "labels": [], "entities": []}, {"text": "While couched within the same general framework (HPSG), our approach differs from that of.", "labels": [], "entities": []}, {"text": "The work described there achieves impressive coverage (83.7% on the EDR corpus of newspaper text) with an underspecified grammar consisting of a small number of lexical entries, lexical types associated with parts of speech, and six underspecified grammar rules.", "labels": [], "entities": [{"text": "coverage", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.9916841983795166}, {"text": "EDR corpus of newspaper text", "start_pos": 68, "end_pos": 96, "type": "DATASET", "confidence": 0.9769395470619202}]}, {"text": "In contrast, our grammar is much larger in terms of the number of lexical entries, the number of grammar rules, and the constraints on both, 1 and takes correspondingly more effort to bring up to that level of coverage.", "labels": [], "entities": []}, {"text": "The higher level of detail allows us to output precise semantic representations as well as to use syntactic, semantic and lexical information to reduce ambiguity and rank parses.", "labels": [], "entities": []}], "datasetContent": [{"text": "The We applied the grammar to unseen data in one of the covered domains, namely the FAQ site of a Japanese bank.", "labels": [], "entities": [{"text": "FAQ site of a Japanese bank", "start_pos": 84, "end_pos": 111, "type": "DATASET", "confidence": 0.9297410647074381}]}, {"text": "The coverage was 61%.", "labels": [], "entities": [{"text": "coverage", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9965819716453552}]}, {"text": "91.2% of the parses output were associated with all well-formed MRSs.", "labels": [], "entities": []}, {"text": "That means that we could get correct MRSs in 55.61% of all sentences.", "labels": [], "entities": [{"text": "correct MRSs", "start_pos": 29, "end_pos": 41, "type": "METRIC", "confidence": 0.7321942150592804}]}], "tableCaptions": []}