{"title": [], "abstractContent": [{"text": "The use of semantic resources is common in modern NLP systems, but methods to extract lexical semantics have only recently begun to perform well enough for practical use.", "labels": [], "entities": []}, {"text": "We evaluate existing and new similarity metrics for thesaurus extraction , and experiment with the trade-off between extraction performance and efficiency.", "labels": [], "entities": [{"text": "thesaurus extraction", "start_pos": 52, "end_pos": 72, "type": "TASK", "confidence": 0.8117329776287079}]}, {"text": "We propose an approximation algorithm, based on canonical attributes and coarse-and fine-grained matching, that reduces the time complexity and execution time of thesaurus extraction with only a marginal performance penalty.", "labels": [], "entities": [{"text": "thesaurus extraction", "start_pos": 162, "end_pos": 182, "type": "TASK", "confidence": 0.6922563761472702}]}], "introductionContent": [{"text": "Thesauri have traditionally been used in information retrieval tasks to expand words in queries with synonymous terms (e.g. Ruge,).", "labels": [], "entities": [{"text": "information retrieval tasks", "start_pos": 41, "end_pos": 68, "type": "TASK", "confidence": 0.7912358542283376}]}, {"text": "Since the development of WordNet) and large electronic thesauri, information from semantic resources is regularly leveraged to solve NLP problems.", "labels": [], "entities": []}, {"text": "These tasks include collocation discovery), smoothing and model estimation) and text classification (.", "labels": [], "entities": [{"text": "collocation discovery", "start_pos": 20, "end_pos": 41, "type": "TASK", "confidence": 0.7207715511322021}, {"text": "smoothing and model estimation", "start_pos": 44, "end_pos": 74, "type": "TASK", "confidence": 0.6766727417707443}, {"text": "text classification", "start_pos": 80, "end_pos": 99, "type": "TASK", "confidence": 0.844257116317749}]}, {"text": "Unfortunately, thesauri are expensive and timeconsuming to create manually, and tend to suffer from problems of bias, inconsistency, and limited coverage.", "labels": [], "entities": [{"text": "coverage", "start_pos": 145, "end_pos": 153, "type": "METRIC", "confidence": 0.9679218530654907}]}, {"text": "In addition, thesaurus compilers cannot keep up with constantly evolving language use and cannot afford to build new thesauri for the many subdomains that NLP techniques are being applied to.", "labels": [], "entities": []}, {"text": "There is a clear need for methods to extract thesauri automatically or tools that assist in the manual creation and updating of these semantic resources.", "labels": [], "entities": []}, {"text": "Much of the existing work on thesaurus extraction and word clustering is based on the observation that related terms will appear in similar contexts.", "labels": [], "entities": [{"text": "thesaurus extraction", "start_pos": 29, "end_pos": 49, "type": "TASK", "confidence": 0.770328938961029}, {"text": "word clustering", "start_pos": 54, "end_pos": 69, "type": "TASK", "confidence": 0.7677525281906128}]}, {"text": "These systems differ primarily in their definition of \"context\" and the way they calculate similarity from the contexts each term appears in.", "labels": [], "entities": []}, {"text": "Most systems extract co-occurrence and syntactic information from the words surrounding the target term, which is then converted into a vector-space representation of the contexts that each target term appears in ().", "labels": [], "entities": []}, {"text": "Other systems take the whole document as the context and consider term co-occurrence at the document level.", "labels": [], "entities": []}, {"text": "Once these contexts have been defined, these systems then use clustering or nearest neighbour methods to find similar terms.", "labels": [], "entities": []}, {"text": "Alternatively, some systems are based on the observation that related terms appear together in particular contexts.", "labels": [], "entities": []}, {"text": "These systems extract related terms directly by recognising linguistic patterns (e.g. X, Y and other Zs) which link synonyms and hyponyms.", "labels": [], "entities": []}, {"text": "Our previous work) has evaluated thesaurus extraction performance and efficiency using several different context models.", "labels": [], "entities": [{"text": "thesaurus extraction", "start_pos": 33, "end_pos": 53, "type": "TASK", "confidence": 0.8784080147743225}]}, {"text": "In this paper, we evaluate some existing similarity metrics and propose and motivate anew metric which outperforms the existing metrics.", "labels": [], "entities": []}, {"text": "We also present an approximation algorithm that bounds the time complexity of pairwise thesaurus extraction.", "labels": [], "entities": [{"text": "pairwise thesaurus extraction", "start_pos": 78, "end_pos": 107, "type": "TASK", "confidence": 0.6097868084907532}]}, {"text": "This results in a significant reduction in runtime with only a marginal performance penalty in our experiments.", "labels": [], "entities": []}], "datasetContent": [{"text": "Early experiments in thesaurus extraction) suffered from the limited size of available corpora, but more recent experiments have used much larger corpora with greater success).", "labels": [], "entities": [{"text": "thesaurus extraction", "start_pos": 21, "end_pos": 41, "type": "TASK", "confidence": 0.8716116547584534}]}, {"text": "For these experiments we ran our relation extractor over the British National Corpus (BNC) consisting of 114 million words in 6.2 million sentences.", "labels": [], "entities": [{"text": "relation extractor", "start_pos": 33, "end_pos": 51, "type": "TASK", "confidence": 0.7523501813411713}, {"text": "British National Corpus (BNC)", "start_pos": 61, "end_pos": 90, "type": "DATASET", "confidence": 0.9732517103354136}]}, {"text": "The POS tagging and chunking took 159 minutes, and the relation extraction took an addi-: Measure functions evaluated tional 7.5 minutes.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 4, "end_pos": 15, "type": "TASK", "confidence": 0.6912365555763245}, {"text": "relation extraction", "start_pos": 55, "end_pos": 74, "type": "TASK", "confidence": 0.8349791169166565}]}, {"text": "The resultant representation contained a total of 28 million relation occurrences over 10 million different relations.", "labels": [], "entities": []}, {"text": "We describe the functions evaluated in these experiments using an extension of the asterisk notation used by, where an asterisk indicates a set ranging overall existing values of that variable.", "labels": [], "entities": []}, {"text": "For example, the set of attributes of the term w is: For convenience, we further extend the notation for weighted attribute vectors.", "labels": [], "entities": []}, {"text": "A subscripted asterisk indicates that the variables are bound together: which is a notational abbreviation of: For weight functions we use similar notation:, pp.", "labels": [], "entities": []}, {"text": "299. When these are used with weighted attributes, if the weight is greater than zero, then it is considered in the set.", "labels": [], "entities": []}, {"text": "Other measures, such as LIN and JACCARD have previously been used for thesaurus extraction.", "labels": [], "entities": [{"text": "LIN", "start_pos": 24, "end_pos": 27, "type": "METRIC", "confidence": 0.9930062294006348}, {"text": "JACCARD", "start_pos": 32, "end_pos": 39, "type": "METRIC", "confidence": 0.8544959425926208}, {"text": "thesaurus extraction", "start_pos": 70, "end_pos": 90, "type": "TASK", "confidence": 0.8255383670330048}]}, {"text": "Finally, we have generalised some set measures using similar reasoning to.", "labels": [], "entities": []}, {"text": "Alternative generalisations are marked with a dagger.", "labels": [], "entities": []}, {"text": "These experiments also cover a range of weight functions as defined in.", "labels": [], "entities": []}, {"text": "The weight functions LIN98A, LIN98B, and GREF94 are taken from existing systems.", "labels": [], "entities": [{"text": "LIN98A", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.6633068323135376}, {"text": "LIN98B", "start_pos": 29, "end_pos": 35, "type": "METRIC", "confidence": 0.6415078043937683}, {"text": "GREF94", "start_pos": 41, "end_pos": 47, "type": "METRIC", "confidence": 0.9938839077949524}]}, {"text": "Our proposed weight functions are motivated by our intuition that highly predictive attributes are strong collocations with their terms.", "labels": [], "entities": []}, {"text": "Thus, we have implemented many of the statistics described in the Collocations chapter of, including the T-Test, \u03c7 2 -Test, Likelihood Ratio, and Mutual Information.", "labels": [], "entities": [{"text": "T-Test", "start_pos": 105, "end_pos": 111, "type": "METRIC", "confidence": 0.8200690746307373}, {"text": "\u03c7 2 -Test", "start_pos": 113, "end_pos": 122, "type": "METRIC", "confidence": 0.7409783899784088}]}, {"text": "Some functions (suffix LOG) have an extra log 2 ( f (w, r, w \ud97b\udf59 ) + 1) factor to promote the influence of higher frequency attributes.", "labels": [], "entities": []}, {"text": "For the purposes of evaluation, we selected 70 single-word noun terms for thesaurus extraction.", "labels": [], "entities": [{"text": "thesaurus extraction", "start_pos": 74, "end_pos": 94, "type": "TASK", "confidence": 0.7624607086181641}]}, {"text": "To avoid sample bias, the words were randomly selected from WordNet such that they covered a range of values for the following word properties:  specificity depth in the WordNet hierarchy; concreteness distribution across WordNet subtrees.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 60, "end_pos": 67, "type": "DATASET", "confidence": 0.9635271430015564}]}, {"text": "lists some example terms with frequency and frequency rank data from the PTB, BNC and REUTERS, as well as the number of senses in WordNet and Macquarie, and their maximum and minimum depth in the WordNet hierarchy.", "labels": [], "entities": [{"text": "PTB", "start_pos": 73, "end_pos": 76, "type": "DATASET", "confidence": 0.9290279746055603}, {"text": "BNC", "start_pos": 78, "end_pos": 81, "type": "DATASET", "confidence": 0.6234629154205322}, {"text": "REUTERS", "start_pos": 86, "end_pos": 93, "type": "METRIC", "confidence": 0.9549154043197632}, {"text": "WordNet", "start_pos": 130, "end_pos": 137, "type": "DATASET", "confidence": 0.9741040468215942}, {"text": "Macquarie", "start_pos": 142, "end_pos": 151, "type": "DATASET", "confidence": 0.7077663540840149}, {"text": "WordNet hierarchy", "start_pos": 196, "end_pos": 213, "type": "DATASET", "confidence": 0.9457069039344788}]}, {"text": "For each term we extracted a thesaurus entry with 200 potential synonyms and their similarity scores.", "labels": [], "entities": []}, {"text": "The simplest method of evaluation is direct comparison of the extracted thesaurus with a manuallycreated gold standard.", "labels": [], "entities": []}, {"text": "However, on small corpora, rare direct matches provide limited information for evaluation, and thesaurus coverage is a problem.", "labels": [], "entities": [{"text": "thesaurus coverage", "start_pos": 95, "end_pos": 113, "type": "TASK", "confidence": 0.47013235092163086}]}, {"text": "Our evaluation uses a combination of three electronic thesauri: the Macquarie (Bernard, 1990), Roget's (Roget, 1911) and Moby thesauri.", "labels": [], "entities": [{"text": "Macquarie (Bernard, 1990)", "start_pos": 68, "end_pos": 93, "type": "DATASET", "confidence": 0.9151856203873953}]}, {"text": "Roget's and Macquarie are topic ordered and the Moby thesaurus is head ordered.", "labels": [], "entities": [{"text": "Roget's and Macquarie", "start_pos": 0, "end_pos": 21, "type": "DATASET", "confidence": 0.894510343670845}, {"text": "Moby thesaurus", "start_pos": 48, "end_pos": 62, "type": "DATASET", "confidence": 0.9705167710781097}]}, {"text": "As the extracted thesauri do not distinguish between senses, we transform Roget's and Macquarie into head ordered format by conflating the sense sets containing each term.", "labels": [], "entities": []}, {"text": "For the 70 terms we create a gold standard from the union of the synonyms from the three thesauri.", "labels": [], "entities": []}, {"text": "With this gold standard in place, it is possible to use precision and recall measures to evaluate the quality of the extracted thesaurus.", "labels": [], "entities": [{"text": "precision", "start_pos": 56, "end_pos": 65, "type": "METRIC", "confidence": 0.9994786381721497}, {"text": "recall", "start_pos": 70, "end_pos": 76, "type": "METRIC", "confidence": 0.9983261227607727}]}, {"text": "To help overcome the problems of direct comparisons we use several measures of system performance: direct matches (DIRECT), inverse rank (INVR), and precision of the top n synonyms (P(n)), for n = 1, 5 and 10.", "labels": [], "entities": [{"text": "direct matches (DIRECT)", "start_pos": 99, "end_pos": 122, "type": "METRIC", "confidence": 0.6181815147399903}, {"text": "inverse rank (INVR)", "start_pos": 124, "end_pos": 143, "type": "METRIC", "confidence": 0.9435734868049621}, {"text": "precision", "start_pos": 149, "end_pos": 158, "type": "METRIC", "confidence": 0.9989221096038818}]}], "tableCaptions": [{"text": " Table 4: Evaluation of measure functions", "labels": [], "entities": []}, {"text": " Table 5: Evaluation of bounded weight functions", "labels": [], "entities": []}, {"text": " Table 6: Evaluation of unbounded weight functions", "labels": [], "entities": []}, {"text": " Table 7: Example performance using techniques described in this paper", "labels": [], "entities": []}]}