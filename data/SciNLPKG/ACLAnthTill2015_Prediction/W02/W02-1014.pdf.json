{"title": [{"text": "Fast LR Parsing Using Rich (Tree Adjoining) Grammars", "labels": [], "entities": []}], "abstractContent": [{"text": "We describe an LR parser of parts-of-speech (and punctuation labels) for Tree Adjoining Grammars (TAGs), that solves table conflicts in a greedy way, with limited amount of backtracking.", "labels": [], "entities": []}, {"text": "We evaluate the parser using the Penn Treebank showing that the method yield very fast parsers with at least reasonable accuracy, confirming the intuition that LR parsing benefits from the use of rich grammars.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 33, "end_pos": 46, "type": "DATASET", "confidence": 0.9952574968338013}, {"text": "accuracy", "start_pos": 120, "end_pos": 128, "type": "METRIC", "confidence": 0.9971952438354492}, {"text": "LR parsing", "start_pos": 160, "end_pos": 170, "type": "TASK", "confidence": 0.6637203991413116}]}], "introductionContent": [{"text": "The LR approach for parsing has long been considered for natural language parsing, but it was not until a more recent past, with the advent of corpusbased techniques made possible by the availability of large treebanks, that parsing results and evaluation started being reported).", "labels": [], "entities": [{"text": "parsing", "start_pos": 20, "end_pos": 27, "type": "TASK", "confidence": 0.9777852296829224}, {"text": "natural language parsing", "start_pos": 57, "end_pos": 81, "type": "TASK", "confidence": 0.6619791090488434}]}, {"text": "The appeal of LR parsing) derives from its high capacity of postponement of structural decisions, therefore allowing for much of the spurious local ambiguity to be automatically discarded.", "labels": [], "entities": [{"text": "LR parsing", "start_pos": 14, "end_pos": 24, "type": "TASK", "confidence": 0.714688628911972}]}, {"text": "But it is still the case that conflicts arise in the LR table for natural language grammars, and in large quantity.", "labels": [], "entities": []}, {"text": "The key question is how one can use the contextual information contained in the parsing stack to cope with the remaining (local) ambiguity manifested as conflicts in the LR tables.", "labels": [], "entities": []}, {"text": "The aforementioned work has concentrated on LR parsing for CFGs which has a clear deficiency in making available sufficient context in the LR states.", "labels": [], "entities": [{"text": "LR parsing", "start_pos": 44, "end_pos": 54, "type": "TASK", "confidence": 0.9178526103496552}]}, {"text": "hints at the relevance of rich grammars on this respect.", "labels": [], "entities": []}, {"text": "They use Tree Adjoining Grammars (TAGs) to defend the possibility of granular incremental computations in LR parsing.", "labels": [], "entities": [{"text": "LR parsing", "start_pos": 106, "end_pos": 116, "type": "TASK", "confidence": 0.835314005613327}]}, {"text": "Incidentally or not, they make use of disambiguation contexts that are only possible in a state of a conceptual LR parser fora rich grammar formalism such as TAG, but not fora CFG.", "labels": [], "entities": [{"text": "CFG", "start_pos": 176, "end_pos": 179, "type": "DATASET", "confidence": 0.9044770002365112}]}, {"text": "Concrete LR-like algorithms for TAGs have only recently been proposed, though their evaluation was restricted to the quality of the parsing table (see also) for earlier attempts).", "labels": [], "entities": [{"text": "TAGs", "start_pos": 32, "end_pos": 36, "type": "TASK", "confidence": 0.9311226606369019}]}, {"text": "In this paper, we revisit the LR parsing technique, applied to a rich grammar formalism: TAG.", "labels": [], "entities": [{"text": "LR parsing", "start_pos": 30, "end_pos": 40, "type": "TASK", "confidence": 0.7540906667709351}, {"text": "TAG", "start_pos": 89, "end_pos": 92, "type": "METRIC", "confidence": 0.7025122046470642}]}, {"text": "Following, conflict resolution is based on contextual information extracted from the so called Instantaneous Description or Configuration: a stack, representing the control memory of the LR parser, and a lookahead sequence, here limited to one symbol.", "labels": [], "entities": [{"text": "conflict resolution", "start_pos": 11, "end_pos": 30, "type": "TASK", "confidence": 0.8681290447711945}]}, {"text": "However, while Briscoe and Carroll invested on massive parallel computation of the possible parsing paths, with pruning and posterior ranking, we ex-periment with a simple greedy depth-first technique with limited amount of backtracking, that resembles to a certain extent the commitment/recovery models from the psycholinguistic research on human language processing, supported by the occurrence of \"garden paths\".", "labels": [], "entities": []}, {"text": "We use the Penn Treebank WSJ corpus, release 2 (, to evaluate the approach.", "labels": [], "entities": [{"text": "Penn Treebank WSJ corpus", "start_pos": 11, "end_pos": 35, "type": "DATASET", "confidence": 0.9774762839078903}]}, {"text": "shows the architecture of our parsing application.", "labels": [], "entities": [{"text": "parsing", "start_pos": 30, "end_pos": 37, "type": "TASK", "confidence": 0.9667245149612427}]}, {"text": "We extract a TAG from apiece of the Penn Treebank, the training corpus, and submit it to an LR parser generator.", "labels": [], "entities": [{"text": "TAG", "start_pos": 13, "end_pos": 16, "type": "METRIC", "confidence": 0.9091711640357971}, {"text": "Penn Treebank, the training corpus", "start_pos": 36, "end_pos": 70, "type": "DATASET", "confidence": 0.7835607528686523}]}, {"text": "The same training corpus is used again to extract statistical information that is used by the driver as follows.", "labels": [], "entities": []}, {"text": "The grammar generation process generates as a subproduct the TAG derivation trees for the annotated sentences compatible with the extracted grammar trees.", "labels": [], "entities": []}, {"text": "This derivation tree is then converted into the sequence of LR parsing actions that would have to be used by the parser to generate exactly that analysis.", "labels": [], "entities": [{"text": "LR parsing", "start_pos": 60, "end_pos": 70, "type": "TASK", "confidence": 0.5872205048799515}]}, {"text": "A parser execution simulation is then performed, guided by the obtained sequence of parsing actions, collecting the statistical information defined in Section 3.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated the approach using the Penn Treebank WSJ Corpus, release 2 (, using Sections 2 to 21 for grammar extraction and training, section 0 for development and 23 for testing.", "labels": [], "entities": [{"text": "Penn Treebank WSJ Corpus", "start_pos": 36, "end_pos": 60, "type": "DATASET", "confidence": 0.9690545946359634}, {"text": "grammar extraction", "start_pos": 102, "end_pos": 120, "type": "TASK", "confidence": 0.8958303332328796}]}, {"text": "Only parts-of-speech were used as input.", "labels": [], "entities": []}, {"text": "A smoothed ranking function is defined as follows: The best was experimentally determined to be 1.", "labels": [], "entities": []}, {"text": "That is: in general, even if there is minimal evidence of the context including the second state, the statistics using this context lead to a better result than using only one state.", "labels": [], "entities": []}, {"text": "For each sentence there is an initial parsing attempt using only as the ranking function with an a maximum of 500 backtracking occurrences.", "labels": [], "entities": []}, {"text": "If it fails, then the sentence is parsed using with a maximum of 3,000 backtracking occurrences.", "labels": [], "entities": []}, {"text": "In table 1 we report the following figures for the development set (Section 0) and test set (Section 23): %failed is the percentage of sentences for which the parser failed (in the two attempts).", "labels": [], "entities": []}, {"text": "Where can be any of the ranking functions, at the state $ , applied to \" . Elsewhere in the paper we have omitted the explicit reference to the state.", "labels": [], "entities": []}, {"text": "However, two new categories were defined: one for time nouns, namely those that appear in the Penn Treebank as heads of constituents marked \"TMP\" (for temporal); another for the word \"that\".", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 94, "end_pos": 107, "type": "DATASET", "confidence": 0.9916514158248901}]}, {"text": "This is similar to)'s and Charniak97's definition of a separate category for auxiliary verbs.", "labels": [], "entities": []}, {"text": "We also included some punctuation symbols among the terminals such as comma, colon and semicolon.", "labels": [], "entities": []}, {"text": "They are extracted into the grammar as if they were regular modifiers.", "labels": [], "entities": []}, {"text": "Their main use is in guiding parsing decisions.", "labels": [], "entities": [{"text": "parsing decisions", "start_pos": 29, "end_pos": 46, "type": "TASK", "confidence": 0.8172783553600311}]}, {"text": "The first two lines report the measures for the parsed sentences as originally generated by the parser.", "labels": [], "entities": []}, {"text": "We purposefully do not report precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 30, "end_pos": 39, "type": "METRIC", "confidence": 0.9802068471908569}]}, {"text": "As we mentioned in the beginning of the paper, the parser assigns to the sentences a much richer hierarchical structure than the Penn Treebank does, which is penalized by the precision measure.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 129, "end_pos": 142, "type": "DATASET", "confidence": 0.9952682256698608}, {"text": "precision measure", "start_pos": 175, "end_pos": 192, "type": "METRIC", "confidence": 0.9836100041866302}]}, {"text": "The reason for such increase in structure is not quite a particular decision of ours, but a consequence of using a sound grammar under the TAG grammatical formalism.", "labels": [], "entities": []}, {"text": "However, having concluded our manifesto, we understand that algorithms that try to keep precision as high as the recall necessarily have losses in recall compared to if they ignored the precision, and therefore in order to have fair comparison with them and to improve the credibility of our results, we flattened the parse trees in a post-processing step, using a simple rule-based technique on top of some frequency measures for individual grammar trees gathered by) and the result is presented in the bottom lines of the table.", "labels": [], "entities": [{"text": "precision", "start_pos": 88, "end_pos": 97, "type": "METRIC", "confidence": 0.9963350296020508}, {"text": "recall", "start_pos": 113, "end_pos": 119, "type": "METRIC", "confidence": 0.9975307583808899}, {"text": "recall", "start_pos": 147, "end_pos": 153, "type": "METRIC", "confidence": 0.9980758428573608}, {"text": "precision", "start_pos": 186, "end_pos": 195, "type": "METRIC", "confidence": 0.9766573905944824}]}, {"text": "By sound we mean a grammar that properly factors recursion in one way or another.", "labels": [], "entities": []}, {"text": "Grammars have been extracted where the right side of a rule reflects exactly each single-level expansion found in the Penn Treebank.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 118, "end_pos": 131, "type": "DATASET", "confidence": 0.9966448545455933}]}, {"text": "We are also aware of a few alternatives in grammatical formalisms that could capture such flatness, e.g., sister adjunction).", "labels": [], "entities": []}, {"text": "The most salient positive result is that the parser is able to parse sentences at a rate of about 20 sentences per second.", "labels": [], "entities": []}, {"text": "Most of the medium-to-high accuracy parsers take at least a few seconds per sentence under the same conditions.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9162163138389587}]}, {"text": "10 This is an enormous speed-up.", "labels": [], "entities": []}, {"text": "As for the accuracy, it is not far from the top performing parser for parts-of-speech that we are aware of, reported by): recall/precision = WI Perhaps the most similar work to ours is.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.999691367149353}, {"text": "recall", "start_pos": 122, "end_pos": 128, "type": "METRIC", "confidence": 0.9988177418708801}, {"text": "precision", "start_pos": 129, "end_pos": 138, "type": "METRIC", "confidence": 0.819669783115387}, {"text": "WI", "start_pos": 141, "end_pos": 143, "type": "METRIC", "confidence": 0.9162116050720215}]}, {"text": "They implemented a standard LR parser for CFGs, and a probabilistic method for conflict resolution similar to ours in that the decisions are conditioned to the LR states but with different methods.", "labels": [], "entities": [{"text": "conflict resolution", "start_pos": 79, "end_pos": 98, "type": "TASK", "confidence": 0.853282243013382}]}, {"text": "In particular, they proceed in a parallel way accumulating probabilities along the paths and using a Viterbi decoder at the end.", "labels": [], "entities": []}, {"text": "Their best published result is of unlabeled bracket recall and precision of 74 % and 73 %, parsing the Susanne corpus.", "labels": [], "entities": [{"text": "recall", "start_pos": 52, "end_pos": 58, "type": "METRIC", "confidence": 0.9783896207809448}, {"text": "precision", "start_pos": 63, "end_pos": 72, "type": "METRIC", "confidence": 0.9994138479232788}, {"text": "Susanne corpus", "start_pos": 103, "end_pos": 117, "type": "DATASET", "confidence": 0.9036934077739716}]}, {"text": "Since the unlabeled bracket measures are much easier than the ones we are reporting, on labeled brackets, our results are clearly superior to theirs.", "labels": [], "entities": []}, {"text": "Also the Susanne corpus is easier than the Penn Treebank.", "labels": [], "entities": [{"text": "Susanne corpus", "start_pos": 9, "end_pos": 23, "type": "DATASET", "confidence": 0.89859738945961}, {"text": "Penn Treebank", "start_pos": 43, "end_pos": 56, "type": "DATASET", "confidence": 0.9962547421455383}]}, {"text": "There are two additional points we want to make.", "labels": [], "entities": []}, {"text": "One is with respect to the ranking function , based on two states.", "labels": [], "entities": []}, {"text": "It is a very rich statistic, but suffers from sparse data problems.", "labels": [], "entities": []}, {"text": "Parsing section 0 with only this statistics (no form of smoothing), with backtracking limit of 3,000 attempts, we could parse only 31 % of the sentences but the non-flattened recall was 88.33 %, which is quite high for using only parts-of-speech.", "labels": [], "entities": [{"text": "backtracking limit", "start_pos": 73, "end_pos": 91, "type": "METRIC", "confidence": 0.9484227895736694}, {"text": "recall", "start_pos": 175, "end_pos": 181, "type": "METRIC", "confidence": 0.9845706820487976}]}, {"text": "The second observation is that when parsing with the smoothed function", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results on the development and test set", "labels": [], "entities": []}]}