{"title": [{"text": "Training a Dialogue Act Tagger For Human-Human and Human-Computer Travel Dialogues", "labels": [], "entities": []}], "abstractContent": [{"text": "While dialogue acts provide a useful schema for characterizing dialogue behaviors in human-computer and human-human dialogues, their utility is limited by the huge effort involved in hand-labelling dialogues with a dialogue act labelling scheme.", "labels": [], "entities": []}, {"text": "In this work, we examine whether it is possible to fully automate the tagging task with the goal of enabling rapid creation of corpora for evaluating spoken dialogue systems and comparing them to human-human dialogues.", "labels": [], "entities": []}, {"text": "We report results for training and testing an automatic classifier to label the information provider's utterances in spoken human-computer and human-human dialogues with DATE (Dialogue Act Tagging for Evaluation) dialogue act tags.", "labels": [], "entities": [{"text": "DATE (Dialogue Act Tagging for Evaluation) dialogue act", "start_pos": 170, "end_pos": 225, "type": "TASK", "confidence": 0.6725752770900726}]}, {"text": "We train and test the DATE tagger on various combinations of the DARPA Communicator June-2000 and October-2001 human-computer corpora, and the CMU human-human corpus in the travel planning domain.", "labels": [], "entities": [{"text": "CMU human-human corpus", "start_pos": 143, "end_pos": 165, "type": "DATASET", "confidence": 0.8867685596148173}]}, {"text": "Our results show that we can achieve high accuracies on the human-computer data, and surprisingly, that the human-computer data improves accuracy on the human-human data, when only small amounts of human-human training data are available.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 42, "end_pos": 52, "type": "METRIC", "confidence": 0.9920552372932434}, {"text": "accuracy", "start_pos": 137, "end_pos": 145, "type": "METRIC", "confidence": 0.9988783001899719}]}], "introductionContent": [{"text": "Recent research on dialogue is based on the assumption that dialogue acts provide a useful way of characterizing dialogue behaviors in both humanhuman (HH) and human-computer (HC) dialogue).", "labels": [], "entities": []}, {"text": "Previous research has used dialogue act tagging for tasks such as improving recognition performance (), identifying important parts of a dialogue (), evaluating and comparing spoken dialogue systems (), as a constraint on nominal expression generation, and for comparing HH to HC dialogues ().", "labels": [], "entities": [{"text": "dialogue act tagging", "start_pos": 27, "end_pos": 47, "type": "TASK", "confidence": 0.6680558621883392}, {"text": "nominal expression generation", "start_pos": 222, "end_pos": 251, "type": "TASK", "confidence": 0.6300716996192932}, {"text": "comparing HH to HC dialogues", "start_pos": 261, "end_pos": 289, "type": "TASK", "confidence": 0.618782103061676}]}, {"text": "Our work builds directly on the previous application of the DATE (Dialogue Act Tagging for Evaluation) tagging scheme to the evaluation and comparison of DARPA Communicator dialogues.", "labels": [], "entities": [{"text": "DATE (Dialogue Act Tagging for Evaluation) tagging", "start_pos": 60, "end_pos": 110, "type": "TASK", "confidence": 0.6855533288584815}, {"text": "comparison of DARPA Communicator dialogues", "start_pos": 140, "end_pos": 182, "type": "TASK", "confidence": 0.6059275090694427}]}, {"text": "The hypothesis underlying the use of dialogue act tagging in spoken dialogue evaluation is that a system's dialogue behaviors have a strong effect on its usability.", "labels": [], "entities": [{"text": "dialogue act tagging in spoken dialogue evaluation", "start_pos": 37, "end_pos": 87, "type": "TASK", "confidence": 0.6613039714949471}]}, {"text": "Because Communicator systems have unique dialogue strategies, and a unique way of representing and achieving particular communicative goals, DATE was developed to consistently label dialogue behaviors across systems so that the potential utility of dialogue act tagging could be explored.", "labels": [], "entities": [{"text": "dialogue act tagging", "start_pos": 249, "end_pos": 269, "type": "TASK", "confidence": 0.6957775553067526}]}, {"text": "In previous work, Walker and Passonneau defined the DATE scheme, and labelled the system utterances in the June 2000 data collection of 663 dialogues from nine participating Communicator systems ().", "labels": [], "entities": [{"text": "June 2000 data collection of 663 dialogues", "start_pos": 107, "end_pos": 149, "type": "DATASET", "confidence": 0.8103981145790645}]}, {"text": "They then derived dialogue act metrics from the DATE tags and showed that when these metrics were used in the PARADISE evaluation framework () that they improved models of user satisfaction by an absolute 5 , and that the new metrics could be used to understand which system's dialogue strategies were most effective.", "labels": [], "entities": [{"text": "DATE tags", "start_pos": 48, "end_pos": 57, "type": "DATASET", "confidence": 0.7534284293651581}, {"text": "PARADISE evaluation framework", "start_pos": 110, "end_pos": 139, "type": "DATASET", "confidence": 0.6601959268252054}]}, {"text": "A major part of evaluation effort using dialogue act tagging, however, is to actually label the dialogues with the dialogue act tags.", "labels": [], "entities": [{"text": "dialogue act tagging", "start_pos": 40, "end_pos": 60, "type": "TASK", "confidence": 0.6639382441838583}]}, {"text": "In previous work (), the DATE labelling of the June-2000 corpus was done using a semi-automatic method that involved collection of a large number of utterance patterns from the different sites participating in the collection and subsequent hand labelling of these patterns.", "labels": [], "entities": [{"text": "DATE labelling", "start_pos": 25, "end_pos": 39, "type": "TASK", "confidence": 0.8482380509376526}, {"text": "June-2000 corpus", "start_pos": 47, "end_pos": 63, "type": "DATASET", "confidence": 0.7819907665252686}]}, {"text": "The 100 coverage and accuracy achieved by the pattern matcher that was implemented for labelling the system utterances was crucially at the cost of maintaining a large labelled pattern database.", "labels": [], "entities": [{"text": "coverage", "start_pos": 8, "end_pos": 16, "type": "METRIC", "confidence": 0.6923691034317017}, {"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9979074001312256}]}, {"text": "1 Furthermore, since the collected patterns were very specific and often exact duplicates of the system utterances in the dialogues, slight variations in the system utterances overtime led to a reduction in coverage of the pattern matcher.", "labels": [], "entities": []}, {"text": "For example, on the October-2001 collection, the tagger was able to label only 60 of the data.", "labels": [], "entities": []}, {"text": "Accounting for the unmatched (and thus unlabeled) utterances turned out again to be a tedious process of manually augmenting the pattern database with new utterance patterns.", "labels": [], "entities": []}, {"text": "We examine whether it is possible to fully automate the tagging task with the goal being to enable rapid creation of new spoken dialogue corpora that can be submitted to evaluation procedures that rely on tagging schemes like DATE.", "labels": [], "entities": [{"text": "tagging task", "start_pos": 56, "end_pos": 68, "type": "TASK", "confidence": 0.9022012948989868}]}, {"text": "Our hypothesis, arising from rough qualitative studies, is that within the same domain there are large similarities not only between different HC dialogues (across system as well as over a period of time) such as the Communicator dialogues, but also between HC dialogues and HH dialogues.", "labels": [], "entities": []}, {"text": "shows a HC dialogue and shows a HH dialogue from the CMUcorpus (U=user, A=human agent, S=computer system) in which the information providers' utterances for information gathering and information presentation across both dialogues are very similar.", "labels": [], "entities": [{"text": "CMUcorpus", "start_pos": 53, "end_pos": 62, "type": "DATASET", "confidence": 0.8080561757087708}, {"text": "information gathering", "start_pos": 157, "end_pos": 178, "type": "TASK", "confidence": 0.7136204838752747}, {"text": "information presentation", "start_pos": 183, "end_pos": 207, "type": "TASK", "confidence": 0.7240476906299591}]}, {"text": "Thus, based on this and other similar observations, we decided to test the extent to which sufficiently general tagging rules can be learned from HC data to label other HC data and HH data in similar domains with high accuracy.", "labels": [], "entities": [{"text": "HH data", "start_pos": 181, "end_pos": 188, "type": "DATASET", "confidence": 0.7937390804290771}, {"text": "accuracy", "start_pos": 218, "end_pos": 226, "type": "METRIC", "confidence": 0.9904869198799133}]}, {"text": "We focus on labelling only the information providers' side of the dialogues.", "labels": [], "entities": []}, {"text": "The primary motivation for this is to be able to characterize system dialogue strategies for cross-site evaluation.", "labels": [], "entities": []}, {"text": "System dialogue behaviors have a large impact on users' ex-S: Leaving from Phoenix, And, what city are you flying to?", "labels": [], "entities": []}, {"text": "U: SEOUL KOREA S: Flying from Phoenix to Seoul, What date would you like to fly?", "labels": [], "entities": [{"text": "SEOUL KOREA", "start_pos": 3, "end_pos": 14, "type": "METRIC", "confidence": 0.7948850393295288}]}, {"text": "U: WEDNESDAY OCTOBER ELEVENTH S: Leaving on the 11th of October, What time woodja like to depart?", "labels": [], "entities": [{"text": "WEDNESDAY OCTOBER ELEVENTH S", "start_pos": 3, "end_pos": 31, "type": "METRIC", "confidence": 0.7621097713708878}]}, {"text": "U: ANY TIME S: From Seoul.", "labels": [], "entities": [{"text": "ANY", "start_pos": 3, "end_pos": 6, "type": "METRIC", "confidence": 0.8667266964912415}, {"text": "TIME", "start_pos": 7, "end_pos": 11, "type": "METRIC", "confidence": 0.7473602890968323}]}, {"text": "Are you returning to Phoenix?", "labels": [], "entities": []}, {"text": "U: YES S: And what date didja wanna return?", "labels": [], "entities": [{"text": "YES", "start_pos": 3, "end_pos": 6, "type": "METRIC", "confidence": 0.9816575050354004}]}, {"text": "U: SUNDAY OCTOBER FIFTEENTH S: I may have misheard You saying the 5th of October.", "labels": [], "entities": [{"text": "SUNDAY OCTOBER FIFTEENTH S:", "start_pos": 3, "end_pos": 30, "type": "METRIC", "confidence": 0.7636337637901306}, {"text": "I", "start_pos": 31, "end_pos": 32, "type": "METRIC", "confidence": 0.5770435333251953}, {"text": "saying the 5th of October", "start_pos": 55, "end_pos": 80, "type": "DATASET", "confidence": 0.785204964876175}]}, {"text": "Please be aware that the return date must be later than the departure date.", "labels": [], "entities": [{"text": "return date", "start_pos": 25, "end_pos": 36, "type": "METRIC", "confidence": 0.9320763051509857}]}, {"text": "Please tell me what date you would like too return.).", "labels": [], "entities": []}, {"text": "In addition, we believe that once the system side of the dialogues is labelled, it will be much easier to label the user side of the dialogues.", "labels": [], "entities": []}, {"text": "This pair of results demonstrates quantitatively that the HC data can be used to improve performance of a tagger for HH data.", "labels": [], "entities": []}, {"text": "However, a larger training corpus of HH data improves performance to 76.6 accuracy, as estimated by 20-fold cross-validation on the CMU-corpus.", "labels": [], "entities": [{"text": "HH data", "start_pos": 37, "end_pos": 44, "type": "DATASET", "confidence": 0.7759212553501129}, {"text": "accuracy", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.9898300766944885}, {"text": "CMU-corpus", "start_pos": 132, "end_pos": 142, "type": "DATASET", "confidence": 0.9765354990959167}]}, {"text": "Section 2 describes the corpora, the DATE dialogue act tagging scheme, methods for tagging the corpora for the experiments, and the features used to train a DATE dialogue act tagger for DATE labelling of the corpora.", "labels": [], "entities": [{"text": "DATE dialogue act tagging", "start_pos": 37, "end_pos": 62, "type": "TASK", "confidence": 0.6493411809206009}, {"text": "DATE dialogue act tagger", "start_pos": 157, "end_pos": 181, "type": "TASK", "confidence": 0.5071469396352768}, {"text": "DATE labelling", "start_pos": 186, "end_pos": 200, "type": "TASK", "confidence": 0.8323411047458649}]}, {"text": "Section 3 presents our results.", "labels": [], "entities": []}, {"text": "We postpone discussion and comparison with related work till Section 4.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Results for Identifying Three-Way DATE Tags in the October-2001 Communicator Corpus, (Dim  = Dimension of Date used for output classification (Maj. Cl. = Majority Class, Acc = Accuracy, SE =  Standard Error)", "labels": [], "entities": [{"text": "Acc = Accuracy", "start_pos": 180, "end_pos": 194, "type": "METRIC", "confidence": 0.8576309084892273}, {"text": "SE =  Standard Error", "start_pos": 196, "end_pos": 216, "type": "METRIC", "confidence": 0.8433358669281006}]}, {"text": " Table 2: Results for Identifying Speech-Act DATE tags in the October-2001 Communicator Corpus, (Dim  = Dimension of Date used for output classification (SPA = Speech Act, Maj. Cl. = Majority Class, Acc =  Accuracy, SE = Standard Error)", "labels": [], "entities": [{"text": "Acc =  Accuracy", "start_pos": 199, "end_pos": 214, "type": "METRIC", "confidence": 0.8596340417861938}, {"text": "SE = Standard Error", "start_pos": 216, "end_pos": 235, "type": "METRIC", "confidence": 0.869457945227623}]}, {"text": " Table 3: Results for Identifying DATE Speech-Act Tags in the CMU Human-Human Corpus (Maj. Cl. =  Majority Class, Acc. = Accuracy, SE = Standard Error)", "labels": [], "entities": [{"text": "Identifying DATE Speech-Act Tags", "start_pos": 22, "end_pos": 54, "type": "TASK", "confidence": 0.7819785475730896}, {"text": "CMU Human-Human Corpus", "start_pos": 62, "end_pos": 84, "type": "DATASET", "confidence": 0.8859902421633402}, {"text": "Acc. =", "start_pos": 114, "end_pos": 120, "type": "METRIC", "confidence": 0.9484091202418009}, {"text": "Accuracy", "start_pos": 121, "end_pos": 129, "type": "METRIC", "confidence": 0.5117596387863159}, {"text": "SE = Standard Error", "start_pos": 131, "end_pos": 150, "type": "METRIC", "confidence": 0.8144987225532532}]}]}