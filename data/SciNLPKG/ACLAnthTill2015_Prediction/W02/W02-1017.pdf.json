{"title": [{"text": "Exploiting Strong Syntactic Heuristics and Co-Training to Learn Semantic Lexicons", "labels": [], "entities": []}], "abstractContent": [{"text": "We present a bootstrapping method that uses strong syntactic heuristics to learn semantic lexicons.", "labels": [], "entities": []}, {"text": "The three sources of information are appositives, compound nouns, and ISA clauses.", "labels": [], "entities": [{"text": "ISA clauses", "start_pos": 70, "end_pos": 81, "type": "TASK", "confidence": 0.8299796283245087}]}, {"text": "We apply heuris-tics to these syntactic structures, embed them in a bootstrapping architecture, and combine them with co-training.", "labels": [], "entities": []}, {"text": "Results on WSJ articles and a pharmaceutical corpus show that this method obtains high precision and finds a large number of terms.", "labels": [], "entities": [{"text": "WSJ articles", "start_pos": 11, "end_pos": 23, "type": "DATASET", "confidence": 0.9193576872348785}, {"text": "precision", "start_pos": 87, "end_pos": 96, "type": "METRIC", "confidence": 0.9983766078948975}]}], "introductionContent": [{"text": "Syntactic structure helps us understand the semantic relationships between words.", "labels": [], "entities": []}, {"text": "Given a text corpus, we can use knowledge about syntactic structures to obtain semantic knowledge.", "labels": [], "entities": []}, {"text": "For example, Hearst learned hyponymy relationships by collecting words in lexico-syntactic expressions, such as \"NP, NP, and other) generated semantically related words by applying statistical measures to syntactic contexts involving appositives, lists, and conjunctions.", "labels": [], "entities": []}, {"text": "Exploiting syntactic structures to learn semantic knowledge holds great promise, but can run into problems.", "labels": [], "entities": []}, {"text": "First, lexico-syntactic expressions that explicitly indicate semantic relationships (e.g., \"NP, NP, and other NPs\") are reliable but a lot of semantic information occurs outside these expressions.", "labels": [], "entities": []}, {"text": "Second, general syntactic structures (e.g., lists and conjunctions) capture a wide range of semantic relationships.", "labels": [], "entities": []}, {"text": "For example, conjunctions frequently join items of the same semantic class (e.g., \"cats and dogs\"), but they can also join different semantic classes (e.g., \"fire and ice\").", "labels": [], "entities": []}, {"text": "Some researchers) have applied statistical methods to identify the strongest semantic associations.", "labels": [], "entities": []}, {"text": "This approach has produced reasonable results, but the accuracy of these techniques still leaves much room for improvement.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 55, "end_pos": 63, "type": "METRIC", "confidence": 0.9982062578201294}]}, {"text": "We adopt an intermediate approach that learns semantic lexicons using strong syntactic heuristics, which are both common and reliable.", "labels": [], "entities": []}, {"text": "We have identified certain types of appositives, compound nouns, and identity (ISA) clauses that indicate specific semantic associations between words.", "labels": [], "entities": []}, {"text": "We embed syntactic heuristics in a bootstrapping process and present empirical results demonstrating that this bootstrapping process produces high-quality semantic lexicons.", "labels": [], "entities": []}, {"text": "In another set of experiments, we incorporate a co-training) mechanism to combine the hypotheses generated by different types of syntactic structures.", "labels": [], "entities": []}, {"text": "Co-training produces a synergistic effect across different heuristics, substantially increasing the coverage of the lexicons while maintaining nearly the same level of accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 168, "end_pos": 176, "type": "METRIC", "confidence": 0.9932271838188171}]}], "datasetContent": [{"text": "We evaluated our system on several semantic categories in two domains.", "labels": [], "entities": []}, {"text": "In one set of experiments, we generated lexicons for PEOPLE and ORGANIZA-TIONS using 2500 Wall Street Journal articles from the Penn Treebank ().", "labels": [], "entities": [{"text": "PEOPLE", "start_pos": 53, "end_pos": 59, "type": "METRIC", "confidence": 0.690697968006134}, {"text": "ORGANIZA-TIONS", "start_pos": 64, "end_pos": 78, "type": "METRIC", "confidence": 0.981295108795166}, {"text": "Wall Street Journal articles from the Penn Treebank", "start_pos": 90, "end_pos": 141, "type": "DATASET", "confidence": 0.9208405613899231}]}, {"text": "In the second set of experiments, we generated lexicons for PEOPLE, ORGANIZATIONS, and PRODUCTS using approximately 1350 press releases from pharmaceutical companies.", "labels": [], "entities": [{"text": "PEOPLE", "start_pos": 60, "end_pos": 66, "type": "METRIC", "confidence": 0.8854694366455078}, {"text": "ORGANIZATIONS", "start_pos": 68, "end_pos": 81, "type": "METRIC", "confidence": 0.988283634185791}]}, {"text": "Our seeding consisted of 5 proper nouns and 5 general nouns for each semantic category.", "labels": [], "entities": []}, {"text": "We used a threshold of 25% for the evidence measure and 5 for the exclusivity ratio.", "labels": [], "entities": [{"text": "exclusivity ratio", "start_pos": 66, "end_pos": 83, "type": "METRIC", "confidence": 0.9849781095981598}]}, {"text": "We ran the bootstrapping process until no new words were learned, which ranged from 6-14 iterations depending on the category and syntactic structure.", "labels": [], "entities": []}, {"text": "shows 10 examples of words learned for each semantic category in each domain.", "labels": [], "entities": []}, {"text": "The people and organization lists illustrate (1) how dramatically the vocabulary can differ across domains, and that the lexicons may include domain-specific word meanings that are not the most common meaning: Examples of Learned Words of a word in general.", "labels": [], "entities": []}, {"text": "For example, the word \"parent\" generally refers to a person, but in a financial domain it nearly always refers to an organization.", "labels": [], "entities": []}, {"text": "The pharmaceutical product category contains many nouns (e.g., drug names) that may not be in a general purpose lexicon such as WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 128, "end_pos": 135, "type": "DATASET", "confidence": 0.9638322591781616}]}, {"text": "show the results of our evaluation.", "labels": [], "entities": []}, {"text": "We ran the bootstrapping algorithm on each type of syntactic structure independently.", "labels": [], "entities": []}, {"text": "The Total column shows the total number of lexicon entries generated by each syntactic structure.", "labels": [], "entities": []}, {"text": "The Correct column contains two accuracy numbers: X/Y.", "labels": [], "entities": [{"text": "Correct", "start_pos": 4, "end_pos": 11, "type": "METRIC", "confidence": 0.9944515824317932}, {"text": "accuracy", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.9987555742263794}]}, {"text": "The first value (X) is the percentage of entries that were judged to be correct, and the second value (Y) is the accuracy after removing entries resulting from parser errors.", "labels": [], "entities": [{"text": "Y)", "start_pos": 103, "end_pos": 105, "type": "METRIC", "confidence": 0.8836809396743774}, {"text": "accuracy", "start_pos": 113, "end_pos": 121, "type": "METRIC", "confidence": 0.999437153339386}]}, {"text": "The PNP lexicons were substantially larger than the GN lexicons, in part because we saved full noun   The Union column tabulates the results obtained from unioning the lexicons produced by the three syntactic structures independently.", "labels": [], "entities": []}, {"text": "Although there is some overlap in their lexicons, we found that many different words are being learned.", "labels": [], "entities": []}, {"text": "This indicates that the three syntactic structures are tapping into different parts of the search space, which suggests that combining them in a co-training model could be beneficial.", "labels": [], "entities": []}, {"text": "Since the number of words contributed by each syntactic structure varied greatly, we evaluated the Union results for the PNP lexicon by randomly sampling 100 words from the unioned lexicons regardless of which structure generated them.", "labels": [], "entities": []}, {"text": "This maintained the same distribution in our evaluation set as exists in the lexicon as a whole.", "labels": [], "entities": []}, {"text": "However, this sampling strategy means that the evaluation results in the Union column are not simply the sum of the results in the preceding columns.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Proper Noun Phrase Lexicon Results", "labels": [], "entities": [{"text": "Proper Noun Phrase Lexicon", "start_pos": 10, "end_pos": 36, "type": "TASK", "confidence": 0.6905225962400436}]}, {"text": " Table 4: Lexicon sizes with and w/o co-training", "labels": [], "entities": []}, {"text": " Table 5: Proper Noun Phrase Lexicon Results after Co-Training", "labels": [], "entities": [{"text": "Proper Noun Phrase Lexicon", "start_pos": 10, "end_pos": 36, "type": "TASK", "confidence": 0.6893342584371567}]}]}