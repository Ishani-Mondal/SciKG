{"title": [{"text": "Convolutional Neural Network for Paraphrase Identification", "labels": [], "entities": [{"text": "Paraphrase Identification", "start_pos": 33, "end_pos": 58, "type": "TASK", "confidence": 0.8895203471183777}]}], "abstractContent": [{"text": "We present anew deep learning architecture Bi-CNN-MI for paraphrase identification (PI).", "labels": [], "entities": [{"text": "paraphrase identification (PI)", "start_pos": 57, "end_pos": 87, "type": "TASK", "confidence": 0.8064504563808441}]}, {"text": "Based on the insight that PI requires comparing two sentences on multiple levels of granu-larity, we learn multigranular sentence representations using convolutional neural network (CNN) and model interaction features at each level.", "labels": [], "entities": [{"text": "PI", "start_pos": 26, "end_pos": 28, "type": "TASK", "confidence": 0.9480477571487427}]}, {"text": "These features are then the input to a logistic classifier for PI.", "labels": [], "entities": []}, {"text": "All parameters of the model (for embeddings, convolution and classification) are directly optimized for PI.", "labels": [], "entities": [{"text": "PI", "start_pos": 104, "end_pos": 106, "type": "TASK", "confidence": 0.8148080110549927}]}, {"text": "To address the lack of training data, we pretrain the network in a novel way using a language mod-eling task.", "labels": [], "entities": []}, {"text": "Results on the MSRP corpus surpass that of previous NN competitors.", "labels": [], "entities": [{"text": "MSRP corpus", "start_pos": 15, "end_pos": 26, "type": "DATASET", "confidence": 0.8472236096858978}]}], "introductionContent": [{"text": "In this paper, we address the problem of paraphrase identification.", "labels": [], "entities": [{"text": "paraphrase identification", "start_pos": 41, "end_pos": 66, "type": "TASK", "confidence": 0.9684747755527496}]}, {"text": "It is usually formalized as a binary classification task: for two sentences (S 1 , S 2 ), determine whether they roughly have the same meaning.", "labels": [], "entities": []}, {"text": "Inspired by recent successes of deep neural networks (NNs) in fields like computer vision (), speech recognition () and natural language processing, we adopt a deep learning approach to paraphrase identification in this paper.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 94, "end_pos": 112, "type": "TASK", "confidence": 0.7608227729797363}, {"text": "natural language processing", "start_pos": 120, "end_pos": 147, "type": "TASK", "confidence": 0.7072918812433878}, {"text": "paraphrase identification", "start_pos": 186, "end_pos": 211, "type": "TASK", "confidence": 0.9684992134571075}]}, {"text": "The key observation that motivates our NN architecture is that the identification of a paraphrase relationship between S 1 and S 2 requires an analysis at multiple levels of granularity.", "labels": [], "entities": []}, {"text": "(A1) \"Detroit manufacturers have raised vehicle prices by ten percent.\"", "labels": [], "entities": []}, {"text": "-(A2) \"GM, Ford and Chrysler have raised car prices by five percent.\"", "labels": [], "entities": [{"text": "GM", "start_pos": 7, "end_pos": 9, "type": "DATASET", "confidence": 0.8474128246307373}]}, {"text": "Example A1/A2 shows that paraphrase identification requires comparison at the word level.", "labels": [], "entities": [{"text": "paraphrase identification", "start_pos": 25, "end_pos": 50, "type": "TASK", "confidence": 0.9533568024635315}]}, {"text": "A1 cannot be a paraphrase of A2 because the numbers \"ten\" and \"five\" are different.", "labels": [], "entities": [{"text": "A1", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.9486296772956848}, {"text": "A2", "start_pos": 29, "end_pos": 31, "type": "METRIC", "confidence": 0.8956907391548157}]}, {"text": "(B1) \"Mary gave birth to a son in 2000.\"", "labels": [], "entities": []}, {"text": "-(B2) \"He is 14 years old and his mother is Mary.\"", "labels": [], "entities": []}, {"text": "PI for B1/B2 can only succeed at the sentence level since B1/B2 express the same meaning using very different means.", "labels": [], "entities": []}, {"text": "Most work on paraphrase identification has focused on only one level of granularity: either on lowlevel features (e.g.,) or on the sentence level (e.g., ARC-I,).", "labels": [], "entities": [{"text": "paraphrase identification", "start_pos": 13, "end_pos": 38, "type": "TASK", "confidence": 0.9544309675693512}]}, {"text": "An exception is the RAE model).", "labels": [], "entities": [{"text": "RAE", "start_pos": 20, "end_pos": 23, "type": "TASK", "confidence": 0.5896613597869873}]}, {"text": "It computes representations on all levels of a parse tree: each node -including nodes corresponding to words, phrases and the entire sentence -is represented as a vector.", "labels": [], "entities": []}, {"text": "RAE then computes an 1 \u00d7 n 2 comparison matrix of the two trees derived from S 1 and S 2 respectively, where n 1 , n 2 are the number of nodes and each comparison is the Euclidean distance between two vectors.", "labels": [], "entities": []}, {"text": "This is then the basis for paraphrase classification.", "labels": [], "entities": [{"text": "paraphrase classification", "start_pos": 27, "end_pos": 52, "type": "TASK", "confidence": 0.9768380522727966}]}, {"text": "RAE) is one of three prior NN architectures that we draw onto design our system.", "labels": [], "entities": []}, {"text": "It embodies the key insight that paraphrase identification involves analysis of information at multiple levels of granularity.", "labels": [], "entities": [{"text": "paraphrase identification", "start_pos": 33, "end_pos": 58, "type": "TASK", "confidence": 0.9778664708137512}]}, {"text": "However, relying on parsing has limitations for noisy text and for other applications in which highly accurate parsers are not available.", "labels": [], "entities": [{"text": "parsing", "start_pos": 20, "end_pos": 27, "type": "TASK", "confidence": 0.965560793876648}]}, {"text": "We extend the basic idea of RAE by exploring stacked convolution layers which on one hand use sliding windows to split sentences into flexible phrases, furthermore, higher layers are able to ex-tract more abstract features of longer-range phrases by combining phrases in lower layers.", "labels": [], "entities": [{"text": "RAE", "start_pos": 28, "end_pos": 31, "type": "TASK", "confidence": 0.9777286648750305}]}, {"text": "A representative way of doing this in deep learning is the work by, the second prior NN architecture that we draw on.", "labels": [], "entities": []}, {"text": "They use convolution to learn representations at multiple levels.", "labels": [], "entities": []}, {"text": "The motivation for convolution is that natural language consists of long sequences in which many short subsequences contribute in a stable way to the structure and meaning of the long sequence regardless of the position of the subsequence within the long sequence.", "labels": [], "entities": []}, {"text": "Thus, it is advantageous to learn convolutional filters that detect a particular feature regardless of position.'s architecture extends this idea in two important ways.", "labels": [], "entities": []}, {"text": "First, k-max pooling extracts the k top values from a sequence of convolutional filter applications and guarantees a fixed length output.", "labels": [], "entities": []}, {"text": "Second, they stack several levels of convolutional filters, thus achieving multigranularity.", "labels": [], "entities": []}, {"text": "We incorporate this architecture as the part that analyzes an individual sentence.", "labels": [], "entities": []}, {"text": "The third prior NN architecture we draw on is ARC proposed by who also attempt to exploit convolution for paraphrase identification.", "labels": [], "entities": [{"text": "paraphrase identification", "start_pos": 106, "end_pos": 131, "type": "TASK", "confidence": 0.8668926954269409}]}, {"text": "Their key insight is that we want to be able to directly optimize the entire system for the task we are addressing, i.e., for paraphrase identification.", "labels": [], "entities": [{"text": "paraphrase identification", "start_pos": 126, "end_pos": 151, "type": "TASK", "confidence": 0.9610287249088287}]}, {"text": "do this by adopting a Siamese architecture: their NN consists of two shared-weight sentence analysis NNs that feed into a binary classifier that is directly trained on labeled sentence pairs.", "labels": [], "entities": []}, {"text": "As we will show below, this is superior to separating the two steps: first learning sentence representations, then training binary classification for fixed, learned sentence representations as, and many others do.", "labels": [], "entities": []}, {"text": "We can now give an overview of our NN architecture ().", "labels": [], "entities": []}, {"text": "We call it Bi-CNN-MI: \"Bi-CNN\" stands for double CNNs used in Siamese framework, \"MI\" for multigranular interaction features.", "labels": [], "entities": []}, {"text": "Bi-CNN-MI has three parts: (i) the sentence analysis network CNN-SM, (ii) the sentence interaction model CNN-IM and (iii) a logistic regression on top of the network that performs paraphrase identification.", "labels": [], "entities": [{"text": "paraphrase identification", "start_pos": 180, "end_pos": 205, "type": "TASK", "confidence": 0.8536018133163452}]}, {"text": "We now describe these three parts in detail.", "labels": [], "entities": []}, {"text": "(i) Following, we design CNN-SM, a convolutional sentence analysis NN that computes representations at four different levels: word, short ngram, long ngram and sentence.", "labels": [], "entities": []}, {"text": "This multigranularity is important because paraphrase identification benefits from analyzing sentences at multiple levels.", "labels": [], "entities": [{"text": "paraphrase identification", "start_pos": 43, "end_pos": 68, "type": "TASK", "confidence": 0.9612956047058105}]}, {"text": "(ii) Following, CNN-IM, the interaction model, computes interaction features ass 1 \u00d7 s 2 matrices, where s i is the number of items of a certain granularity in Si . In contrast to, CNN-IM computes these features at fixed levels and only for comparable units; e.g., we do not compare single words with entire sentences.", "labels": [], "entities": []}, {"text": "(iii) Following, we integrate two copies of CNN-SM into a Siamese architecture that allows to optimize all parameters of the NN for paraphrase identification.", "labels": [], "entities": [{"text": "paraphrase identification", "start_pos": 132, "end_pos": 157, "type": "TASK", "confidence": 0.8845231831073761}]}, {"text": "In our case, these parameters include parameters for word embedding, for convolution filters, and for the classification of paraphrase candidate pairs.", "labels": [], "entities": []}, {"text": "In contrast to, the inputs to the final paraphrase candidate pair classification layer are interaction feature matrices at multiple levels -as opposed to single-level features that do not directly compare an element of S 1 with a potentially corresponding element of S 2 . There is one other problem we have to address to get good performance.", "labels": [], "entities": [{"text": "paraphrase candidate pair classification layer", "start_pos": 40, "end_pos": 86, "type": "TASK", "confidence": 0.6832258522510528}]}, {"text": "Training sets for paraphrase identification are small in comparison with the high complexity of the task.", "labels": [], "entities": [{"text": "paraphrase identification", "start_pos": 18, "end_pos": 43, "type": "TASK", "confidence": 0.9821368157863617}]}, {"text": "Training a complex network like Bi-CNN-MI with a large number of parameters on a small training set is not promising due to sparseness and likely overfitting.", "labels": [], "entities": []}, {"text": "In order to make full use of the training data, we propose anew unsupervised training scheme CNN-LM (CNN Language Model) to pretrain the largest part of the model, the sentence analysis network CNN-SM.", "labels": [], "entities": []}, {"text": "The key innovation is that we use a language modeling task in a setup similar to autoencoding for pretraining (see below for details).", "labels": [], "entities": []}, {"text": "This means that embedding and convolutional parameters can be pretrained on very large corpora since no human labels are required for pretraining.", "labels": [], "entities": []}, {"text": "We will show below that this pretraining is critical forgetting good performance in the paraphrase task.", "labels": [], "entities": [{"text": "pretraining", "start_pos": 29, "end_pos": 40, "type": "METRIC", "confidence": 0.9800174236297607}]}, {"text": "However, the general design principle of this type of unsupervised pretraining should be widely applicable given that next-word prediction training is possible in many NLP applications.", "labels": [], "entities": [{"text": "next-word prediction training", "start_pos": 118, "end_pos": 147, "type": "TASK", "confidence": 0.7606617907683054}]}, {"text": "Thus, this new way of unsupervised pretraining could bean important contribution of the paper independent of paraphrase identification.", "labels": [], "entities": [{"text": "paraphrase identification", "start_pos": 109, "end_pos": 134, "type": "TASK", "confidence": 0.9284163117408752}]}, {"text": "Section 2 discusses related work.", "labels": [], "entities": []}, {"text": "Sections 3 and 4 introduce the sentence model CNN-SM and the sentence interaction model CNN-IM.", "labels": [], "entities": []}, {"text": "Section 5 describes the training regime.", "labels": [], "entities": []}, {"text": "The experiments are presented in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use the Microsoft Research Paraphrase Corpus (MSRP) (.", "labels": [], "entities": [{"text": "Microsoft Research Paraphrase Corpus (MSRP)", "start_pos": 11, "end_pos": 54, "type": "DATASET", "confidence": 0.8594840935298375}]}, {"text": "The training set contains 2753 true and 1323 false paraphrase pairs; the test set contains 1147 and 578 pairs, respectively.", "labels": [], "entities": []}, {"text": "For each triple (label, S 1 , S 2 ) in the training set we also add (label, S 2 , S 1 ) to make best use of the training data; these additions are nonredundant because the interaction feature matrices (Section 4.1) are asymmetric.", "labels": [], "entities": []}, {"text": "Systems are evaluated by accuracy and F 1 .", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9998488426208496}, {"text": "F 1", "start_pos": 38, "end_pos": 41, "type": "METRIC", "confidence": 0.992822676897049}]}], "tableCaptions": [{"text": " Table 1: Performance of different systems on MSRP", "labels": [], "entities": [{"text": "MSRP", "start_pos": 46, "end_pos": 50, "type": "TASK", "confidence": 0.6117386817932129}]}, {"text": " Table 2: Analysis of impact of the four feature classes.  Line 1: majority baseline. Line 10: Bi-CNN-MI result  from Table 1. Lines 2-5: Bi-CNN-MI when only one  feature class is used. Line 6-9: ablation experiment: on  each line one feature class is removed.", "labels": [], "entities": [{"text": "Bi-CNN-MI", "start_pos": 95, "end_pos": 104, "type": "METRIC", "confidence": 0.9258847832679749}]}]}