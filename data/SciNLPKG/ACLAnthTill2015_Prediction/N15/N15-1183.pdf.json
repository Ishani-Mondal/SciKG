{"title": [{"text": "Hierarchic syntax improves reading time prediction", "labels": [], "entities": []}], "abstractContent": [{"text": "Previous work has debated whether humans make use of hierarchic syntax when processing language (Frank and Bod, 2011; Fos-sum and Levy, 2012).", "labels": [], "entities": []}, {"text": "This paper uses an eye-tracking corpus to demonstrate that hier-archic syntax significantly improves reading time prediction over a strong n-gram baseline.", "labels": [], "entities": []}, {"text": "This study shows that an interpolated 5-gram baseline can be made stronger by combining n-gram statistics over entire eye-tracking regions rather than simply using the last n-gram in each region, but basic hierarchic syntactic measures are still able to achieve significant improvements over this improved baseline.", "labels": [], "entities": []}], "introductionContent": [{"text": "In NLP, a concern exists that models of hierarchic syntax maybe increasingly used exclusively to compensate for n-gram sparsity ().", "labels": [], "entities": []}, {"text": "In the context of psycholinguistic modeling, find that hierarchic measures of syntactic processing are not as good at predicting reading times as sequential part-of-speech-based models of processing.", "labels": [], "entities": []}, {"text": "Fossum and Levy (2012) followup on this finding and show that, when better n-gram information is present in the models, measures of hierarchic syntactic processing cost are as good at predicting reading times as the sequential models presented by Frank and Bod.", "labels": [], "entities": []}, {"text": "The present study builds on this finding by showing that cumulative n-gram probabilities significantly improve an n-gram baseline to better capture sequential frequency statistics.", "labels": [], "entities": []}, {"text": "Further, this study shows that measures of hierarchic structural frequencies (as captured by PCFG surprisal) significantly improve reading time predictions over that improved sequential baseline.", "labels": [], "entities": []}, {"text": "First, this work defines a stronger n-gram baseline than that used in previous studies by replacing a bigram baseline computed from 101 million words with an interpolated 5-gram baseline computed over 2.96 billion words.", "labels": [], "entities": []}, {"text": "Second, while previous work has used n-grams from the end of each eye-movement region to model reading times in that region, this paper finds that such models can be significantly improved by combining n-gram statistics over the entire region (Section 3).", "labels": [], "entities": []}, {"text": "Even when this improved baseline is combined with a standard n-gram baseline, this paper demonstrates that PCFG surprisal is a significant predictor of reading times.", "labels": [], "entities": []}, {"text": "This paper also applies region accumulation to total surprisal and finds that it is not significantly better than non-accumulated total surprisal.", "labels": [], "entities": []}, {"text": "In fact, cumulative surprisal is shown not to be a significant predictor of reading times at all when a cumulative ngram factor is included in the baseline.", "labels": [], "entities": []}, {"text": "Finally, this paper compares two different models of hierarchic syntax: the Penn Treebank (PTB) representation) and the psycholinguisticallymotivated Generalized Categorial Grammar (GCG).", "labels": [], "entities": [{"text": "Penn Treebank (PTB) representation", "start_pos": 76, "end_pos": 110, "type": "DATASET", "confidence": 0.9676553805669149}]}, {"text": "Each model of syntax is shown to provide orthogonal improvements to reading time predictions Bigram P(w 4 |w 3 ) P(w 6 |w 5 ) Cumu-Bigram P(w 4 |w 3 ) P(w 6 |w 5 )\u00b7P(w 5 |w 4 ): Bigram factors and their predictions of reading times in example eye-tracking regions.", "labels": [], "entities": []}, {"text": "w i represents word i.", "labels": [], "entities": []}, {"text": "R wj wi represents the region from w i tow j (inclusive).", "labels": [], "entities": []}], "datasetContent": [{"text": "Following van, the GCG calculation of PCFG surprisal comes from a GCGreannotated version of the Penn Treebank whose grammar rules have undergone 3 iterations of the split-merge algorithm ().", "labels": [], "entities": [{"text": "PCFG", "start_pos": 38, "end_pos": 42, "type": "DATASET", "confidence": 0.7620719075202942}, {"text": "Penn Treebank", "start_pos": 96, "end_pos": 109, "type": "DATASET", "confidence": 0.9152505397796631}]}, {"text": "A k-best beam with a width of 5000 is used in order to be comparable to the PTB calculation.", "labels": [], "entities": [{"text": "PTB", "start_pos": 76, "end_pos": 79, "type": "METRIC", "confidence": 0.6088747978210449}]}, {"text": "Significance testing is done as in the preceding evaluations: a baseline model is fit to reading times, each PCFG surprisal factor is added independently to the baseline, and both PCFG surprisal factors are added concurrently to the baseline.", "labels": [], "entities": []}, {"text": "Each model is compared to the next simpler models using likelihood ratio tests.", "labels": [], "entities": [{"text": "likelihood ratio", "start_pos": 56, "end_pos": 72, "type": "METRIC", "confidence": 0.9279956221580505}]}, {"text": "The results show that GCG PCFG surprisal is a significant predictor of reading times even in the presence of the stronger n-gram baseline.", "labels": [], "entities": [{"text": "GCG PCFG surprisal", "start_pos": 22, "end_pos": 40, "type": "METRIC", "confidence": 0.6384198764959971}]}, {"text": "Moreover, both PTB and GCG PCFG surprisal significantly improve reading time predictions even when the other PCFG surprisal measure is also included.", "labels": [], "entities": [{"text": "PTB", "start_pos": 15, "end_pos": 18, "type": "METRIC", "confidence": 0.6400477290153503}]}, {"text": "This suggests that each is contributing something the other is not.", "labels": [], "entities": []}, {"text": "Since the GCG grammar is derived from an automatically reannotated version of the Penn Treebank, there maybe errors in the GCG annotation which cause errors in the estimates of underlying GCG structure.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 82, "end_pos": 95, "type": "DATASET", "confidence": 0.9952346980571747}]}, {"text": "Since the PTB grammar is manually annotated by experts, the PTB grammar maybe receiving credit for correct structural prediction in cases where GCG's estimates are incorrect.", "labels": [], "entities": [{"text": "PTB grammar", "start_pos": 10, "end_pos": 21, "type": "DATASET", "confidence": 0.8837969899177551}, {"text": "PTB grammar", "start_pos": 60, "end_pos": 71, "type": "DATASET", "confidence": 0.9155551791191101}]}, {"text": "However, it seems likely that GCG maybe providing a better fit in cases of long-distance dependencies because such relations are omitted from the PTB grammar.", "labels": [], "entities": [{"text": "PTB grammar", "start_pos": 146, "end_pos": 157, "type": "DATASET", "confidence": 0.8828188180923462}]}, {"text": "A follow-up evaluation (not shown here) using the experimental design from Section 4 but using GCG PCFG surprisal rather than PTB PCFG surprisal revealed that cumulative PCFG surprisal is still not a significant predictor when calculated using GCG.", "labels": [], "entities": [{"text": "GCG", "start_pos": 244, "end_pos": 247, "type": "DATASET", "confidence": 0.9587031006813049}]}, {"text": "since a strength of GCG is in enabling non-local decisions on a local basis (by propagating non-local decisions into the category labels), so any non-local advantage cumulative PCFG surprisal might confer is already compressed into the GCG categories.", "labels": [], "entities": []}, {"text": "The results of this evaluation suggest that reading times are mostly affected by local hierarchic structure, but the fact that GCG PCFG surprisal is able to provide a significant fit even in the presence of the PTB PCFG surprisal predictor suggests that some non-local information affects reading times.", "labels": [], "entities": []}, {"text": "In particular, while this evaluation showed that accumulated syntactic context is not generally a good predictor of reading times, some or all of the non-local information contained in the GCG categories is used by readers and so influences reading time durations over the local structural information reflected in the PTB PCFG surprisal measure.", "labels": [], "entities": [{"text": "PTB PCFG surprisal measure", "start_pos": 319, "end_pos": 345, "type": "DATASET", "confidence": 0.8990271389484406}]}], "tableCaptions": [{"text": " Table 3: Goodness of fit of N-gram models to reading times.", "labels": [], "entities": []}, {"text": " Table 5: Goodness of fit of hierarchic syntax models to reading times. Significance testing was done between  each model and the models in the section above it. Significance for Base+Both applies only to improvement  over the CumuSurp model.  *  p < .01", "labels": [], "entities": []}, {"text": " Table 6: Goodness of fit of models with differing syntactic calculations to reading times. Significance testing  was done between each model and the models in the section above it. Base+Both first pass significance  applies to improvement over PTB (p < .05) and to improvement over GCG (p < .01), Base+Both go-past  significance applies to improvement over each independent model.  \u2020 p < .05  *  p < .01", "labels": [], "entities": [{"text": "first pass significance", "start_pos": 192, "end_pos": 215, "type": "METRIC", "confidence": 0.7688013315200806}, {"text": "PTB", "start_pos": 245, "end_pos": 248, "type": "METRIC", "confidence": 0.606576144695282}, {"text": "GCG", "start_pos": 283, "end_pos": 286, "type": "DATASET", "confidence": 0.5752692818641663}]}, {"text": " Table 7: Fixed effect predictor coefficients for Base+PTB+GCG model.", "labels": [], "entities": [{"text": "Fixed effect predictor", "start_pos": 10, "end_pos": 32, "type": "TASK", "confidence": 0.6266334255536398}, {"text": "Base+PTB+GCG", "start_pos": 50, "end_pos": 62, "type": "METRIC", "confidence": 0.7324171662330627}]}]}