{"title": [{"text": "A Hybrid Generative/Discriminative Approach To Citation Prediction", "labels": [], "entities": [{"text": "Citation Prediction", "start_pos": 47, "end_pos": 66, "type": "TASK", "confidence": 0.9768693447113037}]}], "abstractContent": [{"text": "Text documents of varying nature (e.g., summary documents written by analysts or published , scientific papers) often cite others as a means of providing evidence to support a claim, attributing credit, or referring the reader to related work.", "labels": [], "entities": [{"text": "Text documents of varying nature (e.g., summary documents written by analysts or published , scientific papers) often cite others as a means of providing evidence to support a claim, attributing credit, or referring the reader to related work", "start_pos": 0, "end_pos": 242, "type": "Description", "confidence": 0.8024305874525115}]}, {"text": "We address the problem of predicting a document's cited sources by introducing a novel, discriminative approach which combines a content-based generative model (LDA) with author-based features.", "labels": [], "entities": [{"text": "predicting a document's cited sources", "start_pos": 26, "end_pos": 63, "type": "TASK", "confidence": 0.8555655082066854}]}, {"text": "Further , our classifier is able to learn the importance and quality of each topic within our corpus-which can be useful beyond this task-and preliminary results suggest its metric is competitive with other standard met-rics (Topic Coherence).", "labels": [], "entities": []}, {"text": "Our flagship system, Logit-Expanded, provides state-of-the-art performance on the largest corpus ever used for this task.", "labels": [], "entities": []}], "introductionContent": [{"text": "The amount of digital documents (both online and offline) continues to grow greatly for several reasons, including the eagerness of users to generate content (e.g., social media, Web 2.0) and the decrease in digital storage costs.", "labels": [], "entities": []}, {"text": "Many different types of documents link to or cite other documents (e.g., websites, analyst summary reports, academic research papers), and they do so for various reasons: to provide evidence, attribute credit, refer the reader to related work, etc.", "labels": [], "entities": []}, {"text": "Given the plethora of documents, it can be highly useful to have a system which can automatically predict relevant citations, for this could (1) aid authors in citing relevant, useful sources which they may otherwise not know about; and (2) aid readers in finding useful documents which otherwise might not have been discovered, due to the documents' being unpopular or poorly cited by many authors.", "labels": [], "entities": []}, {"text": "Specifically, we are interested in citation prediction -that is, we aim to predict which sources each report document cites.", "labels": [], "entities": [{"text": "citation prediction", "start_pos": 35, "end_pos": 54, "type": "TASK", "confidence": 0.9135681390762329}]}, {"text": "We define a report as any document that cites another document in our corpus, and a source as a document that is cited by at least one report.", "labels": [], "entities": []}, {"text": "Naturally, many documents within a corpus can be both a report and a source.", "labels": [], "entities": []}, {"text": "Note, we occasionally refer to linking a report and source, which is synonymous with saying the report cites the source.", "labels": [], "entities": []}, {"text": "Citation prediction can be viewed as a special case of the more general, heavily-researched area of link prediction.", "labels": [], "entities": [{"text": "Citation prediction", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.874981552362442}, {"text": "link prediction", "start_pos": 100, "end_pos": 115, "type": "TASK", "confidence": 0.8198662102222443}]}, {"text": "In fact, past research mentioned in Section 2 refers to this exact task as both citation prediction and link prediction.", "labels": [], "entities": [{"text": "citation prediction", "start_pos": 80, "end_pos": 99, "type": "TASK", "confidence": 0.9135715961456299}, {"text": "link prediction", "start_pos": 104, "end_pos": 119, "type": "TASK", "confidence": 0.7923667430877686}]}, {"text": "However, link prediction is a commonly used phrase which maybe used to describe other problems not concerning documents and citation prediction.", "labels": [], "entities": [{"text": "link prediction", "start_pos": 9, "end_pos": 24, "type": "TASK", "confidence": 0.8000562191009521}, {"text": "citation prediction", "start_pos": 124, "end_pos": 143, "type": "TASK", "confidence": 0.779527872800827}]}, {"text": "In these general cases, a link maybe relatively abstract and represent any particular relationship between other objects (such as users' interests or interactions).", "labels": [], "entities": []}, {"text": "Traditionally, popular techniques for link prediction and recommendation systems have included featurebased classification, matrix factorization, and other collaborative filtering approaches -all of which typically use meta-data features (e.g., names and interests) as opposed to modelling complete content such as full text documents ().", "labels": [], "entities": [{"text": "link prediction", "start_pos": 38, "end_pos": 53, "type": "TASK", "confidence": 0.7543520033359528}, {"text": "featurebased classification", "start_pos": 95, "end_pos": 122, "type": "TASK", "confidence": 0.7229604423046112}]}, {"text": "However, starting with seminal work on ci-tation prediction (PHITS), along work (LinkLDA), content-based modelling approaches have extensively used generative models -while largely ignoring meta-data features which collaborative filtering approaches often usethus creating somewhat of a dichotomy between two approaches towards the same problem.", "labels": [], "entities": [{"text": "ci-tation prediction (PHITS)", "start_pos": 39, "end_pos": 67, "type": "TASK", "confidence": 0.8219385147094727}]}, {"text": "We demonstrate that combining (1) a simple, yet effective, generative approach to modelling content with (2) author-based features into a discriminative classifier can improve performance.", "labels": [], "entities": []}, {"text": "We show state-of-theart performance on the largest corpus for this task.", "labels": [], "entities": []}, {"text": "Finally, our classifier learns the importance of each topic within our corpus, which can be useful beyond this task.", "labels": [], "entities": []}, {"text": "In the next section, we describe related research.", "labels": [], "entities": []}, {"text": "In Section 3 we describe our models and motivations for them.", "labels": [], "entities": []}, {"text": "In Section 4 we detail our experiments, including data and results, and compare our work to the current state-of-the-art system.", "labels": [], "entities": []}, {"text": "We finally conclude in Section 5.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: A randomly chosen report and its predicted  sources, per LDA-Bayes, illustrating that a report and  predicted source may be contextually similar but that  their titles may have few words in common.", "labels": [], "entities": []}, {"text": " Table 3: Report-to-Source Citation Prediction Corpora  Cora CiteSeer WebKB  ACL", "labels": [], "entities": [{"text": "Report-to-Source Citation Prediction Corpora  Cora CiteSeer WebKB  ACL", "start_pos": 10, "end_pos": 80, "type": "DATASET", "confidence": 0.6854471527040005}]}, {"text": " Table 4: Performance of each system, averaged across all  reports while returning the top 50 predicted sources for  each. 125 topics were used for every system.", "labels": [], "entities": []}, {"text": " Table 5: Analysis of each feature used in Logit- Expanded. Results based on the first 50 sources returned,  averaged over all reports. Our Starting Point* system  listed within the \"Addage\" columns used LDA-Bayes as  the only feature. Our Starting Point* system within the  \"Removal\" columns used every feature.", "labels": [], "entities": [{"text": "Logit- Expanded", "start_pos": 43, "end_pos": 58, "type": "TASK", "confidence": 0.7009118596712748}]}, {"text": " Table 6: The highest quality topics (out of 125), sorted  according to Logit-Expanded's estimate. Topics are also  ranked according to Pointwise Mutual Information (PMI)  and Topic Coherence (TC).", "labels": [], "entities": []}, {"text": " Table 7: The lowest quality topics (out of 125), sorted  by Logit-Expanded's estimate. Topics are also ranked  according to Pointwise Mutual Information (PMI) and  Topic Coherence (TC).", "labels": [], "entities": []}]}