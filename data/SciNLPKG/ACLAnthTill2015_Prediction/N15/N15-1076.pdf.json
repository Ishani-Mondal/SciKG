{"title": [{"text": "Is Your Anchor Going Up or Down? Fast and Accurate Supervised Topic Models", "labels": [], "entities": [{"text": "Accurate", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.9807039499282837}]}], "abstractContent": [{"text": "Topic models provide insights into document collections, and their supervised extensions also capture associated document-level meta-data such as sentiment.", "labels": [], "entities": []}, {"text": "However, inferring such models from data is often slow and cannot scale to big data.", "labels": [], "entities": []}, {"text": "We build upon the \"anchor\" method for learning topic models to capture the relationship between metadata and latent topics by extending the vector-space representation of word-cooccurrence to include metadata-specific dimensions.", "labels": [], "entities": []}, {"text": "These additional dimensions reveal new anchor words that reflect specific combinations of metadata and topic.", "labels": [], "entities": []}, {"text": "We show that these new latent representations predict sentiment as accurately as supervised topic models, and we find these representations more quickly without sacrificing interpretability.", "labels": [], "entities": []}, {"text": "Topic models were introduced in an unsupervised setting (Blei et al., 2003), aiding in the discovery of topical structure in text: large corpora can be distilled into human-interpretable themes that facilitate quick understanding.", "labels": [], "entities": []}, {"text": "In addition to illuminating document collections for humans, topic models have increasingly been used for automatic downstream applications such as sentiment analysis (Titov and McDonald, 2008; Paul and Girju, 2010; Nguyen et al., 2013).", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 148, "end_pos": 166, "type": "TASK", "confidence": 0.9599975943565369}]}, {"text": "Unfortunately, the structure discovered by unsuper-vised topic models does not necessarily constitute the best set of features for tasks such as sentiment analysis.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 145, "end_pos": 163, "type": "TASK", "confidence": 0.95635125041008}]}, {"text": "Consider a topic model trained on Amazon product reviews.", "labels": [], "entities": []}, {"text": "A topic model might discover a topic about vampire romance.", "labels": [], "entities": []}, {"text": "However, we often want to go deeper, discovering facets of a topic that reflect topic-specific sentiment, e.g., \"buffy\" and \"spike\" for positive sentiment vs. \"twilight\" and \"cullen\" for negative sentiment.", "labels": [], "entities": []}, {"text": "Techniques for discovering such associations, called supervised topic models (Sec-tion 2), both produce interpretable topics and predict metadata values.", "labels": [], "entities": []}, {"text": "While unsupervised topic models now have scalable inference strategies (Hoffman et al., 2013; Zhai et al., 2012), supervised topic model inference has not received as much attention and often scales poorly.", "labels": [], "entities": []}, {"text": "The anchor algorithm is a fast, scalable unsuper-vised approach for finding \"anchor words\"-precise words with unique co-occurrence patterns that can define the topics of a collection of documents.", "labels": [], "entities": []}, {"text": "We augment the anchor algorithm to find supervised sentiment-specific anchor words (Section 3).", "labels": [], "entities": []}, {"text": "Our algorithm is faster and just as effective as traditional schemes for supervised topic modeling (Section 4).", "labels": [], "entities": [{"text": "supervised topic modeling", "start_pos": 73, "end_pos": 98, "type": "TASK", "confidence": 0.6767897307872772}]}, {"text": "1 Anchors: Speedy Unsupervised Models The anchor algorithm (Arora et al., 2013) begins with a V \u00d7 V matrix \u00af Q of word co-occurrences, where V is the size of the vocabulary.", "labels": [], "entities": []}, {"text": "Each word type defines a vector \u00af Q i,\u00b7 of length V so that \u00af Q i,j encodes the conditional probability of seeing word j given that word i has already been seen.", "labels": [], "entities": []}, {"text": "Spectral methods (Anand-kumar et al., 2012) and the anchor algorithm are fast alternatives to traditional topic model inference schemes because they can discover topics via these summary statistics (quadratic in the number of types) rather than examining the whole dataset (proportional to the much larger number of tokens).", "labels": [], "entities": []}, {"text": "The anchor algorithm takes its name from the idea 746", "labels": [], "entities": [{"text": "idea 746", "start_pos": 45, "end_pos": 53, "type": "DATASET", "confidence": 0.9257568717002869}]}], "introductionContent": [], "datasetContent": [{"text": "We use three common sentiment datasets for evaluation: AMAZON product reviews (Jindal and Liu, 2008), YELP restaurant reviews (Jo and Oh, 2011), and TRIPADVISOR hotel reviews (.", "labels": [], "entities": [{"text": "YELP", "start_pos": 102, "end_pos": 106, "type": "METRIC", "confidence": 0.979159951210022}]}, {"text": "For each dataset, we preprocess by tokenizing and removing all non-alphanumeric words and stopwords.", "labels": [], "entities": []}, {"text": "As very short reviews are often inscrutable and lack cues to connect to the sentiment, we only consider documents with at least thirty words.", "labels": [], "entities": []}, {"text": "We also reduce the vocabulary size by keeping only words that appear in a sufficient number of documents: 50 for AMAZON and YELP datasets, and 150 for TRIPADVI-SOR).", "labels": [], "entities": [{"text": "AMAZON and YELP datasets", "start_pos": 113, "end_pos": 137, "type": "DATASET", "confidence": 0.6861707270145416}]}], "tableCaptions": [{"text": " Table 1: Statistics for the datasets employed in the experiments.", "labels": [], "entities": []}, {"text": " Table 2: Runtime statistics (in seconds) for the AMAZON and TRIPADVISOR datasets. Blank cells indicate a timing  which does not apply to a particular model. SUP ANCHOR is significantly faster than conventional methods.", "labels": [], "entities": [{"text": "AMAZON", "start_pos": 50, "end_pos": 56, "type": "DATASET", "confidence": 0.6631194949150085}, {"text": "TRIPADVISOR datasets", "start_pos": 61, "end_pos": 81, "type": "DATASET", "confidence": 0.7116270959377289}, {"text": "SUP ANCHOR", "start_pos": 158, "end_pos": 168, "type": "TASK", "confidence": 0.7530178129673004}]}]}