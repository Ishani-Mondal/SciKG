{"title": [{"text": "Random Walks and Neural Network Language Models on Knowledge Bases", "labels": [], "entities": []}], "abstractContent": [{"text": "Random walks overlarge knowledge bases like WordNet have been successfully used in word similarity, relatedness and disambigua-tion tasks.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 44, "end_pos": 51, "type": "DATASET", "confidence": 0.9626298546791077}, {"text": "word similarity", "start_pos": 83, "end_pos": 98, "type": "TASK", "confidence": 0.7740631997585297}]}, {"text": "Unfortunately, those algorithms are relatively slow for large repositories, with significant memory footprints.", "labels": [], "entities": []}, {"text": "In this paper we present a novel algorithm which encodes the structure of a knowledge base in a continuous vector space, combining random walks and neural net language models in order to produce novel word representations.", "labels": [], "entities": []}, {"text": "Evaluation in word relatedness and similarity datasets yields equal or better results than those of a random walk algorithm, using a dense representation (300 dimensions instead of 117K).", "labels": [], "entities": []}, {"text": "Furthermore, the word representations are complementary to those of the random walk algorithm and to corpus-based continuous representations, improving the state-of-the-art in the similarity dataset.", "labels": [], "entities": []}, {"text": "Our technique opens up exciting opportunities to combine distributional and knowledge-based word representations.", "labels": [], "entities": []}], "introductionContent": [{"text": "Graph-based techniques over Knowledge Bases (KB) like WordNet) have been widely used in NLP tasks, including word sense disambiguation (), semantic similarity and semantic relatedness between terms ().", "labels": [], "entities": [{"text": "WordNet", "start_pos": 54, "end_pos": 61, "type": "DATASET", "confidence": 0.9497956037521362}, {"text": "word sense disambiguation", "start_pos": 109, "end_pos": 134, "type": "TASK", "confidence": 0.7044437726338705}]}, {"text": "For instance, apply a random walk algorithm based on Personalized PageRank to WordNet, pre- senting the best results to date among WordNetbased methods for the well-known WS353 wordsimilarity dataset ().", "labels": [], "entities": [{"text": "WordNet", "start_pos": 78, "end_pos": 85, "type": "DATASET", "confidence": 0.9686117768287659}, {"text": "WS353 wordsimilarity dataset", "start_pos": 171, "end_pos": 199, "type": "DATASET", "confidence": 0.8861850698788961}]}, {"text": "For each target word, the method performs a personalized random walk on the WordNet graph.", "labels": [], "entities": [{"text": "WordNet graph", "start_pos": 76, "end_pos": 89, "type": "DATASET", "confidence": 0.9734129011631012}]}, {"text": "At convergence, the target word is represented as a vector in a multi-dimensional conceptual space, with one dimension for each concept in the KB.", "labels": [], "entities": []}, {"text": "The good results of the algorithm contrast with the large dimensionality of the vectors that it needs to produce, 117K dimensions (one per synset) for WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 151, "end_pos": 158, "type": "DATASET", "confidence": 0.9631242752075195}]}, {"text": "In recent years a wide variety of Neural Network Language Models (NNLM) have been successfully employed in several tasks, including word similarity.", "labels": [], "entities": [{"text": "word similarity", "start_pos": 132, "end_pos": 147, "type": "TASK", "confidence": 0.7880776226520538}]}, {"text": "NNLM extract meaning from unlabeled corpora following the distributional hypothesis, where semantic features of a word are related to its co-occurrence patterns.", "labels": [], "entities": []}, {"text": "NNLM learn word representations in the form of dense scalar vectors in n-dimensional spaces (e.g. 300 dimensions), in which each dimension is a latent semantic feature.", "labels": [], "entities": []}, {"text": "The representations are obtained by optimizing the likelihood of existing unlabeled text.", "labels": [], "entities": []}, {"text": "More recently, Mikolov et al. have developed simpler NNLM architectures (), which drastically reduced computational complexity by deleting the hidden layer, enabling to compute accurate word representations from very large corpora.", "labels": [], "entities": []}, {"text": "The representations obtained by these methods are compact, taking 1.5G for 3M words on 300-dimensional space, and have been shown to outperform other distributional corpus-based methods on several tasks, including the WS353 word similarity dataset (.", "labels": [], "entities": [{"text": "WS353 word similarity dataset", "start_pos": 218, "end_pos": 247, "type": "DATASET", "confidence": 0.802059680223465}]}, {"text": "In this work we propose to encode the meaning of words using the structural information in knowledge bases.", "labels": [], "entities": []}, {"text": "That is, instead of modeling the meaning based on the co-occurrences of words in corpora, we model the meaning based on random walks over the knowledge base.", "labels": [], "entities": []}, {"text": "Each random walk is seen as a context for words in the vocabulary, and fed into the NNLM architecture, which optimizes the likelihood of those contexts (cf.).", "labels": [], "entities": []}, {"text": "The resulting word representations are more compact than those produced by regular random walk algorithms (300 vs. tens of thousands), and produce very good results on two well-known benchmarks on word relatedness and similarity: WS353 () and SL999 (), respectively.", "labels": [], "entities": [{"text": "WS353", "start_pos": 230, "end_pos": 235, "type": "DATASET", "confidence": 0.9469737410545349}]}, {"text": "We also show that the obtained representations are complementary to those of random walks alone and to distributional representations obtained by the same NNLM algorithm, improving the results.", "labels": [], "entities": []}, {"text": "Some recent work has explored embedding KBs in low-dimensional continous vector spaces, representing each entity in a k-dimensional vector and characterizing typed relations between entities in the KB (e.g. born-in-city in Freebase or part-of in WordNet) as operations in the k-dimensional space ().", "labels": [], "entities": []}, {"text": "The model estimates the parameters which maximize the likelihood of the triples, which can then be used to infer new typed relations which are missing in the KB.", "labels": [], "entities": []}, {"text": "In contrast, we use the relations to explicitly model the context of words, in two complementary approaches to embed information in KBs into continuous spaces.", "labels": [], "entities": []}], "datasetContent": [{"text": "We have trained two Neural Network models, CBOW and Skip-gram, with several iterations of random walks over WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 108, "end_pos": 115, "type": "DATASET", "confidence": 0.9763990044593811}]}, {"text": "We trained both models with default parameters ( window size 5.", "labels": [], "entities": []}, {"text": "In order to check how many iterations of the random walk algorithm are needed to learn good word representations, we produced up to 70 \u00b7 10 6 contexts.", "labels": [], "entities": []}, {"text": "The the damping factor (\u03b1) of the random walk algorithm was set to 0.85, a usual value (Agirre et al., 2010).", "labels": [], "entities": [{"text": "the damping factor (\u03b1)", "start_pos": 4, "end_pos": 26, "type": "METRIC", "confidence": 0.7446281065543493}]}, {"text": "All parameters were thus set to default, and we only explored different corpus sizes.", "labels": [], "entities": []}, {"text": "The word representations were evaluated on WS353 () and SL999 (), two datasets on word relatedness and word similarity, respectively.", "labels": [], "entities": [{"text": "WS353", "start_pos": 43, "end_pos": 48, "type": "DATASET", "confidence": 0.9589813947677612}]}, {"text": "In order to compute the similarity of two words, it suffices to calculate the cosine between the respective word representations.", "labels": [], "entities": []}, {"text": "The evaluation measure computes the rank correlation (Spearman) between the human judgments and the system values.", "labels": [], "entities": [{"text": "rank correlation (Spearman)", "start_pos": 36, "end_pos": 63, "type": "METRIC", "confidence": 0.8259909391403198}]}, {"text": "In order to contrast our results with the two related techniques, we used UKB 5 , a publicly available implementation of Personalized PageRank (), and ran it over the same graph as our proposed methods.", "labels": [], "entities": [{"text": "UKB 5", "start_pos": 74, "end_pos": 79, "type": "DATASET", "confidence": 0.9629854261875153}]}, {"text": "We used it out-of-the-box with a damping value of 0.85.", "labels": [], "entities": []}, {"text": "We also downloaded the embeddings learnt by) using Skip-gram over a large text corpus . We used the same cosine algorithm to compute similarity with all word representations.", "labels": [], "entities": []}, {"text": "To distinguish one word representation from the other, we will call our models RWCBOW and RWSGRAM respectively (RW for random-walk), in contrast to the original Personalized PageRank algorithm (PPV) and the corpusbased embeddings learned using Skip-grams (Skipgram).", "labels": [], "entities": [{"text": "RWCBOW", "start_pos": 79, "end_pos": 85, "type": "METRIC", "confidence": 0.7680802941322327}, {"text": "RWSGRAM", "start_pos": 90, "end_pos": 97, "type": "METRIC", "confidence": 0.8843791484832764}]}, {"text": "show the learning curves on the WS353 and SL999 datasets relative to the number  of contexts produced by the random walks on WordNet.", "labels": [], "entities": [{"text": "WS353", "start_pos": 32, "end_pos": 37, "type": "DATASET", "confidence": 0.9827491044998169}, {"text": "SL999 datasets", "start_pos": 42, "end_pos": 56, "type": "DATASET", "confidence": 0.7942096889019012}, {"text": "WordNet", "start_pos": 125, "end_pos": 132, "type": "DATASET", "confidence": 0.9748139977455139}]}, {"text": "The results show that WordNet representations grow quickly (around 7 million contexts), converging around 70M, obtaining practically the same results as PPV for WS353, and better results for SL999 . The results at convergence are shown in, together with those of PPV and Skip-gram.", "labels": [], "entities": [{"text": "WS353", "start_pos": 161, "end_pos": 166, "type": "DATASET", "confidence": 0.9494579434394836}, {"text": "Skip-gram", "start_pos": 271, "end_pos": 280, "type": "DATASET", "confidence": 0.9371459484100342}]}, {"text": "Regarding SL999, we can see that the best results are obtained with RWSGRAM, improving over PPV and Skip-gram.", "labels": [], "entities": [{"text": "RWSGRAM", "start_pos": 68, "end_pos": 75, "type": "METRIC", "confidence": 0.9138612151145935}]}, {"text": "Regarding WS353, all methods except RWSGRAM obtain similar results.", "labels": [], "entities": [{"text": "WS353", "start_pos": 10, "end_pos": 15, "type": "DATASET", "confidence": 0.8010358810424805}, {"text": "RWSGRAM", "start_pos": 36, "end_pos": 43, "type": "DATASET", "confidence": 0.660940945148468}]}, {"text": "The results show that our methods are able to effectively capture the information in WordNet, performing on par to the original PPV algorithm, and better than the corpusbased Skip-gram on the SL999 dataset.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 85, "end_pos": 92, "type": "DATASET", "confidence": 0.9487758278846741}, {"text": "SL999 dataset", "start_pos": 192, "end_pos": 205, "type": "DATASET", "confidence": 0.8998457789421082}]}, {"text": "Note that the best published results for WS353 using WordNet are those of (Agirre et al., 2010) using PPV, which report 0.685.", "labels": [], "entities": [{"text": "WS353", "start_pos": 41, "end_pos": 46, "type": "TASK", "confidence": 0.5993129014968872}, {"text": "WordNet", "start_pos": 53, "end_pos": 60, "type": "DATASET", "confidence": 0.9550079107284546}, {"text": "PPV", "start_pos": 102, "end_pos": 105, "type": "DATASET", "confidence": 0.9507016539573669}]}, {"text": "In order to see if the word representations that we learn are complementary to those of PPV and Skipgram, we combined the scores produced by each word representation.", "labels": [], "entities": [{"text": "Skipgram", "start_pos": 96, "end_pos": 104, "type": "DATASET", "confidence": 0.9193673133850098}]}, {"text": "Given the potentially different scales of the similarity values, we assigned to each item the average of the ranks of the pair in each output.", "labels": [], "entities": []}, {"text": "The top part of repeats the three relevant systems.", "labels": [], "entities": []}, {"text": "The (a+b) row reports an improvement in both datasets, showing that RWSGRAM on WordNet is complementary to PPV in WordNet, and is thus a different representation, even if both use the same knowledge base.", "labels": [], "entities": [{"text": "RWSGRAM", "start_pos": 68, "end_pos": 75, "type": "METRIC", "confidence": 0.7680040001869202}, {"text": "WordNet", "start_pos": 79, "end_pos": 86, "type": "DATASET", "confidence": 0.9525500535964966}, {"text": "WordNet", "start_pos": 114, "end_pos": 121, "type": "DATASET", "confidence": 0.9168241024017334}]}, {"text": "The (a+b) and (a+b+c) show that corpus-based Skip-grams are also complemen-tary, yielding incremental improvements.", "labels": [], "entities": []}, {"text": "In fact, the combination of all three improves over the best published results on SL999, and approaches the best results for WS353, as shown in the last row of the Table.", "labels": [], "entities": [{"text": "SL999", "start_pos": 82, "end_pos": 87, "type": "DATASET", "confidence": 0.8362060785293579}, {"text": "WS353", "start_pos": 125, "end_pos": 130, "type": "DATASET", "confidence": 0.8602429032325745}]}, {"text": "The state of the art on SL999 corresponds to (), who training a Recurrent Neural Net model on bilingual text.", "labels": [], "entities": []}, {"text": "The best results on WS353 correspond to), who combine a Wikipedia-based algorithm with a corpus-based method which uses date-related information from news to learn word representations.", "labels": [], "entities": [{"text": "WS353", "start_pos": 20, "end_pos": 25, "type": "DATASET", "confidence": 0.9132869243621826}]}, {"text": "Note that we have only performed some simple combination to show the complementarity of each information source.", "labels": [], "entities": []}, {"text": "More sophisticated combinations (e.g. learning a regression model) could further improve results.", "labels": [], "entities": []}, {"text": "We have performed some qualitative analysis, which indicates that there is a slight tendency for corpus embeddings (with the window size used in the experiments) to group related words (e.g. physics -proton), and not so much similar words (e.g. vodka -gin), while our KB embeddings include both.", "labels": [], "entities": []}, {"text": "This analysis agrees with the results in, where all KB results are better than corpusbased Skip-gram for the semantic similarity dataset (SL999).", "labels": [], "entities": []}, {"text": "In passing, note that the best published results to date on similarity () use embeddings learnt from bilingual text which suggests that bilingual corpora are better suited to learn embeddings capturing semantic similarity.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Spearman correlation results for our meth- ods (RWSGRAM, RWCBOW) on WordNet random  walks, compared to just random walks (PPV), and  Skip-gram on text corpora.", "labels": [], "entities": [{"text": "correlation", "start_pos": 19, "end_pos": 30, "type": "METRIC", "confidence": 0.8418943285942078}, {"text": "RWCBOW", "start_pos": 67, "end_pos": 73, "type": "METRIC", "confidence": 0.8693512082099915}, {"text": "WordNet", "start_pos": 78, "end_pos": 85, "type": "DATASET", "confidence": 0.9254874587059021}]}]}