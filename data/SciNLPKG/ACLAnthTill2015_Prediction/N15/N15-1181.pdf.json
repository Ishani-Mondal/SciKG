{"title": [{"text": "A Bayesian Model for Joint Learning of Categories and their Features", "labels": [], "entities": [{"text": "Joint Learning of Categories", "start_pos": 21, "end_pos": 49, "type": "TASK", "confidence": 0.7713951915502548}]}], "abstractContent": [{"text": "Categories such as ANIMAL or FURNITURE are acquired at an early age and play an important role in processing, organizing, and conveying world knowledge.", "labels": [], "entities": [{"text": "ANIMAL", "start_pos": 19, "end_pos": 25, "type": "METRIC", "confidence": 0.9523081183433533}, {"text": "FURNITURE", "start_pos": 29, "end_pos": 38, "type": "METRIC", "confidence": 0.9286665916442871}, {"text": "organizing, and conveying world knowledge", "start_pos": 110, "end_pos": 151, "type": "TASK", "confidence": 0.5893213699261347}]}, {"text": "Theories of cat-egorization largely agree that categories are characterized by features such as function or appearance and that feature and category acquisition go hand-in-hand, however previous work has considered these problems in isolation.", "labels": [], "entities": []}, {"text": "We present the first model that jointly learns categories and their features.", "labels": [], "entities": []}, {"text": "The set of features is shared across categories, and strength of association is inferred in a Bayesian framework.", "labels": [], "entities": []}, {"text": "We approximate the learning environment with natural language text which allows us to evaluate performance on a large scale.", "labels": [], "entities": []}, {"text": "Compared to highly engineered pattern-based approaches, our model is cognitively motivated, knowledge-lean, and learns categories and features which are perceived by humans as more meaningful.", "labels": [], "entities": []}], "introductionContent": [{"text": "Categorization is one of the most basic cognitive functions.", "labels": [], "entities": []}, {"text": "It allows individuals to organize their subjective experience of their environment by structuring its contents.", "labels": [], "entities": []}, {"text": "This ability to group different objects into the same category based on their common characteristics underlies major cognitive activities such as perception, learning, and the use of language.", "labels": [], "entities": []}, {"text": "Global categories (such as FURNITURE or ANIMAL) are shared among members of societies, and influence how we perceive, interact with, and argue about the world.", "labels": [], "entities": [{"text": "FURNITURE", "start_pos": 27, "end_pos": 36, "type": "METRIC", "confidence": 0.8082090616226196}, {"text": "ANIMAL", "start_pos": 40, "end_pos": 46, "type": "METRIC", "confidence": 0.7512402534484863}]}, {"text": "Given its fundamental importance, categorization is one of the most studied problems in cognitive science.", "labels": [], "entities": []}, {"text": "The literature is rife with theoretical and experimental accounts, as well as modeling simulations focusing on the emergence, representation, and learning of categories.", "labels": [], "entities": []}, {"text": "Most theories assume that basic level concepts such as dog or chair are characterized by features such as barks or used-for-sitting, and are grouped into categories based on those features.", "labels": [], "entities": []}, {"text": "Although the precise grouping mechanism has been subject to considerable debate (including arguments in favor of exemplars, prototypes, and category utility), it is fairly uncontroversial that categories are associated with featural representations.", "labels": [], "entities": []}, {"text": "Experimental studies show that the development of categories and feature learning mutually influence each other (;: concepts are categorized based on their features, but the perception of features is influenced by already established categories, and, like categories, features evolve overtime.", "labels": [], "entities": []}, {"text": "There is also evidence that features such as barks or runs are grouped into types like behavior, and the distribution of feature types varies across categories.", "labels": [], "entities": []}, {"text": "For instance, living-things such as ANI-MALS have characteristic behavior, whereas artifacts such as TOOLS have characteristic functions, and both categories have characteristic appearance.", "labels": [], "entities": []}, {"text": "In this paper, we investigate the problem of jointly learning categories and their feature types.", "labels": [], "entities": []}, {"text": "Previous modeling work has largely considered these problems in isolation, focusing either on category learning with a fixed set of simplistic features) or feature learning), but not both.", "labels": [], "entities": []}, {"text": "We present a Bayesian model which induces (semantic) categories and feature types from natural language text.", "labels": [], "entities": []}, {"text": "Although language is one of many factors influencing category formation (others include the physical world, how we perceive it, and interact with it), large text corpora encode a surprising amount of extralinguistic information (, and can thus be viewed as an approximation of the learning environment.", "labels": [], "entities": [{"text": "category formation", "start_pos": 53, "end_pos": 71, "type": "TASK", "confidence": 0.7228920161724091}]}, {"text": "Moreover, focusing on textual data, allows us to build categorization models with theoretically unlimited scope, and evaluate categories and their features on a much larger scale than previous work in the cognitive science literature.", "labels": [], "entities": []}, {"text": "Our model induces categories (e.g., ANIMALS) and their feature types (e.g., behavior) from observations of target concepts (e.g., lion, cow) and their co-occurring contexts (e.g., eats, sleeps, large).", "labels": [], "entities": [{"text": "ANIMALS", "start_pos": 36, "end_pos": 43, "type": "METRIC", "confidence": 0.7808561325073242}]}, {"text": "While we can directly evaluate learnt categories through comparison against behavioral data, evaluating feature types is less straightforward.", "labels": [], "entities": []}, {"text": "Previous work has shown that the kinds of features learnable from text are qualitatively different from those produced by humans, which makes direct comparison difficult ().", "labels": [], "entities": []}, {"text": "We circumvent this problem by assessing in a crowd-sourcing experiment whether the induced feature types are relevant fora given category and whether they form a coherent class.", "labels": [], "entities": []}, {"text": "Evaluation results show that our joint model learns accurate categories and feature types achieving results competitive with highly engineered approaches focusing exclusively on feature learning.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we outline our experimental set-up for assessing the performance of the BCF model described above.", "labels": [], "entities": []}, {"text": "We present our data set, briefly introduce the models used for comparison with our approach, and explain how system output was evaluated.", "labels": [], "entities": []}, {"text": "We then report results on a series of experiments which evaluate the quality of the categories and feature types learnt by BCF.", "labels": [], "entities": [{"text": "BCF", "start_pos": 123, "end_pos": 126, "type": "DATASET", "confidence": 0.8645095229148865}]}, {"text": "Data Our experiments used basic-level target concepts (e.g., cat or chair) from two norming studies.", "labels": [], "entities": []}, {"text": "In these studies, humans were presented with concepts and asked for each concept to produce a set of characteristic features.", "labels": [], "entities": []}, {"text": "Ina subsequent study, the concepts were classified into 41 categories (with possible multi-category membership), 34 of which we use as a goldstandard in our categorization experiments (comprising 492 concepts in total).", "labels": [], "entities": []}, {"text": "We excluded very general categories such as THING or STRUCTURE, based on the intuition that it is difficult to identify characteristic features for them.", "labels": [], "entities": [{"text": "THING", "start_pos": 44, "end_pos": 49, "type": "METRIC", "confidence": 0.7127195596694946}, {"text": "STRUCTURE", "start_pos": 53, "end_pos": 62, "type": "METRIC", "confidence": 0.6238037943840027}]}, {"text": "As a heuristic concepts were excluded if they were close to the root of WordNet (e.g., with depth 2 or 4).", "labels": [], "entities": [{"text": "WordNet", "start_pos": 72, "end_pos": 79, "type": "DATASET", "confidence": 0.9733578562736511}]}, {"text": "To obtain the input stimuli for the BCF model, we used a subset of the Wackypedia corpus (, an automatically extracted and POS tagged dump of the English Wikipedia.", "labels": [], "entities": [{"text": "Wackypedia corpus", "start_pos": 71, "end_pos": 88, "type": "DATASET", "confidence": 0.9694728851318359}, {"text": "POS tagged dump of the English Wikipedia", "start_pos": 123, "end_pos": 163, "type": "DATASET", "confidence": 0.49730013523783}]}, {"text": "For each target concept, we identified one corresponding article in Wackypedia.", "labels": [], "entities": [{"text": "Wackypedia", "start_pos": 68, "end_pos": 78, "type": "DATASET", "confidence": 0.9654728174209595}]}, {"text": "Next, we extracted a set of stimuli which consists of (a) every sentence from the concept's corresponding article, and (b) any sentence in a different article which mentions the concept.", "labels": [], "entities": []}, {"text": "This resulted in a data set of 63,076 stimuli which we split into 60% training, 20% development and 20% test.", "labels": [], "entities": []}, {"text": "We removed stopwords as well as words with apart of speech other than noun, verb, and adjective.", "labels": [], "entities": []}, {"text": "Furthermore, we discarded words with an age of acquisition above 10 years () to restrict the vocabulary to frequent and generally familiar words.", "labels": [], "entities": []}, {"text": "Models and Parameters We compared the performance of BCF against BayesCat, a Bayesian model of category acquisition () and Strudel, a pattern-based model which extracts concept features from text (.", "labels": [], "entities": [{"text": "category acquisition", "start_pos": 95, "end_pos": 115, "type": "TASK", "confidence": 0.6993709802627563}]}, {"text": "BayesCat induces categories, which are represented through a distribution over target concepts, and a distribution over features (i.e., individual context words).", "labels": [], "entities": []}, {"text": "In contrast to BCF, it does not learn types of features.", "labels": [], "entities": []}, {"text": "In addition, while BCF induces a hard assignment of concepts to categories, BayesCat learns soft distributions over target concepts for each category.", "labels": [], "entities": []}, {"text": "Soft assignments can be converted into hard assignments by assigning each concept to its most probable category.", "labels": [], "entities": []}, {"text": "We ran BayesCat on the same input stimuli as BCF, with the following parameters: the number of categories was set to K = 40, and the hyperparameters to \u03b1 = 0.7, \u03b2 = 0.1, \u03b3 = 0.1.", "labels": [], "entities": [{"text": "BCF", "start_pos": 45, "end_pos": 48, "type": "DATASET", "confidence": 0.8600557446479797}]}, {"text": "For the BCF model, we used the same number of categories, namely K = 40.", "labels": [], "entities": [{"text": "BCF", "start_pos": 8, "end_pos": 11, "type": "DATASET", "confidence": 0.6096265912055969}]}, {"text": "The number of feature types was set to G = 75, and the hyperparameters to \u03b1 = 0.5, \u03b2 = 0.5, and \u03b3 = 0.1.", "labels": [], "entities": []}, {"text": "Parameters were tuned on the development set.", "labels": [], "entities": [{"text": "Parameters", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9703196287155151}]}, {"text": "For both models, we report results averaged over 10 Gibbs runs, each time we ran the sampler for 1,000 iterations.", "labels": [], "entities": []}, {"text": "We used annealing during learning which proved effective for avoiding local optima.", "labels": [], "entities": []}, {"text": "Strudel automatically extracts features for concepts from text collections following a pattern-based approach.", "labels": [], "entities": []}, {"text": "It takes as input a set of target concepts and a set of patterns, and extracts a list of features for each concept, where each concept-feature pair is weighted with a log-likelihood ratio expressing the pair's strength of association.", "labels": [], "entities": []}, {"text": "show that the learnt representations can be used as a basis for various tasks such as typicality rating, categorization, or clustering of features into types.", "labels": [], "entities": []}, {"text": "In our experiments we obtained Strudel representations from the same Wackypedia corpus used for extracting the input stimuli for BCF (and BayesCat).", "labels": [], "entities": [{"text": "Wackypedia corpus", "start_pos": 69, "end_pos": 86, "type": "DATASET", "confidence": 0.9299929440021515}, {"text": "BCF", "start_pos": 129, "end_pos": 132, "type": "DATASET", "confidence": 0.9105562567710876}]}, {"text": "Note that Strudel, unlike the two Bayesian models, is not a cognitively motivated acquisition model, but an optimized system developed with the aim of obtaining the best possible features from data.", "labels": [], "entities": []}, {"text": "In our first experiment we evaluate the quality of the categories induced by the three models presented above.", "labels": [], "entities": []}, {"text": "The models produce hard categorizations, however, the cognitive gold standard we use for evaluation represents soft categories.", "labels": [], "entities": []}, {"text": "We obtained a hard categorization by assigning members of multiple categories to their most typical category (typicality scores are provided with the data).", "labels": [], "entities": []}, {"text": "1 Method BCF and BayesCat learn a set of categories which we can directly compare to the gold standard.", "labels": [], "entities": [{"text": "BCF", "start_pos": 9, "end_pos": 12, "type": "DATASET", "confidence": 0.8393835425376892}, {"text": "BayesCat", "start_pos": 17, "end_pos": 25, "type": "DATASET", "confidence": 0.8294140100479126}]}, {"text": "For Strudel, we produce a categorization as follows: we represent each concept as a vector over features (obtained from Wackypedia), where each component corresponds to the concept-feature log-likelihood ratios provided by Strudel; following, we then cluster the vectors using K-means and the Cluto toolkit.", "labels": [], "entities": []}, {"text": "As for the other models, we set the number of categories to K = 40.", "labels": [], "entities": []}, {"text": "Metrics To assess the quality of the clusters produced by the models, we measure purity (pur; the extent to which each learnt cluster corresponds to a single gold class) as well as its inverse, collocation (col; the extent to which all items of a particular gold class are represented in a single learnt cluster).", "labels": [], "entities": [{"text": "purity", "start_pos": 81, "end_pos": 87, "type": "METRIC", "confidence": 0.9613184332847595}]}, {"text": "Both measures are based on set-overlap, and we also report their harmonic mean ( f 1; Lang and Lapata 2011).", "labels": [], "entities": []}, {"text": "In addition, we report the V-measure (v1; Rosenberg and Hirschberg 2007) and its factors measuring the homogeneity of clusters (hom) and their completeness (com).", "labels": [], "entities": []}, {"text": "The two factors intuitively correspond to purity and collocation, but are based on information-theoretic measures.", "labels": [], "entities": [{"text": "purity", "start_pos": 42, "end_pos": 48, "type": "METRIC", "confidence": 0.9840474724769592}]}, {"text": "We next investigate the quality of the features our model learns.", "labels": [], "entities": []}, {"text": "We do this by letting the model predict the right concept solely from a set of features.", "labels": [], "entities": []}, {"text": "If the model has acquired informative features, they will be predictive of the unknown concept.", "labels": [], "entities": []}, {"text": "Specifically, the model is presented with a set of previously unseen test stimuli with the target concept removed.", "labels": [], "entities": []}, {"text": "For each stimulus, the model ranks all possible target concepts based on the features f (i.e., context words).", "labels": [], "entities": []}, {"text": "Method In our experiments we compared the ranking performance of BCF, BayesCat, and Strudel.", "labels": [], "entities": [{"text": "BCF", "start_pos": 65, "end_pos": 68, "type": "DATASET", "confidence": 0.9363293051719666}, {"text": "BayesCat", "start_pos": 70, "end_pos": 78, "type": "DATASET", "confidence": 0.8719944953918457}]}, {"text": "For the Bayesian models, we directly exploit the learnt distributions.", "labels": [], "entities": []}, {"text": "For BCF, we compute the score of a target concept c given a set of features as:: Model performance on the concept prediction task.", "labels": [], "entities": [{"text": "BCF", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.5733903646469116}, {"text": "concept prediction task", "start_pos": 106, "end_pos": 129, "type": "TASK", "confidence": 0.8056307236353556}]}, {"text": "Precision at rank 1, 10, 20, and average rank assigned (avg).", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9731609225273132}, {"text": "average rank assigned (avg)", "start_pos": 33, "end_pos": 60, "type": "METRIC", "confidence": 0.8183535933494568}]}, {"text": "\u2212tgt refers to the condition where we remove context words which are identical to the target concept as opposed to using the full context.", "labels": [], "entities": []}, {"text": "Similarly, for BayesCat we compute the score of a concept c given a set of features as follows: For Strudel, we rank concepts according to the cumulative log-likelihood ratio-based association score overall observed features fora particular concept c: Metrics Since we can directly compare model predictions against the actual target concept of the stimulus, we report precision at rank 1, 10, and 20.", "labels": [], "entities": [{"text": "precision", "start_pos": 369, "end_pos": 378, "type": "METRIC", "confidence": 0.9992609620094299}]}, {"text": "We also report the average rank assigned to the correct concept.", "labels": [], "entities": []}, {"text": "All results are based on a random test set of 2,000 previously unseen stimuli.", "labels": [], "entities": []}, {"text": "To control for the possibility that the models are learning a strong (yet trivial) correlation between target concepts and identical words occurring as features, we also report results on a modification of our test set where we remove any mention of the target concept from the context, if present (the \u2212tgt condition).", "labels": [], "entities": []}, {"text": "In this suite of experiments we evaluate two aspects of the feature types induced by our model: (1) Are they relevant to their associated category? and (2) Do they form a coherent class?", "labels": [], "entities": []}, {"text": "Our evaluation followed the intrusion paradigm originally introduced to assess the output of topic models).", "labels": [], "entities": []}, {"text": "We performed two intrusion studies using Amazon's Mechanical Turk crowd-sourcing platform.", "labels": [], "entities": []}, {"text": "In the feature intrusion study, participants were shown examples of categories and their feature types both of which were represented as word clusters (see top).", "labels": [], "entities": []}, {"text": "They were asked to detect the feature type which did not belong to the category.", "labels": [], "entities": []}, {"text": "If a model creates relevant feature types, we would expect participants to be able to identify the intruder relatively easily.", "labels": [], "entities": []}, {"text": "We also conducted a word intrusion study, where participants were shown a single feature type (again represented as a word cluster) and asked to detect the intruder feature/word (see.", "labels": [], "entities": []}, {"text": "If the features are overall coherent and meaningful, it should be relatively straightforward to identify the intruder.", "labels": [], "entities": []}, {"text": "Method We compared the feature types learnt by BCF and Strudel.", "labels": [], "entities": [{"text": "BCF", "start_pos": 47, "end_pos": 50, "type": "DATASET", "confidence": 0.9231516718864441}]}, {"text": "We omitted BayesCat from this evaluation as it does not naturally produce feature types, rather it associates unstructured lists of features with categories.", "labels": [], "entities": []}, {"text": "As mentioned earlier, Strudel does not induce feature types either, however, it associates concepts with features which can be postprocessed to obtain feature types as follows.", "labels": [], "entities": []}, {"text": "Given a category induced by Strudel (as explained in Experiment 1), we collected the features associated with at least half of the concepts in the category with a log likelihood score no less than 19.51.", "labels": [], "entities": [{"text": "log likelihood score", "start_pos": 163, "end_pos": 183, "type": "METRIC", "confidence": 0.7550790111223856}]}, {"text": "We then clustered these features with K-means (using the Cluto toolkit) into K = 5 feature types.", "labels": [], "entities": []}, {"text": "For BCF, for each category k, we select the five feature types g with highest association P(g|k), together with one intruder feature type g which is highly associated with some other category k but not with k.", "labels": [], "entities": [{"text": "BCF", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.5821666717529297}]}, {"text": "For Strudel we took the five feature types elicited through the procedure described above, and one random feature type from the global set of feature types.", "labels": [], "entities": []}, {"text": "Each feature type was represented by a cluster of five words.", "labels": [], "entities": []}, {"text": "With respect to the word intrusion task, participants were only shown feature types (i.e., word clusters) irrespectively of the associated category.", "labels": [], "entities": [{"text": "word intrusion task", "start_pos": 20, "end_pos": 39, "type": "TASK", "confidence": 0.7606292366981506}]}, {"text": "BCF feature types g were represented as the set of the five words w with highest probability P( f |g).", "labels": [], "entities": [{"text": "BCF", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.776707112789154}]}, {"text": "In addition, we added one intruder word which had low probability under g but high probability under some other feature type.", "labels": [], "entities": []}, {"text": "For Strudel, we represented feature types as a random subset of five words, and added an additional intruder word from the global set of features.", "labels": [], "entities": []}, {"text": "For the feature type intrusion task, We evaluated a total of 40 categories for each model.", "labels": [], "entities": []}, {"text": "Each participant assessed 10 categories per session (5 per model).", "labels": [], "entities": []}, {"text": "Categories and feature types were presented in random order.", "labels": [], "entities": []}, {"text": "For the word intrusion task, we evaluated a total of 66 feature types for each model.", "labels": [], "entities": [{"text": "word intrusion task", "start_pos": 8, "end_pos": 27, "type": "TASK", "confidence": 0.8226853013038635}]}, {"text": "Participants saw 11 feature types per session, in randomized order.", "labels": [], "entities": []}, {"text": "In both cases, we collected 10 responses per item.", "labels": [], "entities": []}, {"text": "Metrics We evaluated feature type relevance and coherence by measuring precision (the proportion of intruders identified correctly).", "labels": [], "entities": [{"text": "precision", "start_pos": 71, "end_pos": 80, "type": "METRIC", "confidence": 0.9992239475250244}]}, {"text": "We also use the Kappa coefficient to measure inter-subject agreement on our two tasks.", "labels": [], "entities": [{"text": "Kappa coefficient", "start_pos": 16, "end_pos": 33, "type": "METRIC", "confidence": 0.9358267486095428}]}], "tableCaptions": [{"text": " Table 1: Model performance on the category induc- tion task.", "labels": [], "entities": []}, {"text": " Table 2: Model performance on the concept predic- tion task. Precision at rank 1, 10, 20, and average  rank assigned (avg). \u2212tgt refers to the condition  where we remove context words which are identi- cal to the target concept as opposed to using the full  context.", "labels": [], "entities": [{"text": "Precision", "start_pos": 62, "end_pos": 71, "type": "METRIC", "confidence": 0.9922953844070435}]}, {"text": " Table 2. The Bayesian models out- perform Strudel across all metrics and conditions.  Strudel's extraction algorithm, which relies on pre- defined patterns, might be too restrictive with re- spect to the set of features it extracts and as a re- sult they are not discriminative. BayesCat and BCF", "labels": [], "entities": [{"text": "BCF", "start_pos": 293, "end_pos": 296, "type": "DATASET", "confidence": 0.7910662293434143}]}, {"text": " Table 4: Performance of Strudel and BCF on the  feature type and word intrusion tasks. We report  precision (Prec) and inter-subject agreement (Fleiss'  Kappa; all Kappa values are statistically significant  at p 0.05).", "labels": [], "entities": [{"text": "precision (Prec)", "start_pos": 99, "end_pos": 115, "type": "METRIC", "confidence": 0.7781817615032196}]}]}