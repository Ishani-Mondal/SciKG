{"title": [], "abstractContent": [{"text": "In this demonstration, we will present our online parser 1 that allows users to submit any sentence and obtain an analysis following the specification of AMR (Banarescu et al., 2014) to a large extent.", "labels": [], "entities": [{"text": "AMR", "start_pos": 154, "end_pos": 157, "type": "DATASET", "confidence": 0.672132134437561}]}, {"text": "This AMR analysis is generated by a small set of rules that convert a native Logical Form analysis provided by a pre-existing parser (see Vanderwende, 2015) into the AMR format.", "labels": [], "entities": []}, {"text": "While we demonstrate the performance of our AMR parser on data sets annotated by the LDC, we will focus attention in the demo on the following two areas: 1) we will make available AMR annotations for the data sets that were used to develop our parser, to serve as a supplement to the LDC data sets, and 2) we will demonstrate AMR parsers for German, French, Spanish and Japanese that make use of the same small set of LF-to-AMR conversion rules.", "labels": [], "entities": [{"text": "AMR parser", "start_pos": 44, "end_pos": 54, "type": "TASK", "confidence": 0.8070730865001678}, {"text": "LDC data sets", "start_pos": 284, "end_pos": 297, "type": "DATASET", "confidence": 0.918708860874176}]}], "introductionContent": [{"text": "Abstract Meaning Representation (AMR) () is a semantic representation for which a large amount of manually-annotated data is being created, with the intent of constructing and evaluating parsers that generate this level of semantic representation for previously unseen text.", "labels": [], "entities": [{"text": "Abstract Meaning Representation (AMR)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.8361333211263021}]}, {"text": "Available at: http://research.microsoft.com/msrsplat Already one method for training an AMR parser has appeared in (), and we anticipate that more attempts to train parsers will follow.", "labels": [], "entities": [{"text": "AMR parser", "start_pos": 88, "end_pos": 98, "type": "TASK", "confidence": 0.7659601867198944}]}, {"text": "In this demonstration, we will present our AMR parser, which converts our existing semantic representation formalism, Logical Form (LF), into the AMR format.", "labels": [], "entities": []}, {"text": "We do this with two goals: first, as our existing LF is close in design to AMR, we can now use the manually-annotated AMR datasets to measure the accuracy of our LF system, which may serve to provide a benchmark for parsers trained on the AMR corpus.", "labels": [], "entities": [{"text": "AMR", "start_pos": 75, "end_pos": 78, "type": "DATASET", "confidence": 0.8957411646842957}, {"text": "AMR datasets", "start_pos": 118, "end_pos": 130, "type": "DATASET", "confidence": 0.9161902368068695}, {"text": "accuracy", "start_pos": 146, "end_pos": 154, "type": "METRIC", "confidence": 0.9987151622772217}, {"text": "AMR corpus", "start_pos": 239, "end_pos": 249, "type": "DATASET", "confidence": 0.8393833339214325}]}, {"text": "We gratefully acknowledge the contributions made by towards defining a clear and interpretable semantic representation that enables this type of system comparison.", "labels": [], "entities": []}, {"text": "Second, we wish to contribute new AMR data sets comprised of the AMR annotations by our AMR parser of the sentences we previously used to develop our LF system.", "labels": [], "entities": [{"text": "AMR data sets", "start_pos": 34, "end_pos": 47, "type": "DATASET", "confidence": 0.8894393841425577}, {"text": "AMR", "start_pos": 65, "end_pos": 68, "type": "DATASET", "confidence": 0.8501781821250916}]}, {"text": "These sentences were curated to cover a widerange of syntactic-semantic phenomena, including those described in the AMR specification.", "labels": [], "entities": [{"text": "AMR specification", "start_pos": 116, "end_pos": 133, "type": "DATASET", "confidence": 0.7627927660942078}]}, {"text": "We will also demonstrate the capabilities of our parser to generate AMR analyses for sentences in French, German, Spanish and Japanese, for which no manually-annotated AMR data is available at present.", "labels": [], "entities": []}], "datasetContent": [{"text": "Using smatch, we compare the performance of our LF system to the JAMR system of.", "labels": [], "entities": [{"text": "JAMR", "start_pos": 65, "end_pos": 69, "type": "DATASET", "confidence": 0.7126233577728271}]}, {"text": "Both systems rely on the Illinois Named Entity Tagger (.", "labels": [], "entities": [{"text": "Illinois Named Entity Tagger", "start_pos": 25, "end_pos": 53, "type": "TASK", "confidence": 0.840132012963295}]}, {"text": "LF strives to be abroad coverage parser without bias toward a particular domain.", "labels": [], "entities": [{"text": "abroad coverage parser", "start_pos": 17, "end_pos": 39, "type": "TASK", "confidence": 0.572923094034195}]}, {"text": "Therefore, we wanted to evaluate across a number of corpora.", "labels": [], "entities": []}, {"text": "When trained on all available data, JAMR should be less domain dependent.", "labels": [], "entities": [{"text": "JAMR", "start_pos": 36, "end_pos": 40, "type": "TASK", "confidence": 0.6731517314910889}]}, {"text": "However, the newswire data is both larger and important, so we also report numbers for JAMR trained on proxy data alone.", "labels": [], "entities": [{"text": "newswire data", "start_pos": 13, "end_pos": 26, "type": "DATASET", "confidence": 0.9565163552761078}, {"text": "JAMR", "start_pos": 87, "end_pos": 91, "type": "DATASET", "confidence": 0.8047941327095032}]}, {"text": "To explore the degree of domain dependence of these systems, we evaluate on several genres provided by the LDC: DFA (discussion forums data from English), Bolt (translated discussion forum data), and Proxy (newswire data).", "labels": [], "entities": [{"text": "Bolt (translated discussion forum data", "start_pos": 155, "end_pos": 193, "type": "DATASET", "confidence": 0.6126072257757187}, {"text": "Proxy (newswire data", "start_pos": 200, "end_pos": 220, "type": "DATASET", "confidence": 0.7718813121318817}]}, {"text": "We did not experiment on the consensus, mt09sdl, or Xinhua subsets because the data was pre-tokenized.", "labels": [], "entities": []}, {"text": "This tokenization must be undone before our parser is applied.", "labels": [], "entities": []}, {"text": "We evaluate in two conditions: \"without word sense annotations\" indicates that the specific sense numbers were discarded in both the gold standard and the system output; \"with word sense annotations\" leaves the sense annotations intact.", "labels": [], "entities": []}, {"text": "The AMR specification requires that concepts, wherever possible, be annotated with a sense ID referencing the OntoNotes sense inventory.", "labels": [], "entities": [{"text": "OntoNotes sense inventory", "start_pos": 110, "end_pos": 135, "type": "DATASET", "confidence": 0.7939239740371704}]}, {"text": "Recall that the LF system intentionally does not have a word sense disambiguation component due to the inherent difficulty of defining and agreeing upon task-independent sense inventories (, i.a.).", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 56, "end_pos": 81, "type": "TASK", "confidence": 0.6077466110388438}]}, {"text": "In order to evaluate in the standard evaluation setup, we therefore construct a wordsense disambiguation component for LF lemmas.", "labels": [], "entities": []}, {"text": "Our approach is quite simple: for each lemma, we find the predominant sense in the training set (breaking ties in favor of the lowest sense ID), and use that sense for all occurrences of the lemma in test data.", "labels": [], "entities": []}, {"text": "For those lemmas that occur in the test but not in the training data, we attempt to find a verb frame in OntoNotes.", "labels": [], "entities": [{"text": "OntoNotes", "start_pos": 105, "end_pos": 114, "type": "DATASET", "confidence": 0.8538943529129028}]}, {"text": "If found, we use the lowest verb sense ID not marked with DO NOT TAG; otherwise, the lemma is left unannotated for sense.", "labels": [], "entities": [{"text": "DO", "start_pos": 58, "end_pos": 60, "type": "METRIC", "confidence": 0.9786174297332764}, {"text": "NOT", "start_pos": 61, "end_pos": 64, "type": "METRIC", "confidence": 0.6186364889144897}, {"text": "TAG", "start_pos": 65, "end_pos": 68, "type": "METRIC", "confidence": 0.5060155391693115}]}, {"text": "Such a simple system should perform well because 95% of sense-annotated tokens in the proxy training set use the predominant sense.", "labels": [], "entities": []}, {"text": "An obvious extension would be sensitive to parts-ofspeech.", "labels": [], "entities": []}, {"text": "As shown in, the LF system outperforms JAMR in broad-domain semantic parsing, as measured by macro-averaged F1 across domains.", "labels": [], "entities": [{"text": "JAMR", "start_pos": 39, "end_pos": 43, "type": "DATASET", "confidence": 0.6099295616149902}, {"text": "broad-domain semantic parsing", "start_pos": 47, "end_pos": 76, "type": "TASK", "confidence": 0.675838420788447}, {"text": "F1", "start_pos": 108, "end_pos": 110, "type": "METRIC", "confidence": 0.9132382273674011}]}, {"text": "This is primarily due to its better performance on discussion forum data.", "labels": [], "entities": []}, {"text": "JAMR, when trained on newswire data, is clearly the best system on newswire data.", "labels": [], "entities": [{"text": "JAMR", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8613871932029724}]}, {"text": "Adding training data from other sources leads to improvements on the discussion forum trained on only the proxy corpus; JAMR (all) is the system trained on all data in LDC2014T12; and LF is the system described in this paper.", "labels": [], "entities": [{"text": "JAMR", "start_pos": 120, "end_pos": 124, "type": "DATASET", "confidence": 0.6152915358543396}]}, {"text": "We evaluate with and without sense annotations in three test corpora.", "labels": [], "entities": []}, {"text": "data, but at the cost of accuracy on newswire.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9988823533058167}]}, {"text": "The lack of sophisticated sense disambiguation in LF causes a substantial degradation in performance on newswire.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. Evaluation results: balanced F-measure in percentage points. JAMR (proxy) is the system of  Flanigan et al.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 39, "end_pos": 48, "type": "METRIC", "confidence": 0.9930766820907593}, {"text": "JAMR", "start_pos": 71, "end_pos": 75, "type": "METRIC", "confidence": 0.8483560085296631}]}]}