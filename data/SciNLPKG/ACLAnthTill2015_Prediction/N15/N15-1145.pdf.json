{"title": [{"text": "Improving Update Summarization via Supervised ILP and Sentence Reranking", "labels": [], "entities": [{"text": "Improving Update Summarization", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.8615945180257162}, {"text": "Sentence Reranking", "start_pos": 54, "end_pos": 72, "type": "TASK", "confidence": 0.7450767755508423}]}], "abstractContent": [{"text": "Integer Linear Programming (ILP) based sum-marization methods have been widely adopted recently because of their state-of-the-art performance.", "labels": [], "entities": []}, {"text": "This paper proposes two new modifications in this framework for update sum-marization.", "labels": [], "entities": []}, {"text": "Our key idea is to use discrimi-native models with a set of features to measure both the salience and the novelty of words and sentences.", "labels": [], "entities": []}, {"text": "First, these features are used in a supervised model to predict the weights of the concepts used in the ILP model.", "labels": [], "entities": []}, {"text": "Second , we generate preliminary sentence candidates in the ILP model and then rerank them using sentence level features.", "labels": [], "entities": []}, {"text": "We evaluate our method on different TAC update summariza-tion data sets, and the results show that our system performs competitively compared to the best TAC systems based on the ROUGE evaluation metric.", "labels": [], "entities": [{"text": "TAC update summariza-tion data sets", "start_pos": 36, "end_pos": 71, "type": "DATASET", "confidence": 0.6908676624298096}, {"text": "ROUGE", "start_pos": 179, "end_pos": 184, "type": "METRIC", "confidence": 0.8358280658721924}]}], "introductionContent": [{"text": "Update summarization has attracted significant research focus recently.", "labels": [], "entities": [{"text": "Update summarization", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.9350755512714386}]}, {"text": "Different from generic extractive summarization, update summarization assumes that users already have some information about a given topic from an old data set, and thus fora new data set the system aims to generate a summary that contains as much novel information as possible.", "labels": [], "entities": [{"text": "update summarization", "start_pos": 49, "end_pos": 69, "type": "TASK", "confidence": 0.6701514571905136}]}, {"text": "This task was first introduced at DUC 2007 and then continued until TAC 2011.", "labels": [], "entities": [{"text": "DUC 2007", "start_pos": 34, "end_pos": 42, "type": "DATASET", "confidence": 0.9040899872779846}, {"text": "TAC 2011", "start_pos": 68, "end_pos": 76, "type": "DATASET", "confidence": 0.8948415815830231}]}, {"text": "It is very useful to chronological events in real applications.", "labels": [], "entities": [{"text": "chronological events", "start_pos": 21, "end_pos": 41, "type": "TASK", "confidence": 0.8957796692848206}]}, {"text": "Most basic update summarization methods are variants of multi-document summarization methods, with some consideration of the difference between the earlier and later document sets (.", "labels": [], "entities": [{"text": "update summarization", "start_pos": 11, "end_pos": 31, "type": "TASK", "confidence": 0.6905615329742432}, {"text": "multi-document summarization", "start_pos": 56, "end_pos": 84, "type": "TASK", "confidence": 0.6738829612731934}]}, {"text": "One important line is to use graphbased co-ranking.", "labels": [], "entities": []}, {"text": "They rank the sentences in the earlier and later document sets simultaneously by considering the sentence relationship.", "labels": [], "entities": []}, {"text": "For example, was inspired by the intuition that \"a sentence receives a positive influence from the sentences that correlate to it in the same collection, whereas receives a negative influence from the sentences that correlates to it in the different (or previously read) collection', and proposed a graph based sentence ranking algorithm for update summarization.", "labels": [], "entities": [{"text": "update summarization", "start_pos": 342, "end_pos": 362, "type": "TASK", "confidence": 0.6602557599544525}]}, {"text": "Wan (2012) integrated two co-ranking processes by adding some strict constraints, which led to more accurate computation of sentences' scores for update summarization.", "labels": [], "entities": []}, {"text": "A similar method was also applied earlier by) for multilingual news summarization.", "labels": [], "entities": [{"text": "multilingual news summarization", "start_pos": 50, "end_pos": 81, "type": "TASK", "confidence": 0.7488800287246704}]}, {"text": "In addition, generative models, such as topic models, have also been adopted for this task.", "labels": [], "entities": []}, {"text": "For example, Delort and Alfonseca (2012) proposed a novel nonparametric Bayesian approach, a variant of Latent Dirichlet Allocation (LDA), aiming to distinguish between common information and novel information.", "labels": [], "entities": []}, {"text": "borrowed the idea of evolutionary clustering and proposed a three-level HDP (Hierarchical Dirichlet Process) model to represent the diversity and commonality between aspects discovered from two different document data sets.", "labels": [], "entities": []}, {"text": "One of the most competitive summarization methods is based on Integer Linear Programming (ILP).", "labels": [], "entities": [{"text": "summarization", "start_pos": 28, "end_pos": 41, "type": "TASK", "confidence": 0.9839062690734863}]}, {"text": "It has been widely adopted in the generic summarization task.", "labels": [], "entities": [{"text": "generic summarization task", "start_pos": 34, "end_pos": 60, "type": "TASK", "confidence": 0.8913034995396932}]}, {"text": "In this paper, we use the ILP summarization framework for the update summarization task, and make improvement from two aspects, with the goal to more discriminatively represent both the salience and novelty of words and sentences.", "labels": [], "entities": [{"text": "ILP summarization", "start_pos": 26, "end_pos": 43, "type": "TASK", "confidence": 0.5740123689174652}, {"text": "update summarization task", "start_pos": 62, "end_pos": 87, "type": "TASK", "confidence": 0.6232128540674845}]}, {"text": "First, we use supervised models and a rich set of features to learn the weights for the bigram concepts used in the ILP model.", "labels": [], "entities": []}, {"text": "Second, we design a sentence reranking component to score the summary candidate sentences generated by the ILP model.", "labels": [], "entities": []}, {"text": "This second reranking approach allows us to explicitly model a sentence's importance and novelty, which complements the bigram centric view in the first step of ILP sentence selection.", "labels": [], "entities": [{"text": "novelty", "start_pos": 89, "end_pos": 96, "type": "METRIC", "confidence": 0.9484017491340637}, {"text": "ILP sentence selection", "start_pos": 161, "end_pos": 183, "type": "TASK", "confidence": 0.7821996609369913}]}, {"text": "Our experimental results on multiple TAC data sets demonstrate the effectiveness of our proposed method.", "labels": [], "entities": [{"text": "TAC data sets", "start_pos": 37, "end_pos": 50, "type": "DATASET", "confidence": 0.8458067774772644}]}], "datasetContent": [{"text": "We evaluate our methods using several recent TAC data sets, from 2008 to 2011.", "labels": [], "entities": [{"text": "TAC data sets", "start_pos": 45, "end_pos": 58, "type": "DATASET", "confidence": 0.886044998963674}]}, {"text": "Every topic has two sets of 10 documents (Set A and B).", "labels": [], "entities": []}, {"text": "The update task aims to create a 100-word summary from Set B given a topic query and Set A. When evaluating on one year's data, we use the data from the other three years as the training set.", "labels": [], "entities": []}, {"text": "This applies to both the supervised ILP method and the sentence reranking regression model.", "labels": [], "entities": []}, {"text": "All the summaries are evaluated using ROUGE).", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 38, "end_pos": 43, "type": "METRIC", "confidence": 0.990962564945221}]}, {"text": "An academic free solver 2 does all the ILP decoding and libsvm 3 is used for SVR implementation.", "labels": [], "entities": []}, {"text": "show the R2 and R-SU4 values on different TAC data sets for the following systems.", "labels": [], "entities": [{"text": "TAC data sets", "start_pos": 42, "end_pos": 55, "type": "DATASET", "confidence": 0.8901766737302145}]}], "tableCaptions": [{"text": " Table 1: Features in the supervised ILP model for weight- ing bigrams.", "labels": [], "entities": []}, {"text": " Table 2: ROUGE-2 results on TAC 2008-2011 data.", "labels": [], "entities": [{"text": "ROUGE-2", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9782569408416748}, {"text": "TAC 2008-2011 data", "start_pos": 29, "end_pos": 47, "type": "DATASET", "confidence": 0.8644915223121643}]}, {"text": " Table 3: ROUGE-SU4 results on TAC 2008-2011 data.", "labels": [], "entities": [{"text": "ROUGE-SU4", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9393297433853149}, {"text": "TAC 2008-2011 data", "start_pos": 31, "end_pos": 49, "type": "DATASET", "confidence": 0.8981634577115377}]}]}