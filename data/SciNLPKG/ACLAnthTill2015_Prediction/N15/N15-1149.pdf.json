{"title": [{"text": "Response-based Learning for Machine Translation of Open-domain Database Queries", "labels": [], "entities": [{"text": "Machine Translation of Open-domain Database Queries", "start_pos": 28, "end_pos": 79, "type": "TASK", "confidence": 0.8486145734786987}]}], "abstractContent": [{"text": "Response-based learning allows to adapt a statistical machine translation (SMT) system to an extrinsic task by extracting supervision signals from task-specific feedback.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 42, "end_pos": 79, "type": "TASK", "confidence": 0.7963293890158335}]}, {"text": "In this paper , we elicit response signals for SMT adaptation by executing semantic parses of translated queries against the Freebase database.", "labels": [], "entities": [{"text": "SMT adaptation", "start_pos": 47, "end_pos": 61, "type": "TASK", "confidence": 0.9964613318443298}, {"text": "Freebase database", "start_pos": 125, "end_pos": 142, "type": "DATASET", "confidence": 0.9650483727455139}]}, {"text": "The challenge of our work lies in scaling semantic parsers to the lexical diversity of open-domain databases.", "labels": [], "entities": []}, {"text": "We find that parser performance on incorrect English sentences, which is standardly ignored in parser evaluation, is key in model selection.", "labels": [], "entities": []}, {"text": "In our experiments, the biggest improvements in F1-score for returning the correct answer from a semantic parse fora translated query are achieved by selecting a parser that is carefully enhanced by paraphrases and synonyms.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.9990848302841187}]}], "introductionContent": [{"text": "In response-based learning for SMT, supervision signals are extracted from an extrinsic response to a machine translation, in contrast to using humangenerated reference translations for supervision.", "labels": [], "entities": [{"text": "SMT", "start_pos": 31, "end_pos": 34, "type": "TASK", "confidence": 0.9944763779640198}]}, {"text": "We apply this framework to a scenario in which a semantic parse of a translated database query is executed against the Freebase database.", "labels": [], "entities": [{"text": "Freebase database", "start_pos": 119, "end_pos": 136, "type": "DATASET", "confidence": 0.9438296258449554}]}, {"text": "We view learning from such task-specific feedback as adaptation of SMT parameters to the task of translating opendomain database queries, thereby grounding SMT in the task of multilingual database access.", "labels": [], "entities": [{"text": "SMT", "start_pos": 67, "end_pos": 70, "type": "TASK", "confidence": 0.9883822798728943}, {"text": "translating opendomain database queries", "start_pos": 97, "end_pos": 136, "type": "TASK", "confidence": 0.8206754475831985}, {"text": "SMT", "start_pos": 156, "end_pos": 159, "type": "TASK", "confidence": 0.9895756244659424}, {"text": "multilingual database access", "start_pos": 175, "end_pos": 203, "type": "TASK", "confidence": 0.6222881078720093}]}, {"text": "The success criterion for this task is F1-score in returning the correct answer from a semantic parse of the translated query, rather than BLEU.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.9996123909950256}, {"text": "BLEU", "start_pos": 139, "end_pos": 143, "type": "METRIC", "confidence": 0.9964410662651062}]}, {"text": "Since the semantic parser provides feedback to the response-based learner and defines the final evaluation criterion, the challenge of the presented work lies in scaling the semantic parser to the lexical diversity of open-domain databases such as Freebase.", "labels": [], "entities": []}, {"text": "showed how to use response-based learning to adapt an SMT system to a semantic parser for the Geoquery domain.", "labels": [], "entities": [{"text": "SMT", "start_pos": 54, "end_pos": 57, "type": "TASK", "confidence": 0.9868985414505005}]}, {"text": "The state-of-the-art in semantic parsing on Geoquery achieves a parsing accuracy of over 82% (see for an overview), while the state-of-the-art in semantic parsing on the Free917 data) achieves 68.5% accuracy).", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 24, "end_pos": 40, "type": "TASK", "confidence": 0.6995469331741333}, {"text": "Geoquery", "start_pos": 44, "end_pos": 52, "type": "DATASET", "confidence": 0.9081972241401672}, {"text": "accuracy", "start_pos": 72, "end_pos": 80, "type": "METRIC", "confidence": 0.9081418514251709}, {"text": "semantic parsing", "start_pos": 146, "end_pos": 162, "type": "TASK", "confidence": 0.7319792807102203}, {"text": "Free917 data", "start_pos": 170, "end_pos": 182, "type": "DATASET", "confidence": 0.9897198975086212}, {"text": "accuracy", "start_pos": 199, "end_pos": 207, "type": "METRIC", "confidence": 0.9968861937522888}]}, {"text": "This is due to the lexical variability of Free917 (2,036 word types) compared to Geoquery (279 word types).", "labels": [], "entities": [{"text": "Free917", "start_pos": 42, "end_pos": 49, "type": "DATASET", "confidence": 0.9687050580978394}]}, {"text": "In this paper, we compare different ways of scaling up state-of-the-art semantic parsers for Freebase by adding synonyms and paraphrases.", "labels": [], "entities": []}, {"text": "First, we consider's own extension of the semantic parser of by using paraphrases.", "labels": [], "entities": []}, {"text": "Second, we apply WordNet synonyms for selected parts of speech to the queries in the Free917 dataset.", "labels": [], "entities": [{"text": "Free917 dataset", "start_pos": 85, "end_pos": 100, "type": "DATASET", "confidence": 0.9886920154094696}]}, {"text": "The new pairs of queries and logical forms are added to the dataset on which the semantic parsers are retrained.", "labels": [], "entities": []}, {"text": "We find that both techniques of enhancing the lexical coverage of the semantic parsers result in improved parsing performance, and that the improvements add up nicely.", "labels": [], "entities": []}, {"text": "However, improved parsing performance does not correspond to improved F1-score in answer retrieval when using the respective parser in a response-based learning framework.", "labels": [], "entities": [{"text": "parsing", "start_pos": 18, "end_pos": 25, "type": "TASK", "confidence": 0.9625728726387024}, {"text": "F1-score", "start_pos": 70, "end_pos": 78, "type": "METRIC", "confidence": 0.9991188645362854}, {"text": "answer retrieval", "start_pos": 82, "end_pos": 98, "type": "TASK", "confidence": 0.7969253063201904}]}, {"text": "We show that in order to produce helpful feedback for responsebased learning, parser performance on incorrect En-glish queries needs to betaken into account, which is standardly ignored in parser evaluation.", "labels": [], "entities": []}, {"text": "That is, for the purpose of parsing translated queries, a parser should retrieve correct answers for correct English queries (true positives), and must not retrieve correct answers for incorrect translations (false positives).", "labels": [], "entities": [{"text": "parsing translated queries", "start_pos": 28, "end_pos": 54, "type": "TASK", "confidence": 0.903001089890798}]}, {"text": "In order to measure false discovery rate, we prepare a test set of manually verified incorrect English in addition to a standard test set of original English queries.", "labels": [], "entities": []}, {"text": "We show that if false discovery rate on incorrect English queries is taken into account in model selection, the semantic parser that yields best results for response-based learning in SMT can be found reliably.", "labels": [], "entities": [{"text": "SMT", "start_pos": 184, "end_pos": 187, "type": "TASK", "confidence": 0.9843260645866394}]}], "datasetContent": [{"text": "We use a data dump of Freebase 1 which was has been indexed by the Virtuoso SPARQL engine 2 as our knowledge base.", "labels": [], "entities": []}, {"text": "The corpus used in the experiments is the FREE917 corpus as assembled by and consists of 614 training and 276 test queries in English and corresponding logical forms.", "labels": [], "entities": [{"text": "FREE917 corpus", "start_pos": 42, "end_pos": 56, "type": "DATASET", "confidence": 0.9615258276462555}]}, {"text": "The dataset of negative examples, i.e., incorrect English database queries that should receive incorrect answers, consists of 166 examples that were judged either grammatically or semantically incorrect by the authors.", "labels": [], "entities": []}, {"text": "The translation of the English queries in FREE917 into German, in order to provide a set of source sentences for SMT, was done by the authors.", "labels": [], "entities": [{"text": "FREE917", "start_pos": 42, "end_pos": 49, "type": "DATASET", "confidence": 0.7796755433082581}, {"text": "SMT", "start_pos": 113, "end_pos": 116, "type": "TASK", "confidence": 0.9932224750518799}]}, {"text": "The SMT framework used is CDEC) with standard dense features and additional sparse features as described in . Training of the baseline SMT system was performed on the COMMON CRAWL) dataset consisting of 7.5M parallel English-German segments extracted from the web.", "labels": [], "entities": [{"text": "SMT", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.9775351285934448}, {"text": "SMT", "start_pos": 135, "end_pos": 138, "type": "TASK", "confidence": 0.9695006012916565}, {"text": "COMMON CRAWL) dataset", "start_pos": 167, "end_pos": 188, "type": "DATASET", "confidence": 0.7669144719839096}]}, {"text": "Response-based learning for SMT uses the code described in . For semantic parsing we use the SEMPRE and PARASEMPRE tools of and Berant and Liang (2014) which were trained on the training portion of the FREE917 corpus . Further models use the training data enhanced with synonyms from WordNet as described in Section 4.", "labels": [], "entities": [{"text": "SMT", "start_pos": 28, "end_pos": 31, "type": "TASK", "confidence": 0.9931982159614563}, {"text": "semantic parsing", "start_pos": 65, "end_pos": 81, "type": "TASK", "confidence": 0.7258575856685638}, {"text": "SEMPRE", "start_pos": 93, "end_pos": 99, "type": "METRIC", "confidence": 0.99504154920578}, {"text": "PARASEMPRE", "start_pos": 104, "end_pos": 114, "type": "METRIC", "confidence": 0.9936803579330444}, {"text": "FREE917 corpus", "start_pos": 202, "end_pos": 216, "type": "DATASET", "confidence": 0.9807395339012146}, {"text": "WordNet", "start_pos": 284, "end_pos": 291, "type": "DATASET", "confidence": 0.9472741484642029}]}, {"text": "Following, we evaluate semantic parsers according to precision, defined as the percentage of correctly answered examples out of those for which a parse could be produced, recall, defined as the percentage of total examples answered correctly, and F1-score, defined as harmonic mean of precision and recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 53, "end_pos": 62, "type": "METRIC", "confidence": 0.9994065761566162}, {"text": "recall", "start_pos": 171, "end_pos": 177, "type": "METRIC", "confidence": 0.9994837045669556}, {"text": "F1-score", "start_pos": 247, "end_pos": 255, "type": "METRIC", "confidence": 0.9992326498031616}, {"text": "precision", "start_pos": 285, "end_pos": 294, "type": "METRIC", "confidence": 0.9988540410995483}, {"text": "recall", "start_pos": 299, "end_pos": 305, "type": "METRIC", "confidence": 0.9961782693862915}]}, {"text": "Furthermore, we report false discovery rate (FDR) on the combined set of 276 correct and 166 incorrect database queries.", "labels": [], "entities": [{"text": "false discovery rate (FDR)", "start_pos": 23, "end_pos": 49, "type": "METRIC", "confidence": 0.898751825094223}]}, {"text": "reports standard parsing evaluation metrics for the different parsers SEMPRE (S), PARASEMPRE (P), and extensions of the latter with synonyms from the first one (P1), first two (P2) and first three (P3) synsets which are ordered according to frequency of use of the sense.", "labels": [], "entities": [{"text": "SEMPRE", "start_pos": 70, "end_pos": 76, "type": "METRIC", "confidence": 0.8531147837638855}, {"text": "PARASEMPRE", "start_pos": 82, "end_pos": 92, "type": "METRIC", "confidence": 0.994697093963623}]}, {"text": "As shown in the second column, the size of the training data is increased up to 10 times by using various synonym extensions.", "labels": [], "entities": []}, {"text": "As shown in the third column, PARASEM-PRE improves F1 by nearly 10 points over SEMPRE.", "labels": [], "entities": [{"text": "PARASEM-PRE", "start_pos": 30, "end_pos": 41, "type": "METRIC", "confidence": 0.9951011538505554}, {"text": "F1", "start_pos": 51, "end_pos": 53, "type": "METRIC", "confidence": 0.9995802044868469}, {"text": "SEMPRE", "start_pos": 79, "end_pos": 85, "type": "METRIC", "confidence": 0.7610470056533813}]}, {"text": "Another 0.5 points are added by extending the training data using two synsets.", "labels": [], "entities": []}, {"text": "The third column shows that the system P1 that scored second-worst in terms of F1 score, scores best under the FDR metric 8 . shows an evaluation of the use of different parsing models to retrieve correct answers from the FREE917 test set of correct database queries.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 79, "end_pos": 87, "type": "METRIC", "confidence": 0.9905515909194946}, {"text": "FDR metric 8", "start_pos": 111, "end_pos": 123, "type": "METRIC", "confidence": 0.6072575449943542}, {"text": "FREE917 test set of correct database queries", "start_pos": 222, "end_pos": 266, "type": "DATASET", "confidence": 0.9244853343282428}]}, {"text": "The systems are applied to translated queries, but evaluated in terms of standard parsing metrics.", "labels": [], "entities": []}, {"text": "Statistical significance is measured using an Approximate Randomization test).", "labels": [], "entities": [{"text": "significance", "start_pos": 12, "end_pos": 24, "type": "METRIC", "confidence": 0.6083204746246338}, {"text": "Approximate", "start_pos": 46, "end_pos": 57, "type": "METRIC", "confidence": 0.9951158761978149}]}, {"text": "The baseline system is CDEC as described above.", "labels": [], "entities": [{"text": "CDEC", "start_pos": 23, "end_pos": 27, "type": "DATASET", "confidence": 0.8686136603355408}]}, {"text": "It never sees the FREE917 data during training.", "labels": [], "entities": [{"text": "FREE917 data", "start_pos": 18, "end_pos": 30, "type": "DATASET", "confidence": 0.7484120726585388}]}, {"text": "As a second baseline method we use a stochastic (sub)gradient descent variant of RAM-PION (   This algorithm makes use of positive parser feedback to convert predicted translation into references, in addition to using the original English queries as references.", "labels": [], "entities": [{"text": "RAM-PION", "start_pos": 81, "end_pos": 89, "type": "METRIC", "confidence": 0.8015302419662476}]}, {"text": "Training for both RAMPION and REBOL is performed for 10 epochs over the FREE917 training set, using a constant learning rate \u03b7 that was chosen via cross-validation.", "labels": [], "entities": [{"text": "RAMPION", "start_pos": 18, "end_pos": 25, "type": "METRIC", "confidence": 0.4596801698207855}, {"text": "REBOL", "start_pos": 30, "end_pos": 35, "type": "METRIC", "confidence": 0.8060008883476257}, {"text": "FREE917 training set", "start_pos": 72, "end_pos": 92, "type": "DATASET", "confidence": 0.9347393115361532}]}, {"text": "All methods then proceed to translate the FREE917 test set.", "labels": [], "entities": [{"text": "FREE917 test set", "start_pos": 42, "end_pos": 58, "type": "DATASET", "confidence": 0.8908087213834127}]}, {"text": "Best results in are obtained by using an extension of PARASEMPRE with one synset as parser in responsebased learning with REBOL.", "labels": [], "entities": [{"text": "PARASEMPRE", "start_pos": 54, "end_pos": 64, "type": "METRIC", "confidence": 0.97276771068573}]}, {"text": "This parsing system scored best under the FDR metric in. shows the Spearman rank correlation ( between the F1 / FDR ranking of semantic parsers from and their contribution to F1 scores in for parsing query translations of CDEC, RAMPION or REBOL.", "labels": [], "entities": [{"text": "FDR", "start_pos": 42, "end_pos": 45, "type": "METRIC", "confidence": 0.6922679543495178}, {"text": "Spearman rank correlation", "start_pos": 67, "end_pos": 92, "type": "METRIC", "confidence": 0.7025838096936544}, {"text": "F1", "start_pos": 107, "end_pos": 109, "type": "METRIC", "confidence": 0.9976087808609009}, {"text": "FDR", "start_pos": 112, "end_pos": 115, "type": "METRIC", "confidence": 0.7460622191429138}, {"text": "F1", "start_pos": 175, "end_pos": 177, "type": "METRIC", "confidence": 0.9953717589378357}, {"text": "parsing query translations of CDEC", "start_pos": 192, "end_pos": 226, "type": "TASK", "confidence": 0.7052821636199951}]}, {"text": "The system CDEC cannot learn from parser performance based on query translations, thus best results on translated queries correlate positively with good parsing F1 score per se.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 161, "end_pos": 169, "type": "METRIC", "confidence": 0.940246194601059}]}, {"text": "RAMPION can implicitly take advantage of parsers with good FDR score since learning to move away from translations dissimilar to the reference is helpful if they do not lead to correct answers.", "labels": [], "entities": [{"text": "RAMPION", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.6230246424674988}, {"text": "FDR score", "start_pos": 59, "end_pos": 68, "type": "METRIC", "confidence": 0.9804883301258087}]}, {"text": "REBOL can make the best use of parsers with low FDR score since it can learn to prevent incorrect translations from hurting parsing performance attest time.", "labels": [], "entities": [{"text": "REBOL", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.6117442846298218}, {"text": "FDR score", "start_pos": 48, "end_pos": 57, "type": "METRIC", "confidence": 0.9895084500312805}]}], "tableCaptions": [{"text": " Table 1: Parsing F1 scores and False Discovery Rate  (FDR) for SEMPRE (S), PARASEMPRE (P), and exten- sions of the latter with synonyms from first one (P1),  first two (P2) and first three (P3) synsets, evaluated on  the FREE917 test set of correct database queries for F1  and including the test set of incorrect database queries for  FDR, and trained on #data training queries. Best results  are indicated in bold face.", "labels": [], "entities": [{"text": "F1 scores", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.9332517087459564}, {"text": "False Discovery Rate  (FDR)", "start_pos": 32, "end_pos": 59, "type": "METRIC", "confidence": 0.9738284846146902}, {"text": "PARASEMPRE", "start_pos": 76, "end_pos": 86, "type": "METRIC", "confidence": 0.9965817332267761}, {"text": "FREE917 test set", "start_pos": 222, "end_pos": 238, "type": "DATASET", "confidence": 0.9441041946411133}, {"text": "FDR", "start_pos": 337, "end_pos": 340, "type": "DATASET", "confidence": 0.8287973999977112}]}, {"text": " Table 2: Parsing F1 score on FREE917 test set of trans- lated database queries using different parser models to  provide response for translated queries. Best results are  indicated in bold face. Statistical significance of result  differences at p < 0.05 are indicated by algorithm num- ber in superscript.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9590201377868652}, {"text": "FREE917 test set", "start_pos": 30, "end_pos": 46, "type": "DATASET", "confidence": 0.9296703338623047}]}, {"text": " Table 3: Spearman correlation between F1 / FDR from  Table 1 and CDEC / RAMPION / REBOL F1 from Table 2.", "labels": [], "entities": [{"text": "Spearman correlation", "start_pos": 10, "end_pos": 30, "type": "METRIC", "confidence": 0.8951590359210968}, {"text": "F1", "start_pos": 39, "end_pos": 41, "type": "METRIC", "confidence": 0.9885586500167847}, {"text": "FDR", "start_pos": 44, "end_pos": 47, "type": "METRIC", "confidence": 0.5333319902420044}, {"text": "CDEC / RAMPION / REBOL F1", "start_pos": 66, "end_pos": 91, "type": "METRIC", "confidence": 0.666994700829188}]}]}