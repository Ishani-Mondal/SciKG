{"title": [{"text": "Predicate Argument Alignment using a Global Coherence Model", "labels": [], "entities": [{"text": "Predicate Argument Alignment", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8246790965398153}]}], "abstractContent": [{"text": "We present a joint model for predicate argument alignment.", "labels": [], "entities": [{"text": "predicate argument alignment", "start_pos": 29, "end_pos": 57, "type": "TASK", "confidence": 0.9168152411778768}]}, {"text": "We leverage multiple sources of semantic information, including temporal ordering constraints between events.", "labels": [], "entities": []}, {"text": "These are combined in a max-margin framework to find a globally consistent view of entities and events across multiple documents, which leads to improvements over a very strong local baseline.", "labels": [], "entities": []}], "introductionContent": [{"text": "Natural language understanding (NLU) requires analysis beyond the sentence-level.", "labels": [], "entities": [{"text": "Natural language understanding (NLU)", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.796201710899671}]}, {"text": "For example, an entity maybe mentioned multiple times in a discourse, participating in various events, where each event may itself be referenced elsewhere in the text.", "labels": [], "entities": []}, {"text": "Traditionally the task of coreference resolution has been defined as finding those entity mentions within a single document that co-refer, while crossdocument coreference resolution considers a wider discourse context across many documents, yet still pertains strictly to entities.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 26, "end_pos": 48, "type": "TASK", "confidence": 0.9534486830234528}, {"text": "crossdocument coreference resolution", "start_pos": 145, "end_pos": 181, "type": "TASK", "confidence": 0.6206335028012594}]}, {"text": "Predicate argument alignment, or entity-event cross-document coreference resolution, enlarges the set of possible co-referent elements to include the mentions of situations in which entities participate.", "labels": [], "entities": [{"text": "Predicate argument alignment", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.7546307444572449}, {"text": "cross-document coreference resolution", "start_pos": 46, "end_pos": 83, "type": "TASK", "confidence": 0.6527294814586639}]}, {"text": "This expanded definition drives practitioners towards a more complete model of NLU, where systems must not only consider who is mentioned, but also what happened.", "labels": [], "entities": []}, {"text": "However, despite the drive towards an expanded notion of discourse, models typically are formulated with strong notions of localindependence: viewing a multi-document task as one limited to individual pairs of sentences.", "labels": [], "entities": []}, {"text": "This creates a mis-match between the goals of such work -considering entire documents -with the systemsconsider individual sentences.", "labels": [], "entities": []}, {"text": "In this work, we consider a system that takes a document level view in considering coreference for entities and predictions: the task of predicate argument linking.", "labels": [], "entities": [{"text": "predicate argument linking", "start_pos": 137, "end_pos": 163, "type": "TASK", "confidence": 0.723137617111206}]}, {"text": "We treat this task as a global inference problem, leveraging multiple sources of semantic information identified at the document level.", "labels": [], "entities": []}, {"text": "Global inference for this problem is mostly unexplored, with the exception of (discussed in \u00a7 8).", "labels": [], "entities": [{"text": "Global inference", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8373426198959351}]}, {"text": "Especially novel here is the use of document-level temporal constraints on events, representing a next step forward on the path to full understanding.", "labels": [], "entities": []}, {"text": "Our approach avoids the pitfalls of local inference while still remaining fast and exact.", "labels": [], "entities": []}, {"text": "We use the pairwise features of a very strong predicate argument aligner) (competitive with the state-of-the-art), and add quadratic factors that constrain local decisions based on global document information.", "labels": [], "entities": []}, {"text": "These global factors lead to superior performance compared to the previous state-of-the-art.", "labels": [], "entities": []}, {"text": "We release both our code and data.", "labels": [], "entities": []}], "datasetContent": [{"text": "Data We consider two datasets for evaluation.", "labels": [], "entities": []}, {"text": "The first is a cross-document entity and event coreference resolution dataset called the Extended Event Coref Bank (EECB) created by and based on a corpus from.", "labels": [], "entities": [{"text": "event coreference resolution", "start_pos": 41, "end_pos": 69, "type": "TASK", "confidence": 0.6425618628660837}]}, {"text": "The dataset contains clusters of news articles taken from Google News with annotations about coreference over entities and events.", "labels": [], "entities": [{"text": "coreference over entities and events", "start_pos": 93, "end_pos": 129, "type": "TASK", "confidence": 0.8400891661643982}]}, {"text": "Following the procedure of, we select the first document in every cluster and pair it with every other document in the cluster.", "labels": [], "entities": []}, {"text": "The second dataset (RF) comes from Roth and Frank (2012).", "labels": [], "entities": [{"text": "RF)", "start_pos": 20, "end_pos": 23, "type": "METRIC", "confidence": 0.9596696496009827}]}, {"text": "The dataset contains pairs of news articles that describe the same news story, and are annotated for predicate links between the document pairs.", "labels": [], "entities": []}, {"text": "Due to the lack of annotated arguments, we can only report predicate linking performance and the psa and asp factors do not apply.", "labels": [], "entities": []}, {"text": "Lastly, the size of the RF data should be noted as it is much smaller than EECB: the test set has 60 document pairs and the dev set has 10 document pairs.", "labels": [], "entities": [{"text": "EECB", "start_pos": 75, "end_pos": 79, "type": "DATASET", "confidence": 0.7961702942848206}]}, {"text": "Both datasets are annotated with parses and indocument coreference labels provided by the toolset of and are available with our code release.", "labels": [], "entities": []}, {"text": "Due to the small data size, we use kfold cross validation for both datasets.", "labels": [], "entities": []}, {"text": "We choose k = 10 for RF due to its very small size (more folds give more training examples) and k = 5 on EECB to save computation time (amount of training data in EECB is less of a concern).", "labels": [], "entities": [{"text": "RF", "start_pos": 21, "end_pos": 23, "type": "TASK", "confidence": 0.7341129779815674}, {"text": "EECB", "start_pos": 105, "end_pos": 109, "type": "DATASET", "confidence": 0.9723940491676331}, {"text": "EECB", "start_pos": 163, "end_pos": 167, "type": "DATASET", "confidence": 0.9413365721702576}]}, {"text": "Hyperparameters were chosen by hand using using cross validation on the EECB dataset using F1 as the criteria (rather than Hamming).", "labels": [], "entities": [{"text": "EECB dataset", "start_pos": 72, "end_pos": 84, "type": "DATASET", "confidence": 0.9940859079360962}, {"text": "F1", "start_pos": 91, "end_pos": 93, "type": "METRIC", "confidence": 0.9985817670822144}, {"text": "Hamming", "start_pos": 123, "end_pos": 130, "type": "METRIC", "confidence": 0.7199729084968567}]}, {"text": "Figures report averages across these folds.", "labels": [], "entities": []}, {"text": "Systems Following Roth and Frank (2012) and we include a Lemma baseline for identifying alignments which will align any two predicates or arguments that have the same lemmatized headword.", "labels": [], "entities": []}, {"text": "The Local baseline uses the same features as Wolfe et al., but none of our joint factors.", "labels": [], "entities": [{"text": "Local baseline", "start_pos": 4, "end_pos": 18, "type": "DATASET", "confidence": 0.7922699153423309}]}, {"text": "In addition to running our joint model with all factors, we measure the efficacy of each individual factor by evaluating each with the local features.", "labels": [], "entities": []}, {"text": "For evaluation we use a generous version of F1 that is defined for alignment labels composed of sure, G s , and possible links, G p and the system's proposed links H (following, and).", "labels": [], "entities": [{"text": "F1", "start_pos": 44, "end_pos": 46, "type": "METRIC", "confidence": 0.9985328912734985}]}, {"text": "Note that the EECB data does not have a sure and possible distinction, so G s = G p , resulting in standard F1.", "labels": [], "entities": [{"text": "EECB data", "start_pos": 14, "end_pos": 23, "type": "DATASET", "confidence": 0.9790451526641846}, {"text": "F1", "start_pos": 108, "end_pos": 110, "type": "METRIC", "confidence": 0.9970979690551758}]}, {"text": "In addition to F1, we separately measure predicate and argument F1 to demonstrate where our model makes the largest improvements.", "labels": [], "entities": [{"text": "F1", "start_pos": 15, "end_pos": 17, "type": "METRIC", "confidence": 0.9990082383155823}, {"text": "F1", "start_pos": 64, "end_pos": 66, "type": "METRIC", "confidence": 0.9384835362434387}]}, {"text": "We performed a one-sided paired-bootstrap test where the null hypothesis was that the joint model was no better than the Local baseline (described in).", "labels": [], "entities": []}, {"text": "Cases where p < 0.05 are bolded.", "labels": [], "entities": []}], "tableCaptions": []}