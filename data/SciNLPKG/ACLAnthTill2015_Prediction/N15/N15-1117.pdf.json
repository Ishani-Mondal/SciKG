{"title": [{"text": "Removing the Training Wheels: A Coreference Dataset that Entertains Humans and Challenges Computers", "labels": [], "entities": [{"text": "Removing", "start_pos": 0, "end_pos": 8, "type": "TASK", "confidence": 0.9540930986404419}, {"text": "Coreference Dataset", "start_pos": 32, "end_pos": 51, "type": "DATASET", "confidence": 0.8187152147293091}]}], "abstractContent": [{"text": "Coreference is a core nlp problem.", "labels": [], "entities": []}, {"text": "However , newswire data, the primary source of existing coreference data, lack the richness necessary to truly solve coreference.", "labels": [], "entities": []}, {"text": "We present anew domain with denser references-quiz bowl questions-that is challenging and enjoyable to humans, and we use the quiz bowl community to develop anew coreference dataset, together with an annotation framework that can tag any text data with coreferences and named entities.", "labels": [], "entities": []}, {"text": "We also successfully integrate active learning into this annotation pipeline to collect documents maximally useful to coreference models.", "labels": [], "entities": []}, {"text": "State-of-the-art coreference systems underperform a simple classifier on our new dataset, motivating non-newswire data for future coreference research.", "labels": [], "entities": []}], "introductionContent": [{"text": "Coreference resolution-adding annotations to an input text where multiple strings refer to the same entity-is a fundamental problem in computational linguistics.", "labels": [], "entities": [{"text": "Coreference resolution-adding annotations to an input text", "start_pos": 0, "end_pos": 58, "type": "TASK", "confidence": 0.9033685326576233}]}, {"text": "It is challenging because it requires the application of syntactic, semantic, and world knowledge.", "labels": [], "entities": []}, {"text": "For example, in the sentence Monsieur Poirot assured Hastings that he ought to have faith in him, the strings Monsieur Poirot and him refer to the same person, while Hastings and he refer to a different character.", "labels": [], "entities": []}, {"text": "There area panoply of sophisticated coreference systems, both data-driven) and rule-based).", "labels": [], "entities": []}, {"text": "Recent CoNLL shared tasks provide the opportunity to make a fair comparison between these systems.", "labels": [], "entities": []}, {"text": "However, because all of these shared tasks contain strictly newswire data, 1 it is unclear how existing systems perform on more diverse data.", "labels": [], "entities": []}, {"text": "We argue in Section 2 that to truly solve coreference resolution, the research community needs high-quality datasets that contain many challenging cases such as nested coreferences and coreferences that can only be resolved using external knowledge.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 42, "end_pos": 64, "type": "TASK", "confidence": 0.9635769724845886}]}, {"text": "In contrast, newswire is deliberately written to contain few coreferences, and those coreferences should be easy for the reader to resolve.", "labels": [], "entities": []}, {"text": "Thus, systems that are trained on such data commonly fail to detect coreferences in more expressive, non-newswire text.", "labels": [], "entities": []}, {"text": "Given newswire's imperfect range of coreference examples, can we do better?", "labels": [], "entities": []}, {"text": "In Section 3 we present a specialized dataset that specifically tests a human's coreference resolution ability.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 80, "end_pos": 102, "type": "TASK", "confidence": 0.8512118756771088}]}, {"text": "This dataset comes from a community of trivia fans who also serve as enthusiastic annotators (Section 4).", "labels": [], "entities": []}, {"text": "These data have denser coreference mentions than newswire text and present hitherto unexplored questions of what is coreferent and what is not.", "labels": [], "entities": []}, {"text": "We also incorporate active learning into the annotation process.", "labels": [], "entities": []}, {"text": "The result is a small but highly dense dataset of 400 documents with 9,471 mentions.", "labels": [], "entities": []}, {"text": "We demonstrate in Section 5 that our dataset is significantly different from newswire based on results from the effective, widely-used Berkeley system.", "labels": [], "entities": []}, {"text": "These results motivate us to develop a very simple end-to-end coreference resolution system consisting of a crfbased mention detector and a pairwise classifier.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 62, "end_pos": 84, "type": "TASK", "confidence": 0.841570109128952}]}, {"text": "Our system outperforms the Berkeley system when both have been trained on our new dataset.", "labels": [], "entities": []}, {"text": "This result motivates further exploration into complex coreference types absent in newswire data, which we discuss at length in Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate the widely used Berkeley coreference system (Durrett and Klein, 2013) on our dataset to show that models trained on newswire data cannot effectively resolve coreference in quiz bowl data.", "labels": [], "entities": []}, {"text": "Training and evaluating the Berkeley system on quiz bowl data also results in poor performance.", "labels": [], "entities": [{"text": "Berkeley system on quiz bowl data", "start_pos": 28, "end_pos": 61, "type": "DATASET", "confidence": 0.7062423179546992}]}, {"text": "11 This result motivates us to build an end-to-end coreference resolution system that includes a data-driven mention detector (as opposed to Berkeley's rule-based one) and a simple pairwise classifier.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 51, "end_pos": 73, "type": "TASK", "confidence": 0.8513149321079254}]}, {"text": "Using our mentions and only six feature types, we are able to outperform the Berkeley system on our data.", "labels": [], "entities": []}, {"text": "Finally, we explore the linguistic phenomena that make quiz bowl coreference so hard and draw insights from our analysis that may help to guide the next generation of coreference systems.", "labels": [], "entities": []}, {"text": "11 We use default options, including hyperparameters tuned on OntoNotes", "labels": [], "entities": [{"text": "OntoNotes", "start_pos": 62, "end_pos": 71, "type": "DATASET", "confidence": 0.8742299675941467}]}], "tableCaptions": [{"text": " Table 1: Three newswire sentences and three quiz bowl sentences with annotated coreferences and singleton  mentions. These examples show that quiz bowl sentences contain more complicated types of coreferences  that may even require world knowledge to resolve.", "labels": [], "entities": []}, {"text": " Table 1. Since  quiz bowl is a game, it makes the task of solving  coreference interesting and challenging for an  annotator. In the next section, we use the intrin- sic fun of this task to create a new annotated  coreference dataset.", "labels": [], "entities": []}, {"text": " Table 2: Statistics of both our quiz bowl dataset and  the OntoNotes training data from the CoNLL 2011  shared task.", "labels": [], "entities": [{"text": "quiz bowl dataset", "start_pos": 33, "end_pos": 50, "type": "DATASET", "confidence": 0.6082104643185934}, {"text": "OntoNotes training data from the CoNLL 2011  shared task", "start_pos": 60, "end_pos": 116, "type": "DATASET", "confidence": 0.8525036772092184}]}, {"text": " Table 3: The top half of the table represents Berkeley  models trained on OntoNotes 4.0 data, while the bot- tom half shows models trained on quiz bowl data. The  muc F 1 -score of the Berkeley system on OntoNotes  text is 66.4, which when compared to these results  prove that quiz bowl coreference is significantly dif- ferent than OntoNotes coreference.", "labels": [], "entities": [{"text": "OntoNotes 4.0 data", "start_pos": 75, "end_pos": 93, "type": "DATASET", "confidence": 0.8941652973492941}, {"text": "muc F 1 -score", "start_pos": 164, "end_pos": 178, "type": "METRIC", "confidence": 0.753440660238266}, {"text": "OntoNotes  text", "start_pos": 205, "end_pos": 220, "type": "DATASET", "confidence": 0.8986444175243378}]}]}