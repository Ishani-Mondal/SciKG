{"title": [{"text": "Normalized Word Embedding and Orthogonal Transform for Bilingual Word Translation", "labels": [], "entities": [{"text": "Normalized Word Embedding", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.6952357192834219}, {"text": "Bilingual Word Translation", "start_pos": 55, "end_pos": 81, "type": "TASK", "confidence": 0.6632252633571625}]}], "abstractContent": [{"text": "Word embedding has been found to be highly powerful to translate words from one language to another by a simple linear transform.", "labels": [], "entities": []}, {"text": "However, we found some inconsistence among the objective functions of the embedding and the transform learning, as well as the distance measurement.", "labels": [], "entities": []}, {"text": "This paper proposes a solution which normalizes the word vectors on a hypersphere and constrains the linear transform as an orthogonal transform.", "labels": [], "entities": []}, {"text": "The experimental results confirmed that the proposed solution can offer better performance on a word similarity task and an English-to-Spanish word translation task.", "labels": [], "entities": [{"text": "word similarity task", "start_pos": 96, "end_pos": 116, "type": "TASK", "confidence": 0.821494479974111}, {"text": "English-to-Spanish word translation task", "start_pos": 124, "end_pos": 164, "type": "TASK", "confidence": 0.6802575215697289}]}], "introductionContent": [{"text": "Word embedding has been extensively studied in recent years (.", "labels": [], "entities": []}, {"text": "Following the idea that the meaning of a word can be determined by 'the company it keeps' (, i.e., the words that it co-occurs with, word embedding projects discrete words to a low-dimensional and continuous vector space where co-occurred words are located close to each other.", "labels": [], "entities": []}, {"text": "Compared to conventional discrete representations (e.g., the one-hot encoding), word embedding provides more robust representations for words, particulary for those that infrequently appear in the training data.", "labels": [], "entities": []}, {"text": "More importantly, the embedding encodes syntactic and semantic content implicitly, so that relations among words can be simply computed as the distances among their embeddings, or word vectors.", "labels": [], "entities": []}, {"text": "A well-known efficient word embedding approach was recently proposed by, where two log-linear models (CBOW and skip-gram) are proposed to learn the neighboring relation of words in context.", "labels": [], "entities": []}, {"text": "A following work proposed by the same authors introduces some modifications that largely improve the efficiency of model training ().", "labels": [], "entities": []}, {"text": "An interesting property of word vectors learned by the log-linear model is that the relations among relevant words seem linear and can be computed by simple vector addition and substraction).", "labels": [], "entities": []}, {"text": "For example, the following relation approximately holds in the word vector space: ParisFrance + Rome = Italy.", "labels": [], "entities": [{"text": "ParisFrance", "start_pos": 82, "end_pos": 93, "type": "DATASET", "confidence": 0.9530373215675354}]}, {"text": "In (, the linear relation is extended to the bilingual scenario, where a linear transform is learned to project semantically identical words from one language to another.", "labels": [], "entities": []}, {"text": "The authors reported a high accuracy on a bilingual word translation task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.9994809031486511}, {"text": "bilingual word translation task", "start_pos": 42, "end_pos": 73, "type": "TASK", "confidence": 0.713896818459034}]}, {"text": "Although promising, we argue that both the word embedding and the linear transform are ill-posed, due to the inconsistence among the objective function used to learn the word vectors (maximum likelihood based on inner product), the distance measurement for word vectors (cosine distance), and the objective function used to learn the linear transform (mean square error).", "labels": [], "entities": [{"text": "mean square error)", "start_pos": 352, "end_pos": 370, "type": "METRIC", "confidence": 0.7689439207315445}]}, {"text": "This inconsistence may lead to suboptimal estimation for both word vectors and the bilingual transform, as we will see shortly.", "labels": [], "entities": []}, {"text": "This paper solves the inconsistence by normalizing the word vectors.", "labels": [], "entities": []}, {"text": "Specifically, we enforce the word vectors to be in a unit length during the learning of the embedding.", "labels": [], "entities": []}, {"text": "By this constraint, all the word vectors are located on a hypersphere and so the inner product falls back to the cosine distance.", "labels": [], "entities": []}, {"text": "This hence solves the inconsistence between the embedding and the distance measurement.", "labels": [], "entities": []}, {"text": "To respect the normalization constraint on word vectors, the linear transform in the bilingual projection has to be constrained as an orthogonal transform.", "labels": [], "entities": []}, {"text": "Finally, the cosine distance is used when we train the orthogonal transform, in order to achieve full consistence.", "labels": [], "entities": []}], "datasetContent": [{"text": "We first present the data profile and configurations used to learn monolingual word vectors, and then examine the learning quality on the word similarity task.", "labels": [], "entities": []}, {"text": "Finally, a comparative study is reported on the bilingual word translation task, with Mikolov's linear transform and the orthogonal transform proposed in this paper.", "labels": [], "entities": [{"text": "bilingual word translation task", "start_pos": 48, "end_pos": 79, "type": "TASK", "confidence": 0.6822628229856491}]}], "tableCaptions": [{"text": " Table 1: Performance on word translation with unnor- malized embedding and linear transform. 'D-EN' and  'D-ES' denote the dimensions of the English and Spanish  vector spaces, respectively.", "labels": [], "entities": [{"text": "word translation", "start_pos": 25, "end_pos": 41, "type": "TASK", "confidence": 0.7541903257369995}]}, {"text": " Table 2.  It can be seen that the results with the orthogonal", "labels": [], "entities": []}]}