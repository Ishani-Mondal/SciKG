{"title": [{"text": "Multilingual Open Relation Extraction Using Cross-lingual Projection", "labels": [], "entities": [{"text": "Multilingual Open Relation Extraction", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.7381861209869385}]}], "abstractContent": [{"text": "Open domain relation extraction systems identify relation and argument phrases in a sentence without relying on any underlying schema.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 12, "end_pos": 31, "type": "TASK", "confidence": 0.8048412203788757}]}, {"text": "However, current state-of-the-art relation extraction systems are available only for English because of their heavy reliance on linguistic tools such as part-of-speech tag-gers and dependency parsers.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 34, "end_pos": 53, "type": "TASK", "confidence": 0.7997922301292419}]}, {"text": "We present a cross-lingual annotation projection method for language independent relation extraction.", "labels": [], "entities": [{"text": "language independent relation extraction", "start_pos": 60, "end_pos": 100, "type": "TASK", "confidence": 0.6001546084880829}]}, {"text": "We evaluate our method on a manually annotated test set and present results on three typolog-ically different languages.", "labels": [], "entities": []}, {"text": "We release these manual annotations and extracted relations in ten languages from Wikipedia.", "labels": [], "entities": []}], "introductionContent": [{"text": "Relation extraction (RE) is the task of assigning a semantic relationship between a pair of arguments.", "labels": [], "entities": [{"text": "Relation extraction (RE)", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.885406756401062}, {"text": "assigning a semantic relationship between a pair of arguments", "start_pos": 40, "end_pos": 101, "type": "TASK", "confidence": 0.667359881930881}]}, {"text": "The two major types of RE are closed domain and open domain RE.", "labels": [], "entities": []}, {"text": "While closed-domain RE systems () consider only a closed set of relationships between two arguments, open domain systems () use an arbitrary phrase to specify a relationship.", "labels": [], "entities": []}, {"text": "In this paper, we focus on open-domain RE for multiple languages.", "labels": [], "entities": [{"text": "RE", "start_pos": 39, "end_pos": 41, "type": "TASK", "confidence": 0.830377995967865}]}, {"text": "Although there are advantages to closed domain RE (, it is expensive to construct a closed set of relation types which would be meaningful across multiple languages.", "labels": [], "entities": []}, {"text": "Open RE systems extract patterns from sentences in a given language to identify relations.", "labels": [], "entities": []}, {"text": "For learning these patterns, the sentences are analyzed using apart of speech tagger, a dependency parser and possibly a named-entity recognizer.", "labels": [], "entities": []}, {"text": "In languages other than English, these tools are either unavailable or not accurate enough to be used.", "labels": [], "entities": []}, {"text": "In comparison, it is easier to obtain parallel bilingual corpora which can be used to build machine translation systems.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 92, "end_pos": 111, "type": "TASK", "confidence": 0.7058506160974503}]}, {"text": "In this paper, we present a system that performs RE on a sentence in a source language by first translating the sentence to English, performing RE in English, and finally projecting the relation phrase back to the source language sentence.", "labels": [], "entities": [{"text": "RE", "start_pos": 49, "end_pos": 51, "type": "TASK", "confidence": 0.930653989315033}]}, {"text": "Our system assumes the availability of a machine translation system from a source language to English and an open RE system in English but no any other analysis tool in the source language.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 41, "end_pos": 60, "type": "TASK", "confidence": 0.7300437092781067}]}, {"text": "The main contributions of this work are: \u2022 A pipeline to develop relation extraction system for any source language.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 65, "end_pos": 84, "type": "TASK", "confidence": 0.8514081537723541}]}, {"text": "\u2022 Extracted open relations in ten languages based on Wikipedia corpus.", "labels": [], "entities": [{"text": "Wikipedia corpus", "start_pos": 53, "end_pos": 69, "type": "DATASET", "confidence": 0.9470078349113464}]}, {"text": "\u2022 Manual judgements for the projected relations in three languages.", "labels": [], "entities": []}, {"text": "We first describe our methodology for language independent cross-lingual projection of extracted relations ( \u00a72) followed by the relation annotation procedure and the results ( \u00a73).", "labels": [], "entities": [{"text": "cross-lingual projection of extracted relations", "start_pos": 59, "end_pos": 106, "type": "TASK", "confidence": 0.8106580138206482}]}, {"text": "The manually annotated relations in 3 languages and the automatically extracted relations in 10 languages are available at: http://cs.cmu.edu/ \u02dc mfaruqui/ soft.html.", "labels": [], "entities": []}], "datasetContent": [{"text": "Evaluation for open relations is a difficult task with no standard evaluation datasets.", "labels": [], "entities": []}, {"text": "We first describe the construction of our multilingual relation extraction dataset and then present the experiments.", "labels": [], "entities": [{"text": "multilingual relation extraction", "start_pos": 42, "end_pos": 74, "type": "TASK", "confidence": 0.6387231647968292}]}, {"text": "The current approach to evaluation for open relations) is to extract relations from a sentence and manually annotate each relation as either valid (1) or invalid (0) for the sentence.", "labels": [], "entities": []}, {"text": "For example, in the sentence: \"Michelle Obama, wife of Barack Obama was born in Chicago\", the following are possible annotations: a) (Michelle Obama; born in; Chicago): 1, b) (Barack Obama; born in; Chicago): 0.", "labels": [], "entities": []}, {"text": "Such binary annotations are not available for languages apart from English.", "labels": [], "entities": []}, {"text": "Furthermore, a binary 1/0 label is a coarse annotation that could unfairly penalize an extracted relation which has the correct semantics but is slightly ungrammatical.", "labels": [], "entities": []}, {"text": "This could occur either when prepositions are dropped from the relation phrase or when there is an ambiguity in the boundary of the relation phrase.", "labels": [], "entities": []}, {"text": "Therefore to evaluate our multilingual relation extraction framework, we obtained annotations from professional linguists for three typologically different languages: French, Hindi, and Russian.", "labels": [], "entities": [{"text": "multilingual relation extraction", "start_pos": 26, "end_pos": 58, "type": "TASK", "confidence": 0.6082626680533091}]}, {"text": "The annotation task is as follows: Given a sentence and a pair of arguments (extracted automatically from the sentence), the annotator identifies the most relevant contiguous relation phrase from the sentence that establishes a plausible connection between the two arguments.", "labels": [], "entities": []}, {"text": "If there is no meaningful contiguous relation phrase between the two arguments, the arguments are considered invalid and hence, the extracted relation tuple from the sentence is considered incorrect.", "labels": [], "entities": []}, {"text": "Given the human annotated relation phrase and the automatically extracted relation phrase, we can measure the similarity between the two, thus alleviating the problem of coarse annotation in binary judgments.", "labels": [], "entities": []}, {"text": "For evaluation, we first report the percentage of valid arguments.", "labels": [], "entities": []}, {"text": "Then for sentences with valid arguments, we use smoothed sentence-level BLEU score (max n-gram order = 3) to measure the similarity of the automatically extracted relation relative to the human annotated relation.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 72, "end_pos": 82, "type": "METRIC", "confidence": 0.9822487235069275}]}, {"text": "We extracted relations from the entire Wikipedia 6 corpus in Russian, French and Hindi from all sentences whose lengths are in the range of 10 \u2212 30 words.", "labels": [], "entities": [{"text": "Wikipedia 6 corpus", "start_pos": 39, "end_pos": 57, "type": "DATASET", "confidence": 0.8154887954394022}]}, {"text": "We randomly selected 1, 000 relations for each of these languages and annotated them.", "labels": [], "entities": []}, {"text": "The results are shown in   followed by Hindi and Russian (64.0%).", "labels": [], "entities": []}, {"text": "Surprisingly, Russian obtains the lowest percentage of valid relations but has the highest BLEU score between the automatic and the human extracted relations.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 91, "end_pos": 101, "type": "METRIC", "confidence": 0.983663409948349}]}, {"text": "This could be attributed to the fact that the average relation length (in number of words) is the shortest for Russian.", "labels": [], "entities": []}, {"text": "From table 1, we observe that the length of the relation phrase is inversely correlated with the BLEU score.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 97, "end_pos": 107, "type": "METRIC", "confidence": 0.9796658456325531}]}, {"text": "shows the distribution of the number of extracted relations across bins of similar BLEU scores.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 83, "end_pos": 87, "type": "METRIC", "confidence": 0.9931354522705078}]}, {"text": "Interestingly, the highest BLEU score bin   all three languages.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 27, "end_pos": 37, "type": "METRIC", "confidence": 0.9764239490032196}]}, {"text": "This is an encouraging result since it implies that the majority of the extracted relation phrases are identical to the manually annotated relations.", "labels": [], "entities": []}, {"text": "lists the sizes of automatically extracted relations on 10 different languages from Wikipedia that we are going to make publicly available.", "labels": [], "entities": []}, {"text": "These were selected to include a mixture of high-resource, low-resource, and typologically different languages.", "labels": [], "entities": []}, {"text": "shows examples of randomly selected relations in different languages along with their English translations.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Examples of extracted relations in different languages with English translations (Hindi is transliterated).", "labels": [], "entities": []}, {"text": " Table 1: % of valid relations and BLEU score of the ex- tracted relations across languages with the average rela- tion phrase length (in words).", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 35, "end_pos": 45, "type": "METRIC", "confidence": 0.9752912521362305}, {"text": "rela- tion phrase length", "start_pos": 109, "end_pos": 133, "type": "METRIC", "confidence": 0.8184794545173645}]}]}