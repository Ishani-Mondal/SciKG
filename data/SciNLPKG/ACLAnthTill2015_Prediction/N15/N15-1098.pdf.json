{"title": [{"text": "Do Supervised Distributional Methods Really Learn Lexical Inference Relations?", "labels": [], "entities": [{"text": "Lexical Inference Relations", "start_pos": 50, "end_pos": 77, "type": "TASK", "confidence": 0.6413064400355021}]}], "abstractContent": [{"text": "Distributional representations of words have been recently used in supervised settings for recognizing lexical inference relations between word pairs, such as hypernymy and en-tailment.", "labels": [], "entities": []}, {"text": "We investigate a collection of these state-of-the-art methods, and show that they do not actually learn a relation between two words.", "labels": [], "entities": []}, {"text": "Instead, they learn an independent property of a single word in the pair: whether that word is a \"prototypical hypernym\".", "labels": [], "entities": []}], "introductionContent": [{"text": "Inference in language involves recognizing inference relations between two words (x and y), such as causality (f lu \u2192 f ever), hypernymy (cat \u2192 animal), and other notions of lexical entailment.", "labels": [], "entities": []}, {"text": "The distributional approach to automatically recognize these relations relies on representing each word x as a vector x of contextual features: other words that tend to appear in its vicinity.", "labels": [], "entities": []}, {"text": "Such features are typically used in word similarity tasks, where cosine similarity is a standard similarity measure between two word vectors: sim(x, y) = cos( x, y).", "labels": [], "entities": [{"text": "word similarity tasks", "start_pos": 36, "end_pos": 57, "type": "TASK", "confidence": 0.7573425670464834}]}, {"text": "Many unsupervised distributional methods of recognizing lexical inference replace cosine similarity with an asymmetric similarity function).", "labels": [], "entities": [{"text": "recognizing lexical inference", "start_pos": 44, "end_pos": 73, "type": "TASK", "confidence": 0.856277068456014}]}, {"text": "Supervised methods, reported to perform better, try to learn the asymmetric operator from a training set.", "labels": [], "entities": []}, {"text": "The various supervised methods differ by the way they represent each candidate pair of words (x, y): use concatenation x \u2295 y, others) take the vectors' difference y \u2212 x, and more sophisticated representations, based on contextual features, have also been tested (.", "labels": [], "entities": []}, {"text": "In this paper, we argue that these supervised methods do not, in fact, learn to recognize lexical inference.", "labels": [], "entities": []}, {"text": "Our experiments reveal that much of their previously perceived success stems from lexical memorizing.", "labels": [], "entities": []}, {"text": "Further experiments show that these supervised methods learn whether y is a \"prototypical hypernym\" (i.e. a category), regardless of x, rather than learning a concrete relation between x and y.", "labels": [], "entities": []}, {"text": "Our mathematical analysis reveals that said methods ignore the interaction between x and y, explaining our empirical findings.", "labels": [], "entities": []}, {"text": "We modify them accordingly by incorporating the similarity between x and y.", "labels": [], "entities": []}, {"text": "Unfortunately, the improvement in performance is incremental.", "labels": [], "entities": []}, {"text": "We suspect that methods based solely on contextual features of single words are not learning lexical inference relations because contextual features might lack the necessary information to deduce how one word relates to another.", "labels": [], "entities": []}], "datasetContent": [{"text": "Due to various differences (e.g. corpora, train/test splits), we do not list previously reported results, but apply a large space of state-of-the-art supervised methods and review them comparatively.", "labels": [], "entities": []}, {"text": "We observe similar trends to previously published results, and make the dataset splits available for replication.: Datasets evaluated in this work.", "labels": [], "entities": []}, {"text": "We used 5 labeled datasets for evaluation.", "labels": [], "entities": []}, {"text": "Each dataset entry contains two words (x, y) and a label whether x entails y.", "labels": [], "entities": []}, {"text": "Note that each dataset was created with a slightly different goal in mind, affecting word-pair generation and annotation.", "labels": [], "entities": [{"text": "word-pair generation", "start_pos": 85, "end_pos": 105, "type": "TASK", "confidence": 0.7186640053987503}]}, {"text": "For example, Following Caron (2001), we used the square root of the eigenvalue matrix \u03a3 k for representing words: both of Baroni's datasets are designed to capture hypernyms, while other datasets try to capture broader notions of lexical inference (e.g. causality).", "labels": [], "entities": []}, {"text": "provides metadata on each dataset, and the description below explains how each one was created.", "labels": [], "entities": []}, {"text": "( ) Based on manually annotated entailment graphs of subject-verb-object tuples (propositions).", "labels": [], "entities": []}, {"text": "Noun entailments were extracted from entailing tuples that were identical except for one of the arguments, thus propagating the existence/absence of proposition-level entailment to the noun level.", "labels": [], "entities": [{"text": "Noun entailments", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.762210339307785}]}, {"text": "This dataset is the most realistic dataset, since the original entailment annotations were made in the context of a complete proposition.", "labels": [], "entities": []}, {"text": "The following experiment shows that supervised methods with contextual features are indeed memorizing words from the training set.", "labels": [], "entities": []}, {"text": "We randomly split each dataset into 70% train, 5% validation, and 25% test, and train lexical-feature classifiers, using a one-hot vector representation of y as input features.", "labels": [], "entities": []}, {"text": "By definition, these classifiers memorize words from the training set.", "labels": [], "entities": []}, {"text": "We then add contextual-features (as described in \u00a72.1), on top of the lexical features, and train classifiers analogously.", "labels": [], "entities": []}, {"text": "compares the best lexical-and contextual-feature classifiers on each dataset.", "labels": [], "entities": []}, {"text": "The performance difference is under 10 points in the larger datasets, showing that much of the contextual-feature classifiers' success is due to lexical memorization.", "labels": [], "entities": []}, {"text": "Similar findings were also reported by and, supporting our memorization argument.", "labels": [], "entities": []}, {"text": "To prevent lexical memorization in our following experiments, we split each dataset into train and test sets with zero lexical overlap.", "labels": [], "entities": [{"text": "lexical memorization", "start_pos": 11, "end_pos": 31, "type": "TASK", "confidence": 0.8266740441322327}]}, {"text": "We do this by randomly splitting the vocabulary into \"train\" and \"test\" words, and extract train-only and test-only subsets of each dataset accordingly.", "labels": [], "entities": []}, {"text": "About half of each original dataset contains \"mixed\" examples (one train-word and one test-word); these are discarded.: A comparison of each dataset's best supervised method with: (a) the best result using only y composition; (b) unsupervised cosine similarity cos( x, y).", "labels": [], "entities": []}, {"text": "Performance is measured by F 1 . Uses lexical train/test splits.", "labels": [], "entities": [{"text": "F 1", "start_pos": 27, "end_pos": 30, "type": "METRIC", "confidence": 0.9841353595256805}]}, {"text": "Supervised vs Unsupervised While supervised methods were reported to perform better than unsupervised ones, this is not always the case.", "labels": [], "entities": []}, {"text": "As a baseline, we measured the \"vanilla\" cosine similarity of x and y, tuning a threshold with the validation set.", "labels": [], "entities": []}, {"text": "This unsupervised symmetric method outperforms all supervised methods in 2 out of 5 datasets.", "labels": [], "entities": []}, {"text": "Ignoring x's Information We compared the performance of only y to that of the best configuration in each dataset).", "labels": [], "entities": []}, {"text": "In 4 out of 5 datasets, the difference in performance is less than 5 points.", "labels": [], "entities": []}, {"text": "This means that the classifiers are ignoring most of the information in x.", "labels": [], "entities": []}, {"text": "Furthermore, they might be overlooking the compatibility (or incompatibility) of x toy.", "labels": [], "entities": [{"text": "compatibility", "start_pos": 43, "end_pos": 56, "type": "METRIC", "confidence": 0.9627213478088379}]}, {"text": "reported a similar result, but did not address the fundamental question it beckons: if the classifier cannot capture a relation between x and y, then what is it learning?", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Datasets evaluated in this work.", "labels": [], "entities": []}, {"text": " Table 2: The performance (F 1 ) of lexical versus contex- tual feature classifiers on a random train/test split with  lexical overlap.", "labels": [], "entities": [{"text": "F 1 )", "start_pos": 27, "end_pos": 32, "type": "METRIC", "confidence": 0.9784499605496725}]}, {"text": " Table 3: A comparison of each dataset's best supervised  method with: (a) the best result using only y composi- tion; (b) unsupervised cosine similarity cos( x, y). Perfor- mance is measured by F 1 . Uses lexical train/test splits.", "labels": [], "entities": [{"text": "F", "start_pos": 195, "end_pos": 196, "type": "METRIC", "confidence": 0.9747495651245117}]}, {"text": " Table 5: Performance (F 1 ) of SVM across kernels. LIN  refers to the linear kernel (equations", "labels": [], "entities": [{"text": "F 1 )", "start_pos": 23, "end_pos": 28, "type": "METRIC", "confidence": 0.9550865888595581}, {"text": "LIN", "start_pos": 52, "end_pos": 55, "type": "METRIC", "confidence": 0.9395932555198669}]}]}