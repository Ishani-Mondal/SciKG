{"title": [{"text": "Movie Script Summarization as Graph-based Scene Extraction", "labels": [], "entities": [{"text": "Graph-based Scene Extraction", "start_pos": 30, "end_pos": 58, "type": "TASK", "confidence": 0.605182965596517}]}], "abstractContent": [{"text": "In this paper we study the task of movie script summarization, which we argue could enhance script browsing, give readers a rough idea of the script's plotline, and speedup reading time.", "labels": [], "entities": [{"text": "movie script summarization", "start_pos": 35, "end_pos": 61, "type": "TASK", "confidence": 0.5871588190396627}]}, {"text": "We formalize the process of generating a shorter version of a screenplay as the task of finding an optimal chain of scenes.", "labels": [], "entities": []}, {"text": "We develop a graph-based model that selects a chain by jointly optimizing its logical progression , diversity, and importance.", "labels": [], "entities": []}, {"text": "Human evaluation based on a question-answering task shows that our model produces summaries which are more informative compared to competitive baselines.", "labels": [], "entities": []}], "introductionContent": [{"text": "Each year, about 50,000 screenplays are registered with the WGA 1 , the Writers Guild of America.", "labels": [], "entities": [{"text": "WGA 1", "start_pos": 60, "end_pos": 65, "type": "DATASET", "confidence": 0.6836246103048325}]}, {"text": "Only a fraction of these make it through to be considered for production and an even smaller fraction to the big screen.", "labels": [], "entities": []}, {"text": "How do producers and directors navigate through this vast number of scripts available?", "labels": [], "entities": []}, {"text": "Typically, production companies, agencies, and studios hire script readers, whose job is to analyze screenplays that come in, sorting the hopeful from the hopeless.", "labels": [], "entities": []}, {"text": "Having read the script, a reader will generate a coverage report consisting of a logline (one or two sentences describing the story in a nutshell), a synopsis (a two-to three-page long summary of the script), comments explaining its appeal or problematic aspects, and a final verdict as to whether the script merits further consideration.", "labels": [], "entities": []}, {"text": "A script excerpt The WGA is a collective term representing US TV and film writers.  from \"Silence of the Lambs\", an American thriller released in 1991, is shown in.", "labels": [], "entities": []}, {"text": "Although there are several screenwriting tools for authors (e.g., Final Draft is a popular application which automatically formats scripts to industry standards, keeps track of revisions, allows insertion of notes, and writing collaboratively online), there is alack of any kind of script reading aids.", "labels": [], "entities": []}, {"text": "Features of such a tool could be to automatically grade the quality of the script (e.g., thumbs up or down), generate synopses and loglines, identify main characters and their stories, or facilitate browsing (e.g., \"show me every scene where there is a shooting\").", "labels": [], "entities": []}, {"text": "In this paper we explore whether current NLP technology can be used to address some of these tasks.", "labels": [], "entities": []}, {"text": "Specifically, we focus on script summarization, which we conceptualize as the process of generating a shorter version of a screenplay, ideally encapsulating its most informative scenes.", "labels": [], "entities": [{"text": "script summarization", "start_pos": 26, "end_pos": 46, "type": "TASK", "confidence": 0.7452273666858673}]}, {"text": "The resulting summaries can be used to enhance script browsing, give readers a rough idea of the script's content and plotline, and speedup reading time.", "labels": [], "entities": []}, {"text": "So, what makes a good script summary?", "labels": [], "entities": [{"text": "script summary", "start_pos": 22, "end_pos": 36, "type": "TASK", "confidence": 0.6890232115983963}]}, {"text": "According to modern film theory, \"all films are about nothing -nothing but character\".", "labels": [], "entities": []}, {"text": "Beyond characters, a summary should also highlight major scenes representative of the story and its progression.", "labels": [], "entities": []}, {"text": "With this in mind, we define a script summary as a chain of scenes which conveys a narrative and smooth transitions from one scene to the next.", "labels": [], "entities": []}, {"text": "At the same time, a good chain should incorporate some diversity (i.e., avoid redundancy), and focus on important scenes and characters.", "labels": [], "entities": []}, {"text": "We formalize the problem of selecting a good summary chain using a graph-theoretic approach.", "labels": [], "entities": []}, {"text": "We represent scripts as (directed) bipartite graphs with vertices corresponding to scenes and characters, and edge weights to their strength of correlation.", "labels": [], "entities": []}, {"text": "Intuitively, if two scenes are connected, a random walk starting from one would reach the other frequently.", "labels": [], "entities": []}, {"text": "We find a chain of highly connected scenes by jointly optimizing logical progression, diversity, and importance.", "labels": [], "entities": []}, {"text": "Our contributions in this work are three-fold: we introduce a novel summarization task, on anew text genre, and formalize scene selection as the problem of finding a chain that represents a film's story; we propose several novel methods for analyzing script content (e.g., identifying important characters and their interactions); and perform a large-scale human evaluation study using a question-answering task.", "labels": [], "entities": []}, {"text": "Experimental results show that our method produces summaries which are more informative compared to several competitive baselines.", "labels": [], "entities": []}], "datasetContent": [{"text": "Gold Standard Chains The development and tuning of the chain extraction model presented in Section 4 necessitates access to a gold standard of key scene chains representing the movie's most important content.", "labels": [], "entities": [{"text": "chain extraction", "start_pos": 55, "end_pos": 71, "type": "TASK", "confidence": 0.7378292679786682}]}, {"text": "Our experiments concentrated on a sample of 95 movies (comedies and thrillers) from the ScriptBase corpus (Section 3).", "labels": [], "entities": [{"text": "ScriptBase corpus", "start_pos": 88, "end_pos": 105, "type": "DATASET", "confidence": 0.8043428361415863}]}, {"text": "Performing the scene selection task for such a big corpus manually would be both time consuming and costly.", "labels": [], "entities": [{"text": "scene selection task", "start_pos": 15, "end_pos": 35, "type": "TASK", "confidence": 0.7843510309855143}]}, {"text": "Instead, we used distant supervision based on Wikipedia to automatically generate a gold standard.", "labels": [], "entities": []}, {"text": "Specifically, we assume that Wikipedia plots are representative of the most important content in a movie.", "labels": [], "entities": []}, {"text": "Using the alignment algorithm presented in, we align script sentences to Wikipedia plot sentences and assume that scenes with at least one alignment are part of the gold chain of scenes.", "labels": [], "entities": []}, {"text": "We obtain many-to-many alignments using features such as lemma overlap and word stem similarity.", "labels": [], "entities": []}, {"text": "When evaluated on four movies 8 (from the training set) whose content was manually aligned to Wikipedia plots, the aligner achieved a precision of .53 at a recall rate of .82 at deciding whether a scene should be aligned.", "labels": [], "entities": [{"text": "precision", "start_pos": 134, "end_pos": 143, "type": "METRIC", "confidence": 0.9989631175994873}, {"text": "recall rate", "start_pos": 156, "end_pos": 167, "type": "METRIC", "confidence": 0.9829342067241669}]}, {"text": "Scenes are ranked according to the number of alignments they contain.", "labels": [], "entities": []}, {"text": "When creating gold chains at different compression rates, we start with the best-ranked scenes and then successively add lower ranked ones until we reach the desired compression rate.", "labels": [], "entities": []}, {"text": "System Comparison In our experiments we compared our scene extraction model (SceneSum) against three baselines.", "labels": [], "entities": [{"text": "scene extraction", "start_pos": 53, "end_pos": 69, "type": "TASK", "confidence": 0.7112472504377365}]}, {"text": "The first baseline was based on the minimum overlap (MinOv) of characters in consecutive scenes and corresponds closely to the diversity term in our objective.", "labels": [], "entities": [{"text": "minimum overlap (MinOv)", "start_pos": 36, "end_pos": 59, "type": "METRIC", "confidence": 0.820369827747345}]}, {"text": "The second baseline was based on the maximum overlap (MaxOv) of characters and approximates the importance term in our objective.", "labels": [], "entities": [{"text": "maximum overlap (MaxOv)", "start_pos": 37, "end_pos": 60, "type": "METRIC", "confidence": 0.7152733981609345}]}, {"text": "The third baseline selects scenes at random (averaged over 1,000 runs).", "labels": [], "entities": []}, {"text": "Parameters for our models were tuned on the training set, weights for the terms in the objective were optimized to the following values: \u03bb P = 1.0, \u03bb D = 0.3, and \u03bb I = 0.1.", "labels": [], "entities": [{"text": "Parameters", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9583258032798767}]}, {"text": "We set the restart probability of our random walker to \u03b5 = 0.5, and the sigmoid scaling factor in our diversity term to k = \u22121.2.", "labels": [], "entities": [{"text": "restart probability", "start_pos": 11, "end_pos": 30, "type": "METRIC", "confidence": 0.9809278249740601}]}, {"text": "Evaluation We assessed the output of our model (and comparison systems) automatically against the gold chains described above.", "labels": [], "entities": []}, {"text": "We performed experiments with compression rates in the range of 10% to 50% and measured performance in terms of F1.", "labels": [], "entities": [{"text": "compression", "start_pos": 30, "end_pos": 41, "type": "METRIC", "confidence": 0.9654496908187866}, {"text": "F1", "start_pos": 112, "end_pos": 114, "type": "METRIC", "confidence": 0.999580442905426}]}, {"text": "In addition, we also evaluated the quality of the extracted scenes as perceived by humans, which is necessary, given the approximate nature of our gold standard.", "labels": [], "entities": []}, {"text": "We adopted a question-answering (Q&A) evaluation paradigm which has been used previously to evaluate summaries and document compression.", "labels": [], "entities": [{"text": "summaries", "start_pos": 101, "end_pos": 110, "type": "TASK", "confidence": 0.9789186120033264}, {"text": "document compression", "start_pos": 115, "end_pos": 135, "type": "TASK", "confidence": 0.7080756723880768}]}, {"text": "Under the assumption that the summary is to function as a replacement for the full script, we can measure the extent to which it can be used to find answers to questions which have been derived from the entire script and are representative of its core content.", "labels": [], "entities": []}, {"text": "The more questions a hypothetical system can answer, the better it is at summarizing the script as a whole.", "labels": [], "entities": [{"text": "summarizing the script", "start_pos": 73, "end_pos": 95, "type": "TASK", "confidence": 0.8610213398933411}]}, {"text": "Two annotators were independently instructed to read scripts (from our test set) and create Q&A pairs.", "labels": [], "entities": []}, {"text": "The annotators generated questions relating to the plot of the movie and the development of its characters, requiring an unambiguous answer.", "labels": [], "entities": []}, {"text": "They compared and revised their Q&A pairs until a common agreed-upon set of five questions per movie was reached (see for an example).", "labels": [], "entities": []}, {"text": "In addition, for every movie we asked subjects to name the main characters, and summarize its plot (in no more than four sentences).", "labels": [], "entities": []}, {"text": "Using Amazon Mechanical Turk (AMT) 9 , we elicited answers for eight scripts (four comedies and thrillers) in four summarization con-  ditions: using our model, the two baselines based on minimum and maximum character overlap, and the random system.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk (AMT) 9", "start_pos": 6, "end_pos": 36, "type": "DATASET", "confidence": 0.8664873923574176}]}, {"text": "All models were assessed at the same compression rate of 20% which seems realistic in an actual application environment, e.g., computer aided summarization.", "labels": [], "entities": [{"text": "compression rate", "start_pos": 37, "end_pos": 53, "type": "METRIC", "confidence": 0.9477709531784058}]}, {"text": "The scripts were preselected in an earlier AMT study where participants were asked to declare whether they had seen the movies in our test set (65 in total).", "labels": [], "entities": [{"text": "AMT", "start_pos": 43, "end_pos": 46, "type": "TASK", "confidence": 0.7822914123535156}]}, {"text": "We chose the screenplays which had received the least viewings so as to avoid eliciting answers based on familiarity with the movie.", "labels": [], "entities": []}, {"text": "A total of 29 participants, all self-reported native English speakers, completed the Q&A task.", "labels": [], "entities": [{"text": "Q&A task", "start_pos": 85, "end_pos": 93, "type": "TASK", "confidence": 0.9321959018707275}]}, {"text": "The answers provided by the subjects were scored against an answer key.", "labels": [], "entities": []}, {"text": "A correct answer was marked with a score of one, and zero otherwise.", "labels": [], "entities": []}, {"text": "In cases where more answers were required per question, partial scores were awarded to each correct answer (e.g., 0.5).", "labels": [], "entities": []}, {"text": "The score fora summary is the average of its question scores.", "labels": [], "entities": []}, {"text": "shows the performance of SceneSum, our scene extraction model, and the three comparison systems (MaxOv, MinOv, Random) on the automatic gold standard at five compression rates.", "labels": [], "entities": [{"text": "scene extraction", "start_pos": 39, "end_pos": 55, "type": "TASK", "confidence": 0.7764328122138977}]}, {"text": "As can be seen, MaxOv performs best in terms of F1, followed by SceneSum.", "labels": [], "entities": [{"text": "F1", "start_pos": 48, "end_pos": 50, "type": "METRIC", "confidence": 0.9990334510803223}, {"text": "SceneSum", "start_pos": 64, "end_pos": 72, "type": "DATASET", "confidence": 0.9234103560447693}]}, {"text": "We believe this is an artifact due to the way the gold standard was created.", "labels": [], "entities": [{"text": "gold standard", "start_pos": 50, "end_pos": 63, "type": "DATASET", "confidence": 0.8417420983314514}]}, {"text": "Scenes with large numbers of main characters are more likely to figure in Wikipedia plot summaries and will thus be more frequently aligned.", "labels": [], "entities": []}, {"text": "A chain based on maximum character overlap will focus on such scenes and will agree with the gold standard better compared to chains which take additional script properties into account.", "labels": [], "entities": []}, {"text": "We further analyzed the scenes selected by SceneSum and the comparison systems with respect to their position in the script.", "labels": [], "entities": [{"text": "SceneSum", "start_pos": 43, "end_pos": 51, "type": "DATASET", "confidence": 0.9067006707191467}]}, {"text": "erage percentage of scenes selected from the beginning, middle, and end of the movie (based on an equal division of the number of scenes in the screenplay).", "labels": [], "entities": [{"text": "erage", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9328892230987549}]}, {"text": "As can be seen, the number of selected scenes tends to be evenly distributed across the entire movie.", "labels": [], "entities": []}, {"text": "SceneSum has a slight bias towards the beginning of the movie which is probably natural, since leading characters appear early on, as well as important scenes introducing essential story elements (e.g., setting, points of view).", "labels": [], "entities": [{"text": "SceneSum", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.8751771450042725}]}], "tableCaptions": [{"text": " Table 2: Model performance on automatically generated  gold standard (test set) at different compression rates.", "labels": [], "entities": []}, {"text": " Table 3: Average percentage of scenes taken from the  beginning, middle and ends of movies, on automatic gold  standard test set.", "labels": [], "entities": [{"text": "automatic gold  standard test set", "start_pos": 96, "end_pos": 129, "type": "DATASET", "confidence": 0.6427207112312316}]}, {"text": " Table 4: Percentage of questions answered correctly.", "labels": [], "entities": [{"text": "Percentage", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9725613594055176}]}]}